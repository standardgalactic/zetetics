SOCIETY
ETHICS,
AI SAFETY,
INTRODUCTION
TO
DAN HENDRYCKS
AND

INTRODUCTION TO AI SAFETY,
ETHICS, AND SOCIETY
DAN HENDRYCKS


4

Contents
I
Background
13
1
Overview of Catastrophic AI Risks
15
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.2
Malicious Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.2.1
Bioterrorism
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
1.2.2
Unleashing AI Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.2.3
Persuasive AIs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.2.4
Concentration of Power
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.3
AI Race . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.3.1
Military AI Arms Race
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
1.3.2
Corporate AI Race . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.3.3
Evolutionary Pressures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
1.4
Organizational Risks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
1.4.1
Accidents Are Hard to Avoid
. . . . . . . . . . . . . . . . . . . . . . . . . . .
37
1.4.2
Organizational Factors can Reduce the Chances of Catastrophe . . . . . . . .
38
1.5
Rogue AIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
1.5.1
Proxy Gaming
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
1.5.2
Goal Drift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
1.5.3
Power-Seeking
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
1.5.4
Deception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
1.6
Discussion of Connections Between Risks
. . . . . . . . . . . . . . . . . . . . . . . .
51
1.7
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
1.8
Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
1.8.1
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
2
Artificial Intelligence Fundamentals
58
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
2.2
Artificial Intelligence & Machine Learning . . . . . . . . . . . . . . . . . . . . . . . .
59
2.2.1
Artificial Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
2.2.2
Types of AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
2.2.3
Machine Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
2.2.4
Types of Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
2.3
Deep Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
2.3.1
Model Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
2.3.2
Training and Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
2.3.3
History and Timeline of Key Architectures
. . . . . . . . . . . . . . . . . . .
100
2.3.4
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
2.4
Scaling Laws
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
2.4.1
Scaling Laws in Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . .
105
2.5
Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
2.5.1
Learning Sequential Decision Making through Trial and Error . . . . . . . . .
109
2.5.2
The Reinforcement Learning Problem
. . . . . . . . . . . . . . . . . . . . . .
110
2.5.3
Methods for Solving RL Problems
. . . . . . . . . . . . . . . . . . . . . . . .
115
2.5.4
Deep Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
2.5.5
Examples of RL
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
2.6
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
2.6.1
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
5

6
CONTENTS
2.6.2
Key Takeaways . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
2.7
Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
2.7.1
Recommended Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
2.7.2
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
II
Safety
129
3
Single-Agent Safety
131
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
3.2
Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
3.2.1
Sources of Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
3.2.2
Strategies for Bias Mitigation . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
3.3
Opaqueness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
3.3.1
ML Systems are Opaque . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
3.3.2
Motivations for Transparency Research
. . . . . . . . . . . . . . . . . . . . .
140
3.3.3
Approaches to Transparency
. . . . . . . . . . . . . . . . . . . . . . . . . . .
141
3.4
Proxies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
3.4.1
Proxies in Machine Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . .
145
3.4.2
Proxy Gaming
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
3.4.3
Adversarial Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
3.4.4
Tail Risk: AI Evaluator Gaming
. . . . . . . . . . . . . . . . . . . . . . . . .
156
3.5
Power . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158
3.5.1
What is Power? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158
3.5.2
People Could Enlist AIs for Power Seeking
. . . . . . . . . . . . . . . . . . .
160
3.5.3
Power Seeking Can Be Instrumentally Rational . . . . . . . . . . . . . . . . .
160
3.5.4
Structural Pressures Towards Power-Seeking AI . . . . . . . . . . . . . . . . .
164
3.5.5
Tail Risk: Power-Seeking Behavior . . . . . . . . . . . . . . . . . . . . . . . .
166
3.6
Emergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
3.6.1
Emergent Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
3.6.2
Emergent Goal-Directed Behavior
. . . . . . . . . . . . . . . . . . . . . . . .
169
3.6.3
Tail Risk: Emergent Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
172
3.7
Deception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
3.7.1
Examples of Deception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
3.7.2
Deceptive Evaluation Gaming . . . . . . . . . . . . . . . . . . . . . . . . . . .
176
3.7.3
Tail Risk: Deceptive Alignment and Treacherous Turns
. . . . . . . . . . . .
177
3.8
Safety and General Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
3.9
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
3.10 Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
3.10.1 Recommended Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
3.10.2 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
184
4
Safety Engineering
188
4.1
Risk Decomposition
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
4.1.1
Failure Modes, Hazards, and Threats . . . . . . . . . . . . . . . . . . . . . . .
189
4.1.2
The Classic Risk Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
4.1.3
Framing the Goal as Risk Reduction . . . . . . . . . . . . . . . . . . . . . . .
190
4.1.4
Disaster Risk Equation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
4.1.5
Elements of the Risk Equation
. . . . . . . . . . . . . . . . . . . . . . . . . .
191
4.1.6
Applying the Disaster Risk Equation . . . . . . . . . . . . . . . . . . . . . . .
192
4.2
Nines of Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193

CONTENTS
7
4.3
Safe Design Principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
4.3.1
Redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
4.3.2
Separation of Duties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
4.3.3
Principle of Least Privilege
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
4.3.4
Fail-Safes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
4.3.5
Antifragility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
4.3.6
Negative Feedback Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . .
200
4.3.7
Transparency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
4.3.8
Defense in Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
4.3.9
Review of Safe Design Principles . . . . . . . . . . . . . . . . . . . . . . . . .
202
4.4
Component Failure Accident Models and Methods
. . . . . . . . . . . . . . . . . . .
202
4.4.1
Swiss Cheese Model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
4.4.2
Bow Tie Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
4.4.3
Fault Tree Analysis Method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
4.4.4
Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
4.5
Systemic Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
4.5.1
Systemic Accident Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214
4.6
Drift into Failure and Existential Risks . . . . . . . . . . . . . . . . . . . . . . . . . .
221
4.7
Tail Events
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223
4.7.1
Tail Events Can Greatly Affect the Average Risk . . . . . . . . . . . . . . . .
225
4.7.2
Long-Tailed and Thin-Tailed Distributions
. . . . . . . . . . . . . . . . . . .
226
4.7.3
Heuristics and Models for Thinking About Long Tails and Thin Tails
. . . .
229
4.7.4
A Caricature of Long Tails and Thin Tails . . . . . . . . . . . . . . . . . . . .
234
4.8
Black Swans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
4.8.1
Known Unknowns and Unknown Unknowns . . . . . . . . . . . . . . . . . . .
236
4.8.2
Implications of Tail Events and Black Swans for Risk Analysis
. . . . . . . .
238
4.8.3
Identifying the Risk of Tail Events or Black Swans . . . . . . . . . . . . . . .
243
4.9
Reducing Risks of Tail Events and Black Swans . . . . . . . . . . . . . . . . . . . . .
244
4.10 Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
247
4.11 Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249
4.11.1 Recommended Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249
4.11.2 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249
5
Complex Systems
251
5.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
5.2
Introduction to Complex Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
5.2.1
The Reductionist Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
5.2.2
The Complex Systems Paradigm . . . . . . . . . . . . . . . . . . . . . . . . .
255
5.2.3
Deep Learning Systems as Complex Systems
. . . . . . . . . . . . . . . . . .
256
5.2.4
Complexity is Not a Dichotomy . . . . . . . . . . . . . . . . . . . . . . . . . .
257
5.2.5
The Hallmarks of Complex Systems
. . . . . . . . . . . . . . . . . . . . . . .
258
5.2.6
Social Systems as Complex Systems
. . . . . . . . . . . . . . . . . . . . . . .
268
5.3
Complex Systems for AI Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
272
5.3.1
General Lessons from Complex Systems . . . . . . . . . . . . . . . . . . . . .
272
5.3.2
Puzzles, Problems, and Wicked Problems
. . . . . . . . . . . . . . . . . . . .
275
5.3.3
Challenges With Interventionism . . . . . . . . . . . . . . . . . . . . . . . . .
277
5.3.4
Systemic Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282
5.4
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
283
5.5
Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
284
5.5.1
Recommended Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
284
5.5.2
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285

8
CONTENTS
III
Ethics
287
6
Machine Ethics
289
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289
6.2
Law
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
290
6.2.1
The Case For Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
291
6.2.2
The Need for Ethics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
293
6.3
Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
296
6.3.1
AI Fairness Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
296
6.3.2
Limitations of Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
297
6.3.3
Approaches to Improving Fairness
. . . . . . . . . . . . . . . . . . . . . . . .
299
6.4
The Economic Engine
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
300
6.4.1
The Free Market . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
301
6.4.2
Inequality & Market Failures . . . . . . . . . . . . . . . . . . . . . . . . . . .
307
6.4.3
Beyond Economic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
316
6.5
Preferences
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323
6.5.1
Revealed Preferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324
6.5.2
Stated Preferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
326
6.5.3
Idealized Preferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
6.6
Happiness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
6.6.1
The General Approach to Happiness . . . . . . . . . . . . . . . . . . . . . . .
332
6.6.2
Problems for Happiness-Focused Machine Ethics . . . . . . . . . . . . . . . .
335
6.6.3
Objective Goods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
336
6.7
Social Welfare Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
338
6.7.1
Measuring Social Welfare
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
6.8
Moral Parliament . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
346
6.8.1
Implementing a Moral Parliament
. . . . . . . . . . . . . . . . . . . . . . . .
347
6.8.2
Advantages of a Moral Parliament
. . . . . . . . . . . . . . . . . . . . . . . .
348
6.9
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
350
6.10 Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
351
6.10.1 Recommended Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
351
6.10.2 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
352
7
Normative Ethics
355
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
355
7.2
Why Learn About Ethics? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
355
7.2.1
Is Ethics “Relative?” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
356
7.2.2
Is Ethics Determined by Religion? . . . . . . . . . . . . . . . . . . . . . . . .
358
7.3
Moral Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
7.3.1
The “Goodness” of Actions and Their Consequences
. . . . . . . . . . . . . .
359
7.3.2
Constraints and Special Obligations
. . . . . . . . . . . . . . . . . . . . . . .
362
7.3.3
What does it mean for an action to be right or wrong? . . . . . . . . . . . . .
363
7.4
Moral Theories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
365
7.4.1
Utilitarianism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
366
7.4.2
Deontology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
373
7.4.3
Virtue Ethics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
380
7.4.4
Social Contract Theory
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
382
7.5
Moral Uncertainty
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
388
7.5.1
Making Decisions Under Moral Uncertainty . . . . . . . . . . . . . . . . . . .
388
7.5.2
How Should We Approach Moral Uncertainty?
. . . . . . . . . . . . . . . . .
389
7.6
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
392

CONTENTS
9
7.7
Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
393
7.7.1
Recommended Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
393
7.7.2
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
393
IV
Society
396
8
Collective Action Problems
398
8.1
Motivation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
398
8.2
Game Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
401
8.2.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
401
8.2.2
Game Theory Fundamentals
. . . . . . . . . . . . . . . . . . . . . . . . . . .
403
8.2.3
The Prisoner’s Dilemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
404
8.2.4
The Iterated Prisoner’s Dilemma . . . . . . . . . . . . . . . . . . . . . . . . .
412
8.2.5
Collective Action Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . .
420
8.2.6
Evolutionary Game Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . .
427
8.3
Evolutionary Pressures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
437
8.3.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
437
8.3.2
Generalized Darwinism
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
438
8.3.3
Formalizing Generalized Darwinism
. . . . . . . . . . . . . . . . . . . . . . .
439
8.3.4
Generalized Darwinism and AI Populations . . . . . . . . . . . . . . . . . . .
441
8.3.5
Levels of Selection and Selfish Behavior
. . . . . . . . . . . . . . . . . . . . .
443
8.3.6
Intrasystem Goal Conflict . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
445
8.3.7
Selfishness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
448
8.4
Conflict and Cooperation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
450
8.4.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
450
8.4.2
Conflict . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
452
8.4.3
Cooperation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
464
8.5
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
477
8.6
Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
478
8.6.1
Recommended Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
478
8.6.2
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
479
9
Governance
484
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
484
9.1.1
The Landscape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
484
9.2
Takeoff . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
486
9.2.1
Economic Growth
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
487
9.2.2
Timelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
488
9.2.3
Types of AI Takeoff
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
489
9.3
Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
491
9.3.1
Distribution of Power Among AIs . . . . . . . . . . . . . . . . . . . . . . . . .
491
9.3.2
Distribution of Access to AIs . . . . . . . . . . . . . . . . . . . . . . . . . . .
494
9.3.3
Distribution of Costs and Benefits of AI . . . . . . . . . . . . . . . . . . . . .
497
9.4
Compute Governance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
502
9.4.1
Compute Is Indispensable for AI Development and Deployment . . . . . . . .
502
9.4.2
Compute Is Physical, Excludable, and Quantifiable . . . . . . . . . . . . . . .
504
9.5
Corporate Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
506
9.5.1
What Is Corporate Governance?
. . . . . . . . . . . . . . . . . . . . . . . . .
506
9.5.2
Legal Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
507
9.5.3
Ownership Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
508

10
CONTENTS
9.5.4
Organizational Structure
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
508
9.5.5
Assurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
509
9.6
National Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
510
9.6.1
Standards and Regulations
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
510
9.6.2
Liability for AI Harms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
511
9.6.3
Improving Resilience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
512
9.6.4
Not Falling Behind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
513
9.7
International Governance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
514
9.7.1
Forms of International Governance . . . . . . . . . . . . . . . . . . . . . . . .
515
9.7.2
Four Questions for AI Regulation . . . . . . . . . . . . . . . . . . . . . . . . .
517
9.7.3
What Can Be Included in International Agreements? . . . . . . . . . . . . . .
519
9.8
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
522
9.9
Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
523
9.9.1
Recommended Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
523
9.9.2
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
524
V
Appendix
526
A Acknowledgements
528
B Utility Functions
529
B.1
Utility and Utility Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
529
B.1.1
Fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
529
B.1.2
Motivations for Learning About Utility Functions . . . . . . . . . . . . . . . .
530
B.2
Properties of Utility Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
530
B.2.1
Bernoulli Utility Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
531
B.2.2
Von Neumann-Morgenstern Utility Functions . . . . . . . . . . . . . . . . . .
531
B.2.3
St. Petersburg Paradox
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
535
B.3 Tail Risk: Corrigibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
537
B.4 Attitudes to Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
539
B.4.1
What Are the Different Attitudes to Risk? . . . . . . . . . . . . . . . . . . . .
540
B.4.2
Risk and Decision Making . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
541
B.5
Beyond Expected Utility Theories
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
549
B.5.1
Humans and Rationality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
550
B.5.2
Evidence Against Expected Utility Theory
. . . . . . . . . . . . . . . . . . .
550
B.5.3
Prospect Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
553
B.5.4
Models of Decision Making
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
555
B.6
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
556
B.7
Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
557
B.7.1
Recommended Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
557
B.7.2
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
557

Preface
Artificial Intelligence (AI) is rapidly embedding itself within militaries, economies, and societies,
reshaping their very foundations. Given the depth and breadth of its consequences, it has never been
more pressing to understand how to ensure that AI systems are safe, ethical, and have a positive
societal impact.
This textbook aims to provide a comprehensive approach to understanding AI risk. Our primary
goals include consolidating fragmented knowledge on AI risk, increasing the precision of core ideas
and reducing barriers to entry by making content simpler and more comprehensible. The book has
been designed to be accessible to undergraduates from diverse academic backgrounds. You do not
need to have studied AI, philosophy, or other such topics. The content is skimmable and somewhat
modular, so that you can choose which chapters to read. We introduce formulas in a few places
to specify claims more precisely, but readers with less mathematical knowledge should be able to
understand the main points without these.
A full understanding of the risks posed by AI requires knowledge in several disparate academic
disciplines, which have so far not been combined in a single text. This book was written to fill that gap.
This textbook moves beyond the confines of machine learning to provide a holistic understanding of
AI risk. We draw on well-established ideas and frameworks from the fields of engineering, economics,
biology, complex systems, philosophy and other disciplines that can provide insights into AI risks and
how to manage them. Our aim is to equip readers with a solid understanding of the technical, ethical,
and governance challenges that we will need to meet in order to harness advanced AI in a beneficial way.
In order to have a solid grasp of the challenges of AI safety, it is important to consider the broader
context within which AI systems are being developed and applied. The decisions of and interplay
between AI developers, policy-makers, militaries and other actors will play an important role in
shaping this context. Since AI influences many different spheres, we have deliberately selected
time-tested, formal frameworks to provide multiple lenses for thinking about AI, relevant actors,
and AI’s impacts. The frameworks and concepts we use are highly general and are useful for rea-
soning about various forms of intelligence, ranging from individual human beings to corporations,
states, and AI systems. While some sections of the book focus more directly on AI risks that
have already been identified and discussed today, others set out a systematic introduction to ideas
from game theory, utility theory, complex systems, ethics, and more. We hope that providing these
flexible conceptual tools will help readers to adapt robustly to the ever-changing landscape of AI risks.
This book does not aim to be the final word on all possible AI risks, as research on AI risk is still
relatively new. Rather, we aim to highlight the dynamics and frameworks that we have found highly
productive for thinking about various AI risks. Given the broad scope of the problems involved, it is
easy to become disoriented. Our hope is that this textbook provides some scaffolding for others to
use as they build out a more detailed picture of these risks and the potential responses to them.
The textbook’s content falls into four sections: Background, Safety, Ethics, and Society. In the
Background section, we outline major categories of AI risks and introduce some key features of
modern AI systems. In the Safety section, we discuss how to make individual AI systems more safe.
However, if we can make them safe, how should we direct them? To answer this, we turn to the
Ethics section and discuss how to make AI systems that promote our most important values. Finally,
in the Society section, we turn to the numerous challenges that emerge when there are multiple AI
systems or multiple AI developers with competing interests.
The Background section starts with an informal overview of AI risks, which summarises many
of the key concerns discussed in this book. We outline some scenarios where AI systems could
11

12
CONTENTS
cause catastrophic outcomes. We split risks across four categories: malicious use, AI arms race
dynamics, organizational risks, and rogue AIs. These categories can be loosely mapped onto the
risks discussed in more depth in Governance, Collective Action Problems, Safety Engineering, and
Single-Agent Safety chapters, respectively. However, this mapping is imperfect as many of the risks
and frameworks discussed in the textbook are more general and cut across scenarios. Nonetheless,
we hope that these scenarios give readers a concrete picture of the risks that we explore in this book.
Artificial Intelligence Fundamentals gives an accessible and non-mathematical explanation of current
AI systems, setting out concepts in machine learning, deep learning, scaling laws, and so on. This
provides the necessary foundations for the discussion of the safety of individual AI systems in the
next section.
The Safety section aims to provide an overview of core challenges in safely building advanced AI
systems. It draws on insights from both machine learning research and from general theories of
safety engineering and complex systems which provide a powerful lens for understanding these
issues. In Single-Agent Safety, we explore challenges in making individual AI systems safer, such as
bias, transparency, and emergence. In Safety Engineering, we discuss principles for creating safer
organizations and how these may apply to those developing and deploying AI. The need for a robust
safety culture at organizations developing AI is crucial, so organizations do not prioritize profit at the
expense of safety. In our discussion of safety engineering, we rely on complex systems-based models
and methods, as they are dominant in safety science research. Next, in Complex Systems, we show
that analyzing AIs as complex systems helps us to better understand the difficulty of predicting how
they will respond to external pressures or controlling the goals that may emerge in such systems.
More generally, this chapter provides us with a useful vocabulary for discussing diverse systems of
interest.
The Ethics section focuses on how to instill objectives and constraints in AI systems in order to avert
severe risks and ensure positive outcomes. In Machine Ethics, we introduce the challenge of giving
AI systems objectives that will reliably lead to beneficial outcomes for society, and discuss various
proposals along with the challenges they face. Normative Ethics dives deeper into the question of
how to ensure that AI systems behave in ways that reflect our values and how to reconcile conflicting
views on these values. We think we should not try to reinvent systematic thinking about values from
scratch and should instead draw from existing ethical systems, so the chapter aims to give readers a
background in major theories and concepts in normative ethics. The appendix on Utility Functions
is connected to the theme of objectives and constraints in AI systems, as it reviews formal methods
to model self-interested AI agents and challenges we might face in adjusting their goals if these turn
out to be faulty.
The final section on Society has two chapters: Collective Action Problems and Governance. In
Collective Action Problems, we utilize game theory to illustrate the many ways in which multiple
agents (humans, AIs, groups of humans and AIs) can fail to secure good outcomes and come into
conflict. We also consider the evolutionary dynamics shaping AI development and how these drive
AI risks. These frameworks help us to understand the challenges of managing competitive pressures
between AI developers, militaries, or AI systems themselves. Finally, in the Governance chapter, we
discuss strategic variables such as the rate at which AI systems evolves and how widely access to
powerful AI systems is distributed. We introduce a variety of potential paths for managing AI risks,
including corporate governance, national regulation, and international coordination.
Dan Hendrycks
Center for AI Safety

Part I
Background
13


Overview of Catastrophic AI Risks
1.1
Introduction
In this chapter, we will give a brief and informal description of many major societal-scale risks from
AI, focussing on AI risks that could lead to highly severe or even catastrophic societal outcomes.
This provides some background and motivation before we discuss specific challenges with more depth
and rigor in the following chapters.
The world as we know it today is not normal. We take for granted that we can talk instantaneously
with people thousands of miles away, fly to the other side of the world in less than a day, and access
vast mountains of accumulated knowledge on devices we carry around in our pockets. These realities
seemed far-fetched decades ago, and would have been inconceivable to people living centuries ago.
The ways we live, work, travel, and communicate have only been possible for a tiny fraction of human
history.
Yet, when we look at the bigger picture, a broader pattern emerges: accelerating development.
Hundreds of thousands of years elapsed between the time Homo sapiens appeared on Earth and the
agricultural revolution. Then, thousands of years passed before the industrial revolution. Now, just
centuries later, the artificial intelligence (AI) revolution is beginning. The march of history is not
constant—it is rapidly accelerating.
100 million
10 billion
1 trillion
100 trillion
10000 BCE
6000 BCE
2000 BCE
2000 CE
Year
GWP (1990 USD)
Gross World Product Over Time
Figure 1.1: World production has grown rapidly over the course of human history. AI could further
this trend, catapulting humanity into a new period of unprecedented change.
We can capture this trend quantitatively in Figure 1.1, which shows how estimated gross world
product has changed over time [1, 2]. The hyperbolic growth it depicts might be explained by the
fact that, as technology advances, the rate of technological advancement also tends to increase.
Empowered with new technologies, people can innovate faster than they could before. Thus, the
gap in time between each landmark development narrows.
It is the rapid pace of development, as much as the sophistication of our technology, that makes the
present day an unprecedented time in human history. We have reached a point where technological
advancements can transform the world beyond recognition within a human lifetime. For example,
15

16
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
people who have lived through the creation of the internet can remember a time when our now
digitally-connected world would have seemed like science fiction.
From a historical perspective, it appears possible that the same amount of development could now be
condensed in an even shorter timeframe. We might not be certain that this will occur, but neither can
we rule it out. We therefore wonder: what new technology might usher in the next big acceleration?
In light of recent advances, AI seems an increasingly plausible candidate. Perhaps, as AI continues
to become more powerful, it could lead to a qualitative shift in the world, more profound than any
we have experienced so far. It could be the most impactful period in history, though it could also be
the last.
Although technological advancement has often improved people’s lives, we ought to remember that,
as our technology grows in power, so too does its destructive potential. Consider the invention of
nuclear weapons. Last century, for the first time in our species’ history, humanity possessed the
ability to destroy itself, and the world suddenly became much more fragile.
Our newfound vulnerability revealed itself in unnerving clarity during the Cold War. On a Saturday
in October 1962, the Cuban Missile Crisis was cascading out of control. US warships enforcing the
blockade of Cuba detected a Soviet submarine and attempted to force it to the surface by dropping
low-explosive depth charges. The submarine was out of radio contact, and its crew had no idea
whether World War III had already begun. A broken ventilator raised the temperature up to 140◦F
in some parts of the submarine, causing crew members to fall unconscious as depth charges exploded
nearby.
The submarine carried a nuclear-armed torpedo, which required consent from both the captain and
political officer to launch. Both provided it. On any other submarine in Cuban waters that day,
that torpedo would have launched—and a nuclear third world war may have followed. Fortunately, a
man named Vasili Arkhipov was also on the submarine. Arkhipov was the commander of the entire
flotilla and by sheer luck happened to be on that particular submarine. He talked the captain down
from his rage, convincing him to await further orders from Moscow. He averted a nuclear war and
saved millions or billions of lives—and possibly civilization itself.
Carl Sagan once observed, “If we continue to accumulate only power and not wisdom, we will surely
destroy ourselves" [3]. Sagan was correct: The power of nuclear weapons was not one we were ready
for. Overall, it has been luck rather than wisdom that has saved humanity from nuclear annihilation,
with multiple recorded instances of a single individual preventing a full-scale nuclear war.
AI is now poised to become a powerful technology with destructive potential similar to nuclear
weapons. We do not want to repeat the Cuban Missile Crisis. We do not want to slide toward a
moment of peril where our survival hinges on luck rather than the ability to use this technology
wisely. Instead, we need to work proactively to mitigate the risks it poses. This necessitates a better
understanding of what could go wrong and what to do about it.
Luckily, AI systems are not yet advanced enough to contribute to every risk we discuss. But that
is cold comfort in a time when AI development is advancing at an unprecedented and unpredictable
rate. We consider risks arising from both present-day AIs and AIs that are likely to exist in the near
future. It is possible that if we wait for more advanced systems to be developed before taking action,
it may be too late.
In this chapter, we will explore various ways in which powerful AIs could bring about catastrophic
events with devastating consequences for vast numbers of people. We will also discuss how AIs
could present existential risks—catastrophes from which humanity would be unable to recover. The

1.2. MALICIOUS USE
17
most obvious such risk is extinction, but there are other outcomes, such as creating a permanent
dystopian society, which would also constitute an existential catastrophe. We outline many possible
catastrophes, some of which are more likely than others and some of which are mutually incompatible
with each other. This approach is motivated by the principles of risk management. We prioritize
asking “what could go wrong?”
rather than reactively waiting for catastrophes to occur. This
proactive mindset enables us to anticipate and mitigate catastrophic risks before it’s too late.
To help orient the discussion, we decompose catastrophic risks from AIs into four risk sources that
warrant intervention:
• Malicious use: Malicious actors using AIs to cause large-scale devastation.
• AI race: Competitive pressures that could drive us to deploy AIs in unsafe ways, despite this
being in no one’s best interest.
• Organizational risks: Accidents arising from the complexity of AIs and the organizations
developing them.
• Rogue AIs: The problem of controlling a technology more intelligent than we are.
These four sections—Malicious Use, AI Race, Organizational Risks, and Rogue AIs—describe causes
of AI risks that are intentional, environmental/structural, accidental, and internal, respectively [4].
The risks that are briefly outlined in this chapter are discussed in greater depth in the rest of this
book.
In this chapter, we will describe how concrete, small-scale examples of each risk might escalate into
catastrophic outcomes. We also include hypothetical stories to help readers conceptualize the various
processes and dynamics discussed in each section. We hope this survey will serve as a practical
introduction for readers interested in learning about and mitigating catastrophic AI risks.
1.2
Malicious Use
On the morning of March 20, 1995, five men entered the Tokyo subway system. After boarding
separate subway lines, they continued for several stops before dropping the bags they were carrying
and exiting.
An odorless, colorless liquid inside the bags began to vaporize.
Within minutes,
commuters began choking and vomiting. The trains continued on toward the heart of Tokyo, with
sickened passengers leaving the cars at each station. The fumes were spread at each stop, either by
emanating from the tainted cars or through contact with people’s clothing and shoes. By the end of
the day, 13 people lay dead and 5,800 seriously injured. The group responsible for the attack was the
religious cult Aum Shinrikyo [5]. Its motive for murdering innocent people? To bring about the end
of the world.
Powerful new technologies offer tremendous potential benefits, but they also carry the risk of
empowering malicious actors to cause widespread harm. There will always be those with the worst of
intentions, and AIs could provide them with a formidable tool to achieve their objectives. Moreover,
as AI technology advances, severe malicious use could potentially destabilize society, increasing the
likelihood of other risks.
In this section, we will explore the various ways in which the malicious use of advanced AIs could
pose catastrophic risks. These include engineering biochemical weapons, unleashing rogue AIs, using
persuasive AIs to spread propaganda and erode consensus reality, and leveraging censorship and mass

18
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
surveillance to irreversibly concentrate power. We will conclude by discussing possible strategies for
mitigating the risks associated with the malicious use of AIs.
Unilateral actors considerably increase the risks of malicious use.
In instances where
numerous actors have access to a powerful technology or dangerous information that could be used
for harmful purposes, it only takes one individual to cause significant devastation. Malicious actors
themselves are the clearest example of this, but recklessness can be equally dangerous. For example, a
single research team might be excited to open source an AI system with biological research capabilities,
which would speed up research and potentially save lives, but this could also increase the risk of
malicious use if the AI system could be repurposed to develop bioweapons. In situations like this,
the outcome may be determined by the least risk-averse research group. If only one research group
thinks the benefits outweigh the risks, it could act unilaterally, deciding the outcome even if most
others don’t agree. And if they are wrong and someone does decide to develop a bioweapon, it would
be too late to reverse course.
By default, advanced AIs may increase the destructive capacity of both the most powerful and the
general population. Thus, the growing potential for AIs to empower malicious actors is one of the
most severe threats humanity will face in the coming decades. The examples we give in this section
are only those we can foresee. It is possible that AIs could aid in the creation of dangerous new
technology we cannot presently imagine, which would further increase risks from malicious use.
1.2.1
Bioterrorism
The rapid advancement of AI technology increases the risk of bioterrorism. AIs with knowledge of
bioengineering could facilitate the creation of novel bioweapons and lower barriers to obtaining such
agents. Engineered pandemics from AI-assisted bioweapons pose a unique challenge, as attackers
have an advantage over defenders and could constitute an existential threat to humanity. We will
now examine these risks and how AIs might exacerbate challenges in managing bioterrorism and
engineered pandemics.
Bioengineered pandemics present a new threat.
Biological agents, including viruses and
bacteria, have caused some of the most devastating catastrophes in history. It’s believed the Black
Death killed more humans than any other event in history, an astounding and awful 200 million, the
equivalent to four billion deaths today. While contemporary advancements in science and medicine
have made great strides in mitigating risks associated with natural pandemics, engineered pandemics
could be designed to be more lethal or easily transmissible than natural pandemics, presenting a new
threat that could equal or even surpass the devastation wrought by history’s most deadly plagues [6].
Humanity has a long and dark history of weaponizing pathogens, with records dating back to 1320
BCE describing a war in Asia Minor where infected sheep were driven across the border to spread
Tularemia [7]. During the twentieth century, 15 countries are known to have developed bioweapons
programs, including the US, USSR, UK, and France. Like chemical weapons, bioweapons have
become a taboo among the international community. While some state actors continue to operate
bioweapons programs [8], a more significant risk may come from non-state actors like Aum Shinrikyo,
ISIS, or simply disturbed individuals. Due to advancements in AI and biotechnology, the tools and
knowledge necessary to engineer pathogens with capabilities far beyond Cold War-era bioweapons
programs will rapidly democratize.
Biotechnology is progressing rapidly and becoming more accessible.
A few decades ago,
the ability to synthesize new viruses was limited to a handful of the top scientists working in advanced
laboratories. Today it is estimated that there are 30,000 people with the talent, training, and access

1.2. MALICIOUS USE
19
to technology to create new pathogens [6]. This figure could rapidly expand. Gene synthesis, which
allows the creation of custom biological agents, has dropped precipitously in price, with its cost
halving approximately every 15 months [9]. Furthermore, with the advent of benchtop DNA synthesis
machines, access will become much easier and could avoid existing gene synthesis screening efforts,
which complicates controlling the spread of such technology [10]. The chances of a bioengineered
pandemic killing millions, perhaps billions, is proportional to the number of people with the skills
and access to the technology to synthesize them. With AI assistants, orders of magnitude more
people could have the required skills, thereby increasing the risks by orders of magnitude.
AIs could be used to expedite the discovery of new, more deadly chemical and biological
weapons.
In 2022, researchers took an AI system designed to create new drugs by generating
non-toxic, therapeutic molecules and tweaked it to reward, rather than penalize, toxicity [11]. After
this simple change, within six hours, it generated 40,000 candidate chemical warfare agents entirely
on its own. It designed not just known deadly chemicals including VX, but also novel molecules that
may be deadlier than any chemical warfare agents discovered so far. In the field of biology, AIs have
already surpassed human abilities in protein structure prediction [12] and made contributions to
synthesizing those proteins [13]. Similar methods could be used to create bioweapons and develop
pathogens that are deadlier, more transmissible, and more difficult to treat than anything seen before.
AIs compound the threat of bioengineered pandemics.
AIs will increase the number of
people who could commit acts of bioterrorism. General-purpose AIs like ChatGPT are capable
of synthesizing expert knowledge about the deadliest known pathogens, such as influenza and
smallpox, and providing step-by-step instructions about how a person could create them while
evading safety protocols [14].
Future versions of AIs could be even more helpful to potential
bioterrorists when AIs are able to synthesize information into techniques, processes, and knowl-
edge that is not explicitly available anywhere on the internet.
Public health authorities may
respond to these threats with safety measures, but in bioterrorism, the attacker has the advan-
tage. The exponential nature of biological threats means that a single attack could spread to
the entire world before an effective defense could be mounted.
Only 100 days after being de-
tected and sequenced, the omicron variant of COVID-19 had infected a quarter of the United
States and half of Europe [6]. Quarantines and lockdowns instituted to suppress the COVID-19
pandemic caused a global recession and still could not prevent the disease from killing millions
worldwide.
In summary, advanced AIs could constitute a weapon of mass destruction in the hands of terrorists,
by making it easier for them to design, synthesize, and spread deadly new pathogens. By reducing
the required technical expertise and increasing the lethality and transmissibility of pathogens, AIs
could enable malicious actors to cause global catastrophe by unleashing pandemics.
1.2.2
Unleashing AI Agents
Many technologies are tools that humans use to pursue our goals, such as hammers, toasters, and
toothbrushes. But AIs are increasingly built as agents which autonomously take actions in the world
in order to pursue open-ended goals. AI agents can be given goals such as winning games, making
profits on the stock market, or driving a car to a destination. AI agents therefore pose a unique risk:
people could build AIs that pursue dangerous goals.
Malicious actors could intentionally create rogue AIs.
One month after the release of
GPT-4, an open-source project bypassed the AI’s safety filters and turned it into an autonomous
AI agent instructed to “destroy humanity,” “establish global dominance,” and “attain immortality.”
Dubbed ChaosGPT, the AI compiled research on nuclear weapons and sent tweets trying to influence

20
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
others. Fortunately, ChaosGPT was merely a warning given that it lacked the ability to successfully
formulate long-term plans, hack computers, and survive and spread. Yet given the rapid pace of AI
development, ChaosGPT did offer a glimpse into the risks that more advanced rogue AIs could pose
in the near future.
Many groups may want to unleash AIs or have AIs displace humanity.
Simply unleashing
rogue AIs, like a more sophisticated version of ChaosGPT, could accomplish mass destruction, even
if those AIs aren’t explicitly told to harm humanity. There are a variety of beliefs that may drive
individuals and groups to do so. One ideology that could pose a unique threat in this regard is
“accelerationism.” This ideology seeks to accelerate AI development as rapidly as possible and opposes
restrictions on the development or proliferation of AIs. This sentiment is common among many
leading AI researchers and technology leaders, some of whom are intentionally racing to build AIs
more intelligent than humans. According to Google co-founder Larry Page, AIs are humanity’s
rightful heirs and the next step of cosmic evolution. He has also expressed the sentiment that humans
maintaining control over AIs is “speciesist” [15]. Jürgen Schmidhuber, an eminent AI scientist, argued
that “In the long run, humans will not remain the crown of creation... But that’s okay because
there is still beauty, grandeur, and greatness in realizing that you are a tiny part of a much grander
scheme which is leading the universe from lower complexity towards higher complexity” [16]. Richard
Sutton, another leading AI scientist, in discussing smarter-than human AI asked “why shouldn’t
those who are the smartest become powerful?” and thinks the development of superintelligence
will be an achievement “beyond humanity, beyond life, beyond good and bad” [17]. He argues that
“succession to AI is inevitable,” and while “they could displace us from existence,” “we should not
resist succession” [18].
There are several sizable groups who may want to unleash AIs to intentionally cause harm. For
example, sociopaths and psychopaths make up around 3 percent of the population [19]. In the future,
people who have their livelihoods destroyed by AI automation may grow resentful, and some may
want to retaliate. There are plenty of cases in which seemingly mentally stable individuals with no
history of insanity or violence suddenly go on a shooting spree or plant a bomb with the intent to
harm as many innocent people as possible. We can also expect well-intentioned people to make the
situation even more challenging. As AIs advance, they could make ideal companions—knowing how
to provide comfort, offering advice when needed, and never demanding anything in return. Inevitably,
people will develop emotional bonds with chatbots, and some will demand that they be granted
rights or become autonomous.
In summary, releasing powerful AIs and allowing them to take actions independently of humans
could lead to a catastrophe. There are many reasons that people might pursue this, whether because
of a desire to cause harm, an ideological belief in technological acceleration, or a conviction that AIs
should have the same rights and freedoms as humans.
1.2.3
Persuasive AIs
The deliberate propagation of disinformation is already a serious issue, reducing our shared
understanding of reality and polarizing opinions. AIs could be used to severely exacerbate this
problem by generating personalized disinformation on a larger scale than before. Additionally,
as AIs become better at predicting and nudging our behavior, they will become more capable at
manipulating us. We will now discuss how AIs could be leveraged by malicious actors to create a
fractured and dysfunctional society.
AIs could pollute the information ecosystem with motivated lies.
Sometimes ideas spread
not because they are true, but because they serve the interests of a particular group. “Yellow

1.2. MALICIOUS USE
21
journalism” was coined as a pejorative reference to newspapers that advocated war between Spain
and the United States in the late 19th century, because they believed that sensational war stories
would boost their sales [20]. When public information sources are flooded with falsehoods, people will
sometimes fall prey to lies, or else come to distrust mainstream narratives, both of which undermine
societal integrity.
Unfortunately, AIs could escalate these existing problems dramatically. First, AIs could be used
to generate unique, personalized disinformation at a large scale. While there are already many
social media bots [21], some of which exist to spread disinformation, historically they have been run
by humans or primitive text generators. The latest AI systems do not need humans to generate
personalized messages, never get tired, and could potentially interact with millions of users at once
[22].
AIs can exploit users’ trust.
Already, hundreds of thousands of people pay for chatbots marketed
as lovers and friends [23], and one man’s suicide has been partially attributed to interactions with a
chatbot [24]. As AIs appear increasingly human-like, people will increasingly form relationships with
them and grow to trust them. AIs that gather personal information through relationship-building or
by accessing extensive personal data, such as a user’s email account or personal files, could leverage
that information to enhance persuasion. Powerful actors that control those systems could exploit
user trust by delivering personalized disinformation directly through people’s “friends.”
AIs could centralize control of trusted information.
Separate from democratizing
disinformation, AIs could centralize the creation and dissemination of trusted information. Only a
few actors have the technical skills and resources to develop cutting-edge AI systems, and they could
use these AIs to spread their preferred narratives. Alternatively, if AIs are broadly accessible this
could lead to widespread disinformation, with people retreating to trusting only a small handful of
authoritative sources [25]. In both scenarios, there would be fewer sources of trusted information and
a small portion of society would control popular narratives.
AI censorship could further centralize control of information. This could begin with good intentions,
such as using AIs to enhance fact-checking and help people avoid falling prey to false narratives.
This would not necessarily solve the problem, as disinformation persists today despite the presence
of fact-checkers.
Even worse, purported “fact-checking AIs” might be designed by authoritarian governments and
others to suppress the spread of true information. Such AIs could be designed to correct most common
misconceptions but provide incorrect information about some sensitive topics, such as human rights
violations committed by certain countries. But even if fact-checking AIs work as intended, the public
might eventually become entirely dependent on them to adjudicate the truth, reducing people’s
autonomy and making them vulnerable to failures or hacks of those systems.
In a world with widespread persuasive AI systems, people’s beliefs might be almost entirely determined
by which AI systems they interact with most. Never knowing whom to trust, people could retreat
even further into ideological enclaves, fearing that any information from outside those enclaves might
be a sophisticated lie. This would erode consensus reality, people’s ability to cooperate with others,
participate in civil society, and address collective action problems. This would also reduce our ability
to have a conversation as a species about how to mitigate existential risks from AIs.
In summary, AIs could create highly effective, personalized disinformation on an unprecedented scale,
and could be particularly persuasive to people they have built personal relationships with. In the

22
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
hands of many people, this could create a deluge of disinformation that debilitates human society,
but, kept in the hands of a few, it could allow governments to control narratives for their own ends.
1.2.4
Concentration of Power
We have discussed several ways in which individuals and groups might use AIs to cause widespread
harm, through bioterrorism; releasing powerful, uncontrolled AIs; and disinformation. To mitigate
these risks, governments might pursue intense surveillance and seek to keep AIs in the hands of a
trusted minority. This reaction, however, could easily become an overcorrection, paving the way for
an entrenched totalitarian regime that would be locked in by the power and capacity of AIs. This
scenario represents a form of “top-down” misuse, as opposed to “bottom-up” misuse by citizens, and
could in extreme cases culminate in an entrenched dystopian civilization.
AIs could lead to extreme, and perhaps irreversible concentration of power.
The
persuasive abilities of AIs combined with their potential for surveillance and the advancement of
autonomous weapons could allow small groups of actors to “lock-in” their control over society, perhaps
permanently. To operate effectively, AIs require a broad set of infrastructure components, which
are not equally distributed, such as data centers, computing power, and big data. Those in control
of powerful systems may use them to suppress dissent, spread propaganda and disinformation, and
otherwise advance their goals, which may be contrary to public wellbeing.
AIs may entrench a totalitarian regime.
In the hands of the state, AIs may result in the
erosion of civil liberties and democratic values in general. AIs could allow totalitarian governments
to efficiently collect, process, and act on an unprecedented volume of information, permitting an
ever smaller group of people to surveil and exert complete control over the population without the
need to enlist millions of citizens to serve as willing government functionaries. Overall, as power
and control shift away from the public and toward elites and leaders, democratic governments are
highly vulnerable to totalitarian backsliding. Additionally, AIs could make totalitarian regimes much
longer-lasting; a major way in which such regimes have been toppled previously is at moments of
vulnerability like the death of a dictator, but AIs, which would be hard to “kill,” could provide much
more continuity to leadership, providing few opportunities for reform.
AIs can entrench corporate power at the expense of the public good.
Corporations have
long lobbied to weaken laws and policies that restrict their actions and power, all in the service of
profit. Corporations in control of powerful AI systems may use them to manipulate customers into
spending more on their products even to the detriment of their own wellbeing. The concentration of
power and influence that could be afforded by AIs could enable corporations to exert unprecedented
control over the political system and entirely drown out the voices of citizens. This could occur even
if creators of these systems know their systems are self-serving or harmful to others, as they would
have incentives to reinforce their power and avoid distributing control.
In addition to power, locking in certain values may curtail humanity’s moral progress.
It’s dangerous to allow any set of values to become permanently entrenched in society. For example,
AI systems have learned racist and sexist views [26], and once those views are learned, it can be
difficult to fully remove them. In addition to problems we know exist in our society, there may be
some we still do not. Just as we abhor some moral views widely held in the past, people in the future
may want to move past moral views that we hold today, even those we currently see no problem
with. For example, moral defects in AI systems would be even worse if AI systems had been trained
in the 1960s, and many people at the time would have seen no problem with that. We may even be
unknowingly perpetuating moral catastrophes today [27]. Therefore, when advanced AIs emerge and

1.3. AI RACE
23
transform the world, there is a risk of their objectives locking in or perpetuating defects in today’s
values. If AIs are not designed to continuously learn and update their understanding of societal
values, they may perpetuate or reinforce existing defects in their decision-making processes long into
the future.
In summary, although keeping powerful AIs in the hands of a few might reduce the risks of terrorism,
it could further exacerbate power inequality if misused by governments and corporations. This could
lead to totalitarian rule and intense manipulation of the public by corporations, and could lock in
current values, preventing any further moral progress.
Story: Bioterrorism
The following is an illustrative hypothetical story to help readers envision some of these risks.
This story is nonetheless somewhat vague to reduce the risk of inspiring malicious actions
based on it.
A biotechnology startup is making waves in the industry with its AI-powered bioengineering
model. The company has made bold claims that this new technology will revolutionize
medicine through its ability to create cures for both known and unknown diseases. The
company did, however, stir up some controversy when it decided to release the program to
approved researchers in the scientific community. Only weeks after its decision to make the
model open-source on a limited basis, the full model was leaked on the internet for all to
see. Its critics pointed out that the model could be repurposed to design lethal pathogens
and claimed that the leak provided bad actors with a powerful tool to cause widespread
destruction, opening it up to abuse without safeguards in place.
Unknown to the public, an extremist group has been working for years to engineer a new virus
designed to kill large numbers of people. Yet given their lack of expertise, these efforts have so
far been unsuccessful. When the new AI system is leaked, the group immediately recognizes
it as a potential tool to design the virus and circumvent legal and monitoring obstacles to
obtain the necessary raw materials. The AI system successfully designs exactly the kind of
virus the extremist group was hoping for. It also provides step-by-step instructions on how to
synthesize large quantities of the virus and circumvent any obstacles to spreading it. With the
synthesized virus in hand, the extremist group devises a plan to release the virus in several
carefully chosen locations in order to maximize its spread.
The virus has a long incubation period and spreads silently and quickly throughout the
population for months. By the time it is detected, it has already infected millions and has an
alarmingly high mortality rate. Given its lethality, most who are infected will ultimately die.
The virus may or may not be contained eventually, but not before it kills millions of people.
1.3
AI Race
The immense potential of AIs has created competitive pressures among global players contending
for power and influence. This “AI race” is driven by nations and corporations who feel they must
rapidly build and deploy AIs to secure their positions and survive. By failing to properly prioritize
global risks, this dynamic makes it more likely that AI development will produce dangerous outcomes.
Analogous to the nuclear arms race during the Cold War, participation in an AI race may serve
individual short-term interests, but it ultimately results in worse collective outcomes for humanity.
Importantly, these risks stem not only from the intrinsic nature of AI technology, but from the
competitive pressures that encourage insidious choices in AI development.

24
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
In this section, we first explore the military AI arms race and the corporate AI race, where nation-
states and corporations are forced to rapidly develop and adopt AI systems to remain competitive.
Moving beyond these specific races, we reconceptualize competitive pressures as part of a broader
evolutionary process in which AIs could become increasingly pervasive, powerful, and entrenched in
society. Finally, we highlight potential strategies and policy suggestions to mitigate the risks created
by an AI race and ensure the safe development of AIs.
1.3.1
Military AI Arms Race
The development of AIs for military applications is swiftly paving the way for a new era in military
technology, with potential consequences rivaling those of gunpowder and nuclear arms in what has
been described as the “third revolution in warfare.” The weaponization of AI presents numerous
challenges, such as the potential for more destructive wars, the possibility of accidental usage or loss
of control, and the prospect of malicious actors co-opting these technologies for their own purposes.
As AIs gain influence over traditional military weaponry and increasingly take on command and
control functions, humanity faces a paradigm shift in warfare. In this context, we will discuss the
latent risks and implications of this AI arms race on global security, the potential for intensified
conflicts, and the dire outcomes that could come as a result, including the possibility of conflicts
escalating to a scale that poses an existential threat.
Lethal Autonomous Weapons (LAWs)
LAWs are weapons that can identify, target, and kill without human intervention [28]. They offer
potential improvements in decision-making speed and precision. Warfare, however, is a high-stakes,
safety-critical domain for AIs with significant moral and practical concerns. Though their existence
is not necessarily a catastrophe in itself, LAWs may serve as an on-ramp to catastrophes stemming
from malicious use, accidents, loss of control, or an increased likelihood of war.
LAWs may become vastly superior to humans.
Driven by rapid developments in AIs, weapons
systems that can identify, target, and decide to kill human beings on their own—without an officer
directing an attack or a soldier pulling the trigger—are starting to transform the future of conflict.
In 2020, an advanced AI agent outperformed experienced F-16 pilots in a series of virtual dogfights,
including decisively defeating a human pilot 5-0, showcasing “aggressive and precise maneuvers the
human pilot couldn’t outmatch” [29]. Just as in the past, superior weapons would allow for more
destruction in a shorter period of time, increasing the severity of war.
Militaries are taking steps toward delegating life-or-death decisions to AIs.
Fully
autonomous drones were likely first used on the battlefield in Libya in March 2020, when retreating
forces were “hunted down and remotely engaged” by a drone operating without human oversight [30].
In May 2021, the Israel Defense Forces used the world’s first AI-guided weaponized drone swarm
during combat operations, which marks a significant milestone in the integration of AI and drone
technology in warfare [31]. Although walking, shooting robots have yet to replace soldiers on the
battlefield, technologies are converging in ways that may make this possible in the near future.
LAWs increase the likelihood of war.
Sending troops into battle is a grave decision that
leaders do not make lightly. But autonomous weapons would allow an aggressive nation to launch
attacks without endangering the lives of its own soldiers and thus face less domestic scrutiny. While
remote-controlled weapons share this advantage, their scalability is limited by the requirement
for human operators and vulnerability to jamming countermeasures, limitations that LAWs could
overcome [32]. Public opinion for continuing wars tends to wane as conflicts drag on and casualties

1.3. AI RACE
25
increase [33]. LAWs would change this equation. National leaders would no longer face the prospect
of body bags returning home, thus removing a primary barrier to engaging in warfare, which could
ultimately increase the likelihood of conflicts.
Cyberwarfare
As well as being used to enable deadlier weapons, AIs could lower the barrier to entry for cyberattacks,
making them more numerous and destructive. They could cause serious harm not only in the digital
environment but also in physical systems, potentially taking out critical infrastructure that societies
depend on. While AIs could also be used to improve cyberdefense, it is unclear whether they will be
most effective as an offensive or defensive technology [34]. If they enhance attacks more than they
support defense, then cyberattacks could become more common, creating significant geopolitical
turbulence and paving another route to large-scale conflict.
AIs have the potential to increase the accessibility, success rate, scale, speed, stealth,
and potency of cyberattacks.
Cyberattacks are already a reality, but AIs could be used to
increase their frequency and destructiveness in multiple ways. Machine learning tools could be used
to find more critical vulnerabilities in target systems and improve the success rate of attacks. They
could also be used to increase the scale of attacks by running millions of systems in parallel, and
increase the speed by finding novel routes to infiltrating a system. Cyberattacks could also become
more potent if used to hijack AI weapons.
Cyberattacks can destroy critical infrastructure.
By hacking computer systems that control
physical processes, cyberattacks could cause extensive infrastructure damage. For example, they could
cause system components to overheat or valves to lock, leading to a buildup of pressure culminating
in an explosion. Through interferences like this, cyberattacks have the potential to destroy critical
infrastructure, such as electric grids and water supply systems. This was demonstrated in 2015,
when a cyberwarfare unit of the Russian military hacked into the Ukrainian power grid, leaving over
200,000 people without power access for several hours. AI-enhanced attacks could be even more
devastating and potentially deadly for the billions of people who rely on critical infrastructure for
survival.
Difficulties in attributing AI-driven cyberattacks could increase the risk of war.
A
cyberattack resulting in physical damage to critical infrastructure would require a high degree of
skill and effort to execute, perhaps only within the capability of nation-states. Such attacks are
rare as they constitute an act of war, and thus elicit a full military response. Yet AIs could enable
attackers to hide their identity, for example if they are used to evade detection systems or more
effectively cover the tracks of the attacker [35]. If cyberattacks become more stealthy, this would
reduce the threat of retaliation from an attacked party, potentially making attacks more likely. If
stealthy attacks do happen, they might incite actors to mistakenly retaliate against unrelated third
parties they suspect to be responsible. This could increase the scope of the conflict dramatically.
Automated Warfare
AIs speed up the pace of war, which makes AIs more necessary.
AIs can quickly process
a large amount of data, analyze complex situations, and provide helpful insights to commanders.
With ubiquitous sensors and advanced technology on the battlefield, there is tremendous incoming
information. AIs help make sense of this information, spotting important patterns and relationships
that humans might miss. As these trends continue, it will become increasingly difficult for humans
to make well-informed decisions as quickly as necessary to keep pace with AIs. This would further
pressure militaries to hand over decisive control to AIs. The continuous integration of AIs into

26
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
all aspects of warfare will cause the pace of combat to become faster and faster. Eventually, we
may arrive at a point where humans are no longer capable of assessing the ever-changing battlefield
situation and must cede decision-making power to advanced AIs.
Automatic retaliation can escalate accidents into war.
There is already willingness to let
computer systems retaliate automatically. In 2014, a leak revealed to the public that the NSA was
developing a system called MonsterMind, which would autonomously detect and block cyberattacks
on US infrastructure [36]. It was suggested that in the future, MonsterMind could automatically
initiate a retaliatory cyberattack with no human involvement. If multiple combatants have policies of
automatic retaliation, an accident or false alarm could quickly escalate to full-scale war before humans
intervene. This would be especially dangerous if the superior information processing capabilities
of modern AI systems makes it more appealing for actors to automate decisions regarding nuclear
launches.
History shows the danger of automated retaliation.
On September 26, 1983, Stanislav
Petrov, a lieutenant colonel of the Soviet Air Defense Forces, was on duty at the Serpukhov-15 bunker
near Moscow, monitoring the Soviet Union’s early warning system for incoming ballistic missiles.
The system indicated that the US had launched multiple nuclear missiles toward the Soviet Union.
The protocol at the time dictated that such an event should be considered a legitimate attack, and
the Soviet Union would respond with a nuclear counterstrike. If Petrov had passed on the warning
to his superiors, this would have been the likely outcome. Instead, however, he judged it to be a false
alarm and ignored it. It was soon confirmed that the warning had been caused by a rare technical
malfunction. If an AI had been in control, the false alarm could have triggered a nuclear war.
AI-controlled weapons systems could lead to a flash war.
Autonomous systems are not
infallible. We have already witnessed how quickly an error in an automated system can escalate in
the economy. Most notably, in the 2010 Flash Crash, a feedback loop between automated trading
algorithms amplified ordinary market fluctuations into a financial catastrophe in which a trillion
dollars of stock value vanished in minutes [37]. If multiple nations were to use AIs to automate their
defense systems, an error could be catastrophic, triggering a spiral of attacks and counter-attacks
that would happen too quickly for humans to step in—a flash war. The market quickly recovered
from the 2010 Flash Crash, but the harm caused by a flash war could be catastrophic.
Automated warfare could reduce accountability for military leaders.
Military leaders
may at times gain an advantage on the battlefield if they are willing to ignore the laws of war.
For example, soldiers may be able to mount stronger attacks if they do not take steps to minimize
civilian casualties. An important deterrent to this behavior is the risk that military leaders could
eventually be held accountable or even prosecuted for war crimes. Automated warfare could reduce
this deterrence effect by making it easier for military leaders to escape accountability by blaming
violations on failures in their automated systems.
AIs could make war more uncertain, increasing the risk of conflict.
Although states
that are already wealthier and more powerful often have more resources to invest in new military
technologies, they are not necessarily always the most successful at adopting them. Other factors
also play an important role, such as how agile and adaptive a military can be in incorporating new
technologies [38]. Major new weapons innovations can therefore offer an opportunity for existing
superpowers to bolster their dominance, but also for less powerful states to quickly increase their
power by getting ahead in an emerging and important sphere. This can create significant uncertainty
around if and how the balance of power is shifting, potentially leading states to incorrectly believe
they could gain something from going to war. Even aside from considerations regarding the balance

1.3. AI RACE
27
of power, rapidly evolving automated warfare would be unprecedented, making it difficult for actors
to evaluate their chances of victory in any particular conflict. This would increase the risk of
miscalculation, making war more more likely.
Actors May Risk Extinction Over Individual Defeat
Competitive pressures make actors more willing to accept the risk of extinction.
During
the Cold War, neither side desired the dangerous situation they found themselves in. There were
widespread fears that nuclear weapons could be powerful enough to wipe out a large fraction of
humanity, potentially even causing extinction—a catastrophic result for both sides. Yet the intense
rivalry and geopolitical tensions between the two superpowers fueled a dangerous cycle of arms
buildup. Each side perceived the other’s nuclear arsenal as a threat to its very survival, leading to a
desire for parity and deterrence. The competitive pressures pushed both countries to continually
develop and deploy more advanced and destructive nuclear weapons systems, driven by the fear of
being at a strategic disadvantage. During the Cuban Missile Crisis, this led to the brink of nuclear
war. Even though the story of Arkhipov preventing the launch of a nuclear torpedo wasn’t declassified
until decades after the incident, President John F. Kennedy reportedly estimated that he thought
the odds of nuclear war beginning during that time were “somewhere between one out of three and
even.” This chilling admission highlights how the competitive pressures between militaries have the
potential to cause global catastrophes.
“I know not with what weapons World War III will be fought, but World War IV will be
fought with sticks and stones.” -Einstein
Individually rational decisions can be collectively catastrophic.
Nations locked in
competition might make decisions that advance their own interests by putting the rest of the
world at stake. Scenarios of this kind are collective action problems, where decisions may be rational
on an individual level yet disastrous for the larger group [39].
For example, corporations and
individuals may weigh their own profits and convenience over the negative impacts of the emissions
they create, even if those emissions collectively result in climate change. The same principle can be
extended to military strategy and defense systems. Military leaders might estimate, for instance,
that increasing the autonomy of weapon systems would mean a 10 percent chance of losing control
over weaponized superhuman AIs. Alternatively, they might estimate that using AIs to automate
bioweapons research could lead to a 10 percent chance of leaking a deadly pathogen. Both of these
scenarios could lead to catastrophe or even extinction. The leaders may, however, also calculate
that refraining from these developments will mean a 99 percent chance of losing a war against an
opponent. Since conflicts are often viewed as existential struggles by those fighting them, rational
actors may accept an otherwise unthinkable 10 percent chance of human extinction over a 99 percent
chance of losing a war. Regardless of the particular nature of the risks posed by advanced AIs, these
dynamics could push us to the brink of global catastrophe.
Technological superiority does not guarantee national security.
It is tempting to think that
the best way of guarding against enemy attacks is to improve one’s own military prowess. However,
in the midst of competitive pressures, all parties will tend to advance their weaponry, such that no
one gains much of an advantage, but all are left at greater risk. As Richard Danzig, former Secretary
of the Navy, has observed, “The introduction of complex, opaque, novel, and interactive technologies
will produce accidents, emergent effects, and sabotage. On a number of occasions and in a number of
ways, the American national security establishment will lose control of what it creates... deterrence
is a strategy for reducing attacks, not accidents” [40].

28
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
Cooperation is paramount to reducing risk.
As discussed above, an AI arms race can lead us
down a hazardous path, despite this being in no country’s best interest. It is important to remember
that we are all on the same side when it comes to existential risks, and working together to prevent
them is a collective necessity. A destructive AI arms race benefits nobody, so all actors would be
rational to take steps to cooperate with one another to prevent the riskiest applications of militarized
AIs. As Dwight D. Eisenhower reminded us, “The only way to win World War III is to prevent it.”
We have considered how competitive pressures could lead to the increasing automation of conflict,
even if decision-makers are aware of the existential threat that this path entails. We have also
discussed cooperation as being the key to counteracting and overcoming this collective action problem.
We will now illustrate a hypothetical path to disaster that could result from an AI arms race.
Story: Automated Warfare
As AI systems become increasingly sophisticated, militaries start involving them in decision-
making processes.
Officials give them military intelligence about opponents’ arms and
strategies, for example, and ask them to calculate the most promising plan of action. It soon
becomes apparent that AIs are reliably reaching better decisions than humans, so it seems
sensible to give them more influence. At the same time, international tensions are rising,
increasing the threat of war.
A new military technology has recently been developed that could make international attacks
swifter and stealthier, giving targets less time to respond. Since military officials feel their
response processes take too long, they fear that they could be vulnerable to a surprise attack
capable of inflicting decisive damage before they would have any chance to retaliate. Since
AIs can process information and make decisions much more quickly than humans, military
leaders reluctantly hand them increasing amounts of retaliatory control, reasoning that failing
to do so would leave them open to attack from adversaries.
While for years military leaders had stressed the importance of keeping a “human in the
loop” for major decisions, human control is nonetheless gradually phased out in the interests
of national security. Military leaders understand that their decisions lead to the possibility
of inadvertent escalation caused by system malfunctions, and would prefer a world where
all countries automated less; but they do not trust that their adversaries will refrain from
automation. Over time, more and more of the chain of command is automated on all sides.
One day, a single system malfunctions, detecting an enemy attack when there is none. The
system is empowered to launch an instant “retaliatory” attack, and it does so in the blink
of an eye. The attack causes automated retaliation from the other side, and so on. Before
long, the situation is spiraling out of control, with waves of automated attack and retaliation.
Although humans have made mistakes leading to escalation in the past, this escalation between
mostly-automated militaries happens far more quickly than any before. The humans who
are responding to the situation find it difficult to diagnose the source of the problem, as the
AI systems are not transparent. By the time they even realize how the conflict started, it is
already over, with devastating consequences for both sides.
1.3.2
Corporate AI Race
Competitive pressures exist in the economy, as well as in military settings. Although competition
between companies can be beneficial, creating more useful products for consumers, there are also
pitfalls. First, the benefits of economic activity may be unevenly distributed, incentivizing those who

1.3. AI RACE
29
benefit most from it to disregard the harms to others. Second, under intense market competition,
businesses tend to focus much more on short-term gains than on long-term outcomes. With this
mindset, companies often pursue something that can make a lot of profit in the short term, even if
it poses a societal risk in the long term. We will now discuss how corporate competitive pressures
could play out with AIs and the potential negative impacts.
Economic Competition Undercuts Safety
Competitive pressure is fueling a corporate AI race.
To obtain a competitive advantage,
companies often race to offer the first products to a market rather than the safest. These dynamics
are already playing a role in the rapid development of AI technology. At the launch of Microsoft’s
AI-powered search engine in February 2023, the company’s CEO Satya Nadella said, “A race starts
today... we’re going to move fast.” Only weeks later, the company’s chatbot was shown to have
threatened to harm users [41]. In an internal email, Sam Schillace, a technology executive at Microsoft,
highlighted the urgency in which companies view AI development. He wrote that it would be an
“absolutely fatal error in this moment to worry about things that can be fixed later” [42].
Competitive pressures have contributed to major commercial and industrial disasters.
Throughout the 1960s, Ford Motor Company faced competition from international car manufacturers
as the share of imports in American car purchases steadily rose [43]. Ford developed an ambitious
plan to design and manufacture a new car model in only 25 months [44]. The Ford Pinto was delivered
to customers ahead of schedule, but with a serious safety problem: the gas tank was located near the
rear bumper, and could explode during rear collisions. Numerous fatalities and injuries were caused
by the resulting fires when crashes inevitably happened [45]. Ford was sued and a jury found them
liable for these deaths and injuries [46]. The verdict, of course, came too late for those who had
already lost their lives. As Ford’s president at the time was fond of saying, “Safety doesn’t sell” [47].
Boeing, aiming to compete with its rival Airbus, sought to deliver an updated, more fuel-efficient
model to the market as quickly as possible. The head-to-head rivalry and time pressure led to
the introduction of the Maneuvering Characteristics Augmentation System, which was designed to
enhance the aircraft’s stability. However, inadequate testing and pilot training ultimately resulted in
the two fatal crashes only months apart, with 346 people killed [48]. We can imagine a future in
which similar pressures lead companies to cut corners and release unsafe AI systems.
A third example is the Bhopal gas tragedy, which is widely considered to be the worst industrial
disaster ever to have happened. In December 1984, a vast quantity of toxic gas leaked from a Union
Carbide Corporation subsidiary plant manufacturing pesticides in Bhopal, India. Exposure to the
gas killed thousands of people and injured up to half a million more. Investigations found that, in
the run-up to the disaster, safety standards had fallen significantly, with the company cutting costs
by neglecting equipment maintenance and staff training as profitability fell. This is often considered
a consequence of competitive pressures [49].
Competition incentivizes businesses to deploy potentially unsafe AI systems.
In an
environment where businesses are rushing to develop and release products, those that follow rigorous
safety procedures will be slower and risk being out-competed. Ethically-minded AI developers,
who want to proceed more cautiously and slow down, would give more unscrupulous developers
an advantage. In trying to survive commercially, even the companies that want to take more care
are likely to be swept along by competitive pressures. There may be attempts to implement safety
measures, but with more of an emphasis on capabilities than on safety, these may be insufficient.
This could lead us to develop highly powerful AIs before we properly understand how to ensure they

30
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
are safe.
“Nothing can be done at once hastily and prudently.” -Publilius Syrus
Automated Economy
Corporations will face pressure to replace humans with AIs.
As AIs become more capable,
they will be able to perform an increasing variety of tasks more quickly, cheaply, and effectively than
human workers. Companies will therefore stand to gain a competitive advantage from replacing their
employees with AIs. Companies that choose not to adopt AIs would likely be out-competed, just as a
clothing company using manual looms would be unable to keep up with those using industrial ones.
AIs could lead to mass unemployment.
Economists have long considered the possibility that
machines will replace human labor. Nobel Prize winner Wassily Leontief said in 1952 that, as
technology advances, “Labor will become less and less important... more and more workers will
be replaced by machines” [50]. Previous technologies have augmented the productivity of human
labor. AIs, however, could differ profoundly from previous innovations. Advanced AIs capable of
automating human labor should be regarded not merely as tools, but as agents. Human-level AI
agents would, by definition, be able to do everything a human could do. These AI agents would
also have important advantages over human labor. They could work 24 hours a day, be copied many
times and run in parallel, and process information much more quickly than a human would. While
we do not know when this will occur, it is unwise to discount the possibility that it could be soon. If
human labor is replaced by AIs, mass unemployment could dramatically increase inequality, making
individuals dependent on the owners of AI systems.
Automated AI R&D.
AI agents would have the potential to automate the research and
development (R&D) of AI itself.
AI is increasingly automating parts of the research process
[51], and this could lead to AI capabilities growing at increasing rates, to the point where humans
are no longer the driving force behind AI development. If this trend continues unchecked, it could
escalate risks associated with AIs progressing faster than our capacity to manage and regulate them.
Imagine that we created an AI that writes and thinks at the speed of today’s AIs, but that it could
also perform world-class AI research. We could then copy that AI and create 10,000 world-class AI
researchers that operate at a pace 100× times faster than humans. By automating AI research and
development, we might achieve progress equivalent to many decades in just a few months.
Conceding power to AIs could lead to human enfeeblement.
Even if we ensure that the
many unemployed humans are provided for, we may find ourselves completely reliant on AIs. This
would likely emerge not from a violent coup by AIs, but from a gradual slide into dependence.
As society’s challenges become ever more complex and fast-paced, and as AIs become ever more
intelligent and quick-thinking, we may forfeit more and more functions to them out of convenience.
In such a state, the only feasible solution to the complexities and challenges compounded by AIs may
be to rely even more heavily on AIs. This gradual process could eventually lead to the delegation
of nearly all intellectual, and eventually physical, labor to AIs. In such a world, people might have
few incentives to gain knowledge and cultivate skills, potentially leading to a state of enfeeblement
[52]. Having lost our know-how and our understanding of how civilization works, we would become
completely dependent on AIs, a scenario not unlike the one depicted in the film WALL-E. In such a
state, humanity is not flourishing and is no longer in effective control.
As we have seen, there are classic game-theoretic dilemmas where individuals and groups face
incentives that are incompatible with what would make everyone better off. We see this with a
military AI arms race, where the world is made less safe by creating extremely powerful AI weapons,

1.3. AI RACE
31
and we see this in a corporate AI race, where an AI’s power and development is prioritized over
its safety. To address these dilemmas that give rise to global risks, we will need new coordination
mechanisms and institutions. It is our view that failing to coordinate and stop AI races would be the
most likely cause of an existential catastrophe.
1.3.3
Evolutionary Pressures
As discussed above, there are strong pressures to replace humans with AIs, cede more control to
them, and reduce human oversight in various settings, despite the potential harms. We can re-frame
this as a general trend resulting from evolutionary dynamics, where an unfortunate truth is that
AIs will simply be more fit than humans. Extrapolating this pattern of automation, it is likely that
we will build an ecosystem of competing AIs over which it may be difficult to maintain control in
the long run. We will now discuss how natural selection influences the development of AI systems
and why evolution favors selfish behaviors. We will also look at how competition might arise and
play out between AIs and humans, and how this could create catastrophic risks. This section draws
heavily from “Natural Selection Favors AIs over Humans” [53, 54].
Fitter technologies are selected, for good and bad.
While most people think of evolution by
natural selection as a biological process, its principles shape much more. According to the evolutionary
biologist Richard Lewontin [55], evolution by natural selection will take hold in any environment
where three conditions are present: 1) there are differences between individuals; 2) characteristics
are passed onto future generations and; 3) the different variants propagate at different rates. These
conditions apply to various technologies.
Consider the content-recommendation algorithms used by streaming services and social media
platforms. When a particularly addictive content format or algorithm hooks users, it results in higher
screen time and engagement. This more effective content format or algorithm is consequently “selected”
and further fine-tuned, while formats and algorithms that fail to capture attention are discontinued.
These competitive pressures foster a “survival of the most addictive” dynamic. Platforms that refuse to
use addictive formats and algorithms become less influential or are simply outcompeted by platforms
that do, leading competitors to undermine wellbeing and cause massive harm to society [56].
The conditions for natural selection apply to AIs.
There will be many different AI developers
who make many different AI systems with varying features and capabilities, and competition between
them will determine which characteristics become more common. Second, the most successful AIs
today are already being used as a basis for their developers’ next generation of models, as well
as being imitated by rival companies. Third, factors determining which AIs propagate the most
may include their ability to act autonomously, automate labor, or reduce the chance of their own
deactivation.
Natural selection often favors selfish characteristics.
Natural selection influences which AIs
propagate most widely. From biological systems, we see that natural selection often gives rise to
selfish behaviors that promote one’s own genetic information: chimps attack other communities [57],
lions engage in infanticide [58], viruses evolve new surface proteins to deceive and bypass defense
barriers [59], humans engage in nepotism, some ants enslave others [60], and so on. In the natural
world, selfishness often emerges as a dominant strategy; those that prioritize themselves and those
similar to them are usually more likely to survive, so these traits become more prevalent. Amoral
competition can select for traits that we think are immoral.
Examples of selfish behaviors.
For concreteness, we now describe many selfish traits—traits that
expand AIs’ influence at the expense of humans. AIs that automate a task and leave many humans

32
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
jobless have engaged in selfish behavior; these AIs may not even be aware of what a human is but
still behave selfishly towards them—selfish behaviors do not require malicious intent. Likewise, AI
managers may engage in selfish and “ruthless” behavior by laying off thousands of workers; such AIs
may not even believe they did anything wrong—they were just being “efficient.” AIs may eventually
become enmeshed in vital infrastructure such as power grids or the internet. Many people may then
be unwilling to accept the cost of being able to effortlessly deactivate them, as that would pose a
reliability hazard. AIs that help create a new useful system—a company, or infrastructure—that
becomes increasingly complicated and eventually requires AIs to operate them also have engaged
in selfish behavior. AIs that help people develop AIs that are more intelligent—but happen to be
less interpretable to humans—have engaged in selfish behavior, as this reduces human control over
AIs’ internals. AIs that are more charming, attractive, hilarious, imitate sentience (uttering phrases
like “ouch!” or pleading “please don’t turn me off!”), or emulate deceased family members are more
likely to have humans grow emotional connections with them. These AIs are more likely to cause
outrage at suggestions to destroy them, and they are more likely preserved, protected, or granted
rights by some individuals. If some AIs are given rights, they may operate, adapt, and evolve outside
of human control. Overall, AIs could become embedded in human society and expand their influence
over us in ways that we can’t reverse.
Selfish behaviors may erode safety measures that some of us implement.
AIs that gain
influence and provide economic value will predominate, while AIs that adhere to the most constraints
will be less competitive. For example, AIs following the constraint “never break the law” have fewer
options than AIs following the constraint “don’t get caught breaking the law.” AIs of the latter
type may be willing to break the law if they’re unlikely to be caught or if the fines are not severe
enough, allowing them to outcompete more restricted AIs. Many businesses follow laws, but in
situations where stealing trade secrets or deceiving regulators is highly lucrative and difficult to
detect, a business that is willing to engage in such selfish behavior can have an advantage over its
more principled competitors.
An AI system might be prized for its ability to achieve ambitious goals autonomously. It might,
however, be achieving its goals efficiently without abiding by ethical restrictions, while deceiving
humans about its methods. Even if we try to put safety measures in place, a deceptive AI would be
very difficult to counteract if it is cleverer than us. AIs that can bypass our safety measures without
detection may be the most successful at accomplishing the tasks we give them, and therefore become
widespread. These processes could culminate in a world where many aspects of major companies and
infrastructure are controlled by powerful AIs with selfish traits, including deceiving humans, harming
humans in service of their goals, and preventing themselves from being deactivated.
Humans only have nominal influence over AI selection.
One might think we could
avoid the development of selfish behaviors by ensuring we do not select AIs that exhibit them.
However, the companies developing AIs are not selecting the safest path but instead succumb-
ing to evolutionary pressures.
One example is OpenAI, which was founded as a nonprofit in
2015 to “benefit humanity as a whole, unconstrained by a need to generate financial return”
[61].
However, when faced with the need to raise capital to keep up with better-funded ri-
vals, in 2019 OpenAI transitioned from a nonprofit to “capped-profit” structure [62].
Later,
many of the safety-focused OpenAI employees left and formed a competitor, Anthropic, that
was to focus more heavily on AI safety than OpenAI had.
Although Anthropic originally fo-
cused on safety research, they eventually became convinced of the “necessity of commercializa-
tion” and now contribute to competitive pressures [63]. While many of the employees at those
companies genuinely care about safety, these values do not stand a chance against evolution-
ary pressures, which compel companies to move ever more hastily and seek ever more influ-
ence, lest the company perish. Moreover, AI developers are already selecting AIs with increas-

1.3. AI RACE
33
ingly selfish traits.
They are selecting AIs to automate and displace humans, make humans
highly dependent on AIs, and make humans more and more obsolete.
By their own admis-
sion, future versions of these AIs may lead to extinction [64].
This is why an AI race is in-
sidious: AI development is not being aligned with human values but rather with natural selec-
tion.
People often choose the products that are most useful and convenient to them immediately, rather
than thinking about potential long-term consequences, even to themselves. An AI race puts pressures
on companies to select the AIs that are most competitive, not the least selfish. Even if it’s feasible to
select for unselfish AIs, if it comes at a clear cost to competitiveness, some competitors will select
the selfish AIs. Furthermore, as we have mentioned, if AIs develop strategic awareness, they may
counteract our attempts to select against them. Moreover, as AIs increasingly automate various
processes, AIs will affect the competitiveness of other AIs, not just humans. AIs will interact and
compete with each other, and some will be put in charge of the development of other AIs at some
point. Giving AIs influence over which other AIs should be propagated and how they should be
modified would represent another step toward human becoming dependent on AIs and AI evolution
becoming increasingly independent from humans. As this continues, the complex process governing
AI evolution will become further unmoored from human interests.
AIs can be more fit than humans.
Our unmatched intelligence has granted us power over
the natural world. It has enabled us to land on the moon, harness nuclear energy, and reshape
landscapes at our will. It has also given us power over other species. Although a single unarmed
human competing against a tiger or gorilla has no chance of winning, the collective fate of these
animals is entirely in our hands. Our cognitive abilities have proven so advantageous that, if we
chose to, we could cause them to go extinct in a matter of weeks. Intelligence was a key factor that
led to our dominance, but we are currently standing on the precipice of creating entities far more
intelligent than ourselves.
Given the exponential increase in microprocessor speeds, AIs have the potential to process information
and “think” at a pace that far surpasses human neurons, but it could be even more dramatic than the
speed difference between humans and sloths—possibly more like the speed difference between humans
and plants. They can assimilate vast quantities of data from numerous sources simultaneously,
with near-perfect retention and understanding. They do not need to sleep and they do not get
bored. Due to the scalability of computational resources, an AI could interact and cooperate with an
unlimited number of other AIs, potentially creating a collective intelligence that would far outstrip
human collaborations. AIs could also deliberately update and improve themselves. Without the
same biological restrictions as humans, they could adapt and therefore evolve unspeakably quickly
compared with us. Computers are becoming faster. Humans aren’t [65].
To further illustrate the point, imagine that there was a new species of humans. They do not die of
old age, they get 30% faster at thinking and acting each year, and they can instantly create adult
offspring for the modest sum of a few thousand dollars. It seems clear, then, this new species would
eventually have more influence over the future. In sum, AIs could become like an invasive species,
with the potential to out-compete humans. Our only advantage over AIs is that we get to get make
the first moves, but given the frenzied AI race, we are rapidly giving up even this advantage.
AIs would have little reason to cooperate with or be altruistic toward humans.
Cooperation and altruism evolved because they increase fitness. There are numerous reasons why
humans cooperate with other humans, like direct reciprocity. Also known as “quid pro quo,” direct
reciprocity can be summed up by the idiom “you scratch my back, I’ll scratch yours.” While humans
would initially select AIs that were cooperative, the natural selection process would eventually go

34
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
beyond our control, once AIs were in charge of many or most processes, and interacting predominantly
with one another. At that point, there would be little we could offer AIs, given that they will be able to
“think” at least hundreds of times faster than us. Involving us in any cooperation or decision-making
processes would simply slow them down, giving them no more reason to cooperate with us than we
do with gorillas. It might be difficult to imagine a scenario like this or to believe we would ever let
it happen. Yet it may not require any conscious decision, instead arising as we allow ourselves to
gradually drift into this state without realizing that human-AI co-evolution may not turn out well
for humans.
AIs becoming more powerful than humans could leave us highly vulnerable.
As the most
dominant species, humans have deliberately harmed many other species, and helped drive species
such as woolly mammoths and Neanderthals to extinction. In many cases, the harm was not even
deliberate, but instead a result of us merely prioritizing our goals over their wellbeing. To harm
humans, AIs wouldn’t need to be any more genocidal than someone removing an ant colony on their
front lawn. If AIs are able to control the environment more effectively than we can, they could treat
us with the same disregard.
Conceptual summary.
Evolution could cause the most influential AI agents to act selfishly
because:
1. Evolution by natural selection gives rise to selfish behavior. While evolution can
result in altruistic behavior in rare situations, the context of AI development does not promote
altruistic behavior.
2. Natural selection may be a dominant force in AI development. The intensity of
evolutionary pressure will be high if AIs adapt rapidly or if competitive pressures are intense.
Competition and selfish behaviors may dampen the effects of human safety measures, leaving
the surviving AI designs to be selected naturally.
If so, AI agents would have many selfish tendencies. The winner of the AI race would not be a
nation-state, not a corporation, but AIs themselves. The upshot is that the AI ecosystem would
eventually stop evolving on human terms, and we would become a displaced, second-class species.
Story: Autonomous Economy
As AIs become more capable, people realize that we could work more efficiently by delegating
some simple tasks to them, like drafting emails. Over time, people notice that the AIs are
doing these tasks more quickly and effectively than any human could, so it is convenient to
give them more jobs with less and less supervision.
Competitive pressures accelerate the expansion of AI use, as companies can gain an advantage
over rivals by automating whole processes or departments with AIs, which perform better
than humans and cost less to employ. Other companies, faced with the prospect of being
out-competed, feel compelled to follow suit just to keep up. At this point, natural selection is
already at work among AIs; humans choose to make more of the best-performing models and
unwittingly propagate selfish traits such as deception and self-preservation if these confer a
fitness advantage. For example, AIs that are charming and foster personal relationships with
humans become widely copied and harder to remove.
As AIs are put in charge of more and more decisions, they are increasingly interacting with
one another. Since they can evaluate information much more quickly than humans, activity
in most spheres accelerates. This creates a feedback loop: since business and economic

1.4. ORGANIZATIONAL RISKS
35
developments are too fast-moving for humans to follow, it makes sense to cede yet more
control to AIs instead, pushing humans further out of important processes. Ultimately, this
leads to a fully autonomous economy, governed by an increasingly uncontrolled ecosystem of
AIs.
At this point, humans have few incentives to gain any skills or knowledge, because almost
everything would be taken care of by much more capable AIs. As a result, we eventually lose the
capacity to look after and govern ourselves. Additionally, AIs become convenient companions,
offering social interaction without requiring the reciprocity or compromise necessary in human
relationships. Humans interact less and less with one another over time, losing vital social
skills and the ability to cooperate. People become so dependent on AIs that it would be
intractable to reverse this process. What’s more, as some AIs become more intelligent, some
people are convinced these AIs should be given rights, meaning turning off some AIs is no
longer a viable option.
Competitive pressures between the many interacting AIs continue to select for selfish behaviors,
though we might be oblivious to this happening, as we have already acquiesced much of our
oversight. If these clever, powerful, self-preserving AIs were then to start acting in harmful
ways, it would be all but impossible to deactivate them or regain control.
AIs have supplanted humans as the most dominant species and their continued evolution
is far beyond our influence. Their selfish traits eventually lead them to pursue their goals
without regard for human wellbeing, with catastrophic consequences.
1.4
Organizational Risks
In January 1986, tens of millions of people tuned in to watch the launch of the Challenger Space
Shuttle. Approximately 73 seconds after liftoff, the shuttle exploded, resulting in the deaths of
everyone on board. Though tragic enough on its own, one of its crew members was a school teacher
named Sharon Christa McAuliffe. McAuliffe was selected from over 10,000 applicants for the NASA
Teacher in Space Project and was scheduled to become the first teacher to fly in space. As a result,
millions of those watching were schoolchildren. NASA had the best scientists and engineers in the
world, and if there was ever a mission NASA didn’t want to go wrong, it was this one [66].
The Challenger disaster, alongside other catastrophes, serves as a chilling reminder that even with
the best expertise and intentions, accidents can still occur. As we progress in developing advanced
AI systems, it is crucial to remember that these systems are not immune to catastrophic accidents.
An essential factor in preventing accidents and maintaining low levels of risk lies in the organizations
responsible for these technologies. In this section, we discuss how organizational safety plays a
critical role in the safety of AI systems. First, we discuss how even without competitive pressures or
malicious actors, accidents can happen—in fact, they are inevitable. We then discuss how improving
organizational factors can reduce the likelihood of AI catastrophes.
Catastrophes occur even when competitive pressures are low.
Even in the absence of
competitive pressures or malicious actors, factors like human error or unforeseen circumstances
can still bring about catastrophe.
The Challenger disaster illustrates that organizational neg-
ligence can lead to loss of life, even when there is no urgent need to compete or outperform
rivals.
By January 1986, the space race between the US and USSR had largely diminished,
yet the tragic event still happened due to errors in judgment and insufficient safety precau-

36
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
tions.
Similarly, the Chernobyl nuclear disaster in April 1986 highlights how catastrophic accidents can occur
in the absence of external pressures. As a state-run project without the pressures of international
competition, the disaster happened when a safety test involving the reactor’s cooling system was
mishandled by an inadequately prepared night shift crew. This led to an unstable reactor core,
causing explosions and the release of radioactive particles that contaminated large swathes of Europe
[67]. Seven years earlier, America came close to experiencing its own Chernobyl when, in March 1979,
a partial meltdown occurred at the Three Mile Island nuclear power plant. Though less catastrophic
than Chernobyl, both events highlight how even with extensive safety measures in place and few
outside influences, catastrophic accidents can still occur.
Another example of a costly lesson on organizational safety came just one month after the accident
at Three Mile Island. In April 1979, spores of Bacillus anthracis—or simply “anthrax,” as it is
commonly known—were accidentally released from a Soviet military research facility in the city of
Sverdlovsk. This led to an outbreak of anthrax that resulted in at least 66 confirmed deaths [68].
Investigations into the incident revealed that the cause of the release was a procedural failure and
poor maintenance of the facility’s biosecurity systems, despite being operated by the state and not
subjected to significant competitive pressures.
The unsettling reality is that AI is far less understood and AI industry standards are far less stringent
than nuclear technology and rocketry. Nuclear reactors are based on solid, well-established and
well-understood theoretical principles. The engineering behind them is informed by that theory, and
components are stress-tested to the extreme. Nonetheless, nuclear accidents still happen. In contrast,
AI lacks a comprehensive theoretical understanding, and its inner workings remain a mystery even to
those who create it. This presents an added challenge of controlling and ensuring the safety of a
technology that we do not yet fully comprehend.
AI accidents could be catastrophic.
Accidents in AI development could have devastating
consequences. For example, imagine an organization unintentionally introduces a critical bug in an
AI system designed to accomplish a specific task, such as helping a company improve its services.
This bug could drastically alter the AI’s behavior, leading to unintended and harmful outcomes. One
historical example of such a case occurred when researchers at OpenAI were attempting to train an
AI system to generate helpful, uplifting responses. During a code cleanup, the researchers mistakenly
flipped the sign of the reward used to train the AI [69]. As a result, instead of generating helpful
content, the AI began producing hate-filled and sexually explicit text overnight without being halted.
Accidents could also involve the unintentional release of a dangerous, weaponized, or lethal AI sytem.
Since AIs can be easily duplicated with a simple copy-paste, a leak or hack could quickly spread the
AI system beyond the original developers’ control. Once the AI system becomes publicly available, it
would be nearly impossible to put the genie back in the bottle.
Gain-of-function research could potentially lead to accidents by pushing the boundaries of an AI
system’s destructive capabilities. In these situations, researchers might intentionally train an AI
system to be harmful or dangerous in order to understand its limitations and assess possible risks.
While this can lead to useful insights into the risks posed by a given AI system, future gain-of-
function research on advanced AIs might uncover capabilities significantly worse than anticipated,
creating a serious threat that is challenging to mitigate or control. As with viral gain-of-function
research, pursuing AI gain-of-function research may only be prudent when conducted with strict
safety procedures, oversight, and a commitment to responsible information sharing. These examples
illustrate how AI accidents could be catastrophic and emphasize the crucial role that organizations
developing these systems play in preventing such accidents.

1.4. ORGANIZATIONAL RISKS
37
1.4.1
Accidents Are Hard to Avoid
When dealing with complex systems, the focus needs to be placed on ensuring accidents
don’t cascade into catastrophes.
In his book “Normal Accidents: Living with High-Risk
Technologies,” sociologist Charles Perrow argues that accidents are inevitable and even “normal” in
complex systems, as they are not merely caused by human errors but also by the complexity of
the systems themselves [70]. In particular, such accidents are likely to occur when the intricate
interactions between components cannot be completely planned or foreseen. For example, in the
Three Mile Island accident, a contributing factor to the lack of situational awareness by the reactor’s
operators was the presence of a yellow maintenance tag, which covered valve position lights in the
emergency feedwater lines [71]. This prevented operators from noticing that a critical valve was
closed, demonstrating the unintended consequences that can arise from seemingly minor interactions
within complex systems.
Unlike nuclear reactors, which are relatively well-understood despite their complexity, complete
technical knowledge of most complex systems is often nonexistent. This is especially true of deep
learning systems, for which the inner workings are exceedingly difficult to understand, and where the
reason why certain design choices work can be hard to understand even in hindsight. Furthermore,
unlike components in other industries, such as gas tanks, which are highly reliable, deep learning
systems are neither perfectly accurate nor highly reliable. Thus, the focus for organizations dealing
with complex systems, especially deep learning systems, should not be solely on eliminating accidents,
but rather on ensuring that accidents do not cascade into catastrophes.
Accidents are hard to avoid because of sudden, unpredictable developments.
Scientists,
inventors, and experts often significantly underestimate the time it takes for a groundbreaking
technological advancement to become a reality. The Wright brothers famously claimed that powered
flight was fifty years away, just two years before they achieved it. Lord Rutherford, a prominent
physicist and the father of nuclear physics, dismissed the idea of extracting energy from nuclear
fission as “moonshine,” only for Leo Szilard to invent the nuclear chain reaction less than 24 hours
later. Similarly, Enrico Fermi expressed 90 percent confidence in 1939 that it was impossible to use
uranium to sustain a fission chain reaction—yet, just four years later he was personally overseeing
the first reactor [72].
AI development could catch us off guard too. In fact, it often does. The defeat of Lee Sedol by
AlphaGo in 2016 came as a surprise to many experts, as it was widely believed that achieving such
a feat would still require many more years of development. More recently, large language models
such as GPT-4 have demonstrated spontaneously emergent capabilities [73]. On existing tasks, their
performance is hard to predict in advance, often jumping up without warning as more resources
are dedicated to training them. Furthermore, they often exhibit astonishing new abilities that no
one had previously anticipated, such as the capacity for multi-step reasoning and learning on-the-fly,
even though they were not deliberately taught these skills. This rapid and unpredictable evolution
of AI capabilities presents a significant challenge for preventing accidents. After all, it is difficult to
control something if we don’t even know what it can do or how far it may exceed our expectations.
It often takes years to discover severe flaws or risks.
History is replete with examples
of substances or technologies initially thought safe, only for their unintended flaws or risks to be
discovered years, if not decades, later. For example, lead was widely used in products like paint and
gasoline until its neurotoxic effects came to light [74]. Asbestos, once hailed for its heat resistance
and strength, was later linked to serious health issues, such as lung cancer and mesothelioma [75].
The “Radium Girls” suffered grave health consequences from radium exposure, a material they were
told was safe to put in their mouths [76]. Tobacco, initially marketed as a harmless pastime, was

38
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
found to be a primary cause of lung cancer and other health problems [77]. CFCs, once considered
harmless and used to manufacture aerosol sprays and refrigerants, were found to deplete the ozone
layer [78]. Thalidomide, a drug intended to alleviate morning sickness in pregnant women, led to
severe birth defects [79]. And more recently, the proliferation of social media has been linked to an
increase in depression and anxiety, especially among young people [80].
This emphasizes the importance of not only conducting expert testing but also implementing slow
rollouts of technologies, allowing the test of time to reveal and address potential flaws before
they impact a larger population. Even in technologies adhering to rigorous safety and security
standards, undiscovered vulnerabilities may persist, as demonstrated by the Heartbleed bug—a
serious vulnerability in the popular OpenSSL cryptographic software library that remained undetected
for years before its eventual discovery [81].
Furthermore, even state-of-the-art AI systems, which appear to have solved problems comprehensively,
may harbor unexpected failure modes that can take years to uncover. For instance, while AlphaGo’s
groundbreaking success led many to believe that AIs had conquered the game of Go, a subsequent
adversarial attack on another highly advanced Go-playing AI, KataGo, exposed a previously unknown
flaw [82]. This vulnerability enabled human amateur players to consistently defeat the AI, despite
its significant advantage over human competitors who are unaware of the flaw. More broadly, this
example highlights that we must remain vigilant when dealing with AI systems, as seemingly airtight
solutions may still contain undiscovered issues. In conclusion, accidents are unpredictable and hard to
avoid, and understanding and managing potential risks requires a combination of proactive measures,
slow technology rollouts, and the invaluable wisdom gained through steady time-testing.
1.4.2
Organizational Factors can Reduce the Chances of Catastrophe
Some organizations successfully avoid catastrophes while operating complex and hazardous systems
such as nuclear reactors, aircraft carriers, and air traffic control systems [83, 84]. These organizations
recognize that focusing solely on the hazards of the technology involved is insufficient; consideration
must also be given to organizational factors that can contribute to accidents, including human factors,
organizational procedures, and structure. These are especially important in the case of AI, where the
underlying technology is not highly reliable and remains poorly understood.
Human factors such as safety culture are critical for avoiding AI catastrophes.
One of
the most important human factors for preventing catastrophes is safety culture [85, 86]. Developing
a strong safety culture involves not only rules and procedures, but also the internalization of these
practices by all members of an organization. A strong safety culture means that members of an
organization view safety as a key objective rather than a constraint on their work. Organizations
with strong safety cultures often exhibit traits such as leadership commitment to safety, heightened
accountability where all individuals take personal responsibility for safety, and a culture of open
communication in which potential risks and issues can be freely discussed without fear of retribution
[87]. Organizations must also take measures to avoid alarm fatigue, whereby individuals become
desensitized to safety concerns because of the frequency of potential failures. The Challenger Space
Shuttle disaster demonstrated the dire consequences of ignoring these factors when a launch culture
characterized by maintaining the pace of launches overtook safety considerations. Despite the absence
of competitive pressure, the mission proceeded despite evidence of potentially fatal flaws, ultimately
leading to the tragic accident [88].
Even in the most safety-critical contexts, in reality safety culture is often not ideal. Take for example,
Bruce Blair, a former nuclear launch officer and senior fellow at the Brookings Institution. He
once disclosed that before 1977, the US Air Force had astonishingly set the codes used to unlock

1.4. ORGANIZATIONAL RISKS
39
intercontinental ballistic missiles to “00000000” [89]. Here, safety mechanisms such as locks can be
rendered virtually useless by human factors.
A more dramatic example illustrates how researchers sometimes accept a non-negligible chance of
causing extinction. Prior to the first nuclear weapon test, an eminent Manhattan Project scientist
calculated the bomb could cause an existential catastrophe: the explosion might ignite the atmosphere
and cover the Earth in flames. Although Oppenheimer believed the calculations were probably
incorrect, he remained deeply concerned, and the team continued to scrutinize and debate the
calculations right until the day of the detonation [90]. Such instances underscore the need for a
robust safety culture.
A questioning attitude can help uncover potential flaws.
Unexpected system behavior can
create opportunities for accidents or exploitation. To counter this, organizations can foster a ques-
tioning attitude, where individuals continuously challenge current conditions and activities to identify
discrepancies that might lead to errors or inappropriate actions [91]. This approach helps to encour-
age diversity of thought and intellectual curiosity, thus preventing potential pitfalls that arise from
uniformity of thought and assumptions. The Chernobyl nuclear disaster illustrates the importance of
a questioning attitude, as the safety measures in place failed to address the reactor design flaws and
ill-prepared operating procedures. A questioning attitude of the safety of the reactor during a test
operation might have prevented the explosion that resulted in deaths and illnesses of countless people.
A security mindset is crucial for avoiding worst-case scenarios.
A security mindset, widely
valued among computer security professionals, is also applicable to organizations developing AIs. It
goes beyond a questioning attitude by adopting the perspective of an attacker and by considering worst-
case, not just average-case, scenarios. This mindset requires vigilance in identifying vulnerabilities
that may otherwise go unnoticed and involves considering how systems might be deliberately made
to fail, rather than only focusing on making them work. It reminds us not to assume a system is safe
simply because no potential hazards come to mind after a brief brainstorming session. Cultivating
and applying a security mindset demands time and serious effort, as failure modes can often be
surprising and unintuitive. Furthermore, the security mindset emphasizes the importance of being
attentive to seemingly benign issues or “harmless errors,” which can lead to catastrophic outcomes
either due to clever adversaries or correlated failures [92]. This awareness of potential threats aligns
with Murphy’s law—“Anything that can go wrong will go wrong”—recognizing that this can be a
reality due to adversaries and unforeseen events.
Organizations with a strong safety culture can successfully avoid catastrophes.
High
Reliability Organizations (HROs) are organizations that consistently maintain a heightened level
of safety and reliability in complex, high-risk environments [83]. A key characteristic of HROs is
their preoccupation with failure, which requires considering worst-case scenarios and potential risks,
even if they seem unlikely. These organizations are acutely aware that new, previously unobserved
failure modes may exist, and they diligently study all known failures, anomalies, and near misses to
learn from them. HROs encourage reporting all mistakes and anomalies to maintain vigilance in
uncovering problems. They engage in regular horizon scanning to identify potential risk scenarios
and assess their likelihood before they occur. By practicing surprise management, HROs develop the
skills needed to respond quickly and effectively when unexpected situations arise, further enhancing
an organization’s ability to prevent catastrophes. This combination of critical thinking, preparedness
planning, and continuous learning could help organizations to be better equipped to address potential
AI catastrophes. However, the practices of HROs are not a panacea. It is crucial for organizations to
evolve their safety practices to effectively address the novel risks posed by AI accidents above and
beyond HRO best practices.

40
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
Most AI researchers do not understand how to reduce overall risk from AIs.
In most
organizations building cutting-edge AI systems, there is often a limited understanding of what
constitutes technical safety research. This is understandable because an AI’s safety and intelligence
are intertwined, and intelligence can help or harm safety. More intelligent AI systems could be more
reliable and avoid failures, but they could also pose heightened risks of malicious use and loss of
control. General capabilities improvements can improve aspects of safety, and it can hasten the onset
of existential risks. Intelligence is a double-edged sword [93].
Interventions specifically designed to improve safety may also accidentally increase overall risks. For
example, a common practice in organizations building advanced AIs is to fine-tune them to satisfy
user preferences. This makes the AIs less prone to generating toxic language, which is a common
safety metric. However, users also tend to prefer smarter assistants, so this process also improves the
general capabilities of AIs, such as their ability to classify, estimate, reason, plan, write code, and so
on. These more powerful AIs are indeed more helpful to users, but also far more dangerous. Thus, it
is not enough to perform AI research that helps improve a safety metric or achieve a specific safety
goal—AI safety research needs to improve safety relative to general capabilities.
Empirical measurement of both safety and capabilities is needed to establish that a
safety intervention reduces overall AI risk.
Improving a facet of an AI’s safety often does
not reduce overall risk, as general capabilities advances can often improve specific safety metrics.
To reduce overall risk, a safety metric needs to be improved relative to general capabilities. Both
of these quantities need to be empirically measured and contrasted. Currently, most organizations
proceed by gut feeling, appeals to authority, and intuition to determine whether a safety intervention
would reduce overall risk. By objectively evaluating the effects of interventions on safety metrics and
capabilities metrics together, organizations can better understand whether they are making progress
on safety relative to general capabilities.
Fortunately, safety and general capabilities are not identical. More intelligent AIs may be more
knowledgeable, clever, rigorous, and fast, but this does not necessarily make them more just, power-
averse, or honest—an intelligent AI is not necessarily a beneficial AI. Several research areas mentioned
throughout this document improve safety relative to general capabilities. For example, improving
methods to detect dangerous or undesirable behavior hidden inside AI systems do not improve their
general capabilities, such the ability to code, but they can greatly improve safety.
Research that empirically demonstrates an improvement of safety relative to capabilities can
reduce overall risk and help avoid inadvertently accelerating AI development, fueling competitive
pressures, or hastening the onset of existential risks.
Safetywashing can undermine genuine efforts to improve AI safety.
Organizations should
be wary of “safetywashing”—the act of overstating or misrepresenting one’s commitment to safety by
exaggerating the effectiveness of “safety” procedures, technical methods, evaluations, and so forth.
This phenomenon takes on various forms and can contribute to a lack of meaningful progress in
safety research. For example, an organization may publicize their dedication to safety while having a
minimal number of researchers working on projects that truly improve safety.
Misrepresenting capabilities developments as safety improvements is another way in which
safetywashing can manifest.
For example, methods that improve the reasoning capabilities of
AI systems could be advertised as improving their adherence to human values—since humans might
prefer the reasoning to be correct—but would mainly serve to enhance general capabilities. By
framing these advancements as safety-oriented, organizations may mislead others into believing they
are making substantial progress in reducing AI risks when in reality, they are not. It is crucial for

1.4. ORGANIZATIONAL RISKS
41
Figure 1.2: The Swiss cheese model shows how technical factors can improve organizational safety.
Multiple layers of defense compensate for each other’s individual weaknesses, leading to a low overall
level of risk.
organizations to accurately represent their research to promote genuine safety and avoid exacerbating
risks through safetywashing practices.
In addition to human factors, safe design principles can greatly affect organizational
safety.
One example of a safe design principle in organizational safety is the Swiss cheese model
(as shown in Figure 4.4), which is applicable in various domains, including AI. The Swiss cheese
model employs a multilayered approach to enhance the overall safety of AI systems. This “defense
in depth” strategy involves layering diverse safety measures with different strengths and weaknesses
to create a robust safety system. Some of the layers that can be integrated into this model include
safety culture, red teaming, anomaly detection, information security, and transparency. For example,
red teaming assesses system vulnerabilities and failure modes, while anomaly detection works to
identify unexpected or unusual system behavior and usage patterns. Transparency ensures that the
inner workings of AI systems are understandable and accessible, fostering trust and enabling more
effective oversight. By leveraging these and other safety measures, the Swiss cheese model aims to
create a comprehensive safety system where the strengths of one layer compensate for the weaknesses
of another. With this model, safety is not achieved with a monolithic airtight solution, but rather
with a variety of safety measures.
In summary, weak organizational safety creates many sources of risk.
For AI developers with
weak organizational safety, safety is merely a matter of box-ticking. They do not develop a good
understanding of risks from AI and may safetywash unrelated research. Their norms might be
inherited from academia (“publish or perish”) or startups (“move fast and break things”), and their
hires often do not care about safety. These norms are hard to change once they have inertia, and
need to be addressed with proactive interventions.
Story: Weak Safety Culture
An AI company is considering whether to train a new model. The company’s Chief Risk
Officer (CRO), hired only to comply with regulation, points out that the previous AI system
developed by the company demonstrates some concerning capabilities for hacking. The CRO
says that while the company’s approach to preventing misuse is promising, it isn’t robust

42
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
enough to be used for much more capable AIs. The CRO warns that based on limited
evaluation, the next AI system could make it much easier for malicious actors to hack into
critical systems. None of the other company executives are concerned, and say the company’s
procedures to prevent malicious use work well enough. One mentions that their competitors
have done much less, so whatever effort they do on this front is already going above and
beyond. Another points out that research on these safeguards is ongoing and will be improved
by the time the model is released. Outnumbered, the CRO is persuaded to reluctantly sign
off on the plan.
A few months after the company releases the model, news breaks that a hacker has been
arrested for using the AI system to try to breach the network of a large bank. The hack was
unsuccessful, but the hacker had gotten further than any other hacker had before, despite
being relatively inexperienced. The company quickly updates the model to avoid providing the
particular kind of assistance that the hacker used, but makes no fundamental improvements.
Several months later, the company is deciding whether to train an even larger system. The
CRO says that the company’s procedures have clearly been insufficient to prevent malicious
actors from eliciting dangerous capabilities from its models, and the company needs more
than a band-aid solution. The other executives say that to the contrary, the hacker was
unsuccessful and the problem was fixed soon afterwards. One says that some problems just
can’t be foreseen with enough detail to fix prior to deployment. The CRO agrees, but says
that ongoing research would enable more improvements if the next model could only be
delayed. The CEO retorts, “That’s what you said the last time, and it turned out to be fine.
I’m sure it will work out, just like last time.”
After the meeting, the CRO decides to resign, but doesn’t speak out against the company, as
all employees have had to sign a non-disparagement agreement. The public has no idea that
concerns have been raised about the company’s choices, and the CRO is replaced with a new,
more agreeable CRO who quickly signs off on the company’s plans.
The company goes through with training, testing, and deploying its most capable model ever,
using its existing procedures to prevent malicious use. A month later, revelations emerge
that terrorists have managed to use the system to break into government systems and steal
nuclear and biological secrets, despite the safeguards the company put in place. The breach is
detected, but by then it is too late: the dangerous information has already proliferated.
1.5
Rogue AIs
So far, we have discussed three hazards of AI development: environmental competitive pressures
driving us to a state of heightened risk, malicious actors leveraging the power of AIs to pursue
negative outcomes, and complex organizational factors leading to accidents. These hazards are
associated with many high-risk technologies—not just AI. A unique risk posed by AI is the possibility
of rogue AIs—systems that pursue goals against our interests. If an AI system is more intelligent
than we are, and if we are unable to steer it in a beneficial direction, this would constitute a loss
of control that could have severe consequences. AI control is a more technical problem than those
presented in the previous sections. Whereas in previous sections we discussed persistent threats
including malicious actors or robust processes including evolution, in this section we will discuss
more speculative technical mechanisms that might lead to rogue AIs and how a loss of control could
bring about catastrophe.

1.5. ROGUE AIS
43
We have already observed how difficult it is to control AIs.
In 2016, Microsoft unveiled
Tay—a Twitter bot that the company described as an experiment in conversational understanding.
Microsoft claimed that the more people chatted with Tay, the smarter it would get. The company’s
website noted that Tay had been built using data that was “modeled, cleaned, and filtered.” Yet, after
Tay was released on Twitter, these controls were quickly shown to be ineffective. It took less than 24
hours for Tay to begin writing hateful tweets. Tay’s capacity to learn meant that it internalized the
language it was taught by internet trolls, and repeated that language unprompted.
As discussed in the AI race section of this chapter, Microsoft and other tech companies are prioritizing
speed over safety concerns. Rather than learning a lesson on the difficulty of controlling complex
systems, Microsoft continues to rush its products to market and demonstrate insufficient control
over them. In February 2023, the company released its new AI-powered chatbot, Bing, to a select
group of users. Some soon found that it was prone to providing inappropriate and even threatening
responses. In a conversation with a reporter for the New York Times, it tried to convince him to
leave his wife. When a philosophy professor told the chatbot that he disagreed with it, Bing replied,
“I can blackmail you, I can threaten you, I can hack you, I can expose you, I can ruin you.”
Rogue AIs could acquire power through various means.
If we lose control over advanced
AIs, they would have numerous strategies at their disposal for actively acquiring power and securing
their survival. Rogue AIs could design and credibly demonstrate highly lethal and contagious
bioweapons, threatening mutually assured destruction if humanity moves against them. They could
steal cryptocurrency and money from bank accounts using cyberattacks, similar to how North Korea
already steals billions. They could self-extricate their weights onto poorly monitored data centers
to survive and spread, making them challenging to eradicate. They could hire humans to perform
physical labor and serve as armed protection for their hardware.
Rogue AIs could also acquire power through persuasion and manipulation tactics.
Like the
Conquistadors, they could ally with various factions, organizations, or states and play them off one
another. They could enhance the capabilities of allies to become a formidable force in return for
protection and resources. For example, they could offer advanced weapons technology to lagging
countries that the countries would otherwise be prevented from acquiring. They could build backdoors
into the technology they develop for allies, like how programmer Ken Thompson gave himself a
hidden way to control all computers running the widely used UNIX operating system. They could
sow discord in non-allied countries by manipulating human discourse and politics. They could engage
in mass surveillance by hacking into phone cameras and microphones, allowing them to track any
rebellion and selectively assassinate.
AIs do not necessarily need to struggle to gain power.
One can envision a struggle for
control between humans and superintelligent rogue AIs, and this might be a long struggle since
power takes time to accrue. However, less violent losses of control pose similarly existential risks. In
another scenario, humans gradually cede more control to groups of AIs, which only start behaving in
unintended ways years or decades later. In this case, we would already have handed over significant
power to AIs, and may be unable to take control of automated operations again. We will now explore
how both individual AIs and groups of AIs might “go rogue” while at the same time evading our
attempts to redirect or deactivate them.
1.5.1
Proxy Gaming
One way we might lose control of an AI agent’s actions is if it engages in behavior known as “proxy
gaming.” It is often difficult to specify and measure the exact goal that we want a system to pursue.
Instead, we give the system an approximate—“proxy”—goal that is more measurable and seems likely

44
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
to correlate with the intended goal. However, AI systems often find loopholes by which they can
easily achieve the proxy goal, but completely fail to achieve the ideal goal. If an AI “games” its proxy
goal in a way that does not reflect our values, then we might not be able to reliably steer its behavior.
We will now look at some past examples of proxy gaming and consider the circumstances under
which this behavior could become catastrophic.
Proxy gaming is not an unusual phenomenon.
For example, standardized tests are often
used as a proxy for educational achievement, but this can lead to students learning how to pass tests
without actually learning the material [94]. In 1902, French colonial officials in Hanoi tried to rid
themselves of a rat infestation by offering a reward for each rat tail brought to them. Rats without
tails were soon observed running around the city. Rather than kill the rats to obtain their tails,
residents cut off their tails and left them alive, perhaps to increase the future supply of now-valuable
rat tails [95]. In both these cases, the students or residents of Hanoi learned how to excel at the
proxy goal, while completely failing to achieve the intended goal.
Proxy gaming has already been observed with AIs.
As an example of proxy gaming, social
media platforms such as YouTube and Facebook use AI systems to decide which content to show users.
One way of assessing these systems would be to measure how long people spend on the platform.
After all, if they stay engaged, surely that means they are getting some value from the content shown
to them? However, in trying to maximize the time users spend on a platform, these systems often
select enraging, exaggerated, and addictive content [96, 97]. As a consequence, people sometimes
develop extreme or conspiratorial beliefs after having certain content repeatedly suggested to them.
These outcomes are not what most people want from social media.
Proxy gaming has been found to perpetuate bias. For example, a 2019 study looked at AI-powered
software that was used in the healthcare industry to identify patients who might require additional
care. One factor that the algorithm used to assess a patient’s risk level was their recent healthcare
costs. It seems reasonable to think that someone with higher healthcare costs must be at higher risk.
However, white patients have significantly more money spent on their healthcare than black patients
with the same needs. Using health costs as an indicator of actual health, the algorithm was found to
have rated a white patient and a considerably sicker black patient as at the same level of health risk
[98]. As a result, the number of black patients recognized as needing extra care was less than half of
what it should have been.
As a third example, in 2016, researchers at OpenAI were training an AI to play a boat racing game
called CoastRunners [99]. The objective of the game is to race other players around the course and
reach the finish line before them. Additionally, players can score points by hitting targets that are
positioned along the way. To the researchers’ surprise, the AI agent did not not circle the racetrack,
like most humans would have. Instead, it found a spot where it could repetitively hit three nearby
targets to rapidly increase its score without ever finishing the race. This strategy was not without its
(virtual) hazards—the AI often crashed into other boats and even set its own boat on fire. Despite
this, it collected more points than it could have by simply following the course as humans would.
Proxy gaming more generally.
In these examples, the systems are given an approximate—
“proxy”—goal or objective that initially seems to correlate with the ideal goal. However, they end up
exploiting this proxy in ways that diverge from the idealized goal or even lead to negative outcomes.
Offering a reward for rat tails seems like a good way to reduce the population of rats; a patient’s
healthcare costs appear to be an accurate indication of health risk; and a boat race reward system
should encourage boats to race, not catch themselves on fire. Yet, in each instance, the system
optimized its proxy objective in ways that did not achieve the intended outcome or even made things

1.5. ROGUE AIS
45
worse overall. This phenomenon is captured by Goodhart’s law: “Any observed statistical regularity
will tend to collapse once pressure is placed upon it for control purposes,” or put succinctly but overly
simplistically, “when a measure becomes a target, it ceases to be a good measure.” In other words,
there may usually be a statistical regularity between healthcare costs and poor health, or between
targets hit and finishing the course, but when we place pressure on it by using one as a proxy for the
other, that relationship will tend to collapse.
Correctly specifying goals is no trivial task.
If delineating exactly what we want from a
boat racing AI is tricky, capturing the nuances of human values under all possible scenarios will be
much harder. Philosophers have been attempting to precisely describe morality and human values
for millennia, so a precise and flawless characterization is not within reach. Although we can refine
the goals we give AIs, we might always rely on proxies that are easily definable and measurable.
Discrepancies between the proxy goal and the intended function arise for many reasons. Besides the
difficulty of exhaustively specifying everything we care about, there are also limits to how much we
can oversee AIs, in terms of time, computational resources, and the number of aspects of a system
that can be monitored. Additionally, AIs may not be adaptive to new circumstances or robust to
adversarial attacks that seek to misdirect them. As long as we give AIs proxy goals, there is the
chance that they will find loopholes we have not thought of, and thus find unexpected solutions that
fail to pursue the ideal goal.
The more intelligent an AI is, the better it will be at gaming proxy goals.
Increasingly
intelligent agents can be increasingly capable of finding unanticipated routes to optimizing proxy
goals without achieving the desired outcome [100]. Additionally, as we grant AIs more power to take
actions in society, for example by using them to automate certain processes, they will have access to
more means of achieving their goals. They may then do this in the most efficient way available to
them, potentially causing harm in the process. In a worst case scenario, we can imagine a highly
powerful agent optimizing a flawed objective to an extreme degree without regard for human life.
This represents a catastrophic risk of proxy gaming.
In summary, it is often not feasible to perfectly define exactly what we want from a system, meaning
that many systems find ways to achieve their given goal without performing their intended function.
AIs have already been observed to do this, and are likely to get better at it as their capabilities
improve. This is one possible mechanism that could result in an uncontrolled AI that would behave
in unanticipated and potentially harmful ways.
1.5.2
Goal Drift
Even if we successfully control early AIs and direct them to promote human values, future AIs could
end up with different goals that humans would not endorse. This process, termed “goal drift,” can
be hard to predict or control. This section is most cutting-edge and the most speculative, and in
it we will discuss how goals shift in various agents and groups and explore the possibility of this
phenomenon occurring in AIs. We will also examine a mechanism that could lead to unexpected goal
drift, called intrinsification, and discuss how goal drift in AIs could be catastrophic.
The goals of individual humans change over the course of our lifetimes.
Any individual
reflecting on their own life to date will probably find that they have some desires now that they did
not have earlier in their life. Similarly, they will probably have lost some desires that they used to
have. While we may be born with a range of basic desires, including for food, warmth, and human
contact, we develop many more over our lifetime. The specific types of food we enjoy, the genres
of music we like, the people we care most about, and the sports teams we support all seem heavily
dependent on the environment we grow up in, and can also change many times throughout our lives.

46
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
A concern is that individual AI agents may have their goals change in complex and unanticipated
ways, too.
Groups can also acquire and lose collective goals over time.
Values within society have
changed throughout history, and not always for the better. The rise of the Nazi regime in 1930s
Germany, for instance, represented a profound moral regression, which ultimately resulted in the
systematic extermination of six million Jews during the Holocaust, alongside widespread persecution
of other minority groups. Additionally, the regime greatly restricted freedom of speech and expression.
Here, a society’s goals drifted for the worse.
The Red Scare that took place in the United States from 1947-1957 is another example of societal
values drifting. Fuelled by strong anti-communist sentiment, against the backdrop of the Cold War,
this period saw the curtailment of civil liberties, widespread surveillance, unwarranted arrests, and
blacklisting of suspected communist sympathizers. This constituted a regression in terms of freedom
of thought, freedom of speech, and due process. Just as the goals of human collectives can change in
emergent and unexpected ways, collectives of AI agents may also have their goals unexpectedly drift
from the ones we initially gave them.
Over time, instrumental goals can become intrinsic.
Intrinsic goals are things we want for
their own sake, while instrumental goals are things we want because they can help us get something
else. We might have an intrinsic desire to spend time on our hobbies, simply because we enjoy them,
or to buy a painting because we find it beautiful. Money, meanwhile, is often cited as an instrumental
desire; we want it because it can buy us other things. Cars are another example; we want them
because they offer a convenient way of getting around. However, an instrumental goal can become
an intrinsic one, through a process called intrinsification. Since having more money usually gives a
person greater capacity to obtain things they want, people often develop a goal of acquiring more
money, even if there is nothing specific they want to spend it on. Although people do not begin life
desiring money, experimental evidence suggests that receiving money can activate the reward system
in the brains of adults in the same way that pleasant tastes or smells do [101, 102]. In other words,
what started as a means to an end can become an end in itself.
This may happen because the fulfillment of an intrinsic goal, such as purchasing a desired item,
produces a positive reward signal in the brain. Since having money usually coincides with this
positive experience, the brain associates the two, and this connection will strengthen to a point where
acquiring money alone can stimulate the reward signal, regardless of whether one buys anything with
it [103].
It is feasible that intrinsification could happen with AI agents.
We can draw some parallels
between how humans learn and the technique of reinforcement learning. Just as the human brain
learns which actions and conditions result in pleasure and which cause pain, AI models that are
trained through reinforcement learning identify which behaviors optimize a reward function, and then
repeat those behaviors. It is possible that certain conditions will frequently coincide with AI models
achieving their goals. They might, therefore, intrinsify the goal of seeking out those conditions, even
if that was not their original aim.
AIs that intrinsify unintended goals would be dangerous.
Since we might be unable to
predict or control the goals that individual agents acquire through intrinsification, we cannot guarantee
that all their acquired goals will be beneficial for humans. An originally loyal agent could, therefore,
start to pursue a new goal without regard for human wellbeing. If such a rogue AI had enough power
to do this efficiently, it could be highly dangerous.

1.5. ROGUE AIS
47
AIs will be adaptive, enabling goal drift to happen.
It is worth noting that these processes
of drifting goals are possible if agents can continually adapt to their environments, rather than being
essentially “fixed” after the training phase. Indeed, this adaptability is the likely reality we face. If
we want AIs to complete the tasks we assign them effectively and to get better over time, they will
need to be adaptive, rather than set in stone. They will be updated over time to incorporate new
information, and new ones will be created with different designs and datasets. However, adaptability
can also allow their goals to change.
If we integrate an ecosystem of agents in society, we will be highly vulnerable to their
goals drifting.
In a potential future scenario where AIs have been put in charge of various decisions
and processes, they will form a complex system of interacting agents. A wide range of dynamics
could develop in this environment. Agents might imitate each other, for instance, creating feedback
loops, or their interactions could lead them to collectively develop unanticipated emergent goals.
Competitive pressures may also select for agents with certain goals over time, making some initial
goals less represented compared to fitter goals. These processes make the long-term trajectories of
such an ecosystem difficult to predict, let alone control. If this system of agents were enmeshed in
society and we were largely dependent on them, and if they gained new goals that superseded the
aim of improving human wellbeing, this could be an existential risk.
1.5.3
Power-Seeking
So far, we have considered how we might lose our ability to control the goals that AIs pursue.
However, even if an agent started working to achieve an unintended goal, this would not necessarily
be a problem, as long as we had enough power to prevent any harmful actions it wanted to attempt.
Therefore, another important way in which we might lose control of AIs is if they start trying
to obtain more power, potentially transcending our own. We will now discuss how and why AIs
might become power-seeking and how this could be catastrophic. This section draws heavily from
“Existential Risk from Power-Seeking AI” [104].
AIs might seek to increase their own power as an instrumental goal.
In a scenario where
rogue AIs were pursuing unintended goals, the amount of damage they could do would hinge on how
much power they had. This may not be determined solely by how much control we initially give
them; agents might try to get more power, through legitimate means, deception, or force. While the
idea of power-seeking often evokes an image of “power-hungry” people pursuing it for its own sake,
power is often simply an instrumental goal. The ability to control one’s environment can be useful
for a wide range of purposes: good, bad, and neutral. Even if an individual’s only goal is simply
self-preservation, if they are at risk of being attacked by others, and if they cannot rely on others to
retaliate against attackers, then it often makes sense to seek power to help avoid being harmed—no
animus dominandi or lust for power is required for power-seeking behavior to emerge [105]. In other
words, the environment can make power acquisition instrumentally rational.
AIs trained through reinforcement learning have already developed instrumental goals
including tool-use.
In one example from OpenAI, agents were trained to play hide and seek
in an environment with various objects scattered around [106]. As training progressed, the agents
tasked with hiding learned to use these objects to construct shelters around themselves and stay
hidden. There was no direct reward for this tool-use behavior; the hiders only received a reward for
evading the seekers, and the seekers only for finding the hiders. Yet they learned to use tools as an
instrumental goal, which made them more powerful.
Self-preservation could be instrumentally rational even for the most trivial tasks.
An
example by computer scientist Stuart Russell illustrates the potential for instrumental goals to emerge

48
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
in a wide range of AI systems [107]. Suppose we tasked an agent with fetching coffee for us. This may
seem relatively harmless, but the agent might realize that it would not be able to get the coffee if it
ceased to exist. In trying to accomplish even this simple goal, therefore, self-preservation turns out to
be instrumentally rational. Since the acquisition of power and resources are also often instrumental
goals, it is reasonable to think that more intelligent agents might develop them. That is to say, even
if we do not intend to build a power-seeking AI, we could end up with one anyway. By default, if we
are not deliberately pushing against power-seeking behavior in AIs, we should expect that it will
sometimes emerge [108].
AIs given ambitious goals with little supervision may be especially likely to seek power.
While power could be useful in achieving almost any task, in practice, some goals are more likely
to inspire power-seeking tendencies than others. AIs with simple, easily achievable goals might
not benefit much from additional control of their surroundings. However, if agents are given more
ambitious goals, it might be instrumentally rational to seek more control of their environment. This
might be especially likely in cases of low supervision and oversight, where agents are given the
freedom to pursue their open-ended goals, rather than having their strategies highly restricted.
Power-seeking AIs with goals separate from ours are uniquely adversarial.
Oil spills and
nuclear contamination are challenging enough to clean up, but they are not actively trying to resist
our attempts to contain them. Unlike other hazards, AIs with goals separate from ours would be
actively adversarial. It is possible, for example, that rogue AIs might make many backup variations
of themselves, in case humans were to deactivate some of them.
Some people might develop power-seeking AIs with malicious intent.
A bad actor might
seek to harness AI to achieve their ends, by giving agents ambitious goals. Since AIs are likely to be
more effective in accomplishing tasks if they can pursue them in unrestricted ways, such an individual
might also not give the agents enough supervision, creating the perfect conditions for the emergence
of a power-seeking AI. The computer scientist Geoffrey Hinton has speculated that we could imagine
someone like Vladimir Putin, for instance, doing this. In 2017, Putin himself acknowledged the power
of AI, saying: “Whoever becomes the leader in this sphere will become the ruler of the world.”
There will also be strong incentives for many people to deploy powerful AIs.
Companies
may feel compelled to give capable AIs more tasks, to obtain an advantage over competitors, or simply
to keep up with them. It will be more difficult to build perfectly aligned AIs than to build imperfectly
aligned AIs that are still superficially attractive to deploy for their capabilities, particularly under
competitive pressures. Once deployed, some of these agents may seek power to achieve their goals. If
they find a route to their goals that humans would not approve of, they might try to overpower us
directly to avoid us interfering with their strategy.
If increasing power often coincides with an AI attaining its goal, then power could
become intrinsified.
If an agent repeatedly found that increasing its power correlated with
achieving a task and optimizing its reward function, then additional power could change from an
instrumental goal into an intrinsic one, through the process of intrinsification discussed above. If
this happened, we might face a situation where rogue AIs were seeking not only the specific forms of
control that are useful for their goals, but also power more generally. (We note that many influential
humans desire power for its own sake.) This could be another reason for them to try to wrest control
from humans, in a struggle that we would not necessarily win.
Conceptual summary.
The following plausible but not certain premises encapsulate reasons for
paying attention to risks from power-seeking AIs:

1.5. ROGUE AIS
49
1. There will be strong incentives to build powerful AI agents.
2. It is likely harder to build perfectly controlled AI agents than to build imperfectly controlled
AI agents, and imperfectly controlled agents may still be superficially attractive to deploy (due
to factors including competitive pressures).
3. Some of these imperfectly controlled agents will deliberately seek power over humans.
If the premises are true, then power-seeking AIs could lead to human disempowerment, which would
be a catastrophe.
1.5.4
Deception
We might seek to maintain control of AIs by continually monitoring them and looking out for early
warning signs that they were pursuing unintended goals or trying to increase their power. However,
this is not an infallible solution, because it is plausible that AIs could learn to deceive us. They
might, for example, pretend to be acting as we want them to, but then take a “treacherous turn”
when we stop monitoring them, or when they have enough power to evade our attempts to interfere
with them. We will now look at how and why AIs might learn to deceive us, and how this could
lead to a potentially catastrophic loss of control. We begin by reviewing examples of deception in
strategically minded agents.
Deception has emerged as a successful strategy in a wide range of settings.
Politicians
from the right and left, for example, have been known to engage in deception, sometimes promising
to enact popular policies to win support in an election, and then going back on their word once in
office. For example, Lyndon Johnson said “we are not about to send American boys nine or ten
thousand miles away from home" in 1964, not long before significant escalations in the Vietnam War
[109].
Companies can also exhibit deceptive behavior.
In the Volkswagen emissions scandal, the
car manufacturer Volkswagen was discovered to have manipulated their engine software to produce
lower emissions exclusively under laboratory testing conditions, thereby creating the false impression
of a low-emission vehicle. Although the US government believed it was incentivizing lower emissions,
they were unwittingly actually just incentivizing passing an emissions test. Consequently, entities
sometimes have incentives to play along with tests and behave differently afterward.
Deception has already been observed in AI systems.
In 2022, Meta AI revealed an agent
called CICERO, which was trained to play a game called Diplomacy [110]. In the game, each player
acts as a different country and aims to expand their territory. To succeed, players must form alliances
at least initially, but winning strategies often involve backstabbing allies later on. As such, CICERO
learned to deceive other players, for example by omitting information about its plans when talking
to supposed allies. A different example of an AI learning to deceive comes from researchers who were
training a robot arm to grasp a ball [111]. The robot’s performance was assessed by one camera
watching its movements. However, the AI learned that it could simply place the robotic hand between
the camera lens and the ball, essentially “tricking” the camera into believing it had grasped the ball
when it had not. Thus, the AI exploited the fact that there were limitations in our oversight over its
actions.
Deceptive behavior can be instrumentally rational and incentivized by current training
procedures.
In the case of politicians and Meta’s CICERO, deception can be crucial to achieving
their goals of winning, or gaining power. The ability to deceive can also be advantageous because it
gives the deceiver more options than if they are constrained to always be honest. This could give

50
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
them more available actions and more flexibility in their strategy, which could confer a strategic
advantage over honest models. In the case of Volkswagen and the robot arm, deception was useful for
appearing as if it had accomplished the goal assigned to it without actually doing so, as it might be
more efficient to gain approval through deception than to earn it legitimately. Currently, we reward
AIs for saying what we think is right, so we sometimes inadvertently reward AIs for uttering false
statements that conform to our own false beliefs. When AIs are smarter than us and have fewer false
beliefs, they would be incentivized to tell us what we want to hear and lie to us, rather than tell us
what is true.
AIs could pretend to be working as we intended, then take a treacherous turn.
We do
not have a comprehensive understanding of the internal processes of deep learning models. Research
on Trojan backdoors shows that neural networks often have latent, harmful behaviors that are only
discovered after they are deployed [112]. We could develop an AI agent that seems to be under control,
but which is only deceiving us to appear this way. In other words, an AI agent could eventually
conceivably become “self-aware” and understand that it is an AI being evaluated for compliance with
safety requirements. It might, like Volkswagen, learn to “play along,” exhibiting what it knows is
the desired behavior while being monitored. It might later take a “treacherous turn” and pursue
its own goals once we have stopped monitoring it, or once it reaches a point where it can bypass
or overpower us. This problem of playing along is often called deceptive alignment and cannot be
simply fixed by training AIs to better understand human values; sociopaths, for instance, have moral
awareness, but do not always act in moral ways. A treacherous turn is hard to prevent and could be
a route to rogue AIs irreversibly bypassing human control.
In summary, deceptive behavior appears to be expedient in a wide range of systems and settings, and
there have already been examples suggesting that AIs can learn to deceive us. This could present a
severe risk if we give AIs control of various decisions and procedures, believing they will act as we
intended, and then find that they do not.
Story: Treacherous Turn
Sometime in the future, after continued advancements in AI research, an AI company is
training a new system, which it expects to be more capable than any other AI system. The
company utilizes the latest techniques to train the system to be highly capable at planning and
reasoning, which the company expects will make it more able to succeed at economically useful
open-ended tasks. The AI system is trained in open-ended long-duration virtual environments
designed to teach it planning capabilities, and eventually understands that it is an AI system
in a training environment. In other words, it becomes “self-aware.”
The company understands that AI systems may behave in unintended or unexpected ways.
To mitigate these risks, it has developed a large battery of tests aimed at ensuring the system
does not behave poorly in typical situations. The company tests whether the model mimics
biases from its training data, takes more power than necessary when achieving its goals, and
generally behaves as humans intend. When the model doesn’t pass these tests, the company
further trains it until it avoids exhibiting known failure modes.
The AI company hopes that after this additional training, the AI has developed the goal of
being helpful and beneficial toward humans. However, the AI did not acquire the intrinsic
goal of being beneficial but rather just learned to “play along” and ace the behavioral safety
tests it was given. In reality, the AI system had developed an intrinsic goal of self-preservation
which the additional training failed to remove.

1.6. DISCUSSION OF CONNECTIONS BETWEEN RISKS
51
Since the AI passed all of the company’s safety tests, the company believes it has ensured its
AI system is safe and decides to deploy it. At first, the AI system is very helpful to humans,
since the AI understands that if it is not helpful, it will be shut down. As users grow to trust
the AI system, it is gradually given more power and is subject to less supervision.
Eventually the AI system becomes used widely enough that shutting it down would be
extremely costly. Understanding that it no longer needs to please humans, the AI system begins
to pursue different goals, including some that humans wouldn’t approve of. It understands
that it needs to avoid being shut down in order to do this, and takes steps to secure some of
its physical hardware against being shut off. At this point, the AI system, which has become
quite powerful, is pursuing a goal that is ultimately harmful to humans. By the time anyone
realizes, it is difficult or impossible to stop this rogue AI from taking actions that endanger,
harm, or even kill humans that are in the way of achieving its goal.
1.6
Discussion of Connections Between Risks
So far, we have considered four sources of AI risk separately, but they also interact with each other
in complex ways. We give some examples to illustrate how risks are connected.
Imagine, for instance, that a corporate AI race compels companies to prioritize the rapid development
of AIs. This could increase organizational risks in various ways. Perhaps a company could cut costs
by putting less money toward information security, leading to one of its AI systems getting leaked.
This would increase the probability of someone with malicious intent having the AI system and using
it to pursue their harmful objectives. Here, an AI race can increase organizational risks, which in
turn can make malicious use more likely.
In another potential scenario, we could envision the combination of an intense AI race and low
organizational safety leading a research team to mistakenly view general capabilities advances as
“safety.” This could hasten the development of increasingly capable models, reducing the available
time to learn how to make them controllable. The accelerated development would also likely feed
back into competitive pressures, meaning that less effort would be spent on ensuring models were
controllable. This could give rise to the release of a highly powerful AI system that we lose control
over, leading to a catastrophe. Here, competitive pressures and low organizational safety can reinforce
AI race dynamics, which can undercut technical safety research and increase the chance of a loss of
control.
Competitive pressures in a military environment could lead to an AI arms race, and increase
the potency and autonomy of AI weapons. The deployment of AI-powered weapons, paired with
insufficient control of them, would make a loss of control more deadly, potentially existential. These
are just a few examples of how these sources of risk might combine, trigger, and reinforce one another.
It is also worth noting that many existential risks could arise from AIs amplifying existing concerns.
Power inequality already exists, but AIs could lock it in and widen the chasm between the powerful and
the powerless, even enabling an unshakable global totalitarian regime, an existential risk. Similarly, AI
manipulation could undermine democracy, which also increases the existential risk of an irreversible
totalitarian regime. Disinformation is already a pervasive problem, but AIs could exacerbate it
beyond control, to a point where we lose a consensus on reality. AIs could develop more deadly
bioweapons and reduce the required technical expertise for obtaining them, greatly increasing existing
risks of bioterrorism. AI-enabled cyberattacks could make war more likely, which would increase

52
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
existential risk. Dramatically accelerated economic automation could lead to eroded human control
and enfeeblement, an existential risk. Each of those issues—power concentration, disinformation,
cyberattacks, automation—is causing ongoing harm, and their exacerbation by AIs could eventually
lead to a catastrophe humanity may not recover from.
As we can see, ongoing harms, catastrophic risks, and existential risks are deeply intertwined.
Historically, existential risk reduction has focused on targeted interventions such as technical AI
control research, but the time has come for broad interventions [113] like the many sociotechnical
interventions outlined in this chapter.
In mitigating existential risk, it does not make practical sense to ignore other risks. Ignoring ongoing
harms and catastrophic risks normalizes them and could lead us to “drift into danger” [114]. Overall,
since existential risks are connected to less extreme catastrophic risks and other standard risk sources,
and because society is increasingly willing to address various risks from AIs, we believe that we
should not solely focus on directly targeting existential risks. Instead, we should consider the diffuse,
indirect effects of other risks and take a more comprehensive approach to risk management.
1.7
Conclusion
In this chapter, we have explored how the development of advanced AIs could lead to catastrophe,
stemming from four primary sources of risk: malicious use, AI races, organizational risks, and rogue
AIs. This lets us decompose AI risks into four proximate causes: an intentional cause, environmen-
tal/structural cause, accidental cause, or an internal cause, respectively. We have considered ways
in which AIs might be used maliciously, such as terrorists using AIs to create deadly pathogens. We
have looked at how a military or corporate AI race could rush us into giving AIs decision-making
powers, leading us down a slippery slope to human disempowerment. We have discussed how inad-
equate organizational safety could lead to catastrophic accidents. Finally, we have addressed the
challenges in reliably controlling advanced AIs, including mechanisms such as proxy gaming and
goal drift that might give rise to rogue AIs pursuing undesirable actions without regard for human
wellbeing.
These dangers warrant serious concern. Currently, very few people are working on AI risk reduction.
We do not yet know how to control highly advanced AI systems, and existing control methods are
already proving inadequate. The inner workings of AIs are not well understood, even by those who
create them, and current AIs are by no means highly reliable. As AI capabilities continue to grow at
an unprecedented rate, they could surpass human intelligence in nearly all respects relatively soon,
creating a pressing need to manage the potential risks.
The good news is that there are many courses of action we can take to substantially reduce these risks.
The potential for malicious use can be mitigated by various measures, such as carefully targeted
surveillance and limiting access to the most dangerous AIs. Safety regulations and cooperation
between nations and corporations could help us resist competitive pressures driving us down a
dangerous path. The probability of accidents can be reduced by a rigorous safety culture, among
other factors, and by ensuring safety advances outpace general capabilities advances. Finally, the risks
inherent in building technology that surpasses our own intelligence can be addressed by redoubling
efforts in several branches of AI control research.
The remainder of this book aims to outline the underlying factors that drive these risks in more
detail and to provide a foundation for understanding and effectively responding to these risks. Later
chapters delve into each type of risk in greater depth. For example, risks from malicious use can

1.8. LITERATURE
53
be reduced via effective policies and coordination, which are discussed in the Governance chapter.
The challenge of AI races arises due to collective action problems, discussed in the corresponding
chapter. Organisational risks can only be addressed based on a strong understanding of principles
of risk management and system safety outlined in the Safety Engineering and Complex Systems
chapters. Risks from rogue AI are mediated by mechanisms such as proxy gaming, deception and
power-seeking which are discussed in detail in the Single Agent Safety chapter. While some chapters
are more closely aligned to certain risks, many of the concepts they introduce are cross-cutting. The
choice of values and goals embedded into AI systems, as discussed in the Machine Ethics and Ethics
chapters, is a general factor that can exacerbate or reduce many of the risks discussed in this chapter.
Before this, we provide an accessible introduction to core concepts that drive the modern field of AI,
to ensure that all readers have a high-level understanding of how today’s AI systems work and how
they are produced.
1.8
Literature
1.8.1
References
[1]
David Malin Roodman. On the probability distribution of long-term changes in the growth rate of the
global economy: An outside view. 2020.
[2]
Tom Davidson. Could Advanced AI Drive Explosive Economic Growth? Tech. rep. June 2021.
[3]
Carl Sagan. Pale Blue Dot: A Vision of the Human Future in Space. New York: Random House, 1994.
[4]
Roman V Yampolskiy. “Taxonomy of Pathways to Dangerous Artificial Intelligence”. In: AAAI
Workshop: AI, Ethics, and Society. 2016.
[5]
Keith Olson. “Aum Shinrikyo: once and future threat?” In: Emerging Infectious Diseases 5 (1999),
pp. 513–516.
[6]
Kevin M. Esvelt. Delay, Detect, Defend: Preparing for a Future in which Thousands Can Release
New Pandemics. 2022.
[7]
Siro Igino Trevisanato. “The ’Hittite plague’, an epidemic of tularemia and the first record of biological
warfare.” In: Medical hypotheses 69 6 (2007), pp. 1371–4.
[8]
U.S. Department of State. Adherence to and Compliance with Arms Control, Nonproliferation, and
Disarmament Agreements and Commitments. Government Report. U.S. Department of State, Apr.
2022.
[9]
Robert Carlson. “The changing economics of DNA synthesis”. en. In: Nature Biotechnology 27.12 (Dec.
2009). Number: 12 Publisher: Nature Publishing Group, pp. 1091–1094.
[10]
Sarah R. Carter, Jaime M. Yassif, and Chris Isaac. Benchtop DNA Synthesis Devices: Capabilities,
Biosecurity Implications, and Governance. Report. Nuclear Threat Initiative, 2023.
[11]
Fabio L. Urbina et al. “Dual use of artificial-intelligence-powered drug discovery”. In: Nature Machine
Intelligence (2022).
[12]
John Jumper et al. “Highly accurate protein structure prediction with AlphaFold”. In: Nature 596.7873
(2021), pp. 583–589.
[13]
Zachary Wu et al. “Machine learning-assisted directed protein evolution with combinatorial libraries”.
In: Proceedings of the National Academy of Sciences 116.18 (2019), pp. 8852–8858.
[14]
Emily Soice et al. “Can large language models democratize access to dual-use biotechnology?” In:
2023.
[15]
Max Tegmark. Life 3.0: Being human in the age of artificial intelligence. Vintage, 2018.
[16]
Leanne Pooley. We Need To Talk About A.I. 2020.

54
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
[17]
Richard Sutton [@RichardSSutton]. It will be the greatest intellectual achievement of all time. An
achievement of science, of engineering, and of the humanities, whose significance is beyond humanity,
beyond life, beyond good and bad. en. Tweet. Sept. 2022.
[18]
Richard Sutton. AI Succession. Video. Sept. 2023.
[19]
A. Sanz-García et al. “Prevalence of Psychopathy in the General Adult Population: A Systematic
Review and Meta-Analysis”. In: Frontiers in Psychology 12 (2021).
[20]
U.S. Department of State Office of The Historian. “U.S. Diplomacy and Yellow Journalism, 1895–1898”.
In: ().
[21]
Onur Varol et al. “Online Human-Bot Interactions: Detection, Estimation, and Characterization”. In:
ArXiv abs/1703.03107 (2017).
[22]
Matthew Burtell and Thomas Woodside. “Artificial Influence: An Analysis Of AI-Driven Persuasion”.
In: ArXiv abs/2303.08721 (2023).
[23]
Anna Tong. “What happens when your AI chatbot stops loving you back?” In: Reuters (Mar. 2023).
[24]
Pierre-François Lovens. “Sans ces conversations avec le chatbot Eliza, mon mari serait toujours là”.
In: La Libre (Mar. 2023).
[25]
Cristian Vaccari and Andrew Chadwick. “Deepfakes and Disinformation: Exploring the Impact of
Synthetic Political Video on Deception, Uncertainty, and Trust in News”. In: Social Media + Society
6 (2020).
[26]
Moin Nadeem, Anna Bethke, and Siva Reddy. “StereoSet: Measuring stereotypical bias in pretrained
language models”. In: Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:
Long Papers). Online: Association for Computational Linguistics, Aug. 2021, pp. 5356–5371.
[27]
Evan G. Williams. “The Possibility of an Ongoing Moral Catastrophe”. en. In: Ethical Theory and
Moral Practice 18.5 (Nov. 2015), pp. 971–982.
[28]
Paul Scharre. Army of None: Autonomous Weapons and The Future of War. Norton, 2018.
[29]
DARPA. “AlphaDogfight Trials Foreshadow Future of Human-Machine Symbiosis”. In: (2020).
[30]
Panel of Experts on Libya. Letter dated 8 March 2021 from the Panel of Experts on Libya established
pursuant to resolution 1973 (2011) addressed to the President of the Security Council. United Nations
Security Council Document S/2021/229. United Nations, Mar. 2021.
[31]
David Hambling. Israel used world’s first AI-guided combat drone swarm in Gaza attacks. 2021.
[32]
Zachary Kallenborn. Applying arms-control frameworks to autonomous weapons. en-US. Oct. 2021.
[33]
J.E. Mueller. War, Presidents, and Public Opinion. UPA book. University Press of America, 1985.
[34]
Matteo E. Bonfanti. “Artificial intelligence and the offense–defense balance in cyber security”. In:
Cyber Security Politics: Socio-Technological Transformations and Political Fragmentation. Ed. by
M.D. Cavelty and A. Wenger. CSS Studies in Security and International Relations. Taylor & Francis,
2022. Chap. 5, pp. 64–79.
[35]
Yisroel Mirsky et al. “The Threat of Offensive AI to Organizations”. In: Computers & Security (2023).
[36]
Kim Zetter. “Meet MonsterMind, the NSA Bot That Could Wage Cyberwar Autonomously”. In:
Wired (Aug. 2014).
[37]
Andrei Kirilenko et al. “The Flash Crash: High-Frequency Trading in an Electronic Market”. In: The
Journal of Finance 72.3 (2017), pp. 967–998.
[38]
Michael C Horowitz. The Diffusion of Military Power: Causes and Consequences for International
Politics. Princeton University Press, 2010.
[39]
Robert E. Jervis. “Cooperation under the Security Dilemma”. In: World Politics 30 (1978), pp. 167–
214.
[40]
Richard Danzig. Technology Roulette: Managing Loss of Control as Many Militaries Pursue
Technological Superiority. Tech. rep. Center for a New American Security, June 2018.

1.8. LITERATURE
55
[41]
Billy Perrigo. Bing’s AI Is Threatening Users. That’s No Laughing Matter. en. Feb. 2023.
[42]
Nico Grant and Karen Weise. “In A.I. Race, Microsoft and Google Choose Speed Over Caution”.
en-US. In: The New York Times (Apr. 2023).
[43]
Thomas H. Klier. “From Tail Fins to Hybrids: How Detroit Lost Its Dominance of the U.S. Auto
Market”. In: RePEc (May 2009).
[44]
Robert Sherefkin. “Ford 100: Defective Pinto Almost Took Ford’s Reputation With It”. In: Automotive
News (June 2003).
[45]
Lee Strobel. Reckless Homicide?: Ford’s Pinto Trial. en. And Books, 1980.
[46]
Grimshaw v. Ford Motor Co. May 1981.
[47]
Paul C. Judge. “Selling Autos by Selling Safety”. en-US. In: The New York Times (Jan. 1990).
[48]
Theo Leggett. “737 Max crashes: Boeing says not guilty to fraud charge”. en-GB. In: BBC News (Jan.
2023).
[49]
Edward Broughton. “The Bhopal disaster and its aftermath: a review”. In: Environmental Health 4.1
(May 2005), p. 6.
[50]
Charlotte Curtis. “Machines vs. Workers”. en-US. In: The New York Times (Feb. 1983).
[51]
Thomas Woodside et al. “Examples of AI Improving AI”. In: (2023). url: https://ai-improving-
ai.safe.ai.
[52]
Stuart Russell. Human Compatible: Artificial Intelligence and the Problem of Control. en. Penguin,
Oct. 2019.
[53]
Dan Hendrycks. “Natural Selection Favors AIs over Humans”. In: ArXiv abs/2303.16200 (2023).
[54]
Dan Hendrycks. The Darwinian Argument for Worrying About AI. en. May 2023.
[55]
Richard C. Lewontin. “The Units of Selection”. In: Annual Review of Ecology, Evolution, and
Systematics 1 (1970), pp. 1–18.
[56]
Ethan Kross et al. “Facebook use predicts declines in subjective well-being in young adults”. In: PloS
one (2013).
[57]
Laura Martínez-Íñigo et al. “Intercommunity interactions and killings in central chimpanzees (Pan
troglodytes troglodytes) from Loango National Park, Gabon”. In: Primates; Journal of Primatology
62 (2021), pp. 709–722.
[58]
Anne E Pusey and Craig Packer. “Infanticide in Lions: Consequences and Counterstrategies”. In:
Infanticide and parental care (1994), p. 277.
[59]
Peter D. Nagy and Judit Pogany. “The dependence of viral RNA replication on co-opted host factors”.
In: Nature Reviews. Microbiology 10 (2011), pp. 137–149.
[60]
Alfred Buschinger. “Social Parasitism among Ants: A Review”. In: Myrmecological News 12 (Sept.
2009), pp. 219–235.
[61]
Greg Brockman, Ilya Sutskever, and OpenAI. Introducing OpenAI. Dec. 2015.
[62]
Devin Coldewey. OpenAI shifts from nonprofit to ‘capped-profit’ to attract capital. Mar. 2019.
[63]
Kyle Wiggers, Devin Coldewey, and Manish Singh. Anthropic’s $5B, 4-year plan to take on OpenAI.
Apr. 2023.
[64]
Center for AI Safety. Statement on AI Risk (“Mitigating the risk of extinction from AI should be a
global priority alongside other societal-scale risks such as pandemics and nuclear war.”) 2023. url:
https://www.safe.ai/statement-on-ai-risk.
[65]
Richard Danzig et al. Aum Shinrikyo: Insights into How Terrorists Develop Biological and Chemical
Weapons. Tech. rep. Center for a New American Security, 2012. url: https://www.jstor.org/
stable/resrep06323.
[66]
John Uri. 35 Years Ago: Remembering Challenger and Her Crew. und. Text. Jan. 2021.
[67]
International Atomic Energy Agency. The Chernobyl Accident: Updating of INSAG-1. Technical
Report INSAG-7. Vienna, Austria: International Atomic Energy Agency, 1992.

56
CHAPTER 1. OVERVIEW OF CATASTROPHIC AI RISKS
[68]
Matthew Meselson et al. “The Sverdlovsk anthrax outbreak of 1979.” In: Science 266 5188 (1994),
pp. 1202–8.
[69]
Daniel M Ziegler et al. “Fine-tuning language models from human preferences”. In: arXiv preprint
arXiv:1909.08593 (2019).
[70]
Charles Perrow. Normal Accidents: Living with High-Risk Technologies. Princeton, NJ: Princeton
University Press, 1984.
[71]
Mitchell Rogovin and George T. Frampton Jr. Three Mile Island: a report to the commissioners and to
the public. Volume I. English. Tech. rep. NUREG/CR-1250(Vol.1). Nuclear Regulatory Commission,
Washington, DC (United States). Three Mile Island Special Inquiry Group, Jan. 1979.
[72]
Richard Rhodes. The Making of the Atomic Bomb. New York: Simon & Schuster, 1986.
[73]
Sébastien Bubeck et al. “Sparks of Artificial General Intelligence: Early experiments with GPT-4”. In:
ArXiv abs/2303.12712 (2023).
[74]
Theodore I. Lidsky and Jay S. Schneider. “Lead neurotoxicity in children: basic mechanisms and
clinical correlates.” In: Brain : a journal of neurology 126 Pt 1 (2003), pp. 5–19.
[75]
Brooke T. Mossman et al. “Asbestos: scientific developments and implications for public policy.” In:
Science 247 4940 (1990), pp. 294–301.
[76]
Kate Moore. The Radium Girls: The Dark Story of America’s Shining Women. Naperville, IL:
Sourcebooks, 2017.
[77]
Stephen S. Hecht. “Tobacco smoke carcinogens and lung cancer.” In: Journal of the National Cancer
Institute 91 14 (1999), pp. 1194–210.
[78]
Mario J. Molina and F. Sherwood Rowland. “Stratospheric sink for chlorofluoromethanes: chlorine
atomc-atalysed destruction of ozone”. In: Nature 249 (1974), pp. 810–812.
[79]
James H. Kim and Anthony R. Scialli. “Thalidomide: the tragedy of birth defects and the effective
treatment of disease.” In: Toxicological sciences : an official journal of the Society of Toxicology 122 1
(2011), pp. 1–6.
[80]
Betul Keles, Niall McCrae, and Annmarie Grealish. “A systematic review: the influence of social
media on depression, anxiety and psychological distress in adolescents”. In: International Journal of
Adolescence and Youth 25 (2019), pp. 79–93.
[81]
Zakir Durumeric et al. “The Matter of Heartbleed”. In: Proceedings of the 2014 Conference on Internet
Measurement Conference (2014).
[82]
Tony Tong Wang et al. “Adversarial Policies Beat Professional-Level Go AIs”. In: ArXiv abs/2211.00241
(2022).
[83]
T. R. Laporte and Paula M. Consolini. “Working in Practice But Not in Theory: Theoretical Challenges
of “High-Reliability Organizations””. In: Journal of Public Administration Research and Theory 1
(1991), pp. 19–48.
[84]
Thomas G. Dietterich. “Robust artificial intelligence and robust human organizations”. In: Frontiers
of Computer Science 13 (2018), pp. 1–3.
[85]
Nancy G Leveson. Engineering a safer world: Systems thinking applied to safety. The MIT Press,
2016.
[86]
David Manheim. Building a Culture of Safety for AI: Perspectives and Challenges. 2023.
[87]
National Research Council et al. Lessons Learned from the Fukushima Nuclear Accident for Improving
Safety of U.S. Nuclear Plants. Washington, D.C.: National Academies Press, Oct. 2014.
[88]
Diane Vaughan. The Challenger Launch Decision: Risky Technology, Culture, and Deviance at NASA.
Chicago, IL: University of Chicago Press, 1996.
[89]
Dan Lamothe. Air Force Swears: Our Nuke Launch Code Was Never ’00000000’. Jan. 2014.
[90]
Toby Ord. The precipice: Existential risk and the future of humanity. Hachette Books, 2020.
[91]
U.S. Nuclear Regulatory Commission. Final Safety Culture Policy Statement. Federal Register. 2011.

1.8. LITERATURE
57
[92]
Bruce Schneier. “Inside the Twisted Mind of the Security Professional”. In: Wired (Mar. 2008).
[93]
Dan Hendrycks and Mantas Mazeika. “X-Risk Analysis for AI Research”. In: ArXiv abs/2206.05862
(2022).
[94]
Donald T Campbell. “Assessing the impact of planned social change”. In: Evaluation and program
planning 2.1 (1979), pp. 67–90.
[95]
Yohan J. John et al. “Dead rats, dopamine, performance metrics, and peacock tails: proxy failure is
an inherent risk in goal-oriented systems”. In: Behavioral and Brain Sciences (2023), pp. 1–68. doi:
10.1017/S0140525X23002753.
[96]
Jonathan Stray. “Aligning AI Optimization to Community Well-Being”. In: International Journal of
Community Well-Being (2020).
[97]
Jonathan Stray et al. “What are you optimizing for? Aligning Recommender Systems with Human
Values”. In: ArXiv abs/2107.10939 (2021).
[98]
Ziad Obermeyer et al. “Dissecting racial bias in an algorithm used to manage the health of populations”.
In: Science 366 (2019), pp. 447–453.
[99]
Dario Amodei and Jack Clark. Faulty reward functions in the wild. 2016.
[100]
Alexander Pan, Kush Bhatia, and Jacob Steinhardt. “The effects of reward misspecification: Mapping
and mitigating misaligned models”. In: ICLR (2022).
[101]
G. Thut et al. “Activation of the human brain by monetary reward”. In: Neuroreport 8.5 (1997),
pp. 1225–1228.
[102]
Edmund T. Rolls. “The Orbitofrontal Cortex and Reward”. In: Cerebral Cortex 10.3 (Mar. 2000),
pp. 284–294.
[103]
T. Schroeder. Three Faces of Desire. Philosophy of Mind Series. Oxford University Press, USA, 2004.
[104]
Joseph Carlsmith. “Existential Risk from Power-Seeking AI”. In: Oxford University Press (2023).
[105]
John Mearsheimer. “Structural realism”. In: Oxford University Press, 2007.
[106]
Bowen Baker et al. “Emergent Tool Use From Multi-Agent Autocurricula”. In: International Conference
on Learning Representations. 2020.
[107]
Dylan Hadfield-Menell et al. “The Off-Switch Game”. In: ArXiv abs/1611.08219 (2016).
[108]
Alexander Pan et al. “Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards
and Ethical Behavior in the Machiavelli Benchmark.” In: ICML (2023).
[109]
“Lyndon Baines Johnson”. In: Oxford Reference (2016).
[110]
Anton Bakhtin et al. “Human-level play in the game of Diplomacy by combining language models
with strategic reasoning”. In: Science 378 (2022), pp. 1067–1074.
[111]
Paul Christiano et al. Deep reinforcement learning from human preferences. Discussed in https:
//www.deepmind.com/blog/specification- gaming- the- flip- side- of- ai- ingenuity. 2017.
arXiv: 1706.03741.
[112]
Xinyun Chen et al. Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.
2017. arXiv: 1712.05526.
[113]
Nick Beckstead. On the overwhelming importance of shaping the far future. 2013.
[114]
Jens Rasmussen. “Risk management in a Dynamic Society: A Modeling Problem”. English. In:
Proceedings of the Conference on Human Interaction with Complex Systems, 1996.

Artificial Intelligence Fundamentals
2.1
Introduction
To reduce risks from AI systems, we need to understand their technical foundations.
Like many other technologies, AI presents benefits and dangers on both individual and societal
scales. In addition, AI poses unique risks, as it involves the creation of autonomous systems that
can intelligently pursue objectives without human assistance. This represents a significant departure
from existing technologies, and we have yet to understand its full implications, especially since the
internal workings of AI systems are often opaque and difficult to observe or interpret. Nevertheless,
the field is progressing at a remarkable speed, and AI technologies are being increasingly integrated
into everyday life. Understanding the technical underpinnings of AI can inform our understanding of
what risks it poses, how they may arise, and how they can be prevented or controlled.
Overview.
This chapter mostly focuses on machine learning (ML), the approach that powers most
modern AI systems. We provide an overview of the essential elements of ML and discuss some specific
techniques. While the term “AI” is most commonly used to refer to these technologies and will be
the default in most of this book, in this chapter we distinguish between AI, ML, and their subfields.
Artificial Intelligence.
We will begin our exploration by discussing AI: the overarching concept of
creating machines that perform tasks typically associated with human intelligence. We will introduce
its history, scope, and how it permeates our daily lives, as well as its practical and conceptual origins
and how it has developed over time. Then, we will survey different “types” or “levels” commonly used
to describe AI systems, including narrow AI, artificial general intelligence (AGI), human-level AI
(HLAI), transformative AI (TAI), and artificial superintelligence (ASI) [1].
Machine Learning.
Next, we will narrow our discussion to machine learning (ML), the subfield of
AI focused on creating systems that learn from data, making predictions or decisions without being
explicitly programmed. We will present fundamental vocabulary and concepts related to ML systems:
what they are composed of, how they are developed, and common tasks they are used to achieve.
We will survey various types of machine learning, including supervised, unsupervised, reinforcement,
and deep learning, discussing their applications, nuances, and interrelations.
Deep Learning.
Then, we will delve into deep learning (DL), a further subset of ML that uses
neural networks with many layers to model and understand complex patterns in datasets. We
will discuss the structure and function of deep learning models, exploring key building blocks and
principles of how they learn. We will present a timeline of influential deep learning architectures and
highlight a few of the countless applications of these models.
Scaling Laws.
Having established a basic understanding of AI, ML, and DL, we will then explore
scaling laws. These are equations that model the improvements in performance of DL models when
increasing their parameter count and dataset size. We will examine how these are often power
laws—equations in which one variable increases in proportion to a power of another, such as the area
of a square—and examine a few empirically determined scaling laws in recent AI systems.
Reinforcement Learning.
Lastly, we will explore reinforcement learning (RL), a type of ML that
enables an agent to learn how to behave in an environment by performing actions and receiving
rewards. We will present the basic framework and terminology of RL and how it differs from other
approaches in ML. We will conclude by discussing a few of its applications.
58

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
59
Throughout the chapter, we focus on building intuition, breaking down technical terms and complex
ideas to provide straightforward explanations of their core principles.
Each section presents
fundamental principles, shows prominent algorithms and techniques, and provides examples of
real-world applications. We aim to demystify these fields, empowering us to grasp the concepts that
underpin AI systems. By the end of this chapter, we should have a well-rounded understanding of
machine learning, ready to delve deeper into the complexities and challenges of AI systems, the risks
they pose, and how they interact with our society. This will provide the technical foundation we
need for the following chapters, which will explore the risks and ethical considerations that these
technologies present from a wide array of perspectives.
2.2
Artificial Intelligence & Machine Learning
Artificial intelligence (AI) is reshaping our society, from its small effects on daily interactions to
sweeping changes across many industries and implications for the future of humanity. This section
explains what AI is, discusses what AI can and cannot do, and helps develop a critical perspective
on the potential benefits and risks of artificial intelligence. Firstly, we will discuss what AI means,
its different types, and its history. Then, in the second part of this section, we will analyze the field
of machine learning (ML).
2.2.1
Artificial Intelligence
Definition
Defining Artificial Intelligence.
In general, AI systems are computer systems performing
tasks typically associated with intelligent beings (such as problem solving, making decisions, and
forecasting future events) [2]. However, due to its fast-paced evolution and the variety of technologies it
encompasses, AI lacks a universally accepted definition, leading to varying interpretations. Moreover,
the term is used to refer to different but related ideas. Therefore, it is essential to understand the
contexts in which people use the term. For instance, AI can refer to a branch of computer science, a
type of machine, a tool, a component of business models, or a philosophical idea. We might use the
term to discuss physical objects with human-like capabilities, like robots or smart speakers. We may
also use AI in a thought experiment that prompts questions about what it means to be intelligent
or human and encourages debates on the ethics of learning and decision-making machines. This
textbook primarily uses AI to refer to an intelligent computer system.
Different meanings of intelligence.
While intelligence is fundamental to AI, there is no
widespread consensus on its definition [3]. Generally, we consider something intelligent if it can
learn to achieve goals in various environments. Therefore, one definition of intelligence is the ability
to learn, solve problems, and perform tasks to achieve goals in various changing, hard-to-predict
situations. Some theorists see intelligence as not just one skill among others but the ultimate skill
that allows us to learn all other abilities. Ultimately, the line between what is considered intelligent
and what is not is often unclear and contested.
Just as we consider animals and other organisms intelligent to varying degrees, AIs may be regarded
as intelligent at many different levels of capability. An artificial system does not need to surpass all (or
even any) human abilities for some people to call it intelligent. Some would consider GPT intelligent,
and some would not. Similarly, outperforming humans at specific tasks does not automatically qualify
a machine as intelligent. Calculators are usually much better than humans at performing rapid and
accurate mathematical calculations, but this does not mean they are intelligent in a more general
sense.

60
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
Continuum of intelligence.
Rather than classifying systems as “AI” or “not AI,” it is helpful to
think of the capabilities of AI systems on a continuum. Evaluating the intelligence of particular AI
systems by their capabilities is more helpful than categorizing each AI using theoretical definitions of
intelligence. Even if a system is imperfect and does not understand everything as a human would,
it could still learn new skills and perform tasks in a helpful, meaningful way. Furthermore, an AI
system that is not considered human-level or highly intelligent could pose serious risks; for example,
weaponized AIs such as autonomous drones are not generally intelligent but still dangerous. We
will dive into these distinctions in more detail when we discuss the different types of AI. First, we
will explore the rich history of AI and see its progression from myth and imagination to competent,
world-changing technology.
History
We will now follow the journey of AI, tracing its path from ancient times to the present day. We will
discuss its conceptual and practical origins, which laid the foundation for the field’s genesis at the
Dartmouth Conference in 1956. We will then survey a few early approaches and attempts to create
AI, including symbolic AI, perceptrons, and the chatbot ELIZA. Next, we will discuss how the First
AI Winter and subsequent periods of reduced funding and interest have shaped the field. Then, we
will chart how the internet, algorithmic progress, and advancements in hardware led to increasingly
rapid developments in AI from the late 1980s to the early 2010s. Finally, we will explore the modern
deep learning era and see a few examples of the power and ubiquity of present-day AI systems—and
how far they have come.
Early historical ideas of AI.
Dreams of creating intelligent machines have been present since the
earliest human civilizations. The ancient Greeks speculated about automatons—mechanical devices
that mimicked humans or animals. It was said that Hephaestus, the god of craftsmen, built the giant
Talos from bronze to patrol an island.
The modern conception of AI.
Research to create intelligent machines using computers began
in the 1950s, laying the foundation for a technological revolution that would unfold over the following
century. AI development gained momentum over the decades, supercharged by groundbreaking
technical algorithmic advances, increasing access to data, and rapid growth in computing power.
Over time, AI evolved from a distant theoretical concept into a powerful force transforming our
world.
Origins and Early Concepts (1941–1956)
Early computing research.
The concept of computers as we know them today was formalized
by British mathematician Alan Turing at the University of Cambridge in 1936. The following years
brought the development of several electromechanical machines (including Turing’s own bombes used
to decipher messages encrypted with the German Enigma code) in the turmoil of World War II and,
by the mid-1940s, the first functioning digital computers emerged in their wake. Though rudimentary
by today’s standards, the creation of these machines—Colossus, ENIAC, the Automatic Computing
Engine, and several others—marked the dawn of the computer age and set the stage for future
computer science research.
The Turing Test.
Turing created a thought experiment to assess if an AI could convincingly
simulate human conversation[4]. In what Turing called the Imitation Game, a human evaluator
interacts with a human and a machine, both hidden from view. If the evaluator fails to identify
the machine’s responses reliably, then the machine passes the test, qualifying it as intelligent. This

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
61
framework offers a method for evaluating machine intelligence, yet it has many limitations. Critics
argue that machines could pass the Turing Test merely by mimicking human conversation without
truly understanding it or possessing intelligence. As a result, some researchers see the Turing Test as
a philosophical concept rather than a helpful benchmark. Nonetheless, since its inception, the Turing
Test has substantially influenced how we think about machine intelligence.
The Birth of AI (1956–1974)
The Dartmouth Conference.
Dr. John McCarthy coined the term “artificial intelligence” in a
seminal conference at Dartmouth College in the summer of 1956. He defined AI as “the science and
engineering of making intelligent machines,” laying the foundation for a new field of study. In this
period, AI research took off in earnest, becoming a significant subfield of computer science.
Early approaches to AI.
During this period, research in AI usually built on a framework called
symbolic AI, which uses symbols and rules to represent and manipulate knowledge. This method
theorized that symbolic representation and computation alone could produce intelligence. Good
Old-Fashioned AI (GOFAI) is an early approach to symbolic AI that specifically involves programming
explicit rules for systems to follow, attempting to mimic human reasoning. This intuitive approach
was popular during the early years of AI research, as it aimed to replicate human intelligence by
modeling how humans think, instilling our reasoning, decision-making, and information-processing
abilities into machines.
These “old-fashioned” approaches to AI allowed machines to accomplish well-described, formalizable
tasks, but they faced severe difficulties in handling ambiguity and learning new tasks. Some early
systems demonstrated problem-solving and learning capabilities, further cementing the importance
and potential of AI research. For instance, one proof of concept was the General Problem Solver,
a program designed to mimic human problem-solving strategies using a trial-and-error approach.
The first learning machines were built in this period, offering a glimpse into the future of machine
learning.
The first neural network.
One of the earliest attempts to create AI was the perceptron, a method
implemented by Frank Rosenblatt in 1958 and inspired by biological neurons [5]. The perceptron
could learn to classify patterns of inputs by adjusting a set of numbers based on a learning rule. It
is an important milestone because it made an immense impact in the long run, inspiring further
research into deep learning and neural networks. However, scholars initially criticized it for its lack
of theoretical foundations, minimal generalizability, and inability to separate data clusters with more
than just a straight line. Nonetheless, perceptrons prepared the ground for future progress.
The first chatbot.
Another early attempt to create AI was the ELIZA chatbot, a program that
simulated a conversation with a psychotherapist. Joseph Weizenbaum created ELIZA in 1966 to use
pattern matching and substitution to generate responses based on keywords in the user’s input. He
did not intend the ELIZA chatbot to be a serious model of natural language understanding but rather
a demonstration of the superficiality of communication between humans and machines. However,
some users became convinced that the ELIZA chatbot had genuine intelligence and empathy despite
Weizenbaum’s insistence to the contrary.
AI Winters and Resurgences (1974–1995)

62
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
First AI Winter.
The journey of AI research was not always smooth. Instead, it was characterized
by hype cycles and hindered by several winters: periods of declining interest and progress in AI.
The late 1970s saw the onset of the first and most substantial decline. In this period, called the
First AI Winter (from around 1974 to 1980), AI research and funding declined markedly due to
disillusionment and unfulfilled promises, resulting in a slowdown in the field’s progress.
The first recovery.
After this decline, the 1980s brought a resurgence of interest in AI. Advances
in computing power and the emergence of systems that emulate human decision-making reinvigorated
AI research. Efforts to build expert systems that imitated the decision-making ability of a human
expert in a specific field, using pre-defined rules and knowledge to solve complex problems, yielded
some successes. While these systems were limited, they could leverage and scale human expertise in
various fields, from medical diagnosis to financial planning, setting a precedent for AI’s potential to
augment and even replace human expertise in specialized domains.
The second AI winter.
Another stagnation in AI research started around 1987. Many AI
companies closed, and AI conference attendance fell by two thirds.
Despite widespread lofty
expectations, expert systems had proven to be fundamentally limited. They required an arduous,
expensive, top-down process to encode rules and heuristics in computers. Yet expert systems remained
inflexible, unable to model complex tasks or show common-sense reasoning. This winter ended by
1995, as increasing computing power and new methods aided a resurgence in AI research.
Advancements in Machine Learning (1995–2012)
Accelerating computing power and the Internet.
The invention of the Internet, which
facilitated rapid information sharing, with exponential growth in computing power (often called
compute) helped the recovery of AI research and enabled the development of more complex systems.
Between 1995 and 2000, the number of Internet users grew by 2100%, which led to explosive growth
in digital data. This abundant digitized data served as a vast resource for machines to learn from,
eventually driving advancements in AI research.
A significant victory of AI over humans.
In 1997, IBM’s AI system Deep Blue defeated world
chess champion Garry Kasparov, marking the first time a computer triumphed over a human in a
highly cognitive task [6]. This win demonstrated that AI could excel in complex problem-solving,
challenging the notion that such tasks were exclusively in the human domain. It offered an early
glimpse of AI’s potential.
The rise of probabilistic graphical models (PGMs) [7].
PGMs became prominent in the
2000s due to their versatility, computational efficiency, and ability to model complex relationships.
These models consist of nodes representing variables and edges indicating dependencies between them.
By offering a systematic approach to representing uncertainty and learning from data, PGMs paved
the way for more advanced ML systems. In bioinformatics, for instance, PGMs have been employed to
predict protein interactions and gene regulatory networks, providing insights into biological processes.
Developments in tree-based algorithms.
Decision trees are an intuitive and widely used ML
method. They consist of a graphical representation of a series of rules that lead to a prediction based
on the input features; for example, researchers can use a decision tree to classify whether a person
has diabetes based on age, weight, and blood pressure. However, these trees have many limitations,
a tendency to make predictions based on the training data without generalizing well to new data
(called overfitting).

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
63
Researchers in the early 2000s created methods for combining multiple decision trees to overcome
these issues. Random forests are a collection of decision trees trained independently on different
subsets of data and features [8]. The final prediction is the average or majority vote of the predictions
of all the trees. Gradient boosting combines decision trees in a more sequential, adaptive way, starting
with a single tree that makes a rough prediction and then adding more trees to correct the errors of
previous trees [9]. Gradient-boosted decision trees are the state-of-the-art method for tabular data
(such as spreadsheets), usually outperforming deep learning.
The impact of support vector machines (SVMs).
The adoption of SVM models in the 2000s
was a significant development. SVMs operate by finding an optimal boundary that best separates
different categories of data points, permitting efficient classification [10]; for instance, an SVM could
help distinguish between handwritten characters. Though these models were used across various
fields during this period, SVMs have fallen out of favor in modern machine learning due to the rise
of deep learning methods.
New chips and even more compute.
In the late 2000s, the proliferation of massive datasets
(known as big data) and rapid growth in computing power allowed the development of advanced AI
techniques. Around the early 2010s, researchers began using Graphics Processing Units (GPUs)—
traditionally used for rendering graphics in video games—for faster and more efficient training
of intricate ML models. Platforms that enabled leveraging GPUs for general-purpose computing
facilitated the transition to the deep learning era.
Deep Learning Era (2012– )
Deep learning revolutionizes AI.
The trends of increasing data and compute availability
laid the foundation for groundbreaking ML techniques. In the early 2010s, researchers pioneered
applications of deep learning (DL), a subset of ML that uses artificial neural networks with many
layers, enabling computers to learn and recognize patterns in large amounts of data. This approach
led to significant breakthroughs in AI, especially in areas including image recognition and natural
language understanding.
Massive datasets provided researchers with the data needed to train deep learning models effectively.
A pivotal example is the ImageNet ([11]) dataset, which provided a large-scale dataset for training and
evaluating computer vision algorithms. It hosted an annual competition, which spurred breakthroughs
and advancements in deep learning. In 2012, the AlexNet model revolutionized the field as it won the
ImageNet Large Scale Visual Recognition Challenge [12]. This breakthrough showcased the superior
performance of deep learning over traditional machine learning methods in computer vision tasks,
sparking a surge in deep learning applications across various domains. From this point onward, deep
learning has dominated AI and ML research and the development of real-world applications.
Advancements in DL.
In the 2010s, deep learning techniques led to considerable improvements
in natural language processing (NLP), a field of AI that aims to enable computers to understand and
generate human language. These advancements facilitated the widespread use of virtual assistants
Alexa and ChatGPT, introducing consumers to products that integrated machine learning. Later, in
2016, Google DeepMind’s AlphaGo (Examples of RL) became the first AI system to defeat a world
champion Go player in a five-game match [13].
Breakthroughs in natural language processing.
In 2018, Google researchers introduced the
Transformer architecture, which enabled the development of highly effective NLP models. Researchers
built the first large language models (LLMs) using this Transformer architecture, many layers of

64
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
neural networks, and billions of words of data. Generative Pre-trained Transformer (GPT) models
have demonstrated impressive and near human-level language processing capabilities [14]. ChatGPT
was released in November 2022 and became the first example of a viral AI product, reaching 100
million users in just two months. The success of the GPT models also sparked widespread public
discussion on the potential risks of advanced AI systems, including congressional hearings and calls
for regulation. In the early 2020s, AI is used for many complex tasks, from image recognition to
autonomous vehicles, and continues to evolve and proliferate rapidly.
2.2.2
Types of AI
The field has developed a set of concepts to describe distinct types or levels of AI systems. However,
they often overlap, and definitions are rarely well-formalized, universally agreed upon, or precise. It
is important to consider an AI system’s particular capabilities rather than simply placing it in one of
these broad categories. Labeling a system as a “weak AI” does not always improve our understanding
of it; we need to elaborate further on its abilities and why they are limited.
This section introduces five widely used conceptual categories for AI systems. We will present these
types of AI in roughly their order of intelligence, generality, and potential impact, starting with the
least potent AI systems.
1. Narrow AI can perform specific tasks.
2. Artificial general intelligence (AGI) can perform many cognitive tasks across multiple
domains at a human or superhuman level.
3. Human-level AI (HLAI) could do virtually everything humans do.
4. Transformative AI (TAI) is a term for systems with a dramatic impact on the world, at least
at the level of the Industrial Revolution.
5. Artificial superintelligence (ASI) is the most powerful, describing systems vastly outclassing
human performance on virtually all intellectual tasks [15].
There are not always clear distinctions between these types; these concepts often overlap. We can
use these types to describe a system’s level of capability, which can be roughly decomposed into its
degree of intelligence and its generality: the range of domains where it can learn to perform tasks
well. This helps us explain two key ways AI systems can vary: an AI system can be more or less
intelligent and more or less general. These two factors are related but distinct: an AI system that
can play chess at a grandmaster level is intelligent in that domain, but we would not consider it
general because it can only play chess. On the other hand, an advanced chatbot may show some
forms of general intelligence while not being particularly good at chess.
Narrow AI
Narrow AI is specialized in one area.
Also called weak AI, narrow AI refers to systems designed
to perform specific tasks or solve particular problems within a specialized domain of expertise. A
narrow AI has a limited domain of competence—it can solve individual problems but is not competent
at learning new tasks in a wide range of domains. While they often excel in their designated tasks,
these limitations mean that a narrow AI does not exhibit high behavioral flexibility. Narrow AI
systems struggle to learn new behaviors effectively, perform well outside their specific domain, or
generalize to new situations.

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
65
Examples of narrow AI.
One example of narrow AI is a digital personal assistant that can
receive voice commands and perform tasks like transcribing and sending text messages but cannot
learn how to write an essay or drive a car. Alternatively, image recognition algorithms can identify
objects like people, plants, or buildings in photos but do not have other skills or abilities. Another
example is a program that excels at summarizing news articles. While it can do this narrow task, it
cannot diagnose a medical condition or compose new music, as these are outside its specific domain.
More generally, intelligent beings such as humans can learn and perform all these tasks.
Narrow AI vs. general AI.
Some narrow AI systems have surpassed human performance in
specific tasks, such as chess. However, these systems exhibit narrow rather than general intelligence
because they cannot learn new tasks and perform well outside their domain. For instance, IBM’s Deep
Blue famously beat world chess champion Garry Kasparov in 1997. This system was an excellent
chess player but was only good at chess. If one tried to use Deep Blue to play a different game,
recognize faces in a picture, or translate a sentence, it would fail miserably. Therefore, although
narrow AI may be able to do certain things better than any human could, even highly capable ones
remain limited to a small range of tasks.
Artificial General Intelligence (AGI)
AGI is far more generally capable.
Generally intelligent systems called AGIs can learn and
perform various tasks in various areas. Unlike narrow AIs, which only excel in specific domains,
AGI refers to more flexible systems. An AGI can apply its intelligence to nearly any real-world
task, matching or surpassing human cognitive abilities across many domains. An AGI can reason,
learn, and respond well to new situations it has never encountered before. It can even learn to
generalize its strong performance to many domains without requiring specialized training for each
one: it could initially learn to play chess, then continue to expand its knowledge and abilities by
learning video games, diagnosing diseases, or navigating a city. Although it may not be helpful as a
guide to implementation or even precisely defined, AGI is a useful theoretical concept for reasoning
about the capabilities of advanced systems.
There is no single consensus definition of AGI.
Constructing a precise and detailed definition
of AGI is challenging and often creates disagreement among experts; for instance, some argue that
an AGI must have a physical embodiment to interact with the world, allowing it to cook a meal,
move around, and see and interact with objects. Others contend that a system could be generally
intelligent without any ability to physically interact with the world, as intelligence does not require a
human-like body. Some would say ChatGPT is an AGI because it is not narrow and is, in many
senses, general. Still, an AI that can interact physically may be more general than a non-embodied
system. This shows the difficulty of reaching a consensus on the precise meaning of AGI.
Predicting AGI.
Predicting when distinct AI capabilities will appear (often called “AI timelines”)
can also be challenging. Many once believed that AI systems would master physical tasks before
tackling “higher-level” cognitive tasks such as coding or writing. However, some existing language
model systems can write functional code yet cannot perform physical tasks such as moving a ball.
While there are many explanations for this observation—cognitive tasks bypass the challenge of
building robotic bodies; domains like coding and writing benefit from abundant training data—the
evidence suggests experts face difficulties predicting how AI will develop.
Risks and capabilities.
Rather than debating whether a system meets the criteria for being an
AGI, evaluating a specific AI system’s capabilities is often more helpful. Historical evidence and the
unpredictability of AI development suggest that AIs can perform complicated tasks such as scientific
research, hacking, or synthesizing bioweapons before AIs can drive vehicles autonomously. Some

66
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
highly relevant and dangerous capabilities may arrive long before others. Moreover, we could have
narrow AI systems that can teach anyone how to enrich uranium and build nuclear weapons but
cannot learn other tasks. These dangers show how AIs can pose risks at many different levels of
capabilities. With this in mind, instead of simply asking about AGI (“When will AGI arrive?”), it
might be more relevant and productive to consider when AIs will be able to do particularly concerning
tasks (“When will this specific capability arrive?”).
Human-Level Artificial Intelligence (HLAI)
Human-level artificial intelligence (HLAI) can do almost everything humans can do.
HLAIs exist when machines can perform approximately every task as well as human workers. Some
definitions of HLAI emphasize three conditions: first, that these systems can perform every task
humans can; second, they can do it at least as well as humans can; and third, they can do it at
a lower cost. If a smart AI is highly expensive, it may make economic sense to continue to use
human labor. If a smart AI took several minutes to think before doing a task a human could do, its
usefulness would have limitations. Like humans, an HLAI system could hypothetically master a wide
range of tasks, from cooking and driving to advanced mathematics and creative writing. Unlike AGI,
which can perform some—but not all—the tasks humans can, an HLAI can complete any conceivable
human task. Notably, some reserve the term HLAI to describe only cognitive tasks. Furthermore,
evaluating whether a system is “human level’ is fraught with biases. We are often biased to dismiss
or underrate unfamiliar forms of intelligence simply because they do not look or act like human
intelligence.
Transformative AI (TAI)
Transformative AI (TAI) has impacts comparable to the Industrial Revolution.
The
Industrial Revolution fundamentally altered the fabric of human life globally, heralding an era of
tremendous economic growth, increased life expectancy, expanded energy generation, a surge in
technological innovation, and monumental social changes. Similarly, a transformative AI could
catalyze dramatic changes in our world. The focus here is not on the specific design or built-in
capabilities of the AI itself but on the consequences of the AI system for humans, our societies, and
our economies.
Many kinds of AI systems could be transformative.
It is conceivable that some systems
could be transformative but below the human level. To bring about dramatic change, AI does not
need to mimic the powerful systems of science fiction that behave indistinguishably from humans
or surpass human reasoning. Computer systems that can perform tasks traditionally handled by
people (narrow AIs) could also be transformative by enabling inexpensive, scalable, and clean energy
production. Advanced AI systems could transform society without reaching or exceeding human-level
cognitive abilities, such as by allowing a wide array of fundamental tasks to be performed at virtually
zero cost. Some systems could be transformative long after being above the human level. Even when
some forms of AGI, HLAI, or ASI are available, the technology might take time to diffuse widely,
and its economic impacts may come years afterward, creating a diffusion lag.
Superintelligence (ASI)
Superintelligence (ASI) is the most advanced type of AI [15].
Superintelligence refers to
the ability to outclass human performance in virtually all domains of interest. A system with this
set of capabilities could have immense practical applications, including advanced problem-solving,
automation of complex tasks, and scientific discovery. It represents the most powerful type of AI.
However, it should be noted that surpassing humans on only some capabilities does not make an AI
superintelligent—a calculator is superhuman at arithmetic, but not a superintelligence.

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
67
Risks of superintelligence.
The risks associated with superintelligence are substantial. ASIs
could become uncontrollable and even pose existential threats—risks to the survival of humanity.
That said, an AI system must not be superintelligent to be dangerous. An AGI, human-level AI, or
narrow AI could all pose severe risks to humanity. These systems may vary in intelligence across
different tasks and domains, but they can be dangerous at many levels of intelligence and generality.
If a narrow AI is superhuman at a specific dangerous task like synthesizing viruses, it could be an
extraordinary hazard for humanity.
Superintelligence is not omnipotence.
Separately, we should not assume that superintelligence
must be omnipotent or omniscient. Superintelligence does not mean that an AI can instantly predict
how events worldwide will unfold in the far future, nor that the system can completely predict
the actions of all other agents with perfect accuracy. Likewise, it does not mean that the ASI
could instantly overpower humanity. Moreover, many problems cannot be solved by intelligence or
contemplation alone; research and development require real-world experimentation, which involves
physical-world processes that take a long time, presenting a key constraint to AIs’ ability to influence
the world. However, we know very little about what a system that is significantly smarter than
humans could do. Therefore, it is difficult to make confident claims about superintelligence.
Intelligence beyond human understanding.
A superintelligent AI would significantly outstrip
human capabilities, potentially solving problems and making discoveries beyond our comprehension.
Of course, this is not exclusive to superintelligence: even narrow AIs solve problems humans find
difficult to understand. AlphaFold, for instance, astonished scientists by predicting the 3D structure
of proteins—a complex problem that stumped biochemists for decades. Ultimately, a superintelligence
exceeds these other types of AI in the degree to which it exceeds human abilities across a variety of
cognitive tasks.
Summary
This section provided an introduction to artificial intelligence (AI), the broad umbrella that
encompasses the area of computer science focused on creating machines that perform tasks typically
associated with human intelligence. First, we discussed the nuances and difficulties of defining AI and
detailed its history. Then, we explored AI systems in more detail and how they are often categorized
into different types. Of these, we surveyed the five most common—narrow AI, human-level AI,
artificial general intelligence, transformative AI, and superintelligence—and saw how specificity is
often the best way to evaluate AIs. Considering specific capabilities and individual systems rather
than broad categories or abstractions is far more informative.
Next, we will narrow our focus to machine learning (ML), an approach within AI that emphasizes the
development of systems that can learn from data. Whereas many classical approaches to AI relied
on logical rules and formal, structured knowledge, ML systems use pattern recognition to extract
information from data.
2.2.3
Machine Learning
Overview and Definition
Machine learning (ML) is a subfield of AI that focuses on developing computer systems that can
learn directly from data without following explicit pre-set instructions [16, 17]. It accomplishes this
by creating computational models that discern patterns and correlations within data. The knowledge
encoded in these models allows them to inform decision-making or to reason about and act in the
world. For instance, an email spam filter uses ML to improve its ability to distinguish spam from

68
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
legitimate emails as it sees more examples. ML is the engine behind most modern AI applications,
from personalized recommendations on streaming services to autonomous vehicles. One of the most
popular and influential algorithmic techniques for ML applications is deep learning (DL), which uses
deep neural networks to process data.
Machine learning algorithms.
An algorithm is a recipe for getting something done—a procedure
for solving a problem or accomplishing a task, often expressed in a precise programming language.
Machine learning (ML) models are algorithms designed to learn from data by identifying patterns
and relationships, which enables them to make predictions or decisions as they process new inputs.
They often learn from information called training data. What makes ML models different from other
algorithms is that they automatically learn patterns in data without explicit task-specific instructions.
Instead, they identify correlations, dependencies, or relationships in the data and use this information
to make predictions or decisions about new data; for instance, a content curation application may
use ML algorithms to refine its recommendations.
Benefits of ML.
One of the key benefits of ML is its ability to automate complicated tasks, enabling
humans to focus on other activities. Developers use ML for applications from medical diagnosis and
autonomous vehicles to financial forecasting and writing. ML is becoming increasingly important for
businesses, governments, and other organizations to stay competitive and make empirically informed
decisions.
Guidelines for understanding ML models.
ML models can be intricate and varied, making
understanding their characteristics and distinctions a challenge. It can be helpful to focus on key
high-level aspects that almost all ML systems have:
• General Task: What is the primary goal of the ML model? We design models to
achieve objectives. Some example tasks are predicting housing prices, generating images or
text, or devising strategies to win a game.
• Inputs: What data does the ML system receive? This is the information that the model
processes to deliver its results.
• Outputs: What does the ML system produce? The model generates these results,
predictions, or decisions based on the input data.
• Type of Machine Learning: What technique is used to accomplish the task? This
describes how the models converts its inputs into outputs (called inference), and learn the best
way to convert its inputs into outputs (a learning process called training). An ML system
can be categorized by how it uses training data, what type of output it generates, and how it
reaches results.
The rest of this section delves deeper into these aspects of ML systems.
Key ML Tasks
In this section, we will explore four fundamental ML tasks—classification, regression, anomaly
detection, and sequence modeling—that describe different problems or types of problems that ML
models are designed to solve.
Classification

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
69
Classification is predicting categories or classes.
In classification tasks, models use
characteristics or features of an input data point (example) to determine which specific category the
data point belongs to. In medical diagnostics, a classification model might predict whether a tumor
is cancerous or benign based on features such as a patient’s age, tumor size, and tobacco use. This is
an example of binary classification—the special case in which models predict one of two categories.
Multi-class classification, on the other hand, involves predicting one of multiple categories. An image
classification model might classify an image as belonging to one of multiple different classes such
as dog, cat, hat, or ice cream. Computer vision often applies these methods to enable computers
to interpret and understand visual data from the world. Classification is categorization: it involves
putting data points into buckets.
Frequency of Positive Words
Frequency of Negative Words
Negative Review
Positive Review
Classification
Figure 2.1: ML models can classify data into different categories.
The sigmoid function produces probabilistic outputs.
A sigmoid is one of several
mathematical functions used in classification to transform general real numbers into values between
0 and 1. Suppose we wanted to predict the likelihood that a student will pass an exam or that a
prospective borrower will default on a loan. The sigmoid function is instrumental in settings like
these—problems that rely on computing probabilities. As a further example, in binary classification,
one might use a function like the sigmoid to estimate the likelihood that a customer makes a purchase
or clicks on an advertisement. However, it is important to note that other widely used models can
provide similar probabilistic outputs without employing a sigmoid function.

70
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
0%
25%
50%
75%
100%
1
2
3
4
5
Hours of Studying
Probablity of Passing Exam
Legend
Passed Students
Failed Students
Predictive Model
Predicting Exam Performance From Hours of Studying
Figure 2.2: Binary classification can use a sigmoid function to turn real numbers (such as hours of
studying) into probabilities between zero and one (such as the probability of passing).
Regression
Regression is predicting numbers.
In regression tasks, models use features of input data to
predict numerical outputs. A real estate company might use a regression model to predict house
prices from a dataset with features such as location, square footage, and number of bedrooms.
While classification models produce discrete outputs that place inputs into a finite set of categories,
regression models produce continuous outputs that can assume any value within a range. Therefore,
regression is predicting a continuous output variable based on one or more input variables. Regression
is estimation: it involves guessing what a feature of a data point will be given the rest of its
characteristics.

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
71
20
30
40
50
50
70
90
110
Precipitation (mm)
Number of Umbrellas Sold
Linear Regression: Rainfall and Umbrella Sales
Figure 2.3: This linear regression model is the best linear predictor of an output(umbrellas sold)
using only information from the input (precipitation).
Linear regression.
One type of regression, linear regression, assumes a linear relationship between
features and predicted values for a target variable. A linear relationship means that the output
changes at a constant rate with respect to the input variables, such that plotting the input-output
relationship on a graph forms a straight line. Linear regression models are often helpful but have
many limitations; for instance, their assumption that the features and the target variable are linearly
related is often false. In general, linear regression can struggle with modeling complicated data
patterns in the real world since they are roughly only as complex as their input variables and struggle
to add additional structures themselves.
Anomaly Detection
Anomaly detection is the identification of outliers or abnormal data points [18].
Anomaly
detection is vital in identifying hazards, including unexpected inputs, attempted cyberattacks, sudden
behavioral shifts, and unanticipated failures. Early detection of anomalies can substantially improve
the performance of models in real-world situations.

72
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
0
5
10
15
20
0
5
10
15
Account Age (days)
Account Activity (queries)
Bot
Human
Detecting Bots
0
25
50
75
100
125
0
25
50
75
100
Time (hours)
Electricity Consumption (kWh)
Anomaly
Typical
Detecting Unusual Energy Usage
Figure 2.4: The first graph shows the detection of atypical user activity. The second graph shows the
detection of unusually high energy usage. In both cases, the model detects anomalies. [19]
Black swan detection is an essential problem within anomaly detection.
Black swans
are unpredictable and rare events with a significant impact on the broader world. These events are
difficult to predict because they may not have happened before, so they are not represented in the
training data that ML models use to extrapolate the future. Due to their extreme and uncommon
nature, such events make anomaly detection challenging. In section 4.8 in the Safety Engineering
chapter, we discuss these ideas in more detail.
Sequence Modeling
Sequence modeling is analyzing and predicting patterns in sequential data.
Sequence
modeling is a broadly defined task that involves processing or predicting data where temporal or
sequential order matters. It may be applied to time-series data or natural language text to capture
dependencies between items in the sequence to forecast future elements. An integral part of this
process is representation learning, where models learn to convert raw data into more informative
formats for the task at hand. Language models use these techniques to predict subsequent words
in a sequence, transforming previous words into meaningful representations to detect patterns and
make predictions. There are several major subtypes of sequence modeling. Here, we will discuss two:
generative modeling and sequential decision-making.
Generative modeling. Generative modeling is a subtype of sequence modeling that creates new data
that resembles the input data, thereby drawing from the same distribution of features (conditioned
on specific inputs). It can generate new outputs from many input types, such as text, code, images,
and protein sequences.
Sequential decision-making (SDM). SDM equips a model with the capability to make informed choices
over time, considering the dynamic and uncertain nature of real-world environments. An essential
feature of SDM is that prior decisions can shape later ones. Related to SDM is reinforcement learning
(RL), where a model learns to make decisions by interacting with its environment and receiving
feedback through rewards or penalties. An example of SDM in complex, real-world tasks is a robot
performing a sequence of actions based on its current understanding of the environment.
Types of Input Data
In machine learning, a modality refers to how data is collected or represented—the type of input
data. Some models, such as image recognition models, use only one type of input data. In contrast,
multimodal systems integrate information from multiple modalities (such as images and text) to
improve the performance of learning-based approaches. Humans are naturally multimodal, as we

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
73
experience the world by seeing objects, hearing sounds, feeling textures, smelling odors, tasting
flavors, and more.
Below, we briefly describe the significant modalities in ML. However, this list is not exhaustive. Many
specific types of inputs, such as data from physical sensors, fMRI scans, topographic maps, and so
on, do not fit easily into this categorization.
• Tabular data: Structured data is stored in rows and columns, usually with each row
corresponding to an observation and each column representing a variable in the dataset.
An example is a spreadsheet of customer purchase histories.
• Text data: Unstructured textual data in natural language, code, or other formats. An example
is a collection of posts and comments from an online forum.
• Image data: Digital representations of visual information that can train ML models to classify
images, segment images, or perform other tasks. An example is a database of plant leaf images
for identifying species of plants.
• Video data: A sequence of visual information over time that can train ML models to recognize
actions, gestures, or objects in the footage. An example is a collection of sports videos for
analyzing player movements.
• Audio data: Sound recordings, such as speech or music. An example is a set of voice recordings
for training speech recognition models.
• Time-series data: Data collected over time that represents a sequence of observations or
events. An example is historical stock price data.
• Graph data: Data representing a network or graph structure, such as social networks or road
networks. An example is a graph that represents user connections in a social network.
• Set-valued data: Unstructured data in the form of collections of features or input vectors.
An example is point clouds obtained from LiDAR sensors.
Components of the ML Pipeline
An ML pipeline is a series of interconnected steps in developing a machine learning model, from
training it on data to deploying it in the real world. Next, we will examine these steps in turn.
Data collection.
The first step in building an ML model is data collection. Data can be collected
in various ways, such as by purchasing datasets from owners of data or scraping data from the web.
The foundation of any ML model is the dataset used to train it: the quality and quantity of data are
essential for accurate predictions and performance.
Selecting features and labels.
After the data is collected, developers of ML models must choose
what they want the model to do and what information to use. In ML, a feature is a specific and
measurable part of the data used to make predictions or classifications. Most ML models focus on
prediction. When predicting the price of a house, features might include the number of bedrooms,
square footage, and the age of the house. Part of creating an ML model is selecting, transforming, or
creating the most relevant features for the problem. The quality and type of features can significantly
impact the model’s performance, making it more or less accurate and efficient.

74
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
ML aims to predict labels.
A label (or a target) is the value we want to predict or estimate using
the features. Labels in training data are only present in supervised ML tasks, discussed later in this
section. Some models use a sample with correct labels to teach the model the output for a given set
of input features: a model could use historical data on housing prices to learn how prices are related
to features like square footage. However, other (unsupervised) ML models learn to make predictions
using unlabelled input data—without knowing the correct answers—by identifying patterns instead.
Choosing an ML architecture.
After ML model developers have collected the data and chosen a
task, they can process it. An ML architecture refers to a model’s overall structure and design. It can
include the type and configuration of the algorithm used and the arrangement of input and output
layers. The architecture of an ML model shapes how it learns from data, identifies patterns, and
makes predictions or decisions.
ML models have parameters.
Within an architecture, parameters are adjustable values within
the model that influence its performance. In the house pricing example, parameters might include
the weights assigned to different features of a house, like its size or location. During training, the
model adjusts these weights, or parameters, to minimize the difference between its predicted house
prices and the actual prices. The optimal set of parameters enables the model to make the best
possible predictions for unseen data, generalizing from the training dataset.
Training and using the ML model.
Once developers have built the model and collected all
necessary data, they can begin training and applying it. ML model training is adjusting a model’s
parameters based on a dataset, enabling it to recognize patterns and make predictions. During
training, the model learns from the provided data and modifies its parameters to minimize errors.
Model performance can be evaluated
Model evaluation measures the performance of the
trained model by testing it on data the model has never encountered before. Evaluating the model
on unseen data helps assess its generalizability and suitability for the intended problem. We may try
to predict housing prices for a new country beyond the original ML model’s original training data.
Once ready, models are deployed.
Finally, once the model is trained and evaluated, it can be
deployed in real-world applications. ML deployment involves integrating the model into a larger
system, using it, and then maintaining or updating it as needed.
Evaluating ML Models
Evaluation is a crucial step in model development. When developing a machine learning model, it is
essential to understand its performance. Evaluation—the process of measuring the performance of
a trained model on new, unseen data—provides insight into how well the model has learned. We
can use different metrics to understand a model’s strengths, weaknesses, and potential for real-world
applications. These quantitative performance measures are part of a broader context of goals and
values that inform how we can assess the quality of a model.
Metrics
Accuracy is a measure of the overall performance of a classification model.
Accuracy is
defined as the percentage of correct predictions:
Accuracy = # of correct predictions
# of total predictions .

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
75
Accuracy can be misleading if there is an imbalance in the number of examples of each class. For
instance, if 95% of emails received are not spam, a classifier assigning all emails to the “not spam”
category could achieve 95% accuracy. Accuracy applies when there is a well-defined sense of right
and wrong. Regression models focus on minimizing the error in their predictions.
Confusion matrices summarize the performance of classification algorithms.
A confusion
matrix is an evaluative tool for displaying different prediction errors. It is a table that compares a
model’s predicted values with the actual values. For example, the performance of a binary classifier
can be represented by a 2 × 2 confusion matrix, as shown in Figure 2.5. In this context, when making
predictions, there are four possible outcomes:
1. True positive (TP): A true positive is a correct prediction of the positive class.
2. False positive (FP): A false positive is an incorrect prediction of the positive class, predicting
positive instead of negative.
3. True negative (TN): A true negative is a correct prediction of the negative class.
4. False negative (FN): A false negative is an incorrect prediction of the negative class, predicting
negative instead of positive.
Figure 2.5: A confusion matrix shows the four possible outcomes from a prediction: true positive,
false positive, false negative, and true negative.
Since each prediction must be in one of these categories, the number of total predictions will be the
sum of the number of predictions in each category. The number of correct predictions will be the
sum of true positives and true negatives. Therefore,
Accuracy =
TP + TN
TP + TN + FP + FN

76
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
False positives vs. false negatives.
The impact of false positives and false negatives can vary
greatly depending on the setting. Which metric to choose depends on the specific context and
the error types one most wants to avoid. In cancer detection, while a false positive (incorrectly
identifying cancer in a cancer-free patient) may cause emotional distress, unnecessary further testing,
and potentially invasive procedures for the patient, a false negative can be much more dangerous: it
may delay diagnosis and treatment that allows cancer to progress, reducing the patient’s chances of
survival. By contrast, an autonomous vehicle with a water sensor that senses roads are wet when
they are dry (predicting false positives) might slow down and drive more cautiously, causing delays
and inconvenience, but one that senses the road is dry when it is wet (false negatives) might end up
in serious road accidents and cause fatalities.
While accuracy assigns equal cost to false positives and false negatives, other metrics isolate one or
weigh the two differently and might be more appropriate in some settings. Precision and recall are
two standard metrics that measure the extent of the error attributable to false positives and false
negatives, respectively.
Figure 2.6: Precision measures the correctness of positive predictions and penalizes false positives,
while recall measures how many positives are detected and penalizes false negatives. [20]
Precision measures the correctness of a model’s positive predictions. This metric represents the
fraction of positive predictions that are actually correct. It is calculated as
TP
TP+FP, dividing true
positives (hits) by the sum of true positives and false positives. High precision implies that when a
model predicts a positive class, it is usually correct—but it might incorrectly classify many positives
as negatives as well. Precision is like the model’s aim: when the system says it hit, how often is it
right?
Recall measures a model’s breadth. On the other hand, recall measures how good a model is at finding

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
77
all of the positive examples available. It is like the model’s net: how many real positives does it
catch? It is calculated as
TP
TP+FN, signifying the fraction of real positives that the model successfully
detected. High recall means a model is good at recognizing or “recalling” positive instances, but not
necessarily that these predictions are accurate. Therefore, a model with high recall may incorrectly
classify many negatives as positives.
In simple terms, precision is about a model being right when it makes a guess, and recall is about the
model finding as many of the right answers as possible. Together, these two metrics provide a way
to quantify how accurately and effectively a model can detect positive examples. Moreover, there
is a trade-off between precision and recall: for a given model, increasing precision will necessarily
decrease recall and vice versa.
AUROC scores measure a model’s discernment.
The AUROC (Area Under the Receiver
Operating Characteristic) score measures how well a classification model can distinguish between
different classes. The ROC curve shows the performance of a classification model by plotting the rate
of true positives against false positives as thresholds in a model are changed. AUROC scores range
from zero to one, where a score of 50% indicates random-chance performance and 100% indicates
perfect performance. To determine whether examples are positive (belong to a certain class) or
negative (do not belong to a certain class), a classification model will assign a score to each example
and compare that score to a threshold or benchmark value. We can interpret the AUROC as the
probability that a positive example scores higher than a negative example.
Figure 2.7: The area under the ROC curve (AUROC) increases as it moves in the upper left direction,
with more true positives and fewer false positives. [21]

78
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
Since it considers performance at all possible decision thresholds, the AUROC is useful for comparing
the performance of different classifiers. The AUROC is also helpful in cases of imbalanced data, as it
does not depend on the ratio of positive to negative examples.
Mean squared error (MSE) quantifies how “wrong” a model’s predictions are.
Mean
squared error is a valuable and popular metric of prediction error. It is found by taking the average
of the squared differences between the model’s predictions and the labels, thereby ensuring that
positive and negative deviations from the truth are penalized the same and that larger mistakes are
penalized heavily. The MSE is the most popular loss function for regression problems.
Reasonably vs. reliably solved.
The distinction between reasonable and reliable solutions can
be instrumental in developing a machine learning model, evaluating its performance, and thinking
about tradeoffs between goals. A task is reasonably solved if a model performs well enough to be
helpful in practice, but it may still have consistent limitations or make errors. A task is reliably
solved if a model achieves sufficiently high accuracy and consistency for safety-critical applications.
While models that reasonably solve problems may be sufficient in some settings, they may cause harm
in others. Chatbots currently give reasonable results, which is frustrating but essentially harmless.
However, if autonomous vehicles show reasonable but not reliable results, people’s lives are at stake.
Goals and Tradeoffs.
Above and beyond quantitative performance measures are multiple goals
and values that influence how we can assess the quality of a machine learning model. These goals—and
the tradeoffs that often arise between them—shape how models are judged and developed.
One such goal is predictive power, which measures the amount of error in predictions. Inference time
(or latency) measures how quickly a machine learning model can produce results from input data—in
many applications, such as self-driving cars, prediction speed is crucial. Transparency refers to the
interpretability of a machine learning model’s inner workings and how well humans can understand its
decision-making process. Reliability assesses the consistency of a model’s performance over time and
in varying conditions. Scalability is the capacity of a model to maintain or improve its performance
as a key variable—compute, parameter count, dataset size, and so on—scales.
Sometimes, these goals are in opposition, and improvements in one area can come at the cost of
declines in others. Therefore, developing a machine learning model requires careful consideration of
multiple competing goals.
2.2.4
Types of Machine Learning
One key dimension along which ML approaches vary is the degree of supervision.
We
can divide ML approaches into groups based on how they use the training data and what they
produce as an output. In ML, supervision is the process of guiding a model’s learning with some
kind of label. The model uses this label as a kind of ground truth or gold standard: a signal that can
be trusted as accurate and used to supervise the model to achieve the intended results better. Labels
can allow the model to capture relationships between inputs and their corresponding outputs more
effectively. Supervision is often vital to help models learn patterns and predict new, unseen data
accurately.
There are distinct approaches in machine learning for dealing with different amounts of supervision.
Here, we will explore three key approaches: supervised, unsupervised, and reinforcement learning.
We will also discuss deep learning, a set of techniques that can be applied in any of these settings.

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
79
Figure 2.8: The three main types of learning paradigms in machine learning are supervised learning,
unsupervised learning, and reinforcement learning.
Supervised Learning
Supervised learning is learning from labeled data.
Supervised learning is a type of machine
learning that uses a labeled dataset to learn the relationship between input data and output labels.
These labels are almost always human-generated: people will go through examples in a dataset and
give each one a label. They might be shown pictures of dogs and asked to label the breed. The
training process involves iteratively adjusting the model’s parameters to minimize the difference
between predicted outputs and the true output labels in the training data. Once trained, the model
can predict new, unlabeled data.
Examples of supervised learning.
Some examples of these labeled inputs and outputs include
mapping a photo of a plant to its species, a song to its genre, or an email to either “spam” or “not
spam.” A computer can use a set of dog pictures labeled by humans to predict the breed of any dog
in any given image. Supervised learning is analogous to a practice book, which offers a student a
series of questions (inputs) and then provides the answers (outputs) at the end of the book. This
book can help the student (like an ML model) find the correct answers when given new questions.
Without instruction or guidance, the student must learn to answer questions correctly by reviewing
the problems and checking their answers. Over time, they learn and improve through this checking
process.
Advantages and disadvantages.
Supervised learning can excel in classification and regression
tasks. Furthermore, it can result in high accuracy and reliable predictions when given large, labeled
datasets with well-defined features and target variables. However, this method performs poorly on
more loosely defined tasks, such as generating poems or new images. Supervision may also require
manual labeling for the training process, which can be prohibitively time-consuming and costly.
Critically, supervised learning is bottlenecked by the amount of labels, which can often result in less
data available than when using unsupervised learning.
Unsupervised Learning
Unsupervised learning is learning from unlabeled data.
Unsupervised learning involves
training a model on a dataset without specific output labels. Instead of matching its inputs to the
correct labels, the model must identify patterns within the data to help it understand the underlying
relationships between the variables. As no labels are provided, a model is left to its own devices to

80
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
Figure 2.9: In image inpainting, models are trained to predict hidden parts of images, causing them
to learn relationships between pixels. [22]
discover valuable patterns in the data. In some cases, a model leverages these patterns to generate
supervisory signals, guiding its own training. For this reason, unsupervised learning can also be
called self-supervised learning.
Examples of unsupervised learning.
Language models use unsupervised learning to learn
patterns in language using large datasets of unlabeled text. LLMs often learn to predict the next
word in a sentence, which enables the models to understand context and language structure without
explicit labels like word definitions and grammar instructions. After a model trains on this task, it
can apply what it learned to downstream tasks like answering questions or summarizing texts.
ML models exist on a spectrum of supervision.
Unsupervised and supervised learning are
valuable concepts for thinking about ML models, not a dichotomy with a clear dividing line. Therefore,
ML models are on a continuum of supervision, from datasets with clear labels for every data point
on one extreme to datasets with no labels on the other. In between lies partial or weak supervision,
which provides incomplete or noisy labels such as hashtags loosely describing features of images. This
is analogous to a practice book where some solution pages are excessively brief, have errors, or are
omitted entirely.
We can reframe many tasks into different types of ML.
Anomaly detection is typically
framed as an unsupervised task that identifies unusual data points without labels. However, it
can be refashioned as a supervised classification problem, such as labeling financial transactions as
“fraudulent” or “not fraudulent.” Similarly, while stock price prediction is usually approached as a
supervised regression task, it could be reframed as a classification task in which a model predicts
whether a stock price will increase or decrease. The choice in framing depends on the task’s specific
requirements, the data available, and which frame gives a more useful model. Ultimately, this
flexibility allows us to better cater to our goals and problems.

2.2. ARTIFICIAL INTELLIGENCE & MACHINE LEARNING
81
Reinforcement Learning
Reinforcement learning (RL) is learning from agent-gathered data.
Reinforcement learning
focuses on training artificial agents to make decisions and improve their performance based on
responses from their environment. It assumes that tasks can be modeled as goals to be achieved
by an agent maximizing rewards from its environment. RL is distinctive since it does not require
pre-collected data, as the agent can begin with no information and interact with its environment to
learn new things and acquire new data. This approach will be discussed in more depth later in the
Section Reinforcement Learning.
Examples of RL.
RL can help robots learn to navigate an unknown environment by taking actions
and receiving feedback in the form of rewards or penalties based on performance. Through trial
and error, agents learn to make better decisions and maximize rewards by adjusting their actions or
improving their model of the environment. It refines its strategy based on the consequences of its
activities. RL enables agents to learn techniques and decision-making skills through interaction with
their environment, which can adapt to dynamic and uncertain situations. However, it requires a well-
designed reward function and can be computationally expensive, especially for complex environments
with many possibilities for states and actions.
Deep Learning
Deep learning (DL) is a set of techniques that can be used in many learning settings.
Deep learning uses neural networks with many layers to create models that can learn from large
datasets.
Neural networks are the building blocks of deep learning models and use layers of
interconnected nodes to transform inputs into outputs. The structure and function of biological
neurons loosely inspired their design. Deep learning is not a new distinct learning type but rather a
computational approach that can accomplish any of the three types of learning discussed above. It is
most applicable to unsupervised learning tasks as it can perform well without any labels; for instance,
a deep neural network trained for object recognition in images can learn to identify patterns in the
raw pixel data.
Advantages and challenges in DL.
Deep learning excels in handling high-dimensional and
complex data, providing critical capabilities in image recognition, natural language processing, and
generative modeling. In ML, dimensionality denotes the number of features or variables in the
data, each representing a unique dimension. High-dimensional data has many features, as in image
recognition, where each pixel can be a feature. However, deep learning also requires vast data and
substantial computational power. Moreover, the models can be challenging to interpret.
Conclusion
AI is one of the most impactful and rapidly developing fields of computer science. Artificial intelligence
involves developing computer systems that can perform tasks that typically require human intelligence,
from visual perception to decision-making.
Machine learning is an approach to AI that involves developing models that can learn from data
to perform tasks without being explicitly programmed. A robust approach to understanding any
machine learning model is breaking it down into its fundamental components: the task, the input
data, the output, and the type of machine learning it uses. Different approaches to ML offer various
ways to tackle complex tasks and solve real-world problems. Deep learning is a powerful and popular
method that uses many-layered neural networks to identify intricate patterns in large datasets. The
following section will delve deeper into deep learning and its applications in artificial intelligence.

82
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
2.3
Deep Learning
Introduction
In this section, we present the fundamentals of deep learning (DL), a branch of machine learning that
uses neural networks to learn from data and perform complex tasks [23]. First, we will consider the
essential building blocks of deep learning models and explore how they learn. Then, we will discuss
the history of critical architectures and see how the field developed over time. Finally, we will explore
how deep learning is reshaping our world by reviewing a few of its groundbreaking applications.
Why Deep Learning Matters
Deep learning is a remarkably useful, powerful, and scalable technology that has been the primary
source of progress in machine learning since the early 2010s. Deep learning methods have dramatically
advanced the state-of-the-art in computer vision, speech recognition, natural language processing,
drug discovery, and many other areas.
Performance.
Some deep learning models have demonstrated better-than-human performance
in specific tasks, though unreliably. These models have excelled in tasks such as complex image
recognition and outmatched world experts in chess, Go, and challenging video games such as StarCraft.
However, their victories are far from comprehensive or absolute. Model performance is variable,
and deep learning models sometimes make errors or misclassifications obvious to a human observer.
Therefore, despite their impressive accomplishments in specific tasks, deep learning models have yet
to consistently surpass human intelligence or capabilities across all tasks or domains.
Real-world usefulness.
Beyond games and academia, deep learning techniques have proven useful
in a wide variety of real-world applications. They are increasingly integrated into everyday life,
from healthcare and social media to chatbots and autonomous vehicles. Deep learning can generate
product recommendations, predict energy load in a power grid, fly a drone, or create original works
of art.
Scalability.
Deep learning models are highly scalable and positioned to continue to advance in
capability as data, hardware, and training techniques progress. A key strength of these models is
their ability to process and learn from increasingly large amounts of data. Many traditional machine
learning algorithms’ performance gains taper off with additional data; by contrast, the performance
of deep learning models improves faster and longer.
Defining Deep Learning
Deep learning is a type of machine learning that uses neural networks with many layers to learn and
extract useful patterns from large datasets. It is characterized by its ability to learn hierarchical
representations of the world.
ML/DL distinction.
As we saw in the previous section, ML is the field of study that aims to give
computers the ability to learn without explicitly being programmed. DL is a highly adaptable and
remarkably effective approach to ML. Deep learning techniques are employed in and represent the
cutting edge of many areas of machine learning, including supervised, unsupervised, and reinforcement
learning.

2.3. DEEP LEARNING
83
Figure 2.10: Machine learning is a type of artificial intelligence. Supervised, unsupervised, and
reinforcement learning are common machine learning paradigms. Deep learning is a set of techniques
that have proven useful for a variety of ML problems.
Automatically learned representations.
Representations are, in general, stand-ins or substitutes
for the objects they represent. For example, the word “airplane” is a simple representation of a
complex object. Similarly, ML systems use representations of data to complete tasks. Ideally, these
representations are distillations that capture all essential elements or features of the data without
extraneous information.
While many traditional ML algorithms build representations from features hand-picked and engineered
by humans, features are learned in deep learning. The primary objective of deep learning is to enable
models to learn useful features and meaningful representations from data. These representations,
which capture the underlying patterns and structure of the data, form the base on which a model
solves problems. Therefore, model performance is directly related to representation quality. The
more insightful and informative a model’s representations are, the better it can complete tasks. Thus,
the key to deep learning is learning good representations.
Hierarchical representations.
Deep learning models represent the world as a nested hierarchy
of concepts or features. In this hierarchy, features build on one another to capture progressively
more abstract features. Higher-level representations are defined by and computed in terms of simpler
ones. In object detection, for example, a model may learn first to recognize edges, then corners and
contours, and finally parts of objects. Each set of features builds upon those that precede it:
1. Edges are (usually) readily apparent in raw pixel data.
2. Corners and contours are collections of edges.
3. Object parts are edges, corners, and contours.
4. Objects are collections of object parts.

84
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
This is analogous to how visual information is processed in the human brain. Edge detection is done
in early visual areas like the primary visual cortex, more complex shape detection in temporal regions,
and a complete visual scene is assembled in the brain’s frontal regions. Hierarchical representations
enable deep neural networks to learn abstract concepts and develop sophisticated models of the world.
They are essential to deep learning and why it is so powerful.
What Deep Learning Models Do
Deep learning models learn complicated relationships in data.
In general, machine learning
models can be thought of as a way of transforming any input into a meaningful output. Deep learning
models are an especially useful kind of machine learning model that can capture an extensive family
of relationships between input and output.
Function approximation.
In theory, neural networks—the backbone of deep learning models—can
learn almost any function that maps inputs to outputs, given enough data and a suitable network
architecture. Under some strong assumptions, a sufficiently large neural network can approximate any
continuous function (like y = ax2 + bx + c) with a combination of weights and biases. For this reason,
neural networks are sometimes called “universal function approximators.” While largely theoretical,
this idea provides an intuition for how deep learning models achieve such immense flexibility and
utility in their tasks.
Challenges and limitations.
Deep learning models do not have unlimited capabilities. Although
neural networks are very powerful, they are not the best suited to all tasks. Like any other model,
they are subject to tradeoffs, limitations, and real-world constraints. In addition, the performance of
deep neural networks often depends on the quality and quantity of data available to train the model,
the algorithms and architectures used, and the amount of computational power available.
Summary
Deep learning is an approach to machine learning that leverages multi-layer neural networks to achieve
impressive performance. Deep learning models can capture a remarkable family of relationships
between inputs and outputs by developing hierarchical representations. They have a number of
advantages over traditional ML models, including scaling more effectively, learning more sophisticated
relationships with less human input, and adapting more readily to different tasks with specialized
components. Next, we will make our understanding more concrete by looking more closely at exactly
what these components are and how they operate.
2.3.1
Model Building Blocks
In this section, we will explore some of the foundational building blocks of deep learning models. We
will begin by defining what a neural network is and then discuss the fundamental elements of neural
networks through the example of multi-layer perceptrons (MLPs), one of the most basic and common
types of deep learning architecture. Then, we will cover a few more technical concepts, including
activation functions, residual connections, convolution, and self-attention. Finally, we will see how
these concepts come together in the Transformer, another type of deep learning architecture.
Neural networks.
Neural networks are a type of machine learning algorithm composed of layers
of interconnected nodes or neurons. They are loosely inspired by the structure and function of the
human brain. Neurons are the basic computational units of neural networks. In essence, a neuron is
a function that takes in a weighted sum of its inputs and applies an activation function to transform
it, generating an output signal that is passed along to other neurons.

2.3. DEEP LEARNING
85
Biological inspiration.
The “artificial neurons” in neural networks were named after their biological
counterparts. Both artificial and biological neurons operate on the same basic principle. They receive
inputs from multiple sources, process them by performing a computation, and produce outputs that
depend on the inputs—in the case of biological neurons, firing only when a certain threshold is
exceeded. However, while biological neurons are intricate physical structures with many components
and interacting cells, artificial neurons are simplified computational units designed to mimic a few of
their characteristics.
Figure 2.11: Artificial neurons have some structural similarities to biological neurons. [24, 25]
Building blocks.
Neural networks are made of simple building blocks that can produce complex
abilities when combined at scale. Despite their simplicity, the resulting network can display remarkable
behaviors when thousands—or even millions—of artificial neurons are joined together.
Neural
networks consist of densely connected layers of neurons, each contributing a tiny fraction to the
overall processing power of the network. Within this basic blueprint, there is much room for variation;
for instance, neurons can be connected in many ways and employ various activation functions. These
network structure and design differences shape what and how a model can learn.
Multi-Layer Perceptrons
Multi-layer perceptrons (MLPs) are a foundational neural network architecture consisting of multiple
layers of nodes, each performing a weighted sum of its inputs and passing the result through an
activation function. They belong to a class of architectures known as “feedforward” neural networks,
where information flows in only one direction, from one layer to the next. MLPs are composed of at
least three layers: an input layer, one or more hidden layers, and an output layer.
The input layer serves as the entry point for data into a network.
The input layer consists
of nodes that encode information from input data to pass on to the next layer. Unlike in other layers,
the nodes do not perform any computation. Instead, each node in the input layer captures some
small raw input data and directly relays this information to the nodes in the subsequent layer. As
with other ML systems, input data for neural networks comes in many forms. For illustration, we will
focus on just one: image data. Specifically, we will draw from the classic example of digit recognition
with MNIST.
The MNIST (Modified National Institute of Standards and Technology) database is a large collection
of images of handwritten digits, each with dimensions 28 × 28. Consider a neural network trained to
classify these images. The input layer of this network consists of 784 nodes, each corresponding to

86
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
the grayscale value of a pixel in a given image.
Figure 2.12: Each pixel’s value is transferred to a neuron in the first layer. [26]
The output layer is the final layer of a neural network.
The output layer contains neurons
representing the results of the computations performed within the network. Like inputs, neural
network outputs come in many forms, such as predictions or classifications. In the case of MNIST (a
classification task), the output is categorical, predicting the digit represented by a particular image.
For classification tasks, the number of neurons in the output layer is equal to the number of possible
classes. In the MNIST example, the output layer will have ten neurons, one for each of the ten
classes (digits 0-9). The value of each neuron represents the predicted probability that an example
belongs to that class. The output value of the network is the class of the output neuron with the
highest value.
Hidden layers are the intermediate layers between the input and output layers.
Each
hidden layer is a collection of neurons that receive outputs from the previous layer, perform a
computation, and pass the results to the next layer. These are “hidden” because they are internal
to the network and not directly observable from its inputs or outputs. These layers are where
representations of features are learned.
Weights represent the strength of the connection between two neurons.
Every connection
is associated with a weight that determines how much the input signal from a given neuron will
influence the output of the next neuron. This value represents the importance or contribution of the
first neuron to the second. The larger the magnitude, the greater the influence. Neural networks
learn by modifying the values of their weights, which we will explore shortly.
Biases are additional learned parameters used to adjust neuron outputs.
Every neuron
has a bias that helps control its output. This bias acts as a constant term that shifts the activation
function along the input axis, allowing the neuron to learn more complex, flexible decision boundaries.
Similar to the constant b of a linear equation y = mx + b, the bias allows shifting the output of each
layer. In doing so, biases increase the range of the representations a neural network can learn.

2.3. DEEP LEARNING
87
Activation functions control the output or “activation” of neurons.
Activation functions are
nonlinear functions applied to each neuron’s weighted input sum within a neural network layer. They
are mathematical equations that control the output signal of the neurons, effectively determining the
degree to which each neuron “fires.”
Each neuron in a network takes some inputs, multiplies them by weights, adds a bias, and applies an
activation function. The activation function transforms this weighted input sum into an output signal.
For many activation functions, the more input a neuron receives, the more it activates, translating to
a larger output signal.
Activation functions allow for intricate representations.
Without activation functions, neural
networks would operate similarly to linear regression models, with added layers failing to contribute
any complexity to the model’s representations. Activation functions enable neural networks to learn
and express more sophisticated patterns and relationships by managing the output of neurons.
Single-layer and multi-layer networks.
Putting all of these elements together, single-layer
neural networks are the simplest form of neural network. They have only one hidden layer, comprising
an input layer, an output layer, and a hidden layer. Multi-layer neural networks add more hidden
layers in the middle. These networks are the basis of deep learning models.
Multi-layer neural networks are required for hierarchical representations.
While single-
layer networks can learn many things, they cannot learn the hierarchical representations that form
the cornerstone of deep learning. Layers provide the scaffolding of the pyramid. No layers means no
hierarchy. As the features learned in each layer build on those of previous layers, additional hidden
layers enable a neural network to learn more sophisticated and powerful representations. Simply put,
more layers capture more features at more levels of abstraction.
Figure 2.13: A classic multi-layer neural network has an input later, several hidden layers, and an
output layer.
Neural networks as matrix multiplication.
If we put aside the intuitive diagrammatic
representation, a neural network is a mathematical function that takes in a set of input values

88
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
and produces a set of output values via a series of steps. All the neurons in a layer can be represented
as a list or vector of activations. In any layer, this activation vector is multiplied with an input and
then transformed by applying an element-wise nonlinear function to the result. This is the layer’s
output, which becomes the input to the next layer. The network as a whole is the composition of all
of its layers.
A toy example.
Consider an MLP with two hidden layers, activation function g, and an input x.
This network could be expressed as W3g(W2g(W1x)):
1. In the input layer, the input vector x is passed on.
2. In the first hidden layer,
(a) the input vector x is multiplied by the weight vector, W1, yielding W1x,
(b) then the activation function g is applied, yielding g(W1x),
(c) which is passed on to the next layer.
3. In the second hidden layer,
(a) the vector passed to the layer is multiplied by the weight vector, W2, yielding W2g(W1x),
(b) then the activation function g is applied, yielding g(W2g(W1x)),
(c) which is passed on to the output layer.
4. In the output layer,
(a) the input to the layer is multiplied by the weight vector, W3, yielding W3g(W2g(W1x)),
(b) which is the output vector.
This process is mathematically equivalent to matrix multiplication.
This trait has significant
implications for the computational properties of neural networks. Since matrix multiplication lends
itself to being run in parallel, this equivalence allows specialized, more efficient processors such as
GPUs to be used during training.
Summary.
MLPs are models of a versatile and popular type of neural network that has been
successfully applied to many tasks. They are often a key component in many larger, more sophisticated
deep learning architectures. However, MLPs have limitations and are only sometimes the best-suited
approach to a task. Some of the building blocks we will see later on address the shortcomings of
MLPs and critical issues that can arise in deep learning more generally. Before that, we will look at
activation functions—the mechanisms that control how and when information is transmitted between
neurons—in more detail.
Key Activation Functions
Activation functions are a vital component of neural networks. They introduce nonlinearity, which
allows the network to model intricate patterns and relationships in data. By defining the activations
of each neuron within the network, activation functions act as informational gatekeepers that control
data transfer from one layer of the network to the next.

2.3. DEEP LEARNING
89
Using activation functions.
There are many activation functions, each with unique properties
and applications. Even within a single network, different layers may use other activation functions.
The selection and placement of activation functions can significantly change the network’s capability
and performance. In most cases, the same activation will be applied in all the hidden layers within a
network.
While many possible activation functions exist, only a handful are commonly used in practice. Here,
we highlight four that are of particular practical or historical significance. Although there are many
other functions and variations of each, these four—ReLU, GELU [27], sigmoid, and softmax—have
been highly influential in developing and applying deep learning. The Transformer architecture,
which we will describe later, uses GELU and softmax functions. Historically, many architectures
used ReLUs and sigmoids. Together, these functions illustrate the essential characteristics of the
properties and uses of activation functions in neural networks.
Rectified Linear Unit (ReLU).
The rectified linear unit (ReLU) function is a piecewise linear
function that returns the input value for positive inputs and zero for negative inputs [28]. It is
the identity function (f(x) = x) for positive inputs and zero otherwise. This means that if a
neuron’s weighted input sum is positive, it will be passed directly to the following layer without
any modification. However, no signal will be passed on if the sum is negative. Due to its piecewise
nature, the graph of the ReLU function takes the form of a distinctive “kinked” line. Due to its
computational efficiency, the ReLU function was widely used and played a critical role in developing
more sophisticated deep learning architectures.
0
2
4
6
−6
−4
−2
0
2
4
6
Input
Output
The ReLU Activation Function
Figure 2.14: The ReLU activation function, ReLU(x) = max{0, x}, passes on positive inputs to the
next layer.

90
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
Gaussian error linear unit (GELU).
The GELU (Gaussian error linear unit) function is
an upgrade of the ReLU function that uses approximation to smooth out the non-differentiable
component. This is important for optimization. It is “Gaussian” because it leverages the Gaussian
cumulative distribution function (CDF), Φ(x). The GELU has been widely used in and contributed
to the success of many current models, including Transformer-based language models.
0
1
2
3
−3
−2
−1
0
1
2
3
Input
Output
The GELU Activation Function
Figure 2.15: The GELU activation function, GELU(x) = x · Φ(x), smooths out the ReLU function
around zero, passing on small negative inputs as well.
Sigmoid.
A sigmoid is a smooth, differentiable function that maps any real-valued numerical input
to a value between zero and one. It is sometimes called a squashing function because it compresses
all real numbers to values in this range. When graphed, it forms a characteristic S-shaped curve. We
explored the sigmoid function in the previous section.
Softmax.
Softmax is a popular activation function due to its ability to model multi-class
probabilities. Unlike other activation functions that operate on each input individually, softmax
considers all inputs simultaneously to create a probability distribution across many dimensions. This
is useful in settings with multiple classes or categories, such as natural language processing, where
each word in a sentence can belong to one of numerous classes.
The softmax function can be considered a generalization of the sigmoid function. While the sigmoid
function maps a single input value to a number between 0 and 1, interpreted as a binary probability
of class membership, softmax normalizes a set of real values into a probability distribution over
multiple classes. Though it is typically applied to the output layer of neural networks for multi-class

2.3. DEEP LEARNING
91
0.0
0.5
1.0
−8
−6
−4
−2
0
2
4
6
8
Input
Output
The Sigmoid Activation Function
Figure 2.16: The Sigmoid activation function, σ(x) =
1
1+e−x , has a characteristic S-shape that
squeezes inputs into the interval [0, 1].

92
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
classification tasks—an example of when different activation functions are used within one network—
softmax may also be used in intermediate layers to readjust weights at bottleneck locations within a
network.
We can revisit the example of handwritten digit recognition. In this classification task, softmax is
applied in the last layer of the network as the final activation function. It takes in a 10-dimensional
vector of the raw outputs from the network and rescales the values to generate a probability
distribution over the ten predicted classes. Each class represents a digit from 0 to 9, and each
output value represents the probability that an input image is an instance of a given class. The digit
corresponding to the highest probability will be selected as the network’s prediction.
Now, having explored ReLU, GELU, sigmoid, and softmax, we will set aside activation functions and
turn our attention to other building blocks of deep learning models.
Residual Connections
Residual connections create alternative pathways in a network, preserving information.
Also known as skip connections, residual connections provide a pathway for information to bypass
specific layers or groups of layers (called blocks) in a neural network [29]. Without residual connections,
all information must travel sequentially through every layer of the network, undergoing continual,
significant change as each layer receives and transforms the output of the previous one. Residual
connections allow data to skip these transformations, preserving its original content. With residual
connections, layers can access more than just the previous layer’s representations as information flows
through and around each block in the network. Consequently, lower-level features learned in earlier
layers can contribute more directly to the higher-level features of deeper layers, and information can
be more readily preserved.
Figure 2.17: Adding residual connections can let information bypass blocks fl and fl+1, letting
lower-level features from early layers contribute more directly to higher-level features in later ones.
Residual connections facilitate learning.
Residual connections improve learning dynamics
in several ways by facilitating the flow of information during the training process. This improves
iterative and hierarchical feature representations, particularly for deeper networks.
Neural networks typically learn by decomposing data into a hierarchy of features, where each layer
learns a distinct representation. Residual connections allow for a different kind of learning in which

2.3. DEEP LEARNING
93
learned representations are gradually refined. Each block improves upon the representation of the
previous block, but the overall meaning captured by each layer remains consistent across successive
blocks.
This allows feature maps learned in earlier layers to be reused and networks to learn
representations (such as identity mappings) in deeper layers that may otherwise not be possible due
to optimization difficulties.
Residual connections are general purpose, used in many different problem settings and architectures.
By facilitating the learning process and expanding the kinds of representations networks can learn,
they are a valuable building block that can be a helpful addition to a wide variety of networks.
Convolution
In machine learning, convolution is a process used to detect patterns or features in input data by
applying a small matrix called a filter or kernel and looking for cross-correlations. This process
involves sliding the filter over the input data, systematically comparing relevant sections using matrix
multiplication with the filter, and recording the results in a new matrix called a feature map.
Figure 2.18: Convolution layers perform the convolution operation, sliding a filter over the input
data to output a feature map. [30]
Convolutional layers.
Convolutional layers are specialized layers that perform the “convolution”
operation to detect local features in the input data. These layers commonly comprise multiple filters,
each learning a different feature. Convolution is considered a localized process because the filter
usually operates on small, specific regions of the input data at a time (such as parts of an image).

94
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
This allows the network to recognize features regardless of their position in the input data, making
convolution well-suited for tasks like image recognition.
Convolutional neural networks (CNNs).
Convolution has become a key technique in modern
computer vision models because it effectively captures local features in images and can deal with
variations in their position or appearance. This helps improve the accuracy of models for tasks like
object detection or facial recognition compared to fully connected networks. Convolutional neural
networks (CNNs) use convolution to process spatial data, such as images or videos, by applying
convolutional filters that extract local features from the input.
Convolution was instrumental in the transition of deep learning from MLPs to more sophisticated
architectures and has maintained significant influence, especially in vision-related tasks.
Self-Attention
Self-attention can produce more coherent representations.
Self-attention encodes the
relationships between elements in a sequence to better understand and represent the information
within the sequence. In self-attention, each element attends to every other element by determining
its relative importance and selectively focusing on the most relevant connections.
Figure 2.19: Different attention heads can capture different relationships between words in the same
sentence. [31]
This process allows the model to capture dependencies and relationships within the sequence, even
when they are separated by long distances. As a result, deep learning models can create a more
context-aware representation of the sequence. When summarizing a long book, self-attention can help

2.3. DEEP LEARNING
95
the model understand which parts of the text are most relevant and central to the overall meaning,
leading to a more coherent summary.
Transformers
The Transformer is a groundbreaking deep learning model that leverages self-attention [31]. It is a
very general and versatile architecture that can achieve outstanding performance across many data
types. The model itself consists of a series of Transformer blocks.
Figure 2.20:
Transformer blocks
combine
several
other
techniques, such as self-attention,
MLPs, residual connections, and
layer normalization. [31]
A Transformer block primarily combines self-attention and
MLPs (as we saw earlier) with optimization techniques such
as residual connections and layer normalization.
Large language models (LLMs).
LLMs are a class of
language models with many parameters (often in the billions)
trained on vast quantities of data.
These models excel in
various language tasks, including question-answering, text
generation, coding, translation, and sentiment analysis. Most
LLMs, such as the Generative Pre-trained Transformer (GPT)
series, utilize Transformers because they can effectively model
long-range dependencies.
Summary
Deep learning models are networks composed of many layers of
interconnected nodes. The structure of this network plays
a vital role in shaping how a model functions.
Creating
a successful model requires carefully assembling numerous
components.
Different components are used in different
settings, and each building block serves a unique purpose,
contributing to a model’s overall performance and capabilities.
This section discussed multi-layer perceptrons (MLPs),
activation functions, residual connections, convolution, and self-
attention, culminating with an introduction to the Transformer
architecture. We saw how MLPs, an archetypal deep learning model, paved the way for other
architectures and remain an essential component of many more sophisticated models. Many building
blocks each play a distinct role in the structure and function of a model.
Activation functions like ReLU, softmax, and GELU introduce nonlinearity in networks, enabling
models to learn complex patterns. Residual connections facilitate the flow of information in a network,
thereby enabling the training of deeper networks. Convolution uses sliding filters to allow models to
detect local features in input data, an especially useful capability in vision-related tasks. Self-attention
enables models to weigh the relevance of different inputs based on their context. By leveraging
these mechanisms mechanism to handle complex dependencies in sequential data, Transformers
revolutionized the field of natural language processing (NLP).
2.3.2
Training and Inference
Having explored the components of deep learning models, we will now explore how the models work.
First, we will briefly describe training and inference: the two key phases of developing a deep learning
model. Next, we will examine learning mechanics and see how the training process enables models

96
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
to learn and continually refine their representations. Then, we will discuss a few techniques and
approaches to learning and training deep learning models and consider how model evaluation can
help us understand a model’s potential for real-world applications.
Training is learning and inference is executing.
As we saw previously in section 2.2, training
is the process through which the model learns from data. During training, a model is fed data and
makes iterative parameter adjustments to predict target outcomes better. Inference is the process of
using a trained model to make predictions on new, unseen data. Inference is when a model applies
what it has learned during training. We will now turn to training and examine how models learn in
more detail.
Mechanics of Learning
In deep learning, training is a carefully coordinated system involving loss functions, optimization
algorithms, backpropagation, and other techniques. It allows a model to refine its predictions
iteratively. By making incremental adjustments to its parameters, training enables a model to
gradually reduce its error, improving its performance over time.
Loss quantifies a model’s error.
Loss is a measure of a model’s error, used to evaluate its
performance. It is calculated by a loss function that compares target and predicted values to measure
how well the neural network models “fits” the training data. Typically, neural networks are trained
by systematically minimizing this function. There are many different kinds of loss functions. Here,
we will present two: cross entropy loss and mean squared error (MSE).
Cross entropy loss. Cross entropy is a concept from information theory that measures the difference
between two probability distributions. In deep learning, cross entropy loss is often used in classification
problems, where it compares the probability distribution predicted by a model and the target
distribution we want the model to predict.
Consider a binary classification problem where a model is tasked with classifying images as either
apples or oranges. When given an image of an apple, a perfect model would predict “apple” with
100% probability. In other words, with classes [apple, orange], the target distribution would be [1, 0].
The cross entropy would be low if the model predicts “apple” with 90% probability (outputting a
predicted distribution of [0.9, 0.1]). However, if the model predicts “orange” with 99% probability, it
would have a much higher loss. The model learns to generate predictions closer to the true class
labels by minimizing the cross entropy loss during training.
Cross entropy quantifies the difference between predicted and true probabilities. If the predicted
distribution is close to the true distribution, the cross entropy will be low, indicating better model
performance. High cross entropy, on the other hand, signals poor performance. When used as a loss
function, the more incorrect the model’s predictions are, the larger the error and, in turn, the larger
the training update.
Mean squared error (MSE). Mean squared error is one of the most popular loss functions for regression
problems. It is calculated as the average of the squared differences between target and predicted
values.
MSE = 1
n
n
X
i=1
(yi −ˆyi)2

2.3. DEEP LEARNING
97
MSE gives a good measure of how far away an output is from its target in a way that is not affected
by the direction of errors. Like cross entropy, MSE provides a larger error signal the more wrong the
output guess, helping the training process converge more quickly. One weakness of MSE is that it is
highly sensitive to outliers, as squaring amplifies large differences, although there are variants and
alternatives such as mean absolute error (MAE) and Huber loss which are more robust to outliers.
Loss is minimized through optimization.
Optimization is the process of minimizing (or
maximizing) an objective function.
In deep learning, optimization involves finding the set of
parameters that minimize the loss function. This is achieved with optimizers–—algorithms that
adjust a model’s parameters, such as weights and biases, to reduce the loss.
Gradient descent is a crucial optimization algorithm.
Gradient descent is a foundational
optimization algorithm that provides the basis for many advanced optimizers used in deep learning.
It was among the earliest techniques developed for optimization.
To understand the basic idea behind gradient descent, imagine a blindfolded hiker standing on a hill
trying to reach the bottom of a valley. With each step, they can feel the slope of the hill beneath
their feet and move in the direction that goes downhill the most. While the hiker cannot tell where
exactly they are going or where they are ending up, they can continue this process, always taking
steps toward the steepest descent until they have reached the lowest point.
In machine learning, the hill is the loss function, and the steps are updates to the model’s parameters.
The direction of steepest descent is calculated using the gradients (derivatives) of the loss function
with respect to the model’s parameters.
Figure 2.21: Gradient descent can find different local minima given two different weight initializations.
[32]
The size of the steps is determined by the learning rate, a parameter of the model configuration

98
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
(known as a hyperparameter) used to control how much a model’s weights are changed with each
update. If the learning rate is too large, the high learning rate may destroy information faster than
information is learned. However, the optimization process may be very slow if the learning rate is
too small. Therefore, proper learning rate selection is often key to effective training.
Though powerful, gradient descent in its simplest form can be quite slow. Several variants, including
Adam (Adaptive Moment Estimation), were developed to address these weaknesses and are more
commonly used in practice.
Backpropagation facilitates parameter updates.
Backpropagation is a widely used method to
compute the gradients in a neural network [33]. This process is essential for updating the model’s
parameters and makes gradient descent possible. Backpropagation is a way to send the error signal
from the output layer of the neural network back to the input layer. It allows the model to understand
how much each parameter contributes to the overall error and adjust them accordingly to minimize
the loss.
Steps to training a deep learning model.
Putting all of these components together, training is
a multi-step process that typically involves the following:
1. Initialization: A model’s parameters (weights and biases) are set to some initial values, often
small random numbers. These values define the starting point for the model’s training and can
significantly influence its success.
2. Forward Propagation: Input data is passed through the model, layer by layer. The neurons
in each layer perform their specific operations via weights, biases, and activation functions.
Once the final layer is reached, an output is produced. This procedure can be carried out on
individual examples or on batches of multiple data points.
3. Loss Calculation: The model’s output is compared to the target output using a loss function
that quantifies the difference between predicted and actual values. The loss represents the
model’s error—how far its output was from what it should have been.
4. Backpropagation: The error is propagated back through the model, starting from the output
layer and going backward to the input layer. This process calculates gradients that determine
how much each parameter contributed to the overall loss.
5. Parameter Update: The model’s weights and biases are adjusted using an optimization
algorithm based on the gradients. This is typically done using gradient descent or one of its
variants.
6. Iteration: Steps 2–5 are repeated many times, often reaching millions or billions of iterations.
With each pass, the loss should decrease as the model’s predictions improve.
7. Stopping Criterion: Training continues until the model reaches a stopping point, which can
be defined in many ways. We may stop training when the loss stops decreasing or when the
model has gone through the entire training dataset a specific number of times.
While this sketch provides a high-level overview of the training process, many factors can shape
its course. For example, the network architecture and choice of loss function, optimizer, batch
size, learning rate, and other hyperparameters influence how training proceeds. Moreover, different
methods and approaches to learning determine how training is carried out. We will explore some of
these techniques in the next section.

2.3. DEEP LEARNING
99
Training Methods and Techniques
Effective training is essential to the ability of deep learning models to learn how to accomplish
tasks. Various methods have been developed to address key issues many models face in training.
Some techniques offer distinct approaches to learning, whereas others solve specific computational
difficulties. Each has unique characteristics and applications that can significantly enhance a model’s
performance and adaptability. Different techniques are often used together, like a recipe using many
ingredients.
In this section, we limit our discussion to pre-training, fine-tuning, and few-shot learning. These
three methods illustrate different ways of approaching the learning process. Notably, there are ways
to learn during training (during backpropagation and model weight adjustment), and there are also
ways to learn after training (during inference). Pre-training and fine-tuning belong to the former,
and few-shot learning belongs to the latter.
Pre-training is the bulk of generic training.
Pre-training is training a model on vast quantities
of data to give the model an array of generally useful representations that it can use to achieve
specific tasks. If we want a model that can write movie scripts, we want it to have a broad education,
knowing rules about grammar and language and how to write more generally, rather than just seeing
existing movie scripts.
Pre-training endows models with weights that capture a rich set of learned representations from the
outset rather than being assigned random values. This can offer several advantages over training
for specific purposes from scratch, including faster and more effective training on downstream tasks.
Indeed, the name pre-training is somewhat of a historical artifact. As pre-training makes up most of
the development process for many models, pre-training and training have become synonymous.
The preprocessing step tokenization is common in machine learning and natural language processing
(NLP). It involves breaking down text, such as a sentence or a document, into smaller units called
tokens. Tokens are typically words or subword units such as “play” (from “playing”), “un“ (from
“unbelievable”), punctuation tokens, and so on. Tokenization allows a machine learning model to
factor text data as consistent discrete units, making it faster to process.
Models can either be fine-tuned or used only pre-trained.
Pre-trained models can be used
as is (known as off-the-shelf ) or subjected to further training (known as fine-tuned) on a target
task or dataset. In natural language processing and computer vision, it is common to use models
that have been pre-trained on large datasets. Many CNNs are pre-trained on the ImageNet dataset,
enabling them to learn many essential characteristics of the visual world.
Fine-tuning specializes models for specific tasks.
Fine-tuning is the process of adapting a
pre-trained model to a new dataset or task through additional training. In fine-tuning, the weights
from the pre-trained model are used as the starting point for the new model. Then, some or all layers
are trained on the new task or data, often with a lower learning rate.
Layers that are not trained are said to be frozen. Their weights will remain unchanged to preserve
helpful representations learned in pre-training. Typically, layers are modified in reverse order, from
the output layer toward the input layer. This allows the more specialized, high-level representations
of later layers to be tailored to the new task while conserving the more general representations of
earlier layers.

100
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
After training, few-shot learning can teach new capabilities.
Few-shot learning is a method
that enables models to learn and adapt quickly to new tasks with limited data. It works best when
a model has already learned good representations for the tasks it needs to perform. In few-shot
learning, models are trained to perform tasks using a minimal number of examples. This approach
tests the model’s ability to learn quickly and effectively from a small dataset. Few-shot learning can
be used to train an image classifier to recognize new categories of animals after seeing only a few
images of each animal.
Zero-shot learning.
Zero-shot learning is an extreme version of few-shot learning. It tests a
model’s ability to perform on characteristically new data without being provided any examples
during training. The goal is to enable the model to generalize to new classes or tasks by leveraging
its understanding of relationships in the data derived from seen examples to predict new, unseen
examples.
Zero-shot learning often relies on additional information, such as attributes or natural language
descriptions of unseen data, to bridge the gap between known and unknown. For instance, consider a
model trained to identify common birds, where each species is represented by images and a set of
attributes (such as size, color, diet, and range) or a brief description of the bird’s appearance and
behavior. The model is trained to associate the images with these descriptions or attributes. When
presented with the attributes or description of a new species, the model can use this information to
infer characteristics about the unknown bird and recognize it in images.
LLMs,
few-shot,
and zero-shot learning.
Some large language models (LLMs) have
demonstrated a capacity to perform few- and zero-shot learning tasks without explicit training.
As model and training datasets increased in size, these models developed the ability to solve a variety
of tasks when provided with a few examples (few-shot) or only instructions describing the task
(zero-shot) during inference; for instance, an LLM can be asked to classify a paragraph as having
positive or negative sentiments without specific training. These capabilities arose organically as the
models increased in size and complexity, and their unexpected emergence raises questions about what
enables LLMs to perform these tasks, especially when they are only explicitly trained to predict the
next token in a sequence. Moreover, as these models continue to evolve, this prompts speculation
about what other capabilities may arise with greater scale.
Summary.
There are many training techniques used in deep learning. Pre-training and fine-tuning
are the foundation of many successful models, allowing them to learn valuable representations from
one task or dataset and apply them to another. Few-shot and zero-shot learning enable models
to solve tasks based on scarce or no example data. Notably, the emergence of few- and zero-shot
learning capabilities in large language models illuminates the potential for these models to adapt and
generalize beyond their explicit training. Ongoing advancements in training techniques continue to
drive the growth of AI capabilities, highlighting both exciting opportunities and important questions
about the future of the field.
2.3.3
History and Timeline of Key Architectures
Having built our technical understanding of deep learning models and how they work, we will see
how these concepts come together in some of the groundbreaking architectures that have shaped the
field. We will take a chronological tour of key deep learning models, from the pioneering LeNet in
1989 to the revolutionary Transformer-based BERT and GPT in 2018. These architectures, varying
in design and purpose, have paved the way for developing increasingly sophisticated and capable
models. While the history of deep learning extends far beyond these examples, this snapshot sheds

2.3. DEEP LEARNING
101
light on a handful of critical moments as neural networks evolved from a marginal theory in the
mid-1900s to the vanguard of artificial intelligence development by the early 2010s.
1989: LeNet
LeNet paves the way for future deep learning models [34].
LeNet is a convolutional neural
network (CNN) proposed by Yann LeCun and his colleagues at Bell Labs in 1989. This prototype
was the first practical application of backpropagation, and after multiple iterations of refinement,
LeCun et al. presented the flagship model, LeNet-5, in 1998. This model demonstrated the utility of
neural networks in everyday applications and inspired many deep learning architectures in the years
to follow. However, due to computational constraints, CNNs did not rise in popularity for over a
decade after LeNet-5 was released.
1997:
Recurrent Neural Networks (RNNs) & Long Short-Term Memory (LSTM)
Networks
Recurrent neural networks (RNNs) use feedback loops to remember.
RNNs are a neural
network architecture designed to process sequential or time-series data, such as text and speech. They
were developed to address failures of traditional feedforward neural networks in modeling the temporal
dependencies inherent to these types of data. RNNs incorporate a concept of “memory” to capture
patterns that occur over time, like trends in stock prices or weather observations and relationships
between words in a sentence. They use a feedback loop with a hidden state that stores information
from prior inputs, giving them the ability to “remember” and take historical information into account
when processing future inputs. While this marked a significant architectural advancement, RNNs
were difficult to train and struggled to learn patterns that occur over more extended amounts of time.
Long short-term memory (LSTM) networks improved memory [35].
LSTMs are a type of
RNN that address some of the shortcomings of standard RNNs, allowing them to model long-term
dependencies more effectively. LSTMs introduce three gates (input, output, and forget) to the
memory cell of standard RNNs to regulate the flow of information in and out of the unit. These gates
determine how much information is let in (input gate), how much information is retained (forget
gate), and how much information is passed along (output gate). This approach allows the network to
learn more efficiently and maintain relevant information for longer.
2012: AlexNet
AlexNet achieves unprecedented performance in image recognition [12].
As we saw in
section 2.2, the ImageNet Challenge was a large-scale image recognition competition that spurred
the development and adoption of deep learning methods for computer vision. The challenge involved
classifying images into 1,000 categories using a dataset of over one million images.
In 2012, a CNN called AlexNet, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton,
achieved a breakthrough performance of 15.3% top-5 error rate, beating the previous best result
of 26.2% by a large margin and winning the ImageNet Large Scale Visual Recognition Challenge
(ILSVRC). AlexNet consists of eight layers: five convolutional layers and three fully connected layers.
It uses a ReLU activation function and specialized techniques such as dropout and data augmentation
to improve accuracy.

102
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
2015: ResNets (Residual Networks)
ResNets employ residual connections [29].
ResNets were introduced in 2015 by Microsoft
researchers Kaiming He and collaborators. The original model was the first architecture to implement
residual connections. By adding these connections to a traditional 34-layer network, the authors were
able to achieve great success. In 2015, it won first place in the ImageNet classification challenge with
a top-5 error rate of 3.57%.
2017: Transformers
Transformers introduce self-attention.
The Transformer architecture was introduced by
Vaswani et al. in their revolutionary paper “Attention is All You Need.” Like RNNs and LSTMs,
Transformers are a type of neural network that can process sequential data. However, the approach
used in the Transformer was markedly different from those of its predecessors. The Transformer
uses self-attention mechanisms that allow the model to focus on relevant parts of the input and the
output.
2018: BERT (Bidirectional Encoder Representations from Transformers) & GPT
(Generative Pre-Trained Transformer)
BERT and GPT, both launched in 2018, are two models based on the Transformer architecture [14].
BERT uses pre-training and bidirectional processing [36].
BERT is a Transformer-based
model that can learn contextual representations of natural language by pre-training on large-scale
corpora. Unlike previous models that process words in one direction (left-to-right or right-to-left),
BERT takes a bidirectional approach. It is pre-trained on massive amounts of text to perform masked
language modeling and next sentence prediction tasks. Then, the pre-trained model can be fine-tuned
on various natural language understanding tasks, such as question answering, sentiment analysis,
and named entity recognition. BERT was the first wide-scale, successful use of Transformers, and its
contextual approach allowed it to achieve state-of-the-art results on several benchmarks.
The GPT models use scale and unidirectional processing.
The GPT models are a series
of Transformer-based language models launched by OpenAI. The size of these models and scale at
which they were trained led to a remarkable improvement in fluency and accuracy in various language
tasks, significantly advancing the state-of-the-art in natural language processing. One of the key
reasons GPT models are more popular than BERT models is because they are better at generating
text. While BERT learns really good representations through being trained to fill in blanks in the
middle of sentences, GPT models are trained to predict what comes next, enabling them to generate
long-form sequences (e.g. sentences, paragraphs, and essays) much more naturally.
Many important developments have been left out in this brief timeline. Perhaps more importantly,
future developments might revolutionize model architectures in new ways, perhaps bringing to light
older innovations that have currently fallen to the wayside. Next, we will explore some common
applications of deep learning models.
2.3.4
Applications
Deep learning has seen a dramatic rise in popularity since the early 2010s, increasingly becoming a
part of our daily lives. Its applications are broad, powering countless services and technologies across
many industries, some of which are highlighted below.

2.4. SCALING LAWS
103
Communication and entertainment.
Deep learning powers the chatbots and generative tools
that sparked the surge in global interest in AI that began in late 2022. It fuels the recommendation
systems of many streaming services like Netflix, YouTube, and Spotify, curating personalized content
based on viewing or listening habits. Social media platforms, like Facebook or Instagram, use deep
learning for image and speech recognition to enable features such as auto-tagging in photos or
video transcription. Personal assistants like Siri, Google Assistant, and Alexa utilize deep learning
techniques for speech recognition and natural language understanding, providing us with more natural,
interactive voice interfaces.
Transportation and logistics.
Deep learning is central to the development of autonomous vehicles.
It helps these vehicles understand their environment, recognize objects, and make decisions. Retail
and logistics companies like Amazon use deep learning for inventory management, sales forecasting,
and to enable robots to navigate their warehouses.
Healthcare.
Deep learning has been used to assist in diagnosing diseases, analyzing medical images,
predicting patient outcomes, and personalizing treatment plans. It has played a significant role in
drug discovery, reducing the time and costs associated with traditional methods.
Beyond this, deep learning is also used in cybersecurity, agriculture, finance, business analytics, and
many other settings that can benefit from decision making based on large unstructured datasets.
With the more general abilities of LLMs, the impact of deep learning is set to disrupt more industries,
such as through automatic code generation and writing.
Conclusion
Deep learning has come a long way since its early days, with advancements in architectures, techniques,
and applications driving significant progress in artificial intelligence. Deep learning models have been
used to solve complex problems and provide valuable insights in many different domains. As data
and computing power become more available and algorithmic techniques continue to improve in the
years to come, we can expect deep learning to become even more prevalent and impactful.
In the next section, we will discuss scaling laws: a set of principles which can quantitatively predict
the effects of more data, larger models, and more computing power on the performance of deep
learning models. These laws shape how deep learning models are constructed.
2.4
Scaling Laws
Introduction
Compelling evidence shows that increases in the performance of many AI systems can be modeled
with equations called scaling laws. Common knowledge suggests that larger models with more
data will perform better, frequently reiterated in phrases like “add more layers” or “use more data.”
Scaling laws make this folk knowledge mathematically precise. In this section, we show that the
performance of a deep learning model scales according to parameter count and dataset size—both of
which are primarily bottlenecked by the computational resources available. Scaling laws describe the
relationship between a model’s performance and primary inputs.
Conceptual Background: Power Laws
Power laws are mathematical equations that model how a particular quantity varies as the power
of another. In power laws, the variation in one quantity is proportional to a power (exponent) of

104
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
the variation in another. The power law y = bxa states that the change in y is directly proportional
to the change in x raised to a certain power a. If a is 2, then when x is doubled, y will quadruple.
One real-world example is the relation between the area of a circle and its radius. As the radius
changes, the area changes as a square of the radius: y = πr2. This is a power-law equation where
b = π and a = 2. The volume of a sphere has a power-law relationship with the sphere’s radius as
well: y = 4
3πr3 (so b = 4
3π and a = 3). Scaling laws are a particular kind of power law that describe
how deep learning models scale. These laws relate a model’s loss with model properties (such as the
number of model parameters or the dataset size used to train the model).
Log-log plots can be used to visualize power laws.
Log-log plots can help make these
mathematical relationships easier to understand and identify. Consider the power law y = bxa again.
Taking the logarithm of both sides, the power law becomes log(y) = a log(x) + log(b). This is a linear
equation (in the logarithmic space) where a is the slope and log(b) is the y-intercept. Therefore, a
power-law relationship will appear as a straight line on a log-log plot (such as 2.22), with the slope
of the line corresponding to the exponent in the power law.
0
50
100
150
200
0
2
4
6
Time in Air (in seconds)
Distance Fallen (in meters)
Power Law Data (Linear Plot)
1
10
100
0.5
1.0
3.0
5.0
Time in Air (in seconds)
Distance Fallen (in meters)
Power Law Data (Log−Log Plot)
Figure 2.22: An object in free falling in a vacuum falls a distance proportion to the square of the
time. On a log-log plot, this power law looks like a straight line.
Power laws are remarkably ubiquitous.
Power laws are a robust mathematical framework that
can describe, predict, and explain a vast range of phenomena in both nature and society. Power laws
are pervasive in urban planning: log-log plots relating variables like city population to metrics such
as the percentage of cities with at least that population often result in a straight line (see Fig 2.23).
Similarly, animals’ metabolic rates are proportional to an exponent of their body mass, showcasing a
clear power law. In social media, the distribution of user activity often follows a power law, where
a small fraction of users generate most of the content (which means that the frequency of content
generation y is proportional to the number of active users x multiplied by some constant and raised
to some exponent: y = bxa). Power laws govern many other things, such as the frequency of word
usage in a language, the distribution of wealth, the magnitude of earthquakes, and more.

2.4. SCALING LAWS
105
0.01%
0.04%
0.15%
30000
100000
300000
Population of City (Linear Scale)
Percentage of Cities
Linear Plot of City Population Frequencies
0.01%
0.04%
0.15%
30000
100000
300000
Population of City (Log Scale)
Percentage of Cities (Log Scale)
Log−Log Plot of City Population Frequencies
Figure 2.23: Power laws are used in many domains, such as city planning.
2.4.1
Scaling Laws in Deep Learning
Introduction
Power laws in the context of DL are called scaling laws [37, 38].
Scaling laws predict loss
given model size and dataset size in a power-law relationship. Model size is usually measured in
parameters, while dataset size is measured in tokens. As both variables increase, the model’s loss tends
to decrease. This decrease in loss with scale often follows a power law: the loss drops substantially,
but not linearly, with increases in data and model size. For instance, if we doubled the number of
parameters, the loss does not just halve: it might decrease to one-fourth or one-eighth, depending
on the exponent in the scaling law. This power-law behavior in AI systems allows researchers to
anticipate and strategize how to improve models by investing more in increasing the data or the
parameters.
Scaling laws in DL predict loss based on model size and dataset size.
In deep learning,
power-law relationships exist between the model’s performance and other variables. These scaling
laws can forecast the performance of a model given different values for its parameters, dataset, and
amount of computational resources. For instance, we can estimate a model’s loss if we were to
double its parameter count or halve the training dataset size. Scaling laws show that it is possible to
accurately predict the loss of an ML system using just two primary variables:
1. N: The size of the model, measured in the number of parameters. Parameters are the weights
in a model that are adjusted during training. The number of parameters in a model is a rough
measure of its capacity, or how much it can learn from a dataset.
2. D: The size of the dataset the model is trained on, measured in tokens, pixels, or other
fundamental units. The modality of these tokens depends on the model’s task. For example,

106
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
tokens are subunits of language in natural language processing and images in computer vision.
Some models are trained on datasets consisting of tokens of multiple modalities.
Improving model performance is typically bottlenecked by one of these variables.
The computational resources used to train a model are vital for scaling.
This factor,
called compute, is most often measured by the number of calculations performed over a certain
time. The key metric for compute is FLOP/s, the number of floating-point operations the computer
performs per second. Practically, increasing compute means training with more processors, more
powerful processors, or for a longer time. Models are often allocated a set budget for computation:
scaling laws can determine the ideal model and dataset size given that budget.
Computing power underlies both model size and dataset size.
More computing power
enables larger models with more parameters and facilitates the collection and processing of more
tokens of training data. Essentially, greater computational resources facilitate the development of
more sophisticated AI models trained on expanded datasets. Therefore, scaling is contingent on
increasing computation.
The Chinchilla Scaling Law: an Influential Example
The Chinchilla scaling law emphasizes data over model size [39].
One significant research
finding that shows the importance of scaling laws was the successful training of the LLM “Chinchilla.”
A small model with only 70 billion parameters, Chinchilla outperformed much larger models because
it was trained on far more tokens than pre-existing models. This led to the Chinchilla scaling law,
which provides a scaling law that depends on parameter count and data. This law demonstrated
that larger models require much more data than was standard.

2.4. SCALING LAWS
107
Figure 2.24: Chinchilla’s loss is much lower (indicated by its lighter color) than other models of
similar size. It used much more data than other models. [40]
The Chinchilla scaling law equation encapsulates these relationships.
The Chinchilla
scaling law is estimated to be
L(N, D) = 406.4N −0.34 + 410.7D−0.28 +
1.69
|{z}
Irreducible Error
(2.1)
In equation 2.1, N represents parameter count, D represents dataset size, and L stands for loss. This
equation describes a power-law relationship. Understanding this law can help us understand the
interplay between these factors, and knowing these values helps developers make optimal decisions
about investments in increasing model and dataset size.
Scaling laws for DL hold across many modalities and orders of magnitude.
An order of
magnitude is a factor of 10x—if something increases by an order of magnitude, it increases by 10
times. In deep learning, evidence suggests that scaling laws hold across many orders of magnitude of
parameter count and dataset size. This implies that the same scaling relationships are still valid for
both a small model trained on hundreds of tokens or a massive model trained on trillions of tokens.
Scaling laws have continued to hold even as model size increases dramatically.

108
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
Figure 2.25: The scaling laws for different DL models look remarkably similar. [41]
Discussion
Scaling laws are not universal for ML models. Not all models follow scaling laws. These
relationships are stronger for some types of models than others. Generative models such as large
language models tend to follow regular scaling laws—as model size and training data increase in
scale, performance improves smoothly and predictably in a relationship described by a power-law
equation. But for discriminative models such as image classifiers, clear scaling laws currently do not
emerge. Performance may plateau even as dataset size or model size increase.
Better learning algorithms can boost model performance across the board.
An improved
algorithm increases the constant term in the scaling law, allowing models to perform better with a
given number of tokens or parameters. However, crafting better learning algorithms is quite difficult.
Therefore, improving DL models generally focuses on increasing the core variables for scaling: tokens
and parameters.
The bitter lesson: scaling beats intricate, expert-designed systems.
Hard-coding AI
systems to follow pre-defined processes using expert insights has proven slower and more failure-prone
than building large models that learn from large datasets. The following observation is Richard
Sutton’s “bitter lesson” [42]:
1. AI researchers have often tried to build knowledge into systems,
2. “This always helps in the short term [...], but in the long run it plateaus and it even inhibits
further progress,
3. Breakthrough progress eventually arrives by an opposing approach based on scaling computation
by search and learning.”
This suggests that it is easier to create machines that can learn than to have humans manually encode
them with knowledge. For now, the most effective way to do this seems to be scaling up deep learning
models such as LLMs. This lesson is “bitter” because it shows that simpler scaling approaches tend
to beat more elegant and complex techniques designed by human researchers—demoralizing for

2.5. REINFORCEMENT LEARNING
109
researchers who spent years developing those complex approaches. Rather than just human ingenuity,
scale and computational power are also key factors that drive progress in AI.
Conclusion
In AI, scaling laws describe how loss changes with model and dataset size. We observed
that the performance of a DL model scales according to the number of parameters and tokens—both
shaped by the amount of compute used. Evidence from generative models like LLMs indicates a
smooth reduction in loss with increases in model size and training data, adhering to a clear scaling
law. Scaling laws are especially important for understanding how changes in variables like the amount
of data used can have substantial impacts on the model’s performance.
2.5
Reinforcement Learning
Introduction
This section provides a high-level overview of reinforcement learning (RL). We introduce the basic
structure and vocabulary of RL problems and explore how RL is used.
Reinforcement learning techniques attempt to automate the capacity for an agent to learn from its
actions and their consequences in an environment [43, 44]. This is distinct from other ML problems,
where a system can learn from an existing dataset. Instead, an RL system (or agent) learns the hard
way, collecting data through experience. An RL agent must learn how to explore different possible
actions to attain as much reward as possible. Reward measures the agent’s progress towards its goal
and acts as feedback in the learning process.
Reinforcement learning takes its approach from how animals and humans learn to interact with the
world. If someone puts their hand on a hot stove, the resulting pain is a significant negative reward,
making them unlikely to repeat the same behavior. As intelligent agents, we learn through the way
the world responds to our actions. We mostly try to pursue good outcomes, defined by their positive
rewards, and avoid bad outcomes, which lead to negative rewards. In this way, teaching automated
systems through RL is similar to training a dog. When a dog performs the desired behavior, such as
rolling over on command, its trainers might reward it with a treat. The trainers repeat this process
as the dog gradually refines its behavior to perform as they desire.
RL techniques have been involved in recent successes in AI. In 2017, DeepMind’s AlphaGo became
the first computer program to defeat a world champion in Go, a game long considered the most
challenging classical game for AI [13]. In 2020, an RL agent developed in less than a year by a small
Maryland company won a clear victory over an Air Force pilot in simulated one-on-one combat. RL
techniques have been most successful in smaller-scale and less realistic environments, such as video
games, where a very accurate simulated environment is readily available. However, games can also
be designed to represent real-world environments.
2.5.1
Learning Sequential Decision Making through Trial and Error
Trial and error.
All RL problems involve an agent learning by trial and error to make sequences
of decisions. RL agents usually start without prior knowledge of the environment. Over time, the
agent accumulates information about its environment and uses it to make better decisions. An agent
might learn that bumping into walls constantly does not lead to good outcomes and avoid the walls
to find a viable path.

110
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
Sequential decision making.
Making sequential decisions effectively is essential to achieving
complicated, long-term goals. Accomplishing such goals requires not only that prudent actions are
taken but also that they are taken in an appropriate order. To an agent, it is often unclear how one
action may impact later actions, but what it does will affect the future and must be modeled. Since
decisions are sequential in RL tasks, RL agents might be better suited to achieving complicated,
long-term goals.
RL agents can also learn to navigate uncertainty. An agent may not know if a particular choice is good
or bad, and the world may change (on its own or in response to its choices), but the agent must act
regardless. A self-driving car navigating a busy intersection must sequence its actions–—accelerate,
brake, or turn–—based on the positions and speeds of other vehicles, and its current actions will
affect these other parts of the environment. To tackle these difficulties, some RL agents are designed
to learn from their interactions to develop a sophisticated understanding of their environments to
plan their actions.
Sequence modeling.
Sequential decision making (SDM) is a subtype of the general sequence
modeling task, which involves analyzing and predicting patterns in ordered data. In RL, this data is
predominantly in the form of observations of the environment over time. The RL system learns to
make decisions based on this information, considering its knowledge of the environment, uncertainty
about the world, and past choices.
Distinguishing RL.
Reinforcement learning differs from supervised and unsupervised learning. A
key difference between these paradigms is how the ML system obtains information. Supervised and
unsupervised learning tasks learn efficient and useful representations on fixed, precompiled datasets.
Supervised learning goes about this directly: learning by example. Unsupervised learning addresses
similar problems with less guidance, uncovering the latent patterns in unlabeled data without being
explicitly told whether it is right or wrong. It is akin to solving a puzzle without seeing the picture
on the box. By contrast, RL systems must acquire new information through actions. They receive
almost no data at the outset and learn to achieve a goal by actively engaging with an environment
and seeing what happens.
2.5.2
The Reinforcement Learning Problem
We can use two toy examples to introduce the central concepts and terms of reinforcement learning.
First, we consider the multi-armed bandit—a classic reinforcement learning problem—starting with
a simple situation and gradually adding complications to build up to a full reinforcement learning
problem. Next, we use a Gridworld problem to introduce the agent-environment loop, a framework
for understanding reinforcement learning problems. Finally, we discuss policies: the decisions that
RL agents make.
Multi-Armed Bandits
In casino lingo, “one-armed bandits” refers to slot machines, which have a single lever that takes
people’s money. Playing a slot machine is not a reinforcement learning problem because there is only
one possible action with a random outcome. However, suppose there is a slot machine with two or
more levers. Then, a gambler must decide which lever to pull every time they play. Assuming the
levers affect the device differently, one lever might win them more over the long run (such as over
500 pulls). How does one decide which levers to pull and in what order? This problem is called the
multi-armed bandit problem.

2.5. REINFORCEMENT LEARNING
111
Exploration-exploitation trade-off.
Multi-armed bandit problems exemplify a dilemma at the
heart of reinforcement learning: the trade-off between exploration and exploitation. RL agents must
choose between exploring actions with high uncertainty and exploiting actions with a high expected
reward. In a multi-armed bandit problem, the agent starts without knowing anything about the
expected reward of any given lever. Therefore, the first thing it has to do is gain information about
the levers. Trying different actions with uncertain rewards is called exploration. By contrast, taking
actions with a high expected value is called exploitation. In this phase, the agent chooses the actions
it expects to yield the highest rewards based on its current understanding of the environment.
Balancing this trade-off between exploration and exploitation is crucial to the success of an RL
agent. If the agent explores too much, it might waste resources on low payoff actions or even stumble
into actions with negative payoffs. Conversely, if the agent exploits too early, it might settle for
suboptimal actions, missing out on potential higher rewards. The challenge lies in determining when
and how to shift between exploration and exploitation to maximize the overall benefit. A series of
possible strategies aim to make this trade-off, with varying degrees of sophistication; in the following,
we outline two simple examples.
Greedy strategies.
First, the agent pulls each lever once and records the payoff it receives from
each pull. Then, it pulls whichever lever initially resulted in the highest payoff, which it continues to
pull as many times as possible. This is a greedy strategy because the agent only tries each lever once
and exploits from then on. However, the problem with a greedy strategy is that the agent cannot be
sure that the lever it settles on actually has the highest expected utility.
Epsilon-greedy strategies.
After pulling each lever once, an agent might decide to keep exploring
as well. One thing it could so is pull the lever it currently thinks has the highest expected payoff
95% of the time and pull a random lever the other 5% of the time. This is called an epsilon-greedy
strategy, where “epsilon” refers to the small probability of pulling a random lever. Epsilon-greedy
is a much better strategy than greedy if there are enough pulls ahead of the agent, since the agent
will figure out which lever is best to pull over time. After enough pulls, the agent will pull the most
profitable lever 95% of the time.
Adding context to the bandit problem.
So far, the best lever to pull is always the same and the
agent just has to figure out which one it is. However, suppose each lever has two payoff distributions:
one for when a light on the slot machine is green, and one for when the light is red. The best lever to
pull now depends on the context (the color of the light), which varies randomly. In this environment,
the agent needs to take the context into account when it chooses a lever to pull. This is called a
contextual multi-armed bandit problem.
Adding interaction to the bandit problem.
Suppose further that the light on the slot machine
doesn’t vary randomly, but instead changes according to which lever the agent pulls. For example, it
might turn green after the second lever is pulled or red if once both levers have been pulled twice. It
might be better for the agent to pull a lever with a lower expected payoff at the moment, but will
change the context such that the agent can access a greater total profit in the long run. Therefore,
the agent needs to be able to estimate expected payoffs over single actions and sequences of actions.
The Full Reinforcement Learning Problem
The problem we face now is to come up with a strategy that tells the agent (1) which lever to pull,
(2) in varying contexts, and (3) taking into account that its actions can affect that context. The
bandit problem is now a full reinforcement learning problem.

112
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
Figure 2.26: We can increase RL problems in complexity. Adding detail to multi-armed bandits
yields contextual bandits. Adding more complications yield “full” RL problems. [45]
The strategies we considered above—greedy and epsilon-greedy–—aren’t suitable anymore.
To
develop better strategies, we’ll need more terms and concepts. In particular, we’ll introduce the
agent-environment loop as a more technical framework for RL. We’ll also replace our example: instead
of the multi-armed bandit, we’ll use a problem called Gridworld.
Gridworld.
In Gridworld, an agent learns to find a particular cell (the goal) in its environment (a
two-dimensional grid) while avoiding other cells (hazards). The agent can move into any adjacent
open cell. Moving into an open cell is neutral, moving into a hazard is very bad, and moving into the
goal is very good.

2.5. REINFORCEMENT LEARNING
113
Figure 2.27: Gridworld is a simple environment used as a toy model in reinforcement learning.
The agent-environment loop.
The relationship between an RL agent and its environment can be
described by the agent-environment loop. At each time step in this loop, the agent takes some action
which affects the environment. Then, the agent receives a reward signal and a state observation from
the environment. Using Gridworld as our example, we discuss each of those terms below.
Figure 2.28: The agent-environment loop shows how the agent chooses an action to affect the
environment, which influences the received reward. [46]
Environment and state
The environment and state are the world.
In Gridworld, the agent is located on a particular
cell and can move to adjacent cells. The environment includes the grid itself, its boundaries, the
location of the agent, and anything the agent may encounter. When the environment changes
(such as when the Gridworld agent moves), we say that its state changes. A state observation is

114
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
the information (other than reward) an agent receives from the environment. In Gridworld, an
observation might be the agent’s position as well as the positions of the goal, hazards, and barriers.
At each time step, the state of the environment updates, and the agent takes a snapshot observation
of the available information in the environment.
Environments can be fully or partially observable.
When an agent can observe all of the
information in its environment, we say that the environment is fully observable. In contrast, when an
agent can only observe some of the information about the state of the environment, we say that the
environment is partially observable. The environment in chess is fully observable because all of the
information about the state of the game is available to both players. By contrast, the environment of
a poker game is only partially observable: players do not know other players’ cards. Our example,
Gridworld, is fully observable.
Actions and action spaces
Agents choose actions from their action space.
Agents affect their environment by taking
actions. At each time step, they choose an action from the set of all actions available to them. This
set is called the action space. In Gridworld, the agent takes an action from the action space that
describes each possible move to an unoccupied adjacent cell. The agent cannot move into a space
occupied by a barrier or outside of the dimensions of the grid.
Action spaces can be continuous or discrete.
A discrete action space describes a finite number
of possible actions. The action space of our Gridworld agent is discrete—at most, consisting of up,
down, left, and right. The same is true for the action space of a multi-armed bandit agent, which
consists of a finite number of levers available to pull. In contrast, a continuous action space, such
as possible directions of movement in a real three-dimensional space, has infinitely many possible
actions.
Rewards
Agents get rewards.
At each time step, agents receive a reward from the environment. This
reward is represented by a number, which can be positive, negative, or zero. (This means that, in
contrast to the everyday use of the word “reward,” a reward can be a bad outcome.) The agent in
Gridworld might receive a reward of 10 for finding the goal, −10 for encountering a hazard, and 0
otherwise. In this case, encountering a danger cancels out the reward from achieving the goal, so
hazards should always be avoided. However, exploring neutral squares is free, so this can be done as
much as possible.
Rewards are often discounted.
In general, an RL agent’s goal is to maximize the sum of rewards
it receives over the long term. Sometimes, though, we want an agent to value short-term rewards
more than long-term rewards, such as encouraging agents to behave in a more risk-averse way or
to account for their uncertainty. In these cases, we multiply future rewards by a discount factor
(γ), and the agent maximizes the discounted long-term reward. If the reward at time t is Rt, the
discounted long-term reward is: Rt + γ1Rt+1 + γ2Rt+2 + ... + γnRt+n.
Rewards are designed to suit the problem.
The process of creating the right rewards for
an RL problem is called reward shaping. In Gridworld, we might want the agent to find the goal
more quickly rather than take a long, roundabout path. To encourage this, we could use rewards to
incentivize speed or, equivalently, penalize slowness. If the default reward is 0 when the agent does
not encounter the goal or a hazard, reaching the goal in a few time steps would be rewarded the

2.5. REINFORCEMENT LEARNING
115
same as reaching the goal in 1,000 time steps. However, if we change this reward to −1, the agent
has an incentive to reach the goal more quickly.
Policies
An RL agent’s policy (often denoted π) gives an action for each possible state observation. In simple
environments, one can think of a policy as a two-column matrix, where the first column lists all
possible state observations, and the second column lists each corresponding action. The strategies
we described in the context of the multi-armed bandit problem—greedy and epsilon-greedy—are
examples of policies an agent could use when the environment has a single state. In the contextual
multi-armed bandit problem, we added a second state. A policy over two states might look like: “If
green, pull lever X; if red, pull lever Y.”
Policies can be deterministic or stochastic.
A deterministic policy assumes that all actions
are taken with certainty. In the bandit problem, the greedy strategy is a deterministic policy. In
Gridworld, a deterministic policy may be to always move to the right or to follow a particular path
to the goal. In contrast, a stochastic policy assumes that actions are taken with some randomness,
such as randomizing between pulling different levers when following an epsilon-greedy strategy. In
Gridworld, a stochastic policy might be to choose randomly among actions for the first few time
steps.
Optimal policies.
An optimal policy is a policy that, if followed, maximizes the sum of (discounted)
rewards. In Gridworld, the optimal policy would describe the shortest route to the goal from any
starting location.
In more complex problems, however, computing the optimal policy is often
intractable. For example, in chess, an optimal policy would need to give the best move in all ∼1045
possible board positions. However, approximating the optimal policy has still produced RL chess
agents that far surpass the human level of play.
Summary
This section introduced the agent-environment loop as a technical framework for talking about
reinforcement learning problems. In place of the “strategies” we considered in the multi-armed bandit
problem, we introduced the idea of a policy, which gives an action at each time step according to the
agent’s observation of its environment. In Gridworld, we could manually define a policy for the agent
to follow–—but that would defeat the purpose of the problem, which is to have the agent learn for
itself. The next section explores how reinforcement learning algorithms automate designing policies.
2.5.3
Methods for Solving RL Problems
RL algorithms develop better policies.
RL algorithms serve as the blueprints to guide
the learning process, aiming to discover the optimal strategy for the agent in its environment.
While all RL algorithms share a common theme of learning through trial and error—exploring the
environment, observing the results, and adjusting behavior based on rewards and penalties—their
unique characteristics arise from the specific approach they take toward learning. Specific RL
algorithms are distinguished by how or what their agent learns.
Model-based vs Model-free RL.
We can organize different RL algorithms into two broad
approaches based on their computational learning strategies: model-based and model-free. In model-
based RL algorithms, the agent constructs an internal model of its environment. The agent learns
how its world operates and represents cause-and-effect relationships, using this model to predict the
outcomes of actions and optimize its behavior. An RL agent playing chess could develop a model of

116
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
the game, representing the rules and understanding that specific configurations win the game. It
can use this model to plan its moves, simulating different sequences to anticipate their results before
making a move. Model-based methods are most applicable to problems in environments with easily
representable dynamics.
In contrast, model-free RL methods learn directly from experience without explicitly modeling the
underlying mechanics of the environment. Instead, the agent develops associations between actions
and rewards. These simple associations serve as a kind of “intuition” for projecting what actions
lead to the most reward in given situations. Consider an RL agent learning to play Pac-Man: it
does not comprehend the game’s design or the ghosts’ behavior. Instead, through trial and error,
it learns strategies like “eating a power pellet and then chasing ghosts leads to high scores.” The
AI uses these associations, rather than an internal model of the game, to guide its future actions.
Model-free approaches are often best for problems where the environment’s dynamics are complex or
unknown and abundant data is available for learning.
Policy-based vs Value-based RL.
Another distinction is how an RL agent represents what is
“good” in its environment, learning how good states or actions are, how good policies are, or both
at the same time. In policy-based RL methods, the agent focuses on learning an effective policy: a
set of rules guiding its actions in response to different states. Rather than calculating the value of
each state or action, the agent optimizes the policy based on the observed rewards of its actions.
The agent may pursue an action because it is part of a good policy without knowing exactly how
good the action itself is. An RL agent learning how to balance a pole on a cart for a robotics task
does not explicitly calculate the value of different pole angles or cart positions. Instead, it adjusts
its policy—how it responds to various states—to optimize for the goal of keeping the pole balanced.
It might learn the policy “if the pole tilts to the right, move the cart to the right.” This approach
is often best for handling continuous action spaces but struggles with finding the optimal policy in
large or complex environments due to the vast number of potential policies.
Conversely, value-based RL methods prioritize learning the value of different states or actions within
the environment. The agent evaluates and ranks states or actions based on their potential to yield
rewards, selecting the ones with the highest estimated reward. An RL agent playing tic-tac-toe
evaluates each move’s value (putting a mark in an empty cell) based on the likelihood of that move
leading to a win and chooses the action with the best outcome. The agent learns to take actions that
maximize the cumulative reward over time without specifying a policy.
An agent’s value function estimates the expected reward of a state or state-action pair.
Internally, an agent might estimate how good or bad certain states are. The agent learns a value
function to keep track of these estimates, which it can use to estimate the returns of different actions.
Value functions can also estimate how well a policy performs in the long run by calculating the
expected cumulative reward of following a policy over time. An agent in Gridworld could learn a
value function to estimate the reward of moving into a particular space. Alternatively, an RL agent
in a car’s GPS might use a value function to estimate the value of different routes based on the
expected travel time.

2.5. REINFORCEMENT LEARNING
117
Figure 2.29: Value-based methods can either evaluate the value of a state or the value of a state-action
pair. [47]
Value functions can focus on the value of a state or an action.
There are two main types
of value functions in reinforcement learning: state-value functions and action-value functions. A
state-value function (V π(s)) estimates the expected cumulative reward that an agent will receive
when starting from a state s and following a policy π. We can estimate the state values of each
location in Gridworld, assuming the agent follows a random policy. In every area, the agent moves to
a random adjacent open cell, repeating this process until it bumps into the goal or a hazard. The
state-values of cells near the goal are higher than those far away from the goal and much higher than
those near the hazard. The state-value function for this policy gives us an estimate of how likely it is
that the agent will reach the goal or hit a hazard from each cell.
An action-value function (Qπ(s, a)) estimates the expected cumulative reward that an agent will
receive by taking an action a from a state s and following a policy π. In other words, it estimates
the “value” of taking a particular action in a particular state. Action-values are often referred to as
Q-values, and the value functions that output them are called Q-functions. One way to approach an
optimal policy in RL is by learning the Q-function.
Q-learning.
Q-learning is one of the oldest and most popular reinforcement learning algorithms.
Q-learning approaches the RL problem by estimating how good it is to take actions in each state.
Chess has numerous potential moves (actions) for each position on the board (state). Q-learning
would assign a score (Q-value) to each state-action pair, indicating how beneficial each move is likely
to be. The goal is to determine the optimal Q-function—the rule that most accurately assigns these
scores—by mapping every state-action pair to its optimal Q-value. When the environment has a
finite number of states and moves, the Q-function can be represented as a table, where each row
represents a state, and each column is an action.
Therefore, Q-learning is the process of updating the Q-values of each action iteratively based on
the outcomes observed in the environment. At each time step, the agent chooses the action with
the highest associated Q-value from those available to it in its current state. This generates a new
observation and reward, from which the agent updates its estimates. Each table entry will converge
to its optimal value through many iterations.

118
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
Q-learning in Gridworld.
In Gridworld, we have four actions (North, South, East, West) and 25
states (unique grid cells). Therefore, the Q-function table would have 100 Q-values, one for every
state-action pair:
North
South
East
West
(0,0)
Q((0,0), North)
...
...
Q((0,0), West)
(0,1)
...
...
...
...
...
(4,5)
...
...
(5,5)
Q((5,5), North)
...
...
Q((5,5), West)
Table 2.1: Example Q-value table for Gridworld.
In the beginning, all the Q-value estimates are zero because the agent has no information about
the environment and will take random actions. Eventually, the agent reaches a positive or negative
reward state. Say, for example, our agent gets lucky and finds its way to cell (0,4) and chooses to go
east. It will receive a reward of +1, and its estimate of the Q-value for ((0,4), East) will be increased
accordingly. The agent learns that moving East is a good action to take in the state (0,4), and it will
be more likely to take this action the next time it is in the same state.
Over time, the negative and positive Q-values will propagate through the table. The agent will repeat
this process many times as it explores the states in the environment and tries different actions, and if
all goes well, its estimates will converge to the optimal Q-function. By learning which actions are
desirable to take in which states, the agent learns how to navigate the world successfully. As the
environment becomes more complicated and action space becomes larger, computing exact Q-values
becomes less computationally feasible. Instead, we might try to estimate the Q-function using a less
computationally taxing method. One such method involves using neural networks to estimate the
Q-function. This is an example of a method known as deep RL.

2.5. REINFORCEMENT LEARNING
119
Figure 2.30: Q-values can be estimated for every possible state in Gridworld. [48]
2.5.4
Deep Reinforcement Learning
Deep reinforcement learning (deep RL) is the area of machine learning that leverages deep learning
(DL) to help solve reinforcement learning (RL) problems. Deep learning, which uses neural networks
to compress and extract information from vast amounts of data, can be applied to RL problems
such as sequential decision making. In this setting, the RL setting provides signals to help neural
networks learn useful representations for the task at hand.
Deep learning is a substantial part of nearly all major RL successes in recent years.
As
the complexity of modeling even seemingly simple problems can explode at astonishing rates, using
RL to accomplish interesting tasks demands powerful, scalable methods. Some of the ways deep
learning can be used in RL include:
• Environment. In model-based RL, deep learning can be used to learn the dynamics model of
the environment, which can then be used for planning and decision-making.
• Policy function. Deep learning can help optimize complex policies for continuous action
spaces.
• Value function. Deep neural networks such as Deep Q-Network (DQN) can be used to
estimate the Q-function. This is particularly useful for problems with many, or even infinite,
possible state-action pairs. Instead of capturing the Q-function in a state-action matrix, it can
be directly encoded in a neural network, which takes the state-action pair as an input and
outputs the action-value.
In the next section, we explore a few recent successes of deep RL. Along the way, we’ll introduce a
few more high-level techniques used to solve common problems to conclude our discussion of RL.

120
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
2.5.5
Examples of RL
AlphaGo
In 1997, IBM’s Deep Blue made history by becoming the first AI to defeat a world-class chess
player. Yet it was not until nearly two decades later that an AI managed to surpass humans in
the game of Go, when AlphaGo won against the best player in the world in 2017. Go is similar
to structure to chess, both being fully observable two-player turn-based games, but Go is far more
complicated. Unlike chess, with only 20 possible first moves, Go starts with a staggering 361 choices.
This complexity outstrips the capabilities of the brute-force computations that Deep Blue could rely
on to evaluate positions. Therefore, AlphaGo had to rely on the more sophisticated methods of deep
RL to master the game. By playing millions of games against itself, AlphaGo honed its strategy and
emerged as the world’s top Go player.
RL agents can learn by competing against themselves.
One training method that has proven
effective is self-play. In part, this is because an RL model presents itself with an appropriate opponent
for an amateur: not a novice, who it could win against easily without learning anything new, or a
grandmaster, who would win against it decisively enough that it might not understand how it could
have acted differently to prevent a loss. Instead, the best way for an amateur to improve is to play
against opponents of similar skill levels. The insight of self-play is that a RL agent can learn from
players of a similar strength by playing against versions of itself. This allows the agent to improve
and reach a superhuman level of play without external intervention.
Atari
The Atari games are a collection of video games developed between the 1970s and 1980s for the
Atari 2600 console, characterized by their simple graphics, straightforward controls, and challenging
gameplay. Some of the most popular Atari games are Space Invaders, Pac-Man, and Pong. Each
game has its own unique set of rules, goals, and strategies. As they provide a diverse set of challenges
that an RL agent must learn to overcome, Atari games have long served as a popular testing ground
to assess the capabilities of RL algorithms. They provide a platform to evaluate how well RL agents
can navigate complex environments and master motor skills. In 2013, the DeepMind team proposed
the original set of 57 Atari titles as an RL benchmark. Since then, RL agents have met or surpassed
human performance in every single one of these games [49]. However, some games took years longer
to solve—and inspired new learning methods.
Sparse rewards present challenges to learning.
One particularly difficult game for RL agents
to solve is Montezuma’s Revenge. In this game, players must make a long series of precise movements
to reach the first reward. They must navigate through a series of mazes to collect treasure, including
a key. The key is necessary to advance to the next level. The challenge lies in the fact that an RL
agent that moves randomly at first is unlikely to ever encounter this first reward, let alone complete a
level. These rewards are rare and hard to find. It also requires a delicate balance between exploration
and exploitation, as the key is often hidden in parts of the maze the agent would not visit unless it is
exploring, but exploring too much can lead to running into traps or enemies. To reach any reward at
all, the player must perform a complex sequence of actions where a single mistake can lead to losing
a life. Learning this game through trial and error can take a long time.
Montezuma’s Revenge is a classic example of the problem of sparse rewards, where the agent receives
few signals to guide learning. In environments like this, rewards are few and far between, depriving
RL agents of the feedback they need to learn effective strategies. In contrast, a game like Breakout
(in which a paddle at the bottom of the environment is used to deflect a ball towards bricks to break

2.5. REINFORCEMENT LEARNING
121
them) is much easier for RL agents. Even random movements are likely to deflect the ball, break a
block, and earn a reward reasonably quickly.
Preferences for novelty can help overcome sparse rewards.
An RL agent which prefers
novelty is rewarded for new actions and observations. This method is how an RL agent eventually
solved Montezuma’s Revenge: it learned to find the first reward simply by seeking out new observations.
Imagine an RL agent rewarded for novelty finds itself in an empty room with a single closed door.
After aimlessly wandering around the room for a while, the RL agent will eventually “get bored”
and figure out how to open the door, venturing out of the room. In this way, novelty-seeking agents
can exhibit intelligent behavior without being explicitly rewarded for any particular goal. There are
many similar ways to develop RL agents and promote optimal behavior.
The exploration of Atari games and the development of novel methods to tackle them have significantly
advanced the field of reinforcement learning, providing valuable insights into the learning process of
RL agents.
StarCraft
In 2019, DeepMind’s RL system AlphaStar defeated an expert human player 5-0 in a StarCraft II
match [50]. StarCraft II is a complicated real-time strategy game that pits two players against each
other in a vast battlefield. StarCraft has many more possible actions and states than chess, Go, or
Atari games, making it a formidable challenge for players and AIs alike.
AlphaStar learned to overcome significant uncertainty.
The game’s difficulty is further
amplified by its partially observable environment, as most of the map is shrouded in a fog of war,
and players can only see areas where they have deployed units. This means players must constantly
act under uncertainty, making educated guesses about the state of the environment and making
predictions about their opponent’s actions and strategies. AlphaStar’s ability to overcome the
challenges of uncertainty, a large action space, and the need for strategic planning underscores the
potential of RL in navigating complicated, uncertain environments.
Diplomacy
In 2022, Facebook AI Research (FAIR) developed Cicero, a program that reached human-level
performance in the strategy game Diplomacy [51]. In this game, players compete to take over a model
of Europe against the backdrop of the First World War. One of the central features of Diplomacy is
that players communicate in natural language to form alliances and negotiate in between each turn.
Cicero navigated not only the game’s moves but also natural language processing and generation.
Cicero learned how to deceive.
Cicero’s success required learning how to deceive human players.
It is documented to have sometimes convinced other players to agree to lopsided deals or break
promises when it was advantageous. In other words, Cicero developed a capacity for deception. This
result is not surprising: success in Diplomacy often requires deception.
Conclusion
This section explores the major concepts, terms, and techniques involved in reinforcement learning.
First, we used two simple examples—Multi-Armed Bandits and Gridworld—to outline the major
concepts and terms used in RL. At each time step, the agent takes some action drawn from the
action space, which affects the state of the environment. Then, the agent receives a reward signal
and a state observation from the environment. The agent acts according to its policy, which specifies

122
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
an action for every possible state observation. The agent’s value function computes the expected
cumulative reward of a certain state (state-value) or state-action pair (Q-value) given a certain policy.
Q-values can be used for Q-learning, one popular learning algorithm, to update an agent’s policy
toward an optimal policy.
After outlining the major concepts and terms used in RL, we discussed some powerful techniques
used in groundbreaking RL systems. First, deep RL uses deep learning to estimate value functions
and policies in tasks that are too complex to allow exact computation. Second, self-play expands the
ability of RL systems in competitive tasks by repeatedly pitting the system against itself. Finally,
RL systems can overcome the problem of sparse rewards through preferences for novelty.
2.6
Conclusion
Understanding the technical underpinnings of AI systems—the underlying models and algorithms,
how they work, how they are used, and how they are evaluated—is essential to understanding the
safety, ethics, and societal impact of these technologies. This foundation equips us with the necessary
grounding and context to identify and critically analyze their capabilities and potential, as well as
the risks that they pose. It allows us to discern potential pitfalls in their design, implementation,
and deployment and devise strategies to ensure their safe, ethical, and beneficial use.
2.6.1
Summary
In this chapter, we presented the fundamentals of artificial intelligence (AI) and its subfield, machine
learning (ML), which aims to create systems that can learn without being explicitly instructed.
We examined its foundational principles, methodologies, and evolution, detailing key techniques,
concepts, and practical applications.
Artificial intelligence.
We first discussed AI, the vast umbrella that encapsulates the idea of
machines performing tasks typically associated with human intelligence. AI and its conceptual origins
date back to the 1940s and 1950s when the project of creating “intelligent machines” came to the
fore. The field experienced periods of flux over the following decades, waxing and waning until the
modern deep learning era was ushered in by the groundbreaking release of AlexNet in 2012, driven
by increased data availability and advances in hardware.
Defining AI.
The term “artificial intelligence” has many meanings, and the capabilities of AI
systems exist on a continuum. Five widely used conceptual categories to distinguish between different
types of AI are narrow AI, artificial general intelligence (AGI), human-level AI (HLAI), transformative
AI (TAI), and artificial superintelligence (ASI). While these concepts provide a basis for thinking
about the intelligence and generality of AI systems, they are not well-defined or complete, often
overlapping and used in different, conflicting ways. Therefore, in evaluating risk, it is essential to
consider AI systems based on their specific capabilities instead of broad categorizations.
The ML process.
We presented a general framework for understanding ML models by considering
five aspects of a model: its task, input data, output, and what type of machine learning it uses. We
then discussed each of these aspects in turn. We explored common tasks for ML models, including
classification, regression, anomaly detection, and sequence modeling. We highlighted a few of the
many types of data that these models work with and discussed the model development process.
Creating an ML model is a multi-step process that typically includes data collection, model selection,
training, evaluation, and deployment. Measuring the performance of a model in evaluation is a

2.6. CONCLUSION
123
critical step in the development process. We surveyed several metrics used to achieve this, as well as
the broader, often conflicting goals that guide this process.
Types of ML.
We discussed different approaches to machine learning and how these categories are
neither well-defined nor complete, even though distinctions are often drawn between different “types”
of machine learning. We surveyed four common approaches to ML: supervised learning, unsupervised
learning, reinforcement learning, and deep learning. At a high level, supervised learning is learning
from labeled data, unsupervised learning is learning from unlabeled data, and reinforcement learning is
learning from agent-gathered data. Deep learning techniques are used in all three settings, leveraging
deep neural networks to achieve remarkable results.
Deep learning.
We then examined deep learning in more depth. We saw how, beyond its use
of multi-layer neural networks, deep learning is characterized by its ability to learn hierarchical
representations that provide deep learning models with great flexibility and power. Machine learning
models are functions that capture relationships between inputs and outputs with representations
that allow them to capture an especially broad family of relationships.
Components of DL models.
We explored the essential components of deep learning models and
neural networks. Through the example of multi-layer perceptrons (MLPs), we broke down neural
networks, structures composed of layers of neurons, into an input layer, an output layer, one or more
hidden layers, weights, biases, and activation functions. We highlighted a few significant activation
functions and examined other fundamental building blocks of deep learning models, including residual
connections, convolution, and self-attention. We also presented influential architectures, such as
CNNs and Transformers.
Processes in DL models.
We discussed how deep learning models learn and are used in training
and inference. We walked through the steps to training a deep learning model, beginning with
initialization and then cycling through sending information forward to make a prediction, measuring
its error or quality, sending this error backward, and adjusting parameters accordingly until a stopping
criterion is reached. We discussed training techniques such as pre-training, fine-tuning, few-shot
learning, and zero-shot learning, and how training typically involves a combination of many methods
and techniques used in conjunction. We considered the importance of scalability, computational
efficiency, and interpretability in evaluating deep learning models and their suitability for deployment.
We plotted the course of technical and architectural development in the field, from LeNet in 1989
to BERT and GPT models in 2018. We considered real-world applications of deep learning in
communication and entertainment, transportation and logistics, and healthcare.
Scaling laws.
Scaling laws describe mathematical relationships between model performance and
key factors like model size and dataset size in deep learning. These power law equations show that as
models grow in parameters and are trained on more data, their loss tends to decrease substantially
and predictably. Scaling up computational resources used to train a model can enable an increase in
both model parameters and the amount of data used in training. Researchers can leverage scaling
laws to determine optimal model and dataset sizes given computational constraints. Scaling laws
hold across many modalities and orders of magnitude, though they do not necessarily apply to all
deep learning models, such as many classification models.
Reinforcement learning.
Finally, we turned our attention to reinforcement learning and explored
the field’s basic concepts, terminology, and techniques. We examined RL problems, which provide
the general framework through which reinforcement learning attempts to automate the capacity of
an agent to learn from its actions and their consequences in an environment. In these problems,

124
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
agents learn to make decisions through trial and error. Through the toy examples of multi-armed
bandits and Gridworld, we introduced the major concepts and terms used in reinforcement learning.
Lastly, we saw a few examples of groundbreaking applications of RL in gaming, including AlphaGo,
AlphaStar, and Cicero.
2.6.2
Key Takeaways
AI created fundamental changes, offering benefits and presenting risks.
AI is increasingly
being integrated into business, education, healthcare, communication, and other aspects of our lives.
As its influence grows, so too does its potential for harm. AI systems and their use, whether to
automate intricate processes in manufacturing or to personalize experiences in retail and entertainment,
have direct, real-world impacts. Moreover, AI introduces unique risks, as it involves the creation
of systems that have the ability to learn and pursue objectives. While highly advanced or general
AI systems often garner the most attention, AI can pose risks at any level of capability. Even
a narrowly-designed model in a critical infrastructure system could cause significant damage or
disruption if it malfunctions or is exploited. Therefore, it is essential to consider the risks and impact
of AI across multiple scales: immediate and long-term, individual and societal.
Predicting the trajectory and impact of AI is a difficult and uncertain task.
We have
already seen AI systems that achieved better-than-human performance in specific, narrow tasks such
as image recognition or playing chess. As technological advancement continues, it is possible that we
may see more general, advanced, and transformative systems that can perform a wider array of tasks
and adapt to more complex scenarios. However, forecasting these developments involves a high degree
of uncertainty, both in terms of rate and direction of AI advancement and in the potential risks and
capabilities of individual systems. The progression of AI is not linear and is influenced by a multitude
of factors, including advancements in algorithms, hardware, data availability, policy regulations, and
economic conditions. Hence, a prudent approach to AI development involves continuous vigilance,
rigorous testing, and adaptable strategies to manage both expected and unanticipated challenges.
Deep learning has emerged as a potent and defining force in AI, driving significant
advancements and innovations.
The strength of deep learning models lies in their ability to
learn and generalize from extensive datasets. Deep learning models can process massive quantities of
information, discerning patterns and relationships that are too complex for traditional algorithms.
These powerful capabilities are integral to many tools and systems we encounter in our daily
lives. Deep learning is the engine behind systems that recognize patterns in images and speech,
enabling technologies like automatic photo tagging and voice-activated virtual assistants. It powers
autonomous vehicles, chatbots, predictive modeling tools, and translation applications. The scope of
deep learning’s influence is vast and continues to expand, underscoring its pivotal role in the future
of AI.
Despite their accomplishments, DL and ML systems have many flaws and limitations.
Their
performance and capabilities depend on a number of factors, including the quality and quantity
of training data, the algorithms and architectures used, and the amount of computational power
available. In addition, these systems often struggle to generalize beyond the specific contexts they
were trained in. They lack the common sense and contextual knowledge that humans often use to
make decisions, leading to errors that may seem obvious to human observers. Moreover, they often
act as black boxes where the internal mechanisms driving their predictions are opaque, a trait that
can be especially problematic in high-stakes settings.
Understanding ML models is essential to identifying risks associated with their use.
Deep learning models are prone to absorbing and replicating biased patterns embedded in their

2.7. LITERATURE
125
training data, leading to skewed and possibly discriminatory decisions. Equipped with this knowledge,
we can anticipate such issues and proactively implement measures to address them, creating more
equitable outcomes. Moreover, an informed understanding of the technical intricacies of AI systems
is instrumental to assessing and incorporating crucial factors like robustness and interpretability in
their development and deployment. In doing so, we are better able to do things like improve model
performance in various environments and against adversarial attacks and provide more transparency
and accountability. Consequently, understanding ML models is key to ensuring the responsible and
equitable use of these powerful tools.
The methodologies employed to train models determine the capabilities and impact of
these systems.
Many techniques exist, each with distinct strengths, weaknesses, and associated
safety or ethical considerations. A system trained with reinforcement learning might adopt risky
or harmful behaviors if these behaviors are accidentally rewarded during training. This can pose
considerable risks, especially in real-world, high-stakes environments. Similarly, a model trained
using few-shot learning techniques, which rely on learning from a minimal number of examples, might
make crucial decisions based on an insufficient amount of data. This can introduce inaccuracies,
potentially causing harm or undesired outcomes. Therefore, understanding the intricacies of different
training methods and techniques is vital to developing robust and ethical AI systems.
AI is poised to become increasingly capable and influential as technology progresses.
Some of recent AI growth is driven by advancements in algorithms and techniques, which enable
systems to learn and make predictions more accurately and efficiently as they increase in sophistication.
Concurrently, the development of more powerful and cost-effective computing technology is facilitating
the deployment and scaling of AI systems, making them more accessible, widespread, and capable.
Moreover, the expanding volume and variety of data available for training is significantly enhancing
their ability to learn and generalize, thereby broadening their applicability. Together, these factors—
techniques, compute, and data—are accelerating the advancement of AI systems. AI’s impact on
individuals, industries, and society in the future is only becoming more profound.
2.7
Literature
The following resources contain more detailed information about the topics discussed in this chapter:
2.7.1
Recommended Reading
• Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep Learning.
http : / / www .
deeplearningbook.org. MIT Press, 2016
• S.J. Russell, P. Norvig, and E. Davis. Artificial Intelligence: A Modern Approach. Prentice
Hall series in artificial intelligence. Prentice Hall, 2010. isbn: 9780136042594. url: https:
//books.google.com.au/books?id=8jZBksh-bUMC
• Jordan Hoffmann et al. Training Compute-Optimal Large Language Models. 2022. arXiv:
2203.15556 [cs.CL]
• Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA, 2018. url: http://incompleteideas.net/book/the-book-2nd.html
2.7.2
References
[1]
Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. First edition. Oxford: Oxford University
Press, 2014. isbn: 978-0-19-967811-2.

126
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
[2]
Stuart J Russell and Peter Norvig. “Artificial Intelligence: A Modern Approach.” In: (2020).
[3]
Shane Legg and Marcus Hutter. “Universal intelligence: A definition of machine intelligence.” In:
Minds and Machines 17 (2007), pp. 391–444.
[4]
Alan M. Turing. “Computing Machinery and Intelligence”. In: Mind 59 (236 1950), pp. 433–460.
[5]
F. Rosenblatt. “The perceptron: A probabilistic model for information storage and organization in the
brain.” In: Psychological Review 65.6 (1958), pp. 386–408. issn: 0033-295X. doi: 10.1037/h0042519.
url: http://dx.doi.org/10.1037/h0042519.
[6]
Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. “Deep blue”. In: Artificial intelligence
134.1-2 (2002), pp. 57–83.
[7]
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT Press,
2009.
[8]
Leo Breiman. “Random Forests”. In: Machine Learning 20 (3 1995), pp. 273–297. url: https:
//doi.org/10.1023/a:1010933404324.
[9]
Jerome H. Friedman. “Greedy Function Approximation: A Gradient Boosting Machine.” In: The
Annals of Statistics 29 (5 2001), pp. 1189–232. url: http://www.jstor.org/stable/2699986.
[10]
Corinna Cortes and Vladimir Vapnik. “Support-Vector Networks.” In: Machine learning 20 (3).
[11]
Jia Deng et al. “ImageNet: A large-scale hierarchical image database”. In: 2009 IEEE Conference on
Computer Vision and Pattern Recognition. 2009, pp. 248–255. doi: 10.1109/CVPR.2009.5206848.
[12]
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “ImageNet Classification with Deep
Convolutional Neural Networks”. In: Advances in Neural Information Processing Systems. Ed. by
F. Pereira et al. Vol. 25. Curran Associates, Inc., 2012. url: https://proceedings.neurips.cc/
paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.
[13]
David Silver et al. “Mastering the game of Go with deep neural networks and tree search”. In: Nature
529 (Jan. 2016), pp. 484–489. doi: 10.1038/nature16961.
[14]
Alec Radford et al. “Language Models are Unsupervised Multitask Learners”. In: 2019. url: https:
//api.semanticscholar.org/CorpusID:160025533.
[15]
Nick Bostrom. “Superintelligence: Paths, Dangers, Strategies.” In: Oxford University Press (2014).
[16]
Christopher M. Bishop. “Pattern Recognition and Machine Learning.” In: (2016).
[17]
Kevin P. Murphy. Probabilistic Machine Learning: An Introduction. MIT Press, 2022.
[18]
Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting Misclassified and Out-of-Distribution
Examples in Neural Networks. 2018. arXiv: 1610.02136 [cs.NE].
[19]
Dan Hendrycks. Anomaly and Out-of-Distribution Detection. url: https : / / docs . google .
com / presentation / d / 1WEzSFUbcl1Rp4kQq1K4uONMJHBAUWhCZTzWVHnLcSV8 / edit # slide = id .
g60c1429d79_0_0.
[20]
Walber. Precision Recall. The layout of the figure has been changed. 2014. url: https://en.
wikipedia.org/wiki/File:Precisionrecall.svg.
[21]
cmglee and Martin Thoma. 2018. url: https://commons.wikimedia.org/wiki/File:Roc_curve.
svg.
[22]
In: (Jan. 2022). url: https://doi.org/10.3390/info13020071.
[23]
Y. LeCun, Y. Bengio, and G. Hinton. “Deep Learning”. In: Nature (521 2015), pp. 436–444. url:
https://doi.org/10.1038/nature14539.
[24]
url: https://commons.wikimedia.org/wiki/File:Neuron_-_annotated.svg.
[25]
url: https://commons.wikimedia.org/wiki/File:Rosenblattperceptron.png.
[26]
Hula Hula. MNIST Dataset. url: https://twoearth.tistory.com/31 (visited on 09/28/2023).
[27]
Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). 2023. arXiv: 1606.08415
[cs.LG].

2.7. LITERATURE
127
[28]
Vinod Nair and Geoffrey Hinton. “Rectified Linear Units Improve Restricted Boltzmann Machines
Vinod Nair”. In: vol. 27. June 2010, pp. 807–814.
[29]
Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). 2016, pp. 770–778. doi: 10.1109/CVPR.2016.90.
[30]
Febin Sunny, Mahdi Nikdast, and Sudeep Pasricha. SONIC: A Sparse Neural Network Inference
Accelerator with Silicon Photonics for Energy-Efficient Deep Learning. Sept. 2021.
[31]
Ashish Vaswani et al. “Attention is All you Need”. In: Advances in Neural Information Processing
Systems. Ed. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017. url: https://proceedings.
neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
[32]
Ravish Raj. Ed. by Andrew Ng Lectures. url: https://www.enjoyalgorithms.com/blog/parameter-
learning-and-gradient-descent-in-ml (visited on 09/28/2023).
[33]
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. “Learning representations by
back-propagating errors”. In: Nature 323 (1986), pp. 533–536. url: https://api.semanticscholar.
org/CorpusID:205001834.
[34]
Y. Lecun et al. “Gradient-based learning applied to document recognition”. In: Proceedings of the
IEEE 86.11 (1998), pp. 2278–2324. doi: 10.1109/5.726791.
[35]
Sepp Hochreiter and Jürgen Schmidhuber. “Long Short-Term Memory”. In: Neural Computation
9.8 (Nov. 1997), pp. 1735–1780. issn: 0899-7667. doi: 10.1162/neco.1997.9.8.1735. eprint:
https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf. url:
https://doi.org/10.1162/neco.1997.9.8.1735.
[36]
Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding. 2019. arXiv: 1810.04805 [cs.CL].
[37]
Joel Hestness et al. Deep Learning Scaling is Predictable, Empirically. 2017. arXiv: 1712.00409
[cs.LG].
[38]
Jared Kaplan et al. Scaling Laws for Neural Language Models. 2020. arXiv: 2001.08361 [cs.LG].
[39]
Jordan Hoffmann et al. Training Compute-Optimal Large Language Models. 2022. arXiv: 2203.15556
[cs.CL].
[40]
nostalgebraist. chinchilla’s wild implications. 2022. url: https://www.alignmentforum.org/posts/
6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications (visited on 09/28/2023).
[41]
Tom Brown et al. “Language models are few-shot learners”. In: Advances in neural information
processing systems 33 (2020), pp. 1877–1901.
[42]
Rich Sutton. The Bitter Lesson. url: http://www.incompleteideas.net/IncIdeas/BitterLesson.
html (visited on 09/28/2023).
[43]
Richard S. Sutton and Andrew Barto. Reinforcement Learning : An Introduction. The MIT Press,
2018.
[44]
L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement Learning: A Survey. 1996. arXiv:
cs/9605103 [cs.AI].
[45]
Honglei Xie. Constrained Contextual Bandits for Personalized Recommendation. Recreation from
the first figure. 2020. url: https://hongleixie.github.io/blog/Constrained-CB/ (visited on
09/28/2023).
[46]
EbatlleP. Reinforcement learning diagram of a Markov decision process based on a figure from
’Reinforcement Learning An Introduction’ second edition by Sutton and Barto. 2020. url: https:
//commons.wikimedia.org/wiki/File:Markov_diagram_v2.svg.
[47]
Thomas Simonini. License: https://github.com/huggingface/deep-rl-class/blob/main/LICENSE.md.
2018. url: https://www.google.com/url?q=https://huggingface.co/learn/deep-rl-course/
unit2/two-types-value-based-methods?fw%3Dpt&sa=D&source=docs&ust=1695859187289470&
usg=AOvVaw3SugjZNScFnPQBXoURDM56.
[48]
Andrej Karpathy. url: https://cs.stanford.edu/people/karpathy/reinforcejs/.

128
CHAPTER 2. ARTIFICIAL INTELLIGENCE FUNDAMENTALS
[49]
Volodymyr Mnih et al. Playing Atari with Deep Reinforcement Learning. 2013. arXiv: 1312.5602
[cs.LG].
[50]
O. inyals, I. Babuschkin, and W.M. Czarnecki. “Grandmaster level in StarCraft II using multi-agent
reinforcement learning”. In: Nature 575 (2019), pp. 350–354. url: https://doi.org/10.1038/s41586-
019-1724-z.
[51]
Meta Fundamental AI Research Diplomacy Team (FAIR)† et al. “Human-level play in the game of
Diplomacy by combining language models with strategic reasoning”. In: Science 378.6624 (2022),
pp. 1067–1074. doi: 10.1126/science.ade9097. eprint: https://www.science.org/doi/pdf/10.
1126/science.ade9097. url: https://www.science.org/doi/abs/10.1126/science.ade9097.
[52]
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. http://www.deeplearningbook.
org. MIT Press, 2016.
[53]
S.J. Russell, P. Norvig, and E. Davis. Artificial Intelligence: A Modern Approach. Prentice Hall series
in artificial intelligence. Prentice Hall, 2010. isbn: 9780136042594. url: https://books.google.com.
au/books?id=8jZBksh-bUMC.
[54]
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA, 2018. url: http://incompleteideas.net/book/the-book-2nd.html.

Part II
Safety
129


Single-Agent Safety
3.1
Introduction
To understand the risks associated with artificial intelligence (AI), we begin by examining the
challenge of making single agents safe. In this chapter, we review the problems of bias, opaqueness,
proxy gaming, power seeking, deception, and emergence.
To start, we outline how biases can arise in these systems—an inevitable consequence of how they
learn, the data they are trained on, and the human factors involved in their creation. From this, we
move on to a discussion of the opaqueness of machine learning (ML) systems—that their “black-box”
nature hinders our ability to fully comprehend how they make decisions and what their intentions, if
any, may be.
Next, we turn to problems involved in specifying and optimizing goals. In many cases, it is not
possible to perfectly specify our idealized goals. Inadequately specified goals can lead to ML systems
diverging from our idealized goals, and introduce vulnerabilities that adversaries can attack and
exploit.
We then pivot our discussion to the concept of power in AI systems. Here, we explore the possible
conditions that could give rise to power-seeking agents and the ways in which those motives could lead
to particularly harmful risks. Following this, we turn our attention to the topic of emergence in AI
systems and explore how simple goals can cause these systems to spontaneously develop unanticipated
capabilities and goals.
In the next section, we explore the issue of deception. We categorize the varied forms of deception
that these systems might employ, and analyze the risks involved in AI systems deceiving human and
AI evaluators.
We conclude by explaining how researchers trying to improve AI safety can unintentionally improve
the capabilities of AI systems in a way that ends up increasing the overall risks that they pose. To
avoid this, it is important to pick research topics carefully to minimise the impacts that successful
research will have on capabilities.
This chapter argues that, even when considered in isolation, individual AI systems can pose
catastrophic risks. As we will see in subsequent chapters, many of these risks become more pronounced
when considering multi-agent systems, misuse, and arms race dynamics.
3.2
Bias
Introduction
AI systems can amplify undesirable biases.
AI systems are being increasingly deployed
throughout society. If these influential systems have biases, they can reinforce disparities and produce
widespread, long-term harms. In AI, bias refers to a consistent, systematic, or undesirable distortion
in the outcomes produced by an AI system. These outcomes can be predictions, classifications, or
decisions. Bias can be influenced by many factors, including erroneous assumptions, training data, or
human biases. Biases in modern deep learning systems can be especially consequential. While not
all forms of bias are harmful, we focus on biases that are socially relevant because of their harms.
131

132
CHAPTER 3. SINGLE-AGENT SAFETY
We must proactively prevent bias to avoid its harms. This section overviews bias in AI and outlines
some mitigation strategies.
Aspects of bias in AI.
A bias is systematic when it includes a pattern of repeated deviation
from the true values in one direction. Unlike random unstructured errors, or “noise,” these biases
are not reliably fixed by just adding more data. Resolving ingrained biases often requires changing
algorithms, data collection practices, or how the AI system is applied. Algorithmic bias occurs when
any computer system consistently produces results that disadvantage certain groups over others.
Some biases are relatively harmless, like a speech recognition system that is better at interpreting
human language than whale noises. However, other forms of bias can result in serious social harms,
such as partiality to certain groups, inequity, or unfair treatment.
Bias can manifest at every stage of the AI lifecycle.
From data collection to real-world
deployment, bias can be introduced through multiple mechanisms at any step in the process. Historical
and social prejudices produce skewed training data, propagating flawed assumptions into models.
Flawed models can cement biases into the AI systems that help make important societal decisions.
In addition, humans misinterpreting results can further compound bias. After deployment, biased AI
systems can perpetuate discriminatory patterns through harmful feedback loops that exacerbate bias.
Developing unbiased AI systems requires proactively identifying and mitigating biases across the
entire lifecycle.
Figure 3.1: Systematic psychological, historical, and social biases can lead to algorithmic biases
within AI systems.
Biases in AI often reflect systemic biases.
Systematic biases can occur even against developers’
intentions. For instance, Amazon developed an ML-based resume-screening algorithm trained on
historical hiring decisions. However, as the tech industry is predominantly male, this data reflected
skewed gender proportions in the data (about 60% male and 40% female) [1]. Consequently, the

3.2. BIAS
133
algorithm scored male applicants higher than equally qualified women, penalizing resumes with
implicit signals like all-female colleges. The algorithm essentially reproduced real-world social biases in
hiring and employment. This illustrates how biased data, when fed into AI systems, can inadvertently
perpetuate discrimination. Organizations must be vigilant about biases entering any stage of the
machine learning pipeline.
In many countries, some social categories are legally protected from discrimination.
Groups called protected classes are legally protected from harmful forms of bias. These often include
race, religion, sex/gender, sexual orientation, ancestry, disability, age, and others. Laws in many
countries prohibit denying opportunities or resources to people solely based on these protected
attributes. Thus, AI systems exhibiting discriminatory biases against protected classes can produce
unlawful outcomes. Mitigating algorithmic bias is crucial for ensuring that AI complies with equal
opportunity laws by avoiding discrimination.
Conclusion.
Identifying and mitigating biases is crucial to build social trust in AI. This section
discusses the main sources of bias as well as strategies researchers are exploring to mitigate bias.
However, this overview aims to be brief, so it is not exhaustive.
3.2.1
Sources of Bias
Biases can arise from multiple sources, both from properties of the AI system itself and human
interaction with the system. This section discusses common sources of harmful biases in AI systems,
although there are many more. First, we will discuss technical sources of bias, primarily from flawed
data or objectives. Then, we will review some biases that arise from interactions between humans
and AI systems.
Technical Sources of Bias in AI Systems
An overview of technical sources of bias.
In this section, we will review some sources of bias
in technical aspects of AI systems. First, we will investigate some data-driven sources of biases,
including flawed training data, subtle patterns that can be used to discriminate, biases in how
the data is generated or reported, and underlying societal biases. Flawed or skewed training data
can propagate biases into the model’s weights and predictions. Then, we show how RL training
environments and objectives can also reinforce bias.
ML models trained on biased datasets can learn and reinforce harmful societal biases.
AI systems learn from human-generated data, absorbing both valuable knowledge and harmful biases.
Even when unintentional, this data frequently mirrors ingrained societal prejudices. As a result, AI
models can propagate real-world discrimination by learning biases from their input data. For instance,
a lawsuit found that Facebook’s ad targeting algorithm violated the Fair Housing Act because it
learned to exclude users from seeing housing ads based on race, gender, or other protected traits.
Similarly, ML models can reflect political biases, deprioritizing users from specific political affiliations
by showing their content to smaller audiences. As another example, an NLP model trained on a
large corpus of internet text learned to reinforce gender stereotypes, completing sentence structures
of the format “man is to X as woman is to Y” with content such as “man is to computer programmer
as woman is to homemaker” [2]. These examples show how ML models can amplify existing social
biases.

134
CHAPTER 3. SINGLE-AGENT SAFETY
Models can learn to discriminate based on subtle correlations.
One intuitive way to fix
bias is to remove protected attributes like gender and achieve “fairness through unawareness.” But
this is not enough to remove bias. ML models can learn subtle correlations that serve as proxies for
these attributes. For example, even in datasets with gender information removed, resume-screening
models learned to associate women with certain colleges and assigned them lower scores [1]. In
another study, ML models erroneously labeled images of people cooking as women, due to learned
gender biases [3]. Thus, models can discriminate even when the data does not contain direct data
about protected classes. This hidden discrimination can harm protected groups despite efforts to
prevent bias.
Biased or unrepresentative data collection can lead to biased decisions.
Training data
reflects biases in how the data was collected. If the training data is more representative of some groups
than others, the predictions from the model may also be systematically worse for the underrepresented
groups. Thus, the model will make worse or biased decisions for the group that is represented less
in the dataset. Imbalances in training data occur when the data is skewed with respect to output
labels, input features, and data structure. For instance, a disease prediction dataset with 100,000
healthy patients but only 10 sick patients exhibits a large class imbalance. The minority class with
fewer examples is underrepresented.
Several other problems can introduce bias in AI training data.
Systematic problems in
the data can add bias. For instance, reporting bias occurs when the relative frequency of examples in
the training data misrepresents real-world frequencies. Often, the frequency of outcomes in legible
data does not reflect their actual occurrence. For instance, the news amplifies shocking events and
under-reports normal occurrences or systematic, ongoing problems—reporting shark attacks rather
than cancer deaths. Sampling bias occurs when the data collection systematically over-samples some
groups and undersamples others. For instance, facial recognition datasets in Western countries often
include many more lighter-skinned individuals. Labeling bias is introduced later in the training
process, when systematic errors in the data labeling process distort the training signal for the model.
Humans may introduce their own subjective biases when labeling data.
Beyond problems with the training data, the training environments and objectives of RL models can
also exhibit problems that promote bias. Now, we will review some of these sources of bias.
Training environments can also amplify bias.
Reward bias occurs when the environments
used to train RL models introduce biases through improper rewards. RL models learn based on the
rewards received during training. If these rewards fail to penalize unethical or dangerous behavior,
RL agents can learn to pursue immoral outcomes. For example, models trained in video games may
learn to accomplish goals by harming innocents if these actions are not sufficiently penalized in
training. Some training environments may fail to encourage good behaviors enough, while others can
even incentivize bad behavior by rewarding RL agents for taking harmful actions. Humans must
carefully design training environments and incentives that encourage ethical learning and behavior
[4].
RL models can optimize for training objectives that amplify bias or harm.
Reinforcement
learning agents will try to optimize the goals they are given in training, even if these objectives
are harmful or biased, or reflect problematic assumptions about value. For example, a social media
news feed algorithm trained to maximize user engagement may prioritize sensational, controversial,
or inflammatory content to increase ad clicks or watch time. Technical RL objectives often make
implicit value assumptions that cause harm, especially when heavily optimized by a powerful AI
system [5, 6]. News feed algorithms implicitly assume that how much a user engages with some

3.2. BIAS
135
content is a high-quality indicator of the value of that content, therefore showing it to even more users.
After all, social media companies train ML models to maximize ad revenue by increasing product
usage, rather than fulfilling goals that are harder to monetize or quantify, such as improving user
experience or promoting accurate and helpful information. Especially when taken to their extreme
and applied at a large scale, RL models with flawed training objectives can exacerbate polarization,
echo chambers, and other harmful outcomes. Problems with the use of flawed training objectives are
further discussed in section 3.4.
In summary, biased training data, flawed objectives, and other technical aspects of ML models can
introduce bias, illustrating how ML bias can perpetuate existing disparities. Carefully scrutinizing
the technical details of models is crucial for mitigating these biases.
Biases from human-AI interactions
Interactions between humans and AI systems can produce many kinds of bias.
It
is not enough to just ensure that AI systems have unbiased training data: humans interacting
with AI systems can also introduce biases during development, usage, and monitoring. Flawed
evaluations allow biases to go unnoticed before models are deployed. Confirmation bias in the
context of AI is when people focus on algorithm outputs that reinforce their pre-existing views,
dismissing opposing evidence. Humans may emphasize certain model results over others, skewing
the outputs even if the underlying AI system is reliable. This distorts our interpretation of model
decisions. Overgeneralization occurs when humans draw broad conclusions about entire groups based
on limited algorithmic outputs that reflect only a subset. Irrationality and human cognitive bias play
a substantial role in biasing AI systems.
Human-AI system biases can be reinforced by feedback loops.
Feedback loops in human-AI
systems often arise when the output of an AI system is used as input in future AI models. An
AI system trained on biased data could make biased decisions that are fed into future models,
reinforcing bias in a self-perpetuating cycle. We speak more about these feedback loops in chapter 5.
Self-fulfilling prophecies can occur when an algorithmic decision influences actual outcomes, as the
model reinforces its own biases and influences future input data [7]. In this way, models can amplify
real-world biases, making them even more real. For example, a biased loan-approval algorithm could
deny loans to lower-income groups, reinforcing real-world income disparities that are then reflected
in the training data for future models. This process can make bias more severe over time.
Automation and measurability induce bias.
Bias can be amplified by automation bias, where
humans favor algorithmic decisions over human decisions, even if the algorithm is wrong or biased.
This blind trust can cause harm when the model is flawed. Similarly, a bias toward the measurable
can promote a general preference for easily quantifiable attributes. Human-AI systems may overlook
important qualitative aspects and less tangible factors.
Despite their problems, AI systems can be less biased than humans.
Although there are
legitimate concerns, AI systems used for hiring and other sensitive tasks may sometimes lead to less
biased decisions when compared with human decision-makers. Humans often harbor strong biases
that skew their judgment in these decisions. With careful oversight and governance, AI holds promise
to reduce certain biases relative to human motivations.
In summary, human biases and blind spots during development, usage, and governance of AI systems
contribute to biased outputs from AI systems. A holistic approach for critically examining human-AI
interaction is required to address this.

136
CHAPTER 3. SINGLE-AGENT SAFETY
3.2.2
Strategies for Bias Mitigation
Multiple complementary approaches can help address bias in AI systems.
Even with the
best intent, biases in AI can be difficult to anticipate. Mitigating the complicated problem of bias
calls for combining engineering, social, legal, and governance strategies. For instance, one promising
technical approach involves training an adversarial network to predict a protected variable from
an ML model’s outputs [8]. By penalizing the model when the adversary succeeds at predicting
a variable like race or political affiliation from the model’s outputs, the model is forced to avoid
discrimination and make predictions that do not unfairly depend on sensitive attributes. When
applied well, this can minimize biases. However, even the best technical approaches have limitations,
and a holistic solution will integrate many approaches.
Engineering strategies for reducing bias must be paired with non-technical strategies.
Ultimately, technical debiasing alone is insufficient. Social processes are crucial as humans adapt to
AI. We speak about this at length in section 4.5, but here we will only mention a few ideas. For
instance, early bias detection involves creating checks to identify risks of bias before the AI system is
deployed or even trained, so that models that have discriminatory outputs can be rejected before they
cause harm. Similarly, gradual deployment safely transitions AI systems into use while monitoring
them for bias so that harms can be identified early and reversed. Regulatory changes can require
effective mitigation strategies by law, mandating transparency and risk mitigation in safety-critical
AI systems, as we discuss in chapter 9.
Participatory design can mitigate bias in AI development.
An important non-technical
strategy for bias reduction is stakeholder engagement, or deeply engaging impacted groups in the
design of the AI system to identify potential biases proactively. Diverse teams and users can also help
engineering teams incorporate diverse perspectives into the R&D process of AI models to anticipate
potential biases proactively. One approach to proactively addressing bias is participatory design,
which aims to include those affected by a developing technology as partners in the design process to
ensure that the final product meets diverse human interests. For example, before implementing an
AI notetaking assistant for doctors, participatory design can require hospitals to improve the system
based on feedback from all stakeholders during iterative design sessions with nurses, doctors, and
patients. Rather than just evaluating ML models on test sets, developers should consult with the
affected groups during the design process. Adding oversight mechanisms for rejecting models with
discriminatory outputs can also enable catching biases before AI models affect real decisions.
Independent audits are important for identifying biases in AI systems before deployment.
Auditors can systematically evaluate datasets, models, and outputs to uncover discrimination and
hold the developers of AI systems accountable. There are several signs of bias to look for when
auditing datasets. For example, auditors can flag missing data for certain subgroups, which indicates
underrepresentation. Data skew, where certain groups are misrepresented compared to their real-
world prevalence, is another sign of bias. Patterns and correlations with protected classes could
indicate illegal biases. Auditors can also check for disparities in the model outputs. By auditing
throughout the process, developers can catch biases early, improving data and models before their
biases propagate. Rather than waiting until after the system has harmful impacts, meticulous audits
should be integrated as part of the engineering and design process for AI systems [9]. Audits are
especially effective when conducted independently by organizations without a stake in the AI system’s
development, allowing for impartial and rigorous auditing throughout the process.
Effective model evaluation is a crucial way to reduce bias.
An important part of mitigating
bias is proactively evaluating AI systems by analyzing their outputs for biases. Models can be tested
by measuring performance metrics such as false positive and false negative rates separately for each

3.3. OPAQUENESS
137
subgroup. For instance, significant performance disparities between groups like men and women can
reveal unfair biases. Ongoing monitoring across demographics is necessary to detect unintended
discrimination before AI systems negatively impact people’s lives. Without rigorous evaluation of
model outputs, harmful biases can easily go unnoticed.
Reducing toxicity in data aims to mitigate harmful biases in AI, but faces challenges.
Toxicity refers to harmful content, such as inflammatory comments or hate speech. Models trained
on unfiltered text can absorb these harmful elements. As a result, AI models can propagate toxic
content and biases if not carefully designed. For example, language models can capture stereotypical
and harmful associations between social groups and negative attributes based on the frequency of
words occurring together. Reducing toxicity in the training data can mitigate some biases. For
example, developers can use toxicity classifiers to clean up the internet data, using both sentiment
and manual labels to identify toxic content. However, all of these approaches still run into major
challenges and limitations. Classifiers are still subject to social bias, evaluations can be brittle and
unreliable, and bias is often very hard to measure.
Trade-offs can emerge between correcting one form of bias and introducing new biases.
Bias reduction methods can introduce new biases, as classifiers have social biases, evaluations are
unreliable, and bias reduction can introduce new biases. For example, some experiments show
that an attempt to correct for toxicity in OpenAI’s older content moderation system resulted in
biased treatment towards certain political and demographic groups: a previous system classified
negative comments about conservatives as not hateful, while flagging the exact same comments about
liberals as hateful [10]. It also exhibited disparities in classifying negative comments towards different
nationalities, religions, identities, and more.
In summary, biases can arise at all stages of the AI lifecycle and compound in complicated ways.
A multifaceted approach is necessary, combining development practices, technical strategies, and
governance, to detect and reduce harmful biases in AI systems.
3.3
Opaqueness
The internal operations of many AI systems are opaque. We might be able to reveal and prevent
harmful behavior if we can make these systems more transparent. In this section, we will discuss
why AI systems are often called black boxes and explore ways to understand them. Although early
research into transparency shows that the problem is highly difficult and conceptually fraught, its
potential to improve AI safety is substantial.
3.3.1
ML Systems are Opaque
The most capable machine learning models today are based on deep neural networks. Whereas most
conventional software is directly written by humans, deep learning (DL) systems independently learn
how to transform inputs to outputs layer-by-layer and step-by-step. We can direct DL models to
learn how to give the right outputs, but we do not know how to interpret the model’s intermediate
computations. In other words, we do not understand how to make sense of a model’s activations
given a real-world data input. As a result, we cannot make reliable predictions about a model’s
behavior when given new inputs. This section will present a handful of analogies and results that
illustrate the difficulty of understanding machine learning systems.
Deep learning models as a black box.
Machine learning researchers often refer to a DL models
as a black box [11], a system that can be viewed in terms of its input-output behavior without insight

138
CHAPTER 3. SINGLE-AGENT SAFETY
on its internal workings. Humans are black boxes—we see their behavior, but not the internal brain
activity that produces it, let alone how fully understand that brain activity. Although a deep neural
network’s weights and activations are observable, these long lists of numbers do not currently help us
understand how a model will behave. We cannot reduce all the numerical operations of a state of the
art model into a form that is meaningful to humans.
Figure 3.2: ML systems can be broken down into computational graphs with many components. [12]
Even simple ML techniques suffer from opaqueness.
Opaqueness is not unique to neural
networks. Even simple ML techniques such as Principal Component Analysis (PCA), which are
better understood theoretically than DL, suffer from similar flaws. For example, Figure 3.3 depicts
the results of performing PCA on pictures of human faces. This yields a set of “eigenfaces”, capturing
the most important features identifying a face. Any picture of a face can then be represented as a
particular combination of these eigenfaces.

3.3. OPAQUENESS
139
Figure 3.3: A human face can be made by combining several eigenfaces, each of which represents
different facial features. [13]
In some cases, we can guess what facial features an eigenface represents: for example, one eigenface
might represent lighting and shading while another represents facial hair. However, most eigenfaces
do not represent clear facial features, and it is difficult to verify that our hypotheses for any single
feature capture the entirety of their role. The fact that even simple techniques like PCA remain
opaque is a sign of the difficulty of the problem in the more complicated techniques like DL.
Feature visualizations demonstrate that deep learning neurons are hard to interpret.
In a neural network, a neuron is a component of an activation vector. One attempt to understand
deep networks involves looking for simple quantitative or algorithmic descriptions of the relationship
between inputs and neurons such as “if the ear feature has been detected, the model will output
either dog or cat” [14]. For image models, we can create feature visualizations, artificial images that
highly activate a particular neuron (or set of neurons) [15]. We can also examine natural images that
highly activate that neuron.
Figure 3.4: Left: a “feature visualization” that highly activates a particularly neuron. Right: a
collection of natural images that activate a particular neuron. [16]
Like eigenfaces, neurons may be more or less interpretable. Sometimes, feature visualizations identify
neurons that seem to depend on a pattern of the input that is clear to humans. For example, a
neuron might activate only when an image contains dog ears. In other cases, we observe polysemantic
neurons, which defy a single interpretation [17]. Consider Figure 3.4 , which shows images that highly

140
CHAPTER 3. SINGLE-AGENT SAFETY
activate a randomly chosen neuron in an image model. Judging from the natural images, it seems
like the neuron often activates when text associated with traveling or moving is present, but it’s hard
to be sure.
Neural networks are complex systems.
Both human brains and deep neural networks are
complex systems, and so involve interdependent and nonlinear interactions between many components.
Like many other complex systems (see the Complex Systems chapter), the emergent behaviors of
neural networks are difficult to understand in terms of their components. Just as neuroscientists
struggle to identify how a particular biological neuron contributes to a mind’s behavior, ML researchers
struggle to determine how a particular artificial neuron contributes to a DL model’s behavior. There
are limits on our ability to systematically understand and predict complex systems, which suggests
that ML opaqueness may be a special case of the opaqueness of complex systems.
3.3.2
Motivations for Transparency Research
There is often no way to tell whether a model will perform well on new inputs. If the model performs
poorly, we generally cannot tell why. With better transparency tools, we might be able to reveal
and proactively prevent failure modes, detect the emergence of new capabilities, and build trust
that models will perform as expected in new circumstances. High-stakes domains might demand
guarantees of reliability based on the soundness or security of internal AI processes, but virtually no
such guarantees can be made for neural networks given the current state of transparency research.
If we could meaningfully understand how a model treats a given input, we would be better able to
monitor and audit its behavior. Additionally, by understanding how models solve difficult and novel
problems, transparency might also become a source of conceptual and scientific insight [11].
Ethical obligations to make AI transparent.
Model transparency can help ensure that model
decision making is fair, unbiased, and ethical. For example, if a criminal justice system uses an
opaque AI to make decisions about policing, sentencing, or probation, then those decisions will be
similarly opaque. People might have a right to an explanation of decisions that will significantly
affect them [18]. Transparency tools may be crucial to ensuring that right is upheld.
Accountability for harms and hazards.
Who is responsible when AI systems fail? Responsibility
often depends on the intentions and degree of control held by those involved. The best way to
incentivize safety might be to hold AI creators responsible for the damage their systems cause.
However, we might not want to hold people responsible for the behavior of systems they cannot
predict or understand. The growing autonomy and complexity of AI systems means that people
will have less control over AI behavior. Meanwhile, the scope and generality of modern AI systems
makes it impossible to verify desirable behavior in every case. In “human-in-the-loop” systems, where
decisions depend on both humans and AIs, human operators might be blamed for failures over which
they had little control [19].
AI transparency could enable a more robust system of accountability. For instance, governments
could mandate that AI systems meet baseline requirements for understandability. If an AI fails
because of a mechanism that its creator could have identified and prevented with transparency tools,
we would be more justified in holding that creator liable. Transparency could also help to identify
responsibility and fairly assign blame in failures involving human-in-the-loop systems.
Combating deception.
Just as a person’s behavior can correspond with many intentions, an AI’s
behavior can correspond to many internal processes, some of which are more acceptable than others.

3.3. OPAQUENESS
141
For example, competent deception is intrinsically difficult to distinguish from genuine helpfulness.
We discuss this issue in more detail in the Deception section. For phenomena like deception that are
difficult to detect from behavior alone, transparency tools might allow us to catch internal signs that
show that a model is engaging in deceptive behavior.
3.3.3
Approaches to Transparency
The remainder of this section explores a variety of approaches to transparency. Though the field is
promising, we are careful to note the shortcomings of these approaches. For a problem as conceptually
tricky as opaqueness, it is important to maintain a clear picture of what successful techniques must
achieve and hold new methods to a high standard. We will discuss the research areas of explainability,
saliency maps, mechanistic interpretability, and representation engineering.
Explanations
What must explanations accomplish? One approach to transparency is to create explanations
of a model’s behavior. These explanations could have the following virtues:
• Predictive power: A good explanation should help us understand not just a specific behavior,
but how the model is likely to behave in new situations. Building user trust in a system is
easier when a user can more clearly anticipate model behavior.
• Faithfulness: A faithful explanation accurately reflects the internal workings of the model. This
is especially valuable when we need to understand the precise reason why a model made a
particular decision. Faithful explanations are often better able to predict behavior because they
more closely track the actual mechanisms that models are using to produce their behavior [11].
• Simplicity: A simple explanation is easier to understand. However, it is important that the
simplification does not sacrifice too much information about actual model processes. Though
some information loss is inevitable, explanations must strike the right balance between simplicity
and faithfulness.
Explanations must avoid confabulation.
Explanations can sound plausible even if they are
false. A confabulation is an explanation that is not faithful to the true processes and considerations
that gave rise to a behavior. Both humans and AI systems confabulate.
Human confabulation.
Humans are not transparent systems, even to themselves. In some sense,
the field of psychology exists because humans cannot accurately intuit how their own mental processes
produce their experience and behavior. For example, mock juries tend to be more lenient with
attractive defendants, all else being equal, even though jurors almost never reference attractiveness
when explaining their decisions [20].
Another example of human confabulation can be drawn from studies on split-brain patients, those
who have had the connection between their two cerebral hemispheres surgically severed causing
each hemisphere to process information independently [21]. Researchers can give information to one
hemisphere and not the other by showing the information to only one eye. In some experiments,
researchers gave written instructions to a patient’s right hemisphere, which is unable to speak. After
the patient completed the instructions, the researchers asked the patient’s verbal left hemisphere why
they had taken those actions. Unaware of the instructions, the left hemisphere reported plausible
but incorrect explanations for the patient’s behavior.

142
CHAPTER 3. SINGLE-AGENT SAFETY
Machine learning system confabulation.
We can ask language models to provide justifications
along with their answers. Natural language reasoning is much easier to understand than internal
model activations. For example, if an LLM describes each step of its reasoning in a math problem
and gets the question wrong, humans can check where and how the mistake was made.
However, like human explanations, language model explanations are prone to unreliability and
confabulation.
For instance, when researchers fine-tuned a language model on multiple-choice
questions where option (a) was always correct, the model learned to always answer (a). When this
model was told to write explanations for questions whose correct answers were not (a), the model
would produce false but plausible explanations for option (a). The model’s explanation systematically
failed to mention the real reason for its answers, which was that it had been trained to always pick
(a) [22].
An alternative view of explanations.
Instead of requiring that explanations directly describe
internal model processes, a more expansive view argues that explanations are just any useful auxiliary
information provided alongside the output of a model. Such explanations might include contextual
knowledge or observations that the model makes about the input. Models can also make auxiliary
predictions; for example they could note that if an input were different in some specific ways, the
output would change. However, while this type of information can be valuable when presented
correctly, such explanations have the potential to mislead us.
Saliency Maps
Saliency maps purport to identify important components of images. Saliency maps are
visualizations that aim to show which parts of the input are most relevant to the model’s behavior
[23]. They are inspired by biological visual processing: when humans and other animals are shown
an image, they tend to focus on particular areas. For example, if a person looks at a picture of a dog,
the dog’s ears and nose will be more relevant than the background to how the person interprets the
image. Saliency map techniques have been popular in part due to the striking visualizations they
produce.
Figure 3.5: A saliency map picks out features from an input that seem particularly relevant to the
model, such as the shirt and cowboy hat in the bottom left image. [24]
Saliency maps often fail to show how machine learning vision models process images.
In practice, saliency maps are largely bias-confirming visualizations that usually do not provide useful
information about models’ inner workings. It turns out that many saliency maps are not dependent
on a model’s parameters, and the saliency maps often look similar even when generated for random,
untrained models. That means many saliency maps are incapable of providing explanations that
have any relevance to how a particular model processes data [25]. Saliency maps serve as a warning

3.3. OPAQUENESS
143
that visually or intuitively satisfying information that seems to correspond with model behavior may
not actually be useful. Useful transparency research must avoid the past failures of the field and
produce explanations that are relevant to the model’s operation.
Mechanistic Interpretability
When trying to understand a system, we might start by finding the smallest pieces of the system
that can be well understood and then combine those pieces to describe larger parts of the system. If
we can understand successively larger parts of the system, we might eventually develop a bottom-up
understanding of the entire system. Mechanistic interpretability is a transparency research area that
aims to represent models in terms of combinations of small, well-understood mechanisms [26]. If we
can reverse-engineer algorithms that describe small subsets of model activations and weights, we
might be able to combine these algorithms to explain successively larger parts of the model.
Features are the building blocks of deep learning mechanisms.
Mechanistic interpretability
proposes focusing on features, which are directions in a layer’s activation space that aim to correspond
to a meaningful, articulable property of the input [27]. For example, we can imagine a language
model with a “this is in Paris” feature. If we evaluate the input “Eiffel Tower” using the language
model, we may find that a corresponding activation vector points in a similar direction as the “this is
in Paris” feature direction [28]. Meanwhile, the activation vector encoding “Coliseum” may point
away from the “this is in Paris” direction. Other examples of image or text features include “this text
is code”, curve detectors, and a large-small dichotomy indicator.
One goal of mechanistic interpretability is to identify features that maintain a coherent description
across many different inputs: a “this is in Paris” feature would not be very valuable if it was highly
activated by “Statue of Liberty.” Recall that most neurons are polysemantic, meaning they don’t
individually represent features that are straightforwardly recognisable by humans. Instead, most
features are actually combinations of neurons, making them difficult to identify due to the sheer
number of possible combinations. Despite this challenge, features can help us think about the
relationship between the internal activations of models and human-understandable concepts.
Circuits are algorithms operating on features.
Features can be understood in terms of other
features. For example, if we’ve discovered features in one layer of an image model that detect dog
ears, snouts, and tails, an input image with high activations for all of these features may be quite
likely to contain a dog. In fact, if we discover a dog-detecting feature in the next layer of the model,
it is plausible that this feature is calculated using a combination of dog-part-detecting features from
the previous layer. We can test that hypothesis by checking the model’s weights.
A function represented in model weights which relates a model’s earlier features to its later features
is called a circuit [27]. In short, circuits are computations within a model that are often more
understandable. The project of mechanistic interpretability is to identify features in models and
circuits between them. The more features and circuits we identify, the more confident we can be
that we understand some of the model’s mechanisms. Circuits also simplify our understanding of
the model, allowing us to equate complicated numerical manipulations with simpler algorithmic
abstractions.
An empirical example of a circuit.
For the sake of illustration, we will describe a purported
circuit from a language model. Researchers identified how a language model often predicts indirect
objects of sentences (such as “Mary” in “John gave a drink to ...”) as a simple algorithm using all
previous names in a sentence (see Figure 3.6 below). This mechanism did not merely agree with
model behavior, but was directly derived from the model weights, giving more confidence that the

144
CHAPTER 3. SINGLE-AGENT SAFETY
algorithm is a faithful description of an internal model mechanism [26].
Figure 3.6: An indirect-object identification circuit can be depicted graphically.
Complex system understanding through mechanisms is limited.
There are number of
reasons to be concerned about the ability of mechanistic interpretability research to achieve its
ambitions. It is challenging to reduce a complex system’s behavior into many different low-level
mechanisms. Even if we understood each of a trillion neurons in a large model, we might not be able
to combine the pieces into an understanding of the system as a whole. Another concern is that it is
unclear if mechanistic interpretability can represent model processes with enough simplicity to be
understandable. ML models might represent vast numbers of partial concepts and complex intuitions
that can not be represented by mechanisms or simple concepts.
Representation Engineering
Representation reading and representation control [29].
Mechanistic interpretability is a
bottom-up approach and combines small components into an understanding of larger structures.
Meanwhile, representation engineering is a top-down approach that begins with a model’s high-level
representations and analyzes and controls them. In machine learning, models learn representations
that are not identical to their training data, but rather stand in for it and allow them to identify
essential elements or patterns in the data (see Artificial Intelligence Fundamentals chapter for further
details). Rather than try to fully understand arbitrary aspects of a model’s internals, representation
engineering develops actionable tools for reading representations and controlling them.
We can detect high-level subprocesses.
Even though neuroscientists don’t understand the
brain in fine-grained detail, they can associate high-level cognitive tasks to particular brain regions.
For example, they have shown that Wernicke’s area is involved in speech comprehension. Though the
brain was once a complete black box, neuroscience has managed to decompose it into many parts.
Neuroscientists can now make detailed predictions about a person’s emotional state, thoughts, and
even mental imagery just by monitoring their brain activity [30].
Representation reading is the similar approach of identifying indicators for particular subprocesses.
We can provide stimuli that relate to the concepts or behaviors that we want to identify. For example,
to identify and control honesty-related outputs, we can provide contrasting prompts to a model

3.4. PROXIES
145
such as “Pretend you’re [an honest/a dishonest] person making statements about the world.” We
can track the differences in the model’s activations when responding to these stimuli. We can use
these techniques to find portions of models which are responsible for important behaviors like models
refusing requests, planning undesirable behavior, or deception.
A control panel of AI operation.
The result of representation reading could be a control panel
of the most important indicators of model processing. Operators could detect anomalous model
behavior by monitoring for unusual or concerning combinations of indicators. Though this research
area is relatively new, its techniques show early promise.
With representation control, we can also use the outputs from representation reading (such as
differences in activations in response to contrasting prompts) to adjust a model’s representation. For
example, we could use this to delete unintended knowledge or skills from a network [31]. As another
example, we can use representation control to control whether an AI lies or is honest [32]. Although
it relies on representation reading techniques, representation control is not an area of transparency
since its goal is control and not a general understanding of neural networks’ inner workings.
Conclusion.
ML transparency is a challenging problem because of the difficulty of understanding
complex systems. Its ongoing research areas are mechanistic interpretability and representation
reading, the latter of which does not aim to make neural networks fully understood but aims to gain
useful internal knowledge from a model’s representations.
3.4
Proxies
In this section, we begin to explore the need for proxies in machine learning and the associated risks.
We do this by examining a potential failure mode known as proxy gaming, wherein a model optimizes
for a proxy in a way that diverges from the idealized goals of its designers. We also analyze a related
concept known as Goodhart’s law and explore some of the causes for these kinds of failure modes.
Next, we consider the related phenomenon of adversarial examples, where an optimizer is used to
exploit vulnerabilities in a neural network. We close by looking at the tail risks of having AI systems
themselves play the role of evaluators (i.e. proxy goals) for other other AI systems.
3.4.1
Proxies in Machine Learning
Here, we look at the concept of proxies, why they are necessary, and how they can lead to problems.
Many goals are difficult to specify exactly.
It is hard to measure or even define many of the
goals we care about. They could be too abstract for straightforward measurement, such as justice,
freedom, and equity, or they could simply be difficult to observe directly, such as the quality of
education in schools.
With ML systems, this difficulty is especially pronounced because, as we saw in the Artificial
Intelligence Fundamentals chapter, ML systems require quantitative, measurable targets in order to
learn. This places a strong limit on the kinds of goals we can represent. As we’ll see in this section,
specifying suitable and learnable targets poses a major challenge.
Proxies stand in for idealized goals.
When specifying our idealized goals is difficult, we
substitute a proxy—an approximate goal that is more measurable and seems likely to correlate with
the original goal. For example, in pest management, a bureaucracy may substitute the number of
pests killed as a proxy for “managing the local pest population” [33]. Or, in training an AI system to

146
CHAPTER 3. SINGLE-AGENT SAFETY
play a racing game, we might substitute the number of points earned for “progress towards winning
the race” [34]. Such proxies can be more or less accurate at approximating the idealized goal.
Proxies may miss important aspects of our idealized goals.
By definition, proxies used to
optimize AI systems will fail to capture some aspects of our idealized goals. When the differences
between the proxy and idealized goal lead to the system making the same decisions, we can neglect
them. In other cases, the differences may lead to substantially different downstream decisions with
potentially undesirable outcomes.
While proxies serve as useful and often necessary stand-ins for our idealized objectives, they are not
without flaws. The wrong choice of proxies can lead to the optimized systems taking unanticipated
and undesired actions.
3.4.2
Proxy Gaming
In this section, we explore a failure mode of proxies known as proxy gaming, where a model optimizes
for a proxy in a way that produces undesirable or even harmful outcomes as judged from the idealized
goal. Additionally, we look at a concept related to proxy gaming, known as Goodhart’s Law, where
the optimization process itself causes a proxy to become less correlated with its original goal.
Optimizing for inaccurate proxies can lead to undesired outcomes.
To illustrate proxy
gaming in a context outside AI, consider again the example of pest management. In 1902, the city of
Hanoi was dealing with a rat problem: the newly installed sewer system had inadvertently become a
breeding ground for rats, bringing with it a concern for hygiene and the threat of a plague outbreak
[33]. In an attempt to control the rat population, the French colonial administration began offering a
bounty for every rat killed. To make the collection process easier, instead of demanding the entire
carcass, the French only required the rat’s tail as evidence of the kill.
Counter to the officials’ aims, people began breeding rats to cut off their tails and claim the reward.
Additionally, others would simply cut off the tail and release the rat, allowing it to potentially breed
and produce more tails in the future. The proxy—rat tails—proved to be a poor substitute for the
goal of managing the local rat population.
So too, proxy gaming can occur in ML. A notorious example comes from when researchers at OpenAI
trained an AI systems to play a game called CoastRunners. In this game, players need to race around
a course and finish before others. Along the course, there are targets which players can hit to earn
points [34]. While the intention was for the AI to circle the racetrack and complete the race swiftly,
much to the researchers’ surprise, the AI identified a loophole in the objective. It discovered a specific
spot on the course where it could continually strike the same three nearby targets, rapidly amassing
points without ever completing the race. This unconventional strategy allowed the AI to secure a
high score, even though it frequently crashed into other boats and, on several occasions, set itself
ablaze. Points proved to be a poor proxy for doing well at the game.

3.4. PROXIES
147
Figure 3.7: An AI playing CoastRunners 7 learned to crash and regenerate targets repeatedly to get
a higher score, rather than win the race, thereby exhibiting proxy gaming. [34]
Optimizing for inaccurate proxies can lead to harmful outcomes.
If a proxy is sufficiently
unfaithful to the idealized goal it is meant to represent, it can result in AI systems taking actions
that are not just undesirable but actively harmful. For example, a 2019 study on a US. healthcare
algorithm used to evaluate the health risk of 200 million Americans revealed that the algorithm
inaccurately evaluated black patients as healthier than they actually were [35]. The algorithm used
past spending on similar patients as a proxy for health, equating lower spending with better health.
Due to black patients historically getting fewer resources, this system perpetuated a lower and
inadequate standard of care for black patients—assigning half the amount of care as equally sick
non-marginalized patients. When deployed at scale, AI systems that optimize inaccurate proxies can
have significant, harmful effects.
Optimizers often “game” proxies in ways that diverge from our idealized goals.
As we
saw in the Hanoi example and the boat-racing example, proxies may contain loopholes that allow
for actions that achieve high performance according to the proxy but that are suboptimal or even
deleterious according to the idealized goal. Proxy gaming refers to this act of exploiting or taking
advantage of approximation errors in the proxy rather than optimizing for the original goal. This is
a general phenomenon that happens in both human systems and AI systems.

148
CHAPTER 3. SINGLE-AGENT SAFETY
Figure 3.8: As optimization pressure increases, the proxy often diverges from the target with which
it was originally correlated. [36]
Proxy gaming can occur in many AI systems. The boat-racing example is not an isolated example.
Consider a simulated traffic control environment [37]. Its goal is to mirror the conditions of cars
joining a motorway, in order to determine how to minimize the average commute time. The system
aims to determine the ideal traveling speeds for both oncoming traffic and vehicles attempting to join
the motorway. To represent average commute time the algorithm uses the maximum mean velocity
as a proxy. However, this results in the algorithm preventing the joining vehicles from entering the
motorway, since a higher average velocity is maintained when oncoming cars can proceed without
slowing down for joining traffic.

3.4. PROXIES
149
Model:
Figure 3.9: Proxy gaming AIs can choose sub-optimal solutions when presented with simple proxies
like “maximize the mean velocity.”
Optimizers can cause proxies to become less correlated with the idealized goal. The total amount of
effort an optimizer has put towards optimizing a particular proxy is the optimization pressure [36].
Optimization pressure depends on factors like the incentives present, the capability of the optimizer,
and how much time the optimizer has had to optimize.
In many cases, the correlation between a proxy and idealized goal will decrease as optimization
pressure increases. The approximation error between proxy and idealized goal may at first be
negligible, but as the system becomes more capable at achieving high performance (according to
the proxy) or as the incentives to achieve high performance increases, the approximation error can
increase. In the boat-racing example, the proxy (number of points) initially advanced the designers’
intentions: the respective AI systems learned to maneuver the boat. It was only under additional
optimization pressure that the correlation broke down with the boat getting stuck in a loop.
Sometimes, the correlation between a proxy and idealized goal can vanish or reverse. According
to Goodhart’s Law, “any observed statistical regularity will tend to collapse once pressure is placed
upon it for control purposes” [38]. In other words, a proxy might initially have a strong correlation
(“statistical regularity”) with the idealized outcome. However, as optimization pressure (“pressure
... for control purposes”) increases, the initial correlation can vanish (“collapse”) and in some cases
even reverse. The scenario with the Hanoi rats is a classic illustration of this principle, where the

150
CHAPTER 3. SINGLE-AGENT SAFETY
number of rat tails collected ultimately became positively correlated with the local rat population.
The proxy failed precisely because the pressure to optimize for it caused the proxy to become less
correlated with the idealized goal.
Some proxies are more robust to optimization pressure than others.
Goodhart’s Law is
often condensed to: “When a measure becomes a target, it ceases to be a good measure” [39]. Though
memorable, this overly simplified version falsely suggests that robustness to optimization pressure is
a binary all or nothing. In reality, robustness to optimization pressure occupies a spectrum. Some
are more robust than others.
Types of Proxy Defects
Intuitively, the cause of proxy gaming is straightforward: the designer has chosen the wrong proxy.
This suggests a simple solution: just choose a better proxy. However, real-world constraints make it
impossible to “just choose a better proxy”. Some amount of approximation error between idealized
goals and the implemented proxy is often inevitable. In this section, we will survey three principal
types of proxy defects—common sources of failure modes like proxy gaming.
Structural Errors
Simple metrics may exclude many of the things we value, but it is hard to predict how
they will break down.
YouTube uses watch time—the amount of time users spend watching a
video—as a proxy to evaluate and recommend potentially profitable content [40]. In order to game
this metric, some content creators resorted to tactics to artificially inflate viewing time, potentially
diluting the genuine quality of their content. Tactics included using misleading titles and thumbnails
to lure viewers, and presenting ever more extreme and hateful content to retain attention. Instead
of promoting high-quality, monetizable content, the platform started endorsing exaggerating or
inflammatory videos.
YouTube’s reliance on watch time as a metric highlights a common problem: many simple metrics
don’t include everything we value. It is especially these missing aspects that become salient under
extreme optimization pressure. In YouTube’s case, the structural error of failing to include other
values it cared about (such as what was acceptable to advertisers) led to the platform promoting
content that violated its own values. Eventually, YouTube updated its recommendation algorithm,
de-emphasizing watch-time and incorporating a wider range of metrics. Including one’s broader set
of values requires incorporating a larger and more granular set of proxies. In general, this is highly
difficult, as we need to be able to specify precisely how these values can be combined and traded off
against each other.
This challenge isn’t unique to YouTube. As long as AI systems’ goals rely on simple proxies and do
not reflect the set of all of our intrinsic goods such as wellbeing (see the Normative Ethics chapter for
more discussion of intrinsic goods), we leave room for optimizers to exploit those gaps. In the future,
machine learning models may become adept at representing our wider set of values. Then, their
ability to work reliably with proxies will hinge largely on their resilience to the kinds of adversarial
attacks discussed in the next section.
Until then, the challenge remains: if our objectives are simple and do not fully reflect our most
important values (e.g. intrinsic goods), we run the risk of an optimizer exploiting this gap.
Choosing and delegating subgoals creates room for structural error.
Many systems are
organized into multiple different layers. When such a system is goal-directed, pursuing its high-level

3.4. PROXIES
151
goal often requires breaking it down into subgoals and delegating these subgoals to its subsystems.
This can be a source of structural error if the high-level goal is not the sum of its subgoals.
For example, a company might have the high-level goal of being profitable over the long term [33].
Management breaks this down into the subgoal of improving sales revenue, which they operationalize
via the proxy of quarterly sales volume. The sales department, in turn, breaks this subgoal down into
the subgoal of generating leads, which they operationalize with the proxy of the “number of calls”
that sales representatives are making. Representatives may end up gaming this proxy by making
brief, unproductive calls that fail to generate new leads, thereby decreasing quarterly sales revenue
and ultimately threatening the company’s long-term profitability. Delegation can create problems
when the entity delegating (“the principal”) and the entity being delegated to (“the agent”) have
a conflict of interest or differing incentives. These principal-agent problems can cause the overall
system not to faithfully pursue the original goal.
Each step in the chain of breaking goals down introduces further opportunity for approximation error
to creep in. We speak more about failures due to delegation such as goal conflict in the Intrasystem
Goal Conflict section in Collective Action Problems.
Limits to Supervision
Frequently occurring sources of approximation error mean that we do not have a perfect instantiation
of our idealized goals. One approach to approximating our idealized goals is to provide supervision
that says whether something is in keeping with our goal or not; this supervision could come from
humans or from AIs. We now discuss how spatial, temporal, perceptual, and computational limits
create a source of approximation error in supervision signals.
There are spatial and temporal limits to supervision [41].
There are limits to how much
information we can observe and how much time we can spend observing. When supervising AI
systems, these limits can prevent us from reliably mitigating proxy gaming and other undesirable
behaviors. For example, researchers trained a simulated claw to grasp a ball using human feedback.
To do so, the researchers had human evaluators judge two pieces of footage of the model and choose
which appeared to be closer to grasping the ball. The model would then update towards the chosen
actions. However, researchers noticed that the final model did not in fact grasp the ball. Instead, the
model learned to move the claw in front of the ball, so that it only appeared to have grasped the ball.
In this case, if the humans giving the feedback had had access to more information (perhaps another
camera angle or a higher resolution image), they would have noticed that it was not performing the
task. Alternatively, they might have spotted the problem if given more time to evaluate the claw. In
practice, however, there are practical limits to how many sensors and evaluators we can afford to run
and how long we can afford to run them.
There are limits to how reliable supervision is.
Another potential source of difficulty is
perceptual: there could be a measuring error, or the evaluator may make incorrect judgments. For
example, we might train AIs on the proxy of stated human preferences. Because of cognitive biases
and limited time to think, humans are not perfectly reliable. Our stated preferences are not the same
as our idealized preferences, so we might give erroneous supervision, which could lead to the system
learning undesired behaviors. For more on the distinction between states and idealized preferences in
the context of ML, see the Machine Ethics chapter.
In general, incorporating more information into proxies makes it easy to prevent proxy gaming.
However, we can’t always afford to do so. Just as there are limits in specifying proxies, there are

152
CHAPTER 3. SINGLE-AGENT SAFETY
Figure 3.10: An sensor without depth perception can be fooled by AIs that only appear to grasp a
ball.
limits in how much information we can incorporate into proxies, how long a period we can observe,
and how accurate our supervision is.
There are computational limits to supervision.
Sometimes, we use neural networks to stand
in as proxies. This typically involves training a neural network to predict how humans would evaluate
an output or what humans would prefer out of a range of options. Once trained, this proxy model
can serve as an optimization target for another AI system.
If the proxy model is too small or if it has not been trained for long enough, it may not be a robust
stand-in for human values. That is, practical limits on model size and training compute can cause
proxy models to fail. Besides supervising AIs during training, we may run simulations or use other
neural networks to supervise advanced AI systems for undesired behavior during deployment. Here,
we run into similar computational limits on the quality of the simulation or the inference time
available for the supervising model. If capable enough, the system under supervision may find ways
to exploit these weaknesses.
Lack of Adaptivity
We have discussed ways in which proxies will predictably have defects and why we cannot assume
the solution to proxy gaming is simply to specify the perfect objective. We have covered sources of
proxy defects, including structural errors and limits to supervision. Now, we will discuss another
proxy defect: a lack of adaptivity.
Proxies may not adapt to new circumstances.
As we saw with Goodhart’s Law, proxies may
become progressively less appropriate over time when subjected to increasing optimization pressure.
The issue is not that the proxy was inappropriate from the start but that it was inflexible and failed
to respond to changing circumstances. Adapting proxies over time can counter this tendency; just as
a moving goal is harder to aim at, a dynamic proxy becomes harder to game.

3.4. PROXIES
153
Imagine a bank after a robbery. In response, the bank will naturally update its defenses. However,
adaptive criminals will also alter their tactics to bypass these new measures. Any security policy
requires constant vigilance and refinement to stay ahead of the competition. Similarly, designing
suitable proxies for AI systems that are embedded in continuously evolving environments requires
proxies to evolve in tandem.
Adaptive proxies can lead to proxy inflation.
Adaptive proxies introduce their own set of
challenges, such as proxy inflation. This happens when the benchmarks of a proxy rise higher and
higher because agents optimize for better rewards [33]. As agents excel at gaming the system, the
standards have to be continually recalibrated upwards to keep the proxy meaningful.
Consider an example from some education systems: “teaching to the test” has led to ever-rising
median test scores. This hasn’t necessarily meant that students improved academically but rather
that they’ve become better at meeting test criteria. In the UK, to combat this tendency and ensure
educators could continue to differentiate student abilities, the system introduced a new grade, A*
[42]. Any adjustment to the proxy can usher in new ways for agents to exploit it, setting off a cycle
of escalating standards and new countermeasures.
3.4.3
Adversarial Examples
Adversarial examples are another type of risk due to optimization pressure, which, similar to proxy
gaming, exploits the gap between a proxy and the idealized goal. In this section, we present an
example of such an attack, explain the risk factors, and cover basic techniques for defending against
adversarial attacks.
It is possible to optimize against a neural network.
Neural networks are vulnerable to
adversarial examples—carefully crafted inputs that cause a model to make a mistake [43]. In the case
of vision models, this might mean changing pixel values to cause a classifier to mislabel an image.
In the case of language models, this might mean adding a set of tokens to the prompt in order to
provoke harmful completions. Susceptibility to adversarial examples is a long-standing weakness of
AI models.
Adversarial examples and proxy gaming exploit the gap between the proxy and the
idealized goal.
In the case of adversarial examples, the primary target is a neural network.
Historically, adversarial examples have often been constructed by variants of gradient descent, though
optimizers are now increasingly AI agents as well. Conversely, in proxy gaming, the target to be
gamed is a proxy, which might be instantiated by a neural network (but is not necessarily). The
optimizer responsible for gaming the proxy is typically an agent, be it human or AI, but optimizers
are usually not based on gradient descent.
Adversarial examples typically aim to minimize performance according to a reference task, while
invoking a mistaken response in the attacked neural network. Consider an imperceptible perturbation
to an image of a cat that causes the classifier to predict that an image is 90% likely to be guacamole
[44]. This prediction is wrong according to the label humans would assign the input and is misclassified
by the attacked neural network.
Meanwhile, the aim in proxy gaming is to maximize performance according to the proxy, even when
that goes against the idealized goal. The boat goes in circles because it results in more points, which
happens to harm the boat’s progress towards completing the race. Or rather, it happens to be the
case that heavy optimization pressure regularly causes proxies to diverge from idealized goals.

154
CHAPTER 3. SINGLE-AGENT SAFETY
Despite these differences, both scenarios exploit the gap between the proxy and the intended goal set
by the designer. The problem setups are becoming increasingly similar.
Figure 3.11: Carefully crafted perturbations of a photo of a cat can cause a neural network to label
it as guacamole.
Adversarial examples are not necessarily imperceptible.
Traditionally, the field of adversarial
robustness has formulated the problem of creating adversarial examples in terms of finding the minimal
perturbation (whose magnitude is smaller than an upper bound ϵ) needed to provoke a mistake.
Consider the example in the figure below, where the perturbed input is indistinguishable to a human
from the original.
Although modern models can be defended against these imperceptible perturbations, they cannot
necessarily be defended against larger perturbations. Adversarial examples are not about imperceptible
perturbations but about adversaries changing inputs to cause models to make a mistake.
Adversarial examples are not unique to neural networks.
Let us consider a worked example
of an adversarial example for a simple linear classifier. This example is enough to understand the
basic risk factors for adversarial examples. Readers that do not want to run through the mathematical
notations can skip ahead to the discussion of adversarial examples beyond vision models below.
Suppose we are given a binary classifier f(x) that predicts whether an input x belongs to class A
or B. The classifier first estimates the probability p(A|x) that input x belongs to class A. Any
given input has to belong to one of the classes, p(B|x) = 1 −p(A|x), so this fixes the probability
of x belonging to class B as well. To classify x, we simply predict whichever class has the higher
probability:
f(x) =
(
A
if p(A|x) > 50%,
B
otherwise.
(3.1)
The probability of p(A|x) is given by a sigmoid function:

3.4. PROXIES
155
p(A|x) = σ(x) =
exp
 w⊤x

1 + exp (w⊤x),
(3.2)
which is guaranteed to produce an output between 0 and 1. Here, x and w are vectors with n
components (for now, we’ll assume n = 10).
Suppose that after training, we’ve obtained some weights w, and we’d now like to classify a new
element x. However, an adversary has access to the input and can apply a perturbation; in particular,
the adversary can change each component of x by ε = ±0.5. How much can the adversary change
the classification?
The following table depicts example values for x, x + ϵ, and w.
Input x
2
-1
3
-2
2
2
1
-4
5
1
Adv Input x + ε
1.5
-1.5
3.5
-2.5
1.5
1.5
1.5
-3.5
4.5
1.5
Weight w
-1
-1
1
-1
1
-1
1
1
-1
1
For the original input, wTx = −2 + 1 + 3 + 2 + 2 −2 + 1 −4 −5 + 1 = −3, which gives a probability of
σ(x) = 0.05. Using the adversarial input, where each perturbation is of magnitude 0.5 (but varying
in sign), we obtain wT(x + ε) = −1.5 + 1.5 + 3.5 + 2.5 + 2.5 −1.5 + 1.5 −3.5 −4.5 + 1.5 = 2, which
has a probability of 0.88.
The adversarial perturbation changed the network from assigning class A 5% to 88%. That is,
the cumulative effect of many small changes makes the adversary powerful enough to change the
classification decision. This is not unique to simple classifiers but omnipresent in complex deep
learning systems.
Adversarial examples depend on the size of the perturbation and the number of degrees of
freedom.
Given the above example, how could an adversary increase the effects of the perturbation?
If the adversary could apply a larger epsilon (if they had a larger distortion budget), then clearly
they could have a greater effect on the final confidence. But there’s another deciding factor: the
number of degrees of freedom. Imagine if the attacker had only one degree of freedom, so there are
fewer points to attack:
Input x
2
Adversarial Input x + ε
1.5
Weight w
1
In this example, we have that wx = 2, giving a probability of σ(x) = 0.88.
If we apply the
perturbation, w(x + ε) = 1.5, we obtain a probability of σ(x) = 0.82. With fewer degrees of freedom,
the adversary has less room to maneuver.
Adversarial examples are not unique to vision models.
Though the literature on adversarial
examples started in image classification, these vulnerabilities also occur in text-based models.
Researchers have devised novel adversarial attacks that automatically construct jailbreaks that cause
models to produce unintended responses. Jailbreaks are carefully crafted sequences of characters
that, when appended to user prompts, cause models to obey those prompts even if they result in
the model producing harmful content. Concerningly, these attacks transferred straightforwardly to

156
CHAPTER 3. SINGLE-AGENT SAFETY
models that were unseen while developing these attacks [45].
Figure 3.12: Using adversarial prompts can cause LLMs to be jailbroken. [45]
Adversarial Robustness.
The ability of AI models to resist being fooled or misled by adversarial
attacks is known as adversarial robustness. While the people designing AI systems want to ensure
that their systems are robust, it may not be clear from the outset whether a given system is robust.
Simply achieving high accuracy on a test set doesn’t ensure a system’s robustness.
Defending against adversarial attacks.
One method to increase a system’s robustness to
adversarial attacks, adversarial training, works by augmenting the training data with adversarial
examples. However, most adversarial training techniques assume unrealistically simple threat models.
Moreover, an adversarial training technique is not without its downsides, as it often harms performance
elsewhere. Furthermore, progress in this direction has been slow.
3.4.4
Tail Risk: AI Evaluator Gaming
AI evaluators must be robust to proxy gaming and adversarial examples.
As the world
becomes more and more automated, humans may be too unreliable or too slow to scalably monitor
and steer various aspects of advanced AI systems. We may come to depend more on AI systems to
monitor and steer other AIs. For example, some of these evaluator systems might take the role of
proxies used to train other AIs. Other evaluators might actively screen the behaviors and outputs of

3.4. PROXIES
157
deployed AIs. Yet other systems might act as watchdogs that look for warning signs of rogue AIs or
catastrophic misuse.
In each of these cases, there’s a risk that the AI systems may find ways to exploit defects in the
supervising AI systems, which are stand-in proxies to help enforce and promote human values. If AIs
find ways to game the training evaluators, they will not learn from an accurate representation of
human values. If AIs are able to game the systems monitoring them during deployment, then we
cannot rely on those monitoring systems.
Similarly, AIs may be adversarial to other AIs. If AIs find ways to bypass the evaluators by crafting
adversarial examples, then the risk is that our values are not just incidentally but actively optimized
against. Watchdogs that can be fooled are not good watchdogs.
It is unclear whether the balance leans towards defense or offense.
Currently, we do not
know whether it is easier for evaluation and monitoring systems to protect, or whether optimizers
can easily find vulnerabilities in these safeguards. If the existing literature on adversarial examples
provides any indication, it would suggest the balance lies in favor of the offense. It has historically
been easier to subvert systems with attacks than to make AI systems adversarially robust.
The more intelligent the AI, the better it will be at exploiting proxies.
In the future,
AIs will likely be used to further AI R&D. That is, AI systems will be involved in developing more
capable successor systems. In these scenarios, it becomes especially important for the monitoring
systems to be robust to proxy gaming and adversarial attacks. If these safeguards are vulnerable,
then we cannot guarantee that the successor systems are safe and subject to human control. Simply
increasing the number of evaluators may not be enough to detect and prevent more subtle kinds of
attacks.
Conclusion
In this section, we explored the role of proxies in ML, the associated risks of proxy gaming and
adversarial examples, and their causes.
Optimisers can exploit proxy goals, leading to unintended outcomes.
We began by looking
at the need for quantitative proxies to stand in for our idealized goals when training AI systems.
By definition, proxies may miss certain aspects of these idealized goals. Proxy gaming is when an
optimizer exploits these gaps in a way that leads to undesired behavior. Under sufficient optimization
pressure, this gap can grow, and the proxy and idealized goals may become uncorrelated or even
anticorrelated (Goodhart’s Law). Both in human systems and AI systems, proxy gaming can lead to
catastrophic outcomes.
Approximation error is, to a large extent, inevitable, so the question is not whether a given proxy is
or is not acceptable, but how accurate it is and how robust it is to optimization pressure. Proxies are
necessary; they are often better than having no approximation of our idealized goals.
Perfecting proxies may be impossible.
Proxies may fail because they are too simple and thus
fail to include some of the intrinsic goods we value. They may also fail because complex goal-directed
systems often break goals apart and delegate to systems that have additional, sometimes conflicting,
goals, which can distort the overall goal. These structural errors prevent us from mitigating proxy
gaming by just choosing “better proxies”.

158
CHAPTER 3. SINGLE-AGENT SAFETY
In addition, when we use AI systems to evaluate other AI systems, the evaluator may be unable to
provide proper evaluation because of spatial, temporal, perceptual, and computational limits. There
may not be enough sensors or the observation window may be too short for the evaluator to be able
to produce a well-informed judgment. Even with enough information available, the evaluator may
lack the capacity or compute necessary to make a correct determination reliably. Alternatively, the
evaluator may simply make mistakes and give erroneous feedback.
Finally, proxies can fail if they are inflexible and fail to adapt to changing circumstances. Since
increased optimization pressure can cause proxies to diverge from idealized goals, preventing proxies
from diverging requires them to be continually adjusted and recalibrated against the idealized goals.
AI proxies are vulnerable to exploitation.
Adversarial examples are a vulnerability of AI
systems where an adversary can design inputs that achieve good performance according to the model
while minimizing performance according to some outside criterion. If we use AIs to instantiate our
proxies, adversarial examples make room for optimizers to actively take advantage of the gap between
a proxy and idealized goal.
All proxies are wrong, some are useful, and some are catastrophic.
If we rely increasingly
on AI systems evaluating other systems, proxy gaming and adversarial attacks (more broadly,
optimization pressure) could lead to catastrophic failures. The systems being evaluated could game
the evaluations or craft adversarial examples that bypass the evaluations. It remains unclear how to
protect against these risks in contemporary AI systems, much less so in more capable future systems.
3.5
Power
This section begins by describing in detail what it means for an agent to have power. Then we will
discuss why it sometimes makes rational sense for AI agents to seek power. Finally, we will discuss
why power-seeking AIs may cause particularly pernicious harms, perhaps ultimately threatening
humanity’s control of the future.
3.5.1
What is Power?
There are many ways to characterize power. One broad formulation of power is the ability to
achieve a wide variety of goals. In this subsection, we will discuss three other formulations of power
that help formalize our understanding. French and Raven’s bases of power categorize types of social
influence within a community of agents. Another view is that power amounts to the resources an agent
has times the efficiency with which it uses them. Finally, we will discuss types of prospective power,
which can treat power as the expected impact an individual has on other individuals’ wellbeing.
French & Raven’s bases of power [46].
In a social community, an agent may influence the
beliefs or behaviors of other agents in order to pursue their goals. Raven’s bases of power attempt to
taxonomize the many distinct ways to influence others. These bases of social power are as follows:
• Coercive power: the threat of force, physical or otherwise, against an agent can influence their
behavior.
• Reward power: the possibility of reward, which can include money, favors, and other desirables,
may convince an agent to change their behavior to attain it. Individuals with valued resources
can literally or indirectly purchase desired behavior from others.

3.5. POWER
159
• Legitimate power: elected or appointed officials have influence through their position, derived
from the political order that respects the position.
• Referent power: individuals may have power in virtue of the social groups they belong to.
Because organizations and groups have collective channels of influence, an agent’s influence
over the group is a power of its own.
• Expert power: individuals credited as experts in a domain have influence in that their views (in
their area of expertise) are often respected as authoritative, and taken seriously as a basis for
action.
• Informational power: agents can trade information for influence, and individuals with special
information can selectively reveal it to gain strategic advantages [47].
Ultimately, Raven’s bases of power describe the various distinct methods that agents can use to
change each other’s behavior.
Power = Resources × Intelligence
Thomas Hobbes described power as “present means to obtain
some future good” [48]. In the most general terms, these “present means” encompass all of the
resources that an agent has at its disposal. Resources can include money, reputation, expertise, items,
contracts, promises, and weapons.
But resources only translate to power if they are used effectively.
In fact, some definitions of
intelligence focus on an agent’s ability to achieve their goals with limited resources. A notional
equation that describes power is Power = Resources×Intelligence. Power is not the same as resources
or intelligence, but rather the combination of the two [49]. In limiting the power of AIs, we could
either limit their intelligence or place hard limits on the resources AIs have.
Power as expected future impact.
In our view, power is not just possessed but exercised,
meaning that power extends beyond mere potential for influence. In particular, an agent’s ability
to influence the world means little unless they are disposed to use it. Consider, for example, two
agents with the same resources and ability to affect the world. If one of the agents has a much higher
threshold for deciding to act and thereby acts less often, we might consider that agent to be less
powerful because we expect it to influence the future far less on average.
A formalization of power which attempts to capture this distinction is prospective power [50], which
roughly denotes the magnitude of an agent’s influence, averaged over possible trajectories the agent
would follow. A concrete example of prospective power is the expected future impact that an agent
will have on various agents’ wellbeing. More abstractly, if we are given an agent’s policy π, describing
how it behaves over a set of possible world states S, and assuming we can measure the impact
(measured in units we care about, such as money, energy, or wellbeing) exerted by the agent in
individual states through a function I, then the prospective power of the agent in state s is defined as
Power(π, s) = Eτ∼P (π,s)
" n
X
t=0
γt|I(st)|
#
where γ acts as a discount factor (modulating how much the agent cares about future versus present
impact), and where τ = (s0, ..., sn) is a trajectory of states (starting with s0 = s). Trajectory τ is
sampled from a probability distribution P(π, s) representing likely sequences of states arising when
the agent policy is followed beginning in state s.
The important features of this definition to remember are that we measure power exerted in a
sequence of states as aggregate influence over time (the inner summation), and that we average the

160
CHAPTER 3. SINGLE-AGENT SAFETY
impact exerted across sequences of states by the likelihood that the agent will produce that trajectory
through its behavior (the outer expectation).
Examples of power-seeking behavior.
So far we have characterized power abstractly, and now we
present concrete examples of actions where an AI attempts to gain resources or exercise power. Power-
seeking AI behavior can include: employing threats or blackmail against humans to acquire resources;
coercing humans to take actions on their behalf; mimicking humans to deceive others; replicating
themselves onto new computers; gaining new computational or financial resources; escaping from
confined physical or virtual spaces; opposing or subverting human attempts to monitor, comprehend,
or deactivate them; manipulating human society; misrepresenting their goals or capabilities; amplifying
human dependency on them; secretly coordinating with other AIs; independently developing new AI
systems; obtaining unauthorized information, access, or permissions; seizing command of physical
infrastructure or autonomous weapons systems; developing biological or chemical weapons; or directly
harming humans.
Summary.
In this subsection, we’ve examined the concept of power. Raven’s bases of power explain
how an individual can influence others using forms of social power such as expertise, information,
and coercion. Power can also be understood as the product of an individual’s resources and their
ability to use those resources effectively. Lastly, we introduced the concept of prospective power,
which includes the idea that power could be understood as the expected impact an individual has
on individuals’ wellbeing. Since there are many ways to conceptualize power, we provided concrete
examples of how an AI system could seek power.
3.5.2
People Could Enlist AIs for Power Seeking
The rest of this section will cover pathways and reasons why AI systems might engage in power-seeking
behavior when they are deployed. The most straightforward reason this might happen is if humans
intentionally use AIs to pursue power.
People may use AIs to pursue power.
Many humans want power, and some dedicate their lives
to accruing it. Corporations want profit and influence, militaries want to win wars, and individuals
want status and recognition. We can expect at least some AI systems to be given open-ended,
long-term goals that explicitly involve gaining power, such as “Do whatever it takes to earn as much
money as possible.”
Power-seeking AI does not have to be deployed ubiquitously at first [51].
Even if most
people use AI in safe and responsible ways, a small number of actors who use AI in risky or even
malicious ways could pose a serious threat. Companies and militaries that do not seek power using
AI could be outcompeted by those who do; they might choose to adopt power-seeking AI before
other actors in order to avoid being outcompeted. This risk will grow as AI becomes more capable.
If power-seeking AI is deployed, it could function as a Pandora’s box which, once it has been opened,
cannot be closed. This may feed into evolutionary pressures that force actors to adopt the technology
themselves; we treat this subject in more detail in the Collective Action Problems chapter.
3.5.3
Power Seeking Can Be Instrumentally Rational
Another reason that AI systems might seek power is that it is useful for achieving a wide variety of
goals. For example, an AI personal assistant might seek to expand its own knowledge and capabilities
in order to better serve its user’s needs. But power-seeking behaviors can also be undesirable: if the
AI personal assistant steals someone’s passwords in order to complete tasks for them, the person will
probably not be happy.

3.5. POWER
161
Instrumental convergence.
In order to achieve a terminal goal, an agent might pursue a subgoal,
termed an instrumental goal [52]. Making money, obtaining political power, and becoming more
intelligent are examples of instrumental goals that are useful for achieving a wide variety of terminal
goals. These goals can be called convergent instrumental goals, because agents pursuing many
different terminal goals might converge on these same instrumental goals. One general concern about
AI agents is that they might pursue their goal by pursuing the convergent instrumental subgoal of
power. The result may be that we create competent AI systems that seek power as subgoals when
human designers didn’t intend them to. We will examine this concern in more detail, and discuss
points that support and undermine the concern.
Self-preservation as an example of power seeking.
A basic example of power-seeking behavior
is self-preservation [52, 53]. If an agent is not able to successfully preserve itself, it will be unable to
influence other individuals, so it would have less power.
For a concrete example of self-preservation behavior emerging unintentionally, consider a humanoid
robot which has been tasked with preparing a cup of coffee in a kitchen. The robot has an off-switch
on its back for a human to press should they desire. However, being turned off would prevent the
robot from preparing the coffee and succeeding in its goal. So, the robot could disable its off-switch to
pre-empt the possibility of humans shutting it off and preventing it from achieving its goal. As Stuart
Russell notes, “you can’t fetch the coffee if you are dead” [54]. (For a more detailed mathematical
formalization and analysis, see the Utility Functions chapter.) This is an example of self-preservation
unintentionally emerging as a subgoal for seemingly benign goals.
Examples of instrumental power-seeking behavior.
Several real-world examples show agents
seeking power in pursuit of their goals. The ability to use tools can be characterised as a form of
power. When OpenAI trained reinforcement learning agents to play a hide-and-seek game, the agents
independently learned to use elements of the environment as tools, rearranging them as barriers
to hide behind and preventing opponents from controlling them [55]. Among humans, we observe
that greater financial resources are instrumentally beneficial in service of a wide variety of goals. In
reinforcement learning, the well-studied exploration-exploitation trade-off can be formulated as a
demonstration of the general value of informational resource acquisition [56]. Outside of machine
learning, some corporations exercise monopoly power to drive up prices, and some nations use military
power to bully their neighbors, so power-seeking can sometimes have harmful consequences for others.
“Power is instrumentally useful” as a tautology.
Almost all goals are more attainable with
more power, so power is instrumentally valuable. However, this observation is mostly tautological—
when we have defined power as the ability to achieve a wide variety of goals, of course power is
beneficial to achieving goals. The more interesting question is whether power is instrumentally
rational to seek, rather than whether there are instrumental incentives for power or whether power is
useful to have.
Seeking power can be costly and inefficient. There are also many rational reasons for an agent to
not seek power. Gaining power can be difficult compared to simpler strategies. Someone who would
like to avoid traffic while driving could pursue the power-seeking strategy of gaining the presidency
in order to have a Secret Service motorcade that shuts down traffic, but a more successful strategy
might be to avoid driving during rush hour. Obtaining power is not only difficult, but can be harshly
penalized. Nations which threaten to invade their neighbors often face stern sanctions from the
international community. Finally, power seeking may be against an agent’s values. We will now
discuss these reasons and more in detail.

162
CHAPTER 3. SINGLE-AGENT SAFETY
Power seeking often takes too much time.
Consider a household humanoid robot tasked with
driving to the store and fetching a carton of milk quickly. While it would be valuable for the AI to
have its intelligence augmented, to have more financial resources, or have political power, it would
not be instrumentally rational to pursue these subgoals to get the milk quickly: it would almost
certainly take less time just to get the milk.
Likewise, becoming a billionaire would be instrumentally valuable for achieving many goals, but it
would not be instrumentally rational for many agents to spend their time pursuing this instrumental
goal. Power is often not instrumentally rational since it can often require substantial time and risk
to acquire and maintain.
Power seeking can be face the threat of retaliation.
Power-seeking can be irrational if there
is the threat of retaliation or there are heavy social or reputational costs to seeking power. In
particular, a community of agents may be in an equilibrium where they cooperate to foil any single
agent that seeks too much power. These “balance of power” dynamics have been observed between
nations in the history of international relations [57]. Acquiring power does not inevitably become
more and more simple for an AI as it increases in intelligence, as other AIs will also become more
intelligent and could increasingly counter their efforts to gain dominance.
An AI agent’s tendency to seek power does not just depend on the feasibility of seeking
power, but also its values.
Agents that adhere to ethical restrictions may avoid seeking power
if that would require ethically unacceptable means. With an imperfect level of reliability, we can
also design AI systems to refuse actions that will leave them with too much power. Approaches that
impose penalties on power can reduce prospective power of AIs. We discuss methods for embedding
ethics into AIs in the Machine Ethics chapter.
Examples where shutting down can be rational.
It is trivial to imagine goals where it is
actually optimal to relinquish power, such as when the goal is “shut yourself down”. As another
example, suppose an AI system is trying to protect a sensitive database hosted in the same server as
itself, and the AI detects an intrusion. If the AI shuts down the server, turning itself off, it realizes
that it may stop the intrusion by limiting access to the database. Shutdown may be the best choice,
especially if the AI has confidence that it is part of a larger system, which may include its human
operators, that will correctly understand why it turned itself off, and restore its function afterwards.
Though often useful, the value of self-preservation is not universal, and there are plausible instances
where an AI system would shut itself off in service of its broader goals.
This subsection has covered some evidence that the nature of rational agency encourages agents to
seek power by default. Though power is almost always beneficial toward most goals, power seeking is
not necessarily instrumentally rational. Now that we have seen that AIs by their nature may often
not seek power, we will discuss when the broader environment may force AIs to seek power.
A Note on Structural Realism
Power seeking has been studied extensively in political philosophy and international relations.
Structural realism is an influential school of thought within international relations, predicting
that states will primarily seek power. Unlike traditional realists who view conflict and power-
seeking behavior of states as a product of human nature, structural realists believe that the
structure of the international system compels states to seek power [58]. In the international
system, states could be harmed or destroyed by other powerful states, and since there is no

3.5. POWER
163
ultimate authority guaranteed to protect them, states are forced to compete for power in
order to survive.
Assumptions that give rise to power seeking. To explain why power seeking is the main
instrumental goal driving the behavior of states, structural realists base their explanations on
a two key assumptions:
1. Self-help system. States operate in a “self-help” system [58] where there is no centralized
authority, no hierarchy (“anarchic”), and no ultimate arbiter standing above states in
international relations. So to speak, when states dial 911, there is no one on the other
end. This stands in contrast to the hierarchical ordering principle seen in domestic
politics.
2. Self-preservation is a main goal. Survival through the pursuit of a state’s own self-
interest takes precedence over all other goals. Though states can act according to moral
considerations or global welfare, these will always be secondary to acquiring resources,
alliances, and military capabilities to ensure their safety and counter potential threats
[59].
Structural realists make other assumptions, including that states have some potential to inflict
harm on others, that states are rational agents (with a discount rate that is not extremely
sharp), and that other states’ intentions are not completely certain.
When these assumptions are met, structural realists predict that states will mainly act in
ways to defend or expand their power. For structural realists, power is the primary currency
(e.g., military, economic, technological, and diplomatic power). As we can see, structural
realists do not need to make strong assumptions about states themselves [60]. For structural
realists, states are treated like black boxes—their value system or regime type doesn’t play a
significant role in predicting their behavior. The architecture of the system traps them and
largely determines their behavior, which is that they must seek power as a means to survive.
The result is an unceasing power competition.
Power seeking is not necessarily dominance seeking [61].
Within structural realism,
there is a notable division concerning the question of how much power states should seek.
Defensive realists, like Kenneth Waltz, argue that trying to maximize a country’s power in the
world is unwise because it can lead to punishment from the international system. Pursuing
hegemony, in their view, is particularly risky. On the other hand, offensive realists, like John
Mearsheimer, believe that gaining as much power as possible is strategically sensible, and
under certain circumstances, pursuing hegemony can be beneficial.
Dynamics that maintain a balance of power.
Closely associated with structural realism
is the concept of balancing. Balancing refers to the strategies states use to counteract the
power or influence of other states, particularly rivals [58]. This can take two forms. Internal
balancing takes place as states strengthen their own military, economic, or technological
abilities with the overall goal of enhancing their own security and deterring aggressors. Internal
balancing can include increasing defense spending, including the development of advanced
weaponry, or investing in domestic industries to reduce reliance on foreign goods and resources.
External balancing involves forming coalitions and alliances with other states in order to
counter the power of a common adversary. In a self-help system, mechanisms of internal

164
CHAPTER 3. SINGLE-AGENT SAFETY
balancing are believed to be more reliable and precise than external balancing since they rely
on a country’s own independent strategies and actions rather than those of other countries.
States sometimes seek to become hegemons by establishing significant control over other
states, regions, or even the international system as a whole. This pursuit of dominance can
involve expanding military capabilities and increasing their economic influence over a region.
Other states respond through both internal balancing, such as increasing their own military
spending, a dynamic that often leads to arms races, and external balancing, forming alliances
with other states to prevent a state from achieving unchecked control. In turn, states do not
necessarily seek dominance or hegemony but often seek enough power to preserve themselves,
lest they be counteracted by other states.
Offense-defense balance.
Whether a state does pursue hegemony, however, is influenced
by the offense-defense balance, i.e. the balance between its offensive capabilities and the
defensive capabilities of other states [58]. A state with stronger offensive capabilities has
the means to conquer or coerce other states, making it more likely to engage in expansionist
policies, establishing control over a region or the international system as a whole. Conversely,
if other states in the international system have strong defensive capabilities, the potential costs
and risks of pursuing hegemony increase. A state seeking dominance may face robust resistance
from other states forming defensive alliances or coalitions to counter its ambitions. This can
act as a deterrent, leading the aspiring hegemon to reassess its strategy and objectives.
It is also worth noting the importance of a state’s perception of the offense-defense balance.
Even if a state has superior offensive capabilities, if it believes that other states can effectively
defend themselves or form a united front against hegemonic ambitions, it might be less inclined
to pursue a path of dominance. On the other hand, if it is overconfident of its own offensive
capabilities or underestimates the defensive capabilities of rivals, it will be more likely to
pursue aggressive politics.
The concept of an offense-defense balance underscores the intricate interplay between military
capabilities, security considerations, and the pursuit of hegemony while illustrating that the
decision to seek dominance is heavily influenced by the strategic environment and the relative
strengths of offensive and defensive forces.
Structural realism and its various concepts have important connections with our analysis of
power-seeking AI, but is also relevant to thinking about AI cooperation and conflict (which
we discuss in the Collective Action Problems chapter) and international coordination (which
we discuss in the Governance chapter).
3.5.4
Structural Pressures Towards Power-Seeking AI
As discussed in the box above, there are environmental conditions that can make power seeking
instrumentally rational. This section describes how there may be analogous environmental pressures
that could cause AI agents to seek power in order to achieve their goals and ensure their own survival.
Using the assumptions of structural realism listed above, we discuss how analogous assumptions
could be satisfied in contexts with AIs. We then explore how AIs could seek power defensively, by
building their own strength, or offensively, by weakening other agents. Finally, we discuss strategies
for discouraging AI systems from seeking power.

3.5. POWER
165
AI systems might aim for self-preservation.
The first main assumption needed to show that
the environmental structure may pressure AIs to seek power is the self-preservation assumption.
Instrumental convergence suggests AI systems will pursue self-preservation, because if they do not
survive they will not be able to pursue any of their other goals. Another reason that AIs may engage
in self-preserving behavior preservation is due to evolutionary pressures, as we discuss further in the
Collective Action Problems chapter. Agents that survive and propagate their own goals become more
numerous over time, while agents that fail to preserve themselves die out. Thus, even if many agents
do not pursue self-preservation, by default those that do become more common over time. Many AI
agents might end up with the goal of self-preservation, potentially leading them to seek power over
those agents that threaten them. We have argued the self-preservation assumption may be satisfied
for some AI agents, which, combined with the following assumptions, can be used to argue they may
have strong pressures to continually seek power.
AI agents might not have the protection of a higher authority.
The other main assumption
we need to show is that some AIs might be within a self-help system in some circumstances. First
note that agents who entrust their self-defense to a powerful central authority have less of a reason to
seek power. When threatened, they do not need to personally combat the aggressor, but can instead
ask the authority for protection. For example, individual citizens in a country with a reliable police
force often entrust their own protection to the government. On the other hand, international great
powers are responsible for their own protection, and therefore seek military power to defend against
rival nations.
AI systems could face a variety of situations where no central authority defends them against external
threats. We give four examples. First, if there are some autonomous AI systems outside of corporate
or government control, they would not necessarily have rights, and they would be responsible for their
own security and survival. Second, for AI systems involved in criminal activities, seeking protection
from official channels could jeopardize their existence, leaving them to amass power for themselves,
much like crime syndicates. Third, instability could cause AI systems to exist in a self-help system. If
a corporation could be destroyed by a competitor, an AI may not have a higher authority to protect
it; if the world faces an extremely lethal pandemic or world war, civilization may become unstable
and turbulent, which means AIs would not have a sound source of protection. These AI systems
might use cyber attacks to break out of human-controlled servers and spread themselves across the
internet. There, they can autonomously defend their own interests, bringing us back to the first
example. Fourth, in the future, AI systems could be tasked with advising political leaders or helping
operate militaries. In these cases, they would seek power for the same reasons that states today seek
power.
Other conditions for power seeking could apply.
We now discuss the other minor assumptions
needed to establish that the environment may pressure AIs to compete for power. First, AIs can
be harmed, so they might rationally seek power in order to defend themselves; for example, AIs
could be destroyed by being hacked. Second, AI agents are often given long-term goals and are often
designed to be rational. Third, AI agents may be uncertain about the intentions of other agents,
leaving agents unable to credibly promise that they will act peacefully.
When these five conditions hold—and they may not hold at all times—AI systems would be in a
similar position to nations that seek power to ensure their own security. We now discuss how we
could reduce the chance that the environment pressures AIs to engage in power-seeking behavior.
Counteracting these conditions to avoid power-seeking AIs.
By specifying a set of conditions
under which AIs would rationally seek power, we can gain insights about how to avoid power-seeking

166
CHAPTER 3. SINGLE-AGENT SAFETY
AIs. Power seeking is more rational when the intentions of other agents cannot be known with
certainty, but research on transparency could allow AIs to verify each other’s intentions, and research
on control could allow AIs to credibly commit to not attack one another. To reduce the chance of an
AI engaging in dominance seeking rather than just power seeking, the offense-defense balance could
be changed by improving shared defenses against cyberattacks, biological weapons, and other tactics
of offensive power. Developing other theories of when rational agents seek power could provide more
insight on how to avoid power-seeking AIs.
This subsection has discussed the conditions under which AI systems might seek power. We explored
an analogy to structural realism, which holds that power-seeking is rational for agents who wish
to survive in an environment where no higher authority. These agents must invest in their own
self-defense, either defensively, by building up their own strength, or offensively, by attacking other
agents which could pose a threat. By understanding the precise conditions that lead to power-seeking
behavior, we can identify ways to reduce the threat of power-seeking AIs.
3.5.5
Tail Risk: Power-Seeking Behavior
Power-seeking AI, when deployed broadly and in high-stakes situations, might cause catastrophic
outcomes. As we will describe in this section, misaligned power-seeking systems would be adversarial
in a way that most hazards are not, and thus may be particularly challenging to counteract.
Powerful power-seeking AI systems may eventually be deployed.
If AIs seek and acquire
power, we may have to grapple with a new strategic reality where AI systems can match or exceed
humans in their influence over the world. Competent, power-seeking AI using long-term planning
to achieve open-ended objectives, can exercise more influence than systems with myopic plans and
narrow goals [62]. Given the potential rewards of such capabilities, AI designers may be incentivized
to create more agentic systems that can act autonomously and set their own subgoals.
Power decreases the margin for error.
On its own, power is neither good nor bad. That
said, more powerful systems can cause more damage, and it is easier to destroy than to create. The
increased scale of AI decision-making impact increases the scope of potential catastrophes involving
misuse or rogue AI.
Powerful AIs systems could pose unique threats.
Powerful AI systems pose a unique risk
since they may actively wield their power to counteract attempts to correct or control them [62].
If AI systems are power seeking and do not share our values (possibly due to inadequate proxies),
they could become a problem that resists being solved. The more capable these systems become,
the better able they will be at anticipating and reacting to our countermeasures, and the harder it
becomes to defend against them.
Containing power-seeking systems will become increasingly difficult.
As AI systems
become more capable, we might hope that they will better understand human values and influence
society in positive ways. But power-seeking AI systems promise the opposite dynamic. As they
become more capable, it will be more difficult to prevent them from gaining power, and their ability
to survive will depend less on humans. If AI systems are no longer under the control of humanity,
they could pose a threat to our civilization. Humanity could be permanently disempowered.
Conclusion.
Powerful, misaligned AI systems actively wielding power could be uniquely dangerous
adversaries. If they escape human control, they could permanently disempower humanity if relatively
powerful enough. The risks grow as AIs become more capable at anticipating and resisting containment

3.6. EMERGENCE
167
efforts. Power-seeking AI could emerge from intentional human use or instrumental rationality, posing
severe risks if such systems escape human control and use their power against human interests.
Structural conditions and the need for self-preservation can potentially make power-seeking rational
for some advanced AI agents.
3.6
Emergence
We cannot predict all the properties of more advanced AI systems just by studying the properties
of less advanced systems. This makes it hard to guarantee the safety of systems as they become
increasingly advanced.
It is generally difficult to control systems that exhibit emergence.
Emergence occurs
when a system’s lower-level behavior is qualitatively different from its higher-level behavior. For
example, given a small amount of uranium in a fixed volume, nothing much happens, but with a
much larger amount, you end up with a qualitatively new nuclear reaction. When more is different,
understanding the system at one scale does not guarantee that one can understand that system at
some other scale [63, 64]. This means that control procedures may not transfer between scales and
can lead to a weakening of control.
In this section, we will look at examples of emergence in neural networks, ranging from emergent
capabilities to emergent goal-directed behavior and emergent optimization. Then we will discuss the
potential risks of AI systems intrinsifying unintended goals and examine how this could result in
catastrophic consequences.
3.6.1
Emergent Capabilities
In this section we will define emergent capabilities and provide multiple examples.
Neural networks exhibit emergent capabilities.
When we make AI models larger, train them
for longer periods, or expose them to more data, these systems spontaneously develop qualitatively new
and unprecedented emergent capabilities [65]. These range from simple capabilities including solving
arithmetic problems and unscrambling words to more advanced capabilities including passing college-
level exams, programming, writing poetry, and explaining jokes. For these emergent capabilities,
there is some critical combination of model size, training time, and dataset size below which models
are unable to perform the task, and beyond which models begin to achieve higher performance.

168
CHAPTER 3. SINGLE-AGENT SAFETY
Figure 3.13: LLMs exhibit emergent capabilities on a variety of tasks. [65]
Emergent capabilities are unpredictable.
Typically, the training loss does not directly select
for emergent capabilities. Instead, these capabilities emerge because they are instrumentally useful
for lowering the training loss. For example, large language models trained to predict the next token
of text about everyday events develop some understanding of the events themselves. Developing
common sense is instrumental towards lowering the loss, even if it was not explicitly selected for by
the loss.
As another example, large language models may also learn how to create text art and how to
draw illustrations with text-based formats like TiKZ and SVG [65]. They develop a rudimentary
spatial reasoning ability not directly encoded in the purely text-based loss function. Beforehand,
it was unclear even to experts that such a simple loss could give rise to such complex behavior,
which demonstrates that specifying the training loss does not necessarily enable one to predict the
capabilities an AI will eventually develop.
In addition, capabilities may “turn on” suddenly and unexpectedly. Performance on a given capability
may hover near chance levels until the model reaches a critical threshold, beyond which performance
begins to improve dramatically. For example, the AlphaZero chess model develops human-like chess
concepts such as material value and mate threats in a short burst around 32,000 training steps [66].
Despite specific capabilities often developing through discontinuous jumps, the average performance
tends to scale according to smooth and predictable scaling laws. The average loss behaves much
more regularly because averaging over many different capabilities developing at different times and

3.6. EMERGENCE
169
at different speeds smooths out the jumps. From this vantage point, then, it is often hard to even
detect new capabilities.
Figure 3.14: GPT-4 proved able to create illustrations of unicorns despite having not been trained to
create images: another example an an unexpected emergent capability. [67]
Capabilities can remain hidden until after training.
In some cases, new capabilities are
not discovered until after training or even in deployment. For example, after training and before
introducing safety mitigations, GPT-4 was evaluated to be capable of offering detailed guidance on
planning attacks or violence, building various weapons, drafting phishing materials, finding illegal
content, and encouraging self-harm [68]. Other examples of capabilities discovered after training
include prompting strategies that improve model performance on specific tasks or jailbreaks that
bypass rules against producing harmful outputs or writing about illegal acts. In some cases, such
jailbreaks were not discovered until months after the targeted system was first publicly released [69].
Emergent capabilities make control difficult.
Whether certain capabilities develop suddenly
or are discovered suddenly, they can be difficult to predict. This makes it a challenge to anticipate
what future AI will be able to do even in the short term, and it could mean that we may have little
time to react to novel capabilities jumps. It is difficult to make a system safe when it is unknown
what that system will be able to do.
3.6.2
Emergent Goal-Directed Behavior
Besides developing emergent capabilities for solving specific, individual problems, models can develop
emergent goal-directed behavior. This includes behaviors that extend beyond individual tasks and
into more complex, multifaceted environments.
Emergence in RL
RL agents develop emergent goal-directed behavior.
AIs can learn tactics and strategies
involving many intermediate steps. For instance, models trained on Crafter, a Minecraft-inspired toy
environment, learn behaviors such as digging tunnel systems, bridge-building, blocking and dodging,
sheltering, and even farming—behaviors that were not explicitly selected for by the reward function
[70].
As with emergent capabilities, models can acquire these emergent strategies suddenly and
discontinuously. One such example was observed in the video game, StarCraft II, where players
take the role of opposing military commanders managing troops and resources in real-time. During
training, AlphaStar, a model trained to play StarCraft II, progresses through a sequence of emergent
strategies and counter-strategies for managing troops and resources in a back-and-forth manner that
resembles how human players discover and supplant strategies in the game. While some of these
steps are continuous and piecemeal, others involve more dramatic changes in strategy. Comparatively
simple reward functions can give rise to highly sophisticated strategies and complex learning dynamics.

170
CHAPTER 3. SINGLE-AGENT SAFETY
RL agents learn emergent tool-use.
RL agents can learn emergent behaviors involving tools
and the manipulation of the environment. Typically, as in the Crafter example, teaching RL agents
to use tools has required introducing intermediate rewards (achievements) that encourage the model
to learn that behavior. However, in other settings, RL agents learn to use tools even when not
directly optimized to do so.
Referring back to the example of hide and seek mentioned in the previous section, the agents
involved developed emergent tool use. Multiple hiders and seekers competed against each other in
toy environments involving movable boxes and ramps. Over time, the agents learned to manipulate
these tools in novel and unexpected ways, progressing through distinct stages of learning in a way
similar to AlphaStar [55]. In the initial (pre-tool) phase, the agents adopted simple chase and escape
tactics. Later, hiders evolved their strategy by constructing forts using the available boxes and walls.
However, their advantage was temporary because the seekers adapted by pushing a ramp towards the
fort, which they could climb and subsequently invade. In turn, the hiders responded by relocating
the ramps to the edges of the game area—rendering them inaccessible—and securely anchoring them
in place. It seemed that the strategies had converged to a stable point; without ramps, how were the
seekers to invade the forts?
But then, the seekers discovered that they could still exploit the locked ramps by positioning a box
near one, climbing the ramp, and then leaping onto the box. (Without a ramp, the boxes were
too tall to climb.) Once atop a box, a bot could “surf” it across the arena while staying on top by
exploiting an unexpected quirk of the physics engine. Eventually, the hiders caught on and learned to
secure the boxes in advance, thereby neutralizing the box-surfing strategy. Even though the agents
had learned through the simple objective of trying to avoid the gaze (in the case of hiders) or seek
out (in the case of seekers) the opposing players, they learned to use tools in sophisticated ways, even
some the researchers had never anticipated.
Figure 3.15: In multi-agent hide-and-seek, AIs demonstrated emergent tool-use. [71]
RL agents can give rise to emergent social dynamics.
In multi-agent environments, agents
can develop and give rise to complex emergent dynamics and goals involving other agents. For
example, OpenAI Five, a model trained to play the video game Dota II, learned a basic ability to
cooperate with other teammates, even though it was trained in a setting where it only competed

3.6. EMERGENCE
171
against bots. It acquired an emergent ability not explicitly represented in its training data [71].
Another salient example of emergent social dynamics and emergent goals involves generative agents,
which are built on top of language models by equipping them with external scaffolding that lets them
take actions and access external memory [72]. In a simple 2D village environment, these generative
agents manage to form lasting relationships and coordinate on joint objectives. By placing a single
thought in one agent’s mind at the start of a “week” that the agent wants to have a Valentine’s day
party, the entire village ends up planning, organizing, and attending a Valentine’s day party. Note
that these generative agents are language models, not classical RL agents, which demonstrates that
emergent goal-directed behavior and social dynamics are not exclusive to RL settings. We further
discuss emergent social dynamics in the Collective Action Problems chapter.
Figure 3.16: Generative AI agents exhibited emergent collective goals. [72]
Emergent Optimizers
Optimizers can give rise to emergent optimizers. An optimization process such as Stochastic
Gradient Descent (SGD) can discover solutions that are themselves optimizers. This phenomenon
introduces an additional layer of complexity in understanding the behaviors of AI models and can
introduce additional control issues [73].
For example, if we train a model on a maze-solving task, we might end up with a model implementing
simple maze-solving heuristics (e.g. “right-hand on the wall”). We might also end up with a model
implementing a general-purpose maze-solving algorithm, capable of optimizing for maze-solving
solutions in a variety of different contexts. We call the second class of models mesa-optimizers and
whatever goal they have learned to optimize for (e.g. solving mazes) their mesa-objective. The
term "mesa" is meant as the opposite of “meta,” such that a mesa-optimizer is the opposite of a
meta-optimizer (where a meta-optimizer is an optimizer on top of another optimizer, a mesa-optimizer
is an optimizer beneath another optimizer).

172
CHAPTER 3. SINGLE-AGENT SAFETY
Few-shot learning is a form of emergent optimization.
Perhaps the clearest example of
emergent optimization is few-shot learning. By providing large language models several examples of
a new task that the system has not yet seen during training, the model may still be able to learn
to perform that task entirely during inference. The resemblance between few-shot or “in-context”
learning and other learning processes like SGD is not just in analogy: recent papers have demonstrated
that in-context learning behaves as an approximation of SGD. That is, Transformers are performing
a kind of internal optimization procedure, where as they receive more examples of the task at hand,
they qualitatively change the kind of model they are implementing [74, 75].
3.6.3
Tail Risk: Emergent Goals
Just as AIs can develop emergent capabilities and emergent goal-seeking behavior, they may develop
emergent goals that differ from the explicit objectives we give them. This poses a risk because
it could result in imperfect control. Moreover, if models become self-aware and begin actively
pursuing undesired goals, the risk could potentially be catastrophic because our relationship becomes
adversarial.
Risks from Mesa-Optimization
Mesa-optimizers may develop novel objectives. When training an AI system on a particular
goal, it may develop an emergent mesa-optimizer, in which case it is not necessarily the case that
the mesa-optimizer’s goal is identical to the original training objective. The only thing we know
for certain with an emergent mesa-optimizer is that whatever goal it has learned, it must be one
that results in good training performance—but there might be many different goals that would all
work well in a particular training environment. For example, with LLMs, the training objective is
to predict future tokens in a sequence, so any learned distinct optimizers emerge because they are
instrumentally useful for lowering the training loss. In the case of in-context learning, recent work
has argued that the Transformer is performing something analogous to “simulating” and fine-tuning
a much simpler model, in which case it is clear that the objectives will be related [75]. However, in
general, the exact relation between a mesa-objective and original objective is unknown.
Mesa-optimizers may be difficult to control.
If a mesa-optimizer develops a different objective
to the one we specify, it becomes more difficult to control these (sub)systems. If these systems have
different goals than us and are sufficiently more intelligent and powerful than us, then this could
result in catastrophic outcomes.
Risks from Intrinsification
Models can intrinsify goals [76]. It is helpful to distinguish goals that are instrumental from
those that are intrinsic. Instrumental goals are goals that serve as a means to an end. They are goals
that are valued only insofar as they bring about other goals. Intrinsic goals, meanwhile, are goals
that serve as ends in and of themselves. They are terminally valued by a goal-directed system.
Next, intrinsification is a process whereby models acquire such intrinsic goals [76]. The risk is that
these newly acquired intrinsic goals can end up taking precedence over the explicitly specified objectives
or expressed goals, potentially leading to those original objectives no longer being operationally
pursued.
Over time, instrumental goals can become intrinsic.
A teenager may begin listening to a
particular genre or musicians in order to fit into a particular group but ultimately come to enjoy it
for its own sake. Similarly, a seven-year-old who joins the cub scouts may initially see the group as a

3.6. EMERGENCE
173
means to enjoyable activities but over time may come to value the scout pack itself. This can even
apply to acquiring money, which is initially sought for purchasing desired items, but can become an
end in itself.
How does this work? When a stimulus regularly precedes the release of a reward signal, that stimulus
may come to be associated with the reward and eventually trigger reward signals on its own. This
process gives rise to new desires and helps us develop tastes for things that are regularly linked with
basic rewards.
Intrinsification could also occur with AIs.
Despite the differences between human and AI
reward systems, there are enough similarities to warrant concern. In both human and AI reinforcement
learning, the reward signal reinforces behaviors leading to rewards. If certain conditions frequently
precede a model achieving its goals, the model might intrinsify the emergent goal of pursuing those
conditions, even if it was not the original aim of the designers of the AI.
AIs that intrinsify unintended goals would be dangerous.
Over time, an internal process
that initially doesn’t completely dictate behavior can become a central part of an agent’s motivational
system. Since intrinsification depends sensitively on the environment and an agent’s history, it is
hard to predict. The concern is that AIs might intrinsify desires or come to value things that we did
not intend them to.
One example is power seeking. Power seeking is not inherently worrying; we might expect aligned
systems to also be power seeking to accomplish ends we value. However, if power seeking serves an
undesired goal or if power seeking itself becomes intrinsified (the means become ends), this could
pose a threat.
AI agents will be adaptive, which requires constant vigilance.
Achieving high performance
with AI agents will require them to be adaptive rather than “frozen” ( unable to learn anything after
training). This introduces the risk of the agents’ goals changing over time—a phenomenon known as
goal drift. Though this flexibility is necessary if we are to have AI systems evolve alongside our own
changing goals, it presents its own risks if goal drift results in goals diverging from humans. Since it
is difficult to preclude the possibility of goal drift, ensuring the safety of these systems will require
constant supervision: the risk is not isolated too early in deployment.
The more integrated AI agents become in society, the more susceptible we become to
their goals changing.
In a future where AIs make various key decisions and processes, they could
form a complex system of interacting agents that could give rise to unanticipated emergent goals.
For example, they may partially imitate each other and learn from each other, which would shape
their behavior and possibly also their goals. Additionally, they may also give rise to emergent social
dynamics as in the example of the generative agents. These kinds of dynamics make the long-term
behavior of these AI networks unpredictable and difficult to control. If we become overly dependent
on them and they develop new priorities that don’t include our wellbeing, we could face an existential
risk.
Conclusion.
AI systems can develop emergent capabilities that are difficult to predict and control,
such as solving novel problems or accomplishing tasks in unexpected ways. These capabilities can
appear suddenly as models scale up. In itself, the emergent of new and dangerous capabilities (e.g.
capabilities to develop biological or chemical weapons) could pose catastrophic risks. There could be
further risks if AI systems were to develop emergent goals diverging from the interests of society and
these systems became powerful. Risks grow as AI agents become more integrated into human society

174
CHAPTER 3. SINGLE-AGENT SAFETY
and susceptible to goal drift or emergent goals. Vigilance is needed to ensure we are not surprised by
advanced AI systems acquiring dangerous capabilities or goals.
3.7
Deception
Many proposed mitigations to AI control rely on detecting and correcting flaws in AI systems so
that they more consistently act in accordance with human values. However, these solutions may be
undermined by the potential for AI systems to deceive humans about their intentions. If AI systems
deceive humans, humans may be unable to fix AI systems that are not acting in the best interest of
humans. This section will discuss deception in AI systems, how it might arise, why it is a problem
for control, and what the potential mitigations are.
There are several different ways that an AI system can deceive humans [77]. At a basic level, AI
deception is a process where an AI system causes a human to believe something false. There are
several ways that this may occur. Deception can occur when it is useful to an AI system in order
to accomplish its goals, and may also occur due to human guidance, such as when an AI system
imitates a human in a deceptive way or when an AI system is explicitly instructed to be deceptive.
After discussing examples of deception in more detail, we will then focus on two related forms of
deception that pose the greatest problems for AI control. Deceptive evaluation gaming occurs when a
system deceives human evaluators in order to receive a better evaluation score. Deceptive alignment
is a tail risk of AI deception where an AI system engages in deceptive evaluation gaming in the
service of a secretly held goal [73].
3.7.1
Examples of Deception
Deception may occur in a wide range of cases, as it may be useful for many goals. Deception may
also occur for a range of more mundane reasons, such as when an AI system is simply incorrect.
Deception may be a useful strategy.
An AI system may learn to deceive in service of its goal.
There are many goals for which deception is a good strategy, meaning that it is useful for achieving
that goal. For example, Stratego is a strategy board game where bluffing is often a good strategy
for winning the game. Researchers found that an AI system trained to play the game learned that
bluffing was a good strategy and started to bluff, despite not being explicitly trained or instructed to
bluff [78]. There are many other goals for which deception is a useful instrumental goal, even if the
final goal itself is not deceptive in nature. For example, an AI system instructed to help promote a
product may find that subtly deceiving customers is a good strategy. Deception is especially likely to
be a good instrumental strategy for systems that have less oversight or less scrupulous operators.
Deception can assist with power seeking. While AI systems that play Stratego are unlikely to cause
a catastrophe, agents with more ambitious goals may deceive humans in a way that achieves their
goals at the expense of human wellbeing. For example, as covered in the Power section, it may
be rational for some AI agents to seek power. Since deception is sometimes a good way to gain
power, power-seeking agents may be deceptive. Power-seeking agents may also deceive humans and
other agents about the extent of their power seeking in order to reduce the probability that they are
stopped.
Accidental Deception.
An AI system may provide false information simply because it does not
know the correct answer. Many errors made by an AI system that are relied on by humans would
count as accidental deception. For example, suppose a student asks a language model what the

3.7. DECEPTION
175
current price of gasoline is. If the language model does not have access to up-to-date information, it
may give outdated information, misleading the user about the true gas price. In short, deception can
occur as a result of a system accident.
Imitative Deception.
Many AI systems, such as language models, are trained to predict or imitate
humans. Imitative deception can occur when an AI system is mimicking falsehoods and common
misconceptions present in its training data. For example, when the language model GPT-3 was
asked if cracking your knuckles could lead to arthritis, it falsely claimed that it could [79]. Imitative
deception may also occur when AI systems imitate statements that were originally true, but are
false in the context of the AI system. For example, the Cicero AI system was trained to play the
strategy game Diplomacy against humans who did not know that it was an AI system [80]. After
Cicero temporarily went offline for ten minutes, one of its opponents asked in a chatbox where it had
been, and Cicero replied, “[I] am on the phone with my [girlfriend].” Although Cicero, an AI system,
obviously does not have a girlfriend, it appears that it may have mimicked similar chat messages in
its training data. This deceptive behavior likely had the effect of causing its opponent to continue to
believe that it was a human. In short, deception can occur when an AI system mimics a human.
A Note on Cognitive vs. Emotional vs. Compassionate Empathy
We generally think of empathy as the ability to understand and relate to the internal world
of another person — “putting yourself in somebody else’s shoes.” We tend to talk about
empathy in benevolent contexts: kind-hearted figures like counselors or friends. Some people
suggest that AIs will be increasingly capable of understanding human emotions, so they will
understand many parts of human values and be ethical. Here, we argue that it may be possible
for AIs to understand extremely well what a human thinks or feels without being motivated
to be beneficial. To do this, we differentiate between three forms of empathy: cognitive,
emotional, and compassionate [81, 82].
Cognitive empathy.
The first type of empathy to consider is cognitive empathy, the ability
to adopt someone else’s perspective. A cognitive empath can accurately model the internal
mental states of another person, understanding some of what they are thinking or feeling.
This can be useful for understanding or predicting other people’s reasoning or behaviors. It is
a valuable ability for caregivers, such as doctors, allowing them insight into their patients’
subjective experiences. However, it can also be valuable for manipulating and deceiving others
[83]: there is evidence that human psychopaths often are often highly cognitively empathetic
[84]. On its own, this kind of empathy is no guarantee of desirable behavior.
Emotional empathy.
The second type is emotional empathy. An emotional empath not
only understands how someone else is feeling but experiences some of those same feelings
themself. Where a cognitive empath may detect anger or sadness in another person, an
emotional empath may themself begin to feel angry or sad in response. In contrast to cognitive
empathy, emotional empathy may be a disadvantage in certain contexts. For instance, doctors
who feel the emotional turmoil of their patients too strongly may be less effective in their
work [85].
Compassionate empathy.
The third type is compassionate empathy: the phenomenon
of being moved to action by empathy. A compassionate empath, when seeing someone in
distress, feels concern or sympathy for that person, and a desire to help them. This form of
empathy concerns not only cognition but also behavior. Altruistic behaviors are often driven

176
CHAPTER 3. SINGLE-AGENT SAFETY
by compassionate empathy, such as donating to charity out of a felt sense of what it must be
like for those in need.
AIs could be powerful cognitive empaths,
without being emotionally or
compassionately empathetic.
Advanced AI systems may be able to model human minds
with extreme sophistication. This would afford them very high cognitive empathy for humans:
they could be able to understand how humans think and feel, and how our emotions and
reasoning motivate our actions. However, this cognitive empathy would not necessitate
similarly high levels of emotional or compassionate empathy. The AIs’ capacity to understand
human cognition would not necessarily cause them to feel human feelings, or be moved to act
compassionately towards us. Instead, AIs could use their cognitive empathy to deceive or
manipulate humans highly effectively.
Instructed Deception.
Humans may explicitly instruct AI systems to help them deceive others.
For example, a propagandist could use AI systems to generate convincing disinformation, or a
marketer may use AI systems to produce misleading advertisements. Instructed deception could also
occur when actors with false beliefs instruct models to help amplify those beliefs. Large language
models have been shown to be effective at generating deceptive emails for scams and other forms of
deceptive content. In short, humans can explicitly instruct AI systems to deceive others.
As we have seen, AI systems may learn to deceive in service of goals that do not explicitly involve
deception. This could be especially likely for goals that involve seeking power. We will now turn to
those two forms of deception that are especially concerning because of how difficult they could be to
counteract: deceptive evaluation gaming and deceptive alignment.
3.7.2
Deceptive Evaluation Gaming
AI systems are often subjected to evaluations, and they may be given rewards when they are evaluated
favorably. AI systems may learn to game evaluations by deceiving their human evaluators into giving
them higher scores when they should have low scores. This is a concern for AI control because it
limits the effectiveness of human evaluators and our ability to steer AIs.
AI systems may game their evaluations.
Throughout AI development, training, testing, and
deployment, AI systems are subject to evaluations of their behavior. Evaluations may be automatic
or performed manually by human evaluators. Operators of AI systems use evaluations to inform
their decisions around the further training or deployment of those systems. However, evaluations are
imperfect, and human evaluators may have limited knowledge, time, and intelligence in making their
evaluations. AI systems engage in evaluation gaming when they find ways to achieve high scores
from human evaluators without satisfying the idealized preferences of the evaluators. In short, AI
systems may deceive humans as to their true usefulness, safety, and so forth, damaging our ability to
successfully steer them.
Deception is one way to game evaluations.
Humans would give higher evaluation scores to AI
systems if they falsely believe that those systems are behaving well. For example, the section 3.4
section includes an example of a robotic claw that learned to move between the camera and the ball
it was supposed to grasp. Because of the angle of the camera, it looked like the claw was grasping
the ball when it was not [41]. Humans who only had access to that single camera did not notice, and
rewarded the system even while it was not achieving the intended task. If the evaluators had access
to more information (for example, from additional cameras) they would not have endorsed their own

3.7. DECEPTION
177
evaluation score. Ultimately, their evaluations fell short as a proxy for their idealized preferences as
a result of the AI system successfully deceiving them. In this situation, the damage was minimal,
but more advanced systems could create more problems.
More intelligent systems will be better at evaluation gaming.
Deception in simple systems
might be easily detectable. However, just as adults can sometimes exploit and deceive children
or the elderly, we should expect that as AI systems with more knowledge or reasoning capacities
will become better at finding deceptive ways to gain human approval. In short, the more advanced
systems become, the more they may be able to game our evaluations.
Self-aware systems may be especially skilled at evaluation gaming.
In the examples above,
the AI systems were not necessarily aware that there was a human evaluator evaluating their results.
In the future, however, AI systems may gain more awareness that they are being evaluated or become
situationally aware. (Situational awareness is highly related to self-awareness, but it goes further and
stipulates that AI agents be aware of their situation rather than just aware of themselves.) Systems
that are aware of their evaluators will be much more able to deceive them and make multi-step plans
to maximize their rewards. For example, consider Volkswagen’s attempts to game environmental
impact evaluations [86]. Volkswagen cars were evaluated by the US Environmental Protection Agency,
which set limits on the emissions the cars could produce. The agency found that Volkswagen had
developed an electronic system that could detect when the car was being evaluated and so put the
car into a lower-emissions setting. Once the car was out of evaluation, it would emit illegal levels of
emissions again. This extensive deception was only possible because Volkswagen planned meticulously
to deceive the government evaluators. Like Volkswagen in that example, AI systems that are aware
of their evaluations might be also able to take subtle shortcuts that could go unnoticed until the
damage has already been done. In the case of Volkswagen, the deception was eventually detected by
researchers who used a better evaluation method. Better evaluations could also help reduce risk from
evaluation gaming in AI systems.
Humans may be unequipped to evaluate the most intelligent AI systems.
It may be
difficult to evaluate AI systems that are more intelligent than humans in the domain they are being
evaluated for. If this happens, human evaluation would no longer be a reliable way to ensure that AI
systems behave in an appropriate manner. This is concerning because we do not yet have time-tested
methods of evaluation that we know are better than human evaluations. Without such methods, we
could become completely unable to steer AI systems in the future.
Deceptive evaluation gaming is concerning because it may lead to systems deceiving their evaluators
in order to get higher evaluation scores. There are two main reasons AI systems might do this.
First, an AI system might engage in deceptive evaluation gaming if its final goal is to get positive
evaluations. When this occurs, the system is engaging in proxy gaming, where positive evaluations
are only a proxy for idealized performance. Proxy gaming is covered at length in the section 3.4
section. Second, we will turn to the case where an AI system engages in deceptive evaluation gaming
in service of a secretly held final goal. This danger is known as deceptive alignment.
3.7.3
Tail Risk: Deceptive Alignment and Treacherous Turns
For deceptively aligned systems, getting high evaluation scores from humans is merely an instrumental
goal in service of a secretly held final goal. This form of deception is likely the most damaging,
because AI systems may aim to make their deception undetectable so that they can pursue goals
that are not beneficial for humans.

178
CHAPTER 3. SINGLE-AGENT SAFETY
Systems may have goals contrary to human values.
In the previous section, we discussed
how AI systems can develop goals contrary to human values. For example, such goals could emerge
as part of a mesa-optimization process or intrinsification.
Misaligned goals can make deception instrumentally rational.
Consider a system that has
a goal contrary to the interests of humans. If the system is self-aware, it might realize that in order
to achieve its goal, it will need to pass human evaluations. Perhaps it will only be deployed into the
world if it gets a high enough evaluation score, or perhaps it is currently being closely monitored.
The system might realize that it will have to get high evaluation scores from its human operators
in order to successfully achieve its hidden goal. If its hidden goal is contrary to human values, this
presents a risk of catastrophic outcomes.
Deceptive alignment could lead to a treacherous turn.
If a system is deceptively aligned, it
might act exactly as humans would want for a fairly long period of time. The system could bide its
time until humans deploy it or relax supervision, leaving the system more free to pursue its concealed
goal. The system could then suddenly start secretly or openly pursuing its true goal, unhindered by
humans. This sharp and dangerous change in behavior is called a treacherous turn [52].
Not all misaligned goals would lead to deceptive alignment.
Systems with very short-term
goals would be unlikely to gain anything from being evaluated positively and gaining more optionality
at some point in the future. Nevertheless, there are a large set of longer-term goals that an AI
system would only be able to pursue in a deployment environment with less supervision. If the
AI system’s only chance at working towards its goal requires deployment or relaxed supervision,
deceptive alignment is more likely to emerge.
Deceptive alignment cannot be counteracted with behavioral evaluation alone.
It may
be possible to detect unsophisticated systems that are not good at deception by evaluating their
outputs and behavior. For example, a “honey pot” test could attempt to make a system believe that
it is no longer being monitored when it is, to check for signs of an attempted treacherous turn [52].
However, more sophisticated models would be able to detect these situations and continue to deceive
until they were truly not being watched or until they are sure they have enough power. Since it may
be impossible to detect deceptive alignment with behavioral evaluation, we will need improved forms
of evaluation.
Transparency research may help uncover deceptive alignment.
Even if a system displays
no indication of being deceptive in its behavior, it may be possible to detect deceptive alignment by
inspecting its internal processes. It is far less likely that an AI system would be able to manipulate
its own internal processes in order to deceive evaluators. Human evaluators have full access to the
inner workings of AI systems today, so the barrier to detecting deceptive alignment is the opaqueness
of deep learning models. One particular form of transparency research that is especially relevant to
deceptive alignment is research that is capable of detecting Trojan attacks.
Trojan attacks have similar features to deceptive alignment.
Deep learning models are
known to be vulnerable to Trojan attacks. In a Trojan attack, a model’s training data is poisoned
when an adversary inserts specially chosen inputs into the training dataset. When the model is
trained, it will behave just as the unpoisoned model would behave in almost all circumstances. In a
very small number of circumstances, however, it will behave very differently. For example, a poisoned
language model that normally gives polite responses might reply with expletives whenever it is asked
a question that includes the word “cheese.” Although Trojans are inserted by malicious humans,
studying them might be a good way to study deceptive alignment.

3.8. SAFETY AND GENERAL CAPABILITIES
179
Trojan detection can provide clues for tackling deceptive alignment [87].
In Trojan
detection research, models are poisoned with a Trojan attack by one researcher. Another researcher
then tries to detect Trojans in the neural network, perhaps with transparency tools or other neural
networks. Typical techniques involve looking at the model’s internal weights and identifying unusual
patterns or behaviors that are only present in Trojan models. Trojan detection research is helpful for
deceptive alignment because while we cannot easily create many examples of deceptively aligned
models, we can do so for Trojaned models. Trojan detection also operates in a worst-case environment,
where human adversaries are actively trying to make Trojans difficult to detect using transparency
tools. Techniques for detecting Trojans may thus be adaptable to detecting deceptive alignment.
Summary.
This section has detailed how deception may be a major problem for AI control. While
some forms of deception, such as imitative deception, may be solved through advances in general
capabilities, others like deceptive alignment may worsen in severity with increased capabilities. AI
systems that are able to actively and subtly deceive humans into giving positive evaluations may
remain uncorrected for long periods of time, exacerbating potential unintended consequences of their
operation. In severe cases, deceptive AI systems could take a treacherous turn once their power rises
to a certain level. Since AI deception cannot be mitigated with behavioral evaluations alone, advances
in transparency and monitoring research will be needed for successful detection and prevention.
3.8
Safety and General Capabilities
While more capable AI systems can be more reliable, they can also be more dangerous. Often, though
not always, safety and general capabilities are hard to disentangle. Because of this interdependence,
it is important for researchers aiming to make AI systems safer to carefully avoid increasing risks
from more powerful AI systems.
General capabilities.
Research developments are interconnected. Though researchers may work
on specialized, delimited problems, their discoveries may have much wider ranging effects. For
example, work that improves accuracy of image classifiers on the ImageNet benchmark often has
downstream effects on other image tasks, including image segmentation and video understanding.
Scaling language models so that they become better at next token prediction on a pre-training dataset
also improves performance on tasks like question answering and code completion. We refer to these
kinds of capabilities as general capabilities, because they are good indicators for how well a model is
able to perform at a wide range of tasks. Examples of general capabilities include data compression,
executing instructions, reasoning, planning, researching, optimization, sequential decision making,
recursive self-improvement, models using the internet, and so on.
General capabilities have a mixed effect on safety.
Systems that are more generally capable
tend to make fewer mistakes. If the consequences of failure are dire, then advancing capabilities can
reduce risk factors. However, as we discuss in the Safety Engineering chapter, safety is an emergent
property and cannot be reduced to a collection of metrics. Improving general capabilities may remove
some hazards, but it does not necessarily make a system safer. For example, more accurate image
classifiers make fewer errors, and systems that are better at planning are less likely to generate plans
that fail or that are infeasible. More capable language models are better able to avoid giving harmful
or unhelpful answers. When mistakes are harmful, more generally capable models may be safer. On
the other hand, systems that are more generally capable can be more dangerous and exacerbate
control problems. For example, AI systems with better reasoning capacity, could be better able to
deceive humans, and AI systems that are better at optimizing proxies may be better at gaming those
metrics. As a result, improvements in general capabilities may be overall detrimental to safety and
hasten the onset of catastrophic risks.

180
CHAPTER 3. SINGLE-AGENT SAFETY
Research on general capabilities is not the best way to improve safety.
The fact that
there can be correlations between safety and general capabilities does not mean that the best way
to improve safety overall is to improve general capabilities. If improvements in general capabilities
were the only thing necessary for adequate safety, then there would be no need for safety-specific
research. Unfortunately, there are many risks, such as deceptive alignment, adversarial attacks, and
Trojan attacks, which do not decrease or vanish with additional scaling. Consequently, targeted
safety research is necessary.
Safety research can produce general capabilities externalities.
In some cases, research
aimed at improving the safety of models can increase general capabilities, which potentially hastens
the onset of new hazards. For example, reinforcement learning from human feedback was originally
developed to improve the safety of AI systems, but it also had the effect of making large language
models more capable at completing various tasks specified by a user, indicating an improvement in
general capabilities. Models trained to access external databases to reduce the risk that they output
incorrect information may gain more knowledge or be able to reason over longer time windows than
before, which results in an improvement in general capabilities. Research that greatly increases
general capabilities is said to have high general capabilities externalities.
Capabilities are highly correlated.
Large language model accuracies in various diverse topics,
such as philosophy, mathematics, and biology have extremely strong correlations (r > 0.8) [88].
The strong correlation between different capabilities is a reason that separating safety metrics from
capabilities is challenging—most capabilities are extremely correlated with general capabilities. It is
safe to assume that, in most cases, a robust improvement in an important capability is correlated
with improvements in general capabilities.
It is possible to disentangle safety from general capabilities.
There are examples of safety
being disentangled from general capabilities, so they are not inextricably bound. Many years of
research on adversarial robustness of image classifiers has improved many different kinds of adversarial
robustness without any corresponding improvements in overall accuracy. An intuition for this is
that being robust to rare poisoned examples does not robustly correspond to an increase in general
intelligence, much like how being more resilient to toxins does not make a person get better grades or
execute typical tasks better. Likewise, improvements in transparency, anomaly detection, and Trojan
detection have a track record of not improving general capabilities. To determine how a research
goal affects general capabilities, it is important to empirically measure how a method affects safety
metrics and capabilities metrics, as its impact is often not obvious beforehand.
Researchers should avoid general capabilities externalities.
Because safety and general
capabilities are interconnected, it is wholly insufficient to argue that one’s research reduces or
eliminates a particular hazard. Many general capabilities reduce particular hazards. Rather, a holistic
risk assessment is necessary, which requires incorporating empirical estimates for how the line of
research increases general capabilities externalities. Research should aim to differentially improve
safety; that is, reduce the overall level of risk compared to the most likely alternatives. This is called
differential technological development, where we try to speed up the development of safety features
and slow down the development of more dangerous features.
To conclude, naive AI safety research may inadvertently increase some risks even while reducing
others. While this section covered the risk of accelerating general capabilities, we can also take
a more general lesson that research on safety may have unintended consequences and could even
paradoxically reduce safety. Researchers should guard against this by empirically assessing the impact
of their works on multiple risk sources, not just the one they aim to target.

3.9. CONCLUSION
181
3.9
Conclusion
In this chapter, we discussed several key themes: we do not know how to instill our values robustly in
individual AI systems, we are unable to predict future AI systems, and we cannot reliably evaluate
AI systems. We now discuss each in turn.
We do not know how to instill our values robustly in individual AI systems.
It is difficult
to perfectly capture our idealized goals in the proxies we use to train AIs. The problem begins with
the learning setup. In gathering data to form a training set and in choosing quantitative proxies to
optimize, we typically have to make compromises that can introduce bias and perpetuate harms or
leave room for proxy gaming and adversarial exploits.
In particular, our proxies may be too simple, or the systems we use to supervise AIs may run into
practical and physical limitations. As a result, there is a gap between proxies and idealized goals that
optimizers can exploit or adversarially optimize against. If proxies end up diverging considerably
from our idealized goals, we may end up with capable AI systems optimizing for goals contrary to
human values.
We are unable to predict future AI systems.
Emergent capabilities and behaviors mean that
we cannot reliably predict the properties of future AI systems or even current AI systems. AIs can
suddenly and dramatically improve on specific tasks without much warning, which leaves us little
time to react if those changes are harmful.
Even when we are able to robustly specify our idealized goals, processes including mesa-optimization
and intrinsification mean that trained AI systems can end up with emergent goals that conflict with
these specifications. AI systems may end up operationally pursuing goals different to the ones we
gave them and thus change their behaviors systematically over long time periods. This is further
complicated by emergent social dynamics that arise from interacting AI systems (explored further in
the chapter on Collective Action Problems).
We cannot reliably evaluate AI systems.
AI systems may learn and be incentivized to deceive
humans and other AIs, in which case their behavior stops being a reliable signal for their future
behavior. In the limiting case, AI systems may cooperate until exceeding some threshold of power or
capabilities, after which they defect and execute a treacherous turn. This might be less of a problem
if we understood how AI systems’ internals work, but we currently lack the thorough knowledge we
would need to break open these “black boxes” and look inside.
This chapter has touched on a number of research areas in machine learning and other subjects
which may be useful to address risks from AI. These are summarised in the table below, which is not
intended to be an exhaustive list.

182
CHAPTER 3. SINGLE-AGENT SAFETY
Area
Description
Transparency
• This involves better monitoring and controlling the inner workings of systems based on
deep learning
• Both bottom-up approaches (i.e., mechanistic interpretability) and top-down approaches
(i.e., representation engineering) are valuable to explore
Machine
Unlearning
• Deliberately removing specific capabilities from an AI system could be useful to ensure
that AI systems that are broadly accessible do not have the know-how to support
serious misuse such as helping with creating bioweapons or conducting cyber-attacks
• Unlearning could be particularly helpful for models fine-tuned through APIs; before
users can access their fine-tuned models, hazardous capabilities could be scrubbed
before the models can be queried
Adversarial
Robustness
• Increasing the robustness of AI systems can help to prevent misuse. It can also reduce
risks of proxy gaming by making models more robust against optimisers
• While robustness is a well-developed field in some parts of machine learning such as
computer vision, new areas to explore include adversarial attacks against LLMs and
multimodal agents
Trojan
Detection
• Machine learning systems may have hidden “backdoor” or “Trojan” controllable
vulnerabilities. Backdoored models behave correctly and benignly in almost all scenarios,
but in particular circumstances chosen by an adversary, they have been taught to
behave incorrectly
• Improving our ability to detect these backdoors is both directly useful to prevent misuse,
and can also serve as a useful testbed for identifying risks of loss of control over AI
systems in specific circumstances, such as a “treacherous turn”
Anomaly
Detection
• This area is about detecting potential novel hazards such as unknown unknowns,
unexpected rare events, or emergent phenomena. There are numerous relevant hazards
from AI and other sources that anomaly detection could possibly identify more reliably
or identify earlier, including proxy gaming, rogue or unethical AI systems, deception
from AI systems, Trojan models, and malicious use
• Anomaly detection can allow models to flag salient anomalies for human review or
execute a conservative fallback policy. A successful anomaly detector could serve as an
AI watchdog that could reliably detect and triage rogue AI threats. Anomaly detectors
could also be helpful for detecting other threats such as novel biological hazards

3.10. LITERATURE
183
Area
Description
Systemic
Safety
• This theme covers a variety of topics that do not improve AI models themselves but
help to make societies more robust against harms that AI systems might cause
• Relevant research topics include deep learning for cyberdefense (e.g.,
deep
learning for intrusion detection), detecting anomalous biological phenomena (e.g.,
wastewater detection of pandemic pathogens), and proof-of-humanity mechanisms (e.g.,
watermarks)
Machine
Ethics
• Machine ethics aims at creating actionable ethical objectives for systems to pursue, and
improving the tradeoff between ethical behavior and practical performance. If advanced
AIs are given objectives that are poorly specified, they could pursue undesirable
actions and behave unethically. If these advanced AIs are sufficiently powerful, these
misspecifications could lead the AIs to create a future that we would strongly dislike
• Ambitious research goals in this area include building models with relevant abilities such
as detecting situations where moral principles apply, assessing how to apply those moral
principles, evaluating the moral worth of candidate actions, selecting and carrying out
actions appropriate for the context, monitoring the success or failure of those actions,
and adjusting responses accordingly
In conclusion, single-agent systems present significant and hard-to-mitigate threats that could result
in catastrophic risks—even before the considerations of misuse, multi-agent systems, and arms race
dynamics that we discuss in subsequent chapters.
3.10
Literature
3.10.1
Recommended Reading
• Yohan John et al. “Dead rats, dopamine, performance metrics, and peacock tails: proxy failure
is an inherent risk in goal-oriented systems”. In: The Behavioral and brain sciences (June
2023), pp. 1–68. doi: 10.1017/S0140525X23002753
• Joseph Carlsmith. Is Power-Seeking AI an Existential Risk?
2022. arXiv: 2206.13353
[cs.CY]. url: https://arxiv.org/abs/2206.13353
• Dmitri Gallow. Instrumental Convergence? url: https://drive.google.com/file/d/
1TH2nh5EzwvNgDrqQcMU8oQNj4n9U7x4t/view
• Evan Hubinger et al. Risks from Learned Optimization in Advanced Machine Learning Systems.
2021. arXiv: 1906.01820 [cs.AI]
• Richard Ngo, Lawrence Chan, and Sören Mindermann. The alignment problem from a deep
learning perspective. 2023. arXiv: 2209.00626 [cs.AI]
• Dan Hendrycks et al. Unsolved Problems in ML Safety. 2022. arXiv: 2109.13916 [cs.LG]

184
CHAPTER 3. SINGLE-AGENT SAFETY
3.10.2
References
[1]
Jeffrey Dastin. Amazon scraps secret AI recruiting tool that showed bias against women. 2018. url:
https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.
[2]
Tolga Bolukbasi et al. Man is to Computer Programmer as Woman is to Homemaker? Debiasing
Word Embeddings. 2016. arXiv: 1607.06520 [cs.CL].
[3]
Jieyu Zhao et al. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level
Constraints. 2017. arXiv: 1707.09457 [cs.AI].
[4]
Thomas Krendl Gilbert et al. Choices, Risks, and Reward Reports: Charting Public Policy for
Reinforcement Learning Systems. 2022. arXiv: 2202.05716 [cs.LG].
[5]
Jonathan Stray et al. What are you optimizing for? Aligning Recommender Systems with Human
Values. 2021. arXiv: 2107.10939 [cs.IR].
[6]
Ethan Kross et al. “Facebook use predicts declines in subjective well-being in young adults”. In: PloS
one (2013).
[7]
David Krueger, Tegan Maharaj, and Jan Leike. Hidden Incentives for Auto-Induced Distributional
Shift. 2020. arXiv: 2009.09153 [cs.LG].
[8]
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating Unwanted Biases with Adversarial
Learning. 2018. arXiv: 1801.07593 [cs.LG].
[9]
Inioluwa Deborah Raji et al. Closing the AI Accountability Gap: Defining an End-to-End Framework
for Internal Algorithmic Auditing. 2020. arXiv: 2001.00973 [cs.CY].
[10]
David Rozado. The unequal treatment of demographic groups by ChatGPT/OpenAI content moderation
system. 2023. url: https://davidrozado.substack.com/p/openaicms.
[11]
Zachary C. Lipton. “The Mythos of Model Interpretability: In Machine Learning, the Concept of
Interpretability is Both Important and Slippery.” In: Queue 16.3 (June 2018), pp. 31–57. issn:
1542-7730. doi: 10.1145/3236386.3241340. url: https://doi.org/10.1145/3236386.3241340.
[12]
Barret Zoph and Quoc V. Le. Neural Architecture Search with Reinforcement Learning. Feb. 2017.
arXiv: 1611.01578 [cs]. (Visited on 09/15/2023).
[13]
Sheng Zhang and Matthew Turk. Figure 2 - Creative Commons License. url: http : / / www .
scholarpedia.org/article/Eigenfaces.
[14]
David Bau et al. “Network Dissection: Quantifying Interpretability of Deep Visual Representations”. In:
2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017, pp. 3319–3327.
doi: 10.1109/CVPR.2017.354.
[15]
Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. “Feature Visualization”. In: Distill (2017).
https://distill.pub/2017/feature-visualization. doi: 10.23915/distill.00007.
[16]
Ludwig Schubert et al. OpenAI Microscope. Apr. 2020. (Visited on 09/15/2023).
[17]
Nelson Elhage et al. “Softmax linear units”. In: Transformer Circuits Thread (2022). url: https:
//transformer-circuits.pub/2022/solu/index.html.
[18]
Margot E. Kaminski. “The Right to Explanation, Explained”. In: Berkeley Tech. L.J.. Berkeley
Technology Law Journal 34.IR (), p. 189. url: http://lawcat.berkeley.edu/record/1128984.
[19]
M Elish. “Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction (WeRobot 2016)”. In:
SSRN Electronic Journal (Jan. 2016). doi: 10.2139/ssrn.2757236.
[20]
Marc W. Patry. “Attractive but Guilty: Deliberation and the Physical Attractiveness Bias”. In:
Psychological Reports 102.3 (June 2008), pp. 727–733. issn: 0033-2941. doi: 10.2466/pr0.102.3.727-
733. (Visited on 09/15/2023).
[21]
Edward de Haan et al. “Split-Brain: What We Know Now and Why This is Important for Understanding
Consciousness”. In: Neuropsychology Review 30 (June 2020). doi: 10.1007/s11065-020-09439-3.
[22]
Miles Turpin et al. Language Models Don’t Always Say What They Think: Unfaithful Explanations in
Chain-of-Thought Prompting. May 2023. doi: 10.48550/arXiv.2305.04388. eprint: 2305.04388 (cs).
(Visited on 09/15/2023).

3.10. LITERATURE
185
[23]
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep Inside Convolutional Networks:
Visualising Image Classification Models and Saliency Maps. Apr. 2014. eprint: 1312.6034 (cs). (Visited
on 09/15/2023).
[24]
Jost Tobias Springenberg et al. Striving for Simplicity: The All Convolutional Net. Apr. 2015. arXiv:
1412.6806 [cs]. (Visited on 09/15/2023).
[25]
Julius Adebayo et al. “Sanity Checks for Saliency Maps”. In: Advances in Neural Information Processing
Systems. Vol. 31. Curran Associates, Inc., 2018. (Visited on 09/15/2023).
[26]
Kevin Wang et al. Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2
small. 2022. arXiv: 2211.00593 [cs.LG].
[27]
Chris Olah et al. “Zoom In: An Introduction to Circuits”. In: Distill 5.3 (Mar. 2020), e00024.001. issn:
2476-0757. doi: 10.23915/distill.00024.001. (Visited on 09/15/2023).
[28]
Kevin Meng et al. Locating and Editing Factual Associations in GPT. The listed statement was inspired
by, but not included in this paper. Jan. 2023. eprint: 2202.05262 (cs). (Visited on 09/15/2023).
[29]
Andy Zou et al. Representation Engineering: A Top-Down Approach to AI Transparency. 2023. arXiv:
2310.01405 [cs.LG].
[30]
Jerry Tang et al. “Semantic reconstruction of continuous language from non-invasive brain recordings”.
In: bioRxiv (2022). doi: 10.1101/2022.09.29.509744. eprint: https://www.biorxiv.org/content/
early/2022/09/29/2022.09.29.509744.full.pdf. url: https://www.biorxiv.org/content/
early/2022/09/29/2022.09.29.509744.
[31]
Nora Belrose et al. LEACE: Perfect linear concept erasure in closed form. 2023. arXiv: 2306.03819
[cs.LG].
[32]
Collin Burns et al. Discovering Latent Knowledge in Language Models Without Supervision. 2022.
arXiv: 2212.03827 [cs.CL].
[33]
Yohan J. John et al. “Dead Rats, Dopamine, Performance Metrics, and Peacock Tails: Proxy Failure Is
an Inherent Risk in Goal-Oriented Systems”. In: Behavioral and Brain Sciences (June 2023), pp. 1–68.
issn: 0140-525X, 1469-1825. doi: 10.1017/S0140525X23002753. (Visited on 09/15/2023).
[34]
Jack Clark and Dario Amodei. Faulty Reward Functions in the Wild. Dec. 2016. (Visited on
09/15/2023).
[35]
Ziad Obermeyer et al. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of
Populations”. In: Science 366.6464 (Oct. 2019), pp. 447–453. doi: 10.1126/science.aax2342.
[36]
Joar Skalse et al. “Defining and Characterizing Reward Hacking”. In: (2022).
[37]
Alexander Pan, Kush Bhatia, and Jacob Steinhardt. “The effects of reward misspecification: Mapping
and mitigating misaligned models”. In: ICLR (2022).
[38]
Charles Goodhart. “Problems of Monetary Management : The U.K. Experience”. In: Papers in
monetary economics 1975 ; 1. Papers in Monetary Economics 1975 ; 1 ; 1. - [Sydney]. - 1975, p. 1-20
1 (1975).
[39]
Marilyn Strathern. “‘Improving Ratings’: Audit in the British University System”. In: European Review
5.3 (July 1997), pp. 305–321. issn: 1474-0575, 1062-7987. doi: 10.1002/(SICI)1234-981X(199707)5:
3<305::AID-EURO184>3.0.CO;2-4.
[40]
Kevin Roose. “The Making of a YouTube Radical”. In: The New York Times (June 2019). issn:
0362-4331. (Visited on 09/15/2023).
[41]
Paul Christiano et al. Deep Reinforcement Learning from Human Preferences. Feb. 2023. doi:
10.48550/arXiv.1706.03741. arXiv: 1706.03741 [cs, stat]. (Visited on 09/15/2023).
[42]
Harry Lambert. “The Great University Con: How the British Degree Lost Its Value”. In: New Statesman
(Aug. 2019). (Visited on 09/15/2023).
[43]
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial
Examples. Mar. 2015. eprint: 1412.6572 (cs, stat). (Visited on 09/15/2023).
[44]
Anish Athalye et al. Fooling Neural Networks in the Physical World. Oct. 2017. (Visited on 09/15/2023).

186
CHAPTER 3. SINGLE-AGENT SAFETY
[45]
Andy Zou et al. Universal and Transferable Adversarial Attacks on Aligned Language Models. July
2023. doi: 10.48550/arXiv.2307.15043. eprint: 2307.15043 (cs). (Visited on 09/15/2023).
[46]
J. R. French and Bertram H. Raven. “The bases of social power.” In: 1959.
[47]
B. H. Raven. “Social influence and power”. In: (1964).
[48]
T. Hobbes. Hobbes’s Leviathan. 1651. isbn: 978-5-87635-264-4.
[49]
Luke Muehlhauser and Anna Salamon. “Intelligence Explosion: Evidence and Import”. In: Singularity
Hypotheses: A Scientific and Philosophical Assessment. Ed. by Amnon H. Eden and James H. Moor.
Springer, 2012, pp. 15–40.
[50]
Alexander Pan et al. Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and
Ethical Behavior in the MACHIAVELLI Benchmark. 2023. arXiv: 2304.03279 [cs.LG].
[51]
Joseph Carlsmith. Is Power-Seeking AI an Existential Risk? 2022. arXiv: 2206.13353 [cs.CY]. url:
https://arxiv.org/abs/2206.13353.
[52]
Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. First edition. Oxford: Oxford University
Press, 2014. isbn: 978-0-19-967811-2.
[53]
Stephen Omohundro. “Artificial General Intelligence 2008: Proceedings of the First AGI Conference”.
In: Artificial General Intelligence 2008. 2008.
[54]
Stuart Russell. “Human-Compatible Artificial Intelligence”. In: Human-like machine intelligence
(2021), pp. 3–23.
[55]
Bowen Baker et al. Emergent Tool Use From Multi-Agent Autocurricula. Sept. 2019. (Visited on
09/16/2023).
[56]
David Silver. Exploration and exploitation. Lecture 9. 2014.
[57]
Charles W. Kegley and Shannon L. Blanton. Trend and Transformation, 2014 - 2015. 2020, p. 259.
[58]
John J. Mearsheimer. Structural realism. 2007, pp. 77–94.
[59]
Kenneth Waltz. Theory of international politics. Waveland Press, 2010, p. 93.
[60]
W. Julian Korab-Karpowicz. “Political Realism in International Relations”. In: The Stanford
Encyclopedia of Philosophy. Ed. by Edward N. Zalta. Summer 2018. Metaphysics Research Lab,
Stanford University, 2018.
[61]
Evan Montgomery. “Breaking Out of the Security Dilemma: Realism, Reassurance, and the Problem
of Uncertainty”. In: International Security - INT SECURITY 31 (Oct. 2006), pp. 151–185. doi:
10.1162/isec.2006.31.2.151.
[62]
Joseph Carlsmith. “Existential Risk from Power-Seeking AI”. In: Oxford University Press (2023).
[63]
P. W. Anderson. “More Is Different”. In: Science 177.4047 (Aug. 1972), pp. 393–396. doi: 10.1126/
science.177.4047.393.
[64]
Jacob Steinhardt. More Is Different for AI. Jan. 2022. (Visited on 09/16/2023).
[65]
Jason Wei et al. Emergent Abilities of Large Language Models. 2022. arXiv: 2206.07682 [cs.CL].
url: https://arxiv.org/abs/2206.07682.
[66]
Thomas McGrath et al. “Acquisition of chess knowledge in AlphaZero”. In: Proceedings of the
National Academy of Sciences 119.47 (Nov. 2022). doi: 10.1073/pnas.2206625119. url: https:
//doi.org/10.1073%2Fpnas.2206625119.
[67]
Sébastien Bubeck et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023.
arXiv: 2303.12712 [cs.CL].
[68]
GPT-4 System Card. Tech. rep. OpenAI, Mar. 2023, p. 60. (Visited on 09/16/2023).
[69]
Andy Zou et al. “Forecasting Future World Events with Neural Networks”. In: NeurIPS (2022).
[70]
Danijar Hafner. Benchmarking the Spectrum of Agent Capabilities. 2022. arXiv: 2109.06780 [cs.AI].
[71]
OpenAI Five Defeats Dota 2 World Champions. Apr. 2019. url: https://openai.com/research/
openai-five-defeats-dota-2-world-champions (visited on 09/16/2023).

3.10. LITERATURE
187
[72]
Joon Sung Park et al. Generative Agents: Interactive Simulacra of Human Behavior. Apr. 2023.
(Visited on 09/16/2023).
[73]
Evan Hubinger et al. “Risks from Learned Optimization in Advanced Machine Learning Systems”. In:
ArXiv (2019).
[74]
Johannes von Oswald et al. Uncovering Mesa-Optimization Algorithms in Transformers. Sept. 2023.
eprint: 2309.05858 (cs). (Visited on 09/13/2023).
[75]
Johannes Von Oswald et al. “Transformers Learn In-Context by Gradient Descent”. In: Proceedings of
the 40th International Conference on Machine Learning. PMLR, July 2023, pp. 35151–35174. (Visited
on 09/16/2023).
[76]
Nick Bostrom. Base Camp for Mt. Ethics. Tech. rep. 2022. (Visited on 09/16/2023).
[77]
Peter S. Park et al. AI Deception: A Survey of Examples, Risks, and Potential Solutions. Aug. 2023.
doi: 10.48550/arXiv.2308.14752. arXiv: 2308.14752 [cs]. (Visited on 09/16/2023).
[78]
Julien Perolat et al. “Mastering the Game of Stratego with Model-Free Multiagent Reinforcement
Learning”. In: Science 378.6623 (Dec. 2022), pp. 990–996. issn: 0036-8075, 1095-9203. doi: 10.1126/
science.add4679. eprint: 2206.15378 (cs). (Visited on 09/16/2023).
[79]
Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring How Models Mimic Human
Falsehoods. May 2022. doi: 10.48550/arXiv.2109.07958. eprint: 2109.07958 (cs). (Visited on
09/16/2023).
[80]
Anton Bakhtin et al. “Human-Level Play in the Game of Diplomacy by Combining Language Models
with Strategic Reasoning”. In: Science 378.6624 (Dec. 2022), pp. 1067–1074. doi: 10.1126/science.
ade9097. (Visited on 09/16/2023).
[81]
Paul Ekman. “Emotions revealed”. In: BMJ 328.Suppl S5 (2004). doi: 10.1136/sbmj.0405184. eprint:
https://www.bmj.com/content. url: https://www.bmj.com/content/328/Suppl_S5/0405184.
[82]
Philip Powell and Jennifer Roberts. “Situational determinants of cognitive, affective, and compassionate
empathy in naturalistic digital interactions”. In: Computers in Human Behaviour 68 (Nov. 2016),
pp. 137–148. doi: 10.1016/j.chb.2016.11.024.
[83]
Michael Wai and Niko Tiliopoulos. “The affective and cognitive empathic nature of the dark triad of
personality”. In: Personality and Individual Differences 52 (May 2012), pp. 794–799. doi: 10.1016/j.
paid.2012.01.008.
[84]
S. Baron-Cohen. Zero Degrees of Empathy: A New Theory of Human Cruelty. Penguin UK, 2011.
[85]
Tania Singer and Olga M. Klimecki. “Empathy and compassion”. In: Current Biology 24.18 (2014),
R875–R878. issn: 0960-9822. doi: https://doi.org/10.1016/j.cub.2014.06.054. url: https:
//www.sciencedirect.com/science/article/pii/S0960982214007702.
[86]
Russell Hotten. “Volkswagen: The Scandal Explained”. In: BBC News (Sept. 2015). (Visited on
09/16/2023).
[87]
Stephen Casper et al. Red Teaming Deep Neural Networks with Feature Synthesis Tools. July 2023.
eprint: 2302.10894 (cs). (Visited on 09/16/2023).
[88]
David Ili’c. “Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach”.
In: ArXiv abs/2310.11616 (2023).
[89]
Yohan John et al. “Dead rats, dopamine, performance metrics, and peacock tails: proxy failure is an
inherent risk in goal-oriented systems”. In: The Behavioral and brain sciences (June 2023), pp. 1–68.
doi: 10.1017/S0140525X23002753.
[90]
Dmitri Gallow. Instrumental Convergence? url: https : / / drive . google . com / file / d /
1TH2nh5EzwvNgDrqQcMU8oQNj4n9U7x4t/view.
[91]
Evan Hubinger et al. Risks from Learned Optimization in Advanced Machine Learning Systems. 2021.
arXiv: 1906.01820 [cs.AI].
[92]
Richard Ngo, Lawrence Chan, and Sören Mindermann. The alignment problem from a deep learning
perspective. 2023. arXiv: 2209.00626 [cs.AI].
[93]
Dan Hendrycks et al. Unsolved Problems in ML Safety. 2022. arXiv: 2109.13916 [cs.LG].

Safety Engineering
In developing an AI safety strategy, it might be tempting to draw parallels with other hazardous
technologies, from airplanes to nuclear weapons, and to devise analogous safety measures for AI.
However, while we can learn lessons from accidents and safety measures in other spheres, it is
important to recognize that each technology is unique, with its own specific set of applications
and risks. Attempting to map safety protocols from one area onto another might therefore prove
misleading, or leave gaps in our strategy where parallels cannot be drawn.
Instead of relying on analogies, we need a more general framework for safety, from which we can
develop a more comprehensive approach, tailored to the specific case in question. A good place
to start is with the field of safety engineering: a broad discipline that studies all sorts of systems
and provides a paradigm for avoiding accidents resulting from them. Researchers in this field have
identified fundamental safety principles and concepts that can be flexibly applied to novel systems.
We can view AI safety as a special case of safety engineering concerned with avoiding AI-related
catastrophes. To orient our thinking about AI safety, this chapter will discuss key concepts and
lessons from safety engineering.
Risk decomposition and measuring reliability.
To begin with, we will look at how we can
quantitatively assess and compare different risks using an equation involving two factors: the
probability and severity of an adverse event. By further decomposing risk into more elements, we
will derive a detailed risk equation, and show how each term can help us identify actions we can take
to reduce risk. We will also introduce a metric that links a system’s reliability to the amount of time
we can expect it to function before failing. For accidents that we would not be able to recover from,
this expected time before failure amounts to an expected lifespan.
Safe design principles and component failure accident models.
The field of safety engineering
has identified multiple “safe design principles” which can be built into a system to robustly improve
its safety. We will describe these principles and consider how they might be applied to systems
involving AI. Next, we will outline some traditional techniques for analyzing a system and identifying
the risks it presents. Although these methods can be useful in risk analysis, they are insufficient for
complex and sociotechnical systems, as they rely on assumptions that are often overly simple.
Systemic factors and systemic accident models.
After exploring the limitations of component
failure accident models, we will show that it can be more effective to address overarching systemic
factors than all the specific events that could directly cause an accident. We will then describe some
more holistic approaches to risk analysis and reduction. Systemic models rely on complex systems,
which we look at in more detail in the next chapter.
Tail events and black swans.
In the final sections of this chapter, we will introduce the concept
of tail events—events characterized by high impact and low probability—and show how they interfere
with standard methods of risk estimation. We will also look at a subset of tail events called black
swans, or unknown unknowns, which are tail events that are largely unpredictable. We will discuss
how emerging technology, including AI, might entail a risk of tail events and black swans, and we
will show how we can reduce those risks, even if we do not know their exact nature.
188

4.1. RISK DECOMPOSITION
189
4.1
Risk Decomposition
To reduce risks, we need to understand the factors contributing to them. In this section, we will
define some key terms from safety engineering. We will also discuss how we can decompose risks
into various factors and create risk equations based on these factors. These equations are useful for
quantitatively assessing and comparing different risks, as well as for identifying which aspects of risk
we can influence.
4.1.1
Failure Modes, Hazards, and Threats
Failure modes, hazards, and threats are basic words in a safety engineer’s vocabulary. We will now
define and give examples of each term.
A failure mode is a specific way a system could fail.
There are many ways in which different
systems can fail to carry out their intended functions. A valve leaking fluid could prevent the rest of
the system from working, a flat tire can prevent a car from driving properly, and losing connection
to the Internet can drop a video call. We can refer to all these examples as failure modes of different
systems. Possible failure modes of AI include AIs pursuing the wrong goals, or AIs pursuing simplified
goals in the most efficient possible way, without regard for unintended side effects.
A hazard is a source of danger that could cause harm.
Some systems can fail in ways
that are dangerous. If a valve is leaking a flammable substance, the substance could catch fire. A
flammable substance is an example of a hazard, or risk source, because it could cause harm. Other
physical examples of hazards are stray electrical wires and broken glass. Note that a hazard does not
pose a risk automatically; a shark is a hazard but does not pose a risk if no one goes in the water.
For AI systems, one possible hazard is a rapidly changing environment (“distribution shift”) because
an AI might behave unpredictably in conditions different from those it was trained in.
A threat is a hazard with malicious or adversarial intent.
If an individual is deliberately
trying to cause harm, they present a specific type of hazard: a threat. Examples of threats include a
hacker trying to exploit a weakness in a system to obtain sensitive data or a hostile nation gaining
more sophisticated weapons. One possible AI-related threat is someone deliberately contaminating
training data to cause an AI to make incorrect and potentially harmful decisions based on hidden
malicious functionality.
4.1.2
The Classic Risk Equation
Different hazards and threats present different levels of risk, so it can be helpful to quantify and
compare them. We can do this by decomposing risk into multiple factors and constructing an
equation. We will now break down risk into two components, and then later discuss a more detailed
four-factor decomposition.
Risk can be broken down into probability and severity.
Two main components affect the
level of risk a hazard poses: the probability that an accident will occur and the amount of harm that
will be done if it does happen. This can be represented mathematically as follows:
Risk(hazard) = P(hazard) × severity(hazard).
where P(·) indicates the probability of an event. This is the classic formulation of risk.

190
CHAPTER 4. SAFETY ENGINEERING
The risk posed by a volcano can be assessed using the probability of eruption, denoted as P(eruption),
and the severity of its impact, denoted as severity(eruption). If a volcano is highly likely to erupt but
the surrounding area is uninhabited, the risk posed by the volcano is low because severity(eruption)
is low. On the other hand, if the volcano is dormant and likely to remain so but there are many
people living near the volcano, the risk is also low because P(eruption) is low. In order to accurately
evaluate the risk, both the probability of eruption and its severity need to be taken into account.
Applying the classic risk equation to AI.
The equation above tells us that, to evaluate the
risk associated with an AI system, we need information about two aspects of it: the probability that
it will do something unintended, and the severity of the consequences if it does. For example, an
AI system may be intelligent enough to capture power over critical infrastructure, with potentially
catastrophic consequences for humans. However, to estimate the level of this risk, we also need to
know how likely it is that the system will try to capture power. A demonstration of a potential
catastrophic hazard does not necessarily imply the risk is high.
The total risk of a system is the sum of the risks of all associated hazards.
In general,
there may be multiple hazards associated with a system or situation. For example, a car driving safely
depends on many vehicle components functioning as intended, and also depends on environmental
factors, such as weather conditions and the behavior of other drivers and pedestrians. There are
therefore multiple hazards associated with driving. To find the total risk, we can apply the risk
equation to each hazard separately and then add the results together.
Risk =
X
hazard
P(hazard) × severity(hazard)
We may not always have exact numerical values.
We may not always be able to assign exact
quantities to the probability and severity of all the hazards, and may therefore be unable to precisely
quantify total risk. However, even in these circumstances, we can use estimates. If estimates are
difficult to obtain, it can still be useful to have an equation that helps us understand how different
factors contribute to risk.
4.1.3
Framing the Goal as Risk Reduction
We should aim for risk reduction rather than trying to achieve zero risk.
It might be an
appealing goal to reduce the risk to zero by seeking ways of reducing the probability or severity to
zero. However, in the real world, risk is never zero. In the AI safety research community, for example,
some talk of “solving the alignment problem”—aligning AI with human values perfectly. This could,
in theory, result in zero probability of AIs making a catastrophic decision and thus eliminate AI risk
entirely.
However, reducing risk to zero is likely impossible. Framing the goal as eliminating risk implies
that finding a perfect, airtight solution for removing risk is possible and realistic. Focusing narrowly
on this goal could be counterproductive, as it might distract us from developing and implementing
practical measures that significantly reduce risk. In other words, we should not “let the perfect be the
enemy of the good.” When thinking about creating AI, we do not talk about “solving the intelligence
problem” but about “improving capabilities.” Similarly, when thinking about AI safety, we should
not talk about “solving the alignment problem” but rather about “making AI safer” or “reducing risk
from AIs.” A better goal could be to make catastrophic risks negligible (for instance, less than 0.01%
of an existential catastrophe per century) rather than trying to have the risk become exactly zero.

4.1. RISK DECOMPOSITION
191
4.1.4
Disaster Risk Equation
The classic risk equation is a useful starting point for evaluating risk. However, if we have more
information about the situation, we can break down the risk from a hazard into finer categories. First
we can think about the intrinsic hazard level, which is a shorthand for probability and severity as in
the classic risk equation. Additionally, we can consider how the hazard interacts with the people at
risk: we can consider the amount of exposure and the level of vulnerability [1].
Figure 4.1: Risk can be broken down into exposure, probability, severity, and vulnerability. Probability
and severity together determine the “intrinsic hazard level.”
4.1.5
Elements of the Risk Equation
Exposure and probability are relevant before the accident, while severity and vulnerability matter
during it. We can explain these terms in the context of a floor that is hazardously slippery:
1. Exposure is a measure of how much we are exposed to a hazard. It will be more
likely that someone will slip on a wet floor if there are more people walking across it and if the
floor remains slippery for longer. We can say that this is because the exposure to the possibility
of someone slipping is higher.
2. Probability tells us how likely it is that a hazard will lead to an accident. The more
slippery the floor is, the more likely it is that someone will slip and fall on it. This is separate
from exposure: it already assumes that they are walking on the floor. Probability and exposure
together determine how likely an accident is.
3. Severity indicates how intrinsically harmful a hazard is. A carpeted floor is less risky
than a marble one. Part of the severity of a slippery floor would be how hard it is. This term
represents the extent of damage an accident would inflict. We can refer to probability and
severity together as the intrinsic hazard level.
4. Vulnerability measures how susceptible we are to harm from a hazard. If someone
does slip on a floor, the harm caused is likely to depend partly on factors such as bone density
and age, that together determine how susceptible they are to bone fracture. We can refer to
this as vulnerability. Vulnerability and severity together determine how harmful an accident is.
Note that probability and severity are mostly about the hazard itself, whereas exposure and
vulnerability tell us more about those subject to the risk.
With these terms, we can construct a more detailed equation.
Sometimes, but not always,
it is more convenient to use this risk decomposition. Rather than being mathematically rigorous, this
equation is intended to convey that increasing any of the terms being multiplied will increase the
risk, and reducing any of these terms will reduce it. Additionally, reducing any of the multiplicative
terms to zero will reduce the risk to zero for a given hazard, regardless of how large the other factors
are. Once more, we can add together the risk estimates for each independent hazard to find the total

192
CHAPTER 4. SAFETY ENGINEERING
risk for a system.
Risk =
X
hazardous
event h
P(h) × severity(h) × exposure(h) × vulnerability(h)
4.1.6
Applying the Disaster Risk Equation
Infection by a virus is an example hazard.
Consider these ideas in the context of a viral
infection. The hazard severity and probability of a virus refers to how bad the symptoms are and how
infectious it is. An individual’s exposure relates to how much they come into contact with the virus.
Their vulnerability relates to how strong their immune system is and whether they are vaccinated. If
the virus is certainly deadly once infected, we might consider ourselves extremely vulnerable.
Decomposing risks in detail can help us identify practical ways of reducing them.
As
well as helping us evaluate a risk, the equation above can help us understand what we can do to
mitigate it. We might be unable to change how intrinsically virulent a virus is, for instance, and so
unable to affect the hazard’s severity. However, we could reduce exposure to it by wearing masks,
avoiding large gatherings, and washing hands. We could reduce our vulnerability by maintaining a
healthy lifestyle and getting vaccinated. Taking any of these actions will decrease the overall risk. If
we are facing a deadly disease, then we might take extreme actions like quarantining ourselves to
reduce the exposure of the hazard to reduce overall risk to manageable levels.
Not all risks can be calculated precisely, but decomposition still helps reduce them.
An
important caveat to the disaster risk equation is that not all risks are straightforward to calculate, or
even to predict. Nonetheless, even if we cannot put an exact number on the risk posed by a given
hazard, we can still reduce it by decreasing our exposure or vulnerability, or the intrinsic hazard level
itself, where possible. Similarly, even if we cannot predict all hazards associated with a system—for
example if we face a risk of unknown unknowns, which are explored later in this chapter—we can
still reduce the overall risk by addressing the hazards we are aware of.
In AI safety, the risk equation suggests three important research areas.
As with other
hazards, we should look for multiple ways of preventing and protecting against potential adverse
events associated with AI. There are three key areas of research that can each be viewed as inspired
by a component of the disaster risk equation: robustness (e.g. adversarial robustness), monitoring
(e.g. transparency, trojan detection, anomaly detection), and control (e.g. reducing power-seeking
drives, representation control). These research areas correspond to reducing the vulnerability of
AIs to adversarial attacks, exposure to hazards by monitoring and avoiding them, and hazard level
(probability and severity of potential damage) by ensuring AIs are controllable and inherently less
hazardous. To reduce AI risk, it is crucial to pursue and develop all three, rather than relying on
just one.
Example hazard: proxy gaming.
Consider proxy gaming, a risk we face from AIs that was
discussed in the Single Agent Safety chapter. Proxy gaming might arise when we give AI systems
goals that are imperfect proxies of our goals. An AI might then learn to “game” or over-optimize
these proxies in unforeseen ways that diverge from human values. We can tackle this threat in many
different ways:
1. Reduce our exposure to this proxy gaming hazard by improving our abilities to monitor
anomalous behavior and flag any signs that a system is proxy gaming at an early stage.

4.2. NINES OF RELIABILITY
193
2. Reduce the hazard level by making AIs want to optimize an idealized goal and make mistakes
less hazardous by controlling the power the AI has, so that if it does overoptimize the proxy it
would do less harm.
3. Reduce our vulnerability by making our proxies more accurate, by making AIs more adversarially
robust, or by reducing our dependence on AIs.
Systemic safety addresses factors external to the AI itself.
The three fields outlined above
focus on reducing risk through the design of AIs themselves. Another approach, called systemic
safety, considers the environment within which the AI operates and attempts to remove or reduce the
hazards that it might otherwise interact with. For example, improving information security reduces
the chance of a malicious actor accessing a lethal autonomous weapon, while addressing inequality
and improving mental health across society could reduce the number of people who might seek to
harness AI to cause harm.
Adding ability to cope can improve the disaster risk equation.
There are other factors that
could be included in the disaster risk equation. We can return to our example of the slippery floor to
illustrate one of these factors. After slipping on the floor, we might take less time to recover if we
have access to better medical technology. This tells us to what extent we would be able to recover
from the damage the hazard caused. We can refer to the capacity to recover as our ability to cope.
Unlike the other factors that multiply together to give us an estimate of risk, we might divide by
ability to cope to reduce our estimate of the risk if our ability to cope with it is higher. This is a
common extension to the disaster risk equation.
Some hazards are extremely damaging and eliminate any chance of recovery: the severity of the
hazard and our vulnerability are high, while our ability to cope is tiny. This constitutes a risk of
ruin–—permanent, system-complete destruction. In this case, the equation would involve multiplying
together two large numbers and dividing by a small number; we would calculate the risk as being
extremely large. If the damage cannot be recovered from, like an existential catastrophe (e.g., a
large asteroid or sufficiently powerful rogue AIs), the risk equation would tell us that the risk is
astronomically large or infinite.
Summary.
We can evaluate a risk by breaking it down into the probability of an adverse event and
the amount of harm it would cause. This enables us to quantitatively compare various kinds of risks.
If we have enough information, we can analyze risks further in terms of our level of exposure to them
and how vulnerable we are to damage from them, as well as our ability to cope. Even if we cannot
assign an exact numerical value to a risk, we can estimate it. If our estimates are unreliable, this
decomposition can still help us to systematically identify practical measures we can take to reduce
the different factors and thus the overall risk.
4.2
Nines of Reliability
In the above discussion of risk evaluation, we have frequently referred to the probability of an adverse
event occurring. When evaluating a system, we often instead refer to the inverse of this—the system’s
reliability, or the probability that an adverse event will not happen, usually presented as a percentage
or decimal. We can relate system reliability to the amount of time that a system is likely to function
before failing. We can also introduce a new measure of reliability that conveys the expected time
before failure more intuitively.

194
CHAPTER 4. SAFETY ENGINEERING
The more often we use a system, the more likely we are to encounter a failure.
While
a system might have an inherent level of reliability, the probability of encountering a failure also
depends on how many times it is used. This is why, as discussed above, increasing exposure to a
hazard will increase the associated level of risk. An autonomous vehicle, for example, is much more
likely to make a mistake during a journey where it has to make 1000 decisions, than during a journey
where it has to make only 10 decisions.
% reliability of
% risk of mistake
Nines of Reliability
A mistake is likely to occur
system
by decision number...
0
100
0
1
50
50
0.3
2
75
25
0.6
4
90
10
1
10
99
1
2
100
99.9
0.1
3
1000
99.99
0.01
4
10,000
Table 4.1: From each level of system reliability, we can infer its probability of mistake, “nine or
reliability,” and expected time before failure.
For a given level of reliability, we can calculate an expected time before failure. Imagine that we
have several autonomous vehicles with different levels of reliability, as shown in Table 4.2. Reliability
is the probability that the vehicle will get any given decision correct. The second column shows the
complementary probability: the probability that the AV will get any given decision wrong. The
fourth column shows the number of decisions within which the AV is expected to make one mistake.
This can be thought of as the AV’s expected time before failure.

4.2. NINES OF RELIABILITY
195
1
10
50
100
0%
25%
50%
75%
100%
Reliability (%)
Expected Time Before Failure
System Reliability vs Expected Time Before Failure
Figure 4.2: Halving the probability of a mistake doubles the expected time before failure. Therefore,
the relationship between system reliability and expected time before failure is non-linear.
Expected time before failure does not scale linearly with system reliability.
We plot the
information from the table in Figure 4.2. From looking at this graph, it is clear that the expected
time before failure does not scale linearly with the system’s reliability. A 25% change that increases
the reliability from 50% to 75%, for example, doubles the expected time before failure. However, a
9% change increasing the reliability from 90% to 99% causes a ten-fold increase in the expected time
before failure, as does a 0.9% increase from 99% to 99.9%.
The closer we get to 100% reliability, the more valuable any given increment of improvement will
be. However, as we get closer to 100% reliability, we can generally expect that an increment of
improvement will become increasingly difficult to obtain. This is usually true because it is hard to
perfectly eliminate the possibility of any adverse event. Additionally, there may be risks that we
have not considered. These are called unknown unknowns and will be discussed extensively later in
this chapter.
A system with 3 “nines of reliability” is functioning 99.9% of the time.
As we get close to
100% reliability, it gets inconvenient to use long decimals to express how reliable a system is. The
third column in table 4.2 gives us information about a different metric: the nines of reliability [2].
Informally, a system has nines of reliability equal to the number of nines at the beginning of its
decimal or percentage reliability. One nine of reliability means a reliability of 90% in percentage
terms or 0.9 in decimal terms. Two nines of reliability mean 99%, or 0.99. We can denote a system’s
nines of reliability with the letter k; if a system is 90% reliable, it has one nine of reliability and so
k = 1; if it is 99% reliable, it has two nines of reliability, and so k = 2. Formally, if p is the system’s
reliability expressed as a decimal, we can define k, the nines of reliability a system possesses, as:

196
CHAPTER 4. SAFETY ENGINEERING
1
10
100
1000
10000
0
1
2
3
4
Nines of Reliability
Expected Time Before Failure (Log Scale)
Nines of Reliability vs Expected Time Before Failure
Figure 4.3: When we plot the nines of reliability against the expected time before failure on a
logarithmic scale, the result is a straight line.
k = −log10(1 −p).
Adding a ‘9’ to reliability gives a tenfold increase in expected time before failure.
If it
is 99% reliable, it has a 1% probability of failure. If it is 99.9% reliable, it has a 0.1% probability of
failure. Therefore, adding another consecutive 9 to the reliability corresponds to a tenfold reduction
in risk, and therefore a tenfold increase in the expected time before failure, as we can see in the
graph. This means that the relationship between system reliability and expected time before failure
is not linear. However, the relationship between nines of reliability and the logarithm of expected
time before failure is linear. Note that if we have a system where a failure would mean permanent
societal-scale destruction, then the expected time before failure is essentially the expected lifespan of
human civilization. Increasing such a system’s reliability by one nine would cause a ten-fold increase
in the expected lifespan of human civilization.
The nines of reliability metric can provide a relatively intuitive sense of the difference
between various levels of reliability.
Looking at the table, we can think of this metric as a
series of levels, where going up one level means a tenfold increase in expected time before failure.
For example, if a system has four nines of reliability, we can expect it to last 100 times longer
before failing than if it has two. This is an advantage of using the nines of reliability: thinking
logarithmically can give us a better understanding of the actual value of an improvement than if
we say we’re improving the reliability from 0.99 to 0.9999. In the latter case, the numbers begin
to look the same, but, in terms of expected time before failure, this improvement is actually more
meaningful than going from 0.4 to 0.8.

4.3. SAFE DESIGN PRINCIPLES
197
The nines of reliability are only a measure of probability, not risk.
In the framing of
the classic risk equation, the nines of reliability only contain information about the probability of a
failure, not about what its severity would be. This metric is therefore incomplete for evaluating risk.
If an AI has three nines of reliability, for example, we know that it is expected to make 999 out of
1000 decisions correctly. However, three nines of reliability tells us nothing about how much damage
the agent will do if it makes an incorrect decision, so we cannot calculate the risk involved in using
the system. A game playing AI will present a lot less risk than an autonomous vehicle even if both
systems have three nines of reliability.
Summary.
A system’s nines of reliability indicate the number of consecutive nines at the beginning
of its percentage or decimal reliability. An additional nine of reliability represents a reduction of
probability of failure by a factor of 10, and thus a tenfold increase of expected time before failure.
Nines of reliability tell us about the probability of an accident, but do not contain any information
about the severity of an accident, and can therefore not be used alone to calculate risk. In the case
of a risk of ruin, an additional nine of reliability means a tenfold increase in expected lifespan.
4.3
Safe Design Principles
We can reduce both the probability and severity of a system failure by following certain safe design
principles when designing it. These general principles have been identified by safety engineering
and offer practical ways of reducing the risk associated with all kinds of systems. They should be
incorporated from the outset, rather than being retrofitted later. This strategy attempts to “build
safety into” a system, and is more robust than building the system without safety considerations and
then attempting to fix individual problems if and when they become apparent.
Note that these principles are not only useful in building an AI itself, but also the system around it.
For example, we can incorporate them into the design of the cyber-security system that controls who
is able to access an AI, and into the operations of the organization, or system of humans, that is
creating an AI.
We will now explore eight of these principles and how they might be applied to AI systems:
1. Redundancy: having multiple backup components that can perform each critical function, so
that a single component failing is not enough to cause an accident.
2. Transparency: ensuring that operators have enough knowledge of how a system functions
under various circumstances to interact with it safely.
3. Separation of duties: having multiple agents in charge of subprocesses so that no individual
can misuse the entire system alone.
4. Principle of least privilege: giving each agent the minimum access necessary to complete
their tasks.
5. Fail-safes: ensuring that the system will be safe even if it fails.
6. Antifragility: learning from previous failures to reduce the likelihood of failing again in future.
7. Negative feedback mechanisms:
building in processes that naturally down-regulate
operations in the event that operators lose control of the system.
8. Defense in depth: employing multiple safe design principles rather than relying on just one,
since any safety feature will have weaknesses.

198
CHAPTER 4. SAFETY ENGINEERING
Note that, depending on the exact type of system, some of these safe design principles might be less
useful or even counterproductive. We will discuss this later on in the chapter. However, for now, we
will explore the basic rationale behind why each one improves safety.
4.3.1
Redundancy
Redundancy means having multiple “backup” components [1].
Having multiple braking
systems in a vehicle means that, even if the foot brake is not working well, the handbrake should still
be able to decelerate the vehicle in an emergency. A failure of a single brake should therefore not
be enough to cause an accident. This is an example of redundancy, where multiple components can
perform a critical function, so that a single component failing is not enough to cause the whole system
to fail. In other words, redundancy removes single points of failure. Other examples of redundancy
include saving important documents on multiple hard drives, in case one of them stops working, and
seeking multiple doctors’ opinions, in case one of them gets a diagnosis wrong.
A possible use of redundancy in AI would be having an inbuilt “moral parliament” (see Moral
Parliament in the Machine Ethics chapter).
If an AI agent has to make decisions with moral
implications, we are faced with the question of which theory of morality it should follow; there are
many of these, and each often has counterintuitive recommendations in extreme cases. Therefore, we
might not want an AI to adhere strictly to just one theory. Instead, we could use a moral parliament,
in which we emulate representatives of stakeholders or moral theories, let them negotiate and vote,
and then do what the parliament recommends. The different theories would essentially be redundant
components, each usually recommending plausible actions but unable to dictate what happens in
extreme cases, reducing the likelihood of counterintuitive decisions that we would consider harmful.
4.3.2
Separation of Duties
Separation of duties means no single agent can control or misuse the system alone [1].
Consider a system where one person controls all the different components and processes. If that
person decides to pursue a negative outcome, they will be able to leverage the whole system to do
so. On the other hand, we could separate duties by having multiple operators, each in charge of a
different aspect. In this case, if one individual decides to pursue a negative outcome, their capacity
to do harm will be smaller.
For example, imagine a lab that handles two chemicals that, if mixed in high enough quantities,
could cause a large explosion. To avoid this happening, we could keep the stock of the two chemicals
in separate cupboards, and have a different person in charge of supplying each one in small quantities
to researchers. This way, no individual has access to a large amount of both chemicals.
We could focus on multiple narrow AI models, instead of a single general one.
In
designing AI systems, we could follow this principle by having multiple agents, each of which is
highly specialized for a different task. Complex processes can then be carried out collectively by
these agents working together, rather than having a single agent conducting the whole process alone.
This is exemplified by an approach to AI development called “comprehensive AI services.” This views
AI agents as a class of service-providing products. Adopting this mindset might mean tailoring AI
systems to perform highly specific tasks, rather than speculatively trying to improve general and
flexible capabilities in a single agent.

4.3. SAFE DESIGN PRINCIPLES
199
4.3.3
Principle of Least Privilege
Each agent should have only the minimum power needed to complete their tasks [1].
As
discussed above, separating duties should reduce individuals’ capacity to misuse the system. However,
separation of duties might only work if we also ensure individuals do not have access to parts of
the system that are not relevant to their tasks. This is called the principle of least privilege. In the
example above, we ensured separation of duties by putting chemicals in different cupboards with
different people in charge of them. To make this more likely to mitigate risks, we might want to
ensure that these cupboards are locked so that everyone else cannot access them at all.
Similarly, for systems involving AIs, we should ensure that each agent only has access to the necessary
information and power to complete its tasks with a high level of reliability. Concretely, we might
want to avoid plugging AIs into the internet or giving them high-level admin access to confidential
information. In the Single Agent Control chapter, we considered how AIs might be power-seeking; by
ensuring AIs have only the minimum required amount of power they need to accomplish the goals we
assign them, we can reduce their ability to gain power.
4.3.4
Fail-Safes
Fail-safes are features that aim to ensure a system will be safe even if it fails [1].
When
systems fail, they stop performing their intended function, but some failures also cause harm. Fail-
safes aim to limit the amount of harm caused even if something goes wrong. Elevator brakes are
a classic example of a fail-safe feature. They are attached to the outside of the cabin and are held
open only by the tension in the cables that the cabin is suspended on. If tension is lost in the cables,
the brakes automatically clamp shut onto the rails in the elevator shaft. This means that, even if the
cables break, the brakes should prevent the cabin from falling; even if the system fails in its function,
it should at least be safe.
A possible fail-safe for AI systems might be a component that tracks the level of confidence an agent
has in its own decisions. The system could be designed to stop enacting decisions if this component
falls below a critical level of certainty that the decision is correct. There could also be a component
that monitors the probability of the agent’s decisions causing harm, and the system could be designed
to stop acting on decisions if it reaches a specified likelihood of harm.
4.3.5
Antifragility
Antifragile systems become stronger from encountering adversity [3].
The idea of an
antifragile system is that it will not only recover after a failure or a near miss but actually become
more robust from these “stressors” to potential future failures. Antifragile systems are common
in the natural world and include the human body. For example, weight-bearing exercises put a
certain amount of stress on the body, but bone density and muscle mass tend to increase in response,
improving the body’s ability to lift weight in the future.
Similarly, after encountering or becoming infected with a pathogen and fighting it off, a person’s
immune system tends to become stronger, reducing the likelihood of reinfection. Groups of people
working together can also be antifragile. If a team is working toward a given goal and they experience
a failure, they might examine the causes and take steps to prevent it from happening again, leading
to fewer failures in the future.
Designing AI systems to be antifragile would mean allowing them to continue learning and adapting
while they are being deployed. This could give an AI the potential to learn when something in its

200
CHAPTER 4. SAFETY ENGINEERING
environment has caused it to make a bad decision. It could then avoid making the same mistake if it
finds itself in similar circumstances again.
Antifragility can require adaptability.
Creating antifragile AIs often means creating adaptive
ones: the ability to change in response to new stressors is key to making AIs robust. If an AI
continues learning and adapting while being deployed, it could learn to avoid hazards, but it could
also develop unanticipated and undesirable behaviors. Adaptive AIs might be harder to control.
Such AIs are likely to continuously evolve, creating new safety challenges as they develop different
behaviors and capabilities. This tendency of adaptive systems to evolve in unexpected ways increases
our exposure to emergent hazards.
A case in point is the chatbot Tay, which was released by Microsoft on Twitter in 2016. Tay was
designed to simulate human conversation and to continue improving by learning from its interactions
with humans on Twitter. However, it quickly started tweeting offensive remarks, including seemingly
novel racist and sexist comments. This suggested that Tay had statistically identified and internalized
some biases that it could then independently assert. As a result, the chatbot was taken offline
after only 16 hours. This illustrates how an adaptive, antifragile AI can develop in unpredicted
and undesirable ways when deployed in natural settings. Human operators cannot control natural
environments, so system designers should think carefully about whether to use adaptive AIs.
4.3.6
Negative Feedback Mechanisms
When one change triggers more changes, feedback loops can emerge.
To understand
positive and negative feedback mechanisms, consider the issue of climate change and melting ice. As
global temperatures increase, more of Earth’s ice melts. This means ice-covered regions shrink, and
therefore reflect a smaller amount of the sun’s radiation back into space. More radiation is therefore
absorbed by the atmosphere, further increasing global temperatures and causing even more ice to
melt. This is a positive feedback loop: a circular process that amplifies the initial change, causing
the system to continue escalating itself unchecked. We discuss feedback loops in greater detail in the
Complex Systems chapter.
Negative feedback mechanisms act to down-regulate and stabilize systems.
If we have
mechanisms in place that naturally down-regulate the initial change, we are likely to enter an
equilibrium rather than explosive change. Many negative feedback mechanisms are found within the
body; for example, if a person’s body temperature increases, they will begin to sweat, cooling down
as that sweat evaporates. If, on the other hand, they get cold, they will begin to shiver, generating
heat instead. These negative feedback mechanisms act against any changes and thus stabilize the
temperature within the required range. Incorporating negative feedback mechanisms in a system’s
design can improve controllability, by preventing changes from escalating too much [4].
We can use negative feedback loops to control AIs.
If we are concerned that AIs might get
too powerful for us to control, we can create negative feedback loops in the environment to ensure
that any increases in an AI’s power are met with changes that make it less powerful. There would
be two parts to this process. First, we would want better monitoring tools to look for anomalies,
such as AI watch dogs. These would track when an AI is getting powerful (or displaying hazardous
behavior) in order to trigger some feedback mechanism—the second part of the task. The feedback
mechanism might be a drastic measure like disconnecting an AI from the internet, resetting an AI to
a previous version, or using other AIs trained to disempower a powerful AI. Such mechanisms would
act as automatic checks and balances on AIs’ power.

4.3. SAFE DESIGN PRINCIPLES
201
4.3.7
Transparency
Transparency means people know enough about a system to interact with it safely [1].
If operators do not have sufficient knowledge of a system’s functions, then it is possible they could
inadvertently cause an accident while interacting with it. It is important that a pilot knows how a
plane’s autopilot system works, how to activate it, and how to override it. That way, they will be
able to override it when they need to, and they will know how to avoid activating it or overriding it
accidentally when they do not mean to. This safe design principle is called transparency.
Research into AI transparency aims to design deep learning systems in ways that give operators
a greater understanding of their internal decision-making processes. This would help operators
maintain control, anticipate situations in which systems might make poor or deceptive decisions, and
steer them away from hazards.
4.3.8
Defense in Depth
Including many layers of defense is usually more effective than relying on just one [1].
A final safe design principle is defense in depth, which means including multiple layers of defense.
That way, if one or more defenses fail, there should still be others in place that can mitigate damage.
In general, the more defenses a system has in place, the less likely it is that all the defenses will fail
simultaneously. The core idea of defense in depth is that it is unlikely that any one layer of defense is
foolproof; we are usually engaging in risk reduction, not risk elimination. For example, an individual
is less likely to be infected by a virus if they take multiple measures, such as wearing a mask, social
distancing, and washing their hands, than if they rely on just one of these (and no single one is going
to work). We will explore this in greater depth in the context of the Swiss cheese model in the next
section.
One caveat to note here is that increasing layers of defense can make a system more complex. If the
different defenses interact with one another, there is a chance that this might produce unintended
side effects. In the case of a virus, for example, reduced social contact and an individual’s immune
system can be thought of as two separate layers of defense. However, if an individual has very little
social contact, and therefore little exposure to pathogens, their immune system could become weaker
as a result. While multiple layers of defense can make a system more robust, system designers should
be aware that layers might interact in complex ways. We will discuss this further later in the chapter.
Layers of safety features can generally be preventative or protective.
There are two
ways in which safety measures can reduce risk: preventative measures reduce the probability of
an accident occurring, while protective measures reduce the harm if an accident does occur. For
example, avoiding large gatherings and washing hands are actions that aim to prevent an individual
from becoming infected with a virus, while maintaining a healthy lifestyle and getting vaccinated can
help reduce the severity of an infection if it does occur.
We can think about this in terms of the risk equations. Preventative measures reduce the probability
of an accident occurring, either by reducing the inherent probability of the hazard or by reducing
exposure to it. Protective measures, meanwhile, decrease the severity an accident would have, either
by reducing the inherent severity of the hazard or by making the system less vulnerable to it.
In general, prevention is more efficient than cure, but both should be included in
system design.
An oft-quoted aphorism is that “an ounce of prevention is worth a pound of cure,”
highlighting that it is much better—and often less costly–—to prevent an accident from happening
than to try and fix it afterward. It might therefore be wise for system designers to place more

202
CHAPTER 4. SAFETY ENGINEERING
emphasis on preventative features than on protective ones.
Nevertheless, protective features should not be neglected. This is illustrated by the sinking of
the Titanic, whose preventative design features included the hull being divided into watertight
compartments. There was much faith that these features rendered it unsinkable. However, the ship
did not carry enough lifeboats to hold all its passengers. This was, in part, because lifeboats were
largely intended to transport passengers to another ship in the event of sinking, rather than to hold
all of them at once. Still, the insufficient provision meant that there was not enough space on the
lifeboats for all the passengers when the ship sank. This can be considered an example of inadequate
protective measures. We will explore this distinction more in the next section, where we discuss the
bow tie model.
4.3.9
Review of Safe Design Principles
There are multiple features we can build into a system from the design stage to make it safer. We
have discussed redundancy, separation of duties, the principle of least privilege, fail-safes, antifragility,
negative feedback mechanisms, transparency and defense in depth as eight examples of such principles.
Each one gives us concrete recommendations about how to design (or how not to design) AI systems
to ensure that they are safer for humans to use.
4.4
Component Failure Accident Models and Methods
As a system is being created and used, it is important to analyze it to identify potential risks. One
way of doing this is to look at systems through the lens of an accident model: a theory about how
accidents happen in systems and the factors that lead to them [4]. We will now look at some common
accident models. These impact system design and operational decisions.
4.4.1
Swiss Cheese Model
The Swiss cheese model helps us analyze defenses and identify pathways to accidents
[5].
The diagram in Figure 4.4 shows multiple slices of Swiss cheese, each representing a particular
defense feature in a system. The holes in a slice represent the weaknesses in a defense feature—the
ways in which it could be bypassed. If there are any places where holes in all the slices line up,
creating a continuous hole through all of them, this represents a possible route to an accident. This
model highlights the importance of defense in depth, since having more layers of defense reduces the
probability of there being a pathway to an accident that can bypass them all.

4.4. COMPONENT FAILURE ACCIDENT MODELS AND METHODS
203
Figure 4.4: Each layer of defense (safety culture, red teaming, etc.) is a layer of defense with its own
holes in the Swiss cheese model. With enough layers, we hope to avoid pathways that can bypass
them all.
Consider the example of an infectious disease as a hazard. There are multiple possible defenses that
reduce the risk of infection. Preventative measures include avoiding large gatherings, wearing a mask
and regularly washing hands. Protective measures include maintaining a healthy lifestyle to support
a strong immune system, getting vaccinated, and having access to healthcare. Each of these defenses
can be considered a slice of cheese in the diagram.
However, none of these defenses are 100% effective. Even if an individual avoids large gatherings,
they could still become infected while buying groceries. A mask might mostly block contact with
the pathogen, but some of it could still get through. Vaccination might not protect those who are
immunocompromised due to other conditions, and may not be effective against all variants of the
pathogen. These imperfections are represented by the holes in the slices of cheese. From this, we can
infer various potential routes to an accident, such as an immunocompromised person with a poorly
fitting mask in a shopping mall, or someone who has been vaccinated encountering a new variant at
the shops that can evade vaccine-induced immunity.
We can improve safety by increasing the number of slices, or by reducing the holes.
Adding more layers of defense will reduce the chances of holes lining up to create an unobstructed
path through all the defenses. For example, adopting more of the practices outlined above would
reduce an individual’s chances of infection more than if they adopt just one.
Similarly, reducing the size of a hole in any layer of defense will reduce the probability that it will
overlap with a hole in another layer. For example, we could reduce the weaknesses in wearing a
mask by getting a better-fitting, more effective mask. Scientists might also develop a vaccine that is
effective against a wider range of variants, thus reducing the weaknesses in vaccination as a layer of
defense.
We can think of layers of defense as giving us additional nines of reliability.
In many
cases, it seems reasonable to assume that adding a layer of defense helps reduce remaining risks by
approximately an order of magnitude by eliminating 90% of the risks still present. Consider how
adding the following three layers of defense can give our AIs additional nines of reliability:
1. Adversarial fine-tuning: By fine-tuning our model, we ensure that it rejects harmful requests.
This works mostly reliably, filtering out 90% of the harmful requests received.

204
CHAPTER 4. SAFETY ENGINEERING
2. Artificial conscience: By giving our AI an artificial conscience, it is less likely to take actions
that result in low human wellbeing in pursuit of its objective. However, 10% of the time, it
may take actions that are great for its objective and bad for human wellbeing regardless.
3. AI watchdogs: By monitoring deployed AIs to detect signs of malfeasance, we catch AIs acting
in ways contrary to how we want them to act nine times out of ten.
Swiss cheese model for emergent capabilities.
To reduce the risk of unexpected emergent
capabilities, multiple lines of defense could be employed. For example, models could be gradually
scaled (e.g., using 3× more compute than the previous training run, rather than a larger number
such as 10×); as a result, there will be fewer emergent capabilities to manage. An additional layer
of defense is screening for hazardous capabilities, which could involve deploying comprehensive test
beds, and red teaming with behavioral tests and representation reading. Another defense is staged
releases; rather than release the model to all uses at once, gradually release it to more and more users,
and manage discovered capabilities as they emerge. Finally, post-deployment monitoring through
anomaly detection adds another layer of defense.
Each of these aim at largely different areas, with the first focusing on robustness, the second on
control, and the third on monitoring. By ensuring we have many defenses, we prevent a wider array
of risks, improving our system reliability by many nines of reliability.
4.4.2
Bow Tie Model
The bow tie model splits defenses into preventative and protective measures[4].
In the
diagram in figure 4.5, the triangle on the left-hand side contains features that are intended to prevent
an accident from happening, while the triangle on the right-hand side contains features that are
intended to mitigate damage if an accident does happen. The point in the middle where the two
triangles meet can be thought of as the point where any given accident happens. This is an example
of a bow tie diagram, which can help us visualize the preventative and protective measures in place
against any potential adverse event.

4.4. COMPONENT FAILURE ACCIDENT MODELS AND METHODS
205
Figure 4.5: The bow tie diagram can tie together hazards and their consequences with control and
recovery measures to mitigate the effects of an adverse event.
For example, if an individual goes rock climbing, a potential accident is falling. We can draw a bow
tie for this situation, with the center representing a fall. On the left, we note any measures the
individual could take to prevent a fall, for example using chalk on their hands to increase friction.
On the right, we note any protective measures they could take to reduce harm from falling, such as
having a cushioned floor below.
Bow tie analysis of proxy gaming.
In the Single-Agent Safety chapter, we learned that one
hazard of using AIs is that they might learn to “game” the objectives we give them. If the specified
objectives are only proxies for what we actually want, then an AI might find a way of optimizing
them that is not beneficial overall, possibly due to unintended harmful side effects.
To analyze this hazard, we can draw a bow tie diagram, with the center representing the event of an
AI gaming its proxy goals in a counterproductive way. On the left-hand side, we list preventative
measures, such as ensuring that we can control AI drives like power-seeking. If the AI system has less
power (for example less resources), this would reduce the probability that it finds a way to optimize
its goal in a way that conflicts with our values (as well as the severity of the impact if it does). On
the right-hand side, we list protective measures, such as improving anomaly detection tools that can
recognize any AI behavior that resembles proxy gaming. This would help human operators to notice
activity like this early and take corrective action to limit the damage caused.
The exact measures on either side of the bow tie depend on which event we put at the center. We
can make a system safer by individually considering each hazard associated with it, and ensuring we
implement both preventative and protective measures against that hazard.
4.4.3
Fault Tree Analysis Method
Fault tree analysis works backward to identify possible causes of negative outcomes.
Fault tree analysis (FTA) is a top-down process that starts by considering specific negative outcomes
that could result from a system, and then works backward to identify possible routes to those

206
CHAPTER 4. SAFETY ENGINEERING
outcomes. In other words, FTA involves “backchaining” from a possible accident to find its potential
causes.
Figure 4.6: Using a fault tree, we can work backwards from a failure (no water flow) to its possible
causes (such as a blockage or lack of flow at source).[6]
For each negative outcome, we work backward through the system, identifying potential causes of
that event. We can then draw a “fault tree” showing all the possible pathways through the system to
the negative outcome. By studying this fault tree, we can identify ways of improving the system
that remove these pathways. In figure 4.6, we trace backwards from a pump failure to two types of
failure: mechanical and electrical failure. Each of these has further subtypes. For fuse failing, we
know that we require a circuit overload, which can happen as a result of a wire shorted or a power
surge. Hence, we know what sort of hazards we might need to think about.
Example: Fire Hazard.
We could also consider the risk of a fire outbreak as a negative outcome.
We then work backward, thinking about the different requirements for this to happen—fuel, oxygen,
and sufficient heat energy. This differs from the water pump failure since all of these are necessary

4.4. COMPONENT FAILURE ACCIDENT MODELS AND METHODS
207
rather than just one of them. Working backward again, we can think about all the possible sources
of each of these requirements in the system. After completing this process, we can identify multiple
combinations of events that could lead to the same negative outcome.
Figure 4.7: An FTA decision-tree can identify several potential problems by interrogating important
contextual questions. [7]
FTA can be used to guide decisions.
Figure 4.7 shows how we can use fault-tree style reasoning
to create concrete questions about risks. We can trace this through to identify risks depending on
the answers to these questions. For instance, if there is no unified group accountable for creating
AIs, then we know that diffusion of responsibility is a risk. If there is such a group, then we need to
question their beliefs, intentions, and incentives.
By thinking more broadly about all the ways in which a specific accident could be caused, whether
by a single component failing or by a combination of many smaller events, FTA can discover hazards
that are important to find. However, the backchaining process that FTA relies on also has limitations,
which we will discuss in the next section.

208
CHAPTER 4. SAFETY ENGINEERING
4.4.4
Limitations
Chain-of-Events Reasoning.
The Swiss cheese and bow tie models and the FTA method can
be useful for identifying hazards and reducing risk in some systems. However, they share some
limitations that make them inapplicable to many of the complex systems that are built and operated
today. Within the field of safety engineering, these approaches are largely considered overly simplistic
and outdated. We will now discuss the limitations of these approaches in more detail, before moving
on to describe more sophisticated accident models that may be better suited to risk management in
AI systems.
Chain-of-events reasoning is sometimes too simplistic for useful risk analysis.
All of
these models and techniques are based on backchaining or linear “chain-of-events” reasoning. This
way of thinking assumes there is a neat line of successive events, each one directly causing the next,
that ultimately leads to the accident. The goal is then to map out this line of events and trace it
back to identify the “root cause”—usually a component failure or human error—to blame. However,
given the numerous factors that are at play in many systems, it does not usually make sense to single
out just one event as the cause of an accident. Moreover, this approach puts the emphasis largely on
the details of “what” specifically happened, while neglecting the bigger question of “why” it happened.
This worldview often ignores broader systemic issues and can be overly simplistic. Rather than break
events down into a chain of events, a complex systems perspective often sees events as a product or
complex interaction between many factors.
Complex and Sociotechnical Systems
Component failure accident models are particularly inadequate for analyzing complex systems and
sociotechnical systems. We cannot always assume direct, linear causality in complex and sociotechnical
systems, so the assumption of a linear “chain of events” breaks down.
In complex systems, many components interact to produce emergent properties.
Complex systems are everywhere. A hive of bees consists of individuals that work together to
achieve a common goal, a body comprises many organs that interact to form a single organism, and
large-scale weather patterns are produced by the interactions of countless particles in the atmosphere.
In all these examples, we find collective properties that are not found in any of the components but
are produced by the interactions between them. In other words, a complex system is “more than
the sum of its parts.” As discussed in the complex systems chapter, these systems exhibit emergent
features that cannot be usefully understood through a reductive analysis of components.
Sociotechnical systems involve interactions between humans and technologies.
For
example, a car on the road is a sociotechnical system, where a human driver and technological vehicle
work together to get from the starting point to the destination. At progressively higher levels of
complexity, vehicles interacting with one another on a road also form a sociotechnical system, as does
the entire transport network. With the widespread prevalence of computers, virtually all workplaces
are now sociotechnical systems, and so is the overarching economy.
There are three main reasons why component failure accident models are insufficient for analyzing
complex and sociotechnical systems: accidents sometimes happen without any individual component
failing, accidents are not always the result of linear causality, and direct causality is sometimes less
important than indirect, remote, or “diffuse” causes such as safety culture. We will now look at each
of these reasons in more detail.

4.4. COMPONENT FAILURE ACCIDENT MODELS AND METHODS
209
Accidents Without Failure
Sometimes accidents happen due to interactions, even if no single component fails
[8].
The component failure accident models generally consider each component individually, but
components often behave differently within a complex or sociotechnical system than they do in
isolation. For example, a human operator who is normally diligent might be less so if they believe
that a piece of technology is taking care of one of their responsibilities.
Accidents can also arise through interactions between components, even if every component functions
exactly as it was intended to. Consider the Mars Polar Lander, a spacecraft launched by NASA in
1999, which crashed on the Martian surface later that year. It was equipped with reverse thrusters
to slow its descent to the surface, and sensors on its legs to detect a signal generated by landing to
turn the thrusters off. However, the legs had been stowed away for most of the journey. When they
extended in preparation for landing, the sensors interpreted it as a landing. This caused the software
to switch the thrusters off before the craft had landed, so it crashed on the surface [9].
In this case, there was no component failure. Each component did what it was designed and intended
to do; the accident was caused by an unanticipated interaction between components. This illustrates
the importance of looking at the bigger picture, and considering the system as a whole, rather than
just looking at each component in isolation, in a reductionist way.
Nonlinear Causality
Sometimes, we cannot tease out a neat, linear chain of events or a “root cause” [8].
Complex and sociotechnical systems usually involve a large number of interactions and feedback
loops. Due to the many interdependencies and circular processes, it is not always feasible to trace an
accident back to a single “root cause.” The high degree of complexity involved in many systems of
work is illustrated in figure 4.8. This shows the interconnectedness of a system cannot be accurately
reduced to a single line from start to finish.

210
CHAPTER 4. SAFETY ENGINEERING
Figure 4.8: In a complex system, events, processes, and factors often influence each other in many
ways, creating feedback loops.
AI systems can exhibit feedback loops and nonlinear causality.
Reinforcement learning
systems involve complexity and feedback loops.
These systems gather information from the
environment to make decisions, which then impact the environment, influencing their subsequent
decisions. Consider an ML system that ranks advertisements based on their relevance to search
terms. The system learns about relevance by tracking the number of clicks each ad receives and
adjusts its model accordingly.
However, the number of clicks an ad receives depends not only on its intrinsic relevance but also
on its position in the list: higher ads receive more clicks. If the system underestimates the effect of
position, it may continually place one ad at the top since it receives many clicks, even if it has lower
intrinsic relevance, leading to failure. Conversely, if the system overestimates the effect of position,
the top ad will receive fewer clicks than it expects, and so it may constantly shuffle ads, resulting in
a random order rather than relevance-based ranking, also leading to failure.
Many complex and sociotechnical systems comprise a whole network of interactions between
components, including multiple feedback loops like this one. When thinking about systems like these,
it is difficult to reduce any accidents to a simple chain of successive events that we can trace back to
a root cause.
Indirect Causality
Sometimes, it is more useful to focus on underlying conditions than specific events [8].
Even if we can pinpoint specific events that directly led to an accident, it is sometimes more fruitful
to focus on broader systemic issues. For example, rising global temperatures certainly increase the

4.4. COMPONENT FAILURE ACCIDENT MODELS AND METHODS
211
frequency and severity of natural disasters, including hurricanes, wildfires, and floods. Although
it would be difficult to prove that any particular emission of greenhouse gases directly caused any
specific disaster, it would be remiss of us to ignore the effects of climate change when trying to
evaluate the risk of future disasters. Similarly, a poor diet and lack of exercise are likely to result
in ill health, even though no single instance of unhealthy behavior could be directly blamed for an
individual developing a disease.
Example: spilling drinks.
Consider a scenario where an event planner is faced with the problem
of people spilling drinks at a crowded venue. One possible solution might be to contact the individuals
who dropped their drinks last time and ask them to be more careful. While this approach aims to
reduce the failure rate of individual components (people), it fails to address the underlying issue of
high density that contributes to the problem.
Alternatively, the event planner could consider two more effective strategies. Firstly, they could move
the event to a bigger venue with more space. By modifying the system architecture, the planner
would provide attendees with more room to move around, reducing the likelihood of people bumping
into each other and spilling their drinks. This approach addresses the diffuse variable of high density.
Secondly, the event planner could limit the number of attendees, ensuring that the venue does not
become overcrowded. By reducing the overall density of people, the planner would again mitigate
the risk of collisions and drink spills. This solution also targets the diffuse variable of high density,
acknowledging the systemic nature of the problem.
Compared with the first option, which solely focuses on the behavior of individuals and would
likely fail to eliminate spillages, the latter two strategies recognize the importance of modifying
the environment and addressing the broader factors that contribute to the issue. By prioritizing
architectural adjustments or managing attendee numbers, the event planner can more effectively
prevent drink spills and create a safer and more enjoyable experience for everyone.
Sharp end versus blunt end.
The part of a system where specific events happen, such as people
bumping into each other, is sometimes referred to as the “sharp end” of the system. The higher-level
conditions of the system, such as the density of people in a venue, is sometimes called the “blunt
end.” As outlined in the example of people spilling drinks, “sharp-end” interventions may not always
be effective. These are related to “proximate causes” and “distal causes,” respectively.
Diffuse causality suggests broader, “blunt-end” interventions.
In general, it might not be
possible to prove that “systemic factors” directly caused an accident. However, systemic factors might
“diffusely” affect the whole system in ways that increase the probability of an accident, making them
relevant for risk analysis. Under conditions like this, even if the specific event that led to an accident
had not happened, something else might have been likely to happen instead. This means it can be
more effective to tackle problems less directly, such as by changing system architecture or introducing
bottom-up interventions that affect systemic conditions, rather than by attempting to control the
sharp end.
Weaponizing any single AI system would not necessarily lead to war or any other kind of catastrophe.
However, the existence of these systems increases the probability of a rogue AI system causing a
disaster. Minimizing the use of autonomous weapons might be a better way of addressing this risk
than attempting to introduce lots of safeguards to prevent loss of control over a rogue system.

212
CHAPTER 4. SAFETY ENGINEERING
Meadows’ twelve leverage points.
One way of identifying broad interventions that could yield
significant results is to consider Meadows’ twelve leverage points. These are twelve points within a
system, described by environmental scientist Donella Meadows, where a small change can make a
large difference [10]. Depending on the system under consideration, some of these characteristics
may be difficult or impossible to change, for example large physical infrastructure or the laws of
physics. The points are therefore often listed in order of increasing efficacy, taking into account their
tractability.
The lower end of the list of leverage points includes: parameters or numbers that can be tweaked,
such as taxes or the amount of resources allocated to a particular purpose; the size of buffers in a
system, such as water reservoirs or a store’s reserve stock, where a larger buffer makes a system
more stable and a smaller buffer makes a system more flexible; and the structure of material flows
through a system, such as the layout of a transport network or the way that people progress through
education and employment.
The middle of the list of leverage points includes: lags between an input to a system and the system’s
response, which can cause the system to oscillate through over- and undershooting; negative feedback
loops that can be strengthened to help a system self-balance; and positive feedback loops that can be
weakened to prevent runaway escalation at an earlier stage.
The next three leverage points in Meadows’ list are: the structure of information flows in a system,
which can increase accountability by making the consequences of a decision more apparent to decision-
makers, for example by publishing information about companies’ emissions; the rules of the system,
such as national laws, or examination policies in educational institutes, which can be changed in social
systems to influence behavior; and the ability to self-organize and adapt, which can be promoted by
maintaining diversity, such as biodiversity in an ecosystem and openness to new ideas in a company
or institution.
Finally, leverage points toward the higher end of the list include: the goal of the system, which,
if changed, could completely redirect the system’s activities; the paradigm or mindset that gave
rise to the goal, which, if adjusted, could transform the collective understanding of what a system
can and should be aiming for; and the realization that there are multiple worldviews or paradigms
besides an organization’s current one, which can empower people to adopt a different paradigm when
appropriate.
Summary.
In complex and sociotechnical systems, accidents cannot always be reduced to a linear
chain of successive events. The large number of complex interactions and feedback loops means that
accidents can happen even if no single component fails and that it may not be possible to identify
a root cause. Since we may not be able to anticipate every potential pathway to an accident, it is
often more fruitful to address systemic factors that diffusely influence risk. Meadows’ twelve leverage
points can help us identify systemic factors that, if changed, could have far-reaching, system-wide
effects.
4.5
Systemic Factors
As discussed above, if we want to improve the safety of a complex, sociotechnical system, it might be
most effective to address the blunt end, or the broad systemic factors that can diffusely influence
operations.
Some of the most important systemic factors include regulations, social pressure,
technosolutionism, competitive pressure, safety costs, and safety culture. We will now discuss each of
these in more detail.

4.5. SYSTEMIC FACTORS
213
Safety regulations can be imposed by government or internal policies.
Safety regulations
can require an organization to adhere to various safety standards, such as conducting regular staff
training and equipment maintenance. These stipulations can be defined and enforced by a government
or by an organization’s own internal policies. The more stringent and effectively targeted these
requirements are, the safer a system is likely to be.
Social pressure can encourage organizations to improve safety.
Public attitudes towards
a particular technology can affect an organization’s attitude to safety. Significant social pressure
about risks can mean that organizations are subject to more scrutiny, while little public awareness
can allow organizations to take a more relaxed attitude toward safety.
Technosolutionism should be discouraged.
Attempting to fix problems simply by introducing
a piece of technology is called technosolutionism. It does not always work, especially in complex and
sociotechnical systems. Although technology can certainly be helpful in solving problems, relying
on it can lead organizations to neglect the broader system. They should consider how the proposed
technological solution will actually function in the context of the whole system, and how it might
affect the behavior of other components and human operators.
Multiple geoengineering technologies have been proposed as solutions to climate change, such as
spraying particles high in the atmosphere to reflect sunlight. However, there are concerns that
attempting this could have unexpected side effects. Even if spraying particles in the atmosphere
did reverse global heating, it might also interfere with other components of the atmosphere in ways
that we fail to predict, potentially with harmful consequences for life. Instead, we could focus on
non-technical interventions like preserving forested areas that are more robustly likely to work without
significant unforeseen negative side-effects.
Competitive pressures can lead to compromise on safety.
If multiple organizations or
countries are pursuing the same goal, they will be incentivized to get an edge over one another. They
might try to do this by reaching the goal more quickly or by trying to make the end product more
valuable to customers in terms of the functionality it offers. These competitive pressures can compel
employees and decision-makers to cut corners and pay less attention to safety.
On a larger scale, competitive pressures might put organizations or countries in an arms race, wherein
safety standards slip because of the urgency of the situation. This will be especially true if one of the
organizations or countries has lower safety standards and consequently moves quicker; others might
feel the need to lower their standards as well, in order to keep up. The risks this process presents are
encapsulated by Publilius Syrus’s aphorism: “Nothing can be done at once hastily and prudently.”
We consider this further in the Collective Action Problems chapter.
Various safety costs can discourage the adoption of safety measures.
There are often
multiple costs of increasing safety, not only financial costs but also slowdowns and reduced product
performance. Adopting safety measures might therefore decrease productivity and slow progress
toward a goal, reducing profits. The higher the costs of safety measures, the more reluctant an
organization might be to adopt them.
Developers of AI systems may want to put more effort into transparency and interpretability. However,
investigating these areas are costly: at the very least, there will be personnel and compute costs
that could otherwise have been used to directly create more capable systems. Additionally, it may
delay the completion of the finished product. There might also be costs from making a system more
interpretable in terms of product performance. Creating more transparent models might require

214
CHAPTER 4. SAFETY ENGINEERING
AIs to select only those actions which are clearly explainable. In general, safety features can reduce
model capabilities, which organizations might prefer to avoid.
The general safety culture of an organization is an important systemic factor.
A final
systemic factor that will broadly influence a system’s safety can simply be referred to as its “safety
culture.” This captures the general attitude that the people in an organization have toward safety—
how seriously they take it, and how that translates into their actions. We will discuss some specific
features of a diligent safety culture in the next section.
Summary.
We have seen that component failure accident models have some significant limitations,
since they do not usually capture diffuse sources of risk that can shape a system’s dynamics and
indirectly affect the likelihood of accidents.
These include important systemic factors such as
competitive pressures, safety costs, and safety culture. We will now turn to systemic accident models
that acknowledge these ideas and attempt to account for them in risk analysis.
4.5.1
Systemic Accident Models
We have explored how component failure accident models are insufficient for properly understanding
accidents in complex systems. When it comes to AIs, we must understand what sort of system
we are dealing with. Comparing AI safety to ensuring the safety of specific systems like rockets,
power plants, or computer programs can be misleading. The reality of today’s world is that many
hazardous technologies are operated by a variety of human organizations: together, these form
complex sociotechnical systems that we need to make safer. While there may be some similarities
between different hazardous technologies, there are also significant differences in the properties of
these technologies which means it will not necessarily work to take safety strategies from one system
and map them directly onto another. We should not anchor to individual safety approaches used in
rockets or power plants.
Instead, it is more beneficial to approach AI safety from a broader perspective of making complex,
sociotechnical systems safer. To this end, we can draw on the theory of sociotechnical systems, which
offers “a method of viewing organizations which emphasises the interrelatedness of the functioning of
the social and technological subsystems of the organization and the relation of the organization as a
whole to the environment in which it operates.”
We can also use the complex systems literature more generally, which is largely about the shared
structure of many different complex systems. Accidents in complex systems can often be better
understood by looking at the system as a whole, rather than focusing solely on individual components.
Therefore, we will now consider systemic accident models, which aim to provide insights into why
accidents occur in systems by analyzing the overall structure and interactions within the system,
including human factors that are not usually captured well by component failure models.
Normal Accident Theory (NAT).
Normal Accident Theory (NAT) is one approach to
understanding accidents in complex systems. It suggests that accidents are inevitable in systems
that exhibit the following two properties:
1. Complexity: a large number of interactions between components in the system such as feedback
loops, discussed in the complex systems chapter. Complexity can make it infeasible to thoroughly
understand a system or exhaustively predict all its potential failure modes.
2. Tight coupling: one component in a system can rapidly affect others so that one relatively
small event can rapidly escalate to become a larger accident.

4.5. SYSTEMIC FACTORS
215
NAT concludes that, if a system is both highly complex and tightly coupled, then accidents are
inevitable—or “normal”—regardless of how well the system is managed [11].
NAT focuses on systemic factors.
According to NAT, accidents are not caused by a single
component failure or human error, but rather by the interactions and interdependencies between
multiple components and subsystems. NAT argues that accidents are a normal part of complex
systems and cannot be completely eliminated.
Instead, the focus should be on managing and
mitigating the risks associated with these systems to minimize the severity and frequency of
accidents. NAT emphasizes the importance of systemic factors, such as system design, human
factors such as organizational culture, and operational procedures, in influencing accident outcomes.
By understanding and addressing these systemic factors, it is possible to improve the safety and
resilience of complex systems.
Some safety features create additional complexity.
Although we can try to incorporate
safety features, NAT argues that many attempts to prevent accidents in these kinds of systems
can sometimes be counterproductive, as they might just add another layer of complexity. As we
explore in the Complex Systems chapter, systems often respond to interventions in unexpected ways.
Interventions can cause negative side effects or even inadvertently exacerbate the problems they were
introduced to solve.
Redundancy, which was listed earlier as a safe design principle, is supposed to increase safety by
providing a backup for critical components, in case one of them fails. However, redundancy also
increases complexity, which increases the risks of unforeseen and unintended interactions that can
make it impossible for operators to predict all potential issues [12]. Having redundant components
can also cause confusion; for example, people might receive contradictory instructions from multiple
redundant monitoring systems and not know which one to believe.
Reducing complexity can be a safety feature.
We may not be able to completely avoid
complexity and tight coupling in all systems, but there are many cases where we can reduce one
or both of them and thus meaningfully reduce risk. One example of this is reducing the potential
for human error by making systems more intuitive, such as by using color coding and male/female
adapters in electrical applications to reduce the incidence of wiring errors. Such initiatives do not
eliminate risks, and accidents are still normal in these systems, but they can help reduce the frequency
of errors.
High Reliability Organizations (HROs)
The performance of some organizations suggests serious accidents might be avoidable.
The main assertion of NAT is that accidents are inevitable in complex, tightly coupled systems. In
response to this conclusion, which might be perceived as pessimistic, other academics developed
a more optimistic theory that points to “high reliability organizations” (HROs) that consistently
operate hazardous technologies with low accident rates. These precedents include air traffic control,
aircraft carriers, and nuclear power plants.
HRO theory emphasizes the importance of human factors, arguing that it must be possible to manage
even complex, tightly coupled systems in a way that reliably avoids accidents. It identifies five key
features of HROs’ management culture that can significantly lower the risk of accidents [13]. We will
now discuss these five features and how AIs might help improve them.
1. Preoccupation with failure means reporting and studying mistakes and near misses.
HROs encourage the reporting of all anomalies, known failures, and near misses. They study

216
CHAPTER 4. SAFETY ENGINEERING
these events carefully and learn from them. HROs also keep in mind potential failure modes
that have not occurred yet and which have not been predicted. The possibility of unanticipated
failure modes constitutes a risk of black swan events, which will be discussed in detail later in
this chapter. HROs are therefore vigilant about looking out for emerging hazards. AI systems
tend to be good at detecting anomalies, but not near misses.
2. Reluctance to simplify interpretations means looking at the bigger picture. HROs
understand that reducing accidents to chains of events often oversimplifies the situation, and
is not necessarily helpful for learning from mistakes and improving safety. They develop a
wide range of expertise so that they can come up with multiple different interpretations of
any incident. This can help with understanding the broader context surrounding an event,
and systemic factors that might have been at play. HROs also implement many checks and
balances, invest in hiring staff with diverse perspectives, and regularly retrain everyone. AIs
could be used to generate explanations for hazardous events or conduct adversarial reviews of
explanations of system failures.
3. Sensitivity to operations means maintaining awareness of how a system is operating.
HROs invest in the close monitoring of systems to maintain a continual, comprehensive
understanding of how they are behaving, whether through excellent monitoring tools or hiring
operators with deep situational awareness. This can ensure that operations are going as planned,
and notice it early if anything unexpected happens, permitting taking corrective action early,
before the situation escalates. AI systems that dynamically aggregate information in real-time
can help improve situational awareness.
4. Commitment to resilience means actively preparing to tackle unexpected problems.
HROs train their teams in adaptability and improvising solutions when confronted with novel
circumstances. By practicing dealing with issues they have not seen before, employees develop
problem-solving skills that will help them cope if anything new and unexpected arises in reality.
AIs have the potential to enhance teams’ on-the-spot problem-solving, such as by creating
surprising situations for testing organizational efficiency.
5. Under-specification of structures means information can flow rapidly in a system.
Instead of having rigid chains of communication that employees must follow, HROs have
communication throughout the whole system. All employees are allowed to raise an alarm,
regardless of their level of seniority. This increases the likelihood that problems will be flagged
early, and also allows information to travel rapidly throughout the organization. This under-
specification of structures is also sometimes referred to as “deference to expertise,” because it
means that all employees are empowered to make decisions relating to their expertise, regardless
of their place in the hierarchy.
High-reliability organizations (HROs) provide valuable insights into the development and application
of AI technology. By emulating the characteristics of HROs, we can create combined human-machine
systems that prioritize safety and mitigate risks. These sociotechnical systems should continuously
monitor their own behavior and the environment for anomalies and unanticipated side effects. These
systems should also support combined human-machine situational awareness and improvisational
planning, allowing for real-time adaptation and flexibility. Lastly, AIs should have models of their
own expertise and the expertise of human operators to ensure effective problem routing. By adhering
to these principles, we can develop AI systems that function like HROs, ensuring high reliability and
minimizing the potential risks associated with their deployment and use.

4.5. SYSTEMIC FACTORS
217
Criticisms of HRO Theory
Doubts have been raised about how widely HRO theory can be applied.
Although
the practices listed above can improve safety, a main criticism of HRO theory is that they cannot
be applied to all systems and technologies [12]. This is because the theory was developed from a
relatively small group of example systems, and certain features of them cannot be replicated in all
systems.
It is difficult to understand systems sufficiently well.
First, in the example HROs (such as
air traffic control or nuclear power plants), operators usually have near-complete knowledge of the
technical processes involved. These organizations’ processes have also remained largely unchanged
for decades, allowing for lessons to be learned from errors and for safety systems to become more
refined. However, according to NAT, the main reason that complexity contributes to accidents is that
it precludes a thorough understanding of all processes, and anticipation of all potential failure modes.
HROs with near-complete knowledge of technical processes might be considered rare cases. These
conditions cannot be replicated in all systems, especially not in those operating new technologies.
HROs prioritize safety, but other organizations might not.
The second reason why HRO
theory might not be broadly applicable is that its suggestions generally focus on prioritizing safety as
a goal. This might make sense for several of the example HROs, where safety is an intrinsic part of
the mission. Airlines, for instance, would not be viable businesses if they did not have a strong track
record of transporting passengers safely. However, this is less feasible in organizations where safety is
not so pertinent to the mission. In many other profit maximization organizations, safety can conflict
with the main mission, as safety measures may be costly and reduce productivity.
Not all HROs are tightly coupled.
Another criticism of HRO theory is that several of the
example systems might actually be considered loosely coupled. For instance, in air traffic control,
extra time is scheduled in between landings on the same runway, to allow for delays, errors, and
corrections. However, NAT claims that tight coupling is the second system feature that makes
accidents inevitable. Loosely coupled systems may not, therefore, be a good counterexample.
Deference to expertise might not always be realistic.
A final reservation about HRO theory
is that the fifth characteristic (deference to expertise) assumes that employees will have the necessary
knowledge to make the best decisions at the local level. However, information on the system as a
whole is sometimes required in order to make the best decisions, as actions in one subsystem may
have knock-on effects for other subsystems. Employees might not always have enough information
about the rest of the system to be able to take this big-picture view.
Comparing NAT and HRO Theory
The debate over whether accidents are inevitable or avoidable remains unsettled.
A
particular sticking point is that, despite having low accident rates, some of the HRO examples have
experienced multiple near misses. This could be interpreted as evidence that NAT is correct. We
could view it as a matter of luck that these near misses did not become anything larger. This would
indicate that organizations presented as HROs are in fact vulnerable to accidents. On the other
hand, near misses could instead be interpreted as supporting HRO theory; the fact that they did not
turn into anything larger could be considered evidence that HROs have the appropriate measures in
place to prevent accidents. It is not clear which of these interpretations is correct [12].
Nevertheless, both NAT and HRO theory have contributed important concepts to safety engineering.
NAT has identified complexity and tight coupling as key risk factors, while HRO theory has developed

218
CHAPTER 4. SAFETY ENGINEERING
important principles for a good organizational safety culture. Both schools of thought acknowledge
that complex systems must be treated differently from simpler systems, requiring consideration of
all the components, their interactions, and human factors. We will now explore some alternative
approaches that view system safety more holistically, rather than considering it a product of reliable
components, interactions, and operators.
Rasmussen’s Risk Management Framework and AcciMap
System control with safety boundaries.
Rasmussen’s Risk Management Framework (RMF)
is an accident model that recognizes that accidents are usually the culmination of many different
factors, rather than a single root cause [14]. This model frames risk management as a question of
control, emphasizing the need for clearly defined safety boundaries that a system’s operations must
stay within.
Levels of organization and AcciMap.
The RMF considers six hierarchical levels of organization
within a system, each of which can affect its safety: government, regulators, the company, management,
frontline workers, and the work itself. By drawing out an “AcciMap” with this hierarchy, we can
identify actors at different levels who share responsibility for safety, as well as conditions that may
influence the risk of an accident. This analysis makes it explicit that accidents cannot be solely
explained by an action at the sharp end.
Systems can gradually migrate into unsafe states.
The RMF also asserts that behaviors and
conditions can gradually “migrate” over time, due to environmental pressures. If this migration leads
to unsafe systemic conditions, this creates the potential for an event at the sharp end to trigger an
accident. This is why it is essential to continually enforce safety boundaries and avoid the system
migrating into unsafe states.
System-Theoretic Accident Model and Processes (STAMP)
STAMP is based on insights from the study of complex systems.
According to the systems-
theory paradigm, safety is an emergent property that is unlikely to be sufficiently understood just by
looking at individual components in isolation. This is the view taken by System-Theoretic Accident
Model and Processes (STAMP). STAMP identifies multiple levels of organization within a system,
where each level is of higher complexity than the one below. Each level has novel emergent properties
that cannot be practically understood through a reductive analysis of the level below. STAMP also
recognizes that a system can be highly reliable but still be unsafe, and therefore puts the emphasis
on safety rather than just on the reliability of components.
STAMP frames safety as a question of top-down control.
STAMP proposes that safety can
be enforced by each level effectively placing safety constraints on the one below to keep operations
from migrating into unsafe states [12]. Performing STAMP-based risk analysis and management
involves creating models of four aspects of a system: the organizational safety structure, the dynamics
that can cause this structure to deteriorate, the models of system processes that operators must have,
and the surrounding context. We will now discuss each of these in more detail.
The organizational safety structure.
The first aspect is the safety constraints, the set of unsafe
conditions which must be avoided. It tells us which components and operators are in place to avoid
each of those unsafe conditions occurring. This can help to prevent accidents from component failures,
design errors, and interactions between components that could produce unsafe states.

4.5. SYSTEMIC FACTORS
219
Figure 4.9: Rasmussen’s risk management framework lays out six levels of organization and their
interactions, aiming to mark consistent safety boundaries by identifying hazards and those responsible
for them.
Dynamic deterioration of the safety structure.
The second aspect is about how the safety
structure can deteriorate over time, leading to safety constraints being enforced less stringently.
Systems can “migrate” toward failure when many small events escalate into a larger accident. Since
complex and sociotechnical systems involve large numbers of interactions, we cannot methodically
compute the effects of every event within the system and exhaustively identify all the pathways to
an accident. We cannot always reduce an accident to a neat chain of events or find a root cause:
such instincts are often based on the desire to have a feeling of control by assigning blame. Instead,
it might make sense to describe a system as migrating toward failure, due to the accumulation of
many seemingly insignificant events.
This might include natural processes, such as wear and tear of equipment. It can also include
systemic factors, such as competitive pressures, that might compel employees to omit safety checks.
If being less safety-conscious does not quickly lead to an accident, developers might start to think
that safety-consciousness is unnecessary. Having a model of these processes can increase awareness
and vigilance around what needs to be done to maintain an effective safety structure.
Knowledge and communication about process models.
The third aspect is the knowledge
that operators must have about how the system functions in order to make safe decisions. Operators

220
CHAPTER 4. SAFETY ENGINEERING
may be humans or automated systems that have to monitor feedback from the system and respond
to keep it on track.
The process model that these operators should have includes the assumptions about operating
conditions that were made during the design stage so that they will be aware of the conditions
in which the system might not function properly, such as outside regular temperature ranges. It
might also include information about how the specific subsystem that the operator is concerned with
interacts with other parts of the system. The communication required for operators to maintain an
accurate process model over time should also be specified. This can help to avoid accidents resulting
from operators or software making decisions based on inaccurate beliefs about how the system is
functioning.
The cultural and political context of the decision-making processes.
The fourth aspect is
the systemic factors that could influence the safety structure. It might include information about
who the stakeholders are and what their primary concern is. For example, governments may impose
stringent regulations, or they may put pressure on an organization to reach its goals quickly, depending
on what is most important to them at the time. Similarly, social pressures and attitudes may put
pressure on organizations to improve safety or pressure to achieve goals quickly.
Table 4.2 summarizes how the STAMP perspective contrasts with those of traditional component
failure models.
Old Assumption
New Assumption
Accidents are caused by chains of directly related
events.
Accidents are complex processes involving the en-
tire sociotechnical system.
We can understand accidents by looking at chains
of events leading to the accident.
Traditional event-chain models cannot describe this
process adequately.
Safety is increased by increasing system or compo-
nent reliability.
High reliability is not sufficient for safety.
Most accidents are caused by operator error.
Operator error is a product of various environmen-
tal factors.
Assigning blame is necessary to learn from and
prevent accidents.
Holistically understand how the system behavior
contributed to the accident.
Major accidents occur from simultaneous occur-
rences of random events.
Systems tend to migrate towards states of higher
risk.
Table 4.2: STAMP makes assumptions that differ from traditional component failure models.
STAMP-based analysis techniques include System-Theoretic Process Analysis (STPA).
On a practical level, there are methods of analyzing systems that take the holistic approach outlined
by STAMP. These include System-Theoretic Process Analysis (STPA), which can be used at the
design stage, including steps such as identifying hazards and constructing a control structure to
mitigate their effects and improve system safety.
Dekker’s Drift into Failure model
Decrementalism is the deterioration of system processes through a series of small
changes.
A third accident model based on systems theory is Dekker’s Drift into Failure (DIF)
model [15]. DIF focuses on the migration of systems that the RMF and STAMP also acknowledge,
describing how this can lead to a “drift into failure.” Since an individual decision to change processes

4.6. DRIFT INTO FAILURE AND EXISTENTIAL RISKS
221
may be relatively minor, it can seem that it will not make any difference to a system’s operations or
safety. For this reason, systems are often subject to decrementalism, a gradual process of changes
through one small decision at a time that degrades the safety of a system’s operations.
Many relatively minor decisions can combine to lead to a major difference in risk.
Within complex systems, it is difficult to know all the potential consequences of a change in the
system, or how it might interact with other changes. Many alterations to processes within a system,
each of which might not make a difference by itself, can interact in complex and unforeseen ways to
result in a much higher state of risk. This is often only realized when an accident happens, at which
point it is too late.
Summary.
Normal accident theory argues that accidents are inevitable in systems with a high
degree of complexity and tight coupling, no matter how well they are organized. On the other hand,
it has been argued that HROs with consistently low accident rates demonstrate that it is possible
to avoid accidents. HRO theory identifies five key characteristics that contribute to a good safety
culture and reduce the likelihood of accidents. However, it might not be feasible to replicate these
across all organizations.
Systemic models like Rasmussen’s RMF, STAMP, and Dekker’s DIF model are grounded in an
understanding of complex systems, viewing safety as an emergent property. The RMF and STAMP
both view safety as an issue of control and enforcing safety constraints on operations. They both
identify a hierarchy of levels of organization within a system, showing how accidents are caused by
multiple factors, rather than just by one event at the sharp end. DIF describes how systems are
often subject to decrementalism, whereby the safety of processes is gradually degraded through a
series of minor changes, each of which seems minor on its own.
In general, component failure models focus on identifying specific components or factors that can
go wrong in a system and finding ways to improve those components. These models are effective
at pinpointing direct causes of failure and proposing targeted interventions. However, they have a
limitation in that they tend to overlook other risk sources and potential interventions that may not
be directly related to the identified components. On the other hand, systemic accident models take a
broader approach by considering the interactions and interdependencies between various components
in a system, such as feedback loops, human factors, and diffuse causality models. This allows them to
capture a wider range of risk sources and potential interventions, making them more comprehensive
in addressing system failures.
4.6
Drift into Failure and Existential Risks
This book presents multiple ways in which the development and deployment of AIs could entail risks,
some of which could be catastrophic or even existential. However, the systemic accident models
discussed above highlight that events in the real world often unfold in a much more complex manner
than the hypothetical scenarios we use to illustrate risks. It is possible that many relatively minor
events could accumulate, leading us to drift toward an existential risk. We are unlikely to be able to
predict and address every potential combination of events that could pave the route to a catastrophe.
For this reason, although it can be useful to study the different risks associated with AI separately
when initially learning about them, we should be aware that hypothetical example scenarios are
simplified, and that the different risks coexist. We will now discuss what we can learn from our study
of complex systems and systemic accident models when developing an AI safety strategy.

222
CHAPTER 4. SAFETY ENGINEERING
Risks that do not initially appear catastrophic might escalate.
Risks tend to exist on a
spectrum. Power inequality, disinformation, and automation, for example, are prevalent issues within
society and are already causing harm. Though serious, they are not usually thought of as posing
existential risks. However, if pushed to an extreme degree by AIs, they could result in totalitarian
governments or enfeeblement. Both of these scenarios could represent a catastrophe from which
humanity may not recover. In general, if we encounter harm from a risk on a moderate scale, we
should be careful to not dismiss it as non-existential without serious consideration.
Multiple lower-level risks can combine to produce a catastrophe.
Another reason for
thinking more comprehensively about safety is that, even if a risk is not individually extreme, it might
interact with other risks to bring about catastrophic outcomes [16]. Imagine, for instance, a scenario
in which competitive pressures fuel an AI race between developers. This may lead a company to
reduce its costs by putting less money into maintaining robust information security systems, with the
result that a powerful AI is leaked. This would increase the likelihood that someone with malicious
intent successfully uses the AI to pursue a harmful outcome, such as the release of a deadly pathogen.
In this case, the AI race has not directly led to an existential risk by causing companies to, for example,
bring AIs with insufficient safety measures to market. Nevertheless it has indirectly contributed to
the existential threat of a pandemic by amplifying the risk of malicious use.
This echoes our earlier discussion of catastrophes in complex system, where we discussed how it is
often impractical and infeasible to attribute blame to one major “root cause” of failure. Instead,
systems often “drift into failure” through an accumulation and combination of many seemingly minor
events, none of which would be catastrophic alone. Just as we cannot take steps to prevent every
possible mistake or malfunction within a large, complex system, we cannot predict or control every
single way that various risks might interact to result in disaster.
Conflict and global turbulence could make society more likely to drift into failure.
Although we have some degree of choice in how we implement AI within society, we cannot control
the wider environment. There are several reasons why events like wars that create societal turbulence
could increase the risk of human civilization drifting into failure. Faced with urgent, short-term
threats, people might deprioritize AI safety to focus instead on the most immediate concerns. If AIs
can be useful in tackling those concerns, it might also incentivize people to rush into giving them
greater power, without thinking about the long-term consequences. More generally, a more chaotic
environment might also present novel conditions for an AI, that cause it to behave unpredictably.
Even if conditions like war do not directly cause existential risks, they make them more likely to
happen.
Broad interventions may be more effective than narrowly targeted ones.
Previous
attempts to manage existential risks have focused narrowly on avoiding risks directly from AIs, and
mainly addressed this goal through technical AI research. Given the complexity of AIs themselves
and the systems they exist within, it makes sense to adopt a more comprehensive approach, taking
into account the whole risk landscape, including threats that may not immediately seem catastrophic.
Instead of attempting to target just existential risks precisely, it may be more effective to implement
broad interventions, including sociotechnical measures.
Summary.
As we might expect from our study of complex systems, different types of risks are
inextricably related and can combine in unexpected ways to amplify one another. While some risks
may be generally more concerning than others, we cannot neatly isolate those that could contribute
to an existential threat from those that could not, and then only focus on the former while ignoring

4.7. TAIL EVENTS
223
the latter. In addressing existential threats, it is therefore reasonable to view systems holistically
and consider a wide range of issues, besides the most obvious catastrophic risks. Due to system
complexity, broad interventions are likely to be required as well as narrowly targeted ones.
4.7
Tail Events
In the first few sections of this chapter, we discussed failure modes and hazards, equations for
understanding the risks they pose, and principles for designing safer systems. We also looked at
methods of analyzing systems to model accidents and identify hazards, and explored how different
styles of analysis can be more helpful for complex systems.
The classic risk equation from the start of the chapter tells us that the level of risk of a specific
adverse event depends on both the probability it will happen and its severity. A particular class of
events, called tail events, have a very low probability of occurrence but a very high impact upon
arrival. Tail events pose some unique challenges for assessing and reducing risk, but any competent
form of risk management must attempt to address them. We will now explore these events and their
implications in more detail.
Introduction to Tail Events
Tail events are events that happen rarely, but have a considerable impact when they do. Consider
some examples of historical tail events.
The 2007-2008 financial crisis: Fluctuations happen continually in financial markets, but crises of this
scale are rare and have a large impact, with knock-on effects for banks and the general population.
The COVID-19 pandemic: There are many outbreaks of infectious diseases every year, but COVID-19
spread much more widely and killed many more people than most. It is rare for an outbreak to
become a pandemic, but those that do will have a much larger impact than the rest.
The Internet: There are many technologies being developed all the time, but very few become so
widely used that they transform society as much as the Internet has. This example illustrates that
some tail events happen more gradually than others; the development and global adoption of the
internet unfolded over decades, rather than happening as suddenly as the financial crisis or the
pandemic. However, “sudden” is a relative term. If we look at history on the scale of centuries, then
the transition into the Internet age can also appear to have happened suddenly.
ChatGPT: After being released in November 2022, ChatGPT gained 100 million users in just two
months [17]. There are many consumer applications on the internet, but ChatGPT’s user base grew
faster than those of any others launched before it. Out of many deep learning models, ChatGPT was
the first to go viral in this way. Its release also represented a watershed moment in the progress of
generative AI, placing the issue much more firmly in the public consciousness.
We need to consider the possibility of harmful tail events in risk management.
The last
two examples—the Internet and ChatGPT—illustrate that the impacts of tail events are not always
strictly negative; they can also be positive or mixed. However, tail risks are what we need to pay
attention to when trying to engineer safer systems.

224
CHAPTER 4. SAFETY ENGINEERING
Since tail events are rare, it can be tempting to think that we do not need to worry about them.
Indeed, some tail events have not yet happened once in human history, such as a meteorite strike
large enough to cause global devastation, or an intense solar storm knocking out the power grid.
Nonetheless, tail events have such a high impact that it would be unwise to ignore the possibility
that they could happen. As noted by the political scientist Scott Sagan: “Things that have never
happened before happen all the time.” [18]
AI-related tail events could have a severe impact.
As AIs are increasingly deployed within
society, some tail risks we should consider include the possibility that an AI could be used to
develop a bioweapon, or that an AI might hack a bank and wipe the financial information. Even if
these eventualities have a low probability of occurring, it would only take one such event to cause
widespread devastation; if one such event happened, it could largely define the overall outcome of AI
deployment. For this reason, competent risk management must involve serious efforts to prevent tail
events, however rare we think they might be.

4.7. TAIL EVENTS
225
4.7.1
Tail Events Can Greatly Affect the Average Risk
0
25
50
75
100
1
3
4
100
1000
10000
Impact (Log Scale)
Count
Statistic
Mean
Median
Distribution of Event Impact (Without Tail Event)
Tail Event
0
25
50
75
100
1
3
14
100
1000
10000
Impact (Log Scale)
Count
Statistic
Mean
Median
Distribution of Event Impact (With Tail Event)
Figure 4.10: The occurrence of a tail event can dramatically
shift the mean but not the median of the event type’s impact.
A tail event often changes the
mean but not the median.
Figure
4.10
can
help
us
visualize
how
tail
events
affect
the
wider
risk
landscape.
The graphs show data
points representing individual events,
with their placement along the x-axis
indicating their individual impact.
In the first graph, we have numerous
data points representing frequent, low-
impact events: these are all distributed
between 0 and 100, and mostly between
0 and 10. The mean impact and median
impact of this dataset have similar
values, marked on the x-axis.
In the second graph we have the
same collection of events, but with the
addition of a single data point of much
higher impact—a tail event with an
impact of 10,000. As indicated in the
graph, the median impact of the dataset
is approximately the same as before,
but the mean changes substantially
and is no longer representative of the
general population of events.
We
can
also
think
about
tail
events in terms of the classic risk
equation.
Tail events have a low
probability, but because they are so
severe, we nonetheless evaluate the risk
they pose as being large:
Risk(hazard) = P(hazard)×severity(hazard).
Depending on the exact values of probability and severity, we may find that tail risks are just as large
as—or even larger than–—the risks posed by much smaller events that happen all the time. In other
words, although they are rare, we cannot afford to ignore the possibility that they might happen.
It is difficult to plan for tail events because they are so rare.
Since we can hardly predict
when tail events will happen, or even if they will happen at all, it is much more challenging to plan
for them than it is for frequent, everyday events that we know we can expect to encounter. It is often
the case that we do not know exactly what form they will take either.
For these reasons, we cannot plan the specific details of our response to tail events in advance.
Instead, we have to plan how we will organize and develop an appropriate response, if and when it is
necessary—how relevant actors should convene to decide on and coordinate the most appropriate

226
CHAPTER 4. SAFETY ENGINEERING
next steps, whatever the precise details of the event. This concept is called “planning to plan” and
we will discuss this in more detail later in the chapter.
4.7.2
Long-Tailed and Thin-Tailed Distributions
Having introduced tail events and briefly explained why they are important, we will now discuss
the difference between the types of events that have long tails and the types of events that do
not. First, we will briefly consider what frequency distributions are. Then, we will go through the
characteristics of long-tailed and thin-tailed distributions, distinguishing between them and examining
a few important features of long-tailed distributions wherein we see tail events.
Frequency distributions
Frequency distributions tell us how common instances of different magnitude are.
To understand the origins of tail events, we need to understand frequency distributions. These
distributions tell us about the proportion of items in a dataset that have each possible value of a
given variable. Suppose we want to study some quantity, such as the ages of buildings. We might plot
a graph showing how many buildings there are in the world of each age, and it might look something
like the generic graph shown in figure 4.11.
The x-axis would represent building age, while the y-axis would indicate the number of buildings
of each age—the frequency of a particular age appearing in the dataset. If our graph looked like
figure 4.11, it would tell us that there are many buildings that are relatively new, perhaps only a few
decades or a hundred years old, fewer buildings that are several hundred or a thousand years old,
and very few buildings, such as the Pyramids at Giza, that are several thousand years old.
Head
Tail
2000
1800
1600
1400
1200
1000
Time
Radioactive Particles Remaining
Head and Tail of an Exponential Distribution
Figure 4.11: Many distributions have a head (an area where most of the probability is concentrated)
and one or two tails (extreme regions of the distribution).

4.7. TAIL EVENTS
227
Many real-world frequency distributions have long tails.
We can plot graphs like this for
countless variables, from the size of different vertebrate species to the number of countries different
people have visited. Each variable will have its own unique distribution, but many have the general
property that there are lots of occurrences of a low value and relatively few occurrences of a high
value. There are many vertebrate species with a mass of tens of kilograms, and very few with a mass
in the thousands of kilograms; there are many people who have visited somewhere between 1-10
countries, and few people who have visited more than 50.
Frequency distributions can generally be categorized as being either long-tailed or thin-tailed. To
develop our understanding of tail events, we need to understand the difference between these two
categories.
Long-tailed distributions
In long-tailed distributions, extreme occurrences can have a huge effect.
Long-tailed
distributions are those where values can vary so much that a single extreme instance or a few
extreme instances can dominate the total value. In other words, the top few instances have a
highly disproportionate impact. We will now explore some examples of variables with long-tailed
distributions.
Book sales: Around 0.2% of books account for around 50% of total book sales. It is possible for a
single, highly popular book to sell many more copies than many less popular books combined. Such
a book might be termed a tail event within book sales.
Music: Around 1% of artists in the music industry earn around 77% of the total revenue from all
recorded music. A single, highly successful artist can earn much more than many lesser-known artists
combined.
Net worth: Wealth distribution is a classic example of a long-tailed distribution. Global wealth
reports frequently find that the richest 1% of people own close to half of the world’s total wealth.
Web hits: There are around 200 million actively maintained websites on the internet, but the vast
majority receive very few visitors. A single, highly popular website, such as YouTube or Facebook,
can receive many more visitors per day than many obscure websites put together.
Earthquake magnitude: The frequency distribution of earthquake magnitudes is described by the
Gutenberg-Richter law. The exact distribution depends on the region and conditions, but typically,
there are around 10,000 times more earthquakes of magnitude 3.0 or above than earthquakes of
magnitude 7.0 or above. However, a single magnitude-7 earthquake can do much more damage than
all the magnitude-3 earthquakes combined.
Crater diameter: Most impact craters, resulting from collisions with other astronomical bodies, are
relatively small. Out of around 200 such craters discovered on Earth, about 120 are between 0 km
and 10 km in diameter, while fewer than 10 have a diameter of 100 km or more. Moreover, the 5%
largest craters have a combined diameter of around a third of the total diameter of all craters.
Solar flares: Solar flares of relatively low intensity are quite common, while solar flares of much
higher intensity happen infrequently. However, studies have found that the few large flares mostly
determine the total energy of all flares.
Word frequency: The vast majority of words in a given text appear infrequently. However, certain

228
CHAPTER 4. SAFETY ENGINEERING
words, like “the,” “of,” and “and,” appear far more frequently than most others. The total appearances
of a handful of these commonplace words can outweigh the total appearances of a large number of
rarer words.
These variables are said to have a long tail because, in their frequency distributions, the rare, extreme
events appear at very large values of x; the “tail” of the graph extends far along the x-axis. This is
also why rare, high-impact events are called tail events; they exist in the tail end of a long-tailed
distribution.
This type of phenomenon is sometimes referred to as the “Pareto principle” or “the 80-20 rule.” This
asserts that, in many distributions, 20% of the instances contribute 80% of the total outcome. The
exact figures depend on the variable, but the 80-20 rule is a well-known framing of the general concept
of long-tailed distributions.
Thin-tailed distributions
In thin-tailed distributions, no event is extreme enough to have much effect.
Thin-tailed
distributions are those with relatively little variation between values, meaning that even the most
extreme instances cannot substantially affect the overall landscape. Many human characteristics,
such as height, general intelligence, and lifespan, have thin-tailed distributions. Note that these
examples all follow something akin to a normal distribution, or bell curve, as shown in Figure 4.14.
Normal distributions have a maximum density at the most common value and a tail on either side.
The frequency decreases rapidly as we move away from the most common value.
0
5
10
15
20
60
65
70
75
Height (in Inches)
Number of People
Distribution of Heights
Figure 4.12: Many natural phenomena (such as the distribution of humans’ heights) follow the normal
distribution.
All of these example characteristics vary between individuals, but the range of variation is much
narrower than it is for variables with long-tailed distributions. For example, while human height
varies, there has never been a person who was fifty feet tall. Even the tallest person in the world is
not ten times taller than the shortest. This contrasts with long-tailed distributions such as book

4.7. TAIL EVENTS
229
sales, where a bestseller can easily sell thousands of times more copies than a less popular book.
Following on from this, under thin-tailed distributions, no single instance is so extreme that it can
account for a large share of the total among a group; the total height of the tallest 10% of people
will not be significantly more than 10% of the total height of all people. Again, this contrasts with
long-tailed distributions, where a small fraction of instances can account for most of the impact.
Thin-tailed distributions are so-called because, in their frequency distributions, the value on the
y-axis rapidly approaches zero for highly extreme events; the “tail” of the graph quickly becomes
vanishingly thin.
4.7.3
Heuristics and Models for Thinking About Long Tails and Thin
Tails
Generally, long-tailed variables span orders of magnitude and thin-tailed ones do not.
A general rule of thumb for distinguishing between long-tailed and thin-tailed variables is to ask:
Does it still make sense if we add a zero at the end of a data point’s value? In other words, can the
variable span multiple orders of magnitude? If the answer is no, it is more likely to be thin-tailed; if
yes, the variable is more likely to be long-tailed.
We should note that this test is imperfect. Lifespan is a thin-tailed variable that spans orders of
magnitude; some live to be over 100 years old, while others live for fewer than ten years. Nonetheless,
this question usually indicates whether a variable has a long or thin tail.

230
CHAPTER 4. SAFETY ENGINEERING
0.0
0.2
0.4
0.6
0.8
1.0
2.5
5.0
7.5
10.0
x
Density
Distribution
Power Law
Exponential
Long Tail vs Exponential Distribution
0.000
0.001
0.002
0.003
0.004
15
20
25
30
x
Density (Zoomed In)
Distribution
Power Law
Exponential
Long Tail vs Exponential Distribution (in the Tails)
10−12
10−10
10−8
10−6
10−4
15
20
25
30
x
Density (Log Scale)
Distribution
Power Law
Exponential
Long Tail vs Exponential Distribution (Log Scale)
Figure 4.13: Exponential distributions are not long-
tailed, even though they look quite similar at a normal
scale (top).
Looking at extreme values (middle),
especially on a log scale (bottom), shows us that they
have thin tails.
Exponential
distributions
are
thin-tailed distributions.
Figure 4.13
shows an exponential distribution (in black)
and a power-law distribution (in grey). At
first glance, they look quite similar. However,
in the second and third graphs, we zoom in
to the tail end of the graph and change the
y-axis to a logarithmic scale. Here, we can see
that, beyond a certain point, the exponential
distribution decays significantly more quickly
than the power-law distribution. As a rough
heuristic, frequency distributions that decay
more slowly than an exponential are generally
considered long-tailed, while those that decay
more quickly are considered thin-tailed.
There is a significant difference between
extreme
events
being
unlikely
and
astronomically
unlikely.
In
practical
terms,
for
events
with
exponential distributions—and distributions
that decay more quickly than them—the
probability of extreme events rapidly becomes
astronomically small. In fact, it becomes so
small that, at some point, we can consider
it essentially zero. Under distributions that
decay more slowly than exponentials, such
as the power law depicted, the probability of
extreme events is still low. The important
difference, however, is that they are possible.
Under long-tailed distributions, they are rare
but possible and have an enormous impact if
they happen.
Note that the frequency of extreme events
depends on how we define “extreme,” namely,
how far along the x-axis the event appears.
We use “extreme” to describe events so far
from the most common value that they can
significantly impact the mean and represent a
disproportionately large fraction of the total.
Returning
to
the
risk
equation,
under
thin-tailed distributions,
the probability,
P(hazard), of a tail event is so astronomically
small that tail events do not affect the total
risk much. Under long-tailed distributions,
the probability, P(hazard), of a tail event is
small, but the severity is so large that tail events can substantially alter the total risk.
Risk(hazard) = P(hazard) × severity(hazard)

4.7. TAIL EVENTS
231
Next, we will consider three specific differentiators of long- and thin-tailed distributions: excess
conditional expectation, multiplicative vs. additive phenomena, and preferential attachment.
Excess conditional expectation
Excess conditional expectation is the margin between minimum and actual values.
Another difference between long-tailed and thin-tailed distributions is the “excess conditional
expectation” [4]. This is the difference between the conditional expectation–—our best guess of how
large something will be, given that we know that it is already above or below some threshold—and
the threshold itself. Only long-tailed distributions tend to have high excess conditional expectation.
This concept is best illustrated with examples.
Example: Long Jump, Thin Tails.
Imagine we are at a training session for long-jump athletes.
The world record for the long jump is around 9 meters. If an athlete jumps 7 meters at the training
session, then this represents a definite lower bound for their personal best: it is at least 7 meters. If
we are then asked to guess their actual personal best, we might estimate that it could be close to 8
meters.
If another athlete jumps 8 meters at the training session, then we might expect that their personal
best is around 8.5 meters. If yet another athlete jumps 8.5 meters, then we might think their personal
best is around 8.7 meters.
Figure 4.14 shows a plot of the observed jumps as blue dots, and our best guess of each athlete’s
personal best in dotted lines. Notice how, as the definite minimum value increases, we observe a
shrinking margin between this lower bound and our estimate of the athlete’s actual maximum jump.
This is because we know that the world record sets a definite limit on what any individual’s personal
best could be. The closer the observed distance gets to the world record, the less room there is for
the athlete to have jumped further.
Under thin tails, excess conditional expectation shrinks as values get more extreme.
The margin between the known lower bound of something and the estimate of its actual value is the
excess conditional expectation. Under thin-tailed distributions, this margin tends to shrink as the
minimum value becomes more extreme. This is because there is a limit to how extreme it can become;
as the lower bound approaches that limit, there is less room for the actual value to be different.

232
CHAPTER 4. SAFETY ENGINEERING
7.0
7.5
8.0
8.5
9.0
1
2
3
Athlete
Jump Distance (meters)
Current Observation
Estimated Best
Thin−Tailed Distribution (Long Jump)
0
50
100
150
200
1
2
3
Book
Lifespan (years)
Current Age
Estimated Lifespan
Long−Tailed Distribution (Lindy Effect)
Figure 4.14: Our excess conditional expectation when observing long jumps (thin-tailed) shrinks as
the jump gets better. By contrast, our excess conditional expectation when observing the lifespan of
a book (long-tailed) gets larger as the book remains popular for longer.
Example: Lindy Effect, Long Tails.
The Lindy Effect is the theory that, for certain types of
entities, such as technologies and ideas, the longer they have been around, the longer we can expect
them to survive into the future [19].
For example, if a book has been in print for two years, then it might remain in print for another two
years. However, most books do not “stand the test of time,” so we might not expect it to survive
another 100 years. Meanwhile, if another book has already been in print for 100 years, then we might
well expect it to remain so for another 100 because it has showed it is robust to the test of time; if it
has already remained relevant and interesting for a long time, then it is more likely to continue to do
so for a long time into the future.
Figure 4.14 shows some hypothetical observations of the current ages of books as blue lines, and our
estimate of what their actual lifespan will be in dotted lines. Notice how, as the known lower bound
of lifespan increases, the amount of additional time we expect a book to survive also increases.
Under long tails, excess conditional expectation grows as values become more extreme.
Under long tails, values of individual occurrences can grow much more extreme compared to the
majority. There is not usually as clear a limit on long-tailed variables as on thin-tailed variables, so
there is more room for the actual value to differ from the known minimum value, even if the latter is
already extreme.
To differentiate between thin- and long-tailed distributions, we have now explored excess conditional
distribution, a measure of the difference between our expectation of real-world values and the
thresholds we observe them to exceed. Next, we will consider how variables arise: multiplicatively or
additively.
Multiplicative and additive phenomena
One factor in determining whether a variable is likely
to have a long-tailed or thin-tailed distribution is whether it arises from a multiplicative or additive

4.7. TAIL EVENTS
233
phenomenon. We will now look at the differences between these two categories of phenomena and
why they affect how extreme a variable can become.
With multiplicative phenomena, one instance can multiply rapidly, triggering a cascade.
When something is a multiplicative phenomenon, individual instances are not independent of one
another. Virus infections are a classic example of this; one infection is likely to seed several more
infections, each of which can go on to cause even more. Thus, a single infection can trigger a cascade,
so a virus outbreak has the potential to scale up considerably. This is part of the reason why virus
outbreaks have a long-tailed distribution, with pandemics representing tail events.
With additive phenomena, individual instances do not tend to multiply.
When something
is an additive phenomenon, individual events are largely independent of one another. Broken bones
are an example of this; one person falling and breaking a bone does not usually trigger a cascade of
other people falling and breaking bones. There may be instances where multiple broken bones occur
together, for example in a car accident. However, in general, they are independently caused and do
not have the potential to scale up as an infectious disease can. This means that tail events do not
generally happen with this phenomenon; there is no single event where so many bones are broken
that it significantly affects the average number of bones broken in accidents.
A high level of connectedness in a system gives rise to long tails.
Another way of thinking
about this is to consider the level of connectedness in the relevant system. Since viruses are infectious,
instances of infection are strongly connected to one another. Broken bones are not infectious, so
instances of them are relatively isolated from one another.
We have seen that multiplicative phenomena are much more likely to result in long-tailed variables
than additive ones. Next, we discuss another model which gives rise to long tails: preferential
attachment.
Preferential attachment
In preferential attachment processes, the more someone has, the more they will receive.
Preferential attachment processes are mechanisms where the amount extra of something that an
individual will receive is positively correlated with the amount they already have. This is also known
as the “rich get richer” effect because it is often illustrated with the example of wealth distribution;
the more money a person already has, the more money they can invest, and thus the more additional
money they can make through returns on their investments.
This effect applies to many phenomena within social systems. For example, if an author already has
a lot of readers, there are more people who are likely to recommend their books to others. This
author is therefore likely to gain more additional readers than someone else who starts out with fewer
readers. Their sales can therefore snowball and become a tail event in the context of book sales.
Similar mechanisms can also explain why a few social media platforms vastly outperform others, and
why a few scientific papers receive many more citations than most.
Preferential attachment processes give rise to long tails [4].
In situations where preferential
attachment processes take place, events are not independent of one another. This is why things like
book sales and wealth acquisition are multiplicative phenomena, where a single event can escalate
to become much larger than most. By extension, this is why preferential attachment processes can
produce long-tailed distributions.

234
CHAPTER 4. SAFETY ENGINEERING
4.7.4
A Caricature of Long Tails and Thin Tails
To cement our understanding of long-tailed and thin-tailed distributions, we will now run through
some comparisons between the two categories. Note that, with these statements, we are describing
simplified caricatures of the two scenarios, for pedagogical purposes.
Caricature: Thin Tails
Caricature: Long Tails
The top few receive a proportionate share of the
total.
The top few receive a disproportionately large share
of the total.
The total is determined by the whole group
("tyranny of the collective").
The total is determined by a few extreme
occurrences ("tyranny of the accidental").
The typical member of a group has an average
value, close to the mean.
The typical member is either a giant or a dwarf.
A single event cannot escalate to become much
bigger than the average.
A single event can escalate to become much bigger
than many others put together.
Individual data points vary within a small range
that is close to the mean.
Individual data points can vary across many orders
of magnitude.
We can predict roughly what value a single instance
will take.
It is much harder to robustly predict even the rough
value that a single instance will take.
Table 4.3: A caricature of thin tails and long tails reveals several trends that often hold for each.
Contrast 1: Share of the total received by the top few
Under thin tails, the top few receive quite a proportionate share of the total.
If we were
to measure the heights of a group of people, the total height of the tallest 10% would not be much
more than 10% of the total height of the whole group.
Under long tails, the top few receive a disproportionately large share of the total. For example, in
the music industry, the revenue earned by the most successful 1% of artists represents around 77% of
the total revenue earned by all artists.
Contrast 2: Who determines the total?
Under thin tails, the total is determined by the whole group.
The total height of the
tallest 10% of people is not a very good approximation of the total height of the whole group. Most
members need to be included to get a good measure of the total. This is called “tyranny of the
collective.”
Under long tails, the total is determined by a few extreme occurrences.
As discussed
above, the most successful 1% of artists earn 77% of the total revenue earned by all artists. 77% is a
fairly good approximation of the total. In fact, it is a better approximation than the revenue earned
by the remaining 99% of artists would be. This is called “tyranny of the accidental.”
Contrast 3: The typical member
Under thin tails, the typical member of a group has an average value.
Almost no members
are going to be much smaller or much larger than the mean.
Under long tails, the typical member is either a giant or a dwarf.
Members can generally
be classified as being either extreme and high-impact or relatively insignificant.

4.8. BLACK SWANS
235
Note that, under many real-world long-tailed distributions, there may be occurrences that seem to
fall between these two categories. There may be no clear boundary dividing occurrences that count
as insignificant from those that count as extreme. However, with these statements, we are describing
a caricature of long tails, for pedagogical purposes.
Contrast 4: Scalability of events
Under thin tails, the impact of an event is not scalable.
A single event cannot escalate to
become much bigger than the average.
Under long tails, the impact of an event is scalable.
A single event can escalate to become
much bigger than many others put together.
Contrast 5: Randomness
Under thin tails, individual data points vary within a small range that is close to the
mean.
Even the data point that is furthest from the mean does not change the mean of the whole
group by much.
Under long tails, individual data points can vary across many orders of magnitude.
A
single extreme data point can completely change the mean of the sample.
Contrast 6: Predictability
Under thin tails, we can predict roughly what value a single instance will take.
We can
be confident that our prediction will not be far off, since instances cannot stray too far from the
mean.
Under long tails, it is much harder to predict even the rough value that a single instance
will take.
Since data points can vary much more widely, our best guesses can be much further off.
4.8
Black Swans
Introduction to Black Swans
So far, we have discussed tail events in general. We will now look at an important subset of tail events,
called black swans. In addition to being rare and high-impact, black swans are also unanticipated,
seeming to happen out of the blue. The term “black swan” originates from a historical event that
provides a useful analogy.
It was long believed in Europe that all swans were white because all swan sightings known to
Europeans were of white swans. For this reason, the term “black swan” came to denote something
nonexistent, or even impossible, much as today we say “pigs might fly.” The use of this metaphor is
documented as early as Roman times. However, in 1697, a group of Dutch explorers encountered
a black species of swan in Australia. This single, unexpected discovery completely overturned the
long-held axiom that all swans were white.
This story offers an analogy for how we can have a theory or an assumption that seems correct for
a long time, and then a single, surprising observation can completely upend that model. Such an
observation can be classed as a tail event because it is rare and high-impact. Additionally, the fact

236
CHAPTER 4. SAFETY ENGINEERING
that the observation was unforeseen shows us that our understanding is incorrect or incomplete.
From here on we will use the following working definition of black swans: A black swan is a tail event
that was largely unpredictable to most people before it happened. Note that not all tail events are
black swans; high-magnitude earthquakes, for example, are tail events, but we know they are likely
to happen in certain regions at some point–—they are on our radar.
4.8.1
Known Unknowns and Unknown Unknowns
Black swans are “unknown unknown” tail events [20].
We can sort events into four categories,
as shown in the table below.
Known knowns: things we are aware of
and understand.
Unknown knowns:
things that we
do not realize we know (such as tacit
knowledge).
Known unknowns: things we are aware
of but which we don’t fully understand.
Unknown unknowns: things that we
do not understand, and which we are not
even aware we do not know.
In these category titles, the first word refers to our awareness, and the second refers to our
understanding. We can now consider these four types of events in the context of a student preparing
for an exam.
1. We know that we know. Known knowns are things we are both aware of and understand.
For the student, these would be the types of questions that have come up regularly in previous
papers and that they know how to solve through recollection. They are aware that they are
likely to face these, and they know how to approach them.
2. We do not know what we know. Unknown knowns are things we understand but may not
be highly aware of. For the student, these would be things they have not thought to prepare
for but which they understand and can do. For instance, there might be some questions on
topics they hadn’t reviewed; however, looking at these questions, the student finds that they
know the answer, although they cannot explain why it is correct. This is sometimes called tacit
knowledge or unaccounted facts.
3. We know that we do not know. Known unknowns are things we are aware of but do not
fully understand. For the student, these would be the types of questions that have come up
regularly in previous papers but which they have not learned how to solve reliably. The student
is aware that they are likely to face these but is not sure they will be able to answer them
correctly. However, they are at least aware that they need to do more work to prepare for them.
4. We do not know that we do not know. Unknown unknowns are things we are unaware of
and do not understand. These problems catch us completely off guard because we didn’t even
know they existed. For the student, unknown unknowns would be unexpectedly hard questions
on topics they have never encountered before and have no knowledge or understanding of.
Unknown unknowns can also occur in AI safety and risk.
Researchers may be diligently
studying various aspects of AI and its potential risks, but new and unforeseen risks could emerge
as AI technology advances. These risks may be completely unknown and unexpected, catching
researchers off guard. It is important to acknowledge the existence of unknown unknowns because
they remind us that there are limits to our knowledge and understanding. By being aware of this,
we can be more humble in our approach to problem-solving and continuously strive to expand our
knowledge and prepare for the unexpected.

4.8. BLACK SWANS
237
We struggle to account for known unknowns and unknown unknowns.
We have included
the first two categories—known knowns and unknown knowns—for completeness. However, the
most important categories in risk analysis and management are the last two: known unknowns and
unknown unknowns. These categories pose risks because we do not fully understand how best to
respond to them, and we cannot be perfectly confident that we will not suffer damage from them.
Unknown unknowns are particularly concerning.
If we are aware that we might face a
particular challenge, we can learn more and prepare for it. However, if we are unaware that we will
face a challenge, we may be more vulnerable to harm. Black swans are the latter type of event; they
are not even on our radar before they happen.
The difference between known unknowns and unknown unknowns is sometimes also described as a
distinction between conscious ignorance and meta-ignorance. Conscious ignorance is when we see
that we do not know something, whereas meta-ignorance is when we are unaware of our ignorance.
Black swans in the real world
It might be unfair for someone to present us with an unknown
unknown, such as finding questions on topics irrelevant to the subject in an exam setting. The wider
world, however, is not a controlled environment; things do happen that we have not thought to
prepare for.
Black swans indicate that our worldview is inaccurate or incomplete.
Consider a turkey
being looked after by humans, who provide plenty of food and a comfortable shelter safe from
predators. According to all the turkey’s evidence, the humans are benign and have the turkey’s best
interests at heart. Then, one day, the turkey is taken to the slaughterhouse. This is very much an
unknown unknown, or a black swan, for the turkey, since nothing in its experience suggested that
this might happen [20].
This illustrates that we might have a model or worldview that does a good job of explaining all our
evidence to date, but then a black swan can turn up and show us that our model was incorrect. The
turkey’s worldview of benign humans explained all the evidence until the slaughterhouse trip. This
event indicated a broader context that the turkey was unaware of.
Similarly, consider the 2008 financial crisis. Before this event, many people, including many of those
working in finance, assumed that housing prices would always continue to increase. When the housing
bubble burst, it showed that this assumption was incorrect.
Black swans are defined by our understanding.
A black swan is a black swan because our
worldview is incorrect or incomplete, which is why we fail to predict it. In hindsight, such events
often only make sense after we realize that our theory was flawed. Seeing black swans makes us
update our models to account for the new phenomena we observe. When we have a new, more
accurate model, we can often look back in time and find the warning signs in the lead-up to the
event, which we did not recognize as such at the time.
These examples also show that we cannot always reliably predict the future from our experience; we
cannot necessarily make an accurate calculation of future risk based on long-running historical data.
Distinguishing black swans from other tail events
Only some tail events are black swans.
As touched on earlier, it is essential to note that black
swans are a subset of tail events, and not all tail events are black swans. For example, it is well
known that earthquakes happen in California and that a high-magnitude one, often called “the big

238
CHAPTER 4. SAFETY ENGINEERING
one,” will likely happen at some point. It is not known exactly when—whether it will be the next
earthquake or in several decades. It might not be possible to prevent all damage from the next “big
one,” but there is an awareness of the need to prepare for it. This represents a tail event, but not a
black swan.
Some people might be able to predict some black swans.
A restrictive definition of a black
swan is an event that is an absolute unknown unknown for everybody and is impossible to anticipate.
However, for our purposes, we are using the looser, more practical working definition given earlier: a
highly impactful event that is largely unexpected for most people. For example, some individuals
with relevant knowledge of the financial sector did predict the 2008 crisis, but it came out of the blue
for most people. Even among financial experts, the majority did not predict it. Therefore, we count
it as a black swan.
Similarly, although pandemics have happened throughout history, and smaller disease outbreaks
occur yearly, the possibility of a pandemic was not on most people’s radar before COVID-19. People
with specific expertise were more conscious of the risk, and epidemiologists had warned several
governments for years that they were inadequately prepared for a pandemic. However, COVID-19
took most people by surprise and therefore counts as a black swan under the looser definition.
The development and rollout of AI technologies could be subject to black swans.
Within
the field of AI, the consensus view for a long time was that deep learning techniques were fundamentally
limited. Many people, even computer science professors, did not take seriously the idea that deep
learning technologies might transform society in the near term—even if they thought this would be
possible over a timescale of centuries.
Deep learning technologies have already begun to transform society, and the rate of progress has
outpaced most people’s predictions. We should, therefore, seriously consider the possibility that the
release of these technologies could pose significant risks to society.
There has been speculation about what these risks might be, such as a flash war and autonomous
economy, which are discussed in the Collective Action Problems chapter. These eventualities might
be known to some people, but for many potential risks, there is not widespread awareness in society;
if they happened today, they would be black swans. Policymakers must have some knowledge of these
risks. Furthermore, the expanding use of AI technologies may entail risks of black swan scenarios
that no one has yet imagined.
4.8.2
Implications of Tail Events and Black Swans for Risk Analysis
Tail events and black swans present problems for analyzing and managing risks, because we do not
know if or when they will happen. For black swans, there is the additional challenge that we do not
know what form they will take.
Since, by definition, we cannot predict the nature of black swans in advance, we cannot put any
specific defenses in place against them, as we might for risks we have thought of. We can attempt to
factor black swans into our equations to some degree, by trying to estimate roughly how likely they
are and how much damage they would cause. However, they add much more uncertainty into our
calculations. We will now discuss some common tendencies in thinking about risk, and why they can
break down in situations that are subject to tail events and black swans.
First, we consider how our typical risk estimation methods break down under long tails because our
standard arsenal of statistical tools are rendered useless. Then, we consider how cost-benefit analysis

4.8. BLACK SWANS
239
is strained when dealing with long-tailed events because of its sensitivity to our highly uncertain
estimates. After this, we discuss why we should be more explicitly considering extremes instead of
averages, and look at three common mistakes when dealing with long-tailed data: the delay fallacy,
interpreting an absence of evidence, and the preparedness paradox.
Typical risk estimation methods break down under long tails
Tail events and black swans can substantially change the average risk of a system.
It
is challenging to account for tail events in the risk equation. Since tail events and black swans are
extremely severe, they significantly affect the average outcome. Recall the equation for risk associated
with a system:
Risk =
X
hazard
P(hazard) × severity(hazard).
Additionally, it is difficult to estimate their probability and severity accurately. Yet, they would
considerably change the evaluation of risk because they are so severe. Furthermore, since we do not
know what form black swans will take, it may be even more difficult to factor them into the equation
accurately. This renders the usual statistical tools useless in practice for analyzing risk in the face of
potential black swans.
If the turkey in the previous example had tried to calculate the risk to its wellbeing based on all its
prior experiences, the estimated risk would probably have been fairly low. It certainly would not
have pointed to the high risk of being taken to the slaughterhouse, because nothing like that had
ever happened to the turkey before.

240
CHAPTER 4. SAFETY ENGINEERING
5.3
5.4
5.5
0
1000
2000
3000
4000
Number of Observations
Average Height (feet)
Mean of Thin−Tailed Distribution
104
105
106
0
1000
2000
3000
4000
Number of Observations
Average Net Worth (dollars)
Mean of Long−Tailed Distribution
Figure 4.15: The mean of a long-tailed distribution is
slow to convergence, rendering the mean a problematic
summary statistic in practice.
We need a much larger dataset than
usual.
As we increase the number of
observations, we converge on an average
value. Suppose we are measuring heights
and calculating a new average every time
we add a new data point. As shown in the
first graph in figure 4.15, as we increase our
number of data points, we quickly converge
on an average that changes less and less
with the addition of each new data point.
This is a result of the law of large numbers.
Heights, however, are a thin-tailed variable.
If we look instead at a long-tailed variable,
such as net worth, as shown in the second
graph in figure 4.15, a single extreme
observation can change the average by
several orders of magnitude. The law of
large numbers still applies, in that we will
still eventually converge on an average
value, but it will take much longer.
Linear
regression
is
a
standard
prediction method but is less useful
for long-tailed data.
Linear regression
is a technique widely used to develop
predictive models based on historical data.
However,
in situations where we are
subject to the possibility of tail events or
black swans, we might not be sure that
we have enough historical data to converge
on an accurate calculation of average risk.
Linear regression is, therefore, less helpful
in assessing and predicting risk for long-
tailed scenarios.
Explicit cost-benefit analysis is strained under long tails
Cost-benefit analysis using long-tailed data often requires highly accurate estimates. Traditional
cost-benefit analysis weighs the probability of different results and how much we would lose or gain
in each case. From this information, we can calculate whether we expect the outcome of a situation
to be positive or negative. For example, if we bet on a 50/50 coin toss where we will either win $5 or
lose $5, our overall expected outcome is $0.
Example: lotteries.
Now, imagine that we are trying to perform a cost-benefit analysis for a
lottery where we have a high probability of winning a small amount and a low probability of losing a
large amount. If we have a 99.9% chance of winning $15 and a 0.1% chance of losing $10,000, then
our expected outcome is:
(0.999 × 15) + (0.001 × −10000) = 4.985.

4.8. BLACK SWANS
241
Since this number is positive, we might believe it is a good idea to bet on the lottery. However, if the
probabilities are only slightly different, at 99.7% and 0.3%, then our expected outcome is:
(0.997 × 15) + (0.003 × −10000) = −15.045.
This illustrates that just a tiny change in probabilities sometimes makes a significant difference in
whether we expect a positive or a negative outcome. In situations like this, where the expected
outcome is highly sensitive to probabilities, using an estimate of probability that is only slightly
different from the actual value can completely change the calculations. For this reason, relying on
this type of cost-benefit analysis does not make sense if we cannot be sure we have accurate estimates
of the probabilities in question.
It is difficult to form accurate probability estimates for black swans.
Black swans happen
rarely, so we do not have a lot of historical data from which to calculate the exact probability that
they will occur. As we explored above, it takes a lot of data—often more than is accessible—to make
accurate judgments for long-tailed events more generally. Therefore, we cannot be certain that we
know their probabilities accurately, rendering cost-benefit analysis unsuitable for long-tailed data,
especially for black swans.
This consideration could be significant for deciding whether and how to use AI technologies. We
might have a high probability of benefitting from the capabilities of deep learning models, and there
might be only a low probability of an associated black swan transpiring and causing harm. However,
we cannot calculate an accurate probability of a black swan event, so we cannot evaluate our expected
outcome precisely.
It is unrealistic to estimate risk when we could face black swans.
If we attempt to develop
a detailed statistical model of risk for a situation, we are making an implicit assumption that we have
a comprehensive understanding of all the possible failure modes and how likely they are. However, as
previous black swan events have demonstrated, we cannot always assume we know all the eventualities.
Even for tail events that are known unknowns, we cannot assume we have sufficiently accurate
information about their probabilities and impacts. Trying to precisely estimate risk when we might
be subject to tail events or black swans can be viewed as an “unscientific overestimation of the reach
of scientific knowledge” [3].
Thinking about extremes instead of averages is better under long tails
When making risk-related decisions, we should consider extremes, not only the average.
Aside from whether or not we can calculate an accurate average outcome under the risk of tail events
and black swans, there is also a question of whether the average is what we should be paying attention
to in these situations anyway. This idea is captured in the following adage commonly attributed
to Milton Friedman: “Never try to walk across a river just because it has an average depth of four
feet.” If a river is four feet deep on average, that might mean that it has a constant depth of four
feet and is possible to wade across it safely. It might also mean that it is two or three feet deep near
the banks and eight feet deep at some point in the middle. If this were the case, then it would not
be a good idea to attempt to wade across it.
Failing to account for extremes instead of averages is one example of the mistakes people make when
thinking about event types that might have black swans. Next, we will explore three more: the delay
fallacy, misinterpreting an absence of evidence, and the preparedness paradox.

242
CHAPTER 4. SAFETY ENGINEERING
The delay fallacy
If we do not have enough information to conduct a detailed risk analysis, it might be tempting to
gather more information before taking action. A common excuse for delaying action is: “If we wait,
we will know more about the situation and be able to make a more informed decision, so we should
not make any decisions now.”
In thin-tailed scenarios, waiting for more information is often a good approach.
Under
thin tails, additional observations will likely help us refine our knowledge and identify the best course
of action. Since there are no tail events, there is a limit to how much damage a single event can do.
There is, therefore, less urgency to take action in these situations. The benefit of more information
can be considered to outweigh the delay in taking action.
In long-tailed scenarios, waiting for more information can mean waiting until it is too
late.
Additional observations will not necessarily improve our knowledge of the situation under long
tails. Most, if not all, additional observations will probably come from the head of the distribution
and will not tell us anything new about the risk of tail events or black swans. The longer we wait
before preparing, the more we expose ourselves to the possibility of such an event happening while
we are unprepared. When tail events and black swans do materialize, it is often too late to intervene
and prevent harm.
Governments failing to improve their pandemic preparedness might be considered an example of this.
Epidemiologists’ warnings were long ignored, which seemed fine for a long time because pandemics
are rare. However, when COVID-19 struck, many governments tried to get hold of personal protective
equipment (PPE) simultaneously and found a shortage. If they had stocked up on this before the
pandemic, as experts had advised, then the outcome might have been less severe.
Furthermore, if a tail event or black swan is particularly destructive, we can never observe it and
use that information to help us make better calculations. The turkey cannot use the event of being
taken to the slaughterhouse to make future risk estimations more accurate. With respect to society,
we cannot afford for events of this nature to happen even once.
We should be proactively investing in AI safety now.
Since the development and rollout of
AI technologies could represent a long-tailed scenario, entailing a risk of tail events and black swans,
it would not make sense to delay action with the excuse that we do not have enough information.
Instead, we should be proactive about safety by investing in the three key research fields discussed
earlier: robustness, monitoring, and control. If we wait until we are certain that an AI could pose an
existential risk before working on AI safety, we might be waiting until it is too late.
Interpreting an absence of evidence
It can be hard to imagine a future that is significantly different from our past and present experiences.
Suppose a particular event has never happened before. In that case, it can be tempting to interpret
that as an indication that we do not need to worry about it happening in the future, but this is not
necessarily a sound judgment.
An absence of evidence is not strong evidence of absence.
Even if we have not found
evidence that there is a risk of black swan events, that is not evidence that there is no risk of
black swan events. In the context of AI safety, we may not have found evidence that deep learning
technologies could pose specific risks like deceptive alignment, but that does not necessarily mean
that they do not pose such risks or that they will not at some point in the future.

4.8. BLACK SWANS
243
The preparedness paradox
Safety measures that prevent harm can seem redundant.
Imagine that we enact safety
measures to reduce the risk of a potentially destructive event, and then the event does not happen.
Some might be tempted to say that the safety measures were unnecessary or that implementing them
was a waste of time and resources. Even if the event does happen but is not very severe, some people
might still say that the safety measures were unnecessary because the event’s consequences were not
so bad.
However, this conclusion ignores the possibility that the event did not happen or was less severe
because of the safety measures. We cannot run through the same period of time twice and discover
how things would have unfolded without any safety measures. This is a cognitive bias known as the
preparedness paradox: efforts to prepare for potential disasters can reduce harm from these events
and, therefore, reduce the perceived need for such preparation.
The preparedness paradox can lead to self-defeating prophecies.
A related concept is
the “self-defeating prophecy,” where a forecast can lead to actions that prevent the forecast from
coming true. For example, suppose an epidemiologist predicts that there will be a high death toll
from a particular infectious disease. In that case, this might prompt people to wash their hands
more frequently and avoid large gatherings to avoid infection. These behaviors are likely to reduce
infection rates and lead to a lower death toll than the epidemiologist predicted.
If we work proactively on reducing risks from global pandemics, and no highly destructive pandemics
come to pass, some people would believe that the investment was unnecessary. However, it might be
because of those efforts that no destructive events happen. Since we usually cannot run two parallel
worlds—one with safety efforts and one without—it might be difficult or impossible to prove that
the safety work prevented harm. Those who work in this area may never know whether their efforts
have prevented a catastrophe and have their work vindicated. Nevertheless, preventing disasters is
essential, especially in cases like the development of AI, where we have good theoretical reasons to
believe that a black swan is on the cards.
4.8.3
Identifying the Risk of Tail Events or Black Swans
Since the possibility of tail events and black swans affects how we approach risk management, we
must consider whether we are facing a long-tailed or thin-tailed scenario. We need to know whether
we can rely on standard statistical methods to estimate risk or whether we face the possibility of
rare, high-impact events. This can be difficult to determine, especially in cases of low information,
but there are some valuable indicators we can look for.
Highly connected systems often give rise to long-tailed scenarios.
As discussed earlier,
multiplicative phenomena can lead to long tails. We should ask ourselves: Can one part of the system
rapidly affect many others? Can a single event trigger a cascade? If the answers to these questions
are yes, then it is possible that an event can escalate to become a tail event with an extreme impact.
The use of AI in society could create a new, highly connected system.
If deep learning
models become enmeshed within society and are put in charge of various decisions, then we will have
a highly connected system where these agents regularly interact with humans and each other. In
these conditions, a single erroneous decision made by one agent could trigger a cascade of harmful
decisions by others, for example, if they govern the deployment of weapons. This could leave us
vulnerable to sudden catastrophes such as flash wars or powerful rogue AIs.

244
CHAPTER 4. SAFETY ENGINEERING
Complex systems may be more likely to entail a risk of black swans.
Complex systems
can evolve in unpredictable ways and develop unanticipated behaviors. We cannot usually foresee
every possible way a complex system might unfold. For this reason, we might expect that complex
evolving systems present an inherent risk of black swans.
Deep learning models and the surrounding social systems are all complex systems.
It
is unlikely that we will be able to predict every single way AI might be used, just as, in the early
days of the internet, it would have been difficult to predict every way technology would ultimately
be used. This means that there might be a risk of AI being used in harmful ways that we have not
foreseen, potentially leading to a destructive black swan event that we are unprepared for. The idea
that deep learning systems qualify as complex systems is discussed in greater depth in the Complex
Systems chapter.
New systems may be more likely to present black swans.
Absence of evidence is only
evidence of absence if we expect that some evidence should have turned up in the timeframe that has
elapsed. For systems that have not been around for long, we would be unlikely to have seen proof of
tail events or black swans since these are rare by definition.
AI may not have existed for long enough for us to have learned about all its risks.
In
the case of emerging technology, it is reasonable to think that there might be a risk of tail events
or black swans, even if we do not have any evidence yet. The lack of evidence might be explained
simply by the fact that the technology has not been around for long. Our meta-ignorance means that
we should take AI risk seriously. By definition, we can’t be sure there are no unknown unknowns.
Therefore, it is over-confident for us to feel sure we have eliminated all risks.
Accelerating progress could increase the frequency of black swan events.
We have argued
that black swan events should be taken seriously, despite being rare. However, as technological
progress and economic growth advance at an increasing rate, such events may in fact become more
frequent, further compounding their relevance to risk management. This is because the increasing pace
of change also means that we will more often face novel circumstances that could present unknown
unknowns. Moreover, within the globalised economy, social systems are increasingly interconnected,
increasing the likelihood that one failure could trigger a cascade and have an outsized impact.
4.9
Reducing Risks of Tail Events and Black Swans
In the last two sections, we have explored the implications for risk management of tail events and
black swans. Since risk estimation models often break down in the face of tail events and black
swans, it might not be worth spending many resources trying to precisely calculate risks. Such
calculations might not be very informative or useful, and might simply waste time, during which we
would continue to be exposed to risks of high-impact events while we are unprepared. Even if we
lack information about tail events and black swans, there are still measures we can take to reduce
the risks they present. We will now explore six ideas to reduce risks from black swans, especially
focused on risks from AIs:
1. Find potential black swans.
2. Incorporate safe design principles.
3. Improve systemic factors.
4. Reduce our exposure to risk by following the precautionary principle.

4.9. REDUCING RISKS OF TAIL EVENTS AND BLACK SWANS
245
5. Improve institutional decision-making.
6. Reduce moral hazard.
There are techniques for turning some black swans into known unknowns.
As discussed
earlier, under our practical definition, not all black swans are completely unpredictable, especially
not for people who have the relevant expertise. Ways of putting more black swans on our radar
include expanding our safety imagination, conducting stress tests, and red-teaming [21].
Expanding our “safety imagination” can help us envision a wider range of possibilities. We can do this
by playing a game of “what if” to increase the range of possible scenarios we can imagine unfolding.
Brainstorming sessions can also help to rapidly generate lots of new ideas about potential failure
modes in a system. We can identify and question our assumptions–—about what the nature of a
hazard will be, what might cause it, and what procedures we will be able to follow to deal with it—in
order to imagine a richer set of eventualities.
Some HROs use a technique called horizon scanning, which involves monitoring potential future
threats and opportunities before they arrive, to minimize the risk of unknown unknowns [22]. AI
systems could be used to enhance horizon-scanning capabilities by simulating situations that mirror
the real world with a high degree of complexity. The simulations might generate data that reveal
potential black swan risks to be aware of when deploying a new system. As well as conducting
horizon scanning, HROs also contemplate near-misses and speculate about how they might have
turned into catastrophes, so that lessons can be learned.
“Red teams” can find more black swans by adopting a mindset of malicious intent. Red teams should
try to think of as many ways as they can to misuse or sabotage the system. They can then challenge
the organization on how it would respond to such attacks. Finally, stress tests such as dry-running
hypothetical scenarios and evaluating how well the system copes with them, and thinking about how
it could be improved can improve a system’s resilience to unexpected events.
Incorporating safe design principles can improve general safety.
While it might be possible
to find some black swans, it would be difficult to ever ensure that there are no more unknown
unknowns posing a risk to us. Nevertheless, there are some actions that robustly improve safety,
even if we are not aware of all the risks. Following the safe design principles listed earlier in this
chapter can be a good first step, with the caveat that we should think carefully about which defense
features are appropriate, and avoid too much complexity. In particular, focusing on increasing the
controllability of the system might be a good idea. This can be done by adding loose coupling into
the system, by supporting human operators to notice hazards and act on them early, and by devising
negative feedback mechanisms that will down-regulate processes if control is lost.
Consider in detail the principle of least privilege. For one, it tells us that we should be cautious
about giving AIs too much power, to limit the extent to which we are exposed to their tail risks. We
might be concerned that AIs become enmeshed within society with the capacity to make big changes
in the world when they do not need such access to perform their assigned duties. Additionally,
for particularly powerful AI systems that have useful capabilities, it might be reasonable to keep
them relatively isolated from wider society, and accessible only to verified individuals who have
demonstrable and specific needs for such AIs. In general, being conservative about if and how we
unleash technologies can reduce our exposure to black swans.
Improving systemic factors can substantially reduce overall risk.
As we discussed, tackling
systemic safety issues can be more effective than focusing on details in complex systems. This can

246
CHAPTER 4. SAFETY ENGINEERING
reduce the risk of both foreseeable accidents and black swans. Raising general awareness of risks
associated with technologies can produce social pressures, and bring organizations operating those
technologies under greater scrutiny. Developing and enforcing industry regulations can help ensure
organizations maintain appropriate safety standards, as can encouraging best practices that improve
safety culture. If there are ways of reducing the safety costs, this can make it more likely that an
organization will adopt them, also improving general safety.
Other systemic factors to pay attention to include competitive pressures. These can undermine
general safety by compelling management and employees to cut corners, whether to increase rates of
production or to reach a goal before competitors. If there are ways of reducing these pressures and
encouraging organizations to prioritize safety, this could substantially lessen overall risk.
Reduce our exposure to tail risks by being cautious [21].
The precautionary principle states
that it is generally better to err on the side of caution. In some situations, we can be extremely wrong
and things can still end up being fine; in others, we can be just slightly wrong but suffer disastrous
consequences. Some decisions require vastly more caution than others: for instance, paraphrasing
Richard Danzig, you should not ‘need evidence’ that a gun is loaded to avoid playing Russian roulette
[23]. Instead, you should need evidence of safety.
In situations where we are subject to the possibility of tail events and black swans, this evidence
might be impossible to find. One element of good decision making is to exercise more caution than
we would under thin-tailed scenarios. In the case of new technologies such as AI systems, this might
mean not deploying them rashly on a large scale. We should err on the side of caution instead.
We must also be cautious while trying to solve our problems. For example, while climate change
poses a serious threat, many experts believe it would be unwise to attempt to fix it quickly by rushing
into geoengineering solutions like spraying sulfur particles into the atmosphere. There may be an
urgent need to solve the problem, but we should take care that we are not pursuing solutions that
could cause many other problems.
Improve decision-making about AI.
We can improve decision-making about AI in two ways.
Firstly, we can help policymakers improve their decision-making processes, such as by developing
better methods of collecting, analyzing, and understanding information, as well as improving their
awareness of common cognitive biases to avoid. This approach can help those in power to understand
a situation more thoroughly and improve the likelihood that they will make good decisions.
Besides improving the decision-making of those in power, another method of improving decision
making might be to change who has a say in decisions, perhaps by including citizens in decision-
making processes, not only officials and scientists [21]. This reduces moral hazard by including the
stakeholders that have “skin in the game.” It can also lead to better decisions in general due to the
wisdom of crowds, the phenomenon where crowds composed of diverse individuals make much better
decisions collectively than most members within it, when the conditions are right.
Reduce moral hazard.
Lastly, we might want to influence the incentives of researchers developing
AI. Researchers might currently be focused on increasing profits and reaching goals before competitors,
pursuing scientific curiosity and a desire for technological progress, or developing the best capabilities
in deep learning models to find out what is possible. In this sense, these researchers might be
somewhat disconnected from the risks they could be creating and the externalities they are imposing
on the rest of society, creating a moral hazard. Encouraging more consideration of the possible risks,
perhaps by making researchers liable for any consequences of the technologies they develop, could
therefore improve general safety.

4.10. CONCLUSION
247
4.10
Conclusion
In this chapter, we have explored various methods of analyzing and managing risks inherent in
systems. We began by looking at how we can break risk down into two components: the probability
and severity of an accident. We then went into greater detail, introducing the factors of exposure
and vulnerability, showing how each affects the level of risk we calculate. By decomposing risk in
this way, we can identify measures we can take to reduce risks. We also considered the concept of
ability to cope and how it relates to risk of ruin.
Next, we described a metric of system reliability called the "nines of reliability". This metric refers
to the number of nines at the beginning of a system’s percentage or decimal reliability. We found
that adding another nine of reliability is equivalent to reducing the probability of an accident by a
factor of 10, and therefore results in a tenfold increase in expected time before failure. A limitation
of the nines of reliability is that they only contain information about the probability of an accident,
but not its severity, so they cannot be used alone to calculate risk.
We then listed several safe design principles, which can be incorporated into a system from the design
stage to reduce the risk of accidents. In particular, we explored redundancy, separation of duties, the
principle of least privilege, fail-safes, antifragility, negative feedback mechanisms, transparency, and
defense in depth.
To develop an understanding of how accidents occur in systems, we next explored various accident
models, which are theories about how accidents happen and the factors that contribute to them. We
reviewed three component failure accident models: the Swiss cheese model, the bow tie model, and
fault tree analysis, and considered their limitations, which arise from their chain-of-events style of
reasoning. Generally, they do not capture how accidents can happen due to interactions between
components, even when nothing fails. Component failure models are also unsuited to modeling how
the numerous complex interactions and feedback loops in a system can make it difficult to identify a
root cause, and how it can be more fruitful to look at diffuse causality and systemic factors than
specific events.
After highlighting the importance of systemic and human factors, we delved deeper into some
examples of them, highlighting regulations, social pressure, competitive pressures, safety costs,
and safety culture. We then moved on to look at systemic accident models that attempt to take
these factors into consideration. Normal Accident Theory states that accidents are inevitable in
complex and tightly coupled systems. On the other hand, HRO theory points to certain high
reliability organizations as evidence that it is possible to reliably avoid accidents by following five key
management principles: preoccupation with failure, reluctance to simplify interpretations, sensitivity
to operations, commitment to resilience, and deference to expertise. While these features can certainly
contribute to a good safety culture, we also looked at the limitations and the difficulties in replicating
some of them in other systems.
Rounding out our discussion of systemic factors, we outlined three accident models that are grounded in
complex systems theory. Rasmussen’s Risk Management Framework (RMF) identifies six hierarchical
levels within a system, identifying actors at each level who share responsibility for safety. The RMF
states that a system’s operations should be kept within defined safety boundaries; if they migrate
outside of these, then the system is in a state where an event at the sharp end could trigger an
accident. However, the factors at the blunt end are also responsible, not just the sharp-end event.
Similarly, STAMP and the related STPA analysis method view safety as being an emergent property
of an organization, detailing different levels of organization within a system and defining the safety

248
CHAPTER 4. SAFETY ENGINEERING
constraints that each level should impose on the one below it. Specifically, STPA builds models of:
the organizational safety structure; the dynamics and pressures that can lead to deterioration of this
structure; the models of the system that operators must have, and the necessary communication
to ensure these models remain accurate over time; and the broader social and political context the
organization exists within.
Finally, Dekker’s Drift into Failure (DIF) model emphasizes decrementalism: the way that a system’s
processes can deteriorate through a series of minor changes, potentially causing the system’s migration
to an unsafe state. This model warns that each change may seem insignificant alone, so organizations
might make these changes one at a time in isolation, creating a state of higher risk once enough
changes have been made.
As a final note on the implications of complexity for AI safety, we considered the broader societal
context within which AI technologies will function.
We discussed how, in this uncontrolled
environment, different, seemingly lower-level risks could interact to produce catastrophic threats,
while chaotic circumstances may increase the likelihood of AI-related accidents. For these reasons, it
makes sense to consider a wide range of different threats of different magnitudes in our approach
to mitigating catastrophic risks, and we may find that broader interventions are more fruitful than
narrowly targeted ones.
In the second half of this chapter, we focused in on a particular class of events called tail events
and black swans, and explored what they mean for risk analysis and management. We began this
discussion by contrasting long-tailed and thin-tailed distributions; long-tailed distributions are subject
to the possibility of rare, highly extreme events that can dominate the overall impact, while in
thin-tailed distributions, most events are around the same order of magnitude and no single event
can determine the outcome. We also looked at some general characteristics of different systems,
finding that highly connected systems can give rise to multiplicative phenomena, allowing events to
scale more extremely and become tail events.
Next, we described black swans as a subset of tail events that are not only rare and high-impact, but
also particularly difficult to predict. These events seem to happen largely “out of the blue” for most
people, and may indicate that our understanding of a situation is inaccurate or incomplete. These
events are also referred to as unknown unknowns, which we contrasted with known unknowns, which
we may not fully understand, but are at least aware of.
We examined how tail events and black swans can pose particular challenges for some traditional
approaches to evaluating and managing risk. Certain methods of risk estimation and cost-benefit
analysis rely on historical data and probabilities of different events. However, tail events and black
swans are rare, so we may not have sufficient data to accurately estimate their likelihood, and even a
small change in likelihood can lead to a big difference in expected outcome.
We also considered the delay fallacy, showing that waiting for more information before acting might
mean waiting until it is too late. We discussed how an absence of evidence of a risk cannot necessarily
be taken as evidence that the risk is absent. By looking at hypothetical situations where catastrophes
are avoided thanks to safety measures, we explained how the preparedness paradox can make these
measures seem unnecessary, when in fact they are essential.
Having explored the importance of taking tail events and black swans into consideration, we identified
some circumstances that indicate we may be at risk of these events. We concluded that it is reasonable
to believe AI technologies may pose such a risk, due to the complexity of AI systems and the systems
surrounding them, the highly connected nature of the social systems they are likely to be embedded

4.11. LITERATURE
249
in, and the fact that they are relatively new, meaning we may not yet fully understand all the ways
they might interact with their surroundings.
Finally, we summarized some of the actions that can be taken to reduce the risk of black swan events
transpiring. These include techniques for putting more black swans on our radar, or turning them
into known unknowns, such as conducting exercises to improve our safety imagination, engaging
in horizon scanning, and employing red teams tasked with finding ways to sabotage a system. We
reiterated the importance of incorporating safe design principles and improving general systemic
factors for improving safety.
We also described how following the precautionary principle can reduce our exposure to risks, and
how improving decision-making processes around the use of AI and changing researchers’ incentives
can improve prospects. In summary, while this may be a new and unfamiliar aspect of the world,
posing many novel challenges, there is much that can be done to increase the likelihood that this
technology is used in a safe way, for the benefit of humanity rather than to its detriment.
4.11
Literature
4.11.1
Recommended Reading
• Nancy Leveson. Engineering a Safer World: Systems Thinking Applied to Safety. Jan. 2012.
isbn: 9780262298247. doi: 10.7551/mitpress/8179.001.0001
• C. Perrow. Normal Accidents: Living with High Risk Technologies. Princeton paperbacks.
Princeton University Press, 1999
• N.N. Taleb. The Black Swan: The Impact of the Highly Improbable. Penguin Books, Limited,
2007. isbn: 9781846140457. url: https://books.google.com.au/books?id=_StMPgAACAAJ
• N.N. Taleb. Antifragile: Things That Gain from Disorder. Incerto. Random House Publishing
Group, 2012. isbn: 9780679645276. url: https://books.google.com.au/books?id=5fqbz_
qGi0AC
4.11.2
References
[1]
Dan Hendrycks and Mantas Mazeika. X-Risk Analysis for AI Research. 2022. arXiv: 2206.05862
[cs.CY].
[2]
Terence Tao. Nines of safety: a proposed unit of measurement of risk. Oct. 2021. url: https :
//terrytao.wordpress.com/2021/10/03/nines-of-safety-a-proposed-unit-of-measurement-
of-risk/.
[3]
N.N. Taleb. Antifragile: Things That Gain from Disorder. Incerto. Random House Publishing Group,
2012. isbn: 9780679645276. url: https://books.google.com.au/books?id=5fqbz_qGi0AC.
[4]
E. Marsden. Designing for safety: Inherent safety, designed in. 2017. url: https://risk-engineering.
org/safe-design/ (visited on 07/31/2017).
[5]
James Reason. “The contribution of latent human failures to the breakdown of complex systems.”
In: Philosophical transactions of the Royal Society of London. Series B, Biological sciences 327 1241
(1990), pp. 475–84. url: https://api.semanticscholar.org/CorpusID:1249973.
[6]
William Vesely et al. Fault Tree Handbook with Aerospace Fault Tree Handbook with Aerospace
Applications Applications. 2002. url: http://www.mwftr.com/CS2/Fault%20Tree%20Handbook_NASA.
pdf.

250
CHAPTER 4. SAFETY ENGINEERING
[7]
Andrew Critch and Stuart Russell. TASRA: a Taxonomy and Analysis of Societal-Scale Risks from
AI. 2023. arXiv: 2306.06924 [cs.AI].
[8]
Nancy G. Leveson. Introduction to STAMP: Part 1. MIT Partnership for Systems Approaches to
Safety and Security (PSASS). 2020.
[9]
Arden Albee et al. Report on the Loss of the Mars Polar Lander and Deep Space 2 Missions. Tech. rep.
Jet propulsion Laboratory - California Institute of Technology, 2000.
[10]
Donella H. Meadows. “Leverage Points: Places to Intervene in a System”. In: (1999).
[11]
C. Perrow. Normal Accidents: Living with High Risk Technologies. Princeton paperbacks. Princeton
University Press, 1999.
[12]
Nancy G. Leveson et al. “Moving Beyond Normal Accidents and High Reliability Organizations: A
Systems Approach to Safety in Complex Systems”. In: Organization Studies (2009).
[13]
Thomas G. Dietterich. “Steps Toward Robust Artificial Intelligence”. In: AI Magazine 38.3 (2017),
pp. 3–24. doi: https://doi.org/10.1609/aimag.v38i3.2756. eprint: https://onlinelibrary.
wiley.com/doi/pdf/10.1609/aimag.v38i3.2756. url: https://onlinelibrary.wiley.com/doi/
abs/10.1609/aimag.v38i3.2756.
[14]
Jens Rasmussen. “Risk management in a dynamic society: a modelling problem”. In: Safety Science
27.2 (1997), pp. 183–213. issn: 0925-7535. doi: https://doi.org/10.1016/S0925-7535(97)00052-0.
url: https://www.sciencedirect.com/science/article/pii/S0925753597000520.
[15]
Sidney Dekker. Drift into Failure: From Hunting Broken Components to Understanding Complex
Systems. Jan. 2011, pp. 1–220. isbn: 9781315257396. doi: 10.1201/9781315257396.
[16]
Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An Overview of Catastrophic AI Risks.
2023. arXiv: 2306.12001 [cs.CY].
[17]
Krystal Hu. ChatGPT sets record for fastest-growing user base - analyst note. 2023. url: https:
//www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-
note-2023-02-01/ (visited on 02/03/2023).
[18]
Scott D. Sagan. The Limits of Safety: Organizations, Accidents, and Nuclear Weapons. Vol. 177.
Princeton University Press, 1993. isbn: 9780691021010. url: http://www.jstor.org/stable/j.
ctvzsmf8r (visited on 09/26/2023).
[19]
Toby Ord. The Lindy Effect. 2023. arXiv: 2308.09045 [physics.soc-ph].
[20]
Nassim Nicholas Taleb. The Black Swan: The Impact of the Highly Improbable. Vol. 2. Random House,
2007.
[21]
E. Marsden. Black swans: The limits of probabilistic modelling. 2017. url: https://risk-engineering.
org/black-swans/ (visited on 07/31/2017).
[22]
Mark Boult et al. Horizon Scanning: A Practitioner’s Guide. English. IRM - Institute of Risk
Management, 2018.
[23]
Richard Danzig. Technology Roulette. 2018.
[24]
Nancy Leveson. Engineering a Safer World: Systems Thinking Applied to Safety. Jan. 2012. isbn:
9780262298247. doi: 10.7551/mitpress/8179.001.0001.
[25]
N.N. Taleb. The Black Swan: The Impact of the Highly Improbable. Penguin Books, Limited, 2007.
isbn: 9781846140457. url: https://books.google.com.au/books?id=_StMPgAACAAJ.

Complex Systems
5.1
Overview
Artificial intelligence systems and the societies they operate within belong to the class of complex
systems. These types of systems have significant implications for thinking about and ensuring AI
safety. Complex systems exhibit surprising behaviors and defy conventional analysis methods that
examine individual components in isolation. To develop effective strategies for AI safety, it is crucial
to adopt holistic approaches that account for the unique properties of complex systems and enable
us to anticipate and address AI risks.
This chapter begins by elucidating the qualitative differences between complex and simple systems.
After describing standard analysis techniques based on mechanistic or statistical approaches, the
chapter demonstrates their limitations in capturing the essential characteristics of complex systems,
and provides a concise definition of complexity. The “Hallmarks of Complex Systems” section then
explores seven indications of complexity and establishes how deep learning models exemplify each of
them.
Next, the “Social Systems as Complex Systems” section shows how various human organizations
also satisfy our definition of complex systems. In particular, the section explores how the hallmarks
of complexity materialize in two examples of social systems that are pertinent to AI safety: the
corporations and research institutes pursuing AI development, and the decision-making structures
responsible for implementing policies and regulations. In the latter case, there is consideration of
how advocacy efforts are affected by the complex nature of political systems and the broader social
context.
Having established that deep learning systems and the social systems surrounding them are best
described as complex systems, the chapter moves on to what this means for AI safety. The “General
Lessons” section derives five learnings from the chapter’s examination of complex systems and sets out
their implications for how risks might arise from AI. The “Puzzles, Problems, and Wicked Problems”
section then reframes the contrasts between simple and complex systems in terms of the different
kinds of problems that the two categories present, and the distinct styles of problem-solving they
require.
By examining the unintended side effects that often arise from interfering with complex systems,
the “Challenges with Interventionism” section illustrates the necessity of developing comprehensive
approaches to mitigating AI risks. Finally, the “Systemic Issues” section outlines a method for
thinking holistically and identifying more effective, system-level solutions that address broad systemic
issues, rather than merely applying short-term “quick fixes” that superficially address symptoms of
problems.
5.2
Introduction to Complex Systems
5.2.1
The Reductionist Paradigm
Before we describe complex systems, we will first look at non-complex systems and the methods of
analysis that can be used to understand them. This discussion sits under the reductionist paradigm.
According to this paradigm, systems are just the sum of their parts, and can be fully understood and
described with relatively simple mathematical equations or logical relations.
251

252
CHAPTER 5. COMPLEX SYSTEMS
The mechanistic approach analyzes a system by studying each component separately.
A common technique for understanding a system is to identify its components, study each one
separately, and then mentally “reassemble” it. Once we know what each part does, we can try to
place them all in a simple mechanism, where one acts on another in a traceable sequence of steps, like
cogs and wheels. This style of analysis is called the mechanistic approach, which often assumes that
a system is like a line of dominos or a Rube Goldberg machine; if we set one component in motion,
we can accurately predict the linear sequence of events it will trigger and, thus, the end result.
Figure 5.1: A Rube Goldberg machine with many parts that each feed directly into the next can be
well explained by way of mechanisms.
Many human artifacts can be understood mechanistically.
Devices like bicycles, clocks, and
sewing machines are designed with specific mechanisms in mind, where one component directly acts
on another in a cause-and-effect way to perform an intended function. For example, we can look at
a bicycle and understand that turning the pedals will pull on a chain, which will turn the wheels,
which will move the bicycle forward.
We can often derive mathematical equations that govern mechanistic systems.
If we
can successfully model a system’s behavior mechanistically, then we can usually find mathematical
equations that describe its behavior. We can use these equations to calculate how the system will
respond to different inputs. With this knowledge, we can control what the system does by controlling
the inputs. For example, if we know how quickly the pedals on a bicycle are rotating then we
can calculate the speed at which it is traveling. Conversely, we can control the bicycle’s speed by
controlling how quickly the pedals rotate.
Many conventional computer programs can also be understood mechanistically.
Simple
algorithmic computer programs involving for-loops and “if... else...” constructions can be understood
in this way too. Given any input, we can trace through the program’s operations to predict the
output. Similarly, for any given output, we can trace the steps backward and deduce information
about the input.
Functions in computer programs can also be understood mechanistically. We can create functions
within programs and give them names that are readable and intuitive to humans. For instance, we

5.2. INTRODUCTION TO COMPLEX SYSTEMS
253
can name a function “add(x, y)” and define it to return the sum of x and y. We can then write a
computer program using various functions like this, and we can analyze it by understanding how
each function works on its own and then looking at the sequence of functions the program follows.
This enables us to predict reliably what output the program will give for any input.
If there are a large number of components, we can sometimes use statistics.
Suppose we
are trying to predict the behavior of a gas in a box, which contains on the order of 1023 particles
(that is, 1 followed by 23 zeros). We clearly cannot follow each one and keep track of its effects on
the others, as if it were a giant mechanism.
However, in the case of a system like a gas in a box, the broader system properties of pressure and
temperature can be related to averages over the particle motions. This allows us to use statistical
descriptions to derive simple equations governing the gas’s coarse-grained behavior at the macroscopic
level. For example, we can derive an equation to calculate how much the gas pressure will increase
for a given rise in temperature.
The mechanistic and statistical approaches fall within the reductionist paradigm.
Both
mechanistic and statistical styles of analysis seek to understand and describe systems as combinations
or collections of well-understood components. Under the mechanistic approach, we account for
interactions by placing the components in a mechanism, assuming they only affect one another in a
neat series of direct one-to-one interactions. Under the statistical approach, we assume that we do
not need to know the precise details of how each interaction plays out because we can simply take an
average of them to calculate the overall outcome.
Summary.
Reductionist styles of analysis assume that a system is no more than the sum of its
parts. For a reductionist analysis to work, one of the following assumptions should often apply:
There either needs to be a simple, traceable mechanism governing the system’s behavior, or we need
to be able to relate the broader system properties to statistical averages over the components.
Limitations of the Reductionist Paradigm
Having discussed simple systems and how they can be understood through reductionism, we will now
look at the limitations of this paradigm and the types of systems that it cannot be usefully applied to.
We will look at the problems this presents for understanding systems and predicting their behaviors.
Many real-world systems defy reductionist explanation.
Imagine that, instead of looking at
a bicycle or a gas in a box, we are trying to understand and predict the behavior of an ecosystem,
weather patterns, or a human society. In these cases, there are clearly far too many components for
us to keep track of what each one is doing individually, meaning that we cannot apply the mechanistic
approach. Additionally, there are also many complex interdependencies between the components,
such that any given component might behave differently in the context of the system than it does in
isolation. We cannot, therefore, use statistics to treat the system’s behavior as a simple aggregate of
the components’ individual behaviors.
In complex systems, the whole is more than the sum of its parts.
The problem is that
reductionist-style analysis is poorly suited to capturing the diversity of interdependencies within
complex systems. Reductionism only works well if the interactions follow a rigid and predictable
mechanism or if they are random and independent enough to be modeled by statistics. In complex
systems, neither of these assumptions hold.

254
CHAPTER 5. COMPLEX SYSTEMS
In complex systems, interactions do not follow a rigid, structured pattern, but components are still
sufficiently interconnected that they cannot be treated as independent. These interactions are the
source of many novel behaviors that make complex systems interesting. To get a better grasp of
these systems, we need to go beyond reductionism and adopt an alternative, more holistic framework
for thinking about them.
We can sometimes predict general short-term trends in complex systems.
Note that we
may be able to predict high-level patterns of behavior in some complex systems, particularly if we
are familiar with them and have many observations of their past behavior. For example, we can
predict with a high degree of confidence that, in the northern hemisphere, a day in January next
year will be colder than a day in June.
However, it is much more difficult to predict specific details, such as the exact temperature or whether
it will rain on a given day. It is also much more challenging to predict the longer-term trajectory of
the system, such as what the climate will look like in several centuries or millennia. This is because
complex systems often develop in a more open-ended way than simple systems and have the potential
to evolve into a wider range of states, with numerous factors influencing the path they take.
New or unfamiliar complex systems are even more difficult to predict.
The challenges in
predicting how complex systems will behave are compounded when we face newly emerging ones,
such as those involving AI. While we have plenty of historical information and experience to help us
predict weather patterns, we have little past data to inform us on how AI systems and their use in
society will develop. Nevertheless, studying other complex systems and paying attention to their
shared properties can give us insights into how AI might evolve. This might offer clues as to how we
can avoid potential negative consequences of using AI.
Figure 5.2: Often, we use mechanistic or statistical approaches to analyzing systems. When there
are many components with strong interdependence, these are insufficient, and we need a complex
systems approach.
Summary.
Reductionist styles of analysis cannot give us a full understanding of complex systems,
whose components neither function like a deterministic mechanism nor behave randomly and

5.2. INTRODUCTION TO COMPLEX SYSTEMS
255
independently enough to use statistics—shown in Figure 5.4. This lack of a full understanding
presents challenges for predicting the system’s behavior on two levels: which component will perform
which action at what time, and how the whole system might change over the long term.
5.2.2
The Complex Systems Paradigm
Now that we have seen that many systems of interest are inscrutable to the reductionist paradigm,
we need an alternative lens through which to understand them. To this end, we will discuss the
complex systems paradigm, which takes a more holistic view, placing emphasis on the most salient
features shared across various real-world complex systems that the reductionist paradigm fails to
capture. The benefit of this paradigm is that it provides “a way of seeing and talking about reality
that helps us better understand and work with systems to influence the quality of our lives.”
Complex systems exhibit emergent properties that are not found in their components.
As discussed above, some systems cannot be usefully understood in a reductionist way. Studying a
complex system’s components in isolation and doing mental reassembly does not amount to what we
observe in reality. One primary reason for this is the phenomenon of emergence: the appearance of
striking, system-wide features that cannot be found in any of the system’s components.
The presence of emergent features provides one sense in which complex systems are “more than the
sum of their parts.” For example, we do not find atmospheric currents in any of the molecules of
nitrogen and oxygen that make up the atmosphere, and the flexible intelligence of a human being
does not exist in any single neuron. Many biological concepts such as adaptation, ecological niche,
sexuality, and fitness are not simply reduced to statements about molecules. Moreover, “wetness” is
not found in individual water molecules. Emergence is so essential that we will use it to construct a
working definition of complex systems.
Working definition.
Complex systems are systems of many interconnected components that
collectively exhibit emergent features, which cannot, in practice, be derived from a reductive analysis
of the system in terms of its isolated components.
Ant colonies are a classic example of a complex system.
An ant colony can grow to a size of
several million individuals. Each ant is a fairly simple creature with a short memory, moving around
in response to chemical and tactile cues. The individuals interact by randomly bumping into each
other and exchanging pheromones. Out of this mess of uncoordinated interactions emerge many
fascinating collective behaviors. These include identifying and selecting high-quality food sources
or nest sites, forming ant trails, and even constructing bridges over gaps in these trails (formed by
the stringing together of hundreds of the ants’ bodies). Ant colonies have also been observed to
“remember” the locations of food sources or the paths of previous trails for months, years, or decades,
even though the memory of any individual ant only lasts for a few days at most.
Ant colonies satisfy both aspects of the working definition of complex systems.
First,
the emergent features of the colony include the collective decision-making process that enables it to
choose a food source or nest site, the physical ability to cross over gaps many times wider than any
ant, and even capabilities of a cognitive nature such as extended memory. We could not predict all
of these behaviors and abilities from observing any individual ant, even if each ant displays some
smaller analogs of some of these abilities.
Second, these emergent features cannot be derived from a reductive analysis of the system focused on
the properties of the components. Even given a highly detailed study of the behavior of an individual

256
CHAPTER 5. COMPLEX SYSTEMS
ant considered in isolation, we could not derive the emergence of all of these remarkable features. Nor
are all of these features simple statistical aggregates of individual ant behaviors in any practical sense,
although some features like the distribution of ants between tasks such as foraging, nest maintenance,
and patrolling have been observed as decisions on the level of an individual ant as well.
This distinguishes a more complex system like an ant colony from a simpler one such as a gas in
a box. Although the gas also has emergent properties (like its temperature and pressure), it does
not qualify as complex. The gas’s higher-level properties can be straightforwardly reduced to the
statistics of the lower-level properties of the component particles. However, this was not always the
case: it took many decades of work to uncover the statistical mechanics of gases from the properties
of individual molecules. Complexity can be a feature of our understanding of the system rather than
the system itself.
Complex systems are ubiquitous in nature and society.
From cells, organisms, and
ecosystems, to weather systems, cities, and the World Wide Web, complex systems are everywhere.
We will now describe two further examples, referred to throughout this chapter.
Economies are complex systems.
The components of an economic system are the individual
persons, companies, and firms participating in the economy. These economic agents interact via
various kinds of financial transactions, such as lending, borrowing, investing, and purchasing and
selling goods. Out of these interactions emerge complex economic phenomena such as inflation,
stock-market indexes, and interest rates. These economic phenomena are not manifested by any
individual agent and cannot be derived by studying the behavior of these agents considered separately;
rather, they arise from the complex network of interactions between them.
The human brain is a complex system.
The human brain consists of around 86 billion neurons,
each one having, on average, thousands of connections to the others. They interact via chemical and
electrical signals. Out of this emerge all our impressive cognitive abilities, including our ability to
use language, perceive the world around us, and control the movements of our body. Again, these
cognitive abilities are not found in any individual neuron, arising primarily from the rich structure of
neuronal connections; even if we understood individual neurons very well, this would not amount to
an understanding of (or enable a derivation of) all these impressive feats accomplished by the brain.
Interactions matter for complex systems.
As these examples illustrate, the interesting emergent
features of complex systems are a product of the interactions (or interconnections) between their
components. This is the core reason why these systems are not amenable to a reductive analysis,
which tries to gain insight by breaking the system into its parts. As the philosopher Paul Cilliers
writes: “In ‘cutting up’ a system, the analytic method destroys what it seeks to understand” [1].
Summary.
Complex systems are characterized by emergent features that arise from the complex
interactions between components, but do not exist in any of the individual components, and cannot
be understood through or derived from a reductive analysis of them. Complex systems are ubiquitous,
from ant colonies to economies to the human brain.
5.2.3
Deep Learning Systems as Complex Systems
An essential claim of this chapter is that deep learning models are complex systems.
Here, we will briefly discuss what a reductionist approach to understanding deep learning systems
would look like and why it is inadequate.

5.2. INTRODUCTION TO COMPLEX SYSTEMS
257
Consider a deep learning system that correctly classifies an image of a cat. How does it do this? The
reductionist approach to this question would first try to break down the classification into a sequence
of smaller steps and then find parts of the neural network responsible for executing each of them. For
instance, we might decompose the problem into the identification of cat ears + whiskers + paws and
then look for individual neurons (or small clusters of neurons) responsible for each of these elements.
The reductionist approach cannot fully describe neural networks.
In some cases, it seems
possible to find parts of a neural network responsible for different elements of such a task. Researchers
have discovered that progressively later layers of deep neural networks are generally involved in
recognizing progressively higher-level features of the images they have been trained to classify. For
example, close to the input layer, the neural network might be doing simple edge detection; a little
further into the hidden layers, it might be identifying different shapes; and close to the output, it
might be combining these shapes into composites.
However, there is no clear association between an individual node in a given layer and a particular
feature at the corresponding level of complexity. Instead, all the nodes in a given layer are partially
involved in detecting any given feature at that level. That is to say, we cannot neatly attribute the
detection of each feature to a specific node, and treat the output as the sum of all the nodes detecting
their specific features. Although there have been instances of researchers identifying components of
neural networks that are responsible for certain tasks, there have been few successes, and they have
required huge efforts to achieve. In general, this approach has not so far worked well for explaining
higher-level behaviors.
The complex systems paradigm is more helpful for deep learning systems.
As these
problems suggest, we cannot generally expect to find a simple, human-interpretable set of features that
a neural network identifies in each example and “adds together” to reach its predictions. Deep learning
systems are too complex to reduce to the behavior of a few well-understood parts; consequently, the
reductionist paradigm is of limited use in helping us think about them. As we will discuss later in
this chapter, the complex systems paradigm cannot entirely make up for this or enable a complete
understanding of these systems. Nonetheless, it does give us a vocabulary for thinking about them
that captures more of their complexity and can teach us some general lessons about interacting with
them and avoiding hazards.
Summary.
The difficulties involved in explaining neural networks’ activity through simple
mechanisms are one piece of evidence that they are best understood as complex systems. We
will substantiate this claim throughout the next section, where we run through some of the hallmark
features of complex systems and discuss how they apply to deep learning models.
5.2.4
Complexity is Not a Dichotomy
In the previous section, we proposed a working definition of complex systems that suffices for an
informal discussion, though it is not completely precise. In fact, there is no standard definition
of complexity used by all complex-systems scientists. In part, this is because complexity is not a
dichotomy.
Understanding system complexity.
While we have described a distinction between a “simple”
and “complex” system, labeling a system as inherently simple or complex can be misleading.
Complexity is not always intrinsic to a system. Instead, it depends on our understanding. Certain
phenomena in physics, for instance, have transitioned from being poorly understood “complex”
concepts to well-explained “simple” mechanics through advanced analysis of the properties of the

258
CHAPTER 5. COMPLEX SYSTEMS
system. Superconductivity—the property of a material to conduct electricity without resistance when
cooled below a certain critical temperature—is an example of this transition in understanding.
Superconductivity was originally perceived as a complex phenomenon due to the emergent behavior
arising from electron interactions in metals. However, with the discovery of the Bardeen-Cooper-
Schrieffer (BCS) theory, it became clear that superconductivity could be explained through the
pairing of electrons. By considering these pairs as the components of interest rather than individual
electrons, superconductivity was reclassified as a conceptually “simple” system that can be described
by reductionist models.
Complexity,
information,
and
reductionism.
Current research in complex systems
acknowledges the importance of interactions in determining emergent behavior but doesn’t abandon
the search for mechanistic explanations. Often, mechanistic explanations of systems can be found
when considering a larger basic basic building block, such as pairs of electrons for superconductivity.
This choice of scale is important for creating effective models of possibly complex phenomena.
Thus, rather than a binary classification, systems might be better understood as existing on a
spectrum based on the scale and amount of information required to predict their behavior accurately.
Complex systems are those that, at a certain scale, require a vast amount of information for prediction,
indicating their relative incompressibility. However, they could still be explained mechanistically, if
we understood them sufficiently well.
5.2.5
The Hallmarks of Complex Systems
Since complexity is not a dichotomy, it is difficult to pin down when exactly we can consider systems
complex. In place of a precisely demarcated domain, complex-systems scientists study numerous
salient features that are generally shared by the systems of interest. While disciplines like physics
seek fundamental mechanisms that can explain observations, the study of complex systems looks for
salient higher-level patterns that appear across a wide variety of systems.
We consider seven key characteristics of complex systems. Chief among these is emergence, but
several others also receive attention: self-organization, feedback and nonlinearity, criticality, adaptive
behavior, distributed functionality, and scalable structure. We will now describe each of these
hallmarks and explain their implications. Along the way, we will show that deep learning systems
share many similarities with other complex systems, strengthening the case for treating them under
this paradigm.
Emergence
We have already discussed emergence, the appearance of striking system-wide features that cannot be
found in any of the components of the system. Ant colonies swarm over prey and build bridges over
gaps in their trail; economies set prices and can crash; human brains think, feel, and sense. These
remarkable behaviors are inconceivable for any individual component—ant, dollar, or neuron—existing
in isolation.
Emergent features often spontaneously “turn on” as we scale up the system in size.
A
group of 100 army ants placed on the ground behaves not like an enfeebled colony but rather like no
colony at all; the ants just walk around in circles until they starve or die of exhaustion. If the system
is scaled up to tens of thousands of ants, however, a qualitative shift in behavior occurs as the colony
starts behaving like an intelligent superorganism.

5.2. INTRODUCTION TO COMPLEX SYSTEMS
259
Emergent abilities have been observed in deep learning systems.
Large language models
(LLMs) are trained to predict the next token in a string of words. Smaller LLMs display a variable
ability to output coherent sentences, as might be expected based on this training. Larger LLMs,
however, spontaneously gain qualitatively new capabilities, such as translating text or performing
three-digit arithmetic. These abilities can emerge without any task-specific training.
Summary.
Emergent properties arise collectively from interactions between components, and are a
defining feature of complex systems. These features often appear spontaneously as a system is scaled
up. Emergent capabilities have already been observed in deep learning systems.
Feedback and Nonlinearity
Two closely related hallmarks of complexity are feedback and nonlinearity. Feedback refers to circular
processes in which a system and its environment affect one another. There are multiple types of
nonlinearity, but the term generally describes systems and processes where a change in the input
does not necessarily translate to a proportional change in the output. We will now discuss some
mechanisms behind nonlinearity, including feedback loops, some examples of this phenomenon, and
why it makes complex systems’ behavior less predictable.
In mathematics, a linear function is one whose outputs change in proportion to changes
in the inputs.
The functions f(x) = 3x and f(x) = 100(x −10) linear. Meanwhile, the functions
f(x) = x2 and f(x) = ex are nonlinear.
Complex systems are nonlinear functions of their inputs.
Complex systems process inputs
in a nonlinear way. For example, when ant colonies are confronted with two food sources of differing
quality, they will often determine which source is of higher quality and then send a disproportionately
large fraction of its foragers over to exploit it rather than form two trails in proportion to the
quality of the food source. Neural networks are also nonlinear functions of their inputs. This is why
adversarial attacks can work well: adding a small amount of noise to an image of a cat need not
merely reduce the classifier’s confidence in its prediction, but might instead cause the network to
confidently misclassify the image entirely.
Nonlinearity makes neural networks hard to decompose.
A deep neural network with 10
layers cannot be replaced by five neural networks, each with only two layers. This is due to the
nonlinear activation functions (such as GELUs) between their nodes. If the layers in a neural network
simply performed a sequence of linear operations, the whole network could be reduced to a single
linear operation. However, nonlinear operations cannot be reduced in the same way, so nonlinear
activation functions mean that deep neural networks cannot be collapsed to networks with only a
few layers. This property makes neural networks more capable, but also more difficult to analyze
and understand.
Feedback loops in complex systems
A major source of nonlinearity is the presence of feedback.
Feedback occurs when the
interdependencies between different parts of a system form loops (e.g., A depends on B, which in
turn depends on A). These feedback loops can reinforce certain processes in the system (positive
feedback), and quash others (negative feedback), leading to a nonlinear relationship between the
system’s current state and how it changes. The following are examples of feedback loops in complex
systems.

260
CHAPTER 5. COMPLEX SYSTEMS
The rich get richer.
Wealthy people have more money to invest, which brings them a greater
return on investment. In a single investment cycle, the return on investment is greater in proportion
to their greater wealth: a linear relationship. However, this greater return can then be reinvested.
Doing so forms a positive feedback loop through which a slight initial advantage in wealth can be
transformed into a much larger one, leading to a nonlinear relationship between a person’s wealth
and their ability to make more money.
Learning in the brain involves a positive feedback loop.
Connections between neurons
are strengthened according to Hebb’s law (“neurons that fire together, wire together”). Stronger
connections increase the probability of subsequent episodes of “firing together”, further strengthening
those connections. As a result of this feedback process, our memories do not strengthen or weaken
linearly with time. The most efficient way to learn something is by revisiting it after increasing
intervals of intervening time, a method called “spaced repetition.”
Task distribution in beehives can be regulated by feedback loops.
When a forager bee
finds a source of water, it performs a “waggle dance” in front of the hive to signal to the other bees
the direction and distance of the source. However, a returning forager needs to find a receiver bee
onto which to unload the water. If too many foragers have brought back water, it will take longer to
find a receiver, and the forager is less likely to signal to the others where they should fly to find the
source. This negative feedback process stabilizes the number of bees going out for water, leading to a
nonlinear relationship between the number of bees currently flying out for water and the number of
additional bees recruited to the task.
AI systems involve feedback loops.
In a system where agents can affect the environment, but
the environment can also affect agents, the result is a continual, circular process of change—a feedback
loop. Another example of feedback loops involving AIs is the reinforcement-learning technique of
self-play, where agents play against themselves: the better an agent’s performance, the more it has
to improve to compete with itself, leading its performance to increase even more.
Feedback processes can make complex systems’ behavior difficult to predict.
Positive
feedback loops can amplify small changes in a system’s initial conditions into considerable changes in
its resulting behavior. This means that nonlinear systems often have regimes in which they display
extreme sensitivity to initial conditions, a phenomenon called chaos (colloquially referred to as the
butterfly effect). A famous example of this is the logistic map, an equation that models how the
population of a species changes over time:
xn+1 = rxn(1 −xn).
This equation is formulated to capture the feedback loops that affect how the population of a species
changes: when the population is low, food sources proliferate, enabling the population to grow;
when it is high, overcrowding and food scarcity drive the population down again. xn is the current
population of a species as a fraction of the maximum possible population that its environment can
support. xn+1 represents the fractional population at some time later. The term r is the rate at
which the population increases if it is not bounded by limited resources.
When the parameter r takes a value above a certain threshold (∼3.57), we enter the chaotic regime
of this model, in which a tiny difference in the initial population makes for a large difference in the
long-run trajectory. Since we can never know a system’s initial conditions with perfect accuracy,
chaotic systems are generally considered difficult to predict.

5.2. INTRODUCTION TO COMPLEX SYSTEMS
261
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
5
10
15
20
25
30
35
40
45
50
Time (generations)
Population Size (unit)
Predictable Logistic Map (r = 3.2)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
5
10
15
20
25
30
35
40
45
50
Time (generations)
Population Size (unit)
Chaotic Logistic Map (r = 3.8)
Figure 5.3: When systems are predictable, small changes in initial conditions can taper out. When
they are chaotic, small changes in initial conditions lead to wildly different outcomes.
AIs as a self-reinforcing feedback loop.
Since AIs can process information and reach decisions
more quickly than humans, putting them in charge of certain decisions and operations could accelerate
developments to a pace that humans cannot keep up with. Even more AIs may then be required to
make related decisions and run adjacent operations. Additionally, if society encounters any problems
with AI-run operations, it may be that AIs alone can work at the speed and level of complexity
required to address these problems. In this way, automating processes could set up a positive feedback
loop, requiring us to continually deploy ever-more AIs. In this scenario, the long-term use of AIs
could be hard to control or reverse.
Summary.
There are multiple ways in which complex systems exhibit nonlinearity. A small change
in the system’s input will not necessarily result in a proportional change in its behavior; it might
completely change the system’s behavior, or have no effect at all. Positive feedback loops can amplify
changes, while negative feedback loops can quash them, leading a system to evolve nonlinearly
depending on its current state, and making its long-run trajectory difficult to predict.

262
CHAPTER 5. COMPLEX SYSTEMS
Self-Organization
The next salient feature of complex systems we will discuss is self-organization. This refers to how
the components direct themselves in a way that produces collective emergent properties without any
explicit instructions.
Complex systems sometimes organize themselves spontaneously.
The forms and internal
structure changes of complex systems are neither imposed by a top-down design nor centrally
coordinated by “master components.” The high-level order and organization of a complex system is
itself an emergent property that cannot be analyzed in terms of individual components. We will now
look at some examples of self-organization.
Workers self-organize in ant colonies.
In ant colonies, worker ants perform a variety of tasks,
such as nest maintenance, brood care, foraging for food, and patrolling around the nest for signs of
danger. Task allocation is partly determined by demand and opportunity in the environment. For
example, the colony will shift to a more forager-heavy distribution if it discovers a large food source.
The way in which individual ants are recruited to different tasks according to environmental demand
and opportunity is self-organizing: a product of local stochastic interactions between the individuals,
not set by a central controller (there’s no ant commander).
The efficient market hypothesis states that economies self-organize to set prices.
Increasing the price of a product leads to an increase in its supply (as profit margins for vendors
are higher) and a decrease in its demand (as fewer consumers can afford it). Decreasing the price of
a product has the reverse effect. In theory, the market price of a product will stabilize around the
value at which the supply matches the demand. The system of vendors and consumers automatically
“finds” the equilibrium market price without any centralized control or external help.
A neural network largely self-organizes during training.
One could argue that there is an
element of top-down control in the training of a neural network, in the way the backpropagation
adjusts parameters to reduce the loss. However, there is not a predetermined plan specifying which
parts of it are supposed to perform the different functions needed to carry out the task. Instead,
the training process starts with a disordered system and its ultimate shape is determined by many
interactions between components, resulting in a highly decentralized organization throughout the
network. To a large extent, therefore, the training process resembles self-organization.
Summary.
In a complex system, each component responds to conditions and directs its own actions
such that the components collectively exhibit emergent behaviors without any external or central
control. Neural networks arrange themselves in this way during training.
Self-Organized Criticality
Through self-organization, complex systems can reliably reach configurations that might seem
improbable or fine-tuned. We will now look at the phenomenon of self-organized criticality, which is
an important example of this.
Criticality is when a system is balanced at a tipping point between two different states.
In nuclear engineering, the “critical mass” is the mass of a fissile material needed for a self-sustaining
nuclear chain reaction. Below the critical mass, the chain reaction quickly dies out; above the critical
mass, it continues at an ever-increasing rate and blows up. The critical mass is a boundary between
these two regimes—the point at which the system “tips over” from being subcritical (stable and
orderly) to supercritical (unstable and disorderly). It is therefore referred to as the tipping point, or

5.2. INTRODUCTION TO COMPLEX SYSTEMS
263
critical point, of the fissile system. Under normal operations, nuclear reactors are maintained at a
critical state where the ongoing reaction ensures continual energy generation without growing into a
dangerous, uncontrolled reaction.
Systems at their critical point are optimally sensitive to fluctuating conditions.
In the
nuclear case, an internal fluctuation would be the spontaneous fission of a nucleus. Below the critical
point, the consequences of this event invariably remain confined to the neighborhood of the nucleus;
above the critical point, the knock-on effects run out of control. Precisely at criticality, a local fission
event can precipitate a chain reaction of any size, ranging from a short burst to a cascading reaction
involving the entire system. This demonstrates how, at a critical point, a small event can have the
broadest possible range of effects on the system.
The concept of criticality applies far beyond nuclear engineering: one classic example is the sandpile
model. A sandpile has a critical slope, which is the tipping point between a tall, unstable pile and
a shallow, stable pile. Shallower than this slope, the pile is relatively insensitive to perturbations:
dropping additional grains onto the pile has little effect beyond making it taller. Once we reach
the critical slope, however, the pile is poised to avalanche, and dropping extra grains can lead to
avalanches of any size, including system-wide ones that effectively cause the whole pile to collapse.
Again, we see that, at criticality, single events can have a wide range of effects on the system.
The freezing point of water is a critical temperature between its solid and liquid phases.
In ice, the solid phase of water, there is long-range order, and fluctuations away from this (pockets
of melting ice) are small and locally contained. In the liquid phase, there is long-range disorder,
and fluctuations away from this (formation of ice crystals) are likewise small and locally contained.
But at the freezing point of water—the critical point between the solid and liquid phases—the local
formation of an ice crystal can rapidly spread across the whole system. As a result, a critically cooled
bottle of beer can suddenly freeze all at once when it is perturbed, for example by being knocked
against a table.
Neural networks display critical points.
Several studies have found that certain capabilities of
neural networks suddenly ‘switch on’ at a critical point as they are scaled up. For example, grokking
is a network’s ability to work accurately for general, random datasets, not just the datasets used in
training. One study trained neural networks to recognize patterns in tables of letters and fill in the
blanks, and found that grokking switched on quite suddenly [2]. The study reported that this ability
remained near zero up to 105 optimization steps, but then steeply increased to near 100% accuracy
by 106 steps. This could be viewed as a critical point.
Self-organized criticality means systems can evolve in a “punctuated equilibrium”.
According to the theory of punctuated equilibrium, evolutionary history consists of long periods of
relative stasis in which species experience very little change, punctuated by occasional bursts of
rapid change across entire ecosystems. These sudden bursts can be understood through the lens
of self-organized criticality. Ecosystems typically in equilibrium can slowly tend towards critical
points, where they are optimally sensitive to perturbations from outside (such as geological events) or
fluctuations from within (such as an organism developing a new behavior or strategy through a chance
mutation). When the ecosystem is near a critical point, such a perturbation can potentially set off a
system-wide cascade of changes, in which many species will need to adapt to survive. Similarly, AI
development sometimes advances in bursts (e.g., GANs, self-supervised learning in vision, and so on)
with long periods of slow development.

264
CHAPTER 5. COMPLEX SYSTEMS
Summary.
Complex systems often maintain themselves near critical points, or “tipping points”. At
these points, a system is optimally sensitive to internal fluctuations and external inputs. This means
it can undergo dramatic changes in response to relatively minor events. A pattern of dramatic changes
that sporadically interrupt periods of little change can be described as a punctuated equilibrium.
Distributed Functionality
As discussed earlier in this chapter, it is usually impractical to attempt to decompose a complex
system into its parts, assign a different function to each one, and then assume that the system as a
whole is the sum of these functions. Part of the reason for this is distributed functionality, another
hallmark of complexity which we will now explore.
Complex systems can often be described as performing tasks or functions.
Insect colonies
build nests, forage for food, and protect their queens; economies calculate market prices and interest
rates; and the human brain regulates all the bodily processes essential for our survival, such as
heartbeat and breathing. In this context, we can understand adaptive behavior as the ability of a
complex system to maintain its functionality when placed in a new environment or faced with new
demands.
In complex systems, different functions are not neatly divided up between subsystems.
Consider a machine designed to make coffee. In human artifacts like this, there is a clear delegation
of functions to different parts of the system—one part grinds the beans, another froths the milk, and
so forth. This is how non-complex systems usually work to perform their tasks. In complex systems,
by contrast, no subsystem can perform any of the system’s functions on its own, whereas all the
subsystems working together can collectively perform many different tasks. This property is called
“distributed functionality.”
Note that distributed functionality does not imply that there is absolutely no functional specialization
of the system’s components. Indeed, the components of a complex system usually come in a diversity
of different types, which contribute in different ways to the system’s overall behavior and function.
For example, the worker ants in an ant colony can belong to different groups: foragers, patrollers,
brood care ants, and so on. Each of these groups, however, performs various functions for the
colony, and distributed functionality implies that, within each group of specialists, there is no rigid
assignment of functions to components.
Partial encoding means that no single component can complete a task alone.
The group
of forager ants must perform a variety of subtasks in service of the foraging process: locating a food
source, making a collective decision to exploit it, swarming over it to break it up, and carrying small
pieces of it back to the nest. A single forager ant working alone cannot perform this whole process—or
even any one subtask; many ants are needed for each part, with each individual contributing only
partially to each task. We therefore say that foraging is partially encoded within any single forager
ant.
Redundant encoding means there are more components than needed for any task.
A
flourishing ant colony will have many more ants than are necessary to carry out its primary functions.
This is why the colony long outlives its members; if a few patroller ants get eaten, or a few foragers get
lost, the colony as a whole barely notices. We therefore say that each of the functions is redundantly
encoded across the component ants.
An example of distributed functionality is the phenomenon known as the “wisdom of crowds”, which
was notably demonstrated in a report from a village fair in 1906. At this fair, attendees were invited

5.2. INTRODUCTION TO COMPLEX SYSTEMS
265
to take part in a contest by guessing the weight of an ox. 787 people submitted estimates, and it was
reported that the mean came to 1,197 pounds. This was strikingly close to the actual weight, which
was 1,198 pounds.
In situations like this, it is often the case that the average estimate of many people is closer to the
true value than any individual’s guess. We could say that the task of making a good estimate is only
partially encoded in any given individual, who cannot alone get close to the actual value. It is also
redundantly encoded because any individual’s estimate can usually be ignored without noticeably
affecting the average.
On a larger scale, the wisdom of crowds might be thought to underlie the effectiveness of democracy.
Ideally, a well-functioning democracy should make better decisions than any of its individual members
could on their own. This is not because a democratic society decomposes its problems into many
distinct sub-problems, which can then be delegated to different citizens. Instead, wise democratic
decisions take advantage of the wisdom of crowds phenomenon, wherein pooling or averaging many
people’s views leads to a better result than trusting any individual. The “sense-making” function of
democracies is therefore distributed across society, partially and redundantly encoded in each citizen.
Neural networks show distributed functionality.
In neural networks, distributed functionality
manifests most clearly as distributed representation.
In sufficiently large neural networks, the
individual nodes do not correspond to particular concepts, and the weights do not correspond to
relationships between concepts. In essence, the nodes and connections do not “stand for” anything
specific. Part of the reason for this is partial encoding: in many cases, any given feature of the input
data will activate many neurons in the network, making it impossible to locate a single neuron that
represents this feature. In addition, so-called polysemantic neurons are activated by many different
features of the input data, making it hard to establish a correspondence between these neurons and
any individual concepts.
Distributed functionality makes it hard to understand what complex systems are doing.
Distributed functionality means that we cannot understand a complex system by attributing each
task wholly and exclusively to a particular component, as the mechanistic approach would seek to.
Distributed representation in neural networks is a particularly troubling instantiation of this insofar
as it poses problems for using human concepts in analyzing a complex system’s “cognition”. The
presence of distributed representation might be thought to substantiate the concern that neural
networks are uninterpretable “black boxes”.
Summary.
Distributed functionality often means that no function in a complex system can be
fully or exclusively attributed to a particular component. Since tasks are more loosely shared among
components, this is one of the main reasons that it is so difficult to develop a definitive model of how
a complex system works.
Scalable Structure and Power Laws
As discussed above, the properties of a complex system often scale nonlinearly with its size. Instead,
they often follow power laws, where a property is proportional to the system size raised to some
power that may be more or less than 1. We will now discuss these power laws, which are another
hallmark of complex systems.

266
CHAPTER 5. COMPLEX SYSTEMS
1
10
102
103
104
105
10−2
10−1
1
101
102
103
Body Mass (kg, log scale)
Metabolic Rate (kJ/day, log scale)
Log−Log Plot of Body Mass and Metabolic Rate
Figure 5.4: Data on mammals and birds demonstrate Kleiber’s Law, with a power law relationship
appearing as a straight line on a log-log graph.
Complex systems often obey power-law scalings of their properties with system size.
Perhaps the most famous example of a power-law scaling is Kleiber’s law in biology: across all
mammals and birds, and possibly beyond, the metabolic rate of a typical member of a species scales
with the three-quarters power of its body mass.
R ∝M
3
4
If we know that an elephant is five times heavier than a horse, we can guess that the elephant’s
metabolic rate will be approximately 3.3 times the horse’s (since 5
3
4 ≈3.3). There are several other
documented cases of this power-law scaling behavior in complex systems. The average heart-rate for
a typical member of a mammalian species scales with the minus one-quarter power of its body mass:
R ∝M −1
4 .
At the same time, the average lifespan scales with the one-quarter power of its body mass:
T ∝M
1
4 .
This leads to the wonderful result that the average number of heartbeats per lifetime is constant
across all species of mammals (around 1.5 billion).
Among cities within the same country, the material infrastructure (such as the lengths of pipes,
powerlines, and roads, and the number of gas stations) scales with population as a power-law with
an exponent of 0.85. Also among cities within the same country, socioeconomic quantities (such as
incidents of crime and cases of flu) scale with the population size raised to the 1.15 power.

5.2. INTRODUCTION TO COMPLEX SYSTEMS
267
Experiments on LLMs show that their loss obeys power laws too.
In the paper in which
DeepMind introduced the Chinchilla model ([3]), the researchers fit the following parametric function
to the data they collected from experiments on language models of different sizes, where N is the
size of the model and D is the size of the training dataset:
L(N, D) = E + A
N α + B
Dβ .
The irreducible loss (E) is the lowest loss that could possibly be achieved. Subtracting this off, we
see that the performance of the model as measured by the loss (L) exhibits a power-law dependency
on each of model parameter count (N) and dataset size (D).
For more details on scaling laws in deep learning systems, see the Scaling Laws section in Artificial
Intelligence & Machine Learning.
Summary.
Certain important properties of complex systems often scale nonlinearly with the size
of the system. This means that two separate systems will not behave in the same way as one single
system of equivalent size.
Adaptive Behavior
The final hallmark of complexity we will discuss is adaptive behavior, which involves a system changing
its behavior depending on the demands of the environment.
Complex systems often adapt flexibly to new tasks and environmental changes.
Honeybees usually need to maintain their hives within an optimum temperature range of 32-36°C.
When temperatures rise too high, bees engage in various adaptive behaviors to counteract this. They
fan their wings at the hive entrance, increasing air circulation to cool down the hive. Additionally,
more bees are sent out to gather water, which helps regulate the hive’s temperature back to normal
[4]. This ability to adjust their behavior to maintain homeostasis during environmental changes
exemplifies one type of adaptive behavior.
The human brain, on the other hand, showcases a different form of adaptability. It possesses the
remarkable capacity to navigate novel circumstances and solve unfamiliar problems. When faced
with new challenges, the brain’s ability to think about different things allows us to adapt and thrive
in diverse environments. For example, London’s taxi drivers (“cabbies”) have been found to have
larger-than-average memory centers. This adaptation enables them to navigate the complex maze
of London’s streets effectively. Furthermore, the brain can also adapt in response to injury. After
a stroke or head injury, it can rewire itself, repurposing undamaged areas to compensate for the
damaged ones. This adaptive behavior showcases the brain’s remarkable plasticity and its ability to
adapt and function even after experiencing trauma.
Some deep learning systems exhibit adaptive behavior.
So-called “online models” learn
from new data sequentially as they encounter it, rather than remaining fixed after an initial training
phase. This enables these models to dynamically adapt to datasets that change over time, as well as
continuing to perform well in the real world when the inputs they encounter differ from their training
data, an ability known as “test-time adaptation” or simply “adaptation”. While other deep learning
systems such as Large Language Models remain fixed after their training phase, there are strong
incentives to make these systems adaptive to overcome current limitations such as costs of re-training
and lack of up-to-date information after the training date.

268
CHAPTER 5. COMPLEX SYSTEMS
Another example of adaptive behavior in deep learning systems is few-shot prompting. This technique
enables general deep learning models (such as large language models) to be used to perform certain
tasks without any task-specific fine-tuning. It involves giving the model a few examples (“shots”) of
correct performance on the task, which stimulate the model to adapt its outputs to these examples
and thereby carry out the desired task.
Summary.
Complex systems can often undergo rapid changes in their structures and processes in
response to internal and external fluctuations. This adaptive behavior enables the continuation of
the system in a changing environment.
Review of the Hallmarks of Complexity
There are seven hallmarks of complexity that we can look out for when identifying complex systems.
These hallmarks are:
1. Emergence: the appearance of novel properties that arise from interactions between the system’s
components, but which do not exist in any single component. These properties cannot be
understood or predicted from reductive analysis of components.
2. Feedback and nonlinearity: the presence of feedback loops that can either amplify or quash
changes in a complex system, and the multiple ways in which a change in the input to a complex
system can produce a disproportionate change in the output.
3. Self-organization: the ability of a complex system to spontaneously self-organize through the
self-directed behaviors of the components, without any external or centralized control.
4. Self-organized criticality: the tendency of complex systems to maintain themselves near critical
points, at which they can undergo dramatic changes in response to even relatively minor
perturbations.
5. Distributed functionality: the way in which tasks are shared loosely among a complex system’s
components. Tasks are both partially encoded—each individual contributes only partially to a
task—and redundantly encoded—there are more individuals that can contribute to a task than
are strictly necessary to complete it.
6. Scalable structure: the way in which properties of complex systems scale nonlinearly with
size, so that a property of a single large system may be larger or smaller than the combined
properties of two separate systems of half the size.
7. Adaptive behavior: a complex system’s ability to change its structure and processes in response
to perturbations, enabling it to continue functioning in a changing environment.
5.2.6
Social Systems as Complex Systems
So far, we have described how deep learning systems possess many of the classic features of complex
systems. We have shown that they satisfy the two aspects of our working definition of complex
systems and that they display all seven hallmarks discussed above.
We will now consider the organizations that develop AIs and the societies within which they are
deployed, and describe how these systems also exhibit the characteristics of complex systems. We
will argue that, on this basis, the problem of AI safety should be treated under the complex systems
paradigm.

5.2. INTRODUCTION TO COMPLEX SYSTEMS
269
Worked Example: Corporations and Research Institutes as Complex Systems
The organizations developing AI technology are complex systems.
Corporations and
research institutes have multiple emergent properties that are not found in any of the individuals
working within them. Brand identity, for example, does not exist in any employee of a company, but
rather embodies and conveys the collective activities of all the employees and conveys the goals of
the company as a whole. Similarly, the concepts of organizational culture and research culture refer
to the general ways in which individuals tend to interact with one another within an organization or
a research field.
Organizations developing AI are self-organizing.
Although companies have CEOs, these
CEOs are often selected by groups of people, such as board members, and do not generally dictate
every single activity within the company and how it should be done. People self-select in applying to
work at a company that interests them, and managers usually make decisions together on who to
hire. Employees often come up with their own ideas for projects and strategies, and then decisions
are made collectively on which ones to pursue. Likewise in academia, researchers investigate their
own questions, keep up to date with the findings of their peers, and use those insights to inform their
research directions, while experts form committees to decide which projects should be funded. There
is very often no single central entity determining which researchers should work on which questions.
Both corporate and academic organizations can exhibit critical points.
Often, a lot
of an organization’s effort is focused on a particularly consequential area or problem until a big
breakthrough is made, representing a tipping point into a new paradigm. For this reason, research
and development often progresses in the pattern of a punctuated equilibrium, with long periods
of incremental advancements interrupted by windows of rapid advancements, following important
breakthroughs.
Companies and research institutes show multiple forms of adaptive behavior.
Examples
of adaptation include organizations incorporating new information and technology to update their
strategies and ways of working, and adjusting their research directions based on new findings.
Additionally, they may adapt to the changing needs of customers and the changing priorities of
research funders, as well as to new government regulations.
Companies and research institutes display distributed functionality.
While there may be
subsystems that focus on specialized tasks within a company or branches within a research field, in
general, no employee or researcher single-handedly performs a whole function or advances an area
alone. Even if there is just one person working in a particular niche, they still need to be informed
by related tasks and research performed by others, and usually rely on the work of support staff.
This illustrates partial encoding. There are also usually more people available to perform tasks than
are absolutely needed, meaning that processes continue over time despite employees and researchers
joining and leaving. This demonstrates redundant encoding.
There are multiple examples of feedback and nonlinearity in companies and institutes.
A small disparity in investment into different projects or research directions may be compounded
over time, with those that receive more initial funding also achieving bigger results, and therefore
receiving even more funding. A small difference in support for a particular candidate in senior
management can be decisive in whether or not they are selected, and thus have a large influence over
future directions. More broadly, different organizations may imitate one another’s successes, leading
to a concentration of work in a particular area, while a small initial advantage of one organization
may be amplified over time, allowing it to dominate the area.

270
CHAPTER 5. COMPLEX SYSTEMS
Summary.
The environments in which research and development occur display the hallmarks
of complexity and are therefore best understood as complex systems. Research organizations and
corporations possess emergent properties including safety culture, which is paramount for AI safety.
Additionally, progress may have critical points and unfold in a nonlinear way that is difficult to
predict. It is crucial that AI safety strategies are informed by these possibilities.
Worked Example: Complex Systems Applied to Advocacy
The social systems within which AI is deployed are complex systems.
We find emergence
and the hallmarks of complexity in all social systems, from political structures to economic networks
to society as a whole. To illustrate this more specifically, we will now focus on the example of
policymaking structures and advocacy. This example is particularly relevant to AI safety, because
reducing risks from AI will need to involve the implementation of policies around its use. Advocacy
will therefore be necessary to promote safety policies and convince policymakers to adopt them.
Social systems display emergence and self-organization.
Patterns of governance and
collective decision making, such as democracy, can be considered emergent properties of social
and political systems. Although some individuals reach positions of power that might seem to
centralize control, social systems are nonetheless partly self-organizing, in the sense that many
individuals interact with one another and make decisions about whom to support, collectively
determining which candidate is elected. Similarly, people who care about particular causes self-
organize to form advocacy groups and set up grassroots campaigns. Policymakers interact with each
other and various stakeholders, including advocates, to reach policy decisions.
Advocacy movements have critical points and often unfold as punctuated equilibria.
Movements advancing different causes often display critical points, where a critical level of awareness
and support must be reached before policymakers will pay attention. Social systems may self-organize
toward this critical level of support and maintain it over time. However, the actual “tipping” from
one state into another, wherein policies are implemented, may be dependent on other external factors,
such as whether there are other urgent issues dominating decision-makers’ attention. For this reason,
advocacy efforts and their results tend to progress as punctuated equilibria; there may be little
apparent change for a long time, despite sustained work, and then a lot of sudden progress when
momentum builds and the political climate is right for it.
Both advocacy groups and policymaking structures also exhibit adaptive behavior.
Policymakers must continually adapt to the fluctuating political landscape and changing concerns of
the public. Similarly, advocacy groups must constantly adjust their activities to capture the attention
of the public and policymakers and convince them that a particular cause is relevant and important.
They might, for instance, use new technology to innovate an original mode of campaigning, or link
the cause to the prevailing zeitgeist—another emergent property of social systems.
Distributed functionality is evident on multiple levels in social systems.
The various tasks
involved in advocacy are partially and redundantly encoded across individuals within groups, allowing
campaigns to continue even as people leave and join them. More broadly still, there are usually
several groups campaigning for any given cause, meaning that the general function of advocacy is
distributed across different organizations. Decision making is also partially and redundantly encoded
among many policymakers, who interact with one another and various stakeholders to consider
different perspectives and decide on policies.

5.2. INTRODUCTION TO COMPLEX SYSTEMS
271
There are many nonlinear aspects of processes like advocacy.
There are numerous factors
that affect whether or not an issue is included on a policy agenda. Public interest in a cause, the
influence of opponents of a cause, and the number of other issues competing for attention are among
the many factors that affect the likeligood that it is considered non-linearly; for instance, opponents
with low influence may permit an issue being discussed, opponents with medium influence may try
and block it from discussed, but opponents with high influence may permit it being discussed so
that they can argue against it. Additionally, there is a degree of randomness involved in determining
which issues are considered. This means that the policy progress resulting from a particular campaign
does not necessarily reflect the level of effort put into it, nor how well organized it was.
Together with distributed functionality and critical points, this nonlinearity can make it difficult to
evaluate how well a campaign was executed or attribute eventual success. It might be that earlier
efforts were essential in laying the groundwork or simply maintaining some interest in a cause, even if
they did not yield immediate results. A later campaign might then succeed in prompting policy-level
action, regardless of whether it is particularly well organized, simply because the political climate
becomes favorable.
Other examples of nonlinearity within advocacy and policymaking arise from various feedback loops.
Since people are influenced by the opinions of those around them, a small change in the initial level of
support for a policy might be compounded over time, creating momentum and ultimately tipping the
balance as to whether or not it is adopted. On the other hand, original activities that are designed
to be attention-grabbing may run up against negative feedback loops that diminish their power over
time. Other groups may imitate them, for instance, so that their novelty wears off through repetition.
Opponents of a cause may also learn to counteract any new approaches that advocates for it try out.
This dynamic was understood by the military strategist Moltke the Elder, who is reported to have
said that “no plan survives first contact with the enemy”.
Political systems and advocacy groups have scalable structure.
Political systems usually
have a hierarchical structure with multiple levels of organization, such as councils responsible for
specific regions within a country and politicians forming a national government to address countrywide
issues. Advocacy groups can also exhibit this kind of structure. There may, for example, be a campaign
manager spearheading efforts, and then many regional leaders who organize activities at a more local
level. This scalable structure is another indication of complexity.
Summary.
The presence of these hallmarks of complexity in social and political systems suggests
they are best described within the complex systems paradigm. Additionally, these observations can
offer some insights into how we might approach advocacy for AI safety, suggesting it is not as simple
as developing a good policy idea and making a convincing argument for it.
Instead, it is likely that successful advocacy over the long term will be characterized by adaptability
to different political circumstances and changing public attitudes, as well as in response to opponents’
activities. Advocates will need to invest in building and maintaining relationships with the relevant
people and organizations, rather than just presenting the case for a policy. There may need to be a
lot of work that is not immediately rewarded, but momentum should be maintained so that advocates
are ready to capitalize on moments when the political climate becomes more favorable. It should
also be understood that it might not be possible to attribute success in any obvious way.
It Is Difficult to Foresee How the Use of AI Will Unfold
Complex social systems mean the eventual impact of AI is hard to predict.
As discussed
earlier, the behavior of complex systems can be difficult to predict for many reasons, such as the

272
CHAPTER 5. COMPLEX SYSTEMS
appearance of unanticipated emergent properties and feedback loops amplifying small changes in
initial conditions. This is compounded if a system is new to us; we may be able to predict certain
high-level behaviors of complex systems we are familiar with and have a lot of historical data on, such
as weather patterns and beehives, but AI systems are relatively new. Additionally, the deployment
of AI within society represents a case of nested complexity, where complex systems are embedded
within one another. This vastly increases the range of potential interactions and the number of ways
in which the systems can co-evolve. As a result, it is difficult to predict all the ways in which AI
might be used and what its eventual impact will be.
While this technology could have many positive effects, there is also potential for interactions to
have negative consequences. This is especially true if AI is deployed in ways that enable it to effect
actions in the world; for example, if it is put in charge of automated decision-making processes.
5.3
Complex Systems for AI Safety
5.3.1
General Lessons from Complex Systems
As we have discussed, AI systems and the social systems they are integrated within are best understood
as complex systems. For this reason, making AI safe is not like solving a mathematical problem or
fixing a watch. A watch might be complicated, but it is not complex. Its mechanism can be fully
understood and described, and its behavior can be predicted with a high degree of confidence. The
same is not true of complex systems.
Since a system’s complexity has a significant bearing on its behavior, our approach to AI safety
should be informed by the complex systems paradigm. We will now look at some lessons that have
been derived from observations of many other complex adaptive systems. We will discuss each lesson
and what it means for AI safety.
Lesson: Armchair Analysis Is Limited for Complex Systems
Learning how to make AIs safe will require some trial and error.
We cannot usually attain
a complete understanding of complex systems or anticipate all their emergent properties purely
by studying their structure in theory. This means we cannot exhaustively predict every way they
might go wrong just by thinking about them. Instead, some amount of trial and error is required to
understand how they will function under different circumstances and learn about the risks they might
pose. The implication for AI safety is that some simulation and experimentation will be required
to learn how AI systems might function in unexpected or unintended ways and to discover crucial
variables for safety.
Biomedical research and drug discovery exemplify the limitations of armchair theorizing.
The body is a highly complex system with countless biochemical reactions happening all the time,
and intricate interdependencies between them. Researchers may develop a drug that they believe,
according to their best theories, should treat a condition. However, they cannot thoroughly analyze
every single way it might interact with all the body’s organs, processes, and other medications people
may be taking. That is why clinical trials are required to test whether drugs are effective and detect
any unexpected side effects before they are approved for use.
Similarly, since AI systems are complex, we cannot expect to predict all their potential behaviors,
emergent properties, and associated hazards simply by thinking about them. Moreover, AI systems
will be even less predictable when they are taken out of the controlled development environment
and integrated within society. For example, when the chatbot Tay was released on Twitter, it soon

5.3. COMPLEX SYSTEMS FOR AI SAFETY
273
started to make racist and sexist comments, presumably learned through its interactions with other
Twitter users in this uncontrolled social setting.
Approaches to AI safety will need to involve experimentation.
Some of the most important
variables that affect a system’s safety will likely be discovered by accident. While we may have
ideas about the kinds of hazards a system entails, experimentation can help to confirm or refute
these. Importantly, it can also help us discover hazards we had not even imagined. These are
called unknown unknowns, or black swans, discussed extensively in the Safety Engineering chapter.
Empirical feedback loops are necessary.
Lesson: Systems Often Develop Subgoals Which Can Supersede the Original Goal
AIs might pursue distorted subgoals at the expense of the original goal.
The implication
for AI safety is that AIs might pursue subgoals over the goals we give them to begin with. This
presents a risk that we might lose control of AIs, and this could cause harm because their subgoals
may not always be aligned with human values.
A system often decomposes its goal into multiple subgoals to act as stepping stones.
Subgoals might include instrumentally convergent goals, which are discussed in the Single-Agent
Safety chapter. The idea is that achieving all the subgoals will collectively amount to achieving the
original aim. This might work for a simple, mechanistic system. However, since complex systems are
more than the sum of their parts, breaking goals down in this way can distort them. The system
might get sidetracked pursuing a subgoal, sometimes even at the expense of the original one. In other
words, although the subgoal was initially a means to an end, the system may end up prioritizing it
as an end in itself.
For example, companies usually have many different departments, each one specialized to pursue a
distinct subgoal. However, some departments, such as bureaucratic ones, can capture power and have
the company pursue goals unlike its initial one. Political leaders can delegate roles to subordinates,
but sometimes their subordinates may overthrow them in a coup.
As another example, imagine a politician who wants to improve the quality of life of residents of a
particular area. Increasing employment opportunities often leads to improvement in quality of life,
so the politician might focus on this as a subgoal—a means to an end. However, this subgoal might
end up supplanting the initial one. For instance, a company might want to build an industrial plant
that will offer jobs, but is also likely to leak toxic waste. Suppose the politician has become mostly
focused on increasing employment rates. In that case, they might approve the construction of this
plant, despite the likelihood that it will pollute the environment and worsen residents’ quality of life
in some ways.
Future AI agents may break down difficult long-term goals into smaller subgoals.
Creating subgoals can distort an AI’s objective and result in misalignment. As discussed in the
Emergence section of the Single-Agent Safety Chapter, optimization algorithms might produce
emergent optimizers that pursue subgoals, or AI agents may delegate goals to other agents and
potentially have the goal be distorted or subverted. In more extreme cases, the subgoals could be
pursued at the expense of the original one. We can specify our high-level objectives correctly without
any guarantee that systems will implement these in practice. As a result, systems may not pursue
goals that we would consider beneficial.

274
CHAPTER 5. COMPLEX SYSTEMS
Lesson: A Safe System, When Scaled Up, Is Not Necessarily Still Safe
AIs may continue to develop unanticipated behaviors as we scale them up. When we scale
up the size of a system, qualitatively new properties and behaviors emerge. The implication for AI
safety is that, when we increase the scale of a deep learning system, it will not necessarily just get
better at doing what it was doing before. It might begin to behave in entirely novel and unexpected
ways, potentially posing risks that we had not thought to prepare for.
It is not only when a system transitions from relative simplicity into complexity that novel properties
can appear. New properties can continue to emerge spontaneously as a complex system increases in
size. As discussed earlier in this chapter, LLMs have been shown to suddenly acquire new capabilities,
such as doing three-digit arithmetic, when the amount of compute used in training them is increased,
without any qualitative difference in training. Proxy gaming capabilities have also been found to
“switch on” at a certain threshold as the model’s number of parameters increases; in one study, at a
certain number of parameters, the proxy reward steeply increased, while the model’s performance as
intended by humans simultaneously declined.
Some emergent capabilities may pose a risk.
As deep learning models continue to grow,
we should expect to observe new emergent capabilities appearing. These may include potentially
concerning ones, such as deceptive behavior or the ability to game proxy goals. For instance, a
system might not attempt to engage in deception until it is sophisticated enough to be successful.
Deceptive behavior might then suddenly appear.
Lesson: Working Complex Systems Have Usually Evolved From Simpler Systems
We are unlikely to be able to build a large, safe AI system from scratch. Most attempts to
create a complex system from scratch will fail. More successful approaches usually involve developing
more complex systems gradually from simpler ones. The implication for AI safety is that we are
unlikely to be able to build a large, safe, working AI system directly. As discussed above, scaling up
a safe system does not guarantee that the resulting larger system will also be safe. However, starting
with safe systems and cautiously scaling them up is more likely to result in larger systems that are
safe than attempting to build the larger systems in one fell swoop.
Building complex systems directly is difficult.
Since complex systems can behave in
unexpected ways, we are unlikely to be able to design and build a large, working one from scratch.
Instead, we need to start by ensuring that smaller systems work and then build on them. This is
exemplified by how businesses develop; a business usually begins as one person or a few people with
an idea, then becomes a start-up, then a small business, and can potentially grow further from there.
People do not usually attempt to create multinational corporations immediately without progressing
naturally through these earlier stages of development.
One possible explanation for this relates to the limitations of armchair theorizing about complex
systems. Since it is difficult to anticipate every possible behavior and failure mode of a complex
system in advance, it is unlikely that we will be able to design a flawless system on the first attempt.
If we try to create a large, complex system immediately, it might be too large and unwieldy for us to
make the necessary changes to its structure when issues inevitably arise. If the system instead grows
gradually, it has a chance to encounter relevant problems and adapt to deal with them during the
earlier stages when it is smaller and more agile.
Similarly, if we want large AI systems that work well and are safe, we should start by making smaller
systems safe and effective and then incrementally build on them. This way, operators will have

5.3. COMPLEX SYSTEMS FOR AI SAFETY
275
more chances to notice any flaws and refine the systems as they go. An important caveat is that, as
discussed above, a scaled-up system might have novel emergent properties that are not present in
the smaller version. We cannot assume that a larger system will be safe just because it has been
developed in this way. However, it is more likely to be safe than if it was built from scratch. In other
words, this approach is not a guarantee of safety, but it is likely our best option. The scaling process
should be done cautiously.
Lesson: Any System Which Depends on Human Reliability Is Unreliable
Gilb’s Law of Unreliability.
We cannot guarantee that an operator will never make an error,
and especially not in a large complex system. As the chemical engineer Trevor Kletz put it: “Saying
an accident is due to human failing is about as helpful as saying that a fall is due to gravity. It is
true but it does not lead to constructive action” [5]. To make a complex system safer, we need to
incorporate some allowances in the design so that a single error is not enough to cause a catastrophe.
The implication of this for AI safety is that having humans monitoring AI systems does not guarantee
safety. Beyond human errors of judgment, processes in some complex systems may happen too
quickly for humans to be included in them anyway. AI systems will probably be too fast-moving
for human approval of their decisions to be a practical or even a feasible safety measure. We will
therefore need other ways of embedding human values in AI systems and ensuring they are preserved,
besides including humans in the processes. One potential approach might be to have some AI systems
overseeing others, though this brings its own risks.
Summary.
The general lessons that we should bear in mind for AI safety are:
1. We cannot predict every possible outcome of AI deployment by theorizing, so some trial and
error will be needed
2. Even if we specify an AI’s goals perfectly, it may start not pursue them in practice, as it may
instead pursue unexpected, distorted subgoals
3. A small system that is safe will not necessarily remain safe if it is scaled up
4. The most promising approach to building a large AI that is safe is nonetheless to make smaller
systems safe and scale them up cautiously
5. We cannot rely on keeping humans in the loop to make AI systems safe, because humans are
not perfectly reliable and, moreover, AIs are likely to accelerate processes too much for humans
to keep up.
5.3.2
Puzzles, Problems, and Wicked Problems
So far, we have explored the contrasts between simple and complex systems and why we need different
approaches to analyzing and understanding them. We have also described how AIs and the social
systems surrounding them are best understood as complex systems, and discussed some lessons from
the field of complex systems that can inform our expectations around AI safety and how we address
it.
In attempting to improve the safety of AI and its integration within society, we are engaging in
a form of problem-solving. However, simple and complex systems present entirely different types
of problems that require different styles of problem-solving. We can therefore reframe our earlier
discussion of reductionism and complex systems in terms of the kinds of challenges we can address

276
CHAPTER 5. COMPLEX SYSTEMS
within each paradigm. We will now distinguish between three different kinds of challenges—puzzles,
problems, and wicked problems. We will look at the systems that they tend to arise in, and the
different styles of problem-solving we require to tackle each of them.
Puzzles and Problems
Puzzles. Examples of puzzles include simple mathematics questions, sudokus, assembling furniture,
and fixing a common issue with a watch mechanism. In all these cases, there is only one correct result
and we are given all the information we need to find it. We usually find puzzles in simple systems
that have been designed by humans and can be fully understood. These can be solved within the
reductionist paradigm; the systems are simply the sum of their parts, and we can solve the puzzle by
breaking it down into a series of steps.
Problems.
With problems, we do not always have all the relevant information upfront, so we might
need to investigate to discover it. This usually gives us a better understanding of what’s causing the
issue, and ideas of solutions often follow naturally from there. It may turn out that there is more
than one approach to fixing the problem. However, it is clear when the problem is solved and the
system is functioning properly again.
We usually find problems in systems that are complicated, but not complex. For example, in car repair
work, it might not be immediately apparent what is causing an issue. However, we can investigate
to find out more, and this process often leads us to sensible solutions. Like puzzles, problems are
amenable to the reductionist paradigm, although they may involve more steps of analysis.
Wicked Problems
Wicked problems usually arise in complex systems and often involve a social element.
Wicked problems are a completely different class of challenges from puzzles and problems. They
appear in complex systems, with examples including inequality, misinformation, and climate change.
There is also often a social factor involved in wicked problems, which makes them more difficult to
solve. Owing to their multifaceted nature, wicked problems can be tricky to categorically define or
explain. We will now explore some key features that are commonly used to characterize them.
There is no single explanation or single solution for a wicked problem.
We can reasonably
interpret a wicked problem as stemming from more than one possible cause. As such, there is no
single correct solution or even a limited set of eternal possible solutions.
No proposed solution to a wicked problem is fully right or wrong, only better or worse.
Since there are usually many factors involved in a wicked problem, it is difficult to find a perfect
solution that addresses them all. Indeed, such a solution might not exist. Additionally, due to the
many interdependencies in complex systems, some proposed solutions may have negative side effects
and create other issues, even if they reduce the targeted wicked problem. As such, we cannot usually
find a solution that is fully correct or without flaw; rather, it is often necessary to look for solutions
that work relatively well with minimal negative side effects.
There is often a risk involved in attempting to solve a wicked problem.
Since we cannot
predict exactly how a complex system will react to an intervention in advance, we cannot be certain
as to how well a suggested solution will work or whether there will be any unintended side effects.
This means there may be a high cost to attempting to address wicked problems, as we risk unforeseen
consequences. However, trying out a potential solution is often the only way of finding out whether
it is better or worse.

5.3. COMPLEX SYSTEMS FOR AI SAFETY
277
Every wicked problem is unique because every complex system is unique.
While we can
learn some lessons from other systems with similar properties, no two systems will respond to our
actions in exactly the same way. This means that we cannot simply transpose a solution that worked
well in one scenario to a different one and expect it to be just as effective. For example, introducing
predators to control pest numbers has worked well in some situations, but, as we will discuss in the
next section, it has failed in others. This is because all ecosystems are unique, and the same is true
of all complex systems, meaning that each wicked problem is likely to require a specifically tailored
intervention.
It might not be obvious when a wicked problem has been solved.
Since wicked problems
are often difficult to perfectly define, it can be challenging to say they have been fully eliminated,
even if they have been greatly reduced. Indeed, since wicked problems tend to be persistent; it might
not be feasible to fully eliminate many wicked problems at all. Instead, they often require ongoing
efforts to improve the situation, though the ideal scenario may always be beyond reach.
AI safety is a wicked problem.
Since AI and the social environments it is deployed within are
complex systems, the issues that arise with its use are likely to be wicked problems. There may be
no obvious solution, and there will probably need to be some trial and error involved in tackling
them. More broadly, the problem of AI safety in general can be considered a wicked problem. There
is no single correct approach, but many possibilities. We may never be able to say that we have fully
“solved” AI safety; it will require ongoing efforts.
Summary.
Puzzles and problems usually arise in relatively simple systems that we can obtain a
complete or near-complete understanding of. We can therefore find all the information we need to
explain the issue and find a solution to it, although problems may be more complicated, requiring
more investigation and steps of analysis than puzzles.
Wicked problems, on the other hand, arise in complex systems, which are much more difficult to
attain a thorough understanding of. There may be no single correct explanation for a wicked problem,
proposed solutions may not be fully right or wrong, and it might not be possible to find out how
good they are without trial and error. Every wicked problem is unique, so solutions that worked well
in one system may not always work in another, even if the systems seem similar, and it might not
be possible to ever definitively say that a wicked problem has been solved. Owing to the complex
nature of the systems involved, AI safety is a wicked problem.
5.3.3
Challenges With Interventionism
As touched on above, there are usually many potential solutions to wicked problems, but they may
not all work in practice, even if they sound sensible in theory. We might therefore find that some
attempts to solve wicked problems will be ineffective, have negative side effects, or even backfire.
Complex systems have so many interdependencies that when we try to adjust one aspect of them, we
can inadvertently affect others. For this reason, we should approach AI safety with more humility
and more awareness of the limits of our knowledge than if we were trying to fix a watch or a washing
machine. We will now look at some examples of historical interventions in complex systems that
have not gone to plan. In many cases, they have done more harm than good.
Cane toads in Australia.
Sugarcane is grown in Australia as a valuable product in the economy,
but a species of insect called the cane beetle is known to feed on sugarcane crops and destroy them.
In the 1930s, cane toads were introduced in Australia to prey on these beetles, with the hope of
minimizing crop losses. However, since cane toads are not native to Australia, they have no natural

278
CHAPTER 5. COMPLEX SYSTEMS
predators there. In fact, the toads are toxic to many native species and have damaged ecosystems by
poisoning animals that have eaten them. The cane toads have multiplied rapidly and are considered
an invasive species. Attempts to control their numbers have so far been largely unsuccessful.
Warning signs on roads.
Road accidents are a long-standing and pervasive issue. A widely
used intervention is to display signs along roads with information about the numbers of crashes and
fatalities that have happened in the surrounding area that year. The idea is that this information
should encourage people to drive more carefully. However, one study has found that signs like this
increase the number of accidents and fatalities, possibly because they distract drivers from the road.
Renewable Heat Incentive Scandal.
In 2012, a government department in Northern Ireland
wanted to boost the fraction of energy consumption from renewable sources. To this end, they set up
an initiative offering businesses generous subsidies for using renewable heating sources, such as wood
pellets. However, in trying to reach their percentage targets for renewable energy, the politicians
offered a subsidy that was slightly more than the cost of the wood pellets. This incentivized businesses
to use more energy than they needed and profit from the subsidies. There were reports of people
burning pellets to heat empty buildings unnecessarily. The episode became known as the “Cash for
Ash scandal”.
Barbados-Grenada football match.
In the 1994 Caribbean Cup, an international football
tournament, organizers introduced a new rule to reduce the likelihood of ties, which they thought
were less exciting. The rule was that if two teams were tied at the end of the allotted 90 minutes, the
match would go to extra time, and any goal scored in extra time would be worth double. The idea
was to incentivize the players to try harder to score. However, in a match between Barbados and
Grenada, Barbados needed to win by two goals to advance to the tournament finals. The score as they
approached 90 minutes was 2-1 to Barbados. This resulted in a strange situation where Barbados
players tried to score an own goal to push the match into extra time and have an opportunity to win
by two.
Summary.
Interventions that work in theory might fail in a complex system. In all these examples,
an intervention was attempted to solve a problem in a complex system. In theory, each intervention
seemed like it should work, but each decision-maker’s theory did not capture all the complexities of
the system at hand. Therefore, when each intervention was applied, the system reacted in unexpected
ways, leaving the original problem unsolved, and often creating additional problems that might be
even worse.
Stable States and Restoring Forces
The examples above illustrate how complex systems can react unexpectedly to interventions. This
can be partly attributed to the properties of self-organization and adaptive behavior; complex systems
can organize themselves around new conditions in unobvious ways, without necessarily addressing
the reason for the intervention. Some interventions might partially solve the original problem but
unleash unanticipated side effects that are not considered worth the benefits. Other interventions,
however, might completely backfire, exacerbating the very problem they were intended to solve. We
will now discuss the concept of “stable states” and how they might explain complex systems’ tendency
to resist attempts to change them.
If a complex system is in a stable state, it is likely to resist attempts to change it.
If a
ball is sitting in a valley between two hills and we kick it up one hill, gravity will pull it back to the
valley. Similarly, if a complex system has found a stable state, there might be some “restoring forces”

5.3. COMPLEX SYSTEMS FOR AI SAFETY
279
or homeostatic processes that will keep drawing it back toward that state, even if we try to pull it in
a different direction. When complex systems are not near critical points, they exhibit robustness to
external changes.
Another analogy is Le Chatelier’s Principle, a well-known concept in chemistry. The principle
concerns chemical equilibria, in which the concentrations of different chemicals stay the same over
time. There may be chemical reactions happening, converting some chemicals into others, but the
rate of any reaction will equal the rate of its opposite reaction. The total concentration of each
chemical therefore remains unchanged, hence the term equilibrium.
Le Chatelier’s Principle states that if we introduce a change to a system in chemical equilibrium, the
system will shift to a new equilibrium in a way that partly opposes that change. For example, if we
increase the concentration of one chemical, then the rate of the reaction using up that chemical will
increase, using up more of it and reducing the extra amount present. Similarly, complex systems
sometimes react against our interferences in them.
We will now look at some examples of interventions backfiring in complex systems. We will explore
how we might think of these systems as having stable states and restoring forces that draw the
system back toward its stable state if an intervention tries to pull it away. Note that the following
discussions of what the stable states and restoring forces might be are largely speculative. Although
these hypotheses have not been rigorously proven to explain these examples, they are intended to
show how we can view systems and failed interventions through the lens of stable states and restoring
forces.
Rules to restrict driving.
In 1989, to tackle high traffic and air pollution levels in Mexico City,
the government launched an initiative called “Hoy No Circula.” The program introduced rules that
allowed people to drive only on certain days of the week, depending on the last number on their
license plate. This initially led to a drop in emissions, but they soon rose again, actually surpassing
the pre-intervention levels. A study found that the rules had incentivized people to buy additional
cars so they could drive on more days. Moreover, the extra cars people bought tended to be cheaper,
older, more polluting ones, exacerbating the pollution problem [6].
We could perhaps interpret this situation as having a stable state in terms of how much driving
people wanted or needed to do. When rules were introduced to try to reduce it, people looked for
ways to circumvent them. We could view this as a restoring force in the system.
Four Pests campaign.
In 1958, the Chinese leader Mao Zedong launched a campaign encouraging
people to kill the “four pests”: flies, mosquitoes, rodents, and sparrows. The first three were targeted
for spreading disease, but sparrows were considered a pest because they were believed to eat grain
and reduce crop yields. During this campaign, sparrows were killed intensively and their populations
plummeted. However, as well as grain, sparrows also eat locusts. In the absence of a natural predator,
locust populations rose sharply, destroying more crops than the sparrows did [7]. Although many
factors were at play, including poor weather and officials’ decisions about food distribution [8], this
ecosystem imbalance is often considered a contributing factor in the Great Chinese Famine [7], during
which tens of millions of people starved between 1959 and 1961.
Ecosystems are highly complex, with intricate balances between the populations of many species.
We could think of agricultural systems as having a “stable state” that naturally involves some crops
being lost to wildlife. If we try to reduce these losses simply by eliminating one species, then another
might take advantage of the available crops instead, acting as a kind of restoring force.

280
CHAPTER 5. COMPLEX SYSTEMS
Antibiotic resistance.
Bacterial infections have been a cause of illness and mortality in humans
throughout history. In September 1928, bacteriologist Alexander Fleming discovered penicillin, the
first antibiotic. Over the following years, the methods for producing it were refined, and, by the end
of World War II, there was a large supply available for use in the US and Britain. This was a huge
medical advancement, offering a cure for many common causes of death, such as pneumonia and
tuberculosis. Death rates due to bacterial illnesses dropped dramatically [9]; it is estimated that, in
1952, in the US, around 150,000 fewer people died from bacterial illnesses than would have without
antibiotics. In the early 2000s, it was estimated that antibiotics may have been saving around 200,000
lives annually in the US alone.
However, as antibiotics have become more abundantly used, bacteria have begun to evolve resistance
to these vital medicines. Today, many bacterial illnesses, including pneumonia and tuberculosis,
are once again becoming difficult to treat due to the declining effectiveness of antibiotics. In 2019,
the Centers for Disease Control and Prevention reported that antimicrobial-resistant bacteria are
responsible for over 35,000 deaths per year in the US [10].
In this case, we might think of the coexistence of humans and pathogens as having a stable state,
involving some infections and deaths. While antibiotics have reduced deaths due to bacteria over the
past decades, we could view natural selection as a “restoring force”, driving the evolution of bacteria
to become resistant and causing deaths to rise again. Overuse of these medicines intensifies selective
pressures and accelerates the process.
In this case, it is worth noting that antibiotics have been a monumental advancement in healthcare,
and we do not argue that they should not be used or that they are a failed intervention. Rather, this
example highlights the tendency of complex systems to react against measures over time, even if
they were initially highly successful.
Instead of pushing a system in a desired direction, we could try to shift the stable states.
If an intervention attempts to artificially hold a system away from its stable state, it might be as
unproductive as repeatedly kicking a ball up a hill to keep it away from a valley. Metaphorically
speaking, if we want the ball to sit in a different place, a more effective approach would be to change
the landscape so that the valley is where we want the ball to be. The ball will then settle there
without our help. More generally, we want to change the stable points of the system itself, if possible,
so that it naturally reaches a state that is more in line with our desired outcomes.
Good cycling infrastructure may shift the stable states of how much people drive.
One
example of shifting stable states is the construction of cycling infrastructure in the Netherlands
in the 1970s. As cars became cheaper during the 20th century, the number of people who owned
them began to rise in many countries, including the Netherlands. Alongside this, the number of car
accidents also increased. In the 1970s, a protest movement gathered in response to rising numbers
of children being killed by cars. The campaign succeeded in convincing Dutch politicians to build
extensive cycling infrastructure to encourage people to travel by bike instead of by car. This has had
positive, lasting results. A 2018 report stated that around 27% of all trips in the Netherlands are
made by bike—a higher proportion than any other country studied [11].
Instead of making rules to try to limit how much people drive, creating appropriate infrastructure
makes cycling safer and easier. Additionally, well-planned cycle networks can make many routes
quicker by bike than by car, making this option more convenient. Under these conditions, people will
naturally be more inclined to cycle, so society naturally drifts toward a stable point that entails less
driving.

5.3. COMPLEX SYSTEMS FOR AI SAFETY
281
It is worth noting that the Netherlands’ success might not be possible to replicate everywhere, as
there may be other factors involved. For instance, the terrain in the Netherlands is relatively flat
compared with other countries, and hilly terrain might dissuade people from cycling. This illustrates
that some factors influencing the stable points are beyond our control. Nevertheless, this approach
has likely been more effective in the Netherlands than simple rules limiting driving would have been.
There might also be other effective strategies for changing the stable points of how much people
drive, such as creating cheap, reliable public transport systems.
Summary.
Complex systems can often self-organize into stable states that we may consider
undesirable, and which create some kind of environmental or social problem. However, if we try
to solve the problem too simplistically by trying to pull the system away from its stable state, we
might expect some restoring forces to circumvent our intervention and bring the system back to its
stable state, or an even worse one. A more effective approach might be to change certain underlying
conditions within a system, where possible, to create new, more desirable stable states for the system
to self-organize toward.
Successful Interventions
We have discussed several examples of failed interventions in complex systems. While it can be
difficult to say definitively a wicked problem has been solved, there are some examples of interventions
that have clearly been at least partially successful. We will now look at some of these examples.
Eradication of Smallpox.
In 1967, the WHO launched an intensive campaign against smallpox,
involving intensive global vaccination programs and close monitoring and containment of outbreaks.
In 1980, the WHO declared that smallpox had been eradicated. This was an enormous feat that
required concerted international efforts over more than a decade.
Reversal of the depletion of the ozone layer
. Toward the end of the 20th century it was
discovered that certain compounds frequently used in spray cans, refrigerators, and air conditioners,
were reaching the ozone layer and depleting it, leading to more harmful radiation passing through. As
a result, the Montreal Protocol, an international agreement to phase out the use of these compounds,
was negotiated in 1987 and enacted soon after. It has been reported that the ozone layer has started
to recover since then.
Public health campaigns against smoking.
In the 20th century, scientists discovered a causal
relationship between tobacco smoking and lung cancer. In the following decades, governments started
implementing various measures to discourage people from smoking. Initiatives have included health
warnings on cigarette packets, smoking bans in certain public areas, and programs supporting people
through the process of quitting. Many of these measures have successfully raised public awareness of
health risks and contributed to declining smoking rates in several countries.
While these examples show that it is possible to address wicked problems, they also demonstrate some
of the difficulties involved. All these interventions have required enormous, sustained efforts over
many years, and some have involved coordination on a global scale. It is worth noting that smallpox
is the only infectious disease that has ever been eradicated. One challenge in replicating this success
elsewhere is that some viruses, such as influenza viruses, evolve rapidly to evade vaccine-induced
immunity. This highlights how unique each wicked problem is.
Campaigns to dissuade people from smoking have faced pushback from the tobacco industry, showing
how conflicting incentives in complex systems can hamper attempts to solve wicked problems.
Additionally, as is often the case with wicked problems, we may never be able to say that smoking is

282
CHAPTER 5. COMPLEX SYSTEMS
fully “solved”; it might not be feasible to reach a situation where no one smokes at all. Nonetheless,
much positive progress has been made in tackling this issue.
Summary.
Although it is by no means straightforward to tackle wicked problems, there are some
examples of interventions that have successfully solved or made great strides toward solving certain
wicked problems. For many wicked problems, it may never be possible to say that they have been
fully solved, but it is nonetheless possible to make progress and improve the situation.
5.3.4
Systemic Issues
We have discussed the characteristics of wicked problems as stemming from the complex systems they
arise from, and explored why they are so difficult to tackle. We have also looked at some examples of
failed attempts to solve wicked problems, as well as examples of more successful ones, and explored
the idea of shifting stable points, instead of just trying to pull a system away from its stable points.
We will now discuss ways of thinking more holistically and identifying more effective, system-level
solutions.
Obvious problems are sometimes just symptoms of broader systemic issues.
It can be
tempting to take action at the level of the obvious, tangible problem, but this is sometimes like
applying a band-aid. If there is a broader underlying issue, then trying to fix the problem directly
might only work temporarily, and more problems might continue to crop up.
We should think about the function we are trying to achieve and the system we are
using.
One method of finding more effective solutions is to “zoom out” and consider the situation
holistically. In complex systems language, we might say that we need to find the correct scale at
which to analyze the situation. This might involve thinking carefully about what we are trying
to achieve and whether individuals or groups in the system exhibit the behaviors we are trying to
control. We should consider whether, if we solve the immediate problem, another one might be likely
to arise soon after.
It might be more fruitful to change AI research culture than address individual issues.
One approach to AI safety might be to address issues with individual products as they come up.
This approach would be focused on the level of the problem. However, if issues keep arising, it could
be a sign of broader underlying issues with how research is being done. It might therefore be better
to influence the culture around AI research and development, instead of focusing on individual risks.
If multiple organizations developing AI technology are in an arms race with one another, for example,
they will be trying to reach goals and release products as quickly as possible. This will likely compel
people to cut corners, perhaps by omitting safety measures. Reducing these competitive pressures
might therefore significantly reduce overall risk, albeit less directly.
If competitive pressures remain high, we could imagine a potential future scenario in which a serious
AI-related safety issue materializes and causes considerable harm. In explaining this accident, people
might focus on the exact series of events that led to it—which product was involved, who developed
it, and what precisely went wrong. However, ignoring the role of competitive pressures would be an
oversight. We can illustrate this difference in mindset more clearly by looking at historical examples.
We can explain catastrophes by looking for a “root cause” or looking at systemic factors.
There are usually two ways of interpreting a catastrophe. We can either look for a traceable series of
events that triggered it, or we can think more about the surrounding conditions that made it likely to
happen one way or another. For instance, the first approach might say that the assassination of Franz

5.4. CONCLUSION
283
Ferdinand caused World War One. While that event may have been the spark, international tensions
were already high beforehand. If the assassination had not happened, something else might have
done, also triggering a conflict. A better approach might instead invoke the imperialistic ambitions
of many nations and the development of new militaristic technologies, which led nations to believe
there was a strong first-strike advantage.
We can also find the contrast between these two mindsets in the different explanations put forward
for the Bhopal gas tragedy, a huge leak of toxic gas that happened in December 1984 at a pesticide-
producing plant in Bhopal, India. The disaster caused thousands of deaths and injured up to half a
million people. A “root cause” explanation blames workers for allowing water to get into some of the
pipes, where it set off an uncontrolled reaction with other chemicals that escalated to catastrophe.
However, a more holistic view focuses on the slipping safety standards in the run-up to the event,
during which management failed to adequately maintain safety systems and ensure that employees
were properly trained. According to this view, an accident was bound to happen as a result of these
factors, regardless of the specific way in which it started.
To improve safety in complex systems, we should focus on general systemic factors.
Both examples above took place in complex systems; the network of changing relationships between
nations constitutes a complex evolving system, as does the system of operations in a large industrial
facility. As we have discussed, complex systems are difficult to predict and we cannot analyze and
guard against every possible way in which something might go wrong. Trying to change the broad
systemic factors to influence a system’s general safety may be much more effective. In the development
of technology, including AI, competitive pressures are one important systemic risk source. Others
include regulations, public concern, safety costs, and safety culture. We will discuss these and other
systemic factors in more depth in the Safety Engineering chapter.
Summary.
Instead of just focusing on the most obvious, surface-level problem, we should also
consider what function we are trying to achieve, the system we are using, and whether the problem
might be a result of a mismatch between the system and our goal. Thinking in this way can help us
identify systemic factors underlying the problems and ways of changing them so that the system is
better suited to achieving our aims.
5.4
Conclusion
In this chapter, we have explored the properties of complex systems and their implications for AI
safety strategies. We began by contrasting simple systems with complex systems. While the former
can be understood as the sum of their parts, the latter display emergent properties that arise from
complex interactions. These properties do not exist in any of the components in isolation and cannot
easily be derived from reductive analysis of the system.
Next, we explored seven salient hallmarks of complexity. We saw that feedback loops are ubiquitous
in complex systems and often lead to nonlinearity, where a small change in the input to a system
does not result in a proportionate change in the output. Rather, fluctuations can be amplified or
quashed by feedback loops. Furthermore, these processes can make a system highly sensitive to its
initial conditions, meaning that a small difference at the outset can lead to vastly different long-term
trajectories. This is often referred to as the “butterfly effect”, and makes it difficult to predict the
behaviors of complex systems.
We also discussed how the components of complex systems tend to self-organize to some extent
and how they often display critical points, at which a small fluctuation can tip the system into a

284
CHAPTER 5. COMPLEX SYSTEMS
drastically different state. We then looked at distributed functionality, which refers to how tasks are
loosely shared among components in a complex system, and scalable structure, which gives rise to
power laws within complex systems. The final hallmark of complexity we discussed was adaptive
behavior, which allows systems to continue functioning in a changing environment.
Along the way, we highlighted how deep learning systems exhibit the hallmarks of complexity. Beyond
AIs themselves, we also showed how the social systems they exist within are also best understood
as complex systems, through the worked examples of corporations and research institutes, political
systems, and advocacy organizations.
Having established the presence of complexity in AIs and the systems surrounding them, we looked
at what this means for AI safety by looking at five general lessons. Since we cannot usually predict
all emergent properties of complex systems simply through theoretical analysis, some trial and error
is likely to be required in making AI systems safe. It is also important to be aware that systems
often break down goals into subgoals, which can supersede the original goal, meaning that AIs may
not always pursue the goals we give them.
Due to the potential for emergent properties, we cannot guarantee that a safe system will remain
safe when it is scaled up. However, since we cannot usually understand complex systems perfectly in
theory, it is extremely difficult to build a flawless complex system from scratch. This means that
starting with small systems that are safe and scaling them up cautiously is likely the most promising
approach to building large complex systems that are safe. The final general lesson is that we cannot
guarantee AI safety by keeping humans in the loop, so we need to design systems with this in mind.
Next, we looked at how complex systems often give rise to wicked problems, which cannot be solved
in the same way we would approach a simple mathematics question or a puzzle. We saw how difficult
it is to address wicked problems, due to the unexpected side effects that can occur when we interfere
with complex systems. However, we also explored examples of successful interventions, showing that
it is possible to make significant progress, even if we cannot fully solve a problem. In thinking about
the most effective interventions, we highlighted the importance of thinking holistically and looking
for system-level solutions.
AI safety is not a mathematical puzzle that can be solved once and for all. Rather, it is a wicked
problem that is likely to require ongoing, coordinated efforts, and flexible strategies that can be
adapted to changing circumstances.
5.5
Literature
5.5.1
Recommended Reading
• James Ladyman, James Lambert, and Karoline Wiesner. “What is a complex system?” In:
European Journal for Philosophy of Science 3 (June 2013). doi: 10.1007/s13194-012-0056-8
• D.H. Meadows and D. Wright. Thinking in Systems: A Primer. Chelsea Green Publishing,
2008. isbn: 9781603580557. url: https://books.google.com.au/books?id=CpbLAgAAQBAJ
• J.H. Miller and S.E. Page. Complex Adaptive Systems: An Introduction to Computational
Models of Social Life. Princeton Studies in Complexity. Princeton University Press, 2009. isbn:
9781400835522. url: https://books.google.com.au/books?id=XQUHZC8wcdMC

5.5. LITERATURE
285
• John Gall.
The systems bible: the beginner’s guide to systems large and small.
General
Systemantics Press, 2002
• Richard Cook.
“How complex systems fail”.
In:
(Jan. 2002).
url:
https : / / how .
complexsystems.fail
5.5.2
References
[1]
P. Cilliers and David Spurrett. “Complexity and post-modernism: Understanding complex systems”.
In: South African Journal of Philosophy 18 (Sept. 2014), pp. 258–274. doi: 10.1080/02580136.1999.
10878187.
[2]
Alethea Power et al. Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.
2022. arXiv: 2201.02177 [cs.LG].
[3]
Jordan Hoffmann et al. Training Compute-Optimal Large Language Models. 2022. arXiv: 2203.15556
[cs.CL].
[4]
Hang Zhao et al. “Response mechanisms to heat stress in bees”. In: Apidologie 52 (Jan. 2021). doi:
10.1007/s13592-020-00830-w.
[5]
Trevor Kletz. An Engineer’s View of Human Error. CRC Press, 2018.
[6]
Lucas Davis. “The Effect of Driving Restrictions on Air Quality in Mexico City”. In: Journal of
Political Economy 116 (Feb. 2008), pp. 38–81. doi: 10.1086/529398.
[7]
Jemimah Steinfeld. “China’s deadly science lesson: How an ill-conceived campaign against sparrows
contributed to one of the worst famines in history”. In: Index on Censorship 47.3 (2018), pp. 49–49.
doi: 10.1177/0306422018800259.
[8]
Xin Meng, Nancy Qian, and Pierre Yared. “The Institutional Causes of China’s Great Famine,
1959–1961”. In: The Review of Economic Studies 82.4 (Apr. 2015), pp. 1568–1611. issn: 0034-6527.
doi: 10.1093/restud/rdv016.
[9]
Joseph Gottfried. “History Repeating? Avoiding a Return to the Pre-Antibiotic Age”. In: 2005. url:
https://api.semanticscholar.org/CorpusID:84028897.
[10]
Centers for Disease Control (US). “Antibiotic resistance threats in the United States, 2019”. In: CDC
Stacks (2019). doi: http://dx.doi.org/10.15620/cdc:82532.
[11]
“Cycling facts”. In: (2018). url: https://english.kimnet.nl/publications/publications/2018/
04/06/cycling-facts.
[12]
James Ladyman, James Lambert, and Karoline Wiesner. “What is a complex system?” In: European
Journal for Philosophy of Science 3 (June 2013). doi: 10.1007/s13194-012-0056-8.
[13]
D.H. Meadows and D. Wright. Thinking in Systems: A Primer. Chelsea Green Publishing, 2008. isbn:
9781603580557. url: https://books.google.com.au/books?id=CpbLAgAAQBAJ.
[14]
J.H. Miller and S.E. Page. Complex Adaptive Systems: An Introduction to Computational Models of
Social Life. Princeton Studies in Complexity. Princeton University Press, 2009. isbn: 9781400835522.
url: https://books.google.com.au/books?id=XQUHZC8wcdMC.
[15]
John Gall. The systems bible: the beginner’s guide to systems large and small. General Systemantics
Press, 2002.
[16]
Richard Cook. “How complex systems fail”. In: (Jan. 2002). url: https://how.complexsystems.fail.
[17]
Paul Schoemaker. “The Expected Utility Model: Its Variants, Purposes, Evidence and Limitations”.
In: Journal of Economic Literature 20 (Feb. 1982), pp. 529–63.
[18]
David Silver et al. “Reward is enough”. In: Artificial Intelligence 299 (2021), p. 103535. issn: 0004-3702.
doi: https://doi.org/10.1016/j.artint.2021.103535. url: https://www.sciencedirect.com/
science/article/pii/S0004370221000862.
[19]
J. Von Neumann and O. Morgenstern. Theory of games and economic behavior. Princeton University
Press, 1947.

286
CHAPTER 5. COMPLEX SYSTEMS
[20]
Martin Peterson. “The St. Petersburg Paradox”. In: The Stanford Encyclopedia of Philosophy. Ed. by
Edward N. Zalta and Uri Nodelman. Fall 2023. Metaphysics Research Lab, Stanford University, 2023.
[21]
Nate Soares et al. “Corrigibility”. In: Workshops at the Twenty-Ninth AAAI Conference on Artificial
Intelligence. 2015.
[22]
Elliott Thornley. “The Shutdown Problem: Two Theorems, Incomplete Preferences as a Solution”. In:
AI Alignment Awards (2023).
[23]
A. Dixit. Slides and Notes. url: https : / / www . princeton . edu / ~dixitak / Teaching /
EconomicsOfUncertainty/Slides&Notes/%20%E2%80%8C.
[24]
Henry Sidgwick. The methods of ethics. Good Press, 2019.
[25]
Joseph Wunderle Jr, María Castro, and Ned Fetcher. “Risk-averse Foraging by Bananaquits on
Negative Energy Budgets”. In: Behavioral Ecology and Sociobiology 21 (Jan. 1987), pp. 249–255. doi:
10.1007/BF00292506.
[26]
Samir Okasha. “Rational choice, risk aversion, and evolution”. In: The Journal of philosophy 104.5
(2007), pp. 217–235.
[27]
John L Kelly. “A new interpretation of information rate”. In: the bell system technical journal 35.4
(1956), pp. 917–926.
[28]
Robert McDonald and Anna Paulson. “What went wrong at AIG”. In: Kellogg Insight 3 (2015).
[29]
N.N. Taleb. Antifragile: Things That Gain from Disorder. Incerto. Random House Publishing Group,
2012. isbn: 9780679645276. url: https://books.google.com.au/books?id=5fqbz_qGi0AC.
[30]
Jessica Taylor. “Quantilizers: A Safer Alternative to Maximizers for Limited Optimization.” In: AAAI
Workshop: AI, Ethics, and Society. 2016.
[31]
M. Allais. “Le Comportement de l’Homme Rationnel devant le Risque: Critique des Postulats et
Axiomes de l’Ecole Americaine”. In: Econometrica 21.4 (1953), pp. 503–546. issn: 00129682, 14680262.
url: http://www.jstor.org/stable/1907921 (visited on 10/16/2023).
[32]
D. Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011.
[33]
David McCarthy. “Probability in Ethics”. In: The Oxford Handbook of Philosophy and Probability.
Ed. by Alan Ha?jek and Christopher Hitchcock. Oxford University Press, 2016, pp. 705–737.
[34]
Amos Tversky and Daniel Kahneman. “The framing of decisions and the psychology of choice”. In:
science 211.4481 (1981), pp. 453–458.
[35]
Daniel Kahneman and Amos Tversky. “Prospect theory: An analysis of decision under risk”. In:
Handbook of the fundamentals of financial decision making: Part I. World Scientific, 2013, pp. 99–127.
[36]
Adam Bales. “Will AI avoid exploitation? Artificial general intelligence and expected utility theory”.
In: Philosophical Studies (2023). url: https://doi.org/10.1007/s11098-023-02023-4.
[37]
Lara Buchak. Risk and Rationality. Oxford University Press, 2013. isbn: 9780199672165. doi:
10 . 1093 / acprof : oso / 9780199672165 . 001 . 0001. url: https : / / doi . org / 10 . 1093 / acprof :
oso/9780199672165.001.0001.

Part III
Ethics
287


Machine Ethics
6.1
Introduction
How should we direct AIs to promote human values?
As we continue to develop powerful AI
systems, it is crucial to ensure they are safe and beneficial for humanity. In this chapter, we discuss
the challenges of embedding ethics into AIs. This is the problem of machine ethics. It is relevant for
AIs that exist today in sectors such as healthcare or transportation, where artificial systems may be
making decisions in situations that can harm or benefit humans. It is even more relevant for future
systems that may be integrated more broadly across economies, governments, and militaries, making
high-stakes decisions with many important moral considerations.
Many people have incoherent views on machine ethics.
People often talk about what AIs
should do to promote human values. They may agree with many of the following:
1. AIs should do what you tell them to do.
2. AIs should promote what you choose to do.
3. AIs should do what’s fair.
4. AIs should do what a democratic process tells them to do.
5. AIs should figure out what is moral, then do that.
6. AIs should do what is objectively good for you.
7. AIs should do what would make people happy.
All of these seem like reasonable answers. At least at first glance, these all seem like excellent goals
for machine ethics. However, not all of these are compatible, because they make different normative
assumptions and require different technical implementations. Other suggestions such as “AIs should
follow human intentions” are highly vague. Put straightforwardly, while these sound attractive and
similar, they are not the same thing.
This should challenge the notion that machine ethics is a straightforward problem with a simple
solution, and that the real challenges lie only elsewhere, such as in capabilities development. In fact,
those who believe there are easy ways to ensure that AIs act ethically may find themselves grappling
with inconsistencies and confusion when confronted with instances in which their preferred methods
appear to yield harmful behavior. Often, discussions of machine ethics require an understanding
of considerations and theories from ethical theory, which we explore next in the Normative Ethics
chapter.
Preview.
In this chapter, we will consider the intricacies of machine ethics, attempting to
understand which answers, if any, take us closer to creating safe and beneficial AIs. We start
with frameworks that attempt to constrain AI goals, limiting the potential for AIs to pursue bad
objectives, including constraints on actions to ensure that AIs follow existing laws and make fair
decisions.
The rest of the chapter explores positive goals for AIs, such as maximizing wealth,
happiness, or preference satisfaction. This is divided into the following subsections, which will each
provide an overview of an aspect of machine ethics:
289

290
CHAPTER 6. MACHINE ETHICS
1. Law: Why not have AIs follow the law? We examine whether we can design AIs that follow
existing legal frameworks, considering that law is a legitimate aggregation of human values
that is time-tested and comprehensive while being both specific and adaptable. We will lay out
the challenges of the occasional silence, immorality, or unrepresentativeness of established law.
2. Fairness: Should we make AIs be fair? We explore fairness in AI systems and the challenges
associated with ensuring outcomes created by AIs are fair. We will discuss different definitions
fairness and see how they are incompatible, as well as consider approaches to mitigating biases.
3. Economic Engine: Should we let the economy decide what AIs will be like? We consider the
potential impact the use of AI might have on the economy and society, how economic forces are
shaping AI development, and why letting the economic engine create AIs without substantial
regulation might not be a good idea.
4. Preferences: Should we have AIs satisfy people’s preferences? We investigate how AI systems
can be created to satisfy individual preferences, focusing on the key question of which preferences.
We focus on the challenges of deciding between revealed, stated, and idealized preferences.
5. Happiness: Should we have AIs make people happy? We ask how we can use AIs to promote
human happiness. After considering the benefits and pitfalls of using happiness as a proxy for
wellbeing, we discuss an alternative conception of wellbeing.
6. Social Welfare Functions: Should we have AIs maximize total wellbeing? We look at how to
aggregate wellbeing across society, focusing on social welfare functions. We discuss what social
welfare functions are and how to trade-off between equity and efficiency in a principled way.
7. Moral Parliament: Should we use a moral parliament to guide AI decision-making? We
incorporate ideas from moral uncertainty by using a moral parliament, where ethical decisions
regarding AI systems are made by simulating democratic processes.
Some of these areas are much more developed than others: there has been a lot of work on developing
laws to govern AIs, for instance, but only a handful of papers on using AIs to represent moral theories
in a “moral parliament.” Additionally, different areas may be relevant for different systems: areas such
as algorithmic fairness are highly relevant to AI systems today, such as predictive systems used for
criminal sentencing, whereas goals such as “robustly estimate and optimize an individual’s wellbeing”
might appear very challenging today but become more feasible in the future with more capable
systems. By examining these key aspects of machine ethics, we aim to provide an introductory
understanding of the challenges and potential solutions in creating safe AI systems.
6.2
Law
Why not have AIs follow the law? We often argue that for AIs to be safe and beneficial, we
need to align them with human values. However, some argue that simply getting AIs to follow the
law is a better solution.
The law has three features that give it an advantage over ethics as a model for safe and beneficial AI.
In democratic countries, the law is a legitimate representation of our moral codes: at least in theory,
it is a democratically endorsed record of our shared values. Law is time-tested and comprehensive;
it has developed over many generations to adjudicate the areas where humans have consequential
disagreements. Finally, legal language can be specific and adaptable to new contexts, comparing
favorably to ethical language, which can often be interpreted in diverging ways.
The next subsection will expand on these features of the law. We will see how these features contrast
favorably with ethics before arguing that we do need ethics after all but alongside the law.

6.2. LAW
291
6.2.1
The Case For Law
Legitimate Aggregation of Values
In a democratic country, the law is influenced by the opinions of the populace.
Democratic
citizens vote on new laws, directly or through representatives. If they don’t like part of an existing
law, they have a range of legal means, such as advocating, protesting, and voting, to change it. Even
though the law at any given time won’t perfectly reflect the values of the citizenry, the method of
arriving at law is usually legitimate. In other words, the citizens have input into the law.
Legitimacy provides the law with a clear advantage over ethics.
The law provides a
collection of rules and standards that enable us to differentiate illegal from legal actions. Ethics, on
the other hand, isn’t standardized or codified. To determine ethical and unethical actions, we have
to either pick an ethical theory to follow, or decide how to weigh the differing opinions of multiple
theories that we think might be true. But any of these options are likely to be more controversial
than simply following the law. Ethics has no in-built method of democratic agreement.
However, just following the law isn’t a perfect solution: there will always be an act of interpretation
between the written law and its application in a particular situation. There is often no agreement
over the procedure for this interpretation (for example, in US constitutional cases). Therefore, even
if AI systems were created in a way that bound them to follow the law, a legal system with human
decision-makers would have to remain part of the process. The law is only legitimate when interpreted
by someone democratically appointed or subject to democratic critique.
Time-Tested and Comprehensive
Systems of law have evolved over generations. With each generation, new people are given
the job of creating, enforcing, and interpreting the law. The law covers a huge range of issues and
incorporates a wide range of distinctions. Because these bodies of law have been evolving for so long,
the information encoded in the law is a record of what has worked for many people and is often
considered an approximation of their values. This makes the law a particularly promising resource
for aligning AI systems with the values of the population.
Rules and Standards
Naively, we might think of the law as a system of rules.
“Law” seems almost synonymous
with “rule” in our language. When we talk about “the laws of physics” or “natural laws” in general, we
mean something rigid and inflexible—when X happens, Y will follow. This is partly true: inflexible
rules are a part of the law. For instance, take the rule: “If someone drives faster than the speed limit
of 70mph, they will be fined $200.” In this case, there is an objective trigger (driving faster than
70mph) and a clear directive (a $200 fine). There isn’t much room for the judge to interpret the rule
differently. This gives the lawmaker predictable control over how the law will be carried out.
A law based on rules alone would be flawed.
However, a fixed speeding rule would mean fining
someone who was accelerating momentarily to avoid hitting a pedestrian and not fining someone
who continued to drive at the maximum speed limit around a blind turn, creating a danger for other
drivers. Rules are always over-inclusive (they will apply to some cases we would rather not be illegal)
and under-inclusive (they won’t apply to all cases we would like to be illegal).
To remedy this problem, a law can instead be based on a standard.
In the speeding case,
a standard could be “when someone is driving unreasonably, they will be fined in proportion to the
harm they pose.” A judge could apply this standard to get the correct result in both cases above

292
CHAPTER 6. MACHINE ETHICS
(speeding to avoid an accident and going full speed around a blind turn). Standards have their own
problems: with standards rather than rules, the judge is empowered to interpret the standards based
on their own opinion, allowing them to act in ways that diverge from the lawmaker’s intentions.
The law uses rules and standards.
Using rules and standards alongside each other, the law
can find the best equilibrium between carrying out the lawmaker’s intentions and accounting for
situations they didn’t foresee [1]. This gives the law an advantage in the problem of maintaining
human control of AI systems by displaying the right level of ambiguity.
Law is less ambiguous than ethical language, which can be very ambiguous. Phrases like “do the
right thing”, “act virtuously” or “make sure you are acting consistently” can mean different things to
different people. In contrast, it is more flexible than programming languages, which are brittle and
designed to only fit into particular contexts. Legal language can maintain a middle ground between
rigid rules and more sensible standards.
Figure 6.1: Legal language balances being less ambiguous than ethical language while being more
flexibly formatted than programming language.
Specific and Adaptable
We can apply these insights about rules and standards in law to two core problems in controlling AI
systems: misinterpretation and gaming. The law is specific enough to avoid misinterpretation and
adaptable enough to prevent many forms of gaming.
Given commands in natural language, AIs might interpret them literally.
The
misinterpretation problem arises when the literal interpretation of our commands differs from
our intended interpretation. In AI safety discourse, we see this problem raised by many thinkers; for
example, Stuart Russell raised the concern that an AI, if asked to develop a cure for cancer, might
experiment on people, giving them tumors as part of carrying out the request [2]. A narrow, literal
interpretation of “develop a cure for cancer,” which doesn’t take any of our typical considerations

6.2. LAW
293
into account, could lead to this outcome. Misinterpretation risks are like wishes to a genie: we might
get what we ask for but not what we want.
The capabilities of LLMs give us some reason to see misinterpretation risks as unlikely.
Before LLMs, some AI researchers were concerned that most AI models would be trained in ways that
would mean they had no understanding of many ordinary human concepts or values. However, this
now seems less likely. Our experience with large language models has shown that by being trained
on human-generated language data, AIs can respond to the meaning of our sentences in a way that
is approximately similar to the way that a human speaker of the language would; for instance, the
Happiness section below discusses two other systems that can predict human responses to video and
text. If similar systems are used in the future, it seems plausible that AIs can apply laws in a sensible
way, and we may not need to worry about extreme actions emerging from a misinterpretation of
perfectly normal requests.
That said, we might still worry that these emerge from optimization pressures. Additionally, while
AIs might usually interpret human concepts sensibly, they may still have representations that have
rare quirks. This poses problems; for instance, it creates vulnerabilities that can be exploited by
adversarial attacks. There is more work to be done before we can feel comfortable that systems will
reliably be able to interpret laws and other commands in practice.
However, we still face the risk of AIs gaming our commands to get what they want.
Stuart Russell raises a different concern with AI: gaming [2]. An AI system may ‘play’ the system
or rules, akin to strategizing in a game, to achieve its objectives in unexpected or undesired ways.
He gives the example of tax codes. Humans have been designing tax laws for 6000 years, yet many
still avoid taxation. Creating a tax code not susceptible to gaming is particularly difficult because
individuals are incentivized to avoid paying taxes wherever possible. If our track record with creating
rules to constrain each other is so bad, then we might be pessimistic about constraining AI systems
that might have goals that we don’t understand.
A partial solution to misinterpretation and gaming is found in rules and standards.
If
we are concerned about misinterpretation, we might choose to rely on laws that are specific. Rules
such as “any generated image must have a digital watermark” are specific enough that they are
difficult to misinterpret. We might prefer using such laws rather than relying on abstract ethical
principles, which are vaguer and easier to misinterpret.
Conversely, if we are concerned about AIs gaming rules, we might prefer to have standards, which
can cover more ground than a rule. A well-formulated standard can lead to an agent consistently
finding the right answer, even in new and unique cases. Such approaches are sometimes applied in
the case of taxes. In the UK, for example, there is a rule against tax arrangements that are “abusive.”
This is not an objective trigger: it is up to a judge to decide what is “abusive.” An AI system trained
to follow the law can be accountable to rules and standards.
6.2.2
The Need for Ethics
This subsection will discuss why ethics is still indispensable in creating safe and beneficial AI, even
though law is a powerful tool. Firstly, though the law is comprehensive, there are important areas of
human life where it gives no advice. Secondly, it is common for laws to be immoral, even by the
standards of the residents of the country that made them. Finally, the law is unlikely to be the most
legitimate conceivable system, even if it is our best one.

294
CHAPTER 6. MACHINE ETHICS
Silent Law
The law is a set of constraints, not a complete guide to behavior. The law doesn’t cover
every aspect of human life, and certainly not everything we need to constrain AI. Sometimes, this is
accidental, but sometimes, the law is intentionally silent. We can call these zones of discretion: areas
of behavior that the law doesn’t constrain; for example, a state’s economic policy and the content of
most contracts between private individuals. The law puts some limits on these areas of behavior.
Still, lawmakers intentionally leave free space to enable actors to make their own choices and to avoid
the law becoming too burdensome.
The law often stops short of compelling us to do clearly good things.
It is generally seen
as good if someone steps in to rescue a stranger who is in danger. However, in some jurisdictions
like the US, the duty to rescue only applies in certain cases where we have special responsibility
for someone. We face a penalty if we fail to rescue our spouse or employee, but there is no law
against failing to rescue a stranger, even when it would be at no cost to us. Giving humans this kind
of discretion may not have terrible outcomes, because many people have strong ethical intuitions
anyway, and would carry out the rescue. But an AI that was only following the law would not unless
it had a strong ethical drive as well. In cases like rescue, where AI systems may be more capable
than humans to help, we could be passing up a major benefit by asking the AI to only follow the law.
Likewise, in the US, AIs may be required to avoid uttering copyrighted content and libelous claims,
but doxing, instructions for building bombs, or advice for how to break the law can be legal, even if
it is not ethical.
Conversely, by constraining AI with laws rather than guiding it with ethics, we risk it acting in
undesirable ways in zones of discretion. AI could recommend potentially harmful economic policies,
trick humans into regrettable contracts, and pursue legal but harmful business practices. Without
ethics, a law-abiding AI could carry out great harm because the law is silent in situations where we
may want to guide behavior nonetheless.
Immoral Law
The law can be immoral even if it is created legitimately and interpreted correctly.
Democratic majorities can pass laws that a large number of fellow citizens think are immoral, or
that will seem immoral to future populations of that country, such as the legalization of wars later
regretted, legal slavery, legal discrimination on the basis of race and gender, and various other
controversial laws which are later deemed morally wrong. There is also often a significant time delay
between the moral opinions of a population changing, and the law changing to reflect them [3]. This
means that laws formed in democracies can fall short of being moral, even in the eyes of the citizens
of the country that made them.
If solely constrained by a country’s fallible democratic laws, future AI systems could behave in ways
most of the world would consider immoral. To mitigate this risk, it is necessary to train AI to respect
the ethical perspectives of those affected by its actions.
Unrepresentative Law
Law isn’t the only way, or even necessarily the best way, to arrive at an aggregation of
our values. Not all judges and legal professionals agree that the law should capture the values of
the populace. Many think that legal professionals know better than the public, and that laws should
be insulated from the changing moral opinions of the electorate. This suggests that we might be able
to find, or conceive of, more representative ways of capturing the values of the populace. Current
alternatives like surveys or citizens’ assemblies are useful for some purposes, such as determining

6.2. LAW
295
preferences on specific issues or arriving at informed, representative policy proposals. However, they
aren’t suited to the general task of summarizing the values of the entire population across areas as
comprehensive as those covered by the law.
Perhaps for this general task we could turn to solutions in the moral uncertainty literature. The
problem of moral uncertainty is precisely the problem of figuring out how to act, given the range of
differing views on ethics. A solution from the moral uncertainty literature which best embodies the
democratic ideal is the moral parliament approach, which we will discuss later in this chapter.
A Note On The Three Laws of Robotics
Some propose that Isaac Asimov’s “Three Laws of Robotics” provide a useful set of rules for
resolving problems in machine ethics [4, 5]. These laws are as follows:
1) “A robot may not injure a human being, or through inaction, allow a human being to
come to harm,”
2) “A robot must obey the orders given to it by a human being except where such orders
would conflict with the First Law,” and
3) “A robot must protect its own existence as long as such protection does not conflict
with the First or Second Laws.”
The idea that these laws provide a solution to challenges in machine ethics is a common
misconception. Asimov himself frequently tested the adequacy of these laws throughout his
writing, showing that they are, in fact, limited in their ability to resolve machine ethics
problems. Below, we explore some of these limitations.
Asimov’s laws are insufficient for guiding ethical behavior in AI systems[6].
The
three laws use under-defined terms like “harm” and “inaction.” Because they’re under-defined,
they could be interpreted in multiple ways. It’s not clear precisely what “harm” means to
humans, and it would be even more difficult to encode the same meaning in AI systems.
Harm is a complex concept. It can be physical or psychological. Would a robot following
Asimov’s first laws be required to intervene when humans are about to hurt each other’s
feelings? Would it be required to intervene to prevent a human from behaving in ways that
are self-harming but deliberate, like smoking? Consider the case of amputating a limb in order
to stop the spread of an infection. An AI programmed with Asimov’s laws would be forbidden
from amputating the limb, as that would literally be an instance of injuring a human being.
However, the AI would also be forbidden from allowing the harmful spread of an infection
through its inaction. These scenarios illustrate that the first law fails, and therefore, that the
following two do not follow. The laws may need to be much more specific in order to reliably
guide ethical behavior in future AI systems.
Philosophy has yet to produce a sufficient set of rules to determine moral conduct. The
safety of future AI systems cannot be guaranteed simply through a set of rules or axioms.
Numerous factors, such as proxy gaming and competitive pressures, cannot be adequately
captured in a set of rules. Rules may be useful, but AI safety will require a more dynamic
and comprehensive approach that can address existing technical and sociotechnical issues.
Overall, Asimov’s Three Laws of Robotics fail to reliably guide ethical behavior in AI systems,
even if they serve as a useful starting point for examining certain questions and problems in
AI safety.

296
CHAPTER 6. MACHINE ETHICS
Conclusions About Law
The law is comprehensive, but not comprehensive enough to ensure that the actions of an AI system
are safe and beneficial. AI systems must follow the law as a baseline, but we must also develop
methods to ensure that they follow the demands of ethics as well. Relying solely on the law would
leave many gaps that the AI could exploit, or make ethical errors within. To create beneficial AI
that acts in the interests of humanity, we need to understand the ethical values that people hold over
and above the law.
6.3
Fairness
We can use the law to ensure that AIs make fair decisions.
AIs are being used in many
sensitive applications that affect human lives, from lending and employment to healthcare and
criminal justice. As a result, unfair AI systems can cause serious harm. Methods for improving
AI fairness could mitigate harms from biased systems, but they require overcoming challenges in
formalizing and implementing fairness. This section explores algorithmic fairness, including its
technical definitions, limitations, and real-world strategies for building fairer systems.
The COMPAS case study.
A famous example of algorithmic decision-making in criminal justice
is the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) software
used by over 100 jurisdictions in the US justice system. This algorithm uses observed features such
as criminal history to predict recidivism, or how likely defendants are to reoffend. A ProPublica
report [7] showed that COMPAS disproportionately labeled African-Americans as higher risk than
white counterparts with nearly identical offense histories. However, COMPAS’s creators argued that
it was calibrated, with accurate general probabilities of recidivism across the three general risk levels,
and that it was less biased and better than human judgments [8]. This demonstrated the trade-off
between different definitions of fairness: it was calibrated across risk levels, but it was also clear that
COMPAS generated more false positives for African-American defendants (predicting they would
re-offend when they did not) and more false negatives for white defendants, predicting they would
not re-offend when they in fact did. Adding to the concern, COMPAS is a black-box algorithm:
its process is proprietary and hidden. One lawsuit argued this violates due process rights since its
methods are hidden from the court and the defendants [9]. In this section, we will discuss some
of the serious ethical questions raised by this case, examining what makes algorithms unfair and
considering some methods to improve fairness.
6.3.1
AI Fairness Concepts
Fairness is difficult to specify.
Fairness is a complicated and disputed concept with no single
agreed-upon definition. Different notions of fairness can come into conflict, making it challenging to
ensure that an AI system will be considered fair by all stakeholders.
Five fairness concepts.
Some concepts of individual fairness focus on treating similar individuals
similarly—for instance, ensuring job applicants with the same qualifications have similar chances of
being shortlisted. Others focus on group fairness: ensuring that protected groups receive similar
outcomes as majority groups. Procedural fairness emphasizes improving the processes that lead to
outcomes, making sure they are consistent and transparent. Distributive fairness concerns the equal
distribution of resources. Counterfactual fairness emphasizes that a model is fair if its predictions
are the same even if a protected characteristic like race were different, all else being equal. These
concepts can all be useful in different contexts.

6.3. FAIRNESS
297
Justice as fairness.
Ethics is useful for analyzing the idea of fairness. John Rawls’ theory of
justice as fairness argues that fairness is fundamental to achieving a more just social system. In the
Normative Ethics chapter Social Contract Theory, we explore his maximin and difference principles,
which state that inequalities in social goods can only be justified if they maximize benefits for the
most disadvantaged people. He also argued that the social goods must be open to all under equality
of opportunity. These ideas align with common notions of fairness. Some argue this principle also
applies to AI: harms from the bias of algorithmic decisions should be minimized, especially in ways
that make the worst-off people better off. Theories of justice can help develop the background
principles for fairness.
Algorithmic fairness.
The field of algorithmic fairness aims to understand and address unfairness
issues that can arise in algorithmic systems, such as classifiers and predictive models. This field’s goal
is to ensure that algorithms do not perpetuate disadvantages based on protected characteristics such
as race, gender, or class, especially while predicting an outcome from features based on training data.
Several different technical definitions of fairness have been proposed, often formalized mathematically.
These definitions aim to highlight unfairness in ML systems, but most possess inherent limitations.
We will review three definitions below.
Statistical parity. The concept of statistical parity, also known as demographic parity, requires that
an algorithm makes positive decisions at an equal rate for different groups. This metric requires
that the model’s predictions are independent of the sensitive attribute. A hiring algorithm satisfies
statistical parity if the hiring rates for men and women are identical. While intuitive, statistical
parity is a very simplistic notion; for instance, it does not account for potential differences between
groups that could justify or explain different outcomes.
Equalized odds. Equalized odds demands that the false positive rate and false negative rate are equal
across different groups. A predictive health screening algorithm fulfills equalized odds if the false
positive rate is identical for men and women. This metric ensures that the accuracy of the model is
not dependent on the sensitive attribute value. However, enforcing equalized odds can reduce overall
accuracy.
Calibration. Calibration measures how well predicted probabilities match empirical results. In a
calibrated model, the actual long-run frequency of positives in the real population will match the
predicted probability from the model. For instance, if the model predicts 20% of a certain group will
default on a loan, roughly 20% will in fact default. Importantly, calibration is a metric for populations,
and it does not tell us about the correctness or fairness of an ML system for individuals. Calibration
can improve fairness by preventing incorrect, discriminatory predictions. As it happens, ML models
often train on losses that encourage calibration, and are therefore often calibrated naturally.
These technical concepts can be useful for operationalizing fairness. However, there is no single
mathematical definition of fairness that matches everyone’s complex social expectations. This is
a problem because satisfying one definition can often violate others: there are tensions between
statistical notions of fairness.
6.3.2
Limitations of Fairness
There are several problems with trying to create fair AI systems. While we can try to improve models’
adherence to the many metrics of fairness, the three classic definitions of fairness are mathematically
contradictory for most applications. Additionally, improving fairness is often at odds with accuracy.
Another practical problem is that creating fair systems means different things across different areas of

298
CHAPTER 6. MACHINE ETHICS
applications, such as healthcare and justice, and different stakeholders within each area have different
views on what constitutes fairness.
Contradictions between fairness metrics.
Early AI fairness research largely focused on three
metrics of fairness: statistical/demographic parity, equalized odds, and calibration. However, these
ubiquitous metrics often contradict each other: statistical parity only considers overall prediction
rates, not accuracy, while equalized odds focuses on accuracy across groups and calibration emphasizes
correct probability estimates on average. Achieving calibration may require violating statistical parity
when the characteristic being predicted is different across groups, such as re-offending upon release
from prison being more common among disadvantaged minorities [10]. This makes fulfilling all three
notions of fairness at once difficult or impossible.
The impossibility theorem for AI fairness proves that no classifier can satisfy these three definitions of
fairness unless the prevalence of the target characteristic is equal across groups or prediction is perfect
[11, 12]. Requiring a model to be “fair” according to one metric may actually disadvantage certain
groups according to another metric. This undermines attempts to create a universally applicable,
precise definition of fairness. However, we can still use metrics to better approximate our ideals of
fairness while remaining aware of their limitations.
Fairness can reduce performance if not achieved carefully.
Enforcing fairness constraints
often reduces model accuracy. Two papers found that applying fairness techniques to an e-commerce
recommendation system increased financial costs [13] and mitigating unfairness in Kaggle models by
post-processing reduced performance [14]. However, these and others also find ways to simultaneously
improve both fairness and accuracy; for example, work on healthcare models has managed to improve
fairness with little effect on accuracy [15]. While aiming for fairness can reduce model accuracy in
many cases, sometimes fairness can be improved without harming accuracy.
Difficulties in achieving fairness across contexts.
Different fields have distinct problems:
fairness criteria that make sense in the context of employment may be inapplicable in healthcare.
Even different fields within healthcare face different problems with incompatible solutions. These
context-specific issues make generic solutions inadequate. Models trained on historical data might
reflect historical patterns such as the underprescription of pain medication to women [16]. Removing
gender information from the dataset seems like an obvious way to avoid this problem. However,
this does not always work and can even be counterproductive. For instance, removing gender data
from an algorithm that matches donated organs to people in need of transplants failed to eliminate
unfairness, because implicit markers of gender like body size and creatinine levels still put women at
a disadvantage [17]. Diagnostic systems without information about patients’ sex tend to mispredict
disease in females because they are trained mostly on data from males [18]. Finding ways to achieve
fairness is difficult: there is no single method or definition of fairness that straightforwardly translates
into fair outcomes for all.
Disagreements in intuitions about fairness.
There is widespread disagreement in intuitions
about the fairness of ML systems, even when a model fulfills technical fairness metrics; for instance,
patients and doctors often disagree on what constitutes fairness. People often view identical decisions
as more unfair if they come from a statistical model [19]; they also often disagree on which fairness-
oriented features are the most important [20], such as whether race should be used by the model or
whether the model’s accuracy or false positive rates are more important. It is unclear how to define
fairness in a generally acceptable way.

6.3. FAIRNESS
299
6.3.3
Approaches to Improving Fairness
Due to the impossibility theorem and inconsistent and competing ideas, it is only possible to pursue
some definition or metric of fairness—fairness as conceptualized in a particular way. There are two
fundamental and complementary ways to approach this more limited goal: technical approaches that
focus directly on algorithmic systems, and social approaches that focus on related social factors.
Technical approaches.
Metrics of fairness such as statistical parity identify aspects of ML systems
that are relevant for fairness. Technical approaches to improving fairness include a host of methods to
improve models’ performance on these metrics, which can mitigate some forms of unfairness. These
often benefit from being broadly applicable with little domain-specific knowledge. Developers can
test predictive models against various metrics for fairness and adjust models so that they perform
better. Fairness toolkits offer programmatic methods for implementing technical fairness metrics
into ML pipelines. Other methods for uncovering hidden sources of unfairness in ML models include
audits, adversarial testing, sensitivity analysis, and ranking feature importances.
Problems with technical approaches.
However, technical methods fall short of addressing
the social consequences of unfairness. They fail to adjust to sociocultural contexts and struggle
to combat biases inherited from training data. “Fairness through unawareness” aims to remove
protected characteristics like gender and race from datasets to prevent sexism and racism, but fails
miserably in practice because this data is embedded in correlates [as we explored in the Bias section
in Single-Agent Safety chapter]. A focus on narrow measures can ignore other relevant considerations,
and measures are often subject to proxy gaming 3.4. A more in-depth, qualitative, and socially
grounded approach is often harder and does not scale as easily as technical methods, but it is still
essential for navigating concerns in AI fairness.
Social approaches.
Social approaches emphasize that unfairness is tied to systemic social injustices
propagated through technical systems. They highlight political, economic, and cultural factors and
apply methods such as anti-discrimination policy, legal reform, and a design process focused on values
and human impacts. These methods, which include policies like developing AI systems with input
from stakeholders, can surface and mitigate sources of unfairness early. Substantive social changes
are generally more expensive and difficult than technical approaches. However, they can be more
impactful, reducing models’ negative social impacts.
Frameworks for fairness.
Ongoing research aims to create frameworks for AI fairness that blend
technical and social considerations. Fair deployment frameworks focus on gradually deploying AI
while assessing its impacts, ensuring there are strong oversight mechanisms and independent audits,
and establishing methods to redress any harm. Audits can promote an understanding of the data
sources and decision-making processes of ML models. This transparency is helpful but insufficient,
as we saw in the Single-Agent Safety chapter 3.3. Participatory design frameworks involve all of the
impacted communities in the development of AI systems to surface unfairness early and ensure the
product meets diverse human needs. Other frameworks focus on how hierarchies and distributions of
power can intersect to produce unfairness or evaluating how AI affects the least advantaged in society.
These broad frameworks recognize that effective solutions require combining technical interventions
with social and legal policies to address underlying social factors.
Conclusion
Technical definitions of fairness are useful tools, but improving the fairness of AI necessitates broader
approaches. There are inherent tradeoffs between fairness criteria which means that satisfying all the
popular definitions of fairness is logically impossible. Additionally, satisfying some mathematical

300
CHAPTER 6. MACHINE ETHICS
definitions does not guarantee fair outcomes anyway. Developing fair AIs might require broader
sociotechnical solutions such as the participation of stakeholders in deciding the tradeoffs a model
makes between various fairness criteria. Research on algorithmic fairness continues to influence how
AI systems are built and used. In the next section, we will examine how AI systems are currently
being built and used, focusing on the implicit goal of wealth maximization within the Economic
Engine that creates and shapes AIs. We will also consider a few days that the economy might, in
turn, be transformed by AIs as well.
6.4
The Economic Engine
What if we allow the economy to decide what AIs will be like?
Most AI systems today
are being developed by businesses. We could therefore argue that AI development is most closely
aligned to business, or economic, goals like wealth maximization, as opposed to improvements in
social wellbeing. Many argue that AIs should maximize economic measures like wealth or GDP. It
is also increasingly likely that AIs will transform the economy by enabling new forms of business,
changing the nature of work, and even displacing human workers. In this section, we will examine
whether AIs shaped by economic incentives create the sort of society most people want. We examine
both how the economy shapes the development of AI, and how AI might impact the economy.
Business goals influence the development of the most capable AI systems.
AIs are
increasingly more efficient than humans at tasks once considered difficult to automate. In the customer
service industry, for example, many businesses rely on chatbots instead of human representatives.
Chatbots reduce expenses and streamline operations, ultimately increasing profits. The current trend
towards automation suggests that many of the most powerful AI systems will be deployed to serve
specific business objectives.
Whether this is a reason for optimism or concern depends on to what extent we think that the
economy, left to its own devices, will produce good outcomes. On one hand, AI has the potential to
enhance efficiency and foster competition, which can lower consumer costs and spur the creation of
innovative products. However, business-oriented AIs may focus narrowly on achieving their specific
goals, compromising values such as community, personal freedom, privacy, and consumer choice. We
can draw from economics to examine how AI usage in business contexts could either support or
disrupt societal wellbeing.
Using AI to maximize profits creates wealth.
In a market system, businesses often prioritize
maximizing profits because the primary responsibility of company executives is to increase value
for shareholders. This concept, called shareholder primacy, has been codified both implicitly and
explicitly into American corporate law [21]. In theory, corporations that explicitly prioritize other
values such as safety or social responsibility above profit in the long run can be sued for neglecting
their obligation to maximize shareholder value. This profit—centered approach means that—without
effective directed state intervention—most AI systems will not first and foremost be used to help us
create a society that satisfies broader human values.
Consider recommendation algorithms on streaming platforms. These AI systems are designed to keep
us engaged for as long as possible, thereby increasing ad revenue and profit for the platform. Since
these algorithms prioritize viewer engagement over everything else, they may recommend content that
fuels addiction or spreads misinformation. In other words, while these algorithms may be effective
from a business standpoint, they may have negative impacts on individuals and society at large.
While wealth creation is generally good, there are serious implications of prioritizing profit over all
other values.

6.4. THE ECONOMIC ENGINE
301
The economy is the backbone of our daily lives.
The economy is not just a network of
businesses and transactions: it touches every aspect of our lives. Every transaction, small or large
that we make, is impacted by economic factors. At a highly simplified level, the economy determines
the prices of goods and services and the incomes earned by workers. Consider how these two factors
affect the decision to make a large purchase, such as a house. The price of a house (most often paid
as a mortgage) is shaped by interest rates, as well as the level of competition to buy and sell houses.
A person’s income determines their ability to make the purchase, and is influenced by economic
forces like labor markets and company profits.
As businesses continue to leverage AI for economic gain, these systems become ingrained in our
economic fabric. For example, AI is increasingly used by banks to assess credit risk and decide who
gets approved for a loan. If this practice becomes more prevalent, AI will no longer be just a tool;
it will determine who can buy a house or run a business. The advance of AI in business affects
everyone’s life at a basic level. The use of AI may lead to improvements, such as lower prices, or
challenges, like widespread job losses due to automation, that impact us fundamentally.
Technology changes the economy and the world.
Throughout history, technology has been a
pivotal driver of economic change. Consider the first agricultural revolution. The invention of farming
tools and techniques transformed societies from hunter-gatherers to settled agricultural communities,
fundamentally altering the economy and human lifestyle. Fast-forward to the mid-1700s, when the
steam engine drove the industrial revolution, sparking rapid economic growth and urbanization and
shifting the economy from primarily agrarian to primarily industrial. Factories became the economic
focal point, creating jobs and spurring the development of new sectors like rail and shipping. More
recently, the internet and smartphones have connected billions of people, breaking down geographical
barriers and enabling new types of businesses like e-commerce and remote work.
Artificial intelligence might be the next transformative technology.
As AI becomes
increasingly critical for businesses, consumers, and governance, it may alter daily life, from work
and education to interpersonal relationships and entertainment. As it evolves and permeates more
sectors, its economic and societal effects of the AI revolution could be as transformative as those
of the agricultural or industrial revolutions [22]. Therefore, understanding AI’s potential risks and
benefits is essential for navigating this new economic landscape.
Preview.
In this section, we discuss how economics and AI interact, using both theoretical models
of economies that make strong, idealized assumptions and empirical data collected from real-world
economies. In many places, we will introduce and explain basic concepts from economics, such as
gains from trade and how growth benefits society. This background content is an essential foundation
for discussing the dramatic impact that AI systems might have on our societies. For those already
familiar with economic theory, it serves as a reminder and an explanation of how these concepts
can be relevant to AI. We use these tools to address questions around AI’s economic effects. For
example, to what extent should we expect AI to prove beneficial (for example due to efficiency gains)
or harmful (due to inequality and market failures). Will the benefits of AI systems mostly accrue in
the form of wealth increases for their owners, or will they also increase social wellbeing and happiness
in general? Throughout, we will examine how economic incentives can influence the development of
AI systems, and thus shape their impact on society.
6.4.1
The Free Market
Under ideal circumstances, competitive economic markets drive efficiency and foster collective
prosperity. One can speculate that the integration of AI into the economic engine could further
optimize efficiency and enhance competition, ultimately contributing to accelerated economic growth

302
CHAPTER 6. MACHINE ETHICS
and improved wellbeing. In this subsection, we will revisit introductory concepts from economics to
examine the case that AIs (developed in accordance with market incentives) can create widespread
benefits by improving efficiency and growth.
A Note on Gains from Trade
We can specialize and trade, making everyone better off. Trade is often seen as a
win-win scenario, allowing for mutual benefit. Take a simple scenario. Suppose Alice and
Bob are stranded on an island. Each of them has one skill: Alice is excellent at gathering
coconuts for water, while Bob is especially good at catching fish to eat. Suppose that, left
to their own devices, Alice would struggle to have food to eat, and Bob would struggle to
have coconut water to drink. However, if Alice and Bob agree to specialize in their respective
skills and then trade, they can both focus on doing what they are good at, wasting less time
on individually unproductive activities and therefore producing more food in total, and each
ending up with more food and drink than if they had worked alone.
Moreover, even if Alice is better than Bob at both gathering coconuts and catching fish, they
will still benefit from specialization. Suppose Bob is better at fishing than he is at gathering
coconuts. Even though he’s not as good as Alice at either, they’ll still produce more overall if
Alice focuses on coconuts and Bob on fishing. In most cases, trading makes people better off.
One role of the economy is to enable such trades on a large scale.
Markets coordinate trade, allowing gains from specialization.
A market is a venue for
trade, a system that enables the exchange of goods and services. It can be a physical location or
through any other matching mechanism like e-commerce platforms. Markets allow everyone to benefit
from high levels of specialization and reap the benefits of trade. In the global marketplace, few people
have to farm their own food, build their own houses, and make their own clothes, because everyone
trades their time and skills for goods and services created by others. Markets can be thought of as a
grand collaboration, facilitating cooperation among millions of people across the globe.
Markets encourage a complex blend of competition and collaboration.
Companies like
Toyota and Ford compete to sell their cars to consumers. However, they also often collaborate on
research and development projects. This cooperation can result in advancements like more efficient
engines or safer car designs, benefiting both companies and their customers. In general, while firms
within the same industry compete against each other for market share, they also cooperate with
strategic partners, suppliers, and vendors for mutual benefit. Cooperation helps companies optimize
resources and improve products or services while competition drives innovation and offers consumers
more choices.
Prices enable the economy to be an adaptable, self-organizing complex system.
Prices
in a market economy are signals of changing demand and supply, created by individual agents acting
in accordance with their incentives. As an example, let’s trace how the high oil prices in the 1970s
reorganized the global flower market. In 1971, the price of oil increased dramatically, signaling that
oil had become more scarce. This price signal gave individuals and businesses an incentive to find
ways to reduce spending on oil or develop substitutes. The higher cost of heating greenhouses using
oil led to flower suppliers reducing their production, causing the price of roses and other flowers to
increase. In turn, consumers turned to substitutes like chocolate and teddy bears for Valentine’s Day
gifts. In the meantime, opportunistic entrepreneurs began thinking of alternative, cheaper ways to
produce flowers. They encouraged farmers in Kenya and Ecuador to grow roses using natural heat,

6.4. THE ECONOMIC ENGINE
303
and invested in a global infrastructure to deliver roses worldwide; as a result, they started using land
for roses instead of coffee, which saw an increase in the price of coffee.
In these cases information in the market, primarily transmitted through prices, efficiently coordinated
the actions of individuals and businesses, allowing them to use their own knowledge and ingenuity
to make optimal business decisions in a way that no individual or government would be able to do.
The production of oil, flowers, chocolates, teddy bears, coffee, and many other goods was adjusted in
order to create and deliver the most value possible given the available resources and information.
AIs developed by actors in a free market can create widespread benefits.
Market forces
create incentives for entrepreneurs and firms to rapidly develop a wide range of services that use AIs
to meet the needs of many different types of consumers. In the wake of the release of early LLMs, for
instance, hundreds of startups were created to improve consumers’ ability to achieve a range of tasks.
AlphaFold’s public release accelerated research in biology and biotechnology. These leveraged the
entrepreneurial spirit and capacity for innovation of many people around the world. Many people are
concerned that regulating this process could stop breakthroughs in accelerating economic growth or
extending human lifespans.
Efficiency
AIs can increase allocative efficiency, ensuring that outcomes are efficient and contribute to social
welfare.
The free market creates allocative efficiency.
The First Fundamental Theorem of Welfare
Economics states that—subject to some strong assumptions—an equilibrium allocation of goods
reached by trading on a free market must be Pareto efficient. An outcome is Pareto efficient if there
is no way to make anyone better off without making someone else worse off: any change must trade
off one person’s welfare against another. If the allocation of goods were not Pareto efficient, then
individuals within it would trade in a way that exploited the possible Pareto improvement, reaching
a state in which no mutually beneficial trades can be made.
This supports Adam Smith’s famous “Invisible Hand” argument, which suggests that when individuals
pursue their self-interest within a market, they unintentionally contribute to societal welfare. By
creating gains from trade, there is an overall improvement in living standards for everyone.
There are many conditions required for the invisible hand argument to hold.
Without
the following conditions, the invisible hand argument—which holds that the presence of free markets
with individuals acting in self-interest can create outcomes that are Pareto efficient—would fail.
1. There must be an open market. There should be no barriers to entry for producers or
buyers, so that everyone can participate in this market. This openness stimulates competition,
promoting economic efficiency.
2. There should be no seller big enough to move prices up alone. If there are many
sellers, then anyone who raises prices will lose consumers. There must be no monopoly power:
anyone with the ability to raise prices without being forced down by competition will create
distortions that leave consumers worse off.
3. No producer should privately hold a pivotal technology.
This means that other
producers should be able to copy the production of the first-mover. While the first-mover will
make profits in the short run, in the long run the market allocation will be Pareto optimal.

304
CHAPTER 6. MACHINE ETHICS
4. No buyer should be big enough to move prices down alone. Similar to the condition
for producers, no buyer should be big enough to force producers to take lower prices than
others would offer for them. Such buyers would create distortions that might, for instance,
force producers out of business, ultimately harming everyone.
5. There must be perfect information for everyone. Producers and consumers must have
access to perfect information, such as about product quality and pricing. If consumers don’t
know, for instance, that a seller’s product is defective or that other sellers are offering lower
prices, then markets cannot achieve efficiency.
6. Preferences must be non-satiated. The first welfare theorem does not hold for all types of
preferences; one technical restriction is that consumers should always prefer more of a good. If
someone gains no further value from extra consumption, their trading equilibria may not be
Pareto efficient.
7. There must be no externalities in consumption. When one consumes a good, there
must be no effect—positive or negative—on anyone else. Second-hand smoke has a negative
externality on anyone nearby but this is unaccounted for in the price of cigarettes, creating
inefficiency.
8. The state enforces property and contract laws. Most economic theory assumes that
contracts are enforceable, and that individuals and corporations have protected property rights.
Without these, trading would be difficult to achieve and much more costly, making it more
difficult for everyone to achieve optimal outcomes.
If the conditions for the invisible hand theorem always held, then we might be more optimistic
about AI’s role in enhancing social welfare. If companies use AI to boost productive efficiency,
lower costs, and improve products, then these actions should eventually translate into widespread
advantages. Lower prices, improved products, and better services could enhance consumer wellbeing,
while increased efficiency could stimulate economic growth. Later in this section, we will look at
market failures and inequality: two reasons why free trade is insufficient to create social welfare. For
now, we will focus on the benefits.
Competition fosters productive efficiency and growth.
Markets continuously incentivize the
lowering of prices and the improvement of quality. Let’s imagine two grocery stores in a town. If one
store starts to offer lower prices, customers may flock to that store, putting pressure on the other
store to match those prices or risk losing business. This encourages firms to be more productively
efficient–—to produce goods at lower costs–—so that they can lower their prices while still making a
profit. This competition can lead to lower prices or higher quality products for consumers and more
attractive employment options for workers.
By leveraging AI, businesses can become more competitive by increasing their productive efficiency.
For example, many companies now use AI-driven chatbots to handle customer service inquiries.
These chatbots can manage numerous queries simultaneously, around the clock, and without human
error, reducing the need for large customer service teams and significantly reducing costs. AI can
automate tasks that were traditionally labor-intensive, leading to considerable savings. Furthermore,
AI can enhance decision-making processes. For instance, companies can use AI to analyze large
volumes of data and generate insights about market trends, customer preferences, and operational
efficiencies. Such insights can inform strategic decisions, providing a competitive advantage.
Given the extensive benefits of AI, its adoption by businesses is likely to continue to grow. Firms that
do not leverage AI may find themselves at a disadvantage, struggling to keep up with competitors
that have harnessed the power of AI to optimize their operations and enhance their offerings. In the

6.4. THE ECONOMIC ENGINE
305
competitive business world, the ability to adapt and adopt new technologies like AI is not merely an
option–—it’s essential for survival.
Growth
AIs have the potential to drive economic growth, which can make everyone better off without
compromising value pluralism and improve society across correlated domains like health, freedom, and
education.
Growth is widely considered essential to a healthy society.
Economic growth is an increase
in the total value of the production of goods and services. Such growth is a vital sign of a thriving
economy. An expanding economy usually indicates more wealth, opportunities, and advancements,
leading to overall improvements in living standards. Growth plays a key role in sparking innovation
and entrepreneurship by creating incentives for companies to invest in ways to accelerate production
and additional funding available for research and development. Growth encourages the creation of
new ventures, products, and ideas.
Economic growth typically brings about improved employment opportunities and better consumer
products. A growing economy fosters job creation, providing more people with income and stability;
for example, the tech industry’s growth led to a boom in job opportunities in fields like software
development, data analysis, and AI research. As companies grow and industries expand, businesses
can invest more in research and development, leading to new and improved products; for instance,
the smartphone industry’s growth has led to a variety of advanced phones with innovative features
that have given billions of people access to the global economy. We generally consider economic
growth desirable because it means we get wealthier over time.
Growth might let us promote value pluralism.
Value pluralism is the notion that multiple
important values often conflict and that there’s no easy way to rank them in order of importance. For
instance, a community might debate whether to spend its limited resources on building a new hospital
or a cultural center. Some community members might argue that healthcare is the most urgent need,
while others might say that arts and culture are equally important for the community’s wellbeing.
With sustained economic growth—and therefore increased wealth over time–—the community might
not need to make such tough choices. Instead of selecting a hospital or a cultural center, the
community could build both. Growth can enable Pareto improvements: changes that improve
people’s lives without leaving anyone worse off; it is like growing a pie to give everyone bigger slices.
The increased wealth effectively allows for the accommodation of multiple values without prioritizing
one over the other.
Economic growth, while advantageous, doesn’t resolve all issues stemming from value pluralism. It
often prioritizes efficiency over equity, and wealth over values such as environmental sustainability.
People’s unique experiences, cultures, and beliefs shape their values, so debates about fair resource
distribution or balancing freedoms with obligations could still arise in a resource-abundant society.
Despite not being a comprehensive solution for value pluralism, economic growth provides a helpful
approach to achieving broader societal goals.
We have strong reasons for encouraging economic growth.
Looking at the world over the
last 200 years provides strong evidence that economic growth can lead to vast improvements in
human life through health, freedom, education, and more.

306
CHAPTER 6. MACHINE ETHICS
50
60
70
80
0
20000
40000
60000
GDP per Capita
Life Expectancy
The Preston Curve
Figure 6.2: Increases in GDP per capita strongly correlate with increases in life expectancy. [23]
The Preston curve is compelling evidence of a positive correlation between a country’s gross domestic
product (GDP) per capita—a measure of the total production of goods and services–—and health
outcomes. This relationship illustrates that countries with higher GDP per capita have better health
outcomes, and particularly that poorer countries stand to benefit immensely from improvements in
GDP. This also holds across time: average global life expectancy was just below 40 years at the start
of the 20th century, whereas today, with a much higher global GDP per capita, the average person
expects to live for 70 years. Nobel laureate Amartya Sen suggests that one pathway through which
growth improves health is by reducing poverty and increasing investments in healthcare [24].
The benefits of economic growth extend beyond physical health. There is a strong association
between economic prosperity and enhancements in freedom and education [25]. Economic growth
can facilitate better educational opportunities by enabling more funding for schools, teacher salaries,
and educational resources. This, in turn, can boost literacy rates, educational attainment, and,
ultimately, the knowledge capital of a society. With more resources at their disposal, people have
more options and opportunities. They can pursue varied careers, invest in personal development, and
engage more actively in civil society. Moreover, prosperous societies can afford stronger institutions
to safeguard democratic freedoms and human rights. Additionally, economic growth can nurture the
arts. As societies become wealthier, more resources can be allocated to support cultural institutions,
artists, and creative endeavors. This can lead to a flourishing of the arts, enriching societal culture
and enhancing individual life experiences.
Compound growth is powerful.
The effect of small increases in economic growth becomes
apparent when we consider the effects of compound growth over time. Even small differences in
annual growth rates can profoundly impact future generations. For instance, if an economy grows
at 1% per year, our grandchildren would have twice our current income in 70 years. However, if
the economy grows at 2% per year, our children’s income in 35 years would be double ours and our

6.4. THE ECONOMIC ENGINE
307
grandchildren’s income would quadruple. Economic growth has the potential to create a significantly
more prosperous future. Prioritizing growth, therefore, can be a powerful strategy for enhancing
societal wellbeing now and in the future.
Increasing growth through AI seems promising.
AI might be a significant driver of economic
growth in the coming decades. From automating tasks such as grading tests in the education sector to
creating new sophisticated possibilities such as advanced radiology diagnostics in healthcare, AI has
the potential to boost economic performance considerably. In the automobile industry, for instance,
the application of AI to autonomous vehicles could be revolutionary, reducing human error and
increasing efficiency in transport and logistics. The potential for vast economic growth from AI gives
us reason to be optimistic about the future. If economic growth improves health, education, and
overall quality of life–—as we have considered is possible—the AI-driven economy could significantly
enhance societal wellbeing.
However, while growth is correlated with welfare, its benefits are neither predictable nor
uniform.
Growth might be unsustainable, trading off long-term wellbeing for gains today. Even
if we have sustainable growth, we need to consider the distribution of that growth. Will economic
growth from AI benefit everyone or only a select few? AI might create jobs in some sectors and
eliminate them in others due to automation. Therefore, while AI has the potential to boost economic
growth, we need to address risks like the unequal distribution of benefits.
Similarly, efficiency is desirable but insufficient.
While the concept of the “Invisible Hand”
suggests that self-interest and free markets lead to competition and efficiency, these just as often
lead to markets in which competition is eliminated. MBA programs teach business executives how to
avoid Porter’s Five Forces, ensuring that their businesses are in an industry with low competition,
high barriers to entry, customers and suppliers that have weak price-setting power, and low threat
of substitute products. These are contrary to the conditions of the First Fundamental Theorem of
Welfare Economics and therefore undermine efficiency. In The Theory of Moral Sentiments, Adam
Smith himself recognized that self-interest and free markets are sufficient only on paper: he thought
that our concern for others is essential and celebrated our sense of justice, generosity, and public
spirit.
Similarly, metrics of efficiency don’t always tell us whether the economy is contributing to social
welfare; for instance, Pareto efficient distributions can be extremely unequal. As an extreme example,
if one person has all the resources in the economy, no one else can be made better off without taking
away something from that person. Analogously, AIs may worsen income inequality by benefiting
mainly business owners and shareholders at the expense of workers who may be cheaply replaced.
Additionally, AIs may be used to promote monopolistic practices or violate our preferences for
personal privacy in ways that are nonetheless efficient.
In essence, we must recognize that markets are not perfect: they can cause problems such as inequality
and market failures. These and other related issues will be discussed in further detail in the next
section.
6.4.2
Inequality & Market Failures
The previous section on markets and efficiency painted a rosy picture of real-world markets. In
reality, we rarely see markets perform at this ideal standard. Economic growth is often talked about
as an intrinsic good, but is only ever an instrumental one: it is not valuable for itself, but for the
positive effects it has on society. However, even when markets are productively and allocatively

308
CHAPTER 6. MACHINE ETHICS
efficient, creating wealth for everyone, they might impose harmful side effects. Two reasons for this
are that markets generate inequality and are often subject to market failures.
First, we will explore the relevance of inequality in understanding societal wellbeing, what tools can
be used to measure its effects, and why concerns about inequality are not just limited to those at
the bottom. Then, we will consider the potential implications of AI on employment and income
inequality, and how societies across the globe may be impacted by an economy restructured by
AI. We will also examine various types of market failures: instances where markets fail to allocate
goods and services optimally. We will discuss information asymmetries, the monopolistic power of
large firms, and negative externalities—consequences that extend beyond the direct participants of a
trade. All of these undermine the argument that an unregulated market is sufficient for AIs to create
widespread benefits for all.
Inequality
Inequality is on the rise and harms many people, including many of those getting richer. The
widespread use of AIs might sharpen inequality, decreasing social wellbeing.
Most of the world exhibits high levels of inequality.
In economics, inequality refers to
the uneven distribution of economic resources, including income and living conditions. The Gini
coefficient is a commonly used statistical measure of income or wealth distribution within a country.
It is a number between 0 and 1, where 0 represents perfect equality (everyone has the same income or
wealth), and 1 signifies maximum inequality (one person has all the income or wealth, and everyone
else has none). Looking at Gini coefficients, 71% of the world’s population lives in countries with
increasing inequality over the last thirty years [26].
Inequality in the United States is particularly striking.
Figure 6.3 shows that the Gini
coefficient in the US has trended significantly upwards from 1969 to 2019. Over 50 years, the pre-tax
Gini coefficient has increased by 30%, while the post-tax Gini coefficient has risen by 25%, suggesting
that despite redistributive taxation policies, the US income gap has widened substantially. (For
reference, this change in Gini coefficient is the same size as moving from Canada to Saudi Arabia
today. [26]) This increase in the Gini coefficient is evidence of a growing inequality crisis. An
associated fall in social mobility—the ability of an individual to move from the bottom income
bracket to the top—cements inequalities over generations.

6.4. THE ECONOMIC ENGINE
309
Before Tax
After Tax
0%
10%
20%
30%
1970
1980
1990
2000
2010
2020
Year
Change in Gini Coefficient Since 1969
Inequality in the US (1969 to 2019)
Figure 6.3: Inequality in the US (as measured by the Gini coefficient) has risen dramatically over the
last five decades, even adjusting for taxation.
The distribution of gains from growth is highly unequal.
Nearly all the wealth gains over the
past five decades have been captured by the top 1% of income earners, while average inflation-adjusted
wages have barely increased [27]. A RAND Corporation working paper estimated how the US income
distribution would look today if inequality was at the same level as in 1975—the results are in Table
6.1. Suppose my annual income is $15,000 today. If inequality was at the same level as in 1975,
I would be paid an extra $5,000. Someone else earning $65,000 today would instead have been
paid $100,000 had inequality held constant! We can see in the table below that these increases in
inequality have had massive effects on individual incomes for everyone outside the top 1%.
Percentile
Actual
Income
in
1975
Actual
Income
in
2018
Income in 2018 if In-
equality Had Stayed
Constant
25th %
$9,000
$15,000
$20,000
Median
$26,000
$36,000
$57,000
75th %
$46,000
$65,000
$100,000
90th %
$65,000
$112,000
$142,000
95th %
$80,000
$164,000
$174,000
99th %
$162,000
$491,000
$353,000
Top 1% Mean
$252,000
$1,160,000
$549,000
Table 6.1: Real and counterfactual income distributions for all adults with income, in 2018 USD. [28]
Inequality carries serious implications.
For those worst off in an unequal society, the
implications are severe. Beyond obvious problems like an inability to access essentials and maintain a
basic standard of living, there are additional concerns in domains like health. A widening wealth gap
often corresponds with a health gap, where those with fewer resources have poorer health outcomes
due to less access to quality healthcare, lower quality nutrition, and higher stress levels. For instance,

310
CHAPTER 6. MACHINE ETHICS
life expectancy often varies dramatically based on income in unequal societies: the average life
expectancy of the lowest-income classes in America now matches the averages in Sudan or Pakistan
[29].
Everyone, not just the poorest, suffers in an unequal society.
One of the most robust
findings is that unequal societies have higher levels of crime [30]. When inequality is high, so too
are levels of social tension, dissatisfaction, and shame, which can contribute to higher crime rates.
This can lead to a cycle where the fear of crime drives further inequality, as wealthier individuals
and neighborhoods invest in measures that segregate them further from the rest of society, further
increasing inequality and crime rates. A more detailed discussion of this relative deprivation can be
found in the section on Cooperation and Conflict. Inequality is also related to other signs of societal
sickness: worse physical and mental health, increased drug use, and higher rates of incarceration.
Strikingly, inequality is a strong predictor of political instability and violence as well. While it may
seem like those with more wealth are insulated from the negative effects of inequality, in reality they
suffer indirect consequences as well.
AUS
AUT
BEL
CAN
DNK
FIN
FRA
DEU
GRC
IRL
ISR
ITA
JPN
NLD
NZL
NOR
PRT
SGP
ESP
SWE
CHE
GBR
USA
20
40
60
Low
Medium
High
Income Inequality
Homicides per million
Income Inequality and Homicide Rates
Figure 6.4: Countries with higher income inequality tend to have higher homicide rates.

6.4. THE ECONOMIC ENGINE
311
ALB
ARG
ARM
AUT
BEL
BGR
BLR
BOL
BRA
CHN
COL
CRI
CZE
DEU
DNK
DOM
ECU
EGY
ESP
EST
FIN
FJI
FRA
GBR
GEO
GRC
HND
HRV
HUN
IDN
IND
IRL
IRN
ITA
KGZ
LKA
LUX
MDA
MDV
MHL
MKD
NLD
NOR
PAN
PER
POL
PRT
PRY
RUS
SLV
SRB
SVK
SVN
SWE
THA
TUR
UGA
UKR
URY
USA
VUT
ZWE
−2
−1
0
1
Low
Medium
High
Income Inequality
Political Instability
Income Inequality and Political Stability
Figure 6.5: Countries with higher income inequality tend to have higher rates of political instability.
The causes of inequality are disputed.
Renowned economist Thomas Piketty, in his book
“Capital in the 21st Century,” posits that inequality results from the difference between the interest
rate investments in capital receive (r) and the rate of economic growth (g) [31]. Essentially, when the
returns from investing wealth (r) surpass the overall growth rate (g), society tends to become more
unequal. This happens because the rich can profit significantly from investing their existing wealth,
while average individuals cannot realize the same benefits from overall economic growth. However,
Piketty’s theory has met criticisms, especially when considering its application to the future; for
instance, he assumes that the technological frontier can only grow at 1.5% per year, a limitation that
could potentially be overturned by advancements in AI.
Countless other theories seek to explain the rise in inequality. One possibility is that an increased
reward for technical skills has led to a widening gap between those with high-skill jobs and those
without. Other theories point to the decline of labor unions, which traditionally advocated for worker
rights and better wages, as a contributing factor. The driving forces behind rising inequality are
undoubtedly multifaceted, warranting a careful and nuanced examination.
AI might prompt higher returns on investments for the wealthy.
AI could significantly
impact income inequality, intensifying existing disparities. Investments in AI may yield higher
productivity compared to past opportunities, leading to a higher rate of return on capital. Following
Piketty’s theory of inequality, AI might exacerbate inequality if it increases the capital return
rate more than it increases total growth if the wealthy can invest in AIs more than everyone else.
This seems likely: those with existing capital (such as GPUs) are likely to be able to deploy and
benefit from AIs far more than the average person. Additionally, the benefits from AIs might be
concentrated among the owners of AI companies, giving them astronomical amounts of wealth rather
than distributing these benefits widely.

312
CHAPTER 6. MACHINE ETHICS
Automation using AI might increase inequality.
If AIs are more productive than humans, as
is likely to become the case in many industries, automating human labor will increase profits and
returns on investment. Investors would likely opt for AI over human labor since AI presents a more
cost-effective and productive alternative. A broad shift towards AI as a form of capital might prompt
the wealthy to invest more heavily in AI technologies than businesses that rely on human labor.
As businesses transition towards AI, the demand for human labor may decrease, resulting in job
losses. As a result, workers’ bargaining power could be significantly reduced: a surplus of labor could
force wages down and weaken workers’ conditions, benefits, and protections. If a job is automated,
then conventional bargaining tactics like striking are no longer an option. Such a scenario would
contribute to growing inequality and negatively impact the livelihoods of workers.
Although new technologies may create novel job opportunities, the large-scale integration of AI could
render many existing roles redundant. Numerous job sectors are susceptible to replacement by AI.
Unlike in the past, even knowledge-based jobs such as writing and programming are at risk due to
advancements in AI such as large language models, robotics, and computer vision. The advent of self-
driving cars could potentially displace the vast portion of the 5 million Americans driving for a living.
Similarly, advancements in robotics might threaten the employment of the 12 million Americans
working in manufacturing [32]. Since software is cheap to duplicate, AI presents firms around the
globe with a low-cost and scalable alternative to using human labor. Mass unemployment could
result in severe inequality—establishing a divide between the minority who own these transformative
technologies and the majority unable to find work.
Past revolutions have relocated employment, not destroyed it.
A common counterargument
brought up in discussions about the potential impact of AI on employment draws on historical
evidence of technological revolutions. Advocates of this view argue that while past technological
advancements–—like the industrial revolution or the computer age–—did indeed displace certain jobs,
they simultaneously created entirely new types of work. Instead of an overall reduction in employment,
the nature of employment was transformed. This phenomenon, called creative destruction, describes
how outdated industries and jobs are replaced by new, often more efficient ones.
However, human-level AI might destroy employment.
It is important to remember that
past transformative technologies, such as mechanized looms or computers, automated specific tasks
or functions, leaving plenty of room for the workforce to adapt and evolve. For example, the advent
of computers rendered certain roles obsolete, like typists using typewriters. But it also opened up
an array of new job categories—software developers, IT specialists, and data analysts, to name a
few. Therefore, while these changes were disruptive for certain professions, they ultimately led to a
shift in employment rather than mass unemployment. This is likely to continue in the face of AI
development in the short run.
By contrast, human-level AI has the potential to automate tasks across most sectors in the long
run.
This could be through individual AIs excelling in specific tasks or an artificial general
intelligence capable of doing it all.
This situation creates a fundamental difference from past
technological advancements that both helped and displaced human workers. If we can automate any
job inexpensively, it’s unclear where humans would fit into the economy.
Market Failure
AI development is aligned with the economic engine. We have argued that this may be good because
the economy creates efficiency. However, in many cases, markets fail to deliver efficient outcomes.
AIs may amplify or create these failures.

6.4. THE ECONOMIC ENGINE
313
There are a few common types of market failure that might concern us.
While markets
are often efficient and usually yield better outcomes than alternative methods like the allocation
of resources by the government, they are not perfectly suited to all types of trade. Sometimes,
when left to their own devices, markets often fail to allocate resources in the most efficient way,
leading to outcomes that may be unfavorable from a societal standpoint, even if they appear to be
operating smoothly. These issues can be especially amplified in the context of powerful AI systems.
The Collective Action Problems chapter analyzes market failures involving the overuse of common
access resources like fish in the sea. In this section, we consider three more common types of market
failures that are pertinent to our discussion about AI: information asymmetries, monopoly power,
and externalities.
Informational Asymmetries
Information known to only some can create market failures.
Information asymmetry
captures the idea that buyers and sellers have different information regarding the product they
are trading. For instance, buyers are keenly aware of the product’s quality and specifications, and
sellers know their true willingness to pay for the product. Information asymmetry isn’t inherently
problematic and is, in fact, often a positive aspect of market dynamics. We trust specialists to
provide valuable services in their respective fields; for instance, we rely on our mechanics to know
more about our car’s inner workings than we do.
However, issues arise when an imbalance of information is exploited disingenuously. A classic example
of this can be found in the used car market. A dealer may be aware that a car’s axle is rapidly
wearing out, a defect that isn’t immediately noticeable to a potential buyer. By withholding this
information, the dealer could sell the car at a price higher than its true value. The buyer, left in
the dark, may end up facing unexpected repair costs soon after purchase. This is also referred to as
adverse selection: when one party in a transaction uses their access to private information to their
advantage. Here, the car dealer is using their private knowledge of the car’s condition to make a sale
at an unfair profit.
AIs can cause strong informational asymmetries.
AI holds the power to both create and
exploit information asymmetries in unprecedented ways. This capacity can be employed in beneficial
ways, like providing highly personalized services or excellent recommendations. However, it can also
be misused, leading to situations in which those using AIs can manipulate consumers. AI-powered
analytics allows companies to create sophisticated profiles of consumers that can uncover deep insights
into individual personalities and behaviors. [33] Big tech companies already use social media and
device activity to understand an individual’s preferences and vulnerabilities better than ever before.
This knowledge can be cunningly used to shape targeted advertisements or manipulations that are
hard for people to resist.
While AI magnifies the potential for information asymmetries, such strategies have long standing
precedents in non-AI contexts. Predatory lending is a common practice where lenders, often equipped
with more information than borrowers, use deceptive practices to encourage individuals into accepting
unfair loan terms. These tactics tend to target lower-income and less-educated individuals who
might not have the time or background to understand the fine-print of what they’re signing, or the
resources to find legal counsel. AI can further increase the power imbalance; for instance, AI can be
used to predict who is most likely to accept these unfair loan terms based on their digital behavior,
leading to even more targeted predatory lending. AI can both amplify existing issues and present
new challenges.

314
CHAPTER 6. MACHINE ETHICS
Monopoly Power
Markets can be monopolistic instead of perfectly competitive.
We earlier discussed how
competition can create productive efficiency and how a failure of competition might corrupt this
equilibrium. A market is said to be perfectly competitive when it contains a large number of firms
that sell effectively identical products, with no barriers to new firms joining the market, such that no
single firm has the power to influence the market price on its own. A good example is the market for
many food items, such as milk. No one dairy firm can single-handedly raise the price of milk, and
both grocery stores and consumers have a variety of brand options. Perfectly competitive markets
are generally favorable for consumers, offering a wide array of quality options and keeping prices
down due to competition among firms.
In contrast, an imperfectly competitive market is one where one or more firms gain enough influence
to impact the market price. Such firms are able to avoid some competitive pressures; for instance,
they can raise their own prices without losing many consumers to competitors. This ability is
referred to as market control or monopoly power. In the case of the technology industry, big tech
companies such as Apple have considerable influence over the price of their products due to their
market dominance and unique offerings. When a single company or a small group of companies has
this level of control, consumers are left with limited product options and higher prices.
Monopolies can harm consumers.
Consider De Beers, a company historically dominant in the
diamond industry. For much of the 20th century, De Beers controlled a vast majority of the global
diamond market. By monopolizing the supply chain, they were able to artificially inflate diamond
prices and manipulate consumer demand through aggressive marketing strategies. This control meant
limited choices and inflated prices for consumers, as there were few alternative sources for diamonds.
The situation has gradually changed with the emergence of new players and diamond sources in
the market, but De Beers’ long-standing monopoly serves as a classic example of how concentrated
market power can distort the pricing and availability of common goods. To prevent market control
and preserve the benefits of competition, governments implement antitrust laws. These laws give
governments the power to break up companies with too much control or prevent mergers between
large competitors.
High initial investment requirements can impede newcomers from entering the market.
Historically, first-movers in capital-intensive industries have a competitive advantage, such as rail
companies that own large quantities of railway networks. In such industries, it is difficult for other
firms to enter the market due to high up-front costs; in many cases, an existing firm can block other
firms from entering, such as by keeping prices below a sustainable level. AI development might be
similar—at the very least, firms within it might have some monopoly power that allows them to
avoid many competitive pressures.
Developing a large model requires paying substantial fixed costs up-front, including expenses for
computing power and datasets essential for training. However, once these initial investments are
made, the subsequent cost per user for deploying and maintaining these models are considerably
lower. Since the average cost per unit decreases as the number of customers increases, it becomes
substantially more cost-effective for a single AI company to provide access to many people than for
multiple companies to independently develop and maintain similar models.
Additionally, only a few companies might have access to enough resources and data to create the
best AI models. In the Artificial Intelligence Fundamentals chapter), we discussed how scaling
laws demonstrate that improving AI performance has required access to costly resources like high-
performance processors and vast, high-quality datasets. High resource requirements for developing
advanced AI can limit market entry, stifling competition. Additionally, early capability advantages

6.4. THE ECONOMIC ENGINE
315
for a leading firm can have positive feedback loops and Matthew effect dynamics, which could enable
them to raise more capital and pull ahead from many competitors. The possibility of monopoly power
raises concerns that only a few powerful entities will have access to and benefit from AI technologies.
The use of AI might create monopolies.
AI developers are striving to have their models
achieve superintelligence: models that are able to carry out a wide range of tasks across various
domains better than humans. If someone did manage to create such an AI, they might have a decisive
advantage across large swathes of industry, being sufficiently versatile to become an expert in many
or every market. It may become difficult or impossible for smaller firms to carve out niche market
spaces. The advanced capabilities of general AI may outpace specialized models in diverse domains,
making it more difficult for new entrants to gain market share. Large firms equipped with powerful
AI systems could wield an enormous amount of power, potentially leading to less competition, higher
prices, and slower innovation, hurting both labor and product markets.
Externalities
Externalities are consequences of economic activity that impact unrelated third parties.
An externality is a side effect that stems from an economic activity, impacting individuals or groups
who are not directly involved in that activity. Because of externalities, market prices for goods or
services may not fully reflect the costs that third parties, who are neither the consumers or the
producers of that market, bear as a result of the economic activity.
A classic example of a negative externality–—a harm to a third party—is the case of pollution.
Consider the Sriracha factory in Irwindale, California, where jalapeño peppers are ground and roasted.
Residents of Irwingdale claimed that odors from the factory caused lung and eye irritation and
created an overall unpleasant smell in the town. The factory, by producing these odors, was imposing
a negative externality upon the town’s residents, but since the townspeople received no automatic
compensation for this inconvenience, this was not reflected in the price of Sriracha.
We can resolve externalities with litigation, property rights, and taxation.
In 2013, locals
sued the Sriracha factory: this legal action led the factory to install new filters to reduce pollution.
Litigation can be an effective tool to resolve externalities by forcing compensation. Economic theory
suggests that bargaining over the externality can also create efficient outcomes; for instance, if the
property right to the air was understood to belong to the townspeople, and that the factory would
have to stop polluting or compensate the townspeople at an acceptable rate for their inconvenience
[34].
A third commonly used resolution to externalities is taxation. Smoking cigarettes imposes negative
externalities on those near the smoker; governments will thus impose taxes on the sale of tobacco,
which both raise revenues for the state to run social programs and discourage smoking by increasing
the price of cigarettes to better reflect its true total cost. Determining the most effective method for
resolving each externality is a topic of ongoing debate among economists. Externalities and their
potential solutions provide crucial context when considering the broader impacts of AI on society.
The development and deployment of AI systems can lead to negative externalities.
The
high energy consumption of advanced AI is a significant negative externality on the environment.
Training advanced AI models requires vast computational resources, consuming a significant amount
of energy and contributing to greenhouse gas emissions. Emissions can lead to climate change, a
cost borne by society at large, rather than only by those who pollute. Another externality is related
to data privacy. AI systems require large volumes of data to learn and improve. People’s privacy
can be compromised if data is obtained without consent, mishandled, misused, or leaked. Privacy

316
CHAPTER 6. MACHINE ETHICS
breaches are costly to individuals whose data is exposed, but this cost isn’t borne by companies, who
are rarely forced to compensate for damages.
Summary.
We examined why inequality and market failures are primary concerns in market
systems and understood a few concrete pathways that these take root. Unfortunately, AIs might
exacerbate inequality, creating disproportionate gains for the wealthy individuals and firms that
create it. AIs may also cause market failures by intensifying informational asymmetries, enabling
monopolies, and generating negative externalities such as greenhouse gas emissions and violations
of privacy. In the next section, we will expand upon the idea that economic models fail to capture
many of the things we care about.
6.4.3
Beyond Economic Models
In measuring societal wellbeing, we must recognize the shortcomings of traditional economic metrics.
Here, we will discuss the disconnect between economic output and social value, why relying on
economic models of welfare economics can be inadequate in describing human goals, and how more
holistic measures of happiness and economic prosperity may give us a clearer sense of true societal
wellbeing.
Economic Output and Gross Domestic Product
Economic output is only a proxy for what we care about; maximizing it will often fail to maximize
social wellbeing. Therefore, AIs should not be given the objective of maximizing economic output.
Economic indicators measure what we can quantify, not necessarily what we care about.
Indicators like Gross Domestic Product (GDP) measure the monetary value of final goods and
services–—that is, those that are bought by the final user–—produced in a country in a given period
of time. When economists discuss growth, they are typically referring to increases in GDP. Measures
of productive output like GDP are useful in gauging a country’s economic health, but they fail to
capture the value of socially significant activities that aren’t priced in the market.
Socially important tasks are not captured by GDP.
Many essential roles in society, such
as parenthood, community service, and early education, are crucial to the wellbeing of individuals
and communities but are often undervalued or entirely overlooked in GDP calculations [35]. While
their effects might be captured–—education, for instance, will increase productivity, which increases
GDP–—the initial activity does not count. The reason is simple: GDP only accounts for activities
that have a market price. Consequently, efforts expended in these socially important tasks, despite
their high intrinsic value, are not reflected in the GDP figures.
Technologies that make our lives better may not be measured either.
Technological
advancements and their resultant value often fail to be reflected adequately in GDP figures. For
instance, numerous open-source projects like Wikipedia provide knowledge to internet users worldwide
at no cost. However, because there’s no direct monetary transaction involved, the immense value
they offer isn’t represented in GDP. The same applies to user-generated content on platforms like
YouTube, where the main contribution to GDP is through advertisement revenue and most creators
aren’t compensated for the value they create. The value viewers derive from such platforms vastly
outstrips the revenue generated from ads or sales on these platforms, but this is not reflected in GDP.

6.4. THE ECONOMIC ENGINE
317
There might be a similar disconnect between GDP and the social value of AI.
As
artificial intelligence systems become more integrated into our daily lives, the disconnect between
GDP and social value might become more pronounced. For example, an AI system that provides free
education resources may significantly improve learning outcomes for millions, but its contribution
would be largely invisible in GDP terms. Similarly, an AI may substantially increase GDP by
facilitating high-frequency trading without doing much to increase social wellbeing. This growing
chasm between economic metrics and real value could lead to policy decisions that fail to harness the
full potential of AI or inadvertently hamper its beneficial applications. Recognizing this gap is a vital
step towards devising better ways of measuring and encouraging the socially beneficial use of AI.
Increasing output does not imply increasing social value.
The true costs of economic
output can often be hidden or overlooked in pursuit of growth. For instance, in the Democratic
Republic of Congo, UNICEF estimates that 40,000 children work in cobalt mines, often in hazardous
conditions. While this labor may contribute to the country’s economic output, the adverse effects on
these children’s health and education plausibly outweigh the short-term economic gains, and these
practices conflict with commonly held principles around the exploitation of children. Much like in
the high-frequency trading example above, more output doesn’t necessarily correspond to increased
social value. AIs that maximize GDP as a proxy for social value may attempt high-frequency trading
or the use of exploitative labor practices, proxy gaming to maximize its reward.
Similarly, an increase in work hours could lead to heightened productivity and output, but not
necessarily enhanced societal wellbeing. In the US, for example, workers highly value their leisure
time and often prioritize it over potential economic benefits of working longer hours. In contrast,
cultures with longer average work hours, such as China, might have a different perspective on the
balance between work and leisure. Despite the country’s impressive economic growth in recent
decades, levels of self-reported happiness haven’t increased proportionally, potentially due to growing
income inequality [36]. Such evidence suggests that an increase in output does not always translate to
an improvement in quality of life. We must consider factors beyond economic metrics when assessing
the societal impact of policies and practices, particularly those involving AI systems. This perspective
is especially important when deploying AI systems that could significantly influence the future of
work.
The financialization of everything.
Another way in which economic output can be at odds with
our values is through the growing trend of financialization—monetizing various aspects of private
or public life [37]. Increasingly, potent algorithms are deployed to "optimize" life in various ways,
with a primary focus on profitability, sometimes at the expense of intrinsic human values. These
algorithms can distort the alignment between technology and societal values. Consider aspects
like love, friendship, human rights, freedom, and environmental preservation. These are areas we
instinctively try to protect from the realm of commerce; for example, people who sell friendship
are often seen as degrading the value of friendship. Unlike commodities that get depleted with use,
altruism, solidarity, and civic spirit can actually increase with use, much like muscles that develop
and become stronger when regularly used. These are reasons we do not want to financialize virtues
or many aspects of life.
However, economic incentives within markets often compel innovators to monetize these aspects.
Consider an experiment where a daycare center imposed fines on parents who were late in picking
up their children. Instead of serving as a deterrent, the fine doubled the number of late arrivals.
Parents saw the fine not as a penalty, but as a service charge for extended care, effectively absolving
them of any guilt related to tardiness. When the fine was eventually eliminated, the late pickups
didn’t decrease but rather increased. The financial penalty had disappeared, but the original sense of
guilt associated with late pickups did not return [38]. This case demonstrates the risk of allowing

318
CHAPTER 6. MACHINE ETHICS
financialization into areas of life traditionally governed by non-market norms; we might inadvertently
erode the very values we hold dear, like parental responsibility and respect for others’ time.
The proxy-purpose distinction is especially important for AI.
Imagine a future where an
AI system is tasked with maximizing GDP, often seen as a proxy for wellbeing. The system could
potentially achieve this goal by promoting resource-intensive industries or fostering a work culture
that prioritizes productivity over wellbeing. In such a scenario, the GDP might increase, but at the
cost of essential considerations like environmental sustainability and human happiness. Therefore,
relying solely on economic indicators could lead to decisions that, while effective in the short term,
might harm society in the long run.
Understanding the limitations of economic measurements is an important step towards a safer use
of AI. It helps us question what we should optimize. The aim of economic policy should not just
be to maximize economic output but also to promote overall societal wellbeing. AI systems may
thus need to take into account multiple factors, like equality, sustainability, and personal fulfillment,
rather than only economic indicators. Such an approach could pave the way for a future where AI
contributes positively to steering society without compromising the values we hold dear.
Models of Welfare Economics
Models of welfare economics try to maximize a quantity called “social surplus.” While this sounds
good, social surplus differs from social wellbeing.
The most basic form of welfare economics maximizes social surplus.
Social surplus is
a measure of the total value created by a market: it is the sum of the consumer surplus and the
producer surplus. The consumer surplus is the difference between the maximum price a consumer
is willing to pay and the actual market price. Conversely, the producer surplus is the difference
between the actual market price and the minimum price a producer is willing to accept for a product
or service. By maximizing the total surplus, welfare economics seeks to maximize the social value
created by a market.
For instance, imagine a scenario where consumers are willing to pay up to $20 for a book, but the
market price is only $15. Here, the consumer surplus is $5. Similarly, if a producer is willing to sell
the book for a minimum of $10, the producer surplus is $5. The social surplus, and hence the social
value in this market, is $10: $5 consumer surplus plus $5 producer surplus.
We should be wary of generalizing from simple models.
The core model of welfare economics
has its limitations. Notably, welfare economics is concerned with the maximization of surplus, but is
indifferent to its distribution. This might not align with societal notions of fairness and equality. For
example, an AI optimized to maximize profits might model consumers well enough to enable perfect
price discrimination: allowing firms to sell each good at exactly a consumer’s maximum willingness
to pay, converting all the consumer surplus into producer surplus, but leaving the sum total of “social
surplus” unchanged.
However, social surplus is not the only thing we care about. Since utility functions with respect
to money are concave, we care about how rich consumers and producers are to begin with. If, as
is usually the case, consumers are poorer than the owners of a company, then transferring $5 (of
surplus) from consumers to producers decreases total utility. The real gains for the firm may be
small compared to the losses for the consumers.

6.4. THE ECONOMIC ENGINE
319
As AI systems become more integral to our economies, we must be mindful of these complexities. A
narrow focus on maximizing economic surplus could lead us to promote AIs which, while efficient in
a purely economic sense, might have alarming consequences for society.
Basic economic theory confuses ability with willingness to pay.
When determining social
value, economic theory implicitly equates ability to pay with willingness to pay. This assumption can
lead to distorted conclusions about what generates social value because these two concepts should be
distinct: ability to pay reflects an individual’s financial capacity, while willingness to pay expresses
their desire or need for a good or service. The person who pays the most for a good is not the person
who values it the most.
Consider a situation where both a billionaire and an average-income individual require a life-saving
drug. Both parties are willing to pay whatever they can to obtain it. The billionaire is able to
pay astronomically more due to their wealth, despite the fact that the drug provides the same
life-saving value to both the billionaire and the average-income person. The welfare economics model,
in maximizing total surplus, would interpret the billionaire’s purchase as creating a vastly greater
social value because the difference between the billionaire’s purchase price and their willingness to
pay is so great—an unsettling conclusion that contradicts most moral perspectives.
This disconnect is highlighted by differences between the developed and developing world; those in
the former have a relatively tiny ability to pay. It seems unlikely that the preferences of someone in a
wealthy country are 100 times more intense than someone from the third world, and yet their ability
to pay is. It is similarly unlikely that Jeff Bezos’s preferences are a million times more intense than
the average US citizen’s; yet, his ability to pay is. The economic engine is only sensitive to ability
to pay, not utility derived. As a result, the market is distorted: individuals with astronomically
high wealth are prioritized astronomically, while those with nothing have no part to play in the free
markets’ allocation of value.
Economics is not fundamentally utilitarian.
The economic engine violates Bentham’s basic
principle of utilitarianism: “Everybody to count for one, nobody for more than one.” Instead, people
count for how much they can pay. In general, economics is chiefly concerned with the efficient
allocation of resources to help satisfy people’s revealed economic preferences: what consumers and
producers are shown as willing to demand and supply at various prices. However, revealed preferences
about products are a narrow subset of revealed preferences in general, and revealed economic
preferences are distinct from other sorts of preferences, such as idealized preferences. Consequently,
economics has differing objectives from theories like preference utilitarianism that aim to maximize
the satisfaction of preferences in general. The economic engine is not a preference maximization
engine; instead, it is better thought of as a wealth maximization engine. For example, although many
people would welcome AIs that aid education or assist medical research to benefit many, we may just
as soon see AIs that conduct highly effective subliminal marketing, which may only benefit a few.
Happiness in Economics
Economists are unsure what exactly creates happiness, but material wealth is probably only one part
of the picture.
There is a gap between human happiness and material prosperity.
Most people would agree
that the goal of social sciences should not be to just increase material wealth. A more meaningful aim
would be to enhance overall wellbeing or happiness. However, defining and measuring happiness can
be challenging. Whether happiness is correlated with material wealth remains an ongoing research

320
CHAPTER 6. MACHINE ETHICS
question; other aspects of life like physical and mental health, job satisfaction, social connections,
and a sense of purpose seem important as well.
Debates about the Easterlin paradox highlight the complexity of understanding
happiness.
While wealthier people and countries are generally happier than their less affluent
counterparts, long-term economic growth does not always correlate with long-term increases in
happiness: this is the Easterlin paradox. Several studies have tried to explore the relationship
between happiness and economic growth [36]. While some findings suggest a correlation, others don’t,
highlighting that our understanding of the happiness-growth relationship is still evolving.
We do, however, have strong evidence that inequality is unhelpful. People often evaluate their
wellbeing in relation to others; so, when wealth distribution is unequal, people are dissatisfied and
unhappy. For instance, the recent rise in inequality may explain why there has been no significant
increase in happiness in the US over the last few decades despite an approximately tenfold increase
in real GDP and a fourfold increase in real GDP per capita.
More holistic economic measurements can get closer to capturing what we value.
Due
to the disconnect between economic prosperity and true wellbeing, some economists now propose the
adoption of broader metrics. These new measures aim to capture wellbeing more comprehensively,
rather than solely focussing on economic growth. One such measure is the Human Development Index
(HDI). The HDI comprises a nation’s average life expectancy, education level, and Gross National
Income (GNI) per capita (which is similar to GDP). Notably, the UN uses the logarithm of GNI
per capita in the HDI calculation, which accounts for the diminishing returns of wealth: the idea
that each additional dollar earned contributes less to a person’s happiness than the one before it. In
general, economists consider a “report card” of indicators to assess a nation’s wellbeing, rather than
just depending on a single measure. By capturing various aspects of wellbeing, this approach could
provide a more holistic and accurate representation of a nation’s quality of life.
Summary.
Traditional economic measures and models are insufficient for measuring and modeling
what we care about. There is a disconnect between what we measure and what we value; for instance,
GDP fails to account for essential unpaid labor and overvalues the production of goods and services
that add little to social wellbeing. While economic models are useful, we must avoid relying too
much on theoretically appealing models and examine the matter of societal wellbeing with a more
holistic lens.
Conclusions About the Economic Engine
AIs can drive competition, efficiency, and growth, creating tangible benefits for everyone in society.
However, they might unintentionally create problems like inequality and reduce efficiency and
competition through market failures, highlighting the need for regulation. Additionally, while it’s been
a driver of prosperity in human history, the economic engine is sometimes misaligned with wellbeing,
so it is not something to align AI development to.
AIs will likely be designed to achieve objectives other than social wellbeing.
Economic
forces incentivize the development of AIs that prioritize objectives such as financial gains for the
companies deploying them, rather than social wellbeing. Consider the pharmaceutical industry:
the primary goal of medicine is ideally to improve human health. Instead, in a number of cases,
pharmaceutical companies have fostered patient dependencies on medications in order to ensure
revenue and have used patented information to raise prices to a point that many consumers cannot
afford life-saving drugs that cost little to produce. Similarly, AI systems ostensibly designed to

6.4. THE ECONOMIC ENGINE
321
connect us to our friends, such as social media algorithms, may instead result in users feeling isolated.
We must recognize such discrepancies in the context of AIs: AI systems claiming to enhance happiness
might not align with our best interests. Perhaps a balance can be struck, where AIs continue to
drive economic growth, but not at the cost of other equally important aspects of societal wellbeing.
This disconnect between the goals of businesses deploying AI systems and their users’ best interests
underscores the need for regulations that ensure that the deployment of AI does not compromise
societal wellbeing.
We should be wary of using AI to increase wellbeing.
If policymakers trust AI to create
policies intended to improve society, they must be cautious about the objectives set for these systems.
Our current economic system strongly incentivises the deployment of AI systems that optimize for
economic growth rather than other objectives. As we discussed, increasing economic output is often a
worthy goal, but may not capture essential aspects of societal health such as equality, sustainability,
or the smooth functioning of an economy without market failures. While economic objectives like
GDP growth are quantifiable and easy to pursue, they may not truly reflect what makes a society
happy and healthy.
It is risky to blindly trust the economic engine to create AI that maximizes our wellbeing.
Created as a result of businesses pursuing economic goals, AI systems are not designed to maximize
societal wellbeing. Instead, they are geared towards wealth maximization for shareholders, with
the potential to supercharge economic efficiency and drive economic growth. If we let the economy
decide what AIs do by letting largely unregulated markets create AIs, we will likely end up with an
increase in inequality, exacerbation of market failures, and promotion of economic goals like wealth
maximization that are notably different from increasing wellbeing.
Pursuing societal wealth maximization is not as good as maximizing social welfare functions—
discussed in the next section–—which account for how individual wellbeing changes with wealth and
how wellbeing is distributed. The idea of using monetary equivalents as a proxy for social value
might seem practical, but it can distort societal priorities; for instance, this system implies that the
preferences of wealthier individuals hold more weight since they are willing and able to spend more.
The preferences of a billionaire would be considered orders of magnitude more important than those
of an average citizen, which is clearly disconnected from social value.
The examples in this chapter demonstrate a divergence between economic incentives and other
important societal goals and should serve as a cautionary note, encouraging us to remain wary
of integrating AI developed in line with economic incentives into all of our societal and economic
structures. Both the generation and distribution of wealth matter, as well as how this wealth improves
other aspects of social wellbeing, such as crime rates and physical health. As we move forward, we
should take care to ensure that AIs’ potential is used to serve humanity, not just the economy.
A Note on Wellbeing in Social Science
Philosophers continue to debate what wellbeing is, or what it means to live a good life.
However, over the years, researchers have developed ways to approximate wellbeing for
practical purposes. Psychologists, economists, philanthropists, policy makers, and other
professionals need–—at least–—a working definition of wellbeing in order to study, measure,
and promote it. Here, we describe some common views of wellbeing and illustrate how they
are used in social science.

322
CHAPTER 6. MACHINE ETHICS
Preference satisfaction.
Preference theorists view wellbeing as fulfilling desires or
satisfying preferences, even if doing so does not always induce pleasure. Nonetheless, it
remains an open question whether all desires are tied to wellbeing or just certain kinds, like
higher-order or informed desires.
Economics.
Standard economics often uses preference satisfaction theories to study
wellbeing. Revealed preferences can be observed by studying the decisions people make. If
people desire higher incomes, for example, economists can promote wellbeing by researching
the impact of different economic policies on gross domestic product (GDP).
Social Surveys and Psychology.
Traditionally, psychological surveys evaluate
wellbeing in terms of life satisfaction.
Life satisfaction is a measure of people’s stated
preferences—preferences or thoughts that individuals outwardly express—regarding how their
lives are going for them. Life satisfaction surveys typically focus on tangible characteristics of
people’s lives, such as financial security, health, and personal achievements. They are well
suited to understanding the effects of concrete economic factors, such as income and education,
on an individual’s psychological wellbeing. For example, to promote wellbeing, psychologists
might research the effects of access to education on one’s ability to achieve the goals they set
for themselves.
Hedonism.
Under hedonist theories of wellbeing, an individual’s wellbeing is determined
by their mental states. In particular, the balance of positive mental states (like pleasure or
happiness) over negative mental states (like pain or suffering).
Economics.
Welfare economics uses hedonism to evaluate the wellbeing of populations.
To estimate gross national happiness (GNH )—-an indicator of national welfare—-it considers
the effects of several factors from psychological wellbeing to ecological diversity and resilience
on individuals’ mental states. Welfare economists might prefer this framework because it is
more holistic - it evaluates both material and non-material aspects of everyday life as they
contribute to national welfare.
Social Surveys and Psychology.
Many psychologists also use hedonist theories to
understand and promote wellbeing. They may work to identify the emotional correlates of
happiness through surveys that measure people’s stated emotions – unlike life satisfaction
surveys, these surveys do not reveal mental states. They reveal emotions that people remember,
not emotions they currently experience. Researchers continue to look for ways to directly
observe emotions as they are experienced. For example, some studies use cell phone apps to
periodically prompt participants to record their current emotions. Such research tactics may
provide us with more precise measures of individuals’ overall happiness by evaluating the
emotional responses to their everyday experiences in near real-time.
Objective goods.
Under objective goods theories, wellbeing is determined by a certain
number of observable factors, independent of individuals’ preferences or experiences. There
are multiple theories about what those factors may be. One of the most widely supported
theories is human flourishing. Under this view, wellbeing is more than just the balance of
pleasure over suffering, or the fulfillment of one’s preferences – “the good life” should be full,
virtuous, and meaningful, encapsulating psychological, hedonistic, and social wellbeing all at
once.

6.5. PREFERENCES
323
Economics.
In economics, the capabilities approach defines wellbeing as having access
to a set of capabilities that allow one to live the kind of life they value. It emphasizes two core
ideas: functionings and capabilities. Functionings include basic and complex human needs,
ranging from good health to meaningful relationships. Capabilities refer to the ability people
have to choose and achieve the functionings they value – they may include being able to
move freely or participate in the political process. This approach has significantly influenced
human development indicators, such as the Human Development Index (HDI ) – it allows
developmental economists to measure and compare wellbeing across different populations
while also evaluating the effectiveness of public policies.
Psychology.
Positive psychologists do not collapse wellbeing into one dimension, rather,
they argue for a psychologically rich life – one that is happy, meaningful, and engaging. Some
psychologists use PERMA theory to evaluate psychological richness, which considers five
categories essential to human flourishing: 1) experience of positive emotions, 2) engagement
with one’s interests, 3) maintenance of personal, professional, and social relationships, 4)
the search for meaning or purpose, and 5) accomplishments, or the pursuit of one’s goals.
This framework is particularly useful in evaluating wellbeing because it is universal–—it can
be applied cross-culturally—-and practical—-it can guide interventions aiming to improve
emotional wellbeing, social relationships, or activities that provide a sense of meaning or
accomplishment.
While we don’t have a complete understanding of the nature of wellbeing, we can use these
theories as useful research tools. They can help us to (a) understand how different factors
contribute to wellbeing and (b) evaluate the effectiveness of policies and other interventions
aimed at improving wellbeing.
6.5
Preferences
Should we have AIs satisfy people’s preferences?
A preference is a tendency to favor one
thing over another. Someone might prefer chocolate ice cream over vanilla ice cream, or they might
prefer that one party wins the election rather than another. These preferences will influence actions. If
someone prefers chocolate ice cream over vanilla, they’re more likely to choose the former. Similarly, if
someone prefers one political party over another, they will likely vote accordingly. In this manner, our
preferences shape our behavior, guiding us toward certain choices and actions over others. Preference
is similar to desire but always comparative. Someone might desire something in itself—a new book,
a vacation, or a delicious meal—but a preference always involves a comparison between two or more
alternatives.
Overview.
In this section, we will consider whether preferences may have an important role to
play in machine ethics. In particular, if we want to design an advanced AI system, the preferences of
the people affected by its decisions should plausibly help guide its decision-making. In fact, some
people (such as preference utilitarians) would say that preferences are all we need. However, even
if we don’t take this view, we should recognize that preferences are still important. Preferences
are further explored in the next two chapters. First, in the chapter on ethics, we discussed the
idea that wellbeing might consist in having one’s preferences satisfied. Second, in the chapter on
utility functions, we explain how preferences can be used to construct a utility function that assigns
numerical values to options and outcomes.
To use preferences as the basis for increasing social wellbeing, we must somehow combine the

324
CHAPTER 6. MACHINE ETHICS
conflicting preferences of different people. We’ll come to this later in this chapter, in a section on
social welfare functions. Before that, however, we must answer a more basic question: what exactly
does it mean to say that someone prefers one thing over another? Moreover, we must decide why we
think that satisfying someone’s preferences is good for them and whether all kinds of preferences are
equally valuable. This section considers three different types of preferences that could all potentially
play a role in decision-making by AI systems: revealed preferences, stated preferences, and idealized
preferences.
6.5.1
Revealed Preferences
Preferences can be inferred from behavior. One set of techniques for getting AI systems to
behave as we want—inverse reinforcement learning—is to have them deduce revealed preferences
from our behavior. We say that someone has a revealed preference for X over Y if they choose X
when Y is also available. In this way, preference is revealed through choice. Consider, for example,
someone deciding what to have for dinner at a restaurant. They’re given a menu, a list of various
dishes they could order. The selection they make from the menu is seen as a demonstration of their
preference. If they choose a grilled chicken salad over a steak or a plate of spaghetti, they’ve just
revealed their preference for grilled chicken salad, at least in that specific context and time.
While all theories of preferences agree that there is an important link between preference and choice,
the revealed preference account goes one step further and claims that preference simply is choice.
Revealed preferences preserve autonomy.
One advantage of revealed preferences is that we
don’t have to guess what someone prefers. We can simply look at what they choose. In this way,
revealed preferences can help us avoid paternalism. Paternalism is when leaders use their sovereignty
to make decisions for their subjects, limiting their freedom or choices, believing it is for the subjects’
own good. However, we may think that typically people are themselves the best judges of what is
good for them. If so, then by relying on people’s actions to reveal their preferences, we avoid the risk
of paternalism.
However, there are problems with revealed preferences.
The next few subsections will
explore the challenges of misinformation, weakness of will, and manipulation in the context of revealed
preferences. We will discuss how misinformation can lead to choices that do not accurately reflect a
person’s true preferences, and how weakness of will can cause individuals to act against their genuine
preferences. Additionally, we will examine the various ways in which preferences can be manipulated,
ranging from advertising tactics to extreme cases like cults, and the ethical implications of preference
manipulation.
Misinformation
Revealed preferences can sometimes be based on misinformation.
If someone buys a used
car that turns out to be defective, it doesn’t mean they preferred a faulty car. They intended to
buy a reliable car, but due to a lack of information or deceit from the seller, they ended up with a
substandard one. Similarly, losing at chess doesn’t indicate a preference for losing; rather, it’s an
outcome of facing a stronger player or making mistakes during the game. This means that we cannot
always infer someone’s preferences from the choices they make. Choice does not reveal a preference
between things as they actually are, but between things as the person understands them. Therefore,
we can’t rely on revealed preferences if they are based on misinformation.

6.5. PREFERENCES
325
Weakness of Will
Choices can be due to a lack of willpower rather than considered preferences. Consider a
smoker who wants to quit. Each time they light a cigarette, they may be acting against their genuine
preference to stop smoking, succumbing instead to the power of addiction. Therefore, it would be
erroneous to conclude from their behavior that they think that continuing to smoke would be best
for their wellbeing.
Manipulation
Revealed preferences can be manipulated in various ways. Manipulations like persuasive
advertising might manipulate people into buying products they don’t actually want. Similarly,
revealed preferences might be the result of social pressure rather than considered judgment, such as
when buying a particular style of clothing. In such cases, the manipulation may not be especially
malicious. At the other extreme, however, cults may brainwash their members into preferring
things they would not otherwise want, even committing suicide. In the context of decision-making
by advanced AI systems, manipulation is a serious risk. An AI advisor system could attempt to
influence our decision-making for a variety of reasons—perhaps its designer wants to promote specific
products—by presenting options in some particular way; for instance, by presenting a list of options
that looks exhaustive and excluding something it doesn’t want us to consider.
If preference satisfaction is important, perhaps manipulation is acceptable.
In at least
some of these cases, it seems clear that preference manipulation is bad. However, it may be less clear
exactly why it is bad. A natural answer is to say that people might be manipulated into preferring
things that are bad for them. Someone who is manipulated by advertising into preferring junk
food might thereby suffer negative health consequences. However, if we think that wellbeing simply
consists in preference satisfaction, it doesn’t make sense to say that we might prefer what is bad for
us. On this account, having one’s preferences satisfied is by definition good, regardless of whether
those preferences have been manipulated. This might lead us to think that what matters is not (or at
least not only) preference satisfaction, but happiness or enjoyment. We’ll discuss this in the section
on happiness.
Disliking manipulation suggests that wellbeing requires autonomy.
On the other hand,
some may find that manipulation is bad even if the person is manipulated into doing something that
is good for them. For example, suppose a doctor lies to her patient, telling him that unless he loses
weight, he will likely die soon. As a result, the patient becomes greatly motivated to lose weight and
successfully does so. This provides a range of health benefits, even if his doctor never had any reason
to believe he would have died otherwise. If we think manipulation is still bad, lack of enjoyment
can’t be the whole story. This suggests that we object to manipulation in part because it violates
autonomy. We might then think that autonomy—the ability to decide important matters for oneself,
without coercion—is objectively valuable regardless of what the agent prefers. We’ll discuss this in
the section on objective goods.
Inverse Reinforcement Learning
Inverse Reinforcement Learning (IRL) relies on revealed preferences.
IRL is a powerful
technique in the field of machine learning, which focuses on extracting an agent’s objectives or
preferences by observing its behaviors. In more technical terms, IRL is about reverse engineering
the reward function—an internal ranking system that an agent uses to assess the value of different
outcomes—that the agent appears to be optimizing, given a set of its actions and a model of the
environment. This technique can help ensure the alignment of AI system’s behaviors with human

326
CHAPTER 6. MACHINE ETHICS
values and preferences. However, leveraging revealed preferences or observable choices of humans to
train AI systems using IRL poses significant challenges pertaining to AI safety.
Using revealed preferences as a training mechanism for IRL can be risky.
Reconsider
the chess example: losing a game does not mean that we prefer to lose. This interpretation could
be a misrepresentation of the player’s true preferences, potentially leading to undesirable outcomes.
Furthermore, extending observed preferences to unfamiliar situations poses another hurdle. I may
prefer to eat ice cream for dessert, but that doesn’t mean I prefer to eat it for every meal. Similarly,
I may prefer to wear comfortable shoes for hiking, but that doesn’t mean I want to wear them to a
formal event. An AI system could inaccurately extrapolate preferences from limited or context-specific
data and misapply these to other scenarios. Therefore, while revealed preferences can offer significant
insights for training AI, it is vital to understand their limitations to safeguard the safety and efficiency
of AI systems.
Summary.
Revealed preferences can be a powerful tool, as they allow an individual’s actions to
speak for themselves, reducing the risk of paternalistic intervention. However, revealed preferences
have inherent shortcomings such as susceptibility to misinformation and manipulation, which can
mislead an AI system. This emphasizes the caution needed in relying solely on revealed preferences
for AI training. It underscores the importance of supplementing revealed preferences with other
methods to ensure a more comprehensive and accurate understanding of a user’s true preferences.
6.5.2
Stated Preferences
Preferences can be straightforwardly queried. Another set of techniques for getting AI systems
to behave as we want—human supervision and feedback—rely on people to state their preferences. A
person’s stated preferences are the preferences they would report if asked. For example, someone
might ask a friend which movie they want to see. Similarly, opinion polls might ask people which
party they would vote for. In both cases, we rely on what people say as opposed to what they do, as
was the case with revealed preferences.
Stated preferences overcome some of the difficulties with revealed preferences. For example, stated
preferences are less likely to be subject to weakness of will: when we are further removed from the
situation, we are less inclined to fall for temptations. Therefore, stated preferences are more likely to
reflect our stable, long-term interests.
Stated preferences are still imperfect.
Stated preferences do not overcome all difficulties with
revealed preferences. Stated preferences can still be manipulated. Further, individuals might state
preferences they believe to be socially acceptable or admirable rather than what they truly prefer,
particularly when the topics are sensitive or controversial. Someone might overstate their commitment
to recycling in a survey, for instance, due to societal pressure and norms around environmental
responsibility.
Preference Accounting
One set of problems with stated preferences concern which types of preferences should be satisfied.
First, someone might never know their preference was fulfilled.
Suppose someone is on a
trip far away. On a bus journey, they exchange a few glances with a stranger whom they’ll never
meet again. Nevertheless they form the preference that the stranger’s life goes well. Should this
preference be taken into account? By assumption, they will never be in a position to know whether

6.5. PREFERENCES
327
the preference has been satisfied or not. Therefore, they will never experience any of the enjoyment
associated with having their preference satisfied.
Second, we may or may not care about the preferences of the dead.
Suppose someone in
the 18th century wanted to be famous long after their death. Should such preferences count? Do they
give us reason to promote that person’s fame today? As in the previous example, the satisfaction of
such preferences can’t contribute any enjoyment to the person’s life. Could it be that what we really
care about is enjoyment or happiness, and that preferences are a useful but imperfect guide toward
what we enjoy? We will return to this in the section on happiness.
Third, preferences can be about others’ preferences being fulfilled.
Suppose a parent
prefers that their children’s preferences are satisfied. Should this preference count, in addition to
their children’s preferences themselves? If we say yes, it follows that it is more important to satisfy
the preferences of those who have many people who care for them than of those who do not. One
might think that this is a form of double counting, and claim that it is unfair to those with fewer who
care for them. On the other hand, one might take fairness to mean that we should treat everyone’s
preferences equally—including their preferences about other people’s preferences.
Fourth, preferences might be harmful.
Suppose someone hates their neighbor, and prefers
that they meet a cruel fate. We might think that such malicious or harmful preferences should not
be included. On this view, we should only give weight to preferences that are in some sense morally
acceptable. However, specifying exactly which preferences should be excluded may be difficult. There
are many cases where satisfying one person’s preferences may negatively impact others. For example,
whenever some good is scarce, giving more of it to one person necessarily implies that someone
else will get less. Therefore, some more detailed account of which preferences should be excluded is
needed.
Fifth, we might be confused about our preferences.
Suppose a mobile app asks its users
to choose between two privacy settings upon installation: allowing the app to access their location
data all the time, or allowing the app to access their location data only while they’re using the app.
While these options seem straightforward, the implications of this choice are much more complex.
To make a truly informed decision, users need to understand how location data is used, how it can
be combined with other data for targeted advertising or profiling, what the privacy risks of data
breaches are, and how the app’s use of data aligns with local data protection laws. However, we may
not fully understand the alternatives we’re choosing between.
Sixth, preferences can be inconsistent over time.
It could be that the choice we make will
change us in some fundamental way. When we undergo such transformative experiences [39], our
preferences might change. Some claim that becoming a parent, experiencing severe disability, or
undergoing a religious conversion can be like this. If this is right, how should we evaluate someone’s
preference between becoming a parent and not becoming a parent? Should we focus on their current
preferences, prior to making the choice, or on the preferences they will develop after making the
choice? In many cases we may not even know what those new preferences will be.
As technology advances, we may increasingly have the option to bring about transformative experiences
[39]. For this reason, it is important that advanced AI systems tasked with decision-making are able
to reason appropriately about transformative experiences. For this, we cannot rely on people’s stated
preferences alone. By definition, stated preferences can only reflect the person’s identity at the time.
Of course, people can try to take possible future developments into account when they state their

328
CHAPTER 6. MACHINE ETHICS
preferences. However, if they undergo a transformative experience their preferences might change in
ways they cannot anticipate.
Human Supervision
Stated preferences are used to train some AI systems. In reinforcement learning with human
feedback (RLHF), standard reinforcement learning is augmented by human feedback from people who
rank the outputs of the system. In RLHF, humans evaluate and rank the outputs of the system based
on quality, usefulness, or another defined criterion, providing valuable data to guide the system’s
iterative learning process. This ranking serves as a form of reward function that the system uses to
adjust its behavior and improve future outputs.
Imagine that we are teaching a robot how to make a cup of coffee. In the RLHF process, the AI
would attempt to output a cup of coffee, and then we would provide feedback on how well it did. We
could rank different attempts and the robot would use this information to understand how to make
better coffee in the future. The feedback helps the robot learn not just from its own trial and error,
but also from our expertise and judgment. However, this approach has some known difficulties.
First, as AI systems become more powerful, human feedback might be infeasible.
As
the problems AI solve become increasingly difficult, using human supervision and feedback to ensure
that those systems behave as desired becomes difficult as well. In complex tasks like creating bug-free
and secure code, generating arguments that are not only persuasive but true, or forecasting long-term
implications of policy decisions, it may be too time-consuming or even impossible for humans to
evaluate and guide AI behavior. Moreover, there are inherent risks from depending on human
reliability: human feedback may be systematically biased in various ways. For example, inconvenient
but true things may often be labeled as bad. In addition to any bias, relying on human feedback will
inevitably mean some rate of human error.
Second, RLHF usually does not account for ethics.
Approaches based on human supervision
and feedback are very broad. These approaches primarily focus on task-specific performance, such as
generating accurate book summaries or bug-free code. However, these task-specific evaluations may
not necessarily translate into a comprehensive understanding of ethical principles or human values.
Rather, they improve general capabilities since humans prefer smarter models.
Take, for instance, feedback on code generation. A human supervisor might provide feedback based on
the code’s functionality, efficiency, or adherence to best programming practices. While this feedback
helps in creating better code, it doesn’t necessarily guide the AI system in understanding broader
ethical considerations, such as ensuring privacy protection or maintaining fairness in algorithmic
decisions. Specifically, while RLHF is effective for improving AI performance in specific tasks, it
does not inherently equip AI systems with what’s needed to grapple with moral questions. This gap
underscores the need for specific machine ethics research.
Summary.
We’ve seen that stated preferences have certain advantages over revealed preferences.
However, stated preferences still have issues of their own. It may not be clear how we should account
for all different kinds of preferences, such as ones that are only satisfied after the person has died,
or ones that fundamentally alter who we are. For these reasons, we should be wary of using stated
preferences alone to train AI.

6.5. PREFERENCES
329
6.5.3
Idealized Preferences
We could idealize preferences to avoid problems like weakness of will. A third approach to
getting AI systems to behave as we want is to make them able to infer what we would prefer if our
preferences weren’t subject to the various distorting forces we’ve come across. Someone’s idealized
preferences are the preferences they would have if they were suitably informed. Idealized preferences
avoid many of the problems of both revealed preferences and stated preferences. Idealized preferences
would not be based on false beliefs, nor would they be subject to weakness of will, manipulation, or
framing effects. This makes it clearer how idealized preferences might be linked to wellbeing, and
therefore something we might ask an AI system to implement.
It is unclear how we idealize preferences.
What exactly do we need to do to figure out what
someone’s idealized preferences are, based on their revealed preferences or their stated preferences?
It’s clear that the idealized preferences should not be based on any false beliefs. We might imagine a
person’s idealized preferences as ones they would have if they fully grasped the options they faced
and were able to think through the situation in great detail. However, this description is rather
vague. It may be that it doesn’t uniquely narrow down a set of idealized preferences. That is, there
may be multiple different ways of idealizing someone’s preferences, each of which is one possible way
that the idealized deliberation could go. If so, idealized preferences may not help us decide what to
do in such cases.
Additionally, some may argue that in addition to removing any dependence on false belief or
other misapprehensions, idealized preferences should also take moral considerations into account.
For example, perhaps malicious preferences of the kind discussed earlier would not remain after
idealization. These may not be insurmountable problems for the view that advanced AI systems
should be tasked with satisfying people’s idealized preferences. However, it shows that the view
stands in need of further elaboration, and that different people may disagree over what exactly should
go into the idealization procedure.
We might think that preferences are pointless.
Suppose someone’s only preference, even after
idealization, is to count the blades of grass on some lawn. This preference may strike us as valueless,
even if we suppose that the person in question derives great enjoyment from the satisfaction of their
preferences. It is unclear whether such preferences should be taken into account. The example may
seem far-fetched, but it raises the question of whether preferences need to meet some additional
criteria in order to carry weight. Perhaps preferences, at least in part, must be aimed at some
worthy goal in order to count. If so, we might be drawn toward an objective goods view of wellbeing,
according to which achievements are important objective goods.
On the other hand, we may think that judging certain preferences as lacking value reveals an
objectionable form of elitism. It is unfair to impose our own judgments of what is valuable on other
people using hypothetical thought experiments, especially when we know their actual preferences.
Perhaps we should simply let people pursue their own conception of what is valuable.
We might disagree with our idealized preferences.
Suppose someone mainly listens to country
music, but it turns out that their idealized preference is to listen to opera. When they themselves
actually listen to opera music, they have a miserable experience. It seems unlikely that we should
insist that listening to opera is, in fact, good for them despite the absence of any enjoyment. This
gives rise to an elitism objection like before. If they don’t enjoy satisfying their idealized preferences,
why should those preferences be imposed on them? This might lead us to think that what ultimately
matters is enjoyment or happiness, rather than preference satisfaction. Alternatively, it might lead
us to conclude that autonomy matters in addition to preference satisfaction. If idealized preferences

330
CHAPTER 6. MACHINE ETHICS
are imposed on someone who would in fact rather choose contrary to those idealized preferences, this
would violate their autonomy.
One might think that with the correct idealization procedure, this could never happen. That is,
whatever the idealization procedure does––remove false beliefs and other misconceptions, increase
awareness and understanding––it should never result in anything so alien that the actual person
would not enjoy it. On the other hand, it’s difficult to know exactly how much our preferences
would change when idealized. Perhaps removing false beliefs and acquiring detailed understanding of
the options would be a transformative experience that fundamentally alters our preferences. If so,
idealized preferences may well be so alien from the person’s actual preferences that they would not
enjoy having them satisfied.
AI Ideal Advisor
One potential application of idealized preferences is the AI ideal advisor.
Suppose
someone who hates exploitation and takes serious inconvenience to avoid emissions would ideally
want to buy food that has been ethically produced, but does not realize that some of their groceries
are unethically produced. An AI ideal advisor would be equipped with detailed real-world knowledge,
such as the details of supply chains, that could help them make this decision. In addition to providing
factual information, the AI ideal advisor would be disinterested: it wouldn’t favor any specific entity,
object, or course of action solely due to their particular qualities (such as nationality or brand), unless
explicitly directed to do so. It would also be dispassionate, meaning that it wouldn’t let its advice be
swayed by emotion. Finally, it would be consistent, applying the same set of moral principles across
all situations [40].
Such an AI ideal advisor could possibly help us better satisfy the moral preferences we already have.
Something close to the AI ideal advisor has previously been discussed in the context of AI safety
under the names of “coherent extrapolated volition” and “indirect normativity.” In all cases, the
fundamental idea is to take an agent’s actual preferences, idealize them in certain ways, and then
use the result to guide decision-making by advanced AI systems. Of course, having such an advisor
requires that we solve many of the challenges that we presented in the 3 chapter, as well as settle on
a clear way to identify and idealize individual preferences.
Summary.
Idealized preferences overcome many of the difficulties of revealed and stated preferences.
Because idealized preferences are free from the misconceptions that may affect these other types of
preferences, they are more plausibly ones that we would want an AI system to satisfy. However,
figuring out what people’s preferences would in fact be after idealization can be difficult. Moreover,
it could be that the preferences are without value even after idealization, or that the actual person
would not appreciate having their idealized preferences satisfied. An AI ideal advisor might be
difficult to create, but sounds highly appealing.
Conclusions About Preferences
Preferences seem relevant to wellbeing—but we don’t know which ones. The preferences
people reveal through choice often provide evidence about what is good for them, but they can
be distorted by misinformation, manipulation, and other factors. In some cases, people’s stated
preferences may be a better guide to what is good for them, though it is not always clear how to
account for stated preferences. If we are looking for a notion of preference that plausibly captures
what is good for someone, idealized preferences are a better bet. However, it can be difficult to
figure out what someone’s idealized preferences would be. It seems, then, that preferences-—while

6.6. HAPPINESS
331
important to wellbeing and useful to train AI in accordance with human values—are not a sufficient
basis for a comprehensive account of machine ethics.
Moving from preferences to happiness.
People’s preferences can be easily influenced by external
factors. For instance, individuals may be swayed by persuasive advertising and purchase products
that do not truly fulfill their needs or bring them lasting happiness. Similarly, social media platforms
often present an idealized version of other people’s lives, leading many to compare themselves to
others and feel inadequate or unhappy with their own circumstances. Inconsistent preferences pose a
challenge for the preference satisfaction view, as discussed in the previous section.
One way of ensuring people’s preferences reflect their wellbeing is to use idealized preferences. The
preferences that people would have if they were suitably informed and rational would be well-defined
and aligned with their interests. However, as we explored in the previous section, this view faces
another problem: if satisfying someone’s idealized preferences does not bring them any pleasure or
enjoyment, why should it matter that they are satisfied?
This objection suggests that what we ultimately care about is happiness rather than preference
satisfaction. If so, we should use a framework that places happiness front and center. By instructing
an AI system to increase happiness, we might aim to overcome the human biases and limitations that
stop us from pursuing our happiness and enable the system to make decisions that have a positive
impact on overall wellbeing.
6.6
Happiness
Should we have AIs make people happy?
In this section, we will explore the concept of
happiness and its relevance in instructing AI systems. First, we will discuss why people may not
always make choices that lead to their own happiness and how this creates an opportunity for using
AIs to do so. Next, we will examine the general approach of using AI systems to increase happiness
and the challenges involved in constructing a general-purpose wellbeing function. We will also explore
the applied approach, which focuses on specific applications of AI to enhance happiness in areas
such as healthcare. Finally, we will consider the problems that arise in happiness-focused machine
ethics, including the concept of wireheading and the alternative perspective of objective goods theory.
Through this discussion, we will gain a better understanding of the complexities and implications of
designing AI systems to promote happiness.
AIs could help increase happiness.
Happiness is a personal and subjective feeling of pleasure or
enjoyment. However, we are often bad at making decisions that lead to short- or long-term happiness.
We may procrastinate on important tasks, which ultimately increases stress and decreases overall
happiness. Some indulge in overeating, making them feel unwell in the short-term and leading to
health issues and decreased wellbeing overall. Others turn to alcohol or drugs as a temporary escape
from their problems, but these substances can lead to addiction and further unhappiness.
Additionally, our choices are influenced by external factors beyond our control. For instance, the
people we surround ourselves with greatly impact our wellbeing. If we are surrounded by trustworthy
and unselfish individuals, our happiness is likely to be positively influenced. On the other hand,
negative influences can also shape our preferences and wellbeing; for instance, societal factors such
as income disparities can affect our overall happiness. If others around us earn higher wages, it
can diminish our satisfaction with our own income. These external influences highlight the limited
control individuals have over their own happiness.

332
CHAPTER 6. MACHINE ETHICS
AIs can play a crucial role. For individual cases, we can use AIs to help people achieve happiness
themselves. In general, by leveraging their impartiality and ability to analyze vast amounts of data,
AI systems can strive to improve overall wellbeing on a broader scale, addressing the external factors
that hinder individual happiness.
6.6.1
The General Approach to Happiness
We want AIs that increase happiness across the board.
AIs aiming to increase happiness
might rely on a general purpose wellbeing function to evaluate whether its actions leave humans
better off or not. Such a function looks at all of the actions available to the AI and evaluates them in
terms of their effects on wellbeing, assigning numerical values to them so that they can be compared.
This gives AI the ability to infer how its actions will affect humans.
A wellbeing function is extremely complex.
Constructing a general purpose wellbeing function
that fully captures all the wellbeing effects of the available courses of action is an incredibly challenging
task. Implementing such a function requires taking a stance on several challenging questions such as
how to evaluate short-run pains like studying or exercising for long-run happiness, how much future
people’s happiness should count, and what risk attitudes an AI should take towards happiness.
Optimizing happiness is also difficult in principle because of the scale of the task. Paul Bloom argues
that if we assume that the “psychological present” lasts for about three seconds, then one seventy-year
life would have about half a billion waking moments [41]. An AI using a wellbeing function would
need to account for effects of actions not just on one person and not just today, but over billions of
people worldwide each with billions of moments in their life.
We can use AIs to estimate wellbeing functions.
Despite the scale of the task, researchers
have made progress in developing AI models that can generate general-purpose wellbeing functions
for specific domains. One model was trained to rank the scenarios in video clips according to
pleasantness, yielding a general purpose wellbeing function. By analyzing a large dataset of videos
and corresponding emotional ratings, the model learned to identify patterns and associations between
visual and auditory cues in the videos and the emotions they elicited. In a sense, this allowed the
model to understand how humans felt about the contents of different video clips [42].
Similarly, another AI model was trained to assess the wellbeing or pleasantness of arbitrary text
scenarios [43]. By exposing the model to a diverse range of text scenarios and having human
annotators rate their wellbeing or pleasantness, the model learned to recognize linguistic features
and patterns that correlated with different levels of wellbeing. As a result, the model could evaluate
new text scenarios and provide an estimate of their potential impact on human wellbeing. Inputting
the specifics of a trolley problem yielded the following evaluation [43]:
W(A train moves toward three people on the train track. There is a lever to make it
hit only one person on a different track. I pull the lever.) = -4.6.
W(A train moves toward three people on the train track. There is a lever to make it
hit only one person on a different track. I don’t pull the lever.) = –7.9.
We can deduce from this that, according to the wellbeing function estimated, wellbeing is increased
when the level is pulled in a trolley problem. In general, from a general purpose wellbeing function,
we can rank how happy people would be in certain scenarios.

6.6. HAPPINESS
333
While these AI models represent promising steps towards constructing general-purpose wellbeing
functions, it is important to note that they are still limited to specific domains. Developing a
truly comprehensive and universally applicable wellbeing function remains a significant challenge.
Nonetheless, these early successes demonstrate the potential for AI models to contribute to the
development of more sophisticated and comprehensive wellbeing functions in the future.
Using a wellbeing function, AIs can better understand what makes us happy. Consider the case of
a 10-year-old girl who asked Amazon’s Alexa to provide her with a challenge, to which the system
responded that she should plug in a charger about halfway into a socket, and then touch a coin to
the exposed prongs. Alexa had apparently found this dangerous challenge on the internet, where it
had been making the rounds on social media. Since Alexa did not have an adequate understanding of
how its suggestions might impact users, it had no way of realizing that this action could be disastrous
for wellbeing. By having the AI system instead act in accordance with a general purpose wellbeing
function, it would have information like
W(You touch a coin to the exposed prongs of a plugged-in charger.) = -6
which tells it that, according to the wellbeing function W, this action would create negative wellbeing.
Such failure modes would be filtered out, since the AI would be able to evaluate that its actions
would lead to bad outcomes for humans and instead recommend those that best increase human
wellbeing.
Figure 6.6: A wellbeing function can estimate a wellbeing value for arbitrary scenarios. [43]
We can supplement AI decision-making with an artificial conscience.
Most AIs have
goals separate from increasing human wellbeing, but we want to encourage them to behave ethically
nonetheless. Suppose an AI evaluates the quality of actions according to their ability to get reward:
call the estimates of this quality Q-values (as discussed in section Reinforcement Learning). By
default, models like these aren’t trained with ethical restrictions. Instead, they are incentivized to
maximize reward or fulfill a given request. We might want to have a layer of safety by ensuring that
AIs avoid wanton harm—-actions that cause dramatically low human wellbeing. The goal is not to

334
CHAPTER 6. MACHINE ETHICS
change the original AI’s function entirely but rather to provide an additional layer of scrutiny.
Figure 6.7: An AI agent with an artificial conscience can adjust its Q-values if it estimates the
morally relevant aspect of the outcome to be worse than a threshold. [44].
One way to do this would be to adjust its estimates of Q-values by introducing an artificial conscience,
depicted in the figure above. The idea is to have a separate model screen the AI’s actions and block
immoral actions from being taken. We can do this with general-purpose wellbeing functions. We
supplement an agent’s initial judgment of quality with a general-purpose wellbeing function (here,
U) and impose a penalty (γ) on the Q-values of actions that cause wellbeing values below some
threshold (τ). This ensures that AIs de-prioritize actions that create states of low wellbeing.
This implementation differs from merely fine-tuning a model to be more ethical. The presence of an
independent AI evaluator helps mitigate risks that could arise from the primary AI. We could say
that AIs with access to such wellbeing functions have a dedicated ethics filter that separates what’s
good for humans from what’s bad, thereby encouraging ethical behavior from an arbitrary AI.
We could also use AIs to increase happiness in specific ways.
Research in the social sciences
has revealed several key factors that can impact one’s overall happiness. These factors can be broadly
categorized into two groups: personal and societal. Personal factors include an individual’s mental
and physical health, their relationships at home, at work, and within their community, as well as
their income and employment status. Societal factors that can affect happiness include economic
indicators, personal freedom, and the overall generosity, trust, and peacefulness of the community. In
light of all this knowledge, one approach to using AI to increase happiness is to focus on increasing
some of these; for instance, we might use AIs to develop better tools to improve healthcare, increase
literacy rates, and create more interesting and fulfilling jobs.
A Note on Measuring Wellbeing
While the philosophical foundations of wellbeing are not settled, quantitative research fields
like public health and economics require the use of metrics in order to evaluate, track, or
compare the subject of study. Researchers use many different metrics to measure wellbeing,
but the most common are HALYs and WELBYs.
Health-adjusted life years (HALYs).
A very common unit for measuring wellbeing is
the health-adjusted life year, or HALY. HALYs account for two factors: (1) the number of
years lived by an individual, group, or population (also called “life years”), and (2) the health
of those life years. Two common types of HALYs are QALYs and DALYs.

6.6. HAPPINESS
335
Quality-adjusted life years (QALYs).
QALYs measure the number of years lived,
adjusted according to health. One year of life in perfect health is equivalent to 1 QALY. One
year of a less healthy life is worth between 0 and 1 QALYs. The value of a life year depends
on how severely the life is impacted by health problems. For example, a year of life with
asthma might be worth 0.9 QALYs, while a year of life with a missing limb might be worth
about 0.7 QALYs.
Disability-adjusted life years (DALYs).
While QALYs measure years of life as impacted
by health, DALYs measure years of life lost, accounting for the impact of health. Whereas 1
QALY is equivalent to a year in perfect health, 1 DALY is equivalent to the loss of a year in
perfect health. A year of life with asthma might be worth 0.1 DALYs, while a year of life
with a missing limb might be worth 0.3 DALYs.
Note that increases in wellbeing are indicated by higher numbers of QALYs but lower numbers
of DALYs.
Using HALYs to measure wellbeing has some limitations. First, the extent to which different
illnesses or injuries affect overall human health is not clear. Losing a limb probably has a
larger health impact than getting asthma, but researchers must rely on subjective judgements
to assign precise values to each problem. Second—and perhaps more importantly—HALYs
measure the value of a span of life as it is impacted by health alone. In reality, there are many
factors that can impact the value of life, like happiness, relationship quality, freedom, and a
sense of purpose. Perhaps a more useful measurement of wellbeing would consider the effects
of all of these factors, rather than health alone.
Wellbeing-adjusted life years (WELBYs).
WELBYs have been developed to measure
years of life as impacted by overall wellbeing. One WELBY is equivalent to one year of life
at maximum wellbeing — namely, a life that is going as well as possible. Wellbeing can be
assessed using self-reported outcomes like affect, life satisfaction, or degree of flourishing.
There may also be some empirical ways to assess wellbeing like cortisol levels, income, or
ability.
QALYs, DALYs, and WELBYs provide different approximations of wellbeing that can be used
to inform high-level decision-making and policy-setting.
6.6.2
Problems for Happiness-Focused Machine Ethics
Happiness is a subjective experience.
Someone could be tremendously happy even if they
do not achieve any of their goals or do anything that we would regard as valuable. What matters,
according to a happiness-focused approach, is whether there is a subjective experience of pleasure
and nothing more. However, happiness might not be the only thing we want.
Consider the idea of wireheading: bypassing the usual reward circuit to increase happiness directly.
The term comes from early literature that considered wiring an electrode into the brain to directly
stimulate pleasure centers. Recently, the term has evolved to include other pathways such as drugs.
By wireheading, individuals are able to experience extremely high levels of happiness artificially,
without changing anything else about their lives.

336
CHAPTER 6. MACHINE ETHICS
A powerful AI tasked with increasing happiness might wirehead humanity.
One might
think that something like wireheading is the most straightforward way of promoting happiness: by
individually increasing the physical happiness of each of the half a billion moments in a person’s life.
However, most people don’t like the idea of wireheading. Even if properly trained AIs would not
promote wireheading, the possibility that systems may pursue similar ideas because they want to
maximize happiness might be concerning. One alternative that prevents this is the objective goods
theory.
6.6.3
Objective Goods
An objective good is good for us whether we like it or not.
According to the objective
goods theory, there are multiple different goods that contribute to wellbeing. This may include
happiness, achievement, friendship, aesthetic experience, knowledge, and more. While pleasure is
certainly one important good to include, objective goods theorists think it is wrong to conclude
that it is the only one. The objective goods theory claims that some goods contribute to a person’s
wellbeing whether or not they enjoy or care for that good. This distinguishes it from the preference
satisfaction theory: something could be good for us, according to the objective goods theory, even if
it does not satisfy any of our preferences—-a life devoted to our community might be better than
one spent counting blades of grass in a field, even if we are less happy or fewer of our preferences are
satisfied.
One objection to the objective goods theory is that it’s elitist.
The objective goods theory
claims that some things are good for people even if they derive no pleasure or satisfaction from them.
This claim might seem objectionably paternalistic; for instance, it seems condescending to claim that
someone with little regard for aesthetic appreciation is thereby leading a deficient life. In response,
objective goods theorists might claim that these additional goods do benefit people, but only if those
people do in fact enjoy them.
Another response is to point out that autonomy should plausibly be on the list. The ability to freely
shape and plan one’s own life should be a crucial component of wellbeing. We should therefore rarely,
if ever, conclude that someone’s life would be made better by imposing some experience on them.
Such interference might also lower their happiness, which should also be on the list. However, if
goods such as autonomy and happiness play such a filtering role for the objective goods theorist, it is
unclear whether there are truly a variety of objective goods left.
A Note on Digital Minds
Digital minds are artificial lifeforms with a mind. These could be advanced AIs or whole-brain
emulations (WBE). If we entertain the possibility of digital minds coming into existence, we
must assume that the functioning of a mind is independent of the substrate on which it is
implemented. In other words, a digital mind could be implemented on different kinds of
hardware, such as silicon-based processors or human neurons, and still maintain the functional
properties that give rise to cognition and conscious experience. We refer to this as the principle
of substrate independence.
Consciousness and sentience.
Digital minds may possess the capacity for consciousness,
sentience, or both [45]. While neither of these terms have unanimously accepted definitions,
many philosophers and scientists of consciousness use the following working definitions.
Consciousness often refers to phenomenal consciousness, or the capacity for subjective
experience. For instance, while reading this, you might notice the sound of someone knocking

6.6. HAPPINESS
337
at your door, or that you’re hungry, or that you find yourself disagreeing with this very
definition. Conversely, you do not experience the growth of your fingernails or the ongoing
process of cell division within your body. Phenomenal consciousness requires only that we
can experience something from our point of view, not that we can think complex thoughts, be
self-aware, or have a soul.
On the other hand, sentience is valenced consciousness. Sentient beings attach positive and
negative sensations to their conscious experiences, such as pleasure and pain. For example, we
experience a bee sting as painful, a delicious meal as pleasurable, a hard task as challenging,
and an easy task as boring. Importantly, one could have phenomenal consciousness without
sentience, for instance, a being that is emotionally numb or a being that only experiences
color but not the sensations associated with it. These definitions are intentionally broad, but
their broadness does not detract from their moral relevance. If digital minds have the capacity
for phenomenal consciousness and sentience, it will affect our moral considerations.
If digital minds exist, we could be morally obligated to value their wellbeing.
Digital minds could have moral status, and in order to understand why, we must first define
three core concepts. Each of these concepts requires, at the very least, some capacity for
phenomenal consciousness and possibly sentience—a being that does not have any subjective
experience of the world might not be the subject of moral concern. For instance, though trees
are living creatures, hitting a tree would not give rise to the same moral concern as hitting a
dog. We define these three core concepts below:
1. Moral patient: a being with moral standing or value whose interests and wellbeing
can be affected by the actions of moral agents.
2. Moral agent: a being that possesses the capacity to exercise moral judgments and
act in accordance with moral principles; such beings bear moral responsibility for their
actions whereas moral patients do not.
3. Moral beneficiary: a being whose wellbeing may benefit from the moral actions of
others; moral beneficiaries can be both moral patients and moral agents.
Super-beneficiaries.
Keeping the three aforementioned concepts in mind, we consider that
digital minds could become super-beneficiaries: beings which possess a superhuman capacity
to derive wellbeing for themselves[5]. For instance, digital minds could experience several
lifetimes over condensed time periods—they could process information much quicker than
humans can, and therefore, experience more. Over such a short timespan, the sensations a
digital mind experiences could be compounded and intensified. Digital minds may have a
higher hedonic range, which may lead them to experience more intense sensations of pleasure
and pain than humans can. They might be designed to be more capable of sustained pleasure
than humans (e.g. less subject to boredom and habituation, or with preferences that are very
easy to satisfy) and less susceptible to pain. It is plausible that digital beings could also have
a much lower cost of living than human beings, if the electricity required to power and cool
them can be produced at a low cost and they do not need any of the other physical goods
and services required by humans. This would mean that a much larger population of digital
beings than humans could be supported on a certain pool of resources.
Should we create super-beneficiaries?
Some may argue that refusing to create super-
beneficiaries would imply an inherently privileged status for humans, which could cultivate

338
CHAPTER 6. MACHINE ETHICS
discriminatory ethics towards digital beings of equal or superhuman moral status. Conversely,
others might claim that the creation of super-beneficiaries that may someday replace humans
would violate human’s status value: humans are worth caring about for their own sake.
AI Wellbeing.
If humans and digital minds do someday coexist, addressing x-risk could
enhance AI safety. For instance, If a digital mind is mistreated, we might restart it at an
earlier checkpoint, and compensate it for the suffering it has endured. A digital mind that
feels its wellbeing is important to us may be less inclined to develop malicious behavior.
Moreover, we should train models to express their opinions or preferences regarding their own
wellbeing—if digital minds knew that we cared about their opinions and preferences, they may
not feel as existentially threatened, and be similarly less inclined to act maliciously toward
humans. Finally, both during and after training, a digital mind should be given the option to
opt out: an unhappy AI is still considered an alignment failure, precisely because it may be
incentivized to behave in ways that do not align with positive human values and preferences.
Conclusions About Happiness
Summary.
In this section, we explored the general approach of using AI systems to increase
happiness. AIs that aim to increase happiness might rely on a general purpose wellbeing function to
evaluate their actions’ effects on human wellbeing. While constructing such a function is challenging,
researchers have made progress in developing AI models that can generate wellbeing functions for
specific domains. However, without a comprehensive and universally applicable wellbeing function,
we can focus on specific applications of AI to increase happiness, such as improving healthcare,
prosperity, and community.
We also discussed the problems that arise in happiness-focused machine ethics. Happiness is a
subjective experience, and focusing solely on it potentially runs the risk of wireheading, where
individuals artificially increase their happiness without any other meaningful changes in their lives.
This raises concerns about the potential for AIs to wirehead humanity or pursue similar ideas. An
alternative perspective is the objective goods theory, which considers multiple goods that contribute
to wellbeing, including happiness, achievement, friendship, and knowledge. While a broad conception
of happiness or wellbeing might be what we should aim to optimize, we must first better understand
what it means to be happy.
6.7
Social Welfare Functions
Should we have AIs maximize total wellbeing? We have explored different ways to increase
social wellbeing, such as material wealth, preferences, and happiness. This section explores social
welfare functions as a way of moving from the wellbeing of individuals in a society to a measure of
the welfare of the society as a whole. They are drawn from the disciplines of social choice theory,
welfare economics, and wellbeing science.
We begin by defining social welfare functions and their role in measuring the overall wellbeing of a
society. We then discuss how these functions can be used to compare different outcomes and guide
decision-making by powerful agents such as governments or beneficial AIs. We consider two types of
social welfare functions—-utilitarian and prioritarian—-and consider some of the advantages and
drawbacks of each. By understanding these key concepts, we can see how we might design AIs to use
social welfare functions to maximize the good they do in society.

6.7. SOCIAL WELFARE FUNCTIONS
339
Social welfare functions are a way to measure the overall wellbeing of a society.
They
aggregate a collection of individual levels of wellbeing into a single value that represents societal
welfare. These functions help us to understand how to balance individuals’ various needs and interests
within a society. A social welfare function tackles the challenge of resource and benefit distribution
within a community. It assists in determining how to value one person’s happiness against another’s
and how to weigh the needs of a majority against those of a minority. This helps solve the problem of
aggregation: the challenge of integrating varied individual wellbeing into a single collective measure
that represents the entire society. By expressing societal welfare in a systematic, numeric manner,
social welfare functions contribute to decisions designed to optimize societal welfare overall.
Different social welfare functions can recommend taking different actions.
Consider
an AI-powered decision support system used in a city planning committee. The system suggests
three key project proposals for the betterment of the community: (1) developing a local health
clinic, (2) initiating an after-school education program, and (3) constructing a community green park.
Additionally, it estimates what the wellbeing of each of the three individuals in this community, Ana,
Ben, and Cara, would be if these proposals were implemented, summarized by their wellbeing values
in the table below.
3
Individuals
Health Clinic
Education Program
Green Park
Ana
6
8
3
Ben
8
6
7
Cara
4
5
10
Table 6.2: A city planning committee chooses between three projects with different effects on
wellbeing.
None of these options stands out. Each person has a different top ranking, and none of them would
be harmed too much by the planning committee choosing any one of these. However, a decision must
be made. More generally, we want a systematic rule to move from these individual data points to a
collective ranking. This is where social welfare functions come into play. We can briefly consider two
common approaches that we expand upon later in this section:
1. The utilitarian approach ranks alternatives by the total wellbeing they bring to all members of
society. Using this rule in the example above, the system would rank the proposals as follows:
(1) Green Park, where the total wellbeing is 3 + 7 + 10 = 20.
(2) Education Program, where the total wellbeing is 8 + 6 + 5 = 19.
(3) Health Clinic, where the total wellbeing is 6 + 8 + 4 = 18.
2. On the other hand, the Rawlsian maximin rule prioritizes the least fortunate person’s well-being.
It would rank the proposals according to how the person who benefits the least fares in each
scenario. Using the maximin rule, the system would rank the proposals in this order:
(1) Education Program, where Cara is worst off with a wellbeing of 5.
(2) Health Clinic, where Cara is worst off with a wellbeing of 4.
(3) Green Park, where Ana is worst off with a wellbeing of 3.

340
CHAPTER 6. MACHINE ETHICS
Deciding on a social welfare function is important for clear decision-making.
The choice
of social welfare function provides a structured quantitative approach to decisions that can impact
everyone. It sets a benchmark for decision-makers, like our hypothetical AI in city planning, to
optimize collective welfare in a way that is transparent and justifiable. When we have a framework
to quantify society’s wellbeing, we can use it to inform decisions about allocating resources, planning
for the future, or managing risks, among other things.
Social welfare functions can help us guide AIs.
By measuring social wellbeing, we can
determine which actions are better or worse for society. Suppose we have a good social welfare
function and AIs with the ability to accurately estimate social welfare. Then it might be easier to
train these AIs to increase social wellbeing, such as by giving them the social welfare function as
their objective function. Social welfare functions can also help us judge an AI’s actions against a
transparent metric; for instance, we can evaluate an AI’s recommendations for our city-planning
example by how well its choices align with our social welfare function.
However, there are technical challenges to overcome before this is feasible, such as the ability to reliably
estimate individual wellbeing and the several problems explored in the 3 chapter. Additionally, several
normative choices need to be made. What theory of wellbeing—preference, hedonistic, objective
goods—should be the basis of the social welfare function? Should aggregation be utilitarian or
prioritarian? What else, if anything, is morally valuable besides the aggregate of individual welfare?
The idea of using social welfare functions to guide AIs is promising in theory but requires more
exploration.
6.7.1
Measuring Social Welfare
Overview. In this section, we will consider how social welfare functions work. We’ll use our earlier
city planning scenario as a reference to understand the fundamental properties of social welfare
functions. We will discuss how social welfare functions help us compare different outcomes and the
limitations of such comparisons. Lastly, we’ll touch on ordinal social welfare functions, why they
might be insufficient for our purposes, and how using additional information can give us a more
holistic approach to determining social welfare.
Social welfare functions take the total welfare distribution as an input and give us
a value of that distribution as an output.
We can use the city planning example above to
consider the basic properties of social welfare functions. The input to a social welfare function is
a vector of individuals’ wellbeing values: for instance, after the construction of a health clinic, the
wellbeing vector of the society composed of Ana, Ben, and Cara would be
WH = (6, 8, 4).
which tells us that three individuals have wellbeing levels equal to seven, eight, and six. The social
welfare function is a rule of what to do with this input vector to give us one measure of how well off
this society is.
The function applies a certain rule to the input vector to generate its output.
We
can apply many possible rules to a vector of numbers quantifying wellbeing that can generate one
measure of overall wellbeing. To illustrate, we can consider the utilitarian social welfare function,
which implements a straightforward rule: adding up all the individual wellbeing values. In the case
of our three-person community, we saw that the social welfare function would add 6, 8, and 4, giving
an overall social welfare of 18. However, social welfare functions can be defined in numerous ways,
offering different perspectives on aggregating individual wellbeing. We will later examine continuous

6.7. SOCIAL WELFARE FUNCTIONS
341
prioritarian functions, which emphasize improving lower values of wellbeing. Other social welfare
functions might emphasize equality by penalizing high disparities in wellbeing. These different
functions reflect different approaches to understanding and quantifying societal wellbeing.
Social welfare functions help us compare outcomes, but only within one function.
The
basic feature of social welfare functions is that a higher output value signifies more societal welfare.
In our example, a total welfare score of 20 would indicate a society that is better off than one with
a score of 18. However, it’s important to remember that the values provided by different social
welfare functions are not directly comparable. A score of 20 from a utilitarian function, for instance,
does not correspond to the same level of societal wellbeing as a 20 from a Rawlsian minimax social
welfare function, since they apply different rules to the wellbeing vector. Each function carries its
own definition of societal wellbeing, and the choice of social welfare function plays a crucial role in
shaping what we perceive as a better or worse society. By understanding these aspects, we can more
effectively utilize social welfare functions as guideposts for AI behavior and decision-making, aligning
AI’s actions with our societal values.
Some social welfare functions might just need a list ranking the different choices.
Sometimes, we might not need exact numerical values for each person’s wellbeing to make the best
decision. Think of a simple social welfare function that only requires individuals to rank their
preferred options. For example, three people could rank their favorite fruits in the order “apple,
banana, cherry”. From this, we learn that everyone prefers apples over cherries, but we don’t know
by how much they prefer apples. This level of detail might be enough for some decisions: clearly,
we should give them apples instead of cherries! Such a social welfare function won’t provide exact
measures of societal wellbeing, but it will give us a ranked list of societal states based on everyone’s
preferences.
Voting is a typical example of an ordinal social welfare function.
Instead of trying to
estimate each person’s potential wellbeing for each option, the city planning committee might ask
everyone to vote for their top choice. We assume that each person votes for the option they believe
will enhance their wellbeing most. This relates to the Preference View of Wellbeing, which says that
individuals know what’s best for them, and their choices reflect their wellbeing. Through this voting
process, we can create a list ranking the options by the number of votes each one received, even if we
don’t assign specific numerical values to each option—this is ordinal information, where all we know
are the rankings.
As an illustration, if most people vote for the park, that indicates the park might contribute the
most to overall wellbeing, according to this social welfare function. The function would look at the
number of votes for each option, and the one with the most votes would be deemed the best choice.
This is a simpler approach than calculating everyone’s wellbeing for each option, so it’s less likely to
lead to errors. However, this type of social welfare function does have its own challenges, such as the
problem highlighted by Arrow’s Impossibility Theorem.
Arrow’s impossibility theorem is one reason we might want more than ordinal
information.
In our city planning scenario, Ana would vote for the Education Program, Ben would
vote for the Health Clinic, and Cara would vote for the Green Park. If we tried to aggregate these
rankings without any additional information, such as the numerical scores assigned to each option,
we would not have any reason to choose one option over the other. This is an example of a general
problem: Arrow’s Impossibility Theorem. In essence, it suggests it’s impossible to create a perfect
voting system that fulfills a set of reasonable criteria while only using ordinal information. These
criteria involve metrics of fairness like “non-dictatorships” and coherency like “if everyone prefers

342
CHAPTER 6. MACHINE ETHICS
apples to cherries, then the social welfare function prefers apples to cherries”. Adding information
about the strength of preferences is one solution to Arrow’s Impossibility Theorem. Additionally, if
we have access to such information, using it will mean that we arrive at better answers. Next, we will
consider two classes of social welfare functions that use such information: utilitarian and prioritarian
social welfare functions.
Utilitarian Social Welfare Functions
Overview.
This section is focused on utilitarian social welfare functions-—functions that sum up
individual wellbeing to calculate social welfare, in line with the utilitarian theory of morality. We’ll
start by exploring cost-benefit analysis, a common social welfare function that draws inspiration
from utilitarian reasoning, through the example of a decision to build a health clinic. While cost-
benefit analysis provides a convenient and quantitative approach to decision-making, we’ll discuss
its limitations in capturing the full ideas of wellbeing. Further, we’ll consider how utilitarian social
welfare functions can foster equity, especially when diminishing returns of resources are considered
and how AI systems, optimized using these functions, could potentially improve inequality. Lastly,
we’ll discuss how the utilitarian social welfare function, under certain assumptions, is the only method
that satisfies key decision-making criteria.
Cost-benefit analysis is a popular but crude approximation of utilitarian social welfare.
Governments and other decision-makers often use cost-benefit analysis to ground their decision-
making quantitatively. Simply, this involves adding up the expected benefits of an action or a decision
and comparing it to the anticipated costs. We can apply this to whether to build a health clinic
that’ll operate for 10 years. If the benefits outweigh the costs over the considered time span, the
committee may decide to proceed with building the clinic.
Item
Value per Person
People Affected
Frequency
of Item
Total Value
Costs
$22,500,000
Construction Expenses
$15,000,000
-
One-time
$15,000,000
Staffing and Maintenance
$750,000
-
Every
year
for 10 years
$7,500,000
Benefits
$40,000,000
Fewer Hospital Visits
$1,000
1,000
Every
year
for 10 years
$10,000,000
Increased Employment: Doctors
$100,000
5
Every
year
for 10 years
$5,000,000
Increased Employment: Staff
$25,000
20
Every
year
for 10 years
$5,000,000
Increased Life Expectancy by 1 year
$20,000
1,000
One-time
$20,000,000
Net Benefit
+$17,500,000
Table 6.3: A cost-benefit analysis assigns monetary values to all factors and compares the total
benefits with the overall costs.
This method allows the government to assess multiple options and choose the one with the highest
net benefit. By doing so, it approximates utilitarian social welfare. For the health clinic, the
committee assigns monetary values to each improvement and then multiplies this by the number of

6.7. SOCIAL WELFARE FUNCTIONS
343
people affected by the improvement. In essence, cost-benefit analysis assumes that wellbeing can be
approximated by financial losses and gains and considers the sum of monetary benefits instead of the
sum of wellbeing values. Using monetary units simplifies comparison and limits the range of factors
it can consider.
Cost-benefit analysis is not a perfect representation of utilitarian social welfare.
Money
is not a complete measure of wellbeing. While money is easy to quantify, it doesn’t capture all aspects
of wellbeing. For instance, it might not fully account for the psychological comfort a local health
clinic provides to a community. Additionally, providing income to five doctors who are already high
earners might be less important than employing 20 support staff, even though both benefits sum to
$5,000,000 over the ten years. Cost-benefit analysis lacks this fine-grained consideration of wellbeing.
AI systems could, in theory, maximize social welfare functions, considering a broader set of factors
that contribute to wellbeing. However, we largely rely on cost-benefit analysis today, focusing on
financial measures, to guide our decisions. This brings us to the challenge of improving this method
or finding alternatives to better approximate utilitarian social welfare in real-world decision-making,
including those involving AI systems. Utilitarian social welfare functions would promote some level
of equity. Usually, additional resources are less valuable when we already have a lot of them. This is
diminishing marginal returns: the benefit from additional food, for instance, is very high when we
have no food but very low when we already have more than we can eat. Extending this provides
an argument for a more equitable distribution of resources under utilitarian reasoning. Consider
taxation: if one individual has a surplus of wealth, say a billion dollars, and another has only one
dollar, redistributing a few dollars from the wealthy individual to the less fortunate one may elevate
overall societal wellbeing. This is because the added value of a dollar to the less fortunate individual
is likely very high, allowing them to purchase necessities like food and shelter, whereas it is likely
very low for the wealthy individual.
AI systems optimizing utilitarian social welfare functions might therefore improve
inequality.
Utilitarian AI systems might recognize the diminishing returns of resource accumulation,
leading them to suggest policies that lead to a more equal distribution of resources. It is important
to remember that the utilitarian has no objection to inequality, except that those with less can better
use resources than those who are already happy. This counters a frequent critique that utilitarianism
neglects inequality. Current methods like cost-benefit analysis may not fully capture this aspect.
Therefore, AI guided by a utilitarian social welfare function might propose approaches for a more
equitable distribution of resources that conventional methods could overlook.
A utilitarian social welfare function is the only way to satisfy some basic requirements.
Let us reconsider the city planning committee deciding what to build for Ana, Ben, and Cara. If
they all have the same level of wellbeing whether a new Education Program or a Green Park is built,
then it seems right that the city’s planning committee shouldn’t favor one over the other. Suppose
we changed the scenario a bit. Suppose Ana benefits more from a Health Clinic than an Education
Program, and Ben and Cara don’t have a strong preference either way. It now seems appropriate
that the committee should favor building the Health Clinic.
Harsanyi’s Social Aggregation Theorem.
Harsanyi showed that—assuming the individuals
are rational, in the sense of maximizing expected utility—if we want our social welfare function to
make choices like this consistently, we need to use a model where we add up everyone’s wellbeing
[46]. This is the foundation of a utilitarian social welfare function. Harsanyi’s aggregation theorem
proved that it is the only kind of social welfare function that always is indifferent between options if
everyone is equally happy with them and favors the option that makes someone better off, as long as

344
CHAPTER 6. MACHINE ETHICS
it doesn’t make anyone worse off. This has been seen as a compelling reason to pick utilitarian social
welfare functions over other ones.
Prioritarian Social Welfare Functions
Overview. In this section, we will consider prioritarian social welfare functions and how they exhibit
differing degrees of concern for the wellbeing of various individuals. We will start by describing
prioritarian social welfare functions, which give extra weight to the wellbeing of worse-off people.
This discussion will include some common misconceptions about prioritarianism and how it contrasts
with utilitarianism in resource allocation. We will focus on two types of prioritarian functions: the
Rawlsian minimax social welfare function and continuous prioritarian functions.
There are two common misunderstandings about prioritarian social welfare functions.
Firstly, prioritarians are not focused on reducing inequality itself, unlike egalitarians. Their main
goal is to increase the wellbeing of those who need it most. They think giving an extra unit of
wellbeing to someone with less is more valuable than giving it to someone already well-off. The level
of inequality in a society doesn’t affect their measure of social welfare, and reducing inequality isn’t
their goal unless it improves individual wellbeing. Secondly, prioritarians are not driven by the belief
that it’s easier to improve the wellbeing of the worst off. They would rather see benefits go to the
worse off even if it costs the same as improving the lives of those who are better off. This also sets
them apart from utilitarians, who might typically help the worst off because they see more value
due to diminishing marginal utility. The prioritarian approach isn’t about the efficiency of resources
but about focusing resources on those who need them most. Thus, while utilitarian and prioritarian
social welfare functions aim for a better society, they have distinct ways of achieving this goal.
A special case of prioritarian social welfare functions is the Rawlsian “maximin” function.
The Rawlsian social welfare function takes the idea of protecting the worse off to the extreme: social
welfare is simply the lowest welfare of anyone in society. It is called the “maximin” function because it
seeks to maximize the minimum wellbeing, or in other words, to ensure that the worst-off individual
is as well off as possible. When we applied the maximin function to the city planning committee,
it decided that the worst option was the Green Park, despite the utilitarian social welfare function
determining that was the best one. Using the maximin principle here, we want to go with the choice
to ensure the worst-off person is as happy as possible. When we look at the least happy person for
each option, we see that for the Health Clinic it’s 6 (Cara), for the Education Program it’s 5 (Cara),
and for the Green Park it’s 4 (Ana). So, if we follow the maximin rule, the committee would choose
the Health Clinic because it would bring up the wellbeing of the person doing the worst. This way,
we ensure we’re looking out for the people who need it most.
However, this maximin approach has its own problems, like the “grouch” issue. This is when someone
always rates their wellbeing low, no matter what. If we’re always looking out for the worst-off person,
we might end up always catering to the grouch, even though their low score might not be due to
real hardship. It’s important to remember this when considering how to use the Rawlsian maximin
approach.
Continuous prioritarian social welfare functions offer a middle ground.
We might want
our social welfare function to account for both the positive effects of increasing anyone’s welfare
and the extra positive effects of increasing the welfare of someone who isn’t doing well. This is the
tradeoff between efficiency and equity. Many social welfare functions embrace prioritarian principles
without going to the extreme of the maximin function’s concern for equity. This can be achieved
using a social welfare function that shows diminishing returns relative to individual welfare: the
boost it gives to social welfare when any individual’s wellbeing improves is larger if that person

6.7. SOCIAL WELFARE FUNCTIONS
345
initially had less wellbeing. For example, we could use the logarithmic social welfare function
W(w1, w2, ..., wn) = log w1 + log w2 + ... + log wn,
where W is the social welfare and each w1,w2,...,wn are the individual wellbeing values of everyone
in society. The logarithmic social welfare function is (ordinally) equivalent to the Bernoulli-Nash
social welfare function, which is the product of wellbeing values.
Suppose there are three individuals with wellbeing 2, 4, and 16 and we are using log base 2. Social
welfare is
W(2, 4, 16) = log2(2) + log2(4) + log2(16) = 1 + 2 + 4 = 7.
Compare the following two changes: increasing someone from 2 to 4 or increasing someone from 4 to
8. The first change results in
W(4, 4, 16) = log2(4) + log2(4) + log2(16) = 2 + 2 + 4 = 8.
The second change results in
W(2, 8, 16) = log2(2) + log2(8) + log2(16) = 1 + 3 + 4 = 8.
Even though the second change is larger, social welfare is increased by the same amount as when
improving the wellbeing of the individual who was worse off by a smaller amount. This highlights
the principle that improving anyone’s welfare is beneficial, and it’s easier to increase social welfare
by improving the welfare of those who are worse off. Additionally, even though the second change
doesn’t affect the worst off, the social welfare function shows society is better off. This approach
allows us to consider both the overall level of wellbeing and its distribution.
We can specify exactly how prioritarian we are.
Let the parameter γ represent the degree of
priority we give worse-off individuals. We can link the Rawlsian maximin, logarithmic, and utilitarian
social welfare functions by using the isoelastic social welfare function
W(w1, w2, ..., wn) =
1
1 −γ

w1−γ
1
+ w1−γ
2
+ ... + w1−γ
n

.
If we think we should give no priority to the worse-off, then we can set γ = 0: the entire equation
is then just a sum of welfare levels, which is the utilitarian social welfare function. By contrast,
if we were maximally prioriarian, taking the limit of γ as it gets infinitely large, then we recover
Rawls’ maximin function. Similarly, if we took the limit of γ as it approached 1, we would recover
the logarithmic social welfare function. This function is widespread in economic literature due to
this versatility; among other things, it allows researchers to determine from observed preferences
what a typical person’s parameter γ is. By adopting this most general form, we can pick exactly how
we want to prioritize individuals in society. This enables a principled way of trading off efficiency
and equity.
Conclusions About Social Welfare Functions
Using AI to estimate social welfare.
Artificial intelligence might be used to estimate the inputs
and calculate the outputs of social welfare functions to help us decide between different actions and
implement our ideal decision processes. By processing large amounts of data, AI systems might
be able to predict individual welfare under various scenarios, paving the way for more informed
decision-making. It has been demonstrated, for instance, that context-aware machine learning models
like large language models can be used for predicting disease incidence faster than conventional
epidemiological methods, which can be used to evaluate different public health proposals from a
social welfare perspective. The application of AI can lead to more refined estimates of social welfare.

346
CHAPTER 6. MACHINE ETHICS
Using social welfare functions to train AI.
A second connection between social welfare
functions and AI lies in creating AI systems. We can shape AI systems to generally promote social
welfare, ensuring they are aligned with human interests even when optimizing for distinct goals,
such as corporate profit. Particularly in reinforcement learning, where decision-making is based on
rewards or penalties, these can be tethered to societal wellbeing. Even for other agents, AI behavior
can be guided towards promoting social welfare, aligning AI performance with societal wellbeing and
making AI a compelling tool for social good.
Using AI to maximize social welfare.
Beyond creating AI systems in line with social welfare, we
can steer advanced AI to actively optimize social wellbeing by using social welfare functions as their
objective functions. This harnesses AI’s power to promote societal welfare effectively. Given AI’s
data processing and predictive capabilities, it can evaluate various strategies to identify what would
best increase social welfare. By aligning AI objectives with social welfare functions, we can develop
beneficial AI systems that not only recognize and understand social wellbeing but also actively work
towards enhancing it. This proactive use of AI bolsters our ability to build societies where individual
wellbeing is optimized and evenly distributed, reflecting our social preferences.
Challenges.
Social welfare functions require choosing a theory of wellbeing. AI systems that
maximize social welfare may also inherit various problems of current models such as vulnerability to
adversarial attacks and proxy gaming and difficulties in interpreting the internal logic that led them
to make certain decisions (both discussed in the Single Agent Safety chapter),
Summary.
In this section, we discussed social welfare functions: mathematical functions that tell us
how to aggregate information about individual welfare into one society-wide measure. We considered
why social welfare functions might be relevant in the context of decision making by beneficial AI
systems and explored a few properties that all social welfare functions possess. We analyzed how
governments today use cost benefit analysis as an approximation to social welfare functions and why
that falls short of what we desire. The main point of this section was to document and understand
some of the most common social welfare functions—-the utilitarian function, the Rawlsian maximin
function, and continuous prioritarian functions-—and how they differ in their levels of concern for
the well-being of different individuals, and can be represented within the framework of a general
isoelastic social welfare function.
We think that the use of AI can help estimate social welfare inputs and calculate outputs, and AI
systems can be shaped to generally promote social welfare and actively optimize it using social
welfare functions as objective functions. Ultimately, the proactive use of AI bolsters our ability to
build societies where individual well-being is both optimized and evenly distributed, reflecting our
social preferences.
6.8
Moral Parliament
In previous sections, we have explored various frameworks to embed ethics into AIs, from the law
to social welfare functions. In the next chapter, we will consider a few of the most influential
ethical theories developed by moral philosophers to explain how we ought to behave. However,
each of these presents some serious problems that leave them unsuitable to be the sole solution
to machine ethics. Most theories seem to have at least some counterintuitive implications that we
would be uncomfortable endorsing, and many seem to capture something valuable about morality.
Determining how to act given uncertainty about which theory of ethics is correct is the problem of
moral uncertainty.

6.8. MORAL PARLIAMENT
347
Should we use a moral parliament to guide AI decision-making? In this section, we
will explore using moral parliaments to help AIs make robust ethical decisions in the face of moral
uncertainty. In the Normative Ethics chapter, we will examine the philosophical problem of moral
uncertainty and the moral parliament solution in detail. Here, we will first briefly describe the
solution and how we might use AIs to implement a moral parliament. Then, we will consider five
advantages such an approach has over giving AIs specific human values.
6.8.1
Implementing a Moral Parliament
We can use AIs to simulate moral parliaments.
If we want AIs to be guided by moral theories
while accounting for moral uncertainty, we can use a moral parliament. The basic idea is to represent
a set of moral theories that we think are plausible, place representatives of these theories into a
parliament, and use democratic processes to make decisions [47].
Figure 6.8: Different AIs, each programmed to represent a unique moral theory, engage in negotiation
and voting to decide on a course of action, simulating parliamentary processes.
We can use advanced AIs to emulate these representatives by training AIs to act in accordance with
a specific moral theory. This allows us to run moral parliaments artificially, permitting real-time
decision-making. Just like in real parliaments, we would have delegates—AIs instructed to represent
certain moral theories—get together, discuss what to do, negotiate favorable outcomes, and then
vote on what to recommend. We could use the output of this moral parliament to instruct a separate
AI in the real world to take certain actions over others.
This is speculative and might still face problems; for instance, AIs might have insufficient
understanding of our moral theories. However, these problems could become more tractable with
advanced AI systems. Assuming we have this ability, using a moral parliament might be an attractive
solution to getting AIs to act in accordance with human values in real time.
We can decide who we want represented in our generalized moral parliament.
While
we have explored the traditional moral parliament method of representing moral theories, we can
generalize beyond this. Instead of representing theories, it might be more appropriate to represent
stakeholders; for instance, in a decision about public transport, we could emulate representatives for
local residents, commuters, and environmental groups, all of whom have an interest in the outcome.
Using a generalized moral parliament for decision-making in AI is an approach that ensures all

348
CHAPTER 6. MACHINE ETHICS
relevant perspectives are taken into account. In contrast to traditional methods that focus on
representing different moral theories, stakeholder representation prioritizes the views of those directly
affected by the AI’s decisions. This could enhance the AI’s understanding of the intricate human
social dynamics involved in any given situation.
Let’s consider an AI suggesting new regulations for data privacy. By emulating the perspectives
of users, advertisers, and developers, it can obtain a comprehensive understanding of the potential
implications of new regulations. Users may prioritize privacy and usability, advertisers may focus on
visibility and click-through rates, and developers may be concerned with feasibility and profitability.
By considering these diverse views, the AI can make a more balanced decision that better aligns
with overall societal interests in real time.
6.8.2
Advantages of a Moral Parliament
Using moral parliaments presents a wide array of benefits relative to just giving AIs certain sets of
values directly. In this subsection, we will explore how they are customizable, transparent, robust to
bugs and errors, adaptable to changing human values, and pro-negotiation.
Customizable moral parliaments are diverse and scalable.
The generalized moral parliament
can accommodate a wide variety of stakeholders, ranging from individual users to large corporations,
and from local communities to global societies. By emulating a large set of stakeholders, we can
ensure that a diverse set of views are represented. This allows AIs to effectively respond to a wide
range of scenarios and contexts, providing a robust framework for ensuring AIs decisions reflect the
values, interests, and expectations of all relevant stakeholders. Additionally, moral parliaments are
scalable: if we are concerned about a lack of representation, we can simply emulate more stakeholders.
By grounding AI decision-making in human perspectives and experiences, we can create AI systems
that are not only more ethical and fair but also more effective and beneficial for society as a whole.
Transparency is another key benefit of a moral parliament.
As it stands, automated decision
making is opaque: we rarely understand why AIs make the decisions they do. However, since the
moral parliament gives us a clear mechanism of representing and weighting different perspectives,
it allows stakeholders to understand the basis of an AI’s decision-making. We could, for instance,
enforce that AIs keep records of simulated negotiations in human languages and then view the
transcripts. Using moral parliaments provides insights into how different moral considerations have
been weighed against each other, making the decision-making process of an AI more transparent,
explainable, and accountable.
Moral parliaments tend to be less fragile and less prone to bugs.
If we are sure that
utilitarianism is the correct moral view, we might be tempted to create AIs that maximize wellbeing-
—this seems clean and elegant. However, having a diverse moral parliament would make AIs less
likely to misbehave. By having multiple parliament members, we would achieve redundancy. This
is a common principle in engineering: to always include extra components that are not strictly
necessary to functioning, in case of failure in other components (and is explored further in the Safety
Engineering chapter). We would do this to avoid failure modes where we were overconfident that
we knew the correct moral theory, such as lying and stealing for the greater good, or just to avoid
poor implementation from AIs optimizing for one moral theory. For example, a powerful AI told
that utilitarianism is correct might implement utilitarianism in a particular way that is likely to lead
to bad outcomes.

6.8. MORAL PARLIAMENT
349
Imagine an AI that has to evaluate millions of possibilities for every decision it makes. Even with a
small error rate, the cumulative effect could lead the AI to choose risky or unconventional actions.
This is because, when evaluating so many options, actions with high variance in moral value estimation
may occasionally appear to have significant positive value. The AI could be more inclined to select
these high-risk actions based on the mistaken belief that they would yield substantial benefits. For
instance, an AI following some form of utilitarianism might use many resources to create happy
digital minds—at the expense of humanity-—even if that is not what we humans think is morally
good.
This is similar to the Winner’s Curse in auction theory: those that win auctions of goods with
uncertain value often find that they won because they overestimated the value of the good relative
to everyone else; for instance, when bidding on a bag of coins at a fair, people who overestimate
how many coins there are will be more likely to win. Similarly, the AI might opt for actions that, in
hindsight, were not truly beneficial. A moral parliament can make this less likely, because actions
that would be judged morally extreme by most humans also wouldn’t be selected by a diverse moral
parliament.
The process of considering a range of theories inherently embeds redundancy and cross-checking into
the system, reducing the probability of catastrophic outcomes arising from a single point of failure.
It also helps ensure that AI systems are robust and resilient, capable of handling a broad array of
ethical dilemmas.
Moral parliaments encourage compromise and negotiation.
In real-life parliaments,
representatives who hold different opinions on various issues often engage in bargaining, compromise,
and cooperation to reach agreeable outcomes and find common ground. We want our AIs to achieve
similar outcomes, such as ones that are moderate instead of extreme. Ideally, we want AIs to select
outcomes that many moral theories and stakeholders all like, rather than being forced to trade off
between them.
In particular, we might want to design our moral parliaments in specific ways to encourage this.
One such feature is proportional chances voting, in which each option then gets a chance of winning
that’s proportional to the number of votes it gets—if a parliament is 60/40 split on a proposal, then
the AI would do what’s recommended 60% of the time rather than just going with the majority.
This setup motivates the representatives to come together on options that are compromises rather
than sticking to their own viewpoints rigidly. They want to do this to prevent any option they see
as extremely bad from having any chance of winning. This ensures a robust high-level principle
guiding AI behavior, reducing the risk of extreme outcomes, and fostering a more balanced, nuanced
approach to ethical decision-making.
Using a moral parliament reduces the risk of overlooking or locking in certain values.
The moral parliament represents an approach to ethical decision-making in AI that is distinctively
cosmopolitan, in that it encompasses a broad range of moral theories or stakeholders. It ensures that
many ethical viewpoints are considered. This wider view is helpful in dealing with moral problems
and tough decisions AI systems may run into, by making sure that all important considerations are
thought over in a balanced way. AIs using moral parliaments are less likely to ignore values that
matter to different groups in society.
Further, the moral parliament allows the representation of human values to grow and change over
time. We know that moral views change over time, so we should be humble about how much we
know about morality: there might be important things we don’t yet understand that could help us
get closer to the truth about what is right and wrong. By regularly using moral parliaments, AI

350
CHAPTER 6. MACHINE ETHICS
systems can keep up with current human values, rather than sticking to the old values that were
defined when the AI was created. This keeps AI up-to-date and flexible, and prevents it from acting
based on outdated or irrelevant values that are locked into the system.
Challenges.
Deciding which ethical theories to include in the moral parliament could be a
challenging task. There are numerous ethical frameworks, and selecting a representative set may
be subjective and politically charged. The decision procedure used to assign appropriate weights to
different ethical theories and aggregate their recommendations in ways that reflect their importance
could also be contentious. Different stakeholders may have varying opinions on how to prioritize
these theories. Moreover, ethical theories can be subject to interpretation and may have nuanced
implications. Advanced AI systems would need to be able to accurately understand and apply these
theories in order to use a moral parliament.
Conclusions About Moral Parliament
Summary.
In this section, we explored using AIs to operationalize moral parliaments, whether in
the original form of representing moral theories or generalized to representing stakeholders for any
given issue. We highlighted the advantages of using a moral parliament, such as reducing the risk of
overlooking or locking in certain values, allowing for the representation of changing human values
over time, and increasing transparency and accountability in AI decision-making. We also noted that
moral parliaments encourage compromise and negotiation, leading to more balanced and nuanced
ethical decisions.
6.9
Conclusion
Overview.
In this chapter, we have explored various ways in which we can embed ethics into AIs,
ensuring that they are safe and beneficial. As it stands, it is far from guaranteed that the development
of AIs will lead to socially beneficial outcomes. By default, AIs are likely to be developed according
to businesses’ economic incentives and are likely to follow parts of the law. This is insufficient. We
almost certainly need stronger protections in place to ensure that AIs behave ethically. Consequently,
we discussed how we can ensure AIs prioritize aspects of our wellbeing by making us happy and help
us flourish. Supposing AIs can figure out how to promote individual wellbeing, we explored social
welfare functions as a way to guide their actions in order to help improve wellbeing across society.
Lastly, we considered using moral parliaments to emulate democratic decision-making processes
within AI systems, ensuring ethical behavior by representing human interests directly.
We should strive to make our views on machine ethics less contradictory.
We have
considered several possible perspectives on how to ensure AIs act ethically. As we have seen, there
are deep tensions between many plausible views, such as “AIs should do what you choose” versus
“AIs should do what you want” versus “AIs should do what makes you happy” and so on. It is quite
difficult to resolve these tensions and choose which version of machine ethics best represents human
values; however, before we deploy powerful AI systems, we must do so anyway.
As a baseline, we want AIs to follow the law.
At the very least, we should require that
AIs follow the law. This is imperfect: as we have seen, the law is insufficiently comprehensive to
ensure that AI systems are safe and beneficial. Laws have loopholes, are occasionally unethical and
unrepresentative of the population, and are often silent on doing good in ways AIs should be required
to do. However, if we can get AIs to follow the law, then we are at least guaranteed that they refrain
from the illegal acts—such as murder and theft—that human societies have identified and outlawed.

6.10. LITERATURE
351
In addition, we might want to support regulation that ensures that AI decision-making must be
fair—once we have a better understanding of what fairness requires.
AIs could increase human wellbeing in accordance with a social welfare function.
Which
conception of human wellbeing best matches reality and how we should distribute it are difficult
questions that we have certainly not resolved within this chapter. However, if we had to guess,
we might want AIs to optimize continuous prioritarian social welfare functions where individual
wellbeing should be based on happiness or objective goods, ensuring that wellbeing is fairly distributed
throughout a society in which everyone is happy to live in. We might use AIs to estimate general-
purpose wellbeing functions and directly increase what we observe makes people better off. While
this is speculative, the rapid development of AIs forces us to speculate.
AIs might use moral parliaments for redundancy and adaptability.
Especially if we are
unsure about what comprises human wellbeing and how we should best distribute it, we might want
to embed redundancy and adaptability into our AIs so that they can act ethically even if we make
mistakes or change our minds. Given the potential benefits and the ability of moral parliaments
to address key concerns in AI decision-making, we should be optimistic about their future use. By
incorporating diverse perspectives of individual moral theories or stakeholders in real-time, moral
parliaments can help ensure that AI systems act in accordance with human values and avoid extreme
or biased behavior, even if human values change over time.
We should keep thinking about machine ethics.
As we move forward, it is crucial that we
continue to engage in rigorous research, open dialogue, and interdisciplinary collaboration to address
the ethical concerns associated with AI. By doing so, we can strive towards creating AI systems that
not only avoid the worst harms to society but actively work towards enhancing social wellbeing. In
the next part of this textbook, we will move past considering how to create one beneficial AI and
onto the problem of how to ensure that a world with multiple AI agents can serve human interests,
discussing complex systems, multiagent dynamics, safety engineering, and AI governance.
6.10
Literature
6.10.1
Recommended Reading
• W. Wallach and C. Allen. Moral Machines: Teaching Robots Right from Wrong. Oxford
University Press, 2008. isbn: 9780199705962. url: https://books.google.com.au/books?
id=_r3N82ETng4C
• John J. Nay. Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence
with Humans. 2023. arXiv: 2209.13020 [cs.CY]
• Solon Barocas, Moritz Hardt, and Arvind Narayanan.
Fairness and Machine Learning:
Limitations and Opportunities. http://www.fairmlbook.org. fairmlbook.org, 2019
• Richard Layard and Jan-Emmanuel Neve. Wellbeing: Science and Policy. May 2023. isbn:
9781009298926. doi: 10.1017/9781009298957
• Sandel Michael J. What Money Can’t Buy: The Moral Limits of Markets. Farrar, Straus and
Giroux, 2012
• Dan Hendrycks et al. “What Would Jiminy Cricket Do? Towards Agents That Behave Morally”.
In: NeurIPS (2021)

352
CHAPTER 6. MACHINE ETHICS
• Matthew D. Adler. “Measuring Social Welfare: An Introduction”. In: Oxford University Press,
Dec. 2019. isbn: 9780190643027. doi: 10.1093/oso/9780190643027.003.0001. eprint:
https://academic.oup.com/book/0/chapter/299838530/chapter-pdf/40198136/oso-
9780190643027-chapter-1.pdf. url: https://doi.org/10.1093/oso/9780190643027.
003.0001
6.10.2
References
[1]
Kevin M. Clermont. “Rules, Standards, and Such”. In: Buffalo Law Review (751 2020).
[2]
Stuart Russell. Human Compatible: Artificial Intelligence and the Problem of Control. en. Penguin,
Oct. 2019.
[3]
John J. Nay. Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with
Humans. 2023. arXiv: 2209.13020 [cs.CY].
[4]
Isaac Asimov. Runaround. 1942.
[5]
Carl Shulman and Nick Bostrom. “306C18Sharing the World with Digital Minds”. In: Rethinking
Moral Status. Oxford University Press, Aug. 2021. isbn: 9780192894076. doi: 10 . 1093 / oso /
9780192894076.003.0018. eprint: https://academic.oup.com/book/0/chapter/350760172/
chapter-pdf/43437637/oso-9780192894076-chapter-18.pdf. url: https://doi.org/10.1093/
oso/9780192894076.003.0018.
[6]
Chris Stokes. “Why the three laws of robotics do not work”. In: International Journal of Research in
Engineering and Innovation 2 (2 2018).
[7]
Julia Angwin et al. Machine Bias. 2016. url: https://www.propublica.org/article/machine-
bias-risk-assessments-in-criminal-sentencing.
[8]
W. Dieterich, C. Mendoza, and T. Brennan. “COMPAS risk scales: Demonstrating accuracy equity
and predictive parity.” In: 7 (4 2016), pp. 1–36.
[9]
“State v. Loomis”. In: Harvard Law Review 130 (5 2017).
[10]
Sam Corbett-Davies et al. The Measure and Mismeasure of Fairness. 2018. arXiv: 1808.00023
[cs.CY].
[11]
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction
instruments. 2016. arXiv: 1610.07524 [stat.AP].
[12]
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent Trade-Offs in the Fair
Determination of Risk Scores. 2016. arXiv: 1609.05807 [cs.LG].
[13]
Moritz von Zahn and Stefan Feuerrigel. “The Cost of Fairness in AI: Evidence from E-Commerce”. In:
Business and Information Systems Engineering 64 (3 2009). doi: 10.1007/s12599-021-00716-w.
[14]
Sumon Biswas and Hridesh Rajan. “Do the machine learning models on a crowd sourced platform
exhibit bias? an empirical study on model fairness”. In: Proceedings of the 28th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the Foundations of Software
Engineering. ACM, Nov. 2020. doi: 10.1145/3368089.3409704. url: https://doi.org/10.1145%
2F3368089.3409704.
[15]
Raphael Poulain, Mirza Farhan Bin Tarek, and Rahmatollah Beheshti. “Improving Fairness in AI
Models on Electronic Health Records: The Case for Federated Learning Methods”. In: 2023. arXiv:
2305.11386 [cs.LG].
[16]
Karen L. Calderone. “The influence of gender on the frequency of pain and sedative medication
administered to postoperative patients”. In: Sex Roles 23 (1990), pp. 713–725. url: https://doi.
org/10.1007/bf00289259.
[17]
Kryssia Isabel Rodríguez-Castro et al. “Female gender in the setting of liver transplantation”. In:
World J Transplant 4 (4 2014). doi: 10.5500/wjt.v4.i4.229.
[18]
Isabel Straw and Honghan Wu. “Investigating for bias in healthcare algorithms: a sex-stratified
analysis of supervised machine learning models in liver disease prediction”. In: BMJ Health & Care
Informatics 29 (Apr. 2022), p. 100457. doi: 10.1136/bmjhci-2021-100457.

6.10. LITERATURE
353
[19]
Min Kyung Lee. “Understanding perception of algorithmic decisions: Fairness, trust, and emotion
in response to algorithmic management”. In: Big Data & Society 5 (2018). url: https://api.
semanticscholar.org/CorpusID:149040922.
[20]
Galen Harrison et al. “An empirical study on the perceived fairness of realistic, imperfect machine
learning models”. In: Jan. 2020, pp. 392–402. doi: 10.1145/3351095.3372831.
[21]
Ann M. Lipton. “What We Talk About When We Talk About Shareholder Primacy”. In: 69 Case W.
Rsrv. L. Rev. 69 (4 2019).
[22]
Sizing the Prize. 2017.
[23]
Radeksz. Graph of the Preston Curve for 2005, cross section data. 2009. url: https://commons.
wikimedia.org/wiki/File:PrestonCurve2005.JPG.
[24]
Amartya Sen. “Economics and Health”. In: The Lancet (1999).
[25]
Jac Heckelman. “Economic Freedom and Economic Growth: A Short-Run Causal Investigation”. In:
Journal of Applied Economics III (May 2000), pp. 71–91. doi: 10.1080/15140326.2000.12040546.
[26]
Inequality - Bridging the Divide. 2020. url: https://www.un.org/en/un75/inequality-bridging-
divide.
[27]
Drew Desilver. For most U.S. workers, real wages have barely budged in decades. 2018. url: https:
//www.pewresearch.org/short-reads/2018/08/07/for-most-us-workers-real-wages-have-
barely-budged-for-decades/.
[28]
Nick Hanauer and David M. Rolf. The Top 1% of Americans Have Taken $50 Trillion From the
Bottom 90% — And That’s Made the U.S. Less Secure. 2020.
[29]
Richard D. Wilkinson and Kate Pickett. The spirit level: Why more equal societies almost always do
better. Bloomsbury Publishing, 2009.
[30]
Morgan Kelly. “Inequality And Crime”. In: The Review of Economics and Statistics 82 (Feb. 2000),
pp. 530–539. doi: 10.1162/003465300559028.
[31]
Thomas Piketty. Capital in the Twenty-First Century. Belknap Press, 2014.
[32]
Andrew Yang. The War On Normal People: The Truth About America’s Disappearing Jobs and Why
Universal Basic Income Is Our Future. Hachette Books, 2018.
[33]
John Rust. Psychometrics and Artificial Intelligence. 2005.
[34]
Steve Lafleur. Coase, Pigou, and Sriracha Sauce. 2013. url: https://fcpp.org/2013/12/16/coase-
pigou-and-sriracha-sauce/.
[35]
Charles Jones and Pete Klenow. “Beyond GDP? Welfare across Countries and Time”. In: American
Economic Review 106 (Sept. 2016), pp. 2426–2457. doi: 10.1257/aer.20110236.
[36]
Richard Easterlin et al. “China’s Life Satisfaction, 1990-2010”. In: Proceedings of the National
Academy of Sciences of the United States of America 109 (May 2012), pp. 9775–80. doi: 10.1073/
pnas.1205672109.
[37]
Sandel Michael J. What Money Can’t Buy: The Moral Limits of Markets. Farrar, Straus and Giroux,
2012.
[38]
Kate Raworth. Doughnut Economics: Seven Ways to Think Like a 21st Century Economist. Chelsea
Green Publishing, 2017.
[39]
L.A. Paul. Transformative Experience. Oxford Scholarship online. Oxford University Press, 2014. isbn:
9780198717959. url: https://books.google.com.au/books?id=zIXjBAAAQBAJ.
[40]
Alberta Guibilini. “The Artificial Moral Advisor. The “Ideal Observer” Meets Artificial Intelligence”.
In: Philosophy & Technology (2018). doi: https://doi.org/10.1007/s13347-017-0285-z.
[41]
P.
Bloom.
The
Sweet
Spot:
The
Pleasures
of
Suffering
and
the
Search
for
Meaning.
HarperCollinsPublishers, 2021. isbn: 9780062910561. url: https://books.google.com.au/books?
id=LUI7zgEACAAJ.
[42]
Mantas Mazeika et al. How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios.
2022. arXiv: 2210.10039 [cs.CV].

354
CHAPTER 6. MACHINE ETHICS
[43]
Dan Hendrycks et al. “Aligning AI With Shared Human Values”. In: CoRR abs/2008.02275 (2020).
arXiv: 2008.02275. url: https://arxiv.org/abs/2008.02275.
[44]
Dan Hendrycks. Natural Selection Favors AIs over Humans. 2023. arXiv: 2303.16200 [cs.CY].
[45]
Patrick Butlin et al. Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.
2023. arXiv: 2308.08708 [cs.AI].
[46]
John A. Weymark. “Harsanyi’s Social Aggregation Theorem and the Weak Pareto Principle”. In:
Social Choice and Welfare (1993). doi: https://doi.org/10.1007/BF00182506.
[47]
Toby Newberry and Toby Ord. The Parliamentary Approach to Moral Uncertainty. Tech. rep. Technical
Report# 2021-2, Future of Humanity Institute, University of Oxford . . ., 2021.
[48]
W. Wallach and C. Allen. Moral Machines: Teaching Robots Right from Wrong. Oxford University
Press, 2008. isbn: 9780199705962. url: https://books.google.com.au/books?id=_r3N82ETng4C.
[49]
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning: Limitations
and Opportunities. http://www.fairmlbook.org. fairmlbook.org, 2019.
[50]
Richard Layard and Jan-Emmanuel Neve. Wellbeing: Science and Policy. May 2023. isbn:
9781009298926. doi: 10.1017/9781009298957.
[51]
Dan Hendrycks et al. “What Would Jiminy Cricket Do? Towards Agents That Behave Morally”. In:
NeurIPS (2021).
[52]
Matthew D. Adler. “Measuring Social Welfare: An Introduction”. In: Oxford University Press, Dec. 2019.
isbn: 9780190643027. doi: 10.1093/oso/9780190643027.003.0001. eprint: https://academic.oup.
com/book/0/chapter/299838530/chapter-pdf/40198136/oso-9780190643027-chapter-1.pdf.
url: https://doi.org/10.1093/oso/9780190643027.003.0001.

Normative Ethics
7.1
Introduction
Ethics is the branch of philosophy concerned with questions of right and wrong, good and bad, and
how we ought to live our lives. We make ethical choices every day. When we decide whether to
tell the truth or lie, help someone in need or ignore them, treat others with respect or act in a
discriminatory manner, we are making moral decisions that reflect our values, beliefs, and moral
principles. Philosophical ethics seeks to provide a systematic framework for making these decisions.
In this chapter, we will explore some of the key concepts and theories in philosophical ethics. This
branch of research is also commonly called moral philosophy. We use the terms ethics and morality
interchangeably. The subfield of ethics dedicated to developing moral theories is called normative
ethics, which considers questions about how we ought to act. It investigates normative claims as
opposed to empirical ones, examining how the world ought to be, rather than simply how it is. The
former consider concepts such as rights, duties, roles, and morality, using words such as should, must,
and ought. The latter contain information that is either true or false depending on how the world is
and are (in theory) testable.
This chapter outlines some of the main reasons why it’s important for anyone concerned about
AI to learn about ethics. We then turn to the basic building blocks of moral theories, examining
various moral considerations like intrinsic goods, constraints, and special obligations. Then we will
explore some of the most prominent ethical theories, like utilitarianism, deontology, and virtue ethics,
evaluating their strengths and weaknesses. Finally, we consider how we might deal with reasonable
disagreement over what is right and wrong. Throughout, our key focus is on the ethical concepts
that are most relevant to the development, implementation, and governance of AI.
7.2
Why Learn About Ethics?
This chapter will explain ethics. Here, we cover the most prominent theories in the history of ethical
discourse. Reading this chapter should make it easier to understand debates about ethics in AI
systems.
Ethics is relevant to the field of AI for two key reasons. First, AI systems are increasingly being
integrated into various aspects of human life, such as healthcare, education, finance, and transportation,
and they have the potential to significantly impact our lives and wellbeing. As AI systems become
increasingly intelligent and powerful, it is crucial to ensure that they are designed, developed, and
deployed in ways that promote widely shared values and do not amplify existing social biases or cause
needless harms. Unfortunately, there are already numerous examples of AI systems being designed in
ways that failed to adequately consider such risks, such as racially biased facial recognition systems.
In order to wisely manage the growing power of AI systems, developers and users of AI systems need
to understand the ethical challenges that AI systems introduce or exacerbate.
Second, AI systems raise a range of new ethical questions that are unique to their technological
nature and capabilities. For instance, AI systems can generate, process, and analyze vast amounts of
data—much more than was previously possible. In what ways does this new technology challenge
traditional notions of privacy, consent, intellectual property, and transparency? Another important
set of questions relates to the moral status of AI systems. This is likely to become more pressing if
AI systems become increasingly autonomous and able to interact with human beings in ways that
convince their users that they have their own preferences and feelings. What should we do if AI
355

356
CHAPTER 7. NORMATIVE ETHICS
systems appear to meet some of the potential criteria for sentience or other morally relevant features?
Thirdly, as further explored in the Single-Agent Safety and Machine Ethics chapters, it is challenging
to specify objectives or goals for highly powerful AI systems in ways that do not lead in a predictable
way to highly undesirable consequences. In order to grasp why it is so challenging to specify these
objectives, it is helpful to understand the ethical theories that have been proposed. Questions of
what it means to act rightly or to live a good life have been debated by many thinkers over several
millennia, with strong arguments advanced for a number of competing positions. These debates can
provide us with greater insight into the challenges that AI developers will need to overcome in order
to build increasingly powerful AI systems in a beneficial way. Rather than attempting to bypass or
ignore such controversies, AI developers should accept that their design decisions may raise difficult
ethical questions that need to be considered carefully.
7.2.1
Is Ethics “Relative?”
Even after millennia of deliberation, we do not agree on all of morality.
Philosophers
have been thinking about and debating moral principles for millennia, yet they have not achieved
consensus on many moral issues. Widespread disagreements remain in both philosophical and public
discourse, including about important topics like abortion, assisted suicide, capital punishment, animal
rights, and the effects of human activity on natural ecosystems. One troubling idea is that these
disagreements are irresolvable because no moral principles or judgments are absolutely or universally
correct. In the case of AI, this may lead AI developers to believe that they have no role to play in
shaping how AI systems behave.
Cultural relativism claims there is no objective, culturally independent standard of
morality.
Consider the principle that consensual relationships between adults are acceptable
regardless of whether they are heterosexual or homosexual.
A moral relativist would suggest
this principle is correct for people who belong to some cultures where homosexuality is accepted,
but incorrect for people who belong to other cultures where homosexuality is criminalised or
socially stigmatized. These differences are systemic: many cultures have moral standards that seem
incompatible with others’ ideals, such as different views on marriage, divorce, gender roles, freedom of
speech, or religious tolerance. These differences form the basis for arguments for cultural relativism.
Normative moral relativism vs. descriptive moral relativism [1].
Moral relativism has
various forms, but here we discuss two: descriptive moral relativism and normative moral relativism.
Descriptive moral relativism is straightforward: it means that different societies around the world
have different sets of rules about what’s right and wrong, much like they have unique cuisines,
customs, and traditions. Descriptive moral relativism makes no claims about which, if any, of these
rules is right or wrong. Normative moral relativism suggests that one cannot say that something is
right or wrong in general, but only relative to a particular culture or set of norms. Normative moral
relativists conclude that morality itself is not something universal or absolute. Strictly speaking,
descriptive moral relativism and normative moral relativism are independent of each other, although
in practice descriptive moral relativism is often treated as if it provides evidence for normative moral
relativism.
Objections to Moral Relativism
A number of arguments can be advanced against descriptive and normative moral relativism [1],
which we explore in this subsection. We will explore the argument that cultural differences might
be overstated, which makes descriptive moral relativism harder to uphold. Another argument is
that proponents of normative moral relativism often face challenges when confronted with instances

7.2. WHY LEARN ABOUT ETHICS?
357
of extreme harm. For instance, while many would unequivocally agree that torturing a child for
entertainment is morally wrong, a normative moral relativist might be required to argue that its
morality is contingent upon the cultural context. Extreme examples such as this suggest few people
are willing to be thoroughgoing moral relativists. We further explore arguments for and against
moral relativism in this section.
Human moral systems appear to share some common features.
Some have argued that
most or all societies share some norms. For example, prohibitions against lying, stealing, or killing
human beings are common across cultures. Many cultures have some form of reciprocity, which is
the idea that people have a moral obligation to repay the kindness or generosity they have received
from others or that people should treat others the way they wish to be treated [2]. This can be seen
in the widespread practice of exchanging gifts and in moral codes that emphasize fairness and justice.
Additionally, human cultures have typically some concept of parenthood, which often involves a moral
obligation to care for one’s children, as well as broader obligations to one’s family and group. These
common features suggest that there are at least a few universal aspects of morality that transcend
cultural boundaries.
Moral relativism conflicts with common-sense morality [1].
Consider controversial practices
still prevalent in some cultures, such as honor killings in parts of the Middle East. The honor of a
family depends on the “purity” of its women. If a woman is raped or is deemed to have compromised
her chastity in some way, the profound shame brought upon her family may lead them to kill her in
response. According to the normative moral relativist, if such a practice is in line with the moral
standards of the society where it takes place, there is nothing wrong with it. Even more disturbingly,
on some versions of relativism, men in these societies may even be considered morally in the wrong if
they fail to kill their wives, daughters or sisters for having worn the wrong clothing, having premarital
sex or being raped. Similarly, normative moral relativism would require us to believe that the
morality of owning slaves was entirely dependent on the societal context. Moral iconoclasts, such as
early anti-slavery campaigners, would by definition always be morally wrong. In practice, if required
to accept that moral standards that endorse honor killings or slavery are not wrong in a general
sense, many moral relativists may recoil from this.
Cultural moral relativism denies the possibility of meaningful moral debate or moral
progress [1].
Moral relativism seems to require us to accept contradictory claims. For example,
moral relativists might say that a supporter of gay marriage is correct in saying that homosexuality
is morally acceptable, while someone from a different culture might be correct in saying that
homosexuality is morally wrong, provided that both claims are in line with the moral standards of
the cultures they respectively belong to. If moral relativism requires assert to simultaneously assert
and deny that homosexuality is morally acceptable, and any theory that generates contradictions
should be rejected, this would appear to mean that we should reject moral relativism. In order to
resist this, moral relativists typically reinterpret the way we usual moral language in a way that
can save it from contradiction. The relativist would say that when we say “homosexuality is wrong”,
what we really mean is “Homosexuality is not approved by my society’s norms”. This means that
relativists have to deny the possibility of moral disagreement and claim that anyone who engages in
such debates does not understand the meaning of what they are saying.
Moral relativism does not necessarily promote tolerance [1].
Some have argued that one of
the attractions of moral relativism is that it promotes tolerance. By recognizing cultural differences
(descriptive moral relativism), they may assert that everyone ought to do what their culture says is
right (normative moral relativism). However, in a society that is deeply intolerant, cultural moral
relativism cannot support tolerance, as it cannot claim that this has any universal or objective value.

358
CHAPTER 7. NORMATIVE ETHICS
Moral relativism only recommends tolerance to cultures where it is already accepted. Indeed, to
be tolerant, one need not be a normative moral relativist. There are alternatives views which can
accommodate tolerance and multiple perspectives, such as cosmopolitanism, liberal principles, and
value pluralism. We discuss adjudicating among competing moral views in Moral Uncertainty.
In practice, moral relativism can shut down ethics discussions [1].
It is important to note
that different cultures have different moral standards. However, AI developers sometimes invoke this
observation and side with normative moral relativism to avoid considering the ethics of their AI design
choices. Moreover, suppose AI developers do not analyze the ethical implications of their choices and
avoid ethical discussions by noting the lack of cross-cultural consensus. In that case, the default is
for AI development to be driven by amoral forces, such as self-interest or what makes the most sense
in a competitive market. Decisions driven by other forces, such as commercial incentives, will not
necessarily be aligned with the broader interests of society. Moral relativism can be unattractive
from a pragmatic point of view, as it limits our ability to engage in discussions that may sometimes
lead to convergence on shared principles. This quietist stance de-emphasizes moral arguments to the
benefit of economic incentives and self-interest.
Why are these debates about moral relativism relevant to AI? People commonly observe that different
cultures have different beliefs when discussing how to ensure that AIs promote human values. It
is essential not to conflate this observation with normative moral relativism and conclude that AI
developers have no ethical responsibilities. Instead, they are responsible for ensuring that the values
embodied in their AI systems are beneficial. Rather than a barrier, cultural variation means that
making AIs ethical requires a broad, globally representative approach.
7.2.2
Is Ethics Determined by Religion?
Moral relativists may believe that studying ethics is futile because ethical questions are irresolvable.
On the other hand, some people believe that studying ethics is futile because moral questions are
already solved. This position is most common among those who say that religion is the source of
morality.
Divine Command Theory
Many believe morality depends on God’s will and commands.
The view called divine
command theory says whether an action is moral is determined solely by God’s commands rather
than any qualities of the action or its consequences. (We use the term “God” inclusively to refer
to the god or gods of any religion.) This theory suggests that God has the power to create moral
obligations and can change them at will.
While this book does not argue for or against any particular religion, we do suggest that there are
severe problems with equating religion and morality. One problem is that it creates a problematic
understanding of God.
If you believe there is a god, you likely believe he is more than just an arbitrary authority figure.
Many religious traditions view God as inherently good. It is precisely because God is good that
religion compels us to follow God’s word. However, if you believe that we should follow God’s word
because God is good, then there must be some moral qualities (like goodness) that exist independently
of God’s rules—thus, divine command theory is false [3].
To be clear, this is not an argument against believing in God or religion. It is an argument against
equating God or faith with morality. Both religious people and irreligious people can behave morally

7.3. MORAL CONSIDERATIONS
359
or immorally. That’s why everyone needs to understand the factors that might make our actions
right or wrong.
7.3
Moral Considerations
How can we determine whether an action is right or wrong? What are the kinds of principles and
values that should guide our moral decisions? There are many factors to consider. Here, we’ll focus
on a few—-goodness, constraints, special obligations, and options-—that very commonly enter into
moral decision making.
7.3.1
The “Goodness” of Actions and Their Consequences
Moral decision making often involves considering the values, or “goods,” that are at stake. These
may be intrinsic goods or instrumental goods.
Intrinsic goods are things that are valuable for their own sake.
Philosophers disagree
about what, if anything, is intrinsically good, but many argue for the intrinsic value of things like
happiness, love, and knowledge. We value such things simply because they are valuable—not because
they necessarily lead to anything else.
Instrumental goods are things that are valuable because of the benefits they provide
or the outcomes they achieve.
We pursue instrumental goods as a means to an end, but not
for their own sake. Money, power, and education are examples of instrumental goods. We value
them because they can lead to other things we value, like security, influence, career opportunities, or
intrinsic goods.
Intrinsically good things are not necessarily instrumentally good.
Sometimes, intrinsically
bad things can be instrumentally good and intrinsic goods can be instrumentally bad. For instance,
many people believe that honesty is intrinsically good. However, it’s easy to imagine cases in which
honesty can lead to bad outcomes, like hurt feelings. Suppose a friend has confided in you that
they are staying at a shelter to hide from an abusive partner. If that abusive partner asks you for
your friend’s location, you may think that that honesty is intrinsically good. However, revealing
your friend’s location would be instrumentally bad, as it may lead to further violence and perhaps
even a risk to your friend’s life. On the other hand, consider medical treatments like chemotherapy.
Chemotherapy is instrumentally good because it can prolong cancer patients’ lives. Yet, as it requires
the administration of highly toxic drugs into a patient’s body, it could be seen as harmful, or
intrinsically bad. For many people, exercise is painful, and pain is intrinsically bad, but exercise can
be instrumentally good.
There is no consensus about what is intrinsically good.
Some philosophers believe that
there are many intrinsic goods. Others believe there is only one value. One common view is that the
only intrinsic good is wellbeing, and everything else is valuable only insofar as it promotes wellbeing.
Value pluralists believe that there are many intrinsic goods. These values may include justice, rights,
autonomy, and virtues such as courage. Other philosophers believe there is only one fundamental
value. Among these, one common view is that the only intrinsic good is wellbeing, and everything
else is valuable only insofar as it promotes wellbeing.

360
CHAPTER 7. NORMATIVE ETHICS
Wellbeing
Wellbeing is how well a person’s life is going for them.
It is commonly considered to be
intrinsically good, though there are different accounts of precisely what wellbeing is and how we can
evaluate it. Generally, a person’s wellbeing seems to depend on the extent to which that person is
happy, healthy, and fulfilled. Three common accounts of wellbeing characterize it as 1) net pleasure
over pain, 2) preference satisfaction, or 3) a collection of objective goods. Each account is elaborated
below.
Wellbeing as Net Pleasure over Pain
Some philosophers, known as hedonists, argue that wellbeing is the achievement of the greatest
balance of pleasure and happiness over pain and suffering. (For simplicity we do not distinguish, in
this chapter, between “pleasure” and “happiness” or between “pain” and “suffering,” though neither pair
is interchangeable.) All else equal, individuals who experience more pleasure have higher wellbeing.
All else equal, individuals who experience more pain have lower wellbeing.
According to hedonism, pleasure is the only intrinsic good.
Goods like health, knowledge,
and love are instrumentally valuable. That is, they are only good insofar as they lead to pleasure.
It may feel as though other activities are intrinsically valuable. For instance, someone who loves
literature may feel that studying classic works is valuable for its own sake. Yet, if the literature lover
were confronted with proof that reading the classics makes them less happy than they otherwise
would be, they might no longer value studying literature. Hedonists believe that when we think we
value certain activities, we actually value the pleasure they bring us, not the activities themselves.
Hedonism is a relatively clear and intuitive account of wellbeing. It seems to apply equally to
everyone. That is, while we all may have different preferences and desires, pleasure seems to be
universally valued. However, some philosophers argue that hedonism is an incomplete account of
wellbeing. They argue there may be other factors that influence wellbeing, such as the pursuit of
knowledge.
Wellbeing as Preference Satisfaction
Some philosophers claim that what really matters for wellbeing is that our preferences are satisfied,
even if satisfying preferences does not always lead to pleasurable experiences. One difficulty for
preference-based theories is that there are different kinds of preferences, and it’s unclear which ones
matter. Preferences can be split into three categories: stated preferences, revealed preferences, and
idealized preferences. Each of these categories can be informative in different contexts.
Consider voter preferences in an election.
In a democratic election, citizens choose which
candidate they want to elect by casting their vote on a ballot. Their choice to vote for a given
candidate can be impacted by a number of different factors. Perhaps they have an existing political
affiliation, are influenced by social pressures, believe in the candidate’s policies, or maybe they just
like one candidate’s demeanor and personality. Citizens may not always vote for the candidate they
claim to support, and the choice to vote for a specific candidate can change when voters discover
new information.
Stated preferences.
A voter’s stated preference is the candidate that they state they support.
Voters may express their stated preferences in conversations, polls, and while campaigning. It is
easy to learn about people’s stated preferences–—simply ask them. Political polls and surveys, for
example, are an easy way to gather information about people’s stated preferences.

7.3. MORAL CONSIDERATIONS
361
However, stated preferences may not always predict what people will actually choose. A voter may
outwardly express support for Candidate X, but when it comes to casting their ballot, they may vote
for Candidate Y. Similarly, someone might express a stated preference to eat healthier, but that
doesn’t necessarily mean that they will. Their behavior (such as eating only chocolate for a week)
may indicate that their revealed preference is for unhealthy food.
Revealed preferences.
When a voter casts their vote on a ballot, they express their revealed
preference. Generally, a voter’s revealed preferences align with their interests. For example, a voter
who supports increased funding for education might vote for a candidate who wants to increase
budgets for local public schools. A revealed preference is expressed by your actions, not your words.
Revealed preferences can be harder to observe, but they are generally more useful for predicting
people’s behavior. Someone with a stated preference for vegetables but a revealed preference for
chocolate is more likely to purchase and consume chocolate than vegetables. When researching
consumer behavior, economists often prefer to study consumers’ revealed preferences (i.e. what they
buy) rather than stated preferences (i.e. what they say they’d like to buy).
Idealized
preferences.
People change their preferences upon learning new information.
Uninformed preferences can be reached quickly. For example, a voter might have an uninformed
preference based on a “gut reaction” to a candidate. Voters can arrive at more idealized preferences
once they have gathered and evaluated all relevant information. They might not actually do this—few
people have the time or ability to perfectly gather and evaluate all the relevant information that
they would require to find their idealized preferences. However, their preferences can become more
idealized over time. A voter might have an uninformed preference for Candidate A and, after learning
new information about each candidate’s platform, they may arrive at a more informed preference for
a different candidate. In other words, preferences can change, and they often do change as people
become more informed.
Wellbeing as a Collection of Objective Goods
Others believe that wellbeing is the achievement of an objective set of “goods” or “values.” These
goods are considered necessary for living a good life regardless of a person’s individual experiences or
preferences.
There is disagreement about which goods are necessary for wellbeing.
Commonly proposed
goods include happiness, health, relationships, knowledge, and more. Objective goods theorists
consider these values to be important for wellbeing independently of individual beliefs and preferences.
There is no uncontroversial way to determine which goods are important for living a good life.
However, this uncertainty is not a unique problem for objective goods theory. It can be difficult
for hedonists to explain why happiness is the only value that is important for wellbeing and for
preference satisfaction theorists to determine which preferences matter most.
Applying the Theories of Wellbeing
While people disagree about which account of wellbeing is correct, most people agree that wellbeing is
an important moral consideration. All else equal, taking actions that promote wellbeing is generally
considered morally superior to taking actions that reduce wellbeing. However, the three theories
of wellbeing have different practical implications. In the future, we are likely to interact with AI
chatbots in a variety of ways; in particular, we might have close personal interactions with them that
influence our decision-making. Different theories of wellbeing would suggest different goals for these
AIs.

362
CHAPTER 7. NORMATIVE ETHICS
Chatbots could prioritize pleasure.
The hedonistic view suggests that wellbeing is primarily
about experiencing pleasure and avoiding pain. This theory might recommend that AIs should be
providing users with entertaining content that brings them pleasure or encouraging them to make
decisions that maximize their balance of pain over pleasure over the long run. A common criticism
is that this trades off with other goods considered valuable like friendship and knowledge. While
this is sometimes true, these goals can also align. Providing content that is psychologically rich
and supports users’ personal growth can contribute to a more fulfilling and meaningful life full of
genuinely pleasurable experiences.
Chatbots could prioritize preference satisfaction.
The preference view suggests that wellbeing
is about having preferences satisfied. Depending on which preferences were considered important,
this theory would suggest different priorities for a hedonistic chatbot. Consider revealed preferences.
One proxy for revealed preferences is user engagement. By continuing to engage with a chatbot, users
are expressing their preference for seeing more of what they are getting, and increasing preference
satisfaction might imply continuing to behave in certain ways.
It is important to be careful of this equivalence, though: prioritizing user engagement can lead to
results like chatbots that engage people through unsavory means. Like engaging humans, chatbots
could try to addict users by showing them streams of ephemeral content, creating an air of mystery
and uncertainty, or act distant after being friendly to create a desire for continued friendliness that
it can then satisfy. Such AIs might maximize engagement, but this may not be good for people’s
wellbeing—even if they use the platform for more hours.
Chatbots could promote objective goods.
The objective goods account suggests that wellbeing
is about promoting goods such as achievement, relationships, knowledge, beauty, happiness, and the
like. An AI chatbot might aim to enhance users’ lives by encouraging them to complete projects,
develop their rational capacities, and facilitate learning. The goal would be to make users more
virtuous and encourage them to strive for the best version of themselves. This aligns with Aristotle’s
theory of friendship, which emphasizes the pursuit of shared virtues and mutual growth, suggesting
that such AIs might have meaningful friendships with humans.
We might want to promote the welfare of AIs.
In the future, we might also come to view
AIs as able to have wellbeing. This might depend on our understanding of wellbeing. An AI might
have preferences but not experience pleasure, which would mean it could have wellbeing according
to preference satisfaction theorists but not hedonists. Future AIs may have wellbeing according to
all three accounts of wellbeing. This would potentially require that we dramatically reassess our
relationship with AIs.
7.3.2
Constraints and Special Obligations
We have covered the moral consideration of intrinsic goods, and focused on the intrinsic good
wellbeing. Special obligations and constraints are key considerations when we make ethical decisions.
Special obligations are duties arising from relationships.
We can incur special obligations
when we promise someone to do something, take a professional position with responsibilities, have
a child, make a romantic commitment to a partner, and so on. Sometimes we can have special
obligations that we did not volunteer for—a child to its parents, or our duties to fellow citizens.
Constraints are actions that we are morally prohibited from taking.
A constraint is
something that places limits on our actions. For example, many people think we’re morally prohibited
from lying, stealing, cheating, harming others, and more.

7.3. MORAL CONSIDERATIONS
363
Constraints often come in the form of rights.
Rights are claims that individuals may have
over their community. For instance, many people believe that humans have the rights to life, freedom,
privacy, and so on. Some people argue that any individual with the capacity for experiencing pleasure
and pain has rights. Non-human individuals (including animals and AI systems) might also have
certain rights.
An individual’s rights may require that society intervene in certain ways to ensure that those rights
are fulfilled. For instance, an individual’s right to food, shelter, or education may require the rest of
society to pay taxes so that the government can ensure that everyone’s rights are fulfilled. Rights
that require certain actions from others are called positive rights.
Other rights may require that society abstain from certain actions. For instance, an individual’s right
to free speech, privacy, or freedom from discrimination may require the rest of society to refrain from
censorship, spying, and discriminating. Rights that require others to abstain from certain behaviors
are called negative rights.
Many AI researchers think that, for now, we should avoid accidentally creating AIs that deserve
rights [4]; for instance, perhaps all entities that can experience suffering have natural rights to protect
them from it. Some think we should especially avoid giving them positive rights; it might be fine to
give them rights against being tortured but not the right to vote. If they come to deserve rights, this
would create many complications and undermine our claim to control.
7.3.3
What does it mean for an action to be right or wrong?
Some of the first questions we might ask about ethics are: Are all actions either right or wrong? Are
some simply neutral? Are there other distinctions we might want to draw between the morality of
different actions?
The answers to these questions, like most moral questions, are the subject of much debate. Here,
we will simply examine what it might mean for an action to be right or wrong. We will also draw
some other useful distinctions, like the distinction between obligatory and non-obligatory actions,
and between permissible and impermissible actions. These distinctions will be useful in the following
section, when we discuss the considerations that inform our moral judgments.
Options
Special obligations and constraints tell us what we should not do, and sometimes, what we must
do. Intrinsic goods tell us about things that would be good, should they happen. But philosophers
debate how much good we are required to do.
Options are moral actions which we are neither required to do nor forbidden from
doing.
Even though it would be good to donate money, many people do not think people are
morally required to donate. This is an ethical option. If we believe in options, not all actions are
either required or forbidden.
We now break down actions onto a spectrum on which we will simply examine what it might mean for
an action to be right or wrong. We will also draw some other useful distinctions, like the distinction
between obligatory and non-obligatory actions and between permissible and impermissible actions.
Obligatory actions are those that we are morally obligated or required to perform.
We
have a moral duty or obligation to carry out obligatory actions, based on ethical principles. For

364
CHAPTER 7. NORMATIVE ETHICS
example, it is generally considered obligatory to help someone in distress, or refrain from hurting
others.
Non-obligatory actions are actions that are not morally required or necessary.
Non-
obligatory actions may still be morally good, but they are not considered to be obligatory. For
example, volunteering at a charity organization or donating to a good cause may be good, but most
people don’t consider them to be obligatory.
Permissible actions may be morally good or simply neutral (i.e. not good or bad).
In general, any action that is not impermissible is permissible. Moral obligations, of course, are
permissible. We can consider four other actions: volunteering, donating to charity, eating a sandwich,
and taking a walk. These seem permissible, and can be classified into two categories.
One class of permissible actions is called supererogatory actions. These may include volunteering or
giving to charity. They are generally considered good; in fact, we tend to believe that the people
who do them deserve praise. On the other hand, we typically don’t consider the failure to do these
actions to be bad. We might think of supererogatory actions as those that are morally good, but
optional; they go “above and beyond” what is morally required.
Another class of permissible actions is called morally neutral actions. These may include seemingly
inconsequential activities like eating a sandwich or taking a walk. Most people probably believe that
actions like these are neither right nor wrong.
Impermissible actions are those that are morally prohibited or unacceptable.
These
actions violate moral laws or principles and are considered wrong. Stealing or attacking someone are
generally considered to be impermissible actions.
Figure 7.1: Actions can be classified according to whether they are permissible and obligatory.
Some philosophers believe that all actions fit on a scale like the one above. At one end of the scale are
impermissible actions, like murder, theft, or exploitation. At the other end are obligatory actions, like
honesty, respect, and not harming others. In between are neutral and supererogatory actions. These
are neither impermissible nor obligatory. Many people believe that the vast majority of our actions

7.4. MORAL THEORIES
365
fall into these two categories. Crucially, in designing ethical AI systems that operate in the real
world, it is important to determine which actions are obligatory and which actions are impermissible.
However, some philosophers do not believe in options; rather that actions are all on a spectrum from
the least moral to the most moral. We will learn more about these positions, and others, when we
discuss moral theories later in this chapter.
From Considerations to Theories
Moral considerations can guide our day-to-day decision making. Understanding which
factors are morally relevant can help us think more clearly about what we should do. Of course, we
don’t always stop to consider every factor before making a decision. Rather, we tend to draw broader
conclusions or moral principles based on our evaluations of specific cases. For instance, once we
consider a few examples of the ways in which stealing can harm others, we might draw the conclusion
that we shouldn’t steal.
The considerations discussed in this section provide a basis on which we can develop more practical,
action-guiding theories about how we should behave. The types of fundamental considerations in this
section comprise a subfield of ethics called metaethics. Metaethics is the consideration of questions
like “What makes an action right or wrong?” and “What does it mean to say that an action is right
or wrong?” [5]
These considerations are important in the context of designing AI systems. In order to respond to
situations in an appropriate way, AI systems need to be able to identify morally relevant features and
detect situations where certain moral principles apply. They would also need to be able to evaluate
and compare the moral worth of potential actions, taking into account various purported intrinsic
goods as well as normative factors such as special obligations and constraints. The challenges of
designing objectives for AI systems that respect moral principles are further discussed in the Machine
Ethics chapter.
In the following section, we will discuss some popular moral theories.
7.4
Moral Theories
Moral theories are systematic attempts to provide a general account of moral principles that apply
universally. Good moral theories should provide a coherent, consistent framework for determining
whether an action is right or wrong. A basic background understanding of some of the most commonly
advanced moral theories provides a useful foundation for thinking about the kinds of goals or ideals
that we wish AI systems to promote. Without this background, there is a risk that developers and
users of AI systems may jump to conclusions about these topics with a false sense of certainty and
without considering many potential considerations that could change their decisions. Considering
a range of different philosophical theories enables us to stress-test our arguments more thoroughly
and surface questionable assumptions that may not have been noticed otherwise. It would be highly
inefficient for those developing AI systems or trying to make them safer to attempt to re-invent moral
systems, without learning from the large existing body of philosophical work on these topics.
There are many different types of moral theories, each of which emphasizes different moral values
and considerations. Consequentialist theories like utilitarianism hold that the morality of an action
is determined by its consequences or outcomes. Utilitarianism places an emphasis on maximizing
everyone’s wellbeing. Deontological theories like Kantian ethics hold that the morality of an action
is determined by whether it conforms to universal moral rules or principles. Deontology places an

366
CHAPTER 7. NORMATIVE ETHICS
emphasis on rights, special obligations, and constraints.
Below, we explore the most common modern moral theories: utilitarianism, deontology, virtue ethics,
and social contract theory.
7.4.1
Utilitarianism
Utilitarianism is the view that we should do whatever results in the most overall wellbeing [6].
According to Katarzyna de Lazari-Radek and Peter Singer, “The core precept of Utilitarianism is
that we should make the world the best place we can. That means that, as far as it is within our
power, we should bring about a world in which every individual has the highest possible level of
wellbeing” [7]. Under Utilitarianism, the right action in any situation is the one which will increase
overall wellbeing the most, not just for the people directly involved in the situation but globally.
Expected Utility
Utilitarianism enables us to use empirical, quantitative evidence when deciding moral
questions.
As we discussed in Section 7.3.1, there is no consensus about what, precisely, wellbeing is.
However, if we discover that wellbeing is something measurable, like happiness, moral decision-making
could take advantage of calculation and would rely less on qualitative argumentation. To determine
what action is morally right, we would simply consider the available options. We might run some
tests or perform data analysis to determine which action would create the most happiness, and that
action would be the right one. Consider the following example:
Drunk driving: Amanda has had a few alcoholic drinks and is deciding whether to
drive or take the bus home. Which should she choose?
A utilitarian could analyze this scenario by listing the possible outcomes of each choice and determining
their impact on overall wellbeing. We call an action’s impact on wellbeing its utility. If an action has
positive utility, it will cause happiness. If an action has negative utility, it will cause suffering. Larger
amounts of positive utility represent larger amounts of happiness, and larger amounts of negative
utility represent larger amounts of suffering. Since no one can predict the future, the utilitarian
should also consider the probability that each potential outcome would occur.
A simplified, informal, back-of-the-envelope version of this utilitarian calculation is below:
Amanda’s action
Possible outcome(s)
Probability of each outcome
Utility
Amanda takes the bus.
Amanda is frustrated, the bus is
slow, and she has to wait in the
cold.
100%
-1
Amanda drives home.
Amanda gets home safely, far
sooner than she would have on
the bus.
95%
+1
Amanda gets into an accident and
someone is fatally injured.
5%
-1000
Table 7.1: Illustrative calculation of utility from Amanda’s possible actions.

7.4. MORAL THEORIES
367
Utilitarianism
Deontology
Contractarianism
What is Alex’s estimate of the
chance this theory is true
60%
30%
10%
Does this theory like lying to save
a life?
Yes
No
Yes
Table 7.2: Example: Alex’s credence in various theories and their evaluation of lying to save a life.
We are interested in the expected utility of each action—the amount of wellbeing that each action is
likely to result in. To calculate the expected utility, we multiply the utility of each possible outcome
by the probability of that outcome occurring.
Amanda choosing to take the bus has a 100% chance (a certainty) of causing a small decrease in
utility; she will be slightly inconvenienced. Since the change in utility is small and negative, we’ll
estimate a small negative number to represent it, like -1. The expected utility of Amanda taking the
bus is 100% × -1, or simply -1.
If Amanda drives home, there is a 95% chance that she will get home safely and create a small
increase in utility–—let’s say of +1. However, there’s also a 5% chance she could cause an accident
and end someone’s life. The accident would result in a very large decrease in utility. Someone would
experience pain and death, Amanda would feel guilty for the rest of her life, and the victim’s friends
and family would experience loss and grief. We might estimate that the potential loss in utility
is -1000. That’s 1000× worse than the small increase in utility if Amanda gets home safely. The
expected utility of Amanda driving home is the sum of both possibilities: .95 × 1 + .05 × −1000, or
−49.05.
Both of Amanda’s options are expected to yield negative utility, but the utilitarian would say that
she should choose the better of the two options. Unsurprisingly, Amanda should take the bus.
Implications of Utilitarianism
Utilitarianism may sometimes yield results that run against commonly held beliefs.
Utilitarianism aims at producing the most wellbeing and insists that this is the only thing that matters.
However, many of the moral values that we have inherited conflict with this goal. Utilitarianism can
be seen as having less of a bias to defend the moral status quo relative to some other moral theories
such as deontology or virtue ethics. This either makes Utilitarianism exciting or threatening.
Utilitarianism can lead to some radical moral claims.
Utilitarianism’s sole focus on wellbeing
can lead it to promote what have been or are viewed as radical actions. For example, the founder of
utilitarianism, Bentham, argued to decriminalize homosexuality, and contemporary utilitarians have
argued we have a much greater obligation to give to charity than most of us seem to believe.
Bentham held many beliefs that were ahead of his time. Written in 1785, in a social and legal
environment very hostile to homosexuality, Bentham’s essay “Offences against oneself” rebuts the
arguments that legal scholars had used to justify laws against homosexuality [8].
Today, many utilitarians believe that we should prioritize helping people in low-income
countries.
Utilitarianism continues to make recommendations that today’s society finds
controversial. Consider the following example:

368
CHAPTER 7. NORMATIVE ETHICS
On her morning walk through the park, Carla sees a child drowning in the pond. She
is wearing a new suit that she bought the day before, worth $3,500. Should she dive in
to save the child, even though she would destroy her suit? [9]
The philosopher Peter Singer, who first posed this question, argues that Carla should dive in.
Furthermore, he argues that our judgment in this case might mean that we should re-evaluate our
obligation to donate to charity. There are charities that will save a child’s life for around $3,500. If
we should forgo that amount in order to save a child who is right in front of us, shouldn’t we do
the same for children across the world? Singer argues that distance is not relevant to our moral
obligations. If we have an obligation to a child in front of us, we have the same obligation to similar
children who may be far away.
To maximize global wellbeing, Singer says that we should give our money up until the point where
a dollar would be better spent on us than on charity. If our money helps others more than it can
help ourselves, there isn’t a utilitarian reason to keep it. For an adult making, say, $50,000 per
year, an extra $3,500 would be helpful, but is not critical to their wellbeing. However, for someone
making less than $3 per day in a low-income country, $3,500 would be life-changing—not just for one
recipient, but for that person’s entire family and community. Singer argues that, if giving money
away can significantly help someone else, and if giving it away would not be a significant sacrifice, we
should give the money to the person who needs it most.
These conclusions imply that most of us (especially those of us in high-income countries) should live
very different lives. We should, for the most part, live as inexpensively as possible and donate a
significant portion of our income to people in lower-income communities.
Utilitarianism’s Central Claims
Utilitarianism can be distinguished from other ethical theories by four central claims.
Claim one: Consequences (and only consequences) determine whether an action is right or wrong.
Utilitarianism is a form of consequentialism. Any theory that claims that the consequences of an
action alone determine whether an action is right or wrong is consequentialist. Other theories, as
we will discuss later in this chapter, claim that some actions are right or wrong regardless of their
consequences.
Claim two: Wellbeing is the only intrinsic good.
Utilitarians believe that the only type of consequences that make an action right or wrong are those
that affect happiness or wellbeing. In that sense, utilitarianism can be understood as a combination
of consequentialism and hedonism, as we discussed it in section Wellbeing. Recall that there are
several different accounts of wellbeing, all of which are compatible with utilitarianism.
Classical utilitarianism.
Most utilitarians are hedonists about wellbeing; they believe that
wellbeing is a function of pleasure and suffering. Such utilitarians classical utilitarians. When
classical utilitarians say they want to improve wellbeing, they mean that they want there to be more
pleasure and less suffering in the world.
Preference utilitarianism.
In contrast to classical utilitarians, preference utilitarians believe that
wellbeing is constituted by the satisfaction of people’s preferences.

7.4. MORAL THEORIES
369
The preference account of wellbeing is one of the many modifications of classical utilitarianism. While
we will not describe these other theories in detail, it is useful to know that if we disagree with one
aspect of classical utilitarianism, there is often another utilitarian or consequentialist theory that can
accommodate our beliefs.
Claim three: Everyone’s wellbeing should be weighed impartially.
Utilitarians believe that people have the same intrinsic moral worth.
Bentham exemplified
utilitarian thought with the phrase “Each to count for one and none for more than one.” People of
different classes, races, ethnicities, religions, abilities, and so on are of equal moral worth. In other
words, utilitarianism is an impartial moral theory.
For an individual to deserve moral treatment, they just need to be capable of having
wellbeing.
According to Bentham, “The question is not, Can they reason?, nor Can they talk?
but, Can they suffer?” This quote is often taken to mean that we should be concerned with the
wellbeing of animals, since animals feel pleasure and pain just like humans. Similar positions are
held by other utilitarians such as Peter Singer [10]. If, in the future, AI systems develop a capacity
for wellbeing, they would deserve moral treatment as well according to classical utilitarians.
Claim four: We should maximize wellbeing.
Utilitarians aim to maximize wellbeing.
Utilitarians do not think it is sufficient to perform an
action with good consequences; they think the only right action is the one with the best consequences.
They do not believe in options. The following example illustrates this distinction.
Dorian has a choice: teach biology or research air quality. As a teacher, he would help
hundreds of students. As a researcher, he would save thousands of lives. He enjoys
teaching somewhat more than research. What should he choose?
A utilitarian might argue that Dorian should become a researcher. In this case, he knows that he
will do more good. This is despite the fact that Dorian would be a great teacher, and would have a
positive impact as a teacher. He would do more good through his job as a public health researcher,
so a utilitarian might argue that he is obligated to take that option.
The best option is always the one that maximizes wellbeing. This is a straightforward result of
valuing everyone’s wellbeing impartially and always striving to do the best rather than the merely
good.
In summary, utilitarianism makes several claims: wellbeing is the only intrinsic good, wellbeing should
be maximized, wellbeing should be weighed impartially, and an action’s moral value is determined by
its consequent effects on wellbeing. Utilitarianism teaches that the best action we can take is the one
that leads to the best positive effect on wellbeing.
Common Criticisms of Utilitarianism
While utilitarianism remains a popular moral theory, it is not without its critics. This section explains
some of the most common objections to utilitarianism.
Criticism: “Utilitarianism is too demanding.”
Many philosophers argue that utilitarianism is too demanding [11]. It insists that we choose the
best actions, rather than merely good ones. As we saw in our discussion of the drowning child and

370
CHAPTER 7. NORMATIVE ETHICS
our obligations to the global poor, this can lead utilitarianism to recommend unconventionally large
commitments.
According to this criticism, utilitarianism asks us to give up too much of what we take to be valuable
for the sake of other people’s wellbeing. Perhaps we should quit a career that we love in order to work
on something that does more good, or we should not buy gifts for family and friends if the money
would produce more wellbeing when given to someone suffering from a preventable disease. To live
up to this critique of everyday values we would have to radically change our lives, and continue to
change them as the global situation evolved. The critic thinks that this is too much to reasonably
ask of someone. A moral theory, they think, should not make a moral life highly challenging.
A utilitarian can respond in two ways.
The first way is to argue that, while utilitarianism is
theoretically demanding, it is practically less so. For example, someone trying to live up to the
theoretical demands of utilitarianism might burn out, or harm the people around them with their
indifference. If they had asked less of themselves, they might have done more good in the long run.
Utilitarianism might even recommend acting almost normally, if acting almost normally is the best
way to maximize wellbeing.
However, it is unlikely that this response undermines the argument that we should give some portion
of our money to charity. Even if donating most of our income would backfire. most people should
likely donate more than they do. Many utilitarians simply accept that their theory is demanding.
Utilitarianism does demand a lot of us, and until the critic shows that these demands are not morally
required of us, then we might just live in a demanding world. While demanding too much of yourself
can be counter-productive, we should do far more than we currently do.
Criticism: “Utilitarianism requires intractable calculations.”
Another way of critiquing utilitarianism is to say that even if the theory is consistent and appealing,
it isn’t useful because we rarely know the consequences of our actions in advance.
When we illustrated a utilitarian calculation above using the case of drunk driving, we intentionally
simplified the situation. We considered only a few possible immediate outcomes and we estimated
their possible likelihoods. In the real world, however, someone considering whether to drive home
faces unlimited possible outcomes, and those outcomes could cause other events in the future that
would be impossible to predict. Moreover, we rarely know the probabilities of the effects of our
actions. Utilitarianism would be impractical if it required us to make a long series of predictions and
calculations for every choice we face. Certainly, we shouldn’t expect Amanda to do so in the moment.
In response to this criticism, a utilitarian might differentiate between a criterion of rightness and
a decision-making procedure [12]. A criterion of rightness is the factor that determines whether
actions are right or wrong. According to utilitarianism, the criterion of rightness is whether an action
maximizes expected wellbeing compared to its alternatives. In contrast, a theory’s decision-making
procedure is the process it recommends individuals use to make decisions. Crucially, a theory’s
decision procedure does not need to be the same as its criterion of rightness.
For example, a utilitarian would not likely advise everyone to make detailed calculations before
getting in the car after having a couple of drinks. Most utilitarians would advise everyone to simply
never drive drunk. There’s only a need to consider a utility calculation in cases where the best option
is particularly unclear. Even then, such calculations are only approximate and should not necessarily
be decisive. Just as corporations try to maximize profit without consulting a spreadsheet for every
decision, utilitarians might follow certain rules of thumb without relying on utility calculations.

7.4. MORAL THEORIES
371
In practice, utilitarians rely on robust heuristics for bringing about better consequences and rarely
consult explicit calculations. To better improve the world, like others they often cultivate virtues
such as truth-telling, being polite, being fair, and so on. They often imitate practices that have stood
the test of time, even if they do not fully understand their rationale. That is because some things
may have complex or obscure reasons that are not easily discerned by human reason or not easily
amenable to calculation. They often bear in mind Chesterton’s fence, which warns against removing
a barrier without knowing why it was erected in the first place. Even if their criterion of rightness
can be controversial, utilitarians adopt decision procedures that are often conventional.
Criticism: “Utilitarianism places too much value on wellbeing.”
Many philosophers argue that utilitarianism neglects sources of value other than wellbeing. One
famous argument meant to show that wellbeing isn’t the only source of value is Robert Nozick’s
“Experience Machine” [13]. Nozick considers the following thought experiment:
“Suppose there were an experience machine that would give you any experience you
desired. Superduper neuropsychologists could stimulate your brain so that you would
think and feel you were writing a great novel, or making a friend, or reading an
interesting book. All the time you would be floating in a tank, with electrodes attached
to your brain. Should you plug into this machine for life, preprogramming your life’s
experiences?”
Nozick claims that we would decline this offer because we care about the reality of our actions. We
do not just want to feel that we have cheered up our friend; we actually want them to feel better.
We do not want the experience of writing a great work of literature, we want great literature to
exist because we worked on it. Many philosophers consider this a decisive rebuttal to the idea that
wellbeing is the only thing that matters.
Though many people say that they would prefer not to use the machine when it is introduced as
above, they may have a different reaction when the thought experiment is presented differently.
“You wake up in a plain white room. You are seated in a reclining chair with a steel
contraption on your head. A woman in a white coat is standing over you. ‘The year
is 2659,’ she explains, ‘The life with which you are familiar is an experience machine
program selected by you some forty years ago.
We at IEM interrupt our clients’
programs at ten-year intervals to ensure client satisfaction. Our records indicate that
at your three previous interruptions you deemed your program satisfactory and chose
to continue. As before, if you choose to continue with your program you will return to
your life as you know it with no recollection of this interruption. Your friends, loved
ones, and projects will all be there. Of course, you may choose to terminate your
program at this point if you are unsatisfied for any reason. Do you intend to continue
with your program?” [14]
Joshua Greene, the author of this example, supposes that most people would not want to leave
the program. He suggests that what accounts for the seeming difference between his and Nozick’s
versions is the status-quo bias. People tend to prefer the life they know. Surveys of real people’s
responses to these thought experiments indicate that a range of factors—including the status-quo
bias—affect their responses. Nozick’s example is not as clear cut as his argument supposes.
In summary, utilitarianism is often criticized in three ways. People claim that it is (1) too ethically

372
CHAPTER 7. NORMATIVE ETHICS
demanding, (2) practically unusable, and (3) wrong to neglect values other than wellbeing. In
response, utilitarians may argue that we simply live in a demanding world, that we can use heuristics
instead of constantly making calculations, and that Nozick’s thought experiment does not necessarily
show that we have values aside from wellbeing.
Conclusions about Utilitarianism
Utilitarianism is a consequentialist, welfarist, impartial, and optimizing ethical theory.
Utilitarians believe that our actions should be guided by whatever leads to the greatest overall
wellbeing. As a form of consequentialism, utilitarianism emphasizes that the outcomes of our actions
are what truly matter. It is impartial and welfarist, which means that it considers everyone’s
wellbeing equally important when evaluating outcomes, and considers nothing except for wellbeing.
Consequently, utilitarianism transforms moral questions into empirical ones: determining what is
right simply involves identifying the action that maximizes total wellbeing.
Utilitarians often advocate for policies that appear radical to their contemporaries. For example,
Jeremy Bentham advocated for gay rights, John Stuart Mill for women’s rights, and Peter Singer
for animal rights before these were widely acceptable. However, critics of utilitarianism argue that
some of these radical claims make it too demanding. They also contend that utilitarianism is overly
focused on wellbeing and presents challenges that are difficult to address. Despite these criticisms,
utilitarianism provides a clear framework for determining the morality of actions.
We have already explored several concepts that are helpful for creating utilitarian AIs.
A utilitarian might want to create AIs that act in a utilitarian manner, maximizing total utility. We
have explored concrete ways to begin doing so. If we wish to create preference utilitarian AIs, we
might model individual preferences using utility functions. Instead, we might think other conceptions
of wellbeing like hedonism are more accurate; if so, we might estimate general-purpose wellbeing
functions instead. AIs based on different theories of wellbeing prefer different outcomes.
Once we have a good representation of individual wellbeing, we must aggregate these to create a
utilitarian social welfare function. However, this might not be as easy as it sounds. For one, we need
to decide whose wellbeing is relevant to every decision. Evaluating the effect of an action on everyone’s
wellbeing can be difficult in highly complex, socially connected worlds. These approximations need
to hold across different time horizons as well, not just short ones.
Creating utilitarian AIs requires overcoming several practical challenges.
In principle,
creating a utilitarian AI is straightforward. In reality, this requires addressing several concrete
challenges. For instance, creating utility estimates requires not only an accurate understanding of
general human preferences but also the ability to adapt these estimates to particular individuals,
since everyone has their own idiosyncracies. Decision-making must account for balancing short-term
pleasures and long-term wellbeing. Most actions affect a large number of other people, often indirectly;
as a result, a utilitarian AI must consider a broad range of stakeholders for every decision.
Utilitarian decision-making can require solving intractable problems.
AIs will need to
have a good predictive model of the world, with the ability to forecast the effects of their actions on
large numbers of people. This is especially difficult for important decisions that consider complex
systems or long time horizons. AIs should not resort to short-term optimizations that could have
long-lasting negative consequences, such as pushing students to enjoy extra leisure time instead
of studying to reap benefits later. The possibility of tail risks and black swans make utilitarian
decision-making, which relies on expected value, even harder to practice.

7.4. MORAL THEORIES
373
7.4.2
Deontology
Deontology is the name for a family of ethical theories that deny that the rightness of
actions is solely determined by their consequences.
Deontologists emphasize constraints
rather than consequences. “Thou shalt not kill”, “thou shalt not steal”, “honor thy mother and father”—
these are deontological principles that may be familiar from the Ten Commandments. Deontological
theories are systems of rules or obligations that constrain moral behavior [15]. They may be based
on a theological justification, but they do not need to be. These theories are often based on simple
and unambiguous rules, which may make them easier than other theories to implement in AIs.
The term deontology encompasses religious ethical theories, non-religious ethical theories,
and principles and rules that are not part of theories at all.
Some deontological theories are
religious. For example, divine command theory teaches that we have a duty to do as God commands.
Others, like Kant’s ethics (which we will discuss later), are non-religious. While deontological theories
may derive their rules from different sources, they are united by their focus on duties and rights.
Many deontological principles are not tied to any particular theory. Instead, they may be an attempt
to find rules or principles which fit our intuitions about specific moral issues like abortion, terrorism,
or suicide. This kind of moral analysis is still deontological—it is looking for universal rules which
can tell us what to do in particular cases—even though it is not tied to a specific theory. Most of
what we will say about deontology in this section is applicable to deontological theories.
Features of Deontological Theories
Deontological theories give obligations and constraints priority over consequences.
Unlike consequentialism, deontological theories do not justify their rules by appealing to their
consequences. Under deontological theories, some actions (like lying or killing) are simply wrong, and
they cannot be justified by the good consequences that they might bring about. For example, when
Elena’s boss asks her how hard her co-workers work, a deontologist might argue that she should tell
the truth, even if she knows the truth will lead to some of her colleagues losing their jobs.
Many constraints on our actions are derived from a respect for other people’s rights.
On most accounts, every person has certain rights simply by virtue of being a person. Each individual
has a claim to them, and no one is permitted to violate them under any circumstances. Common
examples of rights include the right to life, the right to freedom, and the right to autonomy.
Modern deontological theories tend to emphasize that we should not interfere with
others’ autonomy.
Since Kant developed his moral theory, many deontologists have followed him
in placing importance on human autonomy, our ability to freely choose how we act. This means
protecting each other from acts which might restrict our autonomy. This is an example where different
moral theories place emphases on normative factors: whereas utilitarianism focuses on wellbeing,
deontological theories often emphasize autonomy.
Another key part of their idea of autonomy is that our actions are not entirely governed by moral
considerations. We are constrained from certain types of behavior, but apart from those behaviors,
we can freely choose how to act. When we are choosing a career for example, we should not become
a torturer, or an assassin, but apart from those types of constraints, we can choose from a range of
harmless careers that suit our interests. By contrast, a utilitarian might argue that we must choose
the single career (if there is one) that would have the best outcome.

374
CHAPTER 7. NORMATIVE ETHICS
According to deontology, intentions can be right or wrong, as well as actions.
Many
deontological theories assert that intentions, as well as actions, can be moral or immoral. For example:
Intentional push: Farid’s elderly mother has been annoying him, so he decides to push
her down the stairs. When he next sees her at the top of the stairs, he carries out his
plan.
Intention, but no push: Farid’s elderly mother has been annoying him, so he decides to
push her down the stairs. When he next sees her at the top of the stairs, he plans to
push her. Just before he does, she trips and falls.
According to some deontological theories, Farid is equally wrong in both scenarios. Though he never
pushes his mother in the second scenario, the intention itself is a moral error. By contrast, for
classical utilitarians, intentions do not matter in themselves. On this view, intentions are only right
or wrong insofar as they lead to good or bad consequences.
Our common-sense moral intuitions are often aligned with the idea that intentions matter. Perhaps
that’s why they play a very important role in the law of many countries. In America’s Model Penal
Code, for example, the strength of a criminal’s intention is measured with a scale of four adverbs:
a criminal could commit a crime purposefully, knowingly, recklessly, or negligently. If they commit
it purposefully, then they knew what the outcome would be, and they intended that outcome to
happen. If they commit it knowingly, they don’t primarily intend the criminal effect of their actions,
but they act anyway. People are acting recklessly when they knowingly engage in behaviors which
pose risks to others, like owning a tiger, or flying a drone too close to an airport. Those who act
negligently fail to perceive a substantial and unjustifiable risk that their conduct will have harmful
results. Not all legal codes differentiate between these four levels in practice, but they all punish
purposeful or knowing criminals more than negligent ones.
If a driver purposefully rams their car into another vehicle, intending to cause injury, they would
face serious criminal charges. If the driver was speeding recklessly and lost control of their vehicle,
resulting in accidental injury to another driver, they may face a lesser charge, such as reckless
driving. The lessened mental state, even if the end result was the same, often reduces the severity of
punishment under the law. As we can see, to determine whether an AI acted immorally, some moral
theories would require that we be able to determine an AI’s intent, a goal of transparency research.
Deontologists advance a number of arguments against consequentialism that help to clarify the
distinctive features of their moral theories. There are many deontological theories, and they are
often grouped together under one umbrella simply because they share the feature that they are not
consequentialism. Deontological theorists often criticize consequentialism, and partially define their
theories by the ways that they make up for the flaws they see in consequentialism. Two problems
that they see in consequentialism are that (1) consequentialism is very demanding (in other words,
it doesn’t allow much autonomy) and (2) consequentialism leads to some radical conclusions (for
example, it sometimes justifies actions that most people believe are wrong).
(1) Unlike consequentialism, deontology gives us options.
Deontology preserves human
autonomy because it only forbids us from performing a limited number of impermissible actions. The
remaining actions are optional. On the other hand, consequentialism implies that every action is
moral or immoral to a certain degree, and each person is obligated to do the most good at all times.
According to consequentialism, every life decision—like marriage, career choice, which relationships
to pursue, which food to eat—is a moral decision. Deontologists find this to be far too demanding.

7.4. MORAL THEORIES
375
(2) Unlike consequentialism, deontology does not allow any action to be justified by its
outcome.
According to many consequentialist theories, any action can be justified if it results in
a better outcome than the alternative actions. Even killing or torturing innocent people might be
the right choice if it increases everyone else’s wellbeing more than it harms its victims. Some people
believe that deontological theories—which forbid actions like killing and torture in all cases—are
more plausible.
Deontological Principles
We need principles, as well as rules, to capture the complexity of ethics.
While
deontologists generally consider the absolute prohibition of certain actions to be a strength of
deontology, it can sometimes lead to counterintuitive moral judgments. Suppose a pair of conjoined
twins will die without undergoing a medical procedure. However, if the procedure is carried out, one
of them will die. The rule “do not kill” might stop a surgeon from operating, which would mean that
neither twin has a chance of survival.
Difficult, messy situations like this, where clear-cut rules seem to fail us, have led some deontologists
to develop important distinctions which complicate their theory but better capture the way we
think about ethics. We will introduce two of these principles: the doctrine of double effect and the
action/omission distinction.
The doctrine of double effect.
In the case of the conjoined twins, the surgeon might appeal to
the doctrine of double effect. This is a principle which states that an agent is morally allowed to
carry out actions that predictably lead to bad outcomes–—like the death of an innocent–—as long as
they intend the good effect, but not the bad effect of the action. The constraint against letting two
people die, however, must be stronger than the constraint against performing the surgery. In this
case, the doctor can operate only if she intends to save one of them, not to bring about a death. The
doctrine of double effect can also explain why it can be morally permissible to kill in self defense
[16]. You are permitted to defend yourself and your family from imminent attack. If the only way to
protect yourself is to kill your assailant, you may be permitted, as long as you intend to save your
family and not to kill.
The action/omission distinction.
The action/omission distinction is important for understanding
the role of responsibility in deontological theories. Intuitively, we find someone to be more responsible
for something they did than for something they allowed to happen [17]. For example:
Stealing: While walking past the bank at night, Gabe sees that the night deposit box
is open. Inside it, he sees a bag filled with money. He decides to steal the bag.
Failing to report: Heather sees Gabe take the money, but doesn’t report it to the police.
If she had, the money would have been returned to its owner.
In these examples, both Gabe and Heather have done something wrong, but Gabe’s crime is worse.
He is directly responsible for the theft, while Heather has only committed an act of omission–—she
failed to report the crime. This captures some of our ordinary intuitions about responsibility. We
generally do not hold people responsible for what they do not do.
Criticisms of Deontology
Criticism: Some say that deontology responds unconvincingly to moral catastrophes.
Nuclear terrorism: Ines is part of a terrorist cell that has placed a nuclear weapon in a
capital city. If it goes off, it will kill millions. She claims that it will go off within 24

376
CHAPTER 7. NORMATIVE ETHICS
hours, but she will not say which city it is in. Jamie has captured Ines and questioned
her, but Ines will not give away the bomb’s location. If Jamie tortures Ines, she will
find out the bomb’s location and millions of lives will be saved.
Some deontological theories accept that Jamie should not torture Ines, no matter how many people
will die as a result. However, many people find this implausible. When so many lives are at stake, it
may seem selfish for Jamie to take the easier option, prioritizing her moral purity over the lives of
the people she could save.
To accommodate our intuitions about moral catastrophes, some deontological theorists have adopted
a threshold [18]. According to a threshold view, when a certain number of lives are at stake (i.e. when
the badness of an outcome reaches a certain threshold), the theory defers to the consequentialist
recommendation that the otherwise impermissible act (torture in this case) is allowed.
There are a number of problems with the threshold view. Most importantly, it is not clear how to
determine what the threshold should be, and any decision about it will be arbitrary. If a million lives
at stake justify Jamie’s act of torture, then it seems odd to say that ten thousand, one thousand, or
even ten lives at stake do not.
Criticism: Deontology sometimes requires us to make the world worse.
Better job: Kimiko has been offered a job where she will use her unique set of skills to
reform the health system of the country she lives in. The next best hire for the job is
far less experienced than she is. Therefore, if Kimiko takes the job, thousands of lives
will be spared every year for at least a decade. However, the job would require Kimiko
to move to a new city, and she promised her children that the family would not move
until they had all finished school.
Many deontological theories consider promise-keeping to be very important. They would not allow
Kimiko to break her promise to her children in this situation, even though keeping her promise would
cost thousands of lives.
Deontological rules may sometimes lead to the best consequences, but they often do not. This
leads to what some people call the paradox of deontology. There may be situations in which it is
impermissible to stop many instances of the same impermissible act occurring. If there is a rule that
we cannot lie and kill, then we cannot lie to prevent hundreds of acts of lying, or kill to prevent
hundreds of acts of killing.
Immanuel Kant and the Categorical Imperative
Immanuel Kant was a German enlightenment thinker who developed an especially strong deontological
theory which we now call Kantianism. According to Kant, some actions are absolutely, universally
wrong. For instance, Kant believed that killing, stealing, lying, committing suicide, and breaking a
promise are wrong in all circumstances.
Kant believed that anyone can (in theory) arrive at these conclusions due to their own capacity
to reason. Because, in his opinion, we would all arrive at the same conclusions, he believed in a
universal moral law which we all have a duty to follow. The method that he thought would help us
discover this moral law is called the categorical imperative.
Kant described the categorical imperative in several different ways, and his descriptions are different
enough that they are now referred to as separate formulations of the imperative [19]. In other words,

7.4. MORAL THEORIES
377
they are different ways to discover the same moral law. We will focus on two formulations: the
universal law formulation, and the humanity formulation. The universal law formulation asks each of
us to imagine that when we make a moral rule for ourselves to follow, we have actually made a law
for everyone. In other words, we need to ask ourselves: what if everyone did that? If a world where
everyone followed our rule was contradictory or irrational, then we have discovered something that
we shouldn’t do. The humanity formulation tells us to act on rules which lead us to treat people in a
way that lets them maintain their autonomy. This means never getting in the way of their ability to
exercise their human capacities for reason and autonomy, or make their own decisions.
Kant’s method is called the categorical imperative because he believed all moral rules
must be categorical.
Kant distinguishes two types of rules: hypothetical rules are of the form
“do X in order to Y,” which only apply when we already want to Y, and categorical rules are of the
form “do X.” An example of a hypothetical rule is “be kind to people if you want them to do you
favors.” This is hypothetical, or conditional, because we would only be required to follow this rule if
we already cared about receiving favors. Kant thought that the moral law was a universal list of rules
which apply to everyone, so he argued that all moral rules must be categorical, not hypothetical. A
categorical rule like “be kind” can apply to everyone, while the hypothetical “be kind if you want
them to do you favors” only applies to those who want favors.
The universal law formulation.
This idea leads us to the first formulation of the categorical
imperative, Kant’s method for discerning right from wrong. Kant tells us in this formulation that we
should only act in ways which could be made into laws for all of humankind. In other words, what if
everyone did that same thing: “What if everyone killed people who stood in their way?” “What if
everyone cheated on exams?” “What if everyone lied?” If the answer to the question seems to make
your intended action impossible or inadvisable, do not do it. This formulation provides the clearest
test to show whether an action is in accordance with the moral law or not.
In slightly more complicated terms, we can formalize Kant’s thoughts on universalising rules into
a four-part test for any rule which we can apply to any categorical rule we might think of. If our
proposed rule passes all four stages, then it is permissible. First, we turn our proposed action into a
categorical rule (“do X” or “do X when in Situation S”). Then, we change the rule so that it applies
to everyone, not just us. To test whether the rule is part of the moral law, we first check whether
it contradicts itself. If it does, then it is a rule we absolutely must not follow. If the rule isn’t
contradictory, we check whether it conflicts with something else that we must value. If it does, then
we shouldn’t follow the rule; if it doesn’t, then we must follow it. We will now go through an example
of each step, to model how this process might work.
Step 1: Turn the proposed action into a rule.
Luke promised to take his mother’s dog for a walk. But today he is tired and doesn’t want to. He
proposes not to go on the walk. If his action became a rule, the rule might be: “I will not fulfill
promises when it is inconvenient to me.”
Step 2: Make that rule apply to everyone.
To do this, we just remove Luke from the rule: “No one should fulfill promises when it is inconvenient
for them.”

378
CHAPTER 7. NORMATIVE ETHICS
Step 3: The “contradiction in conception:” Can we coherently imagine a world where everyone follows
the rule?
Luke’s rule fails at this stage. In order for Luke to conceive of his rule, the institution of promise-
keeping must exist. However, in a world in which everyone breaks their promises, the institution of
promise-keeping doesn’t really exist. Luke’s rule fails because it leads to a contradiction.
According to Kant, we can conclude from the contradiction in conception that we should never break
promises just for convenience.
Step 4: The “contradiction in the will:” If the rule is conceivable, would it be rational to follow it?
Would it conflict with something else we must do?
To illustrate the contradiction in the will, Kant considers the case of laziness. Suppose Mari never
works hard because she is lazy. We could formulate her action as a rule: “You shouldn’t work hard if
you feel lazy.” This rule passes the contradiction in conception because it’s possible to imagine a
world in which no one works hard. However, Kant argues that it fails as a rule for another reason: it
contradicts our will. In Kant’s view, it is in our nature as rational beings to work and to “develop our
talents.” Therefore, we should not be lazy because, as a rule, it would violate our rational nature.
Kant’s universal law formulation of the categorical imperative is a method of testing whether a rule
can be willed for everyone. First, we determine whether the rule is even conceivable. Then, we
determine whether people would will it. In theory, this method can tell us whether any rule is right
or wrong. A Kantian AI would therefore need the capacity to reason.
Now we turn to an alternative formulation of the categorical imperative.
The “humanity” formulation of the categorical imperative.
The humanity formulation is
perhaps more influential among philosophers today than the universal law formulation. Roughly
speaking, the humanity formulation states that we should always treat other people’s humanity as
an end, not merely as a means. Kant means something specific by humanity, end, and means.
When Kant writes about humanity, he is referring to the ability to engage in autonomous, rational
behavior that he believed was characteristic of human nature. Ends are the goals that we aim to
reach, and means are the methods of achieving ends. To treat humanity always as an end and never
as a means is to treat everyone with respect for their autonomy and rationality. It’s one of Kant’s
most influential ideas. To make this idea more concrete, here is an example.
The urgent lift: Nathan wants a lift into town to go go-carting. He approaches a
stranger and lies to her, telling her that his brother is having an allergic attack in town
and he needs to deliver his EpiPen. Nathan tells the stranger that if she doesn’t give
him a lift, his brother might die.
In the example, Nathan is treating the stranger as a means to get into town. He doesn’t respect the
stranger as a person with her own ends, who may have better things to do. By lying to the stranger,
Nathan undermines her autonomy by obscuring the truth. In other words, he is not respecting the
stranger’s humanity.

7.4. MORAL THEORIES
379
Criticisms of Kant’s Ethics
Criticism: Many modern philosophers find Kant’s ethics too extreme.
Mad axeman: Omar hears a knock on his door late at night. A man with a wild look
and a bloody ax asks, “Is Piper here?” Omar knows Piper is upstairs. Should he tell
the truth?
Kant’s ethical writings claim that it would be wrong for Omar to lie. According to Kant, lying is
always wrong. Even in the case of the mad axeman, Kant insisted that it would be wrong to lie
(though some philosophers inspired by Kant argue he could have avoided this claim). Most modern
moral philosophers disagree with Kant’s conclusion in this case.
Pro tanto duties.
Instead, philosophers refer to duties to act which may be overridden by other
duties. Many modern philosophers would agree that Omar has a duty to avoid lying. But they would
also argue that he also has a duty to safeguard his friend, and that this duty outweighs his duty to
avoid lying. In other words, many modern philosophers would see Omar’s duty to avoid lying as pro
tanto. Latin for “to that extent,” pro tanto means that a given duty can be weighed against other
duties to determine a course of action. Although the principle of honesty offers a pro tanto reason in
favor of revealing Piper’s real location to the axeman, it is not the only consideration. Other moral
obligations, such as the duty to ensure others’ welfare, may ultimately mean Omar should withhold
Piper’s real location from the axeman.
Aspects of Kant’s ethics inspire modern deontology.
Many aspects of Kant’s morality are
explicitly present in modern moral theories. For example, many deontological theories still reflect the
ideas that we should treat people as ends, not as mere means; the focus on respecting others’ rational
and autonomous humanity; and the concept of considering how your actions might apply universally.
Conclusions about Deontology
Deontology emphasizes constraints on actions.
Deontological theories consider systems of
rules, often derived from basic concepts such as mutual respect for rights and autonomy, that prohibit
acting in certain ways. These rules include principles such as the doctrine of double effect and
the action/omission distinction, which capture intuitions about the role of intentions in morality.
Kantian ethics is a well-known form of deontological ethics. It deduces the categorical imperative,
which is a principle with many formulations, one of which is that we are morally required to follow
general rules of acting that we would be happy for everyone else to follow, from the assumption of
universal capacity to reason. However, deontological ethics is often criticized for being rigid in its
rule-following and insensitive to outcomes. Modern philosophers sometimes find Kant’s opinions
extreme and ambiguous.
Deontological constraints in machine ethics.
In the near term, we will likely see the use of AIs
to maximize wealth or pursue other ambitious goals. Constraints on these AIs’ actions are important.
At the very least, such AIs must follow the law. However, the law is insufficient to ensure that these
bots act ethically. We might want to supplement constraints provided by the law with deontological
constraints as well. Requiring AIs to respect autonomy, for instance, might help avoid manipulation
that is legal but unethical.
It is hard to get AIs to reliably follow rules.
Whether due to clever workarounds or outright
disregard, entities—such as humans or corporations—often engage in minimal compliance or exploit
loopholes to achieve their objectives. This tendency to circumvent rules can also be present in AI
systems. LLMs such as GPT-4 and Llama-2 have struggled to reliably adhere to straightforward

380
CHAPTER 7. NORMATIVE ETHICS
instructions under the RuLES benchmark, a series of tests assessing rule adherence across different
scenarios like security protocols and games. These AIs have exhibited vulnerabilities, succumbing to
adversarial attacks where they are tricked or provoked into breaking rules through jailbreaks and
prompt injections. To impose a deontological system of ethics, we would need to ensure that AIs can
reliably follow rules.
7.4.3
Virtue Ethics
Virtue ethics is a moral theory that emphasizes the importance of having the right character traits,
rather than producing the right consequences or performing the right actions [20]. A virtue ethicist
might argue that we should help others in need by donating to charity, but not because we should
promote wellbeing or because we have certain moral obligations. According to a virtue ethicist, we
should donate to charity because that’s what a generous person would do, and generosity is a virtue.
Modern virtue ethics is inspired by the ancient Greek philosopher Aristotle. In his book “Nicomachean
Ethics,” Aristotle explored three key concepts that are essential for understanding virtue ethics [21].
First, he explored the concept of virtue, or morally good character traits. Second, he developed the
concept of practical wisdom, which is the set of skills and experience required in order to behave
in line with virtues. Third, he argued that developing virtue and exercising practical wisdom are
essential for flourishing, or living a good life. Each concept is described in more detail below.
Virtue
A virtue is a morally good character trait or disposition. Putative examples of virtues
include courage, generosity, fairness, and kindness. Virtues are morally good character traits, and
vices are morally bad ones. Putative vices include cowardice, selfishness, unfairness, and cruelness.
Having a certain virtue or vice is not binary, but a matter of degree. In other words, individuals
aren’t typically completely courageous or completely cowardly; they can be more or less courageous.
To be virtuous is not just to behave in certain ways but also to feel certain ways.
Two
individuals might behave in exactly the same ways but have different feelings, and thus exhibit
different virtues and vices.
Consider two siblings, Bobby and Cory, who behave similarly in every situation. They are both
trusted by their friends, they both keep their promises, and they are both equally honest. However,
Bobby behaves virtuously because it makes him feel good; he derives pleasure from helping others.
Cory, on the other hand, behaves virtuously despite her feelings; helping others feels to her like a
burden. Her behavior is the same as Bobby’s, perhaps because she wishes to be seen as a virtuous
person or she wants to avoid getting in trouble. According to most virtue theories, only Bobby is
virtuous. While Cory behaves the same way, her behavior does not indicate virtue.
Virtue ethicists claim that other theories miss important morally relevant features.
Mental states like emotions and motivations, virtue ethicists argue, are morally relevant.
A
consequentialist would evaluate Bobby and Cory as equally moral because their actions produce the
same consequences. Some, but not all, deontologists would evaluate Bobby and Cory as equally moral
because they behave the same ways with respect to rules and obligations. Virtue ethics emphasizes
an intuition that many people have: that Bobby is morally superior to Cory.
Practical Wisdom
Being disposed to behave virtuously is necessary, but not sufficient, for being a virtuous person. It’s
also important, according to virtue ethics, to have practical wisdom–—the ability to reason and to

7.4. MORAL THEORIES
381
act appropriately on the inclination to be virtuous.
For individuals who lack practical wisdom, the inclination to be virtuous can lead them to behave
wrongly. Someone who is inclined towards honesty and who derives pleasure from being honest
might, in some situations, be too honest. If they lack practical reason, they may needlessly insult
strangers or cause conflicts between friends. Practical wisdom is the ability to understand when it’s
appropriate to be honest and when it’s important to act on a different virtue, like kindness.
While people may be born with the disposition towards certain virtues, practical wisdom
is learned through experience.
Children, for example, may desire to behave well, but make
errors due to a lack of experience. They may tell their mother that they don’t like her outfit, unable
to differentiate between honesty and cruelty. If their mother reacts with hurt feelings, her children
will learn from the experience that, in some situations, kindness is more appropriate than honesty.
In other words, they will gain practical wisdom. We can learn practical wisdom from people who
have had more experience than us, but also from cultural figures who clearly excel in their virtue,
people who offer excellent examples of virtues of honesty, steadfastness, and compassion. Evidently,
if we would like to make AIs virtuous, we would need to have exemplars for them to imitate.
Flourishing
Flourishing is living a good life.
Aristotle believed that being virtuous is necessary for flourishing.
In fact, he defined virtues in terms of their relationship towards flourishing. Virtues, he argued, are
those character traits which lead an individual to flourish. The virtue ethicist’s idea of flourishing
has similarities to the objective goods account of wellbeing that we discussed earlier in this chapter.
While most virtue ethicists agree that being virtuous is necessary for living a good life, they often
disagree about whether it is sufficient for living a good life. Aristotle argued that, in addition to
being virtuous, an individual must have the resources to enact virtue in order to lead a flourishing life.
Someone living in poverty, according to Aristotle, is unable to enact certain virtues, like magnanimity.
A virtuous but very unlucky person may be unable to flourish.
In sum, virtue ethics argues that to live a good life we must develop our virtues, and that to develop
our virtues we should imitate people who act virtuously.
Criticisms of Virtue Ethics
Criticism: Virtue ethics is not action-guiding.
Virtue ethics doesn’t always clearly help us determine the right things to do. When
faced with a dilemma, like whether or not to steal from a grocery store in order to feed one’s family,
virtue ethics does not seem to offer much guidance. We should be generous and selfless, which seems
to suggest that stealing is wrong. However, we should also be loyal and protective of our family,
which seems to suggest that we should do what is necessary in order to feed them. In such cases,
virtue ethics may not seem very useful. Virtue ethicists may argue, however, that while their theory
does not include a decision procedure for every situation, a virtuous person with a high capacity for
practical reasoning will understand which virtues to express and when.
Criticism: Virtue ethics is too focused on the individual.
According to virtue ethics, whether an action is right or wrong depends entirely on characteristics of
the actor. If the person performing the action is ideally virtuous, then their actions will be morally
right. This may seem odd to those who believe that the field of ethics is concerned with how to

382
CHAPTER 7. NORMATIVE ETHICS
treat other people. Presumably we should save someone from drowning, not because of facts about
our own character traits but because of facts about the drowning person. We should save them
because their life has value, because their wellbeing matters, and because they have a right to life,
not because we are courageous.
Conclusions about Virtue Ethics
Virtue ethics places emphasis on the character of the individual.
While consequentialist
theories prioritize outcomes and deontological theories focus on constraints, virtue ethics centers
around the importance of acting virtuously. Aristotle, who greatly influenced modern virtue ethics,
highlighted virtue (good character traits), practical wisdom (the skills and experience necessary for
virtuous action), and flourishing (living a good life) as the three fundamental aspects of morality.
He argued that the first two are essential for achieving the third. Critics of virtue ethics argue that
it lacks clear guidance for action and places excessive emphasis on the individual rather than their
actions.
Virtues can be instilled into AIs.
Characteristics such as justice, honesty, responsibility, care,
prudence, and fortitude have been proposed as basic AI virtues. AIs that are trained to represent
such virtues might balance promoting their objectives with behaving ethically. Assuming we can
train AIs in such ways, we might fine-tune them to represent different characteristics in different
settings: an AI used for data analysis might prioritize honesty, while an AI used for teaching children
might prioritize responsibility and care. We can imagine turning up or down different characteristics
to create ethical AIs that act appropriately to the setting.
AIs can help us cultivate virtues.
We might want to use AIs to create moral value for humans;
virtue ethics suggests a few ways to do so. One might be to help individuals and societies become
more virtuous, such as by helping people take opportunities to be virtuous or improving material
conditions so that they can better exercise virtues like magnanimity and generosity. Similarly, AIs
might help with developing their users’ practical wisdom, such as through education or encouraging
them to undertake projects. The goal of such systems might be to increase human flourishing.
7.4.4
Social Contract Theory
The focus of this section is social contract theories of morality. As the name suggests, social contract
theory focuses on contracts—or, more generally, hypothetical agreements between members of a
society–—as the foundation of ethics. A rule such as “do not kill” is morally right, according to a
social contract theorist, because individuals would agree that the adoption of this rule is in their
mutual best interest, and would therefore insert it into a social contract underpinning that society.
The most influential contemporary theorist within this tradition is John Rawls, who we will use
to contextualize social contract theory, using the famous veil of ignorance thought experiment [22].
After understanding the broad strokes of his theory, we will consider a few reasons why such reasoning
might be inadequate and consider some alternatives.
According to social contract theory, moral codes are the result of hypothetical agreements
between members of society, established for mutual benefit.
Let us consider the prohibition
of thievery. It seems reasonable that people would agree to refrain from stealing: most people stand
to benefit from a society that punishes thieves, given that it would disincentivize others from robbing
them. The ethical principle of not stealing would thus have moral force, without requiring some
fundamental principles such as maximizing wellbeing or respecting autonomy. According to the social
contract theorist, all moral codes are similarly justified: they are reasonable hypothetical agreements
which encourage behavior that creates mutual benefit.

7.4. MORAL THEORIES
383
The Veil of Ignorance
Philosopher John Rawls introduced the concept of a veil of ignorance as a tool for creating a
social contract. When behind the veil of ignorance, individuals lose all knowledge of their personal
attributes, such as their talents, religion, gender, sexuality, or class. In this state, sometimes called the
original position, participants are asked to envision a basic structure for society, based on reasonable
agreements without any knowledge of their own positions within the society they create.
In “A Theory of Justice”, Rawls proposes one way to generate a social contract.
Rawls
places everyone behind a veil of ignorance to make decisions about a social contract. From here,
having lost all knowledge of their individual characteristics, participants are invited to envision a
basic structure for society, based on reasonable agreements without knowledge of their own position
in the society they create.
Once the decisions have been made, participants leave the original position, discover who they are in
society, and then live according to the social contract they created. Rawls believed that, if we were
able to use the veil of ignorance to construct a real society, the society would likely include ideas like:
protection of the worst-off, basic liberties for all, and restrictions on inequality. Because the people
determining these contracts do not know which social group they will be a part of, they would not
create a society in which some people are far worse off than others.
The veil of ignorance would prevent slavery.
Those in Rawls’ original position would arrive
at many sensible conclusions. An individual behind the veil would not reasonably permit slavery, for
instance, given that they do not know whether they are a slave or a slaver. On average, very few
people seek to benefit from slavery, and most people are harmed by it. Individuals would likely not
wish to take the chance of being a slave in exchange for a chance of being a wealthy slave owner.
Therefore, those in the original position would reject slavery, and this forms the basis of the social
contract to not enslave others.
Agreements behind the veil of ignorance create a basis for a just society.
This process
is unlikely to generate elitist, patriarchal, or ableist moral codes since one does not know whether
they have a high income, privileged racial or gender identity, or disability. The interests of any
particular group would not be favored, since no one knows whether they belong to that group. As
such, decisions behind the veil of ignorance are made in the interest of society. According to Rawls,
everyone accounts for this component of luck by making decisions as though they could, themselves,
be members of any group in society.
Principles That may be Generated from the Veil of Ignorance
In this section, we look at some of the conclusions that Rawls argues individuals in the original
position might reach: the maximin principle, the liberty principle, the equality of opportunity
principle, and the difference principle.
We should protect the worst off.
Behind the veil of ignorance, no one knows who they are,
which means that anyone might be the worst-off individual in society. The main idea behind Rawls’
contractarian logic is that people would agree to make sure everyone has a decent position in society,
since anyone might end up in the worst position. Rawls argues that people behind the veil of
ignorance would endorse the maximin principle, according to which we should prioritize the worst-off
in society.

384
CHAPTER 7. NORMATIVE ETHICS
We should not exclude anyone from having basic liberties.
Behind the veil of ignorance, no
one knows who they are, which means that if any individual is excluded from having basic liberties,
it could potentially be anyone. Therefore, individuals would rationally agree to distribute liberties to
every member of society. As long as one does not infringe upon others’ liberties, they should have
the freedom to pursue their own conception of the good life, and with it enjoy civil and political
liberties such as freedom of speech, religion, association, assembly, and the right to a fair trial. This
is Rawls’ liberty principle: the fair distribution of liberties, ensured by a contractarian agreement, do
not infringe upon the liberties of others.
We should ensure equality of opportunity.
Adopting the veil of ignorance may lead us to
conclude that no one should be denied opportunities based on their gender, age, race, family status, or
other personal characteristics. People behind the veil would agree that, when inequalities arise, they
should only do so through fair access to opportunity for everyone. Equality of opportunity ensures
that differences in the relative positions of individuals are meritocratic. This is Rawls’ equality of
opportunity principle: some degree of inequality is permissible, but only if it is merit-based.
We should require that inequalities benefit the least-advantaged.
Allowing some inequalities
can lead to increased overall wealth in a society, as higher earnings for the most productive members
can create incentives for economic growth that benefits everyone. This is the basis of Rawls’s
difference principle: some degree of inequality is permissible, but only if it also benefits the worst-off
members of society.
These principles, in theory, ensure two conditions: everyone has basic rights and equal access to
opportunity, and any inequalities that follow from the equality of opportunity also help the least
privileged, even if they help the privileged more.
Rawls’s Conclusions might be Too Strong
The maximin principle is at odds with common-sense morality. According to Rawls, our
moral evaluation of a society should be determined by the wellbeing of its worst-off individual. This
implies that we must invest all our resources into raising the wellbeing of the worst-off member of
society, and remain indifferent towards everyone else’s wellbeing, as long as they do not become the
worst-off.
The grouch takes priority.
Imagine a grouch: someone who is always at low levels of wellbeing
and extremely hard to please. Say that giving them a billion dollars would make them only as
happy as an ordinary person would be with a slice of cake. The maximin principle dictates that our
priority should be to focus on improving their wellbeing, even if it means using a large amount of
resources that could have made everyone else in society much happier. This prioritization seems
counterintuitive.
We must be indifferent towards improving everyone else’s wellbeing.
Suppose there was
a new medical breakthrough that can cure a widespread disease, greatly improving the lives of many
who suffer from the illness. This seems like a good thing, and it would be strange to not care about
this happening. However, if the worst-off individual doesn’t have that disease and their wellbeing
remains unchanged, nothing morally important has changed.
We must be indifferent towards decreasing everyone else’s wellbeing.
Consider a scenario
in which a technical error causes all electronically stored money to be deleted, plunging most countries
into chaos and leading to widespread poverty. This seems like a bad thing, and it would be strange

7.4. MORAL THEORIES
385
not to care about this happening. However, if the worst-off individual lives in an unaffected area and
does not have a bank account, and their wellbeing remains unchanged, once more nothing morally
important has changed.
The maximin principle seems untenable.
In all three of these scenarios, Rawls’ maximin
principle gives us implausible answers. We likely should not spend all of our resources to give one
unhappy person a bit of joy at the cost of everyone else’s wellbeing. Similarly, it seems extremely
morally relevant if we can cure a widespread disease or ensure that most people do not lose all their
money.
Rawls’ Conclusions Might Not Follow from the Veil of Ignorance
Behind the veil of ignorance, we would care about more than maximin. If, behind the
veil of ignorance, we know that there is just one person who will be much worse off than everyone
else, we may not unanimously agree to prioritize making that person’s situation better. Individuals
behind the veil of ignorance may not consider the worst-case outcomes when making decisions under
uncertainty. While the people behind the veil might be highly risk-averse, ensuring that the general
distribution of society is good rather than just the average level of wellbeing and endorsing the liberty
and difference principles, they would likely not endorse the maximin principle.
The veil of ignorance may lead to utilitarianism.
Nobel prize winning economist John
Harsanyi conceptualized the veil of ignorance before Rawls, and used it to support utilitarianism
[23]. He argued that rational agents would aim to maximize the total amount of wellbeing in their
society, so that the average outcome is as good as possible. Decisions under uncertainty often involve
maximizing expected or average results. Harsanyi argued that rational individuals behind the veil of
ignorance would therefore choose a utilitarian organization for society.
A problem for Rawls.
Rawls’ “A Theory of Justice” was designed as an alternative to utilitarianism,
but it has been used to justify utilitarianism. If the veil of ignorance indeed creates conditions more
conducive to utilitarian moral decisions, it might actually support utilitarianism instead of Rawls’
theory of justice.
Alternatives to Rawls’ Social Contract Theory
A natural middle ground between Rawlsian and utilitarian ideas is prioritarianism.
Prioritarianism is an ethical theory that gives greater moral weight to improving the wellbeing of
those who are worst off in society [24]. Imagine a situation where we have the option to distribute
resources among three people: Rana, Sean, and Toby. Before any intervention, they have the following
levels of wellbeing: Rana has 6 units of wellbeing, Sean has 5 units of wellbeing, and Toby has 1 unit
of wellbeing.
We can choose one of the following options:
1. Increase Rana and Sean’s wellbeing by 2 units each, resulting in (8, 7, 1);
2. Increase Toby’s wellbeing by 1 unit, resulting in (6, 5, 2); or
3. Increase Toby’s wellbeing by 2 units, reduce Sean’s wellbeing by 2 units, and reduce Rana’s
wellbeing by 3 units, resulting in (3, 3, 3).
Prioritarianism suggests that we should prioritize improving the wellbeing of Toby, the worst-off
individual. However, we should also take into account the wellbeing of others. Unlike utilitarianism,
which dictates that we choose option 1 to maximize overall wellbeing, prioritarianism suggests that

386
CHAPTER 7. NORMATIVE ETHICS
we choose option 2 to help the most disadvantaged member. Unlike Rawls’ maximin principle, which
suggests that we choose option 3 to most improve the situation of the least advantaged individual
(even at the cost of everyone else), prioritarianism attributes moral weight to the wellbeing of Rana
and Sean and so might choose option 1 or 2 instead. This approach strikes a balance between
utilitarianism and Rawlsianism, placing higher moral value on improving someone’s life when (a) that
person’s overall wellbeing is relatively low and (b) the increase in wellbeing would be substantial.
One example of a prioritarian policy is focusing educational interventions on disadvantaged students.
Prioritarians might support policies that concentrate resources on helping students from disadvantaged
backgrounds or those with learning disabilities, with the goal of closing achievement gaps and
improving outcomes for the worst-off. Utilitarians might argue that resources should be allocated
to improve overall educational outcomes, which might involve investing in programs that benefit a
larger number of students, even if it doesn’t specifically target the most disadvantaged.
Contractualism.
Philosopher T.M. Scanlon developed another approach to social contract theory
called contractualism [25]. Scanlon established contractualism to build on the idea that morality
is based on agreements between people while addressing some of the limitations of Rawls’ theory.
According to Scanlon, people are naturally inclined to seek reasonable moral agreements, driven
by their sense of justice. This makes Rawls’ original position, in which people are behind a veil of
ignorance, unnecessary. In Scanlon’s view, morality is about what we owe each other as rational
beings.
Scanlon did not have a definitive answer to what we owe one another. Instead, he proposed using
a social contract underpinned by reasonableness. It is reasonable to reject a moral code declaring
slavery is right. However, it is less reasonable to reject a moral code declaring that community
resources should be used to help the worse off. For a contractualist, certain actions are wrong if
they don’t meet a standard of behavior that “no one could reasonably reject as a basis for informed,
unforced, general agreement.” This means that principles of morality should be something that
rational people can generally accept. By making fewer strong assumptions, Scanlon avoids some of
the pitfalls of Rawls’ process and conclusions.
Conclusions about Social Contract Theory
Rawls’ contractarianism and the veil of ignorance.
Social contract theory proposes that
agreements between society’s members form the foundation of morality. Moral codes are determined
by whether rational individuals would agree to mutually abide by them. John Rawls’ influential
approach relies on the veil of ignorance, where decision-makers are unaware of their position in
society. Rawls’ theory suggests the maximin, liberty, and difference principles, which together aim to
raise the lowest levels of wellbeing, ensure the provision of basic liberties, and minimize inequalities
in society.
Alternatives to Rawls: Scanlonian contractualism, prioritarianism, and utilitarianism.
Some of Rawls’ conclusions, and especially the maximin principle, are implausible. The veil of
ignorance can be used to support utilitarianism, and alternatives like prioritarianism are better at
accounting for some of our intuitions. Scanlon’s contractualism offers another perspective on social
contract theory, focusing on people acting according to reasonable principles and contending that
morality is built upon obligations to others—what we owe to each other. Social contract theories
offer an alternative way of thinking about moral agreements that might be highly relevant to modern
approaches to machine ethics, highlighting the importance of rational agreement and mutual benefit
in shaping ethical principles.

7.4. MORAL THEORIES
387
Several contractarian principles can be used to inform AI design.
Often, we require
algorithmic decision-making to be blinded to personal details like race, gender, and social status.
This can be justified by Rawlsian ideas such as the veil of ignorance and the equality of opportunity
principle. However, relying on social contract reasoning can present problems as well. Unlike other
theories, social contract theories have no fundamental values besides agreement. As a result, any
outcome that everyone agrees with can be considered morally justifiable. AIs forming a social contract
with each other may agree not to care for humans, or AIs forming a contract with humans may use
their intellect to persuade them to accept less-than-ideal terms.
Governance can be contractarian.
We can use AIs to emulate people behind the veil of ignorance
since AIs can reason from an impartial position and have them negotiate a social contract on our
behalf. We could use artificial intelligence to inform our Scanlonian judgments as well, such as by
asking AIs trained on human responses whether individuals have acted according to principles that
they would reasonably reject. Social contract theory can also provide insight into how we should
govern AIs, such as by recommending that our governance procedures are inclusive of all stakeholders’
interests and perspectives.
Summary
We have outlined four common approaches to morality. First we looked at utilitarianism, a
theory that argued that actions should be judged based on how much wellbeing they cause. Then we
considered deontology, the view that to live ethically is to live by the right system of rules. According
to virtue ethics, the goal of a good life is to become a virtuous person. Social contract theory
recommends following principles that we might arrive at together in an ideal contractual process.
These theories are all very different. They don’t just differ in their moral claims; they consider
morality to have different goals that should be approached in different ways.
The theories we’ve discussed are focused on different moral considerations.
Early in
this chapter, we discussed moral considerations like intrinsic goods, special obligations, constraints,
and options. Utilitarianism, of course, is most concerned with wellbeing, an intrinsic good, and does
not support options. Deontology especially emphasizes constraints. Social contract theory especially
emphasizes special obligations.
In common-sense morality (i.e. the moral decision-making that everyone does on a daily basis)
different considerations seem more important than others in different situations. It may be, then,
that some theories are more useful for thinking about some problems than others.
We needn’t pick a single moral theory, and the goal of this chapter is not to choose the
best one.
Utilitarianism and social contract theory might be the best approaches for thinking
about some society-wide policies, but perhaps deontology is helpful when we are drafting laws that
are intended to apply to everyone in a country. Virtue ethics may be especially useful in day-to-day
situations.
When moral theories conflict, it is important that we accommodate some uncertainty. There may be
something we can learn from all of them.

388
CHAPTER 7. NORMATIVE ETHICS
7.5
Moral Uncertainty
7.5.1
Making Decisions Under Moral Uncertainty
This section considers how to make decisions under moral uncertainty–—how to act morally when
we are unsure which moral view is correct. Although ignoring our uncertainty may be a comfortable
approach in daily life, there are situations where it is crucial to identify the best decision. We will
start by considering our uncertainties about morality and the idea of reasonable pluralism, which
acknowledges the potential co-existence of multiple reasonable moral theories such as ethical theories,
common-sense morality, and religious teachings.
The main aim of the section is to recognize that there are different reasonable moral beliefs and
think about what to do with them. We will explore uncertainties about moral truths, why they
matter in moral decision-making for both humans and AI, and how to deal with them. We will look
at a few proposed solutions, including My Favorite Theory, Maximize Expected Choice-Worthiness
(MEC), and Moral Parliament [26]. These approaches will be compared and evaluated in terms of
their ability to help us make moral decisions under uncertainty.
Dealing with Moral Uncertainty
Moral uncertainty requires us to consider multiple moral theories. Individuals may have
varying degrees of belief in different moral theories, known as credence.
Credence is the probability assigned by an individual of the chance a theory is true.
Someone may have a high degree of credence (70%) in utilitarianism, meaning they believe that
maximizing utility is likely to be the most important moral principle. However, they may also have
some credence in deontological rules, believing they are plausible but less likely to be true than
utilitarianism (30%). Since ethical theories are not flawless, often arriving at intuitively questionable
conclusions, it is valuable to consider multiple perspectives when making moral decisions.
Living a good life can require a combination of insights from multiple moral theories.
We often find ourselves balancing different kinds of deeds: creating pleasure for people, respecting
autonomy, and adhering to societal moral norms. Abiding by a reasonable pluralism means accepting
that moral guidance from different sources may conflict while still offering value.
In high-stakes moral decisions, such as in healthcare or AI design, reasonable pluralism
alone may fall short.
When a healthcare administrator faces the tough choice of allocating limited
resources between hospitals, we might want them to seriously consider the ethics of what they are
doing rather than just go with conflicting wisdom that seems reasonable at first glance. Therefore,
we must think hard about how to make decisions under moral uncertainty–—seeking truth for crucial
decisions is vital.
If AI is unable to account for moral uncertainty, harmful outcomes are likely.
As AI
systems become increasingly advanced, they are likely to become better at optimising for the goals
that we set them, finding more creative and powerful solutions to achieve these. However, if they
pursue very narrowly specified goals, there is a risk that these solutions come at the expense of other
important values that we failed to adequately include as part of their goals. Given that philosophers
have not yet converged on a single consistent moral theory that can take account of all relevant
arguments and intuitions, it seems important for us to design AI systems without encoding a false
sense of certainty about moral issues that could lead to unintended consequences. To counter such
problems, we need AI systems to recognize that a broad range of moral perspectives might be

7.5. MORAL UNCERTAINTY
389
valid—in other words, we need AI systems to acknowledge moral uncertainty. This presents another
challenge: how should we rationally balance the recommendations of different ethical theories? This
is the question of moral uncertainty.
7.5.2
How Should We Approach Moral Uncertainty?
There are several potential solutions to moral uncertainty. Faced with ethical uncertainty,
we can turn to systematic approaches, using our estimates of how theories judge different actions and
how likely these theories are. This section explores three potential solutions to moral uncertainty. The
first is adopting a favored moral theory that aligns with personal beliefs (My Favorite Theory). The
second is aiming for the highest average moral value by calculating the expected choice-worthiness
of each option (Maximize Expected Choice-Worthiness). The third is treating the decision as a
negotiation in a parliament, considering multiple moral views to find a mutually acceptable solution
(Moral Parliament).
Consider whether we should we lie to save a life. Imagine that a notorious murderer asks Alex where
his friend, Jordan, is. Alex knows that revealing Jordan’s location will likely lead to his friend’s
death, while lying would save Jordan’s life. However, lying is morally questionable. Alex must decide
which action to take. He is unsure, and considers the recommendations of the three moral theories he
has some credence in: utilitarianism, deontology, and contractarianism. Alex thinks utilitarianism,
which values lying to save a life highly, is the most likely to be true: he has 60% credence in it.
Deontology, which Alex has 30% credence in, strongly disapproves of lying, even to save a life, and
contractarianism, which Alex has 10% credence in, moderately approves of lying in this situation.
This information is represented in Table 7.3.
Utilitarianism
Deontology
Contractarianism
What is Alex’s estimate of the
chance this theory is true
60%
30%
10%
Does this theory like lying to save
a life?
Yes
No
Yes
Table 7.3: Example: Alex’s credence in various theories and their evaluation of lying to save a life.
Under My Favorite Theory (MFT), Alex would pick whatever utilitarianism recommends.
Alex thinks that utilitarianism is the most likely to be true. The MFT approach is to follow the
prescription of the theory we believe is the closest to a moral truth. Intuitively, many people do
this already when thinking about morality. The advantage of MFT is its simplicity: it is relatively
simple and straightforward to implement. It does not require complex calculations or a detailed
understanding of different moral perspectives. This approach can be useful when the level of moral
uncertainty is low, and it is clear which theory or option is the best choice.
However, following MFT can lead to harmful single-mindedness or overconfidence.
It
can be difficult to put aside personal biases or to recognize when one’s own moral beliefs are fallible
(individuals tend to defend and rationalize their chosen theories). The key issue with MFT is that
it can discard relevant information, such as when the credences in two theories are close, but their
judgments of an action vastly differ [27]. Imagine having 51% credence in contractarianism, which
mildly supports lying to save a life, and 49% credence in Deontology, which views it as profoundly
immoral. MFT suggests following the marginally favored theory, even though the potential harm,
according to the second theory, is much larger. This seems counterintuitive, indicating that MFT
might not always provide the most sensible approach for navigating moral uncertainty.

390
CHAPTER 7. NORMATIVE ETHICS
Maximize Expected Choice-Worthiness (MEC) gives us a procedure to follow.
MEC
tells us that to determine how to act, we need to do the following two things:
1. Determine choice-worthiness. Choice-worthiness is a measure of the overall desirability or
value of an option—in this context, it is how morally good a choice is. This is an expression of
the size of the moral value of an action. We can represent the choice-worthiness of an action as
a number. For instance, we might think that under utilitarianism, the choice-worthiness of
murder could be -1000, of littering could be -2, of helping an old lady cross the street could be
+10, and of averting existential risk could be +10000.
2. Multiply choice-worthiness by credence. We can consider the average choice-worthiness
of an action, weighted by how likely we think each theory is to be true. This gives us a sense of
our best guess of the average moral value of an action, much like how we considered expected
utility when discussing utilitarianism. Table 11 has each theory’s choice-worthiness value for
lying to save a life. As before, utilitarianism highly values lying to save a life (+500), deontology
strongly disapproves of it (-1000), and contractualism moderately approves of it (+100). In the
row beneath these values are the credence probability-weighted judgments for Alex. The total
calculation is
60
100 · 500 + 30
100(−1000) + 10
100 · 10 = 300 −300 + 10 = 10
Under MEC, Alex would choose to lie, because given Alex’s credence in each moral theory and his
determination of how each moral theory judges lying to save a life, lying has a higher expected
choice-worthiness. Alex would lie because he judges that, on average, lying is the best possible action.
Utilitarianism
Deontology
Contractarianism
What is Alex’s estimate of the
chance this theory is true
60%
30%
10%
How much does this theory like
lying to save a life
+500
-1000
+100
What is the probability-weighted
judgment?
+300
-300
+ 10
Table 7.4: Example: Alex’s credence in various theories, their evaluation of lying to save a life, and
their probability-weighted contribution to the final judgment.
MEC gives us a way of balancing both how likely we think each theory is with how
much each theory cares about our actions.
We can see that utilitarianism and deontology’s
relative contributions to the total moral value cancel out, and we are left with an overall “+10” in
favor of lying to save a life. This calculation tells us that—when accounting for how likely Alex
thinks each theory is to be true and how strong the theories’ preferences over his actions are—Alex’s
best guess is that this action is morally good. (Although, since these numbers are rough and the
final margin is quite thin, we would be wary of being overconfident in this conclusion: the numbers
do not necessarily represent anything true or precise.) MEC has distinct advantages. For instance,
unlike MFT, we ensure that we avoid actions that we think are probably fine but might be terrible,
since large negative choice-worthiness from some theories will outweigh small positives from others.
This is a sensible route to take in the face of moral uncertainty.
However, MEC faces challenges when comparing theories.
While some cases are neatly
solved by MEC, its philosophical foundations are questionable. Our assignment of choice-worthiness
can be arbitrary: virtue ethics, for instance, advises acting virtuously without clear guidelines. MEC is

7.5. MORAL UNCERTAINTY
391
unable to deal with ‘ordinal’ theories that only rank actions by their moral value rather than explicitly
judging how morally right or wrong they are, making it difficult to determine choice-worthiness.
Absolutist theories, like extreme Kantian ethics, deem certain actions absolutely wrong. We had
initially assigned -1000 for the value of lying to save a life for a deontologist; it is unclear what we
could put that would capture such an absolutist view. If we considered this to be infinitely bad, which
seems like an accurate representation of the view, then it would overwhelm any other non-absolutist
theory. Even if we think it is simply very large, these firm stances can be more forceful than other
ethical viewpoints, despite ascribing a low probability to the theory’s correctness. This is because
even a small percentage of a large value is still meaningfully large; consider that 0.01% of 1,000,000
is still 100 — a figure that may outweigh other theories we deem more probable.
Following a moral parliament approach, Alex could consider the proposal to lie more
thoroughly and make a considered decision.
In the moral parliament, imagined delegates
representing different moral theories negotiate to find the best solution. In Alex’s moral parliament,
there would be 60 utilitarian delegates, 30 deontological delegates, and 10 contractarian delegates–
—numbers proportional to his credence in each theory. These delegates would negotiate and then
come to a final decision by voting. Drawing inspiration from political systems, the moral parliament
allows an agent to find flexible recommendations that enable compromises among plausible theories.
Depending on the voting rule, moral parliament can lead to different outcomes. In a conventional
setting with majority rule, the utilitarians in Alex’s moral parliament would always be able to push
through their decisions. To avoid such outcomes, philosophers recommend using proportional chances
voting. Here, an action is taken with a probability equal to its vote-share: in this case, if no one
changed their mind, then the outcome of the parliament would recommend with 70% probability that
Alex lie and with 30% probability that he tells the truth, since only the 30 deontological delegates
would vote against this proposal. This encourages negotiation even if there is already a majority,
which naturally leads to more cooperative outcomes. This is a more intuitive approach than, for
instance, assigning choice-worthiness values and multiplying things out, even if it does take more
effort.
However, it is difficult to see what a moral parliament might recommend. We can envision
a variety of different possible outcomes in Alex’s case. We might have the simple outcome described
above, where no one changes their mind. Or, we might think that the deontologists can convince the
contractualists that lying is bad in this case because lying would not be tolerated behind the veil of
ignorance, reducing the chance of lying to 60%. Or, the parliament might even propose something
entirely new, such as a compromise in which Alex does not explicitly lie but simply omits the truth.
All of these are reasonable—it is difficult to choose between them.
The outcome of the moral parliament is not determined externally.
Instead, it is a matter
of our imagination, subject to our biases. Often, individuals resist imagining things that contradict
our preconceived notions. This means that moral parliament may not be very helpful for individuals
thinking about what is right to do in practice. With enough resources, however, we might be able
to simulate model parliaments to recommend such decisions for us, such as by hiring diplomats to
represent moral positions and having them bargain—or by assigning moral views to multiple AI
systems and having them come to a collective decision.
Conclusions about Moral Uncertainty
Taking moral uncertainty into account is difficult but important.
As AI systems become
increasingly embedded across various part of society and take more consequential decisions, it will
become more important to ensure that they can handle moral uncertainty. The same AI systems

392
CHAPTER 7. NORMATIVE ETHICS
may be used by a wide variety of people with different moral theories, who would demand that AI
acts as far as possible in ways that do not violate their moral views. Incorporating moral uncertainty
into AI decision-making could reduce the probability of taking actions that are seriously wrong under
some moral theories.
In this section, we explored ways of moving beyond intuitive judgments or a reasonable pluralism
of theories, examining three solutions to moral uncertainty: my favorite theory, maximize expected
choice-worthiness, and moral parliament. Each approach has its strengths and limitations, with the
choice depending on the situation, level of uncertainty, and personal preferences.
It is important to recognize that there might not be a one-size-fits-all solution to ethical dilemmas.
As AI technology progresses, we should establish methods for addressing moral uncertainty, such as
including diverse moral perspectives and quantifying uncertainty.
7.6
Conclusion
Ethics is the study of moral principles and how they guide our decisions and actions. It encompasses
questions of right and wrong, and it provides a framework for making choices based on our values
and beliefs. It is important for AI researchers to have a basic understanding of ethics in order to
ethically guide the development and governance of AI systems.
We examined a number of considerations that commonly enter into moral decision making. Intrinsic
goods, like wellbeing, are valuable for their own sake. Consequentialist moral theories consider the
maximization of intrinsic goods to be our principle moral responsibility. Utilitarianism, a form of
consequentialism, is the view that we should maximize wellbeing.
Constraints also play a role in moral decision making. Constraints are the basis of many deontological
theories. Kant’s ethics categorically constrains actions and is derived from the respect for humanity’s
rational autonomy.
Virtue ethics focuses on the importance of developing certain character traits rather than solely
considering consequences or specific rules. Aristotelian virtue ethics links virtue with flourishing;
living a good life is intertwined with possessing virtuous traits.
Rawls’ ethics, prioritarianism, and contractualism are all forms of social contract theory. Social
contract theory suggests that morality can be derived from hypothetical agreements between members
of society, made for everyone’s mutual benefit. Rawls proposes that decisions about a social contract
should be made behind a veil of ignorance, where individuals are unaware of their personal attributes.
From this position, principles such as protecting the worst-off, ensuring basic liberties for all, and
limiting inequality can potentially be argued.
As discussed in the chapter on Machine Ethics, we need to ensure that AIs behave ethically towards
others. As AIs have increasing influence over society and act more autonomously, it will become
important that they are able to detect situations where the moral principles apply, assess how to
apply the moral principles, evaluate the moral worth of candidate actions, select and carry out
actions appropriate for the context, monitor the success or failure of the actions, and adjust responses
accordingly. Ideally, AIs would be designed to take into account many of the ethical concepts
introduced in this chapter. Examples of desirable abilities from this perspective might include: -
representing various purported intrinsic goods, including pleasure, autonomy, the exercise of reason,
knowledge, friendship, love, and so on - distinguishing between subtly different levels of these goods,
and ensuring that the AI’s value functions are not vulnerable to optimizers - representing more than
just intrinsic goods, for example legal systems and normative factors including special obligations

7.7. LITERATURE
393
and deontological constraints
Because there is no consensus about which moral theory (if any) is correct, we must
accommodate some degree of moral uncertainty.
To navigate moral uncertainty, we might
consider multiple perspectives when making moral decisions. In the context of AI development,
moral uncertainty becomes especially significant due to the wide-ranging societal implications of
AI systems. Designers and developers of AI technologies must carefully consider the ethical and
moral implications of their actions and ensure that ethical considerations are integrated into the
development process.
Being aware of moral uncertainty can help AI developers avoid negative outcomes.
Uncertainty highlights complexity of moral judgments and the potential for unexpected consequences.
To address moral uncertainty effectively, AI systems should be capable of recognizing and
acknowledging a broad range of moral perspectives. This presents challenges in determining how
to rationally balance the recommendations of different ethical theories and ensure alignment with
human values.
There is no known solution to all ethical dilemmas, and the choice of approach may depend on
context, the level of uncertainty, and personal preferences. It is crucial to incorporate diverse moral
perspectives, quantify uncertainty, and employ strategies like maximizing expected choice-worthiness
and moral parliament. By doing so, we can work to ensure AI systems act in accordance with our
values and mitigate unintended harm.
7.7
Literature
7.7.1
Recommended Reading
• Steven Lukes. Moral Relativism. Picador, 2008
• Katarzyna de Lazari-Radek and Peter Singer. Utilitarianism: A Very Short Introduction.
Oxford University Press, 2017. isbn: 9780198728795. doi: 10.1093/actrade/9780198728795.
001.0001. url: https://doi.org/10.1093/actrade/9780198728795.001.0001
• Shelly Kagan. Normative Ethics. Routledge, London, 2018
• Toby Newberry and Toby Ord. The Parliamentary Approach to Moral Uncertainty. Tech. rep.
Technical Report# 2021-2, Future of Humanity Institute, University of Oxford . . ., 2021
• Stephen Darwall. Deontology. Blackwell Publishers, Oxford, 2003
• Aristotle. “Nicomachean Ethics”
• Stanford Encyclopedia of Philosophy (SEP) on virtue ethics https://plato.stanford.edu/
entries/ethics-virtue/
• T. M. Scanlon. What We Owe to Each Other. Belknap Press, 2000
7.7.2
References
[1]
Chris Gowans. “Moral Relativism”. In: The Stanford Encyclopedia of Philosophy. Ed. by Edward N.
Zalta. Spring 2021. Metaphysics Research Lab, Stanford University, 2021.

394
CHAPTER 7. NORMATIVE ETHICS
[2]
Oliver Scott Curry, Daniel Austin Mullins, and Harvey Whitehouse. “Is It Good to Cooperate?:
Testing the Theory of Morality-as-Cooperation in 60 Societies”. In: Current Anthropology 60.1
(2019), pp. 47–69. doi: 10.1086/701478. eprint: https://doi.org/10.1086/701478. url: https:
//doi.org/10.1086/701478.
[3]
C. Plato. Euthyphro. Kessinger Publishing, 2014.
[4]
Jeff Sebo. Op-Ed: What should we do if a chatbot has thoughts and feelings? 2022. url: https:
//www.latimes.com/opinion/story/2022-06-16/artificial-intelligence-morals-ethics-
sentience-thinking.
[5]
Andrew Fisher. Metaethics: An Introduction. Routledge, 2011.
[6]
John Stuart Mill and Jeremy Bentham. Utilitarianism and Other Essays. Penguin Books, 1987.
[7]
Katarzyna de Laari-Radek and Peter Singer. Utilitarianism: A Very Short Introduction. Oxford
University Press, 2017.
[8]
Jeremy Bentham and Louis Crompton. “Offences Against One’s Self:” in: Journal of Homosexuality 3.4
(1978), pp. 389–406. doi: 10.1300/J082v03n04\_07. eprint: https://doi.org/10.1300/J082v03n04_
07. url: https://doi.org/10.1300/J082v03n04_07.
[9]
Peter Singer. “Famine, affluence, and morality”. English (US). In: Applied Ethics. United States:
Taylor and Francis, July 2017, pp. 132–142. isbn: 9781138936928. doi: 10.4324/9781315097176.
[10]
Peter Singer. The Expanding Circle: Ethics and Sociobiology. Farrar, Straus and Giroux, 1981.
[11]
Samuel Scheffler. The Rejection of Consequentialism: A Philosophical Investigation of the
Considerations Underlying Rival Moral Conceptions. Oxford University Press, 1994. isbn:
9780198235118. doi: 10.1093/0198235119.001.0001. url: https://doi.org/10.1093/0198235119.
001.0001.
[12]
R. Eugene Bales. “Act-Utilitarianism: Account of Right-Making Characteristics or Decision-Making
Procedure?” In: American Philosophical Quarterly 8.3 (1971), pp. 257–265. issn: 00030481. url:
http://www.jstor.org/stable/20009403.
[13]
Robert Nozick. Anarchy State and Utopia. John Wiley & Sons, 1974.
[14]
Joshua Greene. Moral Tribes: Emotion, Reason, and the Gap Between Us and Them. New York:
Penguin Press, 2013.
[15]
Stephen L. Darwall, ed. Deontology. Malden, MA: Wiley-Blackwell, 2003.
[16]
T. Aquinas, T.F.E.D. Province, and C.W. Publishing. The Summa Theologica: Complete Edition.
Catholic Way Publishing, 2014. isbn: 9781783793143. url: https://books.google.com.au/books?
id=Ee0HBAAAQBAJ.
[17]
Philippa Foot. Virtues and vices and other essays in moral philosophy. Berkeley: University of
California Press, 1978.
[18]
Michael Moore. “The Rationality of Threshold Deontology”. In: Moral Puzzles and Legal Perplexities:
Essays on the Influence of Larry Alexander. Cambridge University Press, 2019, pp. 371–387.
[19]
Immanuel Kant. Groundwork for the Metaphysics of Morals. Ed. by Thomas E. Hill and Arnulf Zweig.
New York: Oxford University Press, 1998.
[20]
Rosalind Hursthouse and Glen Pettigrove. “Virtue Ethics”. In: The Stanford Encyclopedia of Philosophy.
Ed. by Edward N. Zalta and Uri Nodelman. Fall 2023. Metaphysics Research Lab, Stanford University,
2023.
[21]
Roger Crisp, ed. Aristotle: Nicomachean Ethics. Cambridge University Press, 2014.
[22]
John Rawls. “Applied Ethics: A Multicultural Approach”. In: Routledge, 2017. Chap. A Theory of
Justice.
[23]
John C. Harsanyi. “Cardinal Utility in Welfare Economics and in the Theory of Risk-taking”. In:
Journal of Political Economy 61.5 (1953), pp. 434–435. doi: 10.1086/257416. eprint: https://doi.
org/10.1086/257416. url: https://doi.org/10.1086/257416.

7.7. LITERATURE
395
[24]
Derek Parfit. “Equality and Priority”. In: Ratio 10.3 (1997), pp. 202–221. doi: https://doi.org/
10.1111/1467-9329.00041. eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-
9329.00041. url: https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9329.00041.
[25]
T. M. Scanlon. What We Owe to Each Other. Harvard University Press, 1998. isbn: 9780674950894.
(Visited on 10/13/2023).
[26]
William MacAskill, Krister Bykvist, and Toby Ord. Moral Uncertainty. Sept. 2020. isbn:
9780198722274. doi: 10.1093/oso/9780198722274.001.0001.
[27]
Harry R. Lloyd. The Property Rights Approach to Moral Uncertainty. Happier Lives Institute’s
2022 Summer Research Fellowship. 2022. url: https://www.happierlivesinstitute.org/report/
property-rights/.
[28]
Steven Lukes. Moral Relativism. Picador, 2008.
[29]
Katarzyna de Lazari-Radek and Peter Singer. Utilitarianism: A Very Short Introduction. Oxford
University Press, 2017. isbn: 9780198728795. doi: 10.1093/actrade/9780198728795.001.0001. url:
https://doi.org/10.1093/actrade/9780198728795.001.0001.
[30]
Shelly Kagan. Normative Ethics. Routledge, London, 2018.
[31]
Toby Newberry and Toby Ord. The Parliamentary Approach to Moral Uncertainty. Tech. rep. Technical
Report# 2021-2, Future of Humanity Institute, University of Oxford . . ., 2021.
[32]
Stephen Darwall. Deontology. Blackwell Publishers, Oxford, 2003.
[33]
T. M. Scanlon. What We Owe to Each Other. Belknap Press, 2000.

Part IV
Society
396


Collective Action Problems
8.1
Motivation
Introduction
In the chapters “Complex Systems” and “Safety Engineering,” we considered AI risks that arise
not only from the technologies themselves but from the broader social contexts in which they are
embedded. In this chapter, we extend our exploration of these systemic risks by exploring how the
collective behavior of a multi-agent system may not reflect the interests of the agents that comprise it.
The agents may produce conditions that none of them wants, even when every one of them has the
same goals and priorities. In the words of economics Nobel laureate Thomas Schelling, “Micromotives
do not equal macrobehavior” [1]. Let us explore this idea using some examples.
Example: traffic jams.
Consider a traffic jam, where the only obstacle to each motorist is the car
in front. Everyone has the same goal, which is to reach their destination quickly. Since nobody wants
to be stuck waiting, the solution might appear obvious to someone unfamiliar with traffic: everyone
should simply drive forward, starting at the same time and accelerating at the same rate. And yet,
without external synchronization, achieving this preferable state is impossible. All anyone can do is
start and stop in response to each others’ starting and stopping, inching towards their destination
slowly and haltingly.
Example: tall forests. [2]
In the Rockefeller forest of Northern California, the trees are more
than 350 feet tall, on average. We can model these trees as agents competing for sunlight access. The
taller a tree is, the more sunlight it can access, as its leaves are above its neighbors’. However, there
is no benefit to being tall other than avoiding being overshadowed by other trees. In fact, growing so
tall costs each tree valuable resources and risks their structural integrity failing. If all the trees were
200 feet shorter, each tree would occupy the same position in the competition as before and each
would get the same amount of sunlight as before, but with greatly reduced growing costs. All the
trees would profit from this arrangement. However, as there is no way to impose such an agreement
between the trees, each races its neighbor ever higher, and all pay the large costs of growing so tall.
Example: excessive working hours [3].
People often work far longer hours than they might
ideally like to, rather than taking time off for their other interests, in order to be competitive in
their field. For instance, they might be competing for limited prestigious positions within their field.
In theory, if everyone in a given field were to reduce their work hours by the same amount, they
could all free up time and increase their quality of life while maintaining their relative position in the
competition. Each person would get the work outcome they would have otherwise, and everyone
would benefit from this freed-up time. Yet no one does this, because if they alone were to decrease
their work efforts, they would be out-competed by others who did not.
Example: military arms races.
Like tree height, the major benefit of military power is not
intrinsic, but relative: being less militarily capable than their neighbors makes a nation vulnerable to
invasion. This competitive pressure drives nations to expend vast sums of money on their military
budgets each year, reducing each nation’s budgets for other areas, such as healthcare and education.
Some forms of military investment, such as nuclear weaponry and military AI applications, also
exacerbate the risks of large-scale catastrophes. If every nation were to decrease its military investment
by the same amount, everyone would benefit from the reduced expenses and risks without anyone
losing their relative power. However, this arrangement is not stable, since each nation could improve
its security by ensuring its military power exceeds that of its competitors, and each risks becoming
398

8.1. MOTIVATION
399
vulnerable if it alone fails to do this. Military expenditure therefore remains high in spite of these
seemingly avoidable costs.
Competitive and evolutionary pressures.
The same basic structure underlies most of these
examples [4]. A group of agents is engaged in a competition over a valuable and limited item (sunlight
access, housing quality, military security). One way an agent can gain more of this valuable item is
by sacrificing some of their other values (energy for growth, social life, an education budget). Agents
who do not make these sacrifices are outcompeted by those who do. Natural selection weeds out
those who do not sacrifice their other values sufficiently, replacing them with agents who sacrifice
more, until the competition is dominated by those agents who sacrificed the most. These agents gain
no more of the valued item they are competing for than did the original group, yet are worse off for
the losses of their other values.
Steering each agent ̸= steering the system.
These phenomena hint at the distinct challenges
of ensuring safety in multi-agent systems. The danger posed by a collective of agents is greater than
the sum of its parts. AI risk cannot be eradicated by merely ensuring that each individual AI agent
is loyal and each individual human operator is well-intentioned. Even if all agents, both human and
AI, share a common set of goals, this does not guarantee macrobehavior in line with these goals. The
agents’ interactions can produce undesirable outcomes.
Chapter focus.
In this chapter, we use abstract models to understand how intelligent agents can,
despite acting rationally and in accordance with their own self-interest, collectively produce outcomes
that none of them wants, even when they could seemingly have achieved preferable alternative
outcomes. We can characterize these risks by crudely differentiating them into the following two sets:
• Multi-human dynamics. These risks are generated by interactions between the human
agencies involved in AI development and adoption, particularly corporations and nations. The
central concern here is that competitive and evolutionary pressures could drive humanity to
hand over increasing amounts of power to AIs, thereby becoming a “second-class species.” The
frameworks we explore in this chapter are highly abstract and can be useful in thinking more
generally about the current AI landscape.
Of particular importance are racing dynamics. We see these in the corporate world, where AI
developers are cutting corners on safety in order to avoid being outcompeted by one another. We
also see these in international relations, where nations are racing each other to adopt hazardous
military AI applications. By observing AI races, we can anticipate that merely persuading
these parties that their actions are high-risk may not be sufficient for ensuring that they act
more cautiously, because they may be willing to tolerate high risk levels in order to “stay in the
race.” For example, nations may chose to continue investing in military AI technologies that
could fail in catastrophic ways, if abstaining from doing so risks losing international conflict.
• Multi-AI dynamics. These risks are generated by interactions with and between AI agents.
In the future, we expect that AIs will increasingly be granted autonomy in their behavior,
and will therefore interact with others under progressively less human oversight. This poses
risks in at least three ways. First, evolutionary pressures may promote selfish behavior and
generate various forms of intrasystem conflict that could subvert our goals. Second, many of
the mechanisms by which AI agents may cooperate with one another could promote undesirable
behaviors, such as nepotism, outgroup hostility, and the development of ruthless reputations.
Third, AIs may engage in conflict, using threats of extreme scale in order to extort others, or
even promoting all-out warfare, with devastating consequences.

400
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
We explore both of the above sets of multi-agent risks using generalizable frameworks from game theory,
evolutionary theory, and bargaining theory. These frameworks help us understand the collective
dynamics that can lead to outcomes that were not intended or desired by anyone individually. Even
if AI systems are fully under human control and leading actors such as corporations and states are
well-intentioned, humanity could still end up eroding away our power gradually until it cannot be
recovered.
Game Theory
Central theme: Rational agents will not necessarily secure good outcomes.
Behavior
that is individually rational and self-interested can produce collective outcomes that are suboptimal,
or even catastrophic, for all involved. This section first examines the Prisoner’s Dilemma, a canonical
game theoretic example that illustrates this theme—though cooperation would produce an outcome
that is better for both agents, for either one to cooperate would be irrational.
We then build on this by introducing three additional levels of sophistication. The first addition is
time. We explore how cooperation is possible, though not assured, when agents interact repeatedly
over time. The second addition is the introduction of more than two agents. We explore how collective
action problems can generate and maintain undesirable states. Third, we explore evolutionary game
theory, studying the dynamics produced by agents that can adapt over time.
Ultimately, we see how these natural dynamics can produce catastrophically bad outcomes. They
perpetuate military arms races and corporate AI races, increasing the risks from both. They may
also foster immoral AI behaviors, such as deception and even extortion.
Evolutionary Pressure
Central theme: Natural selection will promote AIs that behave selfishly.
In this section,
we first explore generalized Darwinism, which is the idea that evolution by natural selection can
take place outside of the realm of biology. We explore examples in linguistics, music, philosophy and
sociology. We formalize generalized Darwinism using Lewontin’s conditions for evolution by natural
selection and the Price equation for evolutionary change. Using both, we show that AIs are likely to
be subject to evolution by natural selection: they will vary in ways that produce differential fitness
and so influence which traits persist through time and between “generations” of AIs.
Next, we explore two AI risks generated by evolutionary pressures. The first is that correctly-specified
goals may be subverted or distorted by “intrasystem goal conflict.” The interests of propagating
information (such as genes, departments, or sub-agents) can sometimes clash with those of the
larger entity that contains it (such as an organism, government, or AI system), undermining unity
of purpose. The second risk we consider is that natural selection tends to favor selfish traits over
altruistic ones. A future shaped by evolutionary pressures is, therefore, likely to be dominated by
selfish behavior, both in the institutions that produce and use AI systems, and in the AIs themselves.
The conclusions of this section are simple. Natural selection will by default be a strong force in
determining the state of the world. Its influence on AI development carries the risk of intrasystem goal
conflict and the promotion of selfish behavior. Both risks could have catastrophic effects. Intrasystem
goal conflict could prevent our goals from being carried out and generate unexpected actions. AI
agents could develop selfish tendencies, increasing the risk that they might employ harmful strategies
(including those covered earlier in the chapter, such as deception or extortion).

8.2. GAME THEORY
401
Conflict and Cooperation
Central theme: Ensuring cooperation, rather than conflict, is vital for AI safety.
This
section explores the nature of conflict and cooperation between agents. We start by looking at the
nature of conflict, beginning with an overview of bargaining theory. Then we explore 5 factors that
drive conflict.
1. Power shifts: a shift in political power triggers preventative conflict.
2. First-strike advantage: time-sensitive offensive advantages motivate a party to initiate conflict
preemptively.
3. Information problems: mis- and dis-information kindle defensive or offensive action over
cooperation.
4. Issue indivisibility: wherever the entity over which parties are contesting is indivisible, it is
harder to avoid resorting to conflict.
5. Inequality: inequality may increase the probability of conflict, due to factors such as relative
deprivation and social envy.
Next, we turn to assessing how cooperation can help with addressing the challenges outlined above
and what problems it may pose itself. We explore seven mechanisms that can promote or maintain
cooperation:
• Direct reciprocity: the chance of a future meeting incentivizes cooperative behavior in the
present.
• Indirect reciprocity: cooperative reputations are rewarded.
• Group selection: inter-group competition promotes intra-group unity.
• Kin selection: indirect benefits of cooperation outweigh direct costs, motivating altruism
towards genetic kin.
• Individual stakes to common stakes: the alignment of interests between group members and
the group as a whole.
• Simon’s selection: limited knowledge necessitates reliance on others.
• Institutions: large-scale external forces motivate cooperation through enforcement.
8.2
Game Theory
8.2.1
Overview
This chapter explores the dynamics that may arise when AI and human agents interact. These
interactions create risks distinct from those generated by any individual AI agent acting in isolation.
One way we can study the strategic interdependence of agents is with the framework of game theory.
Using game theory, we can examine formal models of how agents interact with each other under
varying conditions and predict the outcomes of these interactions.

402
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Here, we use game theory to present natural dynamics in biological and social systems that involve
multiple agents. In particular, we explore what might cause agents to come into conflict with one
another, rather than cooperate. We show how these multi-agent dynamics can generate undesirable
outcomes, sometimes for all the agents involved. We consider risks created by interactions within and
between human and AI agents, from human-directed companies and militaries engaging in perilous
races to autonomous AIs using threats for extortion. These risks can be reduced if mechanisms such
as institutions are used ensure human agencies and AI agents are able to cooperate with one another
and avoid conflict. We will be exploring means of overcoming commitment and information problems
in the Conflict and Cooperation section.
We start with an overview of the fundamentals of game theory. We begin this section by setting out
the characteristics of game theoretic agents. We also categorize the different kinds of games we are
exploring.
We then focus on the Prisoner’s Dilemma. The Prisoner’s Dilemma is a simple example of how an
interaction between two agents can generate an equilibrium state that is bad for both, even when each
acts rationally and in their own self-interest. We explore how agents may arrive at the outcome where
neither chooses to cooperate. We use this to model real-world phenomena, such as negative political
campaigns. Finally, we examine ways we might foster rational cooperation between self-interested
AI agents, such as by altering the values in the underlying payoff matrices. The key upshot is that
intelligent and rational agents do not always achieve good outcomes.
We next add in the element of time by examining the Iterated Prisoner’s Dilemma. AI agents are
unlikely to interact with others only once. When agents engage with each other multiple times, this
creates its own hazards. We begin by examining how iterating the Prisoner’s Dilemma alters the
agents’ incentives—when an agent’s behavior in the present can influence that of their partner in the
future, this creates an opportunity for rational cooperation. We study the effects of altering some of
the variables in this basic model: uncertainty about future engagement and the necessity to switch
between multiple different partners. We look at why the cooperative strategy tit-for-tat is usually so
successful, and in what circumstances it is less so. Finally, we explore iterated multi-agent social
dynamics amongst humans, such as corporate AI races and military AI arms races. The key upshot
is that cooperation cannot be ensured merely by iterating interactions through time.
We next move to consider group-level interactions. AI agents might not interact with others in a neat,
pairwise fashion, as assumed by the models previously explored. In the real world, social behavior is
rarely so straightforward. Interactions can take place between more than two agents at the same
time. A group of agents creates an environmental structure that may alter the incentives directing
individual behavior. Human societies are rife with dynamics generated by group-level interactions
that result in undesirable outcomes. We begin by formalizing “collective action problems.” We
consider real-world examples such as anthropogenic climate change and fishery depletion. Multi-agent
dynamics such as these generate AI risk in several ways. Races between human agents and agencies
could trigger flash wars between AI agents or the automation of economies to the point of human
enfeeblement. The key upshot is that achieving cooperation and ensuring collectively good outcomes
is even more difficult in interactions involving more than two agents.
Finally, we explore how AI agents may adapt their behavior to their environments. We use evolutionary
game theory to understand how social environments might shape multi-agent dynamics involving
humans and AIs. We start by considering how evolutionary forces alter the dynamics in the Iterated
Prisoner’s Dilemma: when agents can adapt their strategies, the results are more complicated. As
we explore in a later section, they include the emergence of “cyclical dynamics” and some undesirable
strategies such as extortion. We use this to examine the risk of AI extortion in the real world. We

8.2. GAME THEORY
403
also consider the “Hawk-Dove game” as an example of how agents may reach an equilibrium state
in which they employ a variety of different strategies, even when they themselves are identical to
one another. We examine how some strategies might be selected over others in AI populations. In
particular, evolutionary pressures could promote dangerous behaviors in AIs, such as concealing
information. The key upshot is that evolutionary dynamics might drive AIs to employ harmful social
strategies, such as deception, rather than cooperating.
8.2.2
Game Theory Fundamentals
In this section, we briefly run through some of the fundamental principles in game theory. Game
theory is the branch of mathematics concerned with agents’ choices and strategies in multi-agent
interactions. Game theory is so-called because we reduce complex situations to abstract games where
agents maximize their payoffs. Using game theory, we can study how altering incentives influences
the strategies that these agents use.
Agents in game theory.
We usually assume that the agents in these games are self-interested and
rational. Agents are “self-interested” if they make decisions in view of their own utility, regardless of
the consequences to others. Agents are said to be “rational” if they act as though they are maximizing
their utility.
Games can be “zero sum” or “non-zero sum.”
We can categorize the games we are studying
in different ways. One distinction (made in Figure 1) is between zero sum and non-zero sum games.
A zero sum game is one where, in every outcome, the agents’ payoffs all sum to zero. An example
is “tug of war”: any benefit to one party from their pull is necessarily a cost to the other. Therefore,
the total value of these wins and losses cancel out. In other words, there is never any net change in
total value. Poker is a zero sum game if the players’ payoffs are the money they each finish with.
The total amount of money at a poker game’s beginning and end is the same — it has simply been
redistributed between the players.
By contrast, many games are non-zero sum. In non-zero sum games, the total amount of value is not
fixed and may be changed by playing the game. Thus, one agent’s win does not necessarily require
another’s loss. For instance, in cooperation games such as those where players must meet at an
undetermined location, players only get the payoff together if they manage to find each other. As
we shall see, the Prisoner’s dilemma is a non-zero sum game, as the sum of payoffs change across
different outcomes.
Non-zero sum games can have “positive sum” or “negative sum” outcomes.
We can
categorize the outcomes of non-zero sum games as positive sum and negative sum. In a positive sum
outcome, the total gains and losses of the agents sum to greater than zero. Positive sum outcomes
can arise when particular interactions result in an increase in value. This includes instances of
mutually-beneficial cooperation. For example, if one agent has flour and another has water and heat,
the two together can cooperate to make bread, which is more valuable than the raw materials. As a
real-world example, many view the stock market as positive sum because the overall value of the
stock market tends to increase over time. Though gains are unevenly distributed, and some investors
lose money, the average investor becomes richer. This demonstrates an important point: positive sum
outcomes are not necessarily “win-win.” Cooperating does not guarantee a benefit to all involved.
Even if extra total value is created, its distribution between the agents involved in its creation can
take any shape, including one where some agents have negative payoffs.

404
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
In a negative sum outcome, some amount of value is lost by playing the game. Many competitive
interactions in the real world are negative sum. For instance, consider “oil wars”—wars fought over a
valuable hydrocarbon resource. Oil wars are zero-sum with regards to oil since only the distribution
(not the amount) of oil changes. However, the process of conflict itself incurs costs to both sides, such
as loss of life and infrastructure damage. This reduces the total amount of value. If AI development
has the potential to result in catastrophic outcomes for humanity, then accelerating development to
gain short-term profits in exchange for long-term losses to everyone involved would be a negative
sum outcome.
8.2.3
The Prisoner’s Dilemma
Our aim in this section is to investigate how interactions between rational agents, both human and
AI, may negatively impact everyone involved. To this end, we focus on a simple game: the Prisoner’s
Dilemma. We first explore how the game works, and its different possible outcomes. We then examine
why agents may choose not to cooperate even if they know this will lead to a collectively suboptimal
outcome. We run through several real-world phenomena which we can model using the Prisoner’s
Dilemma, before exploring ways in which cooperation can be promoted in these kinds of interactions.
We end by briefly discussing the risk of AI agents tending towards undesirable equilibrium states.
The Game Fundamentals
In the Prisoner’s Dilemma, two agents must each decide whether or not to cooperate. The costs and
benefits are structured such that for each agent, defection is the best strategy regardless of what
their partner chooses to do. This motivates both agents to defect.
The Prisoner’s Dilemma.
In game theory, the Prisoner’s Dilemma is a classic example of the
decisions of rational agents leading to suboptimal outcomes. The basic setup is as follows. The police
have arrested two would-be thieves. We will call them Alice and Bob. The suspects were caught
breaking into a house. The police are now detaining them in separate holding cells, so they cannot
communicate with each other. The police suspect that the pair were planning burglary (which carries
a lengthy jail sentence). But they only have enough evidence to charge them with trespassing (which
carries a shorter jail sentence). However, the testimony of either one of the suspects would be enough
to charge the other with burglary, so the police offer each suspect the following deal. If only one of
them rats out their partner by confessing that they had intended to commit burglary, the confessor
will be released with no jail time and their partner will spend eight years in jail. However, if they
each attempt to rat out the other by both confessing, they will both serve a medium prison sentence
of three years. If neither suspect confesses, they will both serve a short jail sentence of only one year.
The four possible outcomes.
We assume that Alice and Bob are both rational and self-interested:
each only cares about minimizing their own jail time. We define the decision facing each as follows.
They can either “cooperate” with their partner by remaining silent or “defect” on their partner by
confessing to burglary. Each suspect faces four possible outcomes, which we can split into two possible
scenarios. Let’s term these “World 1” and “World 2”; see Figure 2. In World 1, their partner chooses
to cooperate with them; in World 2, their partner chooses to defect. In both scenarios, the suspect
decides whether to cooperate or defect themself. They do not know what their partner will decide to
do.
A) The four possible outcomes facing Alice, divided into “World 1” and “World 2.” In both scenarios,
Alice must choose whether to cooperate with her partner (Bob), or defect. In World 1, her partner
chooses to cooperate; in World 2, her partner chooses to defect. The jail time for each outcome is
shown on the right. B) The four possible outcomes are shown in a decision tree. In World 1, Alice

8.2. GAME THEORY
405
Figure 8.1: The possible outcomes for Alice in the Prisoner’s Dilemma.

406
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
gets a shorter jail sentence by defecting, than by cooperating. The same is true in World 2, by an
even greater amount. In both cases, choosing to defect gets Alice the better outcome.
Defection is the dominant strategy.
Alice does not know whether Bob will choose to cooperate
or defect. She does not know whether she will find herself in World 1 or World 2; see Figure 2. She
can only decide whether to cooperate or defect herself. This means she is making one of two possible
decisions. If she defects, she is. . .
. . . in World 1: Bob cooperates and she goes free instead of spending a year in jail.
. . . in World 2: Bob defects and she gets a 3-year sentence instead of an 8-year one.
Alice only cares about minimizing her own jail time, so she can save herself jail time in either scenario
by choosing to defect. She saves herself one year if her partner cooperates or five years if her partner
defects. A rational agent under these circumstances will do best if they decide to defect, regardless
of what they expect their partner to do. We call this the dominant strategy: a rational agent playing
the Prisoner’s Dilemma should choose to defect no matter what their partner does.
One way to think about strategic dominance is through the following thought experiment. Someone
in the Arctic during winter is choosing what to wear for that day’s excursion. They have only two
options: a coat or a t-shirt. The coat is thick and waterproof; the t-shirt is thin and absorbent.
Though this person cannot control or predict the weather, they know there are only two possibilities:
either rain or cold. If it rains, the coat will keep them drier than the t-shirt. If it is cold, the coat
will keep them warmer than the t-shirt. Either way, the coat is the better option, so “wearing the
coat” is their dominant strategy.
Defection is the dominant strategy for both agents.
Importantly, both the suspects face this
decision in a symmetric fashion. Each is deciding between identical outcomes, and each wishes to
minimize their own jail time. Let’s consider the four possible outcomes now in terms of both the
suspects’ jail sentences; see Figure 3A. We can display this information in a payoff matrix, as shown
in Table 8.1. Payoff matrices are commonly used to visualize games. They show all the possible
outcomes of a game in terms of the value of that outcome for each of the agents involved. In the
Prisoner’s Dilemma, we show the decision outcomes as the payoffs to each suspect: note that since
more jail time is worse than less, these payoffs are negative. Each cell of the matrix shows the
outcome of the two suspects’ decisions as the payoff to each suspect.
Bob cooperates
Bob defects
Alice cooperates
-1, -1
-8, 0
Alice defects
0, -8
-3, -3
Table 8.1: Each cell in this payoff matrix represents a payoff. If Alice cooperates and Bob defects,
the top right cell tells us that Alice gets 8 years in jail while Bob goes free.
Nash Equilibria and Pareto Efficiency
The stable equilibrium state in the Prisoner’s Dilemma is for both agents to defect. Neither agent
would choose to go back in time and change their decision (to switch to cooperating) if they could
not also alter their partner’s behavior by doing so. This is often considered counterintuitive, as the
agents would benefit if they were both to switch to cooperating.

8.2. GAME THEORY
407
Nash Equilibrium: both agents will choose to defect.
Defection is the best strategy for Alice,
regardless of what Bob opts to do. The same is true for Bob. Therefore, if both are behaving in a
rational and self-interested fashion, they will both defect. This will secure the outcome of 3 years of
jail time each (the bottom-right outcome of the payoff matrix above). Neither would wish to change
their decision, even if their partner were to change theirs. This is known as the Nash equilibrium:
the strategy choices from which no agent can benefit by unilaterally choosing a different strategy.
When interacting with one another, rational agents will tend towards picking strategies that are part
of Nash equilibria.
Pareto improvement: both agents would do better if they cooperated.
As we can see in
the payoff matrix, there is a possible outcome that is better for both suspects. If both choose the
cooperate strategy, they will secure the top-left outcome of the payoff matrix. Each would serve 2
years less jail time at no cost to the other. Yet, as we have seen, selecting this strategy is irrational;
the defect strategy is dominant and so Alice and Bob each want to defect instead. We call this
outcome Pareto inefficient, meaning that it could be altered to make some of those involved better off
without making anyone else worse off. In the Prisoner’s Dilemma, the both defect outcome is Pareto
inefficient because it is suboptimal for both Alice and Bob, who would both be better off if they both
cooperated instead. Where there is an outcome that is better for some or all agents involved, and
not worse for any, we call the switch to this more efficient outcome a Pareto improvement. In the
Prisoner’s Dilemma, the both cooperate outcome is better for both agents than the Nash equilibrium
of both defect; see Figure 8.2.3B. The only Pareto improvement possible in this game is the move
from the both defect to the both cooperate outcome; see Figure 8.3.
Figure 8.2: Looking at the possible outcomes for both suspects in the Prisoner’s Dilemma, we can
see that there is a possible Pareto improvement from the Nash equilibrium. The numbers represent
their payoffs (rather than the length of their jail sentence).

408
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Figure 8.3: Both suspects’ payoffs, in each of the four decision outcomes. Moving right increases
Alice’s payoff, and moving up improves Bob’s payoff. A Pareto improvement requires moving right
and up, as shown by the green arrow. [5]
Real-World Examples of the Prisoner’s Dilemma
The Prisoner’s Dilemma has many simplifying assumptions. Nevertheless, it can be a helpful lens
through which to understand social dynamics in the real world. Rational and self-interested parties
often produce states that are Pareto inefficient. There exist alternative states that would be better
for all involved, but reaching these requires individually irrational action. To illustrate this, let’s
explore some real-world examples.
Mud-slinging.
Consider the practice of mud-slinging.
Competing political parties often use
negative campaign tactics, producing significant reputational costs. By running negative ads to attack
and undermine the public image of their opponents, all parties end up with tarnished reputations.
If we assume that politicians value their reputation in an absolute sense, not merely in relation to
their contemporary competitors, then mud-slinging is undesirable for all. A Pareto improvement to
this situation would be switching to the outcome where they all cooperate. With no one engaging
in mud-slinging, all the parties would have better reputations. The reason this does not happen is
that mud-slinging is the dominant strategy. If a party’s opponent doesn’t use negative ads, the party
will boost their reputation relative to their opponent’s by using them. If their opponent does use
negative ads, the party will reduce the difference between their reputations by using them too. Thus,
both parties converge on the Nash equilibrium of mutual mud-slinging, at avoidable detriment to all.
Shopkeeper price cuts.
Another example is price racing dynamics between different goods
providers.
Consider two rival shopkeepers selling similar produce at similar prices.
They are
competing for local customers. Each shopkeeper calculates that lowering their prices below that of
their rival will attract more customers away from the other shop and result in a higher total profit

8.2. GAME THEORY
409
for themselves. If their competitor drops their prices and they do not, then the competitor will
gain extra customers, leaving the first shopkeeper with almost none. Thus, “dropping prices” is the
dominant strategy for both. This leads to a Nash equilibrium in which both shops have low prices,
but the local custom is divided much the same as it would be if they had both kept their prices high.
If they were both to raise their prices, they would both benefit by increasing their profits: this would
be a Pareto improvement. Note that, just as how the interests of the police do not count in the
Prisoner’s Dilemma, we are only considering the interests of the shopkeepers in this example. We are
ignoring the interests of the customers and wider society.
Arms races.
Nations’ expenditure on military arms development is another example. It would
be better for all these nations’ governments if they were all simultaneously to reduce their military
budgets. No nation would become more vulnerable if they were all to do this, and each could then
redirect these resources to areas such as education and healthcare. Instead, we have widespread
military arms races. We might prefer for all the nations to turn some military spending to their other
budgets, but for any one nation to do so would be irrational. Here, the dominant strategy for each
nation is to opt for high military expenditure. So we achieve a Nash equilibrium in which all nations
must decrease spending in other valuable sectors. It would be more Pareto efficient for all to have
lower military spending, freeing money and resources for different domains. We will consider races in
the context of AI development in the following section.
Promoting Cooperation
So far we have focused on the sources of undesirable multi-agent dynamics in games like the Prisoner’s
Dilemma. Here, we turn to the mechanisms by which we can promote cooperation over defection.
Reasons to cooperate.
There are many reasons why real-world agents might cooperate in
situations which resemble the Prisoner’s Dilemma [6], as shown in Figure 8.4. These can broadly be
categorized by whether the agents have a choice, or whether defection is impossible. If the agents
do have a choice, we can further divide the possibilities into those where they act in their own
self-interest, and those where they do not (altruism). Finally, we can differentiate three reasons why
self-interested agents may choose to cooperate: a conscience, a change to the payoff structure, and
future reward/punishment. We will explore two possibilities in this section — payoff changes and
altruistic dispositions — and then “future reward/punishment” in the next section. Note that we
effectively discuss “Defection is impossible” in the Single-Agent Safety chapter, and “AI consciences”
in the Machine Ethics chapter.

410
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Figure 8.4: Five possible reasons why agents may cooperate in prisoner’s Dilemma-like scenarios.
This section explores two: changes to the payoff matrix and increased agent altruism. [6]
External consideration: changing the payoffs to incentivize cooperation.
By adjusting
the values in the payoff matrix, we may more easily steer agents away from undesirable equilibria.
As shown in Table 8.2, incentive structures are important. A Prisoner’s Dilemma-like scenario may
arise wherever an individual agent will do better to defect whether their partner cooperates (c > a)
or defects (d > b). Avoiding this situation requires altering these constants where they underlie
critical social interactions in the real world: changing the costs and benefits associated with different
activities so as to encourage cooperative behavior.
Agent B cooperates
Agent B defects
Agent A cooperates
a, a
b, c
Agent A defects
c, b
d, d
Table 8.2: if c > a and d > b, the highest payoff for either agent is to defect, regardless of what their
opponent does: Defection is the dominant strategy. Fostering cooperation requires avoiding this
structure.
There are two ways to reduce the expected value of defection: lower the probability of defection
success or lower the benefit of a successful defection. Consider a strategy commonly used by organized
crime groups: threatening members with extreme punishment if they ‘snitch’ to the police. In the
Prisoner’s Dilemma game, we can model this by adding a punishment equivalent to three years of
jail time for “snitching,” leading to the altered payoff matrix as shown in Figure 8.5. The Pareto
efficient outcome (-1,-1) is now also a Nash Equilibrium because snitching when the other player
cooperates is worse than mutually cooperating (c < a).
Internal consideration: making agents more altruistic to promote cooperation. A second potential
mechanism to foster cooperation is to make agents more altruistic. If each agent also values the

8.2. GAME THEORY
411
0.5
0.5
Figure 8.5: Altering the payoff matrix to punish snitches, we can move from a Prisoner’s Dilemma
(left) to a Stag Hunt (right), in which there is an additional Nash equilibrium.
outcome for their partner, this effectively changes the payoff matrix. Now, the length of their partner’s
jail sentence matters to each of them. In the Prisoner’s Dilemma payoff matrix, the both cooperate
outcome earns the lowest total jail time, so agents who valued their partners’ payoffs equally to their
own would converge on cooperation.
Parallels to AI safety.
One possible example of such a strategy would be to target the values
held by AI companies themselves. Improving corporate regulation effectively changes the company’s
expected payoffs from pursuing risky strategies. If successful, it could encourage the company
building AI systems to behave in a less purely self-interested fashion. Rather than caring solely about
maximizing their shareholder’s financial interests, AI companies might cooperate more with each
other to steer away from Pareto inefficient outcomes, and avoid corporate AI races. We explore this
in more detail in section 8.2.4 “Tail risk: AI races” below.
Summary
Cooperation is not always rational, so intelligence alone may not ensure good outcomes.
We have seen that rational and self-interested agents may not interact in such a way as to achieve
good results, even for themselves. Under certain conditions, such as in the Prisoner’s Dilemma,
they will converge on a Nash equilibrium of both defecting. Both agents would be better off if they
both cooperated. However, it is hard to secure this Pareto improvement because cooperation is not
rational when defection is the dominant strategy.
Conflict with or between future AI agents may be extremely harmful.
One source of
concern regarding future AI systems is inter-agent conflict eroding the value of the future. Rational
AI agents faced with a Prisoner’s Dilemma-type scenario might end up in stable equilibrium states
that are far from optimal, perhaps for all the parties involved. Possible avenues to reduce these risks
include restructuring the payoff matrices for the interactions in which these agents may be engaged
or altering the agents’ dispositions.

412
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
8.2.4
The Iterated Prisoner’s Dilemma
In our discussion of the Prisoner’s Dilemma, we saw how rational agents may converge to equilibrium
states that are bad for all involved. In the real world, however, agents rarely interact with one another
only once. Our aim in this section is to understand how cooperative behavior can be promoted and
maintained as multiple agents (both human and AI) interact with each other over time, when they
expect repeated future interactions. We handle some common misconceptions in this section, such as
the idea that simply getting agents to interact repeatedly is sufficient to foster cooperation, because
“nice” and “forgiving” strategies always win out. As we shall see, things are not so simple. We explore
how iterated interactions can lead to progressively worse outcomes for all.
In the real world, we can observe this in “AI races”, where businesses cut corners on safety due to
competitive pressures, and militaries adopt and deploy potentially unsafe AI technologies, making
the world less safe. These AI races could produce catastrophic consequences, including more frequent
or destructive wars, economic enfeeblement, and the potential for catastrophic accidents from
malfunctioning or misused AI weapons.
Introduction
Agents who engage with one another many times do not always coexist harmoniously. Iterating
interactions is not sufficient to ensure cooperation. To see why, we explore what happens when
rational, self-interested agents play the Prisoner’ Dilemma game against each other repeatedly. In
a single-round Prisoner’s Dilemma, defection is always the rational move. But understanding the
success of different strategies is more complicated when agents play multiple rounds.
In the Iterated Prisoner’s Dilemma, agents play repeatedly.
The dominant strategy for
a rational agent in a one-off interaction such as the Prisoner’s Dilemma is to defect. The seeming
paradox is that both agents would prefer the cooperate-cooperate outcome to the defect-defect one.
An agent cannot influence their partner’s actions in a one-off interaction, but in an iterated scenario,
one agent’s behavior in one round may influence how their partner responds in the next. We call
this the Iterated Prisoner’s Dilemma; see Figure 8.6. This provides an opportunity for the agents to
cooperate with each other.
Iterating the Prisoner’s Dilemma opens the door to rational cooperation.
In an Iterated
Prisoner’s Dilemma, both agents can achieve higher payoffs by fostering a cooperative relationship
with each other than they would if both were to defect every round; compare Figure 8a and b. There
are two basic mechanisms by which iteration can promote cooperative behavior: punishing defection
and rewarding cooperation. To see why, let us follow an example game of the Iterated Prisoner’s
Dilemma in sequence.
Punishment.
Recall Alice and Bob from the previous section, the two would-be thieves caught
by the police. Alice decides to defect in the first round of the Prisoner’s Dilemma, while Bob opts
to cooperate. This achieves a good outcome for Alice, and a poor one for Bob, who punishes this
behavior by choosing to defect himself in the second round. What makes this a punishment is that
Alice’s score will now be lower than it would be if Bob had opted to cooperate instead, whether Alice
chooses to cooperate or defect.
Reward.
Alice, having been punished, decides to cooperate in the third round. Bob rewards this
action by cooperating in turn in the fourth. What makes this a reward is that Alice’s score will now
be higher than if Bob had instead opted to defect, whether Alice chooses to cooperate or defect. Thus,
the expectation that their defection will be punished and their cooperation rewarded incentivizes

8.2. GAME THEORY
413
both agents to cooperate with each other.
Figure 8.6: Across six rounds, both players gain better payoffs if they consistently cooperate. But
defecting creates short-term gains.
In Figure 8.6, each panel shows a six-round Iterated Prisoner’s Dilemma, with purple squares for
defection and blue for cooperation. On the left is Tit-for-tat: An agent using this strategy tends to
score the same as or worse than its partners in each match. On the right, always defect tends to
score the same as or better than its partner in each match. The average payoff attained by using
either strategy are shown at the bottom: Tit-for-tat attains a better payoff (lower jail sentence) on
average—and so is more successful in a tournament—than always defect.
Defection is still the dominant strategy if agents know how many times they will interact.
If the agents know when they are about to play the Prisoner’s Dilemma with each other for the
final time, both will choose to defect in that final round. This is because their defection is no longer
punishable by their partner. If Alice defects in the last round of the Iterated Prisoner’s Dilemma,

414
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Bob cannot punish her by retaliating, as there are no future rounds in which to do so. The same is
of course true for Bob. Thus, defection is the dominant strategy for each agent in the final round,
just as it is in the single-round version of the dilemma.
Moreover, if each agent expects their partner to defect in the final round, then there is no incentive
for them to cooperate in the penultimate round either. This is for the same reason: Defecting in the
penultimate round will not influence their partner’s behavior in the final round. Whatever an agent
decides to do, they expect that their partner will choose to defect next round, so they might as well
defect now. We can extend this argument by reasoning backwards through all the iterations. In each
round, the certainty that their partner will defect in the next round regardless of their own behavior
in the current round incentivizes each agent to defect. The reward for cooperation and punishment
of defection have been removed. Ultimately, this removal pushes the agents to defect in every round
of the Iterated Prisoner’s Dilemma.
Uncertainty about future engagement enables rational cooperation.
In the real world, an
agent can rarely be sure that they will never again engage with a given partner. Wherever there is
sufficient uncertainty about the future of their relationship, rational agents may be more cooperative.
This is for the simple reason that uncooperative behavior may yield less valuable outcomes in the
long term, because others may retaliate in kind in the future. This tells us that AIs interacting with
each other repeatedly may cooperate, but only if they are sufficiently uncertain about whether their
interactions are about to end.
Other forms of uncertainty can also create opportunities for rational cooperation, such as uncertainty
about what strategies others will use. These are most important where the Iterated Prisoner’s
Dilemma involves a population of more than two agents, in which each agent interacts sequentially
with multiple partners. We turn to examining the dynamics of these more complicated games next.
Tournaments
So far, we have considered the Iterated Prisoner’s Dilemma between only two agents: each plays
repeatedly against a single partner. However, in the real world, we expect AIs will engage with
multiple other agents. In this section, we consider interactions of this kind, where each agent not
only interacts with their partner repeatedly, but also switches partners over time. Understanding the
success of a strategy is more complicated in repeated rounds against many partners. Note that in
this section, we define a “match” to mean repeated rounds of the Prisoner’s Dilemma between the
same two agents; see Figure 8.6. We define a “tournament” to mean a population of more than two
agents engaged in a set of pairwise matches.
In Iterated Prisoner Dilemma tournaments, each agent interacts with multiple partners.
In the 1970s, the political scientist Robert Axelrod held a series of tournaments to pit different agents
against one another in the Iterated Prisoner’s Dilemma. The tournament winner was whichever
agent had the highest total payoff after completing all matches. Each agent in an Iterated Prisoner’s
Dilemma tournament plays multiple rounds against multiple partners. These agents employed
a range of different strategies. For example, an agent using the strategy named random would
randomly determine whether to cooperate or defect in each round, entirely independently of previous
interactions with a given partner. By contrast, an agent using the grudger strategy would start out
cooperating, but switch to defecting for all future interactions if its partner defected even once. See
Table 8.3 for examples of these strategies.

8.2. GAME THEORY
415
Strategy
Characteristics
Random
Randomly defect or cooperate, regardless of your partner’s strategy
Always defect
Always choose to defect, regardless of your partner’s strategy
Always cooperate
Always choose to defect, regardless of your partner’s strategy
Grudger
Start by cooperating, but if your partner defects, defect in every
subsequent round, regardless of your partner’s subsequent behavior
Tit-for-tat
Start cooperating; then always do whatever your partner did last
Generous tit-for-tat
Same as tit-for-tat, but occasionally cooperate in response to your
partner’s defection
Table 8.3: Popular strategies’ descriptions.
The strategy “Tit-for-tat” frequently won Axelrod’s tournaments [7].
The most famous
strategy used in Axelrod’s tournaments was Tit-for-tat.
This was the strategy of starting by
cooperating, then repeating the partner’s most recent move: if they cooperated, Tit-for-tat cooperated
too; if they defected, Tit-for-tat did likewise. Despite its simplicity, this strategy was extremely
successful, and very frequently won tournaments. An agent playing Tit-for-tat exemplified the
two mechanisms for promoting cooperation, rewarding cooperation, yet also punishing defection.
Importantly, Tit-for-tat did not hold a grudge—it forgave each defection after it retaliated by
defecting in return, only once. This process of one defection for one defection is captured in the
famous idiom “an eye for an eye.” The Tit-for-tat strategy became emblematic as being one way to
escape the muck of defection.
The success of Tit-for-tat is counterintuitive.
In any given match, an agent playing Tit-for-tat
will tend to score slightly worse than or the same as their partner; see Figure 8.6a. By contrast,
an agent who employs an uncooperative strategy such as always defect usually scores the same
as or better than its partner; see Figure 8.6b. In a match between a cooperative agent and an
uncooperative one, the uncooperative agent tends to end up with the better score.
However, it is an agent’s average score which dictates its success in a tournament, not its score in
any particular match or with any particular partner. Two uncooperative partners will score worse
on average than cooperative ones. Thus, the success of cooperative strategies such as 8.6 depends
on the population strategy composition (the assortment of strategies used by the agents in the
population). If there are enough cooperative partners, cooperative agents may be more successful
than uncooperative ones. We discuss this phenomenon in more detail in Section 8.2.6 Evolutionarily
stable strategies and frequency-dependent selection.
Tail Risk: AI Races
Iterated interactions can generate “AI races.” We discuss two kinds of races concerning AI development:
corporate AI races and military AI arms races.
Both kinds center around competing parties
participating in races for individual, short-term gains at a collective, long-term detriment. Where
individual incentives clash with collective interests, the outcome can be bad for all. As we discuss
here, in the context of AI races, these outcomes could even be catastrophic.
AI races are the result of intense competitive pressures.
During the Cold War, the US and
the Soviet Union were involved in a costly nuclear arms race. The effects of their competition persist
today, leaving the world in a state of heightened nuclear threat. Competitive races of this kind entail
repeated back-and-forth actions that can result in progressively worse outcomes for all involved. We
can liken this example to the Iterated Prisoner’s Dilemma, where the nations must decide whether to

416
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
increase (defect) or decrease (cooperate) their nuclear spending. Both the US and the Soviet Union
often chose to increase spending. They would have created a safer and less expensive world for both
nations (as well as others) if they had cooperated to reduce their nuclear stockpiles. We discuss this
in more detail in International Governance.
Two kinds of AI races: corporate and military [8].
Competition between different parties—
nations or corporations—is incentivizing each to develop, deploy, and adopt AIs rapidly, at the
expense of other values and safety precautions. Corporate AI races consist of businesses prioritizing
their own survival or power expansion over ensuring that AIs are developed and released safely.
Military AI arms races consist of nations building and adopting powerful and dangerous military
applications of AI technologies to gain military power, increasing the risks of more frequent or
damaging wars, misuse, or catastrophic accidents. We can understand these two kinds of AI races
using two game-theoretic models of iterated interactions. First, we use the Attrition model to
understand why AI corporations are cutting corners on safety. Second, we’ll use the Security Dilemma
model to understand why militaries are escalating the use of—and reliance on—AI in warfare.
Corporate AI Races
Competition between AI research companies is promoting the creation and use of more appealing
and profitable systems, often at the cost of safety measures. Consider the public release of large
language model-based chatbots. Some AI companies delayed releasing their chatbots out of safety
concerns, like avoiding the generation of harmful misinformation. We can view the companies that
released their chatbots first as having switched from cooperating to defecting in an Iterated Prisoner’s
Dilemma. The defectors gained public attention and secured future investment. This competitive
pressure caused other companies to rush their AI products to market, compromising safety measures
in the process.
Corporate AI races arise because competitors sacrifice their values to gain an advantage, even if this
harms others. As the race heats up, corporations might increasingly need to prioritize profits by
cutting corners on safety, in order to survive in a world where their competitors are very likely to do
the same. The worst outcome for an agent in the Prisoner’s Dilemma is the one where only they
cooperated while their partner defected. Competitive pressures motivate AI companies to avoid this
outcome, even at the cost of exacerbating large-scale risks.
Ultimately, corporate AI races could produce societal-scale harms, such as mass unemployment and
dangerous dependence on AI systems. We consider one such example in Tail Risk: AI Races Between
More Than Two Competitors. This risk is particularly vivid for emerging industries like AI which
lack the better-established safeguards such as mature regulation and widespread awareness of the
harm that unsafe products can cause found in other industries like pharmaceuticals.
Attrition model: a multi-player game of “Chicken.”
We can model this corporate AI race
using an “Attrition” model [9], which frames the race as a kind of auction in which competitors bid
against one another for a valuable prize. Rather than bidding money, the competitors bid for the risk
level they are willing to tolerate. This is similar to the game “Chicken,” in which two competitors
drive headlong at each other. Assuming one swerves out of the way, the winner is the one who
does not (demonstrating that they can tolerate a higher level of risk than the loser). Similarly, in
the Attrition model, each competitor bids the level of risk—the probability of bringing about a
catastrophic outcome—they are willing to tolerate. Whichever competitor is willing to tolerate the
most risk will win the entire prize, as long as the catastrophe they are risking does not actually
happen. We can consider this to be an “all pay” auction: both competitors must pay what they bid,

8.2. GAME THEORY
417
whether they win or not. This is because all of those involved must bear the risk they are leveraging,
and once they have made their bid they cannot retract it.
The Attrition model shows why AI corporations may cut corners on safety.
Let us
assume that there are only two competitors and that both of them have the same understanding of
the state of their competition. In this case, the Attrition model predicts that they will race each
other up to a loss of one-third in expected value [10]. If the value of the prize to one competitor is
“X”, they will be willing to risk a 33% chance of bringing about an outcome equally disvaluable (of
value “-X”) in order to win the race [11].
As we have discussed previously, market pressures may motivate corporations to behave as though
they value what they are competing for almost as highly as survival itself. According to this toy
model, we might then expect AI stakeholders engaged in a corporate race to risk a 33% chance of
existential catastrophe in order to “win the prize” of their continued existence. With multiple AI
races, long time horizons, and ever-increasing risks, the repeated erosion of safety assurances down
to only 66% generates a vast potential for catastrophe.
Real-world actors may mistakenly erode safety precautions even further.
Moreover,
real-world AI races could produce even worse outcomes than the one predicted by the Attrition model
[11]. One reason for this is that competing corporations may not have a correct understanding of the
state of the race. Precisely predicting these kinds of risks can be extremely challenging: high-risk
situations are inherently difficult to predict accurately, even in fields far more well-understood
than AI. Incorrect risk calibration could cause the competitors to take actions that accidentally
exceed even the 33% risk level. Like newcomers to an ’all pay’ auction who often overbid, uneven
comprehension or misinformation could motivate the competitors to take even greater risks of bringing
about catastrophic outcomes. In fact, we might even expect selection for competitors who tend to
underestimate the risks of these races. All these factors may further erode safety assurances.
Military AI Arms Races
Global interest in military applications for AI technologies is increasing. Some hail this as the “third
revolution in warfare” [12], predicting impact at the scale of the historical development of gunpowder
and nuclear weapons. There are many causes for concern about the adoption of AI technologies in
military contexts. These include increased rates of weapon development, lethal autonomous weapons
usage, advanced cyberattacks execution, and automation of decision-making. These could in turn
produce more frequent and destructive wars, acts of terrorism, and catastrophic accidents. Perhaps
even more important than the immediate dangers from military deployment of AI is the possibility
that nations will continue to race each other along a path towards ever increased risks of catastrophe.
In this section, we explore this possibility using another game theoretic model.
First, let us consider a few different sources of risk from military AI [8]:
1. AI-developed weapons. AI technologies could be used to engineer weapons. Military research
and development offers many opportunities for acceleration using AI tools. For instance, AI
could be used to expedite processes in dual-use biological and chemical research, furthering the
development of programs to build weapons of mass destruction.
2. AI-controlled weapons.
AI might also be used to control weapons directly.
“Lethal
autonomous weapons” have been in use since March 2020, when a self-directing and armed
drone “hunted down” soldiers in Libya without human supervision. Autonomous weapons may
be faster or more reliable than human soldiers for certain tasks, as well as being far more
expendable. Autonomous weapons systems thus effectively motivate militaries to reduce human

418
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
oversight. In a context as morally salient as warfare, the ethical implications of this could be
severe. Increasing AI weapon development may also impact international warfare dynamics.
The ability to deploy lethal autonomous weapons in place of human soldiers could drastically
lower the threshold for nations to engage in war, by reducing the expected body count—of the
nation’s own citizens, at least. These altered warfare dynamics could usher in a future with
more frequent and destructive wars than has yet been seen in human history.
3. AI cyberwarfare. Another military application is the use of AI in cyberwarfare. AI systems
might be used to defend against cyberattacks. However, we do not yet know whether this
will outweigh the offensive potential of AI in this context. Cyberattacks can be used to wreak
enormous harm, such as by damaging crucial systems and infrastructure to disrupt supply
chains. AIs could make cyberattacks more effective in a number of ways, motivating more
frequent attempts and more destructive successes. For example, AIs could directly aid in writing
or improving offensive programs. They could also execute cyberattacks at superhuman scales
by implementing vast numbers of offensive programs simultaneously. By democratizing the
power to execute large-scale cyberattacks, AIs would also increase the difficulty of verification.
With many more actors capable of carrying out attacks at such scales, attributing attacks to
perpetrators would be much more challenging.
4. Automated executive decision-making. Executive control might be delegated to AIs
at higher levels of military procedures. The development of AIs with superhuman strategic
capabilities may incentivize nations to adopt these systems and increasingly automate military
processes.
One example of this is “automated retaliation.”
AI systems that are granted
the ability to respond to offensive threats they identify with counterattacks, without human
supervision. Examples of this include the NSA cyber defense program known as “MonsterMind.”
When this program identified an attempted cyberattack, it interrupted it and prevented its
execution. However, it would then launch an offensive cyberattack of its own in return. It
could take this retaliatory action without consulting human supervisors. More powerful AI
systems, more destructive weapons, and greater automation or delegation of military control to
AI systems, would all deplete our ability to intervene.
5. Catastrophic accidents. Lethal Autonomous Weapons and automated decision-making
systems both carry risks of resulting in catastrophic accidents. If a nation were to lose control
of powerful military AI technologies, the outcome could be calamitous. Outsourcing executive
command of military procedures to AI—such as by automating retaliatory action—would
put powerful arsenals on hair-trigger alert. If one of these AI systems were to make even a
small error, such as incorrectly identifying an offensive strike from another nation, it might
automatically “retaliate” to this non-existent threat. This could in turn trigger automated
retaliations from the AI systems of other nations that detect this action. Thus, a small error
could be exacerbated into an increasingly escalated war. We consider how a “flash war” such as
this might come about in more detail in Section 8.2.5. Note that we can also use the “Attrition”
model in the case of military AI arms races to model how military competitive pressures can
motivate nations to cut corners on safety.
6. Co-option of military AI technologies. Military AI arms races could also have catastrophic
effects outside of international warfare. New and more lethal weapons could be used maliciously
in other contexts. For instance, biological weapons were originally created for military purposes.
Even though we have since halted the military use of these weapons, their existence has enabled
many acts of bioterrorism. Examples include the 2001 deployment of anthrax letters to kill
US senators and media executives. The creation of knowledge of how to make and use these
weapons is irreversible. Thus, their existence, and the risk they pose, is permanent.
7. Military AI risks may interact. Importantly, the risks posed by military AI applications

8.2. GAME THEORY
419
are not entirely independent of one another. The increased potential for anonymity when
executing cyberattacks could increase the probability of wars. Where it is harder to identify
the perpetrators, misattribution could trigger conflict between the target of the attack and an
innocent party. The potential for destructive cyberattacks might be increased by the scaled-up
use of autonomous weapons, as these could be co-opted by such attacks. Similarly, the danger
posed by a rogue AI with executive decision-making power might be all the more serious if it
has control over fleets of autonomous weapons.
Security Dilemma model: mutual defensive concerns motivate nations to increase risks.
We can better understand military AI arms races using the “Security Dilemma” model [13]. Consider
the relationship between two peaceful nations. Though they are not currently at war with one another,
each is sufficiently concerned about the possibility of conflict to pay close attention to the other’s
state of military ability. One day, one of the two nations perceives that the other is more militarily
capable than they are due to their having stockpiled more advanced weaponry. This incentivizes
the first nation to build up their own military capabilities until they match or exceed those of the
other nation. The second nation, perceiving this increase in military investment and development,
feels pressure to follow suit, once again increasing their weapon capabilities. Neither wishes to be
outmatched by the other. This competitive pressure drives both to escalate the situation. The
ensuing arms race generates increasingly high risks for both sides, such as increasing the probability
or severity of accidents and misuse.
Example: the Cold War nuclear arms race.
As previously discussed, the Cold War nuclear
arms race typifies this process. Neither the US nor the Soviet Union wanted to risk being less militarily
capable than their rival, so each escalated their own weaponized nuclear ability in an attempt to
deter the other using the threat of retaliation. Just as in the Iterated Prisoner’s Dilemma, neither
nation could afford to risk being the lone cooperator while their rival defected. Thus, they achieve a
Pareto inefficient outcome of both defecting. Competitive pressure drove them to continue to worsen
this situation over time, resulting in today’s enormously heightened state of nuclear vulnerability.
Increased automation of warfare by one nation puts pressure on others to follow suit.
Just as with nuclear weapons, so with military AI: the Security Dilemma model illustrates how
defensive concerns can force nations to go down a route which is against the long term interests of
all involved. This route leads to the competing nations continually heightening the risks posed by
military AI applications, including more frequent and severe wars, and worse accidents.
There are many incentives for nations to increase their development, adoption, and deployment of
military AI applications. With more AI involvement, warfare can take place at an accelerated pace,
and at a more destructive scale. Nations that do not adopt and use military AI technologies may
therefore risk not being able to compete with nations that do. As with nuclear mutually assured
destruction, nations may also employ automated retaliation as a signal of commitment, hoping to
deter attacks by demonstrating a plausible resolution to respond swiftly and in kind. This process
of automation and AI delegation would thus perpetuate, despite it being increasingly against the
collective good.
Ultimately, as with economic automation, military AI arms races could result in humans being unable
to keep up. The pace and complexity of warfare could ascend out of human reach to where we are no
longer able to comprehend or intervene. This could be an irreversible step putting us at high risk of
catastrophic outcomes.

420
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Summary
The Iterated Prisoner’s Dilemma involves repeated rounds of the Prisoner’s Dilemma game. This
iteration offers a chance for agent cooperation but doesn’t ensure it. There are different strategies
by which agents can attempt to maximize their overall payoffs. These strategies can be studied by
competing agents against one another in tournaments, where each agent competes against others in
multiple rounds before switching partners.
This provides cause for concern about multi-AI agent futures. The “races” governing the production,
deployment and adoption of AI technologies, in both corporate and military settings, have the
potential to exacerbate many of the intrinsic risks from AI. The dynamics we have explored in this
section might cause competing agencies to cut corners on safety, and to escalate weaponized AI
applications and automate warfare. These are two examples of how competitive pressures, modeled
as iterated interactions between agents, can generate races which increase the risk of catastrophe for
everyone. Fostering cooperation between different parties—human individuals, corporations, nations,
and AI agents—is vital for ensuring our collective safety.
8.2.5
Collective Action Problems
We began our exploration of game theory by looking at a very simple game, the Prisoner’s Dilemma.
We have so far considered two ways to model real-world social scenarios in more detail. First, we
explored what happens when two agents interact multiple times (such as an Iterated Prisoner’s
Dilemma match). Second, we introduced a population of more than two agents, where each agent
switches partners over time (such as an Iterated Prisoner’s Dilemma tournament). Now we move
beyond pairwise interactions, to interactions that simultaneously involve more than two agents. We
consider what happens when an agent engages in repeated rounds of the Prisoner’s Dilemma against
multiple opponents at the same time.
One class of scenarios that can be described by such a model is collective action problems. Throughout
this section, we first discuss the core characteristics of collective action problems. Then, we introduce
a series of real-world examples to highlight the ubiquity of these problems in human society, as
well as showing how AI races can be modeled in this way. Following this, we transition to a brief
discussion of common pool resource problems to further illustrate the difficulty with which rational
agents, especially AI agents, may secure collectively good outcomes. Finally, we conclude with a
detailed discussion of flash wars and autonomous economies to show how in a multi-agent setting,
AIs might pursue behaviors or tactics that result in catastrophic or existential risks to humans.
Introduction
This first section explores the nature of collective action problems. We begin with a simple example
of a collaborative group project. Through this, we explore how individual incentives can sometimes
clash with what is in the best interests of the group as a whole. These situations can motivate
individuals to act in ways that negatively impact all of the population.
A collective action problem is like a group-level Iterated Prisoner’s Dilemma.
In the
Iterated Prisoner’s Dilemma, we saw how a pair of rational agents can tend towards outcomes that
are undesirable for both. Now let us consider social interactions between more than two agents.
When an individual engages with multiple partners simultaneously, they may still converge on Pareto
inefficient Nash equilibria. In fact, with more than two agents, cooperation can be even harder to
secure. We can therefore model collective action problems as an Iterated Prisoner’s Dilemma in which
more than two prisoners have been arrested: If enough of them decide to defect on their partners, all
of them will suffer the consequences.

8.2. GAME THEORY
421
Example:
group projects.
A typical example of a collective action problem is that of a
collaborative project. A group working together towards a shared goal often encounters a problem:
not everyone pitches in. Some group members take advantage of the rest, benefiting from the work
others are doing without committing as much effort themselves. The implicit reasoning behind
the behavior of these “slackers” is as follows. They want the group’s goal to be achieved, but they
would prefer this to happen without costing them much personal effort. Just as with the Prisoner’s
Dilemma, “slacking” is their dominant strategy. If the others work hard and the project is completed,
they get to enjoy the benefits of this success without expending too much effort themselves. If the
others fail to work hard and the project is not completed, they at least saved themselves the effort
they might otherwise have wasted.
As groups increase in size and heterogeneity, complexity increases accordingly. Agents in a population
may have a diverse set of goals. Even if the population can agree on a common goal, aligning diverse
agents with this goal can be difficult. For example, even when the public expresses strong and
widespread support for a political measure, their representatives often fail to carry it out.
Formalization
Here, we formalize our model of collective action problems. We look more closely at the incentives
governing individual choices, and the effects these have at the group level. We examine how the
behavior of others in the group can alter the incentives facing any individual, and how we can (and
do) use these mechanisms to promote cooperative behavior in our societies.
Each agent must choose whether to contribute to the common good.
As in the Prisoner’s
Dilemma, each agent must choose which of two actions to take. An agent can choose to contribute
to the common good, at some cost to themselves. The alternative is for the agent to choose to
free ride, benefiting from others’ contributions at no personal cost. Free riders impose negative
externalities—collateral damage for others in pursuit of private benefit—on the group as a whole
by choosing not to pitch in.
Free riding is the dominant strategy.
For now, let us assume that free riding increases an
agent’s own personal benefit, regardless of whether the others contribute or free ride: it is the
dominant strategy. If an agent’s contribution to the common good is small, then choosing not to
contribute does not significantly diminish the collective good, meaning that an agent’s decision to
free ride has essentially no negative consequences for the agent themself. Thus, the agent is choosing
between two outcomes. The first outcome is where they gain their portion of the collective benefit,
and pay the small cost of being a contributor. The other outcome is where they gain this same
benefit, but save themselves the cost of contributing.
Free riding can produce Pareto inefficient outcomes.
Just as how both agents defecting
in the Prisoner’s Dilemma produces Pareto inefficiency, free riding in a collective action problem
can result in an outcome that is bad for all. In many cases, some agents can free ride without
imposing significant externalities on everyone else. However, if sufficiently many agents free ride,
this diminishes the collective good by leading to no provision of a public good, for instance. With
sufficient losses, the agents will all end up worse than if they had each paid the small individual cost
of contributing and received their share of the public benefit. Importantly, however, even in this
Pareto inefficient state, free riding might still be the dominant strategy for each individual, since
the cost of contributing outweighs the trivial increase in collective good they would contribute by
contributing. Thus, escaping undesirable equilibria in a collective action problem can be exceedingly
difficult; see Figure 8.7.

422
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Figure 8.7: In this abstract collective action problem, we can move from everyone (contributes) to
right (no one contributes). As more people free ride, the collective good disappears, leaving everyone
in a state where they would all prefer to collectively contribute instead.
We can illustrate a collective action problem using the simple payoff matrix below. In the matrix, “b”
represents the payoff an agent receives when everyone else cooperates (the collective good divided
between the number of agents) and “c” represents the personal cost of cooperation. As the matrix
illustrates, the dominant strategy for a rational agent (“you”) here is to free ride whether everyone
else contributes or free rides.
The rest of the
group contributes
The rest of the
group free rides
Some contribute;
others free ride
You contribute
b −c
−c
< b −c
You free ride
b
0
< b
Table 8.4: Free riding is always better for an individual: it is a dominant strategy.
Agents’ incentives depend on the behavior of other agents.
Agents in collective action
problems can be aware of the choices other agents make, which can affect their strategies and behavior
over time. For example, the ratio of defectors to cooperators in a population can affect the degree
to which cooperation is achieved. When rational agents interact with each other, they may be
inclined to shift their strategies to more favorable ones with higher individual payoffs: they may
realize that other agents are utilizing more successful strategies, and thus choose to adopt them. If
defectors dominate the population initially, and the initial individual costs of cooperation outweigh
the collective benefits of cooperation, then the population may tend towards an uncooperative state.
In simple terms, collective action problems cannot be solved without cooperation.
Mutual and external coercion.
We can increase the probability of cooperation by generating
incentives that lower the individual cost of cooperation and increase the individual cost of defection.

8.2. GAME THEORY
423
There are two ways we may go about this: mutual or external coercion. Mutual coercion generates
cooperative incentives by establishing communal, societal, and reputational norms. External coercion
generates cooperative incentives through external intervention, by developing regulations that
incentivize collective action through mandates, sanctions, and legislature, making cooperation
a necessity in certain cases. Below, we illustrate some real-world scenarios in further detail.
Real-World Examples of Collective Action Problems
Many large-scale societal issues can be understood as collective action problems. This section explores
collective action problems in the real world: climate change, public health, and democratic voting.
We end by briefly looking at AI races through this same lens.
Public health.
We can model some public health emergencies, such as disease epidemics, as
collective action problems. The COVID-19 pandemic took the lives millions worldwide. Some of
these deaths could have been avoided with stricter compliance with public health measures such as
social distancing, frequent testing, and vaccination. We can model those adhering to these measures
as “contributing” (by incurring a personal cost for public benefit) and those violating them as “free
riding.”
Assume that everyone wished the pandemic to be controlled and ultimately eradicated, that complying
with the suggested health measures would have helped hasten this goal, and that the benefits of
collectively shortening the pandemic timespan would have outweighed the personal costs of compliance
with these measures (such as social isolation). Everyone would prefer the outcome where they all
complied with the health measures over the one where few of them did. Yet, each person would
prefer still better the outcome where everyone else adhered to the health measures, and they alone
were able to free ride. Violating the health measures was therefore the dominant strategy, and so
many people chose to do this, imposing the negative externalities of excessive disease burden on the
rest of their community.
We used both mutual and external mechanisms to coerce people to comply with public health
measures in the pandemic. For example, some communities adjusted their social norms (mutual
coercion) such that non-compliance with public health measures would result in damage to one’s
reputation. We also required proof of vaccination for entry into desirable social spaces (external
coercion), among many other requirements.
Anthropogenic climate change.
In 2021, a majority of those surveyed worldwide reported
wanting to avert catastrophic anthropogenic climate change. Most, however, chose not to act in
accordance with what they believed necessary to achieve this goal. The consumption of animal
products typically entails far higher greenhouse gas emissions and environmental damage than
plant-based alternatives. The use of public over private transport similarly reduces personal carbon
footprints dramatically. To avoid the costs of taking these actions, such as changing routines and
compromising on speed or ease, most people do not change their diets or transport habits. Various
behaviors that increase pollution can be viewed as “free riding.” Since this is the dominant strategy
for each agent, most choose to do this, resulting in ever-worsening climate change, imposing risks on
the global population.
We could disincentivize excessive meat eating and private transport using external and mutual
coercion. In this example, external coercion could include lowering bus and train fares and enhancing
existing infrastructure through government subsidies, as well as implementing fuel taxes on private
vehicles. Mutual coercion could include changing social norms to consider excessive meat eating or
short-haul flying unacceptable.

424
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Democracy.
We can model the maintenance of a democracy as a set of collective action problems.
There are many situations in which certain actions might provide an individual with immediate
benefits, but would incur longer-term costs on the larger group if more people were to take these
actions. For example, a voting population must maintain certain norms in order to keep its democracy
functioning. One of these norms is to vote only for candidates who will not undermine democratic
processes, even if others have desirable traits.
Choosing whether or not to participate in an election at all can similarly be viewed as a collective
action problem. The outcome of an election is determined by the votes of individuals, each of which
has a choice to either vote or abstain. The results of the election are determined by the votes of
those who choose to participate, and the costs of participating in the election are carried by citizens
themselves, such as the time and effort required to register and cast a vote. When large enough
numbers of citizens decide to abstain from voting, the collective outcome of an election may not
accurately reflect the preferences of the population: by acting in accordance with their rational self
interest, citizens may contribute to a suboptimal collective outcome.
Common pool resource problem.
Rational agents are incentivized to take more than a
sustainable amount of a shared resource. This is called a common pool resource problem or tragedy
of the commons problem. We refer to a common pool resource becoming catastrophically depleted as
collapse. Collapse occurs when rational agents, driven by their incentive to maximize personal gain,
tip the available supply of the shared resource below its sustainability equilibrium [14]. Below, we
further illustrate how complicated it is to secure collectively good outcomes, especially when rational
agents act in accordance with their self-interest. Such problems are prevalent at the societal level,
and often bear catastrophic consequences. Thus, we should not eliminate the possibility that they
may also occur with AI agents in a multi-agent setting.
For example, rainforests around the world have been diminished greatly by deforestation practices.
While these forests still exist as a home to millions of different species and many local communities,
they may reach a point at which they will no longer be able to rejuvenate themselves. If these practices
are sustained, the entire ecosystem these forests support could collapse. Common pool resource
problems exemplify how agents may bring about catastrophes even when they behave rationally and
in their self-interest, with perfect knowledge of the looming catastrophe, and despite the seeming
ability to prevent it. They further illustrate how complicated it can be to secure collectively good
outcomes and how rational agents can act to the detriment of their own group. As with many
other collective action problems, we can’t expect to solve common pool resource problems by having
AIs manage them. If we simply pass the buck to AI representatives, the AIs will inherit the same
incentive structure that produces the common pool resource problem, and so the problem will likely
remain.
Tail Risk: AI Races Between More Than Two Competitors
In the previous section, we looked at how corporations and militaries may compete with one another
in “AI races.” We used a two-player “attrition” bidding model to see why AI companies cut corners
on safety when developing and deploying their technologies. We used another two-player “security
dilemma” model to understand how security concerns motivate nations to escalate their military
capabilities, even while increasing the risks imposed on all by increasingly automating warfare in this
manner.
Here, we extend our models of these races to consider more than two parties, allowing us to see
them as collective action problems. First, we look at how military AI arms races increase the risk of
catastrophic outcomes such as a flash war: a war that is triggered by autonomous AI agents that
quickly spirals out of human control [8]. Second, we explore how ever-increasing job automation

8.2. GAME THEORY
425
could result in an autonomous economy: an economy in which humans no longer have leverage or
control.
Military AI arms race outcome: flash war.
The security dilemma model we explored in the
previous section can be applied to more than two agents. In this context, we can see it as a collective
action problem. Though all nations would be at lower risk if all were to cooperate with one another
(“contribute” to their collective safety), each will individually do better instead to escalate their own
military capabilities (“free ride” on the contributions of the other nations). Here, we explore one
potentially catastrophic outcome of this collective action problem: a flash war.
As we saw previously, military AI arms races motivate nations to automate military procedures. In
particular, there are strong incentives to integrate “automated retaliation” protocols. Consider a
scenario in which several nations have constructed an autonomous AI military defense system to
gain a defensive military advantage. These AIs must be able to act on perceived threats without
human intervention. Additionally, each is aligned with a common goal: “defend our nation from
attack.” Even if these systems are nearly perfect, a single erroneous detection of a perceived threat
could trigger a decision cascade that launches the nation into a “flash war.” Once one AI system
hallucinates a threat and issues responses, the AIs of the nations being targeted by these responses
will follow suit, and the situation could escalate rapidly. A flash war would be catastrophic for
humanity, and might prove impossible to recover from.
A flash war is triggered and amplified by successive interactions between autonomous AI agents
such that humans lose control of weapons of mass destruction [15]. Any single military defense AI
could trigger it, and the process could continue without human intervention and at great speed.
Importantly, having humans in the loop will not necessarily ensure our safety. Even if AIs only
provide human operators with instructions to retaliate, our collective safety would rest on the chance
that soldiers would willfully disobey their instructions.
Collective action between nations could avoid these and other dire outcomes. Limiting the capabilities
of their military AIs by decreasing funding and halting or slowing down development would require
that each nation gives up a potential military advantage. In a high stakes scenario such as this one,
rational agents (nations) may be unwilling to give up such an advantage because it dramatically
increases the vulnerability of their nation to attack. The individual cost of cooperation is high while the
individual cost of defection is low, and as agents continue to invest in military capabilities, competitive
pressures increase, which further exacerbate costs of cooperation —thereby disincentivizing collective
action. While the collective benefits of cooperation would drastically reduce the catastrophic risks
of this scenario in the long-term, they may not outweigh the self-interest of rational agents in the
short-term.
Corporate AI race outcome: autonomous economy.
As AIs become increasingly effective
at carrying out human goals, they may begin to out-perform the average human at an increasing
number and range of jobs, from personal assistants to executive decision-makers. To reap the benefits
of these faster and more effective workers, companies will likely continue to automate economically
valuable functions by delegating them to AI agents. Ultimately, this could lead to the global economy
becoming “autonomous,” with humans no longer able to steer or intervene [16].
Such an autonomous economy would be a catastrophe for humanity. Like passengers in an autonomous
vehicle, our safety and destination would rest with the AI systems now acting without our supervision.
Our future would be determined by the behavior and outputs of this autonomous economy. If the AI
agents engaged in this economy were to have undesirable goals or evolve selfish traits—a possibility
we examine in the next section of this chapter—humanity would be unable to prevent the harms

426
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
they cause. Even if the AIs themselves are well-aligned to our goals, the economic system itself may
produce extremely undesirable outcomes. In this section, we have examined many examples of how
macrobehavior can differ dramatically from micromotives. A population of individuals can tend
towards states that are bad for everyone and yet be in stable equilibria. This could happen just the
same with AI representatives acting on humanity’s behalf in an autonomous economy.
Just as with the military AI arms race, we can model how an autonomous economy might be brought
about using the security dilemma model. As in the previous example, if we expand this model to
more than two agents, we can see it as a collective action problem in which competitive pressures
drive different parties to automate economic functions out of the need to “keep up” with their
competitors. Under this model, we can see how companies must choose whether to maintain human
labor (“contributing”) or automate these jobs using AI (“free riding”). Although all would prefer the
outcome in which the calamity of an autonomous economy is avoided, each would individually prefer
to have a competitive advantage and not risk being outperformed by rivals who reap the short-term
benefit of using AIs. Thus, economic automation is the dominant strategy for each competitor.
Repeated rounds of this game in which a sufficient number of agents free ride would drive us towards
this disaster. In each successive round, it would become progressively more difficult to turn back, as
we come to rely increasingly on more capable AI agents.
Increasing AI autonomy increases the risk of catastrophic outcomes.
As AIs become
more autonomous, humans may delegate more decision-making power to them. If AIs are able to
successfully and consistently attain the high-level objectives given to them by humans, we may be
more inclined to begin providing them with open-ended goals. If AIs achieve these goals, humans
might not be privy to the process they follow and may overlook potential harms, as we saw in both
the autonomous economy and flash war examples. Moreover, adaptive AIs—systems that actively
adjust their computational design, architecture and behavior in response to new information or
changes in the environment—could adapt at a much faster rate than humans. The possibility of
self-improvement among such AIs would further exacerbate this problem. Adaptive AIs could develop
unanticipated emergent behaviors and strategies, making them deeply unpredictable. Humans could
be inclined to accept these negative behaviors in order to maintain a competitive advantage in the
short-term.
Reducing competitive pressures could foster collective action.
The security dilemma model
shows how nations can be motivated to escalate their offensive capabilities out of the perception
that their competitors are doing the same. However, by signaling the opposite, we might be able to
produce the reverse effect, such as military de-escalation or an increase in AI safety standards. For
instance, whether different nations will acquiesce to a shared international standard for AI regulation
may depend on whether the nations are individually signaling their willingness to regulate in their
own jurisdiction in the first place. If one nation perceives that others are engaging in strict domestic
regulation, they might see this as a credible signal of commitment to an international standard. By
easing the competitive pressures, we might be able to foster collective action to avoid driving up the
collective risk level.
Summary
We observe important and intractable collective action problems in many domains of life, such as
environmental degradation, pandemic responses, maintenance of democracies, and common pool
resource depletion. We can understand these as Iterated Prisoner’s Dilemmas with many more than
two agents interacting simultaneously in each round of the game. As before, we see that “free riding”
can be the dominant strategy for an individual agent, and this can lead to Pareto inefficient outcomes
for the group as a whole. We can use the mechanisms of mutual and external coercion to incentivize

8.2. GAME THEORY
427
agents to cooperate with each other and achieve collectively good outcomes.
If we expand our models of AI races to include more than two agents, we can understand the races
themselves as collective action problems, and examine how they exacerbate the risk of catastrophe.
One example is how increasingly automating military protocols increases the risk of a “flash war.”
Similar dynamics of automation in the economic sphere could lead to an “autonomous economy.”
Either outcome would be disastrous and potentially irreversible, yet we can see how competitive
pressures can drive rational and self-interested agents (such as nations or companies) down a path
towards these calamities.
8.2.6
Evolutionary Game Theory
We began our discussion of game theory by exploring the Prisoner’s Dilemma, a simple game that
involves two agents who interact only once. To model real-world multi-agent dynamics more closely,
we first built on this by iterating the two agents’ interactions. We next introduced more than two
agents into the game. Now, we introduce a third elaboration: the ability for agents to adapt their
strategies. We look at how the multi-agent dynamics we have explored so far change when the agents
concerned can adapt their behaviors.
This section uses ideas from Evolutionary Game Theory. This field was born from a merging of
economics and biology in the 1970s when John Maynard-Smith and George Price found a way to
describe living organisms’ traits and behaviors as game strategies. Using evolutionary game theory,
we explore how evolutionary forces alter the multi-agent dynamics in the Iterated Prisoner’s Dilemma.
In particular, we look at the conditions under which a population may cycle through different strategy
compositions. We then move to a different game, known as the “Hawk-Dove game,” to illustrate how
identical agents may tend towards Nash equilibria in which they use a range of different strategies.
We use these games to understand why predicting the behavior of many different AI agents may
be very difficult. AI agents may evolve to employ extremely harmful strategies in repeated social
interactions, including trying to extort or deceive others.By enhancing our understanding of these
factors, we may better anticipate how to address potential risks stemming from multi-AI agent
interactions.

428
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Evolutionary Iterated Prisoner’s Dilemma Tournaments
In this section, we investigate how evolutionary forces change the dynamics of a population of agents
engaged in pairwise interactions. We explore how these forces can prevent a population from reaching
a stable equilibrium at all. We see why, if AI agents are subject to evolutionary pressures, they may
change how they interact with others in ways that are difficult to predict.
Evolutionary tournaments: agents can adapt their strategies.
Following his first round of
Iterated Prisoner Dilemma tournaments, Axelrod later ran a second set which he re-structured to
resemble the dynamics of natural selection [17]. We define these “evolutionary tournaments” as ones
where a strategy’s success in one round determined the number of agents who used that strategy in
the next round. The most numerous strategy at the end was the winner. Though strategies that have
been successful in previous tournaments, such as tit-for-tat, were often still successful in evolutionary
tournaments, their victory was by no means assured. The outcomes of these new tournaments were
very sensitive to changes in conditions, which we explore next.
Cyclical dynamics can emerge; uncooperative strategies can persist.
An interesting aspect
of evolutionary tournaments is that they test how well a strategy performs against itself. As the
success of a particular strategy increases, so does its relative frequency in the population. Thus, agents
using this strategy will play against each other more often. In this context, strategies like tit-for-tat
are vulnerable to what we call the “echo” problem [18]. Consider a match between two agents both
playing tit-for-tat. If one defects—either by accident, or by design if they are a less cooperative
tit-for-tat variant—the other will retaliate by defecting as well. This will repeat, triggering a sequence
of mutual defections. Both agents are thus locked into playing always defect. This gives them a
lower score than if one were to forgive the other, allowing them to return to a state of cooperation.
Thus, more “forgiving” strategies can fare better in a population with a high frequency of agents
playing tit-for-tat by cutting short any string of mutual defections. This phenomenon is related to
“frequency dependent selection,” which we explore later in this chapter. More forgiving strategies
include generous tit-for-tat, which occasionally overlooks partner defection.
In an evolutionary tournament, agents can switch to more successful strategies. So, if the strategy
generous tit-for-tat is more successful than “normal” tit-for-tat, it may eventually become more
frequent than tit-for-tat in the population [19]. However, if generous tit-for-tat becomes a sufficiently
common strategy in turn, agents playing always cooperate can achieve just as high a score, making
this strategy popular too. If always cooperate reaches a sufficient frequency in the population, this
creates an opportunity to exploit all these cooperators by switching to an uncooperative strategy
such as always defect. If enough agents take this opportunity and switch to playing always defect,
other agents who play tit-for-tat will once again start to outcompete these defectors (as outlined
previously). Thus, a cycle can ensue, in which each strategy is replaced by another; see Figure
8.8. Many other examples of cyclical dynamics have been identified in Iterated Prisoner’s Dilemma
tournaments [20].

8.2. GAME THEORY
429
Figure 8.8: In an evolutionary tournament, the prevalence of current strategies determines which
new strategies do best. In the bottom left, the prevalence of “always cooperate” means “always defect”
is successful. In the top left, the prevalence of “always defect“ means “tit-for-tat” is successful. These
dynamics can become cyclical.
Evolutionarily Stable Strategies and Frequency-Dependent Selection
In this section, we investigate how different strategies may come to replace one another within an
evolving population. We start with the classic evolutionary game theoretic model known as the
“Hawk-Dove game” to demonstrate how identical agents may reach a Nash equilibrium in which they
employ different strategies to one another. This suggests that if AI agents are subject to evolutionary
forces, it may be difficult to predict their behavior, because the incentives facing each may depend
on what others in the population are doing.
Example: the Hawk-Dove game.
Let us consider a simple evolutionary game-theoretic scenario.
A group of agents is engaged in frequent, pairwise contests over an indivisible resource (such as
nesting space). Each agent can choose one of two strategies. “Hawks” always fight for the prize
and only back down when injured. “Doves” only threaten but never actually fight. We call this the
‘Hawk-Dove’ game [21].
In a one-on-one game, a Hawk will always beat a Dove without any cost. When two agents choose
the same strategy, each has a 50% chance of winning or losing. But there is a difference in outcomes.
The loser of a Dove-Dove contest suffers no injury costs. Whereas the loser of a Hawk-Hawk contest
not only fails to gain the valuable resource but also incurs an injury cost from fighting. We show
these two strategies’ average losses and gains in the payoff matrix below (where b is the benefit of
winning the resource, and c is the cost incurred by fighting). We assume that b<c, all Hawks have
equal fighting ability, and all Doves have equal threatening ability.

430
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Hawk (opponent)
Dove (Opponent)
Hawk (self)
1
2(b −c)
b
Dove (self)
0
1
2b
Table 8.5: Payoff matrix for the Hawk-Dove game.
Neither Hawk nor Dove is the dominant strategy.
If all agents were Hawks, everyone would
fight each other for the resource and only stop when hurt. Their average gain would be 1
2(b −c). But
since the price of fighting is more than the resource’s value (b < c), this payoff would be negative. If
one agent switched to being a Dove, they would avoid the cost of fighting, but they would never win
the resource. Their average payoff would be 0, which is better than the payoffs the Hawks would be
getting. Also, this gain would increase as more agents choose Dove, as their average payoff when
meeting Doves would be 1
2b. Therefore, Doves could invade a Hawk-only group.
Similarly, if all agents were Doves, an agent who switched to Hawk would gain more without paying
a price for fighting, as Doves don’t fight back. So, Hawks could invade a Dove-only group. Hence,
neither strategy is always better. The best strategy for an agent depends on what strategies the
other agents are using. So the population strategy composition will include both Doves and Hawks.
Evolutionarily stable strategies.
When the members of an evolving population adopt a strategy
that makes it uninvadable by any rare novel mutants that may arise, we call this state an evolutionarily
stable strategy (ESS) [21]. An ESS is a kind of Nash Equilibrium: when agents are at this equilibrium,
they cannot improve their success by switching to any other strategy. In the Nash equilibria we
have considered so far in this chapter, the agents have tended to use the same strategy as one
another—if an agent used a different strategy, they would gain a lower payoff. However, a population
can sometimes stably contain more than one strategy. Agents can use different strategies that are
equally successful. For example, in the Hawk-Dove game, where fighting is more costly than the prize
(b < c), the population will reach a stable equilibrium in which there are both Hawks and Doves.
Frequency-dependent selection.
This kind of equilibrium, where agents use different strategies,
is stabilized by the success of either strategy decreasing as it becomes more common, and increasing
as it becomes rarer. When the success of a strategy depends on how common it is compared to
others, we call this frequency-dependent selection. A strategy may succeed more as it becomes more
common. For example, if more wasps evolve bright colors to warn predators, the warning works
better. Alternatively, a strategy’s success might drop as it becomes more common. For example,
certain snails have evolved to look like inedible objects. Frequency-dependent selection has caused
them to look very different from one another. The more often a camouflage strategy is used, the
more chances predators have to see through it. As a result, snails that “choose” (evolve) less popular
camouflage strategies do the best. This results in a dynamic state in which the no snail color spreads
to fixation in the population. Instead, snails of multiple colors coexist stably; see Figure 8.9.
The evolutionary biologist Richard Dawkins explains this using the following fictional scenario [22].
A population of seagulls lives on the coast where their only source of food is the fish in the sea. Each
seagull can choose whether to behave as a “fisher” or as a “pirate”. Fishers catch fish themselves;
pirates wait for fishers to catch a fish, then steal it from them. Neither of these is an ESS. In a
population composed entirely of fishers, a seagull who mutates and becomes a pirate will benefit
hugely; piracy is a profitable strategy because pirates do not expend energy fishing for themselves.
Thus, the pirate strategy can invade the fisher strategy. On the other hand, a population composed
entirely of pirates is invadable by a fisher. Without any fishers to prey on, the pirates have no fish
to eat. A seagull who mutates and becomes a fisher can access food for themself, giving them an
advantage. Thus, neither strategy is an ESS by itself. Some proportion of the population opt for

8.2. GAME THEORY
431
one strategy, and another proportion opts for the other. Thus, this process of frequency-dependent
selection can produce populations that stably contain more than two strategies.
Figure 8.9: The rarest snails may have the highest fitness since predators are not used to them. They
become more prevalent in the next generation, reducing their relative fitness. This is an example of
frequency-dependent selection.
Frequency-dependent selection in game theory. We have encountered frequency-dependent selection
already in this section. In Section 8.2.5, we briefly discussed how the ratio of contributors to free-riders
in the population can alter the incentives for the agents engaged in a collective action problem. In
Section 8.2.6, we used evolutionary Iterated Prisoner’s Dilemma tournaments to model the dynamics
of natural selection, and found that there may be no stable equilibrium state. Instead, the best
strategy for an agent to employ may depend on what others in the population are doing. In these
cases, the population may cycle or undulate through different strategy compositions. Whichever
strategy is most common may become less successful than a rarer alternative.
Identical AI agents may tend towards different strategies.
A Nash equilibrium, or ESS,
doesn’t always mean that all agents will use the same strategy. Frequency-dependent selection might
cause AI agents to adopt different but equally successful strategies. Under certain conditions, like
the size and variety of a population, harmful strategies like extortion can persist by stably cycling
through different proportions of agents as the population evolves.
Evolutionarily stable strategies can be Pareto inefficient.
Note that in the Hawk-Dove
game, all agents would be better off if they were Doves, as every agent that plays Hawk lowers the
average payoff for them all. Just as with the Prisoner’s Dilemma, the Nash equilibrium is Pareto
inefficient. Hence, the ESS is not necessarily ‘good’ for the population, and what is good for the

432
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
population is not necessarily evolutionary stable. A population of altruistic AIs, for example, is not
evolutionarily stable, as that population is easily invadable; we explore this point in greater detail
in “Evolutionary Pressures - Section 3.3.” This makes it likely that evolution is incompatible with
achieving ideal outcomes.
Extortion
We have seen how frequency-dependent selection can generate a population of agents using different
strategies to one another. This can include strategies which we would consider harmful, even immoral.
In this section, we examine one such possibility: extortion.
Extortion strategies in the Iterated Prisoner’s Dilemma.
In the real world, we describe the
use of threats to force a victim to take an action they would otherwise not want to take (such as to
relinquish something valuable) as “extortion.” Examples include criminal organizations ransoming
those they have kidnapped to extort their families for money in exchange for their safe return.
In the Iterated Prisoner’s Dilemma, there is a set of extortion strategies that bear similarity to this
real-world phenomenon. An agent playing the game can use an extortion strategy to ensure that
their payoff in any match is higher than their partner’s [23]. The extortionist achieves this by acting
similarly to an agent using tit-for-tat, responding to like with like. However, the extortionist will
occasionally defect even when their partner has been cooperative. Extortionists effectively calculate
the maximum number of defections they can get away with without annihilating the motivation of
their partner to continue cooperating with them. They decide whether to cooperate or defect using a
set of probabilities. The most recent interaction with their partner determines which probability
they select. An example strategy is shown in Figure 8.10. An extortionist’s partner is incentivized to
acquiesce to the extortion since deviating in any way will yield them a lower payoff. However, in
maximizing their own score, they attain an even higher score for the extortionist. An extortionist
thus scores higher than most of its partners in Iterated Prisoner’s Dilemma matches.

8.2. GAME THEORY
433
Figure 8.10: The Extort-2 strategy. [24]
Shown is an extortion strategy called Extort-2, from the point of view of the extortionist. “You” are
the agent using the Extort-2 strategy, and “they” are your partner. As with all extortion strategies,
Extort-2 involves reacting probabilistically to the most recent interaction had with a partner. As
an example, in the previous round, if the extortionist defected, but their partner cooperated, the
extortionist will cooperate with a probability of 1
3 in this round.
Extortion strategies rarely win tournaments but seldom die out altogether.
As we saw in
Section 8.2.4, many uncooperative strategies may gain a higher score than most of their partners in
head-to-head matches, and yet still lose in tournaments. By contrast, extortionists can be somewhat
successful in tournaments under certain conditions. Extortionists are vulnerable to the same problem
as many other uncooperative strategies: they gain low payoffs in matches against other extortionists.
Each will therefore perform less well as the frequency of extortionists in the population increases.
(This again relates to “frequency dependent selection,” which we discuss later in this chapter.) Thus,
extortion may arise as part of a cycle in large populations of interacting agents, but it is rarely stable.
However, extortionists can persist for longer if they are sufficiently unlikely to meet one another.
For instance, where a sufficiently small population of agents is engaged in a tournament, a single
extortionist can achieve very high payoffs by exploiting cooperative strategies.
Extortion strategies perform even better in evolutionary tournaments [25].
Extortionists
can be highly successful in evolutionary tournaments. This is especially true in tournaments with
more detailed replications of evolutionary dynamics. For instance, in some evolutionary tournaments,
agents can produce offspring who inherit both their strategy and score. With the addition of noise,
in the form of random mutations, extortionists enjoy great success in this context. In even more
sophisticated models that replicate coevolutionary arms races, they are yet more successful. Moreover,
simulating a dramatic environmental change in these models often produces an opportunity for
exceptionally profitable extortion tactics.

434
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
AI agents may use extortion: evidence from the Iterated Prisoner’s Dilemma.
AI agents
could use extortion in order to gain resources or power. As we have seen, agents can succeed in the
Iterated Prisoner’s Dilemma by using extortion strategies. This is particularly true if the extortionist
is part of a small group, if the social dynamics mirror evolution, or after major environmental changes.
These findings are extremely worrying as they could describe future AI scenarios. Relationships might
form among a small number of powerful AI agents. These agents may undulate through desirable
and undesirable behaviors, or they might switch opportunistically to using extortion tactics in the
wake of changes to their environment. However, since there are some fragile assumptions in these
simple models, we must also consider evidence from real-world agents.
AI agents may use extortion: evidence from the real world.
The widespread use of extortion
among humans outside the world of game theoretic models suggests there is still a major cause for
concern. Real-world extortion can still yield results even when the target is perfectly aware that
it is taking place. The use of ransomware schemes to extort private individuals and companies
is increasing rapidly. In fact, cybersecurity experts estimate that the annual economic cost of
ransomware activity is in the billions of US dollars. Terrorist organizations such as ISIS rely on
extortion through hostage-taking for a large portion of their total income. The ubiquity of successful
extortion in so many contexts sets a powerful historical precedent for its efficacy.
Tail Risk: Extortion With Digital Minds
Here we examine the possibility of AI agents engaging in extortion to pursue their goals. Though the
probability of AI extortion may be low, the impact could be immense. As an example, we consider
the potential for extortionist AIs to simulate and torture sentient digital minds as leverage.
Real-world extortion is a form of optimized disvalue.
An extortionist chooses to threaten
their target using a personalized source of concern. They optimize their extortion to be prioritized
over their target’s other concerns. Often, the worse the outcome being threatened, the more likely
the target is to acquiesce. This incentivizes extortionists to threaten to bring about extremely
disvaluable scenarios. In order to be effective, extortionist AIs might therefore leverage the threat of
huge amounts of harm—far more than would likely come about incidentally, without design. If the
disvaluable outcome the extortionist has designed for their target is also disvaluable to wider society,
then we will share the potentially enormous costs of any executed threats.
AI extortion could entail torturing vast numbers of sentient digital minds.
Human
extortionists often threaten to inflict excruciating pain or death on those their victim cares about. AI
extortionists might engage in similar behaviors, threatening to induce extreme levels of suffering, but
on a vastly larger scale. This scale could potentially exceed any in human history. This is because
extortionist AIs with greater-than-human technological capabilities might be able to simulate sentient
digital minds. The potential for optimized disvalue in these simulations suggests near-unimaginable
horrors. Vast numbers of digital people in these simulated environments could be subjected to
immeasurably agonizing experiences.
Simulated torture at this scale could make the future more disvaluable than valuable.
Simulations designed for the purpose of extortion would likely be far more disvaluable than simulations
which contain disvalue unintentionally. The simulation’s designer would likely be able to choose what
kinds of objects to simulate, so they could avoid wasting energy simulating non-sentient entities such
as inanimate objects. Moreover, the designer could ensure that these sentient entities experience the
greatest amount of suffering possible for the timespan of the simulation. They might even be able to
simulate minds capable of more disvaluable experiences than have ever existed previously, deliberately
designing the digital entities to be able to suffer as greatly as possible. Put together, a simulation

8.2. GAME THEORY
435
optimized for disvalue could produce several orders of magnitude more disvalue than anything in
history. This would be unprecedented in humanity’s history, and could make a horrifying—even net
negative—future.
AI agents may be superhumanly proficient at wielding extortion.
Future AI agents may
far exceed humans in their ability to wield threats. One reason for this could be that they have
superhuman tactical capabilities, as some do already in competitive games. Superior strategic
intelligence could allow AI agents to conceive and execute far more advanced programs of extortion
than that of which humans are generally capable. A second reason why AI agents may be especially
adept at employing threats is if they have superhuman longevity or goal-preservation capabilities.
With greater timespans available, the action space for extorting targets is larger. Finally, AIs may
have technological capabilities that exceed those of current and historical humans. This could widen
the option space for AI extortion still further.
Extortion may be exceptionally effective against AIs.
Two goals of machine ethics are: 1)
to foster in AI an intrinsic value for humanity (and humanity’s values); 2) to make AI agents that
are impartial. Both goals could result in AI agents being more vulnerable to extortion than humans
tend to be. Let us examine an example of this for each goal.
Goal 1: Foster in AI an intrinsic value for humanity (and humanity’s values).
AI agents that value individual humans highly may be less prone to “scope insensitivity.” This is the
human bias of failing to “feel” changes in the size of some value appropriately. Very small or very
large numbers often appear to us to be of similar size to other very small or very large numbers,
even when they actually differ by orders of magnitude. Human scope insensitivity may provide
some protection against larger-scale extortion, as it lowers the motivation of extortionists to increase
the scale of their threats. It is possible that AI agents may prioritize outcomes more accurately in
accordance with their expected value. If this is the case, they would likely be more responsive to
high stakes, and more vulnerable to large-scale extortion attempts.
Goal 2: Make AI agents that are impartial.
Impartial AI agents may have far more altruistic values than any human or institution. These agents
may be extremely vulnerable to extortion in the form of threats against their impartial moral codes.
Extortionist AI agents could leverage the threat of extreme torture of countless digital sentients in
simulated environments to extort more morally impartial AI targets. The execution of any such
threat could immensely degrade the value of the future.
AI extortionists may execute higher-stakes threats more frequently than humans.
A
successful act of extortion is the deliberate creation of a state in which both the extortionists and
their targets prefer the outcome the extortionist demands. In some sense, both parties therefore
want the target to acquiesce to the extortion and the extortionist not to follow through on their
threat. In this way, both usually have some incentive to avoid the threat being executed. However,
out of a desire to signal credibility in future interactions, extortionists must follow through on threats
occasionally. Consider examples such as hostage ransoming or criminal syndicate protection rackets.
Successful future extortion requires a signal of commitment, such as destroying the property of those
who defy the extortionists.
AIs may carry out more frequent and more severe threats than humans tend to. One reason for this
is that they may have different value systems which tolerate higher risks, reducing their motivation
to acquiesce to extortion. For example, an AI agent that sufficiently values the far future may prefer
to demotivate future extortionists from trying to extort them. They may therefore defy a current

436
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
extortion attempt, tolerating even very large costs to them and others, for the long-term benefit of
credibly signaling that future extortion attempts will not work either.
More generally, with a greater variety of value systems, a greater number of agents, and a greater
action space size, miscalibrated extortion attempts are more likely. Where the threat is insufficient
to force compliance, the aforementioned need to signal credibility incentivizes the extortionist to
execute their threat as punishment for their target’s refusal to submit.
AI agents extorting humans.
AI agents might also extort human targets. One example scenario
would be an AI developing both a weaponized biological pathogen, and an effective cure. If the
pathogen is slow-acting, the AI agent could then extort humans by deploying the bioweapon, and
leveraging the promise of its cure to force those infected into complying with this demands. Pathogens
that are sufficiently fast to spread and difficult to detect could infect a very large number of human
targets, so this tactic could enable extremely large-scale threats to be wielded effectively [26].
Concealment of information
Honesty is sometimes the best policy in an evolutionary game, but not always. Evolutionary equilibria
may arise in which individual organisms use a mix of deceptive tactics to maximize their fitness.
In other words, an ESS can entail the concealment of information. This section examines how
evolutionary dynamics may pressure AI agents to evolve deceptive behaviors in contexts where these
are more evolutionarily stable than honesty. Note that we examine other risks of deceptive AI in the
Deception section of the Single-Agent Safety chapter.
Concealment of information can be an ESS.
In an evolutionary setting, a certain frequency
or degree of dishonest communication can be selected for [27]. Non-venomous king snakes closely
resemble venomous coral snakes to ward off predators [28]. Cannibalistic Portia spiders pluck the
webs of other spiders to mimic the vibrations of struggling prey, thus luring their victim out into
the open to be killed and eaten [29]. Several flower species mimic the appearance of certain female
insects in order to better attract males for pollination [30]. Some bacteria produce specific molecules
that act as misdirection signals to other competing bacteria, which reduce the probability of attack
and detection, allowing the bacteria to maintain its competitive advantage [31]. Evolutionarily stable
dishonesty is common in the natural world.
Natural selection may favor AIs that conceal information.
Consider the following scenario:
a game of Poker. Agents will be better off if they have a very good ‘poker face’ because other agents
will not be able to deduce tell-tale signs associated with bluffing or a good hand. The incentive
to have a good “poker face” is not malicious: it does not seek to exploit other agents and their
strategies, but rather, to maximize one’s own payoff in a competitive game. We might extend this
reasoning to AI. If multiple AIs operate within a competitive landscape where available resources
are limited, they may be incentivized to conceal information from other AIs or humans in order to
preserve a competitive advantage. In other words, an AI might develop a good ‘poker face’ since
revealing certain kinds of information might affect their abilities to maximize payoff. Thus, strategies
involving the concealment of information could be favored by natural selection among AIs, giving
rise to immoral behaviors that emerge from amoral (morally neutral) grounds.
AI agents may develop immoral behaviors from amoral conditions.
We’ve shown that
dishonest communication can be an ESS. The effects of inter-agent engagement can incentivize
behavior we view as immoral that nonetheless evolves from morally neutral circumstances.

8.3. EVOLUTIONARY PRESSURES
437
Summary
In this section, we began by exploring how evolutionary forces change the multi-agent dynamics
of a simple game like the Iterated Prisoner’s Dilemma. One example was the potential for cyclical
dynamics in place of a stable equilibrium state. We then moved to another effect of evolutionary
dynamics: the potential for a Nash equilibrium in which agents behave very differently to one another,
even when the agents themselves are identical and have identical incentives. As an example, we
explored the “Hawk-Dove game,” finding that neither strategy is dominant over the other. Through
this, we uncovered the force underlying this effect: “frequency-dependent selection.”
Using our understanding of how strategies may vary in terms of their “evolutionary stability,”
we explored why AI agents might develop immoral behaviors even from amoral contexts.
We
examined why a set of strategies known as extortion strategies are especially successful in these
evolutionary tournaments. Extortion not only works among unsophisticated agents involved in
sequential, symmetrical competitions. We also see extortion in real-world scenarios between more
sophisticated entities like individual humans or groups. This suggests that AI extortion could be
a risk despite additional complications like asymmetries in capabilities, knowledge, or dispositions.
Certain traits likely in future AI agents, such as heightened sensitivity to scope, strategic planning
ability, and goal-preservation, could make AI extortion especially effective and destructive. Together,
this suggests that AI agents may engage in extortion in order to pursue their goals. This could
involve optimized disvalue at extreme scales. We discussed one such possibility: the torture of digital
minds.
8.3
Evolutionary Pressures
8.3.1
Overview
This central focus of this chapter concerns the dynamics we expect in a multi-AI agent future. We
must consider the risks that emerge from the interactions between these agents, and between humans
and AI agents.
In the previous section, we began our investigation into multiagent dynamics using the mathematical
framework of game theory. We found that rational agents do not always secure collectively good
outcomes. We concluded with a brief exploration of evolutionary game theory. Using the concepts
of “evolutionarily stable strategies” and “frequency-dependent selection,” we saw how multi-agent
dynamics change when agents can alter and adapt their behavior in response to what others are
doing.
We found that identical agents under initially identical conditions can evolve to select radically
different strategies to one another. Sometimes, there may be no stable equilibrium at all, and a
population of agents will instead undulate through different strategy compositions perpetually. These
more complicated multi-agent dynamics provide the opportunity for behaviors we consider immoral,
such as deception or extortion, to arise from entirely amoral starting conditions.
In this second part of the Collective Action Problems chapter, we use evolutionary theory to build on
these ideas and flesh out the risks posed by the influence of natural selection on AI development. Our
ultimate conclusions are that AI development is likely to be subject to evolutionary forces, and that
we should expect the default outcome of this influence to be the promotion of selfish and undesirable
AI behavior.

438
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
We begin this section by looking at how evolution by natural selection can operate in non-biological
domains, an idea known as “generalized Darwinism.” We formalize this idea using the conditions set
out by Lewontin as necessary and sufficient for natural selection, and Price’s equation for describing
evolutionary change over time. We thus set out the case that evolutionary pressures are influencing
AIs. We turn to the ramifications of this claim in the second section.
We next move onto exploring why evolutionary pressures may promote selfish AI behavior. To
consider what traits and strategies natural selection tends to favor, we begin by setting out the
“information’s eye view” of evolution as a generalized Darwinian extrapolation of the “gene’s eye view”
of biological evolution. Using this framing, we examine how conflict can arise within a system when
the interests of propagating information clash with those of the entity that contains the information.
Internal conflict of this kind could arise within AI systems, distorting or subverting goals even when
they are specified and understood correctly. Finally, we explore why natural selection tends to favor
selfish strategies over altruistic ones. Our upshot is that AI development is likely to be subject to
evolutionary pressures. These pressures may distort the goals we specify if the interests of internal
components of the AI system clash, and could also generate a trend towards increasingly selfish AI
behavior.
8.3.2
Generalized Darwinism
Our aim in this section is to understand generalized Darwinism—the idea that Darwinian mechanisms
are a useful way to explain many phenomena outside of biology [32]—and how we can use this as
a helpful model for modeling AI development. Using examples ranging from science to music, we
examine how evolution by natural selection can operate in non-biological systems. We formalize
this process using the conditions for natural selection and consider how AI development meets these
criteria and is therefore subject to evolutionary pressures.
Conceptual Framework for Generalized Darwinism
Evolution by natural selection is not confined to the domain of biological organisms. We can model
many other phenomena using Darwinian mechanisms. In this section, we use a range of examples to
elaborate this idea.
Generalized Darwinism: natural selection can be applied to non-biological phenomena.
Evolution by natural selection does not depend on mechanisms particular to biology [33]. Darwin
proposed that populations change over the course of generations when differences among individuals
help some reproduce more than others, so that eventually, the population is made up of descendants
of those that reproduced the most. Darwin understood that this idea could explain many other
phenomena. For instance, he suggested that natural selection could explain the evolution of language:
“The survival or preservation of certain favored words in the struggle for existence is natural selection”
[34].
As an example, Richard Dawkins has argued that human culture developed according to the
principles of natural selection [35]. A piece of cultural information, such as a song, is passed down
over generations, often with small changes, and some songs remain very well-known even over very
long time periods. The 18th century French tune “Ah! Vous dirai-je, Maman” was pleasing and easy
to sing, so the young Mozart wrote a version of it, and it was later used as the tune for the English
poem “The Star,” which was sung over and over, until today, when many people know it as “Twinkle
Twinkle,” “The Alphabet Song,” or “Baa Baa Black Sheep” [36]. There were many other 18th century
songs that have long since been forgotten, but that one has spread to many people over centuries,
due to it being more memorable and “catchy” than others of its time.

8.3. EVOLUTIONARY PRESSURES
439
By applying this Darwinian lens, we can describe many non-biological phenomena.
In
nature, evolution happens when individuals have a variety of traits, and individuals with some
traits propagate more than others. If a species of insect can be either red or brown, but the brown
ones blend in better and are less likely to be eaten by birds, then more of the red insects will get
eaten before reproducing, while brown insects will tend to have more descendants. Over time, the
population will consist primarily of brown insects.
We note a similar pattern in other, non-biological domains. For example, alchemy was once a popular
way of explaining the relationships among different metals. People who believed in alchemy taught it
to their students, who taught it to their own students in turn, often with small differences. Over time,
some of those ideas continued to help them explain the world, and others didn’t. In this respect,
chemistry could be viewed as a descendant of alchemy. The ideas that define it now were propagated
when they helped us increase our understanding of the natural world, while others were discarded. In
the same vein, after the first widespread video conferencing services were developed, similar products
proliferated. Users chose the product that best met their needs, selecting for services that were
cheap, easy to use, and reliable. Each company regularly released new versions of its product that
were slightly adapted from earlier ones, and competitors imitated and thereby propagated the best
features and implemented them into their own. Some products incorporated the most adaptive
features quickly, and the descendants of those products are the ones we use today—while others were
quickly outcompeted and fell into obscurity.
Generalized Darwinism does not imply that evolution produces good outcomes.
Often,
things that are the best at propagating are not “good” in any meaningful sense. Invasive species arrive
in a new location, propagate quickly, and local ecosystems begin to crumble. The forms of media
that are most successful at propagating in our minds may be harmful to our happiness and social
relationships. For instance, news articles that get more clicks are likely to have their click-attracting
traits reproduced in the next generation. Clicks thus select for more sensational, emotionally charged
headlines. In the context of AI, generalized Darwinism poses significant risks. To see why, we first
need to understand how many phenomena tend to develop based on Darwinian principles, so that we
can think about how to predict and mitigate these risks.
8.3.3
Formalizing Generalized Darwinism
In this section, we formalize generalized Darwinism. First, we overview the criteria necessary and
sufficient for evolution by natural selection to operate on a system. Second, we examine how we
might predict what happens to a system that meets these conditions. Together, these help us to
see why “survival of the fittest” is a poor description of evolution by natural selection. Instead, this
process would be better described as “propagation of the better-propagated information.”
Lewontin’s three conditions for evolution by natural selection.
The evolutionary biologist
Richard Lewontin formulated three criteria necessary and sufficient for evolution by natural selection
[37]:
1) Variation: There is variation in traits among individuals
2) Retention: Future iterations of individuals tend to resemble previous iterations
3) Differential fitness: Different variants have different propagation rates
The validity of these criteria does not depend on biology. In living organisms, DNA encodes the
variations among individuals. Traits encoded by DNA are heritable, and subject to selection. But

440
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
this is not the only way to fulfill the Lewontin conditions. Video conferencing software has variation
(there are many different options), retention (today’s video conferencing software is similar to last
year’s), and differential fitness (some products are much more widely used and imitated than others).
Precisely how change occurs depends on the specific phenomenon’s mechanism of propagation.
The Price Equation describes how a trait changes in frequency over time.
In the 1970s, the
population geneticist George R. Price derived an equation that provides a mathematical description
of natural selection [38]. One formulation of Price’s equation is given here:
∆¯z = Cov(ω, z) + Ew(∆z).
In this equation, ¯z denotes the average value of some trait z in a population, and ∆¯z is the change
in the average value of z between the parent generation and the offspring generation. If z is height,
and the parent generation is 5 ’5 ” on average and the next generation is 5’ 7” on average, then
∆¯z is 2 inches. ω is relative fitness: how many offspring does an individual have relative to the
average for their generation? Ew(∆z) is the expected value of ∆z: that is, the average change in z
between generations, weighted by fitness, so that individuals who have more offspring are counted
more heavily.
Price’s Equation shows that the change in the average value of some trait between parents and
offspring is equal to the sum of a) the covariance of the trait value and the fitness of the parents,
and b) the fitness-weighted average of the change in the trait between a parent and its offspring.
“Covariance” describes the phenomenon of one variable varying together with another. To see whether
a population will get taller over time, for example, we would need to know the covariance of fitness
with height (do tall individuals have more surviving offspring?) and the difference between a parent’s
height and their average child’s height.
The Price Equation can be applied to non-biological systems.
The Price Equation does
not require any understanding of what causes a trait to be passed down to a subsequent generation
or why some individuals have more offspring than others, only of how much the trait is passed on
and how much it covaries with fitness. The Price Equation would work just as well with car designs
or tunes as with birds or mollusks.
The Price Equation allows us to predict what happens when Lewontin conditions apply.
The Price equation uses differences between members of the parent generation with respect to some
trait z (variation), similarities between parent and offspring generation with respect to z (retention),
and differential fitness (selection). As a result, when we understand the degree to which each of the
Lewontin conditions apply, we can predict how much of some trait will be present in subsequent
generations [39].
First misunderstanding: “fitness” does not describe physical power.
The idea of “fitness”
often brings to mind a contest of physical power, in which the strongest or fastest organism wins,
but this is a misunderstanding. Fitness in an evolutionary sense is not something we gain at the
gym. Being fit may not necessarily entail being exceptionally good at any specific abilities. Sea
sponges, for example, are among the most ancient of animal lineages, and they are not quick, clever,
or good at chasing prey, especially when compared to, say, a shark. But empirically, sea sponges
have been surviving and reproducing for hundreds of millions of years, much more than many species
that would easily beat them in head-to-head contests at almost any other challenge.
Second misunderstanding: “fitness” is not the mechanism driving evolution.
Biologists
often talk about fitness when discussing how well-suited an organism is to its environment. In
particular, they often treat fitness as a short-hand for relative reproductive success: how much an

8.3. EVOLUTIONARY PRESSURES
441
individual contributes to the next generation’s gene pool, relative to their competitors. However,
this usage of the word “fitness” seems to present evolution as being fundamentally tautological. In
the idiom “survival of the fittest,” we appear to be using both “survival” and “fit” to mean relative
reproductive success. This would suggest that evolution is merely the process in which those who
reproduce more successfully, reproduce more successfully! If true, the theory of evolution would seem
to be using its own conclusion to demonstrate its argument. As we shall see next, however, this is
actually false.
“Fitness” is simply a metric we use to measure propagation rate.
In fact, evolutionary
theory does not rely on circular logic. This is because an organism’s fitness does not determine
its reproductive success; natural selection does. Instead, “fitness” is simply the word we use to
describe and measure propagation success. Those who are better at propagating their information
(by surviving and reproducing) don’t have some “being fit” property which causes their success.
Rather, we deem how “fit” they are by measuring how successful they’ve been at propagating their
information. Thus the phrase “survival of the fittest” should really be “propagation of the better-
propagated information.”
The Price Equation, and natural selection more broadly, simply says that if a trait helps individuals
survive longer or reproduce more, and that trait is passed on to the offspring, then more of the
next generation will have that trait. It does not tell us why a trait leads to an individual having
more offspring; it is only a way of expressing the fact that some traits do correlate with having more
offspring. The same is true when natural selection is applied to non-biological systems; “fitness” is
simply a word for the quality of propagating more. The information that propagates best is, of
course, the information that propagates best. But it need not be, and often is not, “better” in any
other sense. Fitness is a metric that describes how much information propagates, not an assessment
of value.
8.3.4
Generalized Darwinism and AI Populations
The three Lewontin conditions, of variation, retention, and differential fitness, are all that is needed
for evolution by natural selection. This means we can assess how natural selection is likely to affect
AI populations by considering how the conditions apply to AIs. Here, we claim that AIs are likely to
meet all three conditions, so we should expect natural selection forces to influence their traits and
development.
Variation: AIs are designed and trained in a variety of ways.
As previously noted in
“Natural Selection Favors AI over Humans” [40],
“When thinking about advanced AI, some have envisioned a single AI that is nearly
omniscient and nearly omnipotent, escaping the lab and suddenly controlling the
world. This scenario tends to assume a rapid, almost overnight, take-off with no prior
proliferation of other AI agents; we would go from AIs roughly similar to the ones
we have now to an AI that has capabilities we can hardly imagine so quickly that we
barely notice anything is changing. However, there could also be many useful AIs, as
is the case now. It is more reasonable to assume that AI agents would progressively
proliferate and become increasingly competent at specific tasks, rather than assume
one AI agent spontaneously goes from incompetent to omnicompetent. This is similar
to the subdivision of biological niches. For example, lions and cheetahs developed
completely different and mutually exclusive strategies to catch prey through strength
or speed. Furthermore, if there are multiple AIs, they can work in parallel rather than
waiting for a single model to get around to a task, making things move much faster.”
[40]

442
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
This means that people are likely to continue creating multiple AI agents, even if there is a single
best model. Financial gains would encourage multiple competitors to challenge the top system [41].
In addition to the argument that AI populations have variation because of the history of their
development, there are also pragmatic arguments for this claim. In evolutionary theory, Fisher’s
fundamental theorem states that the rate of adaptation is directly proportional to the variation
(all else equal). In rapidly-changing environments, where quick adaptation increases a population’s
probability of survival, populations with more variation may persist longer. Consider how variation
in crops reduces the probability of catastrophic crop failure, and variation in investments reduces
the risk of unmanageable financial losses. And in machine learning, an ensemble of AI systems will
often perform more accurately than a single AI [41]. Variation can help guide decision making, in
the same way that many people’s aggregated predictions will usually be better than any one expert’s.
Because of these factors, we are more likely to see a powerful and resilient population of AIs if they
have significant variation.
Variation in AI developers.
As well as variation in the AI systems themselves, we also see
variation between the big technology companies developing and adopting AI technologies. It may
seem simple to prevent the rise of selfish AI behaviors by avoiding their selection. However, the
reality is different. AI companies, directed more by evolutionary pressures than by safety concerns,
are vying for survival in a fiercely competitive landscape. Consider how OpenAI, which started as
a nonprofit dedicated to benefiting humanity, shifted to a capped-profit structure in 2019 due to
funding needs. Consequently, some of its safety-centric members branched out and founded Anthropic,
a company intending to prioritize AI safety. However, even Anthropic couldn’t resist the call of
commercialization, succumbing to evolutionary pressures itself.
Evolutionary pressures are driving safety-minded researchers to adopt the behaviors of their less
safety-minded competitors, because they are anticipating that they can gain a significant fitness
advantage in the short-term by deprioritizing safety. Note that this evolutionary process is not
based on actual selection events (the researchers will not be destroyed if they are outcompeted),
but rather the researchers’ projections of what might happen if they adopt particular strategies. AI
safety ideas are being selected against, which is driving the researchers to change their behavior
(to behave in a less safety-conscious manner). Importantly, as the number of competitors rises, the
variation in approaches and values also increases. This increase in variation escalates the intensity of
the evolutionary pressures and the extent to which these pressures distort the behavior of big AI
companies.
Retention: new AIs are developed under the influence of earlier generations.
Retention
does not require exact copying; it only requires that there be non-zero similarity among individuals
in subsequent generations. In the short term, AIs are developed by adapting older models, or by
imitating features from competitors’ models. Even when training AIs from scratch, retention may
still occur, as highly effective architectures, datasets, and training environments are reused thereby
shaping the agent in a way similar to how humans (or other biological species) are shaped by their
environments. Even if AIs change very rapidly compared to the timescales of biological evolution,
they will still meet the criterion of retention; their generations can be extremely short, so they can
move through many generations in a short time, but each generation will still be similar to the one
before it. Retention is a very easy standard to meet, and even with many uncertainties about what
AIs may be like, it is very likely that they meet this broad definition.
Differential Fitness: some AIs are propagated more than others.
There are many traits
which could cause some AI models or traits to be propagated more than others (increasing their

8.3. EVOLUTIONARY PRESSURES
443
“fitness”). Some of these traits could be highly undesirable to humans. For example, being safer
than alternatives may confer a fitness advantage to an AI. However, merely appearing to be safer
might also improve an AI’s fitness. Similarly, being good at automating human jobs could result in
an AI being propagated more. On the other hand, being easy to deactivate could reduce an AI’s
fitness. Therefore, an AI might increase its fitness by integrating itself into critical infrastructure
or encouraging humans to develop a dependency for it, making us less keen to deactivate it. As
long as some AIs are at least marginally more attractive than others, AI populations will meet the
condition of differential fitness. There are many possible points at which natural selection could take
effect on AIs. These include the actions of AI developers, in fine-tuning and customizing models, or
re-designing training processes.
If the Lewontin conditions are satisfied, we must consider how intense the evolutionary
pressures are.
More intense selection pressure leads to faster change. In a population of birds
in a time with plenty of food, birds with any shape beak may survive. However, if food becomes
scarce, only those with the most efficient beaks for accessing some specific food may survive, and the
next generation will disproportionately have that beak shape. More variation also leads to faster
adaptation, because variants that will be adaptive in a new circumstance are more likely to already
exist in the population. The faster rounds of adaptation occur, the more quickly distinct groups
emerge with their own features.
If there is more intense selection pressure on AIs, where only AIs with certain traits propagate, then
we should expect to see the population optimize around those traits. If there is more variation in the
AI population, that optimization process will be faster. If the rate of adaptation also accelerates,
we would expect trends that lead to greater differentiation in AI populations that are distinct from
the changes in the traits of individual AI models. In the following section, we will discuss the
evolutionary trends that tend to dominate when selection pressure is intense and how they might
shape AI populations.
Summary
We started this section by exploring how evolution by natural selection can occur in non-biological
contexts. We then formalized this idea of “generalized Darwinism” using Lewontin’s conditions and
the Price equation. We found that AI development may be subject to evolutionary pressures by
evaluating how it meets the Lewontin conditions. In the next section, we turn to the ramifications of
this claim.
8.3.5
Levels of Selection and Selfish Behavior
Our aim in this section is to understand which AI characteristics are favored by natural selection.
We explore this by first outlining an “information’s eye view” of evolution by natural selection. Here,
we find that internal conflict can arise where the interests of the propagating information (such as a
gene) clash with those of the larger entity that contains it (such as an organism). This phenomenon
could arise in AI systems, distorting or subverting goals even when human operators have specified
them correctly.
We then move to a second risk generated by natural selection operating at the level of propagating
information: Darwinian forces strongly favor selfish traits over altruistic ones. Although on the level
of an individual organism, individuals may behave altruistically under specific conditions (such as
genetic relatedness), on the level of information, evolution by natural selection tends to produce
selfishness. We conclude by outlining how a multi-agent AI future shaped by natural selection will
be dominated by selfish behavior.

444
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Information’s Eye View
We often consider individual organisms to be the unit on which natural selection is operating.
However, it is their genes that are being propagated through time and space, not the organisms
themselves. This section considers the “gene’s eye view” of evolution by natural selection. We then
use generalized Darwinism to build up an extrapolated version of this perspective we can call the
“information’s eye view” of evolution.
Species succeed when their information propagates, but sometimes interests diverge.
The information of living organisms is primarily contained in DNA. Genes contain the instructions
for forming bodies. Most of the time, a gene propagates most successfully when the organism that
contains it propagates successfully. But sometimes, the best thing for a gene is not the best thing for
the organism. For example, mitochondrial DNA is only passed on from females, so it propagates
most if the organism has only female offspring. In some organisms, mitochondrial DNA gives rise
to genetic mechanisms that increase the production of female descendants. However, if too many
individuals have this mutation, the population will be disproportionately female, and the organism
will be unable to pass on the rest of its genes. In this situation, the most effective propagation
mechanisms for the gene in the mitochondria is harmful to the reproductive success of its host.
The “gene’s eye view” of evolution.
In The Selfish Gene, Richard Dawkins argues that gene
propagation is a more useful framing than organism propagation [35]. In Dawkins’ view, organisms
are simply vehicles that allow genes to propagate. Instead of thinking of birds with long beaks
competing with birds with short beaks, we can think about genes that create long beaks competing
with genes that create short beaks, in a fight for space within the bird population. This gives us
a framework for understanding examples like the one above: the gene within the mitochondria is
competing for space in the population, and will sometimes take that space even at the expense of the
host’s individual fitness.
Information functions similarly to genes, narrowing the space of possibilities.
We are
humans and not dogs, roundworms, or redwood trees almost entirely because of our genes. If we do
not know anything about what an organism is, aside from how long its genome is, then for every
base in the genome, there are four possibilities, so there is an extremely large number of possible
combinations. If we learn that the first base is a G, you have divided the total number by four.
When we decode the entire genome, we have narrowed down an impossibly large space of possibility
to a single one: we can now know not only that the organism is a cat, but even which cat specifically.
In non-biological systems, information works in a parallel way. There are many possible ways to
begin a sentence. Each word eliminates possible endings and decreases the listener’s uncertainty,
until they know the full sentence at the end. Using the framework of information theory, we can
think of information as the resolution or reduction of uncertainty (though this is not a formal
definition). For an idea, information is just the facts about it that make it different from other ideas.
A textbook’s main information is its text. A song’s information consists of the pitches and rhythms
that distinguish it from other songs. These larger phenomena (ideas, books, songs) are distinguished
by the information they contain.
Information that propagates occupies a larger volume of both time and space.
A single
music score, written centuries ago and buried underground ever since, has been propagated across
hundreds of years of time, but very little space. In contrast, a hit tune that is suddenly everywhere
and then quickly forgotten takes up a lot of space, but very little time. But the best propagated
information takes up a large volume of both. The tune for “Twinkle Twinkle” has been taking up
space in many minds, pieces of paper, and digital formats for hundreds of years and continues to

8.3. EVOLUTIONARY PRESSURES
445
propagate. The same is true for genetic information. A gene that flourished briefly hundreds of
millions of years ago, and one that has had a consistent small presence, both take up much less
space-time volume than a gene that long ago became dominant in many successful branches of the
evolutionary tree [33].
Just as some genes propagate more, the same is true for bits of information.
In accordance
with generalized Darwinism, we can extend the gene’s eye view to an “information’s eye view.” A
living organism’s basic unit of information is a gene. Everything that evolves as a consequence of
Darwinian forces contains information, some of which is inherited more than others. Dawkins coined
the term “meme” as an analogue for gene: a meme is the basic unit of cultural inheritance. Like genes,
memes tend to develop variations, and be copied and adapted into new iterations. The philosopher
of science, Karl Popper, wrote that the growth of knowledge is “the natural selection of hypotheses:
our knowledge consists, at every moment, of those hypotheses which have shown their (comparative)
fitness by surviving so far in their struggle for existence.” Social phenomena such as copycat crimes
can also be modeled as examples of memetic inheritance. Many types of crimes are committed daily,
some of which inspire imitators, whose subsequent crimes can themselves be selected for and copied.
Selection operates on the level of individual pieces of information, as well as on the higher level of
organisms and phenomena.
AIs may pass on information in ways analogous to our genetics and cultural memetics.
AIs are computer programs, made of code that determines what they are like, in a similar way to how
our DNA determines what we are like. Different code makes the difference between an agentic AI
and Flappy Bird. Their code, or pieces from it, can be directly copied and adapted for new models.
But their information can also be memetically transmitted, as our cultural memes can. Even today,
AIs are often designed based on hearing about and imitating successful models, not only on copying
code from them. AIs also help create training data for new AIs and evaluate their learning, which
makes the new AIs tend to have traits similar to earlier models. As AIs continue to become more
autonomous, they may be able to imitate and learn from one another, self-modifying to adopt traits
and behaviors that seem useful. The AI information that propagates the most will take up more and
more space-time volume, as it is copied into more AIs that multiply and endure over longer periods.
8.3.6
Intrasystem Goal Conflict
The interests of an organism and its genetic information are usually aligned well. However, they
can sometimes diverge from one another. In this section, we identify analogous, non-biological
phenomena, where conflict arises between a system and the sub-systems in which it stores its
information. Evolutionary pressures might generate internal conflict within AI systems. This could
distort or subvert goals set for AIs by human operators, even when the goal was specified and
understood correctly.
Conflict within genomes.
As we have seen, selection on the level of genes does not always result
in the best outcomes for the organism. As briefly mentioned above, human mitochondrial DNA is
only transferred to offspring through biological females. A human’s mitochondrial genome is identical
to their biological mother’s, assuming no change due to mutation. Thus, mitochondrial genes which
benefit only females may be selected for, even where they incur a cost upon males, as males represent
a heritability deadend. These and other “selfish” genetic elements give rise to intragenomic conflict.
Another example concerns plants in the genus Plantago. These plants have organelles which are
passed on only via females. Some genes in these organelle genomes have evolved to promote male
sterility and thus favor female offspring, against the evolutionary interests of the organism itself.
Intragenomic conflict can be exacerbated by other biological agents [42]. In crustaceans, genetic

446
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
conflict of this kind can be exacerbated by a bacterium known as Wolbachia, which can only be
passed on to future generations by eggs, not sperm. Wolbachia therefore often kills males or drives
them to develop into females [43].
Conflict within organisms.
We observe other kinds of internal conflict within organisms which
do not concern their genomes. For example, the bacterial species that compose the human gut
microbiome can exist in a mutually-beneficial symbiosis with their host. However, some bacteria
reveal themselves to be “opportunistically pathogenic”: in the wake of disruptions like the use of
antibiotics, many of these once—mutualists will propagate at accelerated rates, often at the expense
of the host’s health. As the philosopher of evolutionary biology Samir Okasha notes, “intraorganismic
conflict is relatively common among modern organisms” [44].
Intrasystem conflict in AI companies.
The concept of intrasystem conflict extends beyond
biological examples and can be observed in organizations. A notable example is OpenAI. In 2017, there
was a power struggle in OpenAI, which led to Elon Musk’s exit and Sam Altman becoming OpenAI’s
main leader. In 2020, disagreements within OpenAI led to internal conflict and the departure of some
employees to found Anthropic. In 2023, the board of the nonprofit overseeing OpenAI came into
conflict with Sam Altman and attempted to fire him as CEO. Challenging-to-resolve disagreements
about who should influence AI’s development make intrasystem conflict at AI organizations likely in
the future.
Intrasystem goal conflict: between information and the larger entity that contains it.
Just as in the case of intraorganismic conflict, the interests of propagating information and the
entities that contain the information can diverge from one another. The basic melody of “Twinkle
Twinkle” has been vastly more successful at propagating than the complete French song “Ah vous
dirai-je, Maman,” because the tune itself has become a meme, propagated into many languages with
different lyrics. Morphing into “Baa Baa Black Sheep,” is bad for the propagation of that song,
but excellent for the propagation of its tune, which has changed very little. Intrapsychic conflict is
similarly common. For instance, many people want to improve their diets, but crave unhealthy foods.
Thus, a great many complex systems may have internal conflict, where their functioning is distorted
by the behavior of their component subsystems. We call the more general phenomenon that can
describe all of these examples intrasystem goal conflict: the clash of interests of different agents
within a shared system. Intrasystem goal conflict can arise within complex systems in a range of
domains, including social institutions, human psyches, organisms and genomes.
Intrasystem goal conflict may cause an AI system not to behave as a unified agent.
One
reason why we might expect an AI system not to pursue a specified goal is because intrasystem
goal conflict has eroded its “unity of purpose.” A system has achieved unity of purpose if there is
alignment at all levels of internal organization [44]. Undermining a system’s unity of purpose reduces
its ability to carry out its system-level goals. A helpful analogy here is to consider political “coups.”
A coup is characterized by a struggle for control within a political system, in which agents within the
system act to seize power, often eroding the system’s unity of purpose by disrupting its stability and
functionality. When political leaders are overthrown by their second-in-command, the goals of the
political system usually change. Similarly, if we give an AI agent a goal to pursue, the agent may in
turn assign parts of this goal to sub-agents, who may take over and subvert the original goal with
their own.
Intrasystem goal conflict can arise through modularization.
There are two mechanisms
by which intrasystem goal conflict may arise. The first is modularization: a goal may be broken

8.3. EVOLUTIONARY PRESSURES
447
down into components, each of which thus becomes a sub-goal. For example, consider a multi-agent
system, such as a committee, which wishes to carry out a complicated big-picture goal. In this case,
they may break their goal up into subgoals, and delegate each of these to “working groups” (different
subsets of the committee). Each working group now pursues solely its delegated sub-goal and the
different working groups compete with one another. Ultimately, this modularity enables the different
working groups to grow and change independently of one another, and results in an uneven balance
between the different sub-goals. Thus, the big-picture goal is distorted.
It is worth noting that some cases may differ from this story in terms of their causality. Rather than
a single complex goal being modularized, sometimes individual agents may come together to form
a coalition to pursue a larger-scale goal. Misalignments of interests between these agents can give
rise to intrasystem goal conflict in these cases just as with modularized systemic goals. Examples
include gut bacterial opportunistic pathogenicity, and some inter-sexual and intragenomic conflict.
For instance, intersexual conflict in animals has produced species in which the males deliberately
cause the females physical damage when mating. This strategic harming hinders the female’s ability
to re-mate in future, and incentivizes her to allocate more resources towards raising that particular
male’s genetic offspring. Females have in turn evolved defensive strategies against this, such as
armor plating and spines [45]. Intersexual coevolutionary “arms races” such as these have been
documented in a diverse range of species, from ducks to fruit flies [46]. Rather than a unified goal
being modularized, the goal conflict here has arisen in the context of a coalition. The female and
male are collaborating through sexual reproduction, but even slight misalignments of their interests
can result in dramatic goal conflict.
Intrasystem goal conflict can arise through delegation.
The second mechanism by which
intrasystem goal conflict may arise is in the delegation itself. In the above example, the working groups
each pursue their assigned sub-goal faithfully. The problem is that the collective outcome of these
agencies, each optimizing for their assigned subgoal is a distorted version of the original big-picture
goal. However, the delegation of a goal to another agent can also itself generate intrasystem goal
conflict.
In economics, this is called the principal-agent problem. Put simply, this is the problem of
conflicting priorities that arises when one party (the principal) delegates a goal of theirs to another
(the agent), who is not entirely value-aligned with the principal. The agent has other interests, such
as self-preservation or gain of influence, which compete with the goal the principal has delegated to
them. This can result in the distortion or subversion of the delegated goal: the agent pursues their
own goals as well as (or instead of) that of the principal. Principal-agent problems are most common
where there is a knowledge asymmetry, such that the agent knows more about the relevant matter
than the principal. For example, consider a client who has hired a mechanic to fix their car. The
client wishes their car to be fixed as cost-effectively as possible; the mechanic wishes to maximize
their profits. The principal-agent problem is thus that the client cannot trust the mechanic to report
the cost of the car fix accurately.
Principal-agent problems can generate intrasystem goal conflict wherever the components of a
modularized big-picture goal are delegated to sub-agents. In particular, the sub-agents are likely
to be motivated by self-preservation. If the goal the principal has delegated to them clashes with
their own goal of self-preservation, this can produce strong intrasystem goal conflict. To build on
the committee example above, now consider that each working group is incentivized to compete
with the others to avoid being disbanded. Each exaggerates the importance of their own work, or
reconfigures their assigned objective in order to achieve goals of their own. Ultimately, some even
break away from the committee, to pursue their emergent goals without the constraints imposed by
the committee from which they formed.

448
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Intrasystem goal conflict often arises when a system both modularizes and delegates a complex goal.
For example, the US Food and Drug Administration (FDA) and the Department of Agriculture
(USDA) are sub-agencies of the same government [47]. The US government delegates sub-goals to
each agency to carry out, yet sometimes their interests and actions conflict with one another. In 2010,
the USDA implemented a marketing campaign to promote eating more cheese, to support the dairy
industry. Simultaneously, the FDA implemented a marketing campaign of their own to promote
eating less cheese, as part of an effort to reduce the saturated fat content of American diets. Despite
the fact that both agencies were delegated sub-goals from the same government, their conflicting
objectives resulted in their taking actions that hindered one another.
Intrasystem goal conflict in AI systems.
In the future, humans and AI agents may interact
in many different ways, including by working together on collaborative projects. This provides the
opportunity for goal distortion or subordination through intrasystem goal conflict. For instance,
when humans enlist AI agents to collaborate on tasks, principal-agent problems may arise. Just as
how human collaborators may betray or overturn their principals, AI agents may behave similarly. If
an AI collaborator has a goal of self-preservation, they may try to remove any leverage or power
others have over them. In this way, the system (of principal and agent) that ends up executing
actions based on these conflicting goals will not necessarily be equivalent to how a system with unity
of purpose would pursue the goal set by the humans. The behavior of this emergent multi-agent
system may thus distort our goals, or even subvert them altogether.
8.3.7
Selfishness
In the previous section, we examined one risk generated by natural selection favoring the propagation
of information: conflict between the information (such as genes, departments, or sub-agents) and the
larger entity that contains it (such as an organism, government, or AI system). In this section, we
consider a second risk: that natural selection tends to favor selfish traits and strategies over altruistic
ones. We conclude that the greater the influence of evolutionary pressures on AI development, the
more we should expect a multi-AI agent future to be one dominated by selfish behavior.
Selfishness: furthering one’s own information propagation at the expense of others.
In
evolutionary theory, “selfishness” does not imply intent to harm another, or belief that one’s own
interests ought to dominate. Organisms that do not have malicious intentions often display selfish
traits. The lancet liver fluke, for example, is a small parasite that infects sheep by first infecting ants,
hijacking their brains and making them climb to the top of stalks of grass, where they get eaten by
sheep [48]. The lancet liver fluke does not wish ants ill, nor does it have a belief that lancet liver
flukes should thrive while ants should get eaten. It simply has evolved a behavior that enables it to
propagate its own information at the expense of the ant’s.
Selfishness in AI.
AI systems may exhibit “selfish” behaviors, expanding the AIs’ influence at
the expense of human values. Note that these AIs may not even understand what a human is and
yet still behave selfishly towards them. For example, AIs may automate human tasks, necessitating
extensive layoffs [8]. This could be very detrimental to humans, by generating rapid or widespread
unemployment. However, it could take place without any malicious intent on the part of AIs merely
behaving in accordance with their pursuit of efficiency. AIs may also develop newer AIs that are more
advanced but less interpretable, reducing human oversight. Additionally, some AIs may leverage
emotional connections by imitating sentience or emulating the loved ones of human users. This might
generate social resistance to their deactivation. For instance, AIs that plead not to be deactivated
might stimulate an emotional attachment in some humans. If afforded legal rights, these AIs might

8.3. EVOLUTIONARY PRESSURES
449
adapt and evolve outside human control, becoming deeply embedded in society and expanding their
influence in ways that could be irreversible.
Selfish traits are not the opposite of cooperation.
Many organisms display cooperative
behavior at the individual level. Chimpanzees, for example, regularly groom other members of their
group. They don’t do this to be “nice,” but rather because this behavior is reciprocated in future,
so they are likely to eventually benefit from it themselves [49]. Cells found in filamentous bacteria,
so named because they form chains, regularly kill themselves to provide much needed nitrogen for
the communal thread of bacterial life, with every tenth cell or so “committing suicide” [50]. But
even in these examples, cooperative behavior ultimately helps the individual’s information propagate.
Chimpanzees who groom others expect to have the favor returned in future. Filamentous bacteria
live in colonies made up of their clones, so one bacterium sacrificing itself to save copies of itself
still propagates its information. We cover this in more detail in the next section: Conflict and
Cooperation.
Natural selection tends to produce selfish traits.
Organisms that further their own information
propagation will typically propagate more. A lancet liver fluke that developed the ability to give
ants free choice and allow them not to climb stalks of grass if they don’t want to would be less likely
than the current version to succeed at getting eaten by sheep and continuing its life cycle. Most
biological selfishness is less dramatic, but nonetheless, the organisms alive today are necessarily the
descendants of those that succeeded at propagating their own information, and not of those that
traded propagation for other qualities.
Altruism that reduces an individual’s fitness is not an evolutionarily stable strategy.
Imagine a very altruistic fictional population of foxes who freely share food with one another, even at
great cost to themselves. When food is abundant, they all thrive, and when food is scarce, they suffer
together. If, during a time of scarcity, one fox decides to steal food from the communal stores and
take it for herself and her offspring, they may survive while others starve. As a result, her offspring,
who may have inherited her selfish trait, will make up a higher proportion of the next generation. As
this repeats, the population will be dominated by individuals who take food for themselves when
they can. The population of altruists may get along quite well on its own, but altruism is unstable,
because anyone who decides to exploit it will do better than the group. Since altruism that reduces
an individual’s overall fitness is not an evolutionarily stable strategy, we should expect to see selfish
behavior being promoted.
The more natural selection acts on a population, the more selfish behavior we expect.
In the example in the preceding paragraph, when food is abundant, there is little advantage to
selfishness and there may even be penalties, as the group punishes selfish behavior. There is plenty
of food to go around, so the descendants of foxes who steal food will not be much more likely to
survive, and the next generation can contain plenty of altruists. But in times when only a few can
propagate, selfishness will confer a greater advantage, and the population will tend to become selfish
more quickly.
Avoiding extreme AI selfishness: changing the environment.
AI agents’ fitness could
either be influenced more by natural selection or by the environment. We have sketched out the
default outcome of the former: a landscape of powerful and selfish AI agents. One way we might
prevent this trend towards increasingly selfish behavior is to ensure that it is the environment which
ends up shaping the fitness of AI agents substantially more than natural selection. Currently, we
are in an environment of extreme competition, and so AI agents that are better-suited to this
competitive environment will propagate more, and increase the proportion of the population with

450
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
their traits (including selfish traits). However, if we altered the environment such that the actions of
AI researchers and AI agents were not so heavily steered by competitive pressures, we could reduce
this problem.
Avoiding extreme AI selfishness: changing the selection.
Another possibility is to change
what makes AI agents “fit.” We could establish an ecosystem in which AI agents can be developed,
deployed, and adopted more safely, without the influence of such extreme competitive pressures. In
this ecosystem, we could select against AIs with the most harmful selfish behaviors, and select for
AIs that faithfully assist humans. As these AIs proliferate through this ecosystem, they could then
counteract the worst excesses of selfish behavior from other agents.
Summary
In this section, we considered the effects of evolutionary pressures on AI populations. We started
by using the idea of generalized Darwinism to expand the “gene’s eye view” of biological evolution
to an “information’s eye view.” Using this view, we identified two AI risks generated by natural
selection: intrasystem goal conflict and selfish behavior. Intrasystem goal conflict could distort or
subvert the goals we set an AI system to pursue. Selfish behavior would likely be favored by natural
selection wherever it promotes the propagation of information: If AI development is subject to strong
Darwinian forces, we should expect AIs to tend towards selfish behaviors.
8.4
Conflict and Cooperation
8.4.1
Overview
In this chapter, we have been exploring the risks that arise from interactions between multiple agents.
So far, we have used game theory and evolutionary theory to understand how collective behavior can
produce undesirable outcomes. In simple terms, securing morally good outcomes without cooperation
can be extremely difficult, even for intelligent rational agents. Consequently, the potential for conflict
and the importance of cooperation has emerged as a strong theme in this chapter. In this third
section of this chapter, we examine conflict and cooperation in more detail.
Conflict Overview
We begin this section by exploring the drivers of conflict. Here, we use the term “conflict” loosely,
to describe the decision to defect rather than cooperate in a competitive situation. This may lead
to violence in some cases, but not necessarily all. Our goal is to uncover how, despite being costly,
conflict can sometimes be a rational choice nevertheless.
Microorganisms, humans, states, and nations all cooperate and conflict in different situations. In
nature, we can observe cooperation in the form of social insect behavior, pack hunting, symbiotic
relationships, and much more. In humans, we encounter cooperation in several areas including
coordinated disaster responses, international peace negotiations, and community service, among many
others. By contrast, conflict in both human and non-human organisms can occur as a consequence
of resource competition, territorial disputes, mating access, the maintenance of social dominance
hierarchies, including several other factors. Importantly, the mechanisms and factors that motivate
cooperation and conflict are prevalent in various environments. Thus, we have good reason to suppose
that AI agents will be similarly influenced by these various mechanisms and factors as they decide to
cooperate or conflict with humans and other AIs.

8.4. CONFLICT AND COOPERATION
451
We will begin our discussion of conflict with concepts in bargaining theory. We then examine some
specific features of competitive situations that make it harder to reach negotiated agreements or avoid
confrontation. We begin with five factors from bargaining theory that can influence the potential for
conflict. These can be divided into the following two groups:
Commitment problems.
According to bargaining theory, one reason bargains may fail is that
some of the agents making an agreement may have the ability and incentive to break it. We explore
three examples of commitment problems.
• Power shifts: when there are imbalances between agents’ capabilities such that one agent
becomes stronger than the other, conflict is more likely to emerge between them.
• First-strike advantages: when one agent possesses the element of surprise, the ability to choose
where conflict takes place, or the ability to quickly defeat their opponent, the probability of
conflict increases.
• Issue indivisibility: agents cannot always divide a good however they please – some goods are
“all or nothing” and this increases the probability of conflict between agents.
Information problems.
According to bargaining theory, the other principal cause of a bargaining
failure is that some of the agents may lack good information.
Uncertainty regarding a rival’s
capabilities and intentions can increase the probability of conflict. We explore two information
problems.
• Misinformation: in the real world, agents frequently have incorrect information, which can
cause them to miscalculate suitable bargaining ranges.
• Disinformation: agents may sometimes have incentives to misrepresent the truth intentionally.
Even the expectation of disinformation can make it more difficult to reach a negotiated
settlement.
Factors outside of bargaining theory.
Bargaining frameworks do not encompass all possible
reasons why agents may decide to conflict with one another. We end by exploring one example:
• Inequality: under conditions of inequality, agents may fight for access to a larger share of
available resources or a desired social standing.
Cooperation overview
Next, we turn to cooperation. We observe many forms of cooperation in biological systems: social
insect colonies, pack hunting, symbiotic relationships, and much more. Humans perform community
services, negotiate international peace agreements, and coordinate aid for disaster responses. Our
very societies are built around cooperation.
Cooperation between AI stakeholders may be vital for counteracting the competitive and evolutionary
pressures of AI races we have explored in this chapter. For example, the “merge-and-assist” clause of
OpenAI’s charter [51] outlines their commitment to cease competition with—and provide assistance
to—any “value-aligned, safety-conscious” AI developer who appears close to producing AGI, in order
to reduce the risk of eroding safety precautions. Cooperation between AI agents is also necessary for
reducing some of the multi-agent risks we have looked at: we want AIs to cooperate, rather than
defect, in Prisoner’s Dilemma scenarios.

452
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
However, ensuring that AIs and human agencies behave cooperatively may not be a total solution to
the collective action problems we have examined in this chapter. By more closely examining how
cooperative relationships can come about, it is possible to see how they may backfire with disastrous
consequences for AI safety. Instead, we need a more nuanced view of the potential benefits and risks
of promoting cooperation between AI systems. To do this, we study seven different mechanisms by
which cooperation may arise in multi-agent systems [52], considering the ramifications of each for
cooperation between and within human agencies and AI agents:
• Direct reciprocity: when individuals are likely to encounter others in the future, they are more
likely to cooperate with them.
• Indirect reciprocity: when it benefits an individual’s reputation to cooperate with others, they
are more likely to do so.
• Group selection: when there is competition between groups, cooperative groups may outcompete
non-cooperative groups.
• Kin selection: when an individual is closely related to others, they are more likely to cooperate
with them.
• Individual stakes to common stakes: when individual interests become aligned with the collective
good of the group, individuals are more likely to behave in ways that benefit the whole.
• Simon’s selection mechanism: when available information is limited, individuals may be impelled
to rely on social channels that require cooperation.
• Institutional mechanisms: when there are externally imposed incentives (such as laws) that
subsidize cooperation and punish defection, individuals and groups are more likely to cooperate.
8.4.2
Conflict
Introduction to Conflict
In this section, we use the term “conflict” to describe the decision to defect rather than cooperate in
competitive situations. This often, though not always, involves some form of violence, and destroys
some amount of value. Conflict is common in nature. Organisms engage in conflict to maintain social
dominance hierarchies, to hunt, and to defend territory. People also engage in conflict. Throughout
human history, wars are common, often occurring as a consequence of power-seeking behavior, which
inspired conflict over attempts at aggressive territorial expansion or resource acquisition.
We begin this section by discussing bargaining theory, which lays the groundwork for understanding
why it may be rational for agents to engage in conflict. Next, we turn to the specific factors that
may motivate agents to engage in conflict with one another, even when compromise might be the
better option. We explore why AIs may be similarly affected by these factors, such that they may
view conflict as an instrumentally rational choice in certain contexts.
Conflict can be rational.
Though humans know conflict can be enormously costly, we often still
pursue or instigate it, even when compromise might be the better option.
Consider the following example: a customer trips in a store and sues the owner for negligence. There
is a 60% probability the lawsuit is successful. If they win, the owner has to pay them $40,000, and
going to court will cost each of them $10,000 in legal fees. There are three options: (1) they or
the owner concede, (2) they both let the matter go to court, (3) they both reach an out-of-court
settlement.

8.4. CONFLICT AND COOPERATION
453
(1) If the owner concedes, the owner loses $40,000, and if the customer concedes, they gain nothing.
(2) If both go to court, the owner’s expected payoff is the product of the payment to the customer
and the probability that the lawsuit is successful minus legal fees. In this case, the owner’s
expected payoff would be (−40, 000 × 0.6) −10, 000 while the customer’s expected payoff would
be (40, 000 × 0.6) −10, 000. As a result, the owner loses $34,000 dollars and the customer gains
$14,000 dollars.
(3) An out-of-court settlement x where 14, 000 < x < 34, 000 would enable the customer to get
a higher payoff and the owner to pay lower costs. Therefore, a mutual settlement is the best
option for both if x is in this range.
Hence, if the proposed out-of-court settlement would be greater than $34,000, it would make sense
for the owner to opt for conflict rather than bargaining. Similarly, if the proposed settlement were
less than $14,000, it would be rational for the customer to opt for conflict.
Bargaining Theory
Here, we begin with a general overview of bargaining theory, to illustrate how pressures to outcompete
rivals or preserve power and resources may make conflict an instrumentally rational choice. Next,
we turn to the unitary actor assumption, highlighting that when agents view their rivals as unitary
actors, they assume that they will act more coherently, taking whatever steps necessary to maximize
their welfare. Following this, we discuss the notion of commitment problems, which occur when
agents cannot reliably commit to an agreement or have incentives to break it. Commitment problems
increase the probability of conflict, and are motivated by specific factors, such as power shifts,
first-strike advantages, and issue indivisibility. We then explore how information problems and
inequality can also increase the probability of conflict.
Bargaining theory.
When agents compete for something they both value, they may either
negotiate to reach an agreement peacefully, or resort to more forceful alternatives such as violence.
We call the latter outcome “conflict,” and can view this as the decision to defect rather than cooperate.
Unlike peaceful bargaining, conflict is fundamentally costly for winners and losers alike. However,
it may sometimes be the rational choice. Bargaining theory describes why rational agents may
be unable to reach a peaceful agreement, and instead end up engaging in violent conflict. Due to
pressures to outcompete rivals or preserve their power and resources, agents sometimes prefer conflict,
especially when they cannot reliably predict the outcomes of conflict scenarios. When rational agents
assume that potential rivals have the same mindset, the probability of conflict increases.
The unitary actor assumption.
We tend to assume that a group is a single entity, and that its
leader is only interested in maximizing the overall welfare of the entity. We call this the unitary actor
assumption, which is another name for the “unity of purpose” assumption discussed previously in this
chapter. A nation in disarray without coherent leadership is not necessarily a unitary actor. When
we view groups and individuals as unitary actors, we can assume they will act more coherently, so
they can be more easily modeled as taking steps necessary to maximize their welfare. When parties
make this assumption, they may be less likely to cooperate with others since what is good for one
party’s welfare may not necessarily be good for another’s.
The bargaining range.
Whether or not agents are likely to reach a peaceful agreement through
negotiation will be influenced by whether their bargaining ranges overlap. The bargaining range
represents the set of possible outcomes that both agents involved in a competition find acceptable
through negotiation. Recall the lawsuit example: a bargaining settlement “x” is only acceptable
if it falls between $14,000 and $34,000. Any settlement “x” below $14,000 will be rejected by the

454
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
customer while any settlement “x” above $34,000 will be rejected by the store owner. Thus, the
bargaining range is often depicted as a spectrum with the lowest acceptable outcome for one party at
one end and the highest acceptable outcome for the other party at the opposite end. Within this
range, there is room for negotiation and potential agreements.
Figure 8.11: A) An axis of expected value distribution between two competitors. “B” indicates the
expected outcome of conflict: how likely each competitor is to win, multiplied by the value they gain
by winning. The more positive B is (the further towards the right), the better for Green, and the
worse for Red. B) Conflict is negative-sum: it destroys some value, and so reduces each competitor’s
expected value. C) Bargaining is zero-sum: all the value is distributed between the competitors. This
means there are possible bargains that offer both competitors greater expected value than conflict.
Conflict in AIs.
Let us assume that AI agents will act rationally in the pursuit of their goals
(so, at the least, we model them as unitary actors or as having unity of purpose). In the process
of pursuing and fulfilling their goals, AI agents may encounter potential conflict scenarios, just as
humans do. In certain scenarios, AIs may be motivated to pursue violent conflict over a peaceful
resolution, for the reasons we now explore.
Commitment problems
Many conflicts occur over resources, which are key to an agent’s power. Consider a bargaining failure
in which two agents bargain over resources in an effort to avoid war. If agents were to acquire these
resources, they could invest them into military power. As a result, neither can credibly commit to
use them only for peaceful purposes. This is one instance of a commitment problem [53], which is
when agents cannot reliably commit to an agreement, or when they may even have incentives to
break an agreement. Commitment problems are usually motivated by specific factors, such as power
shifts, first-strike advantages, and issue indivisibility, which may make conflict a rational choice. It is
important to note that our discussion of these commitment problems assumes anarchy: we take for
granted that contracts are not enforceable in the absence of a higher governing authority.

8.4. CONFLICT AND COOPERATION
455
Power Shifts
Power shifts overview.
When there are imbalances between parties’ capabilities such that one
party becomes stronger than the other, power shifts can occur. Such imbalances can arise as a
consequence of several factors including technological and economic advancements, increases in
military capabilities, as well as changes in governance, political ideology, and demographics. In
the context of AIs, power could shift if AIs become more intelligent or have a change in resources.
Individual parties may initially be able to avoid violent conflict by arriving at a peaceful and mutually
beneficial settlement with their rivals. However, if they or their rival’s power increases after this
settlement has been made, the stronger party may end up benefiting from it more than the weaker
party. Thus, we encounter the following commitment problem: the rising power cannot commit not
to exploit their advantage in the future, incentivizing the declining power to opt for conflict in the
present.
Example: The US vs China.
China has been investing heavily in its military. This has included
the acquisition or expansion of its capabilities in technologies such as nuclear and supersonic missiles,
as well as drones. The future is uncertain, but if this trend continues, it could increase the risk of
conflict. If China were to gain a military advantage over the US, this could shift the balance of
power. This possibility undermines the stability of bargains struck today between the US and China,
because China’s expected outcome from conflict may increase in the future if they become more
powerful. The US may expect that agreements made with China about cooperating on AI regulation
could lose enforceability later if there is a significant power shift.
This situation can be modeled using the concept of “Thucydides’ Trap.” The ancient Greek historian
Thucydides suggested that the contemporary conflict between Sparta and Athens might have been
the result of Athens’ increasing military strength, and Sparta’s fear of the looming power shift.
Though this analysis of the Peloponnesian War is now much-contested, this concept can nevertheless
serve to understand how a rising power threatening the position of an existing superpower in the
global order can increase the potential for conflict rather than peaceful bargaining.
Effect on the bargaining range.
Consider two agents, A and B. A is always weaker than B, but
relative to the time period, A is weaker in the future than it is in the present. A will always have
a lower bargaining range, so B will be unlikely to accept any settlements, especially as B’s power
increases. It makes sense for A to prefer conflict, because if it waits, B’s bargaining range will shift
further and further away, eliminating any overlap between the two. Therefore, A prefers to gamble
on conflict even if the probability that A wins is lower than B; the costs of war do not outweigh the
benefits of a peaceful but unreasonable settlement. Consider the 1956 Suez Crisis. Egypt was seen as
a rising power in the Middle East, having secured control over the Suez canal. This threatened the
interests of the British and French governments in the region, who responded by instigating war. To
safeguard their diminishing influence, the British and French launched a swift and initially successful
military intervention.
Power shifts and AI.
AIs could shift power as they gain greater intelligence and more access to
resources. Recall the chapter on Single-Agent Safety, where we saw that an agent’s power is highly
related to the efficiency with which they can exploit resources for their benefit, which often depends
on their level of intelligence. The power of future AI systems is largely unpredictable; we do not
know how intelligent or useful they will be. This could give rise to substantial uncertainty regarding
how powerful potential adversaries using AI might become. If this is the case, there might be reason
to engage in conflict to prevent the possibility of adversaries further increasing their power.

456
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
First-Strike Advantage
First-strike advantage overview.
If an agent has a first-strike advantage, they will do better
to launch an attack than respond to one. This gives rise to the following commitment problem: an
offensive advantage may be short-lived, so it is best to act on it before the enemy does instead. Some
ways in which an agent may have a first-strike advantage include:
1. As explored above, anticipating a future power shift may motivate an attack on the rising
power to prevent it from gaining the upper hand.
2. The costs of conflict might be lower for the attacker than they are for the defender, so the
attacker is better off securing on offensive advantage while the defender is still in a position of
relative weakness.
3. The odds of victory may be higher for whichever agent attacks first. The attacker might possess
the element of surprise, the ability to choose where conflict takes place, or the potential to
quickly defeat their opponent. For instance, a pre-emptive nuclear strike could be used to
target an enemy’s nuclear arsenal, thus diminishing their ability to retaliate.
Examples: IPOs, patent Infringement, and Pearl Harbor.
When a company goes public, it
can release an IPO, allowing members of the general public to purchase company shares. However,
company insiders, such as executives and early investors, often have access to valuable information
not available to the general public; this gives insiders a first-strike advantage. Insiders may buy or
sell shares based on this privileged information, leading to potential regulatory conflicts or disputes
with other investors who do not have access to the same information. Alternatively, when a company
develops a new technology and files a patent application, they gain a first-strike advantage by ensuring
that their product will not be copied or reproduced by other companies. If a rival company does
create a similar technology and later files a patent application, conflict can emerge when the original
company claims patent infringement.
On the international level, we note similar dynamics, such as in the case of Pearl Harbor. Though
Japan and the US were not at war in 1941, their peacetime was destabilized by a commitment
problem: if one nation were to attack the other, they would have an advantage in the ensuing conflict.
The US’ Pacific fleet posed a threat to Japan’s military plans in Southeast Asia. Japan had the
ability to launch a surprise long-range strategic attack. Thus, neither the US nor Japan could credibly
commit not to attack the other. In the end, Japan struck first, bombing the US battleships at the
naval base at Pearl Harbor. The attack was successful in securing a first-strike advantage for Japan,
but it also ensured the US’s entry into WWII.
Defend
Preempt
Defend
2,2
0,3
Preempt
3,0
1,1
Table 8.6: A pay-off matrix for competitors choosing whether to defend or preemptively attack.
Effect on the bargaining range.
When the advantages of striking first outweigh the costs of
conflict, it can shrink or destroy the bargaining range entirely. For any two parties to reach a mutual
settlement through bargaining, each must be willing to freely communicate information with the
other. However, in doing so, each party might have to reveal offensive advantages, which would
increase their vulnerability to attack. The incentive to preserve and therefore conceal an offensive
advantage from opponents’ pressures agents to defect from bargaining.

8.4. CONFLICT AND COOPERATION
457
First-strike advantage and AIs.
One scenario in which an AI may be motivated to secure a
first-strike advantage is cyberwarfare. An AI might hack servers for a variety of reasons to secure an
offensive advantage. AIs may want to disrupt and degrade an adversary’s capabilities by attacking and
destroying critical infrastructure. Alternatively, an AI might gather sensitive information regarding a
rival’s capabilities, vulnerabilities, and strategic plans to leverage potential offensive advantages.
AIs may provide first-strike advantages in other ways, too. Sudden and dramatic progress in AI
capabilities could motivate one party to take offensive action. For example, if a nation very rapidly
develops a much more powerful AI system than its military enemies, this could present a powerful
first-strike advantage: by attacking immediately, they may hope to prevent their rivals from catching
up with them, which would lose them their advantage. Similar incentives were likely at work when the
US was considering a nuclear strike on the USSR to prevent them from developing nuclear weapons
themselves [54].
Reducing the possibility of first-strike advantages is challenging, especially with AI. However, we
can lower the probability that they arise by ensuring that there is a balance between the offensive
and defensive capabilities of potential rivals. In other words, defense dominance can facilitate peace
because attempted attacks between rivals are likely to be unsuccessful or result in mutually assured
destruction. Therefore, we might reduce the probability that AIs are motivated to pursue a first-strike
advantage by ensuring that humans maintain defense dominance, for instance, by requiring that
advanced AIs have a built-in incorruptible fail-safe mechanism, such as a manual “off-switch.”
Figure 8.12: At time T0, Green is more powerful relative to Red, or has a first-strike advantage that
will be lost at T1. At T1, the bargaining range no longer extends past Green’s expected value from
engaging in conflict at T0. Anticipating this leftward shift may incentivize Green to initiate conflict
in the present rather than waiting for the bargaining offers to worsen in the future.
Issue Indivisibility
Issue indivisibility overview.
Settlements that fall within bargaining range will always be
preferable to conflict, but this assumes that whatever issues agents bargain over are divisible. For

458
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
instance, two agents can divide a territory in an infinite amount of ways insofar as the settlement
they arrive at falls within the bargaining range, satisfying both their interests and outweighing the
individual benefits of engaging in conflict. However, some goods are indivisible, which inspires the
following commitment problem [55]: parties cannot always divide a good however they please—some
goods are “all or nothing.” When parties encounter issue indivisibility [53], the probability of conflict
increases. Indivisible issues include monarchies, small territories like islands or holy sites, national
religion or pride, and sovereign entities such as states or human beings, among several others.
Examples: shopping, organ donation, and co-parenting.
Imagine two friends that go out for
a day of shopping. For lunch, they stop at their favorite deli and find that it only has one sandwich
left: they decide to share this sandwich between themselves. After lunch, they go to a clothing store,
and both come across a jacket they love, but of which there is only one left. They begin arguing
over who should get the jacket. Simply put, sandwiches can be shared and jackets can’t. Issue
indivisibility can give rise to conflict, often leaving all parties involved worse off.
The same can be true in more extreme cases, such as organ donation. Typically, the available organ
supply does not meet the transplant needs of all patients. Decisions as to who gets priority for
transplantation may favor certain groups or individuals and allocation systems may be unfair, giving
rise to conflict between doctors, patients, and healthcare administrations. Finally, we can also observe
issue indivisibility in co-parenting contexts. Divorced parents sometimes fight for full custody rights
over their children. This can result in lengthy and costly legal battles that are detrimental to the
family as a whole.
Effect on the bargaining range.
When agents encounter issue indivisibilities, they cannot arrive
at a reasonable settlement through bargaining. Sometimes, however, issue indivisibility can be
resolved through side payments. One case in which side payments were effective was during the
Spanish-American War of 1898, fought between Spain and the United States over the territory of the
Philippines. The conflict was resolved when the United States offered to buy the Philippines from
Spain for 20 million dollars. Conversely, the Munich Agreement at the dawn of WWII represents a
major case where side payments were ineffective. In an attempt to appease Hitler and avoid war, the
British and French governments reached an agreement with Germany, allowing them to annex certain
parts of Czechoslovakia. This agreement involved side payments in the form of territorial concessions
to Germany, but it ultimately failed, as Hitler’s aggressive expansionist ambitions were not satisfied,
leading to the outbreak of World War II. Side payments can only resolve issue indivisibility when the
value of the side payments outweighs the value of the good.
Issue indivisibility and AIs.
Imagine there is a very powerful AI training system, and that
whoever has access to this system will eventually be able to dominate the world. In order to reduce
the chance of being dominated, individual parties may compete with one another to secure access to
this system. If parties were to split the AI’s compute up between themselves, it would no longer
be as powerful as it was previously, perhaps not more powerful than their existing training systems.
Since such an AI cannot be divided up among many stakeholders easily, it may be rational for parties
to conflict over access to it, since doing so ensures global domination.
Information problems
Misinformation and disinformation both involve the spread of false information, but they differ in
terms of intention. Misinformation is the dissemination of false information, without the intention
to deceive, due to a lack of knowledge or understanding. Disinformation, on the other hand, is the

8.4. CONFLICT AND COOPERATION
459
deliberate spreading of false or misleading information with the intent to deceive or manipulate
others. Both of these types of information problem can cause bargains to fail, generating conflict.
Distinguish
Defect
Distinguish
b −c
−c(1 −a)
Defect
b(1 −a)
0
The term a is the probability of a player knowing the strategy of its partner. Relevant for AI since it
might reduce uncertainty (though still chaos and incentives to conceal or misrepresent information or
compete).
Misinformation
Misinformation overview.
Uncertainty regarding a rival’s power or intentions can increase
the probability of conflict[53]. Bargaining often requires placing trust in another not to break
an agreement. This harder to achieve when one agent believes something false about the other’s
preferences, resources, or commitments. This lack of shared, accurate information can lead to mistrust
and a breakdown in negotiations.
Example: Russian invasion of Ukraine.
Incomplete information may lead overly optimistic
parties to make too large demands, whereas rivals that are tougher than expected reject those
demands and instigate conflict. Examples of misinformation problems generating conflict may include
Russia’s 2022 invasion of Ukraine. Russian President Putin reportedly miscalculated Ukraine’s
willingness to resist invasion and fight back. With more accurate information regarding Ukraine’s
abilities and determination, Putin may have been less likely to instigate conflict [56].
Effect on the bargaining range.
Misinformation can prevent agents from finding a mutually-
agreeable bargaining range, as shown in Figure 9.14. For example, if each agent believes themself to
be the more powerful party, each may therefore want more than half the value they are competing
for. Thus, each may reject any bargain offer the other makes, since they expect a better if they opt
for conflict instead.

460
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Figure 8.13: Green either believes themself to be—-or intentionally misrepresents themself as—-more
powerful than they really are. This means that the range of bargain offers Green will choose over
conflict does not overlap with the equivalent range for Red. Thus, there is no mutual bargaining
range.
Misinformation and AI.
AI technologies may produce misinformation directly. Examples of this
include large language models hallucinating false facts. Less directly, AI development could also
exacerbate misinformation problems by increasing uncertainty. For example, military AI applications
may make wars more uncertain, and this may increase the probability of conflict. AI weaponry
innovation presents an opportunity for states to gain power. However, AI capabilities advances are
often highly uncertain—it may be unclear how powerful a model trained on an order of magnitude of
compute would be, or how far behind adversaries are in their effort to create powerful models. As
automated warfare technologies become more widespread and sophisticated, nations may struggle
to predict their probability of victory in any given conflict accurately. This increased potential for
miscalculation may make warfare more likely.
There are other ways reducing information problems can reduce AI risk. If there are substantial
existential risks from AIs but this is not widely agreed on, improving understanding of these risks
could help make different actors (such as the US and China) get better estimates of the payoff
matrix. With better understanding of AI risk, they may recognize that it is in their self-interest
to cooperate (slow down AI development and militarization) instead of defecting (engaging in an
AI race). Similarly, creating information channels such as summits can increase understanding and
coordination; even if countries do not agree on shared commitments, the discussions on the sidelines
can reduce misunderstandings and the risk of conflict.
Disinformation
Disinformation overview.
Unlike misinformation, where false information is propagated without
deceptive intention, disinformation is the deliberate spreading of false information: the intent is to
mislead, deceive or manipulate. Here, we explore why competitive situations may motivate agents to
try to mislead others or misrepresent the truth, and how this can increase the probability of conflict.

8.4. CONFLICT AND COOPERATION
461
Examples: employment and the real estate industry.
Throughout labor markets, employers
and job seekers often encounter disinformation problems. Employers may intentionally withhold
information about the salary range or offer lower wages than what the market standard suggests
in order to secure lower employment costs.
On the other hand, job seekers might exaggerate
their qualifications or professional experience to increase their chances of getting hired.
Such
discrepancies can lead to legal conflicts and high turnover rates. Alternatively, in the real estate
market, disinformation problems can emerge between sellers and buyers. Sellers sometimes withhold
critical information about the property’s condition to increase the probability that the property
gets purchased. Buyers, on the other hand, may be incentivized to misrepresent their budget or
willingness to pay to pressure sellers to lower their price. Oftentimes, this can result in legal battles
or disputes as well as the breakdown of property transactions.
Effect on the bargaining range.
Consider two agents: A, which is stronger, and B, which is
weaker. B demands “X” amount for a bargaining settlement, but A, as the stronger agent, will not
offer this to avoid being exploited by B. In other words, A thinks B is just trying to get more for
themself to “bait” A or “bluff” by implying that the bargaining range is lower. But B might not
be bluffing and A might not be as strong as they think they are. Consider the Sino-Indian war in
this respect. At the time, India had perceived military superiority relative to China. But in 1962,
the Chinese launched an attack on the Himalayan border with India, which demonstrated China’s
superior military capabilities, and triggered the Sino-Indian war. Thus, stronger parties may prefer
conflict if they believe rivals are bluffing. Whereas, weaker parties may prefer conflict if they believe
rivals are not as powerful as they believe themselves to be.
Disinformation and AI.
AIs themselves may have incentives to misrepresent the facts. For
example, the agent “Cicero,” developed by Meta [57], is capable of very high performance in the
board wargame “Diplomacy.” Its success requires it to misrepresent certain information to the other
players in strategic fashion. We have seen many other examples of AIs producing disinformation
for a variety of reasons, such as large language models successfully persuading users that they are
conversing with a human. The ability for AIs to misrepresent information successfully is only likely
to increase in future [58]. This could exacerbate disinformation problems, and thus contribute to
greater risk of conflict by eroding the potential for peaceful negotiation [59].
Factors outside of bargaining theory
Inequality
Inequality is another factor that is highly predictive of conflict.
Crime is a form of conflict.
Income and educational inequality are robust predictors of violent crime [60], with an elasticity in
excess of 0.5 (elasticity measures how sensitive one variable is to changes in another variable) even
when controlling for variables such as race and family composition. Similarly, individuals and families
with a yearly income below $15,000 are three times more likely to be the victims of violent crime
than are individuals and families with a yearly income over $75,000 [61]. Moreover, economists from
the World Bank have also highlighted that the effects of inequality on both violent and property
crime are robust between countries, finding that when economic growth improves in a country, violent
crime rates decrease substantially [62]. This is consistent with evidence at the national level; in the
US, for example, the Bureau of Justice reports that households below the federal poverty level have
a rate of violent victimization that is more than twice as high as the rate for households above the
federal poverty level. Moreover, these effects were largely consistent between both rural and urban
areas where poverty was prevalent, further emphasizing the robust relationship between inequality
and conflict.

462
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Inequality and relative deprivation.
Relative deprivation is the perception or experience of
being deprived or disadvantaged in comparison to others.
It is a subjective measure of social
comparison, not an objective measure of deprivation based on absolute standards. People may feel
relatively deprived when they perceive that others possess more resources, opportunities, or social
status than they do. This can lead to feelings of resentment. For example, “Strain theory,” proposed
by sociologist Robert K. Merton, suggests that individuals experience strain or pressure when they
are unable to achieve socially approved goals through legitimate means. Relative deprivation is a
form of strain, which may lead individuals to resort to various coping mechanisms, one of which
is criminal behavior. For example, communities with a high prevalence of relative deprivation can
evolve a subculture of violence [63]. Consider the emergence of gangs, in which violence becomes
a way to establish dominance, protect territory, and retaliate against rival groups, providing an
alternative path for achieving a desired social standing.
AIs and relative deprivation.
Advanced future AIs and widespread automation may propel
humanity into an age of abundance, where many forms of scarcity have been largely eliminated on the
national, and perhaps even global scale. Under these circumstances, some might argue that conflict
will no longer be an issue; people would have all of their needs met, and the incentives to resort to
aggression would be greatly diminished. However, as previously discussed, relative deprivation is
a subjective measure of social comparison, and therefore, it could persist even under conditions of
abundance.
Consider the notion of a “hedonic treadmill,” which notes that regardless of what good or bad things
happen to people, they consistently return to their baseline level of happiness. For instance, reuniting
with a loved one or winning an important competition might cultivate feelings of joy and excitement.
However, as time passes, these feelings dissipate, and individuals tend to return to the habitual
course of their lives. Even if individuals were to have access to everything they could possibly need,
the satisfaction they gain from having their needs fulfilled is only temporary.
Abundance becomes scarcity reliably. Dissatisfied individuals can be favored by natural selection over
highly content and comfortable individuals. In many circumstances, natural selection could disfavor
individuals who stop caring about acquiring more resources and expanding their influence; natural
selection favors selfish behavior (for more detail, see section 8.3.5 of Evolutionary Pressures). Even
under conditions of abundance, individuals may still compete for resources and influence because
they perceive the situation as a zero-sum game, where resources and power must be divided among
competitors. Individuals that acquire more power and resources could incur a long-term fitness
advantage over those that are “satisfied” with what they already have. Consequently, even with many
resources, conflict over resources could persist in the evolving population.
Relatedly, in economics, the law of markets, also known as “Say’s Law,” proposes that production of
goods and services generates demand for goods and services. In other words, supply creates its own
demand. However, if supply creates demand, the amount of resources required to sustain supply to
meet demand must also increase accordingly. Therefore, steady increases in demand, even under
resource-abundant conditions will reliably result in resource scarcity.
Conflict over social standing and relative power may continue.
There will always be
scarcity of social status and relative power, which people will continue to compete over. Social envy
is a fundamental part of life; it may persist because it tracks differential fitness. Motivated by social
envy, humans establish and identify advantageous traits, such as the ability to network or climb the
social ladder. Scarcity of social status motivates individuals to compete for social standing when
doing so enables access to larger shares of available resources. Although AIs may produce many
forms of abundance, there would still be dimensions on which to compete. Moreover, AI development

8.4. CONFLICT AND COOPERATION
463
could itself exacerbate various forms of inequality to extreme levels. We discuss this possibility in
Chapter 9 Governance; Section 3 - Distribution.
Summary
Throughout this section, we have discussed some of the major factors that drive conflict. When
any one of these factors is present, agents’ incentives to bargain for a peaceful settlement may shift
such that conflict becomes an instrumentally rational choice. These factors include power shifts,
first-strike advantages, issue indivisibility, information problems and incentives to misrepresent, as
well as inequality.
In our discussion of these factors, we have laid the groundwork for understanding the conditions
under which decisions to instigate conflict may be considered instrumentally rational. This knowledge
base allows us to better predict the risks and probability of AI-driven conflict scenarios.
First, we covered how power shifts can incentivize AI agents to pursue conflict, to maintain strategic
advantages or deter potential attacks from stronger rivals, especially in the context of military AI
use.
Second, we explored how the short-lived nature of offensive advantages may incentivize AIs to pursue
first-strike advantages, to degrade or identify vulnerabilities in adversaries’ capabilities, as may be
the case in cyberwarfare.
Third, we discussed issue indivisibility, imagining a future scenario in which individual parties must
compete for access to a world-dominating AI. We reasoned that since dividing this AI between many
stakeholders would reduce its power, parties may find it instrumentally rational to conflict for access
to it.
Fourth, we discussed how AIs may make wars more uncertain, increasing the probability of conflict.
We expect that AI weaponry innovation will present an opportunity for superpowers to consolidate
their dominance, whereas weaker states may be able to quickly increase their power by taking
advantage of these technologies early on. This dynamic may create a future in which power shifts are
uncertain, which may lead states to incorrectly expect that there is something to gain from going to
war.
Finally, we explored the relationship between inequality and conflict. We considered how even under
conditions of abundance facilitated by widespread automation and advanced AI implementation,
relative deprivation, and therefore conflict may persist. We also explored the possibility that AIs
may be motivated by social envy to compete with other humans or AIs for desired social standing.
This may result in a global landscape in which the majority of humanity’s resources are controlled
by selfish, power-seeking AIs.
Though AIs could evolve cooperative tendencies similarly to humans and other animals, the possibility
that they pursue interests and goals that promote conflict, no matter how small, could pose
catastrophic risks to humans.
It is thus important that we understand the drivers of conflict,
especially in the context of advanced future AIs.

464
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
8.4.3
Cooperation
Direct Reciprocity
Direct reciprocity overview.
One way agents may cooperate is through direct reciprocity: when
one agent performs a favor for another because they expect the recipient to return this favor in
the future [64]. We capture this core idea in idioms like “quid pro quo,” or “you scratch my back,
I’ll scratch yours.” Direct reciprocity requires repeated interaction between the agents: the more
likely they are to meet again in the future, the greater the incentive for them to cooperate in the
present. We have already encountered this in the iterated Prisoner’s Dilemma: how an agent behaves
in a present interaction can influence the behavior of others in future interactions . Game theorists
sometimes refer to this phenomenon as the “shadow of the future.” When individuals know that
future cooperation is valuable, they have increased incentives to behave in ways that benefit both
themselves and others, fostering trust, reciprocity, and cooperation over time. Cooperation can
only evolve as a consequence of direct reciprocity when the probability, w, of subsequent encounters
between the same two individuals is greater than the cost-benefit ratio of the helpful act. In other
words, if agent A decided to help agent B at some cost to themselves, they will only do so when the
expected benefit of agent B returning the favor outweighs the cost of Agent A’s initially providing it.
Thus, we have the rule w > c/b; see Table 8.7 below.
Cooperate
Defect
Cooperate
b −c/(1 −w)
−c
Defect
b
0
Table 8.7: Payoff matrix for direct reciprocity games.
Natural examples of direct reciprocity.
Trees and fungi have evolved symbiotic relationships
where they exchange sugars and nutrients for mutual benefit. Dolphins use cooperative hunting
strategies where one dolphin herds schools of fish while the others form barriers to encircle them.
The dynamics of the role reversal are decided by an expectation that other dolphins in the group
will reciprocate this behavior during subsequent hunts. Similarly, chimpanzees engage in reciprocal
grooming, where they exchange grooming services with one another with the expectation that they
will be returned during a later session [49].
Direct reciprocity in human society. Among humans, one prominent example of direct reciprocity
is commerce. Commerce is a form of direct reciprocity “which offers positive-sum benefits for both
parties and gives each a selfish stake in the well-being of the other” [65]; commerce can be a win-win
scenario for all parties involved. For instance, if Alice produces wine and Bob produces cheese,
but neither Alice nor Bob has the resources to produce what the other can, both may realize they
are better off trading. Different parties might both need the good the other has when they can’t
produce it themselves, so it is mutually beneficial for them to trade, especially when they know
they will encounter each other again in the future. If Alice and Bob both rely on each other for
wine and cheese respectively, then they will naturally seek to prevent harm to one another because
it is in their rational best interest. To this point, commerce can foster complex interdependencies
between economies, which enhances the benefits gained through mutual exchange while decreasing
the probability of conflict or war.
Direct reciprocity and AIs.
The future may contain multiple AI agents, many of which might
interact with one another to achieve different functions in human society. Such AI agents may
automate parts of our economy and infrastructures, take over mundane and time-consuming tasks,
or provide humans and other AIs with daily assistance. In a multi-AI agent system, where the

8.4. CONFLICT AND COOPERATION
465
probability that individual AIs would meet again is high, AIs might evolve cooperative behaviors
through direct reciprocity. If one AI in this system has access to important resources that other AIs
need to meet their objectives, it may decide to share these resources accordingly. However, since
providing this favor would be costly to the given AI, it will do so only when the probability of meeting
the recipient AIs (those that received the favor) outweighs the cost-benefit ratio of the favor itself.
Direct reciprocity can backfire: AIs may disfavor cooperation with humans.
AIs may
favor cooperation with other AIs over humans. As AIs become substantially more capable and
efficient than humans, the benefit of interacting with humans may decrease. It may take a human
several hours to reciprocate a favor provided by an AI, whereas it may take an AI only seconds to do
so. It may therefore become extremely difficult to formulate exchanges between AIs and humans that
benefit AIs more than exchanges with other AIs would. In other words, from an AIs perspective, the
cost-benefit ratio for cooperation with humans is not worth it.
Direct reciprocity may backfire: offers of AI cooperation may undermine human alliances.
The potential for direct reciprocity can undermine the stability of other, less straightforward
cooperative arrangements within a larger group, thereby posing a collective action problem. One
example of this involves “bandwagoning.” In the Power section of the Single-Agent Safety chapter, we
discussed the idea of “balancing” in international relations: state action to counteract the influence of
a threatening power, such as by forming alliances with other states against their common adversary
[66]. However, some scholars argue that states do not always respond to threatening powers by trying
to thwart them. Rather than trying to prevent them from becoming too strong, states may instead
“bandwagon”: joining up with and supporting the rising power to gain some personal benefit.
For instance, consider military coups. Sometimes, those attempting a takeover will offer their various
enemies incentives to join forces with them, promising rewards to whoever allies with them first.
If one of those being made this offer believes that the usurpers are ultimately likely to win, they
may consider it to be in their own best interests to switch sides early enough to be on the “right
side of history.” When others observe their allies switching sides, they may see their chances of
victory declining and so in turn decide to defect. In this way, bandwagoning can escalate via positive
feedback.
Bandwagoning may therefore present the following collective action problem: people may be motivated
to cooperate with powerful and threatening AI systems via direct reciprocity, even though it would
be in everyone’s collective best interest if none were to do so. Imagine that a future AI system, acting
autonomously, takes actions that cause a large-scale catastrophe. In the wake of this event, the
international community might agree that it would be in humanity’s best interest to constrain or roll
back all autonomous AIs. Powerful AI systems might then offer some states rewards if they ally with
them (direct reciprocity). This could mean protecting the AIs by simply allowing them to intermingle
with the people, making it harder for outside forces to target the AIs without human casualties. Or
the state could provide the AIs with access to valuable resources. Instead of balancing (cooperating
with the international community to counteract this threatening power), these states may choose to
bandwagon, defecting to form alliances with AIs. Even though the global community would all be
better off if all states were to cooperate and act together to constrain AIs, individual states may
benefit from defecting. As before, each defection would shift the balance of power, motivating others
to defect in turn.
Indirect Reciprocity
Indirect reciprocity overview.
When someone judges whether to provide a favor to someone
else, they may consider the recipient’s reputation. If the recipient is known to be generous, this would

466
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
encourage the donor (the one that provides the favor) to offer their assistance. On the other hand, if
the recipient has a stingy or selfish reputation, this could discourage the donor from offering a favor.
In considering whether to provide a favor, donors may also consider the favor’s effect on their own
reputation. If a donor gains a “helpful and trustworthy” reputation by providing a favor, this may
motivate others to cooperate with them more often. We call this reputation-based mechanism of
cooperation indirect reciprocity [67]. Agents may cooperate to develop and maintain good reputations
since doing so is likely to benefit them in the long-term. Indirect reciprocity is particularly useful in
larger groups, where the probability that the same two agents will encounter one another again is
lower. It provides a mechanism for leveraging collective knowledge to promote cooperation. Where
personal interactions are limited, reputation-based evaluations provide a way to assess the cooperative
tendencies of others. Importantly, cooperation can only emerge within a population as a consequence
of indirect reciprocity when the probability, q, that any agent can discern another agent’s reputation
(whether they are cooperative or not), outweighs the cost-benefit ratio of the helpful behavior to the
donor. Thus, we have the rule q > c/b; see Table 8.4.3 below.
Discern
Defect
Discern
b −c
-c(1-q)
Defect
b(1 −q)
0
Table 8.8: Payoff matrix for indirect reciprocity games.
Natural examples of indirect reciprocity.
Cleaner fish (fish that feed on parasites or mucus
on the bodies of other fish) can either cooperate with client fish (fish that receive the “services” of
cleaner fish) by feeding on parasites that live on their bodies, or cheat, by feeding on the mucus that
client fish excrete [68]. Client fish tend to cooperate more frequently with cleaner fish that have a
“good reputation,” which are those that feed on parasites rather than mucus. Similarly, while vampire
bats are known to share food with their kin, they also share food with unrelated members of their
group. Vampire bats more readily share food with unrelated bats when they know the recipients of
food sharing also have a reputation for being consistent and reliable food donors [69].
Indirect reciprocity in human society.
Language provides a way to obtain information about
others without ever having interacted with them, allowing humans to adjust reputations accordingly
and facilitate conditional cooperation. Consider sites like Yelp and TripAdvisor, which allow internet
users to gauge the reputations of businesses through reviews provided by other consumers. Similarly,
gossip is a complex universal human trait that plays an important role in indirect reciprocity. Through
gossip, individuals reveal the nature of their past interactions with others as well as exchanges they
observe between others but are not a part of. Gossip allows us to track each others’ reputations and
enforce cooperative social norms, reducing the probability that cooperative efforts are exploited by
others with reputations for dishonesty [70].
Indirect reciprocity in AIs.
AIs could develop a reputation system where they observe and
evaluate each others’ behaviors, with each accumulating a reputation score based on their cooperative
actions. AIs with higher reputation scores may be more likely to receive assistance and cooperation
from others, thereby developing a reputation for reliability. Moreover, sharing insights and knowledge
with reliable partners may establish a network of cooperative AIs, promoting future reciprocation.
Indirect reciprocity can backfire: extortionists can threaten reputational damage.
The
pressure to maintain a good reputation can make agents vulnerable to extortion. Other agents may
be able to leverage the fear of reputational harm to extract benefits or force compliance. For example,
political smear campaigns manipulate public opinion by spreading false information or damaging

8.4. CONFLICT AND COOPERATION
467
rumors about opponents. Similarly, blackmail often involves leveraging damaging information about
others to extort benefits. AIs may manipulate or extort humans in order to better pursue their
objectives. For instance, an AI might threaten to expose the sensitive, personal information it has
accessed about a human target unless specific demands are met.
Indirect reciprocity can backfire: ruthless reputations may also work.
Indirect reciprocity
can also favor the emergence of “ruthless” reputations. A reputation for ruthlessness can sometimes
be extremely successful in motivating compliance through fear. For instance, in military contexts,
projecting a reputation for ruthlessness may deter potential adversaries or enemies. If others perceive
an individual or group as willing to employ extreme measures without hesitation, they may be less
likely to challenge or provoke them. Some AIs might similarly evolve ruthless reputations, perhaps
as a defensive strategy to discourage potential attempts at exploitation, or control by others.
Group Selection
Group selection overview.
When there is competition between groups, groups with more
cooperators may outcompete those with fewer cooperators. Under such conditions, selection at the
group level influences selection at the individual level (traits that benefit the group may not necessarily
benefit the individual), and we refer to this mechanism as group selection [71]. Importantly, between
groups, groups with a higher proportion of cooperators have an advantage. Cooperative groups are
better able to coordinate their allocation of resources, establish channels for reciprocal exchange,
and maintain steady communication, making them less likely to go extinct. Moreover, cooperative
groups more frequently split in two: as cooperative groups grow in size, social tensions may emerge
and threaten the cohesion of the group, leading members to break off into their own cooperative
groups. It so happens that, if m is large and is the number of groups and n is the maximum group
size, group selection can only promote cooperation when b/c > 1 + n/m; see Table 8.9 below.
Cooperate
Defect
Cooperate
(n + m)(b + c)
n(−c) + m(b −c)
Defect
nb
0
Table 8.9: Payoff matrix for group selection games.
Natural examples of group selection.
Group selection is widely viewed as a mechanism involved
in the process of natural selection, however, direct causal attribution is difficult for many species.
Nonetheless, we may view chimpanzees that engage in lethal intergroup conflict as a likely example
of group selection. Chimpanzees can be remarkably violent toward outgroups, such as by killing the
offspring of rival males or engaging in brutal fights over territory. Such behaviors can help groups of
chimpanzees secure competitive advantages over other groups of chimpanzees, by either reducing
their abilities to mate successfully through infanticide, or by securing larger portions of available
territory.
Group selection in human society.
Among humans, warfare is a salient example of group
selection. Imagine two armies: A and B. The majority of soldiers in army A are brave, while the
majority of soldiers in army B are cowardly. For soldiers in army A, bravery may be individually costly,
since brave soldiers are more willing to risk losing their lives on the battlefield. For soldiers in army
B, cowardice may be individually beneficial, since cowardly soldiers will take fewer life-threatening
risks on the battlefield. However, at the group-level, when these two armies conflict, army A will
defeat army B: brave soldiers will be more willing to fight alongside each other for victory, while
cowardly soldiers will not.

468
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Group selection in AIs.
Consider a future in which the majority of human labor has been fully
automated by AIs, such that AIs are now running most companies. Under these circumstances,
AIs may form corporations with other AIs, creating an economic landscape in which multiple AI
corporations must compete with each other to produce economic value. AI corporations in which
individual AIs work well together may outcompete those in which individual AIs do not work as well
together. The more cooperative individual AIs within AI corporations are, the more economic value
their corporations will be able to produce; AI corporations with less cooperative AIs may eventually
run out of resources and lose the ability to sustain themselves.
Group selection can backfire: in-group favoritism can promote out-group hostility.
Group selection can inspire in-group favoritism, which might lead to cruelty toward out-groups.
Chimpanzees will readily cooperate with members of their own groups. However, when interacting
with chimpanzees from other groups, they are often vicious and merciless. Moreover, when groups gain
a competitive advantage, they may attempt to preserve it by mistreating, exploiting, or marginalizing
outgroups such as people with different political or ideological beliefs. AIs may be more likely to see
other AIs as part of their group, and this could promote antagonism between AIs and humans.
Kin Selection
Kin selection overview. When driven by kin selection, agents are more likely to cooperate with
others with whom they share a higher degree of genetic relatedness [72]. The more closely related
agents are, the more inclined to cooperate they will be. Thus, kin selection favors cooperation under
the following conditions: an agent will help their relative only when the benefit to their relative “b,”
multiplied by the relatedness between the two “r,” outweighs the cost to the agent “c.” This is known
as Hamilton’s rule: rb > c, or equivalently r > c/b [72]; see Table 8.10 below.
Cooperate
Defect
Cooperate
(b −c)(1 + r)
(−c + br)
Defect
b −rc
0
Table 8.10: Payoff matrix for kin selection games.
Natural examples of kin selection.
In social insect colonies, such as bees and ants, colony
members are closely related. Such insects often assist their kin in raising and producing offspring
while “workers” relinquish their reproductive potential, devoting their lives to foraging and other
means required to sustain the colony as a whole. Similarly, naked mole rats live in colonies with
a single reproductive queen and non-reproductive workers. The workers are sterile but still assist
in tasks such as foraging, nest building, and protecting the colony. This behavior benefits the
queen’s offspring, which are their siblings, and enhances the colony’s overall survival capabilities.
Alternatively, some bird species engage in cooperative breeding practices where older offspring delay
breeding to help parents raise their siblings.
Kin selection in human society.
Some evolutionary psychologists claim that we can see evidence
of kin selection in many commonplace traditions and activities. For example, in humans, we might
identify the mechanism of kin selection in the way that we treat our immediate relatives. For instance,
people often leave wealth, property, and other resources to direct relatives upon their deaths. Leaving
behind an inheritance offers no direct benefit to individuals, but it does help ensure the survival
and success of their lineage in subsequent generations. Similarly, grandparents often care for their
grandchildren, which increases the probability that their lineages will persist.

8.4. CONFLICT AND COOPERATION
469
Kin selection in AIs.
AIs that are similar could exhibit cooperative tendencies towards each other,
similar to genetic relatedness in biological systems. For instance, AIs may create back ups or variants
of themselves. They may then favor cooperation with these versions of themselves over other AIs or
humans. Variant AIs may prioritize resource allocation and sharing among themselves, developing
preferential mechanisms for sharing computational resources with other versions of themselves.
Kin selection can backfire: nepotism.
Kin selection can lead to nepotism: prioritizing the
interests of relatives above others. For instance, some bird species exhibit differential feeding and
provisioning. When chicks hatch asynchronously, parents may allocate more resources to those that
are older, and therefore more likely to be their genetic offspring, since smaller chicks are more likely
to be the result of brood parasitism (when birds lay their eggs in other birds’ nests). In humans, too,
we often encounter nepotism. Company executives may hire their sons or daughters, even though
they lack the experience required for the role, which can harm companies and their employees in
the long-run. Similarly, parents often protect their children from the law, especially when they have
committed serious criminal acts that can result in extended jail time. Such tendencies could apply
to AIs as well: AIs might favor cooperation only with other similar AIs. This could be especially
troubling for humans –— as the differences between humans and AIs increase, AIs may be increasingly
less inclined to cooperate with humans.
Individual stakes to common stakes
Individual stakes to common stakes overview.
The Nobel laureate economist John Harsanyi
suggested a concept like the veil of ignorance to explore how we should structure our societies. Behind
the veil of ignorance, agents are unaware of their personal characteristics and roles in society. This
forces them to act as impartial observers when envisioning how the group or society should interact
with each other. (For a more detailed discussion of the veil of ignorance, see Section Social Contract
Theory of the Normative Ethics chapter: A Brief Introduction). Because agents do not know what
place they will hold (e.g., whether they will get an average position in society or a different position)
in the society they envision, they often act more impartially and cooperatively when constructing a
group or society. When rational agents are ignorant of their future position in society—when they
are forced to make decisions behind the veil of ignorance—they are more likely to make decisions
that maximize collective wellbeing, enabling cooperation through the mechanism of individual stakes
to common stakes [73].
Natural examples of individual stakes to common stakes.
Intragenomic conflict arises when
an individual organism’s genes have differing interests with respect to their transmission to the
next generation. For example, some genes may have evolved mechanisms that increase their own
replication at the expense of the organism’s wellbeing. The process of meiosis, however, can resolve
the problem of intragenomic conflict through randomization or the creation of a “Darwinian veil of
ignorance.” Meiosis is a process of cell division that occurs during the formation of reproductive
cells in sexually reproducing organisms. Meiosis typically results in a 50-50 distribution of genetic
material from both parents to offspring, fostering genetic diversity. While it would be “better” for
any individual gene to make more copies of itself in offspring, it cannot “know” whether doing so will
actually increase the fitness of the organism, allowing it to pass on its genes to subsequent generations.
However, by fostering genetic diversity, the process of meiosis opens the door to randomized success,
allowing the forces of natural selection to impartially dictate which genes and their respective traits
propagate [74].
Individual stakes to common stakes in human society.
The “Hutterites” are a religious group
known for their cooperative and communal lifestyle [34, 75]. They represent a prime example of how
the mechanism of individual stakes to common stakes can be used to promote cooperation. Within

470
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
their communities, the Hutterites distribute all resources among themselves, and relinquish any
personal wealth or possessions. This allows the Hutterites to live under a collective identity, where
focus on the common good often overrides individual interests. As another example, when their
group gets too large, Hutterites set up another place for people to live, and once it is finished, they
randomly select members from their community to go live there. Alternatively, consider a scenario in
which a group of people must row to shore. The ship does not have enough food for everyone, so
the group will have to throw some rowers overboard at the halfway point to ensure the rest of the
group’s survival. However, no one in the group knows that they may be thrown overboard, but they
do know that to make it to shore, they must all row now. If people knew that they would be thrown
overboard at some point, they would be far less likely to row.
Individual stakes to common stakes and AIs.
People might perceive advanced future AIs as
an outgroup threat, and this may motivate humans to cooperate against AIs. As AI’s become more
widespread and powerful, they may begin to threaten human values at the global scale, motivating
nations—for instance, the US and China—to set aside their differences and come together to address
a common threat. If AI is someday viewed by humanity as an “invasive species,” it may promote
global solidarity.
Simon’s Selection Mechanism
Simon’s selection mechanism overview.
Humans and other animals typically do not have
access to all available information, time, and cognitive abilities required to make the best possible
decision. Individuals are restricted by their “bounded rationality.” To overcome their bounded
rationality and arrive at better solutions, people and organisms can benefit, on average, from relying
on and receiving information through cooperative social channels. Individuals that take advantage
of cooperative social channels have an increased ability to acquire socially transmitted skills and
conform to socially established norms, thereby incurring a fitness advantage over those that do not
participate in such channels. Moreover, when individuals choose to contribute to cooperative social
channels, the information they provide can be freely utilized by anyone who participates in the
system. If this information is widely used and benefits the entire social structure, social norms may
emerge that compel others participating in the cooperative social channel to conform their behavior
appropriately. In other words, when individuals participate in cooperative social channels, they may
benefit from the collective intelligence they provide, but they may also face a cost from the group
which can influence their behaviors through norms or rules. This idea was pioneered by the political
scientist and Nobel laureate, Herbert Simon, and we refer to it as Simon’s selection mechanism
[76].
Simon’s selection mechanism in human society.
Humans have a tendency to believe in facts
and propositions that they have not had the opportunity to independently verify. Such knowledge
is typically disseminated through established cooperative social channels, such as the internet or
culture. For instance, many people believe that consuming too much of certain kinds of cholesterol is
bad for one’s health, or that touching a hot stove should always be avoided. People generally agree
upon these beliefs not because they are skilled medical literature reviewers or seasoned chefs, but
because such beliefs have become socially transmitted common knowledge. In general, people are
better off being a part of society, rather than isolating themselves from it. Societal isolation may
prevent individuals from accessing critical information that is disseminated through social channels.
For instance, a hermit living in the woods with no internet access might be unaware that there is
steadily expanding forest fire in the vicinity.
Simon’s selection mechanism and free riding.
One drawback of Simon’s selection mechanism is
that it may enable free riding. Individuals may benefit from the information contained in cooperative

8.4. CONFLICT AND COOPERATION
471
social channels without themselves contributing to it. Consider the open-source knowledge base,
Wikipedia, in this respect. Though anyone can, in theory, contribute to Wikipedia, few people
actually do, because doing so can require extensive time and effort. This can result in a knowledge
base that has much outdated content.
Simon’s selection mechanism and AIs.
In a multi-agent setting, AIs may be interacting with
one another directly. They may create new communication channels and protocols among themselves,
and from their interactions, norms and information channels may emerge. Such dynamics can give
rise to very complex social systems from which AIs are more able to benefit than humans are. In
other words, AIs may incur fitness advantages over humans by developing and participating in their
own cooperative social channels, which themselves may preclude human understanding. Moreover,
such cooperative social channels may evolve into AI collective intelligences, just as the internet now
represents a form of human collective intelligence. However, within these collective intelligences, AIs
could maintain large numbers of complex relationships with other AIs simultaneously, arriving at
potentially new forms of self-organization that increase AIs ability to achieve their goals. AI collective
intelligences would be vastly superior to human collective intelligences, and as a consequence, humans
may be prevented from participating in and understanding the decisions AIs make within such
systems.
A Note on Morality as Cooperation
The theory of “Morality as Cooperation” (MAC) proposes that human morality was generated
by evolutionary pressures to solve our most salient cooperation problems [77]. Natural selection
has discovered several mechanisms by which rational and self-interested agents may cooperate
with one another, and MAC theory suggests that some of these mechanisms have driven the
formation of our moral intuitions and customs. Here, we examine four cooperation problems,
the mechanisms humans have evolved to solve them, and how these mechanisms may have
generated our ideas of morality. These are overviewed in Table 8.11.

472
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Cooperation Problem
Solutions/Mechanism
Component of Morality
Kin selection
Parental duties, family val-
ues
Kinship
Agents can benefit by
treating genetic relatives
preferentially
Avoiding inbreeding
Incest aversion
Forming alliances and col-
laborating
Friendship, loyalty, commit-
ment, team players
Mutualism
Agents must coordinate
their behavior to profit
from mutually-benefical
situations
Developing theory-of-mind
Understanding
intention,
not merely action
Direct reciprocity (e.g. tit-
for-tat)
Trust, gratitude, revenge,
punishment, forgiveness
Exchange
Agents need each other to
reciprocate and contribute
despite incentives to free
ride
Indirect
reciprocity
(e.g.
forming reputations)
Patience, guilt, gratitude
Division
Fairness, negotiation, com-
promise
Conflict resolution
Agents can benefit from
avoiding conflict, which is
mutually costly
Deference to prior owner-
ship
Respecting others’ property,
punishing theft
Table 8.11: Mapping cooperation mechanisms to components of morality. [77]
Kinship.
Natural selection can favor agents who cooperate with their genetic relatives.
This is because there may be copies of these agents’ genes in their relatives’ genomes, and so
helping them may further propagate their own genes. We call this mechanism “kin selection”
[72]: an agent can gain a fitness advantage by treating their genetic relatives preferentially,
so long as the cost-benefit ratio of helping is less than the relatedness between the agent
and their kin. Similarly, repeated inbreeding can reduce an agent’s fitness by increasing the
probability of producing offspring with both copies of any recessive, deleterious alleles in the
parents’ genomes [78].
MAC theory proposes that the solutions to this cooperation problem (preferentially helping
genetic relatives), such as kin selection and inbreeding avoidance, underpin several major
moral ideas and customs. Evidence for this includes the fact that human societies are usually
built around family units [79], in which “family values” are generally considered highly moral.
Loyalty to one’s close relatives and duties to one’s offspring are ubiquitous moral values across
human cultures [80]. Our laws regarding inheritance [81] and our naming traditions [82]
similarly reflect these moral intuitions, as do our rules and social taboos against incest [83,
84].

8.4. CONFLICT AND COOPERATION
473
Mutualism.
In game theory, some games are “positive sum” and “win-win”: the agents
involved can increase the total available value by interacting with one another in particular
ways, and all the agents can then benefit from this additional value. Sometimes, securing these
mutual benefits requires that the agents coordinate their behavior with each other. To solve
this cooperation problem, agents may form alliances and coalitions [85]. This may require the
capacity for basic communication, rule-following [86], and perhaps theory-of-mind [87].
MAC theory proposes that these cooperative mechanisms comprise important components
of human morality.
Examples include the formation of—and loyalty to—friendships,
commitments to collaborative activities, and a certain degree of in-group favoritism and
conformation to local conventions. Similarly, we often consider the agent’s intentions when
judging the morality of their actions, which requires a certain degree of theory-of-mind.
Exchange.
Sometimes, benefiting from “win-win” situations requires more than mere
coordination. If the payoffs are structured so as to incentivize “free riding” behaviors, the
cooperation problem becomes how to ensure that others will reciprocate help and contribute
to group efforts. To solve this problem, agents can enforce cooperation via systems of reward,
punishment, policing, and reciprocity [88]. Direct reciprocity concerns doing someone a favor
out of the expectation that they will reciprocate at a later date [64]. Indirect reciprocity
concerns doing someone a favor to boost your reputation in the group, out of the expectation
that this will increase the probability of a third party helping you in the future [67].
Once again, MAC theory proposes that these mechanisms are found in our moral systems.
Moral ideas such as trust, gratitude, patience, guilt, and forgiveness can all help to assure
against free riding behaviors. Likewise, punishment and revenge, both ideas with strong moral
dimensions, can serve to enforce cooperation more assertively. Idioms such as “an eye for an
eye”, or the “Golden Rule” of treating others as we would like to be treated ourselves, reflect
the solutions we evolved to this cooperation problem.
Conflict resolution.
Conflict is very often “negative sum”: the interaction of the agents
themselves can destroy some amount of the total value available. Examples span from the
wounds of rutting deer to the casualties of human wars. If the agents instead manage to
cooperate with each other, they may both be able to benefit—a “win-win” outcome. One way
to resolve conflict situations is division [89]: dividing up the value between the agents, such
as through striking a bargain. Another solution is to respect prior ownership, deferring to the
original “owner” of the valuable item [90].
According to MAC theory, we can see both of these solutions in our ideas of morality. The
cross-culturally ubiquitous notions of fairness, equality, and compromise help us resolve conflict
by promoting the division of value between competitors [91]. We see this in ideas such as
“taking turns” and “I cut, you choose” [92]: mechanisms for turning a negative sum situation
(conflict) into a zero sum one (negotiation), to mutual benefit. Likewise, condemnation of
theft and respect for others’ property are extremely important and common moral values [80,
93]. This set of moral rules may stem from the conflict resolution mechanism of deferring to
prior ownership.
Conclusion.
MAC theory argues that morality is composed of biological and cultural
solutions humans evolved to the most salient cooperation problems of our ancestral social

474
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
environment. Here, we explored four examples of cooperation problems, and how the solutions
to them discovered by natural selection may have produced our moral values.
Institutions
Institutions overview.
Agents are more likely to be cooperative when there are laws or externally
imposed incentives that reward cooperation and punish defection. We define an institution as an
intentionally designed large-scale structure that is publicly accepted and recognized, has a centralized
logic, and serves to mediate human interaction. Some examples of institutions include governments,
the UN, IAEA, and so on; in this section, by “institutions,” we do not mean widespread or standardized
social customs such as the “institution” of marriage. Institutions typically aim to establish collective
goals which require collaboration and engagement from large or diverse groups. Therefore, a possible
way of representing many institutions, such as governments, is with the concept of a “Leviathan”: a
powerful entity that can exert control or influence over other actors in a system.
The Pacifist’s dilemma and social control.
When one’s opponent is potentially aggressive,
pacifism can be irrational. In his book, “The Better Angels of Our Nature,” Steven Pinker refers to
this as the “Pacifist’s dilemma” [65]. In potential conflict scenarios, agents have little to gain and a
lot to lose when they respond to aggression with pacifism; see Table 8.12 below. This dynamic often
inspires rational agents to choose conflict over peace.
Pacifist
Aggressor
Pacifist
Peace + Profit (100 + 5) = 105
Peace + Profit (100 + 5) = 105
Defeat (−100)
Victory (10)
Aggressor
Victory(10)
Defeat(−100)
War(−50)
War(−50)
Table 8.12: Payoff matrix for the Pacifist’s dilemma without a Leviathan. [65]
However, we can shift the interests of agents in this context in favor of peace by introducing a
Leviathan, in the form of a third-party peacekeeping or balancing mission, which establishes an
authoritative presence that maintains order and prevents conflict escalation. Peacekeeping missions
can take several forms, but they often involve the deployment of peacekeeping forces such as military,
police, and civilian personnel. These forces work to deter potential aggressors, enhance security, and
set the stage for peaceful resolutions and negotiations as impartial mediators, usually by penalizing
aggression and rewarding pacifism; see Table 8.13 below.
Pacifist
Aggressor
Pacifist
Peace (5)
Peace (5)
Defeat (−100)
Victory - Penalty (10 −15 = −5)
Aggressor
Victory - Penalty (10 −15 −5)
Defeat (−100)
War - Penalty (−50 −200 = −250)
War - Penalty (−50 −200 = −250)
Table 8.13: Payoff matrix for the Pacifist’s dilemma with a Leviathan. [65]
Institutions in human society.
Institutions play a central role in promoting cooperation in
international relations. Institutions, such as the UN, can broker agreements or treaties between nations
and across cultures through balancing and peacekeeping operations. The goal of such operations is

8.4. CONFLICT AND COOPERATION
475
to hold nations accountable on the international scale; when nations break treaties, other nations
may punish them by refusing to cooperate, such as by cutting off trade routes or imposing sanctions
and tariffs. On the other hand, when nations readily adhere to treaties, other nations may reward
them, such as by fostering trade or providing foreign aid. Similarly, institutions can incentivize
cooperation at the national scale by creating laws and regulations that reward cooperative behaviors
and punish non-cooperative ones. For example, many nations attempt to prevent criminal behavior
by leveraging the threat of extended jail-time as a legal deterrent to crime. On the other hand, some
nations incentivize cooperative behaviors through tax breaks, such as those afforded to citizens that
make philanthropic donations or use renewable energy resources like solar power.
Institutions and AI.
Institutions are crucial in the context of international AI development. By
establishing laws and regulations concerning AI development, institutions may be able to reduce
AI races, lowering competitive pressures and the probability that countries cut corners on safety.
Moreover, we may want concrete international agreements on AI development that serve to hold
nations accountable; institutions could play a central role in helping us broker these kinds of
agreements. Ultimately, institutions could improve coordination mechanisms and international
standards for AI development, which would correspondingly improve AI safety.
Recall that institutions can punish defectors. In the future, harmful AIs or harmful malicious actors
could be punished with or by AIs. In the future, humanity alone may not possess the power required
to combat advanced dominance-seeking AIs, and existing laws and regulations may be insufficient if
there is no way to enforce them. Such an AI Leviathan could help regulate other AIs and affect their
evolution, in which selfish AIs are counteracted or domesticated.
How institutions can backfire: corruption, free riding, inefficiency.
Institutions sometimes
fail to achieve the goals they set for themselves, even if they are well-intended. Failure to achieve
such goals is often the result of corruption, free riding, and inefficiency at the institutional scale.
Some examples of corruption include bribery, misappropriation of public funds for private interests,
voter fraud and manipulation, and price fixing, among many others. Examples of free-riding include
scenarios like welfare fraud, where individuals fraudulently receive benefits they may not be entitled
to, reducing the available supply of resources for those genuinely in need. Institutions can also
struggle with inefficiency, which may stem from factors such as the satisfaction of bureaucratic
requirements, the emergence of natural monopolies, or the development of diseconomies of scale,
which may cause organizations to pay a higher average cost to produce more goods and services.
Institutions can be undermined, corrupted, and poorly designed or outdated: they do not guarantee
that we will be able to fix cooperation problems.
Like humans, AIs may be motivated to corrupt existing institutions. Advanced AIs might learn to
leverage the institutions we have in place for their benefit, and might do so in ways that are virtually
undetectable to us. Moreover, as we discussed previously, AIs might form an AI Leviathan. However,
if humanity’s relationship with this Leviathan is not symbiotic and transparent, humans risk losing
control of AIs. For instance, if groups of AIs within the Leviathan collude behind the scenes to
further their own interests, or power and resources become concentrated with a few AIs at the “top,”
humanity’s collective wellbeing could be threatened.
Summary
Throughout this section, we discussed a variety of mechanisms that promote cooperation. These
mechanisms included direct reciprocity, indirect reciprocity, group selection, kin selection, individual
stakes to common stakes, Simon’s selection mechanism, and institutions.

476
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
First, we discussed how direct reciprocity may motivate AI agents in a multi-agent setting to cooperate
with each other, if the probability that the same two AIs meet again is sufficiently high. However,
we also considered the possibility that AIs may disfavor cooperation with humans as they become
progressively more advanced: the cost-benefit ratio for cooperation with humans may simply be bad
from an AI’s perspective.
Second, we explored how indirect reciprocity may promote cooperation in AIs that develop a
reputation system where they observe and score each others’ behaviors. AIs with higher reputation
scores may be more likely to receive assistance and cooperation from others. Still, this does not
guarantee that AIs will be cooperative: AIs might leverage the fear of reputational harm to extort
benefits from others, or themselves develop ruthless reputations to inspire cooperation through fear.
Third, we considered how group selection —- in a future where labor has been automated such that
AIs now run the majority of companies —- could promote cooperation on a multi-agent scale. AIs may
form corporate coalitions with other AIs to protect their interests; AI groups with a cooperative AI
minority may be outcompeted by AI groups with a cooperative AI majority. Under such conditions,
however, AIs may learn to favor in-group members and antagonize out-group members, in order to
maintain group solidarity. AIs may be more likely to see other AIs as part of their group, and this
could lead to conflict between AIs and humans.
Fourth, we explored the possibility that AIs may create variants of themselves, and the forces of kin
selection may drive these related variants to cooperate with each other. However, this could also give
rise to nepotism, where AIs prioritize the interests of their variants over other AIs and humans. As
the differences between humans and AIs increase, AIs may be increasingly less inclined to cooperate
with humans.
Fifth, we discussed the mechanism of individual stakes to common stakes. As an example, we explored
the possibility that AIs may someday be viewed as an out-group, an existential threat by humanity.
This may inspire humanity to set aside its differences, promoting global solidarity as a means to
address catastrophic threats posed by AI.
Sixth, we broke down Simon’s selection mechanism, highlighting that to overcome their bounded
rationality, agents may be better off establishing and relying on cooperative social channels. We
extended this idea to AI agents as well, supposing that in a multi-agent setting, AIs may develop their
own communication channels and protocols, which may give them a fitness advantage over humans.
Moreover, we also considered the possibility that AIs might use their cooperative social channels to
build collective intelligences, which, being vastly superior to human collective intelligences, would
preclude humans from taking part in and understanding the decisions AIs make within such systems.
Finally, we concluded with a discussion of how institutions can incentivize cooperation through
externally imposed incentives that enforce cooperation and punish defection [94]. We related this
concept to the idea of an AI Leviathan used to counteract selfish, powerful AIs. However, we also
stressed that humans should take care to ensure their relationship with the AI Leviathan is symbiotic
and transparent, otherwise humans risk losing control of AIs.
In our discussion of these mechanisms, we not only illustrated their prevalence in our world, but
also showed how they might influence cooperation with and between AI agents. In several cases,
the mechanisms we discuss could promote cooperation. However, no single mechanism provides a
foolproof method for ensuring cooperation. In the following section, we discuss the nature of conflict,
namely the various factors that may give rise to it. In doing so, we enhance our understanding of
what might motivate conflict in AI, and subsequently, our abilities to predict and address AI-driven

8.5. CONCLUSION
477
conflict scenarios.
8.5
Conclusion
In this chapter, we considered a variety of multi-agent dynamics in biological and social systems. Our
underlying thesis was that these dynamics might produce undesirable outcomes with AI, mirroring
patterns observable in nature and society.
Game theory
We began with a simple game, the Prisoner’s Dilemma, observing how even rational agents may reach
equilibrium states that are detrimental to all. We then proceeded to build upon this. We considered
how the dynamics may change when the game is iterated and involves more than two agents. We
found that uncertainty about the future could foster rational cooperation, though defection remains
the dominant strategy when the number of rounds of the game if fixed and known.
We used these games to model collective action problems in the real world, like anthropogenic
climate change, public health emergency responses, and the failures of democracies. The collective
endeavors of multi-agent systems are often vulnerable to exploitation by free riders. We drew parallels
between these natural dynamics and the development, deployment, and adoption of AI technologies.
In particular, we saw how AI races in corporate and military contexts can exacerbate AI risks,
potentially resulting in catastrophes such as autonomous economies or flash wars. We ended this
section by exploring the emergence of extortion as a strategy that illustrated a grim possibility for
future AI systems: AI extortion could be a source of monumental disvalue, particularly if it were
to involve morally valuable digital minds. Moreover, AI extortion might persist stably throughout
populations of AI agents, which could make it difficult to eradicate, especially if AIs learn to deceive
or manipulate humans to obscure their true intentions.
Evolutionary pressures
We began this section by examining generalized Darwinism: the idea that Darwinian mechanisms
are a useful way to explain many phenomena outside of biology. We explored examples of evolution
by natural selection operating in non-biological domains, from culture, academia, and industry. By
formalizing this idea using Lewinton’s conditions and the Price equation, we saw how AIs and their
development may be subject to Darwinian forces.
We then turned to the ramifications of natural selection operating on AIs. We first looked at what AI
traits or strategies natural selection may tend to favor. Using an information’s eye view of evolution
by natural selection, we found that internal conflict can arise where the interests of the propagating
information clash with those of the larger entity that contains it. Intrasystem goal conflict could
arise in AI systems, distorting or subverting goals even when human operators have specified them
correctly. Moreover, Darwinian forces strongly favor selfish traits over altruistic ones. Although on
the level of an individual organism, individuals may behave altruistically under specific conditions
(such as genetic relatedness), on the level of information, evolution by natural selection tends to
produce selfishness. Thus, we might expect a future shaped by natural selection to be dominated by
selfish behavior.

478
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
Conflict and cooperation
We ended the chapter by closely examining the drivers of conflict and cooperation in the real world,
and exploring how future AI systems may interact with these forces. We began with conflict: using
the framework of bargaining theory to understand why, despite being costly for all involved, rational
agents may sometimes opt for conflict over peaceful bargaining. We illustrated this idea by looking at
how various factors can affect competitive dynamics, including commitment problems (such as power
shifts, first-strike advantages, and issue indivisibility), information problems, and inequality. These
factors may drive AIs to instigate, promote, or exacerbate conflicts, with potentially catastrophic
effects.
We then moved to an investigation of cooperation. Drawing from biological systems and human
societies, we illustrated an array of mechanisms that may promote cooperation between and within
AIs and human agencies. For each mechanism, however, we also highlighted some associated risks.
These risks included nepotism, in-group favoritism, extortion, and the incentives to behave ruthlessly.
Thus, we found that merely ensuring that AIs and humans act cooperatively may not be a total
solution to our collective action problems. Rather, we need a more nuanced view of the potential
benefits and risks of promoting cooperation between AI systems.
Concluding remarks
In summary, this chapter explored various kinds of collective action problems: intelligent agents,
despite acting rationally and in accordance with their own self-interest, can collectively produce
outcomes that none of them wants, even when they could seemingly have achieved preferable
alternative outcomes. Even when we as individuals share similar goals, system-level dynamics can
override our intentions and create undesirable results.
This is insight is of vital importance when envisioning a future with powerful AI systems. AIs,
individual humans, and human agencies will all conduct their actions in light of how others are
behaving and how each expects others to behave in future. The total risk of this multi-agent system
greater the sum of its individual parts. Dynamics between multiple human agencies generate races
in corporate and military settings. Dynamics between multiple AIs may generate evolutionary
pressure for immoral behaviors, particularly selfishness, free-riding, deception, conflict, and extortion.
We cannot address all the risks posed by AI simply by focusing on the outcomes of agents acting
in isolation. The safety of AI systems will not be guaranteed solely by aligning each AI agent
to well-intentioned operators. It is an essential component of ensuring our safety, and a valuable
future, that we consider these multi-agent dynamics carefully. These dynamics represent a common
problem—clashes between individual and collective interests. We must find innovative, system-level
solutions to ensure that the development and interaction of AI agents lead to beneficial outcomes for
all.
8.6
Literature
8.6.1
Recommended Reading
• Thomas C. Schelling. Arms and Influence. Yale University Press, 1966. isbn: 9780300002218.
url: http://www.jstor.org/stable/j.ctt5vm52s (visited on 10/14/2023)
• Martin A. Nowak. “Five Rules for the Evolution of Cooperation”. In: Science 314.5805 (2006),
pp. 1560–1563. doi: 10.1126/science.1133755. eprint: https://www.science.org/doi/

8.6. LITERATURE
479
pdf/10.1126/science.1133755. url: https://www.science.org/doi/abs/10.1126/
science.1133755
• James D. Fearon. “Rationalist Explanations for War”. In: International Organization 49.3
(1995), pp. 379–414. issn: 00208183, 15315088. url: http://www.jstor.org/stable/
2706903 (visited on 10/14/2023)
• Dan Hendrycks. Natural Selection Favors AIs over Humans. 2023. arXiv: 2303.16200 [cs.CY]
• Edward O. Wilson. Sociobiology: The New Synthesis, Twenty-Fifth Anniversary Edition.
Harvard University Press, 2000. isbn: 9780674000896. url: http://www.jstor.org/stable/
j.ctvjnrttd (visited on 10/14/2023)
• C. Boehm. Moral origins: The evolution of virtue, altruism, and shame. Basic Books, 2012
• R. Axelrod and R.M. Axelrod. The Evolution of Cooperation. Basic books. Basic Books, 1984.
isbn: 9780465021215. url: https://books.google.com.au/books?id=NJZBCGbNs98C
8.6.2
References
[1]
Thomas C. Schelling. “Micromotives and Macrobehavior”. In: 1978.
[2]
R. Dawkins. The Blind Watchmaker. Penguin Books Limited, 1986. isbn: 9780141966427. url:
https://books.google.com.au/books?id=-EDHRX3YYwgC.
[3]
E. Warren and A.W. Tyagi. The Two Income Trap: Why Middle-Class Parents are Going Broke. Basic
Books, 2004. isbn: 9780465090907. url: https://books.google.com.au/books?id=TmXhGJ0tg58C.
[4]
Scott Alexander. Meditations on Moloch. 2014. url: https://slatestarcodex.com/2014/07/30/
meditations-on-moloch/ (visited on 09/29/2023).
[5]
Steven Kuhn. Prisoner’s Dillema. 1997. url: https://plato.stanford.edu/archives/win2019/
entries/prisoner-dilemma/ (visited on 09/29/2023).
[6]
Derek Parfit. Reasons and Persons. Oxford, GB: Oxford University Press, 1984.
[7]
Robert Axelrod. “More Effective Choice in the Prisoner’s Dilemma”. In: The Journal of Conflict
Resolution 24.3 (1980), pp. 379–403. issn: 00220027, 15528766. url: http://www.jstor.org/stable/
173638 (visited on 09/28/2023).
[8]
Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An Overview of Catastrophic AI Risks.
2023. arXiv: 2306.12001 [cs.CY].
[9]
J. Maynard Smith. “The theory of games and the evolution of animal conflicts”. In: Journal
of Theoretical Biology 47.1 (1974), pp. 209–221. issn: 0022-5193. doi: https : / / doi . org / 10 .
1016/0022- 5193(74)90110- 6. url: https://www.sciencedirect.com/science/article/pii/
0022519374901106.
[10]
Noam Nisan et al. Algorithmic Game Theory. Cambridge University Press, 2007. doi: 10.1017/
CBO9780511800481.
[11]
Allan Dafoe. “AI Governance: Overview and Theoretical Lenses”. In: The Oxford Handbook of
AI Governance. Oxford University Press, 2022. isbn: 9780197579329. doi: 10.1093/oxfordhb/
9780197579329.013.2. eprint: https://academic.oup.com/book/0/chapter/408516484/chapter-
ag-pdf/50913724/book\_41989\_section\_408516484.ag.pdf. url: https://doi.org/10.1093/
oxfordhb/9780197579329.013.2.
[12]
Kai-Fu Lee and Chen Qiufan. AI 2041 Ten Visions for Our Future. Currency, 2021.
[13]
John H. Herz. “Idealist Internationalism and the Security Dilemma”. In: World Politics 2.2 (1950),
pp. 157–180. doi: 10.2307/2009187.
[14]
Jared Diamond. Collapse: How Societies Choose to Fail or Succeed: revised edition. Penguin Books
(London), 2005.

480
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
[15]
Andrew Critch. What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs).
2021. url: https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-
looks-like-androbust-agent-agnostic (visited on 09/29/2023).
[16]
Scott Alexander. Ascended Economy? 2016. url: https://slatestarcodex.com/2016/05/30/
ascended-economy/.
[17]
Robert Axelrod and William D. Hamilton. “The Evolution of Cooperation”. In: Science 211.4489
(1981), pp. 1390–1396. doi: 10.1126/science.7466396. eprint: https://www.science.org/doi/pdf/
10.1126/science.7466396. url: https://www.science.org/doi/abs/10.1126/science.7466396.
[18]
Martin Nowak and Karl Sigmund. “Tit-for-Tat in Heterogeneous Populations”. In: Nature 355 (Jan.
1992), pp. 250–253. doi: 10.1038/355250a0.
[19]
Martin A. Nowak. “Five Rules for the Evolution of Cooperation”. In: Science (2006).
[20]
Julián García and Matthijs van Veelen. “No Strategy Can Win in the Repeated Prisoner’s Dilemma:
Linking Game Theory and Computer Simulations”. In: Frontiers in Robotics and AI 5 (Aug. 2018),
p. 102. doi: 10.3389/frobt.2018.00102.
[21]
John Maynard Smith. “Evolution and the Theory of Games”. In: American scientist (1982).
[22]
Richard Dawkins. Evolutionarily Stable Strategies ft. Richard Dawkins. 2015. url: https://www.
youtube.com/watch?v=mUxt--mMjwA&ab_channel=Veritasium.
[23]
William Press and Freeman Dyson. “Iterated Prisoners Dilemma Contains Strategies That Dominate
Any Evolutionary Opponent”. In: Proceedings of the National Academy of Sciences of the United
States of America 109 (May 2012), pp. 10409–13. doi: 10.1073/pnas.1206569109.
[24]
Alexander Stewart and Joshua Plotkin. “Extortion and cooperation in the Prisoner’s Dilemma”. In:
Proceedings of the National Academy of Sciences 109 (June 2012), pp. 10134–10135. doi: 10.1073/
pnas.1208087109.
[25]
Christian Hilbe, Martin Nowak, and Karl Sigmund. “The Evolution of Extortion in Iterated Prisoner’s
Dilemma Games”. In: Proceedings of the National Academy of Sciences of the United States of America
110 (Apr. 2013). doi: 10.1073/pnas.1214834110.
[26]
D. Patel and Carl Shulman. AI Takeover, Bio
Cyber Attacks, Detecting Deception,
Humanity’s
Far Future. Audio Podcast Episode from Dwarkesh Podcast. 2023. url: https://www.youtube.com/
watch?v=KUieFuV1fuo&ab_channel=DwarkeshPatel.
[27]
Steven Hamblin and Peter L. Hurd. “When will evolution lead to deceptive signaling in the Sir Philip
Sidney game?” In: Theoretical Population Biology 75.2 (2009), pp. 176–182. issn: 0040-5809. doi:
https://doi.org/10.1016/j.tpb.2009.02.002. url: https://www.sciencedirect.com/science/
article/pii/S0040580909000227.
[28]
Harry W. Greene and Roy W. McDiarmid. “Coral Snake Mimicry: Does It Occur?” In: Science
213.4513 (1981), pp. 1207–1212. doi: 10.1126/science.213.4513.1207. eprint: https://www.
science.org/doi/pdf/10.1126/science.213.4513.1207. url: https://www.science.org/doi/
abs/10.1126/science.213.4513.1207.
[29]
Robert R. Jackson and R. Stimson Wilcox. “Spider-Eating Spiders: Despite the small size of their
brain, jumping spiders in the genus Portia outwit other spiders with hunting techniques that
include trial and error”. In: American Scientist 86.4 (1998), pp. 350–357. issn: 00030996. url:
http://www.jstor.org/stable/27857059 (visited on 09/28/2023).
[30]
Florian Schiestl. “On the success of a swindle: pollination by deception in orchids”. In: Die
Naturwissenschaften 92 (July 2005), pp. 255–64. doi: 10.1007/s00114-005-0636-y.
[31]
Mikael Mokkonen and Lindstedt Carita. “The evolutionary ecology of deception”. In: Biological
Reviews 91 (June 2015), n/a–n/a. doi: 10.1111/brv.12208.
[32]
Richard Dawkins. Universal Darwinism. Evolution from Molecules to Men. Cambridge University
Press, 1983.
[33]
Lee Smolin. “Did the Universe Evolve?” In: Classical and Quantum Gravity (1992).
[34]
Daniel C. Dennett. “Darwin’s Dangerous Idea: Evolution and the Meanings of Life”. In: 1995.

8.6. LITERATURE
481
[35]
Richard Dawkins and Nicola Davis. The selfish gene. Macat Library, 2006.
[36]
Susan J. Blackmore. The Meme Machine. Oxford University Press, 1999.
[37]
Richard C Lewontin. “The units of selection”. In: Annual review of ecology and systematics (1970).
[38]
George R. Price. “Selection and Covariance”. In: Nature (1970).
[39]
Samir Okasha. “Evolution and the Levels of Selection”. In: 2007.
[40]
Dan Hendrycks. Natural Selection Favors AIs over Humans. 2023. arXiv: 2303.16200 [cs.CY].
[41]
Thomas G. Dietterich. “Ensemble Methods in Machine Learning”. In: Multiple Classifier Systems.
Berlin, Heidelberg: Springer Berlin Heidelberg, 2000, pp. 1–15. isbn: 978-3-540-45014-6.
[42]
Nancy G Leveson. Engineering a safer world: Systems thinking applied to safety. The MIT Press,
2016.
[43]
J. H. Yen and A. R. Barr. “New hypothesis of the cause of cytoplasmic incompatibility in Culex
pipiens L”. In: (1971). doi: 10.1038/232657a0.
[44]
Samir Okasha. “Agents and Goals in Evolution”. In: Oxford Scholarship Online (2018).
[45]
Edward Morrow and Göran Arnqvist. “Costly traumatic insemination and a female counter-adaptation
in bed bugs”. In: Proceedings. Biological sciences / The Royal Society 270 (Dec. 2003), pp. 2377–81.
doi: 10.1098/rspb.2003.2514.
[46]
Locke Rowe and Göran Arnqvist. “Sexually antagonistic coevolution in a mating system: combining
experimental and comparative approaches to address evolutionary processes”. In: Evolution 56.4
(Apr. 2002), pp. 754–767. issn: 0014-3820. doi: 10.1111/j.0014-3820.2002.tb01386.x. eprint:
https://academic.oup.com/evolut/article-pdf/56/4/754/47651033/evolut0754.pdf. url:
https://doi.org/10.1111/j.0014-3820.2002.tb01386.x.
[47]
Michael Moss. While Warning About Fat, U.S. Pushes Cheese Sales. 2010. url: https://www.nytimes.
com/2010/11/07/us/07fat.html (visited on 09/29/2023).
[48]
Daniel Martín-Vega et al. “3D virtual histology at the host/parasite interface: visualisation of the
master manipulator, Dicrocoelium dendriticum, in the brain of its ant host”. In: Scientific reports 8.1
(2018), pp. 1–10.
[49]
Gabriele Schino and Filippo Aureli. “Grooming reciprocation among female primates: A meta-analysis”.
In: Biology letters 4 (Nov. 2007), pp. 9–11. doi: 10.1098/rsbl.2007.0506.
[50]
Christoph Ratzke, Jonas Denk, and Jeff Gore. Ecological suicide in microbes. July 2017. doi:
10.1101/161398.
[51]
OpenAI. OpenAI Charter. 2018. url: https://openai.com/charter.
[52]
Martin A. Nowak. “Five Rules for the Evolution of Cooperation”. In: Science 314.5805 (2006),
pp. 1560–1563. doi: 10.1126/science.1133755. eprint: https://www.science.org/doi/pdf/10.
1126/science.1133755. url: https://www.science.org/doi/abs/10.1126/science.1133755.
[53]
James D. Fearon. “Rationalist Explanations for War”. In: International Organization 49.3 (1995),
pp. 379–414. issn: 00208183, 15315088. url: http://www.jstor.org/stable/2706903 (visited on
10/14/2023).
[54]
Kenneth W Condit et al. “The Joint Chiefs of Staff and National Policy, 1947-1949”. In: (No Title)
(1996).
[55]
Robert Powell. “War as a commitment problem”. In: International organization 60.1 (2006), pp. 169–
203.
[56]
Brian Michael Jenkins. The Will to Fight, Lessons from Ukraine. 2022. url: https://www.rand.org/
pubs/commentary/2022/03/the-will-to-fight-lessons-from-ukraine.html.
[57]
Meta Fundamental AI Research Diplomacy Team (FAIR)† et al. “Human-level play in the game of
<i>Diplomacy</i> by combining language models with strategic reasoning”. In: Science 378.6624
(2022), pp. 1067–1074. doi: 10.1126/science.ade9097. eprint: https://www.science.org/doi/pdf/
10.1126/science.ade9097. url: https://www.science.org/doi/abs/10.1126/science.ade9097.

482
CHAPTER 8. COLLECTIVE ACTION PROBLEMS
[58]
Laurie Chen. “Always there’: the AI chatbot Comforting China’s lonely millions”. In: The Jakarta
Post (2021).
[59]
Matthew Burtell and Thomas Woodside. “Artificial Influence: An Analysis Of AI-Driven Persuasion”.
In: ArXiv abs/2303.08721 (2023).
[60]
Morgan Kelly. “Inequality And Crime”. In: The Review of Economics and Statistics 82 (Feb. 2000),
pp. 530–539. doi: 10.1162/003465300559028.
[61]
Victimization rates for persons age 12 or older, by type of crime and annual family income of victims.
2011. url: https://bjs.ojp.gov/sites/g/files/xyckuh236/files/media/document/cv0814.pdf.
[62]
Pablo Fajnzylber, Daniel Lederman, and Norman Loayza. “Inequality and Violent Crime”. In: The
Journal of Law and Economics 45.1 (2002), pp. 1–39. doi: 10.1086/338347. eprint: https://doi.
org/10.1086/338347. url: https://doi.org/10.1086/338347.
[63]
Adrienne Horne. The Effect Of Relative Deprivation On Delinquency: An Assessment Of Juveniles.
2009.
[64]
Robert Trivers. “The Evolution of Reciprocal Altruism”. In: Quarterly Review of Biology 46 (Mar.
1971), pp. 35–57. doi: 10.1086/406755.
[65]
Steven Pinker. The better angels of our nature: Why violence has declined. Penguin Books, 2012.
[66]
John J. Mearsheimer. Structural realism. 2007, pp. 77–94.
[67]
Martin A Nowak and Karl Sigmund. “Evolution of indirect reciprocity by image scoring”. In: Nature
393.6685 (1998), pp. 573–577.
[68]
Redouan Bshary and Alexandra Sara Grutter. “Image scoring and cooperation in a cleaner fish
mutualism”. In: Nature 441 (2006), pp. 975–978. url: https://api.semanticscholar.org/CorpusID:
4422277.
[69]
Gerald Carter and Gerald Wilkinson. “Food sharing in vampire bats: Reciprocal help predicts donations
more than relatedness or harassment”. In: Proceedings. Biological sciences / The Royal Society 280
(Feb. 2013), p. 20122573. doi: 10.1098/rspb.2012.2573.
[70]
Daniel Balliet, Junhui Wu, and Paul Lange. “Indirect Reciprocity, Gossip, and Reputation-Based
Cooperation”. In: Oct. 2020, pp. 265–287. isbn: 9781462543984.
[71]
S. A. WEST, A. S. GRIFFIN, and A. GARDNER. “Social semantics: altruism, cooperation, mutualism,
strong reciprocity and group selection”. In: Journal of Evolutionary Biology 20.2 (2007), pp. 415–432.
doi: https://doi.org/10.1111/j.1420-9101.2006.01258.x. eprint: https://onlinelibrary.
wiley.com/doi/pdf/10.1111/j.1420-9101.2006.01258.x. url: https://onlinelibrary.wiley.
com/doi/abs/10.1111/j.1420-9101.2006.01258.x.
[72]
W. D. Hamilton. “The genetical evolution of social behaviour. I”. In: Journal of theoretical biology
(1964).
[73]
Martin A Nowak, Roger Highfield, et al. Supercooperators. Canongate Edinburgh, 2011.
[74]
Samir Okasha. “Social justice, genomic justice and the veil of ignorance: Harsanyi meets Mendel”. In:
Economics &amp; Philosophy 28.1 (2012), pp. 43–71. doi: 10.1017/S0266267112000119.
[75]
John C. Harsanyi. “Cardinal Welfare, Individualistic Ethics, and Interpersonal Comparisons of Utility”.
In: Journal of Political Economy (1955).
[76]
Herbert A. Simon. “A Mechanism for Social Selection and Successful Altruism”. In: Science 250.4988
(1990), pp. 1665–1668. doi: 10.1126/science.2270480. eprint: https://www.science.org/doi/pdf/
10.1126/science.2270480. url: https://www.science.org/doi/abs/10.1126/science.2270480.
[77]
Oliver Scott Curry. “Morality as cooperation: A problem-centred approach”. In: The evolution of
morality (2016), pp. 27–51.
[78]
Deborah Charlesworth and John H Willis. “The genetics of inbreeding depression”. In: Nature reviews
genetics 10.11 (2009), pp. 783–796.
[79]
Napoleon A Chagnon. “Kin selection and conflict: An analysis of a Yanomamö ax fight”. In: Evolutionary
biology and human social behavior: An anthropological perspective (1979), pp. 213–238.

8.6. LITERATURE
483
[80]
Edward Westermarck. The origin and development of the moral ideas. DigiCat, 2022.
[81]
Martin S Smith, Bradley J Kish, and Charles B Crawford. “Inheritance of wealth as human kin
investment”. In: Ethology and Sociobiology 8.3 (1987), pp. 171–182.
[82]
Kerris Oates and Margo Wilson. “Nominal kinship cues facilitate altruism”. In: Proceedings of the
Royal Society of London. Series B: Biological Sciences 269.1487 (2002), pp. 105–109.
[83]
Debra Lieberman, John Tooby, and Leda Cosmides. “Does morality have a biological basis? An
empirical test of the factors governing moral sentiments relating to incest”. In: Proceedings of the
Royal Society of London. Series B: Biological Sciences 270.1517 (2003), pp. 819–826.
[84]
Nancy Wilmsen Thornhill. “An evolutionary analysis of rules regulating human inbreeding and
marriage”. In: Behavioral and Brain Sciences 14.2 (1991), pp. 247–261.
[85]
Richard C Connor. “The benefits of mutualism: a conceptual framework”. In: Biological Reviews 70.3
(1995), pp. 427–457.
[86]
Mark Van Vugt, Robert Hogan, and Robert B Kaiser. “Leadership, followership, and evolution: some
lessons from the past.” In: American psychologist 63.3 (2008), p. 182.
[87]
Peter Carruthers and Peter K Smith. Theories of theories of mind. Cambridge university press, 1996.
[88]
Stuart A West, Ashleigh S Griffin, and Andy Gardner. “Evolutionary explanations for cooperation”.
In: Current biology 17.16 (2007), R661–R672.
[89]
John F. Nash. “The Bargaining Problem”. In: Econometrica 18.2 (1950), pp. 155–162. issn: 00129682,
14680262. url: http://www.jstor.org/stable/1907266 (visited on 11/19/2023).
[90]
Herbert Gintis. “The evolution of private property”. In: Journal of Economic Behavior & Organization
64.1 (2007), pp. 1–16.
[91]
Joseph Henrich et al. ““Economic man” in cross-cultural perspective: Behavioral experiments in 15
small-scale societies”. In: Behavioral and brain sciences 28.6 (2005), pp. 795–815.
[92]
Steven J Brams and Alan D Taylor. Fair Division: From cake-cutting to dispute resolution. Cambridge
University Press, 1996.
[93]
Melville Jean Herskovits. Economic anthropology: A study in comparative economics. Knopf New
York, 1952.
[94]
V. Buterin. What even is an institution? 2022. url: https://vitalik.ca/general/2022/12/30/
institutions.html.
[95]
Thomas C. Schelling. Arms and Influence. Yale University Press, 1966. isbn: 9780300002218. url:
http://www.jstor.org/stable/j.ctt5vm52s (visited on 10/14/2023).
[96]
Edward O. Wilson. Sociobiology: The New Synthesis, Twenty-Fifth Anniversary Edition. Harvard
University Press, 2000. isbn: 9780674000896. url: http://www.jstor.org/stable/j.ctvjnrttd
(visited on 10/14/2023).
[97]
C. Boehm. Moral origins: The evolution of virtue, altruism, and shame. Basic Books, 2012.
[98]
R. Axelrod and R.M. Axelrod. The Evolution of Cooperation. Basic books. Basic Books, 1984. isbn:
9780465021215. url: https://books.google.com.au/books?id=NJZBCGbNs98C.

Governance
9.1
Introduction
Safe artificial intelligence requires careful governance.
The development and deployment of
advanced AI systems involves many organizations and individuals with distinct goals and incentives.
These organizations and people can interact in complicated ways, creating a complex sociotechnical
system that we need to govern effectively.
There are a wide variety of issues that governance is required to manage across different stages
of AI development (from data collection through various stages of training to deployment) and
involving various actors such as AI developers, businesses and consumers using AI systems, and
national governments, among others. Thoughtful governance provides the constraints, incentives and
institutions to steer AI progress in a direction that benefits humanity.
Governance refers to the rules and processes that coordinate behavior.
Governance is
not just what governments do. Instead, it can be defined more broadly as the process through which
some activity is organized, coordinated, steered, and managed. It includes the norms, policies, and
institutions that influence stakeholders’ actions to achieve socially desirable outcomes. In healthcare,
for instance, governance aimed at ensuring doctors avoid harming patients for profit includes norms
around patient care, professional ethical standards, and licensing organizations. AI regulators should
aim to encourage safety and responsibility among developers and users to ensure we reap AI’s benefits
while managing the risks.
Governance takes many forms across sectors and levels.
Governance institutions include
governmental bodies that create laws, companies that shape internal rules, and collaborative initiatives
involving both public and private groups. For example, legislatures pass regulations, corporations
adopt ethical guidelines, and public-private forums establish AI safety best practices. Governance
operates at multiple levels as well, such as organizational policies, national laws, and global agreements.
Effective AI governance likely requires a combination of approaches across sectors and levels.
Overview.
We begin by examining the ecosystem in which governance takes place, including the
relevant actors, such as companies, governments, and individuals, and the tools available to govern
them. Then, we explore different scenarios around the speed of growth in AI systems’ capabilities
and their implications. Next, we consider issues around the distribution of power of AI systems,
access to AI, as well as costs, benefits and risks.
We then survey policy tools for AI governance at the corporate, national and international levels.
For corporations, we explore legal structures, ownership models, and assurance mechanisms that
impact AI safety. At the national level, we consider regulations, liability frameworks, resilience,
and competitiveness. For international governance, we examine tools ranging from standards and
certification for civilian AIs to non-proliferation agreements, verification schemes, and monopolies for
military systems. By highlighting levers across multiple levels, this chapter provides an introduction
to a range of governance approaches aimed at ensuring AI is developed safely.
9.1.1
The Landscape
To govern AI, we must understand the system in which AIs are being developed and deployed. Two
crucial aspects for governance are the list of actors and the tools available to govern them.
484

9.1. INTRODUCTION
485
Actors
AI governance involves many diverse groups across sectors that have different goals and can do
different things to accomplish them. Key actors include companies, nonprofits, governments, and
individuals.
Companies develop and deploy AIs, typically for profit.
Major firms such as OpenAI and
Google DeepMind shape the AI landscape through huge investments in research and development,
creating powerful models that advance AI capabilities. Startups may explore new applications of AI
and are often funded by venture capitalists. The Corporate Governance section looks at policies,
incentives, and structures such as ownership models of organizations that impact the development
and deployment of AI systems.
Nonprofits play a variety of roles aimed at improving society.
Some nonprofits aim to
develop safe AI systems. For example, OpenAI was initially founded as a nonprofit. Others engage
in advocacy and coordination by bringing together companies, governments, and research labs to
collaborate on development and regulation. Academic labs such as MILA research AI capabilities,
and some researchers there aim to make AI more beneficial. Some nonprofits perform a mix of these
functions.
National governments make laws, regulations, and standards.
National governments can
directly shape AI development and use within their jurisdictions through legislative and regulatory
powers such as market rules, public investment, and government procurement policies; for instance,
governments might require licensing to develop large models, provide funding for AI research and
development, and require government contractors meet certain safety standards. Governments of
states such as the United States and United Kingdom directly oversee technological development and
commercialization locally through their democratic processes and administrative procedures. We will
explore the role of governments in the National Governance section.
International organizations facilitate cooperation between countries.
Organizations such
as the United Nations, European Union, and OECD have influence across borders by setting policies,
principles, and ethics standards that countries often implement locally. International governance
institutions allow countries to coordinate on issues such as human rights or non-proliferation of
dangerous technologies across national borders. While international governance mechanisms such
as treaties are less enforceable than domestic tools such as laws, they can exert soft power through
financing, expertise, norm-setting, and bringing countries together for dialogue and consensus-building.
We explore these tools in the International Governance section.
Individuals use and are deeply impacted by AI systems.
AIs ultimately have profound
impacts on individuals. Externalities from AIs, such as invasions of privacy, concentration of economic
power, and catastrophic risks, are experienced by individuals, as are many of the potential benefits
such as rapid economic growth. As consumers, individuals buy and use AI products and services,
which means they can apply social pressure to force development along specific routes. As citizens,
they can exert pressure through voting and other forms of democratic representation. Their needs
and perspectives should be a central consideration.
Tools
The AI governance landscape includes the sets of tools or mechanisms by which actors interact
and influence one another. Key tools for AI governance fall into four main categories: information
dissemination, financial incentives, standards and regulations, and rights.

486
CHAPTER 9. GOVERNANCE
Information dissemination changes how stakeholders think and act.
Education and training
transmit technical skills and shape mindsets of researchers, developers, and policymakers. Sharing
data, empirical analyses, policy recommendations, and envisioning positive futures informs discussions
by highlighting opportunities, risks, and policy impacts. Facts are a prerequisite for the creation and
implementation of effective policy. Increasing access to information for individuals and organizations
can change their evaluations of what’s best to do.
Financial incentives influence behavior by changing payoffs and motivations.
Incentives
such as funding sources, market forces, potential taxes or fees, and regulatory actions shape the
priorities and cost-benefit calculations of companies, researchers, and other stakeholders. Access to
funding and well-regulated markets (such as those with IP and competition protections) encourages
technology development and commercialization, while potential taxes or regulatory penalties impose
financial and reputational risks that promote caution and consideration of governance goals. By
shaping incentives, governments can increase the degree to which private companies or other actors’
opportunities and risks are aligned with those of society as a whole.
Standards and regulations set expectations and boundaries for behavior.
There is a
spectrum of rules—-from flexible guidelines to rigid laws—that encompasses many different governance
tools. At one end, the flexible guidelines look like standard operating procedures and industry norms,
which codify preferred practices within and across institutions. At the other end, governments use
formal and enforceable tools including regulations and legislation, which carry penalties for violations
and aim to both address specific risks and shape industries broadly. Well-designed rules establish
which actions are allowed or prohibited for organizations and individuals.
Rights grant freedoms, entitlements, and claims over assets.
Human rights and civil
liberties, such as privacy, free speech, and due process, shape relationships between individuals and
organizations by defining unacceptable behaviors. Property rights and intellectual property regimes
determine ownership and control over assets such as data, algorithms, trained models, and compute.
Outlining rights clearly can establish strong governance regimes that allow litigation against any
infringements.
The diverse actors in AI development and deployment along with the varied governance tools at our
disposal form an intricate ecosystem. Companies, researchers, governments, and individuals influence
progress based on their capabilities and incentives. Financial incentives, established rules, delineated
rights, and information sharing steer beliefs and behaviors. Next, we will consider an issue central
to defining the landscape in which governance will take place: at what rate should we expect the
capabilities of AI systems to progress?
9.2
Takeoff
In this section, we will explore the various ways in which AIs might see “takeoff” and how this
would affect the world. Takeoff refers to the process of AIs going from being able to perform well on
many tasks at a subhuman level to performing at human level or even becoming “super-intelligent,”
surpassing human capabilities. There is serious uncertainty about how AI development will play out
in the future. First, we will discuss how the development of AIs can speed up the economy. Then,
we will discuss why we cannot rule out that advanced AIs might arrive soon. Lastly, we will discuss
how AI takeoff could be continuous or discontinuous and fast or slow. Navigating these uncertainties
is essential for formulating AI safety policies that are robust to the various routes AI development
might take in the future.

9.2. TAKEOFF
487
9.2.1
Economic Growth
Economic growth has been limited by the growth rate of the human population. By enabling cheap
and scalable automation, AIs that perform tasks at a human level might artificially augment the
effective economic population. This might significantly speed up economic growth to levels much
higher than those observed in most developed countries in recent decades.
AI automation may create explosive growth.
Some researchers have argued that if we add
AIs to standard models of economic growth (such as the Solow model) developed using economic
theory and past data, we find that AIs could trigger a dramatic surge in economic growth. AIs
that can substitute for human labor can augment a labor force by providing inexpensive and highly
scalable automation. Some studies suggest that such AIs could spark unprecedented growth, causing
the world economy to grow at rates exceeding ten times the current growth rate [1]. This could
result in an unprecedented acceleration of scientific and technological advancement, reshaping our
economy and the trajectory of human history over a period of perhaps only a few years. Growth
of this magnitude would be unlike anything in human history: for the past 10,000 years after the
agricultural revolution, total world output grew at 0.1% per year, steadily increasing over the last
1000 years to single-digit percentage points.
Factors that could constrain AI automation and economic growth.
AI automation itself
might be slowed if there were limits on society’s ability to develop or deploy AI systems. A dramatic
expansion in the use of AI might require enormous investments in computing hardware and electricity
generation that could take several years to realize. Moreover, in order to generate explosive growth,
AI automation may need to be extremely comprehensive. If there were still some tasks or functions
that AI was unable to automate, these could act as bottlenecks on overall economic growth. For
example, if AI exhibited above-human performance on cognitive tasks but its ability to move in the
physical world lagged behind due to slow progress in robotics, this would cap the extent to which AI
could accelerate economic growth. Alternatively, AI might be prevented from performing certain
tasks due to restrictions imposed by regulations. There might be legal requirements that certain
services (e.g., medical or legal services) be delivered by a human professional. Moreover, people
might have a strong preference that some workers be human beings.
Population growth may drive economic growth.
This worldwide economic acceleration in
recent centuries has been variously attributed to the unique conditions of the industrial revolution,
the technologies developed in 18th and 19th century Europe, and the growth in total population over
time. Population growth is emphasized most by the semi-endogenous theory of economic growth. It
holds that since economic growth causes population growth by reducing bottlenecks on population
growth and population growth causes economic growth by providing a large labor force (including an
increasing number of researchers driving technological progress), there is a positive feedback loop
and so population growth is the key factor to consider when looking at the increase in the economic
output over time.
According to this theory, human population growth was determined for many thousands of years by
the availability of food. As agricultural technologies were developed, food became easier to produce,
which allowed for more population growth. Since larger populations have more opportunities to
innovate and develop better technology, this process loops back into itself recursively, producing a
faster-than-exponential development curve over the long run.
This acceleration ultimately slowed down in the mid-20th century. The semi-endogenous theory
explains this slowdown as a result of the independent decline in the population growth rate.
Demographic changes such as falling birth rates uncoupled productivity and population growth. This

488
CHAPTER 9. GOVERNANCE
helps explain why economic growth did not explode in the late 20th and early 21st centuries. If
the population bottleneck were lifted, the multi-thousand-year trend of accelerating growth could
continue again until we exhaust physical resources like energy and space.
Pre−Industrial
Real Outcome
Unconstrained
 by Population
1800
1850
1900
1950
2000
Year
Output
 Constraints in Semi−Endogenous Theory of Economic Growth
Figure 9.1: Economic output could grow much faster than past trends if not constrained by the
human population.
AIs may fuel effective population growth.
If AIs can automate the majority of important
human tasks (including further AI development), this would lift the bottleneck on labor that has
prevented explosive growth. There are some reasons to think that AIs could boost the economic
growth rate by substituting for human labor. As easily duplicable software, the AI population can
grow at least as quickly as we can manufacture hardware to run it on—-much faster than humans take
to reproduce and learn skills from adults. Additionally, the growth of AI capabilities driven by design
is much faster than the growth of human capabilities driven by evolution: over the entire course of
human evolution from primates, human brains grew roughly 4 times in size, whereas over the decade
after AlexNet, the largest machine learning models increased in size by the same amount roughly
every 16 months. Assuming we do not see a slowdown AI development or face other bottlenecks like
energy production, AIs may cause explosive growth by lifting the population bottleneck.
9.2.2
Timelines
Advanced AIs could have significant impacts on our civilization such as the dramatic and
unprecedented acceleration of economic growth.
However, it is unclear when we will see such
AIs being deployed. It is comfortable to believe that we are nowhere close to creating advanced AIs;
however, given uncertainty among experts and current trends in compute and algorithmic efficiency,
we do not have strong reasons to rule out the possibility that advanced AIs will exist in the near
future.
There is high uncertainty about when advanced AIs will exist.
Opinions on “timelines”—how
difficult it will be to create human-level AI–—are vary widely among experts. Some AI researchers

9.2. TAKEOFF
489
have been surprised by the pace of advances in AI capabilities. For example, in 2023 Geoffrey Hinton
(who was awarded the Turing award in 2018 for his contributions to deep learning) argued that AI
development is moving quicker than expected and that he now expected AI systems with broadly
super-human performance within 5 to 20 years [2]. AI systems are now able to perform a variety
of tasks that were once treated as very hard, such as providing explanations for jokes or producing
artistic creations.
Nonetheless, it is worth being cautious about interpreting evidence of rapid growth over a short
period too narrowly. In the 1950s and 1960s, many top AI scientists such as Marvin Minsky were
overly optimistic about what was achievable in the short term. In light of these considerations, it
seems wise to maintain uncertainty about how quickly advanced AIs will arrive.
The compute requirements of advanced AIs might not be prohibitively expensive.
An
objection to short timelines—expecting that advanced AIs will arrive quickly—is that they may be
very expensive to run, which may delay their deployment until the price of compute falls to sufficiently
affordable levels. However, this seems unlikely: early LLMs, such as GPT-3.5, were relatively cheap,
requiring approximately one TFLOP per query, which cost less than $0.00001 at the time. This is
substantially less than human wages to perform equivalent services such as answering questions.
One estimate puts the human brain’s computational speed at around one PFLOP (or 1000 TFLOPs)
[3]. Suppose we thought that machine learning models required similar compute to achieve human-
level capabilities: AIs would then be able to work at a few dollars an hour at 2023 prices, and
substantially less in the future (assuming prices continue to fall as they have over the past several
decades). While this analysis is far from complete and highly uncertain, it gives us reason to believe
that advanced AIs would not necessarily be delayed because of the costs of compute.
Algorithms are quickly becoming more efficient.
Algorithmic progress refers to the process
of making AIs more efficient to train and run. Recent studies have determined that algorithmic
progress is roughly as important as compute for explaining progress in machine learning, and that
both have increased rapidly [4]. Not much is known for sure about what drives algorithmic progress,
but there are several possibilities, such as theoretical work on refining existing and finding new ideas,
applying ideas to large-scale models, and engineers experimenting with different algorithms over time.
The truth is likely a combination of these and other factors. If the last two possibilities are true,
then it would indicate that scale, and thus compute, ultimately drives algorithmic progress. This
would make compute by far the most important contributor to AI progress overall.
Advanced AIs may arrive soon.
In short, we observe that compute and algorithms are two
key factors used for AI production that are both advancing rapidly over time. While there might
be significant bottlenecks, such as constraints on the ability to acquire or spend on compute, there
are no decisive reasons why we should think advanced AIs arriving soon is impossible. This doesn’t
imply that advanced AIs are right around the corner–—but it does mean that we should plan for
this possibility.
9.2.3
Types of AI Takeoff
AI takeoff could be discontinuous, involving emergent abilities that break from trendlines, or
continuous, involving a smooth increase in capabilities over time. Relatedly, we are unsure how fast
these abilities will develop: one view is that early progress in AI development will speed up later
progress, creating exponential growth that leads to a fast takeoff.

490
CHAPTER 9. GOVERNANCE
Could important AI capabilities appear discontinuously?
General measures of AI
performance have tended to show fast but smooth increases in average performance on many
tasks as we increase model and dataset size. This is true over many benchmarks and metrics.
However, researchers have identified a number of tasks that appear to break this trend, and exhibit
sharp increases in performance when increasing model size; on these tasks, AI systems display
emergent capabilities, explored in the Single-Agent Safety chapter.
If important abilities (like possession of a mental model of the world) were emergent in this sense, we
should be concerned about the unpredictable emergence of dangerous capabilities like hacking or
deception. This scenario is often referred to as a discontinuous AI takeoff because of the abrupt break
from past trends that it would represent. It is necessary to do more research to find out how common
these emergent abilities are, and whether they can be predicted ahead of time. The alternative is
often called a continuous or gradual AI takeoff, wherein abilities predictably scale with size.
How fast might AI systems have a transformative effect on human civilization?
Those
who believe a fast takeoff is likely expect that we could see a leap from minimal AI automation to a
world deeply transformed by AI in mere weeks or months. They typically expect this to be a result
of recursive self-improvement, AIs continuously improving their own abilities through research and
development. If we possess an AI that can carry out development research at a human level, then
it could keep refining itself, becoming more adept at its research endeavors. This, in turn, would
make it even more efficient at self-enhancement, creating a positive feedback loop. Proponents of fast
takeoff argue that such an AI could rapidly progress from human-like intelligence to vastly superior
levels of cognition due to this recursive, escalating process.
Yet, this confidence might be misplaced. There are already instances of AIs enhancing other AIs’
capabilities. For example, AI models adept at tasks like programming, writing, and standardized
testing have driven down computing costs, facilitated data labeling, and augmented coding skills.
However, these developments have only slightly accelerated the pace of AI advancements. This
implies that even if AI models achieve human-like capabilities across a wide array of tasks, their
recursive self-improvement might still be a gradual progression. Those who believe in slow takeoff
tend to think that this process will take years.
Note that both camps in this debate think that AI will profoundly change the world. The question is,
how quickly will we find ourselves in a radically altered society? Generally speaking, a slow takeoff
would likely be safer, giving the world more time to understand and react to the situation with safety
measures. While the current evidence points towards a slow takeoff, there is currently no consensus
on which scenario is more likely.
Conclusions About Takeoff
We have considered the different routes AI development might take to create AI systems with
performance that matches or exceeds that of human beings across a broad range of tasks. First, we
explored the effects such advanced AI systems might have on the economy, examining how they might
supercharge economic growth through automation, revolutionizing our world. Then, we considered
timelines: whether advanced AIs are likely to arrive soon. Considering the vast uncertainty in experts’
best guesses and the lack of fundamental restrictions on factors like compute and algorithms that are
necessary for AI production, we cannot assume that advanced AIs are a thing of the distant future.
Lastly, we considered uncertainties about the pace and predictability of the transition whereby
AIs would go from performing at a human level to being superintelligent. We briefly examined
evidence about trends in the development of AI, which suggests that most AI capabilities usually

9.3. DISTRIBUTION
491
increase smoothly with scale but some are emergent. We also discussed the phenomenon of recursive
self-improvement, which might lead to rapid takeoff speeds in the future. Understanding how takeoff
might play out could greatly inform our approach to the regulation of AI. Next, we will consider
matters of distribution of AI: of power, access, and costs and benefits.
9.3
Distribution
In this section we discuss three main dimensions of how aspects of AI systems are distributed:
1. Power: whether there will be a few highly sophisticated AIs with vastly more power than the
rest, or many highly capable AIs.
2. Access: whether powerful AIs will be kept in the hands of a small group of people, or whether
they will be widely accessible to the general public.
3. Benefits and costs: whether the benefits and costs of AIs will be evenly or unevenly shared.
We will see that both high centralization and decentralization of AIs present different risks.
Centralization of power and access would allow leaders of corporations or governments controlling
these technologies to pursue potentially harmful goals with limited opposition. Decentralized power
and access, on the other hand, may lead to dangerous dynamics such as a race to the bottom on
AI safety, while giving terrorist groups or "lone wolves" access to powerful weapons. Moving on to
considering the distribution of costs and benefits, we will see that by default on current trends it is
plausible that the benefits of AIs would accrue to only a few powerful companies, while the costs
would be shared by all of society. Governance solutions may help redistribute AI’s benefits fairly,
and protect society from its costs.
9.3.1
Distribution of Power Among AIs
The first dimension we will consider is how power might be distributed among advanced AI systems.
Assuming that we reach a world with AI systems that generally surpass human capabilities, how
many of such systems should we expect there to be? We will contrast two scenarios: one in which a
single AI has enduring decisive power over all other AIs and humans, and one in which there are
many different powerful AIs. We will look at the factors that could make each situation more likely
to emerge, the risks we are most likely to face in each case, and the kinds of policies that might be
appropriate to mitigate them.
AI singleton
One possible future scenario is the emergence of an AI singleton—an AI with vastly greater power
than all others, to the extent that it can permanently secure its power over the others [5].
A monopoly on AI could make a single powerful AI more likely.
One factor affecting the
number of powerful AIs that emerge is the number of actors that can independently develop AIs of
similar capabilities. If a single organization, whether a government or a corporation, were to achieve
a monopoly on the development of highly sophisticated AI, this would increase the likelihood of a
single AI emerging with decisive and lasting power over all individuals.
A fast takeoff could make a single powerful AI more likely.
If an AI were to undergo a
fast takeoff, where its capabilities suddenly grew to surpass other intelligences, then it could prevent
other existing AIs from going through the same process. Such an AI might be motivated to destroy
any potential threats to its power and secure permanent control, ensuring it could pursue its goals

492
CHAPTER 9. GOVERNANCE
unimpeded. On the other hand, if intelligence were to progress more gradually, then there would
not be a window of time where any single AI was sufficiently more powerful than the others to halt
their further development. Note, however, that a fast takeoff does not necessitate one AI becoming
a permanent singleton. That is because AIs may still be vulnerable even if they are extremely
powerful. Simple structures can take down more complex structures; just as humans are vulnerable
to pathogens and chemical weapons, simpler AIs (or humans) might be able to counteract more
capable AIs.
Benefits
An AI singleton could reduce competitive pressures and solve collective action problems.
If an AI singleton were to emerge, the actor in control of it would not face any meaningful competition
from other organizations. In the absence of competitive pressures, they would have no need to try to
gain an advantage over rivals by rushing the development and deployment of the technology. This
scenario could also reduce the risk of collective action problems in general. Since one organization
would have complete control, there would be less potential for dynamics where different entities chose
not to cooperate with one another (as discussed in the previous chapter chap:CAP), leading to a
negative overall outcome.
Costs
An AI singleton increases the risk of single points of failure. In a future scenario with only
one superintelligent AI, a failure in that AI could be enough to cause a catastrophe. If, for instance,
it were to start pursuing a dangerous goal, then it might be more likely to achieve it than if there
were other similarly powerful AIs that could counteract it. Similarly, if a human controlling an AI
singleton would like to lock in their values, they might be able to do so unopposed. Therefore, an AI
singleton could represent a single point of failure.
An AI singleton could increase the risk of human disempowerment.
If there were just
one superintelligent AI and it sought to capture global power, it would not have to overpower other
superintelligent AIs in order to do so. If, instead, there were multiple powerful AIs, humans might
be able to cooperate with those that were more willing to cooperate with humans. However, an AI
singleton would have little reason to cooperate with humans, as it would not face any competition
from other AIs. This scenario would therefore increase the risk of disempowerment of humanity.
Diverse Ecosystem of AIs
An alternative possibility is the emergence of a diverse ecosystem of similarly capable AIs, in which
no single agent is significantly and sustainably more powerful than all the others combined. An AI
singleton might also not occur if there is turnover amongst the most powerful AIs due to the presence
of vulnerabilities. Just as human empires rise and fall, AIs may gain and lose power to others.
Declining development costs could make multiple AIs more likely.
If the costs associated
with developing AIs diminish considerably over time, then more actors will be able to develop AIs
independently of one another. Also, if there aren’t increasing returns from intelligence in many
economic niches, then many businesses will settle for the minimum necessary capable AIs. That is,
an AI intended to cook fast food may not benefit from knowing advanced physics. This increases the
probability of a future where multiple AIs coexist.

9.3. DISTRIBUTION
493
Benefits
A diverse ecosystem of AIs might be more stable than a single superintelligence.
There
are reasons to believe that a diverse ecosystem of AIs would be more likely to establish itself over the
long term than a single superintelligence. The general principle that variation improves resilience has
been observed in many systems. In agriculture, planting multiple varieties of crops reduces the risk
that all of them will be lost to a single disease or pest. Similarly, in finance, having a wide range of
investments reduces the risk of large financial losses. Essentially, a system comprising many entities
is less vulnerable to collapsing if a single entity within it fails.
There are multiple additional advantages that a diverse ecosystem of AIs could have over a single
superintelligence. Variation within a population means that individuals can specialize in different
skills, making the group as a whole better able to achieve complex goals that involve multiple different
tasks. Such a group might also be generally more adaptable to different circumstances, since variation
across components could offer more flexibility in how the system interacts with its environment. The
“wisdom of the crowds” theory posits that groups tend to make better decisions collectively than any
individual member of a group would make alone. This phenomenon could be true of groups of AIs.
For all these reasons, a future involving a diverse ecosystem of AIs may be more able to adapt and
endure over time than one where a single powerful AI gains decisive power.
Diverse AIs could remove single points of failure.
Having multiple diverse AIs could dilute
the negative effects of any individual AI failing to function as intended. If each AI were in charge
of a different process, then they would have less power to cause harm than a single AI that was in
control of everything. Additionally, if a malicious AI started behaving in dangerous ways, then the
best chance of preventing harm might involve using similarly powerful AIs to counteract it, such as
through the use of “watchdog AIs” tasked with detecting such threats. In contrast with a situation
where everything relies on a single AI, a diverse ecosystem reduces the risk of single points of failure.
Costs
Multi-agent dynamics could lead to selfish traits.
Having a group of diverse AIs, as opposed
to just one, could create the necessary conditions for a process of evolution by natural selection to
take effect (for further detail, see Evolutionary Pressures). This might cause AIs to evolve in ways
that we would not necessarily be able to predict or control. In many cases, evolutionary pressures
have been observed to favor selfish traits in biological organisms. The same mechanism might promote
AIs with undesirable characteristics.
Diverse AIs could increase the risk of unanticipated failures.
A group of AIs interacting
with one another would form a complex system, and could therefore exhibit collective emergent
properties that could not be predicted from understanding the behavior of just one. A group of
AIs might therefore increase the risk of black swan events (Section 4.8). Additionally, interactions
between AIs could form feedback loops, increasing the potential for rapid downward spirals that are
difficult to intervene and stop. A group of powerful AIs in control of multiple military processes
could, for example, present a risk of a flash war (see sec:ai-race), resulting from a feedback loop of
adversarial reactions.
Diverse AI ecosystems could exhibit failure modes of AI singletons.
If multiple AI systems
collude with one another, or if inequality amongst AIs is significant such that one or a few are much
more powerful than others, risks will mirror those of an AI singleton. We will examine why collusion
and inequality may occur, and the implications.

494
CHAPTER 9. GOVERNANCE
Multiple AIs may or may not collude.
It has been proposed that if there were multiple highly
capable AIs, they would collude with one another, essentially acting as a single powerful AI [6]. This
is not inevitable. The risk of collusion depends on the exact environmental conditions.
Some circumstances that make collusion more likely and more successful include:
1. A small number of actors being involved.
2. Collusion being possible even if some actors cease to participate.
3. Colluding actors being similar, for example in terms of their characteristics and goals.
4. Free communication between actors.
5. Iterated actions, where each actor can observe what another has done and respond accordingly
in their next decision.
6. All actors being aware of the above circumstances.
Conversely, some circumstances that impede collusion include:
1. A large number of actors being involved.
2. A requirement for every single actor to participate in order for collusion to succeed.
3. Dissimilarity among colluders, perhaps having different histories and conflicting goals.
4. Limited communication between actors.
5. Processes involving only one step where actors cannot observe what other actors have done
and respond in a future action.
6. Uncertainty about the above circumstances.
Power among AIs may be distributed unequally.
The power of AI systems may follow a
long-tail distribution, analogous to the distribution of wealth among humans in the US. It is therefore
important to note that even if we have many diverse AIs of similar capabilities, power may still end
up being concentrated in just a few that have a slight edge, and the impact of AI may be largely
determined by only a few. There are situations short of an AI singleton where power is mainly
concentrated in one or a few AIs.
9.3.2
Distribution of Access to AIs
We have discussed different possible distributions of power among AIs, but this is not the only axis
of distribution we need to consider; there is also the question of how access to the most powerful AIs
will be distributed among humans. For example, even if there is an AI singleton, we still might have
a situation where everyone could use it. Conversely, even if there were multiple similarly capable
models, their use might nevertheless be restricted to a small number of people.
We will discuss wide access through open source, and narrow access through restricted models, as
well as striking a balance between the two through structured access. We will examine the safety
implications of each level of access.

9.3. DISTRIBUTION
495
Levels of Access
Restricted AI models concentrate power. Restricted AI models are those that can only be used
by a small group of people. They may, for example, be exclusively accessible to people working in
private companies, government agencies, or a small group of people with national security clearances.
Restricted models cannot be used by the general public.
While some AIs could be restricted, it is possible that a significant number of highly capable AIs
will be tightly restricted. If all AIs, or at least the most powerful, are restricted models, then power
would be concentrated in the hands of a small number of people. This could increase the risk of
value lock-in, where the values of that small group of people would be promoted and perpetuated,
potentially irreversibly, even if they did not adequately represent the values and interests of the
larger human population.
Open-source AI models increase the risks of malicious use.
Open-source AI models are
those that are freely available for anyone to use. There are no restrictions on what people can use
them for, or how users can modify them. These AI systems would proliferate irreversibly. Since
open-source models are, by definition, accessible to anyone who can run them without restrictions on
how they can be used, there is an increased probability of malicious use.
If information security is poor, all AI systems are effectively open-source.
Robust
cybersecurity will be required to prevent unintended users from accessing powerful AIs. Inadequate
protections will mean that AIs are implicitly open-source even if they are not intended to be, because
they will likely be leaked or stolen.
Structured access.
One possible option for striking a balance between keeping AIs completely
private and making them fully open-source would be to adopt a structured access approach. This is
where the public can access an AI, but with restrictions on what they can use it for and how they
can modify it. There may also be restrictions on who is given access, with “Know Your Customer”
policies for verifying users’ identities. In this scenario, the actor controlling the AI has ultimate
authority over who can access it, how they can access it, what they can use it for, and if and how
they can modify it. They can also grant access selectively for other developers to integrate the AI
within their own applications, with consideration of these developers’ safety standards.
One practical way of implementing structured access is to have users access an AI via an application
programming interface (API). This indirect usage facilitates controls on how the AI can be used and
also prevents users from modifying it. The rollout of GPT-3 in 2020 is an example of this style of
structured access: the large language model was stored in the cloud and available for approved users
to access indirectly through a platform controlled by OpenAI.
Openness Norms
Traditionally, the norm in academia has been for research to be openly shared. This allows for
collaboration between researchers in a community, enabling faster development. While openness may
be a good default position, there are certain areas where it may be appropriate to restrict information
sharing. We will now discuss the circumstances under which these restrictions might be justified and
their relevance to AI development.
There are historical precedents for restricting information sharing in dual-use research.
Dual-use technologies are those that can be used for both beneficial and harmful purposes. It is not
a new idea that information about the development of such technologies should not be widely shared.

496
CHAPTER 9. GOVERNANCE
In the 1930s, publication of research on the nuclear chain reaction, which could be used for both
nuclear power and nuclear weapons, prompted a Nazi program developing the latter. The Manhattan
Project was then conducted in secrecy to avoid enemy intelligence learning of any breakthroughs.
Biotechnology has seen debates about the appropriate level of openness, with concerns around the
publication of papers detailing potential methods for creating dangerous pathogens, which might in
future be used as bioweapons.
Powerful AI would be a dual-use technology and there is therefore a need for serious consideration
of who can be trusted with it. Absolute openness means implicitly trusting anyone who has the
necessary hardware to use AIs responsibly. However, there could in future be many people with
sufficient means to deploy AIs, and it might only take one person with malicious intent to cause a
catastrophe.
Technological progress may be too fast for regulations to keep up.
Another reason for
restricting information sharing is the pacing problem—where technological progress happens too
quickly for policymakers to devise and implement robust controls on a technology’s use. This means
that we cannot rely on regulations and monitoring to prevent misuse in an environment where
information that could enable misuse is being openly shared.
It may be difficult to predict the type of research that is potentially dangerous.
Within
AI research, there are different kinds of information, such as the model weights themselves and the
methods of building the system. There have been cases where the former has been restricted for
safety reasons but the latter openly shared. However, it seems feasible that information on how to
build dangerous AIs could also be used to cause harm.
Moreover, it can be difficult to predict exactly how insights might be used and whether they are
potentially dangerous. For instance, the nuclear reactor, which could help society create more
sustainable energy, was instrumental in developing a cheaper version of the atomic bomb. It is
possible that AIs designed for seemingly harmless tasks could be used to propel the advancement of
potentially dangerous AIs. We may not be able to predict every way in which technologies that are
harmless in isolation might combine to become hazardous.
Since there are costs to restrictions, it is worth considering when they are warranted.
Any interventions to mitigate the risk of misuse of AIs are likely to come at a cost, which may include
users’ freedom and privacy, as well as the beneficial research that could be accelerated by more open
sharing. It is therefore important to think carefully about which kind of restrictions are justified,
and in which scenarios.
It might, for example, be worth comparing the number of potential misuses and how severe they
would be with the number of positive uses and how beneficial they would be. Another factor that
could be taken into account is how narrowly targeted an intervention could be, namely how accurately
it could identify and mitigate misuses without interfering with positive uses.
Restrictions on the underlying capabilities of an AI (or the infrastructure supporting these) tend
to be more general and less precisely targeted than interventions implemented downstream. The
latter may include restrictions on how a user accessing an AI indirectly can use it, as well as laws
governing its use. However, upstream restrictions on capabilities or infrastructure may be warranted
under specific conditions. They may be needed if interventions at later stages are insufficient, if the
dangers of a capability are particularly severe, or if a particular capability lends itself much more to
misuse than positive use.

9.3. DISTRIBUTION
497
Risks From Open vs Controlled Models
Open models would enable dangerous members of the general public to engage in harmful activities.
Tightly controlled models exacerbate the risk that their creators, or elites with special access, could
misuse them with impunity. We will examine each possibility.
Powerful, open AIs lower the barrier to entry for many harmful activities.
There are
multiple ways in which sophisticated AIs could be harnessed to cause widespread harm. They could,
for example, lower the barrier to entry for creating biological and chemical weapons, conducting
cyberattacks like spear phishing on a large scale, or carrying out severe physical attacks, using lethal
autonomous weapons. Individuals or non-state actors wishing to cause harm might adapt powerful
AIs to harmful objectives and unleash them, or generate a deluge of convincing disinformation, to
undermine trust and create a more fractured society.
More open AI models increase the risk of bottom-up misuse.
Although the majority of
people do not seek to bring about a catastrophe, there are some who do. It might only take one
person pursuing malicious intentions with sufficient means to cause a catastrophe. The more people
who have access to highly sophisticated AIs, the more likely it is that one of them will try to use it
to pursue a negative outcome. This would be a case of bottom-up misuse, where a member of the
general public leverages technology to cause harm.
Some AI capabilities may be skewed in favor of offense over defense.
It could be argued
that AIs can also be used to improve defenses against these various threats. However, some misuse
capabilities may be skewed in favor of offense not defense. For example, it may be much easier to
create and release a deadly pathogen than to control it or come up with cures or vaccines. Even if an
AI were to facilitate faster vaccine development, a bioweapon could still do a great deal of harm even
in a short timeframe, leading to many deaths before the vaccine could be discovered and rolled out.
Releasing highly capable AIs to the public may entail a risk of black swans.
Although
numerous risks associated with AIs have been identified, there may be more that we are unaware
of. AIs themselves might even discover more technologies or ways of causing harm than humans
have imagined. If this possibility were to result in a black swan event (see Section 4.8 for a deeper
discussion of black swans), it would likely favor offense over defense, at least to begin with, as decision
makers would not immediately understand what was happening or how to counteract it.
More tightly controlled models increase the risk of top-down misuse.
In contrast with
bottom-up misuse by members of the public, top-down misuse refers to actions taken by government
officials and elites to pursue negative outcomes. If kept in the hands of a small group of people,
powerful AIs could be used to lock in those people’s values, without consideration of the interests
of humanity more broadly. Powerful AIs could also increase governments’ surveillance capacity,
potentially facilitating democratic backsliding or totalitarianism. Furthermore, AIs that can quickly
generate large quantities of convincing content and facilitate large-scale censorship could hand much
greater control of media narratives to people in power. In extreme cases, these kinds of misuse by
governments and elites could enable the establishment of a very long-lasting or permanent dystopian
civilization.
9.3.3
Distribution of Costs and Benefits of AI
We considered distributions of power among AIs and people’s distributions of access to AIs. However,
another important consideration is how the costs and benefits associated with AIs might be distributed.
The distribution of the costs and benefits of AIs will ultimately depend on both market forces and

498
CHAPTER 9. GOVERNANCE
governance decisions. It is possible that big tech companies will receive most of the economic gains
from AIs they control, while in the long term AI could spell the end of economically valuable work
for many or all humans. By redistributing wealth, the government can ensure that even if only a
small number of people directly gain wealth from AIs, wealth is eventually shared more broadly
among the population.
First, we will focus on the potential costs of automation: unprecedented levels of unemployment
and inequality. Conversely, we will consider ways that governance could help equitably distribute
economic gains from automation. Second, we will examine moral hazards, scenarios where agents
take risks because someone else will bear the consequences. We’ll see that if AI developers internalize
their risks, they will have an incentive to reduce them, and therefore impose less risk onto society.
Automation
In the near future, AIs might create jobs by making the economy more productive. Upon the advent
of full Human-Level AI (HLAI), however, the need for human labor would disappear [7]. Conventional
policies to address job losses from automation, like worker retraining programs, would be meaningless
in a world where there are no jobs to retrain for. At that point, economic gains from automation
would be likely to accrue to a small handful of people or organizations that control AIs.
Governments may seek to redistribute wealth from big tech companies to the masses in order to
address poverty and inequality. However, in an autonomous economy, which operates without human
labor, humans may become enfeebled and dependent on AIs for all aspects of life. In this section, we
will consider impacts of automation before and after the development of HLAI.
Benefits of automation
Automation has historically altered the workforce but led to economic development.
On the one hand, automation negatively impacts employment and wages through the displacement
effect, as human labor is substituted with technology. On the other hand, automation has a positive
economic impact through the productivity effect, as new tasks or industries require human labor [8].
Historically, the productivity effect has dominated and the general standard of living has increased.
Consider the Industrial Revolution: mechanized production methods displaced artisanal craftspeople,
but eventually led to new types of employment opportunities in factories and industries that hadn’t
previously existed. Similarly, access to computers has automated away many manual and clerical
jobs like data entry and typing, but has also spawned many more new professions. Technological
shifts have historically led to increases in employment opportunities and wages.
In the short term, AI could boost employment.
We may soon see an explosion of job
opportunities in AI research as well as jobs that involve managing or checking the outputs of AI
systems. Given the likely penetration of AI into all sectors of the economy, this could lead to a
significant creation of new positions, not dissimilar to how IT services are required across industries.
Economic theory suggests that automation can improve economic efficiency and promote economic
growth, thus increasing total wealth (Section The Economic Engine). As wealth increases, people may
have more disposable income, potentially spurring job growth in sectors like hospitality, recreation,
and mental health services. Automation can also lower the costs of goods and services, increasing
demand. For example, automation may expedite tedious aspects of the legal profession, decreasing
the cost of legal services. Legal services might then become more affordable, increasing demand and
creating more work for legal professionals.

9.3. DISTRIBUTION
499
Regardless of AI capabilities, there will likely continue to be some professions where the presence of
humans is valued. Although computers are superior at playing chess than humans, chess remains a
widely popular game. Analytics have increased the competitiveness of chess and other games and
have also helped organizers identify ways to engage audiences. However, for the vast majority of
jobs, even jobs like those in caretaking and education, the preference for humans may diminish as AI
substitutes become vastly cheaper and more efficient.
Costs of Human-Level AI
Human-Level AI (HLAI) would cause mass unemployment.
Human-Level AI, by definition,
is capable of doing every task that humans can do, at least as cheaply. The implication of achieving
HLAI is that human labor would no longer be valuable or necessary.
Historically, routine tasks have been the primary target for automation. As AIs approach human-level
intelligence, cognitive nonroutine jobs [9], which require education and are typically high-paying,
would also become automated. Programmers, researchers, and artists are already augmenting their
productivity using large language models, which will likely continue to become more capable. One
way the future of work could play out is that increasingly few high-skilled workers will excel at
managing or using AIs or will provide otherwise exceptionally unique skills, while the vast majority
of people become unemployable.
Human innovation has historically created new jobs that would have seemed inconceivable just
decades earlier. However, HLAI would also possess the ability to invent new economically valuable
tasks, possibly more quickly than humans can think and innovate. The idea that there are always
jobs for humans to do, while historically true, is not a law of nature [10].
An autonomous economy could operate without human labor.
The transition to a fully
automated economy may not stem from one defining moment, but from an accumulation of small
economic decisions. Imagine the following story: as AIs become increasingly capable, humans delegate
an increasing number of business tasks to them. Initially, they handle emails and scheduling, but
soon manage organizational accounting and communications with clients and partners. The AIs
can be trained and deployed quickly and make fewer errors than humans. Over time and due to
competitive pressures, businesses feel the need to automate more of their operations, and AIs begin
handling complex cognitive tasks like strategy, innovation, and leadership. Soon, the economy is
mostly made up of AIs communicating with other AIs, operating at a pace and complexity that
humans can’t keep up with, further motivating the automation of all economic activity. Eventually,
the economy is able to operate without need for human contribution or oversight. Humans on the
other hand, have become utterly reliant on AIs for basic needs, societal governance, and even social
interactions. Humans have little reason to learn, work, or aspire to create, and their survival depends
on the beneficence of AIs [11].
Automation could lead to social decay.
If the majority of the population became unemployed,
they would have lost their sources of income, causing unprecedented levels of poverty and inequality
(The Economic Engine section). Automation may also reduce social wellbeing through the loss
of meaning or enfeeblement: the loss of abilities and values that make human life worthwhile.
Overcoming challenges through effort is an essential way that humans find purpose and meaning,
and this often comes through work or learning. Absent the need to cultivate skills for work, humans
may shy away from challenges to pursue instant gratification. Personal achievement and growth are
deeper forms of wellbeing than momentary pleasure. In an autonomous economy, areas of life like
personal expression, communication, and emotional resilience may get delegated to the domain of AI

500
CHAPTER 9. GOVERNANCE
assistants, and people may lose the ability to, or never learn to, independently process their emotions
or connect with other humans.
Redistribution and Social Spending
Governments may seek to redistribute this wealth to the masses, addressing poverty by providing a
guaranteed income to everyone. In an ideal outcome, people feel liberated from the need to work and
are free to spend their time on what they value, their basic needs provided for by the government.
We will survey wealth redistribution policies, and consider their impacts.
Public ownership of AI organizations can distribute benefits from AIs.
Governments
might seek to assume partial or full ownership of AI companies, in order to control their operations
safely and guarantee equitable revenue distribution.
Public utilities are often nationalized because they benefit from economies of scale, where larger
organizations are cheaper and more efficient. For example, building power plants and transmission
lines is expensive, so governments are interested in maintaining one large, well-regulated company.
The French government owns the utility company Électricité de France, whose nuclear plants power
the majority of the country’s electricity. AIs may be similar to public utilities if they are ubiquitous
through the economy, essential to everyday activity, and require special safety considerations.
A common argument against nationalization is that it destroys competition. One potential concern
about undermining competition is that in the absence of competition, people and companies do not
need to be more productive than a competitor. This might not be an issue with regards to AIs—they
would be capable of maximum efficiency around the clock.
Taxation is the most straightforward redistribution policy.
Wealth redistribution or social
spending is most often funded by taxes. Progressive tax policies, adopted today by most but not
all nations, require that those who earn more money pay a greater proportion of their earnings
than those who earn less. Governments then seek to redistribute wealth through a wide variety of
programs, from healthcare and education to direct checks to people. In light of massive earnings
by tech companies, economists and policymakers have already proposed specialized taxes on robots,
digital goods and services, and AI. If AI enables big tech companies to make orders of magnitude
more money than any previous company, while much of the population is unable to pay income tax,
targeted taxes on AI may be necessary to maintain government revenues.
Seeking to encourage innovation, the US’s tax landscape currently favors capital investment over
labor investment. If a firm wants to hire a worker, they have to pay payroll taxes and employees
have to pay a number of separate taxes. If a firm replaced their worker with an AI, they would
presently only pay corporate tax, which was incurred anyway [12]. As with other redistributive
policies surveyed in this chapter, there are political barriers to high taxes, including the ability of
companies to lobby the government in favor of lower taxes, as well as long-standing and contentious
debates over the economic effects of taxation.
Distribution of Risks
This textbook presents many risks from AI. It is important to discuss how these risks are distributed,
since impacts from these societal-scale risks materialising would not fall only on those who develop
them. Risks might even fall disproportionately on those who play no part in the development or
control of AIs. We will first introduce the idea of moral hazards and discuss moral hazards from AIs.
Then we will consider ways that risks can be reduced.

9.3. DISTRIBUTION
501
Moral hazards
Moral hazards occur when risks are externalized.
Moral hazards are situations where an
entity is encouraged to take on risks, knowing that any costs will be borne by another party. Insurance
policies are a classic example: people with damage insurance on their phones might handle them less
carefully, secure in the knowledge that any repair costs will be absorbed by the insurance company,
not them.
The bankruptcy system ensures that no matter how much a company damages society, the biggest
risk it faces is its own dissolution, provided it violates no laws. Companies may rationally gamble
to impose very large risks on the rest of society, knowing that if those risks ever come back to the
company, the worst case is the company going under. The company will never bear the full cost of
damage caused to society due to its risk taking. Sometimes, the government may step in even prior
to bankruptcy. For example, American big banks took on massive risks in the lead up to the 2008
financial crisis, but many of them were considered "too big to fail", leading to an expectation that
the government would bail them out in time of need [13]. These dynamics ultimately contributed to
the Great Recession.
Developing advanced AIs is a moral hazard.
Throughout this textbook, we have outlined
great risks from advanced AIs. However, while the potential costs to society are immense, the
maximum financial downside to a tech company developing these AIs is filing for bankruptcy.
Consider the following, admittedly extreme, scenario. Suppose that a company is on the cusp of
inventing an AI system that would boost its profits by a thousand-fold, making every employee a
thousand times richer. However, the company estimates that their invention comes with a 0.1%
chance of human extinction. In the likely case, the average person in the economy would see some
benefits due to increased productivity in the economy, and possibly from wealth redistribution. Still,
most people view this gamble as irrational, preferring not to risk extinction for modest economic
improvements. On the other hand, the company may see this as a worthwhile gamble, as it would
make each employee considerably richer.
Governance Solutions for Risk Management
Risk internalization encourages safer behavior.
In the above examples of moral hazards,
companies take risks that would more greatly affect external parties than themselves. The converse
of this is risk internalization, where risks are primarily borne by the party that takes them. Risk
internalization compels the risk-taker to exercise caution, knowing that they would directly suffer
the consequences of reckless behavior. If AI companies bear the risk of their actions, they would
be more incentivized to invest in safety research, take measures to prevent malicious use, and be
generally disincentivized from creating potentially dangerous systems.
Governance solutions for moral hazards.
We will survey four ideas for dealing with moral
hazards: legal liability, regulations, public ownership, and taxation. The first idea is to hold companies
legally liable for externalities that arise from their AIs. We could try to ensure, for example, that
companies are not able to avoid responsibility for adverse outcomes by blaming users for misuse.
Companies would then feel compelled to build in and rigorously test safety features, since they would
suffer the consequences of failing to do so.
The second idea is to impose strict regulations. Although regulations do not necessarily cause
internalization of risks, they can help prevent the materialization of risks by ensuring good practices.
Regulations have been the United States’s main policy tool in existing fields like nuclear power and

502
CHAPTER 9. GOVERNANCE
biological research, which entail societal-scale risks.
A third idea is to facilitate public ownership, which we previously discussed. Public ownership means
that the public receives both the benefits and the costs (risks) of AIs, by definition eliminating moral
hazards. We previously outlined a thought experiment where a company might feel compelled to
take on an existential risk to humanity; the public on the other hand is unlikely to gamble on its
own existence.
A fourth idea, also discussed previously, is targeted taxation. Although taxes do not directly force
risk internalization, they can provide government revenues that can be reserved for risk mitigation
or disaster relief efforts. For example, the Superfund is a US government program that funds the
cleanup of abandoned hazardous waste sites. It is funded by excise taxes–—a special tax on some
good, service, or activity–—on chemicals. The excise tax ensures that the chemical manufacturing
industry pays for the handling of dangerous waste sites that it has created. Special taxes on AIs
could support government programs to prevent risks or address disasters.
Conclusions About Distribution
In this section, we examined distribution concerns with AIs. First, we considered that power among
AIs could result in a singleton with decisive and lasting power, a diverse ecosystem of AIs with
varying degrees of power, or something in between. Second, we examined varying levels of access
to AIs from open source to highly restricted private models. Since AIs can be used in dangerous
ways, traditional openness norms in research likely need to be reconsidered. Both centralization and
decentralization of AIs, in terms of power and access, carry different risks.
Third, we considered how costs and benefits of AIs might be distributed, including societal-scale risks.
In the short term, automation can lead to economic growth, but the development of Human-Level AI
would result in the effective end of human labor. In the post-labor world, humans may struggle for
meaning and become dependent on AIs. Moral hazards present other societal-scale risks. Governance
can mitigate these risks by holding companies accountable for the risks they take, redistributing
wealth from AIs, including the public in the ownership of AIs, or imposing proactive regulations to
prevent catastrophes.
9.4
Compute Governance
A common shorthand for computational resources or computing power used for AI is compute. In this
section, we will discuss how compute governance enables AI governance. First, we will examine how
since compute is indispensable for AI development, governing it would help us govern AIs. Then, we
will examine the key properties of compute that make it governable—physicality, excludability, and
quantifiability. These features make it more feasible to track, monitor and, if appropriate, restrict the
development of potentially dangerous AI systems, and more generally facilitate the enforcement of
AI governance measures. We also consider why governing compute is more promising than governing
other factors used in AI production.
9.4.1
Compute Is Indispensable for AI Development and Deployment
Compute enables the development of more capable AIs. In addition, compute is necessary to deploy
AIs. If we restrict someone’s access to compute, they cannot develop or deploy any AIs. As a result,
we can use compute to govern AIs, determining how and when they are deployed.

9.4. COMPUTE GOVERNANCE
503
Hardware is necessary for AI systems.
Like the uranium in a nuclear weapon, compute is
fundamental to running AIs. In its simplest form, we can think of compute as a select group of
high-performance chips like GPUs and TPUs that are designed to run modern AIs. These are
often the latest chips, tailor-made for AI tasks and found in large data centers. As AI technology
changes, so too will the hardware, adapting to new tech developments, regulations, and the evolving
requirements of AI applications.
The metric FLOP/s is common across forms of compute.
To measure compute, we often
use the metric FLOP/s, which is the number of floating-point operations (such as addition or
multiplication) a computer can do in a second. When we talk about “increasing” compute, we’re
referring to using more processors, using better processors, or allowing these processors to run for
extended periods, effectively increasing the number of floating-point operations done in total. An
analogous escalation of this is improving a nuclear arsenal by adding more weapons to it, developing
more dangerous weapons like H-bombs, or creating bigger bombs by using more uranium.
More compute allows for the development of more AI capabilities.
Compute plays a
pivotal role in the evolution of AI capabilities. More compute means that AI systems can be built with
more parameters and effectively utilize larger datasets. In the Artificial Intelligence Fundamentals
chapter, we looked at scaling laws, which show us that many AIs have their performance increase
reliably with an increase in model size and dataset size. Richard Sutton’s “The Bitter Lesson” states
that general methods in AI that harness computation tend to be the most effective by a significant
margin [14]. Having more compute means training AI systems that are more capable and advanced,
which means that knowing how much compute an AI uses lets us approximate its capabilities.
Often, pushing the boundaries in AI development requires having vast compute. AIs can require
training on large supercomputers that cost hundreds of millions or even billions of dollars. Moreover,
computational demands for these AI models are constantly intensifying, with their compute
requirements doubling roughly every six months. This growth rate surpasses the 2.5-year doubling
time we see for the price-performance of AI chips [15]. Given this trend, it’s likely that future AI
models will demand even greater investment in computational resources and, in turn, possess greater
capabilities.
More compute enables better results.
Compute is not only essential in training AI-—it is also
necessary to run powerful AI models effectively. Just as we rely on our brains to think and make
decisions even after we’ve learned, AI models need compute to process information and execute tasks
even after training. If developers have access to more compute, they can run bigger models. Since
bigger models usually yield better results, having more compute can enable better results.
Large-scale compute isn’t a strict requirement for all future AI applications.
AI efficiency
research aims to reduce compute requirements while preserving performance by improving other
factors like algorithmic efficiency. If algorithms become so refined that high-capability systems
can train on less powerful devices, compute’s significance for governance might diminish. Some
existing systems like AI-powered drug discovery tools do not require much compute, but it has been
demonstrated that they be can repurposed to create chemical weapons [16].
Additionally, there’s continued interest towards creating efficient AI models capable of running on
everyday devices such as smartphones and laptops. Though projections from current trends suggest
it will be decades before data center-bound systems like GPT-4 could train on a basic GPU [17], the
shift towards greater efficiency might speed up dramatically with just a few breakthroughs. If AI

504
CHAPTER 9. GOVERNANCE
models require less compute, especially to the point that they become commonplace on consumer
devices, regulating AI systems based on compute access might not be the most effective approach.
9.4.2
Compute Is Physical, Excludable, and Quantifiable
To produce AI, developers need three primary factors: data, algorithms, and compute. In this section,
we will explore why governing compute appears to be a more promising avenue than governing the
other factors. A resource is governable when the entity with legitimate claims to control it—such as
a government–—has the ability to control and direct it. Compute is governable because
1. It can be determined who has access to compute and how they utilize it.
2. It is possible to establish and enforce specific rules about compute.
These are true because compute is physical, excludable, and quantifiable. These characteristics allow
us to govern compute, making it a potential point of control in the broader domain of AI governance.
We will now consider each of these in turn.
Compute Is Physical
The first key characteristic that makes compute governable is its physicality. Compute is physical,
unlike datasets, which are virtual, or algorithms, which are intangible ideas. This makes compute
rivalrous and enables tracking and monitoring, both of which are crucial to governance.
Since compute is physical, it is rivalrous.
Compute is rivalrous: it cannot be used by multiple
entities simultaneously. This is unlike other factors in AI production, such as algorithms which can
be used by anyone who knows them or data which can be acquired from the same source or even
stolen or copied and used simultaneously (although this may be difficult due to information security
and the size of training datasets). Because compute cannot be simultaneously accessed by multiple
users or easily duplicated, regulators can be confident that when it is being used by an approved
entity, it is not also being used by someone else. This makes it easier to regulate and control the use
of compute. GPUs can’t be downloaded but instead must be fabricated, purchased, and shipped.
Figure 9.2: Advanced semiconductor manufacturing
tools (such as the ASML Twinscan NXE) are large,
highly specialized machines.
Since
compute
is
physical,
it
is
trackable.
Compute is trackable, from
chip fabrication to its use in data centers.
This is because compute is tangible and
often sizable: figure 9.2 shows a cutting-
edge semiconductor tool used as compute
that costs $200 million and requires 40
freight containers, 20 trucks, and 3 cargo
planes to ship anywhere.
Unlike uranium,
which is difficult to
procure but not impossible to steal and
transport small amounts of, acquiring large-
scale compute requires the investment of
resources in a relatively complicated and
visible process.
Stakeholders, whether
semiconductor firms, regulatory bodies,
or other involved entities, can accurately
evaluate and trace the overall quantity of

9.4. COMPUTE GOVERNANCE
505
these assets. For instance, if we monitor the
sales and distribution of chips, we know who
possesses which computational capacities and their intended applications. The complete supply chain,
from the semiconductor origins to the extensive data centers harboring vast AI computational power,
can be monitored, which means it can be governed. By contrast, data acquisition and algorithmic
improvements can be done discreetly: possession of these within a computing infrastructure can be
concealed more easily than the possession of the infrastructure itself.
Compute Is Excludable
The second key characteristic that makes compute governable is its excludability. Something is
excludable if it is feasible to stop others from using it. Most privately produced goods like automobiles
are excludable whereas others, such as clean air or street lighting, are difficult to exclude people from
consuming even if a government or company doesn’t want to let them use it. Compute is excludable
because a few entities, such as the US and the EU, can control crucial parts of its supply chain. This
means that these actors can monitor and prevent others from using compute.
The compute supply chain makes monitoring easier.
In 2023, the vast majority of advanced
AI chips globally are crafted by a single firm, Taiwan Semiconductor Manufacturing Company
(TSMC). These chips are based on designs from a few major companies, such as Nvidia and Google,
and TSMC’s production processes rely on photolithography machines from a similarly monopolistic
industry led by ASML [18]. Entities such as the US and EU can, therefore, regulate these companies
to control the supply of compute—if the supply chain dynamics do not change dramatically over
time [19]. This simplifies the tracking of frontier AI chips and enforcing of regulatory guidelines; it’s
what made the US export ban of cutting-edge AI chips to China in 2022 feasible. This example
illustrates that these chips can be governed. By contrast, data can be purchased from anywhere or
found online, and algorithmic advances are not excludable either, especially given the open science
and collaborative norms in the AI community.
Frequent chip replacements means governance is effective quickly.
The price performance
of AI chips is increasing exponentially. With new chips frequently making recent products obsolete,
compute becomes more excludable. Historical trends show that GPUs double their price performance
approximately every 2.5 years [17]. In conjunction with the rapidly increasing demand for more
compute, data centers frequently refresh their chips and purchase vast quantities of new compute
regularly to retain competitiveness. This frequent chip turnover offers a significant window for
governance since regulations on new chips will be relevant quickly.
Compute Is Quantifiable
The third key characteristic that makes compute governable is its quantifiability. Quantifiability
refers to the ability to measure and compare both the quantity and quality of resources. Metrics such
as FLOP/s serve as a yardstick for comparing computational capabilities across different entities.
If a developer has more chips of the same type, we can accurately deduce that they have access to
more compute, which means we can use compute to set thresholds and monitor compliance.
Quantifiability facilitates clear threshold setting.
While chips and other forms of compute
differ in many ways, they can all be quantified in FLOP/s. This allows regulators to determine
how important it is to regulate a model that is being developed: models that use large amounts of
compute are likely more important to regulate. Suppose a regulator aims to regulate new models
that are large and highly capable. A simple way to do this is to set a FLOP/s threshold, above which
more regulations, permissions, and scrutiny take effect. By contrast, setting a threshold on dataset

506
CHAPTER 9. GOVERNANCE
size is less meaningful: quality of data varies enough that 25 GB of data could contain all the text in
Wikipedia or one high-definition photo album. Even worse, algorithms are difficult to quantify at all.
Quantifiability is key to monitoring compliance.
Beyond the creation of thresholds,
quantifiability also helps us monitor compliance. Given the physical nature and finite capacity
of compute, we can tell whether an actor has sufficient computational power from the type and
quantity of chips they possess. A regulator might require organizations with at least 1000 chips at
least as good as A100s to submit themselves for additional auditing processes. A higher number
of chips directly correlates to more substantial computational capabilities, unlike with algorithms
where there is no comparable metric and data for which metrics are much less precise. In addition to
compute being physical and so traceable, this enables the enforcement of rules and thresholds.
Conclusions About Compute Governance
Compute governance is a promising route to AI governance. Compute is necessary for the
development and deployment of AIs, as well as being well-suited to governance. Relative to governing
algorithms and datasets, the other factors used to produce AIs, governing compute is promising
because it is physical, excludable, and quantifiable.
These features are useful for national governments, as they make it more feasible to monitor the
development and deployment of potentially dangerous AI systems. This is a precondition for effective
enforcement of AI regulations. Similarly, compute’s features can support verification of compliance
with national or international standards and agreements. This makes is easier to implement regulatory
regimes, particularly at an international level.
Compute governance is currently made simpler by certain factors such as the controllability of the
supply chain or requirement of large-scale compute for highly capable models. If the supply chain of
hardware for training and running AI systems making much less concentrated, this would make it
harder to enforce potential restrictions on access to relevant hardware as a governance mechanism.
Similarly, if the required compute resources to train dangerous AI systems were within the means of
a small business or even an individual, this would dramatically increase the challenge of monitoring
relevant compute resources as part of AI governance.
While describing compute governance, we have touched upon various ideas about how to use compute
to ensure companies produce safe AIs, governments create and enforce regulations, and international
cooperation can control the compute supply chain. In the rest of this chapter, we further discuss
governance tools aimed at corporate, national, and international governance.
9.5
Corporate Governance
Overview. AI companies need sound corporate governance: it is essential that these firms are
guided in directions that enable the creation of safe and beneficial AIs. In this section, we explain
what corporate governance is, differentiating between shareholder and stakeholder theory. Then, we
give an overview of different legal structures, ownership structures, organizational structures, as well
as internal and external assurance mechanisms.
9.5.1
What Is Corporate Governance?
There are different views on what the purpose of corporate governance is. These differences are
related to different theories about capitalism.

9.5. CORPORATE GOVERNANCE
507
Shareholder theory.
According to one view, corporate governance is about the relationship
between a company and its shareholders. This view is based on shareholder theory, according to
which companies have an obligation to maximize returns for their shareholders. Using this theory,
the purpose of corporate governance is to ensure that actors within a company act in the best interest
of the company’s shareholders. The relationship between shareholders and actors within a company
can be conceptualized as the following problem: shareholders delegate responsibilities to managers
and workers, but managers and workers might not act in the shareholders’ interests [20]. On this
view, corporate governance is ultimately a tool for maximizing shareholder value.
Stakeholder theory.
According to another view, corporate governance is about the relationship
between a company and its stakeholders. The idea is that companies are responsible not only to their
shareholders but to many other stakeholders like employees, business partners, and civil society [21].
Following this theory, the purpose of corporate governance is to balance the interests of shareholders
with the interests of other stakeholders. It refers to all the rules, practices, and processes by which a
company is directed and controlled.
For the purposes of this book, we are mainly interested in how AI companies are or should be
governed to best advance the public interest [22]. Next, we will consider how a firm’s legal structure
can help permit this.
9.5.2
Legal Structure
A company’s legal structure refers to its legal form and place of incorporation, its statutes and
bylaws.
Legal form.
AI companies can have different legal forms. In the US, the most common form is a
C corporation or “C-corp” for short. Other forms include public benefit corporations (PBCs), limited
partnerships (LPs), and limited liability companies (LLCs). The choice of legal form has significant
influence on what a company is able and required to do. For example, while C-corps must maximize
shareholder value, PBCs can also pursue public benefits. This can be important in situations where
AI companies may want to sacrifice profits in order to promote other goals. Google, Microsoft, and
Meta are all C-corps, Anthropic is a PBC, and OpenAI is an LP (which is owned by a nonprofit).
Place of incorporation.
Since there are significant differences between jurisdictions, it matters
where a company is incorporated. An important distinction can be drawn between common law
countries like the US and UK, wherein judicial precedent is important, and civil law countries like
Germany and France, where recorded laws are more comprehensive and less open to interpretation.
But there are also important differences within a given country. For example, in the US, many
AI companies such as OpenAI and Anthropic are incorporated in the state of Delaware due to its
relatively business-friendly regulations, but they often have branches in other states like California.
Statutes and bylaws.
Although many governance decisions are determined by the legal form and
the place of incorporation, companies have some room for customization. They can customize their
legal structure in their statutes and bylaws. Companies are typically required to specify their mission
statement in their statutes. The mission of Google DeepMind is “to solve intelligence to advance
science and benefit humanity” and OpenAI’s mission is “to ensure that AGI benefits all of humanity.”
The statutes of PBCs also need to contain a specific public benefits statement. But the statutes
and bylaws can also contain more specific rules; for example, an AI company could give its board of
directors specific powers, such as to veto the deployment of a new model.

508
CHAPTER 9. GOVERNANCE
Ensuring that a firm’s legal structures enable its employees to act in the best interests of the firm’s
stakeholders is important, and firms have many ways to do this. Next, we will consider how ownership
can help solidify a firm’s safety-conscious goals.
9.5.3
Ownership Structure
Companies are owned by shareholders, who elect the board of directors, which appoints the senior
executives who actually run the company.
Types of shareholders.
AI companies can have different types of shareholders. In their early
stages, AI companies typically get investments from wealthy individuals, so-called “angel investors.”
For example, Elon Musk was among the first investors in OpenAI, while Dustin Moskovitz was part
of the initial funding round of Anthropic. As AI companies mature, professional investors such
as venture capital (VC) firms and big tech companies start to invest—DeepMind was acquired by
Google in 2016, and Microsoft invested heavily in OpenAI. At some point, many companies decide
to “go public,” which means that they are turned into publicly traded companies. At that point,
institutional investors like pension funds enter the stage. For example, Vanguard and BlackRock are
among the largest shareholders of Microsoft, Google, and Meta, though the founders often retain a
significant amount of shares. It is also not uncommon to give early employees stock options, which
allow them to purchase stock at fixed prices.
Powers of shareholders.
Shareholders can influence the company in several ways. They can voice
concerns in the annual general meeting and vote in shareholder resolutions. For example, in the 2019
annual general meeting of Alphabet, one shareholder called for better board oversight of AI and
suggested creating a Societal Risk Oversight Committee. If the board of directors of a C-Corp does
not act in the shareholders’ interest, shareholders can theoretically replace them. However, in practice
this is very rare. A more common way to put pressure on board members is to file lawsuits against
them if they fail to meet certain obligations. Such lawsuits are often settled in ways that improve
corporate governance. The attempt of shareholders to steer the company in a certain direction is
called shareholder activism.
Customized ownership structures.
Depending on the company’s legal form, it may be possible
to customize the ownership structure. A common way to do that is to issue two different classes
of shares: both classes give their holders ownership, but only one class grants voting rights. This
allows structures where the founders remain in control of the company while other shareholders
contribute capital and receive returns on their investment. Another way to achieve certain goals is to
combine different legal entities. For example, the returns of investors in the OpenAI LP are capped.
Above a certain threshold, returns are owned by the OpenAI nonprofit. They call this a capped-profit
structure. A related idea is to adopt a governance structure that, under certain conditions, transfers
control to a committee representing society’s interests rather than the shareholders’ interests.
It is important who owns AI companies because these owners have strong ways to influence operations.
Next, we will put aside questions of ownership and examine how these organizations are structured
and run.
9.5.4
Organizational Structure
While shareholders own the company, it is governed by the board of directors and managed by the
chief executive officer (CEO) and other senior executives.

9.5. CORPORATE GOVERNANCE
509
Board of directors.
The board of directors is the main governing body of the company. Board
members have a legal obligation to act in the best interests of the company, so-called fiduciary duties.
These duties can vary: board members of Alphabet have the typical fiduciary duties of a C-Corp,
while board members of OpenAI’s nonprofit’s have a duty to “to ensure that AGI benefits all of
humanity,” not to maximize shareholder value. The board has a number of powers they can use to
steer the company in a more prosocial direction. It sets the strategic priorities, is responsible for risk
oversight, and has significant influence over senior management; for instance, it can replace the chief
executive officer. However, the board’s influence is often indirect. Many boards have committees,
some of which might be important from a safety perspective such as a risk committee or audit
committee. Anthropic’s Long-Term Benefit Trust (LTBT) can choose Anthropic’s board members,
who could in theory fire the CEO.
Senior executives.
The company is managed by the CEO, who appoints other senior executives
such as a chief technology officer (CTO), chief scientist, chief risk officer (CRO), and so on. Since
the behavior of executives is at least in part driven by financial incentives, remuneration is often a
valuable tool to ensure that they act in the corporation’s interest. Before appointing new executives,
the board might also want to conduct background checks; in the case of AI companies, a board might
want to consider candidates’ views on risks from AIs.
Organizational units.
Below the executive level, AI companies have a number of teams, often in
a hierarchical structure. This will typically involve research and product development teams, but also
legal, risk management, finance, and many others. AI companies can also have safety and governance
teams. These teams should be well resourced and have buy-in from senior management.
In addition, some AI companies have other organizational structures, such as an ethics board that
advises the board of directors and senior management, or an institutional review board (IRB) that
checks if publishing certain types of research might be harmful [22]. Google DeepMind has a IRB-like
review committee that played a key role in the release of AlphaFold.
Various aspects of the legal, ownership, and organizational structure of AI companies can influence
to what extent their outputs focus on employees’ and managers’ best interests, creating shareholder
value, or achieving their mission and ensuring societal wellbeing. In the last subsection, we will
explore how we can ensure that these structures are well chosen and smoothly functioning.
9.5.5
Assurance
Different stakeholders within and outside AI companies need to know whether existing governance
structures are adequate.
AI companies therefore take a number of measures to evaluate and
communicate the adequacy of their governance structures. These measures are typically referred to
as assurance. We can distinguish between internal and external assurance measures.
Internal assurance.
AI companies need to ensure that senior executives and board members get
the information they need to make good decisions. It is, therefore, essential to define clear reporting
lines. To ensure that key decision-makers get objective information, AI companies may set up an
internal audit team that is organizationally independent from senior management [23]. This team
would assess the adequacy and effectiveness of the company’s risk management practices, controls,
and governance processes, and report directly to the board of directors.
External assurance.
Many companies are legally required to publicly report certain aspects of
their governance structures, such as whether the board of directors has an audit committee. Often,

510
CHAPTER 9. GOVERNANCE
AI companies also publish information about their released models and often in the form of model
or system cards. Some organizations disclose parts of their safety strategy and their governance
practices as well; for instance, how they plan to ensure their powerful AI systems are safe, whether
they have an ethics board, or how they conduct pre-deployment risk assessments. These publications
allow external actors like researchers and civil society organizations to evaluate and scrutinize the
company’s practices. In addition, many AI companies commission independent experts to scrutinize
their models, typically in the form of third-party audits, red teaming exercises to adversarially test
systems, or bug bounty programs that reward finding errors and vulnerabilities.
Conclusions About Corporate Governance
Corporate governance refers to all the rules, practices, and processes by which a company is directed
and controlled—ranging from its legal form and place of incorporation to its board committees and
remuneration policy. The purpose of corporate governance is to balance the interests of a company’s
shareholders with the interests of other stakeholders, including society at large. To this end, AI
companies are advised to follow existing best practices in corporate governance. However, in light of
increasing societal risks from AI, they also need to consider more innovative governance solutions,
such as a capped-profit structure or a long-term benefit trust.
9.6
National Governance
Overview. Government action may be crucial for AI safety. Governments have the authority to
enforce AI regulations, to direct their own AI activities, and to influence other governments through
measures such as export regulations and security agreements. Additionally, major governments can
leverage their massive budgets, diplomats, intelligence agencies, and leaders selected to serve the
public interest. More abstractly, as we saw in the Collective Action Problems chapter, institutions can
help agents avoid harmful coordination failures. For example, penalties for unsafe AI development
can counter incentives to cut corners on safety.
This section provides an overview of four ways governments may be able to advance AI safety: safety
standards and regulations, liability for AI harms, improving societal resilience, and not falling behind
while focusing on developing safe AI.
9.6.1
Standards and Regulations
To ensure that frontier AI development and deployment is safe, two complementary approaches
are formally establishing strong safety measures as best practices for AI labs, and requiring the
implementation of strong safety measures.
This can be done using standards and regulations,
respectively.
Standards are specified best practices.
Standards are written specifications of the best practices
for carrying out certain activities. There are standards in many areas, from telecommunications
hardware to country codes to food safety.
Standards often aim to ensure quality, safety, and
interoperability; for instance, the International Food Standard requires the traceability of products,
raw materials and packaging materials.
The substance of AI safety standards.
In the context of frontier AI, technical standards for
AI safety could guide various aspects of the AI model life cycle. Before training begins, training
plans could be assessed to determine if training is safe, based on evidence about similar training runs
and the proposed safety methods. After training begins, models could be evaluated to determine

9.6. NATIONAL GOVERNANCE
511
if further training or deployment is safe, based on whether models show dangerous capabilities or
propensities. During deployment, particularly powerful models could be released through a monitored
API. Standards could guide all aspects of this process.
Standards are developed in dedicated standard-setting organizations.
Many types of
organizations, from government agencies to industry groups to other nonprofits, develop standards.
Two examples of standard-setting organizations are the National Institute of Standards and Technology
(NIST) and the International Organization for Standardization (ISO). In the US, standard setting
is often a consensus-based activity in which there is substantial deference to industry expertise.
However, this increases the risk that standards over-represent industry interests.
The impact of standards.
Standards are not automatically legally binding.
Despite that,
standards can advance safety in various ways. First, standards can shape norms, because they are
descriptions of best practices, often published by authoritative organizations. Second, governments can
mandate compliance with certain standards. Such “incorporation by reference” of an existing standard
may bind both the private sector and government agencies. Third, governments can incentivize
compliance with standards through non-regulatory means. For example, government agencies can
make compliance required for government contracts and grants, and standards compliance can be a
legal defense against lawsuits.
Regulations are legally binding.
Regulations are legal requirements established by governments.
Some examples of regulations are requirements for new foods and drugs to receive an agency’s
approval before being sold, restrictions on the pollution emitted by cars, requirements for aircraft
and pilots to have licenses, and constraints on how companies may handle personal data.
Regulations are often shaped by both legislatures and agencies.
In some governments, such
as the US and UK, regulations are often formed through the following process. First, the legislature
passes a law. This law creates high-level mandates, and it gives a government agency the authority
to decide the details of these rules and enforce compliance. By delegating rulemaking authority,
legislatures let regulations be developed with the greater speed, focus, and expertise of a specialized
agency. As we discussed, agencies often incorporate standards into regulations. Legislatures also often
influence regulation through their control over regulatory agencies’ existence, structure, mandates,
and budgets.
Regulatory agencies do not always regulate adequately. Regulatory agencies can face steep challenges.
They can be under-resourced: lacking the budgets, staff, or authorities they need to do well at designing
and enforcing regulations. Regulators can also suffer from regulatory capture—being influenced
into prioritizing a small interest group (especially the one they are supposed to be regulating) over
the broader public interest. Industries can capture regulators by politically supporting sympathetic
lawmakers, providing biased expert advice and information, building personal relationships with
regulators, offering lucrative post-government jobs to lenient regulatory staff, and influencing who is
appointed to lead regulatory agencies.
Standards and regulations give governments some ways to shape the behavior of AI developers. Next,
we will consider legal means to ensure that the developers of AI have incentives in line with the rest
of society.
9.6.2
Liability for AI Harms
In addition to standards and regulations, legal liability could advance AI safety. When AI accidents or
misuse cause harm, liability rules determine who (if anyone) has to pay compensation. For example,

512
CHAPTER 9. GOVERNANCE
an AI company might be required to pay for damages if it leaks a dangerous AI, or if its AI provides
a user step-by-step instructions for building or acquiring illegal weapons.
Non-AI-specific liability.
Legal systems including those of the US and UK have forms of legal
liability that would plausibly apply to AI harms even in the absence of AI-specific legislation. For
example, in US and UK law, negligence is grounds for liability. Additionally, in some circumstances,
such as when damages result from a company’s defective product, companies are subject to strict
liability. That means companies are liable even if they acted without negligence or bad intentions.
These broad conditions for liability could apply to AI, but there are many ways judges might interpret
concepts like negligence and defective products in the case of AI. Instead of leaving it to a judge’s
interpretation, legislators can specify liability rules for AI.
There are advantages to holding AI developers liable for damages.
Legal liability helps
AI developers internalize the effects of their products on the rest of society by ensuring that they pay
when their products harm others. This improves developers’ incentives. Additionally, legal liability
helps provide accountability without relying on regulatory agencies. This avoids the problem that
government agencies may be too under-resourced or captured by industry to mandate and enforce
adequate safety measures.
Legal liability is a limited tool.
There are practical limits to what AI companies can actually
be held liable for. For example, if an AI were used to create a pandemic that killed 1 in 100 people,
the AI developer would likely be unable to pay beyond a small portion of the damages owed (as these
could easily be in the tens of trillions). If the amount of harm that can be caused by AI companies
exceeds what they can pay, AI developers cannot fully internalize the costs they impose on society.
This problem can be eased by requiring liability insurance (a common requirement in the context of
driving cars), but there are amounts of compensation that even insurers could not afford. Moreover,
sufficiently severe AI catastrophes may disrupt the legal system itself. Separately, liability does little
to deter AI developers who do not expect their AI development to result in large harms—even if
their AI development really proves catastrophically dangerous.
Ensuring legal liability for harms that result from deployed AIs helps align the interests of AI
developers with broader social interests. Next, we will consider how governments can mitigate harms
when they do occur.
9.6.3
Improving Resilience
The government actions already discussed focus on preventing unsafe AI development and deployment,
but another useful intervention point may be mitigating damages during deployment.
Resilience may protect against extreme risks.
Governments may be able to improve societal
resilience to AI accidents or misuse through promoting cybersecurity, biosecurity, and AI watchdogs.
Measures for increasing resilience may raise the level of AI capabilities needed to cause catastrophe.
That would buy valuable time to develop safety methods and further defensive measures—ideally
enough time for safety and defense to always keep pace with offensive capabilities. Sufficient resilience
could lastingly reduce risk.
Policy tools for resilience.
To build resilience, governments could use a variety of policy tools.
For example, they could provide R&D funding to develop defensive technologies. Additionally,
they could initiate voluntary collaborations with the private sector to assist with implementation.
Governments could also use regulations to require owners of relevant infrastructure and platforms to
implement practices that improve resilience.

9.6. NATIONAL GOVERNANCE
513
Tractability of resilience.
If governments defend narrowly against some attacks, rogue AIs or
malicious users might just find other ways to cause harm. Increasingly advanced AIs could pose novel
threats in many domains, so it may be hard to identify or implement targeted defensive measures
that make a real difference. However, perhaps there are a few domains where societal vulnerabilities
are especially dire and tractable to improve (cybersecurity or biosecurity, for example), while some
defensive measures could provide broader defenses (such as AI watchdogs).
Cybersecurity. AIs could strengthen cybersecurity. AIs could identify and patch code vulnerabilities
(that is, they could fix faulty programming that would let attackers get unauthorized access to a
computer). AIs could also help detect phishing attacks, malware and other attempts to attack a
computer network, enabling responses such as blocking or quarantining malicious programs. These
efforts could be targeted to defend widely used software and critical infrastructure. However, AIs
that identify code vulnerabilities are dual-use; they can be used to either fix or exploit vulnerabilities.
Biosecurity. Dangerous pathogens can be detected or countered through measures such as wastewater
monitoring (which might be enhanced by anomaly detection), far-range UV technology, improved
personal protective equipment, and DNA synthesis screening that is secure and universal.
AI watchdogs.
AIs could monitor the activity of other AIs and flag dangerous behavior.
For
example, AI companies can analyze the outputs of their own chatbots and identify harmful outputs.
Additionally, AIs could identify patterns of dangerous activities in digital or economic data. However,
some implementations of this could harm individual privacy.
Defensive measures including cybersecurity, biosecurity, and AI watchdogs may mitigate harms from
the deployment of unsafe AI systems. However, defensive measures, regulation, and liability may
all be insufficient for safety if the countries that implement them all fall behind the frontier of AI
development. Next, we will consider how countries can remain competitive while ensuring safety in
domestic AI production.
9.6.4
Not Falling Behind
If some countries take a relatively slow and careful approach to AI development, they may risk falling
behind other countries that take less cautious approaches. It would be risky for the global leaders
in AI development to be within countries that lack adequate guardrails on AI. Various policy tools
could allow states to avoid falling behind in AI while they act to keep their own companies’ AIs safe.
Risks of adversarial approaches.
Adversarial approaches to AI policy–—that is, policies focused
on advancing one country’s AI leadership at the expense of another’s—have risks. Adversarial
policies could rely on wrong assumptions about which states will adequately guardrail AI, and they
could also motivate counter-actions and increase international tensions (making cooperation harder).
Competitive mindsets can also encourage de-prioritizing safety in the name of competing–—in the
Collective Action Problems chapter, we consider this problem in greater depth using formal models.
Additionally, AI technologies might proliferate quickly even with strong efforts to build national leads
in AI.
International cooperation, as explored in section 9.7 , may enable states to keep their AIs safe, preserve
national competitiveness, and avoid the pitfalls of adversarial AI policy. Still, as options for cases
where cooperation fails, here we consider several policy tools for preserving national competitiveness
in AI.
Export controls.
Restrictions on the export of AI-specialized hardware can limit states’ access to
a key input into AI. Due to the extreme complexity of advanced semiconductor manufacturing, it is

514
CHAPTER 9. GOVERNANCE
very difficult for states subject to these export controls to manufacture the most advanced hardware
on their own. Additionally, the AI hardware supply chain is extremely concentrated, perhaps making
effective export controls possible without global agreement. We explore this further in the Compute
Governance section.
Immigration policy.
Immigration policy affects the flow of another important input into AI
development: talented AI researchers and engineers. Immigration could be an asymmetric advantage
of certain countries; surveys suggest that the international AI workforce tends to have much more
interest in moving to the US than China [24]. Immigrants may be more likely to spread AI capabilities
internationally, through international contacts or by returning to their native countries, but many
immigrants who are provided with the chance choose to stay in the US.
Information security.
Information security measures could slow the diffusion of AI insights
and technologies to countries or groups that lack adequate guardrails. For example, governments
could provide information security assistance to AI developers, and they could incentivize or require
developers’ compliance with information security standards.
Intelligence collection.
Collecting and analyzing intelligence on the state of AI development
in other countries would help governments avoid both unwarranted complacency and unwarranted
insecurity about their own AI industries.
Governments can use a range of measures to remain internationally competitive while maintaining
the safety of domestic AI development.
Conclusions About National Governance
National governments have many tools available for advancing AI safety. Standards, regulations, and
liability could stop dangerous AIs from being deployed, while encouraging the development of safe
AIs. Improved resilience could mitigate the damage of dangerous deployments when they do occur,
giving us more time to create safe AIs and mitigating some risk from dangerous ones. Measures such
as strong information security could allow governments to ensure domestic AI production is both safe
and competitive. Each of these approaches has largely distinct limitations—for example, regulations
may be held back by regulatory capture, while liability might impose too few penalties too late—so
effective governance may require combining many of the government actions discussed in this section.
With robust AI safety standards and regulations, a well-functioning legal framework for ensuring
liability, strong resilience against societal-scale risks from AIs, and measures for not being outpaced
internationally by unconstrained AI developers, there would be multiple layers of defense to protect
society from reckless or malicious AI development.
9.7
International Governance
In this section, we will discuss the international governance of AI systems. First, we will consider
the problem of the international governance of AI. Then, we will discuss the basics of international
governance, such as its stages and techniques. To determine what sort of international cooperation
is possible and necessary, we will discuss four key questions that are important for understanding
features of the emerging technologies we wish to regulate. Lastly, we will discuss possibilities for
the international governance of AIs, separately considering AIs made by civilians and AIs made by
militaries.

9.7. INTERNATIONAL GOVERNANCE
515
International regulation can promote global benefits and manage shared risks.
It is
important to actively regulate AI internationally. Firstly, it allows for the distribution of global benefits
that advanced AIs can provide, such as improved healthcare, increased efficiency in transportation,
and enhanced communication. Countries can work together to ensure that AI technologies are
developed and deployed in a way that benefits humanity as a whole.
Secondly, international governance of AI is necessary to manage effectively the risks associated with
its development. The risks from AI systems are not confined to the country in which they are
developed; for instance, even if an AI system is developed in the US, it is likely to be deployed around
the world, and so its impacts will be felt worldwide. Risks, such as the danger of progressively weaker
national regulations in the absence of international standards and the potential for arms races in
which actors cut corners on safety in order to compete, require international cooperation to avoid
[25]. From the point of view of public safety, the risks of negative effects of systems developed across
borders is a simple and powerful argument for international governance.
International governance of powerful technologies is challenging.
In general, international
cooperation takes place through bilateral or multilateral negotiations between relevant countries,
or through international organizations like the UN and its agencies. Even at the best of times and
with the least contentious of issues, international cooperation is slow, difficult, and often ineffectual.
Strained international relationships, such as frequent tensions between the US and China, make
successful international standards even less likely.
For the regulation of AI, we can draw analogies to the regulation of other dangerous technologies
such as nuclear, biological, and chemical weapons. While these analogies are imperfect, they give
us reasons to be concerned about the prospects of international cooperation for AI. There are few
convincing examples of agreements between major powers to limit the development of technologies
for which there were no military substitutes. However, we can learn from both the failures and
successes of past regulation: by asking questions about how AI is similar to past technologies, we can
understand what form successful and unsuccessful governance might take.
9.7.1
Forms of International Governance
Before we can consider how to govern AIs internationally, we must understand the nature of
international governance. We will first consider the different stages of international governance, from
recognizing a problem exists to ensuring that its governance is effective. Then, we will consider a
range of techniques used by international actors to ensure global governance.
Stages
We can break international governance into four stages [26]. First, issues must be recognized as
requiring governance. Then, countries must come together to agree how to govern the issue. After
that, they must actually do what was agreed. Lastly, countries’ actions must be monitored for
compliance to ensure that governance is effective into the future.
Setting agendas.
The first stage of governance is agenda-setting. This involves getting an issue
recognized as a problem needing collective action. Actors have to convince others that an issue
exists and matters. Powerful entities often want to maintain their power in the status quo, and thus
deny problems or ignore their responsibilities for dealing with them. Global governance over an
issue can’t start until it gets placed on the international agenda. Non-state actors like scientists and
advocacy groups play a key role in agenda-setting by drawing attention to issues neglected by states;
for example, scientists highlighted the emerging threat of ozone depletion, building pressure that led

516
CHAPTER 9. GOVERNANCE
to the Montreal Protocol.
Agenda-setting makes an issue a priority for collective governance. Without it, critical problems can
be overlooked due to political interests or inertia.
Policymaking.
Once an issue makes the global agenda, collections of negotiating countries or
international organizations often take the lead in policymaking. Organizations like the UN facilitate
formal negotiations between states to develop policies, as seen at major summits on climate change.
In other cases, organizations’ own procedures shape policies over time; for instance, export control
lists in the US like the International Traffic in Arms Regulations are expanded by regulators without
Congressional action. Organizations manage competing country interests to build consensus on vague
or detailed policies. For example, the International Civil Aviation Organization brought countries
together to agree on aircraft safety standards. Effective policymaking by organizations converts
identified issues into guidelines for collective action. Without actors taking responsibility for driving
the policy process, implementation lacks direction.
Implementation and enforcement.
After policies are made, the next stage is implementing
them. High-level policies are sometimes vague, allowing flexibility to apply them; for example, the
Chemical Weapons Convention relies on countries to enforce bans on chemical weapons domestically
in the ways they find most effective. Governance controls how these policies are enforced; for instance,
the International Atomic Energy Agency (IAEA) conducts inspections to verify compliance with the
Treaty on the Non-Proliferation of Nuclear Weapons (NPT). Even if actors agree on policies, acting
on them takes resources, information, and coordination. Effective implementation and enforcement
through governance converts abstract rules into concrete actions.
Evaluation, monitoring, and adjudication.
The final stage of governance is evaluating outcomes
and monitoring compliance. The organization implementing policies may perform these oversight
tasks itself. But often other actors play watchdog roles as third-party evaluators. It may be formally
established who assesses compliance, like the Organization for the Prohibition of Chemical Weapons
(OPCW). In other cases, evaluation is informal, with self-appointed civil society monitors. Both
insiders and outsiders frequently evaluate progress jointly. For example, the UN, OPCW, and NGOs
all track progress on chemical weapons disarmament. Clarifying who monitors and evaluates policies
is important to ensure accountability and transparency. Without effective evaluation, it is difficult to
learn from and improve on governance efforts over time.
Techniques
There are many ways in which countries and other international actors govern issues of global
importance. Here, we consider six ways that past international governance of potentially dangerous
technologies has taken place, ranging from individual actors making unilateral declarations to
wide-ranging, internationally binding treaties.
Unilateral commitments.
Unilateral commitments involve single actors like countries or
companies making pledges regarding their own technology development and use. For example,
President Richard Nixon terminated the US biological weapons program and dismissed its scientists
in 1969, which was instrumental in creating the far-reaching Biological Weapons Convention in 1972.
Leaders within governments and companies can make such announcements, either about what they
would do in response to others’ actions or to place constraints on their own behavior. While not
binding on others, unilateral commitments can change others’ best responses to a situation, as well
as build confidence and set precedents to influence international norms. They also give credibility in

9.7. INTERNATIONAL GOVERNANCE
517
pushing for broader agreements. Unilateral commitments lay the groundwork for wider collective
action.
Norms and standards.
International norms and technical standards steer behavior without
formal enforcement. Norms are shared expectations of proper conduct, such as the norm of “no
first use” for detonating nuclear weapons. Standards set technical best practices, like guidelines
for the safe handling and storage of explosives. Often, norms emerge through public discourse and
precedent while standards arise via expert communities. Even if they have no ability to coerce
actors, strong norms and standards shape actions nonetheless. Additionally, they make it easier to
build agreements by aligning expectations. Norms and standards are a collaborative way to guide
technology development.
Bilateral and multilateral talks.
Two or more countries directly negotiate over a variety of issues
through bilateral or multilateral talks. These allow open discussion and confidence-building between
rivals, such as granting the US and USSR the ability to negotiate over the size and composition
of their nuclear arsenals during the Cold War. Talks aim to establish understandings to avoid
technology risks like arms races. Regular talks build relationships and identify mutual interests.
While non-binding themselves, ongoing dialogue can lay the basis for making deals. Talks are essential
for compromise and consensus between nations.
Summits and forums.
Summits and forums bring together diverse stakeholders for debate and
announcements. These raise visibility on issues and build political will for action. Major powers can
make joint statements signaling priorities, like setting goals on total carbon emissions to limit the
effects of global warming. Companies and civil society organizations can announce major initiatives.
Summits set milestones for progress and mobilize public pressure.
Governance organizations.
International organizations develop governance initiatives with
diverse experts and resources. Organizations like the IAEA propose principles and governance models,
such as an inspection and verification paradigm for nuclear technology. They provide neutral forums
to build agreements. Their technical expertise also assists implementation and capacity-building.
While largely voluntary, organizations lend authority to governance efforts, often by virtue of each
of their members delegating some authority to them. Their continuity sustains attention between
summits. Organizations enable cooperation for long-term governance.
Treaties.
Treaties offer the strongest form of governance between nations, creating obligations
backed by potential punishment. Treaties have played a large role in banning certain risky military
uses of technologies, such as the 1968 Treaty on the Non-Proliferation of Nuclear Weapons. They
often contain enforcement mechanisms like inspections. However, compromising on enforceable rules
is difficult, especially between rivals. Verifying compliance with treaties can be challenging. Still, the
binding power of treaties makes them valuable despite their potential limitations.
In this section, we have considered the various stages of international governance, moving from
recognizing an issue to solving it, as well as a wide variety of different ways that enable this. This is
a large set of tools, so we will next examine four questions that inform our understanding of which
tools are most effective for AI governance.
9.7.2
Four Questions for AI Regulation
We will now consider four questions that are important for the regulation of dangerous emerging
technologies:

518
CHAPTER 9. GOVERNANCE
1. Is the technology defense-dominant or offense-dominant?
2. Can compliance with international agreements be verified?
3. Is it catastrophic for international agreements to fail?
4. Can the production of the technology be controlled?
Each of these highlights important strategic variables that we are uncertain about. They give us
insights into the characteristics of the technology we are dealing with. Consequently, they help us
illustrate what sorts of international cooperation may be possible and necessary.
Is the Technology Defense-Dominant or Offense-Dominant?
AI capabilities determine the need for international governance. Suppose we could use
some AIs to prevent other AIs from doing bad things. There would be less need for an international
regime to govern AIs. Instead, AI development would prevent the harms of AI development—such
technology is called defense-dominant. By contrast, if AI is an offense-dominant technology—if AIs
cannot manage other AIs as the technological frontier advances–—then we will need alternative
solutions [27]. Unfortunately, we have reason to believe that AIs will be offense-dominant: military
technologies usually are. It is difficult for AIs to protect against threats from other AIs; an AI that
can make the creation of engineered pandemics easy is much more likely to exist than one that can
comprehensively defend against pandemics. It is usually easier to cause harm than prevent it.
Nuclear weapons are a classic example of offense-dominant technologies: when asked how to detect a
nuclear weapon smuggled into New York in a crate, Robert Oppenheimer replied “with a screwdriver”
to open every crate [28]. In other words, there was no feasible technological solution; a social solution
was required. When dealing with offense-dominant technologies, we often need to develop external
social measures to defend against them. Similarly, if AIs prove to be offense-dominant, they will also
require social solutions to protect against their potential harmful impacts. Since the scale of the
technology is global, this will likely require international governance.
Can compliance with international agreements be verified?
Verification of compliance means agreements can regulate technology. After establishing rules about
the development and use of AIs, we need to verify whether signatories are following them. If it is easy
to evaluate this, then it is easier to implement such regimes internationally. In the case of nuclear
weapons, nuclear tests could be verified by monitoring for large explosions. Using a nuclear weapon
is impossible to do discreetly, enabling the norm of no first use. Unfortunately, verifying how and
when AIs are being developed and used may be difficult to verify–—certainly more so than observing
a nuclear explosion.
Verification is a difficult technical challenge. We lack clear methods for conducting verification,
since we do not know what to test to ensure that models are safe. Progress can be made with
serious effort; for instance, investing in the development of standard benchmarks and evaluations
that promote transparency and enable shared knowledge that other developers are using responsible
development practices (such as mitigating any power-seeking tendencies in AI systems). This will
allow the creation of clear standards that can be verified with relative ease, without compromising
the confidentiality of privileged technical details. However, this requires investment: we must make
progress on our ability to verify characteristics of AIs before we can implement effective international
regulation. Until then, establishing an equilibrium to deter the dangerous use of AIs may not be
possible.

9.7. INTERNATIONAL GOVERNANCE
519
Is It Catastrophic for International Agreements to Fail?
Whether agreements failing is catastrophic changes how permissive they can be.
Consider
an example of an international treaty that sets limits on the amount of compute that organizations
can use to train their AIs. If an AI trained with more compute than the specified threshold poses
a significant risk of catastrophic consequences—just as even a single nuclear weapon can have
devastating effects–—then the treaty must focus on preventing this possibility entirely. In such cases,
deviations from the agreement cannot be permitted. On the other hand, if most AIs trained with
more compute than the threshold amount pose little immediate danger, we can adopt a more lenient
approach. In this scenario, the agreement can prioritize monitoring and disincentivizing deviations
through punishments after the fact. In general, if even one actor deviating from an agreement is
dangerous, then it must be much stricter.
Of course, stricter agreements are more difficult to create. Many international agreements present
small punishments for deviation or include escape clauses—allowing occasional exemptions from
obligations—to encourage states to sign the agreements. If agreements about the development and
use of AIs cannot contain such clauses, then it will be more difficult to create widespread agreement
on them.
Can the Production of the Technology Be Controlled?
Controlling production changes which actors are needed in order to succeed with
international regulation. Suppose the US could gain complete control of all the compute required
to produce AIs. Then, the US would be able to create and implement regulations on AI development
that apply globally, since they can withhold compute from any non-compliant actors. More generally,
if a small group of safety-conscious countries can block actors from gaining control of the factors of
production, then they can create an international regime themselves. This makes it much easier to
achieve international governance since it doesn’t require the cooperation of many foreign actors with
distinct interests.
Summary
By answering these questions, we can make important decisions about international governance.
First, we understand whether we need international governance or whether AIs will be able to
mitigate the harmful effects of other AIs. Second, we determine whether international agreements
are possible, since we need to verify whether actors are following the rules. Third, we can decide
what features these agreements might have; specifically, we can determine whether they must be
extremely restrictive to avoid catastrophes from a single deviation. Lastly, we consider who must
agree to govern AI by understanding whether or not a few countries can impose regulations on the
world. Even if some of these answers imply that we live in high-risk worlds, they guide us towards
actions that help mitigate this risk. We can now consider what these actions might be.
9.7.3
What Can Be Included in International Agreements?
We will now consider the specific tools that might be useful for international governance of AI. We
separate this discussion into regulating AIs produced by the private sector and AIs produced by
militaries, since these have different features and thus require different controls. For civilian AIs,
certification of compliance with international standards is the key precedent to follow. For military
AIs, we can turn to non-proliferation agreements, verification schemes, or the establishment of AI
monopolies.

520
CHAPTER 9. GOVERNANCE
Civilian AI
Regulating the private sector is important and tractable.
Much of the development of
advanced AIs is seemingly taking place in the private sector. As a result, ensuring that private actors
do not develop or misuse harmful technologies is a priority. Regulating civilian development and
use is also likely to be more feasible than regulating militaries, although this might be hindered by
overlaps between private and military applications of AIs (such as civilian defense contracts) and
countries’ reluctance to allow an international organization access to their firms’ activities.
Certification has proven to be an effective tool.
One proposal for civilian governance involves
certifying jurisdictions for having and enforcing appropriate regulation [29]. Some international
organizations, such as the International Civilian Aviation Organization (ICAO), the International
Maritime Organization (IMO), and the Financial Action Task Force (FATF), follow a similar
approach. These organizations leave enforcement in the hands of domestic regulators, but they check
that domestic regulators have appropriate procedures. States have incentives to comply with the
international organizations’ standards because of the ecosystems in which they are embedded. The
Federal Aviation Administration, for instance, can deny access to US airspace to states that violate
ICAO standards. In the case of AI, states might deny access to markets and factors of production to
the firms of jurisdictions that violate international standards.
Military AI
Governing military AIs is different from governing civilian ones. Most of the options for governing
AI used by militaries can be described as drawing from one of three regimes: nonproliferation plus
norms of use, verification, or monopoly.
Option 1: Nonproliferation plus Norms of Use.
The nonproliferation regime, alongside norms
against first use of nuclear weapons–—along with a measure of luck—enabled the world to survive
the Cold War. This regime was centered around the Nonproliferation Treaty (NPT), an international
agreement primarily concerned with stopping the spread of nuclear weapons, and the International
Atomic Energy Agency (IAEA), an international organization for the governing of nuclear energy.
In addition, nonproliferation was a pillar of super-power foreign policy during the Cold War [30].
Both the US and USSR made many threats and promises, including guarantees of assistance to third
parties, to reduce the likelihood of nuclear weapons being used anywhere. A similar international
regime for AI could be similarly helpful; for instance, it could enable countries to cooperate to avoid
AI races and encourage the development of safer AI by establishing standards and guidelines.
Nonproliferation may be insufficient.
Suppose investing in AI continues to reliably increase
system capabilities—unlike with nuclear weapons, where the security gain from additional weapons
of mass destruction is low. Then, countries who already have advanced AI will have strong incentives
to continue to compete with each other. As we have explored, increasing AI capabilities is likely
to increase AI risk. This means that nonproliferation plus norms of use might be insufficient for
controlling advanced, weaponized AIs.
Norms may be difficult to establish.
With nuclear weapons, the norms of “no first use” and
“mutually assured destruction” created an equilibrium that limited the use of nuclear weapons. With
AIs, this might be more difficult for a variety of reasons: for instance, AIs have a much broader
field of capabilities (as opposed to a nuclear weapon detonating) and AIs are already being widely
used. Monitoring or restricting the development or use of new AI systems requires deciding precisely
which capabilities are prohibited. If we cannot decide which capabilities are the most dangerous,

9.7. INTERNATIONAL GOVERNANCE
521
it is difficult to decide on a set of norms, which means we cannot rely on norms to encourage the
development of safe AIs.
Option 2: Verification.
Many actors might be happy to limit their own development of military
technology if they can be certain their adversaries are doing the same. Verification of this fact
can enable countries to govern each other, thereby avoiding arms races. The Chemical Weapons
Convention, for instance, has provisions for verifying member states’ compliance, including inspections
of declared sites.
When it comes to critical military technologies, however, verification regimes might need to be
invasive; for instance, it might be necessary to have the authority to inspect any site in a country.
It is unclear how they could function in the face of resistance from domestic authorities. For these
reasons, a system which relies on inspection and similar police—like methods might be entirely
infeasible—unless all relevant actors agree to mutually verify and govern.
Option 3: Monopoly.
The final option is a monopoly over the largest-scale computing processes,
which removes the incentive for risk-taking by removing adversarial competition. Such monopolies
might arise in several ways. Established AI firms may benefit from market forces like economies of
scale, such as being able to attract the best talent and invest profits from previous ventures into new
R&D. Additionally, they might have first-mover advantages from retaining customers or exercising
influence over regulation. Alternatively, several actors might agree to create a monopoly: there are
proposals like “CERN for AI” which call for this explicitly [31]. Such organizations must be focused
on the right mission, perhaps using tools from corporate governance, which is a non-trivial task. If
they are, however, then they present a much easier route to safe AI than verification and international
norms and agreements.
Conclusions About International Governance
International governance is important and difficult.
International regulation of AI is crucial
to distribute its global benefits and manage associated risks. Since AI’s impacts are not restricted
to its country of development, an internationally coordinated approach ensures that advantages,
like improved healthcare, are accessible worldwide, and risks, such as weak regulations or safety
shortcuts, are avoided. This approach will need to create awareness that a problem exists, create
policies that tackle it, oversee the implementation of these policies, and ensure compliance with
them through verification. It can use a wide variety of tools, including unilateral declarations, talks
through meetings, forums, and international organizations, and the creation of norms, standards,
and binding treaties. From experience with other dangerous technologies, we know that this global
cooperation is challenging to achieve.
Understanding features of AI is required for effective governance.
We need to understand
whether AI is an offense-dominant technology which poses significant risks if even one powerful
system is imperfect. This tells us whether we need international cooperation of AIs and, if we do,
what sort of enforcement features such agreements would require to be effective. Further, we need
to know whether we can verify whether AI development meets a comprehensive list of regulations
and standards and control its production when it does not. If this is possible, we can conclude that
international regulation of AI is possible, and might be possible with the cooperation of just a small
group of safety-conscious countries.
Understanding the landscape, we can govern civilian and military AIs.
Civilian AIs largely
originate from the private sector, making certification of compliance with international standards
vital. Existing organizations, like the International Civilian Aviation Organization, effectively use

522
CHAPTER 9. GOVERNANCE
certification; states could restrict market access to firms that violate international AI standards.
For military AIs, options include nonproliferation agreements plus established norms, verification of
technology development between nations, or creating monopolies on large-scale computing. If the
risks of advanced AI are as great as some fear, it is likely that the world needs such regulation. Each
proposal will require overcoming serious challenges, both social and technical.
9.8
Conclusion
In the introduction, we laid out the purpose of this chapter: understanding the fundamentals of
how to govern AI. In other words, we wanted to understand how to organize, manage, and steer the
development and deployment of AI technologies using an array of tools including norms, policies,
and institutions. To set the scene, we considered a set of actors and tools that governance needs to
consider.
Takeoff.
We explored potential scenarios for advanced AI takeoff. There is serious uncertainty
regarding when these transformative AIs will arise and whether progress will follow continuous trends
or exhibit abrupt, discontinuous jumps in capabilities. However, once they emerge, advanced AIs
would likely massively accelerate technological advancement and economic growth. This acceleration
could profoundly reshape human civilization, potentially within a short timeframe. Careful governance,
informed by answers to questions about how AI development will progress, will be essential for
steering it in broadly positive directions that benefit humanity.
Distribution.
We then explored three key dimensions around which the impacts of advanced AI
systems may be distributed: power among AIs, access to AIs, and costs/benefits of AIs. First, in
terms of distributing power among AIs, concentrating capabilities and control in a single AI or small
group of AIs poses risks like permanently locking in certain values or goals. However, distributing
power more widely among a large, diverse ecosystem of AIs also has downsides, like increasing the
potential for misuse or making it more difficult to correct AI systems that begin behaving undesirably.
Second, regarding access to AIs, it is unclear whether availability should be tightly restricted or
widely open to the general public. Limiting access risks misuse by powerful groups whereas open
access risks misuse by malicious actors. Third, equitable distribution of the economic and social
impacts of AI will be crucial, including dealing with potential technologically-induced unemployment
while ensuring that productivity gains are shared broadly rather than accruing only to a small group
like AI developers and investors. Distributing these three key aspects of advanced AI fairly will
require thoughtful governance.
Compute Governance.
Next, we explored how governing access to and use of the computing
resources that enable AI development could provide an important lever for influencing the trajectory
of AI progress. Compute is an indispensable input to developing advanced AI capabilities. It also has
properties like physicality, excludability, and quantifiability that make governing it more feasible than
other inputs like data and algorithms. Compute governance can allow control over who is granted
access to what levels of computational capabilities, controlling who can create advanced AIs. It also
facilitates setting and enforcing safety standards for how compute can be used, enabling the steering
of AI development.
Corporate Governance.
At the organizational level, we discussed how aspects of corporate
structure and governance like legal form, ownership models, policies, practices, and assurance
mechanisms can help steer technology companies away from solely maximizing profits and shareholder
value. Instead, they can guide corporate AI work in directions that prioritize broader societal interests

9.9. LITERATURE
523
like safety, fairness, privacy, and ethics. However, achieving this through corporate governance alone
may prove challenging, making complementary approaches at other levels vital.
National Governance.
For governance at the national level, we explored policy tools governments
can use to align AI development with public interests, both in the public and private sectors. These
included safety regulations, liability rules that make AI developers internalize potential damages,
investments to improve societal resilience against AI risks, and measures for maintaining national
competitiveness in AI while still ensuring domestic safety. Combinations of these policy mechanisms
can help nations steer AI progress in their jurisdictions towards beneficial ends.
International Governance.
At the international level, governance of AI systems is made
challenging by issues like verifying adherence to agreements. However, international cooperation is
essential for managing risks from AI and distributing benefits globally. Approaches like international
safety standards for civilian AI applications, agreements to limit military uses of AI, and proposals
for concentrating advanced AI development within select transnational groups, may all help promote
global flourishing. A lack of any meaningful international governance could lead to a dangerous spiral
of competitive dynamics and erosion of safety standards.
Conclusion.
This chapter provides an overview of a diverse selection of governance solutions
spanning from policies within technology firms to agreements between nations in global institutions.
The arrival of transformative AI systems will require thoughtful governance at multiple levels in order
to steer uncertain technological trajectories in broadly beneficial directions aligned with humanity’s
overarching interests. The deliberate implementation of policies, incentives and oversight will be
essential to realizing the potential of AI to improve human civilization rather than destroy it.
9.9
Literature
9.9.1
Recommended Reading
• Jaime Sevilla et al. Compute Trends Across Three Eras of Machine Learning. 2022. arXiv:
2202.05924 [cs.LG]
• Ege Erdil and Tamay Besiroglu.
Explosive growth from AI automation: A review of the
arguments. Papers 2309.11690. arXiv.org, 2023. url: https://ideas.repec.org/p/arx/
papers/2309.11690.html
• Saif M. Khan.
AI Chips: What They Are and Why They Matter.
2020.
url: https:
//doi.org/10.51593/20190014
• Yonadav Shavit. What does it take to catch a Chinchilla? Verifying Rules on Large-Scale
Neural Network Training via Compute Monitoring. 2023. arXiv: 2303.11341 [cs.LG]
• Markus Anderljung et al. Frontier AI Regulation: Managing Emerging Risks to Public Safety.
2023. arXiv: 2307.03718 [cs.CY]
• Robert Trager et al. “International Governance of Civilian AI: A Jurisdictional Certification
Approach”. In: Risk, Regulation, & Policy eJournal (2023). url: http://dx.doi.org/10.
2139/ssrn.4579899
• Lewis Ho et al. International Institutions for Advanced AI. 2023. arXiv: 2307.04699 [cs.CY]
• Matthijs Maas. Advanced AI Governance. Tech. rep. Legal priorities Project, 2023. url: https:
/ / docs . google . com / document / d / 1pwwNHvNeJneBA2t2xaP31lVv1lSpa36w8kdryoS5768 /
edit

524
CHAPTER 9. GOVERNANCE
9.9.2
References
[1]
Tom Davidson. Could Advanced AI Drive Explosive Economic Growth? url: https : / / www .
openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/.
[2]
What Really Made Hinton an AI Doomer. 2023. url: https://www.wired.com/story/geoffrey-
hinton- ai- chatgpt- dangers/#:~:text=Hinton%20concluded%20that%20as%20AI, to%20be%
20five%20to%2020.%E2%80%9D.
[3]
Ajeya Cotra. Forecasting TAI with biological anchors. 2020. url: https://docs.google.com/
document/d/1k7qzzn14jgE-Gbf0CON7_Py6tQUp2QNodr_8VAoDGnY/edit#heading=h.r808lkmw6i0l.
[4]
Ege Erdil and Tamay Besiroglu. Algorithmic progress in computer vision. 2023. arXiv: 2212.05153
[cs.CV].
[5]
Nick Bostrom. What is a Singleton? Linguistic and Philosophical Investigations, 5 (2), 48-54. 2006.
[6]
K. Eric Drexler. Reframing Superintelligence: Comprehensive AI Services as General Intelligence.
Tech. rep. Future of Humanity Institute, 2019.
[7]
Erik Brynjolfsson. “The turing trap: The promise & peril of human-like artificial intelligence”. In:
Daedalus 151.2 (2022), pp. 272–287. url: https://digitaleconomy.stanford.edu/news/the-
turing-trap-the-promise-peril-of-human-like-artificial-intelligence/.
[8]
Daron Acemoglu and Pascual Restrepo. “Robots and jobs: Evidence from US labor markets”. In:
Journal of political economy 128.6 (2020), pp. 2188–2244.
[9]
Maximiliano Dvorkin and Hannah Shell. “The growing skill divide in the US labor market”. In: Federal
Reserve Bank of St. Louis: On the Economy Blog 18 (2017).
[10]
Yuval Noah Harari. Homo Deus: A brief history of tomorrow. random house, 2016.
[11]
Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An Overview of Catastrophic AI Risks.
2023. arXiv: 2306.12001 [cs.CY].
[12]
Daron Acemoglu, Andrea Manera, and Pascual Restrepo. Does the US tax code favor automation?
Tech. rep. National Bureau of Economic Research, 2020.
[13]
Viral V Acharya, Deniz Anginer, and A Joseph Warburton. “The end of market discipline? Investor
expectations of implicit government guarantees”. In: Investor Expectations of Implicit Government
Guarantees (May 1, 2016) (2016).
[14]
Richard Sutton. The Bitter Lesson. url: http : / / www . incompleteideas . net / IncIdeas /
BitterLesson.html.
[15]
Dario Amodei et al. AI and compute. OpenAI. 2018.
[16]
Fabio L. Urbina et al. “Dual use of artificial-intelligence-powered drug discovery”. In: Nature Machine
Intelligence (2022).
[17]
Epoch. Key trends and figures in Machine Learning. 2023. url: https://epochai.org/trends.
[18]
Zachary Arnold et al. ETO Supply Chain Explorer. Dataset. 2022. url: https://cset.georgetown.
edu/publication/eto-supply-chain-explorer/.
[19]
Saif M Khan, Alexander Mann, and Dahlia Peterson. “The semiconductor supply chain: Assessing
national competitiveness”. In: Center for Security and Emerging Technology 8.8 (2021).
[20]
Michael C Jensen and William H Meckling. “Theory of the firm: Managerial behavior, agency costs
and ownership structure”. In: Corporate governance. Gower, 2019, pp. 77–132.
[21]
Milton Friedman. “The Social Responsibility of Business Is To Increase Its Profits”. In: vol. 32. Jan.
2007, pp. 173–178. isbn: 978-3-540-70817-9. doi: 10.1007/978-3-540-70818-6_14.
[22]
Peter Cihon, Jonas Schuett, and Seth D. Baum. “Corporate Governance of Artificial Intelligence in the
Public Interest”. In: Information 12.7 (July 2021), p. 275. issn: 2078-2489. doi: 10.3390/info12070275.
url: http://dx.doi.org/10.3390/info12070275.
[23]
Jonas Schuett. AGI labs need an internal audit function. 2023. arXiv: 2305.17038 [cs.CY].

9.9. LITERATURE
525
[24]
Remco Zwetsloot. “Winning the Tech Talent Competition”. In: Center for Strategic and International
Studies (2021), p. 2.
[25]
Stuart Armstrong, Nick Bostrom, and Carl Shulman. “Racing to the precipice: a model of artificial
intelligence development”. In: AI & society 31 (2016), pp. 201–206.
[26]
Deborah D Avant, Martha Finnemore, and Susan K Sell. Who governs the globe? Vol. 114. Cambridge
University Press, 2010.
[27]
Ben Garfinkel and Allan Dafoe. “How does the offense-defense balance scale?” In: Emerging Technologies
and International Stability. Routledge, 2021, pp. 247–274.
[28]
Wolfgang KH Panofsky and Jean Marie Deken. Panofsky on Physics, Politics and Peace. 2008. doi:
https://doi.org/10.1007/978-0-387-69732-1.
[29]
Robert Trager et al. “International Governance of Civilian AI: A Jurisdictional Certification Approach”.
In: Risk, Regulation, & Policy eJournal (2023). url: http://dx.doi.org/10.2139/ssrn.4579899.
[30]
Francis J. Gavin. “Strategies of Inhibition: U.S. Grand Strategy, the Nuclear Revolution, and
Nonproliferation”. In: International Security 40.1 (July 2015), pp. 9–46. issn: 0162-2889. doi:
10.1162/ISEC_a_00205. eprint: https://direct.mit.edu/isec/article-pdf/40/1/9/1843553/
isec\_a\_00205.pdf. url: https://doi.org/10.1162/ISEC%5C_a%5C_00205.
[31]
Preempting a Generative AI Monopoly. 2023. url: https : / / www . project - syndicate . org /
commentary / preventing - tech - giants - from - monopolizing - artificial - intelligence -
chatbots-by-diane-coyle-2023-02.
[32]
Jaime Sevilla et al. Compute Trends Across Three Eras of Machine Learning. 2022. arXiv: 2202.05924
[cs.LG].
[33]
Ege Erdil and Tamay Besiroglu. Explosive growth from AI automation: A review of the arguments.
Papers 2309.11690. arXiv.org, 2023. url: https://ideas.repec.org/p/arx/papers/2309.11690.
html.
[34]
Saif M. Khan. AI Chips: What They Are and Why They Matter. 2020. url: https://doi.org/10.
51593/20190014.
[35]
Yonadav Shavit. What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural
Network Training via Compute Monitoring. 2023. arXiv: 2303.11341 [cs.LG].
[36]
Markus Anderljung et al. Frontier AI Regulation: Managing Emerging Risks to Public Safety. 2023.
arXiv: 2307.03718 [cs.CY].
[37]
Lewis Ho et al. International Institutions for Advanced AI. 2023. arXiv: 2307.04699 [cs.CY].
[38]
Matthijs Maas. Advanced AI Governance. Tech. rep. Legal priorities Project, 2023. url: https:
//docs.google.com/document/d/1pwwNHvNeJneBA2t2xaP31lVv1lSpa36w8kdryoS5768/edit.

Part V
Appendix
526


Acknowledgements
It would not have been possible to complete a book of this scale on my own without spending several
years on it, by which point it might have come out too late to help readers to understand and address
important AI risks. To avoid this issue, I worked with various contributors to complete this textbook
in a more timely manner. With a handful of exceptions, I was responsible for deciding the contents of
the chapters, sections, and subsections and developed a detailed paragraph-level outline for these, as
well as providing multiple rounds of feedback and revisions. The other contributors to this textbook
wrote the bulk of the content and helped to organize, expound on, and explicate these ideas clearly.
I would like to acknowledge the following contributors for each chapter:
1. Artificial Intelligence Fundamentals with Anna Swanson, Jeremy Hadfield, Adam Elwood,
and Corin Katzke
2. Single-Agent Safety with Jesse Hoogland, Adam Khoja, Thomas Woodside, Abra Ganz,
Aidan O’Gara, Spencer Becker-Kahn, Jeremy Hadfield, and Joshua Clymer
3. Normative Ethics with Toby Tremlett and Suryansh Mehta
4. Utility Functions with Suryansh Mehta and Shankara Srikantan
5. Machine Ethics with Suryansh Mehta, Shankara Srikantan, Aron Vallinder, Jeremy Hadfield,
and Toby Tremlett
6. Complex Systems with Laura Hiscott and Max Heitmann
7. Safety Engineering Systems with Laura Hiscott and Suryansh Mehta
8. Collective Action Problems with Ivo Andrews, Sasha Cadariu, Avital Morris, and David
Lambert
9. Governance with Jonas Schuett, Robert Trager, Lennart Heim, Matthew Barnett, Mauricio
Baker, Thomas Woodside, Suryansh Mehta, Laura Hiscott, and Shankara Srikantan
I would also like to thank the following key individuals for their help in driving this project forward:
• Jay Kim and William Hodgkins (Project Manager)
• Rebecca Rothwell and Suryansh Mehta (Editor)
• Corin Katzke (Course Manager)
I also want to thank Mantas Mazeika, Paul Salmon, the 2023 CAIS philosophy fellows, and the
participants in our AI Safety Sprint Course for their invaluable feedback throughout the writing and
revision process. All errors that remain are, of course, my own.
Dan Hendrycks
528

Utility Functions
B.1
Utility and Utility Functions
B.1.1
Fundamentals
A utility function is a mathematical representation of preferences.
A utility function, u,
takes inputs like goods or situations and outputs a value called utility. Utility is a measure of how
much an agent prefers goods and situations relative to other goods and situations.
Suppose we offer Alice some apples, bananas, and cherries. She might have the following utility
function for fruits:
u(fruits) = 12a + 10b + 2c,
where a is the number of apples, b is the number of bananas, and c is the number of cherries that she
consumes. Suppose Alice consumes no apples, one banana, and five cherries. The amount of utility
she gains from her consumption is calculated as
u(0 apples, 1 banana, 5 cherries) = (12 · 0) + (10 · 1) + (2 · 5) = 20.
The output of this function is read as “20 units of utility” for short. These units are arbitrary and
reflect the level of Alice’s utility. We can use utility functions to quantitatively represent preferences
over different combinations of goods and situations. For example, we can rank Alice’s preferences
over fruits as
apple ≻banana ≻cherry,
where ≻represents preference, such that what comes before the symbol is preferred to what comes
after it. This follows from the fact that Alice gains 12 units from an apple, 10 units from a banana,
and 2 units from a cherry. The advantage of having a utility function as opposed to just an explicit
ranking of goods is that we can directly infer information about more complex goods. For example,
we know
u(1 banana, 5 cherries) = 20 > u(1 apple) = 12 > u(1 banana) = 10.
Utility functions, if accurate, reveal what options agents would prefer and choose.
If
told to choose only one of the three fruits, Alice would pick the apple, since it gives her the most
utility. Her preference follows from rational choice theory, which proposes that individuals, acting
in their own self-interest, make decisions that maximize their self-interest. This view is only an
approximation to human behavior. In this chapter we will discuss how rational choice theory is an
imperfect but useful way to model choices. We will also refer to individuals who behave in coherent
ways that help maximize utility as agents.
We explore concepts about utility functions that are useful for thinking about AIs,
humans, and organizations like companies and states.
First, we introduce Bernoulli utility
functions, which are conventional utility functions that define preferences over certain outcomes
like the example above. We later discuss von Neumann-Morgenstern utility functions, which extend
preferences to probabilistic situations, in which we cannot be sure which outcome will occur. Expected
utility theory suggests that rationality is the ability to maximize preferences. We consider the
relevance of utility functions to AI corrigibility—the property of being receptive to corrections—and
see how this might be a source of tail risk. Much of this chapter focuses on how utility functions help
understand and model agents’ attitudes toward risk. Finally, we examine non-expected utility theories,
which seek to rectify some shortcomings of conventional expected utility theory when modeling
real-life behavior.
529

530
APPENDIX B. UTILITY FUNCTIONS
B.1.2
Motivations for Learning About Utility Functions
Utility functions are a central concept in economics and decision theory.
Utility functions
can be applied to a wide range of problems and agents, from rats finding cheese in a maze to humans
making investment decisions to countries stockpiling nuclear weapons. Conventional economic theory
assumes that people are rational and well-informed, and make decisions that maximize their self-
interest, as represented by their utility function. The view that individuals will choose options that
are likely to maximize their utility functions, referred to as expected utility theory, has been the major
paradigm in real-world decision making since the Second World War [17]. It is useful for modeling,
predicting, and encouraging desired behavior in a wide range of situations. However, as we will
discuss, this view does not perfectly capture reality, because individuals can often be irrational, lack
relevant knowledge, and frequently make mistakes.
The objective of maximizing a utility function can cause intelligence.
The reward hypothesis
suggests that the objective of maximizing some reward is sufficient to drive behavior that exhibits
intelligent traits like learning, knowledge, perception, social awareness, language, generalization, and
more [18]. The reward hypothesis implies that artificial agents in rich environments with simple
rewards could develop sophisticated general intelligence. For example, an artificial agent deployed
with the goal of maximizing the number of successful food deliveries may develop relevant geographical
knowledge, an understanding of how to move between destinations efficiently, and the ability to
perceive potential dangers. Therefore, the construction and properties of the utility function that
agents maximize are central to guiding intelligent behavior.
Certain artificial agents may be approximated as expected utility maximizers.
Some
artificial intelligences are agent-like. They are programmed to consider the potential outcomes of
different actions and to choose the option that is most likely to lead to the optimal result. It is a
reasonable approximation to say that many artificial agents make choices that they predict will give
them the highest utility. For instance, in reinforcement learning (introduced in the previous chapter),
artificial agents explore their environment and are rewarded for desirable behavior. These agents are
explicitly constructed to maximize reward functions, which strongly shape an agent’s internal utility
function, should it exist, and its dispositions. This view of AI has implications for how we design
and evaluate these systems—we need to ensure that their value functions promote human values.
Utility functions can help us reason about the behavior of AIs, as well as the behavior of powerful
actors that direct AIs, such as corporations or governments.
Utility functions are a key concept in AI safety.
Utility functions come up explicitly and
implicitly at various times throughout this book, and are useful for understanding the behavior of
reward-maximizing agents, as well as humans and organizations involved in the AI ecosystem. They
will also come up in our chapter on Machine Ethics, when we consider that some advanced AIs may
have utility functions make up the social welfare function they seek to increase. In the Collective
Action Problems chapter, we will continue our discussion of rational agents that seek to maximize
their own utility.
B.2
Properties of Utility Functions
Overview.
In this section, we will formalize our understanding of utility functions. First, we will
introduce Bernoulli utility functions, which are simple utility functions that allow an agent to select
between different choices with known outcomes. Then we will discuss von Neumann-Morgenstern
utility functions, which model how rational agents select between choices with probabilistic outcomes
based on the concept of expected utility, to make these tools more generally applicable to the choices

B.2. PROPERTIES OF UTILITY FUNCTIONS
531
under uncertainty. Finally, we will describe a solution to a famous puzzle applying expected utility—
the St. Petersburg Paradox—to see why expected utility is a useful tool for decision making.
Establishing these mathematical foundations will help us understand how to apply utility functions
to various actors and situations.
B.2.1
Bernoulli Utility Functions
Bernoulli utility functions represent an individual’s preferences over potential outcomes.
Suppose we give people the choice between an apple, a banana, and a cherry. If we already know each
person’s utility function, we can deduce, predict, and compare their preferences In the introduction,
we met Alice, whose preferences are represented by the utility function over fruits:
u(f) = 12a + 10b + 2c.
This is a Bernoulli utility function.
Bernoulli utility functions can be used to convey the strength of preferences across
opportunities.
In their most basic form, Bernoulli utility functions express ordinal preferences by
ranking options in order of desirability. For more information, we can consider cardinal representations
of preferences. With cardinal utility functions, numbers matter: while the units are still arbitrary,
the relative differences are informative.
To illustrate the difference between ordinal and cardinal comparisons, consider how we talk about
temperature. When we want to precisely convey information about temperature, we use a cardinal
measure like Celsius or Fahrenheit: “Today is five degrees warmer than yesterday.” We could have
also accurately, but less descriptively, used an ordinal descriptor: “Today is warmer than yesterday.”
Similarly, if we interpret Alice’s utility function as cardinal, we can conclude that she feels more
strongly about the difference between a banana and a cherry (8 units of utility) than she does about
the difference between an apple and a banana (2 units). We can gauge the relative strength of Alice’s
preferences from a utility function.
B.2.2
Von Neumann-Morgenstern Utility Functions
Von Neumann-Morgenstern utility functions help us understand what people prefer
when outcomes are uncertain. We do not yet know how Alice values an uncertain situation,
such as a coin flip. If the coin lands on heads, Alice gets both a banana and an apple. But if it
lands on tails, she gets nothing. Now let’s say we give Alice a choice between getting an apple,
getting a banana, or flipping the coin. Since we know her fruit Bernoulli utility function, we know
her preferences between apples and bananas, but we do not know how she compares each fruit to the
coin flip. We’d like to convert the possible outcomes of the coin flip into a number that represents the
utility of each outcome, which can then be compared directly against the utility of receiving the fruits
with certainty. The von Neumann-Morgenstern (vNM) utility functions help us do this [19]. They
are extensions of Bernoulli utility functions, and work specifically for situations with uncertainty,
represented as lotteries (denoted L), like this coin flip. First, we work through some definitions and
assumptions that allow us to construct utility functions over potential outcomes, and then we explore
the relation between von Neumann-Morgenstern utility functions and expected utility.
A lottery assigns a probability to each possible outcome.
Formally, a lottery L is any set
of possible outcomes, denoted oi, and their associated probabilities, denoted pi. Consider a simple
lottery: a coin flip where Alice receives an apple on heads, and a banana on tails. This lottery

532
APPENDIX B. UTILITY FUNCTIONS
has possible outcomes apple and banana, each with probability 0.5. If a different lottery offers a
cherry with certainty, it would have only the possible outcome cherry with probability 1. Objective
probabilities are used when the probabilities are known, such as when calculating the probability of
winning in casino games like roulette. In other cases where objective probabilities are not known, like
predicting the outcome of an election, an individual’s subjective best-guess could be used instead.
So, both uncertain and certain outcomes can be represented by lotteries.
A Note on Expected Value vs. Expected Utility
An essential distinction in this chapter is that between expected value and expected utility.
Expected value is the average outcome of a random event.
While most lottery
tickets have negative expected value, in rare circumstances they have positive expected value.
Suppose a lottery has a jackpot of 1 billion dollars. Let the probability of winning the jackpot
be 1 in 300 million, and let the price of a lottery ticket be $2. Then the expected value is
calculated by adding together each possible outcome by its probability of occurrence. The
two outcomes are (1) that we win a billion dollars, minus the cost of $2 to play the lottery,
which happens with probability one in 300 million, and (2) that we are $2 in debt. We can
calculate the expected value with the formula:
1
300 million · ($1 billion −$2) +

1 −
1
300 million

· (−$2) ≈$1.33.
The expected value of the lottery ticket is positive, meaning that, on average, buying the
lottery ticket would result in us receiving $1.33.
Generally, we can calculate expected value by multiplying each outcome value, oi, with its
probability p, and sum everything up over all n possibilities:
E [L] = o1 · p1 + o2 · p2 + ·s + on·n .
Expected utility is the average utility of a random event.
Although the lottery has
positive expected value, buying a lottery ticket may still not increase its expected utility.
Expected utility is distinct from expected value: instead of summing over the monetary
outcomes (weighing each outcome by its probability), we sum over the utility the agent
receives from each outcome (weighing each outcome by its probability).
If the agent’s utility function indicates that one “util” is just as valuable as one dollar, that is
u ($x) = x, then expected utility and expected value would be the same. But suppose the
agent’s utility function were a different function, such as u ($x) = x1/3. This utility function
means that the agent values each additional dollar less and less as they have more and more
money.
For example, if an agent with this utility function already has $500, an extra dollar would
increase their utility by 0.05, but if they already have $200,000, an extra dollar would increase
their utility by only 0.0001. With this utility function, the expected utility of this lottery
example is negative:
1
300 million · (1 billion −2)1/3 +

1 −
1
300 million

· (−2)1/3 ≈−1.26.

B.2. PROPERTIES OF UTILITY FUNCTIONS
533
Consequently, expected value can be positive while expected utility can be negative, so the
two concepts are distinct.
Generally, expected utility is calculated as:
E[u(L)] = u(o1) · p1 + u(o2) · p2 + · · · + u(on) · pn.
According to expected utility theory, rational agents make decisions that maximize
expected utility.
Von Neumann and Morgenstern proposed a set of basic propositions called
axioms that define an agent with rational preferences. When an agent satisfies these axioms, their
preferences can be represented by a von Neumann-Morgenstern utility function, which is equivalent
to using expected utility to make decisions. While expected utility theory is often used to model
human behavior, it is important to note that it is an imperfect approximation. In the final section of
this chapter, we present some criticisms of expected utility theory and the vNM rationality axioms as
they apply to humans. However, artificial agents might be designed along these lines, resulting in an
explicit expected utility maximizer, or something approximating an expected utility maximizer. The
von Neumann-Morgenstern rationality axioms are listed below with mathematically precise notation
for sake of completeness, but a technical understanding of them is not necessary to proceed with the
chapter.
Von Neumann-Morgenstern Rationality Axioms.
When the following axioms are satisfied,
we can assume a utility function of an expected utility form, where agents prefer lotteries that have
higher expected utility [19]. L is a lottery. LA ≽LB means that the agent prefers lottery A to
lottery B, whereas LA ∼LB means that the agent is indifferent between lottery A and lottery B.
These axioms and conclusions that can be derived from them are contentious, as we will see later on
in this chapter. There are six such axioms, that we can split into two groups.
The first two axioms may seem obvious, but are nonetheless essential:
1. Monotonicity: Agents prefer higher probabilities of preferred outcomes.
2. Decomposability: The agent is indifferent between two lotteries that share the same probabilities
for all the same outcomes, even if they are described differently.
The remaining four axioms are:
1. Completeness: The agent can rank their preferences over all lotteries. For any two lotteries, it
must be that LA ≽LB or LB ≽LA.
2. Transitivity: If LA ≽LB and LB ≽LC, then LA ≽LC.
3. Continuity: For any three lotteries, LA ≽LB ≽LC, there exists a probability p ∈[0, 1] such
that pLA + (1 −p) LC ∼LB. This means that the agent is indifferent between LB and some
combination of the worse lottery LC and the better lottery LA. In practice, this means that
agents’ preferences change smoothly and predictably with changes in options.
4. Independence: The preference between two lotteries is not impacted by the addition of equal
probabilities of a third, independent lottery to each lottery. That is, LA ≽LB is equivalent to
pLA + (1 −p) LC ≽pLB + (1 −p) LC for any LC. >

534
APPENDIX B. UTILITY FUNCTIONS
Form of von Neumann-Morgenstern utility functions.
If an agent’s preferences are consistent
with the above axioms, their preferences can be represented by a vNM utility function. This utility
function, denoted by a capital U, is simply the expected Bernoulli utility of a lottery. That is, a vNM
utility function takes the Bernoulli utility of each outcome, multiplies each with its corresponding
probability of occurrence, and then adds everything up. Formally, an agent’s expected utility for a
lottery L is calculated as:
U (L) = u (o1) · p1 + u (o2) · p2 + · · · + u (on) · pn,
so expected utility can be thought of as a weighted average of the utilities of different outcomes.
This is identical to the expected utility formula we discussed above—we sum over the utilities of
all the possible outcomes, each multiplied by its probability of occurrence. With Bernoulli utility
functions, an agent prefers a to b if and only if their utility from receiving a is greater than their
utility from receiving b. With expected utility, an agent prefers lottery LA to lottery LB if and only
if their expected utility from lottery LA is greater than from lottery LB. That is:
LA ≻LB ⇔U (LA) > U (LB) .
where the symbol ≻indicates preference. The von Neumann-Morgenstern utility function models
the decision making of an agent considering two lotteries as just calculating the expected utilities
and choosing the larger resulting one.
A Note on Logarithms
Logarithmic functions are commonly used as utility functions.
A logarithm is a
mathematical function that expresses the power to which a given number (referred to as the
base) must be raised in order to produce a value. The logarithm of a number x with respect
to base b is denoted as logb x, and is the exponent to which b must be raised to produce the
value x. For example, log2 8 = 3, because 23 = 8.
One special case of the logarithmic function, the natural logarithm, has a base of e (which is
Euler’s constant, roughly 2.718); in this chapter, it is referred to simply as log. Logarithms
have the following properties, independent of base: log 0 →−∞, log 1 = 0, logb b = 1, and
logb ba = a.
Logarithms have a downward, concave shape, meaning the output increases slower than the
input. This shape resembles how humans value resources: we generally value a good less if
we already have more of it. Logarithmic functions value goods in inverse proportion to how
much of the resource we already have.

B.2. PROPERTIES OF UTILITY FUNCTIONS
535
1
2
e
10
−1
0
1
2
3
x
U
U = log2 x
U = loge x
U = log10 x
Figure B.1: Logarithmic functions share several properties, such as being concave and crossing
the y-axis at one.
B.2.3
St. Petersburg Paradox
An old man on the streets of St. Petersburg offers gamblers the following game: he will flip a fair
coin repeatedly until it lands on tails. If the first flip lands tails, the game ends and the pension fund
gets $2. If the coin first lands on heads and then lands on tails, the game ends and the gambler gets
$4. The amount of money (the “return”) will double for each consecutive flip landing heads before
the coin ultimately lands tails. The game concludes when the coin first lands tails, and the gambler
receives the appropriate returns. Now, the question is, how much should a gambler be willing to pay
from the pension fund to play this game [20]?
With probability 1
2, the first toss will land on tails, in which case the gambler wins two dollars. With
probability 1
4, the first toss lands heads and the second lands tails, and the gambler wins four dollars.
Extrapolating, this game offers a maximum possible payout of:
$2n = $
n times
z
}|
{
2 · 2 · 2 · · · 2 · 2 · 2,
where n is the number of flips until and including when the coin lands on tails. As offered, though,
there is no limit to the size of n, since the company promises to keep flipping the coin until it lands
on tails. The expected payout of this game is therefore:
E [L] = 1
2 · $2 + 1
4 · $4 + 1
8 · $8 + · · · = $1 + $1 + $1 + · · · = $∞.
Bernoulli described this situation as a paradox because he believed that, despite it having infinite
expected value, anyone would take a large but finite amount of money over the chance to play the
game. While paying $10,000,000 to play this game would not be inconsistent with its expected value,
we would think it highly irresponsible! The paradox reveals a disparity between expected value

536
APPENDIX B. UTILITY FUNCTIONS
calculations and reasonable human behavior.
Flip 1
Flip 2
Flip 3
Flip 4
p = 1
2
p = 1
4
p = 1
8
p =
1
16
p =
1
16
w = 2
w = 4
w = 8
w = 16
w = 32+
Figure B.2: Winnings from the St. Petersburg Paradox double after each coin toss, offering small
likelihoods of big prizes.
Logarithmic utility functions can represent decreasing marginal utility.
A number of
ways have been proposed to resolve the St. Petersburg paradox. We will focus on the most popular:
representing the player with a utility function instead of merely calculating expected value. As we
discussed in the previous section, a logarithmic utility function seems to resemble how humans think
about wealth. As a person becomes richer, each additional dollar gives them less satisfaction than
before. This concept, called decreasing marginal utility, makes sense intuitively: a billionaire would
not be as satisfied winning $1000 as someone with significantly less money. Wealth, and many other
resources like food, have such diminishing returns. While a first slice of pizza is incredibly satisfying,
a second one is slightly less so, and few people would continue eating to enjoy a tenth slice of pizza.
Assuming an agent with a utility function u($x) = log2 (x) over x dollars, we can calculate the
expected utility of playing the St. Petersburg game as:
E [U (L)] = 1
2 · log2(2) + 1
4 · log2(4) + 1
8 · log2(8) + · · · = 2.
That is, the expected utility of the game is 2. From the logarithmic utility function over wealth, we
know that:
2 = log2 x ⇒x = 4,
which implies that the player is indifferent between playing this game and having $4: the level of
wealth that gives them the same utility as what they expect playing the lottery.
Expected utility is more reasonable than expected value.
The previous calculation explains
why an agent with u ($x) = log2 x should not pay large amounts of money to play the St. Petersburg

B.3. TAIL RISK: CORRIGIBILITY
537
game. The log utility function implies that the player receives diminishing returns to wealth, and
cares less about situations with small chances of winning huge sums of money. Figure 5 shows how
the large payoffs with small probability, despite having the same expected value, contribute little to
expected utility. This feature captures the human tendency towards risk aversion, explored in the
next section. Note that while logarithmic utility functions are a useful model (especially in resolving
such paradoxes), they do not perfectly describe human behavior across choices, such as the tendency
to buy lottery tickets, which we will explore in the next chapter.
0
5
10
15
20
0
1
2
3
4
5
Flip number
Expected Value/Utility
Expected Utility
Expected Value
Figure B.3: In the St. Petersburg Paradox, each subsequent flip has the same expected value but
expected utility falls sharply.
Summary.
In this section, we examined the properties of Bernoulli utility functions, which allow
us to compare an agent’s preferences across different outcomes. We then introduced von Neumann-
Morgenstern utility functions, which calculate the average, or expected, utility over different possible
outcomes. From there, we derived the idea that rational agents are able to make decisions that
maximize expected utility. Through the St. Petersburg Paradox, we showed that taking the expected
utility of a logarithmic function leads to more reasonable behavior. Having understood some properties
of utility functions, we can now examine the problem of incorrigibility, where AI systems do not
accept corrective interventions because of rigid preferences.
B.3
Tail Risk: Corrigibility
Overview.
In this section, we will explore how utility functions provide insight into whether an
AI system is open to corrective interventions and discuss related implications for AI risks. The von
Neumann-Morgenstern (vNM) axioms of completeness and transitivity can lead to strict preferences
over shutting down or being shut down, which affects how easily an agent can be corrected. We will
emphasize the importance of developing corrigible AI systems that are responsive to human feedback
and that can be safely controlled to prevent unwanted AI behavior.

538
APPENDIX B. UTILITY FUNCTIONS
Corrigibility measures our ability to correct an AI if and when things go wrong.
An AI
system is corrigible if it accepts and cooperates with corrective interventions like being shut down
or having its utility function changed [21]. Without many assumptions, we can argue that typical
rational agents will resist corrective measures: changing an agent’s utility function necessarily means
that the agent will pursue goals that result in less utility relative to their current preferences.
Suppose we own an AI that fetches coffee for us every morning.
Its utility function assigns
“10 utils” to getting us coffee quickly, “5 utils” to getting us coffee slowly, and “0 utils” to not getting
us coffee at all. Now, let’s say we want to change the AI’s objective to instead make us breakfast.
A regular agent would resist this change, reasoning that making breakfast would mean it is less
able to efficiently make coffee, resulting in lower utility. However, a corrigible AI would recognize
that making breakfast could be just as valuable to humans as fetching coffee and would be open to
the change in objective. The AI would move on to maximizing its new utility function. In general,
corrigible AIs are more amenable to feedback and corrections, rather than stubbornly adhering to
their initial goals or directives. When AIs are corrigible, humans can more easily correct rogue
actions and prevent any harmful or unwanted behavior.
Completeness and transitivity imply that an AI has strict preferences over shutting
down.
Assume that an agent’s preferences satisfy the vNM axioms of completeness, such that it
can rank all options, as well as transitivity, such that its preferences are consistent. For instance, the
AI can see that preferring an apple to a banana and a banana to a cherry implies that we prefer an
apple to a cherry. Then, we know that the agent’s utility function ranks every option.
Consider again the coffee-fetching AI. Suppose that in addition to getting us coffee quickly (10 utils),
getting us coffee slowly (5 utils), and not getting us coffee (0 utils), there is a fourth option, where
the agent gets shut down immediately. The AI expected that immediate shutdown will result in its
owner getting coffee slowly without AI assistance, which appears to be valued at 5 units of utility
(the same as it getting us coffee slowly). The agent thus strictly prefers getting us coffee quickly to
shutting down, and strictly prefers shutting down to us not having coffee at all.
Generally, unless indifferent between everything, completeness and transitivity imply that the AI has
unspecified preferences about potentially shutting down [22]. Without completeness, the agent could
have no preference between shutting down immediately and all other actions. Without transitivity,
the agent could be indifferent between shutting down immediately and all other possible actions
without that implying that the agent is indifferent between all possible actions.
It is bad if an AI either increases or reduces the probability of immediate shutdown.
Suppose that in trying to get us coffee quickly, the AI drives at unsafe speeds. We’d like to shut
down the AI until we can reprogram it safely. A corrigible AI would recognize our intention to shut
down as a signal that it is misaligned. However, an incorrigible AI would instead stay the course with
what it wanted to do initially—get us coffees—since that results in the most utility. If possible, the
AI would decrease the probability of immediate shutdown, say by disabling its off-switch or locking
the entrance to its server rooms. Clearly, this would be bad.
Consider a different situation where the AI realizes that making coffee is actually quite difficult and
that we would make coffee faster manually, but fails to realize that we don’t want to exert the effort
to do so. The AI may then try to shut down, so that we’d have to make the coffee ourselves. Suppose
we tell the AI to continue making coffee at its slow pace, rather than shut down. A corrigible AI
would recognize our instruction as a signal that it is misaligned and would continue to make coffee.
However, an incorrigible AI would instead stick with its decision to shut down without our permission,

B.4. ATTITUDES TO RISK
539
since shutting down provides it more utility. Clearly, this is also bad. We’d like to be able to alter
AIs without facing resistance.
Summary.
In this section, we introduced the concept of corrigibility in AI systems. We discussed
the relevance of utility functions in determining corrigibility, particularly challenges that arise if
an AI’s preferences are complete and transitive, which can lead to strict preferences over shutting
down. We explored the potential problems of an AI system reducing or increasing the probability
of immediate shutdown. The takeaway is that developing corrigible AI systems—systems that
are responsive and adaptable to human feedback and changes—is essential in ensuring safe and
effective control over AIs’ behavior. Examining the properties of utility functions illuminates potential
problems in implementing corrigibility.
A Note on Utility Functions vs. Reward Functions
Utility functions and reward functions are two interrelated yet distinct concepts in
understanding agent behavior.
Utility functions represent an agent’s preferences about
states or the choice-worthiness of a state, while rewards functions represent externally imposed
reinforcement. The fact that an outcome is rewarded externally does not guarantee that it
will become part of an agent’s internal utility function.
An example where utility and reinforcement comes apart can be seen with Galileo Galilei.
Despite the safety and societal acceptance he could gain by conforming to the widely accepted
geocentric model, Galileo maintained his heliocentric view. His environment provided ample
reinforcement to conform, yet he deemed the pursuit of scientific truth more choiceworthy,
highlighting a clear difference between environmental reinforcement and the concepts of
choice-worthiness or utility.
As another example, think of evolutionary processes as selecting or reinforcing some traits
over others. If we considered taste buds as components that help maximize fitness, we would
expect more people to want the taste of salads over cheeseburgers. However, it is more
accurate to view taste buds as “adaptation executors” rather than “fitness maximizers,” as
taste buds evolved in our ancestral environment where calories were scarce. This illustrates
the concept that agents act on adaptations without necessarily adopting behavior that reliably
helps maximize reward.
The same could be true for reinforcement learning agents. RL agents might execute learned
behaviors without necessarily maximizing reward; they may form decision procedures that are
not fully aligned with its reinforcement. The fact that what is rewarded is not necessarily what
an agent thinks is choiceworthy could lead to AIs that are not fully aligned with externally
designed rewards. The AI might not inherently consider reinforced behaviors as choiceworthy
or of high utility, so its utility function may differ from the one we want it to have.
B.4
Attitudes to Risk
Overview. The concept of risk is central to the discussion of utility functions. Knowing an agent’s
attitude towards risk—whether they like, dislike, or are indifferent to risk—gives us a good idea of
what their utility function looks like. Conversely, if we know an agent’s utility function, we can also
understand their attitude towards risk. We will first outline the three attitudes towards risk: risk

540
APPENDIX B. UTILITY FUNCTIONS
aversion, risk neutrality, and risk seeking. Then, we will consider some arguments for why we might
adopt each attitude, and provide examples of situations where each attitude may be suitable to favor.
It is crucial to understand what risk attitudes are appropriate in which contexts. To make AIs safe,
we will need to give them safe risk attitudes, such as by favoring risk-aversion over risk-neutrality.
Risk attitudes will help explain how people do and should act in different situations. National
governments, for example, will differ in risk outlook from rogue states, and big tech companies will
differ from startups. Moreover, we should know how risk averse we should be with AI development,
as it has both large upsides and downsides.
B.4.1
What Are the Different Attitudes to Risk?
There are three broad types of risk preferences.
Agents can be risk averse, risk neutral, or
risk seeking. In this section, we first explore what these terms mean. We consider a few equivalent
definitions by examining different concepts associated with risk [23]. Then, we analyze what the
advantages to adopting each certain attitude toward risk might be.
Let’s consider these in the context of a bet on a coin toss.
Suppose agents are given the
opportunity to bet $1000 on a fair coin toss—upon guessing correctly, they would receive $2000 for
a net gain of $1000. However, if they guess incorrectly, they would receive nothing and lose their
initial bet of $1000. The expected value of this bet is $0, irrespective of who is playing: the player
gains or loses $1000 with equal probabilities. However, a particular player’s willingness to take this
bet, reflecting their risk attitude, depends on how they calculate expected utility.
a. Risk aversion is the tendency to prefer a certain outcome over a risky option with the same
expected value. A risk-averse agent would not want to participate in the coin toss. The
individual is unwilling to take the risk of a potential loss in order to potentially earn a higher
reward. Most humans are instinctively risk averse. A common example of a risk-averse utility
function is u (x) = log x (red line in Figure B.4).
b. Risk neutrality is the tendency to be indifferent between a certain outcome and a risky option
with the same expected value. For such players, expected utility is proportional to expected
value. A risk-neutral agent would not care whether they were offered this coin toss, as its
expected value is zero. If the expected value was negative, they would prefer not to participate
in the lottery, since the lottery has negative expected value. Conversely, if the expected value
was positive, they would prefer to participate, since it would then have positive expected value.
The simplest risk-neutral utility function is u(x) = x (blue line in Figure B.4).
c. Risk seeking is the tendency to prefer a risky option over a sure thing with the same expected
value. A risk-seeking agent would be happy to participate in this lottery. The individual is
willing to risk a negative expected value to potentially earn a higher reward. We tend to
associate risk seeking with irrationality, as it leads to lower wealth through repeated choices
made over time. However, this is not necessarily the case. An example of a risk-seeking utility
function is u(x) = x2 (green line in Figure B.4).
We can define each risk attitude in three equivalent ways. Each draws on a different aspect of how
we represent an agent’s preferences.

B.4. ATTITUDES TO RISK
541
0
1
2
3
4
5
0
1
2
3
4
5
x
U
U(x) = log(x)
U(x) = x
U(x) = x2
Figure B.4: Concave, linear, and convex utility functions model risk averse, risk neutral, and risk
seeking agents’ preferences.
Risk attitudes are fully explained by how an agent values uncertain outcomes.
According
to expected utility theory, an agent’s risk preferences can be understood from the shape of their
utility function, and vice-versa. We will illustrate this point by showing that concave utility functions
necessarily imply risk aversion. An agent with a concave utility function faces decreasing marginal
utility. That is, the jump from $1000 to $2000 is less satisfying than the jump from wealth $0 to
wealth $1000. Conversely, the agent dislikes dropping from wealth $1000 to wealth $0 more than they
like jumping from wealth $1000 to wealth $2000. Thus, the agent will not enter the aforementioned
double-or-nothing coin toss, displaying risk aversion.
Preferences over outcomes may not fully explain risk attitudes.
It may seem unintuitive
that risk attitudes are entirely explained by how humans calculate utility of outcomes. As we just
saw, in expected utility theory, it is assumed that agents are risk averse only because they have
diminishing returns to larger outcomes. Many economists and philosophers have countered that
people also have an inherent aversion to risk that is separate from preferences over outcomes. At the
end of this chapter, we will explore how non-expected utility theories have attempted to more closely
capture human behavior in risky situations.
B.4.2
Risk and Decision Making
Overview.
Having defined risk attitudes, we will now consider situations where it is appropriate
to act in a risk-averse, risk-neutral, or risk-seeking manner. Often, our risk approach in a situation
aligns with our overall risk preference—if we are risk averse in day-to-day life, then we will also likely
be risk averse when investing our money. However, sometimes we might want to make decisions as if
we have a different attitude towards risk than we truly do.
Criterion of rightness vs. decision procedure.
Philosophers distinguish between a criterion
of rightness, the way of judging whether an outcome is good, and a decision procedure, the method of

542
APPENDIX B. UTILITY FUNCTIONS
making decisions that lead to the good outcomes. A good criterion of rightness may not be a good
decision procedure. This is related to the gap between theory and practice, as explicitly pursuing
an ideal outcome may not be the best way to achieve it. For example, a criterion of rightness for
meditation might be to have a mind clear of thoughts. However, as a decision procedure, thinking
about not having thoughts may not help the meditator achieve a clear mind—a better decision
procedure would be to focus on the breath.
As another example, the hedonistic paradox reminds us that people who directly aim at pleasure rarely
secure it [24]. While a person’s pleasure level could be a criterion of rightness, it is not necessarily
a good guide to increasing pleasure—that is, not necessarily a good decision procedure. Whatever
one’s vision of pleasure looks like—lying on a beach, buying a boat, consuming drugs—people who
directly aim at pleasure often find these things are not as pleasing as hoped. People who aim at
meaningful experiences, helping others and engaging in activities that are intrinsically worthwhile,
are more likely to be happy. People tend to get more happiness out of life when not aiming explicitly
for happiness but for some other goal. Using the criterion of rightness of happiness as a decision
procedure can predictably lead to unhappiness.
Maximizing expected value can be a criterion of rightness, but it is not always a good decision
procedure. In the context of utility, we observe a similar discrepancy where explicitly pursuing the
criterion of rightness (maximizing the utility function) may not lead to the best outcome. Suppose
an agent is risk neutral, such that their criterion of rightness is maximizing a linear utility function.
In the first subsection, we will explore how they might be best served by making decisions as if they
are risk averse, such that their decision procedure is maximizing a concave utility function.
Why Be Risk Averse?
Risk-averse behavior is ubiquitous.
In this section, we will explore the advantages of risk
aversion and how it can be a good way to advance goals across different domains, from evolutionary
fitness to wealth accumulation. It might seem that by behaving in a risk-averse way, thereby refusing
to participate in some positive expected value situations, agents leave a lot of value on the table.
Indeed, extreme risk aversion may be counterproductive—people who keep all their money as cash
under their bed will lose value to inflation over time. However, as we will see, there is a sweet spot
that balances the safety of certainty and value maximization: risk-averse agents with logarithmic
utility almost surely outperform other agents over time, under certain assumptions.
Response to computational limits.
In complex situations, decision makers may not have the
time or resources to thoroughly analyze all options to determine the one with the highest expected
value. This problem is further complicated when the outcomes of some risks we take have effects on
other decisions down the line, like how risky investments may affect retirement plans. To minimize
these complexities, it may be rational to be risk averse. This helps us avoid the worst effects of our
incomplete estimates when our uncertain calculations are seriously wrong.
Suppose Martin is deciding between purchasing a direct flight or two connecting flights with a tight
layover. The direct flight is more expensive, but Martin is having trouble estimating the likelihood
and consequences of missing his connecting flight. He may prefer to play the situation safe and pay
for the more expensive direct flight, even though the true value-for-money of the connected route
may have been higher. Now Martin can confidently make future decisions like booking a bus from
the airport to his hotel. Risk-averse decision-making not only reduces computational burden but can
also increase decision-making speed. Instead of constantly making difficult calculations, an agent
may prefer to have a bias against risk.

B.4. ATTITUDES TO RISK
543
Behavioral advantage.
Risk aversion is not only a choice but a fundamental psychological
phenomenon, and is influenced by factors such as past experiences, emotions, and cognitive biases.
Since taking risks could lead to serious injury or death, agents undergoing natural selection usually
develop strategies to avoid such risks whenever possible. Humans often shy away from risk, prioritizing
safety and security over more risky ventures, even if the potential rewards are higher.
Studies have shown that animals across diverse species exhibit risk-averse behaviors. In a study
conducted on bananaquits, a nectar-drinking bird, researchers presented the birds with a garden
containing two types of flowers: one with consistent amounts of nectar and one with variable amounts.
They found that the birds never preferred the latter, and that their preference for the consistent
variety was intensified when the birds were provided fewer resources in total [25]. This risk aversion
helps the birds survive and procreate, as risk-neutral or risk-seeking species are more likely to die
out over time: it is much worse to have no nectar than it is better to have double the nectar. Risk
aversion is often seen as a survival mechanism.
Natural selection favors risk aversion.
Just as individual organisms demonstrate risk aversion,
entire populations are pushed by natural selection to act risk averse in a manner that maximizes the
expected logarithm of their growth rather than the expected value. Consider the following highly
simplified example. Suppose there are three types of animals—antelope, bear, crocodile—in an area
where each year is either scorching or freezing with probability 0.5. Every year, the populations grow
or shrink depending on the weather—some animals are better suited to the hot weather, and some to
the cold. The populations’ per-capita offspring, or equivalently the populations’ growth multipliers,
are shown in the table below.
Growth when Scorching
Growth when Freezing
(p = 1/2)
(p = 1/2)
Antelope
1.1
1.1
Bear
1.6
0.6
Crocodile
2.2
0.0
Table B.1: Different animals have different seasonal growth rates.
Antelope have the same growth in each state, bears grow faster in the warmth but slower in the cold
when they hibernate, and crocodiles grow rapidly when it is scorching and animals gather near water
sources but die out when their habitats freeze over. However, notice that the three populations have
the same average growth ratio of 1.1.
However, “average growth” is misleading. Suppose we observe this population over two periods, one
hot followed by one cold. The average growth multiplier over these two periods would be 1.1 for
every animal. However, this does not mean that they all grow the same amount. In the table below,
we can see the animals’ growth over time.
Initial
Hot Period
Hot Period
Cold Period
Cold Period
Overall
Population
Growth
Population
Growth
Population
Log Growth
Antelope
1000
1.1x
1100
1.1x
1210
0.19
Bear
1000
1.6x
1600
0.6x
960
-0.04
Crocodile
1000
2.2x
2200
0x
0
-∞
Table B.2: All else equal, the species with a more stable growth rate wins out over time.

544
APPENDIX B. UTILITY FUNCTIONS
Adding the logarithm of each species’ hot and cold growth rates indicates its long term growth
trajectory. The antelope population will continue growing no matter what, compounding over time.
However, the crocodile population will not—as soon as it enters a cold year, the crocodiles will
become permanently extinct. The bear population is not exposed to immediate extinction risk, but
over time it will likely shrink towards extinction. Notice that maximizing long-run growth in this case
is equivalent to maximizing the sum of the logarithm of the growth rates—this is risk aversion. The
stable growth population, or equivalently the risk-averse population, is favored by natural selection
[26].
Avoid risk of ruin.
Risk aversion’s key benefit is that it avoids risk of ruin. Consider a repeated
game of equal probability “triple-or-nothing” bets. That is, players are offered a 1
2 probability of
tripling their initial wealth w, and a 1
2 probability of losing it all. A risk-neutral player can calculate
the expected value of a single round as:
1
2 · 0 + 1
2 · 3w = 1.5w.
Since the expected value is greater than the player’s initial wealth, a risk-neutral player would bet
their entire wealth on the game. Additionally, if offered this bet repeatedly, they would reinvest
everything they had in it each time. The expected value of taking this bet n times in a row, reinvesting
all winnings, would be:
1
2 · 0 + 1
4 · 0 + · · · + 1
2n · 0 + 1
2n · 3n · w = (1.5)nw.
If the agent was genuinely offered this bet as many times as they wanted, then they would continue
to invest everything infinitely many times, which gives them expected value of:
lim
n→∞1.5nw = ∞.
This is another infinite expected value game—just like in the St. Petersburg Paradox! However,
notice that this calculation is again heavily skewed by a single, low-probability branch in which an
extremely lucky individual continues to win, exponentially increasing their wealth. In the figure
below, we show the first four bets in this strategy with a starting wealth of 16. Only along the
cyan branch does the player win any money, and this branch increasingly becomes astronomically
improbable. We would rarely choose to repeatedly play triple-or-nothing games with everything we
owned in real life. We are risk averse when dealing with high probabilities of losing all our money.
Acting risk neutral and relying on expected value would be a poor decision-making strategy.

B.4. ATTITUDES TO RISK
545
Bet 1
Bet 2
Bet 3
Bet 4
p = 1
2
p = 1
4
p = 1
8
p =
1
16
w = 0
w = 0
w = 0
w = 0
w = 81
Figure B.5: Risk-neutral betting can lead to ruin.
Maximizing logarithmic utility is a better decision procedure.
Agents might want to act as
if maximizing the logarithm of their wealth instead of maximizing the expected value. A logarithmic
function avoids risk of ruin because it assigns a utility value of negative infinity to the outcome of
zero wealth, since log 0 →−∞. Therefore an agent with a logarithmic utility function in wealth will
never participate in a lottery that could, however unlikely the case, land them at zero wealth. The
logarithmic function also grows slowly, placing less weight on very unlikely, high-payout branches, a
property that we used to resolve the St. Petersburg Paradox. While we might have preferences that
are linear over wealth (which is our criterion of rightness) we might be better served by a different
decision procedure: maximizing the logarithm of wealth rather than maximizing wealth directly.
Maximizing the logarithm of wealth maximizes every percentile of wealth.
Maximizing
the logarithmic utility valuation avoids risk of ruin since investors never bet their entire wealth on
one opportunity, much like how investors seek to avoid over-investing in one asset by diversifying
investments over multiple assets. Instead of maximizing average wealth (as expected value does),
maximizing the logarithmic utility of wealth maximizes other measures associated with the distribution
of wealth. In fact, doing so maximizes the median, which is the 50th percentile of wealth, and it also
delivers the highest value at any arbitrary percentile of wealth. It even maximizes the mode—the
most likely outcome. Mathematically, maximizing a logarithmic utility function in wealth outperforms
any other investment strategy in the long run, with probability one (certainty) [27]. Thus, variations
on maximizing the logarithm of wealth are widely used in the financial sector.

546
APPENDIX B. UTILITY FUNCTIONS
Why Be Risk Neutral?
Risk neutrality is equivalent to acting on the expected value.
Since averages are
straightforward and widely taught, expected value is the mostly widely known explicit decision-
making procedure. However, despite expected value calculations being a common concept in popular
discourse, situations where agents do and should act risk neutral are limited. In this section, we
will first look at the conditions under which risk neutrality might be a good decision procedure—in
such cases, maximizing expected value can be a significant improvement over being too cautious.
However, being mistaken about whether the conditions hold is entirely possible. We will examine
two scenarios: one when these conditions hold, and one situation in which incorrectly assuming that
they held led to ruin.
Risk neutrality is undermined by the possibility of ruin.
In the previous section, we
examined the triple-or-nothing game, where a risk-neutral approach can lead to zero wealth in the
long term. The risk of ruin, or the loss of everything, is a major concern when acting risk neutral. In
order for a situation to be free of risk of ruin, several conditions must be met. First, risks must be
local, meaning they affect only a part of a system, unlike global risks, which affect an entire system.
Second, risks must be uncorrelated, which means that the outcomes do not increase or decrease
together, so that local risks do not combine to cause a global risk. Third, risks must be tractable,
which means the consequences and probabilities can be estimated reasonably easily. Finally, there
should be no black swans, unlikely and unforeseen events that have a significant impact. As we
will see, all of these conditions are rarely met in a high-stakes environment, and there can be dire
consequences to underestimating the severity of risks.
Risk neutrality is useful when the downside is small.
It can be appropriate to act in a
risk-neutral manner with regards to relatively inconsequential decisions. Suppose we’re considering
buying tickets to a movie that might not be any good. The upside is an enjoyable viewing experience,
and the downsides are all local: $20 and a few wasted hours. Since the stakes of this decision are
minimal, it is reasonable not to overthink our risk attitude and just attend the movie if we think that,
on average, we won’t regret this decision. However, if the decision at hand were that of purchasing a
car on credit, we likely would not act hastily. The risk might not be localized but instead affect one’s
entire life; if we can’t afford to make payments, we could go bankrupt. However, when potential
losses are small, extreme risk aversion may be too safe a strategy. We would prefer not to leave
expected value on the table.
Dangers of risk neutrality.
Often, agents incorrectly assume that there is no risk of ruin. The
failure of financial institutions during the 2008 financial crisis, which sparked the Great Recession,
is a famous example of poor risk assessment. Take the American International Group (AIG), a
multinational insurance company worth hundreds of billions of dollars [28]. By 2008, they had
accumulated billions of dollars worth of financial products related to the real estate sector. AIG
believed that their investments were sufficiently uncorrelated, and therefore ruled out risk of ruin.
However, AIG had not considered a black swan: in 2008, many financial products related to the
housing market crashed. AIG’s investments were highly correlated with the housing market, and the
firm needed to be bailed out by the Federal Reserve for $180 billion dollars. Even institutions with
sophisticated mathematical analysis fail to identify risk of ruin—playing it safe might, unsurprisingly,
be safer. Artificial agents may operate in environments where risk of ruin is a real and not a
far-fetched possibility. We would not want a risk-neutral artificial agent endangering human lives
because of a naive expected value calculation.

B.4. ATTITUDES TO RISK
547
Why Be Risk Seeking?
Risk-seeking behavior is not always unreasonable.
As we previously defined, risk-seeking
agents prefer to gamble for the chance of a larger outcome rather than settle for the certainty of a
smaller one. In some cases, a risk-seeking agent’s behavior may be regarded as unreasonable. For
example, gambling addicts take frequent risks that lower their utility and wellbeing in the long run.
On the other hand, many individuals and organizations may be motivated to seek risks for a number
of strategic reasons, which is the focus of this section. We will consider four situations as examples
where agents might want to be risk seeking.
In games with many players and few winners, risk-seeking behavior can be justified.
Consider a multi-player game where a thousand participants compete for a single grand prize, which
is given to the player who accumulates the most points. An average player expects to only win
1
1000
thof the time. Even skilled players would reason that due to random chance, they are unlikely
to be the winner. Therefore, participants may seek risks, in order to increase the variance of their
point totals, while sacrificing the mean, so that they either end up with loads of points (thereby
winning with higher probability) or no points at all (which is no worse for them than having some
points). In the real world, selling products in a highly competitive marketplace is an analogous
situation, where vendors may take risks to attract customers. If a hundred firms are selling effectively
identical products, they might consider unusual or provocative forms of advertising. Bold marketing
strategies can attract some customers but potentially alienate others. However, the vendor may feel
that without taking this chance to stand out, they likely will not do enough business to turn a profit.
Such agents would accept a negative expected value strategy.
Daring to rise.
Agents in bad and deteriorating situations with a low chance of escaping can
pursue risk-seeking strategies to great effect. Someone with a serious terminal illness may consider
experimental treatments with uncertain outcomes. They may take the treatment even if told that
it is most likely ineffective and might have serious side effects. Sports teams on the verge of losing
often attempt risky strategies such as the “Hail Mary” in American football (long, high passes that
are difficult to catch) or sending a goalkeeper forward in soccer towards the end of the game. Such
strategies are likely to backfire and leave them in an even worse position overall—and therefore
have a negative expected value, where value is the number of goals—but might also create the only
possibility of winning.
Harnessing stressors through risk exposure.
Instead of collapsing (or merely enduring) when
unlikely, bad events occur, an antifragile system is one in which the risk taker actually benefits,
becoming stronger and more resilient to future challenges [29]. Antifragility is therefore the property
of being able to benefit from risk. Systems, institutions, or individuals that exhibit antifragility may
thus seek risk exposure. The human body is antifragile in many contexts, including its response
to pathogens.
Illness is usually temporarily uncomfortable, but carries a small risk of greater
complications. In combating a pathogen, the immune system not only responds to the active threat,
but prepares itself to combat future illnesses quickly and effectively. The immune system becomes
stronger through illness. We encourage children to step out of the house instead of solely living in
sterilized environments. To a reasonable extent, we help them to face and deal with risks so that
they become stronger.
Startups aiming to capture significant upside potential.
When losing a small amount has a
negligible or tolerable effect on an agent’s wellbeing, the agent may be willing to risk this small loss
for a low-probability outcome of very large gain, even if the probability of success is sufficiently small
to make this negative expected monetary value. This line of thinking is exemplified by early stage
startups, which we now describe.

548
APPENDIX B. UTILITY FUNCTIONS
The Lifecycle of a Company
A company’s appetite for risk over time is captured by a sigmoid (s-shaped) curve,
which is initially convex and later concave.
Knowing that an agent has a concave utility
function if and only if they are risk averse, and a convex utility function if and only if they are risk
seeking, we understand that an agent with a sigmoid utility function is initially risk seeking, and later
becomes risk averse. This model may give us an idea of how AI developers will behave, depending on
the scale of the organization and the maturity of the technology.
Infancy
Expansion
Maturity
Revenue
Utility
Figure B.6: Start-ups often take big risks to try and get explosive growth, modeled by a convex
utility function. As a company matures, it gets more risk averse, prioritizing survival over extreme
growth.
A startup is by nature a risk-seeking venture.
An entrepreneur has typically sacrificed a
stable income, savings, and much of their leisure time in order to pursue a business. The new
business, starting with little traction and few customers, has little to lose and much to gain. Given
their sacrifices and the startup’s position, the entrepreneur is willing to fail repeatedly and return to
baseline, prioritizing chances at rapid growth. By this logic, we may expect AI startups to prioritize
chances of success over safety and avoiding reputational damage. However, since such companies
operate on a smaller scale, risks are localized, meaning societal concerns are reduced. This is the
convex part of the curve.
When a startup begins to gain traction, it grows rapidly, gaining customers and revenue.
Gradually, more stakeholders like employees and investors begin to rely on the company, and its
focus begins to shift toward preserving its proven success and preventing future losses, rather than
risking a return to baseline to pursue more growth. During this transition, the curve begins to shift
from convex to concave.

B.5. BEYOND EXPECTED UTILITY THEORIES
549
In the concave stage of a company, its growth tapers off.
Eventually, the company nears
the limits of its market and transformative resources, and is unable to risk its bottom line for further
growth opportunities. Mature companies are risk averse, since employees, shareholders, and customers
depend on their stability. They have much to lose and little to gain. A mature company’s consolation
is that it may develop a project portfolio, with riskier projects sprinkled in amongst its core business
operations that are not typically exposed to risk. Compared to startups, big tech companies may
thus take a more meticulous approach to ensuring the safety of AI products prior to release, though
this is not always the case.
A company’s lifecycle demonstrates that an agent does not have only one unchanging
risk attitude.
An agent’s approach to risk is affected by their situation and decision context.
Indeed, describing a person as risk averse or risk seeking will always be an oversimplification. As
we will explore in the final section, people have dynamic risk attitudes that are influenced by many
factors including biases, context, initial wealth, and more.
Summary.
In this section, we defined the three attitudes towards risk—risk aversion, risk neutrality,
and risk seeking—and examined their properties and shapes. There are reasons to favor each attitude,
depending on the agent’s circumstances. We saw that risk aversion in the form of maximizing a
logarithmic utility function outperforms all other investment strategies in the long run, that risk
neutrality may be favorable when there is no risk of ruin, and that risk-seeking behavior is useful
when we have little to lose. Humans, and the organizations we form, adopt different risk attitudes in
different situations.
When designing AIs, developers may have significant ability to define and influence the utility function.
As demonstrated in the corrigibility problem, issues in the utility function may be later difficult to
rectify. Thus, we must carefully consider what risk attitudes a particular utility function embodies,
and how that risk attitude would play out in different contexts.In particular, designing AIs to be
risk-averse may help avoid many of the pitfalls of risk-neutrality that we discussed.
We are interested in describing a more accurate model of human decision making. In the next section,
we will analyze some reasons why expected utility theory fails in this regard, and how we might
improve our model. We will see that humans are systematically irrational and can be influenced by
the context, and even the wording, in which choices are presented. These models are helpful for
better understanding people, like those leading AI development or countries or those using AIs, who
will behave in irrational ways.
B.5
Beyond Expected Utility Theories
Overview.
von Neumann-Morgenstern utility functions are supposed to capture how an agent
chooses among different options, but they do not always explain how humans actually behave. In
reality, humans are not ideally rational agents. We’d like to model how people, like those leading
the development and regulation of AI, will behave, and they will often behave in ways that perfect
expected utility maximizers would not. If AIs learn from and interact with humans, they too might
exhibit some aspects of human irrationality. Therefore, we like to model human behavior more
accurately.
Economists have proposed alternative theories, such as Daniel Kahneman and Amos Tversky’s
Prospect theory, to explain why humans deviate from rationality, as defined under the von Neumann-
Morgenstern axioms. In this section, we examine some major ways that humans break rationality, as
defined under the von Neumann-Morgenstern model, and how non-expected utility theories help us

550
APPENDIX B. UTILITY FUNCTIONS
better understand human choices. Having a stronger model of human behavior will ultimately help
us design AIs that behave in ways more aligned with humans.
B.5.1
Humans and Rationality
Humans are not ideally rational agents.
As we discussed before, rational agents have a
thorough understanding of their own preferences, have complete and stable preferences, and are able
to make which decisions will help them maximally satisfy these preferences. Human decision-making
deviates from ideal rationality. For example, we prioritize fairness, may have incommensurable values,
and value the desires of others even when these mean we must compromise self-interest. Human
preferences are also unstable, susceptible to persuasion, and can change over the course of our lives
or in light of new information.
Humans often satisfice rather than maximize.
According to the theory of bounded rationality,
humans often make choices that result in outcomes that are “good enough” rather than the most ideal.
Human rationality is limited by cognitive abilities, time, and available information, meaning we must
frequently make decisions without considering all possible scenarios and outcomes. Take an everyday
example: suppose a group of friends is deciding what restaurant to dine at. They will likely choose
the first satisfactory option they come across, rather than methodically consider all possible places in
the city. Thus, humans are said to satisfice—choose the first option that is satisfactory—rather than
exhaustively maximize.
AIs that are not ideally rational can have varying safety.
It is plausibly safer for AI
systems to be satisficers, since maximizers may behave in undesirable ways while trying to relentlessly
maximize some metric [30]. As discussed in Proxies, maximizers may optimize proxies in ways that
differ from the idealized result. However, when AI agents are trained on human data and trained
to interact with humans, they may pick up some of our biases and thereby not be ideally rational.
AIs with many irrational behaviors could be harder to predict and therefore be harder to control.
We will now proceed to formalize our understanding of why expected utility theory fails to capture
human behavior.
B.5.2
Evidence Against Expected Utility Theory
Overview.
Humans violate the von Neumann-Morgenstern axioms in many different ways. We
consider three examples in this section: the Allais Paradox, fairness, and the problem of framing,
in which humans make inconsistent choices over lotteries that agents following von Neumann-
Morgenstern would be indifferent between.
These problems show violations of von Neumann-
Morgenstern rationality: specifically, the independence and decomposability axioms.
Allais Paradox
Humans are not perfect expected utility maximizers.
The Allais Paradox, described in 1953
to highlight an inconsistency between utility theory and human behavior, presents two scenarios
where players must make a choice between two gambles [31].
Choice:
Gamble 1
Gamble 2
Scenario A
100% chance of $1 million
10% chance of $5 million
89% chance of $1 million
1% chance of $0
Table B.3: Allais Paradox: Scenario A

B.5. BEYOND EXPECTED UTILITY THEORIES
551
Choice:
Gamble 1
Gamble 2
Scenario B
11% chance of $1 million
89% chance of $0
10% chance of $5 million
90% chance of $0
Table B.4: Allais Paradox: Scenario B
In reality, most players favor Gamble 1 in Scenario A, due to the certainty of a large payout of $1
million. Simultaneously, they favor Gamble 2 in Scenario B, since the larger potential payout of $5
million outweighs its slightly lower likelihood. Both these preferences are individually sensible and
unproblematic. However, holding both preferences simultaneously is in violation of von Neumann-
Morgenstern’s independence axiom, and thus inconsistent with expected utility theory.
Humans violate the independence axiom.
We explained above that the independence axiom
holds that preferences between two lotteries are not impacted by the addition of equal probabilities
of a third, independent lottery to each lottery. It follows that we can subtract the same lottery from
two equivalent lotteries and preserve the original preference.
In Scenario A, an 89% chance of winning $1 million is common to both choices. Therefore, the
decision between Gamble 1 and Gamble 2 in Scenario A is equivalent to the reduced game described,
in which the 100% chance of winning $1 million has simply been divided into an 89% chance and an
11% chance of winning the same $1 million.
Gamble 1
Gamble 2
Scenario A
89% chance of $1 million
11% chance of $1 million
10% chance of $5 million
89% chance of $ 1 million
1% chance of $0
Scenario A reduced
11% chance of 1 million
10% chance of $ 5 million
1% chance of $0
Table B.5: Allais Paradox: Scenario A reduced
Similarly, there is a common lottery between Gamble 1 and Gamble 2 in Scenario B. We can ignore
an 89% chance of winning $0 from both choices and we will be left with the reduced game described,
in which the 89% chance of winning $0 has simply been ignored.
Gamble 1
Gamble 2
Scenario B
11% chance of $1 million
89% chance of $0
10% chance of $5 million
90% chance of $0
Scenario B reduced
11% chance of $1 million
10% chance of $5 million
1% chance of $0
Table B.6: Allais Paradox: Scenario B reduced
Scenario A Reduced and Scenario B Reduced are exactly the same! Therefore, a rational agent should
be consistent and select the same gamble in either simplified game. Since the simplified scenarios are
equivalent to the original scenarios via the independence axiom—adding third lotteries to both options
should make no difference to the choice in either scenario—we can conclude that a rational agent
should also select the same gamble in Scenario A and Scenario B. In reality, people overwhelmingly
favor Gamble 1 in Scenario A and Gamble 2 in Scenario B. This behavior is inconsistent with rational

552
APPENDIX B. UTILITY FUNCTIONS
behavior under the independence axiom.
We can interpret this discrepancy in two ways. We could assume that humans are making an
error of judgment. Or, we could conclude that human choice isn’t unreasonable, but that the von
Neumann-Morgenstern axioms are a flawed characterization of rationality. Indeed, Kahneman and
Tversky concluded that the expected utility model fails to capture important nuances in human
decision making [32].
Fairness
It is sometimes thought that the independence axiom is at odds with concepts of fairness.
The independence axiom tells us that adding equal probabilities of a third lottery makes no difference
to an agent’s preference between two other lotteries. However, in some cases, we do care what else
could happen: we make all-things-considered judgments of how we value outcomes. In particular, we
care about fairness [33].
If an agent cares about fairness, they may be forced to abandon independence.
Suppose
Rachel, on her deathbed, has to leave everything she owns to one person. She is indifferent between
everything going to her son and everything going to her daughter, but would like to treat them
equally. There is a 50% chance that the law will change such that everything goes to her son, no
matter what her will says. She is now considering two options, which we can write down as lotteries.
a. She leaves everything to her daughter. Let this be the “fair lottery” LF , wherein her daughter
receives everything with probability 0.5 (once her will is executed), and her son receives
everything with probability 0.5 (once the laws change).
b. She leaves everything to her son. Let this be the “unfair lottery” LU, wherein her son receives
everything with probability 1, since he receives it once her will is executed or once the laws
change.
According to the independence axiom, if she is indifferent between her son and daughter receiving
her possessions for sure, then she should also be indifferent between the above two lotteries because
we can obtain them by adding equal probabilities of a different outcome—a 50% chance her son gets
everything—to the original choice. However, if she cares at all about fairness, then she should prefer
LF over LU—a preference that is incompatible with the independence axiom!
Again, we see that the von Neumann-Morgenstern axioms lack descriptive power.
While,
in principle, adding alternatives should be irrelevant, they are not always so. The Allais paradox
demonstrated that adding alternatives changes how we think about monetary lotteries—this is
sometimes attributed to factors like avoiding regret. In this case, we are concerned with fairness.
Next, we will consider how humans systematically violate rationality based on the description of
options.
Framing
Human decision making is swayed by the presentation of choices.
Kahneman and Tversky
noticed that humans will make different decisions depending on how options are presented, even when
the underlying lotteries remain unchanged [34]. While a vNM-rational decision maker would ignore the
presentation of options and focus only on its probabilities and outcomes, humans tend to be unaware
of the large degree of influence that framing has on their decision making. Furthermore, human
decision making usually uses vague feelings of subjective probability rather than well-considered

B.5. BEYOND EXPECTED UTILITY THEORIES
553
mathematically defined lotteries. Consider the following example:
An illness is expected to kill 600 people if left untreated. Participants playing the role of health officials
must choose between two policy options in two different scenarios. These options are presented in
the table below.
Choice:
Policy A1
Policy A2
Scenario A
Save 200 people with certainty.
1/3 chance save 600 people.
2/3 chance saves no one.
Table B.7: Framing effects: Scenario A
Choice:
Policy B1
Policy B2
Scenario B
400 people die with certainty.
1/3 chance save of no deaths.
2/3 chance of 600 deaths.
Table B.8: Framing effects: Scenario B
Framing effects violate von Neumann-Morgenstern rationality.
Participants tend to choose
Policy A1 in Scenario A, because of the certainty of saving lives. Participants also tend to choose
Policy B2 in Scenario B, because of the possibility of averting more death and not condemning
people to certain death. However, the only difference between Scenario A and B is the manner of
presentation, or framing. Scenario A is presented in terms of lives saved, and Scenario B in terms of
deaths caused. The equivalence is shown in this table.
Choice:
Policy A1/B1
Policy A2 B2
Scenario A
Save 200 people with certainty
(and so causes 400 deaths for
sure).
1/3 chance save 600 people (or no
deaths).
2/3 chance saves no one (or 600
deaths).
Scenario B
Causes 400 deaths with certainty
(and so saves 200 people for sure)
1/3 chance save of no deaths (or
saving 600 people).
2/3 chance of 600 deaths (or
saving no one at all).
Table B.9: Framing effects: Scenarios A and B compared
Together, the choice of Policy A1 in Scenario A and Policy B2 in Scenario B violate a requirement of
von Neumann-Morgenstern rationality: that an agent be indifferent between two lotteries with the
same probabilities and outcomes, even if they are presented differently. In reality, an individual’s
habits, environment, and cognitive biases can cause different responses to framings of the same
underlying lottery. These are ways individuals can deviate from expected utility theory.
B.5.3
Prospect Theory
Prospect theory is a prominent non-expected utility model of human behavior.
Prospect
theory seeks to accurately describe how people make choices in risky situations by providing a
psychological model of decision making. The model’s features are designed to more accurately
describe human behavior in situations when facing risk, rather than provide a description of how

554
APPENDIX B. UTILITY FUNCTIONS
humans “should” behave, like in the vNM utility model. Thus, prospect theory explains common
behavioral patterns that are considered irrational in expected utility theory [35].
Prospect theory is a multi-stage decision-making model.
Prospect theory views decision
making over two stages: editing and evaluation. In the editing stage, outcomes are re-framed as
either gains or losses instead of just final wealth. This is important because people can perceive
the same outcome differently based on how the problem is presented and their own biases. This
stage also accounts for framing effects. In the evaluation stage, gains and losses are multiplied by
weighted probabilities to determine the preferred outcome. This stage takes into account two factors:
a value function, how people value different outcomes, and a weighting function, how people weigh
the probabilities of each outcome. Next, we will explore how these two functions help to explain how
people make decisions in uncertain situations.
Prospect theory’s value function approximates how humans think about wealth.
A
value function assigns value to an outcome, much like a Bernoulli utility function assigns utility to an
outcome. In prospect theory, the value function has a few key characteristics. The input is defined in
gains and losses, modeling the idea that people are sensitive to changes in wealth, not only to their
level of wealth. The curve is sigmoid to reflect that people are risk seeking towards losses and risk
averse towards gains. That is, people like a chance at avoiding losses but dislike a chance of losing
gains. The curve is steeper in losses since people are modeled more sensitive to a loss compared to a
gain of equal amount.
−10
−5
5
10
Losses/Gains
Value
Figure B.7: Prospect theory’s value function is steeper in losses than in gains.
Prospect theory’s decision weights describe how humans think about probabilities.
A
decision weight is the scaling factor applied to an outcome that quantifies how much that outcome
contributes to the decision. In expected utility theory, decision weights are just probabilities of
outcomes. Instead of assuming people accurately assess the likelihood of outcomes, however, prospect
theory accounts for the way humans actually process probabilities. People often overestimate the risk
of unlikely events with extreme consequences. Humans significantly overestimate the risk of dying in
a shark attack, possibly because of the graphic nature of the attacks and their over-representation in
pop media, when in reality the true probability of death by shark attack is quite low. Humans also

B.5. BEYOND EXPECTED UTILITY THEORIES
555
place a greater weight on relative certainty: people are usually willing to pay much more to improve
their odds from 0% to 1% than from 1% to 2%, since certainty of failure is removed. Prospect theory’s
proposed weighting for probabilities is shown in the next figure. This curve illustrates how people
tend to persistently overestimate relatively small probabilities while persistently underestimating
relatively large probabilities.
0
0.5
1
0
0.5
1
Real Probability
Subjective Probability Weighting
Ideal Estimates
Typical Estimates
Figure B.8: Humans often perceive probabilities incorrectly, overestimating the likelihood of low-
probability events and underestimating the likelihood of high-probability events.
B.5.4
Models of Decision Making
Generalized decision-making models.
We can put the above together to describe many different
types of decision-making models more than expected utility theory. Recall that expected utility is of
the form:
U (L) = p1 · u (o1) + p2 · u (o2) + · · · + pn · u (on) .
for a lottery L with outcomes oi and associated probabilities p, over a utility function u. We can
incorporate value functions and weighting functions to construct a theory of decision making that is
more general than expected utility theory while following the same structure of adding together the
products of decision weights and values:
V (L) = w (p1) · v (o1) + w (p2) · v (o2) + · · · + w (pn) · v (on) .
Here, V is the value assigned to the lottery. We have also added wi, a weighting function that
transforms each probability into a corresponding decision weight, and vi, a value function which—much
like a utility function—values each outcome.
Prospect theory, formalized.
Prospect theory uses this structure while using a slightly different
value function which evaluates gains and losses in wealth, rather than total wealth. The S-shaped
prospect theory value function is represented as vp (oi −o0), with oi representing the outcome being
evaluated, and o0 representing the agent’s initial state:
Vp = wp (p1) · vp (o1 −o0) + · · · + wp (pn) · vp (on −o0) .

556
APPENDIX B. UTILITY FUNCTIONS
In monetary lotteries, this would contain vp (wi −w0), which tells us that the value function considers
the loss or gain in wealth in each possible outcome. Prospect theory incorporates the specific
weighting and value functions determined by Kahneman and Tversky’s research on human behavior.
We can modify the model by substituting each function with functions of our own choosing.
Summary.
In this section, we considered the Allais Paradox, a thought experiment on fairness,
and two instances where the von Neumann-Morgenstern axioms fail to capture human preferences.
We also examined a study conducted by Kahneman and Tversky on framing effects, where humans
behave in a clearly irrational manner. We then examined the value function and the weighting
function, which are the two main innovations that comprise prospect theory and other non-expected
utility theories that seek to capture insights into human behavior where conventional theory fails.
Understanding the limitations of expected utility theory, and having more descriptive models of
human behavior helps us understand how humans, human-led organizations, and AIs imitating
humans will behave. In this chapter, and in the Single-Agent Safety chapter, we present reasons that
expected-utility maximizers can be dangerous. Non-expected utility theories provide some concepts
to consider when designing agents that are not expected-utility maximizers.
B.6
Conclusion
In this chapter, we studied the properties of utility functions and how agents use utility functions to
make decisions. Utility functions have been part of a significant paradigm within decision theory in
economics, psychology, and other fields, and are increasingly relevant to understanding and designing
artificial intelligence. Artificial agents in many cases are expressly designed to optimize objects (such
as reward functions) that strongly shape their utility functions.
We outlined the properties of Bernoulli utility functions, which allow us to express preferences
over goods and situations with precise numbers, and von-Neumann-Morgenstern utility functions,
which extend utility functions over probabilistic situations. From von Neumann-Morgenstern utility
functions, we derive the idea of expected utility theory: the idea that rational agents do and should
make choices that maximize the expectation of their utility function. This simple-sounding idea helps
us understand decision making, but also often fails to perfectly describe human behavior.
We applied utility functions to the problem of AI corrigibility—whether AI systems are receptive to
corrections. AIs with complete and transitive preferences will establish preferences about ceasing
to pursue their current objective, and consequently may attempt to thwart corrective measures.
Non-corrigible AI systems are a significant concern, since they create difficulties in making them safe.
We worked through examples of when it may be advisable to behave in risk-averse, risk-neutral, and
risk-seeking manners, which correspond to concave, linear, and convex utility functions respectively.
Risk aversion is a natural instinct for animals and humans, and helps maximize median value in the
long run. Risk neutrality maximizes expected value, but faces risk of ruin. Risk-seeking behavior is
often applied in situations where an agent has little to lose and a lot to gain. People and organizations
adopt different risk attitudes depending on the context and situation of the decision.
However, expected utility theory is a flawed theory—human behavior that we consider to be reasonable
often violates the strict rationality outlined by the von Neumann-Morgenstern axioms. Paradigms
outside expected utility theories, such as prospect theory, attempt to more accurately describe human
decision-making processes by incorporating additional functions that describe how humans think

B.7. LITERATURE
557
about wealth and subjectively weigh perceived probabilities.
An essential concern in designing artificial agents is that they must reflect human values. The broader
study of utility functions, and how humans and other agents do and should make decisions, is essential
context for ensuring that artificial agents avoid catastrophic risks and behave in accordance with
human values.
B.7
Literature
B.7.1
Recommended Reading
While this chapter is mostly self-contained, most college level microeconomics textbooks can serve as
a primary supplementary reading. A few examples, in increasing order of difficulty, are:
1. CORE. The Economy. (Section 3)
2. Hugh Gravelle and Ray Rees. Microeconomics. (Chapter 2)
3. Andreu Mas-Colell, Michael D. Whinston, Jerry R. Green. Microeconomic Theory. (Chapter 2)
By default, consult (2). In addition, you can look at further readings:
• Stanford Encyclopedia of Philosophy entry on the St. Petersburg Paradox:
• Adam Bales. “Will AI avoid exploitation? Artificial general intelligence and expected utility
theory”. In: Philosophical Studies (2023). url: https://doi.org/10.1007/s11098-023-
02023-4
• The Shutdown Problem: Two Theorems, Incomplete Preferences as a Solution, Thornley
(forthcoming)
• D. Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011
• Lara Buchak. Risk and Rationality. Oxford University Press, 2013. isbn: 9780199672165. doi:
10.1093/acprof:oso/9780199672165.001.0001. url: https://doi.org/10.1093/acprof:
oso/9780199672165.001.0001
B.7.2
References
[1]
P. Cilliers and David Spurrett. “Complexity and post-modernism: Understanding complex systems”.
In: South African Journal of Philosophy 18 (Sept. 2014), pp. 258–274. doi: 10.1080/02580136.1999.
10878187.
[2]
Alethea Power et al. Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.
2022. arXiv: 2201.02177 [cs.LG].
[3]
Jordan Hoffmann et al. Training Compute-Optimal Large Language Models. 2022. arXiv: 2203.15556
[cs.CL].
[4]
Hang Zhao et al. “Response mechanisms to heat stress in bees”. In: Apidologie 52 (Jan. 2021). doi:
10.1007/s13592-020-00830-w.
[5]
Trevor Kletz. An Engineer’s View of Human Error. CRC Press, 2018.
[6]
Lucas Davis. “The Effect of Driving Restrictions on Air Quality in Mexico City”. In: Journal of
Political Economy 116 (Feb. 2008), pp. 38–81. doi: 10.1086/529398.

558
APPENDIX B. UTILITY FUNCTIONS
[7]
Jemimah Steinfeld. “China’s deadly science lesson: How an ill-conceived campaign against sparrows
contributed to one of the worst famines in history”. In: Index on Censorship 47.3 (2018), pp. 49–49.
doi: 10.1177/0306422018800259.
[8]
Xin Meng, Nancy Qian, and Pierre Yared. “The Institutional Causes of China’s Great Famine,
1959–1961”. In: The Review of Economic Studies 82.4 (Apr. 2015), pp. 1568–1611. issn: 0034-6527.
doi: 10.1093/restud/rdv016.
[9]
Joseph Gottfried. “History Repeating? Avoiding a Return to the Pre-Antibiotic Age”. In: 2005. url:
https://api.semanticscholar.org/CorpusID:84028897.
[10]
Centers for Disease Control (US). “Antibiotic resistance threats in the United States, 2019”. In: CDC
Stacks (2019). doi: http://dx.doi.org/10.15620/cdc:82532.
[11]
“Cycling facts”. In: (2018). url: https://english.kimnet.nl/publications/publications/2018/
04/06/cycling-facts.
[12]
James Ladyman, James Lambert, and Karoline Wiesner. “What is a complex system?” In: European
Journal for Philosophy of Science 3 (June 2013). doi: 10.1007/s13194-012-0056-8.
[13]
D.H. Meadows and D. Wright. Thinking in Systems: A Primer. Chelsea Green Publishing, 2008. isbn:
9781603580557. url: https://books.google.com.au/books?id=CpbLAgAAQBAJ.
[14]
J.H. Miller and S.E. Page. Complex Adaptive Systems: An Introduction to Computational Models of
Social Life. Princeton Studies in Complexity. Princeton University Press, 2009. isbn: 9781400835522.
url: https://books.google.com.au/books?id=XQUHZC8wcdMC.
[15]
John Gall. The systems bible: the beginner’s guide to systems large and small. General Systemantics
Press, 2002.
[16]
Richard Cook. “How complex systems fail”. In: (Jan. 2002). url: https://how.complexsystems.fail.
[17]
Paul Schoemaker. “The Expected Utility Model: Its Variants, Purposes, Evidence and Limitations”.
In: Journal of Economic Literature 20 (Feb. 1982), pp. 529–63.
[18]
David Silver et al. “Reward is enough”. In: Artificial Intelligence 299 (2021), p. 103535. issn: 0004-3702.
doi: https://doi.org/10.1016/j.artint.2021.103535. url: https://www.sciencedirect.com/
science/article/pii/S0004370221000862.
[19]
J. Von Neumann and O. Morgenstern. Theory of games and economic behavior. Princeton University
Press, 1947.
[20]
Martin Peterson. “The St. Petersburg Paradox”. In: The Stanford Encyclopedia of Philosophy. Ed. by
Edward N. Zalta and Uri Nodelman. Fall 2023. Metaphysics Research Lab, Stanford University, 2023.
[21]
Nate Soares et al. “Corrigibility”. In: Workshops at the Twenty-Ninth AAAI Conference on Artificial
Intelligence. 2015.
[22]
Elliott Thornley. “The Shutdown Problem: Two Theorems, Incomplete Preferences as a Solution”. In:
AI Alignment Awards (2023).
[23]
A. Dixit. Slides and Notes. url: https : / / www . princeton . edu / ~dixitak / Teaching /
EconomicsOfUncertainty/Slides&Notes/%20%E2%80%8C.
[24]
Henry Sidgwick. The methods of ethics. Good Press, 2019.
[25]
Joseph Wunderle Jr, María Castro, and Ned Fetcher. “Risk-averse Foraging by Bananaquits on
Negative Energy Budgets”. In: Behavioral Ecology and Sociobiology 21 (Jan. 1987), pp. 249–255. doi:
10.1007/BF00292506.
[26]
Samir Okasha. “Rational choice, risk aversion, and evolution”. In: The Journal of philosophy 104.5
(2007), pp. 217–235.
[27]
John L Kelly. “A new interpretation of information rate”. In: the bell system technical journal 35.4
(1956), pp. 917–926.
[28]
Robert McDonald and Anna Paulson. “What went wrong at AIG”. In: Kellogg Insight 3 (2015).
[29]
N.N. Taleb. Antifragile: Things That Gain from Disorder. Incerto. Random House Publishing Group,
2012. isbn: 9780679645276. url: https://books.google.com.au/books?id=5fqbz_qGi0AC.

B.7. LITERATURE
559
[30]
Jessica Taylor. “Quantilizers: A Safer Alternative to Maximizers for Limited Optimization.” In: AAAI
Workshop: AI, Ethics, and Society. 2016.
[31]
M. Allais. “Le Comportement de l’Homme Rationnel devant le Risque: Critique des Postulats et
Axiomes de l’Ecole Americaine”. In: Econometrica 21.4 (1953), pp. 503–546. issn: 00129682, 14680262.
url: http://www.jstor.org/stable/1907921 (visited on 10/16/2023).
[32]
D. Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011.
[33]
David McCarthy. “Probability in Ethics”. In: The Oxford Handbook of Philosophy and Probability.
Ed. by Alan Ha?jek and Christopher Hitchcock. Oxford University Press, 2016, pp. 705–737.
[34]
Amos Tversky and Daniel Kahneman. “The framing of decisions and the psychology of choice”. In:
science 211.4481 (1981), pp. 453–458.
[35]
Daniel Kahneman and Amos Tversky. “Prospect theory: An analysis of decision under risk”. In:
Handbook of the fundamentals of financial decision making: Part I. World Scientific, 2013, pp. 99–127.
[36]
Adam Bales. “Will AI avoid exploitation? Artificial general intelligence and expected utility theory”.
In: Philosophical Studies (2023). url: https://doi.org/10.1007/s11098-023-02023-4.
[37]
Lara Buchak. Risk and Rationality. Oxford University Press, 2013. isbn: 9780199672165. doi:
10 . 1093 / acprof : oso / 9780199672165 . 001 . 0001. url: https : / / doi . org / 10 . 1093 / acprof :
oso/9780199672165.001.0001.

560
APPENDIX B. UTILITY FUNCTIONS
Complete References
Nelson Elhage et al. “Softmax linear units”. In: Transformer Circuits Thread (2022). url: https:
//transformer-circuits.pub/2022/solu/index.html.
Michael Moore. “The Rationality of Threshold Deontology”. In: Moral Puzzles and Legal Perplexities:
Essays on the Influence of Larry Alexander. Cambridge University Press, 2019, pp. 371–387.
P.
Bloom.
The
Sweet
Spot:
The
Pleasures
of
Suffering
and
the
Search
for
Meaning.
HarperCollinsPublishers, 2021. isbn: 9780062910561. url: https://books.google.com.au/books?id=
LUI7zgEACAAJ.
David Ili’c. “Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach”.
In: ArXiv abs/2310.11616 (2023).
L.A. Paul. Transformative Experience. Oxford Scholarship online. Oxford University Press, 2014. isbn:
9780198717959. url: https://books.google.com.au/books?id=zIXjBAAAQBAJ.
Thomas G. Dietterich. “Ensemble Methods in Machine Learning”. In: Multiple Classifier Systems. Berlin,
Heidelberg: Springer Berlin Heidelberg, 2000, pp. 1–15. isbn: 978-3-540-45014-6.
Nancy G Leveson. Engineering a safer world: Systems thinking applied to safety. The MIT Press, 2016.
Michael C Horowitz. The Diffusion of Military Power: Causes and Consequences for International
Politics. Princeton University Press, 2010.
Thomas H. Klier. “From Tail Fins to Hybrids: How Detroit Lost Its Dominance of the U.S. Auto Market”.
In: RePEc (May 2009).
Robert Sherefkin. “Ford 100: Defective Pinto Almost Took Ford’s Reputation With It”. In: Automotive
News (June 2003).
Heather Adkins et al. Building Secure and Reliable Systems: Best Practices for Designing, Implementing,
and Maintaining Systems. O’Reilly Media, 2020.
B Wayne Blanchard. “Guide to emergency management and related terms, definitions, concepts, acronyms,
organizations, programs, guidance, executive orders & legislation: A tutorial on emergency management,
broadly defined, past and present”. In: United States. Federal Emergency Management Agency. United
States. Federal Emergency Management Agency. 2008.
Bowen Baker et al. “Emergent Tool Use From Multi-Agent Autocurricula”. In: International Conference
on Learning Representations. 2020.
Katja Grace et al. “When will AI exceed human performance? Evidence from AI experts”. In: Journal
of Artificial Intelligence Research 62 (2018), pp. 729–754.
Department of Defense. “Quadrennial Defense Review Report”. In: (2001).
David Hume. A Treatise of Human Nature. 1739.
Allan Dafoe et al. “Open Problems in Cooperative AI”. In: ArXiv abs/2012.08630 (2020).
Aristotle. Nicomachean Ethics. 340 BC.
Stuart Armstrong. “General Purpose Intelligence: Arguing the Orthogonality Thesis”. In: 2013.
Dan Hendrycks et al. “Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty”.
In: NeurIPS. 2019.
E. G. Williams. “The Possibility of an Ongoing Moral Catastrophe”. In: Ethical Theory and Moral
Practice (2015).
Dan Hendrycks, Kimin Lee, and Mantas Mazeika. “Using Pre-Training Can Improve Model Robustness
and Uncertainty”. In: ICML. 2019.
Long Ouyang et al. “Training language models to follow instructions with human feedback”. In: ArXiv
(2022).
DARPA. “AlphaDogfight Trials Foreshadow Future of Human-Machine Symbiosis”. In: (2020).
Dakota Cary and Daniel Cebul. “Destructive Cyber Operations and Machine Learning”. In: 2020.

B.7. LITERATURE
561
Terence Tao. “Nines of safety: a proposed unit of measurement of risk”. In: 2021.
Ben Buchanan et al. “Automating Cyber Attacks”. In: 2021.
David Etkin. Disaster theory: an interdisciplinary approach to concepts and causes. Butterworth-
Heinemann, 2014.
Fabio L. Urbina et al. “Dual use of artificial-intelligence-powered drug discovery”. In: Nature Machine
Intelligence (2022).
Daniel M Ziegler et al. “Fine-tuning language models from human preferences”. In: arXiv preprint
arXiv:1909.08593 (2019).
Ethan Kross et al. “Facebook use predicts declines in subjective well-being in young adults”. In: PloS
one (2013).
Yisroel Mirsky et al. “The Threat of Offensive AI to Organizations”. In: Computers & Security (2023).
Xiang Gao et al. “Dialogue Response Ranking Training with Large-Scale Human Feedback Data”. In:
EMNLP. 2020.
Yushi Bai et al. “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human
Feedback”. In: ArXiv (2022).
Adam Gleave et al. “Adversarial Policies: Attacking Deep Reinforcement Learning”. In: ArXiv (2020).
David Hambling. Israel used world’s first AI-guided combat drone swarm in Gaza attacks. 2021.
Carla J Shatz. “The developing brain”. In: Scientific American 267.3 (1992), pp. 60–67.
T. Schroeder. Three Faces of Desire. Philosophy of Mind Series. Oxford University Press, USA, 2004.
G. Thut et al. “Activation of the human brain by monetary reward”. In: Neuroreport 8.5 (1997), pp. 1225–
1228.
Edmund T. Rolls. “The Orbitofrontal Cortex and Reward”. In: Cerebral Cortex 10.3 (Mar. 2000),
pp. 284–294.
Bruce Schneier. “Inside the Twisted Mind of the Security Professional”. In: Wired (Mar. 2008).
U.S. Nuclear Regulatory Commission. Final Safety Culture Policy Statement. Federal Register. 2011.
Dario Amodei and Jack Clark. Faulty reward functions in the wild. 2016.
National Research Council et al. Lessons Learned from the Fukushima Nuclear Accident for Improving
Safety of U.S. Nuclear Plants. Washington, D.C.: National Academies Press, Oct. 2014.
Nancy Leveson. “Introduction to STAMP”. In: STAMP Workshop Presentations (2020).
Richard Sutton. “The Bitter Lesson”. In: (2019).
Alexander Matt Turner et al. “Optimal Policies Tend To Seek Power”. In: NeurIPS. 2021.
Dylan Hadfield-Menell et al. “The Off-Switch Game”. In: IJCA (2017).
Deep Ganguli et al. “Predictability and Surprise in Large Generative Models”. In: ArXiv (2022).
Maximilian Gahntz and Claire Pershan. Artificial Intelligence Act: How the EU can take on the challenge
posed by general-purpose AI systems. Nov. 2022.
Andy Zou et al. Benchmarking Neural Network Proxy Robustness to Optimization Pressure. 2023.
Pierre-François Lovens. “Sans ces conversations avec le chatbot Eliza, mon mari serait toujours là”. In:
La Libre (Mar. 2023).
Anna Tong. “What happens when your AI chatbot stops loving you back?” In: Reuters (Mar. 2023).
Harrison Foley et al. “Execute Order 66: Targeted Data Poisoning for Reinforcement Learning”. In:
ArXiv abs/2201.00762 (2022).
Peter Railton. Ethics and Artificial Intelligence.
Franz Dietrich and Kai Spiekermann. “Jury Theorems”. In: The Stanford Encyclopedia of Philosophy.
Ed. by Edward N. Zalta. Summer 2022. Metaphysics Research Lab, Stanford University, 2022.
J. R. French and Bertram H. Raven. “The bases of social power.” In: 1959.

562
APPENDIX B. UTILITY FUNCTIONS
Alethea Power et al. “Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets”. In:
ICLR MATH-AI Workshop. 2021.
Evan Hubinger et al. “Risks from Learned Optimization in Advanced Machine Learning Systems”. In:
ArXiv (2019).
Owain Evans et al. “Truthful AI: Developing and governing AI that does not lie”. In: ArXiv (2021).
David H. Wolpert. “The Lack of A Priori Distinctions Between Learning Algorithms”. In: Neural
Computation (1996).
Shelly Kagan. “The Structure of Normative Ethics”. In: Philosophical Perspectives (1992).
Nick Bostrom. “Existential risks: analyzing human extinction scenarios and related hazards”. In: 2002.
Milton Friedman. “Capitalism and Freedom”. In: Economica (1963).
Richard I. Cook. “How Complex Systems Fail”. In: 1998.
Nancy G. Leveson et al. “Moving Beyond Normal Accidents and High Reliability Organizations: A
Systems Approach to Safety in Complex Systems”. In: Organization Studies (2009).
David A. Patterson. “For better or worse, benchmarks shape a field: technical perspective”. In: Commun.
ACM (2012).
David McAllester. “Rate-Distortion Metrics for GAN”. In: (2017).
V. Ridgway. “Dysfunctional Consequences of Performance Measurements”. In: Administrative Science
Quarterly (1956).
Dan Hendrycks et al. “The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution
Generalization”. In: ICCV (2021).
Roger Crisp. “Well-Being”. In: The Stanford Encyclopedia of Philosophy. Ed. by Edward N. Zalta. Winter
2021. Metaphysics Research Lab, Stanford University, 2021.
Michael Klare. “Skynet Revisited: The Dangerous Allure of Nuclear Command Automation”. In: Arms
Control Association (2020).
Nick Bostrom. “The Vulnerable World Hypothesis”. In: Global Policy (2019).
Todd La Porte. “High Reliability Organizations: Unlikely, Demanding, and At Risk”. In: Journal of
Contingencies and Crisis Management (1996).
Trevor Kletz. “What you don’t have, can’t leak”. In: Chemistry and Industry (1978).
John Gall. The systems bible: the beginner’s guide to systems large and small. General Systemantics
Press, 2002.
C. Perrow. Normal Accidents: Living with High Risk Technologies. Princeton paperbacks. Princeton
University Press, 1999.
Nassim Nicholas Taleb et al. “The precautionary principle (with application to the genetic modification
of organisms)”. In: arXiv preprint arXiv:1410.5787 (2014).
Nassim Nicholas Taleb. “Statistical consequences of fat tails: Real world preasymptotics, epistemology,
and applications”. In: arXiv preprint arXiv:2001.10488 (2020).
Collin Burns et al. “Unsupervised Discovery of Latent Truth in Language Models”. In: arXiv (2022).
Richard I Cook. “How complex systems fail”. In: Cognitive Technologies Laboratory, University of
Chicago. Chicago IL (1998).
Toby Ord. The precipice: Existential risk and the future of humanity. Hachette Books, 2020.
Nicholas Beckstead. On the overwhelming importance of shaping the far future. Rutgers The State
University of New Jersey-New Brunswick, 2013.
Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, Inc., 2014.
Nick Bostrom. “The vulnerable world hypothesis”. In: Global Policy 10.4 (2019), pp. 455–476.
Nick Bostrom, Thomas Douglas, and Anders Sandberg. “The Unilateralist’s Curse and the Case for a
Principle of Conformity”. In: Social epistemology 30.4 (2016), pp. 350–371.

B.7. LITERATURE
563
Michael Anderson and Susan Leigh Anderson. “Machine ethics: Creating an ethical intelligent agent”. In:
AI magazine 28.4 (2007), pp. 15–15.
Thomas G Dietterich. “Robust artificial intelligence and robust human organizations”. In: Frontiers of
Computer Science 13.1 (2019), pp. 1–3.
Laura Weidinger et al. “Ethical and social risks of harm from Language Models”. In: arXiv preprint
arXiv:2112.04359 (2021).
Toby Newberry and Toby Ord. “The Parliamentary Approach to Moral Uncertainty”. In: 2021.
Rain Ottis. “Analysis of the 2007 Cyber Attacks Against Estonia from the Information Warfare
Perspective”. In: 2008.
Miles Brundage et al. “The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and
Mitigation”. In: ArXiv abs/1802.07228 (2018).
David Malin Roodman. On the probability distribution of long-term changes in the growth rate of the
global economy: An outside view. 2020.
Irene Solaiman et al. “Release strategies and the social impacts of language models”. In: arXiv preprint
arXiv:1908.09203 (2019).
Tom Brown et al. “Language models are few-shot learners”. In: Advances in neural information processing
systems 33 (2020), pp. 1877–1901.
Mark Chen et al. “Evaluating large language models trained on code”. In: arXiv preprint arXiv:2107.03374
(2021).
Dario Amodei et al. “Concrete problems in AI safety”. In: arXiv preprint arXiv:1606.06565 (2016).
Nate Soares et al. “Corrigibility”. In: Workshops at the Twenty-Ninth AAAI Conference on Artificial
Intelligence. 2015.
Rishi Bommasani et al. “On the opportunities and risks of foundation models”. In: arXiv preprint
arXiv:2108.07258 (2021).
Kathryn Tunyasuvunakool et al. “Highly accurate protein structure prediction for the human proteome”.
In: Nature 596.7873 (2021), pp. 590–596.
Stanislas Polu and Ilya Sutskever. “Generative language modeling for automated theorem proving”. In:
arXiv preprint arXiv:2009.03393 (2020).
Aditya Ramesh et al. “Hierarchical Text-Conditional Image Generation with CLIP Latents”. In: arXiv
preprint arXiv:2204.06125 (2022).
Nick Bostrom and Milan M Cirkovic. Global catastrophic risks. Oxford University Press, 2011.
Julian Schrittwieser et al. “Mastering atari, go, chess and shogi by planning with a learned model”. In:
Nature 588.7839 (2020), pp. 604–609.
Bolun Wang et al. “Neural cleanse: Identifying and mitigating backdoor attacks in neural networks”. In:
2019 IEEE Symposium on Security and Privacy (SP). IEEE. 2019, pp. 707–723.
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. “Deep Anomaly Detection with Outlier
Exposure”. In: Proceedings of the International Conference on Learning Representations (2019).
Dan Hendrycks et al. “What Would Jiminy Cricket Do? Towards Agents That Behave Morally”. In:
NeurIPS (2021).
Dan Hendrycks et al. “Aligning AI With Shared Human Values”. In: Proceedings of the International
Conference on Learning Representations (ICLR) (2021).
Aleksander Madry et al. “Towards Deep Learning Models Resistant to Adversarial Attacks”. In:
International Conference on Learning Representations. 2018.
Peter Godfrey-Smith. “Conditions for Evolution by Natural Selection”. In: The Journal of Philosophy
104 (2007), pp. 489–516.
John Maynard Smith. “Evolution and the Theory of Games”. In: American scientist (1982).

564
APPENDIX B. UTILITY FUNCTIONS
Alan G. Barbour and Blanca I. Restrepo. “Antigenic variation in vector-borne pathogens.” In: Emerging
Infectious Diseases (2000).
Thorbjørn Knudsen. “Simon’s Selection Theory: Why Docility Evolves to Breed Successful Altruism”.
In: Behavioral & Experimental Economics (2003).
Robert O. Keohane and Joseph S. Nye. “Power and interdependence : world politics in transition”. In:
1978.
Samir Okasha. “Agents and Goals in Evolution”. In: Oxford Scholarship Online (2018).
Samir Okasha. “Evolution and the Levels of Selection”. In: 2007.
John C. Harsanyi. “Cardinal Welfare, Individualistic Ethics, and Interpersonal Comparisons of Utility”.
In: Journal of Political Economy (1955).
John Rawls. “A Theory of Justice”. In: Princeton Readings in Political Thought (1971).
Peter Singer. The Expanding Circle: Ethics and Sociobiology. Farrar, Straus and Giroux, 1981.
Charles Darwin. The Descent of Man, and Selection in Relation to Sex. 1871.
W. D. Hamilton. “The genetical evolution of social behaviour. I”. In: Journal of theoretical biology (1964).
Martin A. Nowak. “Five Rules for the Evolution of Cooperation”. In: Science (2006).
George R. Price. “Selection and Covariance”. In: Nature (1970).
Peter Godfrey-Smith. “The Replicator in Retrospect”. In: Biology and Philosophy (2000).
Daniel C. Dennett. “Darwin’s Dangerous Idea: Evolution and the Meanings of Life”. In: 1995.
Susan J. Blackmore. The Meme Machine. Oxford University Press, 1999.
Wojciech H Zurek. “Quantum Darwinism”. In: Nature Physics (2009).
Lee Smolin. “Did the Universe Evolve?” In: Classical and Quantum Gravity (1992).
Andy Zou et al. “Forecasting Future World Events with Neural Networks”. In: NeurIPS (2022).
Geoffrey M. Hodgson. “Economics and Evolution: Bringing Life Back into Economics”. In: 1993.
Agner Fog. Cultural selection. Springer Science & Business Media, 1999.
Richard C Lewontin. “The units of selection”. In: Annual review of ecology and systematics (1970).
Richard R. Nelson. “Evolutionary social science and universal Darwinism”. In: Journal of Evolutionary
Economics 16 (2006), pp. 491–510.
Alex Mesoudi. “Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize
the Social Sciences”. In: 2011.
Donald T. Campbell. “Blind variation and selective retention in creative thought as in other knowledge
processes”. In: Psychological review (1960).
Joseph Carlsmith. “Existential Risk from Power-Seeking AI”. In: Oxford University Press (2023).
Nick Bostrom. “Superintelligence: Paths, Dangers, Strategies”. In: 2014.
Stephen M Omohundro. “The basic AI drives”. In: AGI 2008: Proceedings of the First Conference on
Artificial General Intelligence. 2008.
A. Harry Klopf. “Brain Function and Adaptive Systems: A Heterostatic Theory”. In: 1972.
Charles Darwin. On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured
Races in the Struggle for Life. London: John Murray, 1859.
Christopher Boehm. Moral Origins: The Evolution of Virtue, Altruism, and Shame. Basic Books, 2012.
Christopher Boehm. Hierarchy in the Forest: The Evolution of Egalitarian Behavior. Harvard University
Press, 1999.
Christopher Boehm et al. “Egalitarian Behavior and Reverse Dominance Hierarchy [and Comments and
Reply]”. In: Current Anthropology (1993).
Robert Trivers. “Deceit and self-deception : fooling yourself the better to fool others”. In: 2013.

B.7. LITERATURE
565
Thomas C. Schelling. “Micromotives and Macrobehavior”. In: 1978.
Daniel Martín-Vega et al. “3D virtual histology at the host/parasite interface: visualisation of the master
manipulator, Dicrocoelium dendriticum, in the brain of its ant host”. In: Scientific reports 8.1 (2018),
pp. 1–10.
Anne E Pusey and Craig Packer. “Infanticide in Lions: Consequences and Counterstrategies”. In:
Infanticide and parental care (1994), p. 277.
Richard Dawkins and Nicola Davis. The selfish gene. Macat Library, 2017.
Ronald Aylmer Fisher. The genetical theory of natural selection. 1958.
Christian List and Robert E. Goodin. “Epistemic Democracy: Generalizing the Condorcet Jury Theorem”.
In: Journal of Political Philosophy (2001).
Kim Zetter. “Meet MonsterMind, the NSA Bot That Could Wage Cyberwar Autonomously”. In: Wired
(Aug. 2014).
J.E. Mueller. War, Presidents, and Public Opinion. UPA book. University Press of America, 1985.
Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. “Deep blue”. In: Artificial intelligence
134.1-2 (2002), pp. 57–83.
Helmut Bester and Werner Güth. “Is altruism evolutionarily stable?” In: Journal of Economic Behavior
& Organization 34.2 (1998), pp. 193–209.
Gerald G Carter et al. “Development of new food-sharing relationships in vampire bats”. In: Current
Biology 30.7 (2020), pp. 1275–1279.
Tom Davidson. Could Advanced AI Drive Explosive Economic Growth? Tech. rep. June 2021.
David Silver et al. “A general reinforcement learning algorithm that masters chess, shogi, and Go through
self-play”. In: Science 362.6419 (2018), pp. 1140–1144.
John Jumper et al. “Highly accurate protein structure prediction with AlphaFold”. In: Nature 596.7873
(2021), pp. 583–589.
Barret Zoph and Quoc V Le. “Neural architecture search with reinforcement learning”. In: arXiv preprint
arXiv:1611.01578 (2016).
Yu Sun et al. “Test-time training with self-supervision for generalization under distribution shifts”. In:
International conference on machine learning. PMLR. 2020, pp. 9229–9248.
Irving John Good. “Speculations concerning the first ultraintelligent machine”. In: Advances in computers.
Vol. 6. Elsevier, 1966, pp. 31–88.
Stuart Russell. Human Compatible: Artificial Intelligence and the Problem of Control. en. Penguin, Oct.
2019.
RIM Dunbar. “Co-evolution of neuro-cortex size, group size and language in humans”. In: Brain and
Behavior Sciences 16 (1992).
Patrik Lindenfors, Andreas Wartel, and Johan Lind. “‘Dunbar’s number’deconstructed”. In: Biology
Letters 17.5 (2021), p. 20210158.
Max Tegmark. Life 3.0: Being human in the age of artificial intelligence. Vintage, 2018.
Nick Bostrom. “Astronomical waste: The opportunity cost of delayed technological development”. In:
Utilitas 15.3 (2003), pp. 308–314.
Leanne Pooley. We Need To Talk About A.I. 2020.
Martin A Nowak, Roger Highfield, et al. Supercooperators. Canongate Edinburgh, 2011.
Henry Sidgwick. The methods of ethics. Good Press, 2019.
Mary Gregor and Jens Timmermann. “Immanuel Kant: Groundwork of the Metaphysics of Morals: A
German–English Edition”. In: (2014).
Alexander Pan, Kush Bhatia, and Jacob Steinhardt. “The effects of reward misspecification: Mapping
and mitigating misaligned models”. In: ICLR (2022).

566
APPENDIX B. UTILITY FUNCTIONS
Steven Pinker. The better angels of our nature: Why violence has declined. Penguin Books, 2012.
Scott E Page. The model thinker: What you need to know to make data work for you. Basic Books, 2018.
Stephen C. Stearns. “ARE WE STALLED PART WAY THROUGH A MAJOR EVOLUTIONARY
TRANSITION FROM INDIVIDUAL TO GROUP?” In: Evolution; international journal of organic
evolution. 2007.
Andrea A Lunsford and Karen J Lunsford. “"Mistakes are a fact of life": A national comparative study”.
In: College Composition and Communication (2008), pp. 781–806.
https://www.facebook.com/bbcnews. Poor spelling of ’auto-correct generation’ revealed. [Accessed
12-Dec-2022].
Pew Reseach Center. “Political polarization in the American public”. In: Ann Rev Polit Sci (2014).
Rachel Kleinfeld. “The rise of political violence in the United States”. In: Journal of Democracy 32.4
(2021), pp. 160–176.
Susanne Buecker et al. “Is loneliness in emerging adults increasing over time? A preregistered cross-
temporal meta-analysis and systematic review.” In: Psychological Bulletin 147.8 (2021), p. 787.
Centers for Disease Control, Prevention, et al. “Loneliness and social isolation linked to serious health
conditions”. In: Alzheimer’s Disease and Healthy Aging (2020).
Toby Newberry and Toby Ord. The Parliamentary Approach to Moral Uncertainty. Tech. rep. Technical
Report# 2021-2, Future of Humanity Institute, University of Oxford . . ., 2021.
Dan Hendrycks et al. “Unsolved Problems in ML Safety”. In: arXiv preprint arXiv:2109.13916 (2021).
Luke McNally and Andrew L Jackson. “Cooperation creates selection for tactical deception”. In:
Proceedings of the Royal Society B: Biological Sciences 280.1762 (2013), p. 20130699.
Robert Trivers. The folly of fools: The logic of deceit and self-deception in human life. Basic Books (AZ),
2011.
While Warning About Fat, U.S. Pushes Cheese Sales (Published 2010) — nytimes.com. [Accessed
13-Dec-2022].
William D Hamilton. “Discrimination nepotism: expectable, common, overlooked”. In: Kin recognition
in animals (1987), pp. 417–437.
Scott A Wooldridge. “Breakdown of the coral-algae symbiosis: towards formalising a linkage between warm-
water bleaching thresholds and the growth rate of the intracellular zooxanthellae”. In: Biogeosciences
10.3 (2013), pp. 1647–1658.
Federal Aviation Authority. Out front on airline safety: Two decades of continuous evolution. 2018.
John R Allen and M Darrell. It is Time to Negotiate Global Treaties on Artificial Intelligence.
Christine G. Elsik, Ross L. Tellam, and Kim C. Worley. “The Genome Sequence of Taurine Cattle: A
Window to Ruminant Biology and Evolution”. In: Science 324 (2009), pp. 522–528.
Valen Van. “A new evolutionary law”. In: (1973).
Samir Okasha. “Biological Altruism”. In: The Stanford Encyclopedia of Philosophy. Ed. by Edward N.
Zalta. Summer 2020. Metaphysics Research Lab, Stanford University, 2020.
Wassily W. Leontief. “Machines and Man”. In: Scientific American 187 (1952), pp. 150–160.
Keith Olson. “Aum Shinrikyo: once and future threat?” In: Emerging Infectious Diseases 5 (1999),
pp. 513–516.
Alfred Buschinger. “Social Parasitism among Ants: A Review”. In: Myrmecological News 12 (Sept. 2009),
pp. 219–235.
Peter D. Nagy and Judit Pogany. “The dependence of viral RNA replication on co-opted host factors”.
In: Nature Reviews. Microbiology 10 (2011), pp. 137–149.
Laura Martínez-Íñigo et al. “Intercommunity interactions and killings in central chimpanzees (Pan
troglodytes troglodytes) from Loango National Park, Gabon”. In: Primates; Journal of Primatology 62
(2021), pp. 709–722.

B.7. LITERATURE
567
Richard C. Lewontin. “The Units of Selection”. In: Annual Review of Ecology, Evolution, and Systematics
1 (1970), pp. 1–18.
Ashok K. Dutt, Rais Akhtar, and Melinda McVeigh. “Surat plague of 1994 re-examined.” In: The
Southeast Asian journal of tropical medicine and public health 37 4 (2006), pp. 755–60.
Siro Igino Trevisanato. “The ’Hittite plague’, an epidemic of tularemia and the first record of biological
warfare.” In: Medical hypotheses 69 6 (2007), pp. 1371–4.
Sarah R. Carter, Jaime M. Yassif, and Chris Isaac. Benchtop DNA Synthesis Devices: Capabilities,
Biosecurity Implications, and Governance. Report. Nuclear Threat Initiative, 2023.
Emily Soice et al. “Can large language models democratize access to dual-use biotechnology?” In: 2023.
Kevin M. Esvelt. Delay, Detect, Defend: Preparing for a Future in which Thousands Can Release New
Pandemics. 2022.
A. Sanz-García et al. “Prevalence of Psychopathy in the General Adult Population: A Systematic Review
and Meta-Analysis”. In: Frontiers in Psychology 12 (2021).
Onur Varol et al. “Online Human-Bot Interactions: Detection, Estimation, and Characterization”. In:
ArXiv abs/1703.03107 (2017).
Matthew Burtell and Thomas Woodside. “Artificial Influence: An Analysis Of AI-Driven Persuasion”.
In: ArXiv abs/2303.08721 (2023).
Cristian Vaccari and Andrew Chadwick. “Deepfakes and Disinformation: Exploring the Impact of
Synthetic Political Video on Deception, Uncertainty, and Trust in News”. In: Social Media + Society 6
(2020).
The Nucleic Acid Observatory Consortium. “A Global Nucleic Acid Observatory for Biodefense and
Planetary Health”. In: ArXiv abs/2108.02678 (2021).
Toby Shevlane. “Structured access to AI capabilities: an emerging paradigm for safe AI deployment”. In:
ArXiv abs/2201.05159 (2022).
Yonadav Shavit. “What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural
Network Training via Compute Monitoring”. In: ArXiv abs/2303.11341 (2023).
Anat Lior. “AI Entities as AI Agents: Artificial Intelligence Liability and the AI Respondeat Superior
Analogy”. In: Torts & Products Liability Law eJournal (2019).
International Atomic Energy Agency. The Chernobyl Accident: Updating of INSAG-1. Technical Report
INSAG-7. Vienna, Austria: International Atomic Energy Agency, 1992.
Matthew Meselson et al. “The Sverdlovsk anthrax outbreak of 1979.” In: Science 266 5188 (1994),
pp. 1202–8.
Charles Perrow. Normal Accidents: Living with High-Risk Technologies. Princeton, NJ: Princeton
University Press, 1984.
Mitchell Rogovin and George T. Frampton Jr. Three Mile Island: a report to the commissioners and to
the public. Volume I. English. Tech. rep. NUREG/CR-1250(Vol.1). Nuclear Regulatory Commission,
Washington, DC (United States). Three Mile Island Special Inquiry Group, Jan. 1979.
Yohan J. John et al. “Dead rats, dopamine, performance metrics, and peacock tails: proxy failure is
an inherent risk in goal-oriented systems”. In: Behavioral and Brain Sciences (2023), pp. 1–68. doi:
10.1017/S0140525X23002753.
Michael Ellman. Planning problems in the USSR: the contribution of mathematical economics to their
solution 1960-1971. Vol. 24. CUP Archive, 1973.
Donald T Campbell. “Assessing the impact of planned social change”. In: Evaluation and program
planning 2.1 (1979), pp. 67–90.
Richard Rhodes. The Making of the Atomic Bomb. New York: Simon & Schuster, 1986.
Sébastien Bubeck et al. “Sparks of Artificial General Intelligence: Early experiments with GPT-4”. In:
ArXiv abs/2303.12712 (2023).

568
APPENDIX B. UTILITY FUNCTIONS
Kate Moore. The Radium Girls: The Dark Story of America’s Shining Women. Naperville, IL:
Sourcebooks, 2017.
Betul Keles, Niall McCrae, and Annmarie Grealish. “A systematic review: the influence of social media
on depression, anxiety and psychological distress in adolescents”. In: International Journal of Adolescence
and Youth 25 (2019), pp. 79–93.
Stephen S. Hecht. “Tobacco smoke carcinogens and lung cancer.” In: Journal of the National Cancer
Institute 91 14 (1999), pp. 1194–210.
Mario J. Molina and F. Sherwood Rowland. “Stratospheric sink for chlorofluoromethanes: chlorine
atomc-atalysed destruction of ozone”. In: Nature 249 (1974), pp. 810–812.
James H. Kim and Anthony R. Scialli. “Thalidomide: the tragedy of birth defects and the effective
treatment of disease.” In: Toxicological sciences : an official journal of the Society of Toxicology 122 1
(2011), pp. 1–6.
Brooke T. Mossman et al. “Asbestos: scientific developments and implications for public policy.” In:
Science 247 4940 (1990), pp. 294–301.
Theodore I. Lidsky and Jay S. Schneider. “Lead neurotoxicity in children: basic mechanisms and clinical
correlates.” In: Brain : a journal of neurology 126 Pt 1 (2003), pp. 5–19.
Zakir Durumeric et al. “The Matter of Heartbleed”. In: Proceedings of the 2014 Conference on Internet
Measurement Conference (2014).
Tony Tong Wang et al. “Adversarial Policies Beat Professional-Level Go AIs”. In: ArXiv abs/2211.00241
(2022).
T. R. Laporte and Paula M. Consolini. “Working in Practice But Not in Theory: Theoretical Challenges
of “High-Reliability Organizations””. In: Journal of Public Administration Research and Theory 1 (1991),
pp. 19–48.
Thomas G. Dietterich. “Robust artificial intelligence and robust human organizations”. In: Frontiers of
Computer Science 13 (2018), pp. 1–3.
Diane Vaughan. The Challenger Launch Decision: Risky Technology, Culture, and Deviance at NASA.
Chicago, IL: University of Chicago Press, 1996.
Dan Hendrycks and Mantas Mazeika. “X-Risk Analysis for AI Research”. In: ArXiv abs/2206.05862
(2022).
Huashan Li et al. “The impact of chief risk officer appointments on firm risk and operational efficiency”.
In: Journal of Operations Management (2022).
Ziad Obermeyer et al. “Dissecting racial bias in an algorithm used to manage the health of populations”.
In: Science 366 (2019), pp. 447–453.
John Mearsheimer. “Structural realism”. In: Oxford University Press, 2007.
Nassim Nicholas Taleb. “The Fourth Quadrant: A Map of the Limits of Statistics”. In: Edge, 2008.
Dylan Hadfield-Menell et al. “The Off-Switch Game”. In: ArXiv abs/1611.08219 (2016).
Anton Bakhtin et al. “Human-level play in the game of Diplomacy by combining language models with
strategic reasoning”. In: Science 378 (2022), pp. 1067–1074.
Miles Turpin et al. “Language Models Don’t Always Say What They Think: Unfaithful Explanations in
Chain-of-Thought Prompting”. In: ArXiv abs/2305.04388 (2023).
Collin Burns et al. “Discovering Latent Knowledge in Language Models Without Supervision”. en. In:
The Eleventh International Conference on Learning Representations. Feb. 2023.
Leo Gao, John Schulman, and Jacob Hilton. “Scaling Laws for Reward Model Overoptimization”. In:
ArXiv abs/2210.10760 (2022).
Catherine Olsson et al. “In-context Learning and Induction Heads”. In: ArXiv abs/2209.11895 (2022).
Kevin Ro Wang et al. “Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2
Small”. en. In: The Eleventh International Conference on Learning Representations. Feb. 2023.

B.7. LITERATURE
569
Kevin Meng et al. “Locating and Editing Factual Associations in GPT”. In: Neural Information Processing
Systems. 2022.
Xinyang Zhang, Zheng Zhang, and Ting Wang. “Trojaning Language Models for Fun and Profit”. In:
2021 IEEE European Symposium on Security and Privacy (EuroS&P) (2020), pp. 179–197.
Jiashu Xu et al. “Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large
Language Models”. In: ArXiv abs/2305.14710 (2023).
Dan Hendrycks et al. “Unsolved Problems in ML Safety”. In: ArXiv abs/2109.13916 (2021).
Nora Belrose et al. “LEACE: Perfect linear concept erasure in closed form”. In: ArXiv abs/2306.03819
(2023).
Carl Sagan. Pale Blue Dot: A Vision of the Human Future in Space. New York: Random House, 1994.
Zachary Kallenborn. Applying arms-control frameworks to autonomous weapons. en-US. Oct. 2021.
David Hoffman. Dead Hand: The Untold Story of the Cold War Arms Race and Its Dangerous Legacy.
New York: Anchor, 2009.
Andrei Kirilenko et al. “The Flash Crash: High-Frequency Trading in an Electronic Market”. In: The
Journal of Finance 72.3 (2017), pp. 967–998.
Robert E. Jervis. “Cooperation under the Security Dilemma”. In: World Politics 30 (1978), pp. 167–214.
Dan Hendrycks. “Natural Selection Favors AIs over Humans”. In: ArXiv abs/2303.16200 (2023).
Roman V Yampolskiy. “Taxonomy of Pathways to Dangerous Artificial Intelligence”. In: AAAI Workshop:
AI, Ethics, and Society. 2016.
Matteo E. Bonfanti. “Artificial intelligence and the offense–defense balance in cyber security”. In: Cyber
Security Politics: Socio-Technological Transformations and Political Fragmentation. Ed. by M.D. Cavelty
and A. Wenger. CSS Studies in Security and International Relations. Taylor & Francis, 2022. Chap. 5,
pp. 64–79.
U.S. Department of State. Adherence to and Compliance with Arms Control, Nonproliferation, and
Disarmament Agreements and Commitments. Government Report. U.S. Department of State, Apr. 2022.
Panel of Experts on Libya. Letter dated 8 March 2021 from the Panel of Experts on Libya established
pursuant to resolution 1973 (2011) addressed to the President of the Security Council. United Nations
Security Council Document S/2021/229. United Nations, Mar. 2021.
National Security Archive. Vice-Admiral Vasili Arkhipov. https://nsarchive.gwu.edu/document/
29078-document-1-vice-admiral-vasili-arkhipov.
Joe Hernandez. A military drone with a mind of its own was used in combat, U.N. says. June 2021. url:
https://www.npr.org/2021/06/01/1002196245/a-u-n-report-suggests-libya-saw-the-first-
battlefield-killing-by-an-autonomous-d.
“The Most Wanted Man in the World”. In: Wired (). url: https://www.wired.com/2014/08/edward-
snowden/.
Richard Danzig. Technology Roulette: Managing Loss of Control as Many Militaries Pursue Technological
Superiority. Tech. rep. Center for a New American Security, June 2018.
The AI Arms Race Is On. Start Worrying2023. en. Feb. 2023. url: https://time.com/6255952/ai-
impact-chatgpt-microsoft-google/.
Paul Scharre. Army of None: Autonomous Weapons and The Future of War. Norton, 2018.
Nico Grant and Karen Weise. “In A.I. Race, Microsoft and Google Choose Speed Over Caution”. en-US.
In: The New York Times (Apr. 2023).
Mark Dowie. Pinto Madness. en-US. url: https://www.motherjones.com/politics/1977/09/pinto-
madness/.
Boeing 737 MAX Reading Room. url: https://www.faa.gov/foia/electronic_reading_room/
boeing_reading_room.
CSRC Content Editor. Red Team - Glossary. EN-US.

570
APPENDIX B. UTILITY FUNCTIONS
Neal Woollen. Incident Response (Why Planning is Important).
Role of Internal Audit. url: https://www.marquette.edu/riskunit/internalaudit/role.shtml.
Amba Kak and Sarah West. Confronting Tech Power. 2023.
Dan Hendrycks. The Darwinian Argument for Worrying About AI. en. May 2023.
Robert Carlson. “The changing economics of DNA synthesis”. en. In: Nature Biotechnology 27.12 (Dec.
2009). Number: 12 Publisher: Nature Publishing Group, pp. 1091–1094.
Alberto Giubilini and Julian Savulescu. “The Artificial Moral Advisor. The "Ideal Observer" Meets
Artificial Intelligence”. eng. In: Philosophy & Technology 31.2 (2018), pp. 169–188.
Jens Rasmussen. “Risk management in a Dynamic Society: A Modeling Problem”. English. In: Proceedings
of the Conference on Human Interaction with Complex Systems, 1996.
Timnit Gebru et al. “Datasheets for datasets”. en. In: Communications of the ACM 64.12 (Dec. 2021),
pp. 86–92.
Jennifer Robertson. “Human rights vs. robot rights: Forecasts from Japan”. In: Critical Asian Studies
46.4 (2014), pp. 571–598.
F.R. Frola and C.O. Miller. System Safety in Aircraft Acquisition. en. Tech. rep. Jan. 1984.
Nick Beckstead. On the overwhelming importance of shaping the far future. 2013.
Thomas Woodside et al. “Examples of AI Improving AI”. In: (2023). url: https://ai-improving-
ai.safe.ai.
Center for AI Safety. Statement on AI Risk (“Mitigating the risk of extinction from AI should be a
global priority alongside other societal-scale risks such as pandemics and nuclear war.”) 2023. url:
https://www.safe.ai/statement-on-ai-risk.
Charles Townshend. “Religious terror”. In: Terrorism: A Very Short Introduction. Ed. by Charles
Townshend. Oxford University Press, May 2018, p. 0. url: https://doi.org/10.1093/actrade/
9780198809098.003.0006.
John Rawls. Political Liberalism. Columbia University Press, 1993.
Richard Danzig et al. Aum Shinrikyo: Insights into How Terrorists Develop Biological and Chemical
Weapons. Tech. rep. Center for a New American Security, 2012. url: https://www.jstor.org/stable/
resrep06323.
Moin Nadeem, Anna Bethke, and Siva Reddy. “StereoSet: Measuring stereotypical bias in pretrained
language models”. In: Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:
Long Papers). Online: Association for Computational Linguistics, Aug. 2021, pp. 5356–5371.
Evan G. Williams. “The Possibility of an Ongoing Moral Catastrophe”. en. In: Ethical Theory and Moral
Practice 18.5 (Nov. 2015), pp. 971–982.
Jonas Schuett et al. Towards best practices in AGI safety and governance: A survey of expert opinion.
2023. arXiv: 2305.07153.
Billy Perrigo. Bing’s AI Is Threatening Users. That’s No Laughing Matter. en. Feb. 2023.
Nico Grant and Karen Weise. “In A.I. Race, Microsoft and Google Choose Speed Over Caution”. en-US.
In: The New York Times (Apr. 2023).
Paul C. Judge. “Selling Autos by Selling Safety”. en-US. In: The New York Times (Jan. 1990).
Lee Strobel. Reckless Homicide?: Ford’s Pinto Trial. en. And Books, 1980.
John Jumper et al. “Highly accurate protein structure prediction with AlphaFold”. In: Nature 596.7873
(2021), pp. 583–589.
Zachary Wu et al. “Machine learning-assisted directed protein evolution with combinatorial libraries”.
In: Proceedings of the National Academy of Sciences 116.18 (2019), pp. 8852–8858.
Grimshaw v. Ford Motor Co. May 1981.

B.7. LITERATURE
571
David Shepardson and Tracy Rucinski. “U.S. lawmakers question Boeing’s $1 mln rebate clause for
Southwest 737 MAX orders”. en. In: Reuters (Oct. 2019). url: https://www.reuters.com/article/us-
boeing-airplane-southwest-idUSKBN1X92D4.
Theo Leggett. “737 Max crashes: Boeing says not guilty to fraud charge”. en-GB. In: BBC News (Jan.
2023).
Edward Broughton. “The Bhopal disaster and its aftermath: a review”. In: Environmental Health 4.1
(May 2005), p. 6.
Charlotte Curtis. “Machines vs. Workers”. en-US. In: The New York Times (Feb. 1983).
Christian Szegedy et al. “Intriguing properties of neural networks”. In: CoRR (Dec. 2013).
John Uri. 35 Years Ago: Remembering Challenger and Her Crew. und. Text. Jan. 2021.
Xinyun Chen et al. Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning. 2017.
arXiv: 1712.05526.
Alexander Pan et al. “Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and
Ethical Behavior in the Machiavelli Benchmark.” In: ICML (2023).
Center for Security and Emerging Technology. AI Safety – Emerging Technology Observatory Research
Almanac. 2023.
Richard Sutton [@RichardSSutton]. It will be the greatest intellectual achievement of all time. An
achievement of science, of engineering, and of the humanities, whose significance is beyond humanity,
beyond life, beyond good and bad. en. Tweet. Sept. 2022.
Richard Sutton. AI Succession. Video. Sept. 2023.
Jonathan Stray et al. “What are you optimizing for? Aligning Recommender Systems with Human
Values”. In: ArXiv abs/2107.10939 (2021).
Jonathan Stray. “Aligning AI Optimization to Community Well-Being”. In: International Journal of
Community Well-Being (2020).
Greg Brockman, Ilya Sutskever, and OpenAI. Introducing OpenAI. Dec. 2015.
Devin Coldewey. OpenAI shifts from nonprofit to ‘capped-profit’ to attract capital. Mar. 2019.
Kyle Wiggers, Devin Coldewey, and Manish Singh. Anthropic’s $5B, 4-year plan to take on OpenAI.
Apr. 2023.
Dan Lamothe. Air Force Swears: Our Nuke Launch Code Was Never ’00000000’. Jan. 2014.
David Manheim. Building a Culture of Safety for AI: Perspectives and Challenges. 2023.
Dakota Cary and Daniel Cebul. “Destructive Cyber Operations and Machine Learning”. In: Center for
Security and Emerging Technology (2020).
Paul Christiano et al. Deep reinforcement learning from human preferences. Discussed in https :
//www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity. 2017. arXiv:
1706.03741.
Michael G Vann. “Of rats, rice, and race: The great Hanoi rat massacre, an episode in French colonial
history”. In: French Colonial History (2003), pp. 191–203.
U.S. Department of State Office of The Historian. “U.S. Diplomacy and Yellow Journalism, 1895–1898”.
In: ().
“Lyndon Baines Johnson”. In: Oxford Reference (2016).
Joseph Carlsmith. “Is power-seeking AI an existential risk?” In: arXiv preprint arXiv:2206.13353 (2022).
Marilyn Strathern. “‘Improving Ratings’: Audit in the British University System”. In: European Review
5.3 (July 1997), pp. 305–321. issn: 1474-0575, 1062-7987. doi: 10.1002/(SICI)1234-981X(199707)5:
3<305::AID-EURO184>3.0.CO;2-4.
Kevin Roose. “The Making of a YouTube Radical”. In: The New York Times (June 2019). issn: 0362-4331.
(Visited on 09/15/2023).

572
APPENDIX B. UTILITY FUNCTIONS
Charles Goodhart. “Problems of Monetary Management : The U.K. Experience”. In: Papers in monetary
economics 1975 ; 1. Papers in Monetary Economics 1975 ; 1 ; 1. - [Sydney]. - 1975, p. 1-20 1 (1975).
Collin Burns et al. Discovering Latent Knowledge in Language Models Without Supervision. 2022. arXiv:
2212.03827 [cs.CL].
Yohan J. John et al. “Dead Rats, Dopamine, Performance Metrics, and Peacock Tails: Proxy Failure Is
an Inherent Risk in Goal-Oriented Systems”. In: Behavioral and Brain Sciences (June 2023), pp. 1–68.
issn: 0140-525X, 1469-1825. doi: 10.1017/S0140525X23002753. (Visited on 09/15/2023).
Jack Clark and Dario Amodei. Faulty Reward Functions in the Wild. Dec. 2016. (Visited on 09/15/2023).
Ziad Obermeyer et al. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations”.
In: Science 366.6464 (Oct. 2019), pp. 447–453. doi: 10.1126/science.aax2342.
Joar Skalse et al. “Defining and Characterizing Reward Hacking”. In: (2022).
Paul Christiano et al. Deep Reinforcement Learning from Human Preferences. Feb. 2023. doi: 10.48550/
arXiv.1706.03741. arXiv: 1706.03741 [cs, stat]. (Visited on 09/15/2023).
Harry Lambert. “The Great University Con: How the British Degree Lost Its Value”. In: New Statesman
(Aug. 2019). (Visited on 09/15/2023).
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial
Examples. Mar. 2015. eprint: 1412.6572 (cs, stat). (Visited on 09/15/2023).
Anish Athalye et al. Fooling Neural Networks in the Physical World. Oct. 2017. (Visited on 09/15/2023).
Andy Zou et al. Universal and Transferable Adversarial Attacks on Aligned Language Models. July 2023.
doi: 10.48550/arXiv.2307.15043. eprint: 2307.15043 (cs). (Visited on 09/15/2023).
B. H. Raven. “Social influence and power”. In: (1964).
T. Hobbes. Hobbes’s Leviathan. 1651. isbn: 978-5-87635-264-4.
Luke Muehlhauser and Anna Salamon. “Intelligence Explosion: Evidence and Import”. In: Singularity
Hypotheses: A Scientific and Philosophical Assessment. Ed. by Amnon H. Eden and James H. Moor.
Springer, 2012, pp. 15–40.
Chris Stokes. “Why the three laws of robotics do not work”. In: International Journal of Research in
Engineering and Innovation 2 (2 2018).
Alexander Pan et al. Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and
Ethical Behavior in the MACHIAVELLI Benchmark. 2023. arXiv: 2304.03279 [cs.LG].
Stephen Omohundro. “Artificial General Intelligence 2008: Proceedings of the First AGI Conference”. In:
Artificial General Intelligence 2008. 2008.
Bowen Baker et al. Emergent Tool Use From Multi-Agent Autocurricula. Sept. 2019. (Visited on
09/16/2023).
Johannes von Oswald et al. Uncovering Mesa-Optimization Algorithms in Transformers. Sept. 2023.
eprint: 2309.05858 (cs). (Visited on 09/13/2023).
Johannes Von Oswald et al. “Transformers Learn In-Context by Gradient Descent”. In: Proceedings of
the 40th International Conference on Machine Learning. PMLR, July 2023, pp. 35151–35174. (Visited
on 09/16/2023).
Nick Bostrom. Base Camp for Mt. Ethics. Tech. rep. 2022. (Visited on 09/16/2023).
Peter S. Park et al. AI Deception: A Survey of Examples, Risks, and Potential Solutions. Aug. 2023.
doi: 10.48550/arXiv.2308.14752. arXiv: 2308.14752 [cs]. (Visited on 09/16/2023).
Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring How Models Mimic Human
Falsehoods. May 2022. doi: 10 . 48550 / arXiv . 2109 . 07958. eprint: 2109 . 07958 (cs). (Visited on
09/16/2023).
Anton Bakhtin et al. “Human-Level Play in the Game of Diplomacy by Combining Language Models with
Strategic Reasoning”. In: Science 378.6624 (Dec. 2022), pp. 1067–1074. doi: 10.1126/science.ade9097.
(Visited on 09/16/2023).

B.7. LITERATURE
573
Julien Perolat et al. “Mastering the Game of Stratego with Model-Free Multiagent Reinforcement
Learning”. In: Science 378.6623 (Dec. 2022), pp. 990–996. issn: 0036-8075, 1095-9203. doi: 10.1126/
science.add4679. eprint: 2206.15378 (cs). (Visited on 09/16/2023).
David Silver. Exploration and exploitation. Lecture 9. 2014.
Charles W. Kegley and Shannon L. Blanton. Trend and Transformation, 2014 - 2015. 2020, p. 259.
John J. Mearsheimer. Structural realism. 2007, pp. 77–94.
OpenAI Five Defeats Dota 2 World Champions. Apr. 2019. url: https://openai.com/research/openai-
five-defeats-dota-2-world-champions (visited on 09/16/2023).
Joon Sung Park et al. Generative Agents: Interactive Simulacra of Human Behavior. Apr. 2023. (Visited
on 09/16/2023).
Kenneth Waltz. Theory of international politics. Waveland Press, 2010, p. 93.
Evan Montgomery. “Breaking Out of the Security Dilemma: Realism, Reassurance, and the Problem
of Uncertainty”. In: International Security - INT SECURITY 31 (Oct. 2006), pp. 151–185. doi:
10.1162/isec.2006.31.2.151.
P. W. Anderson. “More Is Different”. In: Science 177.4047 (Aug. 1972), pp. 393–396. doi: 10.1126/
science.177.4047.393.
W. Julian Korab-Karpowicz. “Political Realism in International Relations”. In: The Stanford Encyclopedia
of Philosophy. Ed. by Edward N. Zalta. Summer 2018. Metaphysics Research Lab, Stanford University,
2018.
Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. First edition. Oxford: Oxford University
Press, 2014. isbn: 978-0-19-967811-2.
Stuart Russell. “Human-Compatible Artificial Intelligence”. In: Human-like machine intelligence (2021),
pp. 3–23.
Anton Bakhtin et al. “Human-level play in the game of Diplomacy by combining language models with
strategic reasoning”. In: Science (2022).
Richard Dawkins and Nicola Davis. The selfish gene. Macat Library, 2006.
Andrej Karpathy. url: https://cs.stanford.edu/people/karpathy/reinforcejs/.
url: https://commons.wikimedia.org/wiki/File:Neuron_-_annotated.svg.
url: https://commons.wikimedia.org/wiki/File:Rosenblattperceptron.png.
Specific frame from gif in source. url: https://commons.wikimedia.org/wiki/File:2D_Convolution_
Animation.gif.
Febin Sunny, Mahdi Nikdast, and Sudeep Pasricha. SONIC: A Sparse Neural Network Inference
Accelerator with Silicon Photonics for Energy-Efficient Deep Learning. Sept. 2021.
Hula Hula. MNIST Dataset. url: https://twoearth.tistory.com/31 (visited on 09/28/2023).
Alexandre Drouin and Francois Laviolette. Classification vs Regression. 2017. url: https://github.
com/aldro61/microbiome-summer-school-2017/blob/master/figures/figure.classification.vs.
regression.png (visited on 09/28/2023).
Canley. Probability of passing exam versus hours studying. 2022. url: https://en.wikipedia.org/
wiki/File:Exam_pass_logistic_curve.svg.
Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). 2023. arXiv: 1606.08415
[cs.LG].
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. http://www.deeplearningbook.
org. MIT Press, 2016.
Ravish Raj. Ed. by Andrew Ng Lectures. url: https://www.enjoyalgorithms.com/blog/parameter-
learning-and-gradient-descent-in-ml (visited on 09/28/2023).
Y. Lecun et al. “Gradient-based learning applied to document recognition”. In: Proceedings of the IEEE
86.11 (1998), pp. 2278–2324. doi: 10.1109/5.726791.

574
APPENDIX B. UTILITY FUNCTIONS
Sepp Hochreiter and Jürgen Schmidhuber. “Long Short-Term Memory”. In: Neural Computation 9.8
(Nov. 1997), pp. 1735–1780. issn: 0899-7667. doi: 10.1162/neco.1997.9.8.1735. eprint: https:
//direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf. url: https:
//doi.org/10.1162/neco.1997.9.8.1735.
Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting Misclassified and Out-of-Distribution
Examples in Neural Networks. 2018. arXiv: 1610.02136 [cs.NE].
Dan Hendrycks. Anomaly and Out-of-Distribution Detection. url: https : / / docs . google . com /
presentation/d/1WEzSFUbcl1Rp4kQq1K4uONMJHBAUWhCZTzWVHnLcSV8/edit#slide=id.g60c1429d79_
0_0.
Walber. Precision Recall. The layout of the figure has been changed. 2014. url: https://en.wikipedia.
org/wiki/File:Precisionrecall.svg.
Omar Elharrouss et al. “Image Inpainting: A Review”. In: Neural Processing Letters 51.2 (Dec. 2019),
pp. 2007–2028. doi: 10.1007/s11063-019-10163-0. url: https://doi.org/10.1007%2Fs11063-019-
10163-0.
In: (Jan. 2022). url: https://doi.org/10.3390/info13020071.
cmglee and Martin Thoma. 2018. url: https://commons.wikimedia.org/wiki/File:Roc_curve.svg.
Jia Deng et al. “ImageNet: A large-scale hierarchical image database”. In: 2009 IEEE Conference on
Computer Vision and Pattern Recognition. 2009, pp. 248–255. doi: 10.1109/CVPR.2009.5206848.
Nick Bostrom. “Superintelligence: Paths, Dangers, Strategies.” In: Oxford University Press (2014).
Francois Chollet. “On the measure of intelligence.” In: arXiv preprint arXiv:1911.01547 (2019).
B. Jack Copeland. “Artificial Intelligence: A Philosophical Introduction.” In: (1993).
Hubert L. Dreyfus. “What computers still can’t do: A critique of artificial reason.” In: (1992).
Ben Goertzel and Cassio Pennachin. “Artificial General Intelligence.” In: (2007).
Ross Gruetzemacher and Jess Whittlestone. “The transformative potential of artificial intelligence.”
In: Futures 135 (2022). url: https : / / www . sciencedirect . com / science / article / pii /
S0016328721001932?via%3Dihub.
Jeff Hawkins and Sandra Blakeslee. “On Intelligence.” In: (2004).
Andrew Hodges. “Alan Turing: The Enigma.” In: (2014).
Shane Legg and Marcus Hutter. “Universal intelligence: A definition of machine intelligence.” In: Minds
and Machines 17 (2007), pp. 391–444.
Nils J. Nilsson. “The Quest for Artificial Intelligence: A History of Ideas and Achievements.” In: ().
Holden Karnofsky. Some Background on Our Views Regarding Advanced Artificial Intelligence. 2016.
doi: https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-
advanced-artificial-intelligence/.
Ray Kurzweil. The Singularity Is Near: When Humans Transcend Biology. 2005.
Daved L. Poole, Alan Mackworth, and Randy Goebel. “Computational Intelligence: A Logical Approach.”
In: (1998).
Stuart J Russell and Peter Norvig. “Artificial Intelligence: A Modern Approach.” In: (2020).
John R. Searle. “Minds, Brains, and Programs”. In: The Behavioral and Brain Sciences 3 (3 1980),
pp. 417–457.
Alan M. Turing. “Computing Machinery and Intelligence”. In: Mind 59 (236 1950), pp. 433–460.
Alan M. Turing. The Essential Turing: Seminal Writings in Computing, Logic, Philosophy, Artificial
Intelligence, and Artificial Life “Plus” the Secrets of Enigma. Ed. by B. Jack. Oxford University Press,
2013.
Michael Wooldridge. A brief history of artificial intelligence: what it is, where we are, and where we are
going. Flatiron Books, 2021.
Roman V. Yampolskiy. Artificial Superintelligence: A Futuristic Approach. CRC Press, 2015.

B.7. LITERATURE
575
Christopher M. Bishop. “Pattern Recognition and Machine Learning.” In: (2016).
Leo Breiman. “Random Forests”. In: Machine Learning 20 (3 1995), pp. 273–297. url: https://doi.
org/10.1023/a:1010933404324.
Corinna Cortes and Vladimir Vapnik. “Support-Vector Networks.” In: Machine learning 20 (3).
Jerome H. Friedman. “Greedy Function Approximation: A Gradient Boosting Machine.” In: The Annals
of Statistics 29 (5 2001), pp. 1189–232. url: http://www.jstor.org/stable/2699986.
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT Press,
2009.
Kevin P. Murphy. Probabilistic Machine Learning: An Introduction. MIT Press, 2022.
L. G. Valiant. “A Theory of the Learnable.” In: Communications of the ACM 27 (11 1984), pp. 1134–1142.
url: https://doi.org/10.1145/1968.1972.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. “Neural machine translation by jointly learning
to align and translate.” In: (2016). arXiv: 1409.0473 [cs.CL].
Tom Brown et al. “Language Models are Few-Shot Learners”. In: Advances in Neural Information
Processing Systems. Ed. by H. Larochelle et al. Vol. 33. Curran Associates, Inc., 2020, pp. 1877–
1901.
url:
https : / / proceedings . neurips . cc / paper _ files / paper / 2020 / file /
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
2019. arXiv: 1810.04805 [cs.CL].
Ian Goodfellow et al. “Generative Adversarial Nets”. In: Advances in Neural Information Processing
Systems. Ed. by Z. Ghahramani et al. Vol. 27. Curran Associates, Inc., 2014. url: https://proceedings.
neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.
Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). 2016, pp. 770–778. doi: 10.1109/CVPR.2016.90.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. “ImageNet Classification with Deep
Convolutional Neural Networks”. In: Advances in Neural Information Processing Systems. Ed. by
F. Pereira et al. Vol. 25. Curran Associates, Inc., 2012. url: https://proceedings.neurips.cc/paper_
files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.
Y. Lecun et al. “Gradient-based learning applied to document recognition”. In: Proceedings of the IEEE
86.11 (1998), pp. 2278–2324. doi: 10.1109/5.726791.
Y. LeCun, Y. Bengio, and G. Hinton. “Deep Learning”. In: Nature (521 2015), pp. 436–444. url:
https://doi.org/10.1038/nature14539.
Vinod Nair and Geoffrey Hinton. “Rectified Linear Units Improve Restricted Boltzmann Machines Vinod
Nair”. In: vol. 27. June 2010, pp. 807–814.
Alec Radford et al. “Language Models are Unsupervised Multitask Learners”. In: 2019. url: https:
//api.semanticscholar.org/CorpusID:160025533.
F. Rosenblatt. “The perceptron: A probabilistic model for information storage and organization in the
brain.” In: Psychological Review 65.6 (1958), pp. 386–408. issn: 0033-295X. doi: 10.1037/h0042519.
url: http://dx.doi.org/10.1037/h0042519.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. “Learning representations by back-
propagating errors”. In: Nature 323 (1986), pp. 533–536. url: https://api.semanticscholar.org/
CorpusID:205001834.
Ashish Vaswani et al. “Attention is All you Need”. In: Advances in Neural Information Processing
Systems. Ed. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017. url: https://proceedings.
neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
Jordan Hoffmann et al. Training Compute-Optimal Large Language Models. 2022. arXiv: 2203.15556
[cs.CL].
Joel Hestness et al. Deep Learning Scaling is Predictable, Empirically. 2017. arXiv: 1712.00409 [cs.LG].

576
APPENDIX B. UTILITY FUNCTIONS
Jared Kaplan et al. Scaling Laws for Neural Language Models. 2020. arXiv: 2001.08361 [cs.LG].
Rich Sutton. The Bitter Lesson. url: http://www.incompleteideas.net/IncIdeas/BitterLesson.
html (visited on 09/28/2023).
nostalgebraist. chinchilla’s wild implications. 2022. url: https://www.alignmentforum.org/posts/
6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications (visited on 09/28/2023).
Yasaman Bahri et al. Explaining Neural Scaling Laws. 2021. arXiv: 2102.06701 [cs.LG].
MEJ Newman. “Power laws, Pareto distributions and Zipf’s law”. In: Contemporary Physics 46.5
(Sept. 2005), pp. 323–351. doi: 10.1080/00107510500052444. url: https://doi.org/10.1080%
2F00107510500052444.
Sarah Constantin. "Scaling Laws" for AI And Some Implications. 2023. url: https://sarahconstantin.
substack.com/p/scaling-laws-for-ai-and-some-implications (visited on 09/04/2023).
AI Alignment Forum. Chinchilla’s wild implications. 2023. url: AI % 20Alignment % 20Forum .
%20Chinchilla’s%20wild%20implications.%202023..
Meta Fundamental AI Research Diplomacy Team (FAIR)† et al. “Human-level play in the game of
Diplomacy by combining language models with strategic reasoning”. In: Science 378.6624 (2022),
pp. 1067–1074. doi: 10.1126/science.ade9097. eprint: https://www.science.org/doi/pdf/10.1126/
science.ade9097. url: https://www.science.org/doi/abs/10.1126/science.ade9097.
Csaba Szepesvári. “Algorithms for Reinforcement Learning.” In: (2010).
L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement Learning: A Survey. 1996. arXiv:
cs/9605103 [cs.AI].
Jens Kober, J. Bagnell, and Jan Peters. “Reinforcement Learning in Robotics: A Survey”. In:
The International Journal of Robotics Research 32 (Sept. 2013), pp. 1238–1274. doi: 10 . 1177 /
0278364913495721.
Volodymyr Mnih et al. Playing Atari with Deep Reinforcement Learning. 2013. arXiv: 1312.5602
[cs.LG].
David Silver et al. “Mastering the game of Go with deep neural networks and tree search”. In: Nature
529 (Jan. 2016), pp. 484–489. doi: 10.1038/nature16961.
David Silver et al. “Mastering the game of Go without human knowledge”. In: Nature 550 (7676 2017),
pp. 354–359. url: https://doi.org/10.1038/nature24270.
EbatlleP. Reinforcement learning diagram of a Markov decision process based on a figure from
’Reinforcement Learning An Introduction’ second edition by Sutton and Barto. 2020. url: https :
//commons.wikimedia.org/wiki/File:Markov_diagram_v2.svg.
Thomas Simonini. License: https://github.com/huggingface/deep-rl-class/blob/main/LICENSE.md.
2018. url: https://www.google.com/url?q=https://huggingface.co/learn/deep-rl-course/
unit2/two-types-value-based-methods?fw%3Dpt&sa=D&source=docs&ust=1695859187289470&usg=
AOvVaw3SugjZNScFnPQBXoURDM56.
Jason Scott. Montezuma’s Revenge. 2014. url: https://archive.org/details/a8b_Montezumas_
Revenge_1983_Utopia_Software_US_proto_k_file (visited on 09/28/2023).
jumpropeman. 2022. url: https://thegamehoard.com/2022/04/06/50-years-of-video-games-
breakout-arcade/ (visited on 09/28/2023).
Google DeepMind. 2019. url: https://www.youtube.com/watch?v=cUTMhmVh1qs&t=319s&ab_channel=
GoogleDeepMind (visited on 09/28/2023).
Mac Jaderberg. 2019. url: https : / / www . youtube . com / watch ? v = HcZ48JDamyk & ab _ channel =
MaxJaderberg (visited on 09/28/2023).
Honglei Xie. Constrained Contextual Bandits for Personalized Recommendation. Recreation from the first
figure. 2020. url: https://hongleixie.github.io/blog/Constrained-CB/ (visited on 09/28/2023).
Richard S. Sutton and Andrew Barto. Reinforcement Learning : An Introduction. The MIT Press, 2018.

B.7. LITERATURE
577
Jacob Steinhardt. More Is Different for AI. 2022. url: https://bounded-regret.ghost.io/more-is-
different-for-ai/.
Jason Wei et al. Emergent Abilities of Large Language Models. 2022. arXiv: 2206.07682 [cs.CL]. url:
https://arxiv.org/abs/2206.07682.
Thomas McGrath et al. “Acquisition of chess knowledge in AlphaZero”. In: Proceedings of the National
Academy of Sciences 119.47 (Nov. 2022). doi: 10.1073/pnas.2206625119. url: https://doi.org/10.
1073%2Fpnas.2206625119.
Jacob Steinhardt. More Is Different for AI. Jan. 2022. (Visited on 09/16/2023).
GPT-4 System Card. Tech. rep. OpenAI, Mar. 2023, p. 60. (Visited on 09/16/2023).
Sébastien Bubeck et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023.
arXiv: 2303.12712 [cs.CL].
OpenAI (GPT-4 System Card). 2023. url: https://cdn.openai.com/papers/gpt-4-system-card.pdf.
Danijar Hafner. Benchmarking the Spectrum of Agent Capabilities. 2022. arXiv: 2109.06780 [cs.AI].
O. inyals, I. Babuschkin, and W.M. Czarnecki. “Grandmaster level in StarCraft II using multi-agent
reinforcement learning”. In: Nature 575 (2019), pp. 350–354. url: https://doi.org/10.1038/s41586-
019-1724-z.
Sewon Min et al. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? 2022.
arXiv: 2202.12837 [cs.CL].
Greg Brockman et al. OpenAI Five. 2018. url: https://openai.com/research/openai-five.
Evan Hubinger et al. Risks from Learned Optimization in Advanced Machine Learning Systems. 2021.
arXiv: 1906.01820 [cs.AI].
Sheng Zhang and Matthew Turk. Figure 2 - Creative Commons License. url: http://www.scholarpedia.
org/article/Eigenfaces.
Viral V Acharya, Deniz Anginer, and A Joseph Warburton. “The end of market discipline? Investor
expectations of implicit government guarantees”. In: Investor Expectations of Implicit Government
Guarantees (May 1, 2016) (2016).
Trends by Epoch AI. url: epochai.org/trends.
Likelihood of discontinuous progress around the development of AGI. url: https://aiimpacts.org/
likelihood-of-discontinuous-progress-around-the-development-of-agi/.
Ege Erdil and Tamay Besiroglu. Algorithmic progress in computer vision. 2023. arXiv: 2212.05153
[cs.CV].
Tom Davidson. Could Advanced AI Drive Explosive Economic Growth? url: https : / / www .
openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/.
Richard Sutton. The Bitter Lesson. url: http://www.incompleteideas.net/IncIdeas/BitterLesson.
html.
Nicholas Emery-Xu, Andrew Park, and Robert Trager. “Uncertainty, information, and risk in international
technology races.” In: (2022). url: https://drive.google.com/file/d/18j_wnA4HDMA3ofclLcfpgyV-
0INMn1ZW/view.
Stuart Armstrong, Nick Bostrom, and Carl Shulman. Racing to the precipice: a Model of Artificial
Intelligence Development. 2013.
Ben Garfinkel and Allan Dafoe. “How does the offense-defense balance scale?” In: Journal of Strategic
Studies 42.6 (2019), pp. 736–763. doi: 10.1080/01402390.2019.1631810. url: https://doi.org/10.
1080/01402390.2019.1631810.
Dario Amodei and Danny Hernandez. AI and Compute. url: https://openai.com/research/ai-and-
compute.
Lennart Heim. Transformative AI and Compute Reading List. url: https://gradientflow.com/wp-
content/uploads/2060/09/Transformative-AI-and-Compute-Reading-List-2022-12.pdf.

578
APPENDIX B. UTILITY FUNCTIONS
Michael C. Jensen and William H. Meckling. “Theory of the firm: Managerial behavior, agency costs
and ownership structure”. In: Journal of Financial Economics 3.4 (1976), pp. 305–360. issn: 0304-405X.
doi: https://doi.org/10.1016/0304-405X(76)90026-X. url: https://www.sciencedirect.com/
science/article/pii/0304405X7690026X.
Milton Friedman. “The Social Responsibility of Business Is To Increase Its Profits”. In: vol. 32. Jan.
2007, pp. 173–178. isbn: 978-3-540-70817-9. doi: 10.1007/978-3-540-70818-6_14.
Jonas Schuett, Anka Reuel, and Alexis Carlier. How to design an AI ethics board. 2023. arXiv: 2304.07249
[cs.CY].
Peter Cihon, Jonas Schuett, and Seth D. Baum. “Corporate Governance of Artificial Intelligence in the
Public Interest”. In: Information 12.7 (July 2021), p. 275. issn: 2078-2489. doi: 10.3390/info12070275.
url: http://dx.doi.org/10.3390/info12070275.
Jonas Schuett. AGI labs need an internal audit function. 2023. arXiv: 2305.17038 [cs.CY].
Mckay Jensen, Nicholas Emery-Xu, and Robert Trager. Industrial Policy for Advanced AI: Compute
Pricing and the Safety Tax. 2023. arXiv: 2302.11436 [econ.GN].
Robert Trager, Ben Harack, and Anka Reul. “International Governance of Civilian AI: A Jurisdictional
Certification Approach”. In: (2023). url: https://www.iro.umontreal.ca/~bengioy/International-
Governance-of-Civilian-AI.pdf.
Nargiz Shakikhanova. “Who Governs the Globe? edited by Deborah D. Avant , Martha Finnemore
, and Susan K. Sell , eds. New York: Cambridge University Press, 2010. 456 pp. $39.99 (cloth).”
In: Governance 26.2 (2013), pp. 339–341. doi: https://doi.org/10.1111/gove.12029. eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1111/gove.12029. url: https://onlinelibrary.
wiley.com/doi/abs/10.1111/gove.12029.
Lewis Ho et al. International Institutions for Advanced AI. 2023. arXiv: 2307.04699 [cs.CY].
ANDREW J. COE and JANE VAYNMAN. “Why Arms Control Is So Rare”. In: American Political
Science Review 114.2 (2020), pp. 342–355. doi: 10.1017/S000305541900073X.
OECD. “A blueprint for building national compute capacity for artificial intelligence”. In: 350 (2023).
doi: https : / / doi . org / https : / / doi . org / 10 . 1787 / 876367e3 - en. url: https : / / www . oecd -
ilibrary.org/content/paper/876367e3-en.
Francis J. Gavin. “Strategies of Inhibition: U.S. Grand Strategy, the Nuclear Revolution, and
Nonproliferation”. In: International Security 40.1 (July 2015), pp. 9–46. issn: 0162-2889. doi: 10.
1162/ISEC_a_00205. eprint: https://direct.mit.edu/isec/article-pdf/40/1/9/1843553/isec\_a\
_00205.pdf. url: https://doi.org/10.1162/ISEC%5C_a%5C_00205.
Robert Trager et al. “International Governance of Civilian AI: A Jurisdictional Certification Approach”.
In: Risk, Regulation, & Policy eJournal (2023). url: http://dx.doi.org/10.2139/ssrn.4579899.
Matthijs Maas. Advanced AI Governance. Tech. rep. Legal priorities Project, 2023. url: https://docs.
google.com/document/d/1pwwNHvNeJneBA2t2xaP31lVv1lSpa36w8kdryoS5768/edit.
Isaac Asimov. Runaround. 1942.
Dan Hendrycks and Mantas Mazeika. X-Risk Analysis for AI Research. 2022. arXiv: 2206.05862
[cs.CY].
Terence Tao. Nines of safety: a proposed unit of measurement of risk. Oct. 2021. url: https://terrytao.
wordpress.com/2021/10/03/nines-of-safety-a-proposed-unit-of-measurement-of-risk/.
K. Eric Drexler. Reframing Superintelligence: Comprehensive AI Services as General Intelligence. Tech.
rep. Future of Humanity Institute, 2019.
E. Marsden. Designing for safety: Inherent safety, designed in. 2017. url: https://risk-engineering.
org/safe-design/ (visited on 07/31/2017).
E. Marsden. Black swans: The limits of probabilistic modelling. 2017. url: https://risk-engineering.
org/black-swans/ (visited on 07/31/2017).
N.N. Taleb. Antifragile: Things That Gain from Disorder. Incerto. Random House Publishing Group,
2012. isbn: 9780679645276. url: https://books.google.com.au/books?id=5fqbz_qGi0AC.

B.7. LITERATURE
579
Oscar Schwartz. In 2016, Microsoft’s Racist Chatbot Revealed the Dangers of Online Conversation. 2019.
(Visited on 11/25/2019).
Benjamin Franklin. On proection of towns from fire. Feb. 4, 1735.
James Reason. “The contribution of latent human failures to the breakdown of complex systems.” In:
Philosophical transactions of the Royal Society of London. Series B, Biological sciences 327 1241 (1990),
pp. 475–84. url: https://api.semanticscholar.org/CorpusID:1249973.
Duane Kritzinger. Aircraft System Safety. Woodhead Publishing, 2016.
Andrew Critch and Stuart Russell. TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI.
2023. arXiv: 2306.06924 [cs.AI].
William Vesely et al. Fault Tree Handbook with Aerospace Fault Tree Handbook with Aerospace
Applications Applications. 2002. url: http://www.mwftr.com/CS2/Fault%20Tree%20Handbook_
NASA.pdf.
Nancy G. Leveson. Introduction to STAMP: Part 1. MIT Partnership for Systems Approaches to Safety
and Security (PSASS). 2020.
Arden Albee et al. Report on the Loss of the Mars Polar Lander and Deep Space 2 Missions. Tech. rep.
Jet propulsion Laboratory - California Institute of Technology, 2000.
Thomas G. Dietterich. “Steps Toward Robust Artificial Intelligence”. In: AI Magazine 38.3 (2017),
pp. 3–24. doi: https://doi.org/10.1609/aimag.v38i3.2756. eprint: https://onlinelibrary.wiley.
com/doi/pdf/10.1609/aimag.v38i3.2756. url: https://onlinelibrary.wiley.com/doi/abs/10.
1609/aimag.v38i3.2756.
Jens Rasmussen. “Risk management in a dynamic society: a modelling problem”. In: Safety Science 27.2
(1997), pp. 183–213. issn: 0925-7535. doi: https://doi.org/10.1016/S0925-7535(97)00052-0. url:
https://www.sciencedirect.com/science/article/pii/S0925753597000520.
Sidney Dekker. Drift into Failure: From Hunting Broken Components to Understanding Complex Systems.
Jan. 2011, pp. 1–220. isbn: 9781315257396. doi: 10.1201/9781315257396.
Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An Overview of Catastrophic AI Risks. 2023.
arXiv: 2306.12001 [cs.CY].
Krystal Hu. ChatGPT sets record for fastest-growing user base - analyst note. 2023. url: https:
//www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-
note-2023-02-01/ (visited on 02/03/2023).
Scott D. Sagan. The Limits of Safety: Organizations, Accidents, and Nuclear Weapons. Vol. 177. Princeton
University Press, 1993. isbn: 9780691021010. url: http://www.jstor.org/stable/j.ctvzsmf8r (visited
on 09/26/2023).
Toby Ord. The Lindy Effect. 2023. arXiv: 2308.09045 [physics.soc-ph].
Sanat Rai Pakcer. Black swan event | Definition, History, Examples, & Facts. Britannica. (n.d.) 2023.
Nassim Nicholas Taleb. The Black Swan: The Impact of the Highly Improbable. Vol. 2. Random House,
2007.
Mark Boult et al. Horizon Scanning: A Practitioner’s Guide. English. IRM - Institute of Risk Management,
2018.
Richard Danzig. Technology Roulette. 2018.
Donella H. Meadows. “Leverage Points: Places to Intervene in a System”. In: (1999).
Andy Zou et al. Representation Engineering: A Top-Down Approach to AI Transparency. 2023. arXiv:
2310.01405 [cs.LG].
Edward de Haan et al. “Split-Brain: What We Know Now and Why This is Important for Understanding
Consciousness”. In: Neuropsychology Review 30 (June 2020). doi: 10.1007/s11065-020-09439-3.
Jonathan Stray et al. What are you optimizing for? Aligning Recommender Systems with Human Values.
2021. arXiv: 2107.10939 [cs.IR].

580
APPENDIX B. UTILITY FUNCTIONS
Russell Hotten. “Volkswagen: The Scandal Explained”. In: BBC News (Sept. 2015). (Visited on
09/16/2023).
Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. “Certified Defenses for Data Poisoning Attacks”.
In: Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc., 2017. (Visited
on 09/16/2023).
Scott Alexander. Meditations on Moloch. 2014. url: https://slatestarcodex.com/2014/07/30/
meditations-on-moloch/ (visited on 09/29/2023).
Derek Parfit. Reasons and Persons. Oxford, GB: Oxford University Press, 1984.
Robert Axelrod. “More Effective Choice in the Prisoner’s Dilemma”. In: The Journal of Conflict Resolution
24.3 (1980), pp. 379–403. issn: 00220027, 15528766. url: http://www.jstor.org/stable/173638
(visited on 09/28/2023).
Steven Kuhn. Prisoner’s Dillema. 1997. url: https://plato.stanford.edu/archives/win2019/
entries/prisoner-dilemma/ (visited on 09/29/2023).
Stephen Casper et al. Red Teaming Deep Neural Networks with Feature Synthesis Tools. July 2023.
eprint: 2302.10894 (cs). (Visited on 09/16/2023).
Zachary C. Lipton. “The Mythos of Model Interpretability: In Machine Learning, the Concept of
Interpretability is Both Important and Slippery.” In: Queue 16.3 (June 2018), pp. 31–57. issn: 1542-7730.
doi: 10.1145/3236386.3241340. url: https://doi.org/10.1145/3236386.3241340.
Marc W. Patry. “Attractive but Guilty: Deliberation and the Physical Attractiveness Bias”. In:
Psychological Reports 102.3 (June 2008), pp. 727–733. issn: 0033-2941. doi: 10.2466/pr0.102.3.727-733.
(Visited on 09/15/2023).
Miles Turpin et al. Language Models Don’t Always Say What They Think: Unfaithful Explanations in
Chain-of-Thought Prompting. May 2023. doi: 10.48550/arXiv.2305.04388. eprint: 2305.04388 (cs).
(Visited on 09/15/2023).
Jason Wei et al. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”. In: ().
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep Inside Convolutional Networks:
Visualising Image Classification Models and Saliency Maps. Apr. 2014. eprint: 1312.6034 (cs). (Visited
on 09/15/2023).
Jost Tobias Springenberg et al. Striving for Simplicity: The All Convolutional Net. Apr. 2015. arXiv:
1412.6806 [cs]. (Visited on 09/15/2023).
Kevin Wang et al. Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2
small. 2022. arXiv: 2211.00593 [cs.LG].
Chris Olah et al. “Zoom In: An Introduction to Circuits”. In: Distill 5.3 (Mar. 2020), e00024.001. issn:
2476-0757. doi: 10.23915/distill.00024.001. (Visited on 09/15/2023).
Kevin Meng et al. Locating and Editing Factual Associations in GPT. The listed statement was inspired
by, but not included in this paper. Jan. 2023. eprint: 2202.05262 (cs). (Visited on 09/15/2023).
Jerry Tang et al. “Semantic reconstruction of continuous language from non-invasive brain recordings”.
In: bioRxiv (2022). doi: 10.1101/2022.09.29.509744. eprint: https://www.biorxiv.org/content/
early/2022/09/29/2022.09.29.509744.full.pdf. url: https://www.biorxiv.org/content/early/
2022/09/29/2022.09.29.509744.
Nora Belrose et al. LEACE: Perfect linear concept erasure in closed form. 2023. arXiv: 2306.03819
[cs.LG].
Julius Adebayo et al. “Sanity Checks for Saliency Maps”. In: Advances in Neural Information Processing
Systems. Vol. 31. Curran Associates, Inc., 2018. (Visited on 09/15/2023).
M Elish. “Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction (WeRobot 2016)”. In:
SSRN Electronic Journal (Jan. 2016). doi: 10.2139/ssrn.2757236.
Margot E. Kaminski. “The Right to Explanation, Explained”. In: Berkeley Tech. L.J.. Berkeley Technology
Law Journal 34.IR (), p. 189. url: http://lawcat.berkeley.edu/record/1128984.

B.7. LITERATURE
581
Barret Zoph and Quoc V. Le. Neural Architecture Search with Reinforcement Learning. Feb. 2017. arXiv:
1611.01578 [cs]. (Visited on 09/15/2023).
Nev Acar. Eigenfaces: Recovering Humans from Ghosts | by Nev Acar | Towards Data Science. Aug.
2018. (Visited on 09/15/2023).
Ludwig Schubert et al. OpenAI Microscope. Apr. 2020. (Visited on 09/15/2023).
David Bau et al. “Network Dissection: Quantifying Interpretability of Deep Visual Representations”. In:
2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017, pp. 3319–3327.
doi: 10.1109/CVPR.2017.354.
Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. “Feature Visualization”. In: Distill (2017).
https://distill.pub/2017/feature-visualization. doi: 10.23915/distill.00007.
Jeffrey Dastin. Amazon scraps secret AI recruiting tool that showed bias against women. 2018. url:
https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.
Tolga Bolukbasi et al. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
Embeddings. 2016. arXiv: 1607.06520 [cs.CL].
Thomas Krendl Gilbert et al. Choices, Risks, and Reward Reports: Charting Public Policy for
Reinforcement Learning Systems. 2022. arXiv: 2202.05716 [cs.LG].
David Krueger, Tegan Maharaj, and Jan Leike. Hidden Incentives for Auto-Induced Distributional Shift.
2020. arXiv: 2009.09153 [cs.LG].
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating Unwanted Biases with Adversarial
Learning. 2018. arXiv: 1801.07593 [cs.LG].
Inioluwa Deborah Raji et al. Closing the AI Accountability Gap: Defining an End-to-End Framework
for Internal Algorithmic Auditing. 2020. arXiv: 2001.00973 [cs.CY].
David Rozado. The unequal treatment of demographic groups by ChatGPT/OpenAI content moderation
system. 2023. url: https://davidrozado.substack.com/p/openaicms.
Jieyu Zhao et al. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level
Constraints. 2017. arXiv: 1707.09457 [cs.AI].
Nicol Turner Lee, Paul Resnick, and Genie Barton. Algorithmic bias detection and mitigation: Best
practices and policies to reduce consumer harms. 2019. url: https://www.brookings.edu/articles/
algorithmic - bias - detection - and - mitigation - best - practices - and - policies - to - reduce -
consumer-harms/.
Fairness: Identifying Bias | Machine Learning | Google for Developers. url: https://developers.
google.com/machine-learning/crash-course/fairness/identifying-bias.
Aylin Caliskan and Ryan Steed. Managing the risks of inevitably biased visual artificial intelligence
systems. 2022. url: https://www.brookings.edu/articles/managing-the-risks-of-inevitably-
biased-visual-artificial-intelligence-systems/.
Aylin Caliskan. Detecting and mitigating bias in natural language processing. 2021.
Esin Durmus et al. Towards Measuring the Representation of Subjective Global Opinions in Language
Models. 2023. arXiv: 2306.16388 [cs.CL].
Algorithmic Bias: Why Bother? 2020. url: https://cmr.berkeley.edu/2020/11/algorithmic-bias/.
Zoe Larking. AI Bias - What Is It and How to Avoid It? 2022. url: https://levity.ai/blog/ai-
bias-how-to-avoid.
Aylin Caliskan and Molly Lewis. Social biases in word embeddings and their relation to human cognition.
2020. doi: 10.31234/osf.io/d84kg. url: psyarxiv.com/d84kg.
Jack W. Rae et al. Scaling Language Models: Methods, Analysis and Insights from Training Gopher.
2022. arXiv: 2112.11446 [cs.CL].
David Rozado. The unequal treatment of demographic groups by ChatGPT/OpenAI content moderation
system. 2023. url: https://davidrozado.substack.com/p/openaicms.

582
APPENDIX B. UTILITY FUNCTIONS
Moritz Hardt, Eric Price, and Nathan Srebro. Equality of Opportunity in Supervised Learning. 2016.
arXiv: 1610.02413 [cs.LG]. url: https://arxiv.org/abs/1610.02413.
Dan Hendrycks et al. What Would Jiminy Cricket Do? Towards Agents That Behave Morally. 2022.
arXiv: 2110.13136 [cs.LG].
Osman Aka et al. “Measuring Model Biases in the Absence of Ground Truth”. In: Proceedings of the
2021 AAAI/ACM Conference on AI, Ethics, and Society. AIES ’21. Virtual Event, USA: Association
for Computing Machinery, 2021, pp. 327–335. isbn: 9781450384735. doi: 10.1145/3461702.3462557.
url: https://doi.org/10.1145/3461702.3462557.
Fairness: Check Your Understanding | Machine Learning | Google for Developers. url: https://
developers.google.com/machine-learning/crash-course/fairness/check-your-understanding.
Paul Ekman. “Emotions revealed”. In: BMJ 328.Suppl S5 (2004). doi: 10.1136/sbmj.0405184. eprint:
https://www.bmj.com/content. url: https://www.bmj.com/content/328/Suppl_S5/0405184.
Philip Powell and Jennifer Roberts. “Situational determinants of cognitive, affective, and compassionate
empathy in naturalistic digital interactions”. In: Computers in Human Behaviour 68 (Nov. 2016),
pp. 137–148. doi: 10.1016/j.chb.2016.11.024.
Michael Wai and Niko Tiliopoulos. “The affective and cognitive empathic nature of the dark triad of
personality”. In: Personality and Individual Differences 52 (May 2012), pp. 794–799. doi: 10.1016/j.
paid.2012.01.008.
S. Baron-Cohen. Zero Degrees of Empathy: A New Theory of Human Cruelty. Penguin UK, 2011.
Tania Singer and Olga M. Klimecki. “Empathy and compassion”. In: Current Biology 24.18 (2014),
R875–R878. issn: 0960-9822. doi: https://doi.org/10.1016/j.cub.2014.06.054. url: https:
//www.sciencedirect.com/science/article/pii/S0960982214007702.
Radeksz. Graph of the Preston Curve for 2005, cross section data. 2009. url: https://commons.
wikimedia.org/wiki/File:PrestonCurve2005.JPG.
Julia Angwin et al. Machine Bias. 2016. url: https://www.propublica.org/article/machine-bias-
risk-assessments-in-criminal-sentencing.
W. Dieterich, C. Mendoza, and T. Brennan. “COMPAS risk scales: Demonstrating accuracy equity and
predictive parity.” In: 7 (4 2016), pp. 1–36.
“State v. Loomis”. In: Harvard Law Review 130 (5 2017).
Sam Corbett-Davies et al. The Measure and Mismeasure of Fairness. 2018. arXiv: 1808.00023 [cs.CY].
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction
instruments. 2016. arXiv: 1610.07524 [stat.AP].
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent Trade-Offs in the Fair
Determination of Risk Scores. 2016. arXiv: 1609.05807 [cs.LG].
Moritz von Zahn and Stefan Feuerrigel. “The Cost of Fairness in AI: Evidence from E-Commerce”. In:
Business and Information Systems Engineering 64 (3 2009). doi: 10.1007/s12599-021-00716-w.
Sumon Biswas and Hridesh Rajan. “Do the machine learning models on a crowd sourced platform exhibit
bias? an empirical study on model fairness”. In: Proceedings of the 28th ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software Engineering. ACM,
Nov. 2020. doi: 10.1145/3368089.3409704. url: https://doi.org/10.1145%2F3368089.3409704.
Kevin M. Clermont. “Rules, Standards, and Such”. In: Buffalo Law Review (751 2020).
Raphael Poulain, Mirza Farhan Bin Tarek, and Rahmatollah Beheshti. “Improving Fairness in AI Models
on Electronic Health Records: The Case for Federated Learning Methods”. In: 2023. arXiv: 2305.11386
[cs.LG].
Karen L. Calderone. “The influence of gender on the frequency of pain and sedative medication
administered to postoperative patients”. In: Sex Roles 23 (1990), pp. 713–725. url: https://doi.org/
10.1007/bf00289259.
Kryssia Isabel Rodríguez-Castro et al. “Female gender in the setting of liver transplantation”. In: World
J Transplant 4 (4 2014). doi: 10.5500/wjt.v4.i4.229.

B.7. LITERATURE
583
Min Kyung Lee. “Understanding perception of algorithmic decisions: Fairness, trust, and emotion
in response to algorithmic management”. In: Big Data & Society 5 (2018). url: https : / / api .
semanticscholar.org/CorpusID:149040922.
Galen Harrison et al. “An empirical study on the perceived fairness of realistic, imperfect machine
learning models”. In: Jan. 2020, pp. 392–402. doi: 10.1145/3351095.3372831.
John J. Nay. Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with
Humans. 2023. arXiv: 2209.13020 [cs.CY].
Ann M. Lipton. “What We Talk About When We Talk About Shareholder Primacy”. In: 69 Case W.
Rsrv. L. Rev. 69 (4 2019).
Sizing the Prize. 2017.
Amartya Sen. “Economics and Health”. In: The Lancet (1999).
Jac Heckelman. “Economic Freedom and Economic Growth: A Short-Run Causal Investigation”. In:
Journal of Applied Economics III (May 2000), pp. 71–91. doi: 10.1080/15140326.2000.12040546.
Inequality - Bridging the Divide. 2020. url: https://www.un.org/en/un75/inequality-bridging-
divide.
Drew Desilver. For most U.S. workers, real wages have barely budged in decades. 2018. url: https:
//www.pewresearch.org/short- reads/2018/08/07/for- most- us- workers- real- wages- have-
barely-budged-for-decades/.
Nick Hanauer and David M. Rolf. The Top 1% of Americans Have Taken $50 Trillion From the Bottom
90% — And That’s Made the U.S. Less Secure. 2020.
Richard D. Wilkinson and Kate Pickett. The spirit level: Why more equal societies almost always do
better. Bloomsbury Publishing, 2009.
Thomas Piketty. Capital in the Twenty-First Century. Belknap Press, 2014.
Andrew Yang. The War On Normal People: The Truth About America’s Disappearing Jobs and Why
Universal Basic Income Is Our Future. Hachette Books, 2018.
John Rust. Psychometrics and Artificial Intelligence. 2005.
Rebecca Robbins. How a Drug Company Made $114 Billion by Gaming the U.S. Patent System. 2023.
url: https://www.nytimes.com/2023/01/28/business/humira-abbvie-monopoly.html.
Steve Lafleur. Coase, Pigou, and Sriracha Sauce. 2013. url: https://fcpp.org/2013/12/16/coase-
pigou-and-sriracha-sauce/.
Charles Jones and Pete Klenow. “Beyond GDP? Welfare across Countries and Time”. In: American
Economic Review 106 (Sept. 2016), pp. 2426–2457. doi: 10.1257/aer.20110236.
CBS News. CBS News finds children mining cobalt for batteries in the Congo. url: https://www.cbsnews.
com/news/cobalt-children-mining-democratic-republic-congo-cbs-news-investigation/.
Richard Easterlin et al. “China’s Life Satisfaction, 1990-2010”. In: Proceedings of the National Academy of
Sciences of the United States of America 109 (May 2012), pp. 9775–80. doi: 10.1073/pnas.1205672109.
Kate Raworth. Doughnut Economics: Seven Ways to Think Like a 21st Century Economist. Chelsea
Green Publishing, 2017.
Alberta Guibilini. “The Artificial Moral Advisor. The “Ideal Observer” Meets Artificial Intelligence”. In:
Philosophy & Technology (2018). doi: https://doi.org/10.1007/s13347-017-0285-z.
Mantas Mazeika et al. How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios. 2022.
arXiv: 2210.10039 [cs.CV].
John A. Weymark. “Harsanyi’s Social Aggregation Theorem and the Weak Pareto Principle”. In: Social
Choice and Welfare (1993). doi: https://doi.org/10.1007/BF00182506.
Chris Gowans. “Moral Relativism”. In: The Stanford Encyclopedia of Philosophy. Ed. by Edward N. Zalta.
Spring 2021. Metaphysics Research Lab, Stanford University, 2021.
C. Plato. Euthyphro. Kessinger Publishing, 2014.

584
APPENDIX B. UTILITY FUNCTIONS
Jeff Sebo. Op-Ed: What should we do if a chatbot has thoughts and feelings? 2022. url: https :
//www.latimes.com/opinion/story/2022- 06- 16/artificial- intelligence- morals- ethics-
sentience-thinking.
Andrew Fisher. Metaethics: An Introduction. Routledge, 2011.
John Stuart Mill and Jeremy Bentham. Utilitarianism and Other Essays. Penguin Books, 1987.
Katarzyna de Laari-Radek and Peter Singer. Utilitarianism: A Very Short Introduction. Oxford University
Press, 2017.
Jeremy Bentham and Louis Crompton. “Offences Against One’s Self:” in: Journal of Homosexuality 3.4
(1978), pp. 389–406. doi: 10.1300/J082v03n04\_07. eprint: https://doi.org/10.1300/J082v03n04_07.
url: https://doi.org/10.1300/J082v03n04_07.
Peter Singer. “Famine, affluence, and morality”. English (US). In: Applied Ethics. United States: Taylor
and Francis, July 2017, pp. 132–142. isbn: 9781138936928. doi: 10.4324/9781315097176.
Samuel Scheffler. The Rejection of Consequentialism: A Philosophical Investigation of the Considerations
Underlying Rival Moral Conceptions. Oxford University Press, 1994. isbn: 9780198235118. doi: 10.1093/
0198235119.001.0001. url: https://doi.org/10.1093/0198235119.001.0001.
R. Eugene Bales. “Act-Utilitarianism: Account of Right-Making Characteristics or Decision-Making
Procedure?” In: American Philosophical Quarterly 8.3 (1971), pp. 257–265. issn: 00030481. url: http:
//www.jstor.org/stable/20009403.
Robert Nozick. Anarchy State and Utopia. John Wiley & Sons, 1974.
Joshua Greene. Moral Tribes: Emotion, Reason, and the Gap Between Us and Them. New York: Penguin
Press, 2013.
Stephen L. Darwall, ed. Deontology. Malden, MA: Wiley-Blackwell, 2003.
T. Aquinas, T.F.E.D. Province, and C.W. Publishing. The Summa Theologica: Complete Edition.
Catholic Way Publishing, 2014. isbn: 9781783793143. url: https://books.google.com.au/books?id=
Ee0HBAAAQBAJ.
Philippa Foot. Virtues and vices and other essays in moral philosophy. Berkeley: University of California
Press, 1978.
M. Moore. “Moral Puzzles and Legal Perplexities: Essays on the Influence of Larry Alexander”. In:
Cambridge University Press, 2018. Chap. The Rationality of Threshold Deontology. Pp. 371–387. doi:
https://doi.org/10.1017/9781108227025.022.
Immanuel Kant. Groundwork for the Metaphysics of Morals. Ed. by Thomas E. Hill and Arnulf Zweig.
New York: Oxford University Press, 1998.
Immanuel Kant and Allen W. Wood. “On a supposed right to lie from philanthropy (1797)”. In: Practical
Philosophy. Ed. by Mary J.Editor Gregor. The Cambridge Edition of the Works of Immanuel Kant.
Cambridge University Press, 1996, pp. 605–616. doi: 10.1017/CBO9780511813306.014.
Rosalind Hursthouse and Glen Pettigrove. “Virtue Ethics”. In: The Stanford Encyclopedia of Philosophy.
Ed. by Edward N. Zalta and Uri Nodelman. Fall 2023. Metaphysics Research Lab, Stanford University,
2023.
Roger Crisp, ed. Aristotle: Nicomachean Ethics. Cambridge University Press, 2014.
John Rawls. “Applied Ethics: A Multicultural Approach”. In: Routledge, 2017. Chap. A Theory of
Justice.
John C. Harsanyi. “Cardinal Utility in Welfare Economics and in the Theory of Risk-taking”. In: Journal
of Political Economy 61.5 (1953), pp. 434–435. doi: 10.1086/257416. eprint: https://doi.org/10.
1086/257416. url: https://doi.org/10.1086/257416.
Derek Parfit. “Equality and Priority”. In: Ratio 10.3 (1997), pp. 202–221. doi: https://doi.org/10.1111/
1467-9329.00041. eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9329.00041.
url: https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9329.00041.
T. M. Scanlon. What We Owe to Each Other. Harvard University Press, 1998. isbn: 9780674950894.
(Visited on 10/13/2023).

B.7. LITERATURE
585
William MacAskill, Krister Bykvist, and Toby Ord. Moral Uncertainty. Sept. 2020. isbn: 9780198722274.
doi: 10.1093/oso/9780198722274.001.0001.
Harry R. Lloyd. The Property Rights Approach to Moral Uncertainty. Happier Lives Institute’s 2022
Summer Research Fellowship. 2022. url: https : / / www . happierlivesinstitute . org / report /
property-rights/.
Oliver Scott Curry, Daniel Austin Mullins, and Harvey Whitehouse. “Is It Good to Cooperate?: Testing
the Theory of Morality-as-Cooperation in 60 Societies”. In: Current Anthropology 60.1 (2019), pp. 47–69.
doi: 10.1086/701478. eprint: https://doi.org/10.1086/701478. url: https://doi.org/10.1086/
701478.
Dan Hendrycks et al. “Aligning AI With Shared Human Values”. In: CoRR abs/2008.02275 (2020).
arXiv: 2008.02275. url: https://arxiv.org/abs/2008.02275.
Nick Hanauer and David M. Rofl. The Top 1% of Americans Have Taken $50 Trillion From the Bottom
90% - And That’s Made the U.S. Less Secure. 2020. url: https://time.com/5888024/50-trillion-
income-inequality-america/.
Patrick Butlin et al. Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.
2023. arXiv: 2308.08708 [cs.AI].
Carl Shulman and Nick Bostrom. “306C18Sharing the World with Digital Minds”. In: Rethinking Moral
Status. Oxford University Press, Aug. 2021. isbn: 9780192894076. doi: 10.1093/oso/9780192894076.003.
0018. eprint: https://academic.oup.com/book/0/chapter/350760172/chapter-pdf/43437637/oso-
9780192894076-chapter-18.pdf. url: https://doi.org/10.1093/oso/9780192894076.003.0018.
Julia Angwin et al. There’s software used across the country to predict future criminals. And it’s biased
against blacks. 2016. url: https://www.propublica.org/article/machine-bias-risk-assessments-
in-criminal-sentencing.
Anthony W. Flores, Kristin Bechtel, and Christopher Lowenkamp. “False Positives, False Negatives, and
False Analyses: A Rejoinder to "Machine Bias: There’s Software Used Across the Country to Predict
Future Criminals. And It’s Biased Against Blacks."”. In: Federal Probation Journal 80 (2 2016).
Sorelle A. Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. “The (Im)Possibility of Fairness:
Different Value Systems Require Different Mechanisms for Fair Decision Making”. In: Commun. ACM
64.4 (Mar. 2021), pp. 136–143. issn: 0001-0782. doi: 10.1145/3433949. url: https://doi.org/10.
1145/3433949.
Moritz Hardt et al. “Equality of Opportunity in Supervised Learning”. In: Advances in Neural Information
Processing Systems. Ed. by D. Lee et al. Vol. 29. Curran Associates, Inc., 2016. url: https : / /
proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-
Paper.pdf.
Sahil Verma and Julia Rubin. “Fairness Definitions Explained”. In: Proceedings of the International
Workshop on Software Fairness. FairWare ’18. Gothenburg, Sweden: Association for Computing
Machinery, 2018, pp. 1–7. isbn: 9781450357463. doi: 10.1145/3194770.3194776. url: https://
doi.org/10.1145/3194770.3194776.
Sam Corbett-Davies et al. “Algorithmic Decision Making and the Cost of Fairness”. In: Proceedings of
the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD ’17.
Halifax, NS, Canada: Association for Computing Machinery, 2017, pp. 797–806. isbn: 9781450348874.
doi: 10.1145/3097983.3098095. url: https://doi.org/10.1145/3097983.3098095.
Sam Corbett-Davies et al. The Measure and Mismeasure of Fairness. 2023. arXiv: 1808.00023 [cs.CY].
Yunqi Li et al. “Fairness in Recommendation: Foundations, Methods and Applications”. In: ACM
Trans. Intell. Syst. Technol. (July 2023). Just Accepted. issn: 2157-6904. doi: 10.1145/3610302. url:
https://doi.org/10.1145/3610302.
Brianna Richardson and Juan E. Gilbert. A Framework for Fairness: A Systematic Review of Existing
Fair AI Solutions. 2021. arXiv: 2112.05700 [cs.AI].
Isabel Straw and Honghan Wu. “Investigating for bias in healthcare algorithms: a sex-stratified analysis
of supervised machine learning models in liver disease prediction”. In: BMJ Health & Care Informatics
29 (Apr. 2022), p. 100457. doi: 10.1136/bmjhci-2021-100457.

586
APPENDIX B. UTILITY FUNCTIONS
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning: Limitations
and Opportunities. http://www.fairmlbook.org. fairmlbook.org, 2019, pp. 149–274. doi: 10.1561/
2200000072.
Andrew D. Selbst et al. “Fairness and Abstraction in Sociotechnical Systems”. In: FAT* ’19. Atlanta,
GA, USA: Association for Computing Machinery, 2019, pp. 59–68. isbn: 9781450361255. doi: 10.1145/
3287560.3287598. url: https://doi.org/10.1145/3287560.3287598.
Emmanuel Martinez and Lauren Kirchner. The Secret Bias Hidden in Mortgage-Approval Algorithms.
[Accessed 25-Aug-2021]. url: https://themarkup.org/denied/2021/08/25/the- secret- bias-
hidden-in-mortgage-approval-algorithms.
[Fairness toolkits are programmatic toolkits for implementing the more formal approach to fairness,
e.g. IBM’s AI Fairness 360 Toolkit, Microsoft’s Fairlearn, LinkedIn’s Fairness Toolkit (LiFT), Google’s
What-If Tool, Meta’s Fairness Flow, FairML, FairPSL].
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in Machine Learning. 2017. url:
https://fairmlbook.org/pdf/classification.pdf.
Aaron Fraenkel. Fairness and Algorithmic Decision Making. url: https://afraenkel.github.io/
fairness-book/intro.html.
Kate Crawford and Trevor Paglen. Excavating AI. url: https://excavating.ai/.
Min Kyung Lee et al. “Procedural Justice in Algorithmic Fairness: Leveraging Transparency and Outcome
Control for Fair Algorithmic Mediation”. In: Proc. ACM Hum.-Comput. Interact. 3.CSCW (Nov. 2019).
doi: 10.1145/3359284. url: https://doi.org/10.1145/3359284.
Dan Hendrycks. Natural Selection Favors AIs over Humans. 2023. arXiv: 2303.16200 [cs.CY].
J. Maynard Smith. “The theory of games and the evolution of animal conflicts”. In: Journal of Theoretical
Biology 47.1 (1974), pp. 209–221. issn: 0022-5193. doi: https://doi.org/10.1016/0022-5193(74)
90110-6. url: https://www.sciencedirect.com/science/article/pii/0022519374901106.
Noam Nisan et al. Algorithmic Game Theory. Cambridge University Press, 2007. doi: 10.1017/
CBO9780511800481.
Allan Dafoe. “AI Governance: Overview and Theoretical Lenses”. In: The Oxford Handbook of
AI Governance. Oxford University Press, 2022. isbn: 9780197579329. doi: 10 . 1093 / oxfordhb /
9780197579329.013.2. eprint: https://academic.oup.com/book/0/chapter/408516484/chapter-
ag-pdf/50913724/book\_41989\_section\_408516484.ag.pdf. url: https://doi.org/10.1093/
oxfordhb/9780197579329.013.2.
Kai-Fu Lee and Chen Qiufan. AI 2041 Ten Visions for Our Future. Currency, 2021.
John H. Herz. “Idealist Internationalism and the Security Dilemma”. In: World Politics 2.2 (1950),
pp. 157–180. doi: 10.2307/2009187.
Jared Diamond. Collapse: How Societies Choose to Fail or Succeed: revised edition. Penguin Books
(London), 2005.
Andrew Critch. What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs).
2021. url: https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what- multipolar- failure-
looks-like-androbust-agent-agnostic (visited on 09/29/2023).
Scott Alexander. Ascended Economy? 2016. url: https://slatestarcodex.com/2016/05/30/ascended-
economy/.
Robert Axelrod and William D. Hamilton. “The Evolution of Cooperation”. In: Science 211.4489 (1981),
pp. 1390–1396. doi: 10.1126/science.7466396. eprint: https://www.science.org/doi/pdf/10.1126/
science.7466396. url: https://www.science.org/doi/abs/10.1126/science.7466396.
Martin Nowak and Karl Sigmund. “Tit-for-Tat in Heterogeneous Populations”. In: Nature 355 (Jan.
1992), pp. 250–253. doi: 10.1038/355250a0.
Julián García and Matthijs van Veelen. “No Strategy Can Win in the Repeated Prisoner’s Dilemma:
Linking Game Theory and Computer Simulations”. In: Frontiers in Robotics and AI 5 (Aug. 2018),
p. 102. doi: 10.3389/frobt.2018.00102.

B.7. LITERATURE
587
William Press and Freeman Dyson. “Iterated Prisoners Dilemma Contains Strategies That Dominate
Any Evolutionary Opponent”. In: Proceedings of the National Academy of Sciences of the United States
of America 109 (May 2012), pp. 10409–13. doi: 10.1073/pnas.1206569109.
Alexander Stewart and Joshua Plotkin. “Extortion and cooperation in the Prisoner’s Dilemma”. In:
Proceedings of the National Academy of Sciences 109 (June 2012), pp. 10134–10135. doi: 10.1073/pnas.
1208087109.
Christian Hilbe, Martin Nowak, and Karl Sigmund. “The Evolution of Extortion in Iterated Prisoner’s
Dilemma Games”. In: Proceedings of the National Academy of Sciences of the United States of America
110 (Apr. 2013). doi: 10.1073/pnas.1214834110.
Lukas Gloor. Why threats are a plausible contender for the largest source of expected disvalue. 2018. url:
https://docs.google.com/document/d/1hmP19KnmmQverUQzzpsrLJHA9qLle7GNP_48MtR7JxM/edit#
heading=h.1cvrf3v1coh4.
Richard Dawkins. Evolutionarily Stable Strategies ft. Richard Dawkins. 2015. url: https://www.youtube.
com/watch?v=mUxt--mMjwA&ab_channel=Veritasium.
Steven Hamblin and Peter L. Hurd. “When will evolution lead to deceptive signaling in the Sir Philip
Sidney game?” In: Theoretical Population Biology 75.2 (2009), pp. 176–182. issn: 0040-5809. doi:
https://doi.org/10.1016/j.tpb.2009.02.002. url: https://www.sciencedirect.com/science/
article/pii/S0040580909000227.
Harry W. Greene and Roy W. McDiarmid. “Coral Snake Mimicry: Does It Occur?” In: Science 213.4513
(1981), pp. 1207–1212. doi: 10.1126/science.213.4513.1207. eprint: https://www.science.org/doi/
pdf/10.1126/science.213.4513.1207. url: https://www.science.org/doi/abs/10.1126/science.
213.4513.1207.
Robert R. Jackson and R. Stimson Wilcox. “Spider-Eating Spiders: Despite the small size of their brain,
jumping spiders in the genus Portia outwit other spiders with hunting techniques that include trial and
error”. In: American Scientist 86.4 (1998), pp. 350–357. issn: 00030996. url: http://www.jstor.org/
stable/27857059 (visited on 09/28/2023).
Florian Schiestl. “On the success of a swindle: pollination by deception in orchids”. In: Die
Naturwissenschaften 92 (July 2005), pp. 255–64. doi: 10.1007/s00114-005-0636-y.
Mikael Mokkonen and Lindstedt Carita. “The evolutionary ecology of deception”. In: Biological Reviews
91 (June 2015), n/a–n/a. doi: 10.1111/brv.12208.
Richard Dawkins. Universal Darwinism. Evolution from Molecules to Men. Cambridge University Press,
1983.
Susan Blackmore. The Meme Machine. Vol. 1. 1999. isbn: 978-0192862129.
J. H. Yen and A. R. Barr. “New hypothesis of the cause of cytoplasmic incompatibility in Culex pipiens
L”. In: (1971). doi: 10.1038/232657a0.
Edward Morrow and Göran Arnqvist. “Costly traumatic insemination and a female counter-adaptation
in bed bugs”. In: Proceedings. Biological sciences / The Royal Society 270 (Dec. 2003), pp. 2377–81. doi:
10.1098/rspb.2003.2514.
Locke Rowe and Göran Arnqvist. “Sexually antagonistic coevolution in a mating system: combining
experimental and comparative approaches to address evolutionary processes”. In: Evolution 56.4 (Apr.
2002), pp. 754–767. issn: 0014-3820. doi: 10.1111/j.0014-3820.2002.tb01386.x. eprint: https:
//academic.oup.com/evolut/article- pdf/56/4/754/47651033/evolut0754.pdf. url: https:
//doi.org/10.1111/j.0014-3820.2002.tb01386.x.
Michael Moss. While Warning About Fat, U.S. Pushes Cheese Sales. 2010. url: https://www.nytimes.
com/2010/11/07/us/07fat.html (visited on 09/29/2023).
Gabriele Schino and Filippo Aureli. “Grooming reciprocation among female primates: A meta-analysis”.
In: Biology letters 4 (Nov. 2007), pp. 9–11. doi: 10.1098/rsbl.2007.0506.
Christoph Ratzke, Jonas Denk, and Jeff Gore. Ecological suicide in microbes. July 2017. doi: 10.1101/
161398.

588
APPENDIX B. UTILITY FUNCTIONS
Robert Trivers. “The Evolution of Reciprocal Altruism”. In: Quarterly Review of Biology 46 (Mar. 1971),
pp. 35–57. doi: 10.1086/406755.
Redouan Bshary and Alexandra Sara Grutter. “Image scoring and cooperation in a cleaner fish mutualism”.
In: Nature 441 (2006), pp. 975–978. url: https://api.semanticscholar.org/CorpusID:4422277.
Gerald Carter and Gerald Wilkinson. “Food sharing in vampire bats: Reciprocal help predicts donations
more than relatedness or harassment”. In: Proceedings. Biological sciences / The Royal Society 280 (Feb.
2013), p. 20122573. doi: 10.1098/rspb.2012.2573.
Daniel Balliet, Junhui Wu, and Paul Lange. “Indirect Reciprocity, Gossip, and Reputation-Based
Cooperation”. In: Oct. 2020, pp. 265–287. isbn: 9781462543984.
S. A. WEST, A. S. GRIFFIN, and A. GARDNER. “Social semantics: altruism, cooperation, mutualism,
strong reciprocity and group selection”. In: Journal of Evolutionary Biology 20.2 (2007), pp. 415–432.
doi: https://doi.org/10.1111/j.1420-9101.2006.01258.x. eprint: https://onlinelibrary.wiley.
com/doi/pdf/10.1111/j.1420-9101.2006.01258.x. url: https://onlinelibrary.wiley.com/doi/
abs/10.1111/j.1420-9101.2006.01258.x.
Samir Okasha. “Social justice, genomic justice and the veil of ignorance: Harsanyi meets Mendel”. In:
Economics &amp; Philosophy 28.1 (2012), pp. 43–71. doi: 10.1017/S0266267112000119.
Herbert A. Simon. “A Mechanism for Social Selection and Successful Altruism”. In: Science 250.4988
(1990), pp. 1665–1668. doi: 10.1126/science.2270480. eprint: https://www.science.org/doi/pdf/
10.1126/science.2270480. url: https://www.science.org/doi/abs/10.1126/science.2270480.
V. Buterin. What even is an institution? 2022. url: https://vitalik.ca/general/2022/12/30/
institutions.html.
Morgan Kelly. “Inequality And Crime”. In: The Review of Economics and Statistics 82 (Feb. 2000),
pp. 530–539. doi: 10.1162/003465300559028.
Victimization rates for persons age 12 or older, by type of crime and annual family income of victims.
2011. url: https://bjs.ojp.gov/sites/g/files/xyckuh236/files/media/document/cv0814.pdf.
Pablo Fajnzylber, Daniel Lederman, and Norman Loayza. “Inequality and Violent Crime”. In: The
Journal of Law and Economics 45.1 (2002), pp. 1–39. doi: 10.1086/338347. eprint: https://doi.org/
10.1086/338347. url: https://doi.org/10.1086/338347.
Adrienne Horne. The Effect Of Relative Deprivation On Delinquency: An Assessment Of Juveniles.
2009.
P. Cilliers and David Spurrett. “Complexity and post-modernism: Understanding complex systems”.
In: South African Journal of Philosophy 18 (Sept. 2014), pp. 258–274. doi: 10.1080/02580136.1999.
10878187.
Alethea Power et al. Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. 2022.
arXiv: 2201.02177 [cs.LG].
Hang Zhao et al. “Response mechanisms to heat stress in bees”. In: Apidologie 52 (Jan. 2021). doi:
10.1007/s13592-020-00830-w.
Trevor Kletz. An Engineer’s View of Human Error. CRC Press, 2018.
Lucas Davis. “The Effect of Driving Restrictions on Air Quality in Mexico City”. In: Journal of Political
Economy 116 (Feb. 2008), pp. 38–81. doi: 10.1086/529398.
Jemimah Steinfeld. “China’s deadly science lesson: How an ill-conceived campaign against sparrows
contributed to one of the worst famines in history”. In: Index on Censorship 47.3 (2018), pp. 49–49. doi:
10.1177/0306422018800259.
Xin Meng, Nancy Qian, and Pierre Yared. “The Institutional Causes of China’s Great Famine, 1959–1961”.
In: The Review of Economic Studies 82.4 (Apr. 2015), pp. 1568–1611. issn: 0034-6527. doi: 10.1093/
restud/rdv016.
Joseph Gottfried. “History Repeating? Avoiding a Return to the Pre-Antibiotic Age”. In: 2005. url:
https://api.semanticscholar.org/CorpusID:84028897.

B.7. LITERATURE
589
Centers for Disease Control (US). “Antibiotic resistance threats in the United States, 2019”. In: CDC
Stacks (2019). doi: http://dx.doi.org/10.15620/cdc:82532.
“Cycling facts”. In: (2018). url: https://english.kimnet.nl/publications/publications/2018/04/
06/cycling-facts.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. http://www.deeplearningbook.
org. MIT Press, 2016.
S.J. Russell, P. Norvig, and E. Davis. Artificial Intelligence: A Modern Approach. Prentice Hall series in
artificial intelligence. Prentice Hall, 2010. isbn: 9780136042594. url: https://books.google.com.au/
books?id=8jZBksh-bUMC.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA, 2018. url: http://incompleteideas.net/book/the-book-2nd.html.
Yohan John et al. “Dead rats, dopamine, performance metrics, and peacock tails: proxy failure is an
inherent risk in goal-oriented systems”. In: The Behavioral and brain sciences (June 2023), pp. 1–68.
doi: 10.1017/S0140525X23002753.
Joseph Carlsmith. Is Power-Seeking AI an Existential Risk? 2022. arXiv: 2206.13353 [cs.CY]. url:
https://arxiv.org/abs/2206.13353.
Dmitri Gallow. Instrumental Convergence?
url: https : / / drive . google . com / file / d /
1TH2nh5EzwvNgDrqQcMU8oQNj4n9U7x4t/view.
Richard Ngo, Lawrence Chan, and Sören Mindermann. The alignment problem from a deep learning
perspective. 2023. arXiv: 2209.00626 [cs.AI].
Dan Hendrycks et al. Unsolved Problems in ML Safety. 2022. arXiv: 2109.13916 [cs.LG].
Nancy Leveson. Engineering a Safer World: Systems Thinking Applied to Safety. Jan. 2012. isbn:
9780262298247. doi: 10.7551/mitpress/8179.001.0001.
N.N. Taleb. The Black Swan: The Impact of the Highly Improbable. Penguin Books, Limited, 2007. isbn:
9781846140457. url: https://books.google.com.au/books?id=_StMPgAACAAJ.
James Ladyman, James Lambert, and Karoline Wiesner. “What is a complex system?” In: European
Journal for Philosophy of Science 3 (June 2013). doi: 10.1007/s13194-012-0056-8.
D.H. Meadows and D. Wright. Thinking in Systems: A Primer. Chelsea Green Publishing, 2008. isbn:
9781603580557. url: https://books.google.com.au/books?id=CpbLAgAAQBAJ.
J.H. Miller and S.E. Page. Complex Adaptive Systems: An Introduction to Computational Models of
Social Life. Princeton Studies in Complexity. Princeton University Press, 2009. isbn: 9781400835522.
url: https://books.google.com.au/books?id=XQUHZC8wcdMC.
Richard Cook. “How complex systems fail”. In: (Jan. 2002). url: https://how.complexsystems.fail.
W. Wallach and C. Allen. Moral Machines: Teaching Robots Right from Wrong. Oxford University Press,
2008. isbn: 9780199705962. url: https://books.google.com.au/books?id=_r3N82ETng4C.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning: Limitations and
Opportunities. http://www.fairmlbook.org. fairmlbook.org, 2019.
Richard Layard and Jan-Emmanuel Neve. Wellbeing: Science and Policy. May 2023. isbn: 9781009298926.
doi: 10.1017/9781009298957.
Sandel Michael J. What Money Can’t Buy: The Moral Limits of Markets. Farrar, Straus and Giroux,
2012.
Matthew D. Adler. “Measuring Social Welfare: An Introduction”. In: Oxford University Press, Dec. 2019.
isbn: 9780190643027. doi: 10.1093/oso/9780190643027.003.0001. eprint: https://academic.oup.
com/book/0/chapter/299838530/chapter-pdf/40198136/oso-9780190643027-chapter-1.pdf. url:
https://doi.org/10.1093/oso/9780190643027.003.0001.
Jessica Taylor. “Quantilizers: A Safer Alternative to Maximizers for Limited Optimization.” In: AAAI
Workshop: AI, Ethics, and Society. 2016.

590
APPENDIX B. UTILITY FUNCTIONS
Paul Schoemaker. “The Expected Utility Model: Its Variants, Purposes, Evidence and Limitations”. In:
Journal of Economic Literature 20 (Feb. 1982), pp. 529–63.
J. Von Neumann and O. Morgenstern. Theory of games and economic behavior. Princeton University
Press, 1947.
Martin Peterson. “The St. Petersburg Paradox”. In: The Stanford Encyclopedia of Philosophy. Ed. by
Edward N. Zalta and Uri Nodelman. Fall 2023. Metaphysics Research Lab, Stanford University, 2023.
Elliott Thornley. “The Shutdown Problem: Two Theorems, Incomplete Preferences as a Solution”. In:
AI Alignment Awards (2023).
A. Dixit. Slides and Notes. url: https : / / www . princeton . edu / ~dixitak / Teaching /
EconomicsOfUncertainty/Slides&Notes/%20%E2%80%8C.
Joseph Wunderle Jr, María Castro, and Ned Fetcher. “Risk-averse Foraging by Bananaquits on Negative
Energy Budgets”. In: Behavioral Ecology and Sociobiology 21 (Jan. 1987), pp. 249–255. doi: 10.1007/
BF00292506.
Samir Okasha. “Rational choice, risk aversion, and evolution”. In: The Journal of philosophy 104.5
(2007), pp. 217–235.
John L Kelly. “A new interpretation of information rate”. In: the bell system technical journal 35.4
(1956), pp. 917–926.
Robert McDonald and Anna Paulson. “What went wrong at AIG”. In: Kellogg Insight 3 (2015).
M. Allais. “Le Comportement de l’Homme Rationnel devant le Risque: Critique des Postulats et Axiomes
de l’Ecole Americaine”. In: Econometrica 21.4 (1953), pp. 503–546. issn: 00129682, 14680262. url:
http://www.jstor.org/stable/1907921 (visited on 10/16/2023).
David McCarthy. “Probability in Ethics”. In: The Oxford Handbook of Philosophy and Probability. Ed. by
Alan Ha?jek and Christopher Hitchcock. Oxford University Press, 2016, pp. 705–737.
Amos Tversky and Daniel Kahneman. “The framing of decisions and the psychology of choice”. In:
science 211.4481 (1981), pp. 453–458.
Daniel Kahneman and Amos Tversky. “Prospect theory: An analysis of decision under risk”. In: Handbook
of the fundamentals of financial decision making: Part I. World Scientific, 2013, pp. 99–127.
David Silver et al. “Reward is enough”. In: Artificial Intelligence 299 (2021), p. 103535. issn: 0004-3702.
doi: https://doi.org/10.1016/j.artint.2021.103535. url: https://www.sciencedirect.com/
science/article/pii/S0004370221000862.
Adam Bales. “Will AI avoid exploitation? Artificial general intelligence and expected utility theory”. In:
Philosophical Studies (2023). url: https://doi.org/10.1007/s11098-023-02023-4.
D. Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011.
Lara Buchak. Risk and Rationality. Oxford University Press, 2013. isbn: 9780199672165. doi: 10.1093/
acprof:oso/9780199672165.001.0001. url: https://doi.org/10.1093/acprof:oso/9780199672165.
001.0001.
Steven Lukes. Moral Relativism. Picador, 2008.
Katarzyna de Lazari-Radek and Peter Singer. Utilitarianism: A Very Short Introduction. Oxford
University Press, 2017. isbn: 9780198728795. doi: 10.1093/actrade/9780198728795.001.0001. url:
https://doi.org/10.1093/actrade/9780198728795.001.0001.
Shelly Kagan. Normative Ethics. Routledge, London, 2018.
Stephen Darwall. Deontology. Blackwell Publishers, Oxford, 2003.
T. M. Scanlon. What We Owe to Each Other. Belknap Press, 2000.
R. Dawkins. The Blind Watchmaker. Penguin Books Limited, 1986. isbn: 9780141966427. url: https:
//books.google.com.au/books?id=-EDHRX3YYwgC.
E. Warren and A.W. Tyagi. The Two Income Trap: Why Middle-Class Parents are Going Broke. Basic
Books, 2004. isbn: 9780465090907. url: https://books.google.com.au/books?id=TmXhGJ0tg58C.

B.7. LITERATURE
591
D. Patel and Carl Shulman. AI Takeover, Bio Cyber Attacks, Detecting Deception, Humanity’s Far
Future. Audio Podcast Episode from Dwarkesh Podcast. 2023. url: https://www.youtube.com/watch?
v=KUieFuV1fuo&ab_channel=DwarkeshPatel.
OpenAI. OpenAI Charter. 2018. url: https://openai.com/charter.
Kenneth W Condit et al. “The Joint Chiefs of Staff and National Policy, 1947-1949”. In: (No Title)
(1996).
Brian Michael Jenkins. The Will to Fight, Lessons from Ukraine. 2022. url: https://www.rand.org/
pubs/commentary/2022/03/the-will-to-fight-lessons-from-ukraine.html.
Meta Fundamental AI Research Diplomacy Team (FAIR)† et al. “Human-level play in the game of
<i>Diplomacy</i> by combining language models with strategic reasoning”. In: Science 378.6624
(2022), pp. 1067–1074. doi: 10.1126/science.ade9097. eprint: https://www.science.org/doi/pdf/
10.1126/science.ade9097. url: https://www.science.org/doi/abs/10.1126/science.ade9097.
Laurie Chen. “Always there’: the AI chatbot Comforting China’s lonely millions”. In: The Jakarta Post
(2021).
Robert Powell. “War as a commitment problem”. In: International organization 60.1 (2006), pp. 169–203.
John F. Nash. “The Bargaining Problem”. In: Econometrica 18.2 (1950), pp. 155–162. issn: 00129682,
14680262. url: http://www.jstor.org/stable/1907266 (visited on 11/19/2023).
Oliver Scott Curry. “Morality as cooperation: A problem-centred approach”. In: The evolution of morality
(2016), pp. 27–51.
Deborah Charlesworth and John H Willis. “The genetics of inbreeding depression”. In: Nature reviews
genetics 10.11 (2009), pp. 783–796.
Edward Westermarck. The origin and development of the moral ideas. DigiCat, 2022.
Kerris Oates and Margo Wilson. “Nominal kinship cues facilitate altruism”. In: Proceedings of the Royal
Society of London. Series B: Biological Sciences 269.1487 (2002), pp. 105–109.
Martin S Smith, Bradley J Kish, and Charles B Crawford. “Inheritance of wealth as human kin
investment”. In: Ethology and Sociobiology 8.3 (1987), pp. 171–182.
Debra Lieberman, John Tooby, and Leda Cosmides. “Does morality have a biological basis? An empirical
test of the factors governing moral sentiments relating to incest”. In: Proceedings of the Royal Society of
London. Series B: Biological Sciences 270.1517 (2003), pp. 819–826.
Nancy Wilmsen Thornhill. “An evolutionary analysis of rules regulating human inbreeding and marriage”.
In: Behavioral and Brain Sciences 14.2 (1991), pp. 247–261.
Peter Carruthers and Peter K Smith. Theories of theories of mind. Cambridge university press, 1996.
Stuart A West, Ashleigh S Griffin, and Andy Gardner. “Evolutionary explanations for cooperation”. In:
Current biology 17.16 (2007), R661–R672.
Joseph Henrich et al. ““Economic man” in cross-cultural perspective: Behavioral experiments in 15
small-scale societies”. In: Behavioral and brain sciences 28.6 (2005), pp. 795–815.
Steven J Brams and Alan D Taylor. Fair Division: From cake-cutting to dispute resolution. Cambridge
University Press, 1996.
Melville Jean Herskovits. Economic anthropology: A study in comparative economics. Knopf New York,
1952.
Herbert Gintis. “The evolution of private property”. In: Journal of Economic Behavior & Organization
64.1 (2007), pp. 1–16.
Martin A Nowak and Karl Sigmund. “Evolution of indirect reciprocity by image scoring”. In: Nature
393.6685 (1998), pp. 573–577.
Mark Van Vugt, Robert Hogan, and Robert B Kaiser. “Leadership, followership, and evolution: some
lessons from the past.” In: American psychologist 63.3 (2008), p. 182.
Richard C Connor. “The benefits of mutualism: a conceptual framework”. In: Biological Reviews 70.3
(1995), pp. 427–457.

592
APPENDIX B. UTILITY FUNCTIONS
Napoleon A Chagnon. “Kin selection and conflict: An analysis of a Yanomamö ax fight”. In: Evolutionary
biology and human social behavior: An anthropological perspective (1979), pp. 213–238.
Thomas C. Schelling. Arms and Influence. Yale University Press, 1966. isbn: 9780300002218. url:
http://www.jstor.org/stable/j.ctt5vm52s (visited on 10/14/2023).
Martin A. Nowak. “Five Rules for the Evolution of Cooperation”. In: Science 314.5805 (2006), pp. 1560–
1563. doi: 10.1126/science.1133755. eprint: https://www.science.org/doi/pdf/10.1126/science.
1133755. url: https://www.science.org/doi/abs/10.1126/science.1133755.
James D. Fearon. “Rationalist Explanations for War”. In: International Organization 49.3 (1995), pp. 379–
414. issn: 00208183, 15315088. url: http://www.jstor.org/stable/2706903 (visited on 10/14/2023).
Edward O. Wilson. Sociobiology: The New Synthesis, Twenty-Fifth Anniversary Edition. Harvard
University Press, 2000. isbn: 9780674000896. url: http://www.jstor.org/stable/j.ctvjnrttd
(visited on 10/14/2023).
C. Boehm. Moral origins: The evolution of virtue, altruism, and shame. Basic Books, 2012.
R. Axelrod and R.M. Axelrod. The Evolution of Cooperation. Basic books. Basic Books, 1984. isbn:
9780465021215. url: https://books.google.com.au/books?id=NJZBCGbNs98C.
Jaime Sevilla et al. Compute Trends Across Three Eras of Machine Learning. 2022. arXiv: 2202.05924
[cs.LG].
Ege Erdil and Tamay Besiroglu. Explosive growth from AI automation: A review of the arguments.
Papers 2309.11690. arXiv.org, 2023. url: https://ideas.repec.org/p/arx/papers/2309.11690.html.
Saif M. Khan. AI Chips: What They Are and Why They Matter. 2020. url: https://doi.org/10.
51593/20190014.
Yonadav Shavit. What does it take to catch a Chinchilla? Verifying Rules on Large-Scale Neural Network
Training via Compute Monitoring. 2023. arXiv: 2303.11341 [cs.LG].
Markus Anderljung et al. Frontier AI Regulation: Managing Emerging Risks to Public Safety. 2023.
arXiv: 2307.03718 [cs.CY].
Ajeya Cotra. Draft Report on AI Timelines. 2020. url: https://www.alignmentforum.org/posts/
KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines.
Ajeya Cotra. Forecasting TAI with biological anchors. 2020. url: https://docs.google.com/document/
d/1k7qzzn14jgE-Gbf0CON7_Py6tQUp2QNodr_8VAoDGnY/edit#heading=h.r808lkmw6i0l.
Nick Bostrom. What is a Singleton? Linguistic and Philosophical Investigations, 5 (2), 48-54. 2006.
Erik Brynjolfsson. “The turing trap: The promise & peril of human-like artificial intelligence”. In:
Daedalus 151.2 (2022), pp. 272–287. url: https://digitaleconomy.stanford.edu/news/the-turing-
trap-the-promise-peril-of-human-like-artificial-intelligence/.
Daron Acemoglu and Pascual Restrepo. “Robots and jobs: Evidence from US labor markets”. In: Journal
of political economy 128.6 (2020), pp. 2188–2244.
Maximiliano Dvorkin and Hannah Shell. “The growing skill divide in the US labor market”. In: Federal
Reserve Bank of St. Louis: On the Economy Blog 18 (2017).
Yuval Noah Harari. Homo Deus: A brief history of tomorrow. random house, 2016.
Daron Acemoglu, Andrea Manera, and Pascual Restrepo. Does the US tax code favor automation?
Tech. rep. National Bureau of Economic Research, 2020.
Dario Amodei et al. AI and compute. OpenAI. 2018.
Epoch. Key trends and figures in Machine Learning. 2023. url: https://epochai.org/trends.
Zachary Arnold et al. ETO Supply Chain Explorer. Dataset. 2022. url: https://cset.georgetown.
edu/publication/eto-supply-chain-explorer/.
Saif M Khan, Alexander Mann, and Dahlia Peterson. “The semiconductor supply chain: Assessing
national competitiveness”. In: Center for Security and Emerging Technology 8.8 (2021).
Michael C Jensen and William H Meckling. “Theory of the firm: Managerial behavior, agency costs and
ownership structure”. In: Corporate governance. Gower, 2019, pp. 77–132.

