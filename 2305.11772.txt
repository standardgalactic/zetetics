Neural Foundations of Mental Simulation:
Future Prediction of Latent Representations on
Dynamic Scenes
Aran Nayebi1,*, Rishi Rajalingham1,3, Mehrdad Jazayeri1,2, and Guangyu Robert Yang1,2
1McGovern Institute for Brain Research, MIT; Cambridge, MA 02139
2Department of Brain and Cognitive Sciences, MIT; Cambridge, MA 02139
3Reality Labs, Meta; 390 9th Ave, New York, NY 10001
*Correspondence: anayebi@mit.edu
Abstract
Humans and animals have a rich and ﬂexible understanding of the physical world,
which enables them to infer the underlying dynamical trajectories of objects and
events, plausible future states, and use that to plan and anticipate the consequences
of actions. However, the neural mechanisms underlying these computations are
unclear. We combine a goal-driven modeling approach with dense neurophysiolog-
ical data and high-throughput human behavioral readouts to directly impinge on
this question. Speciﬁcally, we construct and evaluate several classes of sensory-
cognitive networks to predict the future state of rich, ethologically-relevant en-
vironments, ranging from self-supervised end-to-end models with pixel-wise or
object-centric objectives, to models that future predict in the latent space of purely
static image-based or dynamic video-based pretrained foundation models. We ﬁnd
strong differentiation across these model classes in their ability to predict neural
and behavioral data both within and across diverse environments. In particular, we
ﬁnd that neural responses are currently best predicted by models trained to predict
the future state of their environment in the latent space of pretrained foundation
models optimized for dynamic scenes in a self-supervised manner. These models
also approach the neurons’ ability to predict the ground truth ball position and
velocity while it is occluded, despite not being explicitly trained to do so. Finally,
we ﬁnd that not all foundation model latents are equal. Notably, models that future
predict in the latent space of video foundation models that are optimized to support
a diverse range of sensorimotor tasks, reasonably match both human behavioral
error patterns and neural dynamics across all environmental scenarios that we
were able to test. Overall, these ﬁndings suggest that the neural mechanisms and
behaviors of primate mental simulation are thus far most consistent with being
optimized to future predict on dynamic, reusable visual representations that are
useful for embodied AI more generally.
1
Introduction
Within the span of a couple seconds, we are able to draw rich inferences and make predictions about
novel scenes [Smith and Vul, 2013, Battaglia et al., 2013]. A dominant cognitive theory has been
that the brain builds mental models of the physical world, using those models to make inferences
about the future state of its environment [Craik, 1943]. In the past decade, this hypothesis has
been supported by comparisons of human behavior to computational models which predict what
will happen next in physical scenarios via forward simulations resembling those of game engines
Preprint. Under review.
arXiv:2305.11772v1  [cs.AI]  19 May 2023

in modern video games [Battaglia et al., 2013, Hamrick et al., 2016, Ullman et al., 2017]. Both
neuroimaging work in humans [Zacks, 2008, Fischer et al., 2016, Pramod et al., 2022] and recent
electrophysiological work in monkeys [Rajalingham et al., 2022a,b] has further provided evidence
for the neurobiological basis of mental simulations in the frontoparietal network (FPN) of primates, a
large-scale network consisting of several interacting brain regions. In this work, we make progress
towards understanding the neural and behavioral mechanisms of mental simulation by constructing
models that perform this behavior in rich, naturalistic environments. Speciﬁcally, we aim to determine
what inductive biases (in the form of a loss function, architecture class, pretraining environment)
enable the brain to generally perform mental simulation across a range of environments and scenarios,
from unstructured, continuous sensory inputs. In particular, we assess not only model generalization
to novel, high-variation scenarios within the same environment, but also structural generalization to
new environments and scenarios altogether.
Predicting the physical dynamics of environments is also critical to progress in embodied AI. One
common paradigm for learning these dynamics has been as a next frame prediction problem via a
pixel-wise loss [Villegas et al., 2019, Wu et al., 2021, Babaeizadeh et al., 2021, Nash et al., 2022].
These losses emphasize prioritizing accurate prediction of every detail of a given scenario’s dynamics.
However, ﬁne-grained prediction of upcoming video frames would require near-perfect knowledge of
the world’s physical state (akin to Laplace’s Demon), which may explain the observation why many
of these models tend to underﬁt in high variation, naturalistic visual environments, with recent efforts
aimed at scaling these methods up primarily by increasing their parameter count [Dasari et al., 2019,
Babaeizadeh et al., 2021]. It is therefore unclear how much these resultant learned representations
are able to successfully capture general physical understanding.
Another recent class of approaches involves the design of visual “foundation models” [Bommasani
et al., 2021], trained on large amounts of webscale images and egocentric videos to develop an
implicit representation of the world, that can then be deployed to downstream robotic manipulation
tasks [Nair et al., 2022, Ma et al., 2023, Majumdar et al., 2023]. Of course, these models are not
directly designed to do explicit physical simulation, but we equip them with a forward dynamics
model that can be rolled out for an arbitrary number of timesteps. We ask whether such dynamically-
equipped foundation models have learned physical knowledge by evaluating their generalization both
to new scenarios and environments, and whether their representations bear any similarity to humans
and non-human primates performing the same tasks?
In particular, we ﬁnd strong constraints on primate mental simulation, especially when examining
generalization within and across diverse environments. Our core result is that a small class of
models best match primate frontal cortex neural dynamics while the animal plays a ball interception
task in which the ball trajectory was partially occluded (“Mental-Pong”), developed previously
by Rajalingham et al. [2022a]. Overall, one model class can match both neural response dynamics
and human behavioral patterns reasonably well – namely, dynamics that are optimized to future
predict in the latent space of VC-1, a video foundation model pretrained on the largest variety of
sensorimotor tasks overall. We therefore currently observe a tight correspondence between the ability
to predict ﬁne-grained neural and behavioral responses for the mental simulation phenomenon, and
developing useful representations for embodied AI more generally.
2
Related Work
Mental simulations have been studied at the level of neural activity only very recently. Prior
human neuroimaging studies [Zacks, 2008, Fischer et al., 2016, Pramod et al., 2022] in the past
decades showed elevated levels of blood-oxygen-level-dependent (BOLD) signal to mental simulation,
although they do not have the required resolution to verify that these dynamics are actually represented
in underlying neural activity. Rajalingham et al. [2022b] was the ﬁrst study to show that neural
dynamics recorded from macaque dorsomedial frontal cortex (DMFC), track the occluded ball by
comparing these dynamics to Recurrent Neural Networks (RNNs) that simulate the occluded ball’s
position in Mental-Pong. However, monkeys can perform these tasks without substantial training,
suggesting that they are already equipped with the necessary neural foundations for mental simulation
in this environment. Therefore, we aim to also build networks that are not explicitly trained on Mental-
Pong itself, but are tasked to generalize to this novel setting as a test of their general understanding of
physical scene dynamics – chieﬂy developed through three factors: their architecture, optimization
objective, and pretraining on a naturalistic environment.
2

Additionally, we constrain our models by evaluating them against high-throughput human behavioral
data (from Bear et al. [2021]) in more naturalistic, 3D environments than Mental-Pong alone, which
goes beyond prior behavioral studies that either rely on a narrow range of physical scenarios [Shepard
and Metzler, 1971, Cooperau and Shepard, 1973], such as block towers with several cubes of different
colors [Groth et al., 2018, Li et al., 2016], or 2D environments that may not generalize to the real
world [Bakhtin et al., 2019]. A key challenge to addressing these questions is a common standard
to evaluating the everyday physical scene understanding and neural predictivity of these models,
especially since they are usually trained on vastly different scenarios and input types. Towards
this end, we require models to operate under similar constraints as the brain, namely (i) to take in
unstructured visual inputs across a range of physical phenomena, (ii) to generate physical predictions
for each scenario (e.g. producing “behavioral outputs”), and (iii) to consist of internal units that can
be compared to biological units (e.g. containing “artiﬁcial neurons”).
Taken together, these three requirements encompass a large class of functionally reasonable hypothe-
ses that we call “sensory-cognitive networks”, and includes the two broad approaches mentioned in
§1. However, they do exclude some approaches – for example, particle-relation graph neural network
dynamics predictors [Battaglia et al., 2016, 2018, Li et al., 2019, Sanchez-Gonzalez et al., 2020]
that take the ground truth simulator state as input (which may not be readily available in real-world
situations, failing to satisfy requirement (i)) or probabilistic programs [Battaglia et al., 2013, Hamrick
et al., 2016, Ullman et al., 2017] (which fail to satisfy requirements (i) and (iii)). Nonetheless, we
believe these latter approaches are a useful guide for building improved models from pixels that
satisfy the three requirements – especially in terms of assessing whether the prediction problem
lies at the level of vision, or the dynamics that interfaces with it. For example, Bear et al. [2021]
demonstrated that graph neural networks with access to the ground truth simulator state best match
human-level physical predictions across a diverse range of scenarios, which indicates that the onus
mainly rests on learning a good visual representation, as these models assume perfect observability
of physical dynamics. This latter observation further motivates our enforcement of requirement (i)
and our approach of evaluating a range of visual foundation models, pretrained on a diverse set of
scenarios, which we expand on below.
3
Methods
To tackle this question, we took a hypothesis-driven approach and built sensory-cognitive networks
that performed mental simulations of their environment in a variety of different ways (schematized
in Figure 1). Speciﬁcally, we tasked models to operate on the Physion dataset [Bear et al., 2021],
a large-scale video dataset that focuses on everyday physical understanding, consisting of eight
different scenarios in a simulated Unity3D-based environment (the ThreeDWorld simulator [Gan
et al., 2021]) with roughly 2,000 examples each. Models are trained on all eight scenarios, which
are: Dominoes, Support, Collide, Contain, Drop, Link, Roll, and Drape; which together cover many
scenarios involving rigid and soft bodies.
We additionally train a subset of models on Kinetics 700 [Carreira et al., 2019], which consists of
over 500,000 training videos from real-world (rather than simulated) scenes from 700 different action
categories, in order to assess whether a different dataset of increased scale is beneﬁcial or not, relative
to Physion.
We consider models from several classes:
1. End-to-end self-supervised future prediction:
(a) Pixel-wise: This class of models consists of three parts: encoder, dynamics, and
decoder; which are altogether trained end-to-end with a pixel-wise loss to predict
the next frame. We consider a current state-of-the-art model in this model family,
FitVid [Babaeizadeh et al., 2021], and we train it on Physion (with and without
temporal augmentations such as RandAugment [Cubuk et al., 2020]); along with an
earlier variant, SVG [Denton and Fergus, 2018], that can be additionally trained at
larger image scales.
(b) Object-centric: This class of models is based on the Contrastive Learning of Structured
World Models framework (“C-SWM” [Kipf et al., 2020]) which contrastively learns
object-centric latent states. It consists of an encoder (whose size we vary: “small”,
“medium”, and “large”) that outputs a ﬁxed number of object slots N = 10, and a graph
3

Observed + Simulated
?
?
?
Time
?
2. 
Macaque Neurophysiology: Mental-Pong
A
P
M
L
DMFC
Model Evaluations
ball    paddle  occluder
Time
Observed epoch
(1240±350 ms)
Occluded epoch
(895±270 ms)
Feedback
1.
Human Behavior: Physion Object Contact Prediction (OCP)
Yes/No?
NO
acc. = 0.89
...
Observed Stimuli
Time
Unobserved Outcome
last frame
true label
cue
stimulus
YES
acc. = 0.96
...
Example Scenarios
Model Pretraining
Physion
Sensory-Cognitive Hypothesis Classes
End-to-End Future Prediction:
Latent Future Prediction:
Dominoes
Support
Link
Drape
2. Dynamics Training Stage
1.Pretraining Stage
Pixel-wise
Ego4D, etc
T+1
T+1
Ground Truth
Prediction
Foundation Model
T
Encoder
Decoder
Object-centric
Encoder
(A)
(B)
Inputs
Figure 1: Model Pretraining and Evaluation Pipeline: We built and evaluate sensory-cognitive
networks for mental simulation. (A) Models are pretrained to predict the future state of naturalistic
environments in a variety of ways. Their dynamics are then compared to high-throughput human
behavioral judgements and dense primate neurophysiological response dynamics. Model pretraining
(green) can be (1) two-stage, or (2) end-to-end. For two-stage, as in the case of latent future prediction
models, there is ﬁrst visual network pretraining, then dynamics network pretraining. (B) Evaluations
are multi-fold. (1) Comparison to human behavior in OCP, (2) Comparison to neural activity in
Mental-Pong. In both cases, the model weights are ﬁxed, so this is a strong test of generalization.
neural network forward dynamics module that operates on this representation. Since
the dynamics module itself does not have temporal dependencies (only spatial), we
enable temporal dependencies by passing frames in the input channel dimension of the
encoder, as a sliding window of temporal dimension of T context frames to predict the
object-centric latent at timestep T + 1. This model family is one instantiation of the
cognitive theory that humans tend to reason about scenes with regards to objects and
their relations (e.g. “Spelke core knowledge” [Baillargeon et al., 1985, Spelke, 1990,
Spelke and Kinzler, 2007]). These Gestalt principles therefore may provide physical
knowledge that are better-positioned to generalize than more ﬁne-grained (pixel-wise)
prediction – a hypothesis that we explicitly evaluate on high-throughput neural and
behavioral data, in what follows.
2. Latent self-supervised future prediction:
These models consist of two parts: an encoder E that produces a latent state space h and a
dynamics module D that predicts the future state purely in the latent space h of E. E is ﬁxed
and pretrained to perform a challenging vision task (“foundation model”), through which it
learns a partial, implicit representation of the physical world. However, since E is not trained
to do explicit physical simulation, we train additional dynamics D on Physion, that can later
be “rolled out” an arbitrary number of steps. More concretely, given T context frames, E
produces the latent sequence h1:T , from which D is trained to predict hT +1. D is relatively
simple, being either an LSTM [Hochreiter and Schmidhuber, 1997] or a continuous-time
RNN (CTRNN) [Miller and Fumarola, 2012]; or “No Dynamics” as a control, which always
outputs the last context frame latent of E.
We consider a variety of foundation encoders, divided into two primary classes based on
their type of large-scale pretraining dataset:
4

(a) Image Foundation Models (Static scenes):
• Standard Convolutional Neural Networks (CNNs) VGG16 [Simonyan and Zisser-
man, 2014] and ResNet-50 [He et al., 2016] trained on ImageNet with a supervised
categorization objective.
• Vision Transformer (ViT) [Dosovitskiy et al., 2021] based architectures such as
DeiT [Touvron et al., 2021] and DINO [Caron et al., 2021] trained on ImageNet
with a self-supervised objective.
• DINOv2 [Oquab et al., 2023], which is a very recent ViT trained on a larger curated
dataset called LVD-142M (142 million images).
• CLIP [Radford et al., 2021], which is a ViT trained on 400 million image-text pairs
curated from the Internet (250 times larger than ImageNet).
(b) Video Foundation Models (Dynamic scenes):
• R3M [Nair et al., 2022], which is a ResNet-50 architecture trained with a temporally-
contrastive video-language alignment objective on 5 million frames from a subset
of the recent large-scale Ego4D human video dataset [Grauman et al., 2022].
• VIP [Ma et al., 2023], which is a ResNet-50 architecture trained with a goal-
conditioned value function objective on 5 million frames from a subset of Ego4D.
• VC-1 [Majumdar et al., 2023], which is a very recent ViT trained on 7 different
egocentric video sources (over 5.6 million frames, including Ego4D) relevant to
sensorimotor skills, using a self-supervised masked autoencoding (MAE) [He et al.,
2022] objective.
In total, these networks encompass both qualitatively distinct hypotheses (pixel-wise vs. object-
centric vs. latent future prediction), alongside several variations within each hypothesis class. This
combination of diverse networks allows us to differentiate hypothesis classes across a range of
functionally reasonable instantiations of each hypothesis.
4
Comparison to Human Physical Judgements
4.1
OCP Task Evaluation
In the object contact prediction (OCP) task [Bear et al., 2021], each evaluation scenario involves
a red “agent” object and a yellow “patient” object (which did not appear during model training),
and both humans and models are tasked to predict the probability that they will come into contact.
This prediction requires understanding of the relevant physical phenomenon in the given scenario,
corresponding to a higher-order readout of the underlying scene dynamics.
4.2
End-to-end pixel-wise future predictors best explain human behavior in the same
environment
The model comparison results are summarized in Figure 2. In Figure 2A, we examine held-out
accuracy of models across all eight scenarios in the OCP task, where chance performance is 50%.
We can see that humans are quite reliable on this task, attaining 74.04% average accuracy across
scenarios (grey horizontal line). Furthermore, the best models that approach human accuracy are
models that are trained end-to-end with pixel-wise losses (FitVid and SVG). Despite their differences
in architecture, both FitVid and SVG are comparable to one another in their OCP test set accuracy
(61.12%-63.31%), attaining non-signiﬁcant differences in the distribution of their accuracies across
scenarios (Paired t-test, minimum Bonferroni corrected p-value of 0.444 across pairwise comparisons).
For a ﬁxed architecture, training with increased image size helps improve accuracy somewhat on
the OCP prediction task (SVG 128 × 128, rightmost periwinkle bar, vs. SVG 64 × 64, leftmost
one), along with temporal augmentations for a high capacity model (FitVid, rightmost dark green
bar vs. leftmost one), but these differences are overall non-signiﬁcant (Paired t-test p = 0.461 and
p = 0.196 within the SVG and FitVid architectures, respectively). However, not all end-to-end
models match human accuracy well. In particular, we see that the object-centric C-SWM class
of models matches human accuracies least well compared to other model classes, despite varying
the encoder size. The class of latent future prediction models overall better matches the explicitly
object-centric C-SWM models, but there does not appear to be strong differentiation across models,
and also not much differentiation between having a dynamics module and using the encoder latents
5

(A)
(B)
Human Accuracy
Human-to-Human Consistency
(C)
R≈0.8587, p << 0.001
End-to-End
Latent Future Prediction
64x64
Pixel-wise
C-SWM
128x128
64x64
64x64 + RandAugment
Image Foundation Models
Video Foundation Models
SVG FitVid
Small
 Large
Medium
No Dynamics
LSTM Dynamics
CTRNN Dynamics
VGG16 ResNet-50 DeiT
DINO DINOv2
Object
-centric
No Dynamics
LSTM Dynamics
CTRNN Dynamics
CLIP
VIP
VC-1
R3M
CTRNN Dynamics
LSTM Dynamics
End-to-End
Latent Future Prediction
C-SWM
SVG FitVid
VGG16 ResNet-50 DeiT
DINO DINOv2 CLIP
VIP
VC-1
R3M
64x64
Pixel-wise
128x128
64x64
64x64 + RandAugment
Image Foundation Models
Video Foundation Models
Small
 Large
Medium
No Dynamics
LSTM Dynamics
CTRNN Dynamics
Object
-centric
No Dynamics
LSTM Dynamics
CTRNN Dynamics
CTRNN Dynamics
LSTM Dynamics
End-to-End
Image Foundation Models
Video Foundation Models
Heldout OCP Accuracy
Correlation to Average Human Response
(Pearson’s R)
Heldout OCP Accuracy
Correlation to Average Human Response
(Pearson’s R)
Figure 2: Model Comparisons to Human Physical Judgements: (A) Each model is compared to
binary (“yes/no”) accuracy on the OCP task (chance is 50%). Grey horizontal bar represents the
human accuracy on this task. Mean and s.e.m. across all eight Physion scenarios, weighted by number
of stimuli in each scenario. Hatched bars represent models trained on the Kinetics 700 dataset, rather
than on Physion. (B) Each model is compared to human subject judgement probabilities for the OCP
task, via Pearson’s correlation from the Logistic Regression classiﬁer trained on each of the model
dynamics. Grey is the correlation of human judgement probabilities to each other. Mean and s.e.m.
across all eight Physion scenarios, weighted by number of stimuli in each scenario. (C) A model’s
match to human judgement probabilities is strongly correlated with its OCP task accuracy.
directly (rightmost two bars vs. leftmost bar in each group). This suggests that at least for the OCP
task, either Physion is not sufﬁciently high-variation enough to train the dynamics module or most of
the predictivity comes from the physical scene understanding encapsulated in the foundation model
encoder latents. The latter appears to be the case, since training the dynamics module on a larger-scale
dataset such as Kinetics 700 for a subset of the models (VIP+LSTM/CTRNN, VC-1+LSTM/CTRNN,
and R3M+LSTM/CTRNN; hatched bars) does not improve OCP accuracy over the base encoder
latents either, relative to training them on Physion (Paired t-test, minimum Bonferroni corrected
p-value of 0.066 across architectures).
Additionally, we can look at ﬁner-grained error patterns of the probabilities reported by humans
compared to those of models (rather than accuracies alone), summarized in Figure 2B. Here we see
that despite the metric being more detailed, humans are quite consistent with each other, suggesting
that this type of behavioral metric is quite reliable (grey horizontal line). In fact, all models appear to
be further from the human-to-human consistency than the OCP accuracy. However, overall, similar
trends appear to hold across models as with the OCP accuracy measure – where the end-to-end
pixel-wise models (FitVid and SVG) match these consistency scores the best across models, and the
object-centric C-SWM models match them the least well. From an AI perspective, it is actually quite
relevant to work towards matching human error patterns, as it is highly correlated with the primarily
performance based measure of OCP accuracy, as seen in Figure 2C (R ≈0.8587, p ≪0.001).
5
Comparison to Dense Neurophysiological Data
To gain more insight into model generalization, we compared the above models to neural dynamics
recorded from macaque dorsomedial frontal cortex (DMFC), which was shown by Rajalingham
6

et al. [2022b] to simulate the ball’s position in Mental-Pong while behind an occluder, until it was
intercepted by the paddle.
5.1
Inter-animal consistency and neural behavioral decoders
For each of the 79 conditions (different randomized start position of the ball), we present the frames
in the visible epoch (the time up until the ball reaches the occluder) as context frames to the models,
and unroll the model dynamics during the occluded epoch of the condition. We then build detailed,
physical mappings (schematized in Figure 3A) from the committed model layer latent dynamics to
match every single neuron in the population, and the ground truth ball state while occluded, when
mental simulation takes place. We enforce the requirement, established in prior work [Nayebi et al.,
2021, Cao and Yamins, 2021], that models are at least as similar to a given animal’s neural responses
as two conspeciﬁcs’ neural responses are to each other. When we perform these mappings, we see
that both the inter-animal neural predictivity consistency and ground truth ball state decoding from
DMFC is quite high (grey horizontal lines in Figures 3B and C, respectively), indicating that these
are very reliable neural and behavioral measures of the mental simulation phenomenon.
(A)
(B)
(C)
(D)
ball    paddle  occluder
Time
Observed epoch
(1240±350 ms)
Occluded epoch
(895±270 ms)
Feedback
DMFC
DMFC
Monkey M
Monkey P
Model
LM
P
LModel
P

LModel
A


LP
M

L M
Ball

L P
Ball

LModel
Ball

End-to-End
Image Foundation Models
Video Foundation Models
DMFC Predictivity
C-SWM
SVG FitVid
VGG16 ResNet-50 DeiT DINO DINOv2 CLIP
VIP
VC-1
R3M
End-to-End
Latent Future Prediction
Pixel-wise
Image Foundation Models
Video Foundation Models
Object
-centric
Small
 Large
Medium
No Dynamics
LSTM Dynamics
CTRNN Dynamics
No Dynamics
LSTM Dynamics
CTRNN Dynamics
CTRNN Dynamics
LSTM Dynamics
64x64
128x128
64x64
64x64+ RandAugment
R≈0.683, p << 0.001
Ball Position + Velocity Predictivity
(Pearson’s R)
Ball Position + Velocity Predictivity
(Pearson’s R)
Neural Predictivity
(Pearson’s R)
End-to-End
Latent Future Prediction
Inter-animal Consistency
C-SWM
SVG FitVid
VGG16ResNet-50DeiT DINODINOv2 CLIP
VIP
VC-1
R3M
Pixel-wise
Image Foundation Models
Video Foundation Models
64x64
128x128
64x64
64x64+ RandAugment
Small
 Large
Medium
Object
-centric
No Dynamics
LSTM Dynamics
CTRNN Dynamics
No Dynamics
LSTM Dynamics
CTRNN Dynamics
CTRNN Dynamics
LSTM Dynamics
Oracles
Ball Position
Ball Position + Velocity
Ball Velocity
Neural Predictivity
(Pearson’s R)
Encoder
Dynamics
Figure 3: Model Comparisons to Neural Response Dynamics:
(A) We measure how similar
model dynamics (red) are to each primate’s DMFC neural dynamics up to at least how (linearly)
similar primates are to each other, via the linear transform LN (black arrows). We also assess how
well models can decode ball state (position and velocity) up to how well it can be decoded from
DMFC via the linear transform LB (dark blue arrows). (B) DMFC test set neural predictivity of each
model, averaged across ﬁve train-test splits. Median and s.e.m. across 1, 889 recorded units. For each
of the latent future prediction models, there are three bars: the ﬁrst corresponding to ﬁnal context
encoder latent itself (no dynamics); the last two bars with LSTM or CTRNN dynamics, respectively.
For the VIP, VC-1, and R3M based models, we also train the LSTM and CTRNN dynamics on
Kinetics 700, presented in the last two additional hatched bars. The grey horizontal bar represents how
well each primate’s DMFC can explain the other’s via the linear transform LN schematized in (A),
setting a consistency ceiling on model predictivity. Position, velocity, and position + velocity oracles
are the ground truth ball state values for those variables while it is occluded (and not visible to the
primate during game play). (C) Ball position and velocity predictivity of each model, averaged across
ﬁve train-test splits, from the best DMFC ﬁtting model layer used in (B) via the linear transform LB
schematized in (A). The grey horizontal bar represents the ball position and velocity predictivity from
DMFC units. Median and s.e.m. across four quantities: the ground truth (x, y) position and velocity
of the ball while it was occluded (in degrees of visual angle). (D) Mental-Pong ball position and
velocity predictivity (y-axis) vs. neural predictivity (x-axis). Dotted golden line represents best linear
ﬁt to the data.
7

5.2
Neural response predictivity strongly separates models
When we map model units to DMFC units across 79 conditions (different randomized ball start
position), we see in Figure 3B that across all architectures, only dynamically equipped models from
the video foundation model class class explains DMFC dynamics best (46.34-48.83% explained
variance), speciﬁcally in the latent space of the VC-1 and R3M encoders. However, the VC-1 and
R3M encoder’s latents alone are insufﬁcient to explain the data (19.03% and 24.79% explained
variance, respectively). Training end-to-end on Physion with either a pixel-wise loss (SVG and
FitVid; 14.44%-25.35% explained variance) or an object-centric loss (C-SWM with varying encoder
sizes; 13.69%-23.05% explained variance) is not sufﬁcient either, indicating a strong constraint on
the neural mechanisms of mental simulation being performed on a suitable latent space. However, it
is not the case that any latent space works, since all dynamics trained on encoder latents trained on
static images, regardless of their imageset scale, explained at most 29.67% of the variance. This is
suggestive that for a relatively small but time-varying stimulus such as the Pong ball, models that are
trained on static images may not be equipped to handle the temporal coherence of a single object
moving through time, as prior studies have mainly examined generalization of ImageNet-optimized
CNNs to novel yet static, single-object scenes [Hong et al., 2016].
Moreover, even for video optimized foundation models, we see differentiation at the level of loss
function. In particular, goal-conditioned self-supervised learning on videos, as instantiated by VIP
(which differs from R3M primarily in the choice of objective function, as it shares the same ResNet-50
backbone and is also trained on Ego4D), fails to match DMFC neural dynamics well (7.37%-18.12%
explained variance). We also see parsimony at the level of the architecture and training dataset of
the dynamics module, as the VC-1/R3M+CTRNN either attains comparable or improved neural
predictivity over the more sophisticated VC-1/R3M+LSTM (middle vs. rightmost bars for each
encoder type), and even when training on a larger dataset like Kinetics 700 (Figure S1) – suggesting
that simple dynamics can be sufﬁcient at explaining neural response dynamics given the appropriate
visual encoder. Both the VC-1/R3M+LSTM and VC-1/R3M+CTRNN models approach the neural
predictivity of the ground truth ball position oracle model (50.74% explained variance, leftmost
golden bar in Figure 3B), with the joint position and velocity oracle performing the best (60.65%
explained variance, rightmost golden bar). This suggests that a substantial amount of the neural
response variability is devoted to simulating the ball’s state while it is occluded, as opposed to other
static features of the environment.
In Figure 3C, we see that the most neurally predictive dynamically-equipped VC-1/R3M-based
models generalize to the Mental-Pong task, approaching DMFC’s ability to track the ground truth
position and velocity of the ball while it is occluded, despite not being explicitly trained in this
environment. In particular, there is a linear relationship (R ≈0.683, p ≪0.001) between the model’s
ability to generalize to the Mental-Pong environment and its ability to predict DMFC neural dynamics
(Figure 3D), indicating that predicting the underlying neural dynamics is in fact behaviorally relevant
to effectively simulating the ball’s dynamics.
6
Dynamically Equipped Sensorimotor Foundation Models Can Match Both
Human Behavioral and Neural Response Patterns Across Environments
Both the OCP human behavioral error pattern metric and the DMFC Mental-Pong neural predictivity
metric are linearly correlated to their corresponding behavior of OCP accuracy and ball position
prediction (as seen in Figures 2C and 3D), which suggests that these more ﬁne-grained metrics are
relevant to the underlying behavioral goal that they represent. But how do they relate to each other,
and is there a model class that can match both reasonably well?
As we can see in Figure 4A, the two behavioral goals of Mental-Pong ball state predictivity and
Physion OCP accuracy do not appear to be very related across models (R ≈−0.243, p ≈0.129).
This suggests that accuracy within environment, despite being to novel scenarios, does not imply
that the same model will generalize to completely new environmental dynamics. In particular, it is
important to consider generalization to new environments, since the end-to-end pixel models such as
SVG and FitVid subtly overﬁt to this scenario (attaining the highest OCP accuracy), but they fail to
generalize as well to the completely out-of-distribution Mental-Pong setting (rightmost periwinkle
and dark green points in Figure 4A). For example, for FitVid, this failure is visualizable, as it predicts
the Mental-Pong ball to be static, regardless of enlarging the ball during evaluation (Figure S2).
8

(A)
(B)
VC-1+CTRNN
VC-1+LSTM
Better Models
End-to-End
Image Foundation Models
Video Foundation Models
Ball Position + Velocity Predictivity
(Pearson’s R)
Heldout OCP Accuracy
Neural Predictivity
(Pearson’s R)
Correlation to Average Human Response
(Pearson’s R)
Figure 4: Sensorimotor Foundation Model Can Match Both Metrics: (A) Ball state (position
and velocity) predictivity in Mental-Pong (y-axis) vs. OCP accuracy in Physion (x-axis). Across
models, these two metrics appear to be largely independent. (B) The dynamically-equipped VC-1
models (VC-1+LSTM/CTRNN trained on either Physion or Kinetics 700) can reasonably match
neural response dynamics in Mental-Pong and match human error patterns in the OCP task, relative
to other models. Dotted grey line is the unity line to indicate that better models will occupy the top
right of this scatter plot.
Delving into the more challenging, ﬁne-grained measures of DMFC Mental-Pong neural predictivity
and correlation to human error patterns, we observe in particular that the dynamically-equipped
VC-1-based models (VC-1+CTRNN and VC-1+LSTM in cyan) both reasonably match human error
patterns in Physion and Mental-Pong DMFC neural dynamics (Figure 4B).
Just as VC-1 on its own does not universally dominate on every speciﬁc sensorimotor task, but instead
outperforms the best prior existing visual foundation models on average across the 17 CortexBench
tasks [Majumdar et al., 2023], our ﬁnding that future prediction in the latent space of this model
class reasonably matches both human behavioral patterns and neural responses is therefore consistent
with this observation. Taken together, our results suggest that future prediction in the latent space of
video foundation models for embodied AI is a promising paradigm for developing models of physical
dynamics that are both a better match to neural and behavioral recordings, and can structurally
generalize across diverse environments and scenarios within.
7
Discussion
Overall, we ﬁnd that structural generalization to novel environments and matching dense neurophysi-
ological data to be a strong constraint on models of physical simulation. Going forward, we believe
that we are at a crucial turning point whereby foundation models that engage with the visually rich,
dynamic scenes that humans and animals naturally interface with, will be jointly critical for progress
in embodied AI and neuroscience, addressing the recent call to action to build AI that can be as
grounded in the physical world as animals are [Zador et al., 2023]. On our existing benchmarks,
there are a few ways that we envision our current models can be improved. First, we believe that
training video foundation models with other self-supervised methods [Balestriero et al., 2023] that
better leverage temporal relationships on naturalistic, sensorimotor tasks would yield better latent rep-
resentations. This is based on the observation that dynamically-equipped R3M models best matched
DMFC neural dynamics compared to the MAE objective of VC-1 and goal-conditioned objective of
VIP. However, likely R3M’s match to human behavioral patterns in the OCP task could be improved
by pretraining its temporally-contrastive loss on a larger video dataset similar to CortexBench, rather
than Ego4D alone. Furthermore, the dynamics architecture could beneﬁt from better leveraging a
more “factorized” representation of temporally-active state variables (as suggested by the high neural
predictivity of the joint, ground truth position + velocity oracle in Figure 3B). One method could be
by including multiple timescales of hierarchy, with some recent neurobiological evidence of this type
of temporal hierarchy already existing in frontal cortex [Sarafyazd and Jazayeri, 2019].
In addition to building improved models, we also want to benchmark them against neural recordings
from primates in more complex environments that involve multi-object scenarios, both procedurally
generated at scale in 2D and 3D virtual settings [Watters et al., 2021, Gan et al., 2021], and also
involving real world objects.
9

8
Acknowledgments
A.N., M.J., and G.R.Y. acknowledge the generous support of the K. Lisa Yang Integrative Computa-
tional Neuroscience (ICoN) Center at MIT. M.J. is also supported by NIH (NIMH-MH122025), the
Simons Foundation, the McKnight Foundation, and the McGovern Institute. R.R. is supported by
the Helen Hay Whitney Foundation. A.N. thanks Rahul Venkatesh and Chengxu Zhuang for helpful
discussions.
Broader Impact
Almost everything humans and animals do revolves around an intuitive understanding of the physical
dynamics of the world we are embedded in, enabling us to make long-range plans and perform
actions that ensure our survival. Furthermore, engineering at least this same level of intuitive physical
understanding in silico will be critical for progress in robotics, autonomous vehicles, and any other
embodied applications that involve safely taking actions in the real world. Our work not only provides
a strong measurement of the degree of alignment between our current best engineered systems to those
of humans and animals, but through the differentiation across choices of the architecture, objective
function, and pretraining dataset, also provides scientiﬁc insight into the evolutionary constraints
underlying the neural mechanisms of mental simulation. In particular, we quantitatively observe that
future prediction in a latent space optimized for diverse, dynamic scenes is our current best theory
that explains neural dynamics during mental simulation behavior, compared to popular pixel-wise
and object-centric alternatives. This set of observations suggests that making progress in embodied
AI will also correspondingly yield an improved understanding of mental simulation in human and
animal brains.
References
M. Babaeizadeh, M. T. Saffar, S. Nair, S. Levine, C. Finn, and D. Erhan. Fitvid: Overﬁtting in pixel-level video
prediction. arXiv preprint arXiv:2106.13195, 2021.
R. Baillargeon, E. S. Spelke, and S. Wasserman. Object permanence in ﬁve-month-old infants. Cognition, 20(3):
191–208, 1985.
A. Bakhtin, L. van der Maaten, J. Johnson, L. Gustafson, and R. Girshick. Phyre: A new benchmark for physical
reasoning. Advances in Neural Information Processing Systems, 32, 2019.
R. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bordes, A. Bardes, G. Mialon,
Y. Tian, et al. A cookbook of self-supervised learning. arXiv preprint arXiv:2304.12210, 2023.
P. Battaglia, R. Pascanu, M. Lai, D. Jimenez Rezende, et al. Interaction networks for learning about objects,
relations and physics. Advances in neural information processing systems, 29, 2016.
P. W. Battaglia, J. B. Hamrick, and J. B. Tenenbaum. Simulation as an engine of physical scene understanding.
Proceedings of the National Academy of Sciences, 110(45):18327–18332, 2013.
P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti,
D. Raposo, A. Santoro, R. Faulkner, et al. Relational inductive biases, deep learning, and graph networks.
arXiv preprint arXiv:1806.01261, 2018.
D. Bear, E. Wang, D. Mrowca, F. J. Binder, H.-Y. Tung, R. Pramod, C. Holdaway, S. Tao, K. A. Smith, F.-Y.
Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. In Thirty-ﬁfth
Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut,
E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. S. Chatterji, A. S. Chen, K. A. Creel,
J. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh,
L. Fei-Fei, C. Finn, T. Gale, L. E. Gillespie, K. Goel, N. D. Goodman, S. Grossman, N. Guha, T. Hashimoto,
P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. F. Icard, S. Jain, D. Jurafsky, P. Kalluri,
S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. S. Krass, R. Krishna, R. Kuditipudi, A. Kumar,
F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. P.
Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C.
Niebles, H. Nilforoshan, J. F. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance,
10

C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. H. Roohani, C. Ruiz, J. Ryan, C. R’e, D. Sadigh,
S. Sagawa, K. Santhanam, A. Shih, K. P. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tramèr, R. E.
Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. A. Zaharia, M. Zhang, T. Zhang,
X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. On the opportunities and risks of foundation models.
ArXiv, 2021. URL https://crfm.stanford.edu/assets/report.pdf.
R. Cao and D. Yamins. Explanatory models in neuroscience: Part 1–taking mechanistic abstraction seriously.
arXiv preprint arXiv:2104.01490, 2021.
M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in
self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision
(ICCV), 2021.
J. Carreira, E. Noland, C. Hillier, and A. Zisserman. A short note on the kinetics-700 human action dataset.
arXiv preprint arXiv:1907.06987, 2019.
L. A. Cooperau and R. N. Shepard. The time required to prepare for a rotated stimulus. Memory & Cognition, 1
(3):246–250, 1973.
K. J. W. Craik. The nature of explanation, volume 445. CUP Archive, 1943.
E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation with a
reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
workshops, pages 702–703, 2020.
S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn. Robonet:
Large-scale multi-robot learning. Conference on Robot Learning (CoRL), arXiv preprint arXiv:1910.11215,
2019.
E. Denton and R. Fergus. Stochastic video generation with a learned prior. In International conference on
machine learning, pages 1174–1183. PMLR, 2018.
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,
G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale.
International Conference on Learning Representations (ICLR), arXiv preprint arXiv:2010.11929, 2021.
J. Fischer, J. G. Mikhael, J. B. Tenenbaum, and N. Kanwisher. Functional neuroanatomy of intuitive physical
inference. Proceedings of the national academy of sciences, 113(34):E5072–E5081, 2016.
C. Gan, J. Schwartz, S. Alter, D. Mrowca, M. Schrimpf, J. Traer, J. De Freitas, J. Kubilius, A. Bhandwaldar,
N. Haber, et al. Threedworld: A platform for interactive multi-modal physical simulation. Advanced in
Neural Information Processing Systems (NeurIPS) 2021, arXiv preprint arXiv:2007.04954, 2021.
P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate,
large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu,
et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 18995–19012, 2022.
O. Groth, F. B. Fuchs, I. Posner, and A. Vedaldi. Shapestacks: Learning vision-based physical intuition for
generalised object stacking. In Proceedings of the European Conference on Computer Vision (ECCV), pages
702–717, 2018.
J. B. Hamrick, P. W. Battaglia, T. L. Grifﬁths, and J. B. Tenenbaum. Inferring mass in complex scenes by mental
simulation. Cognition, 157:61–76, 2016.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.
K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision learners. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000–16009,
2022.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
H. Hong, D. L. Yamins, N. J. Majaj, and J. J. DiCarlo. Explicit information for category-orthogonal object
properties increases along the ventral stream. Nature neuroscience, 19(4):613–622, 2016.
11

D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. International Conference on Learning
Representations (ICLR), arXiv preprint arXiv:1412.6980, 2015.
T. Kipf, E. Van der Pol, and M. Welling. Contrastive learning of structured world models. International
Conference on Learning Representations (ICLR), arXiv preprint arXiv:1911.12247, 2020.
W. Li, S. Azimi, A. Leonardis, and M. Fritz. To fall or not to fall: A visual approach to physical stability
prediction. arXiv preprint arXiv:1604.00066, 2016.
Y. Li, J. Wu, R. Tedrake, J. B. Tenenbaum, and A. Torralba. Learning particle dynamics for manipulating rigid
bodies, deformable objects, and ﬂuids. International Conference on Learning Representations (ICLR), arXiv
preprint arXiv:1810.01566, 2019.
Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards universal visual reward
and representation via value-implicit pre-training. International Conference on Learning Representations
(ICLR), arXiv preprint arXiv:2210.00030, 2023.
A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik,
et al. Where are we in the search for an artiﬁcial visual cortex for embodied intelligence? arXiv preprint
arXiv:2303.18240, 2023.
K. D. Miller and F. Fumarola. Mathematical equivalence of two common forms of ﬁring rate models of neural
networks. Neural computation, 24(1):25–31, 2012.
S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual representation for robot
manipulation. 6th Annual Conference on Robot Learning (CoRL), arXiv:2203.12601, 2022.
C. Nash, J. Carreira, J. Walker, I. Barr, A. Jaegle, M. Malinowski, and P. Battaglia. Transframer: Arbitrary frame
prediction with generative models. arXiv preprint arXiv:2203.09494, 2022.
A. Nayebi, A. Attinger, M. Campbell, K. Hardcastle, I. Low, C. S. Mallory, G. Mel, B. Sorscher, A. H. Williams,
S. Ganguli, et al. Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks.
Advances in Neural Information Processing Systems, 34:12167–12179, 2021.
M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-
Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193,
2023.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information
processing systems, 32, 2019.
R. Pramod, M. A. Cohen, J. B. Tenenbaum, and N. Kanwisher. Invariant representation of physical stability in
the human brain. Elife, 11:e71736, 2022.
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,
et al. Learning transferable visual models from natural language supervision. In International conference on
machine learning, pages 8748–8763. PMLR, 2021.
R. Rajalingham, A. Piccato, and M. Jazayeri. Recurrent neural networks with explicit representation of dynamic
latent variables can mimic behavioral patterns in a physical inference task. Nature Communications, 13(1):
5865, 2022a.
R. Rajalingham, H. Sohn, and M. Jazayeri. Dynamic tracking of objects in the macaque dorsomedial frontal
cortex. bioRxiv, pages 2022–06, 2022b.
A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. Battaglia. Learning to simulate complex
physics with graph networks. In International conference on machine learning, pages 8459–8468. PMLR,
2020.
M. Sarafyazd and M. Jazayeri. Hierarchical reasoning by neural circuits in the frontal cortex. Science, 364
(6441):eaav8911, 2019.
R. N. Shepard and J. Metzler. Mental rotation of three-dimensional objects. Science, 171(3972):701–703, 1971.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.
K. A. Smith and E. Vul. Sources of uncertainty in intuitive physics. Topics in cognitive science, 5(1):185–199,
2013.
12

E. S. Spelke. Principles of object perception. Cognitive science, 14(1):29–56, 1990.
E. S. Spelke and K. D. Kinzler. Core knowledge. Developmental science, 10(1):89–96, 2007.
H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jegou. Training data-efﬁcient image
transformers & distillation through attention. In International Conference on Machine Learning, volume 139,
pages 10347–10357, July 2021.
T. D. Ullman, E. Spelke, P. Battaglia, and J. B. Tenenbaum. Mind games: Game engines as an architecture for
intuitive physics. Trends in cognitive sciences, 21(9):649–665, 2017.
R. Villegas, A. Pathak, H. Kannan, D. Erhan, Q. V. Le, and H. Lee. High ﬁdelity video prediction with large
stochastic recurrent neural networks. Advances in Neural Information Processing Systems, 32, 2019.
N. Watters, J. Tenenbaum, and M. Jazayeri. Modular object-oriented games: a task framework for reinforcement
learning, psychology, and neuroscience. arXiv preprint arXiv:2102.12616, 2021.
B. Wu, S. Nair, R. Martin-Martin, L. Fei-Fei, and C. Finn. Greedy hierarchical variational autoencoders for
large-scale video prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 2318–2328, 2021.
J. M. Zacks. Neuroimaging studies of mental rotation: a meta-analysis and review. Journal of cognitive
neuroscience, 20(1):1–19, 2008.
A. Zador, S. Escola, B. Richards, B. Ölveczky, Y. Bengio, K. Boahen, M. Botvinick, D. Chklovskii, A. Church-
land, C. Clopath, et al. Catalyzing next-generation artiﬁcial intelligence through neuroai. Nature Communica-
tions, 14(1):1597, 2023.
13

Limitations
While our study identiﬁes clear separations between model hypothesis classes, our best models still have not
reached the consistency ceiling of the neural and behavioral benchmarks we have compared against. While we
believe our dynamically-equipped foundation model paradigm to be a generally promising way forward towards
models with strong internal simulations, we identify in the Discussion (§7), several ways that their encoder and
dynamics components can be improved, which we plan to explore in future work. Overall, we believe it will be
important for foundation models to learn more factorized representations of object state dynamics, which we
provide quantitative, neurophysiological evidence for through the testing of a variety of oracle models. Finally,
as we saw, dense neurophysiological data was a strong constraint on the models. We therefore believe that
neural recordings from animals performing mental simulations in richer, multi-object environments will further
constrain models, allowing us to better pinpoint the neural mechanisms of this behavior.
A
Model Training
All models are supplied with T = 7 initial context frames, and are trained with the Adam optimizer [Kingma
and Ba, 2015] using PyTorch 2.0 [Paszke et al., 2019]. For all models except FitVid and SVG 128 × 128 (SVG
trained on 128 × 128 pixel images), a single NVIDIA A100 GPU was sufﬁcient to train it. FitVid required 8
A100 GPUs, and SVG 128 × 128 required 2 A100 GPUs. Model code and weights will be made available here:
[LINK REDACTED FOR ANONYMITY].
A.1
Physion Dataset
All models were simultaneously trained across all eight scenarios of the Physion Dynamics Training Set,
constituting around 16,000 total training scenarios (2,000 examples per scenario) [Bear et al., 2021], with a
temporal subsample factor of 6 frames for a total sequence length of 25 frames that is randomly sampled.
Pre-existing end-to-end models were trained with their previously prescribed hyperparameters, and we always
ensured that their training loss converged on the Physion dataset. FitVid Babaeizadeh et al. [2021] was trained
on its standard 64 × 64 pixel input (both with and without RandAugment [Cubuk et al., 2020]), with its
recommended batch size of 128 with a learning rate of 1e-3 and gradient clipping of magnitude 100, for 3125
epochs. When applying RandAugment across frames, it was crucial to ﬁx the random seed of the cropping
operations, to ensure that the same random transformation was applied across all frames in a single sequence.
SVG [Denton and Fergus, 2018] was trained on either its usual 64 × 64 or 128 × 128 pixel inputs, with a
batch size of 100 with a learning rate of 2e-3 for 300 epochs, as recommended in the original paper. Each
C-SWM [Kipf et al., 2020] model was trained on 224 × 224 pixel inputs, with N = 10 object slots and its
recommended batch size of 32 with a learning rate of 5e-4 for 100 epochs.
Given T context frames, all of the two-stage dynamics models (LSTM and CTRNN) are trained on top of a
frozen pretrained foundation model, to predict the encoder latent at the next timestep T + 1 with a mean squared
error loss function, using a batch size of 32 with a learning rate of 1e-4 (except for VGG16 and DeiT, which used
a learning rate of 1e-2) for 100 epochs. All models were trained with 224 × 224 pixel inputs, using the same
image augmentations used in the original evaluation of the corresponding foundation model. “No Dynamics”
corresponds to ﬁxing and propagating the last context latent of the foundation model through time, and therefore
does not require any additional training.
A.2
Kinetics 700 Dataset
We additionally trained the dynamics of the two-stage VIP+LSTM, VIP+CTRNN, VC-1+LSTM, VC-1+CTRNN,
R3M+LSTM, and R3M+CTRNN models on Kinetics 700 [Carreira et al., 2019]. We ﬁrst wrote out each frame
of the mp4 videos as a jpeg with the smallest side set to 480 pixels via inter-area interpolation. We then trained
the models on videos of length at least 8 or more frames (since we need at least T = 7 context frames and
one additional frame to predict), that is randomly sampled during training. This constraint resulted in 536,640
training videos, and 33,966 validation videos. We trained models for 100 epochs each on a single A100 GPU
with a batch size of 256 with a linearly rescaled learning rate of 8e-4 [Goyal et al., 2017].
B
Comparison to Human Physical Judgements
Given T = 7 initial context frames, each model that we consider above is stimulus-computable and can be
decomposed into an encoder E that yields a state representation per frame and a dynamics component D
predicts unseen states given the prior state observations from the encoder. We ﬁrst freeze model parameters
and present all models with the eight Physion Readout Training Set scenarios (∼1, 000 stimuli per scenario)
for 25 timesteps with a subsample factor of 6 frames (same as during Dynamics Training in Section A.1).
14

We combine the observed context and simulated model dynamics, which corresponds to the recommended
all observed+simulated protocol in Physion, as it is agnostic to any particular scenario and best tests general
physical understanding insofar as it can be assessed by Physion [Bear et al., 2021]. Speciﬁcally, we ﬂatten these
outputs and train a logistic regression classiﬁer (cross-validated via a stratiﬁed 5-fold procedure) for 20,000 total
training iterations to ensure convergence. To test the models after training the readout, we then ﬁx the readout
and present the same stimuli that the 100 human participants saw. For each stimulus, we compute the proportion
of “hit” responses by humans, and correspondingly we will extract the hit probability generated by the logistic
regression readout from the models. The Correlation to Average Human Response is the Pearson’s correlation
between the model probability-hit vector and the human proportion-hit vector, across stimuli per scenario. The
Heldout OCP Accuracy of humans and models is the average accuracy, across stimuli per scenario.
To give the ﬁnal values of the two quantities, we then compute the weighted mean and s.e.m. of the above per
scenario quantities xi across scenarios, weighted by the number of stimuli wi per scenario1, computed as such:
weighted_mean =
Ps
i=1 wixi
Ps
i=1 wi .
variance =
Ps
i=1 wi(xi −weighted_mean)2
Ps
i=1 wi
,
effective_sample_size =
 Ps
i=1 wi
2
Ps
i=1 w2
i
,
weighted_sem =
s
variance
effective_sample_size,
where s = 8 is the number of scenarios.
C
Comparison to Neurophysiological Recordings from Macaque
Dorsomedial Frontal Cortex (DMFC)
To perform the model to brain (and brain to brain) comparisons, we ﬁrst produce an (NcondNframes) × Nunits
matrix representation for both models and primate DMFC, following a similar procedure used by Rajalingham
et al. [2022b]. For each of the Ncond = 79 conditions, we present the models with T = 7 uniformly sampled
context frames from the Mental-Pong stimulus up until the last frame that the ball is visible. We then use the
above determined temporal spacing from the context frames to determine the number of roll-out steps for the
model up until the total number of frames for the current video (89 ≤Nframes ≤217 frames across conditions).
Note that these values are therefore different for each condition, but always the same across all models. Neural
responses from dorsomedial frontal cortex (DMFC) are originally in 50ms bins, which we also interpolate to
match the number of frames for the current video. Monkey P had 1, 552 units recorded with 64-channel linear
probes (Plexon V-probes) and monkey M had 337 units recorded with high-density 384-channel silicon probes
(Neuropixels), resulting in 1, 889 recorded units in total.
Once model and neural responses were temporally upsampled to match the number of frames of each video, we
then compared their dynamics in the timepoints when the Pong ball was occluded. This was done by training a
single cross-validated Ridge regressor LN shared across timepoints and conditions that maximized the neural
predictivity of responses between monkey P and monkey M. The Ridge regressor was always reﬁt to each
source but using the same cross-validated hyperparameters found between animals, and was trained on 50%
of the conditions (and their corresponding occluded epoch timepoints), and tested on the remaining 50% of
conditions, across ﬁve train-test splits (cross-validation was done separately per train-test split via a grouped
ﬁve-fold iterator). All neural predictivities are reported on heldout conditions and their timepoints. For each
animal A ∈{P, M} and a given train-test split, heldout neural predictivity per unit in A is measured on the test
set as:
NP(S, A) :=
Corr(LN
S7−→A(S), A)
s
g
Corr

LN
S1
1/27−→A1
1/2

S1
1/2

, LN
S2
1/27−→A2
1/2

S2
1/2

· g
Corr

A1
1/2, A2
1/2
,
(1)
where the source S ∈{Model, P, M}; A is the trial-averaged neural response of animal A; A1
1/2 and A2
1/2 are
the trial-averaged responses to the ﬁrst and second random split-half of responses in A, respectively; S1
1/2 and
S2
1/2 are the trial-averaged responses to the ﬁrst and second random split-half of responses in S, respectively
(when S is a model, then S = S1
1/2 = S2
1/2); Corr is Pearson’s correlation; and g
Corr is Spearman-Brown
1Speciﬁcally, there were 150 stimuli in Dominoes, 149 in Support, 150 in Collide, 150 in Contain, 150 in
Drop, 150 in Link, 94 in Roll, and 149 in Drape.
15

correction applied to Pearson’s correlation since the numerator uses the full set of trials and the denominator
uses half the available trials. The rationale of using a reliability-adjusted correlation is to account for variance
that arises from noise in neural responses that no model can be expected to predict, as it is not replicable by
experiment condition (for a detailed derivation, refer to [Nayebi et al., 2021, Appendix §B.1]). This quantity is
then averaged across the ﬁve train-test splits, and aggregated across units in both monkeys P and M, and we
report the median and s.e.m. across all 1, 889 units in both monkeys. Note that if S is a perfect replica of A,
then this quantity will be 1.0, regardless of the ﬁnite amount of data collected.
When building neural behavioral decoders to the ball’s (x, y) position and velocity, we concatenated monkey
P and M’s neural responses during the occluded epoch, and regressed it against the interpolated ground truth
ball position and velocity (measured in degrees of visual angle). This ﬁve-fold cross-validated Ridge regressor
LB was optimized to maximize the median of these four values using the same measure in equation (1) with A
replaced by the ball state instead. The behavioral predictivity of each model is therefore the median and s.e.m.
of these four values as measured by equation (1), with the regressor LN replaced by LB.
D
Supplementary Figures
VC-1+CTRNN
VC-1+LSTM
VIP+CTRNN
VIP+LSTM
R3M+LSTM
R3M+CTRNN
Physion
Kinetics 700
Figure S1: Training On Kinetics 700 Does Not Improve Neural Predictivity Over Training On
Physion: Neural predictivity of VIP, VC-1, and R3M equipped with CTRNN or LSTM dynamics,
which are trained on Physion (x-axis) vs. Kinetics 700 (y-axis). Dotted black line represents unity
line.
16

Predicted Frame 95
Predicted Frame 120
FitVid + RandAugment
FitVid + RandAugment
FitVid 
(no augmentations)
(A)
(B)
(C)
Figure S2: FitVid Fails To Track Mental-Pong Ball: Example predictions of FitVid across frames,
when evaluated on a given Mental-Pong video, after being (A) trained on Physion with RandAugment,
(B) trained without augmentations, and (C) trained with RandAugment but with the Pong ball enlarged
to be a square during evaluation.
17

