Multi-Party Chat:
Conversational Agents in Group Settings with Humans and Models
Jimmy Wei∗
Cornell University
Kurt Shuster
Meta AI
Arthur Szlam†
Meta AI
Jason Weston
Meta AI
Jack Urbanek
Meta AI
Mojtaba Komeili
Meta AI
Abstract
Current dialogue research primarily studies
pairwise (two-party) conversations, and does
not address the everyday setting where more
than two speakers converse together. In this
work, we both collect and evaluate multi-party
conversations to study this more general case.
We use the LIGHT environment1 to construct
grounded conversations, where each partici-
pant has an assigned character to role-play. We
thus evaluate the ability of language models to
act as one or more characters in such conversa-
tions. Models require two skills that pairwise-
trained models appear to lack: (1) being able
to decide when to talk; (2) producing coher-
ent utterances grounded on multiple characters.
We compare models trained on our new dataset
to existing pairwise-trained dialogue models,
as well as large language models with few-
shot prompting. We ﬁnd that our new dataset,
MultiLIGHT, which we will publicly release,
can help bring signiﬁcant improvements in the
group setting.
1
Introduction
Conversational AI in general and chatbots in par-
ticular have made remarkable breakthroughs in the
last few years (Shuster et al., 2022b; Thoppilan
et al., 2022; OpenAI, 2022). Despite having human-
like (and sometimes super-human) responses, the
presupposed setting is constrained to a turn-based
(round-robin) conversation. Further, almost all set-
tings consider a single AI model and a single hu-
man user as the only participants in the conver-
sation. Although a reasonable assumption for a
rudimentary AI assistant, this restriction limits the
social abilities of AI, such as cooperation and team-
work, which are essential for an intelligent agent
(Seeber et al., 2020; Dafoe et al., 2021).
∗Work done during a Meta AI internship.
†Work done while at Meta AI, now at DeepMind.
1LIGHT: https://github.com/facebookresearch/LIGHT
Figure 1: An example extracted from a conversation in
the newly collected MultiLIGHT dataset.
The aim of this work is to both evaluate how
good current models are when extended to the
multi-party dialogue setting, and to investigate how
to improve such models for aspects where they
fail. There are two main challenges that this study
focuses on:
1. Turn-taking: deciding when to speak next
is important for the ﬂow of the conversation.
Speaking out of turn or being silent when a
response is expected negatively affects our
assessment of a speaker.
2. Coherence of the utterances: generating ap-
propriate responses requires taking into ac-
count the dialogue from multiple people in
the conversation. The model must distinguish
arXiv:2304.13835v1  [cs.CL]  26 Apr 2023

the states and characteristics of each of the
participants to produce good responses. For
example, even in pairwise (two-party) conver-
sations, maintaining identity is known to be
challenging for models (Shuster et al., 2021c).
We ﬁrst collect a new dataset of group conver-
sations, MultiLIGHT, each with three human par-
ticipants. We assign a fantasy location setting to
each conversation, and participants are given a char-
acter descriptions (personas) to role-play. The lo-
cations and characters are extracted from previous
works on this game (Urbanek et al., 2019). Figure 1
shows a small snippet of an example from the new
dataset. This setting allows us to control the con-
versational situations and have knowledge of the
roles the agents should be playing in the conversa-
tion, making evaluation of systems more tractable
compared to completely uncontrolled settings.
Using this setting, we then study the two afore-
mentioned challenges via evaluating the use of lan-
guage models in various settings. We use these
ﬁndings, in addition to the training data collected
from MultiLIGHT, to then build the best combined
approach in each of these challenging dimensions.
Finally, running interactive sessions between var-
ious models and human evaluators, we assess the
quality of models along a set of different human
judgments. We ﬁnd that our selected model can
outperform the existing state of the art in terms of
consistency, engagingness, identity, (lack of) con-
tradictions, and sensibility.
2
Related Work
With the success of data-driven dialogue systems,
there is an abundance of corpora for training lan-
guage models for conversational agents (Serban
et al., 2015). However, we believe one can still
argue that there is a dearth of datasets and research
on multi-party dialogue. Although the importance
of multi-party human-computer interaction and
the lack of corpus for this research direction was
pointed out by Kirchhoff and Ostendorf (2003)
about two decades ago, a recent survey (Maha-
jan and Shaikh, 2021) also highlights the “need for
. . . data collection for multi-party dialogue”.
Datasets built from social media interactions on
platforms such as Twitter (Ritter et al., 2010), Red-
dit (Baumgartner et al., 2020), Ubuntu chat logs
(Lowe et al., 2015) have been used successfully
for training models in the past (Adiwardana et al.,
2020; Roller et al., 2020). Despite being valuable
corpora for generating text responses, they do not
provide complete in-domain features for real-time
interactions between humans and the agent. For
instance, they often suffer from redundant para-
phrases of the same utterances, for example the re-
peated answers reported in Ubuntu chat logs (Lowe
et al., 2015). Also, the ambiguity of the involved
participants and the timeline of the responses is
challenging for grounding the dialogue responses
in the full context.
Here we focus on real-time interactions and
open-domain dialogue with a ﬁxed number of par-
ticipants. This is more aligned with corpora on
dialogues and negotiations in games such as the Set-
tlers of Catan (Afantenos et al., 2012) or Diplomacy
(FAIR et al., 2022). Transcribed public records
have also been utilized for similar research in the
past (Hawes et al., 2009). These datasets how-
ever cover a speciﬁc and often narrow scope and
are hardly suitable for building models capable of
having engaging dialogues in more diverse scenar-
ios. For example, Diplomacy games center on dis-
cussions of attacking provinces/countries. Agents
trained on datasets of movie scripts have also been
explored (Serban et al., 2016). Although one can
amass large datasets from digital movie scripts,
they admittedly leave out the non-verbal context
that makes them challenging to use with conver-
sational agents: the omitted context inhibits mod-
els’ ability to ground its responses on the existing
knowledge, which in turn makes the agent prone
to hallucination (Shuster et al., 2021a). The MPC
corpus (Shaikh et al., 2010) has the closest resem-
blance to the ideal dataset we have in mind. Al-
though limited in size (a total of 14 conversations
and 7317 utterances), it goes beyond simple text
and includes detailed annotations for modeling so-
cial phenomenon and dynamics of the discourse.
LIGHT is a text-based fantasy adventure game
developed as a platform for dialogue and natural
language research (Urbanek et al., 2019). It pro-
vides a controlled environment for supervised in-
teractions between multiple agents and their envi-
ronment. The game consists of human and NPC
(Non-Player Character) players with assigned per-
sonas (e.g., wizard, queen). Throughout the game,
characters engage in conversation and emit actions
that involve their environment and other characters.
Our study here is implemented within the LIGHT
framework with a focus on developing skills for AI
to engage in conversation with more than one other

player. For that reason, it requires distinguishing
between characters present in the setting, including
maintaining one’s own identity successfully which
is often challenging (Shuster et al., 2021c).
3
The MultiLIGHT dataset
We ﬁrst design a conversational setting which in-
volves a group chat between three conversation-
alists. We extend the conversational setup from
Urbanek et al. (2019) (which otherwise previously
focused on two-party conversations). We use the
Mephisto framework (Urbanek and Ringshia, 2023)
along with Amazon Mechanical Turk to conduct
conversations between humans using a crowdsourc-
ing task. We initially collect conversations between
three humans in this setting to both supply train-
ing data and a gold evaluation setting. Later, we
replace some of the humans with model agents
instead. Importantly, players can send their mes-
sage at any time and it will be shown to all the
participants. For models to conduct dialogue in
such a setting they will hence also have to choose
both when to speak appropriately, in addition to
producing coherent and relevant messages.
Characters and Settings
Research on open-
domain dialogue has found that taking into account
character personas and interests makes the col-
lected conversations more engaging (Dinan et al.,
2020; Shuster et al., 2021b).
We thus assign
each conversationalist (which we also refer to as a
“player”) a role to play (referred to as their given
persona). Roles are selected from a set of 955
characters from the LIGHT dataset (Urbanek et al.,
2019), which range from tavern owner to boat cap-
tain to even talking animal characters such as a
mouse. We also provide a description of the set-
ting in which their interactions happen from a set
of 579 locations from LIGHT, which is a diverse
collection from bamboo hut to unicorn palace. We
collect train, valid, and test conversations with ran-
domized combinations of characters and settings.
All crowdworkers are required to complete an on-
boarding in order to provide data, in addition to
several checks after they complete their work to
monitor data quality. More details on the data col-
lection protocol are given in Appendix A.
Figure 1 shows a snippet of a sample conver-
sation from our training set and the information
provided to the participants at that point during the
conversation with the given setting and character
dialogue.
Overall Dataset
Table 1 summarizes the statis-
tics of our newly collected dataset, which we call
MultiLIGHT. MultiLIGHT has 10,917 conversa-
tions and 313,433 utterances, collected from more
than 300 crowdworkers. Of those, 390 conversa-
tions (11,005 utterances) are reserved for the vali-
dation set, and 323 conversations (9,164 utterances)
for the test set. The dataset and the crowdsourcing
code will be open-sourced as part of the LIGHT
codebase2.
4
Methods
We investigate methods that tackle the two major
challenges of multi-party dialogue which we dis-
cussed previously in section 1; namely turn-taking
ability and utterance generation coherence. We an-
alyze four approaches for building a conversational
agent that tackle these issues. As we discuss later,
some of these address both issues at once, while
others only focus on one of them.
Each model is designed to be deployed in a live,
multi-party conversational setting where it must
communicate with other agents in the same loca-
tion – where the other agents are either controlled
by other models or by humans. All the ﬁnal meth-
ods we will compare employ transformer-based
language models to generate dialogue, given the
history of conversation between the various parties.
The models differ in how they combine turn-taking
(when to generate an utterance) with the actual
generation process.
4.1
Silence OR utterance model
In this approach there is effectively a separate
model for each of the AI-controlled characters
(even if they share the same model weights). At the
beginning of every turn, each agent generates an
output which determines its judgment on whether
it is appropriate for its character to speak on that
round or remain silent. If it determines it should
speak, that output determines the utterance itself.
Hence, the model either generates the actual ut-
terance or else emits a special output (called the
silence token) which will be interpreted as relin-
quishing the turn in favor of the other players. One
could have dedicated models/model weights for
each character, or alternatively one could share
model weights and distinguish characters by using
different context-based prompts. In the latter case,
the context hence needs to include features for the
2https://github.com/facebookresearch/LIGHT

MultiLIGHT Task
Train
Valid
Test
Total
Number of Dialogues
10,204
390
323
10,917
Number of Utterances
293,264
11,005
9,164
313,433
Average Utterances per Dialogue
28.7
28.2
28.4
28.7
Table 1: Multi-party LIGHT (MultiLIGHT) Dataset Statistics.
Model Type
Challenge
Silence OR utterance
Speaker AND utterance
Speaker only
Utterance only
Turn-taking
!
!
!
%
Coherence
!
!
%
!
Table 2: Suggested approaches for building a multi-party conversational agent, and the challenges they address.
model to determine which character it will be play-
ing (e.g., adding the current character name to the
end of the context, in addition to its persona).
Since the decisions are made independently for
each of the characters, it is possible that we arrive at
situations where multiple AI-controlled characters
want to speak, or an impasse where none do. While
those are plausible situations in realistic scenarios,
in our evaluations we use some rules to handle these
situations. In particular, breaking the tie using the
most probable utterance, or waiting for humans to
speak, with a certain time limit.
4.2
Speaker AND utterance model
In this method, the model is trained to generate
the name/ID for the next speaker followed by the
utterance itself for all the characters in the conver-
sation. For example, "King: I am blessing you with
my visit.", where the speaker is "King", and the
utterance is the rest of the sequence after the colon
and its trailing space.
This model can be easily adapted for real-time
conversation by running it to play the role of all the
characters, which ends up predicting who should
speak next (the ﬁrst part of the predicted sequence).
If the prediction is a human speaker (or any other
agent that it is not in charge of), then the model will
refrain from answering. Otherwise the predicted
utterance is the one that is output, for characters
that this model directly controls.
4.3
Speaker only model
This method is dedicated to only predicting the next
speaker. It does not predict what the content of their
utterance would be, and only focuses on controlling
the ﬂow of the conversation (turn-taking ability).
This can be implemented either as a classiﬁer or by
using sequence generation, for example a language
model which generates the next speaker name/ID,
similar to the ﬁrst part of the Speaker AND utter-
ance model output sequence. In our experiments
we use the latter language modeling approach. This
makes it easier to generalize to an arbitrary number
of participants, dynamically determined by the set
of names given in the prompt.
4.4
Utterance only model
After ofﬂoading the ﬂow control (turn-taking) part
to another model, (e.g., using the approach given in
4.3), a separate model can just focus on utterance
generation. Similar to the silence OR utterance
models, here we may have dedicated models for
each of the AI-controlled agents, or use a single
model for all of them. In the latter case this ends up
as a similar training task to subsection 4.2 except
the name/ID would appear in the context rather
than as part of the target output that should be
generated by the model. Then similarly at inference
time, we need features in the context that tell the
model which character is going to speak, as in
subsection 4.1.
4.5
Overall Model Capabilities
Some of the above approaches handle both our chal-
lenges simultaneously, while others solely address
one. Table 2 summarizes the capabilities of these
models. A successful agent may utilize any combi-
nation of these approaches with full or partial use.
For example, one can train a speaker AND utter-
ance model, but then only use it to predict the turns
by discarding its generated utterance component,
and only keeping the generated speaker name. In
that case, one could rely on a separate, utterance
only model, for utterance generation. This kind
of training may give better performance than the
Speaker only model.
In our experiments we analyze how to best build
a multi-party conversational agent that uses the best

performing combination of the above approaches.
The following section describes these experiments,
with the aim of ﬁnding these optimal choices.
5
Experiments
We compare the approaches proposed in section 4
by utilizing the MultiLIGHT dataset for training
and evaluating models. Our model input (prompts)
include the description of the location, personas
of the characters present, and the history of the
conversation. The current speaker name may or
may not be appended with that depending on the
speciﬁc approach.
We use the pre-trained 2.7B parameter Trans-
former model R2C2 (Shuster et al., 2022a) as the
base language model for most of our experiments,
which was previously shown to compare well with
other models of a similar size. For further details on
the training of our models, please see Appendix B.
5.1
Turn-taking Evaluation
We ﬁrst measure the turn-taking ability of the vari-
ous proposed models. For this we construct a new
task with the MultiLIGHT validation set, wherein
the goal is to predict the next speaker given previ-
ous dialogue turns. As our metric, we measure the
accuracy of a model in predicting the right speaker
for the next turn, over all turns. A random baseline
in this task has an accuracy in predicting the next
turn speaker of 33.3%.
5.1.1
Silence OR utterance
This model is trained to either generate a special
silence token (__SILENCE__ ) or the actual utter-
ance from its respective character, if the model
predicts it is the character’s turn. Because of the
over-presence of examples with the silence token as
response ( 2
3 of the examples), the direct use of the
dataset is prone to producing models that always
output __SILENCE__ . Thus, we experimented with
different values of “silence drop out” (referred to
as SDO), where we drop a fraction of these silence
examples. During the evaluation we generate with
the model for each of the agents, then aggregate
the results to determine the next speaker. Our ap-
proach breaks ties by selecting the agent with the
lowest probability of generating __SILENCE__ , i.e.
is most conﬁdent that it should speak (as we know
in the evaluation someone does speak).
This approach was not able to achieve speaker
prediction accuracy noticeably higher than the base-
line (our best was 36.5%, achieved with SDO of
0.9). We believe that this is due to the accumula-
tion of errors from all three generations, which is
discussed in more detail in Appendix D.
5.1.2
Speaker AND utterance
Training a model to predict the speaker name and
the utterance, we can use only the ﬁrst part (speaker
name) to predict the actual speaker, and thus eval-
uate its performance in controlling the ﬂow of the
conversation. Using this approach, our best model
trained on MultiLIGHT achieved an accuracy of
49.5%.
5.1.3
Speaker only
Finally, we explore models that generate the
speaker name only. Here, as well as evaluating
R2C2-based models, as before, we also include
smaller models in our comparisons: BART (400m)
and T5 (700m). We also ran experiments on pre-
dicting the speaker given not only the dialogue
history, but also the current generated utterance as
part of the context (knowing what was said, and
asking the model who said it). The latter should
be an easier task and is one way of measuring up-
per bounds / headroom for the more difﬁcult task.
Table 3 summarizes the accuracy of these models,
as well as comparing them to the other approaches
from subsubsection 5.1.1 and subsubsection 5.1.2.
For models using the same underlying R2C2 lan-
guage model we see performance that is similar
between the Speaker only and Speaker AND Utter-
ance models, with the Speaker AND Utterance be-
ing slightly better. This indicates the extra training
targets of the latter model do not make the speaker
identiﬁcation task harder, and may actually bring
a beneﬁt. Changing the base language model, we
also see superior performance of the BART model
over T5 and R2C2 — where BART happens to be
the smallest model in size (number of parameters).
Comparing all of these numbers to including the
current utterance (last column), we see a clear im-
provement on speaker prediction accuracy, with
large gains. There may still be headroom for much
better models.
5.2
Coherence Evaluation
The
next
step
after
investigating
turn-
taking/conversational
ﬂow,
is
investigating
the quality and the coherence of the utterances
themselves. In these evaluations we assume the
speaker is already known, following the choices
given in the validation set annotations, and only

Given dialogue
Given history
Model
history only
+ current utterance
Silence OR utterance (2.7B)
35.8
-
Speaker AND Utterance (2.7B)
49.5
-
Speaker only, R2C2 (2.7B)
49.2
82.2
Speaker only, T5 (700M)
49.6
68.9
Speaker only, BART (400M)
54.4
85.0
Table 3: The accuracy of predicting the speaker for the next utterance, either given no knowledge of what speaker
or utterance is coming next, or given the current utterance itself (but not the speaker name) which is an easier task.
LIGHT
LIGHT Wild
MultiLIGHT
PPL
PPL
PPL
Model
8k
50k
F1
8k
50k
F1
8k
50k
F1
BlenderBot 1 (2.7B)
16.88
13.95
17.76
12.51
19.32
12.36
BlenderBot 2 (2.7B)
19.01
10.48
18.83
11.53
22.88
10.23
Prompted OPT (175B)
12.74
13.35
13.71
13.17
13.43
14.07
Prompted BlenderBot 3 (175B)
9.78
15.95
9.32
13.96
10.65
13.36
LIGHT SotA (2.7B)
12.35
16.03
14.48
14.34
14.77
13.52
Utterance only [L]
14.51
16.25
15.87
14.21
17.46
13.71
Utterance only [LW]
13.64
17.43
14.08
15.21
17.44
14.11
Utterance only [LWM]
13.62
17.34
14.25
15.14
13.25
15.86
Utterance only W Cringe Loss [LWM]
14.75
16.64
14.78
14.95
15.28
15.05
Silence OR Utterance (SDO=0.5) [M]
-
-
-
-
16.82
11.53
Silence OR Utterance (SDO=0.9) [M]
-
-
-
-
16.06
11.37
Silence OR Utterance (SDO=0.999) [M]
-
-
-
-
15.74
12.08
Speaker AND Utterance [L]
14.12
16.70
15.80
14.67
16.39
14.39
Speaker AND Utterance [LW]
14.03
16.69
14.05
16.15
16.29
14.48
Speaker AND Utterance [LWM]
14.40
16.92
14.45
15.91
13.69
15.95
Speaker AND Utterance [M]
16.95
14.91
19.05
12.50
13.55
15.66
Table 4: Language Modeling Metrics on three tasks, LIGHT and LIGHT Wild (which are two-party) and Mul-
tiLIGHT (which is multi-party). We compare the performance of existing state of the art models (top 5 rows) to
R2C2 based models (2.7B parameters) with three approaches to training: Utterance only: given the full context
and the prompt for the next-speaker, it generates the utterance; Silence OR Utterance: given the full context and
the prompt for the next-speaker, generates a silence token or the utterance; Speaker AND Utterance: given the
full context, generates the next speaker name/ID along with the utterance from that speaker. The datasets that
models are trained on is indicated in the brackets (L: LIGHT, W: LIGHT Wild, M: MultiLIGHT). PPL values are
split into two columns based on the model dictionary sizes.
evaluate the quality of various models at generating
utterances given the speaker choice.
5.2.1
Baselines and Setup
We start with establishing baselines using the ex-
isting dialogue models from the literature that are
comparable with our setup. We choose BlenderBot
1 (Roller et al., 2020), BlenderBot 2 (FAIR, 2021),
along with a prompted version of pre-trained-only
OPT (Zhang et al., 2022) and its dialogue-ﬁne-
tuned variant BlenderBot 3 (Shuster et al., 2022b).
For OPT, we crafted custom prompts that included
a few examples from the MultiLIGHT training set.
For BlenderBot 2 and 3, we do not use their long
term memory and internet search modules and use
only their utterance generation module to output a
given turn.
This work is a multi-party extension of previous
works on the text-based adventure game, LIGHT.
As a result, we have access to two other in-domain
datasets which, although not multi-party dialogue,
provide more data for better ﬁne-tuning. They also
provide additional evaluation setups for our models,
which allow us to check if they perform well in the
two-party, as well as the multi-party case. Specif-
ically we use the LIGHT (Urbanek et al., 2019)
and LIGHT Wild (Shuster et al., 2021b) datasets,
which are both two-party conversational setups in
a crowdworker or organic user set, respectively.
We thus also include in our baselines the current
state of the art model for LIGHT from the best
performing Expanded Attention model in (Shuster
et al., 2021c) which was trained on the latter two
datasets; we refer to it as LIGHT SotA for the rest
of this paper.
Table 4 provides our main results. The ﬁrst 5

rows show the performance of our baseline mod-
els, in terms of perplexity and unigram F1, on the
validation split of our three main datasets. The
baseline models, except OPT, are ﬁne-tuned on a
large corpora of two-party turn-based dialogues.
BlenderBot 3 and LIGHT SotA have LIGHT and
LIGHT Wild in their ﬁne-tuning data. BlenderBot
1 and 2, and the LIGHT SotA models are using the
same dictionary with 8008 tokens; OPT, Blender-
Bot 3, and R2C2 share a different dictionary with
50,264 tokens. Perplexities across different dictio-
naries are not comparable, but are comparable for
models that share the same dictionary. With that in
mind, we can see the improved performance of the
LIGHT SotA model compared to BlenderBot 1 and
2, across all three datasets, which is clearly related
to its in-domain ﬁne-tuning. Comparing OPT to
BlenderBot 3 shows better perplexity metrics for
BlenderBot 3, but better F1 for the OPT model on
MultiLIGHT. We attribute this to the observation
that BlenderBot 3 is ﬁne-tuned on dialogue data,
including LIGHT and LIGHT Wild, but is biased
towards two-party dialogues.
Next, we study the improvements we can gain
from training new models that take advantage of
the new MultiLIGHT dataset. We use the three
approaches from section 4 which are capable of
response generation.
5.2.2
Utterance only
Rows 6 to 9 in Table 4 summarize the results of
using this approach when ﬁne-tuning an R2C2
model. The ﬁrst three results are vanilla training
of the transformer models with different training
sets, while the last row is from using the Cringe
loss to also penalize negative examples (Adolphs
et al., 2022). The details of generating positive
and negative examples for the Cringe loss model
is presented in Appendix E. Final results indicate
that using the simple training (with no Cringe loss)
that is multi-tasked with all the three datasets has
the best performance. While using MultiLIGHT
does not strongly impact the performance on the
LIGHT and LIGHT Wild datasets, it makes a large
difference in improving metrics for the multi-party
case, e.g. validation PPL is reduced from 17.44 to
13.25.
5.2.3
Silence OR utterance
Rows 10 to 12 in Table 4 shows the results for
three selected values of silence drop out (SDO).
Note here we are computing the utterance quality
metrics only on turns that the speaker is not silent,
and we force the model to generate an utterance in
these cases for evaluation purposes. We trained this
approach with only the multi-party MultiLIGHT
dataset and evaluated it on MultiLIGHT. The per-
formance drop from this approach, compared to
other techniques, e.g. Speaker and Utterance mod-
els trained on MultiLIGHT only, is quite notice-
able.
5.2.4
Speaker AND utterance
The last four rows in Table 4 show the metrics
on the quality of the utterances generated by the
speaker AND utterance models. In order to only
evaluate utterance coherence for this model, we pro-
vide the correct speaker label as a preﬁx (to avoid
generating for the wrong speaker) and compute
perplexity and unigram F1 only on the utterance
part of the generated text, by masking the speaker
name and its subsequent tokens (colon and trail-
ing space). Again, we ﬁnd that the multi-tasked
model is outperforming the other models on Mul-
tiLIGHT with a minimal deterioration on the two-
party LIGHT and LIGHT Wild tasks. Overall, it
shows performance marginally, if at all, worse than
the utterance-only approach, exceeding it in 2 out
of 6 metrics in the table.
5.3
Human Evaluations
In order to test our ﬁndings in practice, we run
human evaluations (Smith et al., 2022). This is
another round of crowdsourced tasks in which we
ask crowdworkers to have an interactive conversa-
tion with our agents (models). We replicate a setup
similar to the original crowdsourcing task for Mul-
tiLIGHT: there are three characters, in a described
location, all role-playing (either role-played by a
human, or by a model) to their assigned personas.
Here, the crowdworker takes the role of one of the
characters, while the models play the other two
roles. After each response from the bot, the hu-
man participant (i.e., the crowdworker) tags model
responses for attributes such as consistency, con-
tradiction, engagingness, sounding out of turn or
nonsensical, and having mistaken identity. The
crowdworkers are also asked to rate the overall
quality of the conversation at the end of the chat.
We provide further details about the task and evalu-
ation in Appendix F.
We compare two different approaches for turn-
taking (a random baseline vs.
a BART-based
speaker only model) and two for utterance gen-

Mistaken
Overall
Number of
Model
Consistent↑
Engaging↑
Identity↓
Contradictory↓
Nonsensical↓
Rating (5)↑
Messages
LIGHT SotA
28.1%
23.9%
25.5%
8.8%
7.5%
2.5
2198
Our best
55.8%
31.6%
2.2%
4.9%
2.4%
3.8
2136
Table 5: Human Evaluation Results comparing our best utterance generation model (Utterance only [LWM]) to
the existing state of the art model shows clear improvements from adding the new MultiLIGHT dataset. We ﬁnd a
statistically signiﬁcant improvement across all metrics (t-test, p-value of 0.05).
Figure 2: Human evaluations for selected models, with
two combinations of speaker prediction (blue vs. yel-
low bars) and utterance generation (solid vs. hollow
bars). The arrow on a metric shows the direction for its
improvement (i.e., higher or lower).
eration (LIGHT SotA as a baseline vs. Utterance
only [LWM]). We thus devised four different agents
for this experiment, combining these approaches.
Figure 2 shows the outcome of our human eval-
uation on the combinations of the turn-taking and
the utterance generation models. There is signiﬁ-
cant improvement across the board on the quality
of the utterance when comparing our best utter-
ance generation model (solid bars) to the baseline
(hollow bars). The same conclusion can not be
made between the random baseline (blue bars) to
the trained turn-taking model (yellow bars). We
hypothesize that this is due to the nature of the con-
versations in this dataset: we are in an open-ended
conversational setting, e.g. without any strict rules
for accomplishing a task. One could argue that
no matter who speaks next, if the utterance makes
sense there is little if any harm to the ﬂow of the
conversation. We could even notice some utter-
ances that can perhaps be reordered in the sample
provided in Figure 1.
Factoring out the turn-taking approach, which
was concluded to have insigniﬁcant contribution to
the quality, we compare the human evaluation re-
sults from our two dialogue generations models in
Table 5. This provides the overall comparison be-
tween LIGHT SotA to our best utterance generation
model (i.e., Utterance only [LWM]). Note that here
we removed the “out of turn” criterion, because
it was attributed to the ﬂow control (turn-taking)
models and not the utterance generation model. Re-
sults clearly shows the improvement gained from
the MultiLIGHT dataset on all the attributes. These
result show a statistically signiﬁcant improvement
across all the metrics (t-test, p-value of 0.05).
6
Conclusion
We have studied conversational AI agents conduct-
ing multi-party chat, comparing several presented
techniques using a newly collected dataset. The
lack of a corpus with adequate size for training as
well as evaluating practical models has been one of
the bottlenecks for this research topic. We built a
new dataset, MultiLIGHT, to train agents that act
in a multi-party setting with a given persona and
in a given setting. This was done through train-
ing language models to be proﬁcient at two tasks:
(1) controlling the ﬂow of the conversation (turn-
taking); (2) generating text utterances. Comparing
our best approach on each of these two subtasks to
existing state of the art baselines we showed that
the new training set has a notable effect in increas-
ing the quality of utterances in a multi-party chat,
across many metrics. The ﬂow control subtask,
however, has much less contribution to the overall
quality of the conversational agent as measured by
humans.
Results here raise future research questions
around the value of a turn-taking model and the
settings in which strict turn-taking is important for
improving conversation quality. It is possible that
our dataset and the given setting do not strongly
require exact turn order for an engaging and coher-
ent conversation, and evaluation of performance
in this area – in situations where it becomes more
important – requires building further benchmarks.

7
Social Impact
The impact of training in multi-party settings is not
particularly well understood, as much less prior
work exists in this space compared to the two-party
setting. We hope that our contribution in this area
can foster more research, and future work may
want to pay attention to how awareness of multi-
party turn-taking in bots can lead to more inclusive
experiences for those that can be left out in these
settings.
Our crowdsourced dataset was collected with
attention to worker fairness. We estimated the
amount of time that it takes the crowdworkers to
submit a task. Using this we assured that they are
compensated fairly, well above minimum wage for
all of the tasks used in this work. This rate was used
both for the main phase as well as for the eligibility
phase, wherein we allowed workers to attempt the
task and selected a pool of eligible workers that
met the task requirements.
We note that our dataset relies on the underlying
LIGHT environment, which may have some is-
sues with fairness and representation. While some
of these have been studied in previous works and
mitigations are included (Dinan et al., 2019), we
acknowledge that these issues are not entirely re-
solved. We rely on the characters and personas
from this work, but no analysis has been done on
biases that are present in the original LIGHT loca-
tions or in created layouts. Given we use settings
generated in the same way, our dataset would suffer
from the same issues should they exist.
References
Daniel Adiwardana, Minh-Thang Luong, David R So,
Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,
Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
et al. 2020.
Towards a human-like open-domain
chatbot. arXiv preprint arXiv:2001.09977.
Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster,
Sainbayar Sukhbaatar, and Jason Weston. 2022. The
cringe loss: Learning what language not to model.
arXiv preprint arXiv:2211.05826.
Stergos Afantenos, Nicholas Asher, Farah Benamara,
Anais Cadilhac, Cédric Dégremont, Pascal De-
nis, Markus Guhe, Simon Keizer, Alex Lascarides,
Oliver Lemon, et al. 2012.
Developing a corpus
of strategic conversation in the settlers of catan. In
Workshop on Games and NLP (GAMNLP-12).
Jason Baumgartner, Savvas Zannettou, Brian Keegan,
Megan Squire, and Jeremy Blackburn. 2020. The
pushshift reddit dataset.
Allan Dafoe, Yoram Bachrach, Gillian Hadﬁeld, Eric
Horvitz, Kate Larson, and Thore Graepel. 2021. Co-
operative ai: machines must learn to ﬁnd common
ground. Nature, 593(7857):33–36.
Emily Dinan, Angela Fan, Adina Williams, Jack Ur-
banek, Douwe Kiela, and Jason Weston. 2019.
Queens are powerful too: Mitigating gender bias in
dialogue generation.
In Conference on Empirical
Methods in Natural Language Processing.
Emily Dinan, Varvara Logacheva, Valentin Malykh,
Alexander Miller, Kurt Shuster, Jack Urbanek,
Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
Lowe, et al. 2020.
The second conversational in-
telligence challenge (convai2). In The NeurIPS’18
Competition: From Machine Learning to Intelligent
Conversations, pages 187–208. Springer.
Meta AI FAIR. 2021. Blender bot 2.0: An open source
chatbot that builds long-term memory and searches
the internet.
Meta AI FAIR, Anton Bakhtin, Noam Brown, Emily
Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried,
Andrew Goff, Jonathan Gray, Hengyuan Hu, et al.
2022. Human-level play in the game of diplomacy
by combining language models with strategic rea-
soning. Science, 378(6624):1067–1074.
Angela Fan, Jack Urbanek, Pratik Ringshia, Emily
Dinan, Emma Qian, Siddharth Karamcheti, Shri-
mai Prabhumoye, Douwe Kiela, Tim Rocktaschel,
Arthur Szlam, and Jason Weston. 2020.
Gener-
ating interactive worlds with text.
Proceedings
of the AAAI Conference on Artiﬁcial Intelligence,
34(02):1693–1700.
Timothy Hawes, Jimmy Lin, and Philip Resnik. 2009.
Elements of a computational model for multi-party
discourse:
The turn-taking behavior of supreme
court justices. Journal of the American Society for
Information Science and Technology, 60(8):1607–
1615.
Katrin Kirchhoff and Mari Ostendorf. 2003.
Direc-
tions for multi-party human-computer interaction re-
search.
In Proceedings of the HLT-NAACL 2003
Workshop on Research Directions in Dialogue Pro-
cessing, pages 7–9.
Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. arXiv preprint arXiv:1506.08909.
Khyati Mahajan and Samira Shaikh. 2021.
On the
need for thoughtful data collection for multi-party
dialogue: A survey of available corpora and collec-
tion methods. In Proceedings of the 22nd Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 338–352, Singapore and On-
line. Association for Computational Linguistics.
Kevin P Murphy. 2012. Machine learning: a proba-
bilistic perspective. MIT press.

OpenAI. 2022. Chatgpt: Optimizing language models
for dialogue.
https://openai.com/blog/
chatgpt/. [Online; accessed 21-February-2023].
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of Twitter conversations. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 172–180,
Los Angeles, California. Association for Computa-
tional Linguistics.
Stephen Roller, Y-Lan Boureau, Jason Weston, An-
toine Bordes, Emily Dinan, Angela Fan, David
Gunning, Da Ju, Margaret Li, Spencer Poff, et al.
2020.
Open-domain conversational agents: Cur-
rent progress, open problems, and future directions.
arXiv preprint arXiv:2006.12442.
Isabella Seeber, Eva Bittner, Robert O Briggs, Tri-
parna De Vreede, Gert-Jan De Vreede, Aaron Elkins,
Ronald Maier, Alexander B Merz, Sarah Oeste-Reiß,
Nils Randrup, et al. 2020. Machines as teammates:
A research agenda on ai in team collaboration. In-
formation & management, 57(2):103174.
Iulian Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the AAAI conference on artiﬁcial intelligence, vol-
ume 30.
Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Lau-
rent Charlin, and Joelle Pineau. 2015. A survey of
available corpora for building data-driven dialogue
systems.
Samira Shaikh, Tomek Strzalkowski, Aaron Broadwell,
Jennifer Stromer-Galley, Sarah Taylor, and Nick
Webb. 2010. MPC: A multi-party chat corpus for
modeling social phenomena in discourse.
In Pro-
ceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC’10),
Valletta, Malta. European Language Resources As-
sociation (ELRA).
Kurt Shuster, Mojtaba Komeili, Leonard Adolphs,
Stephen Roller, Arthur Szlam, and Jason We-
ston. 2022a.
Language models that seek for
knowledge: Modular search & generation for di-
alogue and prompt completion.
arXiv preprint
arXiv:2203.13224.
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,
and Jason Weston. 2021a.
Retrieval augmenta-
tion reduces hallucination in conversation.
arXiv
preprint arXiv:2104.07567.
Kurt Shuster, Jack Urbanek, Emily Dinan, Arthur
Szlam, and Jason Weston. 2021b. Dialogue in the
wild: Learning from a deployed role-playing game
with humans and bots. In Findings of the Associ-
ation for Computational Linguistics: ACL-IJCNLP
2021, pages 611–624.
Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason
Weston. 2021c. Am i me or you? state-of-the-art
dialogue models cannot maintain an identity. arXiv
preprint arXiv:2112.05843.
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,
Eric Michael Smith, Stephen Roller, Megan Ung,
Moya Chen, Kushal Arora, Joshua Lane, Morteza
Behrooz, William Ngan, Spencer Poff, Naman
Goyal, Arthur Szlam, Y-Lan Boureau, Melanie
Kambadur, and Jason Weston. 2022b. Blenderbot
3:
a deployed conversational agent that continu-
ally learns to responsibly engage.
arXiv preprint
arXiv:2208.03188.
Eric Michael Smith,
Orion Hsu,
Rebecca Qian,
Stephen Roller, Y-Lan Boureau, and Jason Weston.
2022. Human evaluation of conversations is an open
problem: comparing the sensitivity of various meth-
ods for evaluating dialogue agents. arXiv preprint
arXiv:2201.04723.
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,
YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,
Amin Ghafouri, Marcelo Menegali, Yanping Huang,
Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao
Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,
Maarten Bosma, Yanqi Zhou, Chung-Ching Chang,
Igor Krivokon, Will Rusch, Marc Pickett, Kath-
leen S. Meier-Hellstern, Meredith Ringel Morris,
Tulsee Doshi, Renelito Delos Santos, Toju Duke,
Johnny Soraker, Ben Zevenbergen, Vinodkumar
Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen
Olson, Alejandra Molina, Erin Hoffman-John, Josh
Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna,
Matthew Lamm, Viktoriya Kuzmina, Joe Fenton,
Aaron Cohen, Rachel Bernstein, Ray Kurzweil,
Blaise Aguera-Arcas, Claire Cui, Marian Croak,
Ed H. Chi, and Quoc Le. 2022.
Lamda:
Lan-
guage models for dialog applications.
CoRR,
abs/2201.08239.
Jack Urbanek, Angela Fan, Siddharth Karamcheti,
Saachi Jain, Samuel Humeau, Emily Dinan, Tim
Rocktäschel, Douwe Kiela, Arthur Szlam, and Ja-
son Weston. 2019. Learning to Speak and Act in
a Fantasy Text Adventure Game.
In Proceedings
of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 673–683, Hong
Kong, China. Association for Computational Lin-
guistics.
Jack Urbanek and Pratik Ringshia. 2023. Mephisto: A
framework for portable, reproducible, and iterative
crowdsourcing. arXiv preprint arXiv:2301.05154.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al.
2022. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068.

A
MultiLIGHT dataset generation task
We use Amazon Mechanical Turk service to ac-
quire crowdworkers. Workers are required to go
through our screening step which assures they un-
derstand the objectives of our task, and present high
quality work; we call this our task on-boarding.
During the on-boarding, we guide the workers
through a ﬁxed and scripted interactive scenario,
and gradually introduce them to the aspects of the
task that they need to pay attention to. Returning
workers are not required to go through this process
a second time and get directed to the main task.
After the on-boarding, workers are assigned to
a chat room of three and they start the main task.
Figure 3 shows an example screenshot from our
main crowdsourcing task. Each worker has an as-
signed persona to role-play. They can see personas
of their own character and other participants on
the information pane on the left side; which also
includes the description of the setting/location that
this conversation happens. Workers can send a
message anytime during the conversation: there
is no blocking of users due to turns. Along with
the message, we record the relative time that each
message was sent. After reaching a certain number
of utterances, they can choose to ﬁnish the chat
and submit their work. There is a form at the end
that allows workers to report any unacceptable or
inappropriate behaviour from their partners. We
also run automated tests on the context of the chat
to make sure of its quality and safety.
As far as the size of the dataset is concerned,
using ParlAI’s re tokenizer, MultiLIGHT has a
total of about 4.9 million utterance tokens in the
dataset. The training split contains 4.6 million of
them, with 36,297 unique tokens.
There are a total of 579 unique locations in the
training, 281 in the valid, and 243 in the test split of
the dataset. All the location in the valid and test set
are also present in the training set. Similarly there
are 955, 440, and 413 unique characters in the train,
valid, and test splits, respectively. Locations were
selected randomly from a ﬁltered list of eligible
rooms in the original LIGHT dataset, excluding
rooms that appeared too similar or ones that make
sense for a medieval fantasy setting but we didn’t
want to include in these chats (such as a "Torture
room" in a dungeon). Characters were then selected
from the candidate sets by a starspace model as in
Fan et al. (2020), using a random selection of three
of the top ﬁve candidates for each room.
We sampled the submissions and vetted them
carefully for assuring high quality. We identiﬁed
high quality workers (a total of 286) and created
a quality tier system based on that. Tier 1 data
is our high quality data in which all the workers
belong to our vetted set of workers and there is no
violation report. Tier 2 contains everything else:
there is either 1 reported violation or not all the
participating workers are vetted. Conversations
with very low quality after a manual check were
removed from the original dataset. We also add
the individual workers quality to the dataset for fur-
ther studies that may focus on interaction between
workers from different tiers. The entire valid and
test splits, and 76% of the train set is composed of
the tier 1 data.
B
Model training details
We used ParlAI platform3 for training our models.
For most of our R2C2 models we ran a series of
sweeps on different values of the batch size (8, 32)
and learning rate (1e −6, 5e −7), then selected
the best performing models to run our evaluations
on. Our training cluster uses V100 GPUs and we
used 1 GPU per batch example. The wall time is
set at 48 hours, with an early stoppage if the criteria
metric (mostly perplexity) does not improve over 3
consecutive evaluations; we ran an evaluation every
half epoch.
Our BART model uses a learning rate of 1e −6,
batch size of 32, and T5 uses learning rate of 1e−5,
batch size of 16. Both run on 4 GPUs with the same
GPU type and wall time as before.
During the inference, for generating the results,
we used greedy generation for turn-taking models
and beam search for utterance generation. Gener-
ally, we choose a beam size of 3 with a minimum
length of 20 tokens, and beam blocking tri-grams
(ngrams of size 3).
C
Turn-taking performance baseline
Here we discuss our baseline performance values
for turn-taking models. While calculating the nu-
meric values, we assume that there are only 3 char-
acters present to arrive at numbers relevant to our
dataset.
C.1
From a single character viewpoint
Let y be a random variable that is 1 if the model se-
lects its relevant speaker to be the next speaker and
3https://parl.ai/

Figure 3: Screenshot sample of the task interface used by crowdsourcers for creating the main dataset.
0, otherwise. Similarly, have ˆy to be 1 if the next
turn is in fact this speaker and 0 otherwise. Clearly
these two random variables are independent since
one is coming from the model response/sampling,
and the other is a function of the dataset.
From the viewpoint of a single character if
the model selects uniformly at random, we have
p(y = 1) = 1
3 as the chance of selecting its relevant
character as the next speaker, and p(ˆy = 1) = 1
3
for the next actual speaker being in fact this char-
acter. Keeping in mind the independence of these
variables the probability of correct decision for the
next turn is
p(correct) = p(y = 1, ˆy = 1) + p(y = 0, ˆy = 0)
= p(y = 1) × p(ˆy = 1)
+ p(y = 0) × p(ˆy = 0)
1
3
2
+
2
3
2
= 5
9,
which in fact equals our expected accuracy.
Using the above random variables we can ﬁnd
the precision as p(y = 1|ˆy = 1) (Murphy, 2012),
which given the variables independence becomes
p(y = 1) =
1
3. Similarly the recall comes to
p(ˆy = 1|y = 1) = p(ˆy) =
1
3. Putting these
together in for computing F1 score we arrive at 1
3.
At this point it is interesting to consider a model
that always remains silent (for example by gener-
ating __SILENCE__ ). Repeating the above with
p(y = 1) = 0, we get an accuracy of 2
3, and a
precision of 0, hence an F1 values of 0. Conversely,
a model with no silence turn reaches an F1 values
Figure 4: Model performance metrics in next speaker
prediction for the silence OR utterance models, with
different values of SDO.
of 1
2, because of p(y = 1) = 1 and p(ˆy) = 1
3; and
accuracy of 1
3.
C.2
Next speaker prediction
The baseline random model for choosing the next
speaker is selecting the next speaker uniformly at
random from the pool of the existing characters.
Having three characters, it is trivial to arrive at the
accuracy of 1
3 for correctly predicting the character
who has the next utterance.
D
Next turn prediction with silence OR
utterance models
Figure 4 shows the main metrics for evaluating the
turn-taking performance of the silence OR utter-

ance model. The values on this graph are show-
ing widely different metrics; but since they are
all dimensionless with values in [0, 1] interval, we
place them on the same graph for easier compari-
son and visualizing their trends. Here, in the case
of a tie during selecting the next speaking agents,
our heuristic algorithm computes the perplexity
of the silence token between the candidates (all
the agents if they all generated __SILENCE__ ; the
agents who want to speak, if there is more than
one) and picks the agent with the highest perplexity
on __SILENCE__ as the next speaker.
Silence drop out (SDO) values play an important
role in performance of the silence OR utterance
models when predicting the next speaker. The blue
and the orange lines show the self-turn (from the
agent viewpoint) prediction F1 and accuracy, re-
spectively. They match the two extremes for always
or never silent as we predicted in C.1. These met-
rics are from the viewpoint of a single character,
and therefore not directly comparable to the values
we discussed in 5.1.
The green line shows the accuracy of predicting
the next speaker (which includes the tie-breaking
using the heuristic) and is the metric we discussed
in 5.1 about this class of models. As it can be seen,
it remains relatively ﬂat with a very subtle increase
around SDO of 0.9. Overlaying that with the yel-
low line which shows the rate at which the model
is generating the silence token (notice it being 1 at
low and 0 at high SDO values), we see that this
small increase happens when the model crosses
from always silent to the never silent regimes. Re-
gardless, the jump is not enough to make the model
superior to the baseline. We argue that this is due
to the fact that having multiple models, there is
low probability for all three to generate the right
prediction simultaneously. In fact, if not having a
wrong speaker predicted, in many case we revert to
our heuristic for selecting the next speaker, which
its rate is show with the black line. This puts us-
ing the silence OR utterance model at disadvantage
compared to other approaches.
E
Cringe Loss
Training a model using Cringe loss requires deﬁn-
ing a set of positive and negative examples. Here,
the former comes from the regular training exam-
ples, which we used with the rest of the models in
this approach. We created two different types of
negative examples:
1. Wrong order: removing the last seen mes-
sage from the context of the conversation,
with the aim of imitating a message that is
sent out of order.
2. Wrong speaker: keeping the message but
changing its speaker to another one of the
present characters.
F
Human evaluation task
This task utilizes Mephisto again, but we also take
advantage of ParlAI agents, which build the core of
our conversational model (AI agent). These agents
are built on the premise of trun-based conversations
and run on a cycle of observe and act. We ﬁt our
framework of multi-party asynchronous chat into
this by running an agent act at the beginning of
each round of conversation. This act ﬁrst uses the
turn-taking model (random or trained) to choose
the next speaker. If the next speaker is determined
to be the human, it passes the control to the hu-
man and waits for a response, otherwise it blocks
the human from sending a message and meanwhile
uses the utterance generation model and produces a
response. When utilizing the utterance generation
models we craft the context such that it matches
the original format from its training. In both cases,
the response is shown to the model (to be taken
into account for updating the history of the conver-
sation). The cycle repeats until collecting a total of
15 message from either side.
On the user interface side, every time that there
is an utterance from the bot, the process prompts
the human to rate the response by attributes that
applies to the message (see Figure 5). Note that
there is a None option to choose when none of the
selected categories apply. Also, at the end of the
conversation, we ask about the overall quality of
the conversation from the human annotators—this
is in fact the state that the screenshot in Figure 5 is
at.

Figure 5: Screenshot sample of the task interface used by crowdsourcers for evaluating the models.

