ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION:
DOES IT PAY TO DISOBEY?
Vaishnavh Nagarajan 1 Aditya Krishna Menon 1 Srinadh Bhojanapalli 1 Hossein Mobahi 1 Sanjiv Kumar 1
Abstract
Knowledge distillation has been widely-used to
improve the performance of a “student” net-
work by hoping to mimic soft probabilities of
a “teacher” network. Yet, for self-distillation to
work, the student must somehow deviate from
the teacher (Stanton et al., 2021). But what is
the nature of these deviations, and how do they
relate to gains in generalization? We investigate
these questions through a series of experiments
across image and language classiﬁcation datasets.
First, we observe that distillation consistently de-
viates in a characteristic way: on points where the
teacher has low conﬁdence, the student achieves
even lower conﬁdence than the teacher. Secondly,
we ﬁnd that deviations in the initial dynamics of
training are not crucial — simply switching to dis-
tillation loss in the middle of training can recover
much of its gains. We then provide two parallel
theoretical perspectives to understand the role of
student-teacher deviations in our experiments, one
casting distillation as a regularizer in eigenspace,
and another as a gradient denoiser. Our analy-
sis bridges several gaps between existing theory
and practice by (a) focusing on gradient-descent
training, (b) by avoiding label noise assumptions,
and (c) by unifying several disjoint empirical and
theoretical ﬁndings.
1. Introduction
Distillation (Bucilˇa et al., 2006; Hinton et al., 2015) has
emerged as a highly effective model compression technique,
wherein one trains a small “student” model to match the
predicted soft label distribution of a large “teacher” model,
rather than match one-hot labels the teacher was trained on.
An actively developing literature has sought to explore ap-
plications of this technique to various settings (Radosavovic
et al., 2018; Furlanello et al., 2018; Xie et al., 2019), design
more effective variants of the above recipe (Romero et al.,
2015; Anil et al., 2018; Park et al., 2019; Beyer et al., 2022),
and better understand theoretically when and why distilla-
1Google Research. Correspondence to: Vaishnavh Nagarajan
<vaishnavh@google.com>.
Figure 1. A characteristic deviation in student-teacher proba-
bilities:
For each training sample (x, y), we plot φ(pte
yte(x))
versus φ(pst
yte(x)), which are the teacher and student probabilities
on the teacher’s predicted label yte, transformed monotonically
as φ(u) = log [u/(1 −u)]. We ﬁnd that the distilled student pre-
dictions characteristically deviate from the X = Y line (dashed)
by underﬁtting the teacher on instances where the teacher has low
conﬁdence (i.e., we ﬁnd Y ≤X for small X). That is, the student
exaggerates the teacher’s low conﬁdence. See §3.1 for details.
tion is effective (Lopez-Paz et al., 2016; Phuong & Lampert,
2019; Mobahi et al., 2020; Allen-Zhu & Li, 2020; Menon
et al., 2021; Dao et al., 2021; Ren et al., 2022).
On paper, distillation intends to transfer the teacher’s soft
probabilities over to the student. However, Stanton et al.
(2021) challenge this premise: they show there is often a
mismatch of student and teacher probabilities, and in fact,
that a greater mismatch is correlated with better student per-
formance. Indeed, in the self-distillation setting (Furlanello
et al., 2018; Zhang et al., 2019) — where the student and
teacher architectures are identical — some form of deviation
(in the representation, if not in the probabilities) is necessary
for the student’s generalization to supercede the teacher.
In this work, we are interested in better characterizing these
deviations, and in understanding how they play a role in
the (distillation-trained) student outperforming the (one-hot-
loss trained) teacher. More concretely, our key contributions
are in addressing the following questions:
(i) What student-teacher deviations exist?
Across a
wide range of architectures and image/language clas-
siﬁcation data, we demonstrate (§3.1) that at the end
of training, the student tends to underﬁt the teacher on
points where the teacher is low in conﬁdence (Fig 1).
This is a novel observation revealing a systematic way
arXiv:2301.12923v1  [cs.LG]  30 Jan 2023

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Figure 2. Effect of late loss-switching: We switch the loss during
training (in this case, in the last quarter of training) and ﬁnd that
switching to distillation (OneHot to KD line) recovers nearly all
the gains of distillation (KD). Thus, the initial phase of training is
not critical for distillation to help here. See §3.2 for details.
in which students deviate from their teachers.
(ii) Which deviations matter for generalization? We em-
pirically argue (§3.2) that other student-teacher devia-
tions unique to the early phase of neural network train-
ing – such as those proposed in Allen-Zhu & Li (2020);
Jha et al. (2020) – are not by themselves adequate to
explaining the success of distillation, but the underﬁt-
ting may be. We infer this from the fact that switching
from one-hot loss in the middle of training to distillation
loss can (a) recover a considerable fraction of distilla-
tion’s gains (Fig 2), (b) and also recover the ﬁnal-epoch
underﬁtting behavior on TinyImageNet and CIFAR100.
Next, we ask how deviations arise and why they help. For
this we provide two complementary theoretical views:
(iii) Eigenspace view: Akin to the seminal non-gradient-
descent result of Mobahi et al. (2020), we prove for
linear regression that distillation exaggerates the (well-
understood) beneﬁcial implicit bias of early-stopped
gradient ﬂow in converging faster along the top data
eigendirections (Theorem 4.1). Furthermore, we empir-
ically verify this exaggerated bias in more general set-
tings, including a multi-layer perceptron (Fig 3, Fig 24).
Since top eigendirections are well-understood to be bet-
ter generalizing directions, we put forth the exaggerated
preference of these directions as a way of understand-
ing (a) why the student improves over its teacher and
(b) why the student exaggerates the teacher’s low conﬁ-
dence per Fig 1.
(iv) Gradient space view: As a complementary viewpoint,
we formalize how distillation can denoise gradients
in the presence of class similarities in the data (Theo-
rem 4.3) and thereby create beneﬁcial student-teacher
deviations that improve generalization. We also propose
0.0
0.5
1.0
1.5
2.0
Top e.v. [2]
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Bottom e.v [52]
0.25
0.50
0.75
1.00
Top e.v. [14]
0.1
0.2
0.3
0.4
0.5
0.6
Bottom e.v [44]
Teacher
Student
Figure 3. Verifying the regularization effect of distillation un-
der gradient descent: We consider a linear model trained on a
noisy MNIST-based dataset. Each plot shows the parameter tra-
jectory projected onto two randomly picked eigendirections; the ⋆
corresponds to the ﬁnal parameters. First, as expected, standard
GD training converges faster along the higher eigendirection: the
one-hot teacher does move faster towards its ﬁnal X axis value
than the ﬁnal Y axis value. But crucially, we ﬁnd that distillation
exaggerates this bias: the student moves even faster towards its ﬁ-
nal X axis value. If appropriately early-stopped, the student would
rely more than the teacher on the top direction. We argue this
manifests as the exaggerated student conﬁdence values in Fig 1.
See §4.1 for details.
this as a way to understand why a midway switch to
distillation can still be helpful. Importantly, unlike prior
work capturing the effect of class similarities (Menon
et al., 2021; Dao et al., 2021; Ren et al., 2022; Zhou
et al., 2021), we demonstrate our denoising effect even
in the absence of explicit label noise assumptions.
Finally, we provide a discussion of how both these views
can be used to understand intuitive concepts in distillation
literature, such as dark knowledge and early-stopping. Over-
all, in making the above contributions, our work also draws
connections between several disjoint ﬁndings, namely, the
empirically observed deviations in Stanton et al. (2021), the
theoretical effects of regularization in Mobahi et al. (2020)
and of class similarities in Menon et al. (2021). Thus, we
hope our paper paints a more coherent picture of the dis-
jointed state of the art in distillation theory and practice.
As a takeaway for practitioners, our ﬁndings suggest that
not matching the teacher probabilities exactly can be a good
thing, especially for low-conﬁdence points. Future work
may consider ﬁnding ways to explicitly induce such devi-
ations to exaggerate the beneﬁts of distillation. Our work
may also enable other algorithmic developments since it
brings theory and practice closer in multiple ways, as more
concretely discussed in the following subsection.
1.1. Bridging gap between theory and practice
Below, we explicate some key gaps between current distilla-
tion theory and practice that we help bridge:

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
1. Analyzing gradient descent: Our proofs crucially in-
volve analyzing gradient-based training. Existing in-
sights of the two lines of work of Mobahi et al. (2020)
and Menon et al. (2021) do not apply to gradient-based
learning, which is key to understanding how deep net-
works generalize. Although our formal results are for
linear models, we show how their insights carry over to
more general settings.
2. Experiments connecting to practice: We empirically
demonstrate the regularization effect (Fig 3, 24) and
draw connections to our empirically-observed under-
ﬁtting behavior. Such practical demonstrations of the
corresponding regularization effect from Mobahi et al.
(2020) have so far not been shown.
3. Signiﬁcance of early-stopping: Our theory solidiﬁes
(and clariﬁes) the role of early-stopping, which has time
and again been discussed empirically as an important
factor in distillation (Dong et al., 2019; Cho & Hariha-
ran, 2019; Ji & Zhu, 2020; Wang et al., 2022a).
2. Background and Notation
Our interest in this paper is multiclass classiﬁcation prob-
lems.
This involves learning a classiﬁer h: X →Y
which, for input x ∈X, predicts the most likely label
h(x) ∈Y = [K]
.= {1, 2, . . . , K}. Such a classiﬁer is
typically implemented by computing logits f : X →RK
that score the plausibility of each label, and then computing
h(x) = argmaxy∈Y fy(x). In neural models, these logits
are parameterised as f(x) = W⊤Z(x) for learned weights
W ∈RD×K and embeddings Z(x) ∈RD. One may learn
such logits by minimising the empirical loss on a training
sample S
.= {(xn, yn)}N
n=1:
Remp(f)
.= 1
N
X
n∈[N]
e(yn)⊤ℓ(f(xn)),
(1)
where e(y) ∈{0, 1}K denotes the one-hot encoding of y,
ℓ(·)
.= [ℓ(1, ·), . . . , ℓ(K, ·)] ∈RK denotes the loss vector
of the predicted logits, and each ℓ(y, f(x)) is the loss of
predicting logits f(x) ∈RK when the true label is y ∈
[K]. Typically, we set ℓto be the softmax cross-entropy
ℓ(y, f(x)) = −log py(x), where p(x) ∝exp(f(x)) is the
softmax transformation of the logits.
Equation 1 guides the learner via one-hot targets e(yn) for
each input. Distillation (Bucilˇa et al., 2006; Hinton et al.,
2015) instead guides the learner via a target label distri-
bution pte(xn) provided by a teacher, which are the soft-
max probabilities from a distinct model trained on the same
dataset. In this context, the learned model is referred to as a
student, and the training objective is
Rdist(f)
.= 1
N
X
n∈[N]
pte(xn)⊤ℓ(f(xn)).
(2)
(a) Cross-architecture
(b) Language data
Figure 4. Teacher-student logit-transformed probability plots
We ﬁnd that underﬁtting of teacher’s low-conﬁdence points (as
shown in Figure 1) is not unique to self-distillation on image
datasets, and holds for other scenarios too.
One may also consider a weighted combination of Remp
and Rdist , but we focus on the above objective since we are
interested in understanding each objective individually.
Compared to training on Remp, distillation often results in
improved performance for the student (Hinton et al., 2015).
Typically, the teacher model is of higher capacity than the
student model; the performance gains of the student may
thus informally be attributed to the teacher transferring rich
information about the problem to the student. In such set-
tings, distillation may be seen as a form of model com-
pression. Intriguingly, however, even when the teacher and
student are of the same capacity (a setting known as self-
distillation), one may see gains from distillation (Furlanello
et al., 2018; Zhang et al., 2019). The questions we explore
in this paper are motivated by the self-distillation setting;
however, for a well-rounded analysis, we empirically study
both the self- and cross-architecture-distillation settings.
3. A ﬁne-grained look at teacher-student
deviations
Stanton et al. (2021) found that, contrary to the premise
of distillation, more accurate students are poorer at match-
ing the teacher probabilities. They quantiﬁed the student-
teacher deviation by measuring the disagreement or the KL
divergence between the student and teacher probabilities,
in expectation over all points. Could the average deviation
in probabilities be attributed merely to an arbitrary lack of
precision in matching the probabilities during training? In
the following section, through a closer study of per-sample
relationship between the teacher and student predictions, we
do not ﬁnd this to be the case; rather, there are discernible
patterns in student-teacher deviations.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
3.1. What deviations exist: A per-sample view
Setup. Suppose we have teacher and distilled student mod-
els f te, f st : X →RK respectively. We seek to analyze
the deviations in the corresponding predicted probability
vectors pte(x) and pst(x) for each (x, y) in the train and
test set, rather than in the aggregated sense as in Stan-
ton et al. (2021). To visualize the deviations, we need
a scalar summary of these vectors.
An obvious candi-
date is the probabilities (pte
y⋆(x), pst
y⋆(x)) assigned to the
ground truth label y⋆. However, since the student does not
have access to the ground truth label, and is only trying to
mimic the teacher, we examine deviations of probabilities of
the teacher’s predicted label, i.e., (pte
yte(x), pst
yte(x)) where
yte
.= argmaxy′∈[K] pte
y′(x). To make patterns easier to
detect, we further perform a monotonic logit transforma-
tion φ(u) = log [u/(1 −u)] that produces real-values in
(−∞, +∞). Thus, we compare φ(pte
yte(x)) and φ(pst
yte(x))
for each train and test sample (x, y).
We report a scatter plot of φ(pte
yte(x)) (X-axis) vs.
φ(pst
yte(x)) (Y -axis) on the training set for some repre-
sentative self-distillation settings in Figures 1 and cross-
architecture distillation settings in Fig 4a. In all plots, the
dashed line indicates the X = Y line. All values are com-
puted at the end of training. The tasks considered include
image classiﬁcation benchmarks, namely CIFAR10, CIFAR-
100, Tiny-ImageNet, and text classiﬁcation tasks from the
GLUE benchmark (e.g., MNLI (Williams et al., 2018), AG-
News (Zhang et al., 2015). See §C.1 for details on the ex-
perimental hyperparameters. Additional plots for all other
settings and for the test data are presented in §C.2.1.
Distilled students tend to underﬁt low-conﬁdence sam-
ples. Our main ﬁnding is that in most of our cases, there is
a distinctive subset of points where (a) the teacher has as-
signed low probability to its top class (i.e., low conﬁdence),
and (b) the student achieves even lower probability on that
class. In the plots, this visually corresponds to points where
the X axis value is small and Y ≤X. We note that this
pattern is even stronger on test data (see §C.2.1 for test
data plots); the underﬁtting appears in the test data even in
certain exceptions where it does not appear in the training
data e.g., CIFAR-10 plots in Fig 10. In §C.2.1, we also
discuss the exceptions where these patterns fail to appear
e.g., in cross-architecture language settings. We also con-
duct ablation studies in §C.3 and take a closer look at the
underﬁt points in §C.2.2. But overall, the majority trend is
that the student exaggerates the teacher’s conﬁdence levels
particularly on low-conﬁdence points.
3.2. Which deviations matter: A study of loss-switching
Motivation. Another kind of student-teacher deviation that
prior work has looked at are deviations in the training tra-
Figure 5. Results of loss-switching: We report the performance
of the teacher, the one-hot student (Standalone; same as teacher
in self-distillation), and distilled student (denoted by KD upto T
steps where T) and late-distilled student (denoted by KD from
Tswitch step). We ﬁnd that switching to distillation in the middle
can recover large fractions of the gains of regular distillation.
jectory that emerge very early on during neural network
training: these deviations perhaps condition the network
into a more favorable representation (Allen-Zhu & Li, 2020)
or a basin in the non-convex loss (Jha et al., 2020). In this
section, we examine the relevance of these early-phase devi-
ations to the gains of distillation, and also on the probability
deviations observed in the previous section.
Experiment design. To test the relevance of early-phase
deviations, we propose late distillation where we train a
network with one-hot loss for a fair part (say, half) of train-
ing, and then switch to distillation loss. If distillation relied
crucially on early-phase deviations, late distillation should
not result in substantial improvements over one-hot training.
Observations. We make three key observations here. First,
from Figures 5 (and §C.3.1 Fig 19), we observe that replac-
ing the ﬁrst 1/4th of training with one-hot is able to recover
all of distillation’s gains in our CIFAR100 and TinyIma-
genet settings; replacing the ﬁrst 1/2 of training, recovers a
signifcant fraction of the gains. This happens despite the fact
that when we switch to distillation, we have a much smaller
learning rate (due to our chosen schedule, see §C.1). For CI-
FAR100 in particular, we are able to begin self-distillation
even after 95% of training is complete, and still expect to see
strong gains. This implies that early phase deviations are
not as critical for distillation to help as previously thought.
Conversely, we also switch from distillation to one-hot in
Fig 2 and Fig 21. Here we ﬁnd that switching to one-hot un-
does a considerable fraction of the gains that distillation has
achieved thus far, given a sufﬁciently long one-hot training.
This suggests an inherent destructive effect in one-hot gradi-
ents that we formalize in §4.2. We provide a more nuanced
discussion of these experiments in §C.3.1 (and Fig 20) of

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
(a) One-hot and self-distillation.
(b) Loss-switching to distillation/one-hot at 15k steps.
Figure 6. Evolution of teacher-student logit plots over various steps of training for CIFAR100 ResNet56 self-distillation setup: On
the left, we present plots for one-hot training (top) and distillation (bottom). On the right, we present similar plots for experiments from
Section 3.2, with the loss switched to distillation (top) and one-hot (bottom) at 15k steps. From the last two plots in these two, we ﬁnd
that switching to distillation “unlearns” low-conﬁdence points, while switching to one-hot “ﬁxes” the underﬁtting.
interest to practitioners.1
For our ﬁnal observation, we examine how the underﬁtting
behavior of distillation evolves under loss-switching. In
Fig 6b, we plot a series of scatter plots in the style of §3.1,
where the X axis is the teacher’s ﬁnal top class probabil-
ity on a point, and the Y axis is the student’s probability
taken as a snapshot at some intermediate timestep. Here,
we ﬁnd that even though early-phase one-hot training has
already ﬁt all examples to a certain extent, we surprisingly
ﬁnd that a subsequent switch to distillation “unlearns” the
low-conﬁdence examples to force the typical underﬁtting
behavior from §3.1. Conversely, switching to one-hot ﬁxes
the underﬁtting induced by an initial distillation phase.
In summary, we argue that these experiments emphasize
that the underﬁtting phenomenon from §3.1 must be fun-
damental to the behavior of distillation, while early-phase
student-teacher deviations are not critical.
4. Why student-teacher deviations arise and
why they help: a formal study
To understand the above observations on underﬁtting and
loss-switching, and their connections to the beneﬁts of dis-
tillation, we provide two complementary perspectives of
distillation: an eigenspace perspective and a gradient-space
perspective. We use these perspectives to also paint a more
coherent picture of disjoint threads of theoretical research.
1Note that switching to one-hot does not as aggressively undo
the gains of distillation when compared to the converse scenario
where a switch to distillation aggressively improves over one-hot
training.
4.1. The eigenspace view: distillation as a regularizer
The underﬁtting of low-conﬁdence points indicates that dis-
tillation induces a form of regularization. Indeed, Mobahi
et al. (2020) have proven that self-distillation loss acts as a
regularizer that focuses on simpler basis functions. How-
ever, their result is not in a GD setting. We bridge the gap
between this intuition and practice (a) by proving that a
similar effect emerges under gradient-ﬂow training of linear
models and (b) by empirically demonstrating this effect in
more general settings.
Concretely, we analyze a continuous-ﬂow GD model on
a linear regression setting with early-stopping, a typical
design choice in distillation practice (Dong et al., 2019;
Cho & Hariharan, 2019; Ji & Zhu, 2020). Consider an
n × p dataset X (where n is the number of samples, p the
number of parameters) with target labels y. Assume that
the Gram matrix XX⊤is invertible. Note that this setting
includes overparameterized scenarios (p > n) such as when
X corresponds to the linearized (NTK) features of neural
networks (Jacot et al., 2018; Lee et al., 2019). Then, a
standard calculation reveals that the weights learned at time
t under GD on the loss (1/2) · ∥X −y∥2 can be written as:
β(t) = X⊤(XX⊤)−1A(t)y
(3)
where A(t) := I −e−tXX⊤.
(4)
Intuitively, A is a “sparsifying” matrix that skews down
the weight assigned to an eigendirection of eigenvalue λ
by the value 1 −e−λt. As t →∞this factor goes to
1 for all directions, thus becoming irrelevant; but for any
early-stopped ﬁnite time t, the topmost direction would
have a larger factor than the rest, thus producing some sort
of sparsiﬁcation. Our argument is that, distillation further
exaggerates this implicit bias. Consider a setting where the
teacher is trained to time T te. Through a simple calculation,

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
the student’s weights can be written as:
˜β(˜t) = X⊤(XX⊤)−1 ˜A(˜t)y
(5)
where ˜A(˜t) := A(t)A(T te).
(6)
One can then argue that the sparsiﬁer ˜A corresponding to
the student is more skewed towards the top eigenvectors
than the teacher. More formally:
Theorem 4.1. Let αk(t), ˜αk(t) be the eigenvalues of the
k’th eigendirection in A(t) and ˜A(t) respectively. Let λk
be the corresponding eigenvalue of the Gram matrix XX⊤.
For any two indices k1 < k2 such that λk1 > λk2 (assuming
such a pair exists), and for any α⋆
k1 ∈(0, 1) (a required
“convergence lower bound” on the higher eigenvector), there
exists a choice of distillation hyperparameters T te, ˜t such
that (a) ˜αk1(˜t) ≥α⋆
k1, and (b) for any choice of time t
for the standalone early-stopped model satisfying αk1(t) ≥
α⋆
k1, the components along the lower eigenvectors satisfy:
˜αk2(˜t)
| {z }
Student’s component
along lower eigenvector
<
αk2(t).
| {z }
Teacher’s component
along lower eigenvector
(7)
The result roughly says that the student relies less on the
bottom directions than the teacher, if we compare them at
any instant when they have both converged equally well
along the top directions.
Remark 4.2. Our result brings out an important nuance
regarding early-stopping. Mobahi et al. (2020) argue that
early-stopping and distillation have opposite regularization
effects, wherein the former has a densifying effect while the
latter a sparsifying effect. We clarify that this holds only
in their non-GD setting where the function is picked from
a Hilbert space with ℓ2 regularization. In GD settings, we
argue that distillation ampliﬁes the effect of early stopping,
rather than oppose it.
Empirical verﬁcation of exaggerated bias. While our the-
ory applies to a linear regression setting with gradient ﬂow,
we now verify our insights in more general settings where
we use GD with cross-entropy loss on (non-)linear mod-
els. In Fig 23 (a subset of which was shown in Fig 3)
and Fig 24, we plot the trajectories of the weights of a lin-
ear model and the ﬁrst-layer weights of a MLP in some
MNIST-based datasets (see §C.4 for details). We visualize
this by projecting the trajectories onto two randomly picked
eigendirections of the data. First, we ﬁnd (as is conven-
tionally expected) that the one-hot trained teacher shows
a bias towards converging faster along the top directions.
But crucially, we ﬁnd that the distilled student has a more
exaggerated version of this bias, leading it to traverse a
different part of the parameter space with greater reliance
on the top directions. Notably, we also ﬁnd underﬁtting of
low-conﬁdence points in this setting (Fig 22).
(a) Self-distillation
(b) Cross-architecture
Figure 7. Teacher-student logit-transformed probability plots
under explicit label noise: We ﬁnd that of all the teacher’s low
conﬁdence points, the student underﬁts the points that are misla-
beled.
Connection to underﬁtting. We argue that the student’s
exaggeration of the one-hot trained teacher’s implicit bias
must translate to the student’s exaggeration of the teacher’s
conﬁdence levels as seen in §3.1. This connection is best ar-
gued by considering a setting where a portion of CIFAR100
one-hot labels are mislabeled. First, it is well-known that
the implicit bias of early-stopped GD ﬁts the noisy subset
of the data more slowly than the clean data, because noisy
labels correspond to bottom eigendirections (Li et al., 2020;
Dong et al., 2019; Arpit et al., 2017; Kalimeris et al., 2019).
In Fig 7, we indeed ﬁnd that mislabeled points have the
smallest teacher probabilities (small X axis values).
Our theory then suggests that the distilled student must rely
even less on the lower eigendirections, implying an even
poorer ﬁt of the mislabeled datapoints (i.e., Y < X in the
plots). Indeed, we ﬁnd that of all the points that the teacher
has low conﬁdence on (which sometimes includes clean
data as well), the student underﬁts all the mislabeled data —
which must naturally help generalization. Note that underﬁt-
ting may not always correspond to improved generalization
(cf. §C.2.1, Table 2), just like regularization of any form
may not always be helpful. However, the exaggerated im-
plicit bias of distillation and its associated underﬁtting has a
beneﬁcial denoising effect at least under explicit label noise.
4.2. The gradient-space view: distillation as a gradient
denoiser
We now argue how distillation can have a denoising effect
even in the absence of explicit label noise, and can lead to
beneﬁcial student-teacher deviations in the gradients. Our
key argument is that such a denoising effect can emerge
when there are class similarities in the data. While prior
work has formalized the effect of class similarities in distilla-
tion (Menon et al., 2021; Dao et al., 2021; Ren et al., 2022),
they (a) do not consider GD, (b) assume that the one-hot

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
labels are noisy samples of a soft ground truth labeling and
most importantly, (c) assume the student does not deviate
from the teacher. The highlight of our argument is that it
demonstrates how denoising occurs even in a perfectly clas-
siﬁable dataset – where prior insights fall apart – and how
the student can deviate from and supersede the teacher.
A concrete example. While we provide a more general
intuitive argument later, we will consider a concrete linear
example to build our case formally. Consider a K-class
classiﬁcation dataset, where the ith datapoint’s features can
be written as a K-“channel” input xi = (x(i)
1 , . . . , x(i)
K )
where each channel x(i)
k
∈RD is D-dimensional. Assum-
ing a uniform distribution over K classes, given a label
y, we generate xy from a Gaussian N(0, I/D) truncated
to the support xy · µ⋆
yi = τ for a pre-deﬁned τ > 0
and a “ground truth” class vector µ⋆
k. We also assume
that for point xi, we pick a non-target “similar” class zi
for which we pick xzi from a Gaussian N(0, I/D) trun-
cated to the support xzi · µ⋆
zi = τ/2.
All other co-
ordinates are zero for xi. We consider a simpliﬁed linear
architecture whose K-dimensional output is of the form
f(x) = (w1 · x1, . . . , wK · xK) where the k’th node acts
only on the k’th channel.
Observe that this is a perfectly classiﬁable dataset with
deterministic labels since if we set the model weights to
be wk = µ⋆
k, we obtain that for the corresponding logits
f ⋆, arg maxk f ⋆
k(x) = y⋆. In other words, the ground
truth probabilities are one-hot, which can be recovered by
computing softmax({α · fk(x)}K
k=1) as α →∞. Thus, it
may seem wisest to chose to train on the one-hot loss here.
However, we show here that gradients under self-distillation
are more optimally aligned with ground truth than (a) the
one-hot gradients and the (b) teacher’s weights themselves,
thus proving that the student can deviate from and improve
upon the teacher. To formalize this, let cos-sim(·, ·) denote
the cosine-similarity between two vectors. Then:
Theorem 4.3.
(informal; see Thm B.4) Consider a stu-
dent whose weights satisfy wk = αµ⋆
k. Consider an im-
perfect teacher with weights such that ∥wte
k ∥= αte
k and
cos-sim(wte
k , µ⋆
k)· ≥1 −ϵ. Let vk be the one-hot update
on node k of the student, and ˜vk the distillation update,
both under cross-entropy loss. Then, for α ≪αte and
αte ≤O( 1
τ log K), and sufﬁciently large τ, D:
cos-sim(˜vk, µ⋆
k)
|
{z
}
Quality of distillation update
> max

cos-sim(vk, µ⋆
k)
|
{z
}
Quality of one-hot update
,
cos-sim(wte
k , µ⋆
k)
|
{z
}
Quality of teacher weights

(8)
This result shows that, under class similarities, one-hot gra-
dients can be inherently sub-optimal at any timestep in train-
ing even if the model is already initialized at an optimal
solution. Conversely, distillation can help improve these
gradients even if the supervising teacher is imperfect.
Generality of this insight. While we cross-verify the above
concrete example empirically in §C.5, we also intuitively ar-
gue why these insights hold in any generic setting. Imagine
in a generic setting, that at some point of training, we have
recovered a good f such that f ≈α · f ⋆(for some ﬁnite scal-
ing factor α). Now as we train longer, we ideally hope that
(a) α continues increasing towards ∞to recover the one-hot
ground truth probabilities, and (b) the approximation of the
ground truth logits continues to hold throughout.
Counterintuitively, this is not the case with the one-hot
updates. Consider any point (x, y) where f ⋆
k(x) is high for
some k ̸= y i.e., the point has similarities to a non-target
class. On such a point, the ideal dynamics of increasing α
requires that we increase the logit on the non-target node
fk(x). But the one-hot update would try to suppress this
logit, since the target probability for this node is 0. We argue
that this is a destructive gradient since it directs the model
in the opposite direction of the ideal dynamics.
Distillation on the other hand, can ﬂip the sign of these
gradients (assuming the teacher is early-stopped), thus de-
noising the gradients — even if there is no explicit noise
in the data. Notably, this sign-ﬂipping provides a “fresh
example” for the non-target node k, which is a privilege
unavailable to the teacher. This is crucial to prove that the
student is more denoised than the the teacher.
Connection to loss-switching. The gradient-space view
gives us a direct way to make sense of the ﬁndings in §3.2.
In particular, this tells us why each gradient in itself may
be constructive (as in the case of distillation) or destructive
(in the case of one-hot). Thus, switching to distillation (or
one-hot) even in the middle of training can still be helpful
(or hurtful), given a sufﬁciently early switch.
4.3. Connecting the two perspectives
We now discuss how various themes in existing distillation
theory can be understood within both of the above per-
spectives. Through this, we hope to piece together a more
coherent and uniﬁed picture of the related works of Mobahi
et al. (2020) and Menon et al. (2021).
Dark knowledge and denoising. Distillation is said to
beneﬁt by implicitly introducing “dark knowledge” in the
form of new information absent in the observed labels of
the original datasets (Hinton et al., 2015). We view dark
knowledge as recovering denoised features:
(i) In the gradient-space view, the dark knowledge comes
from the teacher’s non-target logits (unavailable in one-
hot training), which allows distillation to denoise the

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
gradients.
(ii) In the eigenspace view, dark knowledge is transferred
via the top eigenvector(s). Crucially, these eigenvec-
tors are independent of (potentially noisy) labels; hence
these correspond to “denoised” features in the data,
which distillation more readily relies on.
The gap between early-stopping and distillation. We see
a subtle distinction between early-stopping and distillation:
(i) In the eigenspace perspective, although both induce a
sparsiﬁcation effect, distillation ampliﬁes this effect.
(ii) In the gradient-space view, early-stopping the teacher
would help by reduce the overall magnitude of destruc-
tive one-hot gradients accumulated over time. Distil-
lation substantially ampliﬁes this effect by altogether
ﬂipping the sign of these gradients.
5. Relation to Existing Work
Distillation as a probability matching process. Distilla-
tion has been touted to be a process that beneﬁts from match-
ing the teacher’s probabilities (Hinton et al., 2015). Indeed,
many distillation algorithms have been designed in a way
to more aggressively match the student and teacher func-
tions (Czarnecki et al., 2017; Beyer et al., 2022). Theoretical
analyses too rely on explaining the beneﬁts of distillation
based on a student that obediently matches the teacher’s
probabilities (Menon et al., 2021). But, building on Stanton
et al. (2021), our work demonstrates why we may desire
that the student deviate from the teacher.
Theories of distillation. A long-standing intuition for why
distillation helps is that the teacher’s probabilities contain
“dark knowledge” about class similarities (Hinton et al.,
2015; Müller et al., 2019), As discussed in §4.2, several
works (Menon et al., 2021; Dao et al., 2021; Ren et al.,
2022; Zhou et al., 2021) have formalized these similarities
via inherently noisy class memberships. However, some
works (Furlanello et al., 2018; Yuan et al., 2020; Tang et al.,
2020) have argued that this hypothesis cannot be the sole
explanation, because distillation can help even if the student
is only taught information about the target probabilities (e.g.,
by smoothing out all non-target probabilities).
This has resulted in various alternative hypotheses. Some
have proposed faster convergence (Phuong & Lampert,
2019; Rahbar et al., 2020; Ji & Zhu, 2020) which only
explains why the student would converge fast to teacher, but
not why it may deviate from and supersede a one-hot teacher.
Another hypothesis is better feature learning (Allen-Zhu
& Li, 2020), (a) which would not apply in our linear set-
tings and (b) which we question in §3.2. Another line of
work casts distillation as a regularizer, either in the sense
of Mobahi et al. (2020) or in the sense of instance-speciﬁc
label smoothing (Zhang & Sabuncu, 2020; Yuan et al., 2020;
Tang et al., 2020). Finally, we also refer the reader to Lopez-
Paz et al. (2016); Kaplun et al. (2022); Wang et al. (2022b)
who theoretically study distillation in orthogonal settings.
Early-stopping and knowledge distillation.
Early-
stopping has received much attention in the context of distil-
lation (Liu et al., 2020; Ren et al., 2022; Dong et al., 2019;
Cho & Hariharan, 2019; Ji & Zhu, 2020). We closely build
on Dong et al. (2019), who argue how early-stopping a GD-
trained teacher can automatically denoise the labels due to
regularization in the eigenspace. However, neither Dong
et al. (2019) nor any of the other works provide a formal
argument for why distillation can outperform the teacher.
Empirical studies of distillation.
Our study crucially
builds on observations from (Stanton et al., 2021; Lukasik
et al., 2021) demonstrating student-teacher deviations in
an aggregated sense than in a sample-wise sense. Other
studies (Abnar et al., 2020; Ojha et al., 2022) investigate in
what ways the student is similar to the teacher in terms of
out-of-distribution behavior, calibration, and so on. Deng &
Zhang (2021) show how a smaller student can outperform
the teacher if it was allowed to better match the teacher on
more data, which is orthogonal to our setting.
6. Discussion and Future Work
We highlight the key insights from our work valuable for
future research in distillation practice:
1. Not matching the teacher probabilities exactly can be a
good thing. Perhaps encouraging underﬁtting of teach-
ers’ low-conﬁdence points can further exaggerate the
beneﬁts of the regularization effect.
2. It is possible to run distillation only for a part of training
without damaging its gains. A more systematic study
of loss-scheduling may give us an approach that uses
computationally-expensive teacher annotations only for
a part of training.
3. One-hot gradients on non-target nodes can be actively
destructive. Simply zeroing them out strategically may
yield a teacher-free way to obtaining some beneﬁts of
distillation.
We also highlight a few theoretical directions for future
work. First, it would be valuable to extend our eigenspace
view to multi-layered models where the eigenspace regular-
ization effect may “compound” across layers. Furthermore,
one could explore ways to exaggerate the regularization ef-
fect in our simple linear setting and then extend the idea to a
more general distillation approach. Finally, it would be prac-
tically useful to extend these insights to semi-supervised dis-
tillation (Cotter et al., 2021), non-classiﬁcation settings such
as ranking models (Hofstätter et al., 2020), or intermediate-

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
layer-based distillation (Romero et al., 2015).
References
Abnar, S., Dehghani, M., and Zuidema, W. H.
Trans-
ferring inductive biases through knowledge distillation.
abs/2006.00555, 2020. URL https://arxiv.org/
abs/2006.00555.
Allen-Zhu, Z. and Li, Y. Towards understanding ensem-
ble, knowledge distillation and self-distillation in deep
learning. CoRR, abs/2012.09816, 2020. URL https:
//arxiv.org/abs/2012.09816.
Anil, R., Pereyra, G., Passos, A., Ormandi, R., Dahl, G. E.,
and Hinton, G. E. Large scale distributed neural net-
work training through online distillation. In International
Conference on Learning Representations, 2018.
Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio,
E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville,
A. C., Bengio, Y., and Lacoste-Julien, S. A closer look
at memorization in deep networks. In Proceedings of
the 34th International Conference on Machine Learning,
ICML 2017, Proceedings of Machine Learning Research.
PMLR, 2017.
Beyer, L., Zhai, X., Royer, A., Markeeva, L., Anil, R., and
Kolesnikov, A. Knowledge distillation: A good teacher is
patient and consistent. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 10925–10934, June 2022.
Bucilˇa, C., Caruana, R., and Niculescu-Mizil, A. Model
compression. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, KDD ’06, pp. 535–541, New York, NY,
USA, 2006. ACM.
Cho, J. H. and Hariharan, B. On the efﬁcacy of knowledge
distillation. In 2019 IEEE/CVF International Conference
on Computer Vision (ICCV), pp. 4793–4801, 2019.
Cotter, A., Menon, A. K., Narasimhan, H., Rawat, A. S.,
Reddi, S. J., and Zhou, Y. Distilling double descent.
CoRR, abs/2102.06849, 2021. URL https://arxiv.
org/abs/2102.06849.
Czarnecki, W. M., Osindero, S., Jaderberg, M., Swirszcz,
G., and Pascanu, R. Sobolev training for neural networks.
In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H.,
Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Ad-
vances in Neural Information Processing Systems 30, pp.
4278–4287. Curran Associates, Inc., 2017.
Dao, T., Kamath, G. M., Syrgkanis, V., and Mackey, L.
Knowledge distillation as semiparametric inference. In
International Conference on Learning Representations,
2021.
Deng, X. and Zhang, Z. Can students outperform teach-
ers in knowledge distillation based model compression?,
2021. URL https://openreview.net/forum?
id=XZDeL25T12l.
Dong, B., Hou, J., Lu, Y., and Zhang, Z.
Distillation
≈early stopping? harvesting dark knowledge utilizing
anisotropic information retrieval for overparameterized
neural network, 2019.
Furlanello, T., Lipton, Z. C., Tschannen, M., Itti, L., and
Anandkumar, A. Born-again neural networks. In Proceed-
ings of the 35th International Conference on Machine
Learning, ICML 2018, pp. 1602–1611, 2018.
He, K., Zhang, X., Ren, S., and Sun, J.
Identity map-
pings in deep residual networks. In Leibe, B., Matas,
J., Sebe, N., and Welling, M. (eds.), Computer Vi-
sion - ECCV 2016 - 14th European Conference, Am-
sterdam, The Netherlands, October 11-14, 2016, Pro-
ceedings, Part IV, volume 9908 of Lecture Notes in
Computer Science, pp. 630–645. Springer, 2016a. doi:
10.1007/978-3-319-46493-0\_38. URL https://doi.
org/10.1007/978-3-319-46493-0_38.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual
learning for image recognition. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
2016b.
Hinton, G. E., Vinyals, O., and Dean, J. Distilling the
knowledge in a neural network. CoRR, abs/1503.02531,
2015.
Hofstätter, S., Althammer, S., Schröder, M., Sertkan, M.,
and Hanbury, A. Improving efﬁcient neural ranking mod-
els with cross-architecture knowledge distillation. 2020.
URL https://arxiv.org/abs/2010.02666.
Jacot, A., Hongler, C., and Gabriel, F. Neural tangent kernel:
Convergence and generalization in neural networks. pp.
8580–8589, 2018.
Jafari, A., Rezagholizadeh, M., Sharma, P., and Ghodsi, A.
Annealing knowledge distillation. In Proceedings of the
16th Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume, pp.
2493–2504, Online, April 2021. Association for Compu-
tational Linguistics. doi: 10.18653/v1/2021.eacl-main.
212. URL https://aclanthology.org/2021.
eacl-main.212.
Jha, N. K., Saini, R., and Mittal, S. On the demystiﬁcation
of knowledge distillation: A residual network perspective.
2020.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Ji, G. and Zhu, Z. Knowledge distillation in wide neural net-
works: Risk bound, data efﬁciency and imperfect teacher.
In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.,
and Lin, H. (eds.), Advances in Neural Information Pro-
cessing Systems, 2020.
Kalimeris, D., Kaplun, G., Nakkiran, P., Edelman, B. L.,
Yang, T., Barak, B., and Zhang, H. SGD on neural net-
works learns functions of increasing complexity. In Ad-
vances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, pp. 3491–3501, 2019.
Kaplun, G., Malach, E., Nakkiran, P., and Shalev-Shwartz,
S. Knowledge distillation: Bad models can be good role
models. CoRR, 2022.
Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Novak, R.,
Sohl-Dickstein, J., and Pennington, J. Wide neural net-
works of any depth evolve as linear models under gradient
descent. pp. 8570–8581, 2019.
Li, M., Soltanolkotabi, M., and Oymak, S. Gradient descent
with early stopping is provably robust to label noise for
overparameterized neural networks. In The 23rd Interna-
tional Conference on Artiﬁcial Intelligence and Statistics,
AISTATS 2020, Proceedings of Machine Learning Re-
search. PMLR, 2020.
Liu, S., Niles-Weed, J., Razavian, N., and Fernandez-
Granda, C. Early-learning regularization prevents mem-
orization of noisy labels. In Advances in Neural Infor-
mation Processing Systems 33: Annual Conference on
Neural Information Processing Systems 2020, NeurIPS
2020, 2020.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov,
V.
Roberta: A robustly optimized BERT pretraining
approach. CoRR, 2019. URL http://arxiv.org/
abs/1907.11692.
Lopez-Paz, D., Schölkopf, B., Bottou, L., and Vapnik, V.
Unifying distillation and privileged information. In Inter-
national Conference on Learning Representations (ICLR),
November 2016.
Lukasik, M., Bhojanapalli, S., Menon, A. K., and Kumar,
S. Teacher’s pet: understanding and mitigating biases in
distillation. CoRR, abs/2106.10494, 2021. URL https:
//arxiv.org/abs/2106.10494.
Menon, A. K., Rawat, A. S., Reddi, S. J., Kim, S., and
Kumar, S. A statistical perspective on distillation. In
Proceedings of the 38th International Conference on Ma-
chine Learning, ICML 2021, volume 139 of Proceedings
of Machine Learning Research, pp. 7632–7642. PMLR,
2021.
Mobahi, H., Farajtabar, M., and Bartlett, P. L.
Self-
distillation ampliﬁes regularization in hilbert space. In
Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, 2020.
Müller, R., Kornblith, S., and Hinton, G. E. When does
label smoothing help? In Advances in Neural Information
Processing Systems 32, pp. 4696–4705, 2019.
Ojha, U., Li, Y., and Lee, Y. J. What knowledge gets distilled
in knowledge distillation? 2022. doi: 10.48550/arXiv.
2205.16004. URL https://doi.org/10.48550/
arXiv.2205.16004.
Park, W., Kim, D., Lu, Y., and Cho, M. Relational knowl-
edge distillation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition
(CVPR), June 2019.
Phuong, M. and Lampert, C. Towards understanding knowl-
edge distillation. In Proceedings of the 36th International
Conference on Machine Learning, pp. 5142–5151, 2019.
Radosavovic, I., Dollár, P., Girshick, R. B., Gkioxari, G.,
and He, K. Data distillation: Towards omni-supervised
learning. In 2018 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2018, Salt Lake City, UT,
USA, June 18-22, 2018, pp. 4119–4128, 2018.
Rahbar, A., Panahi, A., Bhattacharyya, C., Dubhashi, D.,
and Chehreghani, M. H. On the unreasonable effective-
ness of knowledge distillation: Analysis in the kernel
regime, 2020.
Ren, Y., Guo, S., and Sutherland, D. J. Better supervisory
signals by observing learning paths. In The Tenth Inter-
national Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022. OpenReview.net,
2022.
Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta,
C., and Bengio, Y. Fitnets: Hints for thin deep nets.
In Bengio, Y. and LeCun, Y. (eds.), 3rd International
Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, 2015.
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and
Chen, L.-C. Mobilenetv2: Inverted residuals and linear
bottlenecks. In 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2018.
Stanton, S., Izmailov, P., Kirichenko, P., Alemi, A. A., and
Wilson, A. G. Does knowledge distillation really work?
In Advances in Neural Information Processing Systems
34: Annual Conference on Neural Information Processing
Systems 2021, NeurIPS 2021, 2021.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Tang, J., Shivanna, R., Zhao, Z., Lin, D., Singh, A., Chi,
E. H., and Jain, S. Understanding and improving knowl-
edge distillation. CoRR, abs/2002.03532, 2020.
Wang, C., Yang, Q., Huang, R., Song, S., and Huang, G.
Efﬁcient knowledge distillation from model checkpoints.
arXiv preprint arXiv:2210.06458, 2022a.
Wang, H., Lohit, S., Jones, M. J., and Fu, Y. What makes
a ”good” data augmentation in knowledge distillation
- a statistical perspective.
In Advances in Neural In-
formation Processing Systems, 2022b. URL https:
//openreview.net/forum?id=6avZnPpk7m9.
Williams, A., Nangia, N., and Bowman, S.
A broad-
coverage challenge corpus for sentence understanding
through inference. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers), pp. 1112–1122. As-
sociation for Computational Linguistics, 2018.
URL
http://aclweb.org/anthology/N18-1101.
Xie, Q., Hovy, E., Luong, M.-T., and Le, Q. V.
Self-
training with noisy student improves imagenet classiﬁca-
tion, 2019.
Yuan, L., Tay, F. E. H., Li, G., Wang, T., and Feng, J.
Revisiting knowledge distillation via label smoothing reg-
ularization. In 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 3902–3910. IEEE,
2020.
Zhang, L., Song, J., Gao, A., Chen, J., Bao, C., and Ma,
K. Be your own teacher: Improve the performance of
convolutional neural networks via self distillation. In
2019 IEEE/CVF International Conference on Computer
Vision, ICCV 2019, 2019.
Zhang, X., Zhao, J. J., and LeCun, Y. Character-level convo-
lutional networks for text classiﬁcation. In NIPS, 2015.
Zhang, Z. and Sabuncu, M. R. Self-distillation as instance-
speciﬁc label smoothing. In Larochelle, H., Ranzato, M.,
Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in
Neural Information Processing Systems, 2020.
Zhou, H., Song, L., Chen, J., Zhou, Y., Wang, G., Yuan,
J., and Zhang, Q. Rethinking soft labels for knowledge
distillation: A bias-variance tradeoff perspective. In 9th
International Conference on Learning Representations,
ICLR 2021, 2021.
Zhou, Z., Zhuge, C., Guan, X., and Liu, W. Channel dis-
tillation: Channel-wise attention for knowledge distil-
lation.
CoRR, abs/2006.01683, 2020.
URL https:
//arxiv.org/abs/2006.01683.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
A. Limitations
We highlight a few key limitations to our results that may be relevant for future work to look at:
1. While we show underﬁtting across many datasets, we do not exhaustively establish when this form of underﬁtting
can help in situations besides label noise. Indeed, we ﬁnd that, in our language datasets, underﬁtting can occur even
in the absence of any improvement in generalization. Future work may explore more sophisticated measures of
student-teacher deviation that correlates with the gains of distillation. We highlight further caveats of our empirical
results in §C.2.1.
2. Both our formalizations are based on linear models. On the one hand, this implies that our results are general and
fundamental in that they are not unique to neural network training. On the other, this means our result may not capture
other interesting effects in the context of distillation (such as that of feature-learning as in Allen-Zhu & Li (2020)).
Indeed, we hypothesize that the eigenspace regularization effect may compound across multiple layers of a neural
network. This is an open direction for future work.
3. While our gradient-space view suggests why switching to one-hot can hurt, it does not explain why switching to
one-hot for a very short amount of time increases the performance — even beyond distillation — in the speciﬁc case
of ResNet50 self-distillation (cf. §C.3.1 Fig 21 and Fig 20). Analyzing this nuanced effect of distillation-to-one-hot
switching is an interesting question for future work.
B. Eigenspace view
Below, we provide the proof for the eigenspace sparsity levels of an early-stopped teacher and early-stopped student
(Theorem 4.1). Recall that for any lower bound α⋆
k1 on the top eigenvector component, we want to show that (a) it
is possible to learn a student that has achieved this lower bound using appropriate early-stopping time (i.e., ∃˜t, T te for
which ˜αk1(˜t) ≥α⋆
k1), and (b) for any choice of the early-stopping time for the standalone model that achieves the
above lower bound (αk1(t) ≥α⋆
k1), the lower eigenvector component of the standalone is larger than that of the student
(αk2(t) > ˜αk2(˜t)).
Proof. (of Theorem 4.1) For the early-stopped model, we need 1 −e−λk1t ≥α⋆
k1. For any value of t that satisﬁes this, we
can lower bound αk2(t) as:
αk2(t) = 1 −e−λk2t
(9)
= 1 −
 e−λk1t
λk2
λk1
(10)
≥1 −
 1 −α⋆
k1

λk2
λk1 .
(11)
Now, recall that we need to show that there exists a set of distillation hyperparameters ˜t, T te that satisfy our required
bounds. For convenience, we will show that such a set of hyperparameters exist under the simpliﬁcation that T te = ˜t,
although it should be possible to prove these bounds for a wider range of values. Thus, for the student model, we need
(1 −e−λk1 ˜t)(1 −e−λk1 ˜t) ≥α⋆
k1. To achieve the largest skew, we should choose the smallest ˜t, which is achieved when
(1 −e−λk1 ˜t)2 = α⋆
k1. Plugging this into the expression for ˜αk2(˜t), we get:
˜αk2(˜t) = (1 −e−λk2 ˜t)2
(12)
=
 
1 −

e−λk1 ˜t λk2
λk1
!2
(13)
=

1 −(1 −
q
α⋆
k1)
λk2
λk1
2
.
(14)

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
For simplicity, let us deﬁne the fraction κ :=
λk2
λk1 , which is strictly positive and less than 1. Putting the above two sets of
equation together, we have:
αk2(t) −˜αk2(˜t) ≤1 −
 1 −α⋆
k1
κ −

1 −(1 −
q
α⋆
k1)κ2
(15)
≤1 −
 1 −α⋆
k1
κ −1 −((1 −
q
α⋆
k1)2)κ + 2(1 −
q
α⋆
k1)κ
(16)
≤2(1 −
q
α⋆
k1)κ −((1 −
q
α⋆
k1)2)κ −
 1 −α⋆
k1
κ
(17)
≤2 −(1 −
q
α⋆
k1)

(1 −
q
α⋆
k1))κ −

1 −
q
α⋆
k1
κ
(18)
≤0
(19)
The last step follows from the power means inequality: we have that since κ ∈(0, 1),

(1−√α⋆
k1))κ+(1+√α⋆
k1))κ
2
κ
is at
most

(1−√α⋆
k1))+(1+√α⋆
k1))
2

= 1.
B.1. Gradient-space view
In this section, we prove Theorem 4.3 where we analyzed the quality of one-hot and distillation gradients in a setting with
class similarity and zero label noise. We ﬁrst recall the setting below.
Setting. We consider a K-class classiﬁcation dataset, where the ith datapoint’s features can be written as a K-“channel”
input xi = (x(i)
1 , . . . , x(i)
K ) where each channel x(i)
k
∈RD is D-dimensional. Assuming a uniform distribution over K
classes, given a label y, we generate xy from a Gaussian N(0, I/D) truncated to the support xy · µ⋆
yi = τ for τ > 0 and
for a pre-deﬁned class vector µ⋆
k. We also assume that for point xi, we pick a non-target “similar” class zi for which we
pick xzi from a Gaussian N(0, I/D) truncated to the support xzi · µ⋆
zi = τ/2. All other co-ordinates are set to zero. We
consider a simpliﬁed linear architecture whose K-dimensional output is of the form f(x) = (w1 · x1, . . . , wK · xK) where
the k’th node acts only on the k’th channel.
Notations: Throughout the rest of the discussion, we ﬁx a node of interest k, and correspondingly deﬁne S(k)
+
= {(x, y) ∈
S|y = k} and S(k)
−
= {(x, y) ∈S|y ̸= k, z = k}. Intuitively, S(k)
+
is the subset of samples that provide “constructive”
gradients for node k under the one-hot loss, while S(k)
−
is the subset that provides “destructive” gradients under the one-hot
loss (i.e., gradients in the opposite direction of optimality). We will drop the superscript k where it is clear which node is
being referred to. With an abuse of notation, we will write x ∈S to denote an x such that (x, y) ∈S.
We will use pte to denote the probabilities of a “perfect” teacher with weights wk such that wk = αteµ⋆
k. We will also
consider an imperfect teacher model with weights wk such that ∥wk∥= αte and ∥w⊥
k ∥2
∥w∥
k∥2 ≤ϵte for some ϵte > 0.
Note that αte denotes the “scale” of the teacher’s logits — think of this as a proxy for how long the teacher has been run to
minimize one-hot loss, because as we train for inﬁnite time, αte →∞. Similarly, α denotes the scale of the student model’s
logits.
Finally, for any channel k, and for any vector w ∈RD of weights/gradients corresponding to that channel, we will use w∥
to denote the projection w∥= (w · µ⋆
k)µk, and w⊥to denote the orthogonal vector, w∥= w −(w · µ⋆
k)µk.
We now state some assumptions we make for the sake of convenience. The more important assumptions are stated within
the formal results themselves.
Assumption B.1. We make the following simplifying assumptions:
1. We assume that we have a training dataset S such that S = mK. The dataset is balanced in that2 for every k,
2Note that S
k S(k)
+
= S and S
k S(k)
−
= S.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
|S(k)
+ | = |S(k)
−| = m.
2. We assume ∥µ⋆
k∥= 1.
3. We assume ϵte < 1 and τ > 1.
4. We assume ϵte ≤
1
4αteτ .
Proof structure. In Lemma B.1, we show that as long as both the teacher and the student are not trained for too long (and
thus their scales αte and α are low), the (perfect) teacher’s supervision is “constructive” on node k for all points in both S(k)
+
(points of class k) and S(k)
−(the points similar to class k). This is crucial for us to show that distillation is able to denoise the
one-hot gradients (which are destructive on S(k)
−). In the subsequent Lemma B.2, we bound the difference in supervision of
a perfect and imperfect teacher. Using this, we can show that despite imperfection, the supervision of the teacher can be
constructive for node k on both S(k)
−and S(k)
+ .
In particular, we are also able to show that the distillation gradients can be superior to the teacher’s own weights. Intuitively,
imagine that the teacher has learned node k via constructive signals from S(k)
+ . Subsequently, this node will be able to
produce a non-trivial probability on node k even for points from S(k)
−. Thus, distillation using this teacher would provide
constructive signals to node k from a larger set of independent examples, namely S(k)
+
and S(k)
−, while the teacher did not
have access to the constructive gradients from S(k)
−.
We now state Lemma B.1 which (indirectly) proves that given sufﬁcient early-stopping of the teacher and student, the
distillation gradients become constructive for any node k on S−and S+.
Lemma B.1. If K > 2 and αteτ ≤log(K −2) and αte −α ≥2
τ log
3
1−κ for some κ ∈(0, 1], then for any x ∈S+ ∪S−,
pte
k(x)−pk(x)
pte
k(x)
≥κ.
The ﬁrst condition above is an early-stopping condition on the perfect teacher; the second a more aggressive early-stopping
condition on the student. The end result is that for any point in S−and S+, the teacher’s supervision probability for node
k will be larger than the student’s current probability. This would imply that under distillation, the student would use all
examples in S−and S+ as a “positive” example for node k, which is constructive as all these points either belong to class k
or have similarity to class k.
Proof. If the point belongs to S−, then:
pk(x) =
eατ/2
eατ + eατ/2 + K −2 and
pte
k (x) =
eαteτ/2
eαteτ + eαteτ/2 + K −2.
(20)
Then, the ratio is :
pk(x)
pte
k (x) = eαteτ/2 + 1 + (K −2)e−αteτ/2
eατ/2 + 1 + (K −2)e−ατ/2 .
(21)
Since αteτ ≤log(K −2), we can upper-bound the ﬁrst term in the numerator by the third term. Similarly, since 1 ≤eαteτ/2
(both αte > 0, τ > 0 by deﬁnition), we can upper-bound the second term in the numerator by the third term as well. Thus,
pk(x)
pte
k (x) ≤3(K −2)e−αteτ/2
(K −2)e−ατ/2
≤3e(α−αte) τ
2 ≤1 −κ.
(22)
The last step follows from assuming αte −α ≥2
τ log
3
1−κ in the lemma statement.
If the point belongs to S+, then:
pk(x) =
eατ
eατ + eατ/2 + K −2 and
pte
k (x) =
eαteτ
eαteτ + eαteτ/2 + K −2.
(23)

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
The ratio is
pk(x)
pte
k (x) = 1 + e−αteτ/2 + (K −2)e−αteτ
1 + e−ατ/2 + (K −2)e−ατ .
(24)
Again, via the same argument, we can upper-bound each term in the numerator by the last term and derive a similar inequality
as Eq 22.
pk(x)
pte
k (x) ≤3(K −2)e−αteτ
(K −2)e−ατ
≤3e(α−αte)τ ≤1 −κ.
(25)
From Eq 22 and Eq 25, we get the ﬁnal result.
Next, we bound the difference in supervision provided by the perfect and imperfect teachers. While this difference naturally
scales with the level of imperfection, we make some additional simplifying assumptions that allows us to derive a simple
linear scaling.
Lemma B.2. Consider a perfect teacher model with weights αteµ⋆
k and corresponding probabilities pte(·). Consider an
ϵte-imperfect teacher model with weights wk such that ∥wk∥= αte and ∥w⊥
k ∥2
∥w∥
k∥2 ≤ϵte. Let the corresponding probabilities
be given by qte(·).
Assume for simplicity that 4αteτϵte ≤1, ϵte < 1 and τ > 1. Then, on any x ∈S+ ∪S−,
|pte
k (x) −qte
k (x)| ≤12αteτ ·
√
ϵte · pte
k (x).
(26)
Proof. The dot-product of the weights with the input can be written as
wk · xk = ∥w∥
k∥· (xk · µ⋆
k) + w⊥
k · x⊥
k .
(27)
Let us upper and lower bound each of these terms individually. First, ∥w∥∥≤αte and ∥w∥∥≥αte
1
√
1+ϵte ≥αte(1 −ϵte).
Next, we have |xk · µ⋆
k| ≤τ. Thirdly, ∥w⊥∥≤αteq
ϵte
1+ϵte ≤αte√
ϵte. Finally, limD→∞∥x⊥
k ∥≤1. Putting these together,
we get the following bound on the difference between the logits of the perfect and imperfect teachers:
|wk · xk −αte(xk · µ⋆
k)| ≤αteτϵte + αte√
ϵte ≤2αteτ
√
ϵte.
(28)
Here, we’ve assumed for simplicity that ϵte < 1 and τ > 1.
The above gives us a bound on the differences in the logits. But to bound the probabilities themselves, note that due to
the structure of the softmax, both the numerator and the denominator can either grow or diminish by a multiplicative
factor of e2αteτ
√
ϵte when the imperfection is introduced into the logits. Thus, we have the upper and lower bounds,
e4αteτ
√
ϵte ≥qte
k (x)
pte
k(x) ≥e−4αteτ
√
ϵte. We know from the properties of the exp function that e−4αteτ
√
ϵte ≥1 −4αteτ
√
ϵte.
Also, given 4αteτϵte ≤1 for simplicity, we can use the properties of the exp function to say that e4αteτϵte ≤1 + e · 4αteτϵte.
Rearranging this gives us our ﬁnal inequality.
Next, we prove that given a perfect teacher, the distillation gradients become constructive when compared to one-hot
gradients which are sub-optimal due to their destructive nature.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Lemma B.3. Consider a student model with weights wk = αµ⋆
k such that α ≤1
τ log(K −2). Let uk denote the one-hot
gradients on this model under the cross-entropy loss on the dataset S. Consider a perfect teacher model with weights wk
such that wk = αteµ⋆
k. Assume the teacher model is early-stopped in that αte ≤1
τ log(K −2).
Assuming αte ≥1
τ log 3, let κ > 0 denote the level of early-stopping in the student, such that αte −α ≥2
τ log
3
1−κ. Let ˜uk
denote the gradient under distillation loss with this teacher on the above student model.
Assume that the simplifying assumptions of Assumption B.1 hold. Then,
lim
D→∞
∥u∥
k∥2
∥u⊥
k ∥2 ≤mτ 2

1 −e−ατ
9

|
{z
}
Upper-bound on quality of one-hot gradients
≤mτ 2  1 + κ2
≤lim
D→∞
∥˜u∥
k∥2
∥˜u⊥
k ∥2
|
{z
}
Lower-bound on quality of distillation gradients
.
(29)
Proof. We have assumed |S+| = |S−| = m. Note that for all x in S+, pk(x) = p+ for some constant p+ > 0. Similarly,
for all x in S−, pk(x) = p−for some constant p+ > 0.
Recall that the gradient descent update on node k be written as uk = u∥
k + u⊥
k , where u⊥
k · µ⋆
k = 0.
The one-hot gradient at node k can be written as:
uk =
X
x∈S+
(1 −pk(x))xk −
X
x∈S−
pk(x)xk
|
{z
}
destructive gradients
(30)
= (1 −p+)
X
x∈S+
xk −p−

X
x∈S−
xk

.
(31)
We project the sum total of all these gradients along µ⋆
k to get
∥u∥
k∥= uk · µ⋆
k = mτ ((1 −p+) −p−) .
(32)
Next, we want to compute the total magnitude of all the gradients orthogonal to µ⋆
k, namely ∥u⊥
k ∥. Note that the projection
of any sample x orthogonal to µ⋆
k is essentially a D −1 multivariate zero-mean Gaussian with variance 1/D along each
direction. Thus, the projection of u⊥
k along each of these D −1 directions is a zero-mean Gaussian with variance equal to
m
D
 (1 −p+)2 + p2
−

. Thus ∥u⊥
k ∥2 is the sum of squared of D −1 of these random variables. Therefore,
lim
D→∞∥u⊥
k ∥= m
 (1 −p+)2 + p2
−

.
(33)
Thus, the ratio can be written as:
lim
D→∞
∥u∥
k∥2
∥u⊥
k ∥2 = m2τ 2 ((1 −p+) −p−)2
m
 (1 −p+)2 + p2
−

≤mτ 2

1 −
2(1 −p+)p−
(1 −p+)2 + p2
−

(34)
≤mτ 2
 
1 −
p−
p+
2!
.
(35)
We can lower bound p−/p+ using Eq 20 and Eq 23 as:
p−
p+
= 1 + (K −2)e−ατ + e−ατ/2
1 + (K −2)e−ατ/2 + eατ/2 ≥
(K −2)e−ατ
3(K −2)e−ατ/2 ≥e−ατ/2
3
,
(36)

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
where we use ατ ≤log(K −2) to upper bound each of the denominator terms by (K −2)e−ατ/2.
Plugging this back, we get the following bound on the quality of one-hot gradients:
lim
D→∞
∥u∥
k∥2
∥u⊥
k ∥2 ≤mτ 2

1 −e−ατ
9

.
(37)
Now let pte
+ and pte
−denote the probabilities of a perfect teacher. We can apply an identical argument for distillation to bound
the ratio of the gradients as:
lim
D→∞
∥˜u∥∥2
∥˜u⊥∥2 := m2τ 2((pte
+ −p+) + (pte
−−p−))
m((pte
+ −p+)2 + (pte
−−p−)2) ≥mτ 2

1 + 2
(pte
+ −p+)(pte
−−p−)
(pte
+ −p+)2 + (pte
−−p−)2

(38)
≥mτ 2

1 + 2
κ2pte
+pte
−
(pte
+)2 + (pte
−)2

≥mτ 2(1 + κ2),
(39)
where in the second line we’ve used Lemma B.1 to lower-bound the teacher-student probability gap, subsequently followed
by the AM-GM inequality. Here, we have used the fact that pte
−−p−> 0, or in other words, that the gradients from S−are
constructive.
We are now ready to state and prove our main theorem:
Theorem B.4. Consider a student model with weights wk = αµ⋆
k such that α ≤1
τ log(K −2). Let uk denote the one-hot
gradients on this model under the cross-entropy loss on the dataset S.
Consider an imperfect teacher model with weights wk such that ∥wk∥= αte and ∥w⊥
k ∥2
∥w∥
k∥2 ≤ϵte where ϵte >
1
mτ 2 . Assume
the teacher model is early-stopped in that αte ≤1
τ log(K −2) where K > 5.
Assuming αte ≥1
τ log 3, let κ > 0 denote the level of early-stopping in the student, such that αte −α ≥2
τ log
3
1−κ. Let ˜uk
denote the gradient under distillation loss with this teacher on the above student model.
Assume that the simplifying assumptions of Assumption B.1 hold. Then, for a sufﬁciently large τ, there exists values of α, κ
such that α ∈[0, αte] such that κ3 ≥192αteτ
√
mϵte and so we have for any class k that:
lim
D→∞
∥u∥
k∥2
∥u⊥
k ∥2 ≤mτ 2

1 −e−ατ
9

|
{z
}
Upper-bound on quality of one-hot gradients
≤mτ 2

1 + κ2
2

≤lim
D→∞
∥˜u∥
k∥2
∥˜u⊥
k ∥2
|
{z
}
Lower-bound on quality of distillation gradients
.
(40)
Furthermore, the quality of the teacher weights can be upper bounded by the quality of distillation gradients as :
1
ϵte ≤mτ 2

1 + κ2
2

≤lim
D→∞
∥˜u∥
k∥2
∥˜u⊥
k ∥2
|
{z
}
Lower-bound on quality of distillation gradients
.
(41)
Proof. We will now extend the proof of Lemma B.3 to an imperfect teacher with probabilities qte(x). For convenience, let
us assume that for x ∈S+ |pte
k (x) −qte
k (x)| ≤γ+ and x ∈S−, |pte
k (x) −qte
k (x)| ≤γ−for some constants γ+ and γ−. We
can say that:

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
∥˜u∥∥2 = τ 2


X
x∈S−∪S+
qte
k (x) −pk(x)


2
≥m2τ 2  (pte
+ −p+) + (pte
−−p−) −γ+ −γ−
2 .
(42)
If we have γ−≤ϵ′
2 κpte
−for some ϵ′, from Lemma B.1, we have γ−≤ϵ′
2 κ(pte
−−p−). We can make a similar claim for γ+.
Thus,
∥˜u∥∥2 ≥m2τ 2  (pte
+ −p+) + (pte
−−p−)
2 
1 −ϵ′
2
2
(43)
≥m2τ 2  (pte
+ −p+) + (pte
−−p−)
2 (1 −ϵ′) .
(44)
(45)
As for the orthogonal gradient term, recall that ∥u⊥
k ∥2 is essentially the summation of D −1 squared random variables.
With a perfect teacher, we could write each of the D −1 variables as a sum of m Gaussians each scaled by a constant
probability (pte
+ −p+) and (pte
−−p−) independent of the sample x. However, with the imperfect teacher, the probabilities
are themselves random variables dependent on the draw of x. So we ﬁrst bound the gap between this norm and the norm
under the perfect teacher.
Without loss of generality, let us denote the D −1 dimensions of xk that is orthogonal to µk⋆as xk,1, xk,2, . . . , xk,D−1.
Then,
lim
D→∞∥˜u⊥∥2 ≤lim
D→∞
D−1
X
j=1


X
x∈S+∪S−
(pte
k (x) −pk(x))xk,j


2
(46)
≤lim
D→∞
D−1
X
j=1



X
x∈S+∪S−
(qte
k (x) −pk(x))xk,j + xk,j (pte
k (x) −qte
k (x))
|
{z
}
≤γ+ or ≤γ−



2
(47)
≤lim
D→∞
D−1
X
j=1


X
x∈S+∪S−
(pte
k (x) −pk(x))xk,j


2
|
{z
}
≤m((pte
+−p+)2+(pte
−−p−)2)
(48)
+ lim
D→∞
D−1
X
j=1

X
x∈S+
γ+|xk,j| +
X
x∪S−
γ−|xk,j|


2
|
{z
}
Aj
(49)
+ lim
D→∞2
D−1
X
j=1
X
x∈S+∪S−
(pte
k (x) −pk(x))|xk,j|

X
x∈S+
γ+|xk,j| +
X
x∈S−
γ−|xk,j|


|
{z
}
Bj
.
(50)
(51)
For each j, Aj is the squared of the sum of the absolute value m Gaussians with variance
γ2
+
D and another m with variance
γ2
−
D . The means of these variables are γ+
q
1
D
2
π and γ−
q
1
D
2
π respectively. The expectation of Aj is the squared sum of the

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
means and the sum of the variances of all these variables, equalling
m(γ2
++γ2
−)
D
+ m2 
γ+
q
D
2
π + γ−
q
1
D
2
π
2
. The second
term can be upper bounded by m2  γ2
+
D +
γ2
−
D

. Therefore, we can conclude that limD→∞
PD−1
j=1 Aj ≤3m2(γ2
+ + γ2
−).
For each j, Bj can be expanded into (2m)2 terms. The expected value of each of these terms can be bounded in the form of
(pte
k (x) −pk(x))
  γ+
D

or (pte
k (x) −pk(x))
  γ−
D

. Thus, the expected value of Bj is m
  γ−
D + γ+
D
 P
x(pte
k (x) −pk(x)).
Let limD→∞Bj = m (γ−+ γ+) P
x(pte
k (x) −pk(x)).
Now, if we have that √mγ−≤ϵ′
4 κpte
−and √mγ+ ≤ϵ′
4 κpte
+, for some ϵ′ we have:
lim
D→∞∥˜u⊥∥2 ≤m((pte
+ −p+)2 + (pte
−−p−)2)
(52)
+ 3m(ϵ′)2
16
 (κpte
+)2 + (κpte
−)2
+ mϵ′
4 (κpte
+ + κpte
−)(pte
+ −p+) + (pte
−−p−)).
(53)
(54)
In the second term, we can upper bound (κpte
+)2+(κpte
−)2 by ((pte
+−p+)2+(pte
−−p−)2) using Lemma B.1. In the third term,
we can similarly upper bound κpte
+ and κpte
−followed by an AM-GM inequality to upper-bound by 2((pte
+−p+)2+(pte
−−p−)2)
. Therefore,
lim
D→∞∥˜u⊥∥2 ≤m((pte
+ −p+)2 + (pte
−−p−)2)

1 + 3(ϵ′)2
16 + ϵ′
4

(55)
≤m((pte
+ −p+)2 + (pte
−−p−)2) (1 + ϵ′) .
(56)
(57)
Then, we have that for our imperfect teacher:
lim
D→∞
∥˜u∥∥2
∥˜u⊥∥2 ≥mτ 2(1 + κ2)(1 −ϵ′)
(1 + ϵ′) ≥m(1 + κ2)(1 −ϵ′)2
(58)
≥mτ 2(1 + κ2)(1 −2ϵ′) ≥mτ 2(1 + κ2 −4ϵ′) ≥m

1 + κ2
2

.
(59)
(60)
The last step follows if we have ϵ′ ≤κ2
8 . Plugging this back in our required inequality of the form √mγ−≤ϵ′
4 κpte
−, we
need √mγ−≤κ3
16pte
−and similarly for γ+. From Lemma B.2, this means we need κ3 ≥192αteτ
√
mϵte.
Now, we need to make sure that there are possible values of α for which the above lower-bound on κ can be realized. Note
that κ can only attain values of 1 −3e−(αte−α) τ
2 for various values of α. This follows from the statement of Lemma B.1.
Thus, we want 1 −3e−(αte−α) τ
2 >

192αteτ
√
mϵte

for there to be any values of α to achieve our lower-bound on κ.
Furthermore, we have that ϵ =
c
mτ 2 for some c > 1. Thus, we need 1 −3e−(αte−α) τ
2 >
 192αte√c

to have feasible
solutions for α. This is possible as long as τ is sufﬁciently large.
C. Experiments and plots
C.1. Details of experimental setup
We present details on relevant hyper-parameters for our experiments.
Model architectures. For all image datasets (CIFAR10, CIFAR100, Tiny-ImageNet), we use ResNet-v2 (He et al., 2016a)
and MobileNet-v2 (Sandler et al., 2018), models. Speciﬁcally, for CIFAR, we consider the CIFAR ResNet-{56, 20}
family and MobileNet-v2 architectures; for Tiny-ImageNet, we consider the ResNet-{50, 18} family and MobileNet-v2
architectures; For all ResNet models, we employ standard augmentations as per He et al. (2016b).

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Parameter
CIFAR10*
Tiny-ImageNet
Weight decay
10−4
5 · 10−4
Batch size
1024
128
Epochs
450
200
Peak learning rate
1.0
0.1
Learning rate warmup epochs
15
5
Learning rate decay factor
0.1
0.1
Learning rate decay epochs
200, 300, 400
75, 135
Nesterov momentum
0.9
0.9
Distillation weight
1.0
1.0
Distillation temperature
4.0
4.0
Gradual loss switch window
1k steps
10k steps
Table 1. Summary of training settings.
For all text datasets (MNLI, AGNews, QQP, IMDB), we ﬁne-tune a pre-trained RoBERTa (Liu et al., 2019) model. We
consider combinations of cross-architecture- and self-distillation with RoBERTa -Base, -Medium and -Small architectures.
Training settings. We train using minibatch SGD applied to the softmax cross-entropy loss. For all image datasets, we
follow the settings in Table 1. For all text datasets, we use a batch size of 64, and train for 25000 steps. We use a peak
learning rate of 10−5, with 1000 warmup steps, decayed linearly. For the distillation experiments, we use a distillation
weight of 1.0. We use temperature τ = 2.0 for MNLI, τ = 16.0 for IMDB, τ = 1.0 for QQP, and τ = 1.0 for AGNews.
C.2. Additional results
C.2.1. SCATTER PLOTS OF PROBABILITIES
In this section, we present additional scatter plots of the teacher-student logit-transformed probabilities for the class
corresponding to the teacher’s top prediction: Fig 8 (for CIFAR100), Fig 9 (for TinyImagenet), Fig 10 (for CIFAR10), Fig 11
(for MNLI and AGNews), Fig 12 (for self-distillation on QQP, IMDB and AGNews) and Fig 13 (for cross-architecture
distillation on language datasets). We ﬁnd underﬁtting of the low-conﬁdence points in a majority of the cases, with a few
notable exceptions and caveats:
1. For MobileNet self-distillation on CIFAR100, and for a majority of the CIFAR10 experiments, we ﬁnd no underﬁtting
of the lower-conﬁdence points on the training dataset. However, we do ﬁnd underﬁtting of the lower-conﬁdence points
on the test dataset. Interestingly, we also ﬁnd an underﬁtting of easier points in the training dataset.
2. In the language datasets, we generally ﬁnd the plots to be different in pattern from the image datasets. In particular
we ﬁnd that, for lower-conﬁdence points, there is both signiﬁcant underﬁtting and overﬁtting. For easier points, there
is less deviation, and if any, the deviation is from overﬁtting. We interpret this as the regularization from distillation
deprioritizing the lower-conﬁdence points.
3. Our patterns generally break down in the cross-architecture settings of language datasets (with the exception of a
couple of settings). We suspect that this may be because certain cross-architecture effects dominate over the more
subtle underﬁtting effect.
4. The underﬁtting we observe in the language datasets are not always associated with an improvement in the student’s
generalization. e.g., we ﬁnd no improvement in AGNews, and a decrease in generalization in QQP (see Table 2). Note
that we did not ﬁnetune hyperparameters in these settings to make distillation work.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Dataset
Teacher
Student
Train accuracy
Test accuracy
Teacher
Student (OH)
Student (DIST)
Teacher
Student (OH)
Student (DIST)
CIFAR10
ResNet-56
ResNet-56
100.00
100.00
100.00
93.72
93.72
93.9
ResNet-56
ResNet-20
100.00
99.95
99.60
93.72
91.83
92.94
ResNet-56
MobileNet-v2-1.0
100.00
100.00
99.96
93.72
85.11
87.81
MobileNet-v2-1.0
MobileNet-v2-1.0
100.00
100.00
100.00
85.11
85.11
86.76
CIFAR100
ResNet-56
ResNet-56
99.97
99.97
97.01
72.52
72.52
74.55
ResNet-56
ResNet-20
99.97
94.31
84.48
72.52
67.52
70.87
MobileNet-v2-1.0
MobileNet-v2-1.0
99.97
99.97
99.96
54.32
54.32
56.32
ResNet-56
MobileNet-v2-1.0
99.97
99.97
99.56
72.52
54.32
62.4
CIFAR100 noise
ResNet-56
ResNet-56
99.9
99.9
95.6
69.8
69.8
72.7
ResNet-56
ResNet-20
99.9
91.4
82.8
69.8
64.9
69.2
Tiny-ImageNet
ResNet-50
ResNet-50
98.62
98.62
94.84
66
66
66.44
ResNet-50
ResNet-18
98.62
93.51
91.09
66
62.78
63.98
ResNet-50
MobileNet-v2-1.0
98.62
89.34
87.90
66
62.75
63.97
MobileNet-v2-1.0
MobileNet-v2-1.0
89.34
89.34
82.26
62.75
62.75
63.28
MNLI
RoBERTa-Base
RoBERTa-Small
92.9
72.1
72.6
87.4
69.9
70.3
MNLI
RoBERTa-Small
RoBERTa-Small
72.1
72.1
71.0
69.9
69.9
69.9
MNLI
RoBERTa-Medium
RoBERTa-Medium
88.2
88.2
85.6
83.8
83.8
83.5
IMDB
RoBERTa-Small
RoBERTa-Small
100.0
100.0
99.1
90.4
90.4
91.0
QQP
RoBERTa-Small
RoBERTa-Small
85.0
85.0
83.2
83.5
83.5
82.5
QQP
RoBERTa-Medium
RoBERTa-Medium
92.3
92.3
90.5
89.7
89.7
89.0
AGNews
RoBERTa-Small
RoBERTa-Small
96.5
96.5
95.9
93.8
93.8
93.9
AGNews
RoBERTa-Medium
RoBERTa-Medium
98.6
98.6
97.9
94.6
94.6
94.6
Table 2. Summary of train and test performance of various distillation settings.
Figure 8. Teacher-student logit plots for CIFAR100 experiments: We report plots for various distillation settings involving ResNet56,
ResNet20 and MobileNet-v2. We ﬁnd underﬁtting of the low-conﬁdence points in the training set in all but the MobileNet self-distillation
setting. Nevertheless, even in the MobileNet self-distillation setting, we ﬁnd signiﬁcant underﬁtting in the test dataset.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Figure 9. Teacher-student logit plots for Tiny-Imagenet experiments: We report plots for various distillation settings involving
ResNet50, ResNet18 and MobileNet-v2. We ﬁnd underﬁtting of the low-conﬁdence points in all the settings. We also ﬁnd overﬁtting of
the easier points when the student is a ResNet.
Figure 10. Teacher-student logit plots for CIFAR10 experiments: We report plots for various distillation settings involving ResNet56,
ResNet20 and MobileNet-v2. We ﬁnd that the underﬁtting phenomenon is almost non-existent in the training set (except for ResNet50 to
ResNet20 distillation). However the phenomenon is prominent in the test dataset.
C.2.2. TEACHER’S PREDICTED CLASS VS. GROUND TRUTH CLASS
Recall that in all our scatter plots we have looked at the probabilities of the teacher and the student on the teacher’s predicted
class i.e., (pte
yte(x), pst
yte(x)) where yte
.= argmaxy′∈[K] pte
y′(x). Another natural alternative would have been to look at the
probabilities for the ground truth class, (pte
y⋆(x), pst
y⋆(x)) where y⋆is the ground truth label. We chose to look at yte however,
because we are interested in the “shortcomings” of the distillation procedure where the student only has access to teacher
probabilities and not ground truth labels.
Nevertheless, one may still be curious as to what the probabilities for the ground truth class look like. First, we note that the
plots look almost identical for the training dataset owing to the fact that the teacher model typically ﬁts the data to zero

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
(a) Self-distillation in MNLI
(b) Cross-architecture distillation in MNLI and AGNews
Figure 11. Teacher-student logit plots for MNLI and AGNews experiments: We report plots for various distillation settings involving
RoBERTa models. On the left, in the self-distillation settings on MNLI, we ﬁnd signiﬁcant underﬁtting of low-conﬁdence points (and
also overﬁtting), while easy points are completely overﬁt. On the right, we report cross-architecture ( Base to Medium) distillation for
MNLI and AGNews. Here the plots are not as typical of our other plots. Neverthless, we observe signiﬁcant underﬁtting and overﬁtting
of low-conﬁdence points, and overﬁtting of the extremely easy points. We interpret this as distillation reducing its “precision” on the
lower-conﬁdence points (perhaps by ignoring lower eigenvectors that provide ﬁner precision).
Figure 12. Teacher-student logit plots for self-distillation in language datasets (QQP, IMDB, AGNews): We report plots for various
self-distillation settings involving RoBERTa models. As in the other language dataset settings, we ﬁnd both signiﬁcant underﬁtting and
overﬁtting for lower-conﬁdence points (indicating lack of precision), and with more precision for easier points (typically with more
overﬁtting).
training error (we skip these plots to avoid redundancy). However, we ﬁnd stark differences in the test dataset as shown in
Fig 14. In particular, we see that the underﬁtting phenomenon is no longer prominent, and almost non-existent in many of
our settings. This is surprising as this suggests that the student somehow matches the probabilities on the ground truth class
of the teacher despite not knowing what the ground truth class is.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Figure 13. Teacher-student logit plots for cross-architecture distillation in language datasets (AGNews, QQP, IMDB, MNLI): We
report plots for various cross-architecture distillation settings involving RoBERTa models. While we ﬁnd signiﬁcant student-teacher
deviations in these settings, our typical patterns do not apply here. We believe that effects due to “cross-architecture gaps” may have likely
drowned out the underﬁtting patterns, which is a more subtle phenomenon that shines in self-distillation settings.
In Fig 15, we dissect the test data plots into four parts depending on which of the teacher and student model gets the
prediction matching the ground truth. We consistently ﬁnd that the points that the student gets right but the teacher not, fall
in the underﬁt set of points. Interestingly, the underﬁt set of points is roughly equivalent to the set of all points where at
least one of the models is incorrect. This suggests that in its attempt to underﬁt some of the points, the student can get some
incorrect, potentially points which are inherently fuzzy (e.g., they are similar to multiple classes). Theorem 4.3 suggests that
the student would use these points to improve its features, thereby increasing accuracy on other points that are not inherently
as fuzzy.
Finally, we note that previous work (Lukasik et al., 2021) has examined deviations on ground truth class probabilities albeit
in an aggregated sense (at a class-level rather than at a sample-level). While they ﬁnd that the student tends to have lower
ground truth probability than the teacher on problems with label imbalance, they do not ﬁnd any such difference on standard
datasets without imbalance. This is in alignment with what we ﬁnd above.
C.3. Ablations
Here, we conduct two experiments showing that the underﬁtting phenomenon holds under other conditions, speciﬁcally (a)
longer training of the student and (b) smaller batch sizes and learning rate. We also experiment with other transformation
metrics besides the logit transformation used in the main paper.
Longer training: In Fig 16 (left two images), we conduct experiments where we run knowledge distillation with the
ResNet-56 student on CIFAR100 for 2.3× longer (50k steps instead of 21.6k steps overall) and with the ResNet-50 student
on TinyImagenet for about 2× longer (300k steps over instead of roughly 150k steps). We ﬁnd the resulting plots to continue
to have the same underﬁtting as the earlier plots. It is worth noting that in contrast, in a linear setting, it is reasonable to
expect the underﬁtting to disappear after sufﬁciently long training. Therefore, the persistent underﬁtting in the non-linear
setting is remarkable and suggests one of two possibilities:
• The underﬁtting is persistent simply because the student is not trained sufﬁciently long enough i.e., perhaps, when
trained 10× longer, the network might end up ﬁtting the teacher probabilities perfectly.
• The network has reached a local optimum of the knowledge distillation loss and can never ﬁt the teacher perfectly. This
may suggest an added regularization effect in distillation, besides the eigenspace regularization that introduces the
low-conﬁdence-point-underﬁtting in the ﬁrst place. This unknown regularization effect may perhaps disappear if the

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Figure 14. Scatter plots for ground truth class: Unlike in other plots where we report the probabilities for the class predicted by the
teacher, here we focus on the ground truth class. Recall that the X-axis corresponds to the teacher, the Y -axis to the student, and all the
probabilities are log-transformed. Surprisingly, we observe a much more subdued underﬁtting here, with the phenomenon completely
disappearing in many situations. This suggests that the student magically preserves the ground-truth probabilities despite no knowledge of
what the ground-truth class is, while underﬁtting on the teacher’s predicted class.
learning rate was even smaller or if the student network was larger than the teacher network, allowing to reach higher
precision.
Smaller batch size/learning rate: Finally, in Fig 16 (right image), we also verify that in the CIFAR100 setting if we set
peak learning rate to 0.1 (rather than 1.0) and batch size to 128 (rather than 1024), our observations still hold.
Scatter plot for other metrics: In the main paper, recall that we look at student-teacher deviations via scatter plots of the
probabilities of either models on the teacher’s top class, after applying a logit transformation. It is natural to ask what these
plots would look like under other variations. We explore this in Fig 17 for the CIFAR100 ResNet-56 self-distillation setting.
For quick reference, in the top row of Fig 17, we ﬁrst show the standard logit-transformed probabilities plot where we
ﬁnd the underﬁtting phenomenon. In the second ﬁgure, we then directly plot the probabilities instead of applying the logit
transformation on top of it. We ﬁnd that the underﬁtting phenomenon does not prominently stand out here (although visible
upon scrutiny, if we examine below the X = Y line for X ≈0). This illegibility is because small probability values tend to
concentrate around 0; the logit transform however helps magnify the behavior of small probability values. For the third
plot, we provide a scatter plot of entropy values of the teacher and student probability values to determine if the student
distinctively deviates in terms of entropy from the teacher. It is not clear what characteristic behavior appears in this plot.
In the bottom plots, on the Y axis we plot the KL-divergence of the student’s probability from the teacher’s probability.
Along the X axis we plot the same quantities as in the top row’s three plots. Here, we observe interesting behavior across the
board: the KL-divergence of the student tends to be higher on teacher’s lower-conﬁdence points, where “lower conﬁdence”
can be interpreted as either points where its top probability is low, or points where the teacher is “confused” enough to have
high entropy.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
(a) CIFAR100 MobileNet-v2 self-distillation
(b) CIFAR100 ResNet56 self-distillation
(c) TinyImageNet ResNet50 self-distillation
Figure 15. Dissecting the underﬁt points: Across a few settings on TinyImagenet and CIFAR100, we separate the teacher-student scatter
plots of logit-transformed probabilities into four subsets: subsets where both models’s top prediction is correct (titled as Both), where
only the student gets correct (Only_student), where only the teacher gets correct (Only_teacher), where neither get correct (Neither).
We consistently ﬁnd that the student’s “underﬁt” points are points where at least one of the models go wrong.
Figure 16. Underﬁtting holds for longer runs and for smaller batch sizes: For the self-distillation setting in CIFAR100 and TinyIm-
agenet (left two ﬁgures), we ﬁnd that the student underﬁts teacher’s low-conﬁdence points even after an extended period of training
(roughly 2× longer). On the right, we ﬁnd in the CIFAR100 setting that underﬁtting occurs even for smaller batch sizes.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Figure 17. Scatter plots for various metrics: While in the main paper we presented scatter plots of logit-transformed probabilities, here
we present scatter plots for various metrics, including the probabilities themselves, entropy of the probabilities, and the KL divergence
of the student probabilities from the teacher. We ﬁnd that the KL-divergence plots capture similar intuition as our logit-transformed
probability plots. On the other hand, directly plotting the probabilities themselves is not as visually informative.
C.3.1. THE EFFECT OF LOSS SWITCHING
We provide additional results on loss-switching in this section. Fig 19 presents the accuracy values under a one-hot-to-
distillation switch. Fig 20 presents the accuracy values under a distillation-to-one-hot switch. Fig 18 presents student-teacher
scatter plots over the course of training under all the four methods (with and without loss switches). Fig 21 presents the
trajectory of accuracies for ResNet50 on TinyImagenet under all the four settings.
It is worth noting a nuance in this story. First, switching to one-hot typically preserves the gains of distillation if it is not run
for too long — and in the rare case of ResNet50 self-distillation on TinyImagenet even outperforms distillation — which
supports earlier ﬁndings (Cho & Hariharan, 2019; Zhou et al., 2020; Jafari et al., 2021) that advocate a soft ﬁnal switch to
one-hot training. Overall, this seems to suggest that, the early phase of distillation may be a sufﬁcient requirement to gain all
the beneﬁts of distillation (even if our earlier experiments suggest that they are not necessary). However, we consistently
ﬁnd that switching to and training with one-hot for a sufﬁciently long time deteriorates the gains made by distillation (Fig 2,
Fig 21).

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
(a) One-hot and self-distillation.
(b) Loss-switching to distillation/one-hot at 100k steps.
Figure 18. Evolution of logit-logit plots over various steps of training for TinyImageNet ResNet50 self-distillation setup: On the
left, we present plots for one-hot training (top) and distillation (bottom). On the right, we present similar plots for experiments from
Section 3.2, with the loss switched to distillation (top) and one-hot (bottom) at 100k steps. From the last two visualized plots in each,
observe that switching to distillation introduces (a) underﬁtting of low-conﬁdence points and overﬁtting of easier points, (b) while
switching to one-hot curiously undoes both of this.
Figure 19. Accuracies under late distillation for CIFAR100 and TinyImageNet: We report the ﬁnal accuracies for various distillation
settings under ResNet56, ResNet20 and MobileNet-v2, with the loss switched to distillation in the middle of training. Observe that
replacing the initial fourth/ﬁfth of the training has zero effect on the distillation loss suggesting that deviations in the initial phase of
training is not a crucial factor behind the success of distillation. Note that for MobileNet architectures, the loss-switch caused instability,
which explains the strong dip in accuracy, even going below the one-hot model (top right).
C.4. Verifying the eigenspace view empirically
In this section, we demonstrate the eigenspace view from §4.1 in practice even in situations where our theoretical assumptions
do not hold good. We go beyond our theoretical assumptions in the following ways:
1. We consider two self-distillation settings: ﬁrst, a linear random features model trained on a noisy version of the MNIST
dataset and next a 2-layer multi-layer perceptron (MLP) model trained on a subset of the MNIST dataset.
2. Both are trained with the cross-entropy loss (and not the squared error loss as used in our theory).

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Figure 20. Accuracies under late one-hot for CIFAR100 and TinyImageNet: We report the ﬁnal accuracies for various distillation-
to-one-hot settings under ResNet56, ResNet20 and MobileNet-v2, with the loss switched to one-hot in the middle of training. Here,
surprisingly, we ﬁnd that in a majority of cases, switching to one-hot preserves the gains of distillation, and may even result an increase in
accuracy, echoing the ﬁndings of (Zhang & Sabuncu, 2020; Yuan et al., 2020; Tang et al., 2020). However, in Fig 21, we ﬁnd that these
gains are subsequently destroyed by a longer one-hot training.
3. We consider a multi-class problem and so the output is not a single scalar value.
4. We use a ﬁnite learning rate with minibatches and Adam.
We provide exact details of these two settings at the end of the section
Observations. In short, we ﬁrst observe in Fig 22 that in both these settings, the lower-conﬁdence points of the teacher are
underﬁt as usual. At the same time, we observe in Fig 23 and Fig 24 that the convergence rate of the student is much faster
along the top eigendirections, when compared to the teacher (explained shortly). We show train-test accuracy plots in Fig 25.
To verify our eigenspace view theory, we show 2D projections of the trajectory of the teacher and student along two
eigendirections picked at random (with the higher eigendirection one plotted along the X axis). The ﬁnal solution found is
marked by a ‘⋆’, (typically at the top-right of the plot).
Here, we observe that the teacher already has an implicit bias towards converging faster along the top eigendirection (as
is well-known). This can be inferred from the fact that the trajectories move quickly along the X axis towards its ﬁnal X
axis value, before making progress along the Y axis. 3 But more interestingly, we ﬁnd that for the student, the bias in this
trajectory is more exaggerated; the student converges faster towards the ﬁnal X value of the teacher than the rate at which
teacher gets there. In doing so, the student covers a completely different part of parameter space never traversed by the
teacher. In this sense, the bias of distillation is an exaggerated but non-identical version of the bias of standard gradient
descent.
In the linear setting, we ﬁnd this bias to hold in all of the 15 different random pairs of eigendirections, while in the MLP
setting this holds in all but two of the 15 different random pairs of eigendirections. Note that these eigendirections are picked
at random from the set of all directions and not cherry-picked. Speciﬁcally, the top direction is sampled at random from the
top 15 directions (without replacement), and the bottom from the directions with indices in [20, 60] (without replacement).
3Intuitively, when this bias is extreme, the trajectory would reach its ﬁnal X axis value ﬁrst with no displacement along the Y axis,
and only then progress along the Y axis. Instead, we see a softer form of this bias, where the trajectory takes a “convex” shape.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
10000
20000
30000
Step
0.66
0.68
0.70
0.72
0.74
0.76
Test accuracy
Gradual loss switch
Res56 to Res56 CIFAR100
Method
KD
OneHot
KD to OneHot
OneHot to KD
Figure 21. Trajectory of test accuracy for loss-switching over longer periods of time: We gradually change the loss for our self-
distillation settings in CIFAR100 and TinyImagenet and extend training for a longer period of time. We ﬁnd that in all cases, while
switching to distillation gains in accuracy, switching to one-hot actively deteriorates the accuracy gains made by knowledge distillation.
However, we note that for the Tiny-Imagenet, there is a small window of time for which there is a signiﬁcant increase in accuracy under a
switch to one-hot – this is later destroyed with longer training. Nevertheless, the overall ﬁnding here reinforces the intuition that the
one-hot loss results in destructive gradients as discussed in Section 4.2.
To compute the eigendirections, in the case of the random features setting, we compute the directions of the random-features-
transformed data. We then project the weight matrix W along the eigendirection v, and then take the ℓ2 norm ∥W⊤v∥2 to
compute the projection. Note that in the theory, we dealt with a model with a scalar-valued output, and so W⊤v would have
been a scalar. In the case of the 2-layer MLP, we directly compute the eigendirections of the input data. We then take the
ﬁrst layer matrix W and compute the projection similarly.
Takeaway. We ﬁnd that distillation leads to an exaggerated bias in terms of the rate of convergence along various
eigendirections. This happens even in a setting trained with cross-entropy loss, and with a non-linear neural network, going
beyond our theoretical assumptions. Thus, our insights from the linear regression setting in §4.1 apply to a wider range
of settings. We also ﬁnd that underﬁtting happens in these settings, reinforcing the connection between the eigenspace
regularization effect and underﬁtting.
Other details. For the random features setting, we train on a subset of 128 datapoints with 5000 ReLU random features.
The training data has 0.25 probability of a mislabeling. We use a batch size of 16 and learning rate of 0.001. Both teacher
and student are trained for 40 epochs, and the student with a temperature of 5.
In the MNIST setting, we use a 2-layer MLP with 1000 hidden units trained on a subset of 128 datapoints with no label noise.
We use a batch size of 16 and learning rate of 0.0001. Both teacher and student are trained for 20 epochs; the distillation
loss uses a temperature of 4. All other details are identical to the previous setting.
C.5. Verifying the gradient space view in practice
We now demonstrate that the conclusions of Theorem 4.3 indeed hold good empirically (even in a settings where our
theoretical assumptions don’t exactly hold e.g., we will use mini-batch instead of full-batch). To do this, we build a synthetic
perfectly classiﬁable dataset (very similar to that used in Theorem 4.3) with class similarities encoded at the logit level.
We then show that distillation helps in better aligning the weights of that dataset, thus verifying that it has experienced
“denoised” gradients.

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
1.5
1.0
0.5
RF
1.75
1.50
1.25
1.00
0.75
0.50
0.25
RF to RF
NoisyMNIST Train
Clean
Mislabeled
Figure 22. Probability scatter plots verifying the eigenspace theory: We observe the underﬁtting of low-conﬁdence points. The left
plot above corresponds to the linear random features model trained on a noisy MNIST dataset, and the right corresponds to a 2-layer MLP
trained on a noisy MNIST dataset. We demonstrate that the student underﬁts the low-conﬁdence points in our two MNIST settings, while
simultaneously in Fig 23 and Fig 24 we observe that our theoretical predictions for eigenspace convergence holds true.
0.25
0.50
0.75
1.00
Top e.v. [14]
0.1
0.2
0.3
0.4
0.5
0.6
Bottom e.v [44]
Teacher
Student
0.0
0.5
1.0
1.5
Top e.v. [6]
0.2
0.4
0.6
0.8
Bottom e.v [47]
0.5
1.0
Top e.v. [4]
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Bottom e.v [56]
0.5
1.0
Top e.v. [9]
0.1
0.2
0.3
0.4
0.5
Bottom e.v [51]
0.5
1.0
Top e.v. [0]
0.2
0.4
0.6
0.8
Bottom e.v [21]
0.00
0.25
0.50
0.75
1.00
Top e.v. [13]
0.2
0.4
0.6
0.8
1.0
Bottom e.v [41]
0.5
1.0
1.5
Top e.v. [5]
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Bottom e.v [35]
0.2
0.4
0.6
0.8
Top e.v. [10]
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Bottom e.v [26]
0.5
1.0
Top e.v. [12]
0.2
0.4
0.6
0.8
Bottom e.v [43]
0.0
0.5
1.0
1.5
2.0
Top e.v. [2]
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Bottom e.v [52]
0.0
0.5
1.0
1.5
Top e.v. [7]
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Bottom e.v [20]
0.25
0.50
0.75
1.00
Top e.v. [3]
0.1
0.2
0.3
0.4
0.5
0.6
Bottom e.v [29]
0.25
0.50
0.75
1.00
Top e.v. [1]
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Bottom e.v [32]
0.25
0.50
0.75
1.00
Top e.v. [8]
0.2
0.4
0.6
0.8
1.0
1.2
Bottom e.v [27]
0.5
1.0
Top e.v. [11]
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Bottom e.v [48]
Figure 23. Eigenspace convergence plots verifying the eigenspace theory for NoisyMNIST-RandomFeatures setting: In all these
plots, the X axis corresponds to the top eigenvector and the Y axis to the bottom eigenvector (see §C.4 for how they are randomly picked).
Each plot shows the trajectory projected onto the two eigendirections with the ⋆corresponding to the ﬁnal parameters. In all cases we ﬁnd
that both the student and the teacher converge faster to their ﬁnal X value, than to their Y value showing that both have a bias towards
higher eigendirections. But importantly, this bias is exaggerated for the student in all cases, proving our main theoretical claim in §4.1 in a
more general setting with multi-class cross-entropy loss, ﬁnite learning rate etc.,
Dataset details. We consider a K-class classiﬁcation dataset (K = 10) where the ith datapoint’s features can be written as
a K-“channel” input xi = (x(i)
1 , . . . , x(i)
K ) where each channel x(i)
k
∈RD is D-dimensional (D = 100). For each k, we
pick a global “ground truth” class vector µ⋆
k sampled from the D-dimensional normal distribution. Assuming a uniform

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
1.00
1.05
1.10
Top e.v. [9]
1.040
1.045
1.050
1.055
1.060
Bottom e.v [34]
Teacher
Student
1.0
1.2
Top e.v. [4]
1.016
1.018
1.020
1.022
1.024
Bottom e.v [54]
1.060
1.065
1.070
Top e.v. [14]
1.03
1.04
1.05
1.06
1.07
Bottom e.v [20]
1.08
1.10
Top e.v. [12]
1.050
1.055
1.060
1.065
Bottom e.v [32]
1.075
1.100
1.125
Top e.v. [10]
1.056
1.058
1.060
1.062
Bottom e.v [50]
1.10
1.15
Top e.v. [13]
1.092
1.094
1.096
1.098
1.100
Bottom e.v [53]
1.1
1.2
1.3
Top e.v. [1]
1.038
1.040
1.042
1.044
1.046
1.048
1.050
Bottom e.v [44]
1.10
1.15
Top e.v. [7]
1.015
1.020
1.025
1.030
1.035
Bottom e.v [52]
1.1
1.2
1.3
Top e.v. [3]
1.078
1.079
1.080
1.081
1.082
1.083
Bottom e.v [45]
1.1
1.2
1.3
Top e.v. [2]
1.046
1.048
1.050
1.052
1.054
Bottom e.v [39]
1.08
1.10
1.12
Top e.v. [11]
1.012
1.013
1.014
1.015
1.016
1.017
Bottom e.v [56]
1.10
1.15
1.20
1.25
Top e.v. [6]
1.0320
1.0325
1.0330
1.0335
1.0340
Bottom e.v [59]
1.1
1.2
Top e.v. [5]
1.060
1.065
1.070
1.075
1.080
1.085
Bottom e.v [31]
1.100
1.125
1.150
Top e.v. [8]
1.0675
1.0680
1.0685
1.0690
1.0695
1.0700
1.0705
Bottom e.v [47]
1.1
1.2
1.3
Top e.v. [0]
1.050
1.055
1.060
1.065
1.070
Bottom e.v [35]
Figure 24. Eigenspace convergence plots verifying the eigenspace theory for MNIST-MLP setting : In all but two cases , we ﬁnd
that the student converges faster to the ﬁnal X value of the teacher than it does along the Y axis. This demonstrates our main theoretical
claim in §4.1 in a neural network setting.
Figure 25. Test/train accuracies for MNIST settings for §C.4: In X axis, we plot the training accuracy, and in the Y axis the test
accuracy, computed at various points of time in the trajectory. We note that in the linear case (left), there is little difference in the
accuracies of the student and the teacher, likely because this is a very simple setting where ignoring the lower eigendirections (as seen in
Fig 23 to distillation) has little effect. But for the MLP setting (right), we ﬁnd that the student achieves higher test accuracies than the
teacher for a given training accuracy. This is evidence that the student uses “better” directions (i.e., top eigendirections) to ﬁt the data.
distribution over K classes, given a label y, we generate yth channel input xy as xy = αµ⋆
y + (1 −α)zy, where α = 0.01
and zy is uniformly sampled from the D-dimensional unit hypersphere truncated to xy · µ⋆
y > 0. Here, the αµ⋆
y is added so
as to provide sufﬁcient non-zero margin for the points from the decision boundary.
Next, we also set the co-ordinates corresponding to a few other classes. We ﬁrst randomly pick 3 other classes meant to be
“similar non-target” classes. Then for each such class k, we set xk = β(αµ⋆
k + (1 −α)zk) where α and zk are sampled as

ON STUDENT-TEACHER DEVIATIONS IN DISTILLATION: DOES IT PAY TO DISOBEY?
Figure 26. Plots verifying the gradient space view in a synthetic dataset: On the left, we show the trajectory of test and train accuracies
for the teacher and student, demonstrating that distillation indeed helps in our non-noisy, perfectly-classiﬁable dataset. In the middle,
for each class, we plot the cos-similarity of the weights with the ground truth class mean. We ﬁnd that the student consistently has
higher alignment with the ground truths than the teacher; this demonstrates that distillation has the ability to denoise the gradients
(and outperform the teacher). In the right, we report logit-logit scatter plots demonstrating the low-conﬁdence-point-underﬁtting effect.
Speciﬁcally here, easier points are overﬁt, while lower-conﬁdence ones are more likely to be underﬁt.
before, and β is a random value rescaled Beta distribution with parameters a = 4 and b = 1 rescaled by a multiplicative
factor equal to 0.8 · (xy · µ⋆
y). This scaling factor ensures that xk · µ⋆
y is smaller than the ground truth margin xy · µ⋆
y), and
so the dataset remains linearly separable.
Besides the target class and the 3 non-target similar classes, all other co-ordinates are set to be zero for x.
Training details. For training, we consider a linear classiﬁer that has K output nodes, each drawing input from the
corresponding D dimensions of its class. We use Adam with a learning rate of 0.1, with batch size 128, and 512 training
datapoints. We train both teacher and student for 10 epochs, and the student with temperature 5.
Observations. In this setting, although both the teacher and student have high accuracy, the self-distilled student outperforms
the teacher by about 0.7% (see Fig 26 left).
Next, we compute the number of (x, k) pairs in the training set whose gradients have the correct sign under either loss. For
each point x and each node k, we check whether xk · µ⋆
k > 0 if and only if the target probability under the given loss is
non-zero. This can verify our key proof idea that distillation denoises the gradients even in the absence of explicit noise in
the dataset. According to this computation, we ﬁnd that for one-hot loss, only 73.22% of such pairs have the right sign of
gradient, while for distillation this is 97.77%, thus verifying that distillation is indeed able to denoise.
Inspired by the formulation in Theorem 4.3, we then analyze the cos-similarity between the weights learned for each class,
and its corresponding class vector. We ﬁnd in Fig 26 middle that the student has a better alignment than the teacher, despite
the fact that the student is trained with the teacher’s logits (and no extra information is given). This proves the main result of
Theorem 4.3. Thus, even in a perfectly classiﬁable dataset, we are able to make the student outperform the teacher because
of class similarities at the logit level.
Notably, we also observe the underﬁtting phenomenon in this setting (see Fig 26 right). This suggests that the underﬁtting
phenomenon is indeed connected to how distillation denoises gradients in the presence of class similarities.

