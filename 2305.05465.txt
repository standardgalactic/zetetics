THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION
DYNAMICS
BORJAN GESHKOVSKI, CYRIL LETROUIT, YURY POLYANSKIY,
AND PHILIPPE RIGOLLET
Abstract. Viewing Transformers as interacting particle systems, we describe
the geometry of learned representations when the weights are not time depen-
dent. We show that particles, representing tokens, tend to cluster toward par-
ticular limiting objects as time tends to inﬁnity. The type of limiting object
that emerges depends on the spectrum of the value matrix. Additionally, in
the one-dimensional case we prove that the self-attention matrix converges to
a low-rank Boolean matrix. The combination of these results mathematically
conﬁrms the empirical observation made by Vaswani et al. [23] that leaders
appear in a sequence of tokens when processed by Transformers.
1. Introduction
The introduction of Transformers in 2017 [23] marked a turning point in the AI
revolution, powering breakthroughs in natural language modeling and computer vi-
sion. With remarkable empirical success, Transformers enable large language mod-
els to compute very powerful representations using the self-attention mechanism.
Yet, little is known about the geometric structure of these representations. As the
size of these models grows at an astonishing rate, the need to understand their
inner workings is becoming a pressing scientiﬁc challenge. In this work, we make a
ﬁrst step in this direction by describing the geometry of learned representations.
To provide a transparent presentation of our ﬁndings, we take a leaf out of the
literature on continuous-time dynamics such as neural ordinary diﬀerential equa-
tions (ODEs) [2, 28, 8]. By viewing layers as a time variable, this formalism has
emerged as a ﬂexible mathematical framework to implement and study ResNets [9]
as particular discrete-time versions of a parametrized dynamics of the form
9xptq “ fθpxptqq,
t P r0, Ts.
Here θ is the trained parameter of a neural network and fθ is characterized by the
precise architecture of the ResNet1. In turn, an input (e.g., an image) xp0q P Rd is
mapped to its representation xpTq.
Unlike neural ODEs and ResNets, the representation map of Transformers is
not solely a function of an individual input xp0q P Rd but rather of a sequence
px1p0q, . . . , xnp0qq of n ě 1 d-dimensional tokens.
These tokens then evolve in
time by interacting with each other per the self-attention mechanism.
Namely,
following [22], we view tokens as particles, and the transformer dynamics as an
1A classical choice is θ “ pW, A, bq P Rdˆd ˆ Rdˆd ˆ Rd and fθpxq “ WσpAx ` bq where σ is
an elementwise nonlinearity such as the ReLU ([10]).
1
arXiv:2305.05465v1  [cs.LG]  9 May 2023

2
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
interacting particle system of the form
9xiptq “
n
ÿ
j“1
PijptqV xjptq,
t P r0, `8q,
(1.1)
for any i P rns, where Pijptq are the entries of a n ˆ n stochastic matrix Pptq, given
by
Pijptq :“
exQxiptq,Kxjptqy
řn
ℓ“1 exQxiptq,Kxℓptqy ,
pi, jq P rns2.
(1.2)
Here the matrices Q (Query), K (Key), and V (Value) are learned from data. Note
that Q, K need not be square.
The n ˆ n matrix Pptq is called self-attention matrix. The wording attention
stems precisely from the fact that Pijptq captures the attention given by token i to
token j relatively to all tokens ℓP rns. The matrices Q and K in (1.2) warp the
geometry of the input tokens, so that a trained attention matrix contains weights
which indicate semantic relations between words. Such conclusions have been drawn
in the context of language processing tasks in [23, Figures 3-5].
Our goal is to showcase the fact that self-attention, which itself is the core
novelty of Transformers, entails a clustering eﬀect. To that end, we focus on the
pure self-attention dynamics described in (1.1). In particular, we do not model
variations such as multiple heads, feed-forward layers, and layer normalization that
are typically adjoined to self-attention dynamics of (1.1). However, on this last
point, we note that our theoretical ﬁndings indicate that without any normalization,
the dynamics (1.1) can diverge in some (or even all) directions over time. We leave
these additional questions for future research; see Section 12.
Figure 1.
For V
“ I3 tokens
cluster toward the vertices of a
convex polytope (Theorem 3.1).
1.1. Our contributions. The goal of this paper
is to characterize clustered representations of a
trained Transformer by studying the asymptotic
behavior of a sequence of tokens px1ptq, . . . , xnptqq
as they evolve through the layers of a transformer
architecture using the dynamics (1.1).
In this
setup, a Transformer is completely described by
the weight matrices pQ, K, V q obtained during
training. Note that we assume that these three
matrices are time-independent.
While this as-
sumption is motivated by mathematical conve-
nience, it is worth noting that such weight-sharing
scenarios are not rare in practice—see, e.g., AL-
BERT [15]—as they drastically reduce the num-
ber of parameters of a network.
The main conclusion of our analysis is that the set of tokens tx1ptq, . . . , xnptqu,
appropriately rescaled, tends to a clustered conﬁguration as t Ñ 8.
Our theo-
retical ﬁndings justify the empirical observation made in [23] that leaders appear
in a sequence of tokens when processed by Transformers. We now list our main
contributions.
(i) As a warm-up to the geometric characterization of the limits of sequences of
tokens, we show in Section 2 that when d “ 1 and V ą 0, the self-attention
matrix Pptq converges to a low-rank matrix with entries 0 and 1 as t Ñ `8 thus

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
3
revealing the emergence of a small number of leaders that drive the transformer
dynamics. The restriction d “ 1 follows from technical considerations, and some
pathological phenomena may occur in higher dimensions (see Remark 7.9). But
numerical experiments (as well as past empirical work) indicate that the result
may extend to higher dimensions for almost all initial sequences of tokens. The
proof is given in Section 7.
(ii) In Section 3 we focus on the case V “ Id. We introduce a time re-scaling
reminiscent of the layer normalization heuristics to alleviate the possible divergence
of tokens. We show that along this scale the tokens converge to the boundary of a
convex polytope. For almost all initial sequences they even converge to the vertices
of the polytope, the number of which is signiﬁcantly smaller than n. This elucidates
the clustering phenomenon. (See Figure 1.) When V “ ´Id, all tokens following
the dynamics (1.1) collapse to 0. The proofs are given in Section 8.
(iii) We build on these results and in Section 4 consider the case wherein V is
only assumed to have a simple and positive leading eigenvalue. We show that along
the particular time-scale, tokens cluster toward one of at most three hyperplanes
which are determined by the corresponding eigenvector. The proof may be found
in Section 9.
(iv) In Section 5 we complete the results of Sections 3 and 4 by addressing the
case where the leading eigenvalue has multiplicity. This results in clustering toward
the vertices of a convex polytope in some directions, and a linear subspace in the
others. The proof is given in Section 10.
(v) We also prove the global existence and uniqueness of solutions of all dynamics
considered in this work (including the mean ﬁeld limit). We refer the reader to
Section 6 for more details.
We also observed numerically that our conclusions extend to more compound
architectures (see Conjecture 4, Section 11, and Section 12).
Value
Key and Query
Limit geometry
Reference
V “ Id
QJK ą 0
vertices of convex polytope
Theorem 3.1
λ1pV q ą 0, simple
xQϕ1, Kϕ1y ą 0
union of 3 parallel hyperplanes
Theorem 4.2
V paranormal
QJK ą 0
polytope ˆ subspaces
Theorem 5.2
V “ ´Id
QJK “ Id
single cluster at origin˚
Theorem 8.5
Table 1. Summary of the clustering results of this work.
˚All results
except for the case V “ ´Id hold for the time-scaled dynamics (3.1).
Remark 1.1 (Discrete time). While we focus on the idealized setting of self-
attention dynamics in continuous-time, this is solely done for convenience and all
of our methods are straightforwardly applicable to the discrete-time setting. The
discrete-time analog of (1.1) with time-step ∆t ą 0 (equal to 1 in practice) is
simply the forward Euler iteration
xippk ` 1q∆tq “ xipk∆tq ` ∆t
n
ÿ
j“1
ˆ
exQxipk∆tq,Kxjpk∆tqy
řn
ℓ“1 exQxipk∆tq,Kxℓpk∆tqy
˙
V xjpk∆tq,
(1.3)

4
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for k P N. (See also Remark 3.3.)
1.2. Notation. We henceforth denote by x¨, ¨y and } ¨ } the Euclidean dot product
and norm respectively, and we use the shorthand rns :“ t1, . . . , nu. For any matrix
M P Rdˆd, we order its eigenvalues (repeated according to multiplicity) by decreas-
ing order of modulus: |λ1pMq| ě . . . ě |λdpMq|. We denote by }M}op the ℓ2—
operator norm of the matrix M, equal to the largest singular value of M. Given a set
S Ă Rd, we deﬁne the distance of a point x P Rd to S as distpx, Sq :“ infsPS }x´s},
and by convpSq the convex hull of S.
1.3. Related work. Our study and results build on several diﬀerent lines of work,
and we draw some parallels in what follows.
1.3.1. Analysis of attention-based models. Given the widespread use of Transform-
ers in natural language processing, there has been a surge of interest in understand-
ing the function and signiﬁcance of attention layers within these models. In [30],
the authors show that when treated as discrete-time systems with additional dense
layers and multiple heads appended to the core attention mechanism, Transformers
exhibit the universal approximation property. In [17], the authors present, to the
best of our knowledge, the ﬁrst interacting particle systems perspective on Trans-
formers. They then leverage the similarities between Transformers (with an addi-
tional feed-forward layer compared to (1.1)) and convection-diﬀusion equations to
slightly improve the performance of Transformers by employing a Strang-Marchuk
splitting scheme for time discretization. In [22], the authors interpret system (1.1)
as the characteristics of a continuity equation. Drawing on the similarities between
(1.1) and Sinkhorn iterations, they propose a novel architecture dubbed Sinkformer,
which possesses the desirable property of being a Wasserstein gradient ﬂow.
1.3.2. Quadratic complexity of Transformers. The major computational challenge
of Transformers is their high computational complexity, particularly when process-
ing long sequences. Transformers require quadratic time and space complexity to
process sequences, because each self-attention layer contains n2 products of the
form xQxi, Kxjy (for i, j P rns). The empirical observation that the self-attention
matrix P is close to a low rank matrix—see [16, Section 4.4] for references—is cited
as the inspiration behind Linformers [27]. However, this architecture exploits the
fact that the sequence of tokens n is ﬁnite via the Johnson–Lindenstrauss lemma
but does not actually exploit the approximately low rank of P itself. Other methods
called sparse attention and block attention have been proposed to reduce the qua-
dratic complexity—see [27, Section 2.2] for references. In the spirit of these works,
a foreshadowing of the clustering mechanism was invoked in [26], where queries
are clustered into groups, again in view of reducing the quadratic complexity of
self-attention.
Compared to the usual BERT, ALBERT [15] uses parameter-sharing across layers,
meaning that the weight matrices Q, K, V in (1.1)-(1.2) do not depend on time, as
in the present paper. This does not reduce the theoretical Opn2q complexity of
the original Transformer, but, quoting [15], it "signiﬁcantly reduce[s] the number
of parameters for BERT without seriously hurting performance, thus improving
parameter-eﬃciency.
An ALBERT conﬁguration similar to BERT-large has 18x
fewer parameters and can be trained about 1.7x faster. The parameter reduction
techniques also act as a form of regularization that stabilizes the training and helps
with generalization".

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
5
1.3.3. Neural collapse. Our results and conclusions bear a resemblance to some
geometric aspects of neural collapse for classiﬁcation tasks [19]. A key geometric
aspect of neural collapse is the observation that, during the training of deep neural
networks, the representation of diﬀerent classes in the later layers of the network
tends to form a tight cluster around the vertices of a simplex. The emergence of a
simplex structure in the representation space provides insights into how the neural
network organizes and separates the diﬀerent classes.
1.3.4. Clustering in interacting particle systems. The form of (1.1) allows one to
draw parallels with the vast literature on nonlinear systems arising in the modeling
of opinion dynamics and ﬂocking phenomena. The model which is most similar to
(1.1) is the Krause model [14]
9xiptq “
n
ÿ
j“1
aijpxjptq ´ xiptqq,
aij “
φp}xi ´ xj}2q
řn
k“1 φp}xi ´ xk}2q.
which is non-symmetric in general (aij ‰ aji), much like (1.1). When φ is compactly
supported, it has been shown in [13] that the particles xiptq assemble in several
clusters as t Ñ `8. Other models of opinion dynamics and ﬂocking have been
proposed and studied, among which the Vicsek model [24], the Hegselmann-Krause
model [11] and the Cucker-Smale model [4].
These models may also exhibit a
clustering behavior under various assumptions (see [18, 3, 7] and the references
therein).
2. Asymptotic low-rankness of the self-attention matrix
As mentioned in Section 1.3, numerical experiments in [27] show that the self-
attention matrix P, deﬁned in (1.2), has an almost low-rank structure. This obser-
vation has then been leveraged to reduce the quadratic complexity in the sequence
length n which is inherent to Transformers, resulting in a non-negligible decrease
in the cost of training.
Pσ


1
0
. . .
0
...
...
...
...
1
0
. . .
0
∗
∗
. . .
∗
0
. . .
0
1
...
...
...
...
0
. . .
0
1


Figure 2. Matrices in P.
Here Pσ is an arbitrary nˆ
n matrix which permutes
the rows of the right factor.
Asterisks denote arbitrary
non-negative real numbers
which add up to 1.
As a warm-up to deriving complete geometric repre-
sentations of the dynamics, our ﬁrst result shows, in the
simple 1d case that Pptq indeed converges exponentially
fast toward a matrix which is typically both Boolean and
low-rank (see Figure 3). Although there are clear ob-
structions to a rigorous extension of this result to higher
dimensions (Remark 7.9), numerical experiments appear
to show that this result holds in greater generality, for
almost all initial sequences (Section 11).
To set this up, we introduce the set P of n ˆ n ma-
trices having the form illustrated in Figure 2, where
the asterisks denote arbitrary non-negative real numbers
which add up to 1. The row of asterisks may actually
be any row between the ﬁrst and the last one.
Theorem 2.1 (Self-attention matrix converges to a
low-rank Boolean matrix). Let d “ 1.
Suppose that
the scalars pQ, K, V q satisfy V
ą 0 and QK ą 0.
For any initial sequence of pairwise distinct tokens

6
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
px1p0q, . . . , xnp0qq P Rn, there exists some P ˚ P P such that the self-attention
matrix Pptq deﬁned in (1.2) converges to P ˚ as t Ñ `8.
Remark 2.2. The rate of convergence toward P ˚ is in fact doubly exponential in
t for coeﬃcients outside the row of asterisks in Figure 2. The proof the theorem
also reveals that for almost all initial sequences of pairwise distinct tokens, P ˚ is
actually of rank 1 or 2, i.e., the row of asterisks is equal to either e1 “ p1, 0, . . . , 0q
or en “ p0, . . . , 0, 1q.
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 0.0, rank= 11
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 3.0, rank= 23
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 5.0, rank= 14
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 10.0, rank= 2
Figure 3. An illustration of the asymptotics of Pptq entailed by Theo-
rem 2.1 for n “ 40 tokens, with Q “ K “ 1 and V “ 1. (See Section 11
for details on computing.) Increasing n has no eﬀect on this behavior of
Pptq—see Figure 10.
The proof may be found in Section 7. The interpretation of Theorem 2.1 is that
in the 1d case, at most three tokens capture the attention of all tokens except at
most one. Typically, these leading tokens are those carrying the largest amount of
information. This is also illustrated in Figure 4. Since the tokens xi here evolve
on R, the right-most and left-most ones (which typically tend toward ˘8) capture
the attention of all the others.
t = 0.0
t = 2.0
t = 9.0
Figure 4. The clouds tKxiptquiPr20s (green) and tQxjptqujPr20s for d “
2 where pairwise points of clouds are connected by a line of width equal
to Pijptq. Here V ą 0 and Q ą 0 are random matrices and K “ I2.
The creation of clusters is reﬂected by the rank ď 2 structure of the
self-attention matrix Pptq. This interaction echoes ﬁndings illustrated
in the original paper [23]—for instance, Figures 3-5 therein.

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
7
3. Clustering toward vertices of convex polytopes
In the rest of the paper, we seek to taxonomize various clustering results for
the solutions to (3.1) when t Ñ `8, depending the sign and the multiplicity of
the eigenvalues of V . We begin by focusing on what may appear to be the most
natural2 case V “ Id, as is done in [22].
The transformer dynamics considered in (1.1) does not contain a layer normal-
ization mechanism typically encountered in practice [23]. This heuristic is used to
prevent tokens diverging to inﬁnity as in Theorem 2.1 for d “ 1. In fact, the norm of
the tokens xiptq typically diverges toward `8 exponentially fast for any dimension
d: this is expected, by analogy with the non-trivial solutions to 9yptq “ yptq.
To remedy this situation, we take inspiration from the solution yptq “ etV yp0q
to 9yptq “ V yptq. Namely, for any i P rns we consider the rescaled tokens
ziptq :“ e´tV xiptq,
which solve
9ziptq “
n
ÿ
j“1
˜
exQetV ziptq,KetV zjptqy
řn
k“1 exQetV ziptq,KetV zkptqy
¸
V pzjptq ´ ziptqq
for t P r0, `8q.
(3.1)
The initial condition remains the same: xip0q “ zip0q for any i P rns. More im-
portantly, the coeﬃcients of the self-attention matrix for the rescaled tokens ziptq
are the same as those for the original tokens xiptq. Whence, the conclusion of The-
orem 2.1 also applies to the dynamics (3.1). We see this rescaling of tokens as a
mathematically justiﬁed surrogate for the layer normalization.
The appearance of the exponential factor within the self-attention kernel facili-
tates the analysis of (3.1) compared to (1.1), and it is in fact instrumental in the
proofs of all results that follow. Each result on the rescaled tokens ziptq then gives
information on the dynamics of the original tokens xiptq by virtue of the relation
xiptq “ etV ziptq.
We are now in a position to state the main result of this section on the case
V “ Id. The following theorem shows that the tokens ziptq evolving per dynamics
(3.1) converge to the boundary of a convex polytope as t Ñ `8. We present here
a simpliﬁed, but slightly weaker, version of our result for convenience and refer the
reader to Theorem 8.1 in the Section for a complete mathematical statement.
Theorem 3.1 (Convergence toward points on the boundary of a convex poly-
tope). Suppose V “ Id and QJK ą 0. Then, for any initial sequence of tokens
pz1p0q, . . . , znp0qq P pRdqn, there exists a convex polytope K Ă Rd such that for any
i P rns, ziptq converges either to 0 or to a point of the boundary BK as t Ñ `8.
The convex polytope K is completely determined by the initial sequence of tokens
(see Claim 1). Numerical experiments also lead us to claim that for almost all initial
sequences of tokens, one should expect convergence of ziptq, for i P rns, toward some
vertex of K as t Ñ `8. Furthermore, the number of vertices of K is often found to
be signiﬁcantly smaller than n. However, it may happen that for initial sequences
taken in a null set (which are not seen when taking the tokens in the sequence at
2Note that the case V “ ´Id may appear equally natural. For such a choice of V , we show
in Section 8.2 that the dynamics converge to a single cluster located at the origin. Multiplicative
constants preserving the sign, i.e., V “ ˘cId, c ą 0 trivially yield the same same conclusions.

8
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
random), some tokens converge to other points of the boundary BK, in the interior
of facets.
−5
0
5
−5
0
5
−5
0
5
t = 0.0
−5
0
5
−5
0
5
−5
0
5
t = 1.0
−5
0
5
−5
0
5
−5
0
5
t = 2.0
−5
0
5
−5
0
5
−5
0
5
t = 5.0
Figure 5. A toy example illustrating Theorem 3.1 with n “ 40 tokens
in R3. Here Q “ K “ I3. The tokens converge to one of the vertices
(leaders) of the limiting convex polytope.
Recall that the points xiptq “ etziptq when V “ Id follow the original dynam-
ics (1.1). Akin to Theorem 2.1, this result also shows the emergence of a set of
leaders (given by the vertices of K) attracting all tokens as t grows. It has been
experimentally observed (ﬁrst in [23]) that in trained Transformers, tokens focus
their attention on local leaders in a way that seems to reproduce the syntactic and
semantic structure of sentences.
The proof of Theorem 3.1 is postponed to Section 8, and is quite close in spirit
to that of Theorem 4.2: in the rescaled dynamics (3.1), a factor et appears in
the exponents which ampliﬁes the attraction exerted by the points located on the
boundary of the convex hull convptziptquiPrnsq. All points ziptq then converge toward
the boundary of the limiting convex polytope K, deﬁned as the limit of the convex
hulls convptziptquiPrnsq as t Ñ `8.
Remark 3.2 (Rate of convergence). Although Theorem 3.1 (as well as Theorems
4.2 and 5.2 stated below) does not specify a rate of convergence toward BK, we expect
(and observe through numerics) that convergence happens very quickly—after few
layers, most tokens are already clustered. What "few layers" means here necessarily

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
9
depends on the typical modulus of the initial tokens, since the dynamics (1.1) is not
invariant under multiplication of all initial conditions by a ﬁxed real number.
Remark 3.3 (Discrete time). As alluded to in Remark 1.1, all our results extend to
the discrete-time Transformers (1.3). Indeed, just as in the continuous-time case,
there is a natural rescaled dynamics, which is the discrete analogue of (3.1): if we
set R “ Id`V ∆t, and assume that R is invertible (which is the case for suﬃciently
small ∆t), then zipk∆tq “ R´kxipk∆tq :“ zrks
i
satisﬁes
zrk`1s
i
“ zrks
i
` ∆t
n
ÿ
j“1
¨
˝
e
A
QRkzrks
i
,KRkzrks
j
E
řn
ℓ“1 e
A
QRkzrks
i
,KRkzrks
ℓ
E
˛
‚R´1V
´
zrks
j
´ zrks
i
¯
,
for k P N. The proofs of Theorems 2.1, 8.5, 3.1, 4.2, and 5.2 carry through with
straightforward modiﬁcations.
4. Clustering toward hyperplanes
While being a natural example to consider, value matrices found empirically are
much more general than V “ Id, which we considered in the previous section. We
now turn our attention to a signiﬁcantly more general setting of value matrices,
which we formalize as follows.
Deﬁnition 4.1. We call pQ, K, V q a good triple if the two following conditions are
satisﬁed:
‚ the eigenvalue of V with largest modulus is real, positive, and simple; namely,
λ1pV q ą |λ2pV q| ě . . . ě |λdpV q|.
‚ xQϕ1, Kϕ1y ą 0 for any ϕ1 P Rd lying on the line kerpV ´ λ1pV qIdq.
The second condition simply states that the quadratic form x¨, K¨y is positive
deﬁnite along the eigenspace associated to the leading eigenvalue of V . Note also
that if all entries of V are positive, the ﬁrst condition is automatically satisﬁed by
virtue of the Perron-Frobenius theorem.
Our clustering result in the setting of good triples can be summarized as follows:
the coordinate xziptq,
ϕ1
}ϕ1}y of any token ziptq along the eigenspace spanned by ϕ1
converges, as t Ñ `8, toward one among possibly 3 real scalars. Consequently, all
the tokens ziptq converge toward one among at most three parallel hyperplanes; see
Figure 6 for an illustration.
Theorem 4.2 (Convergence toward ď 3 hyperplanes). Assume that pQ, K, V q is a
good triple in the sense of Deﬁnition 4.1. Then, for any initial sequence of tokens
pz1p0q, . . . , znp0qq P pRdqn, there exist at most three parallel hyperplanes in Rd such
that for any i P rns, the distance of the solution ziptq to (3.1) to one of these
hyperplanes converges to 0 as t Ñ `8.
The proof may be found in Section 9. The important role played by λ1pV q in the
dynamics may be seen in (3.1): the component of ziptq along ϕ1 determines the size
of etV ziptq in the exponent appearing in (3.1). The tokens zjptq attracting other
tokens ziptq are those for which this component along ϕ1 is largest in modulus. This
attraction process forms the clusters. These leaders, as in all our results, have been
empirically observed to be the ones carrying the largest amount of information in
the sentence (see Supplementary material in [23]).

10
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
0
10
−10
0
10
t = 0.0
0
10
−10
0
10
t = 1.0
0
10
−10
0
10
t = 5.0
0
10
−10
0
10
t = 15.0
Figure 6. Illustrating Theorem 4.2 with n “ 40 tokens in R2. Here Q “
K “ I2, V is a random symmetric matrix with eigenvalues t1.35, ´0.07u,
and ϕ1 “ p0.76, 0.65q. The components of the tokens in the direction of
ϕ1 (orange arrow) cluster over time. (See Figures 12–13 for examples
in R3.) We also observe that tokens typically cluster toward only two
hyperplanes—a third one (passing through the origin) may appear for
non-generic initial sequences. The hyperplanes are perpendicular to ϕ1
since V is diagonalizable.
Furthermore, Theorem 4.2 can also be interpreted in more classical machine
learning terms. On the one hand, it can be seen as an instance of K-ﬂats clus-
tering [1, 25]—points in the input sequence are clustered, based on their intrinsic
similarity, to at most 3 "ﬂats" of dimension d ´ 1. On the other hand, it ensues
that for a good triple pQ, K, V q, (3.1) generates a linearly separable representation
of tokens.
Beyond a single direction? Numerical experiments (Figure 7 for instance) indi-
cate that a similar phenomenon may emerge when V has a more complex spectrum.
We formulate following conjecture which is a natural generalization of Theorem 4.2.
Conjecture (Codimension conjecture). Let k ě 1 be the number of eigenvalues of
V with positive real part. Then there exist at most three parallel Euclidean subspaces
of Rd of codimension k such that for any i P rns, the distance of ziptq to one of
these subspaces converges to 0 as t Ñ `8.
5. A mix of hyperplanes and polytopes
We now turn our attention to an even more general version of Theorem 4.2, which
does not require the leading eigenvalue of V to be simple. The resulting theorem
can be viewed as a combination of Theorem 4.2 and Theorem 3.1. Speciﬁcally,
we assume that V behaves as the identity when acting on the eigenspace of the
leading eigenvalue. This property is automatically satisﬁed if V is normal—so that
its eigenvectors form an orthonormal basis—so we call such a V paranormal.
Deﬁnition 5.1. We call pQ, K, V q a good triple with multiplicity if the following
conditions are satisﬁed:
‚ QJK is positive deﬁnite: QJK ą 0;
‚ V is paranormal: there exist two linear subspaces F, G Ă Rd which are
invariant under V , and such that F‘ G “ Rd, V|F “ λId for λ ą 0, and

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
11
−5
0
5
−5
0
5
−5
0
5
t = 0.0
−10
0
10
−10
0
10
−5
0
5
t = 5.0
−40
−20
0
20
−25
0
25
−10
0
10
t = 10.0
−100 −50
0
50
−100
0
100
−40
−20
0
20
t = 15.0
Figure 7. Illustrating the claim of Conjecture 4 with n “ 40 tokens
in R3. Here Q “ K “ I3 and V is a random matrix with eigenvalues
t1.96, ´0.22, 0.25u.
We see that the k “ 2 positive eigenvalues of V
generate attraction between the tokens ziptq, and even convergence in
the corresponding eigenspaces—this explains the codimension k state-
ment in the conjecture. The negative eigenvalue generates a repulsive
eﬀect between the tokens, and we see a divergence along two lines as t
is increased (note the diﬀerent scales between the four ﬁgures).
ρpV|Gq ă λ, where ρp¨q denotes the spectral radius (the maximal modulus of
eigenvalues).
An example of such a V is used for Figure 8. We may now state our main result
in the setting of good triples with multiplicity. The proof may be found in Section
10.
Theorem 5.2 (Clustering for λ1 with multiplicity). Suppose that pQ, K, V q is a
good triple with multiplicity in the sense of Deﬁnition 5.1. Then, for any initial
sequence pz1p0q, . . . , znp0qq P pRdqn, there exists a bounded convex polytope K Ă F
such that setting H :“ pBK Y t0uq ˆ G, for any i P rns, we have distpziptq, Hq Ñ 0
as t Ñ `8.

12
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
−5
0
5
−5
0
5
−5
0
5
t = 0.0
−5
0
5
−5
0
5
−50
−25
0
t = 5.0
−5
0
5
−5
0
5
−500
0
t = 10.0
−5
0
5
−5
0
5
−10000
−5000
0
t = 15.0
Figure 8. Illustrating Theorem 5.2 with n “ 40 tokens in R3. As be-
fore, Q “ K “ Id, and we take V “ diagp1, 1, ´ 1
2q. A convex polytope
K emerges before time 5, toward which two coordinates of the tokens
cluster, and persists throughout the evolution, while the tokens diverge
along the coordinate corresponding to the eigenvalue ´ 1
2 (note the dif-
ferent scales between the four ﬁgures).
6. Well-posedness
We collect several facts regarding the global-in-time existence and uniqueness
of solutions to all systems under consideration. Throughout the remainder of the
paper, we use the terminology "tokens" and "particles" interchangeably.
To prove these results, we leverage the underlying continuity equation (see (6.1)),
the setup for which requires several well-known notions.
6.1. Notation. We denote by PcpRdq the set of compactly supported probability
measures on Rd, and by P2pRdq the set of probability measures µ on Rd having ﬁnite
second moment:
ş
Rd }x}2 dµpxq ă `8. Let C0pR; PcpRdqq denote the Banach space
of continuous curves R Q t ÞÑ µptq P PcpRdq. Here PcpRdq is endowed with the weak
topology, which coincides with the topology induced by the Wasserstein distance Wp
for any p P r1, `8q. As seen below, for compactness purposes regarding solutions
to the continuity equation, we consider an additional property on the support for
such curves, summarized by the following deﬁnition.

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
13
Deﬁnition 6.1 (Equi-compactly supported curves). The set C0
copR; PcpRdqq con-
sists of all elements µ P C0pR; PcpRdqq such that for any t0, t1 P R, there exists a
compact subset K Ă Rd such that supppµptqq Ă K for any t P rt0, t1s.
We emphasise that there exist elements in C0pR; PcpRdqq which do not satisfy
this property with regard to their support—e.g., µptq “ p1 ´ e´ 1
t2 qδ0 ` e´ 1
t2 δ 1
t .
6.2. Well-posedness of the ODEs. For any initial datum, i.e. a sequence of n
points in Rd, the dynamics (1.1) is well-posed, in the sense that it admits a unique
solution deﬁned for all times.
Proposition 6.2. For any initial datum X0 “ px0
1, . . . , x0
nq P pRdqn, there exists a
unique Lipschitz continuous function R Q t ÞÑ Xptq “ px1ptq, . . . , xnptqq such that
xip¨q solves (1.1) and satisﬁes xip0q “ x0
i for any i P rns.
We postpone the proof which is seen as a corollary of the well-posedness for
the corresponding continuity equation. It follows that the equation (3.1) is also
well-posed:
Proposition 6.3. For any initial datum Z0 “ pz0
1, . . . , z0
nq P pRdqn, there exists a
unique Lipschitz continuous function R Q t ÞÑ Zptq “ pz1ptq, . . . , znptqq such that
zip¨q solves (3.1) and satisﬁes zip0q “ z0
i for any i P rns.
Proof of Proposition 6.3. Since the equations (1.1) and (3.1) are related by the
change of variables xiptq “ etV ziptq, Proposition 6.3 is an immediate consequence
of Proposition 6.2.
□
6.3. The continuity equation. To prove Proposition 6.2, we ﬁrst prove a more
general result concerning global existence and uniqueness of solutions for the cor-
responding continuity equation3
#
Btµ ` ∇x ¨ pXrµsµq “ 0
in p0, `8q ˆ Rd
µ|t“0 “ µ0
in Rd,
(6.1)
when Xrµs is the attention kernel
Xrµspxq :“
ż
Rd exQx,KyyV y dµpyq
ż
Rd exQx,Kyy dµpyq
.
(6.2)
We be making use of the following notion of solution.
Deﬁnition 6.4. Fix µ0 P PcpRdq. We say that t ÞÑ µptq “: µt is a solution to the
Cauchy problem (6.1) if µ P C0
copR, PcpRdqq, the function
R Q t ÞÑ
ż
Rd gpxq dµtpxq
is absolutely continuous for every g P C8
c pRdq, and
ż
Rd gpxq dµtpxq “
ż
Rd gpxq dµ0pxq `
ż t
0
ż
Rd x∇gpxq, Xrµtspxqy dµspxq ds
holds for almost every t P R.
3which can be seen as a mean-ﬁeld limit, and is sometimes also referred to as a Vlasov equation.

14
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
The following general existence and uniqueness result is adapted from [21, The-
orem 2.3]. In Proposition 6.5, Xrµs is an arbitrary vector ﬁeld, and we check in
Corollary 6.6 that the assumptions of Proposition 6.5 are veriﬁed for Xrµs given
by (6.2).
Proposition 6.5. Suppose that for any R ą 0, there exists a constant CR ą 0 such
that for any µ, ν P PcpRdq with support in Bp0, Rq, and for every px, yq P Bp0, Rq2,
there holds
}Xrµspxq ´ Xrµspyq} ď CR}x ´ y},
(6.3)
}Xrµspxq} ď CR,
(6.4)
}Xrµsp¨q ´ Xrνsp¨q}L8pRdq ď CRW2pµ, νq.
(6.5)
Then, for any µ0 P PcpRdq, the Cauchy problem (6.1) has a unique solution µ P
C0
copR; PcpRdqq in the sense of Deﬁnition 6.4. Furthermore, we have the following
stability estimate for solutions: for any R ą 0 and T ą 0, there exists a constant
CT,R ą 0 such that for all µ0, ν0 P PcpRdq with support in Bp0, Rq,
W2pµptq, νptqq ď eCT,RtW2pµ0, ν0q
(6.6)
for any t P r0, Ts, where µptq and νptq solve (6.1) with initial conditions µ0 and ν0
respectively.
Results of this nature can be found in the literature—see for instance [21]. They
are however not suﬃcient for our purposes. We wrote Proposition 6.5 in the W2
setting instead of the usual W1 (used for instance for the classical Dobrushin esti-
mate [5, 6]) because it allows to extend the results of [29] without diﬃculty from
classical ResNets to self-attention dynamics. We recall that the goal of [29] is to
import classical (mean-ﬁeld) optimal control tools such as the Pontryagin maximum
principle and the analysis of Hamilton-Jacobi-Bellman equations to deep learning,
and relies heavily on W2 estimates (e.g, in [29, Section 4]).
Proof. To ease reading, we split the proof in two parts.
Part 1: Existence. Given a vector ﬁeld X “ Xpt, xq satisfying the assumptions
of the Cauchy-Lipschitz theorem, we henceforth denote by Φt
X the ﬂow of diﬀeo-
morphisms of Rd generated by the vector ﬁeld X—namely, the unique solution to
the Cauchy problem
#
BtΦpt, xq “ Xpt, Φpt, xqq
t P R,
Φp0, xq “ x
for any ﬁxed x P Rd. Fix an arbitrary T ą 0. For k P N, we set τk :“
T
2k , and we
deﬁne a sequence of curves µk : r0, Ts Ñ PcpRdq by the following scheme:
(i) µkp0q “ µ0;
(ii) µkpℓτk ` tq “
´
Φt
Xrµkpℓτkqs
¯
# µkpℓτkq for ℓP t0, . . . , 2k ´ 1u and t P p0, τks.
In other words, to deﬁne µk, we "freeze" the vector ﬁeld X on each interval of
the form rℓτk, pℓ` 1qτkq. For any k P N, by recurrence over ℓP t0, . . . , 2k ´ 1u and
using (6.4), we can readily that the measures µkptq, for t P r0, Ts, have equibounded
support. Then (6.4) together with point (ii) yields the inequality
W2
`
µkpℓτk ` tq, µkpℓτkq
˘
ď CRt

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
15
for any ℓP t0, . . . , 2k ´ 1u and t P p0, τks. Gluing these inequalities (for diﬀerent ℓ
and t) with the triangle inequality yields
W2
`
µkptq, µkpsq
˘
ď CR|t ´ s|
for any t P r0, Ts.
Since µkp0q “ µ0 for any k P N, and since P2pRdq is the
completion of Pc for the Wasserstein distance W2, the Ascoli-Arzelà theorem implies
the existence of a subsequence uniformly converging to some µ˚ : r0, Ts Ñ P2pRdq.
Since the curves µkptq, for t P r0, Ts and k P N, have equi-bounded support, we
even deduce that µ˚ P C0
copR, PcpRdqq. Note moreover that µ˚p0q “ µ0 and that
W2pµ˚ptq, µ˚psqq ď CR|t ´ s|
for any t, s P r0, Ts.
The fact that µ˚ is a solution of (6.1) follows exactly from the same computations
as in [21, p. 4711-4712], starting from (A.2) therein. We do not reproduce here this
argument since the computations are the same word for word. The fact that for any
T ą 0 we have suptPr0,T s W1pµ˚ptq, µkptqq Ñ 0 as k Ñ `8, which is instrumental
in [21, p.
4711-4712], follows in our case from the following inequality relating
Wasserstein distances of diﬀerent orders: for any p ě 1 and any bounded set B, for
all Radon measures µ, ν supported in B,
W1pµ, νq ď Wppµ, νq ď diampBq1´ 1
p W1pµ, νq1{p.
(6.7)
Part 2: Uniqueness. Regarding uniqueness, we proceed as follows. We ﬁrst recall
the following estimate (see [20]): if p ě 1, v and w are two bounded and Lipschitz
vector ﬁelds on Rd, of Lipschitz constant L, and µ, ν P PcpRdq, then
Wp
`
pΦt
vq#µ, pΦt
wq#ν
˘
ď e
p`1
p LtWppµ, νq ` e
Lt
p peLt ´ 1q
L
}v ´ w}C0pRdq.
(6.8)
Let us assume that there are two solutions µ and ν of (6.1), whose spatial
support is locally bounded in time, having the same initial condition. Set vpt, xq :“
Xrµptqspxq and wpt, xq :“ Xrνptqspxq. We also deﬁne
t0 :“ inftt ě 0 | W2pµptq, νptqq ‰ 0u,
and we assume that t0 ‰ `8. We ﬁx T ą t0 and take R ą 0 such that µt and νt
are supported in Bp0, Rq for any t P r0, Ts. Using (6.8) with p “ 2, we have
W2pµpt0 ` sq, νpt0 ` sqq ď e2CRsW2pµpt0q, νpt0qq
` eCRs eCRs ´ 1
CR
sup
τPrt0,t0`ss
}vpτ, ¨q ´ wpτ, ¨q}C0pRdq.
We choose s ą 0 suﬃciently small so that eCRs ´ 1 ď 2CRs. Then, by virtue of
(6.5) and the fact that W2pµpt0q, νpt0qq “ 0, we ﬁnd
W2pµpt0 ` sq, νpt0 ` sqq ď 2CRseCRs
sup
τPrt0,t0`ss
W2pµpτq, νpτqq.
(6.9)

16
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We choose s1 ą 0 satisfying both eCRs1 ´ 1 ď 2CRs1 and 2CRs1eCRs1 ă 1. Applying
(6.9) to every s P r0, s1s we obtain
sup
sPr0,s1s
W2pµpt0 ` sq, νpt0 ` sqq ď 2CRs1eCRs1
sup
τPrt0,t0`s1s
W2pµpτq, νpτqq
ă
sup
sPr0,s1s
W2pµpt0 ` sq, νpt0 ` sqq,
which is clearly a contradiction. Therefore µptq “ νptq for any t ě 0, which proves
uniqueness, as desired.
Part 3: Stability. We do not detail the proof of estimate (6.6), which is very
similar to the proof of (2.3) in Theorem 2.3 of [21]: it follows from (6.8) with p “ 2,
and the argument after (A.7) in [21], with W2 instead of W1.
□
Corollary 6.6. Fix µ0 P PcpRdq. There exists a unique solution µ P C0
copR, PcpRdqq
to the Cauchy problem (6.1), with X as in (6.2).
Proof. It suﬃces to check that the assumptions of Proposition 6.5, namely (6.3),
(6.4) and (6.5), hold for X as in (6.2). As |V y| ď }V }opR whenever y P Bp0, Rq,
(6.4) readily follows.
Let us show (6.3), which boils down to showing a bound on the gradient of the
map x ÞÑ Xrµspxq. To this end, we set
Gpx, yq :“ exQx,Kyy.
When we compute the gradient of x ÞÑ Xrµspxq, we obtain a diﬀerence of two
terms. The ﬁrst one is
ż
Rd ∇xGpx, yqV y dµpyq
ż
Rd Gpx, yq dµpyq
,
whose modulus is bounded above by }V }op}QJK}opR2. The second term is
¨
˚
˚
˝
ż
Rd Gpx, yqV y dµpyq
ż
Rd Gpx, yq dµpyq
˛
‹‹‚
¨
˚
˚
˝
ż
Rd ∇xGpx, yq dµpyq
ż
Rd Gpx, yq dµpyq
˛
‹‹‚.
The modulus of the ﬁrst fraction is readily seen to be bounded from above by
}V }opR. On the other hand, the modulus of the second fraction is also bounded
from above, since the denominator is bounded from below by 1. Whence, (6.3)
follows.
We ﬁnally prove (6.5). If we use the fact that
ş
Rd Gpx, yq dµpyq and
ş
Rd Gpx, yq dνpyq
are bounded from below by 1, we see that it would suﬃce to bound from above
above the following quantity:
ˇˇˇˇ
ż
Rd Gpx, yqV y dµpyq
ż
Rd Gpx, yq dνpyq ´
ż
Rd Gpx, yqV y dνpyq
ż
Rd Gpx, yq dµpyq
ˇˇˇˇ .
We rewrite this diﬀerence by making µ´ν appear artiﬁcially, and we then use the tri-
angle inequality along with the fact that both
ş
Rd Gpx, yqV y dµpyq and
ş
Rd Gpx, yq dµpyq
are bounded from above (by e}QJK}opR2 maxp1, }V }opRq). We thus end up with the

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
17
task of bounding from above the absolute values of
ż
Rd Gpx, yqp dν ´ dµqpyq
and
ż
Rd Gpx, yqV yp dν ´ dµqpyq.
(6.10)
For the ﬁrst integral, from Kantorovich-Rubinstein duality we deduce
ˇˇˇˇ
ż
Rd Gpx, yqp dν ´ dµqpyq
ˇˇˇˇ ď LippGpx, ¨qqW1pµ, νq.
Then, using (6.7) and the fact that the Lipschitz constant LippGpx, ¨qq is uniformly
bounded for }x} ď R (for any R ą 0) by some constant CR ą 0 in the inequality
just above, we end with
ˇˇˇˇ
ż
Rd Gpx, yqp dν ´ dµqpyq
ˇˇˇˇ ď CRW2pµ, νq.
The same chain of inequalities applies to the second integral in (6.10) (with the
additional multiplier }V }opR), which ﬁnally leads us to (6.5).
□
We conclude this section with the proof of Proposition 6.2, which follows as a
corollary of the above derivations.
Proof of Proposition 6.2. We ﬁrst show existence.
We apply Corollary 6.6 with
µ0 :“ 1
n
řn
j“1 δx0
i , which in turn yields a solution µptq to (6.1). Following the proof
of Proposition 6.5, we also know that this solution satisﬁes µptq “ pΦXrµptqsq#µ0
for any t P R, and the vector ﬁeld Xrµptqs satisﬁes the assumptions of the Cauchy-
Lipschitz theorem. In particular, µptq is of the form µptq “ 1
n
řn
j“1 δxiptq for some
Lipschitz curves R Q t ÞÑ xiptq, for i P rns. Then t ÞÑ µptq “
1
n
řn
j“1 δxiptq is a
solution to the Cauchy problem (6.1)-(6.2) in the sense of Deﬁnition 6.4.
Secondly, we show uniqueness. Suppose that Xptq “ px1ptq, . . . , xnptqq and X˚ptq
are two Lipschitz solutions to (1.1), with the same initial conditions. Then for a.e.
t ě 0, using the equation (1.1) and the fact that the attention matrix coeﬃcients
Pijptq deﬁned in (1.2) belong to r0, 1s, we obtain
1
2
d
dt max
iPrns }xiptq}2 ď }V }op max
iPrns }xiptq}2
(and analogously for x˚
i ptq). Using Grönwall’s inequality, we deduce the existence
of two constants c1, c2 ą 0 such that for any t ą 0 and for any i P rns, }xiptq}
and }x˚
i ptq} are bounded from above by c1ec2t. It then follows that the empirical
measures µp¨q “ 1
n
řn
j“1 δxip¨q and µ˚p¨q “ 1
n
řn
j“1 δx˚
i p¨q belong to C0
copR, PcpRdqq.
Moreover, they satisfy µptq “ pΦXrµptqsq#µ0 and µ˚ptq “ pΦXrµ˚ptqsq#µ0 and are
thus solutions to (6.1). Using the uniqueness result of Corollary 6.6, we obtain that
µ “ µ˚ which concludes the proof.
□
7. Proof of Theorem 2.1
Throughout this section we focus on the following dynamics:
9xiptq “
n
ÿ
j“1
ˆ
exxiptq,xjptqy
řn
k“1 exxiptq,xkptqy
˙
xjptq.
(7.1)
Note that for d “ 1, the dot products in (7.1) are just multiplications of scalars.
We begin by recalling the following result, which holds for any d ě 1.

18
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Lemma 7.1. For any x1, . . . , xn P Rd, the function f : Rd Ñ R deﬁned as
f : x ÞÑ log
˜ n
ÿ
j“1
exx,xjy
¸
(7.2)
is convex.
Proof. Using the elementary inequality pa ` bq ě 2pabq
1
2 for any a, b ě 0, we have
exppfpxq ` fpyqq “
˜ n
ÿ
j“1
exppxx, xjyq
¸ ˜ n
ÿ
j“1
exppxy, xjyq
¸
“ 1
2
n
ÿ
j“1
n
ÿ
k“1
exp pxx, xjy ` xy, xkyq ` exppxx, xky ` xy, xjyq (7.3)
ě
n
ÿ
j“1
n
ÿ
k“1
exp
ˆBx ` y
2
, xj ` xk
F˙
(7.4)
“ exp
ˆ
2f
ˆx ` y
2
˙˙
.
Taking the log on both sides yields the statement.
□
The following lemma also holds for any d ě 1.
Lemma 7.2. Let R Q t ÞÑ px1ptq, . . . , xnptqq be a solution to (7.1). For any i, j P
rns, the map R Q t ÞÑ }xiptq ´ xjptq} is non-decreasing.
Proof. The dynamics (7.1) can be equivalently written as
9xiptq “ ∇fpxiptqq
where f is as in (7.2). By convexity of f (Lemma 7.1),
1
2
d
dt}xiptq ´ xjptq}2 “ x 9xiptq ´ 9xjptq, xiptq ´ xjptqy
“ x∇fpxiptqq ´ ∇fpxjptqq, xiptq ´ xjptqy ě 0,
as desired.
□
We now present the proof of Theorem 2.1, which assumes d “ 1. We recall that
in the statement, V is a positive scalar, but by reparametrizing time we may assume
that V “ 1, so the 1d dynamics under consideration is really given by (7.1). Also,
to ease notations we focus on QK “ 1, but the proof adapts straightforwardly to
the setting QK ą 0 assumed in the statement of Theorem 2.1.
As seen in Section 7.1, it is not diﬃcult to prove the convergence of the coeﬃcients
Pijptq of the attention matrix for indices i P rns for which xiptq becomes unbounded
as t Ñ `8. This is the case for at least n ´ 1 of the particles xiptq (Lemma 7.6).
But should one particle xiptq remains bounded, proving the convergence of Pijptq
for j P rns is slightly tedious (Section 7.2).
Since d “ 1, up to relabelling, we can order the initial collection of particles
(which, we recall, are assumed distinct):
x1p0q ă . . . ă xnp0q.
(7.5)
We set
c :“
min
iPrn´1s |xi`1p0q ´ xip0q|.
(7.6)

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
19
According to Lemma 7.2, we have |xiptq ´ xjptq| ě c for any i ‰ j and any t ě 0.
In particular, tokens never "collide".
7.1. Results about unbounded particles. In this section we gather several re-
sults concerning the indexes i corresponding to particles xiptq which are not uni-
formly bounded in time. In particular, in Lemma 7.4 we show that for such i, Pijptq
converges toward 0 or 1 for any j P rns.
Lemma 7.3. Let A ą 0 denote the unique positive real number satisfying
A2 “ n2 expp´A2q.
If xnpt0q ą A for some time t0 ě 0, then there exists c1 ą 0 such that xnptq ě c1et
for any suﬃciently large t ą 0. Similarly, if x1pt0q ă ´A for some t0 ě 0, then
x1ptq ď ´c1et for any suﬃciently large t ą 0.
Proof. The two cases are symmetric since the evolution (7.1) commutes with the
involution of pRdqn given by px1, . . . , xnq ÞÑ p´x1, . . . , ´xnq. We thus focus on the
case xnpt0q ą A.
If xnptq ě 0 for some t ě 0, then
9xnptq “
n
ÿ
j“1
ˆ
exnptqpxjptq´xnptqq
řn
k“1 exnptqpxkptq´xnptqq
˙
xjptq
(7.7)
ě
xnptq
1 ` pn ´ 1qe´cxnptq `
ÿ
tjPrns|xjptqă0u
exnptqpxjptq´xnptqqxjptq
(7.8)
ě
xnptq
1 ` pn ´ 1qe´cxnptq ´ ne´xnptq2
xnptq
(7.9)
ě xnptq
n
´ ne´xnptq2
xnptq .
(7.10)
Indeed, to pass from (7.7) to (7.8), when j “ n we use exnptqpxkptq´xnptqq ď e´cxnptq
for any k P rns by virtue of (7.6), while the sum over indices j such that xjptq ě 0
is non-negative, and, ﬁnally, řn
k“1 exnptqpxkptq´xnptqq ě 1 for all indices j such that
xjptq ă 0. To pass from (7.8) to (7.9), we use exnptqzz ě ´
1
xnptq, which holds for
any z ď 0.
For any B ą A, we clearly have
B
n ´ ne´B2
B
ą 0.
We then deduce from (7.9) and the fact that xnpt0q ą A that xnptq Ñ `8 as
t Ñ `8. Moreover due to the fact that the expression in (7.10) is bounded from
below by xnptq
2n
whenever xnptq is suﬃciently large, we deduce that xnptq ě c0e
t
2n
for any suﬃciently large t ą 0.
Coming back to (7.9), we ﬁnd that for suﬃciently large t ą 0,
9xnptq ě xnptq
˜
1
1 ` pn ´ 1qe´cc0e
t
2n ´ e´c2
0e
t
n
¸
.
This implies that
d
dt logpxnptqq ě 1 ´ O
´
e´ t
3n
¯
,

20
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
whence logpxnptqq ě t ` Op1q, as desired.
□
Lemma 7.4. If i P rns is such that xiptq is not uniformly bounded with respect to
t ą 0, then xiptq converges to either ´8 or `8 as t Ñ `8. Moreover4,
(1) if xiptq Ñ `8, then for any j P rns, Pijptq converges to δnj as t Ñ `8,
with doubly exponential rate.
(2) if xiptq Ñ ´8, then for any j P rns, Pijptq converges to δ1j as t Ñ `8,
with doubly exponential rate.
Proof. We assume that xiptq is not uniformly bounded with respect to t ą 0.
Without loss of generality, we assume that there exists a sequence of positive times
ttku`8
k“1 with tk Ñ `8 such that xiptkq Ñ `8. Necessarily, xnptkq Ñ `8. We
notice that if xiptq ą 0 for some t ě 0, then, arguing as in (7.7)-(7.8)-(7.9), we have
9xiptq “
n
ÿ
j“1
ˆ
exiptqpxjptq´xnptqq
řn
k“1 exiptqpxkptq´xnptqq
˙
xjptq ě xnptq
n
´
n
xiptqe´xiptqxnptq.
(7.11)
For suﬃciently large k, from (7.11) we get that 9xiptkq ą 0 and 9xnptkq ą 0. But as
xi and xn increase, the lower bound in (7.11) becomes larger. It follows that
9xiptq ě xnptq
2n
ě xiptq
2n
for suﬃciently large t, implying that xiptq Ñ `8 with exponential rate.
We now prove point 1. regarding Pptq. We assume that xiptq Ñ `8 as t Ñ `8.
In this case, for j ‰ n (namely j P rn ´ 1s),
Pijptq “
exiptqxjptq
řn
k“1 exiptqxkptq ď exiptqpxjptq´xnptqq ď e´cxiptq,
thus Pijptq converges to 0 as t Ñ `8 (with doubly exponential rate). Consequently,
we also deduce that Pinptq “ 1 ´ řn´1
j“1 Pijptq converges to 1, also with doubly
exponential rate.
The case where xiptq Ñ ´8 is symmetric. This concludes the proof.
□
Our last result is useful in the next section.
Lemma 7.5. For any i P rns such that xiptq is not uniformly bounded with respect
to t ą 0, there exists some γi P R, γi ‰ 0 such that xiptq “ γiet ` opetq as t Ñ `8.
Proof. Without loss of generality we assume that xiptq Ñ `8 as t Ñ `8. For
j ‰ n, we ﬁnd
Pijptq “
exiptqxjptq
řn
k“1 exiptqxkptq “
exiptqpxjptq´xnptqq
řn
k“1 exiptqpxkptq´xnptqq ď e´cxiptq.
Consequently, Pinptq ě 1´ne´cxiptq. Therefore, using Lemma 7.3 and the fact that
xiptq ě bie
t
2n for some bi ą 0 thanks to (7.11), we gather that
9xiptq ě
´
1 ´ ne´cxiptq¯
xnptq ´ ne´cxiptqc1et
ě
ˆ
1 ´ ne´cbie
t
2n
˙
xnptq ´ ne´cbie
t
2n c1et.
(7.12)
4In what follows, δjk denotes the Kronecker symbol.

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
21
We also notice that due to (7.1), 9xiptq ď xnptq. Using (7.12), ﬁrstly for i “ n,
together with the trivial upper bound xnptq ď Cet (immediately seen from (7.1))
we obtain
9xnptq “ xnptq
ˆ
1 ` o
ˆ
e´cbie
t
3n
˙˙
as t Ñ `8, which yields xnptq “ γnet ` opetq for some γn ą 0. Now using (7.12)
for the index i, we gather that
9xiptq “ xnptq ` o
ˆ
e´cbie
t
3n
˙
,
and so we deduce that xiptq “ γnet ` opetq. Similarly, should xiptq Ñ ´8, then
xiptq “ γ1et ` opetq. This proves Lemma 7.5 (and shows that γi P tγ1, γnu).
□
7.2. Results about bounded particles. In this section, we gather results con-
cerning particles which remain uniformly bounded in time. The following lemma
entails that there can be at most one particle with this property.
Lemma 7.6. Consider
B :“ ti P rns | xip¨q P L8pr0, `8qqu.
Then #B P t0, 1u.
Proof. We ﬁrst prove that either x1ptq Ñ ´8 or xnptq Ñ `8. By contradiction,
if this is not the case, then by Lemma 7.3, px1ptq, . . . , xnptqq P r´A, Asn for any
t ě 0. We denote by I the set of conﬁgurations px˚
1, . . . , x˚
nq P r´A, Asn such
that |x˚
i ´ x˚
j | ě |xip0q ´ xjp0q| ą 0 for any distinct i, j P rns. For any sequence
X˚ “ px˚
1, . . . , x˚
nq P I, the function f deﬁned in (7.2) (with anchor points given
by X˚) is strictly convex—the equality in the inequality between (7.3) and (7.4) is
never achieved. Therefore, the proof of Lemma 7.2 shows that if X˚ is seen as an
initial datum for the dynamics (7.1), then
vpX˚q :“ d
dt |t“0|x˚
1ptq ´ x˚
nptq| ą 0.
Since I is compact, v0 :“ infX˚PIvpX˚q ą 0. Hence, t ÞÑ |x1ptq ´ xnptq| grows at
least linearly, which is a contradiction.
We may therefore assume without loss of generality that x1ptq Ñ ´8 as t Ñ `8.
We prove that xnptq converges either to ´8, to 0 or to `8, as t Ñ `8. We assume
in the sequel that xnptq does not converge to ´8 or 0. For any i P rns, if there
exists ε ą 0 and a sequence of positive times tskukPN tending to `8 such that
xipskq ď ´ε, then it follows from (7.11) that xiptq Ñ ´8.
Therefore, by our
assumptions, we have lim inftÑ`8 xnptq ě 0. Also, since xnptq ↛0, there exists
ε ą 0 and a sequence of positive times ttkukPN tending to `8 such that xnptkq ě ε
for any k P N. For any t ě 0 such that xnptq ě ε, we introduce the set of indices
Nptq “ ti P rns | xiptq ă 0u,
and we write
9xnptq ě
exnptq2xnptq
řn
k“1 exnptqxkptq `
ř
jPNptq exjptqxnptqxjptq
řn
k“1 exnptqxkptq
ě ε
n ` 1
eε2
ÿ
jPNptq
eεxjptqxjptq.
(7.13)

22
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
According to Lemma 7.4, any point xiptq which takes negative values for arbitrarily
large times and does not converge to ´8 has to converge to 0. Therefore, the second
term in the lowermost bound in (7.13) is lower bounded by ´ ε
2n for suﬃciently large
t. All in all, we gather that 9xnptq ě
ε
2n and xnptq converges to `8 as t Ñ `8. If
it converges to 0, then necessarily xn´1ptq Ñ ´8 by combining Lemma 7.2 with
Lemma 7.4. This proves Lemma 7.6 in this case.
From now on we assume that xnptq Ñ `8. Using (7.11) we see that if there
exists ε ą 0 such that xiptq ą ε for an unbounded sequence of times t, then
xiptq Ñ `8. The same is true symmetrically when xiptq ă ´ε for an unbounded
sequence of times t. Thus if i P B, necessarily xiptq Ñ 0. By Lemma 7.2 this can
be true for at most one index i, which concludes the proof of Lemma 7.6.
□
If B “ H, Theorem 2.1 follows from Lemma 7.4. From now on, we assume that
#B “ 1, and we denote by i0 P rns its unique element. We distinguish two cases:
either i0 P t1, nu (Lemma 7.7), or i0 R t1, nu (Lemma 7.8).
Lemma 7.7. If xnptq is bounded as t Ñ `8, then Pnnptq Ñ 1 as t Ñ `8, and
for any j P rn ´ 1s, Pnjptq Ñ 0. Similarly, if x1ptq is bounded as t Ñ `8, then
P11ptq Ñ 1 as t Ñ `8, and for any j P rn ´ 1s, P1jptq Ñ 0.
Proof. The two cases (t ÞÑ xnptq bounded or t ÞÑ x1ptq bounded) are symmetric
since the evolution (7.1) commutes with the involution px1, . . . , xnq ÞÑ p´x1, . . . , ´xnq
of pRdqn. Whence, we only address the ﬁrst one: we assume that xnptq is bounded
as t Ñ `8. We ﬁrst notice that all particles xjptq for j P rn ´ 1s tend to ´8 as
t Ñ `8 due to Lemma 7.6. We now prove the following properties:
1. xnptq ą 0 for any suﬃciently large t;
2. xnptq Ñ 0 as t Ñ `8;
3. for any j P rn ´ 1s, Pnjptq Ñ 0 as t Ñ `8.
To prove point 1., we notice that for suﬃciently large t, xiptq ď 0 for any i P rn´1s.
If in addition xnptq ď 0, then due to (7.1), all xiptq, i P rns remain negative and
due to (7.1), xnptq Ñ ´8 as t Ñ `8, a contradiction.
For point 2., we ﬁx ε ą 0, and set
T `
ε
“ tt ě 0 | xnptq ě εu.
We prove that if T `
ε
is unbounded, then xnptq Ñ `8 as t Ñ `8, which is a
contradiction. As a consequence, T `
ε
is bounded for any ε ą 0, which implies (in
conjunction with point 1.) that xnptq Ñ 0 as t Ñ `8. So let us assume that T `
ε
is unbounded. We notice that for any δ ą 0, if t P T `
ε
is suﬃciently large then
|exnptqxjptqxjptq| ď δ for any j P rn ´ 1s since xjptq Ñ `8. Therefore,
n
ÿ
j“1
exnptqxjptqxjptq ě eε2ε ´ pn ´ 1qδ ě 0,
where we took δ ą 0 suﬃciently small for the last inequality. Consequently,
9xnptq “
řn
j“1 exnptqxjptqxjptq
řn
j“1 exnptqxjptq
ě exnptq2xnptq ´ pn ´ 1qδ
exnptq2 ` n ´ 1
.
It is not diﬃcult to see that this implies that xnptq Ñ `8 as t Ñ `8, which is a
contradiction.

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
23
For point 3., we ﬁrst notice that for any j ‰ n, since xjptq Ñ ´8,
9xjptq “
n
ÿ
k“1
ˆ
exjptqpxkptq´xnptqq
řn
ℓ“1 exjptqpxℓptq´xnptqq
˙
xkptq ď x1ptq
n
` n
ε e´xjptqxnptq.
Using Lemma 7.3, we deduce the existence of some c2 ą 0 such that xjptq ď ´c2et
for any suﬃciently large t ą 0. We now prove that for any j ‰ n,
xjptqxnptq ´ xnptq2 ÝÑ
tÑ`8 ´8.
(7.14)
Due to the ordering of the particles, it is enough to prove (7.14) for j “ n ´ 1. Fix
j “ n ´ 1 and κ ą 0, and assume that xnptqxjptq ě xnptq2 ´ κ for some t ě 0.
Then, using the fact that xnptqxjptq ě xnptqxkptq for any k P rn ´ 2s, we get
Pnjptq ě
exjptqxnptq
exnptq2 ` pn ´ 1qexnptqxjptq ě ε,
where ε “
1
n`eκ . We obtain
9xnptq ď Pnnptqxnptq ` Pnjptqxjptq ď xnptq ` εxjptq,
hence
d
dt
`
xnptqpxnptq ´ xjptqq
˘
“ 9xnptqp2xnptq ´ xjptqq ´ xnptq 9xjptq
ď pxnptq ` εxjptqqp2xnptq ´ xjptqq ´ xnptq 9xjptq
“ ´εxjptq2 ` xnptqp2εxjptq ` 2xnptq ´ xjptq ´ 9xjptqq
ď ´εxjptq2 ` xnptqp2xnptq ´ 2x1ptqq,
(7.15)
where in the last line we used the fact that 9xjptq ě x1ptq, which is due to (7.1), and
that x1ptq ă xjptq, which is due to the ordering of the particles. Since xjptq ď ´c2et
and x1ptq ě ´c1et, the upper bound in (7.15) is negative if t is large enough. We
therefore conclude that for any ﬁxed κ, if there exist unbounded times t such that
xnptqxjptq ě xnptq2 ´ κ, then xnptqxjptq ě xnptq2 ´ κ for any t large enough. But
this is excluded since xnptq ą 0 and xjptq Ñ ´8. This concludes the proof of
(7.14), and the lemma follows by plugging into the deﬁnition of Pnjptq.
□
Lemma 7.8. If i0 R t1, nu and xi0ptq remains uniformly bounded in t, then for any
j P rn ´ 1s, there exists some αj P r0, 1s such that Pi0jptq Ñ αj as t Ñ `8.
Proof. Assume that i0 R t1, nu. Then x1ptq Ñ ´8 and xnptq Ñ `8 as t Ñ `8.
Also, xi0ptq Ñ 0 due to (7.11).
We write xi0ptq “ yi0ptqe´t. Since γn ą 0 and γ1 ă 0, we notice that the function
g : θ ÞÑ
ř
i‰i0 eγiθγi
1 ` ř
i‰i0 eγiθ
takes value ´8 at ´8, and `8 at `8, and has positive derivative. Thus it takes
the value 0 exactly once, and we denote this point by θ0. We prove that yi0ptq Ñ θ0
as t Ñ `8. We observe that exi0ptq2 “ 1 ` op1q. Using Lemma 7.5 we have
9yi0ptq “ et 9xi0ptq ´ yi0ptq
“ pPi0i0ptq ´ 1qyi0ptq ` e2t ÿ
j‰i0
˜
eyi0ptqpγj`op1qq
1 ` op1q ` ř
k‰i0 eyi0ptqpγk`op1qq
¸
pγj ` op1qq.

24
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We recognize that the sum in the above expression is roughly equal to gpyi0q. If the
latter is not close to 0 for large times, then 9yi0ptq necessarily have a huge magnitude
due to the e2t factor, leading to a contradiction. Fix ε ą 0. If yi0ptq ą θ0 ` ε for
some large time t ą 0, then, noticing that |yi0ptq| “ et|xi0ptq| “ opetq, we get
9yi0ptq “ opetq ` e2t pgpyi0ptq ` opyi0ptqqqq
But gpyi0ptqq ě δ “ δpεq. Hence
9yi0psq ě δ
2e2s
for any larger time s ě t, which contradicts the fact that |yi0psq| “ opesq. We get a
similar contradiction if yi0ptq ă θ0 ´ ε for large enough t. This concludes the proof
that yi0ptq Ñ θ0.
As a consequence, xi0ptqxiptq Ñ θ0γi for any i ‰ i0, and we deduce Lemma
7.8.
□
7.3. Concluding the proof of Theorem 2.1.
Proof of Theorem 2.1. By Lemma 7.6, there is at most one index i0 P rns for which
the particle xi0ptq remains bounded for any t ą 0. In turn, for any i P rnszti0u, we
may invoke Lemma 7.4 which entails that Pijptq converges to either δ1j or δnj as
t Ñ `8 (with doubly exponential rate). And by ordering of the particles, for indices
i1 ď i2 diﬀerent from i0, and Pi1jptq Ñ δnj then necessarily Pi2jptq Ñ δnj as well.
Consequently, all but at most one row of Pptq converge to either e1 “ p1, 0, . . . , 0q or
en “ p0, . . . , 1q as t Ñ `8. For the i0-th row, we may invoke either Lemma 7.7 or
Lemma 7.8. The former applies if i0 P t1, nu, and entails that the i0-th row of Pptq
converges either to e1 or en, while the latter applies if i0 R t1, nu, and entails that
the i0-th row of Pptq converges to some vector α P Rd with non-negative entries.
Finally, since the i0-th row of Pptq has entries which sum up to 1, then so does α.
These conclusions lead us to a ﬁnal limit matrix P ˚ which has precisely the form
indicated in Figure 2 (namely, P ˚ P Pq, as desired.
□
7.4. The higher dimensional case.
Remark 7.9. The extension of Theorem 2.1 to d ě 2 is not straightforward due
to rare pathological situations. For example, suppose d “ 2, n “ 2, and the initial
conﬁguration x1p0q “ p1, εq and x2p0q “ p1, ´εq. One can check that xiptq Ñ p1, 0q
as t Ñ `8, for i “ 1, 2, which means that a single cluster appears. However, the
self-attention matrix converges toward the identity (which has rank 2). Therefore,
it is not true in full generality that the rank of the limiting self-attention matrix is
equal to the number of clusters as t Ñ `8, although we believe that the result is
true for almost all initial conditions.
8. Proofs of Theorems 3.1 and 8.5
In this section, we focus on proving our result in the case
V “ Id.
We also provide a full picture of the behavior of the dynamics in the case V “ ´Id
in Section 8.2.

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
25
8.1. Clustering towards vertices of convex polytopes: Theorem 3.1. In
this section, we prove Theorem 3.1—namely, we show that particles tziptquiPrns
following the rescaled dynamics
9ziptq “
n
ÿ
j“1
˜
ee2ctxAziptq,Azjptqy
řn
k“1 ee2ctxAziptq,Azkptqy
¸
pzjptq ´ ziptqq
(8.1)
converge, as t Ñ 8, toward the boundary of a particular convex polytope. In (8.1)
we made use the shorthand notation
A :“
`
QJK
˘ 1
2 .
(8.2)
The precise statement is the following:
Theorem 8.1 (Convergence toward points on the boundary of a convex polytope).
Suppose V “ Id and QJK ą 0. For any initial datum pz1p0q, . . . , znp0qq P pRdqn,
the solution to (8.1) is such that its convex hull convtz1ptq, . . . , znptqu converges to
some convex polytope K Ă Rd as t Ñ `8. Furthermore, let V “ tv1, . . . , vmu,
m ď n, denote the set of vertices of K, and consider
S :“
"
x P K | }Ax}2 “ max
jPrmsxAx, Avjy
*
,
with A deﬁned in (8.2). Then S has ﬁnite cardinality, and V Ă S Ă BK Y t0u.
Finally, for any i P rns there exists a point ¯z P S such that ziptq Ñ ¯z as t Ñ `8.
In particular, ziptq converges either to the boundary of K, or to 0.
8.1.1. The convex hull is shrinking. To prove Theorem 8.1, we begin with the fol-
lowing illustrative result.
Proposition 8.2. Suppose V “ Id and QJK ą 0. Then the solution tzip¨quiPrns
to (8.1) is such that t ÞÑ convptziptquiPrnsq is non-increasing in the sense of set-
inclusion.
Proof of Proposition 8.2. Fix t ą 0 and let H Ă Rd be a closed half-space which
does not contain any of the points ziptq. We deﬁne the map
α : s ÞÑ min
iPrns distpzipsq, Hq
for s ⩾0. We claim that
α is non-decreasing on rt, `8q.
(8.3)
Before proving (8.3), let us show how to conclude the proof of Proposition 8.2
using this claim. It follows from (8.3) that if convptziptquiPrnsq X H “ H, then
convptzipt1quiPrnsqXH “ H for any t1 ě t. Writing the convex set convptziptquiPrnsq
as
convptziptquiPrnsq “
č
H1 open half-space
convptziptquiPrnsqĂH1
H1 “
č
H closed half-space
convptziptquiPrnsqXH“H
RdzH,
we get that convptzipt1quiPrnsq Ă convptziptquiPrnsq for any t1 ě t.
We now turn to the proof of the claim (8.3). Denoting by n the unit outer normal
to H and by projH the orthogonal projection onto the closed set H, we have
distpx, Hq “ xx ´ projHpxq, ny.

26
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
If t ÞÑ xptq is a diﬀerentiable curve, writing 9xptq “ x 9xptq, nyn ` vptq where vptq P H
we have
d
dtpprojHpxptqqq “ vptq, whence
d
dtdistpxptq, Hq “ x 9xptq, ny.
(8.4)
Let T ą t denote the inﬁmum of the times for which one of the points ziptq lies
in H. Now ﬁx s P rt, Tq, and denote by Mpsq the set of indexes i P rns such that
distpzipsq, Hq is minimal. For h Ñ 0, we have
αps ` hq “ min
iPMpsq distpzips ` hq, Hq
“ min
iPMpsq
ˆ
distpzipsq, Hq ` h d
dtdistpzipsq, Hq ` ophq
˙
“ αpsq ` h
ˆ
min
iPMpsq
d
dtdistpzipsq, Hq
˙
` ophq.
Consequently,
dα
dt psq “ min
iPMpsq
d
dtdistpzipsq, Hq.
Moreover, for any i P Mpsq, one has
d
dtdistpzipsq, Hq
(8.4)
“ x 9zipsq, ny “
n
ÿ
j“1
Pijpsqxzjpsq ´ zipsq, ny ě 0,
where the last inequality comes from the fact that each term in the sum is non-
negative, since i P Mpsq. This proves (8.3) (and, as a byproduct, that T “ `8).
□
The following fact immediately ensues.
Corollary 8.3. For any i P rns and t ě 0, ziptq P convptzip0quiPrnsq. In particular,
zip¨q is uniformly bounded in time.
8.1.2. Proof of Theorem 8.1.
Proof of Theorem 8.1. As a consequence of Proposition 8.2, the set convptziptquiPrnsq
converges as t Ñ `8 toward some convex polytope K. In the remainder of the
proof, we look to show that the particles ziptq can in fact converge only to some
well-distinguished points lying on the boundary of this polytope.
Step 1. The candidate set of limit points. We denote by V “ tv1, . . . , vmu the
set of vertices of K. Writing any x P K as a convex combination of these vertices:
x “ řm
j“1 αjvj for some weights αj ě 0 with řm
j“1 αj “ 1, we gather that
}Ax}2 “
C
Ax,
m
ÿ
j“1
αjAvj
G
“
m
ÿ
j“1
αj xAx, Avjy ď max
jPrmsxAx, Avjy.
(8.5)
Let S Ă K denote the set of points w P K such that
}Aw}2 “ max
jPrmsxAw, Avjy.
(8.6)
The following holds—we postpone the proof to after that of the theorem.
Claim 1. V Ă S. Moreover, if 0 P K, then 0 P S. Finally, S Ă BK Y t0u, and S
has ﬁnite cardinality.

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
27
Now, for δ ą 0, we deﬁne the set Sδ of points in K at distance at most δ from S:
Sδ :“ tx P K | distpx, Sq ď δu.
Since S is ﬁnite, there exists a suﬃciently small δ0 ą 0 such that for any δ ď δ0,
the set Sδ has M :“ #S connected components, with any two of these connected
components being separated by a distance of at least δ0. Our goal is to prove that
for any i P rns, and for suﬃciently large t, the particle ziptq remains in one of these
connected components. In the sequel, we ﬁx i P rns.
K
S
Sδ
Figure 9. An example conﬁguration of the sets S and Sδ in R2. The
set S consists of all green nodes along the boundary of BK, while Sδ is
the union of all yellow "hemispheres". The latter are pairwise disjoint
and are the connected components of Sδ, which we denote by Ck, for
k P rMs.
Step 2. ziptq must grow if it is not already in Sδ. We now prove that there
exists some γ “ γpKq ą 0 (depending only on the geometry of K) such that for any
δ P p0, δ0s, there exists Tpδq ą 0 such that if t ě Tpδq and ziptq R Sδ, then
d
dt}Aziptq}2 ě γδ.
(8.7)
To this end, we observe that
1
2
d
dt}Aziptq}2 “ xA 9ziptq, Aziptqy “
n
ÿ
j“1
exAziptq,Azjptqye2t
řn
k“1 exAziptq,Azkptqye2t xApzjptq ´ ziptqq, Aziptqy
“
n
ÿ
j“1
eajptqe2t
řn
k“1 eakptqe2t ajptq
looooooooooomooooooooooon
:“bjptq
.
(8.8)
where we have set
ajptq :“ xApzjptq ´ ziptqq, Aziptqy.
(To obtain the last equality in (8.8), divide both the numerator and the denominator
by e}Aziptq}2e2t.) The following holds.

28
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Claim 2. There exists some constant γ1 “ γ1pKq ą 0 depending only on the geom-
etry of K such that the following holds. Fix δ P p0, δ0s. There exists T 1pδq ą 0 such
that if t ě T 1pδq and ziptq R Sδ, then there exists j P rns such that ajptq ě γ1δ.
We postpone the proof of this claim to after that of the theorem. We seek to
use this claim in obtaining a lower bound of bjptq for any j, whenever δ is small
enough and t is large enough. Since by Corollary 8.3, for any j P rns, t ÞÑ zjptq is
uniformly bounded on r0, `8q, we gather that ajp¨q P L8p0, `8q. So we may set
κ :“ max
jPrns sup
tě0
|ajptq|.
Let t ě 0 be ﬁxed. We deﬁne
Bptq :“ tj P rns | ajptq ě 0u.
We pick an index j0ptq maximizing ajptq, namely
j0ptq P argmaxjPrnsajptq.
Observe that j0ptq P Bptq since aj0ptqptq ě aiptq “ 0. Clearly
bjptq ě 0
for all j P Bptq.
(8.9)
In fact, we also have
bj0ptqptq ě aj0ptqptq
n
.
(8.10)
Now suppose that j R Bptq; since ajptq ě ´κ, and
eajptqe2t
řn
k“1 eakptqe2t ď
1
řn
k“1 eakptqe2t ď e´aj0ptqe2t,
we gather that
bjptq ě ´κe´aj0ptqptqe2t
for all j P rnszBptq.
(8.11)
Using (8.9), (8.10) and (8.11) in (8.8), we ﬁnd
1
2
d
dt}Aziptq}2 ⩾aj0ptqptq
n
´ κne´aj0ptqptqe2t.
The above inequality along with Claim 2 lead us to deduce that there exists Tpδq ą 0
(possibly larger than T 1pδq) such that (8.7) holds whenever t ⩾Tpδq, with γ “ γ1
2n,
as desired.
Step 3: ziptq cannot circulate indeﬁnitely between the connected com-
ponents of Sδ. Since zi P L8pr0, `8qq by Corollary 8.3, from (8.1) we gather
that 9zi P L8pr0, `8qq as well. And since any two connected components of Sδ0 are
separated by a distance at least δ0, we deduce that it takes a time at least
T0 :“
δ0
} 9zi}L8pr0,`8qq
for zi to go from one connected component of Sδ0 to another one. Fix δ P p0, δ0q
such that
δ ă
T0γδ0
8R}A}op
,
(8.12)
where R :“ maxjPrns }zj}L8pr0,`8qq. Denote by
C1, . . . , CM

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
29
the connected components of Sδ, each of which being the intersection of K with
a Euclidean ball of radius δ centered at some point of S (see Figure 9). For any
k P rMs,
sup
xPCk
}Ax}2 ´ inf
xPCk }Ax}2 ď 4R}A}opδ.
(8.13)
We introduce the following binary relation on rMs:
k ą ℓðñ inf
xPCk }Ax}2 ą sup
xPCℓ
}Ax}2,
which is transitive. The underlying idea is the following: if t is suﬃciently large, and
if zi starts from some connected component Cℓ, then the only connected components
Ck which zi is able to visit later on are those for which k ą ℓ. This travel of zi
has to stop after some time since rMs is ﬁnite, ą is transitive, and for any ℓ, the
relation ℓą ℓdoes not hold.
Let T “ Tpδq be as in Step 2. Suppose that t2 ą t1 ě T and k1, k2 P rMs are
distinct and such that zipt1q P Ck1, zipt2q P Ck2 and ziptq R Sδ for any t P pt1, t2q.
Per Step 2 (more speciﬁcally, (8.7)),
}Azipt2q}2 ě }Azipt1q}2 ` T0γδ0.
Therefore using (8.13) twice and since δ is chosen as in (8.12), we gather that
inf
xPCk2
}Ax}2 ě }Azipt2q}2 ´ 4R}A}opδ ě }Azipt1q}2 ` T0γδ0 ´ 4R}A}opδ
ě inf
xPCk1
}Ax}2 ` T0γδ0 ´ 4R}A}opδ
ě sup
xPCk1
}Ax}2 ` T0γδ0 ´ 8R}A}opδ
ą sup
xPCk1
}Ax}2.
(8.14)
Whence k2 ą k1. We therefore deduce that there exist some T 1 ě T and k P rMs
such that ziptq R SδzCk for any t ě T 1.
Step 4. Conclusion. To conclude, it remains to be shown that ziptq stays in Ck
for t large enough. For this, in addition to (8.12), we impose
δ
1
4 ă
γT0
8R}A}opδ0
.
(8.15)
For r ą 0, we denote by Cr
k the intersection of K with the closed Euclidean ball of
radius δr having the same center as Ck. In particular, C1
k “ Ck. If, after time T 1,
zi travels from Ck to the complement of C
1
4
k , it spends a time at least
pδ
1
4 ´ δ
1
2 q
} 9zi}L8pr0,`8qq
in C
1
4
k zC
1
2
k . Per Step 2 (used with δ
1
2 ), }Azi}2 has to increase by at least
γδ
1
2
´
δ
1
4 ´ δ
¯
} 9zi}L8pr0,`8qq
ě
γδ
3
4
2} 9zi}L8pr0,`8qq
ą 4R}A}opδ
(8.16)
during this travel (the last inequality in (8.16) stems from (8.15)). This implies
that zi cannot reenter Ck after having reached the boundary of C
1
4
k , due to (8.13).
Thus ziptq R Sδ for any suﬃciently large t, which is impossible due to Step 2 and

30
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
the uniform boundedness of t ÞÑ }Aziptq}. Hence, for suﬃciently large t, ziptq P C
1
4
k .
Since δ may be chosen arbitrarily small, this concludes the proof of Theorem 8.1.
□
8.1.3. Proving Claims 1 and 2. We now address the proofs of the two claims which
were instrumental in what precedes (along with a sketch of the proof of V Ă S, as
implied).
Proof of Claim 1. The fact that 0 P S if 0 P K is immediate. We now show that
S is ﬁnite and S Ă BK Y t0u. Let w P Szt0u. As w “ řm
j“1 αjvj for some αj ⩾0
with řm
j“1 αj “ 1, and since (8.6) holds by deﬁnition, it follows that αj “ 0 for
any j not attaining the maximum in (8.6). Let I Ă rms denote the set of all such
indices. We have that w “ ř
jPI αjvj with }Aw}2 “ xAw, Avjy for any j P I.
Whence w is the orthogonal projection onto spantvjujPI with respect to xA¨, A¨y.
This yields S Ă BK. Moreover, since for each subset I Ă rms there exists a unique
such projection w, S is ﬁnite.
□
Sketch of proof of V Ă S. We notice that for any i P rns and for t large enough, we
have
9ziptq “
n
ÿ
j“1
˜
ee2txAziptq,Azjptqy
řn
k“1 ee2txAziptq,Azkptqy
¸
pzjptq ´ ziptqq
(8.17)
«
ÿ
jPMiptq
˜
ee2txAziptq,Azjptqy
řn
k“1 ee2txAziptq,Azkptqy
¸
pzjptq ´ ziptqq,
(8.18)
where Miptq is the subset of rns containing all indices j such that
max
kPrnsxAziptq, Azkptqy ´ xAziptq, Azjptqy ď e´t
(all other terms in the sum (8.17) are negligible).
Due to the convergence of
convptziptquiPrnsq toward K, we also know that for t large enough,
‚ all the points ziptq are contained in a small neighborhood of K,
‚ near any element of V, there exists some particle ziptq.
Assume for the sake of a contradiction that there exists a vertex vj P V such
that vj R S.
We deﬁne the convex set C :“ convptviuiPrmsztjuq.
In particular,
distpvj, Cq ą 0 since vj is a vertex of K. If I Ă rns denotes the set of indices i such
that ziptq lies near vj, then MiptqXI “ H for any i P I since vj R S. For i P I, using
(8.18), we ﬁnd that distpziptq, Cq decays as t Ñ `8 as long as i R Miptq—indeed,
(8.18) implies that ziptq is attracted by C. This implies that vj R convptzkpt1qukPrnsq
for t1 large enough. This is a contradiction since K Ă convptzkptqukPrnsq for any
t ě 0 according to Proposition 8.2.
□
Proof of Claim 2. To simplify the notation, we only prove Claim 2 when A “ Id.
Assume that t ě 0 and that ziptq R Sδ.
First case. Firstly, we prove the claim in the case where ziptq R Sδ0. For this, we
notice that the function
f : x ÞÑ max
jPrnsxvj, xy ´ }x}2
is continuous, and by deﬁnition of S, f is strictly positive on the compact set
KzIntpSδ0q (the complement in K of the interior of Sδ0). Hence fpxq ě c1 in this

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
31
set for some constant c1 ą 0. Setting
Kε :“ tx P Rd | distpx, Kq ď εu,
by continuity we ﬁnd that fpxq ě c1
2 for x P KεzIntpSδ0q and for suﬃciently small
ε ą 0 (ﬁxed in the sequel). For suﬃciently large t, we have ziptq P Kε for any
i P rns, thus
max
jPrnsxziptq, zjptq ´ ziptqy ě max
jPrmsxziptq, vj ´ ziptqy ě c1
2 .
Since c1 is independent of δ, we deduce the claim in this case (notice that it suﬃces
to prove the claim for suﬃciently small δ).
Second case. Secondly, we prove the claim when ziptq P Sδ0zSδ. The proof mainly
relies on the following result:
Lemma 8.4. For any w P S, there exists β ą 0 such that if5 x P K X Bpw, δ0q,
then
max
jPrmsxx, vj ´ xy ě β}x ´ w}.
(8.19)
We postpone the proof of Lemma 8.4 and show how to conclude the proof of
Claim 2. Fix δ ą 0. We set
η :“ βδ
6R
where R :“ maxjPrns }zj}L8pRq. Since convptzjptqujPrnsq converges to K as t Ñ `8,
there exists Tpδq ą 0 such that for any t ě Tpδq, if ziptq P Bpw, δ0qzBpw, δq for
some w P S, then }ziptq ´ x} ď η for some x P K X pBpw, δ0qzBpw, δqq. Therefore,
using Lemma 8.4,
max
jPrmsxziptq, vj ´ ziptqy ě max
jPrmsxx, vj ´ xy ´ 3Rη ě βδ ´ 3Rη “ β
2 δ.
To summarize, we have found that for any δ ą 0 there exists Tpδq ą 0 such that if
t ě Tpδq and ziptq P Sδ0zSδ, then
max
jPrmsxziptq, vj ´ ziptqy ě β
2 δ.
(8.20)
Combining (8.20) with
max
jPrnsxziptq, zjptq ´ ziptqy ě max
jPrmsxziptq, vj ´ ziptqy
concludes the proof of Claim 2 in this second case.
□
Proof of Lemma 8.4. Let us ﬁrst address the case where w “ 0. Writing any point
x P Kzt0u as a convex combination of the vertices as x “ řm
j“1 αjvj, we ﬁnd
0 “
C
x,
m
ÿ
j“1
αjpvj ´ xq
G
“
m
ÿ
j“1
αjxx, vj ´ xy.
(8.21)
We can exclude having xx, vj ´xy “ 0 for all j P rms, as this would necessarily imply
that }x}2 “ 2 řm
j“1 αjxx, vj´xy “ 0. From (8.21) we deduce maxjPrmsxx, vj´xy ą 0
5Here, Bpy, rq denotes the closed ball with center y P Rd and radius r ą 0.

32
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
for any x P Kzt0u. Hence, it is suﬃcient to prove (8.19) for }x} small enough. We
notice that for any x P Kzt0u written as above,
}x}2 “
m
ÿ
j“1
αjxvj, xy.
Hence x ÞÑ maxjPrmsxvj, xy is positive for x P Kzt0u. Since this function is contin-
uous and homogeneous in x, we deduce the existence of β ą 0 such that
max
jPrmsxvj, xy ě 2β}x}
for any x P K. For x P K with }x} suﬃciently small, we obtain (8.19).
We now assume that w P Szt0u. We set
Iw :“ tj P rns | }w}2 “ xw, vjyu
and
A :“ spanptvj ´ w | j P Iwuq
which is orthogonal to w. We also introduce
R :“ pRw ‘ AqK.
and we denote by πR the orthogonal projection on R. We claim that there exists
ρ ą 0 such that for any j P rms we have
xw ´ vj, wy ě ρ}πRvj}.
Indeed, this follows from the observation that rms is ﬁnite, and that }πRvj} ą 0
implies xw´vj, wy ą 0. Therefore, for any x P K, writing x as a convex combination
of the vertices, namely x “ řm
j“1 αjvj, we ﬁnd that
ρ}πRx} ď
m
ÿ
j“1
αj}πRvj} ď
m
ÿ
j“1
αjxw ´ vj, wy “ xw ´ x, wy.
(8.22)
Fix x P K X Bpw, δ0q. We write x “ w ` δ1u with 0 ď δ1 ď δ0 and }u} “ 1. Then
we have the orthogonal decomposition
u “ bw ` a ` r
(8.23)
where a P A, r P R and b P R.
Since a is a convex combination of the form
a “ ř
jPIw βjpvj ´ wq, we have }a}2 “ ř
jPIw βjxvj ´ w, ay, whence
max
jPIwxa, vj ´ wy ě }a}2.
We deduce that
max
jPIwxx, vj ´ xy “ max
jPIwxw ` δ1u, pvj ´ wq ´ δ1uy
“ ´δ1b}w}2 ´ δ12 ` δ1 max
jPIwxa, vj ´ wy
ě ´δ1b}w}2 ´ δ12 ` δ1}a}2.
(8.24)
Notice that b ď 0 by combining (8.22) and (8.23). Since }u} “ 1 and using (8.22)
we have
1 “ b2 ` }a}2 ` }r}2 ď }a}2 ` κb2 ď κp}a}2 ` b2q

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
33
where κ :“ 1`ρ´2}w}4. We deduce that either }a}2 ě p2κq´1 or ´b “ |b| ě p2κq´ 1
2 .
Plugging this knowledge in (8.24) and using the fact that }w} ą 0, we ﬁnally deduce
the existence of an α ą 0 (independent of δ ą 0 and x P K X Bpw, δ0q) such that
max
jPrmsxx, vj ´ xy ě αδ1 ´ δ12 “ α}x ´ w} ´ }x ´ w}2.
This proves (8.19) when }x ´ w} ď α
2 .
It thus remains to show that (8.19) holds for all x P K X pBpw, δ0qzBpw, α
2 qq. To
this end, we notice that x ÞÑ maxjPrmsxx, vj ´ xy is continuous in the connected set
K X pBpw, δ0qzBpw, α
2 qq, non-negative according to (8.5), and it is nowhere 0 (by
deﬁnition of S). Therefore, it is strictly positive, and denote by α1 ą 0 some lower
bound. Then for x P K X pBpw, δ0qzBpw, α
2 qq, we have
max
jPrmsxx, vj ´ xy ě α1 ě α1
δ0
}x ´ w}.
This concludes the proof of Lemma 8.4.
□
8.2. A cluster at the origin. We complete this section by addressing the case
V “ ´Id, for which the convergence of the solutions of (1.1) is the simplest, since
a unique cluster forms at the origin. We also suppose that QJK “ Id: in other
words, we consider the dynamics
9xiptq “ ´
n
ÿ
j“1
ˆ
exxiptq,xjptqy
řn
k“1 exxiptq,xkptqy
˙
xjptq,
t P r0, `8q,
(8.25)
with a prescribed initial condition px1p0q, . . . , xnp0qq P pRdqn.
Theorem 8.5 (Convergence toward the origin). Suppose V “ ´Id and QJK “ Id.
Then, for any initial sequence of tokens px1p0q, . . . , xnp0qq P pRdqn, and for any
i P rns, we have }xiptq} Ñ 0 as t Ñ `8.
Remark 8.6. In the setting of Theorem 8.5, the self-attention matrix Pptq deﬁned
in (1.2) converges, as t Ñ `8, to the n ˆ n matrix with all entries equal to 1{n.
8.2.1. Proof of Theorem 8.5. We begin by showing that for any i P rns, the solu-
tion to (8.25) is uniformly bounded for all t ą 0. In the sequel, we ﬁx an initial
conﬁguration px1p0q, . . . , xnp0qq P pRdqn.
Lemma 8.7. The trajectories of (8.25) are uniformly bounded in time—namely,
there exists R ą 0 (depending solely on n and the initial conﬁguration) such that
the solution xip¨q to (8.25) satisﬁes }xiptq} ⩽R for any i P rns and t ⩾0.
Proof of Lemma 8.7. We ﬁx i P rns. For t ě 0, we denote by Diptq the set of points
xkptq such that xxiptq, xkptqy ě 0. We also set
Siptq :“
ÿ
kPDiptq
exxiptq,xkptqyxxiptq, xkptqy,
and
Riptq :“
n
ÿ
k“1
exxiptq,xkptqy.
Since 1 ` x ď ex whence e´xx ď 1, we deduce that
1
2
d
dt}xiptq}2 “ ´
řn
k“1 exxiptq,xkptqyxxiptq, xkptqy
Riptq
ď ´Siptq ` n
Riptq
.

34
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Now since 1 ´ x ď e´x whence ex ď 1 ` exx, we ﬁnd that Riptq ď n ` Siptq.
Consequently, if we assume that }xiptq}2 ě 2n then Siptq ě 2n, and therefore
1
2
d
dt}xiptq}2 ď ´Siptq ` n
n ` Siptq ď ´1.
This shows that }xiptq} ď maxt}xip0q},
?
2nu for any t ě 0, which concludes the
proof.
□
By virtue of Lemma 7.1, we are able to characterize the stationary conﬁgurations
for the dynamics (8.25)—namely, the set of points p¯x1, . . . , ¯xnq P pRdqn satisfying
n
ÿ
j“1
ˆ
ex¯xi,¯xjy
řn
k“1 ex¯xi,¯xky
˙
¯xj “ 0
for all i P rns.
Lemma 8.8. The unique stationary conﬁguration for (8.25) is ¯x1 “ . . . “ ¯xn “ 0.
Proof. Assume that p¯x1, . . . , ¯xnq P pRdqn is a stationary conﬁguration for the dy-
namics (8.25). We consider f : Rd Ñ R deﬁned as
f : x ÞÑ log
˜ n
ÿ
j“1
exx,¯xjy
¸
.
Per Lemma 7.1, f is convex, whence
fpxq ě fp¯xiq ` x∇fp¯xiq, x ´ ¯xiy
for x P Rd and i P rns. Since ∇fp¯xiq “ 0 for any i P rns, we gather that fpxq ě
fp¯xiq, whence ¯xi is a global minimizer of f for any i P rns. By convexity, f is
constant on convpt¯xiuiPrnsq. Since f is analytic on the aﬃne space E spanned by
the points ¯xi, i P rns, it is then constant on E as well. Now assume that not all
of the points ¯xi are equal, and pick an index i0 P rns such that ¯xi0 is not equal
to the projection of the origin onto E. Then there exists some j0 P rns such that
x¯xi0 ´ ¯xj0, ¯xi0y ‰ 0. For any s P R, we set Ps :“ ¯xj0 ` sp¯xi0 ´ ¯xj0q P E, and we
notice that fpPsq ě xPs, ¯xi0y, where the lower bound tends to `8 either when
s Ñ `8 or when s Ñ ´8. This contradicts the fact that f is constant on E. We
conclude that the ¯xi are all equal for i P rns. The only value they can then take is
necessarily 0.
□
Lemma 8.9. The trajectories of (8.25) satisfy
ż `8
0
} 9xiptq}2 dt ă `8 for any
i P rns.
Proof. The function
L : t ÞÑ
n
ÿ
i“1
n
ÿ
j“1
exxiptq,xjptqy
is non-increasing, as demonstrated by the following simple computation:
dLptq
dt
“ 2
n
ÿ
i“1
n
ÿ
j“1
exxiptq,xjptqyx 9xiptq, xjptqy “ 2
n
ÿ
i“1
C
9xiptq,
n
ÿ
j“1
exxiptq,xjptqyxjptq
G
“ ´2
n
ÿ
i“1
n
ÿ
j“1
exxiptq,xjptqy} 9xiptq}2.

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
35
Being non-negative, Lptq thus converges as t Ñ `8. Since xxiptq, xjptqy ě R for
some (possibly negative) R P R by virtue of Lemma 8.7, we deduce that
ż `8
0
} 9xiptq}2 dt ď e´R
ż `8
0
n
ÿ
i“1
n
ÿ
j“1
exxiptq,xjptqy} 9xiptq}2 dt “ e´RpLp0q´ lim
tÑ`8 Lptqq,
which concludes the proof.
□
We are now able to conclude the proof of Theorem 8.5.
Proof of Theorem 8.5. We set Xptq :“ px1ptq, . . . , xnptqq P pRdqn. If Xptq does not
converge to 0, the compactness provided by Lemma 8.7 implies that there is a
sequence ttku`8
k“1 with tk Ñ `8, and X˚ “ px˚
1, . . . , x˚
nq P pRdqnzt0u, such that
Xptkq Ñ X˚ as k Ñ `8. To conclude the proof, it suﬃces to show that X˚ is
a stationary conﬁguration of the dynamics: this directly leads to a contradiction
per Lemma 8.8. Therefore, assume that X˚ is not a stationary conﬁguration of
the dynamics. We denote by X˚ptq “ px˚
1ptq, . . . , x˚
nptqq the solution of (8.25) with
initial condition X˚.
Then, there exists i P rns such that 9x˚
i p0q ‰ 0.
We set
ε “ } 9x˚
i p0q}. We select T0 ą 0 (possibly small) such that } 9x˚
i ptq} ě ε
2 for t P r0, T0s.
It follows from (6.6) in Proposition 6.5 (which is veriﬁed according to Corollary 6.6)
that for any δ ą 0 there exists k0 P N such that }Xptk ` tq ´ X˚ptq} ď δ for any
t P r0, T0s and any k ě k0. By (6.5) in Proposition 6.5 (which is veriﬁed according
to Corollary 6.6), we obtain that } 9xiptk ` tq ´ 9x0
i ptq} ď Cδ for t P r0, T0s and any
k ě k0. Choosing δ ą 0 suﬃciently small, we obtain that } 9xiptk ` tq} ě
ε
4 for
t P r0, T0s and any k ě k0. This contradicts Lemma 8.9.
□
9. Proof of Theorem 4.2
To ensure clarity, we present the proof of Theorem 4.2 under the assumption
that V is diagonalizable. However, this assumption is not necessary. In Remark
9.5, we explain how the proof can be modiﬁed to accommodate non-diagonalizable
V .
Let us therefore assume that V is diagonalizable. Let pϕ1, . . . , ϕdq be an or-
thonormal basis of eigenvectors associated to eigenvalues pλ1, . . . , λdq, ordered in a
decreasing manner regarding their modulus: |λ1| ě . . . ě |λd|. (Starting from this
point and throughout, we use the symbol λ exclusively to denote the eigenvalues of
V .) With the exception of λ1 P R, all the other eigenvalues (and eigenvectors) may
be complex. We denote by pϕ˚
1, . . . , ϕ˚
dq the dual basis of pϕ1, . . . , ϕdq.
9.1. Some monotonicity properties and bounds. To start, we present some
general facts that are prove useful in all subsequent sub-cases.
Lemma 9.1. Suppose k P rds is such that λk ě 0. Then t ÞÑ maxjPrns ϕ˚
kpzjptqq
is a non-increasing and bounded function, and t ÞÑ minjPrns ϕ˚
kpzjptqq is a non-
decreasing and bounded function. In particular, t ÞÑ ϕ˚
kpziptqq is uniformly bounded
as a function on r0, `8q for any i P rns.
Proof. For any k P rds and any t ě 0, set
αkptq “ min
jPrns ϕ˚
kpzjptqq,
βkptq “ max
jPrns ϕ˚
kpzjptqq.

36
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Let i P rns be an index such that αkptq “ ϕ˚
kpziptqq. Then we have
d
dtϕ˚
kpziptqq “
n
ÿ
j“1
Pijptqϕ˚
k pV pzjptq ´ ziptqqq
“ λk
n
ÿ
j“1
Pijptqpϕ˚
kpzjptqq ´ ϕ˚
kpziptqqq ě 0
where the last inequality stems from the fact that λk ě 0 and the choice of index
i. This proves that αkp¨q is non-decreasing, as desired. Arguing similarly, one ﬁnds
that βkp¨q is non-increasing. As a consequence, αkp0q ď αkptq ď βkptq ď βkp0q for
any t ě 0, which shows that αkp¨q and βkp¨q are bounded.
□
Corollary 9.2. If V only has real non-negative eigenvalues (specpV q Ă r0, `8q),
then zip¨q P L8pr0, `8qq.
Lemma 9.3. Fix k P rds and i P rns. Then there exists a constant C ą 0 such that
ˇˇϕ˚
k
`
etV ziptq
˘ˇˇ ď Ce|λk|t
holds for all t ⩾0.
Proof. We naturally make use of the equation for xiptq :“ etV ziptq. Fix t ⩾0. We
have
d
dt |ϕ˚
kpxiptqq|2 “ 2ℜ
ˆ
ϕ˚
kpxiptqq d
dtϕ˚
kpxiptqq
˙
“ 2ℜ
˜ n
ÿ
j“1
Pijptqϕ˚
kpV xjptqqϕ˚
kpxiptqq
¸
“ 2ℜ
˜ n
ÿ
j“1
Pijptqλkϕ˚
kpxjptqqϕ˚
kpxiptqq
¸
ď 2|λk| max
jPrns |ϕ˚
kpxjptqq|2 .
Choosing i P rns running over the set of indices such that |ϕ˚
kpxiptqq| is maximal,
we obtain
d
dt max
jPrns |ϕ˚
kpxjptqq|2 ď 2|λk| max
jPrns |ϕ˚
kpxjptqq|2.
We conclude the proof by applying Grönwall’s lemma.
□
9.2. Proof of Theorem 4.2. We now prove Theorem 4.2. We again recall that
λ1 is simple and positive, and the eigenvalues of V are ordered in decreasing order
of modulus: λ1 ą |λ2| ě . . . ě |λd|.
Proof of Theorem 4.2. We look to prove that for any i P rns, the component of ziptq
along the principal eigenvector ϕ1, i.e. ϕ˚
1pziptqq, converges as t Ñ `8. We also
show that there exists a set of at most 3 real numbers (depending on the initial
datum pz1p0q, . . . , znp0qq) such that for any i P rns the limit of ϕ˚
1pziptqq belongs to
this set. Theorem 4.2 directly follows from these facts.
Let i P rns be ﬁxed. Recall from Lemma 9.1 that ϕ˚
1pziptqq is uniformly bounded
for any t P r0, `8q. We set
a :“ lim
tÑ`8 min
jPrns ϕ˚
1pzjptqq,
b :“ lim
tÑ`8 max
jPrns ϕ˚
1pzjptqq.
(9.1)

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
37
(Note that by Lemma 9.1, a ě minjPrns ϕ˚
1pzjp0qq and b ď maxjPrns ϕ˚
1pzjp0qq.) For
c P t0, a, bu, we deﬁne the candidate limiting hyperplanes for ziptq:
Hc :“ tx P Rd | ϕ˚
1pxq “ cu.
We show that ziptq converges either to H0, to Ha or to Hb. If a “ b “ 0, then
according to (9.1) all particles converge to H0 and there is nothing left to prove.
We now distinguish two scenarios:
(i) either for any ε ą 0, |ϕ˚
1pziptqq| ď ε for t large enough—in which case, we
deduce that ziptq converges toward H0 as t Ñ `8—,
(ii) or |ϕ˚
1pziptkqq| ą ε0 for some ε0 ą 0 and for some sequence of positive times
ttku`8
k“1 with tk Ñ `8.
Since case (i) is straightforward, let us handle case (ii). Without loss of generality,
we can extract a subsequence of times (which we do not relabel, for simplicity of
notation) along which
ϕ˚
1pziptkqq ą ε0.
(9.2)
Let ε P p0, ε0s be ﬁxed and to be chosen later. We set
wjptq :“
@
QetV ziptq, KetV zjptq
D
,
so that
1
λ1
d
dtϕ˚
1pziptqq “
n
ÿ
j“1
ewjptq
řn
k“1 ewkptq pϕ˚
1pzjptqq ´ ϕ˚
1pziptqqq .
(9.3)
We look to obtain a lower bound for the right-hand side in the above identity. Let
us use the shorthand
ckℓ:“ xQϕk, Kϕℓy
for k, ℓP rds. By assumption, c11 ą 0. We have ϕ˚
kpetV ziptqq “ etλkϕ˚
kpziptqq and
the following spectral expansion holds:
etV ziptq “
dÿ
k“1
etλkϕ˚
kpziptqqϕk.
Using this fact, as well as Lemma 9.3, we gather that
ˇˇˇwjptq ´ c11e2λ1tϕ˚
1pziptqqϕ˚
1pzjptqq
ˇˇˇ “
ˇˇˇˇˇˇ
ÿ
pk,ℓq‰p1,1q
ckℓϕ˚
kpetV ziptqqϕ˚
ℓ
`
etV zjptq
˘
ˇˇˇˇˇˇ
ď
ÿ
pk,ℓq‰p1,1q
|ckℓ|
ˇˇϕ˚
k
`
etV ziptq
˘ˇˇ ˇˇϕ˚
ℓ
`
etV zjptq
˘ˇˇ
ď C2}QJK}op
ÿ
pk,ℓq‰p1,1q
ep|λk|`|λℓ|qt
ď C2}QJK}oppd ´ 1q2
loooooooooooomoooooooooooon
“:C1
epλ1`|λ2|qt
(9.4)
holds for all t ě 0 and j P rns. Now since λ1 ą 0, Lemma 9.1 implies that for any
t ě 0 there exists an index i0ptq P rns such that
ϕ˚
1pzi0ptqptqq ě b.
(9.5)
With j0ptq P argmaxjPrnswjptq, using (9.4) and (9.5) we see that
wj0ptqptq ě wi0ptqptq ě c11ϕ˚
1pziptqqbe2λ1t ´ C1epλ1`|λ2|qt.
(9.6)

38
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Now for any t within the sequence ttku`8
k“1, combining the ﬁrst inequality in (9.6)
with the fact that c11 ą 0, (9.2) and (9.4), we deduce that
ϕ˚
1pzj0ptqptqq ´ ϕ˚
1pzi0ptqptqq ě ´ 2C1
c11εe´pλ1´|λ2|qt.
(9.7)
As λ1 ą |λ2|, for t large enough, we ﬁnd that we can lower bound the above
expression by ´ ε
4. We now deﬁne the set of indices
Nptq :“ tj P rns | ϕ˚
1pziptqq ´ ϕ˚
1pzjptqq ě 0u.
Take t within the sequence ttku`8
k“1 such that ϕ˚
1pziptqq ď b ´ ε and large enough so
that (9.7) is lower bounded by ´ ε
4 (if such a t does no exist, we immediately conclude
that ϕ˚
1pziptqq Ñ b as t Ñ `8). Using (9.5) and the subsequent derivations, we
deduce that
ϕ˚
1pzj0ptqptqq ´ ϕ˚
1pziptqq ě 3ε
4 ,
and since ϕ˚
1pzjptqq ´ ϕ˚
1pziptqq ě 0 for j R Nptq, we expand in (9.3) to get
1
λ1
d
dtϕ˚
1pziptqq ě
ewj0ptqptq
řn
k“1 ewkptq
3ε
4 `
ÿ
jPNptq
ewjptq
řn
k“1 ewkptq pϕ˚
1pzjptqq ´ ϕ˚
1pziptqqq .
(9.8)
On another hand, for j P Nptq, we may use (9.4) to ﬁnd
wjptq ď c11ϕ˚
1pziptqq2e2λ1t ` C1epλ1`|λ2|qt.
(9.9)
We set
C0 :“ max
jPrns ϕ˚
1pzjp0qq ´ min
jPrns ϕ˚
1pzjp0qq.
Using the monotonicity properties from Lemma 9.1, as well as (9.9) in (9.8), we
obtain
1
λ1
d
dtϕ˚
1pziptqq ě 3ε
4n ´ C0n
exp
´
c11ϕ˚
1pziptqq2e2λ1t ` C1epλ1`|λ2|qt¯
exp
´
c11ϕ˚
1pziptqqbe2λ1t ´ C1epλ1`|λ2|qt
¯.
Given our choice of t, we have ϕ˚
1pziptqq2 ´ bϕ˚
1pziptqq ď ´εpb ´ εq, so we conclude
from the inequality just above that
1
λ1
d
dtϕ˚
1pziptqq ě 3ε
4n ´ C0n exp
´
´ c11εpb ´ εqe2λ1t ` 2C1epλ1`|λ2|qt¯
.
(9.10)
Since λ1 ą |λ2|, it follows from (9.10) that there exists T ą 0 such that for any t
within the sequence ttku`8
k“1 for which t ě T and ϕ˚
1pziptqq P rε, b ´ εs, there holds
d
dtϕ˚
1pziptqq ě λ1ε
2n .
This shows the existence of a larger time horizon T 1 ą T such that ϕ˚
1pziptqq ě b´ε
whenever t ě T 1.
And since ε can be taken arbitrarily small, we deduce that
ϕ˚
1pziptqq converges toward b, namely that ziptq converges toward Hb, as t Ñ `8.
Arguing in the same way as above, and assuming without loss of generality that
a ă 0, we may ﬁnd that all indices i P rns for which ϕ˚
1pziptkqq ď ´ε0 for some
ε0 ą 0 and some sequence tk Ñ `8, the particle ziptq converges toward Ha as
t Ñ `8. This concludes the proof.
□

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
39
9.3. Remarks.
Remark 9.4. Theorem 4.2 establishes the convergence of ϕ˚
1pziptqq for any i P rns
as t Ñ `8, but does not preclude the fact that }ziptq} may diverge toward `8 (along
the hyperplane) as t Ñ `8. This is indeed expected (and observed numerically—
see Figure 6) when V has some negative eigenvalues. We also note that when all
the eigenvalues of V are non-negative, Corollary 9.2 shows that all the ziptq remain
bounded.
Remark 9.5 (The case where V is not diagonalizable). If V is not assumed to
be diagonalizable, Lemma 9.3 (or, at least the proof thereof) requires some modi-
ﬁcations. Let δ :“ λ1 ´ |λ2| ą 0. Let ε ą 0 be ﬁxed and to be chosen later. We
decompose V in Jordan blocks, and we consider
Cd “
m
à
k“1
Fk,
(9.11)
where Fk is the span of the Jordan chain corresponding to the k-th Jordan block.
By a slight abuse of notation (solely for the purpose of this remark), we denote by
λk the eigenvalue associated to the k-th Jordan block. We recall that we can choose
a basis pϕk,1, . . . , ϕk,jkq of each Fk in a way that V|Fk reads in this basis as6
»
————–
λk
ε
...
...
...
ε
λk
ﬁ
ﬃﬃﬃﬃﬂ
.
(9.12)
We observe that if ε is chosen suﬃciently small (depending only on δ), Lemma 9.3
may be replaced by the following estimate in each Fk:
DC ą 0, @t ě 0, @i P rns,
››πFk
`
etV ziptq
˘›› ď Cep|λk|`δqt.
(9.13)
Here, πFk denotes the orthogonal projection onto Fk. To prove estimate (9.13), we
follow the proof of Lemma 9.3, with
d
dt}πFkpxiptqq}2 playing the role of
d
dt |ϕ˚
kpxiptqq|2.
The key observation is that combining (9.11) and (9.12) we obtain
}πFkpV xiptqq} ď p|λk| ` δq}πFkpxiptqq},
provided ε is chosen suﬃciently small. Then (9.13) follows as in Lemma 9.3.
With (9.11) at hand, the proof of Theorem 4.2 carries through, under the im-
pactless modiﬁcation that Cepλ1`|λ2|`δqt replaces (9.4) (and subsequent estimates
are modiﬁed in the same way).
10. Proof of Theorem 5.2
In this section, we establish the proof for Theorem 5.2. Since the proof is essen-
tially a combination of the proofs of Theorems 4.2 and 8.1, we may occasionally
skip certain details and refer to the proofs of these two results. As done throughout
this work, we set
A :“ pQJKq
1
2 .
6Recall that Jordan blocks are commonly written with a `1 in the superdiagonal. This can
be replaced by any non-zero complex scalar as done here—see [12, Chapter 3, Corollary 3.1.21].

40
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
We denote by πF : Rd Ñ F the projection onto F parallel to G, and by πG :
Rd Ñ G the projection onto G parallel to F. The set πFpconvptziptquiPrnsqq is a
convex subset of F which is non-increasing with respect to t (the proof of this fact
is identital to that of Proposition 8.2). It therefore converges toward some convex
polytope K as t Ñ `8.
Fix i P rns. We have
πFp 9ziptqq “
n
ÿ
j“1
˜
exAetV ziptq,AetV zjptqy
řn
k“1 exAetV ziptq,AetV zkptqy
¸
πFpV pzjptq ´ ziptqqq
“
n
ÿ
j“1
˜
exAetV ziptq,AetV pzjptq´ziptqqy
řn
k“1 exAetV ziptq,AetV pzkptq´ziptqqy
¸
πFpV pzjptq ´ ziptqqq.
From this point on, we follow the proof of Theorem 8.1, and we solely highlight the
changes compared to the original proof. Roughly speaking, this new proof amounts
to adding projections πF at several places. We denote by S Ă F the set of points
w P K such that
}πFpAwq}2 “ max
jPrms xπFpAwq, πFpAvjqy .
The fact that S Ă BK and that S has ﬁnite cardinality is proved precisely as Claim
1 (in the proof of Theorem 8.1), simply by replacing all occurrences of A¨ by πFpA¨q.
Once again, Sδ denotes the set of all points in K at distance ď δ to some point of
S.
Step 2 in the proof of Theorem 8.1 (i.e., (8.7)) is replaced by the following
statement:
Step 2’: There exists a constant γ “ γpKq ą 0 (depending only on the geometry
of K) such that for any δ P p0, δ0s, there exists T “ Tpδq ą 0 such that if t ě T and
πFpziptqq R Sδ, then
d
dt}πFpAziptqq}2 ě γδ.
We now proceed in proving this statement.
Proof of Step 2’. We set
ajptq :“ xπFpAziptqq, πFpApzjptq ´ ziptqqy
and
rjptq :“
@
AetV ziptq, AetV pzjptq ´ ziptqq
D
´ ajptqe2λ1t.
We ﬁnd
1
2
d
dt}πFpAziptqq}2 “ xπFpA 9ziptqq, πFpAziptqqy
“
n
ÿ
j“1
˜
exAetV ziptq,AetV zjptqy
řn
k“1 exAetV ziptq,AetV zkptqy
¸
xπFpApzjptq ´ ziptqqq, πFpAziptqqy
“
n
ÿ
j“1
˜
exAetV ziptq,AetV pzjptq´ziptqqy
řn
k“1 exAetV ziptq,AetV pzkptq´ziptqqy
¸
xπFpApzjptq ´ ziptqqq, πFpAziptqqy
“
n
ÿ
j“1
eajptqe2λ1t`rjptq
řn
k“1 eakptqe2λ1t`rkptq ajptq
loooooooooooooooomoooooooooooooooon
“:bjptq
.
(10.1)

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
41
We now make use of the following adaptation of Claim 2.
Claim 3. There exists some constant γ1 “ γ1pKq ą 0 depending only on the geom-
etry of K such that the following holds. Fix δ P p0, δ0s. There exists T “ Tpδq ą 0
such that if t ě T and ziptq R Sδ ˆ G, then there exists j P rns such that ajptq ě γ1δ.
Compared to Step 2 in the proof of Theorem 8.1, we now have to estimate the
coeﬃcients rjptq. To this end, setting yjptq :“ AetV zjptq for j P rns, we notice that
rjptq “ P1ptq ` P2ptq ` P3ptq where
P1ptq “ xπFpyiptqq, πGpyjptq ´ yiptqqy,
P2ptq “ xπGpyiptqq, πFpyjptq ´ yiptqqy,
P3ptq “ xπGpyiptqq, πGpyjptq ´ yiptqqy.
By virtue of Lemma 9.3 we have |πFpyjptqq| ď Ceλ1t and |πGpyjptqq| ď Cet|λ2| for
any t ě 0 (or Cet|λ2|`ε if V|G is not diagonalizable—see Remark 9.5), hence
|rjptq| ď Cetpλ1`|λ2|q.
(10.2)
Since πFpzjptqq is uniformly bounded in t P r0, `8q for any j P rns due to
Corollary 8.3, we get ajp¨q P L8p0, `8q. So we may set
κ :“ max
jPrns sup
tě0
|ajptq|.
Let t ě 0. We deﬁne
Bptq :“
␣
j P rns | ajptqe2λ1t ` rjptq ě 0
(
.
Let j0ptq P argmaxjPrnspajptqe2λ1t ` rjptqq. Note that j0ptq P Bptq since
aj0ptqe2λ1t ` rj0ptq ě aiptqe2λ1t ` riptq “ 0.
We notice the following three properties:
‚ For j “ j0ptq, we have bj0ptqptq ě
aj0ptqptq
n
(recall the deﬁnition of bj in
(10.1));
‚ for any j P Bptqztj0u, we have bjptq ě 0;
‚ for any j R Bptq, we have
bjptq ě ´κ exp
´
´aj0ptqe2λ1t ` Cepλ1`|λ2|qt¯
.
Indeed, using the fact that j P Bptq and (10.2), we ﬁnd
exp
`
ajptqe2λ1t ` rjptq
˘
řn
k“1 exp pakptqe2λ1t ` rkptqq ď
1
řn
k“1 exp pakptqe2λ1t ` rkptqq
ď
1
exp paj0ptqe2λ1t ` rj0ptqq
ď exp
´
´aj0ptqe2λ1t ` Cepλ1`|λ2|qt¯
.
Making use of these properties in (10.1) yields the desired lower bound—indeed,
if t is suﬃciently large and ziptq R Sδ ˆ G, we have tj P rns | ajptq ě γ1δu ‰ H
according to Claim 3, and so we deduce that
1
2
d
dt}Aziptq}2 ě γ1δ
n ´ κne´γ1δe2λ1t`Cepλ1`|λ2|qt.
Taking t possibly larger (and depending on δ), we obtain the result of Step 2’.
□

42
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
Steps 3 and 4 in the proof of Theorem 8.1 are essentially unchanged—we re-
place all the occurrences of }A ¨ } by }πFpA¨q} (for instance in (8.13) and (8.14)).
Although }Aziptq} may not be uniformly bounded in t, it is important to note
that }πFpAziptqq} is uniformly bounded. Similarly, while 9ziptq R L8pr0, `8qq, we
do have } d
dtπFpzip¨qq}L8pr0,`8qq ă `8. The sets Sδ, Ck and Cr
k are replaced by
Sδ ˆ G, Ck ˆ G and Cr
k ˆ G respectively. The conclusion is that }πFpAziptqq}2 has
to increase by at least
γδ
1
2 pδ
1
4 ´ δq
} 9zi}L8pr0,`8qq
ě
δ
3
4
2} 9zi}L8pr0,`8qq
ą 4R}A}opδ
during a travel from Ck ˆ G to the complement of C
1
4
k ˆ G. As in the proof of
Theorem 8.1 this implies that for any i P rns there exists s P S such that ziptq
remains at distance at most δ away from tsu ˆ G. This being true for any δ ą 0,
we obtain the desired result.
11. Numerical experiments
11.1. Setup. All ﬁgures presented in this paper were generated by discretizing
the underlying dynamics (either (1.1) or (3.1)) using a fourth order Runge-Kutta
scheme with a step size of 0.1. All points in the initial sequence were sampled
independently from the uniform distribution over the hypercube r´5, 5sd.
Ran-
dom matrices (e.g., Q, K, V ) have entries sampled independently from the uniform
distribution on r´1, 1s. Animated plots of all examples may be found online at
https://github.com/borjanG/2023-transformers.
We now present some experiments which motivate some conjectures and claims
made in what precedes.
11.2. Experiments related to Theorem 2.1. We begin with the setup of The-
orem 2.1, which we recall was proven to hold in the case d “ 1. Herein we present
a couple of examples (Figures 10 and 11) which elucidate the role that d and n
appear to play in this fact.
Notably, as seen in Figure 4, we believe that the conclusion of Theorem 2.1 could
plausibly be extended to any d ą 1, assuming V ą 0.
0
20
40
60
80
0
20
40
60
80
t = 0.0, rank= 11
0
20
40
60
80
0
20
40
60
80
t = 3.0, rank= 27
0
20
40
60
80
0
20
40
60
80
t = 5.0, rank= 15
0
20
40
60
80
0
20
40
60
80
t = 10.0, rank= 3
Figure 10. We expand on Figure 3—for the same setup, consider n “
100. The sequence length n does not appear to inﬂuence the rank of
Pptq, which is expected since the rank of P corresponds to the number
of leaders.

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
43
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 0.0, rank= 40
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 0.2, rank= 40
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 1.0, rank= 2
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 10.0, rank= 2
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 0.0, rank= 40
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 0.5, rank= 7
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 5.0, rank= 2
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 10.0, rank= 2
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 0.0, rank= 40
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 0.5, rank= 4
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 1.0, rank= 2
0
5
10
15
20
25
30
35
0
5
10
15
20
25
30
35
t = 10.0, rank= 2
Figure 11. We consider n “ 40, Q “ K “ Id and a random matrix
V ą 0 in dimensions d “ 10 (ﬁrst row), d “ 40 (second row), and d “ 80
(third row). The conclusion of Theorem 2.1 appears to transfer to the
higher dimensional case, and this would actually follow from Conjecture
4 (should it hold).
11.3. Illustrating Theorem 4.2 in R3. To precisely illustrate the appearance of
at most three hyperplanes in the setting of Theorem 4.2, we gave an example in R2.
We expand on this and provide a couple of toy examples in R3 for the purpose of
visualization (we recall that these are toy models, as Transformers in practice are
high-dimensional), and namely focus in both examples on the case where the two
latter eigenvalues are complex. In Figure 13, we see the eﬀect of having eigenvalues
with a negative real part, and the complementary case is illustrated in Figure 12.
12. Outlook
Several important directions regarding the mathematical theory of Transformers
remain unexplored. An important extension of our work would amount to studying
multi-headed Transformers—borrowing the notation from Remark 3.3, they amount
to:
xrk`1s
i
“ xrks
i
` ∆t
H
ÿ
h“1
n
ÿ
j“1
¨
˝
e
A
Qhxrks
i
,Khxrks
j
E
řn
ℓ“1 e
A
Qhxrks
i
,Khxℓpkq
E
˛
‚Vhxrks
j ,
k P N.
For each h P rHs (corresponding to a diﬀerent head), the weight matrices Qh, Kh, Vh
are constant. Proofs regarding clustering or convergence of the self-attention matrix
for such dynamics is an open problem. Preliminary numerical investigations seem

44
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
−5
0
5
−5
0
5
−5
0
5
10
t = 0.0
−5
0
5
−5
0
5
−5
0
5
10
t = 5.0
−5
0
5
−5
0
5
−5
0
5
10
t = 40.0
Figure 12. We consider n “ 25, Q “ K “ Id, and V a random matrix
with positive entries and eigenvalues t1, 0.1 ` 0.08i, 1 ´ 0.08iu.
The
pair of complex eigenvalues have a positive real part.
We not only
see convergence to one of two hyperplanes determined by the direction
ϕ1 “ p0.38, 0.8, 0.47q, but in fact the particles appear to collapse to two
points. In other words, the "hyperplanes" are of codimension 3, which
is in line with Conjecture 4.
−10
0
10
0
10
−5
0
5
t = 0.0
−10
0
10
0
10
−5
0
5
t = 5.0
−10
0
10
0
10
−5
0
5
t = 10.0
−50
0
50
−2502550
−25
0
25
50
t = 30.0
−50
0
50
−2502550
−25
0
25
50
t = 35.0
−50
0
50
−2502550
−25
0
25
50
t = 40.0
Figure 13. We consider n “ 25, Q “ K “ Id, and V a random matrix
with positive entries and eigenvalues t1, ´0.05 ` 0.25i, ´0.05 ´ 0.25iu.
The pair of complex eigenvalues have a negative real part, which en-
tails the rotation of the particles.
We see that the particles rotate
within a couple of 2-dimensional hyperplanes determined by ϕ1 “
p´0.3, ´0.8, ´0.45q, as implied by Theorem 4.2.
to indicate that interesting clustering phenomena also occur in this context.
A

THE EMERGENCE OF CLUSTERS IN SELF-ATTENTION DYNAMICS
45
characterization or properties of optimal weights by invoking the optimal control
correspondence in the spirit of [28] is also an interesting avenue for future research.
Acknowledgments. We are thankful to Pierre Ablin, Léonard Boussioux, Enric
Boix-Adsera, Yair Shenfeld and Emmanuel Trélat for helpful discussions. B.G. was
funded by TotalEnergies. C.L. was supported by the Simons Foundation Grant
601948, DJ. P. Rigollet is supported by NSF grants IIS-1838071, DMS-2022448,
and CCF-2106377. Y.P. is supported in part by the MIT-IBM Watson AI Lab.
References
[1] Bradley, P. S., and Mangasarian, O. L. K-plane clustering. Journal of Global optimiza-
tion 16 (2000), 23–32.
[2] Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural ordinary
diﬀerential equations. Advances in Neural Information Processing Systems 31 (2018).
[3] Cho, J., Ha, S.-Y., Huang, F., Jin, C., and Ko, D. Emergence of bi-cluster ﬂocking
for the Cucker–Smale model. Mathematical Models and Methods in Applied Sciences 26, 06
(2016), 1191–1218.
[4] Cucker, F., and Smale, S. Emergent behavior in ﬂocks. IEEE Transactions on Automatic
Control 52, 5 (2007), 852–862.
[5] Dobrushin, R. L. Vlasov equations. Funktsional’nyi Analiz i ego Prilozheniya 13, 2 (1979),
48–58.
[6] Golse, F. Mean ﬁeld kinetic equations. Course of Polytechnique (2013).
[7] Ha, S.-Y., Kim, J., Park, J., and Zhang, X. Complete cluster predictability of the
Cucker–Smale ﬂocking model on the real line. Archive for Rational Mechanics and Analysis
231 (2019), 319–365.
[8] Haber, E., and Ruthotto, L. Stable architectures for deep neural networks. Inverse prob-
lems 34, 1 (2017).
[9] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition (2016),
pp. 770–778.
[10] He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In
Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,
October 11–14, 2016, Proceedings, Part IV 14 (2016), Springer, pp. 630–645.
[11] Hegselmann, R., and Krause, U. Opinion dynamics and bounded conﬁdence: models,
analysis and simulation. Journal of Artiﬁcal Societies and Social Simulation (JASSS) 5, 3
(2002).
[12] Horn, R. A., and Johnson, C. R. Matrix analysis. Cambridge University Press, 2012.
[13] Jabin, P.-E., and Motsch, S. Clustering and asymptotic behavior in opinion formation.
Journal of Diﬀerential Equations 257, 11 (2014), 4165–4187.
[14] Krause, U. A discrete nonlinear and non-autonomous model of consensus. In Communica-
tions in Diﬀerence Equations: Proceedings of the Fourth International Conference on Dif-
ference Equations (2000), CRC Press, p. 227.
[15] Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. ALBERT:
A Lite BERT for Self-supervised Learning of Language Representations. In International
Conference on Learning Representations (2020).
[16] Lin, T., Wang, Y., Liu, X., and Qiu, X. A survey of transformers. AI Open 3 (2022),
111–132.
[17] Lu, Y., Li, Z., He, D., Sun, Z., Dong, B., Qin, T., Wang, L., and Liu, T.-Y.
Understanding and improving transformer from a multi-particle dynamic system point of
view. In International Conference on Learning Representations (2020).
[18] Motsch, S., and Tadmor, E. Heterophilious dynamics enhances consensus. SIAM review
56, 4 (2014), 577–621.
[19] Papyan, V., Han, X., and Donoho, D. L. Prevalence of neural collapse during the terminal
phase of deep learning training. Proceedings of the National Academy of Sciences 117, 40
(2020), 24652–24663.

46
GESHKOVSKI, LETROUIT, POLYANSKIY, AND RIGOLLET
[20] Piccoli, B., and Rossi, F. Transport equation with nonlocal velocity in Wasserstein spaces:
convergence of numerical schemes. Acta Applicandae Mathematicae 124 (2013), 73–105.
[21] Piccoli, B., Rossi, F., and Trélat, E. Control to ﬂocking of the kinetic Cucker–Smale
model. SIAM Journal on Mathematical Analysis 47, 6 (2015), 4685–4719.
[22] Sander, M. E., Ablin, P., Blondel, M., and Peyré, G. Sinkformers: Transformers
with doubly stochastic attention. In International Conference on Artiﬁcial Intelligence and
Statistics (2022), PMLR, pp. 3515–3530.
[23] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in Neural Information
Processing Systems 30 (2017).
[24] Vicsek, T., Czirók, A., Ben-Jacob, E., Cohen, I., and Shochet, O. Novel type of
phase transition in a system of self-driven particles. Physical Review Letters 75, 6 (1995),
1226.
[25] Vidal, R. Subspace clustering. IEEE Signal Processing Magazine 28, 2 (2011), 52–68.
[26] Vyas, A., Katharopoulos, A., and Fleuret, F. Fast transformers with clustered atten-
tion. Advances in Neural Information Processing Systems 33 (2020), 21665–21674.
[27] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 (2020).
[28] Weinan, E. A proposal on machine learning via dynamical systems. Communications in
Mathematics and Statistics 1, 5 (2017), 1–11.
[29] Weinan, E., Han, J., and Li, Q. A mean-ﬁeld optimal control formulation of deep learning.
Research in Mathematical Sciences 6, 1 (2019), 10.
[30] Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and Kumar, S. Are transformers
universal approximators of sequence-to-sequence functions? In International Conference on
Learning Representations (2020).
Borjan Geshkovski
Department of Mathematics
MIT
Cambridge, MA
02139 USA
e-mail: borjan@mit.edu
Cyril Letrouit
Department of Mathematics
MIT
Cambridge, MA
02139 USA
e-mail: letrouit@mit.edu
Yury Polyanskiy
Department of Electrical Engineering
and Computer Science
MIT
Cambridge, MA
02139 USA
e-mail: yp@mit.edu
Philippe Rigollet
Department of Mathematics
MIT
Cambridge, MA
02139 USA
e-mail: rigollet@mit.edu

