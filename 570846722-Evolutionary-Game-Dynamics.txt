                                                                                                                    
                                                                                                               

AMS SHORT COURSE LECTURE NOTES
Introductory Survey Lectures
published as a subseries of
Proceedings of Symposia in Applied Mathematics
                                   
                                                                                                                    
                                                                                                               

This page intentionally left blank 
                                                                                                                    
                                                                                                               

Proceedings of Symposia in
APPLIED MATHEMATICS
Evolutionary Game 
Dynamics  
American Mathematical Society  
Short Course 
January 4–5, 2011 
New Orleans, Louisiana
Karl Sigmund  
Editor
Volume 69
American Mathematical Society
Providence, Rhode Island
                                                                                                                    
                                                                                                               

EDITORIAL COMMITTEE
Mary Pugh
Lenya Ryzhik
Eitan Tadmor (Chair)
2010 Mathematics Subject Classiﬁcation. Primary 91A22.
Library of Congress Cataloging-in-Publication Data
American Mathematical Society. Short Course (2011 : New Orleans, LA.)
Evolutionary game dynamics : American Mathematical Society Short Course, January 4–5,
2011, New Orleans, LA / Karl Sigmund, editor.
p. cm. — (Proceedings of symposia in applied mathematics ; v. 69)
Includes bibliographical references and index.
ISBN 978-0-8218-5326-9 (alk. paper)
1. Game theory—Congresses.
I. Sigmund, Karl, 1945–
II. Title.
QA269.A465
2011
519.3—dc23
2011028869
Copying and reprinting. Material in this book may be reproduced by any means for edu-
cational and scientiﬁc purposes without fee or permission with the exception of reproduction by
services that collect fees for delivery of documents and provided that the customary acknowledg-
ment of the source is given. This consent does not extend to other kinds of copying for general
distribution, for advertising or promotional purposes, or for resale. Requests for permission for
commercial use of material should be addressed to the Acquisitions Department, American Math-
ematical Society, 201 Charles Street, Providence, Rhode Island 02904-2294, USA. Requests can
also be made by e-mail to reprint-permission@ams.org.
Excluded from these provisions is material in articles for which the author holds copyright. In
such cases, requests for permission to use or reprint should be addressed directly to the author(s).
(Copyright ownership is indicated in the notice in the lower right-hand corner of the ﬁrst page of
each article.)
c⃝2011 by the American Mathematical Society. All rights reserved.
The American Mathematical Society retains all rights
except those granted to the United States Government.
Copyright of individual articles may revert to the public domain 28 years
after publication. Contact the AMS for copyright status of individual articles.
Printed in the United States of America.
⃝
∞The paper used in this book is acid-free and falls within the guidelines
established to ensure permanence and durability.
Visit the AMS home page at http://www.ams.org/
10 9 8 7 6 5 4 3 2 1
16 15 14 13 12 11
                                                                                                                    
                                                                                                               

Contents
Preface
vii
Introduction to Evolutionary Game Theory
Karl Sigmund
1
Beyond the Symmetric Normal Form: Extensive Form Games, Asymmetric
Games and Games with Continuous Strategy Spaces
Ross Cressman
27
Deterministic Evolutionary Game Dynamics
Josef Hofbauer
61
On Some Global and Unilateral Adaptive Dynamics
Sylvain Sorin
81
Stochastic Evolutionary Game Dynamics: Foundations, Deterministic
Approximation, and Equilibrium Selection
William H. Sandholm
111
Evolution of Cooperation in Finite Populations
Sabin Lessard
143
Index
173
v
                                                                                                                    
                                                                                                               

This page intentionally left blank 
                                                                                                                    
                                                                                                               

Preface
Evolutionary game theory studies basic types of social interactions in popula-
tions of players. It is the ideal mathematical tool for methodological individualism,
i.e., the reduction of social phenomena to the level of individual actions. Evolu-
tionary game dynamics combines the strategic viewpoint of classical game theory
(independent, rational players trying to outguess each other) with population dy-
namics (successful strategies increase their frequencies).
A substantial part of the appeal of evolutionary game theory comes from its
highly diverse applications, such as social dilemmas, the evolution of language, or
mating behavior in animals. Moreover, its methods are becoming increasingly pop-
ular in computer science, engineering, and control theory.
They help to design
and control multi-agent systems, often with large number of agents (for instance,
when routing drivers over highway networks, or data packets over the Internet).
While traditionally these ﬁelds have used a top down approach, by directly control-
ling the behavior of each agent in the system, attention has recently turned to an
indirect approach: allowing the agents to function independently, while providing
incentives that lead them to behave in the desired way. Instead of the traditional as-
sumption of equilibrium behavior, researchers opt increasingly for the evolutionary
paradigm, and consider the dynamics of behavior in populations of agents employ-
ing simple, myopic decision rules. The methods of evolutionary game theory are
used in disciplines as diverse as microbiology, genetics, animal behavior, evolu-
tionary psychology, route planning, e-auctions, common resources management or
micro-economics.
The present volume is based on a mini-course held at the AMS meeting in New
Orleans in January 2011. The lectures deal mostly with the mathematical aspects
of evolutionary game theory, i.e., with the deterministic and stochastic dynamics
describing the evolution of frequencies of behavioral types.
An introductory part of the course is devoted to a brief sketch of the origins of
the ﬁeld, and in particular to the examples that motivated evolutionary biologists
to introduce a population dynamical viewpoint into game theory. This leads to
some of the main concepts: evolutionary stability, replicator dynamics, invasion
ﬁtness, etc. Much of it can be explained by means of simple examples such as the
Rock-Paper-Scissors game. It came as a surprise when childish games of that sort,
intended only for the clariﬁcation of concepts, were found to actually lurk behind
important classes of real-life social and biological interactions. The transmission
of successful strategies by genetic and cultural means results in a rich variety of
stochastic processes and, in the limit of very large populations, deterministic ad-
justment dynamics including diﬀerential inclusions and reaction-diﬀusion equations.
vii
                                                                                                                    
                                                                                                               

viii
PREFACE
Some economists view these types of dynamics merely as tools for so-called equi-
librium reﬁnement and equilibrium selection concepts. (Indeed, most games have
so many equilibria that it is hard to select the ‘right one’). However, evolutionary
games have also permitted us to move away from the equilibrium-centered view-
point. Today, we understand that it is often premature to assume that behavior
converges to an equilibrium. In particular, an evolutionarily stable strategy need
not be reachable. A homogeneous population using that strategy cannot be invaded
by a minority of dissidents, but a homogeneous population with a slightly diﬀerent
strategy can evolve away from it. Limit phenomena such as periodic or heteroclinic
cycles, or chaotic attractors, may be considered, perhaps not as ‘solutions of the
game’, but as predictions of play. On the other hand, large classes of games leading
to global convergence are presently much better understood.
This book oﬀers a succinct state-of-the-art introduction to the increasingly
sophisticated mathematical techniques behind evolutionary game theory.
                                                                                                                    
                                                                                                               

Proceedings of Symposia in Applied Mathematics
Volume 69, 2011
Introduction to Evolutionary Game Theory
Karl Sigmund
Abstract. This chapter begins with some basic terminology, introducing ele-
mentary game theoretic notions such as payoﬀ, strategy, best reply, Nash equi-
librium pairs etc. Players who use strategies which are in Nash equilibrium
have no incentive to deviate unilaterally. Next, a population viewpoint is intro-
duced. Players meet randomly, interact according to their strategies, and ob-
tain a payoﬀ. This payoﬀdetermines how the frequencies in the strategies will
evolve. Successful strategies spread, either (in the biological context) through
inheritance or (in the cultural context) through social learning. The simplest
description of such an evolution is based on the replicator equation. The ba-
sic properties of replicator dynamics are analyzed, and some low-dimensional
examples such as the Rock-Scissors-Paper game are discussed. The relation
between Nash equilibria and rest points of the replicator equation are inves-
tigated, which leads to a short proof of the existence of Nash equilibria. We
then study mixed strategies and evolutionarily stable strategies. This intro-
ductory chapter continues with a brief discussion of other game dynamics, such
as the best reply dynamics, and ends with the simplest extension of replicator
dynamics to asymmetric games.
1. Predictions and Decisions
Predictions can be diﬃcult to make, especially, as Niels Bohr quipped, if they
concern the future.
Reliable forecasts about the weather or about some social
development may seem to oﬀer comparable challenges, at ﬁrst sight. But there is a
fundamental diﬀerence: a weather forecast does not inﬂuence the weather, whereas
a forecast on economy can inﬂuence the economic outcome. Humans will react if
they learn about the predictions, and they can anticipate that others will react,
too.
When the economist Oskar Morgenstern, in the early ’thirties, became aware
of the problem, he felt that he had uncovered an ’impossibility theorem’ of a simi-
larly fundamental nature as the incompleteness theorem of his friend, the logician
Kurt G¨odel. Morgenstern was all the more concerned about it as he was director
of the Vienna-based Institut f¨ur Konjunkturforschung, the Institute for Business
Cycles Research, whose main task was actually to deliver predictions on the Aus-
trian economy. Oscar Morgenstern expained his predicament in many lectures and
publications, using as his favorite example the pursuit of Sherlock Holmes by the
2000 Mathematics Subject Classiﬁcation. Primary 91A22.
The author wants to thank Ross Cressman for his helpful comments.
1
                                           
                                                                                                                    
                                                                                                               

2
KARL SIGMUND
infamous Professor Moriarty [24]. These two equally formidable adversaries would
never arrive at a conclusive solution in mutually outguessing each other.
We can describe the fundamental nature of the problem by using some of the
mathematical notation which later was introduced through game theory. Let us
suppose that player I has to choose between n options, or strategies, which we
denote by e1,..., en, and player II between m strategies f1,..., fm. If I chooses ei
and II chooses fj, then player I obtains a payoﬀaij and player II obtains bij. The
game, then, is described by two n × m payoﬀmatrices A and B: alternatively, we
can describe it by one matrix whose element, in the i-th row and j-th column, is the
pair (aij, bij) of payoﬀvalues. The payoﬀis measured on a utility scale consistent
with the players’ preference ranking.
The two players could engage in the game ’Odd or Even?’ and decide that the
loser pays one dollar to the winner. At a given signal, each player holds up one or
two ﬁngers. If the resulting sum is odd, player I wins. If the sum is even, player II
wins. Each player then has to opt for even and odd, which correspond to e1 and
e2 for player I and f1 and f2 for player II, and the payoﬀmatrix is
(1.1)

(−1, 1)
(1, −1)
(1, −1)
(−1, 1)

If the two players graduate to the slightly more sophisticated Rock-Scissors-Paper
game, they would each have to opt between three strategies, numbered in that
order, and the payoﬀmatrix would be
(1.2)
⎛
⎝
(0, 0)
(1, −1)
(−1, 1)
(−1, 1)
(0, 0)
(1, −1)
(1, −1)
(−1, 1)
(0, 0)
⎞
⎠.
If both Rock-Scissors-Paper players opt for the same move, the game is a tie and
both obtain payoﬀzero. If the outcome is (0, 0) or (−1, 1), then player I (who
chooses the row of the payoﬀmatrix) would have done better to choose another
strategy; if the outcome is (1, −1) or (0, 0), then it is player II, the column player,
who would have done better to switch.
If a prediction is made public, then at
least one of the players would have an incentive to deviate. The other player would
anticipate this, and deviate accordingly, and both would be launched into a vicious
circle of mutual outguessing.
A few years, however, after Morgenstern had started to broadcast his impossi-
bility result, the topologist Cech pointed out to him that John von Neumann had
found, in an earlier paper on parlor games, a way to avoid Morgenstern’s dead
end [42]. It consists in randomizing, i.e. letting chance decide. Clearly, if players
opt with equal probability for each of their alternatives, none has an incentive to
deviate. Admittedly, this would lead to the expected payoﬀ0, somewhat of an
anti-climax. But John von Neumann’s minimax theorem holds for a much larger
class of games. Most importantly, it led, in the ’forties, to a collaboration of John
von Neumann with Oscar Morgenstern which gave birth to game theory [43]. A
few years later, John Nash introduced an equilibrium notion valid in an even more
general context, which became the cornerstone of game theory [33].
2. Mixed strategies and best replies
Suppose that player I opts to play strategy ei with probability xi. This mixed
strategy is thus given by a stochastic vector x = (x1, ..., xn) (with xi ≥0 and
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
3
x1 + ... + xn = 1). We denote the set of all such mixed strategies by Δn: this is a
simplex in Rn, spanned by the unit vectors ei of the standard base, which are said
to be the pure strategies, and correspond to the original set of alternatives.
Similarly, a mixed strategy for player II is an element y of the unit simplex Δm
spanned by the unit vectors fj. If player I uses the pure strategy ei and player II
uses strategy y, then the payoﬀfor player I (or more precisely, its expected value)
is
(2.1)
(Ay)i =
m

j=1
aijyj.
If player I uses the mixed strategy x, and II uses y, the payoﬀfor player I is
(2.2)
x · Ay =

i
xi(Ay)i =

i,j
aijxiyj,
and the payoﬀfor player II, similarly, is
(2.3)
x · By =

i,j
bijxiyj.
If player I knows the strategy y of the co-player, then player I should use a
strategy which is a best reply to y. The set of best replies is the set
(2.4)
BR(y) = arg max
x
x · Ay,
i.e. the set of all x ∈Δn such that z · Ay ≤x · Ay holds for all z ∈Δn. Player I
has no incentive to deviate from x and chose another strategy z instead.
Since the function z →z · Ay is continuous and Δn is compact, the set of best
replies is always non-empty. It is a convex set. Moreover, if x belongs to BR(y),
so do all pure strategies in the support of x, i.e. all ei for which xi > 0. Indeed, for
all i,
(2.5)
(Ay)i = ei · Ay ≤x · Ay.
If the inequality sign were strict for some i with xi > 0, then xi(Ay)i < xi(x · Ay);
summing over all i = 1, ..., n then leads to a contradiction. It follows that the set
BR(y) is a face of the simplex Δn. It is spanned by the pure strategies which are
best replies to y.
If player I has found a best reply to the strategy y of player II, then player I has
no incentive not to use it, as long as player II sticks to y. But will player II stick
to y? Only if player II has no incentive either to use another strategy, i.e. has also
hit upon a best reply. Two strategies x and y are said to form a Nash equilibrium
pair if each is a best reply to the other, i.e., if x ∈BR(y) and y ∈BR(x), or
alternatively if
(2.6)
z · Ay ≤x · Ay
holds for all z ∈Δn, and
(2.7)
x · Bw ≤x · By
holds for all w ∈Δm. A Nash equilibrium pair (x, y) satisﬁes a minimal consistency
requirement: no player has an incentive to deviate (as long as the other player does
not deviate either).
A basic result states that there always exist Nash equilibrium pairs, for any
game (A, B). The result holds for vastly wider classes of games than considered so
                                                                                                                    
                                                                                                               

4
KARL SIGMUND
far; it holds for any number of players, any convex compact sets of strategies, any
continuous payoﬀfunctions, and even beyond (see, e.g., [30]). But it would not
hold if we had not allowed for mixed strategies, as is shown by the Rock-Scissors-
Paper game. In that case, the mixed strategy which consists in choosing, with equal
probability 1/3, among the three alternative moves, clearly leads to an equilibrium
pair. No player has a reason to deviate. On the other hand, if player I uses any
other strategy (x1, x2, x3) against the (1/3, 1/3, 1/3) of player II, player I would
still have an expected payoﬀof 0. However, the other player II would then have an
incentive to deviate, presenting I with an incentive to deviate in turn, and so on.
In this example, (x, y) with x = y = (1/3, 1/3, 1/3) is the unique Nash equilib-
rium pair. We have seen that as long as player II chooses the equilibrium strategy
y, player I has no reason to deviate from the equilibrium strategy x, but that on the
other hand, player I has no reason not to deviate, either. This would be diﬀerent
if (x, y) were a strict Nash equilibrium pair, i.e. if
(2.8)
z · Ay < x · Ay
holds for all z ̸= x, and
(2.9)
x · Bw < x · By
holds for all w ̸= y. In this case, i.e. when both best-reply sets are singletons, and
hence correspond to pure strategies, each player will be penalized for unilaterally
deviating from the equilibrium.
Whereas every game admits a Nash equilibrium pair, some need not admit a
strict Nash equilibrium pair, as our previous examples show.
3. Excurse to zero sum
Historically, game theory focused ﬁrst on zero-sum games, for which aij = −bij
for all i, j, i.e., A = −B (the gain of player I is the loss of player II). This condition
clearly holds for a large set of parlor games. But it certainly restricts the range of
applications. For most types of social and economic interactions, the assumption
that the interests of the two players are always diametrically opposite does not
hold.
Even in military confrontations, there often exist outcomes both parties
want to avoid. Most interactions are of mixed motive type, and contain elements
of cooperation as well as competition. Interestingly, John von Neumann did not
greatly appreciate the solution concept proposed by the undergraduate student
John Nash. A short interview ended when John von Neumann remarked, somewhat
dismissively, ’Oh, just a ﬁxed point theorem’ [32]. We shall see that the existence
proof for Nash equilibrium pairs does indeed reduce to a ﬁxed point theorem, and
a rather simple one at that. Nevertheless, it yields a very powerful result, as can
be seen by applying it to the special case of zero sum games, where it leads to a
three-liner proof of the celebrated maximin theorem, a proof which is considerably
simpler than John von Neumann’s original, brute-force demonstration.
It is easy to see that (¯x, ¯y) is a Nash equilibrium pair of a zero-sum game iﬀ
(3.1)
x · A¯y ≤¯x · A¯y ≤¯x · Ay
for all x ∈Δn, y ∈Δm. Suppose that player II correctly guesses that player I plays
x. Then player II will use a strategy which is a best reply, i.e., minimizes player
I’s payoﬀ, which will reduce to g(x) := miny x · Ay. A player I who expects to be
anticipated, then, ought to maximize g(x). Any strategy ˆx yielding this maximum
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
5
is said to be a maximin strategy for player I. Such a maximin strategy is deﬁned
by ˆx := arg maxx g(x), and it guarantees player I a security level
(3.2)
wu := max
x
min
y x · Ay.
Similarly, we can expect player II to maximize the own security level, i.e., since
A = −B, to play a minimax strategy ˆy such that player I has a payoﬀbounded
from above by
(3.3)
wo := min
y max
x
x · Ay.
The pair (ˆx, ˆy) is said to be a maximin pair. It satisﬁes
(3.4)
min
y
ˆx · Ay = wu,
max
x
x · Aˆy = wo,
and leads to a payoﬀwhich clearly satisﬁes
(3.5)
wu ≤ˆx · Aˆy ≤wo.
If (¯x, ¯y) is a Nash equilibrium pair for a zero sum game, then it is a maximin pair.
Indeed, by (3.1),
(3.6)
max
x
x · A¯y ≤¯x · A¯y ≤min
y
¯x · Ay.
Now by (3.3), wo is less than the left hand side of the previous inequality and by
(3.2) wu larger than the right hand side. Since wu ≤wo by (3.5), we must actually
have equality everywhere.
But wu = miny ¯x · Ay means that ¯x is a maximin
solution, and maxx x · A¯y = wo that ¯y is a minimax solution.
For zero-sum games, the existence of a Nash equilibrium pair thus implies the
existence of a maximin pair. The previous argument implies wu = wo, i.e.,
(3.7)
min
y max
x
x · Ay = max
x
min
y x · Ay.
Conversely, it is easy to see that if (ˆx, ˆy) is a maximin pair of a zero sum game,
then it is a Nash equilibrium pair.
4. Concerns about the Nash solution
Let us note that if (ˆx, ˆy) and (¯x, ¯y) are two Nash equilibrium pairs for a zero
sum game, then so are (ˆx, ¯y) and (¯x, ˆy). Indeed,
(4.1)
ˆx · Aˆy ≤ˆx · A¯y ≤¯x · A¯y ≤¯x · Aˆy ≤ˆx · Aˆy,
hence equality holds everywhere, and therefore for all x and y:
(4.2)
x · A¯y ≤ˆx · A¯y ≤ˆx · Ay
so that (ˆx, ¯y) is a Nash equilibrium pair.
The same need not hold for general (non zero-sum) games. Consider for in-
stance
(4.3)

(1, 1)
(−1, −1)
(−1, −1)
(1, 1)

It is easy to see that (e1, f1) and (e2, f2) are two Nash equilibrium pairs.
But
(e1, f2) or (e2, f1) are not. How should the two players coordinate their choice?
The problem becomes even more acute for a coordination game given by
(4.4)

(2, 2)
(−100, 0)
(0, −100)
(1, 1)

.
                                                                                                                    
                                                                                                               

6
KARL SIGMUND
Again, (e1, f1) and (e2, f2) are two Nash equilibrium pairs. The former has the
advantage of yielding a higher payoﬀto both players: it is said to be Pareto-optimal.
But the second is less risky, and therefore said to be risk-dominant. Indeed, it can
be very costly to go for the Pareto-optimum if the other player fails to do so. It
may actually be best to decide against using the Pareto-optimum right away. In
any case, if the game is not zero-sum, Nash equilibrium pairs may not oﬀer much
help for decision makers.
Moreover, even if there exists a unique Nash equilibrium pair, it can lead to
frustration, as in the following example:
(4.5)

(10, 10)
(−5, 15)
(15, −5)
(0, 0)

.
In this case, e2 is the best reply to every (pure or mixed) strategy of player II, and
similarly f2 is always the best reply for player II. Hence (e2, f2) is the unique Nash
equilibrium pair, and it is strict. This game is an example of a Prisoner’s Dilemma
game. The payoﬀmatrix may occur, for instance, if two players are asked to choose,
independently and anonymously, whether or not to provide a gift of 15 dollars to
the co-player, at a cost of 5 dollars to themselves. It the two players cooperate by
both opting for their ﬁrst strategy, they will end up with 10 dollars each. But each
has an incentive to deviate. It is only when both opt for their second solution and
defect, that they cannot do better by choosing to deviate. But then, they end up
with zero payoﬀ. Let us remark that this dilemma cannot be solved by appealing to
non-monetary motivations. It holds whenever the payoﬀvalues reﬂect each players’
preference ordering, which may well include a concern for the other.
5. Population Games
So far, we have considered games between two speciﬁc players trying to guess
each other’s strategy and ﬁnd a best reply. This belongs to the realm of classical
game theory, and leads to interesting mathematical and economic developments.
Starting with the ’sixties and ’seventies, both theory and applications were con-
siderably stimulated by problems in evolutionary biology, such as sex-ratio theory
or the investigation of ﬁghting behavior [12, 27].
It required a radical shift in
perspective and the introduction of thinking in terms of populations [29]. It pro-
vided a welcome tool for the analysis of frequency dependent selection and, later,
of learning processes.
Let us therefore consider a population of players, each with a given strategy.
From time to time, two players meet randomly and play the game, using their
strategies. We shall consider these strategies as behavioral programs. Such pro-
grams can be learned, or inherited, or imprinted in any other way. In a biological
setting, strategies correspond to diﬀerent types of individuals (or behavioral phe-
notypes). The outcome of each encounter yields payoﬀvalues which are no longer
measured on utility scales reﬂecting the individual preferences of the players, but in
the one currency that counts in Darwinian evolution, namely ﬁtness, i.e., average
reproductive success. If we assume that strategies can be passed on to the oﬀspring,
whether through inheritance or through learning, then we can assume that more
successful strategies spread.
In order to analyze this set-up, it is convenient to assume, in a ﬁrst approach,
that all individuals in the population are indistinguishable, except in their way
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
7
of interacting, i.e.
that the players diﬀer only by their strategy.
This applies
well to games where both players are on an equal footing. Admittedly, there are
many examples of social interactions which display an inherent asymmetry between
the two players: for instance, between buyers and sellers, or between parents and
oﬀspring. We will turn to such interactions later.
Thus we start by considering only symmetric games. In the case of two-player
games, this means that the game remains unchanged if I and II are permuted. In
particular, the two players have the same set of strategies. Hence we assume that
n = m and fj = ej for all j; and if a player plays strategy ei against someone using
strategy ej (which is the former fj), then that player receives the same payoﬀ,
whether labeled I or II. Hence aij = bji, the payoﬀfor a ei-player against a ej-
players does not depend on who is labelled I and who is II, or in other words B = AT .
Thus a symmetric game is speciﬁed by the pair (A, AT), and therefore is deﬁned by
a single, square payoﬀmatrix A. All examples encountered so far are symmetric,
with the exception of ’Even or Odd’. A zero-sum game which is symmetric must
satisfy AT = −A and hence corresponds to a skew-symmetric payoﬀmatrix.
It is easy to see that the symmetric game given by
(5.1)

−1
1
1
−1

,
where success depends on doing the opposite of the co-player, admits (e1, e2) and
(e2, e1) as asymmetric Nash equilibrium pairs.
These are plainly irrelevant as
solutions of the game, since it is impossible to distinguish players I and II. Of
interest are only symmetric Nash equilibrium pairs, i.e. pairs of strategies (x, y)
with x = y. A symmetric Nash equilibrium, thus, is speciﬁed by one strategy x
having the property that it is a best reply to itself (i.e. x ∈BR(x)). In other
words, we must have
(5.2)
z · Ax ≤x · Ax
for all z ∈Δn. A symmetric strict Nash equilibrium is accordingly given by the
condition
(5.3)
z · Ax < x · Ax
for all z ̸= x.
We shall soon prove that every symmetric game admits a symmetric Nash
equilibrium. But ﬁrst, we consider a biological toy model which played an essential
role in the emergence of evolutionary game theory [27]. It is due to two eminent
theoretical biologists, John Maynard Smith and George Price, who tried to explain
the evolution of ritual ﬁghting in animal contests. It had often been observed that
in conﬂicts within a species, animals did not escalate the ﬁght, but kept to certain
stereotyped behavior, such as posturing, glaring, roaring or engaging in a pushing
match. Signals of surrender (such as oﬀering the unprotected throat) stopped the
ﬁght as reliably as a towel thrown into the boxing ring. Interestingly, thus, animal
ﬁghts seem to be restrained by certain rules, without even needing a referee. Such
restraint is obviously all for the good of the species, but Darwinian thinking does
not accept this as an argument for its emergence. An animal ignoring these ’gloved
ﬁst’-type of rules, and killing its rivals, should be able to spread its genes, and the
readiness to escalate a conﬂict should grow, even if this implies, in the long run,
suicide for the species.
                                                                                                                    
                                                                                                               

8
KARL SIGMUND
Maynard Smith and Price imagined, in their thought experiment, a population
consisting of two phenotypes (or strategies). Strategy e1 is a behavioral program
to escalate the conﬂict until death or injury settles the outcome. Strategy e2 is a
behavioral program to ﬂee as soon as the opponent starts getting rough. The former
strategy is called ’Hawk’, the latter ’Dove’. Winning the conﬂict yields an expected
payoﬀG, and losing an escalated ﬁght costs C > G (with G and C measured on
the scale of Darwinian ﬁtness). If we assume that whenever two ’Hawks’ meet, or
two ’Doves’, both are equally likely to win the contest, then their expected payoﬀ
is G/2 −C/2 resp. G/2. The payoﬀmatrix thus is
(5.4)

G−C
2
G
0
G
2

.
Clearly, neither ’Hawk’ nor ’Dove’ is a Nash equilibrium. In terms of evolutionary
biology, a homogeneous ’Dove’ population could be invaded by a minority of ’Hawks’
who win all their contests hands down; but similarly, a homogeneous ’Hawk’ pop-
ulation could be invaded by a minority of ’Doves’, whose payoﬀ0 is larger than
the negative payoﬀ(G −C)/2 experienced by the ’Hawks’ tearing at each other. It
is better to experience no change in reproductive success, rather than a reduction.
In this sense neither ’Hawk’ nor ’Dove’ is an evolutionarily stable strategy. On
the basis of this extremely simpliﬁed model, we must expect evolution to lead to a
mixed population.
6. Population dynamics
Let us consider a symmetric game with payoﬀmatrix A and assume that in
a large, well-mixed population, a fraction xi uses strategy ei, for i = 1, ..., n. The
state of the population is thus given by the vector x ∈Δn. A player with strategy
ei has as expected payoﬀ
(6.1)
(Ax)i =

j
aijxj.
Indeed, this player meets with probability xj a co-player using ej. The average
payoﬀin the population is given by
(6.2)
x · Ax =

i
xi(Ax)i.
It should be stressed that we are committing an abuse of notation.
The same
symbol x ∈Δn which denoted in the previous sections the mixed strategy of one
speciﬁc player (cf. (2.1) and (2.2)) now denotes the state of a population consisting
of diﬀerent types, each type playing its pure strategy. (We could also have the
players use mixed strategies, but will consider this case only later.)
Now comes an essential step: we shall assume that populations can evolve,
in the sense that the relative frequencies xi change with time. Thus we let the
state x(t) depend on time, and denote by ˙xi(t) the velocity with which xi changes.
The assumption of diﬀerentiability implies an inﬁnitely large population, or the
interpretation of xi as an expected value, rather than a bona ﬁde frequency. Both
ways of thinking are familiar to mathematical ecologists.
In keeping with our
population dynamical approach, we shall be particularly interested in the (per
capita) growth rates ˙xi/xi of the frequencies of the strategies.
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
9
How do the frequencies of strategies evolve? There are many possibilities for
modeling this process. We shall at ﬁrst assume that the state of the population
evolves according to the replicator equation (see [40, 16, 46] and, for the name,
[37]). This equation holds if the growth rate of a strategy’s frequency corresponds
to the strategy’s payoﬀ, or more precisely to the diﬀerence between its payoﬀ(Ax)i
and the average payoﬀx · Ax in the population. Thus we posit
(6.3)
˙xi = xi[(Axi) −x · Ax]
for i = 1, ..., n. Accordingly, a strategy ei will spread or dwindle depending on
whether it does better or worse than average.
This yields a deterministic model for the state of the population. Before we
try to motivate the replicator equation, let us note that 	 ˙xi=0. Furthermore, it
is easy to see that the constant function xi(t) = 0 for all t obviously satisﬁes the
i-th component of equation (6.3). Hence the hyperplanes 	 xi = 1 and xi = 0 are
invariant. From this follows that the state space, i.e. the simplex Δn, is invariant:
if x(0) ∈Δn then x(t) ∈Δn for all t ∈R. The same holds for all sub-simplices of
Δn (which are given by xi = 0 for one or several i), and hence also for the boundary
bdΔn of Δn (i.e. the union of all such sub-simplices), and moreover also for the
interior intΔn of the simplex (the subset satisfying xi > 0 for all i). From now on
we only consider the restriction of (6.3) to the state simplex Δn.
7. Basic properties of the replicator equation
It is easy to see that if we add an arbitrary function b(x) to all payoﬀterms
(Ax)i, the replicator equation (6.3) remains unchanged: what is added to the payoﬀ
is also added to the average payoﬀx · Ax, since 	 xi = 1, and cancels out in the
diﬀerence of the two terms. In particular, this implies that we can add a constant
cj to the j-th column of A (for j = 1, ..., n) without altering the replicator dynamics
on Δn. We shall frequently use this to simplify the analysis.
Another useful property is the quotient rule: if xj > 0, then the time-derivative
of the quotient satisﬁes
(7.1)
( xi
xj
). = ( xi
xj
)[(Ax)i −(Ax)j].
Thus the relative proportions of two strategies change according to their payoﬀ
ranking. More generally, if V = 
 xpi
i
then
(7.2)
˙V = V [p · Ax −(

pi)x · Ax].
The rest points z of the replicator equation are those for which all payoﬀvalues
(Az)i are equal, for all indices i for which zi > 0. The common value of these
payoﬀs is the average payoﬀz · Az. In particular, all vertices ei of the simplex Δn
are rest points. (Obviously, if all players are of the same type, mere copying leads
to no change.) The replicator equation admits a rest point in intΔn if there exists
a solution (in intΔn) of the linear equations
(7.3)
(Ax)1 = ... = (Ax)n.
Similarly, all rest points on each face can be obtained by solving a corresponding
system of linear equations. Generically, each sub-simplex (and Δn itself) contains
one or no rest point in its interior.
                                                                                                                    
                                                                                                               

10
KARL SIGMUND
One can show that if no rest point exists in the interior of Δn, then all orbits
in intΔn converge to the boundary, for t →±∞. In particular, if strategy ei is
strictly dominated, i.e., if there exists a w ∈Δn such that (Ax)i < w · Ax holds
for all x ∈Δn, then xi(t) →0 for t →+∞[21]. In the converse direction, if there
exists an orbit x(t) bounded away from the boundary of Δn (i.e. such that for some
a > 0 the inequality xi(t) > a holds for all t > 0 and all i = 1, ..., n), then there
exists a rest point in intΔn [18]. One just has to note that for i = 1, ..., n,
(7.4)
(log xi). = ˙xi/xi = (Ax(t))i −x(t) · Ax(t).
Integrating for t ∈[0, T], and dividing by T, leads on the left hand side to [log xi(T)−
log xi(0)]/T, which converges to 0 for T →+∞. The corresponding limit on the
right hand side implies that for the accumulation points zi of the time averages
(7.5)
zi(T) = 1
T
 T
0
xi(t)dt,
the relations zi ≥a > 0, 	 zi = 1, and
(7.6)

a1jzj = ... =

anjzj
must hold. Using (7.3), we see that z is a rest point in intΔn.
8. The Lotka-Volterra connection
There is an intimate connection between Lotka-Volterra equations, which are
the staple fare of mathematical population ecology, and the replicator equation
[18]. More precisely, there exists a diﬀeomorphism from Δ−
n = {x ∈Δn : xn > 0}
onto Rn−1
+
mapping the orbits of the replicator equation (6.3) onto the orbits of
the Lotka-Volterra equation
(8.1)
˙yi = yi(ri +
n−1

j=1
dijyj),
where ri = ain −ann and dij = aij −anj. Indeed, let us deﬁne yn ≡1 and consider
the transformation y →x given by
(8.2)
xi =
yi
	n
j=1 yj
i = 1, . . . , n
which maps {y ∈Rn
+ : yn = 1} onto Δ−
n . The inverse x →y is given by
(8.3)
yi = yi
yn
= xi
xn
i = 1, . . . , n .
Now let us consider the replicator equation in n variables given by (6.3). We
shall assume that the last row of the n × n matrix A = (aij) consists of zeros:
since we can add constants to columns, this is no restriction of generality. By the
quotient rule (7.1)
(8.4)
˙yi = ( xi
xn
)[(Ax)i −(Ax)n].
Since (Ax)n = 0, this implies
(8.5)
˙yi = yi(
n

j=1
aijxj) = yi(
n

j=1
aijyj)xn .
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
11
By a change in velocity, we can remove the term xn > 0. Since yn = 1, this yields
(8.6)
˙yi = yi(ain +
n−1

j=1
aijyj)
or (with ri = ain) equation (8.1).
The converse direction from (8.1) to (6.3) is analogous.
Results about Lotka-Volterra equations can therefore be carried over to the
replicator equation and vice versa. Some properties are simpler to prove (or more
natural to formulate) for one equation and some for the other.
For instance, it is easy to prove for the Lotka-Volterra equation that the interior
of Rn
+ contains α- or ω- limit points if and only if it admits an interior rest point.
Indeed, let L : x →y be deﬁned by
(8.7)
yi = ri +
n

j=1
aijxj
i = 1, . . . , n .
If (8.1) admits no interior rest point, the set K = L(int Rn
+) is disjoint from 0.
A well known theorem from convex analysis implies that there exists a hyperplane
H through 0 which is disjoint from the convex set K. Thus there exists a vector
c= (c1, . . . , cn) ̸= 0 orthogonal to H (i.e. c · x = 0 for all x ∈H) such that c · y is
positive for all y ∈K. Setting
(8.8)
V (x) =

xci
i ,
we see that V is deﬁned on int Rn
+. If x(t) is a solution of (8.1) in int Rn
+, then the
time derivative of t →V (x(t)) satisﬁes
(8.9)
˙V = V

ci
˙xi
xi
= V

ciyi = V c · y > 0 .
Thus V is increasing along each orbit. But then no point z ∈int Rn
+ may belong to
an ω-limit: indeed, by Lyapunov’s theorem, the derivative ˙V would have to vanish
there. This contradiction completes the proof.
In particular, if intΔn contains a periodic orbit of the replicator equation (6.3),
it must also contain a rest point.
9. Two-dimensional examples
Let us discuss the replicator equation when there are only two types in the
population. Since the equation remains unchanged if we subtract the diagonal term
in each column, we can assume without restricting generality that the 2 × 2-matrix
A is of the form
(9.1)
 0
a
b
0

.
Since x2 = 1 −x1, it is enough to consider x1, which we denote by x.
Thus
x2 = 1 −x, and
(9.2)
˙x = x[(Ax)1 −x · Ax] = x[(Ax)1 −(x(Ax)1 + (1 −x)(Ax)2)],
and hence
(9.3)
˙x = x(1 −x)[(Ax)1 −(Ax)2],
                                                                                                                    
                                                                                                               

12
KARL SIGMUND
which reduces to
(9.4)
˙x = x(1 −x)[a −(a + b)x].
We note that
(9.5)
a = lim
x→0
˙x
x.
Hence a corresponds to the limit of the per capita growth rate of the missing
strategy e1. Let us omit the trivial case a = b = 0: in this case all points of the
state space Δ2 (i.e. the interval 0 ≤x ≤1) are rest points. The right hand side of
our diﬀerential equation is a product of three factors, the ﬁrst vanishing at 0 and
the second at 1; the third factor has a zero ˆx =
a
a+b in ]0, 1[ if and only if ab > 0.
Thus we obtain three possible cases.
(1) There is no rest point in the interior of the state space. This happens if
and only if ab ≤0. In this case, ˙x has always the same sign in ]0, 1[. If this sign is
positive (i.e. if a ≥0 and b ≤0, at least one inequality being strict), this means
that x(t) →1 for t →+∞, for every initial value x(0) with 0 < x(0) < 1. The
strategy e1 is said to dominate strategy e2. It is always the best reply, for any
value of x ∈]0, 1[. Conversely, if the sign of ˙x is negative, then x(t) →0 and e2
dominates. In each case, the dominating strategy converges towards ﬁxation.
As an example, we consider the Prisoner’s Dilemma Game from (4.5). The
two strategies e1 and e2 are usually interpreted as ’cooperation’ (by providing a
beneﬁt to the co-player) and ’defection’ (by refusing to provide a beneﬁt). The
payoﬀmatrix is transformed, by adding appropriate constants to each column, into
(9.6)

0
−5
5
0

and defection dominates.
(2) There exists a rest point ˆx in ]0, 1[ (i.e. ab > 0), and both a and b are
negative. In this case ˙x < 0 for x ∈]0, ˆx[ and ˙x > 0 for x ∈]ˆx, 1[. This means that
the orbits lead away from ˆx: this rest point is unstable. As in the previous case,
one strategy will be eliminated: but the outcome, in this bistable case, depends on
the initial condition. If x is larger than the threshold ˆx, it will keep growing; if it
is smaller, it will vanish – a positive feedback.
As an example, we can consider the coordination game (4.3). The payoﬀmatrix
is transformed into
(9.7)

0
−2
−2
0

and it is best to play e1 if the frequency of e1-players exceeds 50 percent. Bistability
also occurs if the Prisoner’s Dilemma game given by (4.5) is repeated suﬃciently
often. Let us assume that the number of rounds is a random variable with mean
value m, for instance, and let us consider only two strategies of particular interest.
One, which will be denoted by e1, is the Tit For Tat strategy which consists in
cooperating in the ﬁrst round and from then on imitating what the co-player did in
the previous round. The other strategy, denoted as e2, consists in always defecting.
The expected payoﬀvalues are given by the matrix
(9.8)

10m
−5
15
0

                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
13
which can be transformed into
(9.9)

0
−5
15 −10m
0

.
If m > 3/2, it is best to do what the co-player does. Loosely speaking, one should
go with the trend. The outcome, in such a population, would be the establishment
of a single norm of behavior (either always defect, or play Tit For Tat). Which
norm emerges depends on the initial condition.
(3) There exists a rest point ˆx in ]0, 1[ (i.e. ab > 0), and both a and b are
positive. In this case ˙x > 0 for x ∈]0, ˆx[ and ˙x < 0 for x ∈]ˆx, 1[. This negative
feedback means that x(t) converges towards ˆx, for t →+∞: the rest point ˆx
is a stable attractor. No strategy eliminates the other: rather, their frequencies
converge towards a stable coexistence.
This situation can be found in the Hawk-Dove-game, for example. The payoﬀ
matrix (5.4)is transformed into
(9.10)

0
G
2
C−G
2
0

and the rest point corresponds to x = G/C. The higher the cost of injury, i.e.,
C, the lower the frequency of escalation. Another well-known example is the so-
called snowdrift game. Suppose that two players are promised 40 dollars each if
they contribute 30 dollars to the experimenter. They have to decide independently
whether to come up with such a fee or not. If both contribute, they can split the
cost equally, and pay only 15 dollars. If e1 is the decision to contribute, and e2 not
to contribute, the payoﬀmatrix is
(9.11)

25
10
40
0

which can be normalized to
(9.12)

0
10
15
0

.
In this case, it is best to do the opposite of what the co-player is doing, i.e., to swim
against the stream.
10. Rock-Scissors-Paper
Turning now to n = 3, we meet a particularly interesting example if the three
strategies dominate each other in a cyclic fashion, i.e., if e1 dominates e2, in the
absence of e3, and similarly e2 dominates e3, and e3, in turn, dominates e1. Such
a cycle occurs in the game of Rock-Scissors-Paper shown in (1.2). It is a zero-sum
game: one player receives what the other player loses. Hence the average payoﬀin
the population, x · Ax, is zero. There exist only four rest points, one in the center,
m = (1/3, 1/3, 1/3) ∈intΔ3, and the other three at the vertices ei.
Let us consider the function V := x1x2x3, which is positive in the interior of
Δ3 (with maximum at m) and vanishes on the boundary. Using (7.2), we see that
t →V (x(t)) satisﬁes
(10.1)
˙V = V (x2 −x3 + x3 −x1 + x1 −x2) = 0.
Hence V is a constant of motion: all orbits t →x(t) of the replicator equation
remain on constant level sets of V . This implies that all orbits in intΔn are closed
                                                                                                                    
                                                                                                               

14
KARL SIGMUND
orbits surrounding m.
The invariant set consisting of the three vertices ei and
the orbits connecting them along the edges of Δ3 is said to form a heteroclinic set.
Any two points on it can be connected by ’shadowing the dynamics’. This means to
travel along the orbits of that set and, at appropriate times which can be arbitrarily
rare, to make an arbitrarily small step. In the present case, it means for instance
to ﬂow along an edge from e2 towards e1, and then stepping onto the edge leading
away from e1 and toward e3. This step can be arbitrarily small: travellers just
have to wait until they are suﬃciently close to the ’junction’ e1.
Now let us consider the generalized Rock-Scissors-Paper game with matrix
(10.2)
⎛
⎝
0
a
−b
−b
0
a
a
−b
0
⎞
⎠.
with a, b > 0, which is no longer zero-sum, if a ̸= b. It has the same structure of
cyclic dominance and the same rest points. The point m is a Nash equilibrium and
the boundary of Δ3 is a heteroclinic set, as before. But now,
(10.3)
x · Ax = (a −b)(x1x2 + x2x3 + x3x1),
and hence
(10.4)
˙V = V (a −b)[1 −3(x1x2 + x2x3 + x3x1)],
which implies
(10.5)
˙V = V (a −b)
2
[(x1 −x2)2 + (x2 −x3)2 + (x3 −x1)2].
This expression vanishes on the boundary of Δ3 and at m. It has the sign of a −b
everywhere else on Δ3. If a > b, this means that all orbits cross the constant-level
sets of V in the uphill direction, and hence converge to m. For a > b, the function
V (x) is a strict Lyapunov function: indeed ˙V (x) ≥0 for all x, and equality holds
only when x is a rest point. This implies that ultimately, all three types will be
present in the population in equal frequencies: the rest point m is asymptotically
stable. But for a < b, the orbits ﬂow downhill towards the boundary of Δ3. The
Nash equilibrium m corresponds to an unstable rest point, and the heteroclinic
cycle on the boundary attracts all other orbits.
Let us follow the state x(t) of the population, for a < b. If the state is very
close to a vertex, for instance e1, it is close to a rest point and hence almost at rest.
For a long time, the state does not seem to change. Then, it picks up speed and
moves towards the vicinity of the vertex e3, where it slows down and remains for a
much longer time, etc. This looks like a recurrent form of ’punctuated equilibrium’:
long periods of quasi-rest followed by abrupt upheavals.
The same holds if all the a’s and b’s, in (10.2), are distinct positive numbers.
There exists a unique rest point m in the interior of Δ3 which, depending on the
sign of det A (which is the same as that of m · Am) is either globally stable, i.e.,
attracts all orbits in intΔ3, or is surrounded by periodic orbits, or is repelling. In
the latter case, all orbits converge to the heteroclinic cycle formed by the boundary
of Δn.
Interestingly, several biological examples for Rock-Scissors-Paper cycles have
been found. We only mention two examples: (A) Among the lizard species Uta
stansburiana, three inheritable types of male mating behavior are e1: attach your-
self to a female and guard her closely, e2: attach yourself to several females and
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
15
guard them (but inevitably, less closely); and e3: attach yourself to no female, but
roam around and attempt sneaky matings whenever you encounter an unguarded
female [39]. (B) Among the bacteria E. coli, three strains occur in the lab through
recurrent mutations, namely e1: the usual, so-called wild type; e2: a mutant pro-
ducing colicin, a toxic substance, together with a protein conferring auto-immunity;
and e3: a mutant producing the immunity-conferring protein, but not the poison
[23]. In case (A), selection leads to the stable coexistence of all three types, and in
case (B) to the survival of one type only.
There exist about 100 distinct phase portraits of the replicator equation for
n = 3, up to re-labeling the vertices [1].
0f these, about a dozen are generic.
Interestingly, none admits a limit cycle [19]. For n > 3, limit cycles and chaotic
attractors can occur. A classiﬁcation seems presently out of reach.
11. Nash equilibria and saturated rest points
Let us consider a symmetric n×n-game with payoﬀmatrix A, and z a symmetric
Nash equilibrium. With x = ei, condition (5.2) implies
(11.1)
(Az)i ≤z · Az
for i = 1, ..., n. Equality must hold for all i such that zi > 0. Hence z is a rest
point of the replicator dynamics. Moreover, it is a saturated rest point: this means
by deﬁnition that if zi = 0, then
(11.2)
(Az)i −z · Az ≤0.
Conversely, every saturated rest point is a Nash equilibrium. The two concepts
are equivalent.
Every rest point in intΔn is trivially saturated; but on the boundary, there
may be rest points which are not saturated, as we shall presently see.
In that
case, there exist strategies which are not present in the population z, but which
would do better than average (and better, in fact, than every type that is present).
Rest points and Nash equilibria have in common that there exists a c such that
(Az)i = c whenever zi > 0; the additional requirement, for a Nash equilibrium, is
that (Az)i ≤c whenever zi = 0.
Hence every symmetric Nash equilibrium is a rest point, but the converse does
not hold. Let us discuss this for the examples from the previous section. It is clear
that the rest points in the interior of the simplex are Nash equilibria. In case n = 2
and dominance, the strategy that is dominant is a Nash equilibrium, and the other
is not. In case n = 2 with bi-stability, both pure strategies are Nash equilibria.
Generically (and in contrast to the example (9.7)), one of the pure strategies fares
better than the other in a population where both are equally frequent. This is the
so-called risk-dominant equilibrium. It has the larger basin of attraction. In the
case n = 2 leading to stable co-existence, none of the pure strategies is a Nash
equilibrium. If you play a bistable game, you should choose the same strategy as
your co-player; but in the case of stable coexistence, you should choose the opposite
strategy. In both cases, however, the two of you might have diﬀerent ideas about
who plays what.
In the case n = 3 with the Rock-Scissors-Paper structure, the interior rest point
m is the unique Nash equilibrium. Each of the vertex rest points can be invaded.
A handful of results about Nash equilibria and rest points of the replicator
dynamics are known as folk theorem of evolutionary game theory [5]. For instance,
                                                                                                                    
                                                                                                               

16
KARL SIGMUND
any limit, for t →+∞, of a solution x(t) starting in intΔn is a Nash equilibrium;
and any stable rest point is a Nash equilibrium. (A rest point z is said to be stable
if for any neighborhood U of z there exists a neighborhood V of z such that if
x(0) ∈V then x(t) ∈U for all t ≥0). Both results are obvious consequences of the
fact that if z is not Nash, then there exists an i and an ϵ such that (Ax)i−x·Ax > ϵ
for all x close to z. In the other direction, if z is a strict Nash equilibrium, then z is
an asymptotically stable rest point (i.e. not only stable, but in addition attracting
in the sense that for some neighborhood U of z, x(0) ∈U implies x(t) →z for
t →+∞). The converse statements are generally not valid.
In order to prove the existence of a symmetric Nash equilibrium for the sym-
metric game with n × n matrix A, i.e. the existence of a saturated rest point for
the corresponding replicator equation (6.3), we perturb that equation by adding a
small constant term ϵ > 0 to each component of the right hand side. Of course,
the relation 	 ˙xi = 0 will no longer hold. We compensate this by subtracting the
term nϵ from each growth rate (Ax)i −x · Ax. Thus we consider
(11.3)
˙xi = xi[(Ax)i −x · Ax −nϵ] + ϵ.
Clearly, 	 ˙xi = 0 is satisﬁed again. On the other hand, if xi = 0, then ˙xi = ϵ > 0.
This inﬂux term changes the vector ﬁeld of the replicator equation: at the boundary
of Δn (which is invariant for the unperturbed replicator equation), the vector ﬁeld
of the perturbed equation points towards the interior.
Brouwer’s ﬁxed point theorem implies that (11.3) admits at least one rest point
in intΔn, which we denote by zϵ. It satisﬁes
(11.4)
(Azϵ)i −zϵ · Azϵ = ϵ(n −
1
(zϵ)i
).
Let ϵ tend to 0, and let z be an accumulation point of the zϵ in Δn. The limit on
the left hand side exists, and is given by (Az)i −z · Az. Hence the right hand side
also has a limit for ϵ →0. This limit is 0 if zi > 0, and it is ≤0 if zi = 0. This
implies that z is a saturated rest point of the (unperturbed) replicator equation
(6.3), and hence corresponds to a Nash equilibrium (see also [15, 38]).
12. Mixed strategies and evolutionary stability
Let us now consider the case when individuals can also use mixed strategies,
for instance escalate a conﬂict with a certain probability. Thus let us assume that
there exist N types, each using a (pure or mixed) strategy p(i) ∈Δn (we need not
assume n = N). The average payoﬀfor a p(i)-player against a p(j)-player is given
by uij = p(i) · Ap(j), and if x ∈ΔN describes the frequencies of the types in the
population, then the average strategy within the population is p(x) = 	 xip(i).
The induced replicator dynamics on ΔN, namely ˙xi = xi[(Ux)i −x · Ux] can be
written as
(12.1)
˙xi = xi[(p(i) −p(x)) · Ap(x)].
This dynamics on ΔN induces a dynamics t →p(x(t)) of the average strategy on
Δn.
Let us now turn to the concept of an evolutionarily stable strategy, or ESS. If
all members of the population use such a strategy ˆp ∈Δn, then no mutant minority
using another strategy p can invade (cf. [29, 25]). Thus a strategy ˆp ∈Δn is said
to be evolutionarily stable if for every p ∈Δn with p ̸= ˆp, the induced replicator
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
17
equation describing the dynamics of the population consisting of these two types
only (the ’resident’ using ˆp and the ’invader’ using p) leads to the elimination of
the invader. By (9.4) this equation reads (if x is the frequency of the invader):
(12.2)
˙x = x(1 −x)[x(p · Ap −ˆp · Ap) −(1 −x)(ˆp · Aˆp −p · Aˆp)]
and hence the rest point x = 0 is asymptotically stable iﬀthe following conditions
are satisﬁed:
(a) (equilibrium condition)
(12.3)
p · Aˆp ≤ˆp · Aˆp
holds for all p ∈Δn;
(b) (stability condition)
(12.4)
if
p · Aˆp = ˆp · Aˆp
then
p · Ap < ˆp · Ap.
The ﬁrst condition means that ˆp is a Nash equilibrium: no invader does better than
the resident, against the resident. The second condition states that if the invader
does as well as the resident against the resident, then it does less well than the
resident against the invader. Based on (7.2), it can be shown that the strategy ˆp
is an ESS iﬀ
i xˆpi
i
is a strict local Lyapunov function for the replicator equation,
or equivalently iﬀ
(12.5)
ˆp · Ap > p · Ap
for all p ̸= ˆp in some neighborhood of ˆp [16, 18]. If ˆp ∈intΔn, then Δn itself is
such a neighborhood.
In particular, an ESS corresponds to an asymptotically stable rest point of
(6.3). The converse does not hold in general [46]. But the strategy ˆp ∈Δn is an
ESS iﬀit is strongly stable in the following sense: whenever it belongs to the convex
hull of p(1), ..., p(N) ∈Δn, the strategy p(x(t)) converges to ˆp, under (12.1), for
all x ∈ΔN for which p(x) is suﬃciently close to ˆp [4].
The relation between evolutionary and dynamic stability is particularly simple
for the class of partnership games.
These are deﬁned by payoﬀmatrices A =
AT . In this case the interests of both players coincide. For spartnership games,
ˆp is an ESS iﬀit is asymptotically stable for (6.3). This in turn holds iﬀit is
a strict local maximum of the average payoﬀx · Ax [18]. Replicator equations
for partnership games occur prominently in population genetics.
They describe
the eﬀect of selection on the frequencies xi of alleles i on a single genetic locus,
for i ∈{1, ..., n}. In this case, the aij correspond to the survival probabilities of
individuals with genotype (i, j) (i.e., having inherited the alleles i and j from their
parents).
13. Generalizations of the replicator dynamics
We have assumed so far that the average payoﬀfor a player using strategy
i is given by a linear function (Ax)i of the state of the population. This makes
sense if the interactions are pairwise, with co-players chosen randomly within the
population.
But many interesting examples lead to non-linear payoﬀfunctions
ai(x), for instance if the interactions occur in groups with more than two members.
This is the case, for instance, in the sex-ratio game, where the success of a strategy
                                                                                                                    
                                                                                                               

18
KARL SIGMUND
(i.e., an individual sex ratio) depends on the aggregate sex ratio in the population.
Nonlinear payoﬀfunctions ai(x) lead to the replicator equation
(13.1)
˙xi = xi(ai(x) −¯a)
on Δn, where ¯a = 	
i xiai(x) is again the average payoﬀwithin the population.
Many of the previous results can be extended in a straightforward way. For instance,
the dynamics is unchanged under addition of a function b to all payoﬀfunctions
ai. Equation (13.1) always admits a saturated rest point, and a straight extension
of the folk theorem is still valid. The notion of an ESS has to be replaced by a
localized version.
Initially, the replicator dynamics was intended to model the transmission of be-
havioral programs through inheritance. The simplest inheritance mechanisms lead
in a straightforward way to (6.3), but more complex cases of Mendelian inheritance
through one or several genetic loci yield more complex dynamics [13, 7, 45, 17].
The replicator equation (6.3) can also be used to model imitation processes [14, 2,
36, 34]. A rather general approach to modeling imitation processes leads to
(13.2)
˙xi = xi[f(ai(x)) −

xjf(aj(x))]
for some strictly increasing function f of the payoﬀ, and even more generally to the
imitation dynamics given by
(13.3)
˙xi = xigi(x)
where the functions gi satisfy 	 xigi(x) = 0 on Δn. The simplex Δn and its faces
are invariant. Such an equation is said to be payoﬀmonotonic if
(13.4)
gi(x) > gj(x) ⇔ai(x) > aj(x),
where the ai correspond to the payoﬀfor strategy i. For payoﬀmonotonic equations
(13.3), the folk theorem holds again [31, 8]: Nash equilibria are rest points, strict
Nash equilibria are asymptotically stable, and rest points that are stable or ω-limits
of interior orbits are Nash equilibria.
The dynamics (13.3) can be reduced (through a change in velocity) to a repli-
cator equation (13.1) if it has the following property:
(13.5)
y · g(x) > z · g(x) ⇐⇒y · a(x) > z · a(x)
for all x, y, z ∈Δn.
14. Best reply dynamics
It is worth emphasizing that imitation (like selection, in genetics) does not pro-
duce anything new. If a strategy ei is absent from the population, it will remain so
(i.e. if xi(t) = 0 holds for some time t, it holds for all t). An equation such as (13.1)
or more generally (13.3) does not allow the introduction of new strategies. There
exist game dynamics which are more innovative. For instance, clever players could
adopt the strategy which oﬀers the highest payoﬀ, even if no one in the population
is currently using it. We describe this dynamics presently. Other innovative dy-
namics arise if we assume a steady rate of switching randomly to other strategies.
This can be interpreted as an ’exploration rate’, and corresponds to a mutation
term in genetics [35].
The best-reply dynamics assumes more sophistication than mere learning by
copying others.
Let us assume that in a large population, a small fraction of
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
19
the players revise their strategy, choosing best replies BR(x) to the current mean
population strategy x. This approach, which postulates that players are intelligent
enough to know the current population state and to respond optimally, yields the
best reply dynamics
(14.1)
˙x ∈BR(x) −x.
Since best replies are in general not unique, this is a diﬀerential inclusion rather than
a diﬀerential equation [26]. For continuous payoﬀfunctions ai(x), the set of best
replies BR(x) is a non-empty convex, compact subset of Δn which is upper semi-
continuous in x. Hence solutions exist, they are Lipschitz functions x(t) satisfying
(14.1) for almost all t ≥0. If BR(x) is a uniquely deﬁned (and hence pure) strategy
b, the solution of (14.1) is given by
(14.2)
x(t) = (1 −e−t)b + e−tx
for small t ≥0, which describes a linear orbit pointing straight towards the best
response. This can lead to a state where b is no longer the unique best reply. But
for each x there always exists a b ∈BR(x) which, among all best replies to x, is
a best reply against itself (i.e. a Nash equilibrium of the game restricted to the
simplex BR(x)) [20]. In this case b ∈BR((1 −ϵ)x + ϵb) holds for small ϵ ≥0, if
the game is linear. An iteration of this construction yields at least one piecewise
linear solution of (14.1) starting at x and deﬁned for all t > 0. One can show that
for generic linear games, essentially all solutions can be constructed in this way. For
the resulting (multi-valued) semi-dynamical system, the simplex Δn is only forward
invariant and bdΔn need no longer be invariant: the frequency of strategies which
are initially missing can grow, in contrast to the imitation dynamics. In this sense,
the best reply dynamics is an innovative dynamics.
For n = 2, the phase portraits of (14.1) diﬀer only in details from that of the
replicator dynamics. If e1 is dominated by e2, there are only two orbits: the rest
point e2, and the semi-orbit through e1 which converges to e2. In the bistable situa-
tion with interior Nash equilibrium p, there are inﬁnitely many solutions starting at
p besides the constant one, staying there for some time and then converging mono-
tonically to either e1 or e2. In the case of stable coexistence with interior Nash
equilibrium p, the solution starting at some point x between p and e1 converges
toward e2 until it hits p, in ﬁnite time, and then remains there forever.
For n = 3, the diﬀerences to the replicator dynamics become more pronounced.
In particular, for the generalized Rock-Scissors-Paper game given by (10.2), all
orbits converge to the Nash equilibrium p whenever det A > 0 (just as with the
replicator dynamics); but for det A < 0, all orbits (except possibly p) converge to
a limit cycle, the so-called Shapley triangle spanned by the three points Ai (given
by the intersections of the lines (Ax)2 = (Ax)3 etc. in Δ3). In fact, the piecewise
linear function V (x) :=|maxi(Ax)i | is a Lyapunov function for (14.1). In this case,
the orbits of the replicator equation (6.3) converge to the boundary of Δn; but
interestingly, the time averages
(14.3)
z(T) := 1
T
 T
0
x(t)dt
have the Shapley triangle as the set of accumulation points, for T →+∞. Similar
parallels between the best reply dynamics and the behavior of time-averages of the
replicator equation are quite frequent [9, 10].
                                                                                                                    
                                                                                                               

20
KARL SIGMUND
15. A brief look at asymmetric games
So far, we have considered evolutionary games in the symmetric case only. Thus
players are indistinguishable (except by their strategies), and the game is described
by a single n × n payoﬀmatrix A. In the ﬁrst section, however, we had started out
with two players I and II having strategies ei and fj respectively (with 1 ≤i ≤n
and 1 ≤j ≤m), and a game was deﬁned by two n × m payoﬀmatrices A and B
There is an obvious way to turn the non-symmetric game (A, B) into a symmetric
game: simply by letting a coin toss decide who of the two players will be labeled
player I. A strategy for this symmetrized game must therefore specify what to do in
role I, and what in role II, i.e., such a strategy is given by a pair (ei, fj). A mixed
strategy is given by an element z = (zij) ∈Δnm, where zij denotes the probability
to play ei when in role I and fj when in role II. To the probability distribution z
correspond its marginals: xi = 	
j zij and yj = 	
i zij. The vectors x = (xi) and
y = (yj) belong to Δn and Δm, respectively.
The expected payoﬀfor a player using (ei, fj) against a player using (ek, fl),
with i, k ∈{1, ..., n} and j, l ∈{1, ..., m}, is given by
(15.1)
cij,kl = 1
2ail + 1
2bkj.
Since every symmetric game has a symmetric Nash equilibrium, it follows immedi-
ately that every game (A, B) has a Nash equilibrium pair.
Let us now turn to population games. Players meet randomly and engage in a
game (A, B), with chance deciding who is in role I and who in role II. For simplicity,
we assume that there are only two strategies for each role. The payoﬀmatrix is
(15.2)

(A, a)
(B, b)
(C, c)
(D, d)

.
The strategies for the resulting symmetric game will be denoted by G1 = e1f1,
G2 = e2f1, G3 = e2f2 and G4 = e1f2. The payoﬀfor a player using Gi against a
player using Gj is given, up to the factor 1/2 which we shall henceforth omit, by
the (i, j)-entry of the matrix
(15.3)
M =
⎛
⎜
⎜
⎝
A + a
A + c
B + c
B + a
C + a
C + c
D + c
D + a
C + b
C + d
D + d
D + b
A + b
A + d
B + d
B + b
⎞
⎟
⎟
⎠.
This corresponds to (15.1). For instance, a G1-player meeting a G3-opponent is
with probability 1/2 in role I, plays e1 against the co-player’s f2, and obtains B.
With probability 1/2, the G1-player is in role II, plays f1 against the co-players’
e2, and obtains c.
The replicator dynamics
(15.4)
˙xi = xi[(Mx)i −x · Mx]
describes the evolution of the state x = (x1, x2, x3, x4) ∈Δ4. Since the dynamics
is unaﬀected if each mij is replaced by mij −m1j (for i, j ∈{1, 2, 3, 4}), we can use
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
21
the matrix
(15.5)
⎛
⎜
⎜
⎝
0
0
0
0
R
R
S
S
R + r
R + s
S + s
S + r
r
s
s
r
⎞
⎟
⎟
⎠
with R := C −A, r := b −a, S := D −B and s := d −c. We shall denote this
matrix again by M. It has the property that
(15.6)
m1j + m3j = m2j + m4j
for j = 1, 2, 3, 4. Hence
(15.7)
(Mx)1 + (Mx)3 = (Mx)2 + (Mx)4
holds for all x.
From this and (7.2) follows that the function V = x1x3/x2x4
satisﬁes
(15.8)
˙V = V [(Mx)1 + (Mx)3 −(Mx)2 −(Mx)4] = 0
in the interior of Δ4, and hence that V is an invariant of motion for the replicator
dynamics: its value remains unchanged along every orbit.
Therefore, the interior of the state simplex Δ4 is foliated by the surfaces
(15.9)
WK := {x ∈Δ4 : x1x3 = Kx2x4},
with 0 < K < ∞. These are saddle-like surfaces which are spanned by the quad-
rangle of edges G1G2, G2G3, G3G4 and G4G1 joining the vertices of the simplex
Δ4.
The orientation of the ﬂow on the edges can easily be obtained from the previous
matrix. For instance, if R = 0, then the edge G1G2 consists of rest points. If
R > 0, the ﬂow along the edge points from G1 towards G2 (which means that
in the absence of the strategies G3 and G4, the strategy G2 dominates G1), and
conversely, if R < 0, the ﬂow points from G2 to G1.
Generically, the parameters R, S, r and s are non-zero. This corresponds to 16
orientations of the quadrangle G1G2G3G4, which by symmetry can be reduced to
4. Since (Mx)1 trivially vanishes, the rest points in the interior of the simplex Δ4
must satisfy (Mx)i = 0 for i = 2, 3, 4. This implies for S ̸= R
(15.10)
x1 + x2 =
S
S −R,
and for s ̸= r
(15.11)
x1 + x4 =
s
s −r .
Such solutions lie in the simplex if and only if RS < 0 and rs < 0. If this is the
case, one obtains a line of rest points which intersects each WK in exactly one point.
These points can be written as
(15.12)
xi = mi + ξ
for i = 1, 3 and
(15.13)
xi = mi −ξ
for i = 2, 4, with ξ as parameter and
(15.14)
m =
1
(S −R)(s −r)(Ss, −Sr, Rr, −Rs) ∈W1.
                                                                                                                    
                                                                                                               

22
KARL SIGMUND
Of particular interest is the so-called Wright-manifold W1, where the strategies,
in the two roles, are independent of each other. (On W1, the probability that a
randomly chosen individual uses strategy e1f1 is the product of the probabilities
x := x1 + x4 and y := x1 + x2 of choosing e1 when in role I, resp. f1 when in role
II. Indeed, x1 = (x1 + x4)(x1 + x2)). It then follows that
(15.15)
˙x = x(1 −x)(s −(s −r)y),
and
(15.16)
˙y = y(1 −y)(S −(S −R)x).
If rR > 0, each interior rest point is a saddle point within the corresponding
manifold WK, and the system is bistable: depending of the inital condition, orbits
converge either to G1 or to G3, if r < 0, and either to G2 or to G4, if r > 0. If
rR < 0, each rest point has (in addition of the eigenvalue 1) a pair of complex
conjugate eigenvalues.
Within the corresponding manifold WK, the eigenvalues
spiral around this rest point. Depending on whether K is larger or smaller than 1,
they either converge to the rest point (which must be a spiral sink), or else toward
the heteroclinic cycle deﬁned by the quadrangle of the edges forming the boundary
of WK. For K = 1, the orbits are periodic.
16. Applications
In this lecture course, the authors aim to stress the variety of plausible dynam-
ics which describe adaptive mechanisms underlying game theory. The replicator
equation and the best reply dynamics describe just two out of many dynamics. For
applications of evolutionary game theory, it does not suﬃce to specify the strate-
gies and the payoﬀvalues.
One also has to be explicit about the transmission
mechanisms describing how strategies spread within a population.
We end this introductory part with some signposts to the literature using evo-
lutionary games to model speciﬁc social interactions. The ﬁrst applications, and
indeed the motivation, of evolutionary game theory are found in evolutionary biol-
ogy, where by now thousands of papers have proved the fruitfulness of this approach,
see [6]. In fact, questions of sex-ratio, and more generally of sex-allocation, even
pre-date any explicit formulation in terms of evolutionary game theory. It was R.F.
Fisher, a pioneer in both population genetics and mathematical statistics, who used
frequency-dependent selection to explain the prevalence of a 1:1 sex ratio, and W.D.
Hamilton who extended this type of thinking to make sense of other, odd sex ratios
[12]. We have seen how Price and Maynard Smith coined their concept of evolu-
tionary stability to explain the prevalence of ritual ﬁghting in intraspeciﬁc animal
contests. The subtleties of such contests are still a favorite topic among the students
of animal behavior. More muted, but certainly not less widespread conﬂicts arise
on the issues of mate choice, parental investment, and parent-oﬀspring conﬂicts.
Social foraging is another ﬁeld where the success of a given behavior (scrounging,
for instance) depends on its prevalence; so are dispersal and habitat selection. Com-
munication (alarm calls, threat displays, sexual advertisement, gossip), with all its
opportunities for deceit, is replete with game theoretical problems concerning bluﬀ
and honest signaling. Predators and their prey, or parasites and their hosts, oﬀer
examples of games between two populations, with the success of a trait depending
on the state of the other population. Some strategic interactions are surprisingly
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
23
sophisticated, considering the lowly level of the players: for instance, bacteria can
engage in quorum sensing as cue for conditional behavior.
Quite a few biological games turned out to have the same structure as games
that had been studied by economists, usually under another name [3]: the biolo-
gists’ ’Hawk-Dove’ game, for example, has the same structure as the economists’
’Chicken’-game. Evolutionary game theory has found a large number of applica-
tions in economic interactions [44, 22, 41, 8, 11].
One zone of convergence for studies of animal behavior and human societies is
that of cooperation. Indeed, the theory of evolution and economic theory have each
their own paradigm of selﬁshness, encapsulated in the slogans of the ’selﬁsh gene’
and the ’homo economicus’. Both paradigms conﬂict with wide-spread evidence
of social, ’other-regarding’ behavior. In ant and bee societies, the relatedness of
individuals is so close that their genetic interests overlap and their communities
can be viewed as ’super-organisms’. But in human societies, close cooperation can
also occur between individuals who are unrelated. In many cases, such cooperation
is based on reciprocation. Positive and negative incentives, and in particular the
threat of sanctions oﬀer additional reasons for the prevalence of cooperation [38].
This may lead to two or more stable equilibria, corresponding to behavioral norms.
If everyone adopts a given norm, no player has an incentive to deviate. But which
of these norms eventually emerges depends, among other things, on the history of
the population.
Animal behavior and experimental economics fuse in this area. Experimental
economics, has greatly ﬂourished in the last few years.
It often reduces to the
investigation of very simple games which can be analyzed by means of evolutionary
dynamics. These and other games display the limitations of ’rational’ behavior in
humans, and have assisted in the emergence of new ﬁelds, such as behavioral game
theory and neuro-economics.
References
1. I.M. Bomze, Non-cooperative two-person games in biology: a classiﬁcation, Int. J. Game
Theory 15 (1986), 31-57.
2. T. B¨orgers and R. Sarin, Learning through reinforcement and replicator dynamics, J. Eco-
nomic Theory 77 (1997), 1-14.
3. A.M. Colman, Game Theory and its Applications in the Social and Biological Sciences, Oxford:
Butterworth-Heinemann (1995).
4. R. Cressman, The Stability concept of Evolutionary Game Theory, Springer, Berlin (1992).
5. R. Cressman, Evolutionary Dynamics and Extensive Form Games, MIT Press (2003)
6. L.A. Dugatkin and H.K. Reeve (eds.), Game Theory and Animal Behavior, Oxford UP (1998).
7. I. Eshel, Evolutionarily stable strategies and viability selection in Mendelian populations,
Theor. Population Biology 22 (1982), 204-217.
8. D. Friedman, Evolutionary games in economics, Econometrica 59 (1991), 637-66.
9. A. Gaunersdorfer, Time averages for heteroclinic attractors, SIAM J. Appl. Math 52 (1992),
1476-89.
10. A. Gaunersdorfer and J. Hofbauer, Fictitious play, Shapley polygons and the replicator equa-
tion, Games and Economic Behavior 11 (1995), 279-303.
11. H. Gintis, Game Theory Evolving, Princeton UP (2000).
12. W.D. Hamilton, Extraordinary sex ratios, Science 156, 477-488.
13. P. Hammerstein and R. Selten, Game theory and evolutionary biology, in R.J. Aumann, S.
Hart (eds.), Handbook of Game Theory II, Amsterdam, North-Holland (1994), 931-993.
14. D. Helbing, Interrelations between stochastic equations for systems with pair interactions,
Physica A 181 (1992), 29-52.
                                                                                                                    
                                                                                                               

24
KARL SIGMUND
15. J. Hofbauer, From Nash and Brown to Maynard Smith: equilibria, dynamics and ESS, Selec-
tion 1 (2000), 81-88.
16. J. Hofbauer, P. Schuster, and K. Sigmund, A note on evolutionarily stable strategies and game
dynamics, J. Theor. Biology 81 (1979), 609-612.
17. J. Hofbauer, P. Schuster, and K. Sigmund, Game dynamics for Mendelian populations, Biol.
Cybernetics 43 (1982), 51-57.
18. J. Hofbauer and K. Sigmund, The Theory of Evolution and Dynamical Systems, Cambridge
UP (1988).
19. J. Hofbauer and K. Sigmund, Evolutionary Games and Population Dynamics, Cambridge UP
(1998).
20. J. Hofbauer and K. Sigmund, Evolutionary game dynamics, Bulletin of the American Math-
ematical Society 40, (2003) 479-519.
21. J. Hofbauer and J. W. Weibull, Evolutionary selection against dominated strategies, J. Eco-
nomic Theory 71 (1996), 558-573.
22. M. Kandori: Evolutionary Game Theory in Economics, in D. M. Kreps and K. F. Wallis
(eds.), Advances in Economics and Econometrics: Theory and Applications, I, Cambridge
UP (1997).
23. B. Kerr, M.A. Riley, M.W. Feldman, and B.J.M. Bohannan, Local dispersal promotes biodi-
versity in a real-life game of rock-paper-scissors, Nature 418 (2002), 171-174.
24. R. Leonard, Von Neumann, Morgenstern and the Creation of Game Theory: from Chess to
Social Science, 1900-1960, Cambridge, Cambridge UP (2010).
25. S. Lessard, Evolutionary stability: one concept, several meanings, Theor. Population Biology
37 (1990), 159-70.
26. A. Matsui, Best response dynamics and socially stable strategies, J. Econ. Theory 57 (1992),
343-362.
27. J. Maynard Smith and G. Price, The logic of animal conﬂict, Nature 246 (1973), 15-18.
28. J. Maynard Smith, Will a sexual population converge to an ESS?, American Naturalist 177
(1981), 1015-1018.
29. J. Maynard Smith, Evolution and the Theory of Games, Cambridge UP (1982)
30. R. Myerson, Game Theory: Analysis of Conﬂict, Cambridge, Mass., Harvard University Press
(1997)
31. J. Nachbar, ”Evolutionary” selection dynamics in games: convergence and limit properties,
Int. J. Game Theory 19 (1990), 59-89.
32. S. Nasar, A Beautiful Mind: A Biography of John Forbes Nash, Jr., Winner of the Nobel
Prize in Economics, New York, Simon and Schuster (1994).
33. J. Nash, Non-cooperative games, Ann. Math. 54 (1951), 287-295.
34. M.A. Nowak, Evolutionary Dynamics, Cambridge MA, Harvard UP (2006).
35. W.H. Sandholm, Population Games and Evolutionary Dynamics, Cambridge, MA, MIT Press
(2010).
36. K.H. Schlag, Why imitate, and if so, how? A boundedly rational approach to multi-armed
bandits, J. Econ. Theory 78 (1997), 130-156.
37. P. Schuster and K. Sigmund, Replicator Dynamics, J. Theor. Biology 100 (1983), 533-538.
38. K. Sigmund, The Calculus of Selﬁshness, Princeton, Princeton UP (2010).
39. B. Sinervo and C.M. Lively, The rock-paper-scissors game and the evolution of alternative
male strategies, Nature 380 (1996), 240-243.
40. P.D. Taylor and L. Jonker, Evolutionarily stable strategies and game dynamics, Math. Bio-
sciences 40 (1978), 145-156.
41. F. Vega-Redondo, Evolution, Games, and Economic Theory, Oxford UP (1996).
42. J. von Neumann, Zur Theorie der Gesellschaftsspiele, Mathematische Annalen 100 (1928),
295-320.
43. J. von Neumann and O. Morgenstern Theory of Games and Economic Behavior, Princeton
UP (1944).
44. J. Weibull, Evolutionary Game Dynamics, MIT Press, Cambridge, Mass. (1995).
45. F. Weissing: Evolutionary stability and dynamic stability in a class of evolutionary normal
form games, in R. Selten (ed.) Game Equilibrium Models I, Berlin, Springer (1991), 29-97.
46. E.C. Zeeman, Population dynamics from game theory, in Global Theory of Dynamical Sys-
tems, Springer Lecture Notes in Mathematics 819 (1980).
                                                                                                                    
                                                                                                               

INTRODUCTION TO EVOLUTIONARY GAME THEORY
25
Faculty of Mathematics, University of Vienna, A-1090 Vienna, Austria
and International Institute for Applied Systems Analysis, A-2361 Laxenburg, Austria
E-mail address: karl.sigmund@univie.ac.at
                                                                                                                    
                                                                                                               

This page intentionally left blank 
                                                                                                                    
                                                                                                               

Proceedings of Symposia in Applied Mathematics
Volume 69, 2011
Beyond the Symmetric Normal Form: Extensive Form
Games, Asymmetric Games and Games with Continuous
Strategy Spaces
Ross Cressman
Abstract. Evolutionary games are typically introduced through developing
theory and applications for symmetric normal form games. This chapter gener-
alizes evolutionary game theory to three other classes of games that are equally
important; namely, extensive form games, asymmetric games and games with
continuous strategy spaces. Static solution concepts such as Nash equilibrium
(NE) and evolutionarily stable strategy (ESS) are extended to these games
and connections made with the stability of deterministic evolutionary dynam-
ics (speciﬁcally, the replicator equation). The similarities as well as the diﬀer-
ences with the corresponding concepts from symmetric normal form games are
highlighted. The theory is illustrated through numerous well-known examples
from the literature.
Introduction
The initial development of evolutionary game theory and evolutionary stability
typically assumed
1. that an individual’s payoﬀdepends either on his strategy and that of his opponent
used during a single interaction with another player (normal form game) or on his
strategy and the current behavioral distribution of the population through a single
random interaction (population game; playing-the-ﬁeld model),
2. that the (pure) strategy set S available to an individual is ﬁnite and the same
for each player (symmetric game).
In this chapter these assumptions are relaxed in three diﬀerent ways and the conse-
quences are investigated for evolutionary dynamics, especially the replicator equa-
tion.
First, suppose that pairs of individuals have a series of interactions with each
other and that the set of actions available at later interactions may depend on what
choices were made in earlier ones. Many parlour games (e.g. tic-tac-toe, chess) are
2000 Mathematics Subject Classiﬁcation. Primary 91A22.
The author thanks Sabin Lessard for his thorough review and constructive comments. Also
appreciated is the assistance of Francisco Franchetti and Bill Sandholm in preparing the phase
diagrams for the ﬁgures showing trajectories of the replicator equation. The software is available
at W. H. Sandholm, E. Dokumaci, and F. Franchetti (2011). Dynamo: Diagrams for Evolutionary
Game Dynamics, version 1.0. http://www.ssc.wisc.edu/ whs/dynamo.
c
⃝2011 American Mathematical Society
27
                                           
                                                                                                                    
                                                                                                               

28
ROSS CRESSMAN
of this sort and it is my contention that most important ”real-life” games involving
humans or other species include a series of interactions among the same individuals.
It is often more appropriate to represent such games in extensive form rather than
normal form (Section 1).
Second, in many cases, it is more reasonable to assume that strategies available
to one player are diﬀerent than those available to another. For instance, choices
available when Black moves in chess are not usually the same as for White. Simi-
larly, if players are from two diﬀerent species, their strategy sets will almost surely
be diﬀerent (e.g. predator and prey). Suppose that there are two (or more) types
of players and a ﬁnite set of strategies for each type. If there are exactly two types
and the only interactions are single ones between a player of each type, we have a
bimatrix game. Otherwise, it is a more general asymmetric game in either extensive
or normal form (Section 2).
Finally, Section 3 considers brieﬂy symmetric (asymmetric) games where the
pure strategy set for each (type of) player is a continuum such as a subinterval of
real numbers. Now the replicator equation is an inﬁnite dimensional dynamical sys-
tem on the space(s) of probability measures over the subinterval(s) that correspond
to the distribution(s) of individual behaviors. Generalizations of the ESS (evolu-
tionarily stable strategy) concept can be deﬁned that characterize stability of single
strategies (i.e. Dirac delta distributions) under the replicator equation as well as
under the simpler canonical equation of adaptive dynamics that approximates the
evolution of the mean distribution(s).
In these three sections, each standard result taken from the literature is given
as a Theorem, with a reference where its proof can be found. Partial proofs are
provided here for some of the Theorems when they complement the presentation in
the main text.
1. Extensive Form Games
Although (ﬁnite, two-player) extensive form games are most helpful when used
to represent a game with long (but ﬁnite) series of interactions between the same
two players, diﬀerences with normal form intuition already emerge for short games
with perfect information.
1.1. Perfect information games. A (ﬁnite, two-player) perfect information
game is given by a rooted game tree Γ where each non-terminal node is a decision
point of one of the players or of nature. A path to a node x is a sequence of edges
and nodes connecting the root to x. The edges leading away from the root at each
player decision node are this player’s choices (or actions) at this node. There must
be at least two choices at each player decision node and any such choice that does
not yield a terminal node must be on some path to a decision node of the other
player. A pure (behavior) strategy for a player speciﬁes a choice at all of his decision
nodes. A mixed behavior strategy for a player speciﬁes a probability distribution
over the set of actions at each of his decision nodes. Payoﬀs to both players are
speciﬁed at each terminal node z ∈Z. A probability distribution over Z is called
an outcome.
Example 1. (Weibull, 1995) Figure 1 is an elementary perfect information game
with no moves by nature (i.e. at each non-terminal node, either player 1 or player 2
has a decision point). At each terminal node, payoﬀs to both players are indicated
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
29
Figure 1. Extensive form for Example 1.
with the payoﬀof player 1 above that of player 2. Player 1 has one decision node u
where he chooses between the actions L and R. If he takes action L, player 1 gets
payoﬀ1 and player 2 gets 4. If he takes action R, then we reach the decision point
v of player 2 who then chooses between ℓand r leading to both players receiving
payoﬀ0 or both payoﬀ2 respectively.
What are the Nash equilibria (NE) for this example? If players 1 and 2 choose
R and r respectively with payoﬀ2 for both, then
1. player 2 does worse through unilaterally changing his strategy by playing r with
probability q less than 1 (since 0(1 −q) + 2q < 2) and
2. player 1 does worse through unilaterally changing his strategy by playing L with
positive probability p (since 1p + 2(1 −p) < 2).
Thus, the strategy pair (R, r) is a strict NE corresponding to the outcome (2, 2).1
In fact, if player 1 plays R with positive probability at a NE, then player 2
must play r. From this it follows that player 1 must play R with certainty (i.e.
p = 0) (since his payoﬀof 2 is better than 1 obtained by switching to L). Thus any
NE with p < 1 must be (R, r). On the other hand, if p = 1 (i.e. player 1 chooses
L), then player 2 is indiﬀerent to what strategy he uses since his payoﬀis 4 for any
(mixed) behavior. Furthermore, player 1 is no better oﬀby playing R with positive
probability if and only if player 2 plays ℓat least half the time (i.e. 0 ≤q ≤1
2).
Thus
G ≡{(L, (1 −q)ℓ+ qr | 0 ≤q ≤1
2}
1Recall that a NE is strict if each player does worse by unilaterally changing his strategy.
When the outcome is a single node, this is understood by saying the outcome is the payoﬀpair
at this node.
                                                                                                                    
                                                                                                               

30
ROSS CRESSMAN
Figure 2. Trajectories of the replicator equation for Example 1.
is a set of NE, all corresponding to the outcome (1, 4). G is called a NE component
since it is a connected set of NE that is not contained in any larger connected
set of NE. The NE structure of Example 1 consists of the single strategy pair
G∗= {(R, r)} and the set G. These are indicated as a solid point and line segment
respectively in Figure 2 where G∗= {(p, q) | p = 0, q = 1} = {(0, 1)}.
Remark 1. Example 1 is a famous game known as the Entry Deterrence Game or
the Chain Store Game introduced by the Nobel laureate Reinhard Selten (Selten,
1978). Player 2 is a monopolist who wants to keep the potential entrant (player 1)
from entering the market that has a total value of 4. He does this by threatening to
ruin the market (play ℓwith both payoﬀs 0) if player 1 enters (plays R), rather than
accepting the entrant (play r and split the total value of 4 to yield payoﬀ2 for each
player). However, this is often viewed as an incredible (i.e. unbelievable) threat
since the monopolist should accept the entrant if his decision point is reached (i.e.
if player 1 enters) since this gives the higher payoﬀto him (i.e. 2 > 0).
Some game theorists argue that a generic perfect information game has only
one rational NE equilibrium outcome and this can be found by backward induction.
This procedure starts at a ﬁnal player decision point (i.e. a player decision point
that has no player decision points following it) and decides which unique action this
player chooses there to maximize his payoﬀin the subgame with this as its root.
The original game tree is then truncated at this node by creating a terminal node
there with payoﬀs to the two players given by this action. The process is continued
until the game tree has no player decision nodes left and yields the subgame perfect
NE (SPNE). That is, the strategy constructed by backward induction produces a
NE in each subgame Γu corresponding to the subtree with root at the decision node
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
31
u (Kuhn, 1953). For generic perfect information games (see Remark 2), the SPNE
is a unique pure strategy pair and is indicated by the double lines in the game tree.
If a NE is not subgame perfect, then this perspective argues that there is some
player decision node where an incredible threat would be used.
Example 1. (Continued) Can evolutionary dynamics be used to select one of the
two NE outcomes of the Chain Store Game? Suppose players 1 and 2 use mixed
strategies p and q respectively. The payoﬀs of pure strategies L and R are 1 and
(1−q)0+2q respectively and the payoﬀs of pure strategies ℓand r are 4p+(1−p)0
and 4p + (1 −p)2 respectively. Thus, the expected payoﬀs are p + (1 −p)2q and
(1 −q)4p + q(4p + (1 −p)2) for players 1 and 2 respectively. Under the replicator
equation, the probability of using a pure strategy increases if its payoﬀis higher
than these expected payoﬀs. For this example, the replicator equation is (Weibull,
1995)
˙p
=
p(1 −(p + (1 −p)2q)) = p(1 −p)(1 −2q)
(1.1)
˙q
=
q(4p + (1 −p)2 −[(1 −q)4p + q(4p + (1 −p)2)]) = q(1 −q)2(1 −p).
The rest points are the two vertices {(0, 0), (0, 1)} and the edge {(1, q) | 0 ≤q ≤1}
joining the other two vertices. Notice that, for any interior trajectory, q is strictly
increasing and that p is strictly increasing (decreasing) if and only q < 1
2 (q > 1
2).
Trajectories of (1.1) are shown in Figure 2.
The following results for Example 1 are straightforward to prove.
1. Every NE outcome is a single terminal node.2
2. Every NE component G includes a pure strategy pair.
3. The outcomes of all elements of G are the same.
4. Every interior trajectory of the replicator equation converges to a NE.
5. Every pure strategy NE is stable but not necessarily asymptotically stable.
6. Every NE that has a neighborhood whose only rest points are NE is stable.
7. If a NE component is interior attracting, it includes the SPNE.
8.
Suppose (p, q) is a NE. It is asymptotically stable if and only if it is strict.
Furthermore, (p, q) is asymptotically stable if and only if playing this strategy pair
reaches every player decision point with positive probability (i.e. (p, q) is pervasive).
From Result 8, the SPNE of the Chain Store Game is the only asymptotically
stable NE. That is, asymptotic atability of the evolutionary dynamics selects a
unique outcome for Example 1 whereby player 1 enters the market and the monop-
olist is forced to accept this. In general, we have the following theorem.
Theorem 1.
(Cressman, 2003) Results 2 to 8 are true for all generic perfect
information games. Result 1 holds for generic perfect information games without
moves by nature.
Remark 2. By deﬁnition, an extensive form game Γ is generic if no two pure
strategy pairs that yield diﬀerent outcomes have the same payoﬀfor one of the
players. If Γ is a perfect information game and there are no moves by nature, this
is equivalent to the property that no two terminal nodes have the same payoﬀfor
one of the players. If Γ is not generic, the SPNE outcome may not be unique since
2For Example 1, this is either (2, 2) or (1, 4).
                                                                                                                    
                                                                                                               

32
ROSS CRESSMAN
Figure 3. Centipede game of length ten.
several choices may arise at some player decision point in the backward induction
process if there are payoﬀties. Some of the results of Theorem 1 are true for general
perfect information games and some are not. For instance, Result 1 is not true for
some non-generic games or for generic games with moves by nature. Result 4, which
provides the basis to connect dynamics with NE in Results 5 to 8, remains an open
problem for non-generic perfect information games.
Theorem 1 applies to all generic perfect information games such as those in
Figures 3 and 4. For the centipede game of Figure 3 (Rosenthal, 1981), the SPNE
is for both players to play D (down) at each of their ﬁve decision points. In fact,
the only NE outcome is (0, 0) (i.e. player 1 plays D immediately) and so every
interior trajectory converges to a NE with this outcome (the dynamics here is in
a 2(25 −1) = 62 dimensional space).
Note that, at each player decision point
besides the last, both players are better oﬀif this player plays A (across) there
and his opponent plays A at the next decision point. From this it follows that, if
any choice D is eliminated, then the SPNE of the new perfect information game
is the terminal node that immediately follows the last D eliminated. Centipede
games of any ﬁxed length can be easily constructed with the same properties as
Figure 3. They provide a large class of perfect information games for which no
NE is asymptotically stable since the only equilibrium outcome is not pervasive
(cf.
Theorem 1, Results 8).
These games with a long chain of decision points
are often used to question the rationality assumptions behind equilibrium behavior
when such outcomes have many unreached decision points.
Since no pure strategy pair in Figure 4 can reach both the left-side subgame
and the right-side subgame, none are pervasive. Thus no NE can be asymptotically
stable by Theorem 1 (Results 1 and 8), a more elementary example than Figure
3. In fact, Figure 4 is probably the easiest example (Cressman, 2003) of a perfect
information game where the NE component G∗of the SPNE outcome (2, 3) is
not interior attracting (i.e. there are interior initial points arbitrarily close to G∗
whose interior trajectory under the replicator equation does not converge to this NE
component). That is, Figure 4 illustrates that the converse of Result 7 (Theorem
1) is not true.
To see this, some notation is needed. The (mixed) strategy space of player
1 is the one-dimensional strategy simplex Δ({T, B}) = {(pT , pB) | pT + pB =
1, 0 ≤pT , pB ≤1}.
This is also denoted Δ2 ≡{(p1, p2) | p1 + p2 = 1, 0 ≤
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
33
Figure 4. Perfect information game with unstable SPNE component.
pi ≤1}.3
Similarly, the strategy simplex for player 2 is the ﬁve-dimensional
set Δ({Lℓ, Lm, Lr, Rℓ, Rm, Rr}) = {(qLℓ, qLm, qLr, qRℓ, qRm, qRr) ∈Δ6}.
The
replicator equation is then a dynamics on the 6 dimensional space Δ({T, B}) ×
Δ({Lℓ, Lm, Lr, Rℓ, Rm, Rr}). The SPNE component (i.e. the NE component con-
taining the SPNE) is
G∗= {(T, q) | qLℓ+ qLm + qLr = 1, qLm + 3qLr ≤2}
corresponding to the set of strategy pairs with outcome (2, 3) where neither player
can improve his payoﬀby unilaterally changing his strategy. For example, if player
1 switches to B, his payoﬀof 2 changes to 0qLℓ+ 1qLm + 3qLr ≤2. The only other
pure strategy NE is {B, Rℓ} with outcome (0, 2) and corresponding NE component
G = {(B, q) | qLℓ+ qRℓ= 1, 1
2 ≤qRℓ≤1}. In particular, (T, 1
2qLm + 1
2qLr) ∈G∗
and (B, Rℓ) ∈G.
The face Δ({T, B}) × Δ({Lm, Lr}) has the same structure as the Chain Store
Game of Example 1. Speciﬁcally, the dynamics is given in Figure 2 where p cor-
responds to the probability player 1 uses T and q the probability player 2 uses
Lr.
Thus, points in the interior of this face with qLr >
1
2 that start close to
(T, 1
2qLm + 1
2qLr) converge to (B, Lr). The weak domination of Lm by Lr implies
qLr(t) > qLr(t) for t ≥0 along all trajectories starting suﬃciently close to these
points. Since the payoﬀto B is larger than to T if qLr(t) > qLr(t) on the face
Δ({T, B}) × Δ({Lm, Lr, Rℓ}), it can be shown that such trajectories on this face
converge to the NE (B, Lr), which is a strict NE for the game restricted to this
strategy space. By the stability of (B, Lr) for the full game (Theorem 1, Result 5)
and the continuous dependence of trajectories over ﬁnite time intervals on initial
conditions, there are trajectories in the interior of the full game that start arbitrar-
ily close to G∗that converge to a point in the NE component of G. That is, G∗is
not interior attracting.
The partial dynamic analysis of Figure 4 given in the preceding two paragraphs
illustrates nicely how the extensive form structure (i.e. the game tree for this perfect
information game) helps with properties of NE and the replicator equation (see also
Remark 6).
3In general, Δn is the set of vectors in Rn with nonnegative components that sum to 1.
                                                                                                                    
                                                                                                               

34
ROSS CRESSMAN
Remark 3. Extensive form games can always be represented in normal form. The
bimatrix normal form of Example 1 is
Ruin (ℓ)
Accept (r)
Not Enter (L)
Enter (R)

1, 4
1, 4
0, 0
2, 2

.
By convention, player 1 is the row player and player 2 the column player. Each
bimatrix entry speciﬁes payoﬀs received (with player 1’s given ﬁrst) when the two
players use their corresponding pure strategy pair. The bimatrix normal forms are
also denoted

A, BT 
where A and B are the payoﬀmatrices for player 1 and 2
respectively. In Example 1,
A =

1
1
0
2

and B =

4
4
0
2
T
=

4
0
4
2

.
This elementary example already shows a common feature of the normal form
approach for such games; namely, that some payoﬀentries are repeated in the
bimatrix. As a normal form, this means the game is non-generic even though it
arose from a generic perfect information game. For this reason, most normal form
games cannot be represented as perfect information games. However, they can all
be represented as one-stage simultaneity games (e.g. Figure 7).
1.2. Simultaneity games. A (ﬁnite, two-player) simultaneity game is an ex-
tensive form game that involves n stages such that, at each stage, both players know
all actions that have already occurred in previous stages but not the opponent’s
action in the current stage. If there are moves by nature, each of these nodes occur
at the beginning of some stage and both players know what action nature takes at
these nodes. Thus, the ﬁrst player decision point x at any stage is the root of a
subgame (and so an information set u for this player). Any decision node y follow-
ing x in the same stage must be a decision point of the other player. The set v of
all such y forms an information set for this other player and so each y in v has the
same set of actions. A choice by this player at information set v is given by taking
the same action at each of the decision points y in v. The simultaneity game is
symmetric if there is a bijection between the information sets and actions of player
1 and those of player 2 that makes its normal form representation a symmetric
game (as explained in the following example based on Figure 5).
For what follows, it is important to understand both the standard normal form
and the reduced-strategy normal form of an extensive form game with game tree
Γ. A player’s pure strategy for the standard normal form of Γ speciﬁes a choice at
each of his information sets. On the other hand, a player’s pure strategy for the
reduced-strategy normal form speciﬁes a choice at only those information sets v
of this player which are relevant given choices already speciﬁed by this strategy at
information sets of this player on the path to v. Here, v is relevant if there exists
a pure strategy of the other player so that v is reached if this pure-strategy pair is
used. In Section 1.1, the standard normal form is identical to the reduced-strategy
normal form for Figure 1 and for Figure 4. However, for the centipede game of
Figure 3, there are 25 = 32 pure strategies for player 1 for the standard normal form
(specifying a choice between Across and Down at each of his ﬁve decision points)
whereas there are only 6 pure strategies (D, AD, AAD, AAAD, AAAAD, AAAAA)
in the reduced-strategy normal form.
The replicator equation of Section 1.1 is
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
35
Figure 5. Extensive form of Example 2.
implicitly based on the standard normal form, although the results of Theorem 1
(when suitably interpreted) remain true for the reduced-strategy normal form.
Example 2. (van Damme, 1991) Figure 5 is an example of an elementary two-stage
symmetric simultaneity game. The two information sets of player 2 both include
two decision points and are indicated by dashed horizontal lines. At each of these
information sets, player 2 must make the same choice at both decision points. The
reduced-strategy normal form is
L
Rℓ
Rr
L
Rℓ
Rr
⎡
⎣
0, 0
1, 1
1, 1
1, 1
−5, −5
5, −4
1, 1
−4, 5
4, 4
⎤
⎦.
This is a symmetric game since the column player (player 2) has payoﬀmatrix
⎡
⎣
0
1
1
1
−5
−4
1
5
4
⎤
⎦
T
which is the same as the payoﬀmatrix A =
⎡
⎣
0
1
1
1
−5
5
1
−4
4
⎤
⎦of
the row player (player 1). Thus, a player’s payoﬀdepends only on the strategy pair
used and not on his designation as a row or column player.
                                                                                                                    
                                                                                                               

36
ROSS CRESSMAN
Figure 6. Trajectories of the replicator equation for Example 2.
To apply backward induction to this example, the only proper subgame Γu2
has root at u2 and payoﬀmatrix given by4
Au2 =
ℓ
r

−5
5
−4
4

.
This is equivalent to a Hawk-Dove Game with a unique symmetric NE 1
2ℓ+ 1
2r
(which is also an ESS) and corresponding payoﬀ0. The truncated game with root
at u1 has payoﬀmatrix
L
R

0
1
1
0

and it also has unique ESS at 1
2L + 1
2R and corresponding payoﬀ1
2. Thus, 1
2L +
1
2

 1
2Rℓ+ 1
2Rr

= 1
2L + 1
4Rℓ+ 1
4Rr is a symmetric NE of Example 2 which can be
easily conﬁrmed since
Ap∗=
⎡
⎣
1/2
1/2
1/2
⎤
⎦where p∗=
⎡
⎣
1/2
1/4
1/4
⎤
⎦.
Somewhat surprisingly, p∗is not an ESS of A since, for example, e3·Ap∗= p∗·Ap∗=
1
2 and p∗·Ae3 < e3·Ae3 (i.e. 1
2 + 5
4 + 4
4 < 4).5 However, p∗is globally asymptotically
stable under the replicator equation (Figure 6) by the following theorem.
Recall that the replicator equation for a symmetric game with n × n payoﬀ
matrix A is
˙pi = (ei −p) · Ap
4Γu2 is a symmetric subgame and so only the payoﬀs to player 1 are required in Au2.
5Recall that p∗is an ESS of a symmetric normal form game with n × n payoﬀmatrix A if i)
p · Ap∗≤p∗· Ap∗for all p ∈Δn and ii) p∗· Ap > p · Ap whenever p · Ap∗= p∗· Ap∗and p ̸= p∗.
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
37
for i = 1, ..., n where ei is the unit vector (corresponding with the ith pure strategy)
that has 1 in its ith component and 0 everywhere else and pi is the proportion of
the population using strategy ei.
Theorem 2. (Cressman, 2003) Suppose that Γ is a symmetric simultaneity game.
If p∗is an asymptotically stable NE of the standard normal form of Γ under the
replicator equation, then p∗is pervasive and subgame perfect. If Γ has no moves by
nature, then a pervasive NE p∗of the reduced-strategy normal form of Γ is asymp-
totically stable under the replicator equation if and only if p∗is given by backward
induction applied to the asymptotically stable pervasive NE of the subgames of Γ
and their truncations.
Proof. First, consider the standard normal form of Γ. For a symmetric ex-
tensive form game, a strategy is pervasive if if reaches every player information set
when played against itself. If p∗is a NE that is not pervasive, then there is an
information set u that is not reached when both players use p∗. Since Γ is a sym-
metric simultaneity game, we may assume that u is an information set of player 1.
Since there are at least two actions at u, we can change p∗to a diﬀerent strategy p
so that p∗and p induce the same behavior strategy at each player 1 information set
reachable by p∗. It can be shown that any convex combination of p∗and p is then
a rest point of the replicator equation. In particular, none of these points in this
connected set can be asymptotically stable. On the other hand, any NE induces a
NE in each subgame that it reaches when played against itself (Kuhn, 1953; Selten,
1983). Thus, if p∗is pervasive, it induces a NE in every subgame and so is a SPNE.
Now consider the reduced-strategy normal form of Γ. The proof of the last
statement of the theorem is considerably more diﬃcult (see Cressman (2003) for
more details). The key is that the replicator equation at p on Γ induces the repli-
cator equation in each subgame Γu up to constant multiples given by functions of
p that are positive for p close to p∗when p∗is pervasive. Asymptotic stability of
p∗is then equivalent to asymptotic stability for the truncated game. The proof is
completed by applying these results to a subgame at the last stage of Γu and using
induction on the number of subgames of Γ.
□
Remark 4. Selten mistakenly asserted in 1983 (correcting himself in 1988) that
the backward induction procedure applied to the ESSs of subgames and their trun-
cations yields a direct ESS (i.e. an ESS in behavior strategies) for symmetric ex-
tensive form games. Example 2 is the well-known counterexample due to Eric van
Damme (1991). By Theorem 2, Selten’s assertion is true when “ESS” is replaced
by “asymptotic stability under the replicator equation” and there are no moves by
nature. One must be careful extending this result when there are moves by nature
as illustrated by Example 3 below.
Every (symmetric) normal form game can be represented as a single-stage (sym-
metric) simultaneity game. Thus, unlike perfect information games, generic sym-
metric simultaneity games can have a NE outcome (and ESS) whose NE component
                                                                                                                    
                                                                                                               

38
ROSS CRESSMAN
Figure 7. Extensive form of the standard RSP Game.
does not include a pure strategy. For instance, the standard zero-sum Rock-Scissors-
Paper (RSP) Game with payoﬀmatrix
R
S
P
⎡
⎣
0
1
−1
−1
0
1
1
−1
0
⎤
⎦
and unique NE (1/3, 1/3, 1/3) has extensive form given in Figure 7.
The following example uses the generalized RSP game with payoﬀmatrix
(1.2)
⎡
⎣
0
b2
−a3
−a1
0
b3
b1
−a2
0
⎤
⎦=
⎡
⎣
0
6
−4
−4
0
4
2
−2
0
⎤
⎦
and unique NE p∗= (10/29, 8/29, 11/29). All such games with positive parameters
ai and bi exhibit cyclic dominance whereby R beats S (i.e. R strictly dominates S
in the two-strategy game based on these two strategies), S beats P, and P beats
R. They all have a unique NE that is in the interior of Δ3. From Hofbauer and
Sigmund (1998, Section 7.7), p∗is not an ESS for (1.2) since b1 < a3 (i.e. 2 < 4)
but it is globally asymptotically stable under the replicator equation (Figure 8)
since a1a2a3 < b1b2b3 (i.e. 2 · 4 · 4 < 2 · 4 · 6).
Example 3. (Chamberland and Cressman, 2000) Suppose that, on even numbered
days, players play the left-hand subgame of Figure 9 and on odd numbered days
the right-hand subgame (an alternative interpretation is that nature ﬂips a fair coin
at the root of Figure 9 that determines which subgame is played). However, for
both types of days, the same RSP game given by (1.2) is played. In this single-
stage symmetric simultaneity game, both players have 9 pure (behavior) strategies
RR, RS, ..., PP that specify a choice of R, S or P in each of the subgames. The
unique symmetric NE outcome is for both players to play p∗= (10/29, 8/29, 11/29)
in both subgames. The corresponding NE component is a four-dimensional set E of
points p = (p11, ..., p33) formed by intersecting a four-dimensional hyperplane with
the eight-dimensional strategy simplex Δ9. These are the points whose induced
marginal strategies in the two subgames (i.e.
p1 and p2 where p1
i ≡3
j=1 pij
and p2
j ≡3
i=1 pij) are both equal to p∗.
It can be shown (Chamberland and
Cressman, 2000) that some points in E near the boundary of Δ9 are unstable since
the linearization of the replicator equation there yields an eigenvalue with positive
real part.
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
39
Figure 8. Trajectories of the replicator equation for the game
with payoﬀmatrix (1.2).
Figure 9. Extensive form of a single-stage simultaneity game with
a move by nature and identical generalized RSP subgames.
The reason this can occur is that, at a general p ∈Δ9, the evolution of strat-
egy frequencies in one subgame can be inﬂuenced by payoﬀs received in the other
subgame. In particular, the frequency of R use in the left-hand subgame can be
increasing even if the population state there is mostly P users.
To avoid this
type of unintuitive situation, the replicator equation can be restricted to the four-
dimensional invariant Wright manifold
W ≡{p ∈Δ9 | pij = p1
i p2
j}.
On W, the dynamics for the induced strategy in each subgame is the same as the
replicator equation for the payoﬀmatrix (1.2). Thus, each interior trajectory that
starts on W converges to the single point p∗with p∗
ij = p∗
i p∗
j.
                                                                                                                    
                                                                                                               

40
ROSS CRESSMAN
Figure 10. Extensive form of an asymmetric two-role game.
Remark 5. The Wright manifold W can be deﬁned for all simultaneity games (in
fact, all extensive form games) and it is invariant under the replicator equation. On
W, Theorem 2 is true for all symmetric simultaneity games whether or not there
are moves by nature.
Remark 6. Extensive form games, with an explicit description of the sequential
feature of the players’ possible actions, played a central role in the initial devel-
opment of classical game theory by von Neumann and Morgenstern (1944). On
the other hand, most dynamic analyses of evolutionary games are based on their
normal forms One consequence of this is that typical normal form examples con-
sidered in evolutionary game theory have a small number of pure strategies since
it is well-known that the high-dimensional systems of evolutionary dynamics as-
sociated to a large number of pure strategies can exhibit all the complexities of
arbitrary dynamical systems such as periodic orbits, limit cycles, bifurcations and
chaos. The above discussion was meant to convince you that the extensive form
structure (which is usually associated with a large number of pure strategies) im-
parts special properties on the evolutionary dynamics that makes its analysis more
tractable than would otherwise be expected.
2. Asymmetric Games
A (ﬁnite, two-player) asymmetric game has a set {u1, u2, ..., uN} of N roles.
Players 1 and 2 are assigned roles uk and uℓrespectively with probability ρ(uk, uℓ).
We assume that role assignment is independent of player designation (i.e. ρ(uk, uℓ) =
ρ(uℓ, uk)). If players are assigned the same role (i.e. k = ℓ), then they play a
symmetric (normal form) game with payoﬀmatrix Akk. When they are assigned
diﬀerent roles (i.e. k ̸= ℓ), they play a bimatrix (normal form) game with payoﬀ
matrices Akℓand Aℓk.
Figure 10 is the extensive form of a two role game with two pure strategies in
role u1 and three in role u2. Here, the initial move by nature indicates ρ(uk, uℓ) = 1
4
for all 1 ≤k, ℓ≤2. On the other hand, if N = 1, then ρ(u1, u1) = 1 and we have
a symmetric game (e.g. only the left-hand subtree of Figure 9 formed by nature
following the left-most direction at the root with probability 1). Similarly, if N = 2
and ρ(u1, u2) = ρ(u2, u1) = 1
2, then ρ(u1, u1) = ρ(u2, u2) = 0 and so we have a
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
41
bimatrix game (e.g. only the middle two subtrees of Figure 9 formed by nature
following these two directions at the root with probability 1
2). Thus, asymmetric
games include both symmetric and bimatrix normal form games as special cases.
All asymmetric games have a single-stage extensive form representation with
an initial move by nature and information sets u1, u2, ..., uN for both players. A
pure strategy for player 1 speciﬁes a choice at each of his information sets. It has
the form ei where i = (i1, ..., iN) is a multi-index with ik giving the choice of ei
at uk. Each mixed strategy p is a discrete probability distribution over the ﬁnite
set {ei} with weight pi on ei. This p induces a local behavior strategy pk at each
information set uk given by
pk
r =

{i|ik=r}
pi
and the Wright manifold is W ≡{p | pi = p(i1,...,iN ) = p1
i1p2
i2...pN
iN }. W is invariant
under the replicator equation.
2.1. Bimatrix games. Here, N = 2, ρ(u1, u2) = ρ(u2, u1) = 1
2 and ρ(u1, u1) =
ρ(u2, u2) = 0. By abuse of notation, let A12 = A, A21 = B, p1 = p, p2 = q, ei be the
pure strategies in role u1, and fj be the pure strategies in role u2. Then, on W,
˙pi
=
pi(ei −p) · Aq
˙qj
=
qj(fj −q) · Bp
is the replicator equation restricted to W when the pure strategies are given as
the appropriate unit vectors. This is the bimatrix replicator dynamics which we
illustrate in Example 4.
Example 4. (Cressman, 2003) Consider the game with bimatrix
H
C
T
I

5, 4
1, 6
4, 0
3, −2

.
There is no NE given by a pure strategy pair (e.g. at (T, H), player 2 does better
by switching to C since 6 > 4).6 In fact, no strategy pair is a NE if either player
uses a pure strategy. Thus, any NE (p∗, q∗) must be a completely mixed strategy
for each player. In particular, there is a unique NE given by (p∗
1, q∗
1) =

 1
2, 2
3

since
Aq∗=

5
1
4
3
 
2/3
1/3

=

11/3
11/3

and Bp∗=

4
0
6
−2
 
1/2
1/2

=

2
2

.
However, this NE is not asymptotically stable since H(p1, q1) ≡p2
1(1−p1)2q2
1(1−q1)
is a constant of motion under the replicator equation (i.e.
dH
dt = 0) whose level
curves are given in Figure 11. Trajectories of the replicator equation
(2.1)
˙p1 = p1(1 −p1)(3q1 −2)
˙q1 = q1(1 −q1)(2 −4p1)
evolve clockwise around the interior rest point (p∗
1, q∗
1) along these curves.
Another way to see that (p∗
1, q∗
1) =

 1
2, 2
3

is not asymptotically stable is to
consider the time-adjusted replicator equation in the interior of the unit square
6This contrasts with Result 2 of Theorem 1 for perfect information games.
                                                                                                                    
                                                                                                               

42
ROSS CRESSMAN
Figure 11. Trajectories of the bimatrix replicator equation for
Example 4.
that divides the vector ﬁeld in (2.1) by the Dulac function p1(1 −p1)q1(1 −q1) to
obtain
(2.2)
˙p1 =
(3q1−2)
q1(1−q1)
˙q1 =
(2−4p1)
p1(1−p1)
.
Trajectories of (2.2) are the same curves as those of (2.1) and evolve in the same
direction. In particular, both dynamics have the same asymptotically stable interior
points. Under the adjusted dynamics (2.2), a rectangle Δp1Δq1 in the interior does
not change area as it evolves since its horizontal and vertical cross-sections maintain
the same lengths under this dynamics. (This invariance of area also follows from
Liouville’s result that ”volumes” remain constant when the vector ﬁeld is divergence
free.) Thus no interior point can be asymptotically stable since no small rectangle
containing it evolves to this point (i.e. to a region with zero area).
Example 4 is the well-known Buyer-Seller Game where a Buyer of some mer-
chandise can either Trust the Seller to give an accurate value of this item or the
Buyer can have the item inspected (i.e. play Inspect) to determine its true value.
The Seller has a choice between Honest (give an accurate value) or Cheat (misrep-
resent its true value). The clockwise rotation of the trajectories in Figure 11 is not
surprising given the cycling of the best responses to pure strategy pairs. What is not
a priori clear is why trajectories cannot spiral in to the interior rest point making it
asymptotically stable. This follows from the analysis above for the two-dimensional
dynamics of Figure 11. It is also a consequence of part (a) of the following theorem
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
43
for general bimatrix games (which can be proved using Liouville’s result in higher
dimensions).
Theorem 3.
(Hofbauer and Sigmund, 1998) (a) A strategy pair (p∗, q∗) is an
asymptotically stable rest point of the bimatrix replicator equation if and only if it
is a strict NE. In particular, (p∗, q∗) is a pure strategy pair.
(b) If there is no interior NE, then all trajectories of the bimatrix replicator equation
converge to the boundary.
2.2. Two-species ESS. Asymmetric games with two roles (i.e. N = 2) can
be interpreted as games between two species by equating intraspeciﬁc interactions
as between individuals playing a symmetric game in the same roles and interspeciﬁc
interactions between individuals playing a bimatrix game in opposite roles. From
this perspective, Figure 10 is then an example where there are both intra and inter
speciﬁc interactions. On the other hand, bimatrix games such as the Buyer-Seller
Game of Example 4 are then ones where all interactions are interspeciﬁc.
Suppose we extend Maynard Smith’s original idea by saying that a (two-species)
ESS is a monomorphic system with strategy pair (p∗, q∗) that cannot be successfully
invaded by a rare (mutant) subsystem using a diﬀerent strategy pair (p, q). That is,
deﬁne (p∗, q∗) as a two-species ESS if it is asymptotically stable under the replicator
equation based on the strategy pairs (p∗, q∗) and (p, q) whenever (p, q) ̸= (p∗, q∗).
Suppose that A and D are the payoﬀmatrices for intraspeciﬁc interactions (i.e.
symmetric games) of species one and two respectively whereas B and C form the
bimatrix game corresponding to interspeciﬁc interactions.
For the two-dimensional replicator equation based on the strategy pairs (p∗, q∗)
and (p, q), let ε be the frequency of p in species one (so p∗has frequency 1 −ε) and
δ be the frequency of q in species two. The payoﬀof p is p · [A(εp + (1 −ε) p∗) +
B(δq +(1 −δ) q∗)] and the average payoﬀin species one is (εp+(1 −ε) p∗)·[A(εp+
(1 −ε) p∗) + B(δq + (1 −δ) q∗)]. By the analogous expressions for the payoﬀs of
species two, this replicator equation is
(2.3)
˙ε = (1 −ε) ((εp + (1 −ε) p∗) −p∗) · [A(εp + (1 −ε) p∗) + B(δq + (1 −δ) q∗)]
˙δ = (1 −δ) ((δq + (1 −δ) q∗) −q∗) · [C(εp + (1 −ε) p∗) + D(δq + (1 −δ) q∗)].
Then
Theorem 4. (Cressman, 2003) (a) (p∗, q∗) is a two-species ESS if and only if
(2.4)
either
p∗· (Ap + Bq) > p · (Ap + Bq)
or
q∗· (Cp + Dq) > q · (Cp + Dq)
for all strategy pairs (p, q) that are suﬃciently close (but not equal) to (p∗, q∗).
(b) If (p∗, q∗) is a two-species ESS, then it is asymptotically stable for the two-
species replicator equation (i.e.
based on all pure strategies of the asymmetric
game).
Proof. (a) Fix (p, q) with p ̸= p∗and q ̸= q∗.
Notice that the dynamics
(2.3) leaves the unit square and each of its edges invariant. We claim that (0, 0) is
                                                                                                                    
                                                                                                               

44
ROSS CRESSMAN
asymptotically stable if and only if
(2.5)
either ((εp + (1 −ε) p∗) −p∗) · [A(εp + (1 −ε) p∗) + B(δq + (1 −δ) q∗)] < 0
or
((δq + (1 −δ) q∗) −q∗) · [C(εp + (1 −ε) p∗) + D(δq + (1 −δ) q∗)] < 0
for all nonnegative ε and δ with (ε, δ) suﬃciently close (but not equal) to (0, 0).
This result applied to all choices of (p, q) completes the proof of part (a).
If (p −p∗) · [Ap∗+ Bq∗] ̸= 0 or (q −q∗) · [Cp∗+ Dq∗] ̸= 0, it is straightforward
to show (0, 0) is asymptotically stable if and only if (p −p∗) · [Ap∗+ Bq∗] ≤0 and
(q −q∗)·[Cp∗+Dq∗] ≤0 and that these last two inequalities hold if and only if (2.5)
holds. Thus, for the remainder of the proof, assume that (p −p∗) · [Ap∗+ Bq∗] = 0
and (q −q∗) · [Cp∗+ Dq∗] = 0. Then (2.3) becomes
˙ε
=
ε (1 −ε) (p −p∗) · [Aε(p −p∗) + Bδ(q −q∗)]
˙δ
=
δ (1 −δ) (q −q∗) · [Cε(p −p∗) + Dδ(q −q∗)]
and the two-species ESS condition (2.4) can be rewritten as
either
(p −p∗) · [A(p −p∗) + B(q −q∗)] < 0
or
(q −q∗) · [C(p −p∗) + D(q −q∗)] < 0.
Since the ε−axis (i.e. when δ = 0) is invariant, ε = 0 is asymptotically stable on
it if and only if (p −p∗)·A(p−p∗) < 0. By the same argument applied to the δ−axis,
asymptotic stability of (0, 0) implies (p −p∗)·A(p−p∗) < 0 and (q −q∗)·D(q−q∗) <
0. Suppose that these two strict inequalities are true. If (p −p∗) · B(q −q∗) ≤0
or (q −q∗) · C(p −p∗) ≤0, then (0, 0) is asymptotically stable and (2.4) holds. On
the other hand, if (p −p∗) · B(q −q∗) > 0 and (q −q∗) · C(p −p∗) > 0, then ˙ε = 0
on the line
ε = −(p −p∗) · B(q −q∗)
(p −p∗) · A(p −p∗)δ
through the origin with positive slope and ˙ε < 0 below this ε−isocline. Similarly
˙δ = 0 on the line
ε = −(q −q∗) · D(q −q∗)
(q −q∗) · C(p −p∗)δ
through the origin with positive slope and ˙δ < 0 above this δ−isocline.
From
the phase diagram, it follows that (0, 0) is asymptotically stable if and only if the
ε−isocline is steeper than the δ−isocline. On the other hand, (2.4) holds if the
ε−isocline is steeper than the δ−isocline. That is, asymptotic stability of (0, 0)
implies (2.4). Conversely, if (2.4) is not true for some choice of (p, q) ̸= (p∗, q∗), it
is again clear from the phase diagram that (0, 0) is not asymptotically stable for
the replicator equation based on these two strategy pairs.
(b) Suppose that species one (respectively, two) has n (respectively m) pure
strategies. We only consider the case where (p∗, q∗) is in the interior of Δn × Δm.
The key to the stability of (p∗, q∗) under the replicator equation on Δn × Δm is
that (2.4) is equivalent to the existence of an r > 0 such that
(p −p∗) · [A(p −p∗) + B(q −q∗)] + r (q −q∗) · [C(p −p∗) + D(q −q∗)] < 0
for all (p, q) ∈Δn×Δm that are not equal to (p∗, q∗) (see Cressman, 2003). Consider
the function
V (p, q) ≡n
i=1(pi)p∗
i
m
j=1(qj)
q∗
j
r
,
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
45
which is deﬁned on Δn ×Δm and has a global maximum at (p∗, q∗). It can then be
shown that ˙V (p, q) > 0 if (p, q) ̸= (p∗, q∗) and (p, q) is in the interior of Δn × Δm.
That is, V is a strict Lyapunov function (Hofbauer and Sigmund, 1998) and so
(p∗, q∗) is globally asymptotically stable.
□
If there are no intraspeciﬁc interactions (take A and D as the zero matrices
0 of the appropriate size), then (p∗, q∗) is a two-species ESS if and only if it is a
strict NE (e.g. by taking q = q∗, we have that p∗· Bq∗> p · Bq∗if p ̸= p∗since
q∗· Cp = q · Cp). That is, by Theorems 3 and 4, (p∗, q∗) is a two-species ESS for
a bimatrix game if and only if it is a strict NE and this is true if and only if it is
asymptotically stable under the bimatrix replicator equation.
At the other extreme, suppose that there are no interspeciﬁc interactions (take
B and C as zero matrices). Then, (p∗, q∗) is a two-species ESS if and only p∗is a
single-species ESS for species one and q∗is a single-species ESS for species two. For
example, when q = q∗, we need p∗· Ap > p · Ap for all p that are suﬃciently close
(but not equal) to p∗. Recall that this inequality condition, called local superiority
by Weibull (1995), characterizes the single-species ESS (of species one). From this
result, there can be two-species ESSs that are not strict NE (see Example 5 below).
In particular, there can be completely mixed ESSs.
From these two extremes, we see that the concept of a two-species ESS combines
and generalizes the concepts of single-species ESS of symmetric games and the strict
NE of bimatrix games.
Example 5. (Krivan et al., 2008) Suppose that there are two species competing in
two diﬀerent habitats (or patches) and that the overall population size (i.e. density)
of each species is ﬁxed. Also assume that the ﬁtness of an individual depends only
on its species, the patch it is in and the density of both species in this patch. Then
strategies of species one and two can be parameterized by the proportions p1 and q1
respectively of these species that are in patch one. If individual ﬁtness (i.e. payoﬀ)
is positive when a patch is unoccupied and linearly decreasing in patch densities,
it is of the form
Fi
=
ri

1 −piM
Ki
−αiqiN
Ki

Gi
=
si

1 −qiN
Li
−βipiM
Li

.
Here, Fi is the ﬁtness of a species one individual in patch i, Gi is the ﬁtness of a
species two individual in patch i, p2 = 1 −p1 and q2 = 1−q1. All other parameters
are ﬁxed and positive (see Remark 7 below).
By linearity, these ﬁtnesses can be represented by a two-species asymmetric
game with payoﬀmatrices
A =

r1 −r1M
K1
r1
r2
r2 −r2M
K2

B =

−α1r1N
K1
0
0
−α2r2N
K2

C =

−β1s1M
L1
0
0
−β2s2M
L2

D =

s1 −s1N
L1
s1
s2
s2 −s2N
L2

.
                                                                                                                    
                                                                                                               

46
ROSS CRESSMAN
p1
q1
Figure 12. Vector ﬁelds for the two-patch Habitat Selection
Game.
The equal ﬁtness lines of species one (dashed line) and
species two (dotted line) intersect in the unit square. Solid dots
are ESSs. (a) A unique ESS in the interior. (b) Two ESSs on the
boundary.
For example, Fi = ei · (Ap + Bq). At an equilibrium (p, q), all individuals present
in species one must have the same ﬁtness as do all individuals present in species
two.
Suppose that both patches are occupied at the equilibrium (p, q). Then (p, q)
is a NE and (p1, q1) is a point in the interior of the unit square that satisﬁes
r1

1 −p1M
K1
−α1q1N
K1

=
r2

1 −(1 −p1)M
K2
−α2(1 −q1)N
K2

s1

1 −q1N
L1
−β1p1M
L1

=
s2

1 −(1 −q1)N
L2
−β2(1 −p1)M
L2

.
That is, these two “equal ﬁtness” lines (which have negative slopes) intersect at
(p1, q1) as in Figure 12.
The interior NE (p, q) is a two-species ESS if and only if the equal ﬁtness line of
species one is steeper than that of species two (cf. the proof of Theorem 4). That
is, (p, q) is an interior two-species ESS in Figure 12A but not in Figure 12B. The
interior two-species ESS in Figure 12A is globally asymptotically stable under the
replicator equation.
Figure 12B has two two-species ESSs, both on the boundary of the unit square.
One is a pure strategy pair strict NE with species one and two occupying separate
patches (p1 = 1, q1 = 0) and the other has species two in patch one and species one
split between the two patches (0 < p1 < 1, q1 = 1). Both are locally asymptotically
stable under the replicator equation with basins of attraction formed by an invariant
separatrix, joining the two vertices corresponding to both species in the same patch,
on which trajectories evolve to the interior NE.
If the equal ﬁtness lines do not intersect in the interior of the unit square, then
there is exactly one two-species ESS. This is on the boundary (either a vertex or
on an edge) and is globally asymptotically stable under the replicator equation.
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
47
Remark 7. Example 5 is called a (two-patch) Habitat Selection Game for two
competitive species.
The ﬁxed parameters then have biological interpretations.
Speciﬁcally, for species one, M is the total population size; ri, Ki and αi are its
intrinsic growth rate, carrying capacity and interspeciﬁc competition coeﬃcient
(modelling the eﬀect of the second species on the ﬁrst) in patch i respectively. The
analogous parameters for species two are N; si, Li and βi. Linearity of the ﬁtness
functions corresponds to Lotka-Volterra interactions.
Habitat selection games for a single species were already introduced before
evolutionary game theory was developed when Fretwell and Lucas (1969) deﬁned
an ideal free distribution (IFD) to be a patch distribution whereby the ﬁtness of any
individual in an occupied patch was the same as the ﬁtness of any other individual
and at least as high as what would be the ﬁtness in any unoccupied patch. If patch
ﬁtness is decreasing in patch density, then the IFD and ESS concepts are identical
for a single species. In fact, there is a unique IFD and it is globally asymptotically
stable under the replicator equation.
For two species, some authors consider an interior NE to be a (two-species)
IFD. Example 5 shows such NE may be unstable (Figure 12B) and so justiﬁes the
perspective of others who restrict the IFD concept to two-species ESSs.
Remark 8. The generalization of Theorem 4 to three (or more) species is a diﬃcult
problem (Cressman et al., 2001). It is possible to characterize a monomorphic three-
species ESS as one where, at all nearby strategy distributions, at least one species
does better using its ESS strategy. However, such an ESS concept does not always
imply stability of the three-species replicator equation that is based on the entire
set of pure strategies for each species.
3. Games with Continuous Strategy Spaces
When players can choose from a continuum of pure strategies, the connections
between NE and dynamic stability become more complicated. For example, the
standard result forming one part of the Folk Theorem of Evolutionary Game Theory
(Hofbauer and Sigmund, 1998) that a strict NE is asymptotically stable under the
replicator equation (as well as most other deterministic evolutionary dynamics) is
true for all games that have a ﬁnite set of pure strategies including the symmetric
and asymmetric games of extensive or normal form in Sections 1 and 2. However,
a strict NE is not always dynamically stable for games with continuous strategy
spaces as seen in Example 6 below. Now, dynamic stability requires additional
conditions such as that of a continuously stable strategy (CSS) or a neighborhood
invader strategy (NIS) introduced by Eshel (1983) and Apaloo (1997) respectively.
In fact, the exact requirement depends on the form of the evolutionary dynamics.
3.1. Symmetric games with a continuous strategy space. In this sec-
tion, static conditions are developed for stability under the two standard dynamics
for games with a continuous strategy space S; namely, adaptive dynamics and
the replicator equation. The canonical equation of adaptive dynamics (Dieckmann
and Law, 1996; Dercole and Rinaldi, 2008) models the evolution of the population
mean strategy x ∈S by assuming that the population is always monomorphic at
its mean. Then x evolves through trait substitution in a direction y of nearby mu-
tants that can invade due to their higher payoﬀthan x when playing against this
monomorphism.
                                                                                                                    
                                                                                                               

48
ROSS CRESSMAN
The replicator equation is now a dynamic on the space Δ(S) of Borel proba-
bility measures over the strategy space S (Bomze, 1991). This inﬁnite-dimensional
dynamical system restricts to the replicator equation of a symmetric normal form
game when a ﬁnite subset of S is taken as the strategy set. From the perspective
of the replicator equation that describes the evolution of the population strategy
distribution P ∈Δ(S) rather than the evolution of the population mean, the canon-
ical equation becomes a heuristic tool that approximates how the mean evolves by
ignoring eﬀects due to the diversity of strategies in the population.
3.1.1. One-dimensional strategy space. Suppose that S is a convex compact
subset of R (i.e. a closed and bounded interval). Following Hofbauer and Sigmund
(1990), if the payoﬀto y interacting with x, π(y, x), is continuously diﬀerentiable,
then the canonical equation of adaptive dynamics has the form (up to a change in
time scale)
(3.1)
˙x = ∂π(y, x)
∂y
|y=x
at interior points of S (i.e. for x ∈int(S)). That is, x increases if π(y, x) is an
increasing function of y for y close to x.
An x∗∈int(S) is an equilibrium if π1(x∗, x∗) = 0 (here π1 is the partial deriva-
tive of π with respect to the ﬁrst variable). Such an equilibrium is called convergence
stable (Christiansen, 1991) if it is asymptotically stable under (3.1). If π(y, x) has
continuous partial derivatives up to second order, then x∗is convergence stable if
(3.2)
d
dx
∂π(y, x)
∂y
|y=x

|x=x∗< 0.
If πij is the second order partial derivative of π with respect to i and j, then this
inequality condition is π11(x∗, x∗) + π12(x∗, x∗) ≡π11 + π12 < 0. Conversely, if x∗
is convergence stable, then π11 +π12 ≤0. The following example illustrates the NE
and convergence stability concepts for quadratic payoﬀfunctions.
Example 6. Let S = [−1, 1] be the set of pure strategies and π(x, y) = ax2 + bxy
be the payoﬀto x playing against y for all x, y ∈S where a and b are ﬁxed real
numbers. Then x∗in the interior of S is a NE (i.e. π(x, x∗) ≤π(x∗, x∗) for all
x ∈S) if and only if x∗= 0 and a ≤0.
Also, x∗= 0 is a strict NE if and
only if a < 0. These results exclude the degenerate case where 2a + b = 0 and
π(x, y) = a(x −y)2 −ay2. In this case (which we ignore from now on), every x ∈S
is a strict NE if a < 0, a NE if a = 0, and there are no pure strategy NE if a > 0.
From (3.1), adaptive dynamics is now
˙x = ∂π(y, x)
∂y
|y=x= (2a + b)x.
The only equilibrium is x∗= 0 and it is convergence stable if and only if 2a+b < 0.
In particular, x∗= 0 may be a strict NE but not convergence stable (e.g. a < 0 and
2a+b > 0) or may be a convergence stable rest point that is not a NE (e.g. a > 0 and
2a+b < 0). In the ﬁrst case, the strict NE is a rest point of adaptive dynamics that
is unattainable from nearby monomorphic populations. The population evolves to
the endpoint of S closest to the initial value of x (e.g. x evolves to 1 if x is positive
initially).
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
49
In the latter case, once x∗= 0 becomes established as the population monomor-
phism, it is vulnerable to invasion by mutants since π(x, x∗) > π(x∗, x∗) for all
nonzero x ∈S. Alternatively, an x ∈S closer to x∗than y cannot invade a dimor-
phic population that is evenly split between the strategies ±y for | y |>| x | since the
expected payoﬀto x (namely, 1
2π(x, y)+ 1
2π(x, (−y))) is less than the expected pay-
oﬀto either y or −y. In fact, x can invade this dimorphism if and only if | x |>| y |.
For either of these reasons, it is usually assumed that evolutionary stability (from
the perspective of adaptive dynamics) of an x∗∈S requires a convergence stable
equilibrium that is also a strict NE (see the following Deﬁnition 1 and Theorem 5).
A convergence stable rest point that is not a strict NE forms the basis of an initial
evolutionary branching (Geritz et al., 1998; Doebeli and Dieckmann, 2000) into a
dimorphic system.
There is some disagreement whether the strict NE condition for x∗should hold
for all x ∈S or be restricted to those strategies close to x∗. In the following, we
take the second approach and call these neighborhood strict NE to make this choice
clear. Such NE are also called ESS (Marrow et al., 1996) or are said to satisfy the
ESS Maximum Principle (Vincent and Brown, 2005). We will not use the ESS
terminology in Section 3 since the meaning of ESS is not universally accepted for
games with a continuous strategy space (Apaloo et al, 2009).
Deﬁnition 1. (Eshel, 1983) Suppose the strategy space S is a subinterval of real
numbers. An x∗∈S is a neighborhood continuously stable strategy (CSS) if there
exists an ε > 0 such that, for all x ∈S with 0 <| x −x∗|< ε, the following two
conditions hold.
(i) π(x, x∗) < π(x∗, x∗) (neighborhood strict NE condition).
(ii) There exists η > 0 (which depends on x) such that, for all x′ ∈S with
0 <| x′ −x |< η, π(x′, x) > π(x, x) if and only if | x′ −x∗|<| x −x∗| (convergence
stability condition).
The convergence stability condition of Deﬁnition 1 (ii) is used in Theorem 5
(b). The proof there shows it has the same relationship to π11 +π12 as the dynamic
approach to convergence stability through inequality (3.2).
The CSS concept,like the ESS deﬁnition of Maynard Smith (1982), is deﬁned
in terms of static payoﬀcomparisons that are meant to predict the outcome of
evolutionary dynamics. For the CSS (as well as the related notion of neighborhood
half superiority given in Deﬁnition 2), the connection to dynamic stability under
the canonical equation is summarized in Theorem 5.
Neighborhood superiority
for a pure strategy requires the extension of the payoﬀfunctions to distributions
P ∈Δ(S). Let δx be the Dirac delta distribution with full weight on the point x ∈S
and assume that individuals interact in random pairwise contests. Then π(δx, P) ≡

S π(x, y)P(dy) (also denoted by π(x, P)) is the expected payoﬀto an individual
who uses strategy x if the population strategy distribution is P and π(P, P) ≡

S π(x, P)P(dx) is the mean payoﬀof the population (Bomze and P¨otscher, 1989).
The following deﬁnition is given for general multi-dimensional strategy spaces.
The support of P is the closed set given by {x ∈S | P({y :| y −x |> ε}) > 0 for all
ε > 0}.
Deﬁnition 2.
(Cressman, 2009) Suppose the strategy space S of a symmetric
game is a subset of Rn and 0 ≤p∗< 1 is ﬁxed. Strategy x∗∈S is neighborhood
                                                                                                                    
                                                                                                               

50
ROSS CRESSMAN
p∗-superior if π(x∗, P) > π(P, P) for all P ∈Δ(S) with 1 > P({x∗}) ≥p∗and
the support of P suﬃciently close to x∗. It is neighborhood superior (respectively,
neighborhood half-superior) if p∗= 0 (respectively, p∗= 1
2). Strategy x∗∈S is
globally p∗-superior if π(x∗, P) > π(P, P) for all P ∈Δ(S) with 1 > P({x∗}) ≥p∗
Theorem 5. Suppose that S is one dimensional and x∗∈int(S) is a rest point of
adaptive dynamics (3.1) (i.e. π1(x∗, x∗) = 0).
(a) If π11 < 0, then x∗is a neighborhood strict NE. Conversely, if x∗is a neighbor-
hood strict NE, then π11 ≤0.
(b) If π11 + π12 < 0, then x∗is convergence stable. Conversely, if x∗is convergence
stable, then π11 + π12 ≤0.
(c) If π11 < 0 and π11 + π12 < 0, then x∗is a neighborhood CSS and neigh-
borhood half-superior. Conversely, if x∗is a neighborhood CSS or neighborhood
half-superior, then π11 ≤0 and π11 + π12 ≤0.
Proof. These results follow from the Taylor expansion of π(x, y) about (x∗, x∗);
namely,
π(x, y)
=
π(x∗, x∗) + π1(x∗, x∗)(x −x∗) + π2(x∗, x∗)(y −x∗)
+1
2

π11(x −x∗)2 + 2π12(x −x∗)(y −x∗) + π22(y −x∗)2
+ higher order terms.
For x∗∈int(S), ˙x = π1(x∗, x∗) at x = x∗and so π1 = 0 since x∗is a rest point of
(3.1).
(a) From the Taylor expansion, π(x, x∗) −π(x∗, x∗) =
1
2π11(x −x∗)2 up to
second order terms. Thus, x∗is a neighborhood strict NE (i.e. π(x, x∗) < π(x∗, x∗)
when x is suﬃciently close (but not equal) to x∗) if π11 < 0. Conversely, if π11 > 0,
x∗is not a neighborhood strict NE.
(b) From the Taylor expansion, π(x′, x) −π(x, x) is given by
π2(x∗, x∗)(x −x∗) + 1
2

π11(x′ −x∗)2 + 2π12(x′ −x∗)(x −x∗) + π22(x −x∗)2
−

π2(x∗, x∗)(x −x∗) + 1
2

π11(x −x∗)2 + 2π12(x −x∗)2 + π22(x −x∗)2
=
1
2π11

(x′ −x∗)2 −(x −x∗)2
+ π12(x′ −x∗−(x −x∗))(x −x∗)
=
(x′ −x)
x′ + x
2
−x∗

π11 + (x −x∗)π12

up to second order terms. If | x′−x |< η with η <| x−x∗| small, then x′+x
2
∼= x and
so π(x′, x)−π(x, x) ∼= (x′ −x) (x−x∗) (π11 + π12). Suppose | x′−x∗|<| x−x∗| (i.e.
x′ is closer to x∗than x). Then (x′ −x) (x −x∗) < 0 and so π(x′, x) −π(x, x) > 0
if π11 + π12 < 0 and π(x′, x) −π(x, x) < 0 if π11 + π12 > 0.
(c) Parts (a) and (b) combine to show that x∗is a neighborhood CSS if π11 < 0
and π11 + π12 < 0 and that, if x∗is a neighborhood CSS, then π11 ≤0 and
π11+π12 ≤0. Assume that x∗is neighborhood half-superior. With P = 1
2δx+ 1
2δx∗,
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
51
P({x∗}) = 1
2 and
π(x∗, P) −π(P, P)
=
1
2 [π(x∗, x) + π(x∗, x∗)]
−1
4 [π(x, x) + π(x, x∗) + π(x∗, x) + π(x∗, x∗)]
=
1
4 [π(x∗, x∗) + π(x∗, x) −π(x, x∗) −π(x, x)]
∼=
−1
4 (π11 + π12) (x −x∗)2.
Thus, π11 +π12 ≤0 since π(x∗, P)−π(P, P) > 0. Now take P = εδx +(1−ε)δx∗. A
similar calculation yields π(x∗, P) −π(P, P) ∼= ε (π(x∗, x∗) −π(x, x∗)) up to linear
terms in ε. Thus, x∗is a neighborhood strict NE. For the converse statements
involving neighborhood half-superiority, see Cressman (2009).
□
Except in borderline cases when π11 = 0 or π11+π12 = 0, Theorem 5 character-
izes both the stability of interior neighborhood strict NE under adaptive dynamics
and those x∗∈int(S) that are neighborhood half-superior (i.e. p∗= 1
2). On the
other hand, a neighborhood CSS need not be stable under the replicator equa-
tion. This is shown in the continuation of Example 6 below that follows a brief
development of the essential properties of this latter dynamics.
A trajectory Pt for t ≥0 is a solution of the replicator equation if the weight
Pt(B) assigned to any Borel subset B of S satisﬁes
(3.3)
˙Pt(B) =

B
(π(δx, Pt) −π(Pt, Pt)) Pt(dx).
If π is continuous, there is a unique solution given any initial P0 ∈Δ(S) (Oechssler
and Riedel, 2001). Stability under this replicator equation is typically analyzed
with respect to the weak topology for Δ(S). We are most interested in the dynamic
stability of a monomorphic population (i.e. of δx∗for some x∗∈S) when x∗is in
the support of P0. Every neighborhood of δx∗in the weak topology contains the set
of all distributions P whose support is within ε of x∗(i.e. P({x :| x−x∗|> ε}) = 0)
for some ε > 0 and so δx is in this neighborhood if | x −x∗|< ε.
Example 6. (Continued) Consider Example 6 again where now all individuals
play either x∗= 0 or a nearby strategy x (which is ﬁxed).
For this restricted
two-strategy game, the replicator equation becomes the one-dimensional dynamics
˙p = p(1 −p)(a + bp)x2 (here p is the frequency of strategy x) corresponding to the
symmetric game whose normal form is
x
x∗
x
x∗

(a + b)x2
ax2
0
0

.
Take a = −2 and b = 3 so that x∗= 0 is a CSS and also half-superior (i.e.
a < 0 and 2a + b < 0).
Since a + b > 0 and a < 0, both pure strategies are
strict NE and so locally asymptotically stable for the replicator equation applied to
this two-strategy game. However, neither is asymptotically stable for the inﬁnite-
dimensional replicator equation for the full game since, in the weak topology on
Δ(S), any neighborhood of x∗includes all probability measures whose support is
suﬃciently close to x∗. Thus, asymptotic stability of x∗requires that x∗is globally
                                                                                                                    
                                                                                                               

52
ROSS CRESSMAN
stable for the two-strategy game and so a+b ≤0. Instability in this example results
from a + b = 1 > 0.
In the following section, we extend the CSS concept to multi-dimensional strat-
egy spaces and examine stability conditions with respect to the replicator equation.
3.1.2. Multi-dimensional strategy space. The above one-dimensional model and
theory can be extended to multi-dimensional strategy spaces S that are compact
convex subsets of Rn with x∗∈S in its interior. Following the static approach of
Lessard (1990), x∗is a neighborhood CSS if it is a neighborhood strict NE that
satisﬁes condition (ii) of Deﬁnition 1 along each line through x∗. Theorem 6 then
generalizes Theorem 5 in terms of the Taylor expansion about (x∗, x∗) of the payoﬀ
function
π(x, y)
=
π(x∗, x∗) + ∇1π(x∗, x∗)(x −x∗) + ∇2π(x∗, x∗)(y −x∗)
+1
2 [(x −x∗) · A(x −x∗) + 2(x −x∗) · B(y −x∗) + (y −x∗) · C(y −x∗)]
+ higher order terms.
Here, ∇1 and ∇2 are gradient vectors with respect to x and y respectively (e.g. the
ith component of ∇1π(x∗, x∗) is ∂π(x′,x∗)
∂x′
i
|x′=x∗) and A, B, C are the n×n matrices
with ijth entries (all partial derivatives are evaluated at x∗)
Aij ≡

∂2
∂x′
j∂x′
i
π(x′, x∗)

; Bij ≡
 ∂
∂x′
i
∂
∂xj
π(x′, x)

; Cij ≡

∂
∂x′
j
∂
∂x′
i
π(x∗, x′)

.
An n × n matrix M is negative deﬁnite (respectively, negative semi-deﬁnite) if, for
all nonzero x ∈Rn, x · Mx < 0 (respectively, x · Mx ≤0).
Adaptive dynamics for multi-dimensional strategy spaces generalizing (3.1) now
has the form
(3.4)
˙x = C1(x)∇1π(y, x) |y=x
where C1(x) is an n × n covariance matrix modeling the mutation process (and
its rate) in diﬀerent directions (Leimar, 2009). We will assume that C1(x) is a
positive-deﬁnite symmetric matrix for x ∈int(S) that depends continuously on x.
System (3.4) is called the canonical equation of adaptive dynamics (when S is
multi-dimensional).
Theorem 6. (Cressman, 2009; Leimar, 2009) Suppose x∗∈int(S) is a rest point
of (3.4) (i.e. ∇1π(x∗, x∗) = 0).
(a) If A is negative deﬁnite, then x∗is a neighborhood strict NE. Conversely, if x∗
is a neighborhood strict NE, then A is negative semi-deﬁnite.
(b) If A + B is negative deﬁnite, then x∗is convergence stable for any choice of
covariance matrix C1(x) (i.e. x∗is an asymptotically stable rest point of (3.4)).
Conversely, if x∗is convergence stable for any choice of covariance matrix C1(x),
then A + B is negative semi-deﬁnite.
(c) If A and A + B are negative deﬁnite, then x∗is a neighborhood CSS and neigh-
borhood half-superior. Conversely, if x∗is a neighborhood CSS or neighborhood
half-superior, then A and A + B are negative semi-deﬁnite.
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
53
Proof. The proofs of parts (a) and (c) are similar to the corresponding proofs
of Theorem 5.
(b) Suppose A+B is negative deﬁnite and C1(x) is a positive deﬁnite symmetric
matrix that depends continuously on x. From the Taylor expansion of π(x, y), the
canonical equation (3.4) has the form
˙x = C1 (A + B) (x −x∗) + higher order terms
where C1 = C1(x∗).
Let V (x) ≡

C−1
1 (x −x∗)

· (x −x∗).
Since C−1
1
is also
positive deﬁnite and symmetric, V (x) ≥0 for all x ∈Rn with equality only at
x = x∗. Furthermore
˙V (x)
=
(C−1
1
˙x) · (x −x∗) +

C−1
1 (x −x∗)

· ˙x + higher order terms
∼=
2C−1
1 C1 (A + B) (x −x∗) · (x −x∗)
=
2(x −x∗) · (A + B) (x −x∗).
Thus, for x suﬃciently close (but not equal) to x, ˙V (x) < 0.
That is, V is a
local Lyapunov function (Hofbauer and Sigmund, 1998) and so x∗is asymptotically
stable.
Conversely, suppose that x∗is convergence stable. If A + B is not negative
semi-deﬁnite, then there exists an x ∈Rn such that x · (A + B) x > 0. Let C1 be
the orthogonal projection of Rn onto the line {ax | a ∈R} through the origin and
x. C1 is a positive semi-deﬁnite symmetric matrix. The line {x∗+ ax | a ∈R} is
invariant under the canonical equation when C1(x) = C1 and the linearization at
x∗is
˙a = (x · (A + B) x) a.
Thus the canonical equation with respect to C1 has the positive eigenvalue x ·
(A + B) x at x∗. For any positive deﬁnite covariance matrix C1 suﬃciently close to
C1, the linearization still has an eigenvalue with positive real part. Since x∗is not
stable for this choice of C1, x∗is not convergence stable (a contradiction).
□
In the continuation of Example 6, we saw that dynamic stability with respect
to the replicator equation requires more than the CSS concept. In Theorem 7, it is
NIS that takes the place of the convergence stability CSS condition and attractivity
replaces dynamics stability for the replicator equation. These are deﬁned as follows.
Deﬁnition 3. (a) (Apaloo, 1997)7 Suppose the strategy space S of a symmetric
game is a subset of Rn. Strategy x∗∈S is a neighborhood invader strategy (NIS)
if π(x∗, x) > π(x, x) for all x ∈S suﬃciently close (but not equal) to x∗.
(b) (Cressman et al., 2006) δx∗is neighborhood attracting under the replicator
equation if, for any initial distribution P0 with support suﬃciently close to x∗and
P0({x∗}) > 0, Pt converges to δx∗in the weak topology.
Theorem 7. (Cressman et al., 2006) Suppose x∗is in the interior of S and satisﬁes
∇1π(x∗, x∗) = 0.
(a) If A + 2B is negative deﬁnite, then x∗is a NIS. Conversely, if x∗is a NIS, then
A + 2B is negative semi-deﬁnite.
7A NIS is also called a ”good invader” strategy (Kisdi and Mesz´ena, 1995) and an ”invading
when rare” strategy (Courteau and Lessard, 2000).
                                                                                                                    
                                                                                                               

54
ROSS CRESSMAN
(b) If A and A + 2B are negative deﬁnite, then x∗is neighborhood superior and
neighborhood attracting under the replicator equation. Conversely, if x∗is neigh-
borhood superior or neighborhood attracting, then A and A + 2B are negative
semi-deﬁnite.
Proof. (a) From the Taylor expansion,
π(x∗, x) −π(x, x)
=
∇1π(x −x∗) + 1
2 [x −x∗) · C(x −x∗)]
−

∇1π(x −x∗) + 1
2 [(x −x∗) · (A + 2B + C)(x −x∗)]

+ higher order terms.
∼=
−1
2(x −x∗) · (A + 2B)(x −x∗).
Thus, x∗is a NIS if A + 2B is negative deﬁnite.
Conversely, if A + 2B is not
negative semi-deﬁnite, then there is an x ∈S arbitrarily close to x∗such that
π(x∗, x) −π(x, x) < 0 (i.e. x∗is not a NIS).
(b) The proof of the relationship between neighborhood superiority and the
matrices A and A+2B follows the same steps as the proof of Theorem 5 (c). Specif-
ically, the negative deﬁniteness of A is considered through strategy distributions of
the form P = εδx+(1−ε)δx∗. On the other hand, for negative deﬁniteness of A+2B,
consider P = δx for x close to x∗. Then π(x∗, P) −π(P, P) = π(x∗, x) −π(x, x)
and this is related to A + 2B by part (a).
Now suppose that A and A + 2B are negative deﬁnite. From (3.3),
˙Pt({x∗}) = (π(δx∗, Pt) −π(Pt, Pt)) Pt({x∗}).
Since the support of Pt is invariant under the replicator equation (Oechssler and
Riedel, 2001), Pt({x∗}) is increasing if P0 has support suﬃciently close to x∗. From
this it follows that Pt converges to δx∗in the weak topology (see Cressman (2010)
for details of this and of the converse statement).
□
Remark 9. The analysis in Cressman et al. (2006) shows that one must be careful
in extending the statements of Theorem 7, part b, from neighborhood attractivity
to asymptotic stability or from P0({x∗}) > 0 to x∗in the support of P0, especially
if the payoﬀfunction is not symmetric (i.e. π(x, y) ̸= π(y, x) for some x, y ∈S).
In fact, there remain open problems in these cases.
On the other hand, there
are examples that show neither negative deﬁniteness nor negative semi-deﬁniteness
provide complete characterizations in any part of Theorems 6 and 7. For example,
there are borderline cases with x∗a neighborhood strict NE and A + 2B negative
semi-deﬁnite for which x∗is a NIS in one case but not in the other (Cressman et
al., 2006).
3.2. Asymmetric games with continuous strategy spaces. The above
theory of multi-dimensional CSS and NIS as well as their connections to evolution-
ary dynamics have been extended to asymmetric games with continuous strategy
spaces (Cressman, 2009, 2010). When there are two roles, it is shown there that
the CSS and NIS can be characterized (excluding borderline cases) by payoﬀcom-
parisons similar to those found for the two-species ESS when both roles have a
ﬁnite number of strategies (see Theorem 4 and Deﬁnition 4). In this section, we
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
55
will assume that the continuous strategy sets S and T for the two roles are both
one-dimensional compact intervals and that payoﬀfunctions have continuous par-
tial derivatives up to second order in order to avoid technical and/or notational
complications.
For (x, y) ∈S × T, let π1(x′; x, y) (respectively, π2(y′; x, y)) be the payoﬀto a
player in role 1 (respectively, in role 2) using strategy x′ ∈S (respectively y′ ∈T)
when the population is monomorphic at (x, y). Note that π1 has a diﬀerent meaning
here than in Section 3.1 where it was used to denote a partial derivative. With this
terminology, the canonical equation of adaptive dynamics (c.f. (3.1)) becomes
(3.5)
˙x = k1(x, y) ∂
∂x′ π1(x′; x, y) |x′=x
˙y = k2(x, y) ∂
∂y′ π2(y′; x, y) |y′=y
where ki(x, y) for i = 1, 2 are positive continuous functions of (x, y). At an interior
rest point (x∗, y∗) of (3.5),
∂π1
∂x′ = ∂π2
∂y′ = 0.
In particular, if (x∗, y∗) is a neighborhood strict NE (i.e. if π1(x; x∗, y∗) < π1(x∗; x∗, y∗)
and π2(y; x∗, y∗) < π2(y∗; x∗, y∗) for all x and y suﬃciently close but not equal to x
and y respectively) in the interior of S × T, then it is a rest point (x∗, y∗) of (3.5).
(x∗, y∗) is called convergence stable (or strongly convergence stable as in Leimar,
2009) if it is asymptotically stable under (3.5) for any choice of k1 and k2.
The characterizations of these concepts in the following theorem are given in
terms of the linearization of (3.5) about (x∗, y∗); namely,
(3.6)

˙x
˙y

=

k1(x∗, y∗)
0
0
k2(x∗, y∗)
 
A + B
C
D
E + F
 
x −x∗
y −y∗

where
A
≡
∂2
∂x′∂x′ π1(x′; x∗, y∗); B ≡
∂
∂x′
∂
∂xπ1(x′; x, y∗); C ≡
∂
∂x′
∂
∂y π1(x′; x∗, y)
D
≡
∂
∂y′
∂
∂xπ2(y′; x, y∗); E ≡
∂
∂y′
∂
∂y π2(y′; x∗, y); F ≡
∂2
∂y′∂y′ π2(y′; x∗, y∗)
and all partial derivatives are evaluated at the equilibrium.
Theorem 8. (Cressman, 2010) Suppose (x∗, y∗) is a rest point (x∗, y∗) of (3.5) in
the interior of S × T.
(a) (x∗, y∗) is a neighborhood strict NE if A and F are negative. Conversely, if
(x∗, y∗) is a neighborhood NE, then A and F are non-positive.
(b) (x∗, y∗) is convergence stable if, for all nonzero (x, y) ∈R2, either x((A + B) x+
Cy) < 0 or y (Dx + (E + F) y) < 0. Conversely, if (x∗, y∗) is convergence stable,
then either x ((A + B) x + Cy) ≤0 or y (Dx + (E + F) y) ≤0 for all (x, y) ∈R2.
(c) (x∗, y∗) is convergence stable if A + B < 0, E + F < 0 and (A + B) (E + F) >
CD. Conversely, if (x∗, y∗) is convergence stable, then A + B ≤0, E + F ≤0 and
(A + B) (E + F) ≥CD.
Proof. (a) These statements are straightforward consequences of the Taylor
expansion of the payoﬀfunctions π1(x′; x, y) and π2(y′; x, y) about (x∗, y∗).
                                                                                                                    
                                                                                                               

56
ROSS CRESSMAN
(b) (x∗, y∗) is convergence stable if both eigenvalues of the linearization (3.6) have
negative real parts for any choice of positive k1(x∗, y∗) and k2(x∗, y∗). This lat-
ter condition holds if and only if the trace is negative (i.e. k1(x∗, y∗) (A + B) +
k2(x∗, y∗) (E + F) < 0) and the determinant is positive
(i.e. k1(x∗, y∗)k2(x∗, y∗)[(A + B) (E + F) −DC] > 0).
Assume that either x ((A + B) x + Cy) < 0 or y (Dx + (E + F) y) < 0 for all
nonzero (x, y) ∈R2.
In particular, with (x, y) = (x, 0), we have A + B < 0.
Analogously E + F < 0 and so the trace is negative. For a ﬁxed nonzero y, let
x ≡−
C
A+B y. Then (A + B) x + Cy = 0 and so y (Dx + (E + F) y) < 0. That is,
y

−CD
A + B y + (E + F) y

= (A + B) (E + F) −CD
A + B
y2
is negative and this implies the determinant is positive. Thus, (x∗, y∗) is conver-
gence stable.
Conversely, assume that (x∗, y∗) is convergence stable. Then, the trace must be
non-positive and the determinant non-negative for any choice of positive k1(x∗, y∗)
and k2(x∗, y∗) (otherwise, there is an eigenvalue with positive real part). In partic-
ular, A + B ≤0 and E + F ≤0.
Case 1. If CD ≤0, then either xCy ≤0 or yDx ≤0. Thus, either x ((A + B) x + Cy) ≤
0 or y (Dx + (E + F) y) ≤0 for all (x, y) ∈R2.
Case 2. If CD > 0, we may assume without loss of generality that C > 0 and
D > 0. Suppose that x ((A + B) x + Cy) > 0. Then xy > −(A+B)x2
C
> 0. Thus
y (Dx + (E + F) y)
=
y
x(Dx2 + (E + F)xy)
<
y
x(Dx2 −(A + B)(E + F)x2
C
) ≤0.
(c) These statements follow from the arguments used to prove part b.
□
As in Section 3.1 for symmetric games, a neighborhood CSS is a neighborhood
strict NE that is convergence stable (when borderline cases are excluded). For one-
dimensional strategy spaces, S and T, parts b and c of Theorem 8 give equivalent
conditions for convergence stability. Although the inequalities in part c are the
easiest to use in practical examples, it is the approach in part b that is most
directly tied to the theory of CSS, NIS and neighborhood superiority as well as their
connections to evolutionary dynamics, especially as the strategy spaces become
multi-dimensional. It is again neighborhood superiority according to part a of the
following deﬁnition that uniﬁes this theory (see Theorem 9 that assumes borderline
cases are excluded).
Deﬁnition 4. (Cressman, 2010) Suppose (x∗, y∗) is in the interior of S × T.
(a) Fix 0 ≤p∗< 1. Strategy pair (x∗, y∗) is neighborhood p∗−superior if
(3.7)
either π1(x∗; P, Q) > π1(P; P, Q) or π2(y∗; P, Q) > π2(Q; P, Q)
for all (P, Q) ∈Δ(S) × Δ(T) with 1 ≥P({x∗}) ≥p∗, 1 ≥Q({y∗}) ≥p∗and the
support of (P, Q) suﬃciently close (but not equal) to (x∗, y∗). (x∗, y∗) is neighbor-
hood half-superior if p∗= 1
2. (x∗, y∗) is neighborhood superior if p∗= 0. (x∗, y∗)
is (globally) p∗−superior if the support of (P, Q) in (3.7) is an arbitrary subset of
S × T (other than {(x∗, y∗)}).
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
57
(b) Strategy pair (x∗, y∗) is a neighborhood invader strategy (NIS) if, for all (x, y)
suﬃciently close (but not equal) to (x∗, y∗), either π1(x∗; x, y) > π1(x; x, y)
or
π2(y∗; x, y) > π2(y; x, y).
Theorem 9. (Cressman, 2010) Suppose that (x∗, y∗) is in the interior of S × T.
(a) (x∗, y∗) is a neighborhood CSS if and only if it is neighborhood half-superior.
(b) (x∗, y∗) is a neighborhood strict NE and NIS if and only if it is neighborhood
superior.
(c) Consider evolution under the replicator equation generalizing (3.3) to asym-
metric games and initial population distributions (P0, Q0) ∈Δ(S) × Δ(T) that
satisfy P0({x∗})Q0({y∗}) > 0. If (x∗, y∗) is a strict neighborhood NE and a NIS,
then (Pt, Qt) converges to (δx∗, δy∗) in the weak topology whenever the support of
(P0, Q0) is suﬃciently close to (x∗, y∗). Conversely, if (Pt, Qt) converges to (δx∗, δy∗)
in the weak topology for every (P0, Q0) with support suﬃciently close to (x∗, y∗),
then (x∗, y∗) is a neighborhood strict NE and NIS.
Theorem 8 is the (two-role) asymmetric counterpart of Theorem 5 for sym-
metric games when the continuous strategy spaces are one dimensional. Deﬁnition
4 and Theorem 9 generalize Deﬁnition 3 and Theorems 6 and 7 of Section 3.1 to
these asymmetric games. Based on a thorough analysis of the Taylor expansions
of the two payoﬀfunctions, their statements remain correct when S and T are
multi-dimensional.
4. Conclusion
The static payoﬀcomparisons (e.g. the ESS conditions) introduced by Maynard
Smith (1982) to predict the behavioral outcome of evolution in symmetric games
with ﬁnitely many strategies have been extended in many directions during the
intervening years. These include biological extensions to multiple species and to
population games as well as the equally important extensions to predict rational
individual behavior in human conﬂict situations. As is apparent from this chapter,
there is a complex relationship between these static conditions and evolutionary
stability of the underlying dynamical system.
This chapter has emphasized evolutionary stability in (symmetric or asymmet-
ric) extensive form games and games with continuous strategy spaces under the
deterministic replicator equation that is based on random pairwise interactions.
Evolutionary stability is also of much current interest for other game-theoretic
models such as those that incorporate stochastic eﬀects due to ﬁnite populations;
models with assortative (i.e. non-random) interactions (e.g. games on graphs);
models with multi-player interactions (e.g. public goods games). As the evolution-
ary theory behind these (and other) models is a rapidly expanding area of current
research, it is impossible to know in what guise the evolutionary stability conditions
will emerge in future applications. On the other hand, it is certain that Maynard
Smith’s original idea will continue to play a central role.
                                                                                                                    
                                                                                                               

58
ROSS CRESSMAN
References
[1] Apaloo, J. (1997) Revisiting strategic models of evolution: The concept of neighborhood
invader strategies. Theor. Pop. Biol. 52, 71-77.
[2] Apaloo, J., J.S. Brown and T.L. Vincent (2009) Evolutionary game theory: ESS, convergence
stability, and NIS. Evol. Ecol. Res. 11, 489-515.
[3] Bomze, I.M. (1991) Cross entropy minimization in uninvadable states of complex populations.
J. Math. Biol. 30, 73-87.
[4] Bomze, I.M. and B.M. P¨otscher (1989) Game Theoretical Foundations of Evolutionary Sta-
bility. Springer, Berlin.
[5] Chamberland, M. and R. Cressman (2000) An example of dynamic (in)consistency in sym-
metric extensive form evolutionary games. Games and Econ. Behav. 30, 319-326.
[6] Christiansen, F.B. (1991) On conditions for evolutionary stability for a continuously varying
character. Amer. Nat. 138, 37-50.
[7] Courteau, J. and S. Lessard (2000) Optimal sex ratios in structured populations. J. Theor.
Biol., 207, 159-175.
[8] Cressman, R. (2003) Evolutionary Dynamics and Extensive Form Games. MIT Press, Cam-
bridge, MA.
[9] Cressman, R. (2009) Continuously stable strategies, neighborhood superiority and two-player
games with continuous strategy spaces. Int. J. Game Theory 38, 221-247.
[10] Cressman, R. (2010) CSS, NIS and dynamic stability for two-species behavioral models with
continuous trait spaces. J. Theor. Biol. 262, 80-89.
[11] Cressman, R., J. Garay and J. Hofbauer (2001) Evolutionary stability concepts for N-species
frequency-dependent interactions. J. Theor. Biol. 211, 1-10.
[12] Cressman, R., J. Hofbauer and F. Riedel (2006) Stability of the replicator equation for a
single-species with a multi-dimensional continuous trait space. J. Theor. Biol. 239, 273-288.
[13] Dercole, F. and S. Rinaldi (2008) Analysis of Evolutionary Processes. The Adaptive Dynamics
Approach and its Applications. Princeton University Press, Princeton.
[14] Dieckmann, U. and R. Law (1996) The dynamical theory of coevolution: a derivation from
stochastic ecological processes. J. Math. Biol. 34, 579-612.
[15] Doebeli, M. and U. Dieckmann (2000) Evolutionary branching and sympatric speciation
caused by diﬀerent types of ecological interactions. Am. Nat. 156, S77-S101.
[16] Eshel, I. (1983) Evolutionary and continuous stability. J. Theor. Biol. 103, 99-111.
[17] Fretwell, D.S. and H.L. Lucas (1969) On territorial behavior and other factors inﬂuencing
habitat distribution in birds. Acta Biotheoretica 19, 16-32.
[18] Geritz, S.A.H., ´E. Kisdi, G. Mesz´ena and J.A.J. Metz (1998) Evolutionarily singular strategies
and the adaptive growth and branching of the evolutionary tree. Evol. Ecol. 12, 35-57.
[19] Hofbauer, J. and K. Sigmund (1998) Evolutionary Games and Population Dynamics. Cam-
bridge University Press, Cambridge.
[20] Kisdi, ´E. and G. Mesz´ena (1995) Life histories with lottery competition in a stochastic envi-
ronment: ESSs which do not prevail. Theor. Pop. Biol. 47, 191-211.
[21] Krivan, V., R. Cressman and C. Schneider (2008) The ideal free distribution: A review and
synthesis of the game theoretic perspective. Theor. Pop. Biol. 73, 403-425.
[22] Kuhn, H. (1953) Extensive games and the problem of information. In H. Kuhn and A. Tucker,
eds., Contributions to the Theory of Games II. Annals of Mathematics 28. Princeton Uni-
versity Press, Princeton.
[23] Leimar, O. (2009). Multidimensional convergence stability. Evol. Ecol. Res. 11, 191-208..
[24] Lessard, S. (1990) Evolutionary stability: one concept, several meanings. Theor. Pop. Biol.
37, 159-170.
[25] Marrow P., U. Dieckmann and R. Law (1996) Evolutionary dynamics of predator-prey sys-
tems: an ecological perspective. J. Math. Biol. 34, 556-578.
[26] Maynard Smith, J. (1982) Evolution and the Theory of Games. Cambridge University Press,
Cambridge.
[27] Oechssler, J. and F. Riedel (2001) Evolutionary dynamics on inﬁnite strategy spaces. Econ.
Theory 17, 141-162.
[28] Rosenthal, R. (1981) Games of perfect information, predatory pricing and the chain-store
paradox. J. Econ. Theor. 25, 92-100.
[29] Selten, R. (1978) The chain-store paradox. Theory and Decision 9, 127-159.
                                                                                                                    
                                                                                                               

BEYOND THE SYMMETRIC NORMAL FORM
59
[30] Selten, R. (1983) Evolutionary stability in extensive two-person games. Math. Soc. Sci. 5,
269-363.
[31] Selten, R. (1988) Evolutionary stability in extensive two-person games - correction and further
development. Math. Soc. Sci. 16, 223-266.
[32] van Damme, E. (1991) Stability and Perfection of Nash Equilibria (2nd Edition). Springer-
Verlag, Berlin.
[33] Vincent, T.L. and J.S. Brown (2005) Evolutionary Game Theory, Natural Selection and
Darwinian Dynamics, Cambridge University Press.
[34] von Neumann, J. and O. Morgenstern (1944) Theory of Games and Economic Behavior.
Princeton University Press, Princeton.
[35] Weibull, J. (1995) Evolutionary Game Theory. MIT Press, Cambridge, MA.
Department of Mathematics, Wilfrid Laurier University Waterloo, Ontario N2L
3C5 Canada
E-mail address: rcressman@wlu.ca
                                                                                                                    
                                                                                                               

This page intentionally left blank 
                                                                                                                    
                                                                                                               

Proceedings of Symposia in Applied Mathematics
Volume 69, 2011
Deterministic Evolutionary Game Dynamics
Josef Hofbauer
Abstract. This is a survey about continuous time deterministic evolutionary
dynamics for ﬁnite games. In particular, six basic dynamics are described: the
replicator dynamics, the best response dynamics, the Brown–von Neumann–
Nash dynamics, the Smith dynamics, and the payoﬀprojection dynamics.
Special classes of games, such as stable games, supermodular games and part-
nership games are discussed. Finally a general nonconvergence result is pre-
sented.
1. Introduction: Evolutionary Games
We consider a large population of players, with a ﬁnite set of pure strate-
gies {1, . . . , n}. xi denotes the frequency of strategy i. Δn = {x ∈Rn : xi ≥
0, n
i=1 xi = 1} is the (n −1)–dimensional simplex which will often be denoted by
Δ if there is no confusion.
The payoﬀto strategy i in a population x is ai(x), with ai : Δ →R a continuous
function (population game). The most important special case is that of a symmetric
two person game with n × n payoﬀmatrix A = (aij); with random matching this
leads to the linear payoﬀfunction ai(x) = 
j aijxj = (Ax)i.
ˆx ∈Δ is a Nash equilibrium (NE) iﬀ
(1.1)
ˆx·a(ˆx) ≥x·a(ˆx)
∀x ∈Δ.
Occasionally I will also look at bimatrix games (played between two player
populations), with n × m payoﬀmatrices A, B, or at N person games.
Evolutionarily stable strategies. According to Maynard Smith [36], a mixed
strategy ˆx ∈Δ is an evolutionarily stable strategy (ESS) if
(i)
x·Aˆx
≤
ˆx·Aˆx
∀x ∈Δ,
and
(ii)
x·Ax
<
ˆx·Ax
for x ̸= ˆx, if there is equality in (i).
The ﬁrst condition (i) is simply Nash’s deﬁnition (1.1) for an equilibrium. It is easy
to see that ˆx is an ESS, iﬀˆx·Ax > x·Ax holds for all x ̸= ˆx in a neighbourhood
of ˆx. This property is called locally superior in [61]. For an interior equilibrium
2000 Mathematics Subject Classiﬁcation. Primary 91A22.
I thank Karl Sigmund for comments, and Bill Sandholm and Francisco Franchetti for pro-
ducing the ﬁgures with their Dynamo software [50].
c
⃝2011 JH
61
                                           
                                                                                                                    
                                                                                                               

62
JOSEF HOFBAUER
ˆx, the equilibrium condition ˆx·Aˆx = x·Aˆx for all x ∈Δ together with (ii) implies
(ˆx −x)·A(x −ˆx) > 0 for all x and hence
(1.2)
z·Az < 0
∀z ∈Rn
0 = {z ∈Rn :

i
zi = 0}
with
z ̸= 0.
Condition (1.2) says that the mean payoﬀx·Ax is a strictly concave function on
Δ. Conversely, games satisfying (1.2) have a unique ESS (possibly on the bound-
ary) which is also the unique Nash equilibrium of the game. The slightly weaker
condition
(1.3)
z·Az ≤0
∀z ∈Rn
0
includes also the limit cases of zero–sum games and games with an interior equi-
librium that is a ‘neutrally stable’ strategy (i.e., equality is allowed in (ii)). Games
satisfying (1.3) need no longer have a unique equilibrium, but the set of equilibria
is still a nonempty convex subset of Δ.
For the rock–scissors–paper game with (a cyclic symmetric) pay-oﬀmatrix
(1.4)
A =
⎛
⎝
0
−b
a
a
0
−b
−b
a
0
⎞
⎠
with a, b > 0
with the unique Nash equilibrium E = ( 1
3, 1
3, 1
3) we obtain the following: for z ∈R3
0,
z1 + z2 + z3 = 0,
z·Az = (a −b)(z1z2 + z2z3 + z1z3) = b −a
2
[z2
1 + z2
2 + z2
3].
Hence for 0 < b < a, the game is negative deﬁnite, and E is an ESS. On the other
hand, if 0 < a < b, the game is positive deﬁnite:
(1.5)
z·Az > 0
∀z ∈Rn
0 \ {0},
the equilibrium E is not evolutionarily stable, indeed the opposite, and might be
called an ‘anti–ESS’.
For a classical game theorist, all RPS games are the same. There is a unique
Nash equilibrium, even a unique correlated equilibrium [60], for any a, b > 0. In
evolutionary game theory the dichotomy a < b versus a > b is crucial, as we will
see in the next sections, in particular in the ﬁgures 1–6.
2. Game Dynamics
In this section I present 6 special (families of) game dynamics. As we will see
they enjoy a particularly nice property: Interior ESS are globally asymptotically
stable.
The presentation follows largely [22, 24, 28].
1. Replicator dynamics
2. Best response dynamics
3. Logit dynamics (and other smoothed best reply dynamics)
4. Brown–von Neumann–Nash dynamics
5. Payoﬀcomparison dynamics
6. Payoﬀprojection dynamics
                                                                                                                    
                                                                                                               

DETERMINISTIC EVOLUTIONARY GAME DYNAMICS
63
Figure 1. Replicator dynamics for Rock-Paper-Scissors games:
a > b versus a < b
Replicator dynamics.
(2.1)
˙xi = xi (ai(x) −x·a(x)) ,
i = 1, . . . , n
(REP)
In the zero-sum version a = b of the RSP game, all interior orbits are closed,
circling around the interior equilibrium E, with x1x2x3 as a constant of motion.
Theorem 2.1. In a negative deﬁnite game satisfying (1.2), the unique Nash
equilibrium p ∈Δ is globally asymptotically stable for (REP). In particular, an
interior ESS is globally asymptotically stable.
On the other hand, in a positive deﬁnite game satisfying (1.5) with an interior
equilibrium p, i.e., an anti-ESS, p is a global repellor. All orbits except p converge
to the boundary bd Δ.
The proof uses V (x) =  xpi
i
as a Lyapunov function.
For this and further results on (REP) see Sigmund’s chapter [53], and [9, 26, 27,
48, 61].
Best response dynamics. In the best response dynamics1 [14, 35, 19] one
assumes that in a large population, a small fraction of the players revise their
strategy, choosing best replies2 BR(x) to the current population distribution x.
(2.2)
˙x ∈BR(x) −x.
Since best replies are in general not unique, this is a diﬀerential inclusion rather
than a diﬀerential equation. For continuous payoﬀfunctions ai(x) the right hand
side is a non-empty convex, compact subset of Δ which is upper semi-continuous
in x. Hence solutions exist, and they are Lipschitz functions x(t) satisfying (2.2)
for almost all t ≥0, see [1].
For games with linear payoﬀ, solutions can be explicitely constructed as piece-
wise linear functions, see [9, 19, 27, 53].
For interior NE of linear games we have the following stability result [19].
1For bimatrix games, this dynamics is closely related to the ‘ﬁctitious play’ by Brown [6],
see Sorin’s chapter [56].
2Recall the set of best replies BR(x) = Argmaxy∈Δ y·a(x) = {y ∈Δ : y·a(x) ≥z·a(x)∀z ∈
Δ} ⊆Δ.
                                                                                                                    
                                                                                                               

64
JOSEF HOFBAUER
Figure
2. Best
response
dynamics
for
Rock-Paper-Scissors
games, left one: a ≥b, right one: a < b
Let B = {b ∈bdΔn : (Ab)i = (Ab)j for all i, j ∈supp(b)} denote the set of all
rest points of (REP) on the boundary. Then the function3
(2.3)
w(x) = max
	 
b∈B
b·Ab u(b) : u(b) ≥0,

b∈B
u(b) = 1,

b∈B
u(b)b = x

can be interpreted in the following way. Imagine the population in state x being
decomposed into subpopulations of size u(b) which are in states b ∈B, and call this
a B–segregation of b. Then w(x) is the maximum mean payoﬀthe population x
can obtain by such a B–segregation. It is the smallest concave function satisfying
w(b) ≥b·Ab for all b ∈B.
Theorem 2.2. The following three conditions are equivalent:
(a) There is a vector p ∈Δn, such that p·Ab > b·Ab holds for all b ∈B.
(b) V (x) = maxi(Ax)i −w(x) > 0
for all
x ∈Δn.
(c) There exist a unique interior equilibrium ˆx, and ˆx·Aˆx > w(ˆx).
These conditions imply:
The equilibrium ˆx is reached in ﬁnite and bounded time by any BR path.
The proof consists in showing that the function V from (b) decreases along the
solutions of the BR dynamics (2.2).
In the rock–scissors–paper game the set B reduces to the set of pure strategies,
the Lyapunov function is simply V (x) = maxi(Ax)i and satisﬁes ˙V = −V (except at
the NE), see [13]. Since minx∈Δ V (x) = V (ˆx) = ˆx·Aˆx = a−b
3
> 0 the exponentially
decreasing V (x(t)) reaches this minimum value after a ﬁnite time. So all orbits
reach the NE in ﬁnite time.
If p ∈int Δ is an interior ESS then condition (a) holds not only for all b ∈B
but for all b ̸= p. In this case the Lyapunov function V (x) = maxi(Ax)i −x·Ax ≥0
can also be used. This leads to
Theorem 2.3. [22] For a negative semideﬁnite game (1.3) the convex set of
its equilibria is globally asymptotically stable for the best–response dynamics.4
3If B is inﬁnite it is suﬃcient to take the ﬁnitely many extreme points of its convex pieces.
4Using the tools in Sorin’s chapter [56, section 1] this implies also global convergence of
(discrete time) ﬁctitious play. A similar result holds also for nonlinear payoﬀfunctions, see [24].
                                                                                                                    
                                                                                                               

DETERMINISTIC EVOLUTIONARY GAME DYNAMICS
65
Proof. The Lyapunov function V (x) = maxi(Ax)i −x·Ax ≥0 satisﬁes ˙V =
˙x·A ˙x −˙x·Ax < 0 along piecewise linear solutions outside the set of NE.
□
Note that for zero–sum games, ˙V = −V , so V (x(t)) = e−tV (x(0)) →0 as
t →∞, so x(t) converges to the set of NE. For negative deﬁnite games, ˙V < −c−V
for some c > 0 and hence x(t) reaches the NE in ﬁnite time.
For positive deﬁnite RSP games (b > a), V (x) = maxi(Ax)i still satisﬁes
˙V = −V . Hence the NE is a repeller and all orbits (except the constant one at
the NE) converge to the set where V (x) = maxi(Ax)i = 0 which is a closed orbit
under the BR dynamics. It is called the Shapley triangle of the game, as a tribute
to [52], see ﬁgure 2 (right). In this case the equilibrium payoﬀa−b
3
is smaller than
0, the payoﬀfor a tie. This is the intuitive reason why the population tries to get
away from the NE and closer to the pure states.
Interestingly, the times averages of the solutions of the replicator dynamics
approach for b > a the very same Shapley triangle, see [13]. The general reason for
this is explained in Sorin’s chapter [56, ch. 3].
For similar cyclic games with n = 4 strategies several Shapley polygons can
coexist, see [16]. For n ≥5 chaotic dynamics is likely to occur.
Smoothed best replies. The BR dynamics can be approximated by smooth
dynamics such as the logit dynamics [5, 12, 31]
(2.4)
˙x = L(Ax
ε ) −x
with
L : Rn →Δ,
Lk(u) =
euk

j euj .
with ε > 0. As ε →0, this approaches the best reply dynamics, and every family
of rest points5 ˆxε accumulates in the set of Nash equilibria.
There are (at least) two ways to motivate and generalize this ‘smoothing’.
Whereas BR(x) is the set of maximizers of the linear function z 	→
i ziai(x)
on Δ, consider bεv(x), the unique maximizer of the function z 	→
i ziai(x)+εv(z)
on int Δ, where v : int Δ →R is a strictly concave function such that |v′(z)| →∞as
z approaches the boundary of Δ. If v is the entropy − zi log zi, the corresponding
smoothed best reply dynamics
(2.5)
˙x = bεv(x) −x
reduces to the logit dynamics (2.4) above [12]. Another choice6 is v(x) = 
i log xi
used by Hars´anyi [17] in his logarithmic games.
Another way to perturb best replies are stochastic perturbations. Let ε be a
random vector in Rn distributed according to some positive density function. For
z ∈Rn, let
(2.6)
Ci(z) = Prob(zi + εi ≥zj + εj
∀j),
and b(x) = C(a(x)) the resulting stochastically perturbed best reply function. It
can be shown [23] that each such stochastic perturbation can be represented by
a deterministic perturbation as described before. The main idea is that there is a
5These are the quantal response equilibria of McKelvey and Palfrey [37].
6A completely diﬀerent approximate best reply function appears already in Nash’s Ph.D. the-
sis [40], in his ﬁrst proof of the existence of equilibria by Brouwer’s ﬁxed point theorem.
                                                                                                                    
                                                                                                               

66
JOSEF HOFBAUER
Figure 3. Logit dynamics for Rock-Paper-Scissors games: a ≥b
versus a < b
potential function W : Rn →R, with ∂W
∂ai = Ci(a) which is convex, and has −v as
its Legendre transform. If the (εi) are i.i.d. with the extreme value distribution
F(x) = exp(−exp(−x)) then C(a) = L(a) is the logit choice function and we obtain
(2.4).
Theorem 2.4. [22] In a negative semideﬁnite game (1.3), the smoothed BR
dynamics (2.5) (including the logit dynamics) has a unique equilibrium ˆxε. It is
globally asymptotically stable.
The proof uses the Lyapunov function
V (x) = πεv(bεv(x), x) −πεv(x, x) ≥0
with
πεv(z, x) = z·a(x) + εv(z).
I will return to these perturbed dynamics in section 5. For more information
on the logit dynamics see Sorin’s chapter [56] and references therein, and [43].
The Brown–von Neumann–Nash dynamics. The Brown–von Neumann–
Nash dynamics (BNN) is deﬁned as
(2.7)
˙xi = ˆai(x) −xi
n

j=1
ˆaj(x),
where
(2.8)
ˆai(x) = [ai(x) −x·a(x)]+
(with u+ = max(u, 0)) denotes the positive part of the excess payoﬀfor strategy i.
This dynamics is closely related to the continuous map f : Δ →Δ deﬁned by
(2.9)
fi(x) =
xi + hˆai(x)
1 + h n
j=1 ˆaj(x)
which Nash [41] used (for h = 1) to prove the existence of equilibria, by applying
Brouwer’s ﬁxed point theorem: It is easy to see that ˆx is a ﬁxed point of f iﬀit is a
rest point of (2.7) iﬀˆai(ˆx) = 0 for all i, i.e. iﬀˆx is a Nash equilibrium of the game.
Rewriting the Nash map (2.9) as a diﬀerence equation, and taking the limit
limh→0
f(x)−x
h
yields (2.7).
This diﬀerential equation was considered earlier by
                                                                                                                    
                                                                                                               

DETERMINISTIC EVOLUTIONARY GAME DYNAMICS
67
Figure 4. BNN dynamics for Rock-Paper-Scissors games: a ≥b
versus a < b
Brown and von Neumann [7] in the special case of zero–sum games, for which they
proved global convergence to the set of equilibria.
In contrast to the best reply dynamics, the BNN dynamics (2.7) is Lipschitz
(if payoﬀs are Lipschitz) and hence has unique solutions.
Equation (2.7) deﬁnes an ’innovative better reply’ dynamics. A strategy not
present that is a best (or at least a better) reply against the current population will
enter the population.
Theorem 2.5. [7, 22, 24, 42] For a negative semideﬁnite game (1.3), the
convex set of its equilibria is globally asymptotically stable for the BNN dynamics
(2.7).
The proof uses the Lyapunov function V = 1
2

i ˆai(x)2, since V (x) ≥0 with
equality at NE, and
˙V = ˙x·A ˙x −˙x·Ax

i
ˆai(x) ≤0,
with equality ony at NE.
Dynamics based on pairwise comparison. The BNN dynamics is a proto-
type of an innovative dynamics. A more natural way to derive innovative dynamics
is the following,
(2.10)
˙xi =

j
xjρji −xi

j
ρij,
in the form of an input–output dynamics. Here xiρij is the ﬂux from strategy i to
strategy j, and ρij = ρij(x) ≥0 is the rate at which an i player switches to the j
strategy.
                                                                                                                    
                                                                                                               

68
JOSEF HOFBAUER
Figure 5. Smith’s pairwise diﬀerence dynamics for Rock-Paper-
Scissors games: a ≥b versus a < b
A natural assumption on the revision protocol7 ρ is
ρij > 0 ⇔aj > ai,
and
ρij ≥0.
Here switching to any better reply is possible, as opposed to the BR dynamics where
switching is only to the optimal strategies (usually there is only one of them), or the
BNN dynamics where switching occurs only to strategies better than the population
average.
An important special case is when the switching rate depends on the payoﬀ
diﬀerence only, i.e.,
(2.11)
ρij = φ(aj −ai)
where φ is a function with φ(u) > 0 for u > 0 and φ(u) = 0 for u ≤0. The resulting
dynamics (2.10) is called pairwise comparison dynamics. The natural choice seems
φ(u) = u+, given by the proportional rule
(2.12)
ρij = [aj −ai]+.
The resulting pairwise diﬀerence dynamics (PD)
(2.13)
˙xi =

j
xj[ai −aj]+ −xi

j
[aj −ai]+
was introduced by Michael J. Smith [55] in the transportation literature as a dy-
namic model for congestion games. He also proved the following global stability
result.
Theorem 2.6. [55] For a negative semideﬁnite game (1.3), the convex set of
its equilibria is globally asymptotically stable for the PD dynamics (2.13).
7All the basic dynamics considered so far can be written in the form (2.10) with a suitable
revision protocol ρ (with some obvious modiﬁcation in the case of the multi–valued BR dynamics).
Given the revision protocol ρ, the payoﬀfunction a, and a ﬁnite population size N, there is a
natural ﬁnite population model in terms of a Markov process on the grid {x ∈Δ : Nx ∈Zn}. The
diﬀerential equation (2.10) provides a very good approximation of the behavior of this stochastic
process, at least over ﬁnite time horizons and for large population sizes. For all this see Sandholm’s
chapter [49].
                                                                                                                    
                                                                                                               

DETERMINISTIC EVOLUTIONARY GAME DYNAMICS
69
The proof uses the Lyapunov function V (x) = 
i,j xj[ai(x) −aj(x)]2
+, by
showing V (x) ≥0 and V (x) = 0 iﬀx is a NE, and
2 ˙V = ˙x·A ˙x +

k,j
xkρkj

i
(ρ2
ji −ρ2
ki) < 0
except at NE. This result extends to pairwise comparison dynamics (2.10,2.11), see
[24].
The payoﬀprojection dynamics. A more recent proof of the existence of
Nash equilibria, due to G¨ul–Pearce–Stacchetti [15] uses the payoﬀprojection map
Phx = ΠΔ(x + ha(x))
Here h > 0 is ﬁxed and ΠΔ : Rn →Δ is the projection onto the simplex Δ, assigning
to each vector u ∈Rn the point in the compact convex set Δ which is closest to u.
Now ΠΔ(z) = y iﬀfor all x ∈Δ, the angle between x−y and z −y is obtuse, i.e., iﬀ
(x −y)·(z −y) ≤0 for all x ∈Δ. Hence, Phˆx = ˆx iﬀfor all x ∈Δ, (x −ˆx)·a(ˆx) ≤0,
i.e., iﬀˆx is a Nash equilibrium. Since the map Ph : Δ →Δ is continuous Brouwer’s
ﬁxed point theorem implies the existence of a Nash equilibrium.
Writing this map as a diﬀerence equation, we obtain in the limit h →0
(2.14)
˙x = lim
h→0
ΠΔ(x + ha(x)) −x
h
= ΠT (x)a(x)
with
T(x) = {ξ ∈Rn :

i
ξi = 0, ξi ≥0 if xi = 0}
being the cone of feasible directions at x into Δ.
This is the payoﬀprojection dynamics (PP) of Lahkar and Sandholm [34].
The latter equality in (2.14) and its dynamic analysis use some amount of convex
analysis, in particular the Moreau decomposition, see [1, 34].
For x ∈int Δ we obtain
˙xi = ai(x) −1
n

k
ak(x)
which, for a linear game, is simply a linear dynamics. It appeared in many places
as a suggestion for a simple game dynamics, but how to treat this on the boundary
has been rarely dealt with. Indeed, the vector ﬁeld (2.14) is discontinuous on bd Δ.
However, essentially because Ph is Lipschitz, solutions exist for all t ≥0 and are
unique (in forward time). This can be shown by rewriting (2.14) as a viability
problem in terms of the normal cone ([1, 34])
˙x ∈a(x) −NΔ(x),
x(t) ∈Δ.
Theorem 2.7. [34] In a negative deﬁnite game (1.2), the unique NE is globally
asymptotically stable for the payoﬀprojection dynamics (2.14).
The proof uses as Lyapunov function the Euclidean distance to the equilibrium
V (x) = 
i(xi −ˆxi)2.
                                                                                                                    
                                                                                                               

70
JOSEF HOFBAUER
Figure 6. Payoﬀprojection dynamics for Rock-Paper-Scissors
games: a = b versus a < b
Summary. As we have seen many of the special dynamics are related to maps
that have been used to prove existence of Nash equilibria. The best response dy-
namics, the perturbed best response dynamics, and the BNN dynamics correspond
to the three proofs given by Nash himself: [39, 40, 41]. The payoﬀprojection dy-
namics is related to [15]. Even the replicator dynamics can be used to provide such
a proof, if only after adding a mutation term, see [26, 27], or Sigmund’s chapter
[53, (11.3)]:
(2.15)
˙xi = xi (ai(x) −x·a(x)) + εi −xi

j
εj,
i = 1, . . . , n
with εi > 0 describing mutation rates.
Moreover, there is a result analogous to Theorem 2.4.
Theorem 2.8. For a negative semideﬁnite game (1.3), and any εi > 0, (2.15)
has a unique rest point ˆx(ε) ∈Δ. It is globally asymptotically stable, and for ε →0
it approaches the set of NE of the game.
I show a slightly more general result. With the notation φi(xi) = εi
xi , let us
rewrite (2.15) as
(2.16)
˙xi = xi

(Ax)i + φi(xi) −x·Ax −¯φ

where ¯φ = 
i xiφi(xi). In the following, I require only that each φi is a strictly
decreasing function.
Theorem 2.9. If A is a negative semideﬁnite game (1.3) and the functions φi
are strictly decreasing, then there is a unique rest point ˆx(φ) for ( 2.16) which is
globally asymptotically stable for ( 2.16).
                                                                                                                    
                                                                                                               

DETERMINISTIC EVOLUTIONARY GAME DYNAMICS
71
Proof. By Brouwer’s ﬁxed point theorem, (2.16) has a rest point ˆx ∈Δ.
Consider now the function L(x) = 
i ˆxi log xi deﬁned on int Δ. Then
˙L
=

i
ˆxi
˙xi
xi
=

i
ˆxi

(Ax)i + φi(xi) −x·Ax −¯φ

=

i
(ˆxi −xi) ((Ax)i −φi(xi))
=
−(ˆx −x)·A(ˆx −x) −

i
(xi −ˆxi) (φi(ˆxi) −φi(xi)) ≥0
with equality only at x = ˆx. Hence L is a Lyapunov function for ˆx, and hence ˆx is
globally asymptotically stable (w.r.t. int Δ).
□
The six basic dynamics described so far enjoy the following common properties.
1. The unique NE of a negative deﬁnite game (in particular, any interior ESS)
is globally asymptotically stable.
2. Interior NE of a positive deﬁnite game (‘anti-ESS’) are repellors.
Because of the nice behaviour of negative (semi-)deﬁnite games with respect to
these basic dynamics, Sandholm christened them stable games.
For nonlinear games in a single population these are games whose payoﬀfunc-
tion a : Δ →Rn satisﬁes
(2.17)
(a(x) −a(y))(x −y) ≤0
∀x, y ∈Δ
or equivalently, if a is smooth,
z · a′(x)z ≤0
∀x ∈Δ, z ∈Rn
0
Examples are congestion games [48], the war of attrition [36], the sex–ratio game
[36], the habitat selection game [10], or simply the nonlinear payoﬀfunction a(x) =
Ax + φ(x) in (2.16).
The global stability theorems 2.1, 2.3, 2.4, 2.5, 2.6, 2.14 hold for general stable
N population games, see [24, 48].
3. Bimatrix games
The replicator dynamics for an n × m bimatrix game (A, B) reads
˙xi
=
xi

(Ay)i −x·Ay

,
i = 1, . . . , n
˙yj
=
yj

(BT x)j −x·By

j = 1, . . . , m
For its properties see [26, 27] and especially [21]. N person games are treated in
[61] and [44].
The best reply dynamics for bimatrix games reads
(3.1)
˙x ∈BR1(y) −x
˙y ∈BR2(x) −y
See Sorin [56, section 1] for more information.
For 2 × 2 games the state space [0, 1]2 is two-dimensional and one can com-
pletely classify the dynamic behaviour. There are four robust cases for the replicator
dynamics, see [26, 27], and additionally 11 degenerate cases. Some of these degen-
erate cases arise naturally as extensive form games, such as the Entry Deterrence
                                                                                                                    
                                                                                                               

72
JOSEF HOFBAUER
Game, see Cressman’s chapter [10]. A complete analysis including all phase por-
traits are presented in [9] for (BR) and (REP), and in [46] for the BNN and the
Smith dynamics.
For bimatrix games, stable games include zero-sum games, but not much more.
We call an n × m bimatrix game (A, B) a rescaled zero-sum game [26, 27] if
(3.2)
∃c > 0 :
u·Av = −cu·Bv
∀u ∈Rn
0, v ∈Rm
0
or equivalently, there exists an n × m matrix C, αi, βj ∈R and γ > 0 s.t.
aij = cij + αj,
bij = −γcij + βi,
∀i = 1, . . . , n, j = 1, . . . , m
For 2 × 2 games, this includes an open set of payoﬀmatrices, corresponding to
games with a cyclic best reply structure, or equivalently, those with a unique and
interior Nash equilibrium. Simple examples are the Odd or Even game [53, (1.1)],
or the Buyers and Sellers game [10]. However, for larger n, m this is a thin set of
games, e.g. for 3 × 3 games, this set has codimension 3.
For such rescaled zero-sum games, the set of Nash equilibria is stable for (REP),
(BR) and the other basic dynamics.
One of the main open problems in evolutionary game dynamics concerns the
converse.
Conjecture 3.1. Let (p, q) be an isolated interior equilibrium of a bimatrix
game (A, B), which is stable for the BR dynamics or for the replicator dynamics.
Then n = m and (A, B) is a rescaled zero sum game.
4. Dominated Strategies
A pure strategy i (in a single population game with payoﬀfunction a : Δ →Rn)
is said to be strictly dominated if there exists some y ∈Δ such that
(4.1)
ai(x) < y·a(x)
for all x ∈Δ. A rational player will not use such a strategy.
In the best response dynamics, ˙xi = −xi and hence xi(t) →0 as t →∞.
Similarly, for the replicator dynamics, L(x) = log xi −
k yk log xk satisﬁes
˙L(x) < 0 for x ∈int Δ and hence xi(t) →0 along all interior orbits of (REP).
A similar result holds for extensions of (REP), given by diﬀerential equations
of the form
(4.2)
˙xi = xigi(x)
where the functions gi satisfy  xigi(x) = 0 on Δ. The simplex Δ and its faces are
invariant. Such an equation is said to be payoﬀmonotonic [61] if for any i, j, and
x ∈Δ
(4.3)
gi(x) > gj(x) ⇔ai(x) > aj(x).
All dynamics arising from an imitative revision protocol have this property. For such
payoﬀmonotonic dynamics, if the pure strategy i is strictly dominated by another
pure strategy j, i.e., ai(x) < aj(x) for all x ∈Δ then
xi
xj goes monotonically to
zero, and hence xi(t) →0. However, if the dominating strategy is mixed, this need
no longer be true, see [20, 30].
The situation is even worse for all other basic dynamics from section 2, in
particular, (BNN), (PD) and (PP). As shown in [4, 25, 34] there are games with
a pure strategy i being strictly dominated by another pure strategy j such that
                                                                                                                    
                                                                                                               

DETERMINISTIC EVOLUTIONARY GAME DYNAMICS
73
i survives in the long run, i.e., lim inft→+∞xi(t) > 0 for an open set of initial
conditions.
5. Supermodular Games and Monotone Flows
An interesting class of games are the supermodular games (also known as games
with strict strategic complementarities [59]). They make use of the natural order
among pure strategies and are deﬁned by
(5.1)
ai+1,j+1 −ai+1,j −ai,j+1 + ai,j > 0
∀i, j
where ai,j = aij are the entries of the payoﬀmatrix A. This means that for any
i < n, ai+1,k −aik increases strictly with k.
In the case of n = 2 strategies this reduces to a22 −a21 −a12 + a11 > 0, which
means that the game is positive deﬁnite (1.5). In particular, every bistable 2 × 2
game is supermodular.
For n ≥3 there is no simple relation between supermodular games and positive
deﬁnite games, although they share some properties, such as the instability of
interior NE. For example, the RPS game with b > a is positive deﬁnite but not
supermodular. Indeed, a supermodular game cannot have a best reply cycle among
the pure strategies, see below. On the other hand, an n × n pure coordination
game (where the payoﬀmatrix is a positive diagonal matrix) is positive deﬁnite,
but supermodular only if n = 2.
Stochastic dominance deﬁnes a partial order on the simplex Δ:
(5.2)
p ⪰p′
⇔
m

k=1
pk ≤
m

k=1
p′
k
∀m = 1, . . . , n −1.
If all inequalities in (5.2) are strict, we write p ≻p′. The intuition is that p has
more mass to the right than p′. This partial order extends the natural order on the
pure strategies:
1 ≺2 ≺· · · ≺n. Here k is identiﬁed with the kth unit vector, i.e., a corner of
Δ.
Lemma 5.1. Let (uk) be an increasing sequence, and x ⪯y. Then  ukxk ≤
 ukyk. If (uk) is strictly increasing and x ⪯y, x ̸= y then  ukxk <  ukyk. If
(uk) is increasing but not constant and x ≺y then  ukxk <  ukyk.
The proof follows easily from Abel summation (the discrete analog of integra-
tion by parts): set xk −yk = ck and un−1 = un −vn−1, un−2 = un −vn−1 −vn−2,
etc.
Lemma 5.2. For i < j and x ⪯y, x ̸= y:
(5.3)
(Ax)j −(Ax)i < (Ay)j −(Ay)i.
Proof. Take uk = ajk −aik as strictly increasing sequence in the previous
lemma.
□
The crucial property of supermodular games is the monotonicity of the best
reply correspondence.
Theorem 5.3. [59] If x ⪯y, x ̸= y then max BR(x) ≤min BR(y), i.e., no
pure best reply to y is smaller than a pure best reply to x.
                                                                                                                    
                                                                                                               

74
JOSEF HOFBAUER
Proof. Let j = max BR(x). Then for any i < j (5.3) implies that (Ay)j >
(Ay)i, hence i /∈BR(y). Hence every element in BR(y) is ≥j.
□
Some further consequences of Lemma 5.2 and Theorem 5.3 are:
The extreme strategies 1 and n are either strictly dominated strategies or pure
Nash equilibria.
There are no best reply cycles: Every sequence of pure strategies which is
sequential best replies is ﬁnally constant and ends in a pure NE.
For results on the convergence of ﬁctitious play and the best response dynamics
in supermodular games see [3, 32].
Theorem 5.4. Mixed (=nonpure) equilibria of supermodular games are unsta-
ble under the replicator dynamics.
Proof. W.l.o.g., we can assume that the equilibrium ˆx is interior (otherwise
restrict to a face).
A supermodular game satisﬁes aij + aji < aii + ajj for all
i ̸= j (set x = i, y = j in (5.3)). Hence, if we normalize the game by aii = 0,
ˆx·Aˆx = 
i,j(aij + aji)ˆxiˆxj < 0. Now it is shown in [27, p.164] that −ˆx·Aˆx equals
the trace of the Jacobian of (REP) at ˆx, i.e., the sum of all its eigenvalues. Hence
at least one of the eigenvalues has positive real part, and ˆx is unstable.
□
For diﬀerent instability results of mixed equilibria see [11].
The following is a generalization of Theorem 5.3 to perturbed best replies, due
to [23]. I present here a diﬀerent proof.
Theorem 5.5. For every supermodular game
x ⪯y, x ̸= y
⇒
C(a(x)) ≺C(a(y))
holds if the choice function C : Rn →Δn is C1 and the partial derivatives Ci,j =
∂Ci
∂xj satisfy for all 1 ≤k, l < n
(5.4)
k

i=1
l

j=1
Ci,j > 0,
and for all 1 ≤i ≤n,
(5.5)
n

j=1
Ci,j = 0.
Proof. It is suﬃcient to show that the perturbed best reponse map is strongly
monotone:
x ⪯y, x ̸= y
⇒
C(Ax) ≺C(Ay)
From Lemma 5.2 we know: If x ⪯y, x ̸= y then (Ay −Ax)i increases strictly in i.
Hence, with a = Ax and b = Ay, it remains to show:
Lemma 5.6. Let a, b ∈Rn with b1 −a1 < b2 −a2 < · · · < bn −an. Then
C(a) ≺C(b).
This means that for each k: C1(a) + · · · + Ck(a) ≥C1(b) + · · · + Ck(b).
Taking derivative in direction u = b−a, this follows from k
i=1
n
j=1 Ci,juj < 0
which by Lemma 5.1 holds whenever (xj−yj =)cj = k
i=1 Ci,j satisﬁes l
j=1 cj > 0
for l = 1, . . . , n −1 and n
j=1 cj = 0.
□
                                                                                                                    
                                                                                                               

DETERMINISTIC EVOLUTIONARY GAME DYNAMICS
75
The conditions (5.4, 5.5) on C hold for every stochastic choice model (2.6),
since there Ci,j < 0 for i ̸= j. As a consequence, the perturbed best reply dynamics
(5.6)
˙x = C(a(x)) −x
generates a strongly monotone ﬂow: If x(0) ⪯y(0), x(0) ̸= y(0) then x(t) ≺y(t)
for all t > 0. The theory of monotone ﬂows developped by Hirsch and others (see
[54]) implies that almost all solutions of (5.6) converge to a rest point of (5.6).
It seems that the other basic dynamics do not respect the stochastic dominance
order (5.2). They do not generate a monotone ﬂow for every supermodular game.
Still there is the open problem
Problem 5.7. In a supermodular game, do almost all orbits of (BR), (REP),
(BNN), (PD), (PP) converge to a NE?
For the best reponse dynamics this entails to extend the theory of monotone
ﬂows to cover discontinuous diﬀerential equations or diﬀerential inclusions.
6. Partnership games and general adjustment dynamics
We consider now games with a symmetric payoﬀmatrix A = AT (aij = aji for
all i, j). Such games are known as partnership games [26, 27] and potential games
[38]. The basic population genetic model of Fisher and Haldane is equivalent to the
replicator dynamics for such games, which is then a gradient system with respect
to the Shahshahani metric and the mean payoﬀx·Ax as potential, see e.g. [26, 27].
The resulting increase of mean ﬁtness or mean payoﬀx·Ax in time is often referred
to as the Fundamental Theorem of Natural Selection. This statement about the
replicator dynamics generalizes to the other dynamics considered here.
The generalization is based on the concept, deﬁned by Swinkels [58], of a
(myopic) adjustment dynamics which satisﬁes ˙x·Ax ≥0 for all x ∈Δ, with equality
only at equilibria. If A = AT then the mean payoﬀx·Ax is increasing for every
adjustment dynamics since (x·Ax)· = 2 ˙x·Ax ≥0.
It is obvious that the best
response dynamics (2.2) is an adjustment dynamics and it is easy to see that the
other special dynamics from section 2 are as well.
As a consequence, we obtain the following result.
Theorem 6.1. [20, 22] For every partnership game A = AT , the potential
function x·Ax increases along trajectories.
Hence every trajectory of every ad-
justment dynamics (in particular (2.1), (2.2), (2.7), and (2.13)) converges to (a
connected set of) equilibria. A strict local maximizer of x·Ax is asymptotically
stable for every adjustment dynamics.
Generically, equilibria are isolated. Then the above result implies convergence
for each trajectory. Still, continua of equilibria occur in many interesting appli-
cations, see e.g. [45]. Even in this case, it is known that every trajectory of the
replicator dynamics converges to a rest point, and hence each interior trajectory
converges to a Nash equilibrium, see e.g. [26, ch. 23.4] or [27, ch. 19.2]. It is an
open problem whether the same holds for the other basic dynamics.
For the perturbed dynamics (2.5) (for a concave function v on int Δ) and (2.15)
there is an analog of Theorem 6.1: A suitably perturbed potential function serves
as a Lyapunov function.
                                                                                                                    
                                                                                                               

76
JOSEF HOFBAUER
Theorem 6.2. [22, 26] For every partnership game A = AT :
the function P(x) = 1
2x·Ax+εv(x) increases monotonically along solutions of (2.5),
the function P(x) = 1
2x·Ax + 
i εi log xi is a Lyapunov function for (2.15).
Hence every solution converges to a connected set of rest points.
For bimatrix games the adjustment property is deﬁned as
˙x·Ay ≥0,
x·B ˙y ≥0.
A bimatrix game is a partnership/potential game if A = B, i.e., if both players ob-
tain the same payoﬀ[26, ch. 27.2]. Then the potential x·Ay increases monotonically
along every solution of every adjustment dynamics.
For the general situation of potential games between N populations with non-
linear payoﬀfunctions see [48].
7. A universal Shapley example
The simplest example of persistent cycling in a game dynamics is probably the
RSP game (1.4) with b > a for the BR dynamics (2.2) which leads to a triangular
shaped limit cycle, see ﬁgure 1 (right). Historically, Shapley [52] gave the ﬁrst such
example in the context of 3 × 3 bimatrix games (but it is less easy to visualize
because of the 4d state space).
Our six basic dynamics show a similar cycling
behavior for positive deﬁnite RSP games.
But given the huge pool of adjustment dynamics, we now ask: Is there an
evolutionary dynamics, which converges for each game from each initial condition
to an equilibrium?
Such a dynamics is assumed to be given by a diﬀerential equation
(7.1)
˙x = f(x, a(x))
such that f depends continuously on the population state x and the payoﬀfunction
a.
For N player binary games (each player chooses between two strategies only)
general evolutionary dynamics are easy to describe:
The better of the two strategies increases, the other one decreases, i.e.,
(7.2)
˙xi1 = −˙xi2 > 0 ⇔ai(1, x−i) > ai(2, x−i)
holds for all i at all (interior) states. Here xij denotes the frequency of strategy j
used by player i, and ai(j, x−i) his payoﬀ. In a common interest game where each
player has the same payoﬀfunction P(x), along solutions x(t), P(x(t)) increases
monotonically:
(7.3)
˙P =
N

i=1
2

k=1
ai(k, x−i) ˙xik =
N

i=1

ai(1, x−i) −ai(2, x−i)

˙xi1 ≥0.
A familiy of 2 × 2 × 2 games. Following [29], we consider 3 players, each
with 2 pure strategies. The payoﬀs are summarized in the usual way as follows.
-1,-1,-1
0, 0, ε
0, ε, 0
ε, 0, 0
ε, 0, 0
0, ε, 0
0, 0, ε
-1,-1,-1
The ﬁrst player (left payoﬀ) chooses the row, the second chooses the column, the
third (right payoﬀ) chooses one of the matrices. For ε ̸= 0, this game has a unique
equilibrium E =
 1
2, 1
2, 1
2

at the centroid of the state space, the cube [0, 1]3. This
                                                                                                                    
                                                                                                               

DETERMINISTIC EVOLUTIONARY GAME DYNAMICS
77
equilibrium is regular for all ε. For ε > 0, this game has a best response cycle among
the six pure strategy combinations 122 →121 →221 →211 →212 →112 →122.
For ε = 0, this game is a potential game: Every player gets the same payoﬀ
P(x) = −x11x21x31 −x21x22x32.
The minimum value of P is −1 which is attained at the two pure proﬁles 111 and
222. At the interior equilibrium E, its value is P(E) = −1
4. P attains its maximum
value 0 at the set Γ of all proﬁles, where two players use opposite pure strategies,
whereas the remaining player may use any mixture.
All points in Γ are Nash
equilibria. Small perturbations in the payoﬀs (ε ̸= 0) can destroy this component
of equilibria.
For every natural dynamics, P(x(t)) increases. If P(x(0)) > P(E) = −1
4 then
P(x(t)) →0 and x(t) →Γ. Hence Γ is an attractor (an asymptotically stable
invariant set) for the dynamics, for ε = 0.
For small ε > 0, there is an attractor Γε near Γ whose basin contains the set
{x : P(x) > −1
4 + γ(ε)}, with γ(ε) →0, as ε →0. This follows from the fact that
attractors are upper–semicontinuous against small perturbations of the dynamics
(for proofs of this fact see, e.g., [25, 2]). But for ε > 0, the only equilibrium is E.
Hence we have shown
Theorem 7.1. [29] For each dynamics satisfying the assumptions (7.2) and
continuity in payoﬀs, there is an open set of games and an open set of initial
conditions x(0) such that x(t) stays away from the set of NE, for large t > 0.
Similar examples can be given as 4 × 4 symmetric one population games, see
[27], and 3×3 bimatrix games, see [29]. The proofs follow the same lines: For ε = 0
these are potential games, the potential maximizer is a quadrangle or a hexagon,
and this component of NE disappears for ε ̸= 0 but continues to a nearby attractor
for the dynamics.
A diﬀerent general nonconvergence result is due to Hart and Mas-Colell [18].
For speciﬁc dynamics there are many examples with cycling and even chaotic
behavior: Starting with Shapley [52] there are [8, 13, 16, 47, 57] for the best
response dynamics. For other examples and a more complete list of references see
[48, ch. 9].
References
1. Aubin, J.P. and A. Cellina: Diﬀerential Inclusions. Springer, Berlin. 1984.
2. Bena¨ım M., J. Hofbauer and S. Sorin: Perturbation of Set–valued Dynamical Systems, with
Applications to Game Theory. Preprint 2011.
3. Berger, U.: Learning in games with strategic complementarities revisited. J. Economic Theory
143 (2008) 292–301.
4. Berger, U. and Hofbauer, J.: Irrational behavior in the Brown–von Neumann–Nash dynamics.
Games Economic Behavior 56 (2006), 1–6.
5. Blume, L.E.:
The statistical mechanics of strategic interaction. Games Economic Behavior
5 (1993), 387–424.
6. Brown, G. W.: Iterative solution of games by ﬁctitious play. In: Activity Analysis of Produc-
tion and Allocation, pp. 374–376. Wiley. New York. 1951.
7. Brown, G. W., von Neumann, J.: Solutions of games by diﬀerential equations. Ann. Math.
Studies 24 (1950), 73–79.
8. Cowan, S.: Dynamical systems arising from game theory. Dissertation (1992), Univ. Califor-
nia, Berkeley.
                                                                                                                    
                                                                                                               

78
JOSEF HOFBAUER
9. Cressman, R.: Evolutionary Dynamics and Extensive Form Games. M.I.T. Press. 2003.
10. Cressman, R.: Extensive Form Games, Asymmetric Games and Games with Continuous
Strategy Spaces. This Volume.
11. Echenique, F and A. Edlin: Mixed equilibria are unstable in games of strategic complements.
J. Economic Theory 118 (2004), 61–79.
12. Fudenberg, D. and Levine, D. K.: The Theory of Learning in Games. MIT Press. 1998.
13. Gaunersdorfer, A. and J. Hofbauer: Fictitious play, Shapley polygons and the replicator equa-
tion. Games Economic Behavior 11 (1995), 279–303.
14. Gilboa, I., Matsui, A.: Social stability and equilibrium. Econometrica 59 (1991), 859–867.
15. G¨ul, F., D. Pearce and E. Stacchetti: A bound on the proportion of pure strategy equilibria
equilibria in generic games. Math. Operations Research 18 (1993) 548–552.
16. Hahn, M.: Shapley polygons in 4 × 4 games. Games 1 (2010) 189–220.
17. Harsanyi, J. C.: Oddness of the number of equilibrium points: a new proof. Int. J. Game
Theory 2 (1973) 235–250.
18. Hart S. and A. Mas-Colell: Uncoupled dynamics do not lead to Nash equilibrium. American
Economic Review 93 (2003) 1830–1836.
19. Hofbauer, J.: Stability for the best response dynamics. Preprint (1995).
20. Hofbauer, J.: Imitation dynamics for games. Preprint (1995).
21. Hofbauer, J.: Evolutionary dynamics for bimatrix games: A Hamiltonian system? J. Math.
Biology 34 (1996) 675–688.
22. Hofbauer, J.: From Nash and Brown to Maynard Smith: Equilibria, dynamics and ESS.
Selection 1 (2000), 81–88.
23. Hofbauer, J. and W. H. Sandholm: On the global convergence of stochastic ﬁctitious play.
Econometrica 70 (2002), 2265–2294.
24. Hofbauer, J. and W. H. Sandholm: Stable games and their dynamics. J. Economic Theory
144 (2009) 1665–1693.
25. Hofbauer, J. and W. H. Sandholm: Survival of Dominated Strategies under Evolutionary
Dynamics. Theor. Economics (2011), to appear.
26. Hofbauer, J. and K. Sigmund: The Theory of Evolution and Dynamical Systems. Cambridge
Univ. Press. 1988.
27. Hofbauer, J. and K. Sigmund: Evolutionary Games and Population Dynamics. Cambridge
University Press. 1998.
28. Hofbauer, J. and K. Sigmund: Evolutionary Game Dynamics. Bull. Amer. Math. Soc.
40
(2003) 479–519.
29. Hofbauer, J. and J. Swinkels: A universal Shapley example. Preprint. 1995.
30. Hofbauer, J. and J. W. Weibull:
Evolutionary selection against dominated strategies. J.
Economic Theory 71 (1996), 558–573.
31. Hopkins, E.: A note on best response dynamics. Games Economic Behavior 29 (1999), 138–
150.
32. Krishna, V.: Learning in games with strategic complementarities, Preprint 1992.
33. Kuhn, H. W. and S. Nasar (Eds.): The Essential John Nash. Princeton Univ. Press. 2002.
34. Lahkar, R. and W. H. Sandholm: The projection dynamic and the geometry of population
games. Games Economic Behavior 64 (2008), 565–590.
35. Matsui, A.: Best response dynamics and socially stable strategies. J. Economic Theory 57
(1992), 343–362.
36. Maynard Smith, J.: Evolution and the Theory of Games. Cambridge University Press. 1982.
37. McKelvey, R. D. and T. D. Palfrey: Quantal response equilibria for normal form games.
Games Economic Behavior 10 (1995), 6–38.
38. Monderer, D. and L. Shapley: Potential games. Games Economic Behavior 14 (1996), 124–143
39. Nash, J.: Equilibrium points in N–person games. Proc. Natl. Ac. Sci. 36 (1950), 48–49.
40. Nash, J.: Non-cooperative games. Dissertation, Princeton University, Dept. Mathematics.
1950. Published in [33].
41. Nash, J.: Non-cooperative games. Ann. Math. 54 (1951), 287–295.
42. Nikaido, H.: Stability of equilibrium by the Brown–von Neumann diﬀerential equation. Econo-
metrica 27 (1959), 654–671.
43. Ochea, M.I.: Essays on nonlinear evolutionary game dynamics. Ph.D. Thesis. University of
Amsterdam. 2010. http://dare.uva.nl/document/157994
                                                                                                                    
                                                                                                               

DETERMINISTIC EVOLUTIONARY GAME DYNAMICS
79
44. Plank, M.: Some qualitative diﬀerences between the replicator dynamics of two player and n
player games. Nonlinear Analysis 30 (1997), 1411–1417.
45. Pawlowitsch, C.: Why evolution does not always lead to an optimal signaling system. Games
Econ. Behav. 63 (2008) 203–226.
46. Rahimi, M.: Innovative Dynamics for Bimatrix Games. Diplomarbeit. Univ. Vienna. 2009.
http://othes.univie.ac.at/7816/
47. Rosenm¨uller,
J.:
¨Uber
Periodizit¨atseigenschaften
spieltheoretischer
Lernprozesse.
Z.
Wahrscheinlichkeitstheorie Verw. Geb. 17 (1971) 259–308.
48. Sandholm, W. H.: Population Games and Evolutionary Dynamics. MIT Press, Cambridge.
2010.
49. Sandholm, W. H.: Stochastic Evolutionary Game Dynamics: Foundations, Deterministic
Approximation, and Equilibrium Selection. This volume.
50. Sandholm, W.H., E. Dokumaci, and F. Franchetti: Dynamo: Diagrams for Evolutionary
Game Dynamics. 2011. http://www.ssc.wisc.edu/ whs/dynamo.
51. Schlag, K.H.:
Why imitate, and if so, how? A boundedly rational approach to multi-armed
bandits, J. Economic Theory 78 (1997) 130–156.
52. Shapley, L.:
Some topics in two-person games. Ann. Math. Studies 5 (1964), 1–28.
53. Sigmund, K.: Introduction to evolutionary game theory. This volume.
54. Smith, H.: Monotone Dynamical Systems: An Introduction to the Theory of Competitive and
Cooperative Systems. Amer. Math. Soc. Math. Surveys and Monographs, Vol.41 (1995).
55. Smith, M. J.: The stability of a dynamic model of traﬃc assignment—an application of a
method of Lyapunov. Transportation Science 18 (1984) 245–252.
56. Sorin, S.: Some global and unilateral adaptive dynamics. This volume.
57. Sparrow, C., S. van Strien and C. Harris: Fictitious play in 3×3 games: the transition between
periodic and chaotic behavior. Games and Economic Behavior 63 (2008) 259–291.
58. Swinkels, J. M.: Adjustment dynamics and rational play in games. Games Economic Behavior
5 (1993), 455–484.
59. Topkis, D. M.: Supermodularity and Complementarity. Princeton University Press. 1998.
60. Viossat Y.: The replicator dynamics does not lead to correlated equilibria, Games and Eco-
nomic Behavior 59 (2007), 397–407.
61. Weibull, J. W.: Evolutionary Game Theory. MIT Press. 1995.
Department of Mathematics, University Vienna, Austria
E-mail address: Josef.Hofbauer@univie.ac.at
                                                                                                                    
                                                                                                               

This page intentionally left blank 
                                                                                                                    
                                                                                                               

Proceedings of Symposia in Applied Mathematics
Volume 69, 2011
On Some Global and Unilateral Adaptive Dynamics
Sylvain Sorin
Abstract. The purpose of this chapter is to present some adaptive dynamics
arising in strategic interactive situations. We will deal with discrete time and
continuous time procedures and compare their asymptotical properties. We
will also consider global or unilateral frameworks and describe the wide range of
applications covered by this approach. The study starts with the discrete time
ﬁctitious play procedure and its continuous time counterpart which is the best
reply dynamics. Its smooth unilateral version presents interesting consistency
properties. We then analyze its connection with the time average replicator
dynamics. Several results rely on the theory of stochastic approximation and
basic tools are brieﬂy presented in a last section.
1. Fictitious Play and Best Reply Dynamics
Fictitious play is one of the oldest and most famous dynamical processes in-
troduced in game theory. It has been widely studied and is a good introduction to
the ﬁeld of adaptive dynamics. This procedure is due to Brown (1949, 1951) and
corresponds to an interactive adjustment process with (increasing and unbounded)
memory.
1.1. Discrete ﬁctitious play.
Consider a game in strategic form with a ﬁnite set of players i ∈I, each having
a ﬁnite pure strategy set Si. For each i ∈I, the mixed strategy set Xi = Δ(Si)
corresponds to the simplex on Si. F i : S = 
j∈I Sj →R is the payoﬀof player i
and we deﬁne F i(y) = EyF(s) for every y ∈Δ(S), where E stands for the expecta-
tion.
The game is played repeatedly in discrete time. Given an n-stage history, which is
the sequence of proﬁles of past moves of the players, hn = (x1 = {xi
1}i=1,...,I, x2, ..., xn) ∈
Sn, the ﬁctitious play procedure requires the move xi
n+1 of each player i at stage
n + 1 to be a best reply to the “time average moves” of her opponents.
There are two variants, that coincide in the case of two-player games :
- independent FP: for each i, let
xi
n = 1
n
n
m=1xi
m
1991 Mathematics Subject Classiﬁcation. Primary 91A22.
I want to thank Bill Sandholm for useful comments.
This research was partially supported by grant ANR-08-BLAN-0294-01 (France).
c
⃝2011 American Mathematical Society
81
                                           
                                                                                                                    
                                                                                                               

82
SYLVAIN SORIN
and x−i
n = {xj
n}j̸=i. Player i computes, at each stage n and for each of her opponents
j ∈I, the empirical distribution of her past moves and considers the product
distribution. Then, her next move at stage n + 1 satisﬁes:
(1.1)
xi
n+1 ∈BRi(x−i
n )
where BRi denotes the best reply correspondence of player i, from Δ(S−i) to Xi,
with S−i = 
j̸=i Si: BRi(y−i) = {xi ∈Xi; F i(xi, y−i) = maxzi∈Xi F i(zi, y−i)}.
- correlated FP: one deﬁnes a point ˜x−i
n
in Δ(S−i) by :
˜x−i
n = 1
n
n
m=1x−i
m
which is the empirical distribution of the joint moves of the opponents −i of player
i. Here the discrete time process satisﬁes:
(1.2)
xi
n+1 ∈BRi(˜x−i
n ).
Since one deals with time averages one has
xi
n+1 = nxi
n + xi
n+1
n + 1
hence the stage diﬀerence is expressed as
xi
n+1 −xi
n = xi
n+1 −xi
n
n + 1
so that (1.1) can also be written as :
(1.3)
xi
n+1 −xi
n ∈
1
(n + 1)[BRi(x−i
n ) −xi
n].
Deﬁnition. A sequence {xn} of moves in S satisﬁes discrete ﬁctitious play
(DFP) if (1.3) holds.
Remarks.
xi
n does not appear explicitly any more in (1.3): the natural state variable of
the process is xn which is the product of the marginal empirical averages xj
n ∈Xj.
One can deﬁne a procedure based, for each player, on her past vector payoﬀs
gi
n = {F i(si, x−i
n )}si∈Si ∈RSi, rather than on the past moves of all players, as
follows: xi
n+1 ∈bri(¯gi
n) with bri(U) = argmaxXi⟨x, U⟩and ¯gi
n =
1
n
n
m=1 gi
m.
Due to the linearity of the payoﬀs, this corresponds to the correlated ﬁctitious play
procedure. Note that xn is no longer the common state variable but rather the
correlated empirical distribution of moves ˜xn which satisﬁes:
˜xn+1 = n˜xn + xn+1
n + 1
and has the same marginal on each factor space Xi. The joint process (1.2) is
deﬁned by:
(1.4)
˜xn+1 −˜xn ∈
1
(n + 1)[

i
BRi(˜xn) −˜xn].
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
83
1.2. Continuous ﬁctitious play and best reply dynamics.
The continuous time (formal) counterpart of the above diﬀerence inclusion (1.3)
is the diﬀerential inclusion, called continuous ﬁctitious play (CFP):
(1.5)
˙Xi
t ∈1
t [BRi(X−i
t ) −Xi
t].
The change of time Zs = Xes leads to
(1.6)
˙Zi
s ∈[BRi(Z−i
s ) −Zi
s]
which is the (continuous time) best reply dynamics (CBR) introduced by
Gilboa and Matsui (1991), see Section 12 in K. Sigmund’s chapter.
Note that the asymptotic properties of (CFP) or (CBR) are the same, since the
diﬀerential inclusions diﬀer only by their time scales.
The interpretation of (CBR) in evolutionary game theory is as follows: at each
stage n a randomly selected fraction ε of the current population Zn dies and is
replaced by newborns Yn+1 selected according to their abilities to adjust to the
current population. The discrete time process is thus
Zn+1 = εYn+1 + (1 −ε)Zn
with Yn+1 ∈BR(Zn) leading to the diﬀerence inclusion
Zn+1 −Zn ∈ε[BR(Zn) −Zn].
Note that it is delicate in his framework to justify the fact that the step size ε
(which is induced by the choice of the time unit) should go to 0. However numer-
ous asymptotic results are available for small step sizes.
Comments. Recall that a solution of a diﬀerential inclusion of the form
(1.7)
˙zt ∈Ψ(zt)
where Ψ is a correspondence deﬁned on a subset of Rn with values in Rn, is an ab-
solutely continuous function z from R to Rn that satisﬁes (1.7) almost everywhere.
Let Z be a compact convex subset of Rn and Φ : Z⇒Z a correspondence from Z
to itself, upper semi continuous and with non empty convex values. Consider the
diﬀerential inclusion
(1.8)
˙zt ∈Φ(zt) −zt.
Lemma 1.1. For every z(0) ∈Z, ( 1.8) has a solution with zt ∈Z and z0 =
z(0).
See e.g. Aubin and Cellina (1984).
In particular this applies to (CBR) where Z =  Xi is the product of the sets of
mixed strategies.
Note also that rest points of (1.8) coincide with ﬁxed points of Φ.
1.3. General properties.
We recall brieﬂy here basic properties of (DFP) or (CFP), in particular the link
to Nash equilibrium.
Deﬁnition. A process zn (discrete) or zt (continuous) converges to a subset Z of
some metric space if d(zn, Z) or d(zt, Z) goes to 0 as n or t →∞.
Proposition 1.1. If (DFP) or (CFP) converges to a point x, x is a Nash
equilibrium.
                                                                                                                    
                                                                                                               

84
SYLVAIN SORIN
Proof. If x is not a Nash equilibrium then d(x, BR(x)) = δ > 0. Hence by
upper semicontinuity of the best reply correspondence d(y, BR(z)) ≥δ/2 > 0 for
each y and z in a neighborhood of x which prevents convergence of the discrete
time or continuous time processes.
□
The dual property is clear:
Proposition 1.2. If x is a Nash equilibrium, it is a rest point of (CFP).
Comments.
(i) (DFP) is “predictable”: in the game with payoﬀs
√
2
0
0
1
if player 1 follows (DFP) her move is always pure, since the past frequency of Left,
say y, is a rational number so that y
√
2 = 1 −y is impossible; hence player 1 is
guaranteed only 0. It follows that the unilateral (DFP) process has bad properties,
see Section 2.
(ii) Note also the diﬀerence between convergence of the marginal distribution and
convergence of the product distribution of the moves and in particular the conse-
quences in terms of payoﬀs. In the next game
L
R
T
1
0
B
0
1
a sequence of TR, BL, TR, ... induces asymptotical average marginal distributions
(1/2, 1/2) for both players (hence optimal strategies) but the average payoﬀis 0
while an alternative sequence TL, BR, ... would have the same average marginal
distributions and payoﬀ1.
We analyze now (DFP) and (CFP) in some classes of games. We will deduce
properties of the initial discrete time process from the analysis of the continuous
time counterpart.
1.4. Zero-sum games.
This is the framework in which (DFP) was initialy introduced in order to gen-
erate optimal strategies. The continuous time model is mathematically easier to
analyze.
1.4.1. Continuous time.
We ﬁrst consider the ﬁnite case.
1) Finite case : Harris (1998); Hofbauer (1995); Hofbauer and Sandholm (2009).
The game is deﬁned by a bilinear map F = F 1 = −F 2 on a product of simplexes
X × Y .
Introduce a(y) = maxx∈X F(x, y) and b(x) = miny∈Y F(x, y) that correspond to
the best reply levels, then the duality gap at (x, y) is W(x, y) = a(y) −b(x) ≥0.
Moreover (x∗, y∗) belongs to the set of optimal strategies, XF ×YF , iﬀW(x∗, y∗) =
0, see Section 3 of K. Sigmund’s chapter. Consider the evaluation of the duality
gap W(xt, yt) along a trajectory of (1.5).
Proposition 1.3. The “duality gap” criteria converges to 0 at a speed of 1/t
in (CFP).
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
85
Proof. Let (xt, yt) be a solution of (CBR) (1.6) and introduce
αt = xt + ˙xt ∈BR1(yt)
βt = yt + ˙yt ∈BR2(xt).
The duality gap along the trajectory is given by wt = W(xt, yt). Note that a(yt) =
F(αt, yt) hence taking derivative with respect to the time
d
dta(yt) = D1F(αt, yt) ˙αt + D2F(αt, yt) ˙yt
but the ﬁrst term is 0 (envelope theorem). As for the second one
D2F(αt, yt) ˙yt = F(αt, ˙yt)
by linearity. Thus:
˙wt = F(αt, ˙yt) −F( ˙xt, βt) = F(xt, ˙yt) −F( ˙xt, yt)
= F(xt, βt) −F(αt, yt) = b(xt) −a(yt) = −wt.
It follows that exponential convergence holds for (CBR)
wt = e−tw0
hence convergence at a rate 1/t in the original (CFP).
□
This proof in particular implies the minmax theorem and is reminiscent of the
analysis due to Brown and von Neumann (1950).
The analysis extends to the framework of continuous strategy space as follows.
2) Saddle case : Hofbauer and Sorin (2006)
Deﬁne the condition (H) : F is a continuous, concave/convex real function deﬁned
on a product X × Y of two compact convex subsets of an euclidean space.
Proposition 1.4. Under (H), any solution wt of (CBR) satisﬁes
˙wt ≤−wt a.e.
The proof, while much more involved, is in the spirit of Proposition 1.3 and
the main application is (see Section 5 for the deﬁnitions):
Corollary 1.1. For (CBR)
i) XF × YF is a global attractor .
ii) XF × YF is a maximal invariant subset.
Proof. From the previous Proposition 1.4 one deduces the following property:
∀ε > 0, ∃T such that for all (x0, y0), t ≥T implies
wt ≤ε
hence in particular the value vF of the game F exists and for t ≥T
b(xt) ≥vF −ε.
Continuity of F ( and hence of the function b) and compactness of X imply that
for any δ > 0, there exists T ′ such that d(xt, XF ) ≤δ as soon as t ≥T ′. This shows
that XF × YF is a global attractor.
Now consider any invariant trajectory. By Proposition 1.4 at each point w one can
write, for any t, w = wt ≤e−tw0, but the duality gap w0 is bounded, hence w equal
to 0 which gives ii).
□
To deduce properties of the discrete time process we introduce a general pro-
cedure.
                                                                                                                    
                                                                                                               

86
SYLVAIN SORIN
1.4.2. Discrete deterministic approximation.
Consider again the framework of (1.8).
Let αn a sequence of positive real numbers with  αn = +∞.
Given a0 ∈Z, deﬁne inductively an through the following diﬀerence inclusion:
(1.9)
an+1 −an ∈αn+1[Φ(an) −an].
The interpretation is that the evolution of the process satisﬁes an+1 = αn+1˜an+1 +
(1−αn+1)an with some ˜an+1 ∈Φ(an), and where αn+1 is the step size at stage n+1.
Deﬁnition. A sequence {an} ∈Z following (1.9) is a discrete deterministic
approximation (DDA) of (1.8).
The associated continuous time trajectory A : R+ →Z is constructed in two stages.
First deﬁne inductively a sequence of times {τn} by: τ0 = 0, τn+1 = τn + αn+1;
then let Aτn = an and extend the trajectory by linear interpolation on each interval
[τn, τn+1]:
At = an +
(t −τn)
(τn+1 −τn)(an+1 −an).
Since  αn = +∞the trajectory is deﬁned on R+.
To compare A to a solution of (1.8) we will need the approximation property
corresponding to the next proposition: it states that two diﬀerential inclusions de-
ﬁned by correspondences having graphs close one to the other will also have sets of
solutions close one to each other, on a given compact time interval.
Notations. Let A(Φ, T, z) = {z; z is a solution of (1.8) on [0, T] with z0 = z},
DT (y, z) = sup0≤t≤T ∥yt −zt∥. GΦ is the graph of Φ and Gε
Φ is an ε-neighborhood
of GΦ.
Proposition 1.5. ∀T ≥0, ∀ε > 0, ∃δ > 0 such that
inf{DT (y, z); z ∈A(Φ, T, z)} ≤ε
for any solution y of
˙yt ∈˜Φ(yt) −yt
with y0 = z and d(GΦ, G˜Φ) ≤δ.
See e.g. Aubin and Cellina (1984), Chapter 2.
Let us now compare the two dynamics deﬁned by {an} and A.
Case 1 Assume αn decreasing to 0.
In this case the set L({an}) of accumulation points of the sequence {an} coincides
with the limit set of the trajectory: L(A) = ∩t≥0A[t,+∞).
Proposition 1.6.
i) If Z0 is a global attractor for ( 1.8), it is also a global attractor for ( 1.9).
ii) If Z0 is a maximal invariant subset for ( 1.8), then L({an}) ⊂Z0.
Proof. i) Given ε > 0, let T1 be such that any trajectory z of (1.8) is within
ε of Z0 after time T1. Given T1 and ε, let δ > 0 be deﬁned by Proposition 1.5.
Since αn decreases to 0, given δ > 0, for n ≥N large enough for an, hence t ≥T2
large enough for At, one can write :
˙At ∈Ψ(At)
with
GΨ ⊂Gδ
Φ−Id.
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
87
Consider now At for some t ≥T1 + T2.
Starting from any position At−T1 the
continuous time process z deﬁned by (1.8) approaches within ε of Z0 at time t.
Since t −T1 ≥T2, the interpolated process As remains within ε of the former zs on
the interval [t −T1, t], hence is within 2ε of Z0 at time t. In particular this shows:
∀ε, ∃N0 such that n ≥N0 implies
d(an, Z0) ≤2ε.
ii) The result follows from the fact that L(A) is invariant.
In fact consider a ∈L(A), hence let tn →+∞and Atn →a. Given T > 0 let
Bn denote the translated solution At−tn deﬁned on [tn −T, tn + T]. The sequence
{Bn} of trajectories is equicontinuous and has an accumulation point B satisfying
B0 = a and Bt is a solution of (1.8) on [−T, +T]. This being true for any T the
result follows.
□
Case 2 αn small not vanishing.
Proposition 1.7. If Z0 is a global attractor for ( 1.8), then for any ε > 0
there exists α such that if lim supn→∞αn ≤α, there exists N with d(an, Z0) ≤ε
for n ≥N. Hence a neighborhood of Z0 is still a global attractor for ( 1.9).
Proof. The proof of Proposition 1.6 implies easily the result.
□
We are now in position to study the initial discrete time ﬁctitious play proce-
dure.
1.4.3. Discrete time.
Recall that XF × YF denote the product of the sets of optimal strategies in the
zero-sum game with payoﬀF.
Proposition 1.8. (DFP) converges to XF ×YF in the continuous saddle zero-
sum case.
Proof. The result follows from 1) the properties of the continuous time pro-
cess, Corollary 1.1, 2) the approximation result, Proposition 1.6 and 3) the fact that
the discrete time process (DFP) is a DDA of the continuous time one (CFP).
□
The initial convergence result in the ﬁnite case is due to Robinson (1951). Her
proof is quite involved and explicitly uses the ﬁniteness of the strategy sets.
In this framework one has also the next result on the payoﬀs which is not implied
by the convergence of the marginal empirical plays. In fact the distribution of the
moves at each stage need not converge.
Proposition 1.9. (Rivi`ere, 1997)
The average of the realized payoﬀs along (DFP) converges to the value in the ﬁnite
zero-sum case.
Proof. Write X = Δ(I), Y = Δ(J) and let Un = n
p=1 F(., jp) be the sum of
the columns played by player 2. Consider the sum of the realized payoﬀs
Rn =
n

p=1
F(ip, jp) =
n

p=1
(U ip
p −U ip
p−1)
Thus
Rn =
n

p=1
U ip
p −
n−1

p=1
U ip+1
p
= U in
n +
n−1

p=1
(U ip
p −U ip+1
p
)
                                                                                                                    
                                                                                                               

88
SYLVAIN SORIN
but the ﬁctitious property implies, since ip+1 is a best reply to ¯Up, that
U ip
p −U ip+1
p
≤0.
Thus lim sup Rn
n
≤lim sup maxi
Ui
n
n ≤v by the previous Proposition 1.8 and the
dual property implies the result.
□
To summarize, in zero sum games the average empirical marginal distribution
of moves are close to optimal strategies and the average payoﬀclose to the value
when the number of repetitions is large enough and both players follow (DFP).
We turn now to general I player games.
1.5. Potential games.
For a general presentation of this class, see the chapter by W. Sandholm. Since
we are dealing with best-reply based processes, we can assume that the players
share the same payoﬀfunction.
Hence the game is deﬁned by a continuous payoﬀfunction F from X to R where
each Xi, i ∈I is a compact convex subset of an euclidean space. Let NE(F) be
the set of Nash equilibria of the game deﬁned by F.
1.5.1. Discrete time.
We study here the ﬁnite case and we follow Monderer and Shapley (1996).
Recall that xn converges to NE(F) if d(xn, NE(F)) goes to 0. Since F is continuous
and X is compact, an equivalent property is to require that for any ε > 0, for any
n large enough xn is an ε-equilibrium in the sense that:
F(xn) + ε ≥F(xi, x−i
n )
for all xi ∈Xi and all i ∈I.
Proposition 1.10. (DFP) converges to NE(F).
Proof. Since F is multilinear and bounded, one has:
F(¯xn+1) −F(¯xn) = F(¯xn +
1
n + 1(xn+1 −¯xn)) −F(¯xn)
hence, by a Taylor approximation
F(¯xn+1) −F(¯xn) ≥

i
1
n + 1[F(xi
n+1, ¯x−i
n ) −F(¯xn)] −
K1
(n + 1)2
for some constant K1 independent of n. Let an+1 = 
i[F(xi
n+1, ¯x−i
n ) −F(¯xn)],
which is ≥0 by deﬁnition of (DFP). Adding the previous inequality implies
F(¯xn+1) ≥
n+1

m=1
am
m −K2
for some constant K2. Since am ≥0 and F is bounded, n+1
m=1
am
m converges.
This property in turn implies
(1.10)
lim
N→∞
1
N

n≤N
an = 0,
Now a consequence of (1.10) is that, for any ε > 0,
(1.11)
#{n ≤N; ¯xn /∈NEε(F)}
N
→0,
as N →∞.
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
89
In fact, there exists δ > 0 such that ¯xn /∈NEε(F) forces an+1 ≥δ. Inequality
(1.11) in turns implies that ¯xn belongs to NE2ε(F) for n large enough. Otherwise
¯xm /∈NEε(F) for all m in a neighborhood of n of non negligible relative size of the
order O(ε) . (This is a general property of Cesaro mean of Cesaro means).
□
1.5.2. Continuous time.
The ﬁnite case was studied in Harris (1998), the compact case in Benaim, Hofbauer
and Sorin (2005).
Let (H’) be the following hypothesis: F is deﬁned on a product X of compact
convex subsets Xi of a euclidean space, C1 and concave in each variable.
Proposition 1.11. Under (H’), (CBR) converges to NE(F).
Proof. Let W(x) = 
i[Gi(x) −F(x)] where Gi(x) = maxs∈Xi F(s, x−i).
Thus x is a Nash equilibrium iﬀW(x) = 0. Let xt be a solution of (CBR) and
consider ft = F(xt). Then ˙ft = 
i DiF(xt) ˙xi
t. By concavity one obtains:
F(xi
t, x−i
t ) + DiF(xi
t, x−i
t ) ˙xi
t ≥F(xi
t + ˙xi
t, x−i
t )
which implies
˙ft ≥

i
[F(xi
t + ˙xi
t, x−i
t ) −F(xt)] = W(xt) ≥0
hence f is increasing but bounded. f is thus constant on the limit set L(x). By
the previous inequality, for any accumulation point x∗one has W(x∗) = 0 and x∗
is a Nash equilibrium.
□
In this framework also, one can deduce the convergence of the discrete time
process from the properties of the continuous time analog, however NE(F) is not a
global attractor and the proof is much more involved (Benaim, Hofbauer and Sorin,
2005).
Proposition 1.12. Assume F(XF ) with non empty interior.
Then (DFP)
converges to NE(F).
Proof. Contrary to the zero-sum case where XF × YF was a global attractor
the proof uses here the tools of stochastic approximation, see Section 5, Proposition
5.3, with −F as Lyapounov function and NE(F) as critical set and Theorem 5.3.
□
Remarks. Note that one cannot expect uniform convergence. See the standard
symmetric coordination game:
(1, 1)
(0, 0)
(0, 0)
(1, 1)
The only attractor that contains NE(F) is the diagonal. In particular convergence
of (CFP) does not imply directly convergence of (DFP). Note that the equilibrium
(1/2, 1/2) is unstable but the time to go from (1/2+, 1/2−) to (1, 0) is not bounded.
1.6. Complements.
We assume here the payoﬀto be multilinear and we state several properties of
(DFP) and (CFP).
                                                                                                                    
                                                                                                               

90
SYLVAIN SORIN
1.6.1. General properties.
Strict Nash are asymptotically stable and stricly dominated strategies are elimi-
nated.
1.6.2. Anticipated and realized payoﬀ.
Monderer, Samet and Sela (1997) introduce a comparison between the anticipated
payoﬀat stage n (Ei
n = F i(xi
n, ¯x−i
n−1)) and the average payoﬀup to stage n (exclu-
sive) (Ai
n =
1
n−1
n−1
p=1 F i(xp)).
Proposition 1.13. Assume (DFP) for player i (with 2 players or correlated
(DFP)), then
(1.12)
Ei
n ≥Ai
n.
Proof. In fact, by deﬁnition of (DFP) and by linearity:
(1.13)

m≤n−1
F i(xi
n, x−i
m ) ≥

m≤n−1
F i(s, x−i
m ),
∀s ∈Xi.
Write (n −1)Ei
n = bn = 
m≤n−1 a(n, m) for the left hand side.
By choosing
s = xi
n−1 one obtains
bn ≥a(n −1, n −1) + bn−1
hence by induction
Ei
n ≥Ai
n =

m≤n−1
a(m, m)/(n −1).
□
Remark
This is a unilateral property: no hypothesis is made on the behavior of player −i.
Corollary 1.2. The average payoﬀs converge to the value for (DFP) in the
zero-sum case.
Proof. Recall that in this case E1
n (resp. E2
n) converges to v (resp. −v), since
¯x−i
n converges to the set of optimal strategies of −i.
□
The corresponding result in the continuous time setting is
Proposition 1.14. Assume (CFP) for player i in a two-person game, then
lim
t→+∞(Ei
t −Ai
t) = 0.
Proof. Denote by αs the move at time s so that:
txt =
 t
0
αsds.
and αt ∈BR1(yt). One has
t ˙xt + xt = αt
which is
˙xt ∈1
t [BR1(yt) −xt].
Hence the anticipated payoﬀfor player 1 is
E1
t = F 1(αt, yt)
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
91
and the past average payoﬀsatisﬁes
tA1
t =
 t
0
F 1(αs, βs)ds.
Taking derivatives one obtains
d
dt[tA1
t] = F 1(xt + t ˙xt, yt + t ˙yt) = F 1(αt, βt)
d
dt[tE1
t ] = E1
t + t d
dtE1
t .
But D1F 1(α, y) = 0 (envelope theorem) and D2F 1(α, y) ˙y = F 1(α, ˙y) by linearity.
Using again linearity one obtains
d
dt[tE1
t ] = F 1(xt + t ˙xt, yt) + F 1(xt + t ˙xt, t ˙yt) = d
dt[tA1
t]
hence there exists C such that
Et −At = C
t .
□
Corollary 1.3. Convergence of the average payoﬀs to the value holds for
(CFP) in the zero-sum case.
Proof. Since yt converges to YF , E1
t and the average payoﬀconverges to the
value.
□
1.6.3. Improvement principle.
An interesting property is due to Monderer and Sela (1993). Note that it is not
expressed in the usual state variable (¯xn) but is related to Myopic Adjustment
Dynamics satisfying: F( ˙x, x) ≥0.
Proposition 1.15. Assume (DFP) for player i with 2 players; then
(1.14)
F i(xi
n, x−i
n−1) ≥F i(xn−1).
Proof. In fact the (DFP) property implies
(1.15)
F i(xi
n−1, x−i
n−2) ≥F i(xi
n, x−i
n−2)
and
(1.16)
F i(xi
n, x−i
n−1) ≥F i(xi
n−1, x−i
n−1).
Hence if equation (1.14) is not satisﬁed adding it to (1.15) and using the linearity
of the payoﬀwould contradict (1.16).
□
These properties will be useful in proving non convergence.
                                                                                                                    
                                                                                                               

92
SYLVAIN SORIN
1.7. Shapley’s example.
Consider the next two player game, due to Shapley (1964):
G =
(0, 0)
(a, b)
(b, a)
(b, a)
(0, 0)
(a, b)
(a, b)
(b, a)
(0, 0)
with a > b > 0. Note that the only equilibrium is (1/3, 1/3, 1/3).
Proposition 1.16. (DFP) does not always converge.
Proof.
Proof 1. Starting from a Pareto entry the improvement principle (1.14) implies
that (DFP) will stay on Pareto entries. Hence the sum of the stage payoﬀs will
always be (a+b). If (DFP) converges then it converges to (1/3, 1/3, 1/3) so that the
anticipated payoﬀconverges to the Nash payoﬀa+b
3
which contradicts inequality
(1.12).
Proof 2. Add a line to the Shapley matrix G deﬁning a new matrix
G’ =
(0, 0)
(a, b)
(b, a)
(b, a)
(0, 0)
(a, b)
(a, b)
(b, a)
(0, 0)
(c, 0)
(c, 0)
(c, 0)
with 2a > b > c > a+b
3 .
By the improvement principle (1.14), starting from a Pareto entry one will stay
on the Pareto set, hence line 4 will not be played so that (DFP) in G′ is also
(DFP) in G. If there were convergence it would be to a Nash equilibrium hence
to (1/3, 1/3, 1/3) in G, thus to [(1/3, 1/3, 1/3, 0); (1/3, 1/3, 1/3)] in G′. But a best
reply for player 1 to (1/3, 1/3, 1/3) in G′ is the fourth line, contradiction.
Proof 3. Following Shapley (1964) let us study explicitly the (DFP) trajectory.
Starting from (12), there is a cycle : 12, 13, 23, 21, 31, 32, 12,... Let r(ij) be
the duration of the corresponding entry and α the vector of cumulative payoﬀs of
player 1 at the beginning of the cycle i.e. if it occurs at stage n + 1, given by:
αi =
n

m=1
Aimjm
which is proportional to the payoﬀof move i against the empirical average ¯yn.
Thus, after r(12) stages of (12) and r(13) stages of (13) the new vector α′ satisﬁes
α′
1 = α1 + r(12)a + r(13)b
α′
2 = α2 + r(12)0 + r(13)a
and then player 1 switches to move 2, hence one has
α′
2 ≥α′
1
but also
α1 ≥α2
(because 1 was played) so that
α′
2 −α2 ≥α′
1 −α1
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
93
which gives
r(13)(a −b) ≥r(12)a
and by induction at the next round
r′(11) ≥[
a
(a −b)]6r(11)
so that exponential growth occurs and the empirical distribution does not converge
(compare with the Shapley triangle, see Gaunersdorfer and Hofbauer (1995) and
the chapter by J. Hofbauer).
□
1.8. Other classes.
1.8.1. Coordination games.
A coordination game is a two person (square) game where each diagonal entry
deﬁnes a pure Nash equilibrium. There are robust examples of coordination games
where (DFP) fails to converge, Foster and Young (1998). Note that it is possible to
have convergence of (DFP) and convergence of the payoﬀs to a non Nash payoﬀ- like
always mismatching. Better processes allow to select among the memory: choose
s dates among the last m ones or work with ﬁnite memory adding a perturbation,
see the survey in Young (2004).
1.8.2. Dominance solvable games.
Convergence properties are obtained in Milgrom and Roberts (1991).
1.8.3. Supermodular games.
In this class, convergence results are proved in Milgrom and Roberts (1990). For
the case of strategic complementarity and diminishing marginal returns see Krishna
and Sj¨ostrom (1997,1998), Berger (2008).
2. Unilateral Smooth Best Replies and Consistency
We consider here an unilateral process that will exhibit robust properties and
which is deeply related to (CFP).
2.1. Consistency.
2.1.1. Model and deﬁnitions.
Consider a discrete time process {Un} of vectors in U = [0, 1]K.
At each stage n, a player having observed the past realizations U1, ..., Un−1, chooses
a component kn in K. Then Un is announced and the outcome at that stage is
ωn = U kn
n .
A strategy σ in this prediction problem is speciﬁed by σ(hn−1) ∈Δ(K) (the simplex
of RK) which is the probability distribution of kn given the past history hn−1 =
(U1, k1, ..., Un−1, kn−1).
External regret
The regret given k ∈K and U ∈RK is deﬁned by the vector R(k, U) ∈RK with
R(k; U)ℓ= U ℓ−U k, ℓ∈K.
Hence the evaluation at stage n is Rn = R(kn, Un) i.e. Rk
n = U k
n −ωn.
Given a sequence {um}, we deﬁne as usual ¯un = 1
n
n
m=1 um. Hence the average
external regret vector at stage n is Rn with
R
k
n = U
k
n −ωn
It compares the actual (average) payoﬀto the payoﬀcorresponding to a constant
choice of a component, see Foster and Vohra (1999), Fudenberg and Levine (1995).
                                                                                                                    
                                                                                                               

94
SYLVAIN SORIN
Definition 2.1. A strategy σ satisﬁes external consistency (EC) if, for every
process {Um}:
max
k∈K[R
k
n]+ −→0 a.s., as n →+∞
or, equivalently
n
m=1(U k
m −ωm) ≤o(n),
∀k ∈K.
Internal regret
The evaluation at stage n is given by a K × K matrix Sn deﬁned by:
Skℓ
n =

U ℓ
n −U k
n
for k = kn
0
otherwise.
Hence the average internal regret matrix is
S
kℓ
n = 1
n
n

m=1,km=k
(U ℓ
m −U k
m).
This involves a comparison, for each component k, of the average payoﬀobtained
on the dates where k was played, to the payoﬀthat would have been induced by
an alternative choice ℓ, see Foster and Vohra (1999), Fudenberg and Levine (1999).
Note that we normalize by 1
n to ignore the scores of unfrequent moves.
Definition 2.2. A strategy σ satisﬁes internal consistency (IC) if, for every
process {Um} and every couple k, ℓ:
[S
kℓ
n ]+ −→0 a.s., as n →+∞
Note that no assumption is made on the process {Un} (like stationarity or the
Markov property), moreover the player has no a priori beliefs on the law of {Un}: we
are not in a Bayesian framework and there is in general no learning, but adaptation.
2.1.2. Application to games.
Consider a ﬁnite game with #I players having action spaces Sj, j ∈I. The game
is repeated in discrete time and after each stage the previous proﬁle of moves is
announced. Each player i knows her payoﬀfunction Gi : S = Si × S−i →R and
her observation is the vector of moves of her opponents, s−i ∈S−i.
Fix i and let K = Si. Player i knows in particular after stage n his stage payoﬀ
ωn = Gi(kn, s−i
n ) as well as his vector payoﬀUn = Gi(., s−i
n ) ∈RK. The previous
process describes precisely the situation that a player faces in a repeated game
(with complete information and standard monitoring). She ﬁrst has to choose her
action, then she discovers the proﬁle played and can evaluate her regret.
Introduce zn = 1
n
n
m=1 sm ∈Δ(S) with sm = {sj
m}, j ∈I which is the empirical
distribution of proﬁle of moves up to stage n so that by linearity
¯Rn = {Gi(k, z−i
n ) −Gi(zn); k ∈K}.
Then we can express the property on the payoﬀs as a property on the moves.
σ satisﬁes EC is equivalent to : zn →Hi a.s. with
Hi = {z ∈Δ(S); Gi(k, z−i) −Gi(z) ≤0, ∀k ∈K}.
Hi is the Hannan’s set of player i, Hannan (1957).
Similarly ¯Sn = S(zn) with
Sk,j(z) =

ℓ∈S−i
[Gi(j, ℓ) −Gi(k, ℓ)]z(k, ℓ)
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
95
and σ satisﬁes IC is equivalent to zn →Ci a.s. with
Ci = {z ∈Δ(S); Sk,j(z) ≤0, ∀k, j ∈K}
This corresponds to the set of correlated distributions z where, for each move k ∈Si,
k is a best reply of player i to the conditional distribution of z given k on S−i.
Note that ∩iCi is the set of correlated equilibrium distributions, Aumann
(1974).
In particular the existence of internally consistent procedures will provide an al-
ternative proof of existence of correlated equilibrium distributions: consider any
accumulation point of a trajectory generated by players using IC procedures.
2.2. Smooth ﬁctitious play.
We described here a procedure that will satisﬁes IC. There are two connections
with the previous section. First we will deduce properties of the random discrete
time process from properties of a deterministic continuous time counterpart. Second
the strategy is based on a smooth version of (DFP). Note that this procedure relies
only on the previous observations of the process {Un} and not on the moves of
the predictor, hence the regret needs not to be known, see Fudenberg and Levine
(1995).
Definition 2.3. A smooth perturbation of the payoﬀU is a map
V ε(x, U) = ⟨x, U⟩+ ερ(x),
with 0 < ε < ε0, such that:
(i) ρ : X →R is a C1 function with uniform norm ∥ρ∥≤1,
(ii) argmaxx∈XV ε(., U) reduces to one point and deﬁnes a continuous map brε :
U →X
called a smooth best reply function,
(iii) D1V ε(brε(U), U).Dbrε(U) = 0
(for example D1U ε(., U) is 0 at brε(U)).
A typical example is obtained via the entropy function
(2.1)
ρ(x) = −

k
xk log xk.
which leads to the smooth perturbed best reply function
(2.2)
[brε(U)]k =
exp(U k/ε)

j∈K exp(U j/ε).
Let
W ε(U) = max
x
V ε(x, U) = V ε(brε(U), U)
that is close to the largest component of U and will be the evaluation criteria. A
useful property is the following:
Lemma 2.1. (Fudenberg and Levine (1999))
DW ε(U) = brε(U).
Let us ﬁrst consider external consistency.
Definition 2.4. A smooth ﬁctitious play strategy σε associated to the smooth
best response function brε (in short a SFP(ε) strategy) is deﬁned by:
σε(hn) = brε(U n).
                                                                                                                    
                                                                                                               

96
SYLVAIN SORIN
The corresponding discrete dynamics written in the spaces of both vectors and
outcomes is
(2.3)
U n+1 −U n =
1
n + 1[Un+1 −U n].
(2.4)
ωn+1 −ωn =
1
n + 1[ωn+1 −ωn].
with
(2.5)
E(ωn+1|Fn) = ⟨brε(U n), Un+1⟩
which express the fact that the choice of the component of the unknown vector
Un+1 is done according to σε(hn) = brε(U n).
We now use the properties of Section 5 to obtain, following Bena¨ım, Hofbauer
and Sorin (2006):
Lemma 2.2. The process (U n, ωn) is a Discrete Stochastic Approximation of
the diﬀerential inclusion with values in RK × R
(2.6)
( ˙u, ˙ω) ∈{(U −u, ⟨brε(u), U⟩−ω); U ∈U}.
The main property of the continuous dynamics is given by:
Theorem 2.1. The set {(u, ω) ∈U × R : W ε(u) −ω ≤ε} is a global attracting
set for the continuous dynamics.
In particular, for any η > 0, there exists ¯ε such that for ε ≤¯ε, lim supt→∞W ε(u(t))−
ω(t) ≤η (i.e. continuous SFP(ε) satisﬁes η-consistency).
Proof. Let q(t) = W ε(u(t)) −ω(t).
Taking time derivative one obtains, using the previous two Lemmas:
˙q(t)
=
DW ε(u(t)). ˙u(t) −˙ω(t)
=
⟨brε(u(t)), ˙u(t)⟩−˙ω(t)
=
⟨brε(u(t)), U −u(t)⟩−(⟨brε(u(t)), U⟩−ω(t))
≤
−q(t) + ε.
so that q(t) ≤ε + Me−t for some constant M.
□
In particular we deduce from Theorem 5.3 properties of the discrete time pro-
cess:
Theorem 2.2. For any η > 0, there exists ¯ε such that for ε ≤¯ε, SFP(ε) is η-
consistent.
Let us now consider internal consistency.
Deﬁne ¯Un[k] as the average of Um on the dates 1 ≤m ≤n, where k was played.
σ(hn) is now an invariant measure for the matrix deﬁned by the columns
{brε( ¯Un[k])}k∈K.
Properties similar to the above shows that σ satisﬁes IC, see Bena¨ım, Hofbauer and
Sorin (2006).
For general properties of global smooth ﬁctitious play procedures, see Hofbauer
and Sandholm (2002).
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
97
Aternative consistent procedures can be found in Hart and Mas Colell (2000, 2003),
see also Cesa-Bianchi and Lugosi (2006).
3. Best Reply and Average Replicator Dynamics
3.1. Presentation.
We follow here Hofbauer, Sorin and Viossat (2009).
Recall that in the framework of a symmetric 2 person game with K × K payoﬀ
matrix A played within a single population, the replicator dynamics is deﬁned
on the simplex Δ of RK by
(3.1)
˙xk
t = xk
t

ekAxt −xtAxt
	
,
k ∈K
(RD)
where xk
t denotes the frequency of strategy k at time t. It was introduced by Taylor
and Jonker (1978) as the basic selection dynamics for the evolutionary games of
Maynard Smith (1982).
In this framework the best reply dynamics is the diﬀerential inclusion on Δ
(3.2)
˙zt ∈BR(zt) −zt,
t ≥0
(CBR)
which is the prototype of a population model of rational (but myopic) behaviour.
Despite the diﬀerent interpretation and the diﬀerent dynamic character there
are amazing similarities in the long run behaviour of these two dynamics, that have
been summarized in the following heuristic principle:
For many games, the long run behaviour (t →∞) of the time averages Xt =
1
t

 t
0 xsds of the trajectories xt of the replicator equation is the same as for the BR
trajectories.
We provide here a rigorous statement that largely explains this heuristic by
showing that for any interior solution of (RD), for every t ≥0, xt is an approximate
best reply against Xt and the approximation gets better as t →∞. This implies
that Xt is an asymptotic pseudo trajectory of (CBR), see section 5, and hence the
limit set of Xt has the same properties as a limit set of a true orbit of (CBR), i.e.
it is invariant and internally chain transitive under (CBR).
The main tool to prove this is via the logit map which is a canonical smoothing of
the best response correspondence. We show that xt equals the logit approximation
at Xt with error rate 1
t .
3.2. Unilateral processes.
The model will be in the framework of an I-person game but we consider the
dynamics for one player, without hypotheses on the behavior of the others. The
framework is unilateral, as in the previous section, but now in continuous time.
Hence, from the point of view of this player, she is facing a (measurable) vector
outcome process U = {Ut, t ≥0}, with values in the cube C = [−c, c]K where K is
her move set and c is some positive constant. U k
t is the payoﬀat time t if k is the
move at that time. The cumulative vector outcome up to stage t is St =

 t
0 Usds
and its time average is denoted ¯Ut = 1
t St.
br denotes the (payoﬀbased) best reply correspondence from C to Δ deﬁned by
br(U) = {x ∈Δ; ⟨x, U⟩= max
y∈Δ⟨y, U⟩}.
The U-best reply process (CBR) is deﬁned on Δ by
(3.3)
˙Xt ∈[br( ¯Ut) −Xt].
                                                                                                                    
                                                                                                               

98
SYLVAIN SORIN
The U-replicator process (RP) is speciﬁed by the following equation on Δ:
(3.4)
˙xk
t = xk
t [U k
t −⟨xt, Ut⟩],
k ∈K.
Explicitly, in the framework of an I-player game with payoﬀfor player 1 deﬁned
by a function G from 
i∈I Si to R, with Xi = Δ(Si), U is the vector payoﬀi.e.
Ut = G(., x−1
t ).
If all the players follow a (payoﬀbased) continuous time correlated ﬁctitious play
dynamics, each time average strategy satisﬁes (3.3).
If all the players follow the replicator dynamics then (3.4) is the replicator dynamics
equation.
3.3. Logit rule and perturbed best reply.
Deﬁne the map L from RK to Δ by
(3.5)
Lk(V ) =
exp V k

j exp V j .
Given η > 0, let [br]η be the correspondence from C to Δ with graph being the
η-neighborhood for the uniform norm of the graph of br.
The L map and the br correspondence are related as follows:
Proposition 3.1. For any U ∈C and ε > 0
L(U/ε) ∈[br]η(ε)(U)
with η(ε) →0 as ε →0.
Remarks. L is also given by
L(V ) = argmaxx∈Δ{⟨x, V ⟩−

k
xk log xk}.
Hence introducing the (payoﬀbased) perturbed best reply brε from C to Δ deﬁned
by
brε(U) = argmaxx∈Δ{⟨x, U⟩−ε

k
xk log xk}
one has L(U/ε) = brε(U).
The map brε is the logit approximation, see (2.2).
3.4. Explicit representation of the replicator process.
The following procedure has been introduced in discrete time in the framework
of on-line algorithms under the name “multiplicative weight algorithm”, Little-
stone and Warmuth (1994). We use here the name (CEW) (continuous exponential
weight) for the process deﬁned, given U, by
xt = L(
 t
0
Usds).
The main property of (CEW) that will be used is that it provides an explicit solution
of (RP).
Proposition 3.2. (CEW) satisﬁes (RP).
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
99
Proof. Straightforward computations lead to
˙xk
t = xk
t U k
t −xk
t

j
U j
t exp

 t
0 U j
vdv

j exp

 t
0 U j
vdv
which is
˙xk
t = xk
t [U k
t −⟨xt, Ut⟩]
hence gives the previous (RP) equation (3.4).
□
The link with the best reply correspondence is the following:
Proposition 3.3. (CEW) satisﬁes
xt ∈[br]δ(t)( ¯Ut)
with δ(t) →0 as t →∞.
Proof. Write
xt = L(
 t
0
Usds) = L(t ¯Ut).
Then
xt = L(U/ε) ∈[br]η(ε)(U)
with U = ¯Ut and ε = 1/t, by Proposition 3.1. Let then δ(t) = η(1/t).
□
We describe here the consequences for the time average process. Deﬁne
Xt = 1
t
 t
0
xsds.
Proposition 3.4. If xt follows (CEW) then Xt satisﬁes
(3.6)
˙Xt ∈1
t ([br]δ(t)( ¯Ut) −Xt).
with δ(t) →0 as t →∞.
Proof. One has, taking derivatives:
t ˙Xt + Xt = xt
and the result follows from the properties of xt.
□
3.5. Consequences for games.
Consider a 2 person (bimatrix) game (A, B).
If the game is symmetric this gives rise to the single population replicator dynamics
(RD) and best reply dynamics (BRD) as deﬁned in section 1.
Otherwise, we consider the two population replicator dynamics
(3.7)
˙xk
t = xk
t

ekAyt −xtAyt
	
,
k ∈S1
˙yk
t = yk
t

xtBek −xtByt
	
,
k ∈S2
and the corresponding BR dynamics as in (3).
Let M be the state space (a simplex Δ or a product of simplices Δ1 ×Δ2). We now
use the previous results with the process U being deﬁned by Ut = Ayt for player 1,
hence ¯Ut = AYt. Note that br(AY ) = BR1(Y ).
Proposition 3.5. The limit set of every replicator time average process Xt
starting from an initial point x0 ∈M is a closed subset of M which is invariant
and internally chain transitive under (CBR).
                                                                                                                    
                                                                                                               

100
SYLVAIN SORIN
Proof. Equation (3.6) implies that Xt satisﬁes a perturbed version of (CFP)
hence Xet is a perturbed solution to the diﬀerential inclusion (CBR), according to
Section 5 and Theorems 5.1 and 5.2 apply.
□
In particular this implies:
Proposition 3.6. Let A be the global attractor (i.e., the maximal invariant
set) of (CBR). Then the limit set of every replicator time average process Xt is a
subset of A.
3.6. External consistency.
The natural continuous time counterpart of the (discrete time) notion is the
following: a procedure satisﬁes external consistency if for each process U taking
values in RK, it produces a process xt ∈Δ, such that for all k
 t
0
[U k
s −⟨xs, Us⟩]ds ≤Ct = o(t)
where, using a martingale argument, we have replaced the actual random payoﬀ
at time s by its conditional expectation ⟨xs, Us⟩.
This property says that the
(expected) average payoﬀinduced by xt along the play is asymptotically not less
than the payoﬀobtained by any ﬁxed choice k ∈K.
Proposition 3.7. (RP) satiﬁes external consistency.
Proof. By integrating equation (3.4), one obtains, on the support of x0:
 t
0
[U k
s −⟨xs, Us⟩]ds =
 t
0
˙xk
s
xks
ds = log(xk
t
xk
0
) ≤−log xk
0.
□
This result is the unilateral analog of the fact that interior rest points of (RD)
are equilibria. A myopic unilateral adjustment process provides asymptotic optimal
properties in terms of no regret.
Back to a game framework this implies that if player 1 follows (RP) the set of
accumulation points of the empirical correlated distribution process will belong to
her reduced Hannan set:
¯H1 = {θ ∈Δ(S); G1(k, θ−1) ≤G1(θ), ∀k ∈S1}
with equality for at least one component.
The example due to Viossat (2007, 2008) of a game where the limit set for the
replicator dynamics is disjoint from the unique correlated equilibrium shows that
(RP) does not satisfy internal consistency.
This later property uses additional information that is not taken into account in
the replicator dynamics. This topic deserves further study.
3.7. Comments.
We can now compare several processes in the spirit of (payoﬀbased) ﬁctitious
play.
The original ﬁctitious play process (I) is deﬁned by
xt ∈br( ¯Ut)
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
101
The corresponding time average satisﬁes (CFP).
With a smooth best reply process one has (II)
xt = brε( ¯Ut)
and the corresponding time average satisﬁes a smooth ﬁctitious play process.
Finally the replicator process (III) satisﬁes
xt = br1/t( ¯Ut)
and the time average follows a time dependent perturbation of the ﬁctitious play
process.
While in (I), the process xt follows exactly the best reply correspondence, the
induced average Xt does not have good unilateral properties.
One the other hand for (II), Xt satisﬁes a weak form of external consistency, with
an error term α(ε) vanishing with ε.
In contrast, (III) satisﬁes exact external consistency due to a both smooth and
time dependent approximation of br.
4. General Adaptive Dynamics
We consider here random processes corresponding to adaptive behavior in re-
peated interactions.
The analysis is done from the point of view of one player, having a ﬁnite set K of
actions. Time is discrete and the behavior of the player depends upon a parameter
z ∈Z.
At stage n, the state is zn−1 and the process is deﬁned by two functions:
a decision map σ from Z to Δ(K) (the simplex on K) deﬁning the law πn of the
current action kn as a function of the parameter:
πn = σ(zn−1)
and given the observation ωn of the player, after the play at stage n, an updating
rule for the state variable, that depends upon the stage:
zn = Φn(zn−1, ωn).
Remark
Note that the decision map is stationary but that the updating rule may depend
upon the stage.
A typical assumption in game theory is that the player knows his payoﬀfunction
G : K × L →R and that the observation ω is the vector of moves of his opponents,
ℓ∈L. In particular ωn contains the stage payoﬀgn = G(kn, ℓn) as well as the
vector payoﬀUn = G(., ℓn) ∈RK.
Example 1: Fictious Play
The state space is usually the empirical distribution of actions of the opponents
but one can as well take ωn = Un, the vector payoﬀ, then zn = U n is the average
vector payoﬀthus satisﬁes:
zn = (n −1)zn−1 + Un
n
and
σ(z) ∈BR(z)
or
σ(z) = BRε(z).
                                                                                                                    
                                                                                                               

102
SYLVAIN SORIN
Example 2: Potential regret dynamics
Here
Rn = Un −gn1
is the “regret vector” at stage n and the updating rule zn = Φn(zn−1, ωn) is simply
zn = Rn.
Choose P to be a “potential function” for the negative orthant D = RK
−and for
z /∈D let σ(z) be proportional to ∇P(z).
Example 3: Cumulative proportional reinforcement
The observation ωn is only the stage payoﬀgn (we assume all payoﬀs ≥1).
The updating rule is
zk
n = zk
n−1 + gn I{kn=k}
and the decision map is σ(z) proportional to the vector z.
There is an important literature on such reinforcement dynamics, see e.g. Beggs
(2005), B¨orgers, Morales and Sarin (2004), B¨orgers and Sarin (1997), Hopkins
(2002), Hopkins and Posch (2005), Laslier, Topol and Walliser (2001), Leslie and
Collins (2005), Pemantle (2007), Posch (1997).
Note that these three procedures can be written as
zn = (n −1)zn−1 + vn
n
where vn is a random variable depending on the action(s) ℓof the opponent(s) and
on the action kn having distribution σ(zn−1). Thus
zn −zn−1 = 1
n[vn −zn−1].
Write
vn = Eπn(vn|z1, ..., zn−1) + [vn −Eπn(vn|z1, ..., zn−1)]
and deﬁne
S(zn−1) = Co{Eπn(vn|z1, ..., zn−1); ℓ∈L}
where Co stands for the convex hull. Thus
zn −zn−1 ∈1
n[S(zn−1) −zn−1].
The diﬀerential inclusion is
(4.1)
˙z ∈S(z) −z
and the process zn is a Discrete Stochastic Approximation of (4.1), see section 5.
For further results with explicit applications of this procedure see e.g. Hof-
bauer and Sanholm (2002), Bena¨ım, Hofbauer and Sorin (2006), Cominetti, Melo
and Sorin (2010).
In conclusion, a large class of adaptive dynamics can be expressed in discrete
time as a random diﬀerence equation with vanishing step size. Information on the
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
103
asymptotic behavior can then be obtained by studing the continuous time deter-
ministic analog obtained as above.
5. Stochastic Approximation for Diﬀerential Inclusions
We summarize here results from Bena¨ım, Hofbauer and Sorin (2005).
5.1. Diﬀerential inclusions.
Given a correspondence F from Rm to itself, consider the diﬀerential inclusion
˙x ∈F(x)
(I)
It induces a set-valued dynamical system {Φt}t∈R deﬁned by
Φt(x) = {x(t) : x is a solution to (I) with x(0) = x}.
We also write x(t) = φt(x).
Definition 5.1.
1) x is a rest point if 0 ∈F(x).
2) A set C is strongly forward invariant (SFI) if Φt(C) ⊂C for all t ≥0.
3) C is invariant if for any x ∈C there exists a complete solution: φt(x) ∈C for
all t ∈R.
4) C is Lyapounov stable if:
∀ε > 0, ∃δ > 0 such that d(y, C) ≤δ implies
d(Φt(y), C) ≤ε for all t ≥0, i.e.
Φ[0,+∞)(Cδ) ⊂Cε.
5) C is a sink if there exists δ > 0 such that for any y ∈Cδ and any φ:
d(φt(y), C) →0 as t →∞.
A neighborhood U of C having this property is called a basin of attraction of C.
6) C is attracting if it is compact and the previous property is uniform. Thus there
exist δ > 0, ε0 > 0 and a map T : (0, ε0) →R+ such that: for any y ∈Cδ, any
solution φ, φt(y) ∈Cε for all t ≥T(ε), i.e.
Φ[T (ε),+∞)(Cδ) ⊂Cε,
∀ε ∈(0, ε0).
A neighborhood U of C having this property is called a uniform basin of attraction
of C and we will write (C; U) for the couple.
7) C is an attractor if it is attracting and invariant.
8) C is forward precompact if there exists a compact K and a time T such that
Φ[T,+∞)(C) ⊂K.
9) The ω-limit set of C is deﬁned by
(5.1)
ωΦ(C) = ∩s≥0∪y∈C ∪t≥s Φt(y) = ∩s≥0Φ[s,+∞)(C)
where A denotes the closure of the set A.
Definition 5.2.
i) Given a closed invariant set L, the induced set-valued dynamical system ΦL is
deﬁned on L by
ΦL
t (x) = {x(t) : x is a solution to (I) with x(0) = x and x(R) ⊂L}.
Note that L = ΦL
t (L) for all t.
ii) Let A ⊂L be an attractor for ΦL. If A ̸= L and A ̸= ∅, then A is a proper
attractor.
An invariant set L is attractor free if ΦL has no proper attractor.
                                                                                                                    
                                                                                                               

104
SYLVAIN SORIN
5.2. Attractors.
The next notion is fondamental in the analysis.
Definition 5.3.
C is asymptotically stable if it has the following properties
i) invariant
ii) Lyapounov stable
iii) sink.
Proposition 5.1. Assume C compact. Attractor is equivalent to asymptoti-
cally stable.
Proposition 5.2. Let A be a compact set, U be a relatively compact neighbor-
hood and V a function from U to R+. Consider the following properties
i) U is (SFI)
ii) V −1(0) = A
iii) V is continuous and strictly decreasing on trajectories on U \ A:
V (x) > V (y),
∀x ∈U \ A, ∀y ∈φt(x),
∀t > 0
iv) V is upper semi continuous and strictly decreasing on trajectories on U \ A.
a) Then under i), ii) and iii) A is Lyapounov stable and (A; U) is attracting.
b) Under i), ii) and iv), (B; U) is an attractor for some B ⊂A.
Definition 5.4.
A real continuous function V on U open in Rm is a Lyapunov function for A ⊂U
if : V (y) < V (x) for all x ∈U \ A, y ∈φt(x), t > 0; and V (y) ≤V (x) for all
x ∈A, y ∈φt(x) and t ≥0.
Note that for each solution φ, V is constant along its limit set
L(φ)(x) = ∩s≥0φ[s,+∞)(x).
Proposition 5.3. Suppose V is a Lyapunov function for A. Assume that V (A)
has empty interior. Let L be a non empty, compact, invariant and attractor free
subset of U. Then L is contained in A and V is constant on L.
5.3. Asymptotic pseudo-trajectories and internally chain transitive
sets.
5.3.1. Asymptotic pseudo-trajectories.
Definition 5.5. The translation ﬂow Θ : C0(R, Rm) × R →C0(R, Rm) is
deﬁned by
Θt(x)(s) = x(s + t).
A continuous function z : R+→Rm is an asymptotic pseudo-trajectory (APT) for Φ
if for all T
(5.2)
lim
t→∞
inf
x∈Sz(t)
sup
0≤s≤T
∥z(t + s) −x(s)∥= 0.
where Sx denotes the set of all solutions of (I) starting from x at 0 and S =

x∈RmSx.
In other words, for each ﬁxed T, the curve: s →z(t + s) from [0, T] to Rm shadows
some trajectory for (I) of the point z(t) over the interval [0, T] with arbitrary
accuracy, for suﬃciently large t. Hence z has a forward trajectory under Θ attracted
by S. One extends z to R by letting z(t) = z(0) for t < 0.
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
105
5.3.2. Internally chain transitive sets.
Given a set A ⊂Rm and x, y ∈A, we write x →A y if for every ε > 0 and
T > 0 there exists an integer n ∈IN, solutions x1, . . . , xn to (I), and real numbers
t1, t2, . . . , tn greater than T such that
a) xi(s) ∈A for all 0 ≤s ≤ti and for all i = 1, . . . , n,
b) ∥xi(ti) −xi+1(0)∥≤ε for all i = 1, . . . , n −1,
c) ∥x1(0) −x∥≤ε and ∥xn(tn) −y∥≤ε.
The sequence (x1, . . . , xn) is called an (ε, T) chain (in A from x to y) for (I).
Definition 5.6.
A set A ⊂Rm is internally chain transitive (ICT) if it is compact and x →A y for
all x, y ∈A.
Lemma 5.1. An internally chain transitive set is invariant.
Proposition 5.4. Let L be internally chain transitive. Then L has no proper
attracting set for ΦL.
This (ICT) notion of recurrence due to Conley (1978) for classical dynamical
systems is well suited to the description of the asymptotic behavior of APT, as
shown by the following theorem. Let
L(z) =

t≥0
{z(s) : s ≥t}
be the limit set.
Theorem 5.1. Let z be a bounded APT of (I). Then L(z) is internally chain
transitive.
5.4. Perturbed solutions.
The purpose of this paragraph is to study trajectories which are obtained as
(deterministic or random) perturbations of solutions of (I).
5.4.1. Perturbed solutions.
Definition 5.7.
A continuous function y : R+ = [0, ∞) →Rm is a perturbed solution to (I) if it
satisﬁes the following set of conditions (II):
i) y is absolutely continuous.
ii) There exists a locally integrable function t →U(t) such that
lim
t→∞sup
0≤v≤T

 t+v
t
U(s) ds
 = 0
for all T > 0
iii)
dy(t)
dt
−U(t) ∈F δ(t)(y(t))
for almost every t > 0, for some function δ : [0, ∞) →R with δ(t) →0 as t →∞.
Here F δ(x) := {y ∈Rm : ∃z : ∥z −x∥< δ, d(y, F(z)) < δ}.
The aim is to investigate the long-term behavior of y and to describe its limit
set L(y) in terms of the dynamics induced by F.
Theorem 5.2. Any bounded solution y of (II) is an APT of (I).
                                                                                                                    
                                                                                                               

106
SYLVAIN SORIN
5.4.2. Discrete stochastic approximation.
As will be shown here, a natural class of perturbed solutions to F arises from certain
stochastic approximation processes.
Definition 5.8.
A discrete time process {xn}n∈IN with values in Rm is a solution for (III) if it
veriﬁes a recursion of the form
xn+1 −xn −γn+1Un+1 ∈γn+1F(xn),
(III)
where the characteristics γ and U satisfy
i) {γn}n≥1 is a sequence of nonnegative numbers such that

n
γn = ∞,
lim
n→∞γn = 0;
ii) Un ∈Rm are (deterministic or random) perturbations.
To such a process is naturally associated a continuous time process as follows.
Definition 5.9.
Let τ0 = 0
and
τn = n
i=1 γi
for n ≥1, and deﬁne the continuous time aﬃne
interpolated process w : R+ →Rm by
w(τn + s) = xn + sxn+1 −xn
τn+1 −τn
,
s ∈[0, γn+1).
(IV )
5.5. From interpolated process to perturbed solutions.
The next result gives suﬃcient conditions on the characteristics of the discrete
process (III) for its interpolation (IV ) to be a perturbed solution (II).
If (Ui) are random variables, assumptions (i) and (ii) below hold with probability
one.
Proposition 5.5. Assume that the following hold:
(i) For all T > 0
lim
n→∞sup

k−1

i=n
γi+1Ui+1
 : k = n + 1, . . . , m(τn + T)

= 0,
where
(5.3)
m(t) = sup{k ≥0 : t ≥τk};
(ii) supn ∥xn∥= M < ∞.
Then the interpolated process w is a perturbed solution of (I).
We describe now suﬃcient conditions.
Let (Ω, Ψ, P) be a probability space and {Ψn}n≥0 a ﬁltration of Ψ (i.e., a non-
decreasing sequence of sub-σ-algebras of Ψ). A stochastic process {xn} given by
(III) satisﬁes the Robbins–Monro condition with martingale diﬀerence noise if its
characteristics satisfy the following:
i) {γn} is a deterministic sequence.
ii) {Un} is adapted to {Ψn}, which means that Un is measurable with respect to
Ψn for each n ≥0.
iii) E(Un+1 | Ψn) = 0.
The next proposition is a classical estimate for stochastic approximation processes.
Note that F does not appear, see Bena¨ım (1999) for a proof and further references.
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
107
Proposition 5.6. Let {xn} given by (III) be a Robbins–Monro process. Sup-
pose that for some q ≥2
sup
n E(∥Un∥q) < ∞
and

n
γ1+q/2
n
< ∞.
Then assumption (i) of Proposition 5.5 holds with probability 1.
Remark. Typical applications are
i) Un uniformly bounded in L2 and γn = 1
n,
ii) Un uniformly bounded and γn = o(
1
log n).
5.6. Main result.
Consider a random discrete process deﬁned on a compact subset of RK and
satisfying the diﬀerential inclusion :
Yn −Yn−1 ∈an[T(Yn−1) + Wn]
where
i) T is an u.s.c. correspondence with compact convex values
ii) an ≥0, 
n an = +∞, 
n a2
n < +∞
iii) E(Wn|Y1, ..., Yn1) = 0.
Theorem 5.3. The set of accumulation points of {Yn} is almost surely a com-
pact set, invariant and attractor free for the dynamical system deﬁned by the dif-
ferential inclusion:
˙Y ∈T(Y ).
References
[1] Aubin J.-P. and A. Cellina (1984) Diﬀerential Inclusions, Springer.
[2] Auer P., Cesa-Bianchi N., Freund Y. and R.E. Shapire (2002) The nonstochastic multi-
armed bandit problem, SIAM J. Comput., 32, 48-77.
[3] Aumann R.J. (1974) Subjectivity and correlation in randomized strategies, Journal of Math-
ematical Economics, 1, 67-96.
[4] Beggs A. (2005) On the convergence of reinforcement learning, Journal of Economic Theory,
122, 1-36.
[5] Bena¨ım M. (1996) A dynamical system approach to stochastic approximation, SIAM Journal
on Control and Optimization, 34, 437-472.
[6] Bena¨ım M. (1999) Dynamics of Stochastic Algorithms, S´eminaire de Probabilit´es, XXIII,
Az´ema J. and alii eds, Lectures Notes in Mathematics, 1709, Springer, 1-68.
[7] Bena¨ım M. and M.W. Hirsch (1996) Asymptotic pseudotrajectories and chain recurrent
ﬂows, with applications, J. Dynam. Diﬀerential Equations, 8, 141-176.
[8] Bena¨ım M. and M.W. Hirsch (1999) Mixed equilibria and dynamical systems arising from
ﬁctitious play in perturbed games, Games and Economic Behavior, 29, 36-72.
[9] Bena¨ım M., J. Hofbauer and S. Sorin (2005) Stochastic approximations and diﬀerential
inclusions, SIAM J. Opt. and Control, 44, 328-348.
[10] Bena¨ım M., J. Hofbauer and S. Sorin (2006) Stochastic approximations and diﬀerential
inclusions. Part II: applications, Mathematics of Operations Research, 31, 673-695.
[11] Berger U. (2005) Fictitious play in 2×n games, Journal of Economic Theory, 120, 139-154.
[12] Berger U. (2008) Learning in games with strategic complementarities revisited, Journal of
Economic Theory, 143, 292-301.
[13] B¨orgers T., A. Morales and R. Sarin (2004) Expedient and monotone learning rules,
Econometrica, 72, 383-406.
[14] B¨orgers T. and R. Sarin (1997) Learning through reinforcement and replicator dynamics,
Journal of Economic Theory, 77, 1-14.
                                                                                                                    
                                                                                                               

108
SYLVAIN SORIN
[15] Brown G. W. (1949) Some notes on computation of games solutions, RAND Report P-78,
The RAND Corporation, Santa Monica, California.
[16] Brown G. W. (1951) Iterative solution of games by ﬁctitious play, in Koopmans T.C. (ed.)
Activity Analysis of Production and Allocation , Wiley, 374-376.
[17] Brown G.W. and J. von Neumann (1950) Solutions of games by diﬀerential equations,
Contributions to the Theory of Games I, Annals of Mathematical Studies, 24, 73-79.
[18] Cesa-Bianchi N. and G. Lugosi (2006) Prediction, Learning and Games, Cambridge Uni-
versity Press.
[19] Cominetti R., E. Melo and S. Sorin (2010) A payoﬀ-based learning procedure and its
application to traﬃc games, Games and Economic Behavior, 70, 71-83.
[20] Conley C.C. (1978) Isolated Invariant Sets and the Morse Index, CBMS Reg. Conf. Ser. in
Math. 38, AMS, Providence, RI, 1978.
[21] Foster D. and R. Vohra (1997) Calibrated leaning and correlated equilibria, Games and
Economic Behavior, 21, 40-55.
[22] Foster D. and R. Vohra (1999) Regret in the on-line decision problem, Games and Eco-
nomic Behavior, 29, 7-35.
[23] Foster D. and P. Young (1998) On the nonconvergence of ﬁctitons play in coordination
games, Games and Economic Behavior, 25, 79-96.
[24] Fudenberg D. and D. K. Levine (1995) Consistency and cautious ﬁctitious play, Journal
of Economic Dynamics and Control, 19, 1065-1089.
[25] Fudenberg D. and D. K. Levine (1998) The Theory of Learning in Games, MIT Press.
[26] Fudenberg D. and D. K. Levine (1999) Conditional universal consistency, Games and
Economic Behavior, 29, 104-130.
[27] Gaunersdorfer A. and J. Hofbauer
(1995) Fictitious play, Shapley polygons and the
replicator equation, Games and Economic Behavior, 11, 279-303.
[28] Gilboa I. and A. Matsui (1991) Social stability and equilibrium, Econometrica, 59, 859-867.
[29] Hannan J.
(1957) Approximation to Bayes risk in repeated plays, in Drescher M., A.W.
Tucker and P. Wolfe (eds.),Contributions to the Theory of Games, III, Princeton University
Press, 97-139.
[30] Harris C. (1998) On the rate of convergence of continuous time ﬁctitious play, Games and
Economic Behavior, 22, 238-259.
[31] Hart S. (2005) Adaptive heuristics, Econometrica, 73, 1401-1430.
[32] Hart S. and A. Mas-Colell (2000) A simple adaptive procedure leading to correlated
equilibria, Econometrica, 68, 1127-1150.
[33] Hart S. and A. Mas-Colell (2003) Regret-based continuous time dynamics, Games and
Economic Behavior, 45, 375-394.
[34] Hofbauer J. (1995) Stability for the best response dynamics, mimeo.
[35] Hofbauer J. (1998) From Nash and Brown to Maynard Smith: equilibria, dynamics and
ESS, Selection, 1, 81-88.
[36] Hofbauer J. and W. H. Sandholm (2002) On the global convergence of stochastic ﬁctitious
play, Econometrica, 70, 2265-2294.
[37] Hofbauer J. and W. H. Sandholm (2009) Stable games and their dynamics, Journal of
Economic Theory, 144, 1665-1693.
[38] Hofbauer J. and K. Sigmund (1998) Evolutionary Games and Population Dynamics, Cam-
bridge U.P.
[39] Hofbauer J. and K. Sigmund (2003) Evolutionary games dynamics, Bulletin A.M.S., 40,
479-519.
[40] Hofbauer J. and S. Sorin (2006) Best response dynamics for continuous zero-sum games,
Discrete and Continuous Dynamical Systems-series B, 6, 215-224.
[41] Hofbauer J., S. Sorin and Y. Viossat
(2009) Time average replicator and best reply
dynamics, Mathematics of Operations Research, 34, 263-269.
[42] Hopkins E. (1999) A note on best response dynamics, Games and Economic Behavior, 29,
138-150.
[43] Hopkins E. (2002) Two competing models of how people learn in games, Econometrica, 70,
2141-2166.
[44] Hopkins E. and M. Posch
(2005) Attainability of boundary points under reinforcement
learning, Games and Economic Behavior, 53, 110-125.
                                                                                                                    
                                                                                                               

ADAPTIVE DYNAMICS
109
[45] Krishna V. and T. Sj¨ostrom (1997) Learning in games: Fictitious play dynamics, in Hart
S. and A. Mas-Colell (eds.), Cooperation: Game-Theoretic Approaches, NATO ASI Serie A,
Springer, 257-273.
[46] Krishna V. and T. Sj¨ostrom (1988) On the convergence of ﬁctitious play, Mathematics of
Operations Research, 23, 479- 511.
[47] Laslier J.-F., R. Topol and B. Walliser (2001) A behavioral learning process in games,
Games and Economic Behavior, 37, 340-366.
[48] Leslie D. S. and E.J. Collins, (2005) Individual Q-learning in normal form games, SIAM
Journal of Control and Optimization, 44, 495-514.
[49] Littlestone N. and M.K. Warmuth (1994) The weighted majority algorithm, Information
and Computation, 108, 212-261.
[50] Maynard Smith J. (1982) Evolution and the Theory of Games, Cambridge U.P.
[51] Milgrom P. and J. Roberts (1990) Rationalizability, learning and equilibrium in games
with strategic complementarities, Econometrica, 58, 1255-1277.
[52] Milgrom P. and J. Roberts (1991) Adaptive and sophisticated learning in normal form
games, Games and Economic Behavior, 3, 82-100.
[53] Monderer D., Samet D. and A. Sela (1997) Belief aﬃrming in learning processes, Journal
of Economic Theory, 73, 438-452.
[54] Monderer D. and A. Sela (1996) A 2x2 game without the ﬁctitious play property, Games
and Economic Behavior, 14, 144-148.
[55] Monderer D. and L.S. Shapley (1996) Potential games, Games and Economic Behavior,
14, 124-143.
[56] Monderer D. and L.S. Shapley (1996) Fictitious play property for games with identical
interests, Journal of Economic Theory, 68, 258-265.
[57] Pemantle R. (2007) A survey of random processes with reinforcement, Probability Surveys,
4, 1-79.
[58] Posch M. (1997) Cycling in a stochastic learning algorithm for normal form games, J. Evol.
Econ., 7, 193-207.
[59] Rivi`ere P. (1997) Quelques Mod`eles de Jeux d’Evolution, Th`ese, Universit´e P. et M. Curie-
Paris 6.
[60] Robinson J. (1951)
An iterative method of solving a game, Annals of Mathematics, 54,
296-301.
[61] Shapley L. S. (1964) Some topics in two-person games, in Dresher M., L.S. Shapley and
A.W. Tucker (eds.), Advances in Game Theory, Annals of Mathematics 52, Princeton U.P.,
1-28.
[62] Sorin S. (2007) Exponential weight algorithm in continuous time, Mathematical Program-
ming, Ser. B , 116, 513-528.
[63] Taylor P. and L. Jonker (1978) Evolutionary stable strategies and game dynamics, Math-
ematical Biosciences, 40, 145-156.
[64] Viossat Y. (2007) The replicator dynamics does not lead to correlated equilibria, Games
and Economic Behavior, 59, 397-407.
[65] Viossat Y. (2008) Evolutionary dynamics may eliminate all strategies used in correlated
equilibrium, Mathematical Social Science, 56, 27-43.
[66] Young P. (2004) Strategic Learning and its Limits, Oxford U.P. .
Combinatoire et Optimisation, IMJ, CNRS UMR 7586, Facult´e de Math´ematiques,
Universit´e P. et M. Curie - Paris 6, Tour 15-16, 1i`ere ´etage, 4 Place Jussieu, 75005 Paris
and Laboratoire d’Econom´etrie, Ecole Polytechnique, France
E-mail address: sorin@math.jussieu.fr
http://www.math.jussieu.fr/ sorin/
                                                                                                                    
                                                                                                               

This page intentionally left blank 
                                                                                                                    
                                                                                                               

Proceedings of Symposia in Applied Mathematics
Volume 69, 2011
Stochastic Evolutionary Game Dynamics:
Foundations, Deterministic Approximation,
and Equilibrium Selection
William H. Sandholm
Abstract. We present a general model of stochastic evolution in games played
by large populations of anonymous agents. Agents receive opportunities to
revise their strategies by way of independent Poisson processes. A revision
protocol describes how the probabilities with which an agent chooses each of
his strategies depend on his current payoﬀopportunities and the current be-
havior of the population. Over ﬁnite time horizons, the population’s behavior
is well-approximated by a mean dynamic, an ordinary diﬀerential equation de-
ﬁned by the expected motion of the stochastic evolutionary process. Over the
inﬁnite time horizon, the population’s behavior is described by the stationary
distribution of the stochastic evolutionary process. If limits are taken in the
population size, the level of noise in agents’ revision protocols, or both, the
stationary distribution may become concentrated on a small set of popula-
tion states, which are then said to be stochastically stable. Stochastic stability
analysis allows one to obtain unique predictions of very long run behavior even
when the mean dynamic admits multiple locally stable states. We present a
full analysis of the asymptotics of the stationary distribution in two-strategy
games under noisy best protocols, and discuss extensions of this analysis to
other settings.
1. Introduction
Evolutionary game theory studies the behavior of large populations of agents
who repeatedly engage in anonymous strategic interactions—that is, interactions
in which each agent’s outcome depends not only on his own choice, but also on the
distribution of others’ choices. Applications range from natural selection in animal
populations, to driver behavior in highway networks, to consumer choice between
diﬀerent technological standards, to the design of decentralized controlled systems.
In an evolutionary game model, changes in agents’ behavior may be driven
either by natural selection via diﬀerences in birth and death rates in biological
contexts, or by the application of myopic decision rules by individual agents in
economic contexts. The resulting dynamic models can be studied using tools from
the theory of dynamical systems and from the theory of stochastic processes, as
1991 Mathematics Subject Classiﬁcation. Primary 91A22; Secondary 60J20, 37N40.
Key words and phrases. Evolutionary game theory, Markov processes.
I thank Sylvain Sorin for helpful comments. Financial support from NSF Grant SES-0851580 is
gratefully acknowledged.
111
                                           
                                                                                                                    
                                                                                                               

112
WILLIAM H. SANDHOLM
well as those from stochastic approximation theory, which provides important links
between the two more basic ﬁelds.
In these notes, we present a general model of stochastic evolution in large-
population games, and oﬀer a glimpse into the relevant literature by presenting a
selection of basic results. In Section 2, we describe population games themselves,
and oﬀer a few simple applications. In Sections 3 and 4, we introduce our stochas-
tic evolutionary process. To deﬁne this process, we suppose that agents receive
opportunities to revise their strategies by way of independent Poisson processes.
A revision protocol describes how the probabilities with which an agent chooses
each of his strategies depend on his current payoﬀopportunities and the current
behavior of the population. Together, a population game, a revision protocol, and
a population size implicitly deﬁne the stochastic evolutionary process, a Markov
process on the set of population states.
In Section 4, we show that over ﬁnite
time horizons, the population’s behavior is well-approximated by a mean dynamic,
an ordinary diﬀerential equation deﬁned by the expected motion of the stochastic
evolutionary process.
To describe behavior over very long time spans, we turn to an inﬁnite-horizon
analysis, in which the population’s behavior is described by the stationary distri-
bution of the stochastic evolutionary process. We begin the presentation in Section
5, which reviews the relevant deﬁnitions and results from the theory of ﬁnite-state
Markov processes and presents a number of examples. In order to obtain tight
predictions about very long run play, one can examine the limit of the station-
ary distributions as the population size grows large, the level of noise in agents’
decisions becomes small, or both. The stationary distribution may then become
concentrated on a small set of population states, which are said to be stochasti-
cally stable. Stochastic stability analysis allows one to obtain unique predictions of
very long run behavior even when the mean dynamic admits multiple locally stable
states. In Sections 6 and 7 we introduce the relevant deﬁnitions, and we present a
full analysis of the asymptotics of the stationary distribution for the case of two-
strategy games under noisy best response protocols. This analysis illustrates how
the speciﬁcation of the revision protocol can inﬂuence equilibrium selection results.
We conclude in Section 8 by discussing extensions of our analyses of inﬁnite-horizon
behavior to more complicated strategic settings.
This presentation is based on portions of Chapters 10–12 of Sandholm (2010c),
in which a complete treatment of the topics considered here can be found.
2. Population Games
We consider games played by a single population (i.e., games in which all
agents play equivalent roles).
We suppose that there is a unit mass of agents,
each of whom chooses a pure strategy from the set S = {1, . . . , n}. The aggregate
behavior of these agents is described by a population state; this is an element of
the simplex X = {x ∈Rn
+ : 
j∈S xj = 1}, with xj representing the proportion of
agents choosing pure strategy j. We identify a population game with a continuous
vector-valued payoﬀfunction F : X →Rn. The scalar Fi(x) represents the payoﬀ
to strategy i when the population state is x.
Population state x∗is a Nash equilibrium of F if no agent can improve his payoﬀ
by unilaterally switching strategies. More explicitly, x∗is a Nash equilibrium if
(1)
x∗
i > 0 implies that Fi(x) ≥Fj(x) for all j ∈S.
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
113
Example 2.1. In a symmetric two-player normal form game, each of the two players
chooses a (pure) strategy from the ﬁnite set S; which we write generically as S =
{1, . . . , n}. The game’s payoﬀs are described by the matrix A ∈Rn×n. Entry Aij
is the payoﬀa player obtains when he chooses strategy i and his opponent chooses
strategy j; this payoﬀdoes not depend on whether the player in question is called
player 1 or player 2.
Suppose that the unit mass of agents are randomly matched to play the sym-
metric normal form game A. At population state x, the (expected) payoﬀto strat-
egy i is the linear function Fi(x) = 
j∈S Aijxj; the payoﬀs to all strategies can be
expressed concisely as F(x) = Ax. It is easy to verify that x∗is a Nash equilibrium
of the population game F if and only if x∗is a symmetric Nash equilibrium of the
symmetric normal form game A.
♦
While population games generated by random matching are especially simple,
many games that arise in applications are not of this form.
Example
2.2. Consider the following model of highway congestion, due to Beck-
mann et al. (1956). A pair of towns, Home and Work, are connected by a network
of links. To commute from Home to Work, an agent must choose a path i ∈S con-
necting the two towns. The payoﬀthe agent obtains is the negation of the delay on
the path he takes. The delay on the path is the sum of the delays on its constituent
links, while the delay on a link is a function of the number of agents who use that
link.
Population games embodying this description are known as a congestion games.
To deﬁne a congestion game, let Φ be the collection of links in the highway network.
Each strategy i ∈S is a route from Home to Work, and so is identiﬁed with a set of
links Φi ⊆Φ. Each link φ is assigned a cost function cφ : R+ →R, whose argument
is link φ’s utilization level uφ:
uφ(x) =

i∈ρ(φ)
xi , where ρ(φ) = {i ∈S : φ ∈Φi}
The payoﬀof choosing route i is the negation of the total delays on the links in this
route:
Fi(x) = −

φ∈Φi
cφ(uφ(x)).
Since driving on a link increases the delays experienced by other drivers on
that link (i.e., since highway congestion involves negative externalities), cost func-
tions in models of highway congestion are increasing; they are typically convex as
well. Congestion games can also be used to model positive externalities, like the
choice between diﬀerent technological standards; in this case, the cost functions are
decreasing in the utilization levels.
♦
3. Revision Protocols and the Stochastic Evolutionary Process
We now introduce foundations for our models of evolutionary dynamics. These
foundations are built on the notion of a revision protocol, which describes both
the timing and results of agents’ myopic decisions about how to continue playing
the game at hand. This approach to deﬁning evolutionary dynamics was developed
in Bj¨ornerstedt and Weibull (1996), Weibull (1995), Hofbauer (1995), and Bena¨ım
and Weibull (2003), and Sandholm (2003, 2010b).
                                                                                                                    
                                                                                                               

114
WILLIAM H. SANDHOLM
3.1. Deﬁnitions. A revision protocol is a map ρ : Rn ×X →Rn×n
+
that takes
the payoﬀvectors π and population states x as arguments, and returns nonnegative
matrices as outputs. For reasons to be made clear below, scalar ρij (π, x) is called
the conditional switch rate from strategy i to strategy j.
To move from this notion to an explicit model of evolution, let us consider a
population consisting of N < ∞members. A number of the analyses to follow will
consider the limit of the present model as the population size N approaches inﬁnity.
When the population is of size N, the set of feasible social states is the ﬁnite set
X N = X ∩1
N Zn = {x ∈X : Nx ∈Zn}, a grid embedded in the simplex X.
A revision protocol ρ, a population game F, and a population size N deﬁne a
continuous-time evolutionary process—a Markov process {XN
t }—on the ﬁnite state
space X N. A one-size-ﬁts-all description of this process is as follows. Each agent in
the society is equipped with a “stochastic alarm clock”. The times between rings
of of an agent’s clock are independent, each with a rate R exponential distribution.
The ringing of a clock signals the arrival of a revision opportunity for the clock’s
owner. If an agent playing strategy i ∈S receives a revision opportunity, he switches
to strategy j ̸= i with probability ρij/R. If a switch occurs, the population state
changes accordingly, from the old state x to a new state y that accounts for the
agent’s change in strategy.
To describe the stochastic evolutionary process {XN
t } formally, it is enough to
specify its jump rates {λN
x }x∈X N , which describe the exponential rates of transitions
from each state, and its transition probabilities {P N
xy}x,y∈X N , which describe the
probabilities that a transition starting at state x ends at state y.
If the current social state is x ∈X N, then Nxi of the N agents are playing
strategy i ∈S. Since agents receive revision opportunities independently at ex-
ponential rate R, the basic properties of the exponential distribution imply that
revision opportunities arrive in the society as a whole at exponential rate NR.
When an agent playing strategy i ∈S receives a revision opportunity, he
switches to strategy j ̸= i with probability ρij/R. Since this choice is indepen-
dent of the arrivals of revision opportunities, the probability that the next revision
opportunity goes to an agent playing strategy i who then switches to strategy j is
Nxi
N
× ρij
R = xiρij
R
.
This switch decreases the number of agents playing strategy i by one and increases
the number playing j by one, shifting the state by
1
N (ej −ei).
Summarizing this analysis yields the following observation.
Observation 3.1. A population game F, a revision protocol ρ, a constant R,
and a population size N deﬁne a Markov process {XN
t } on the state space X N.
This process is described by some initial state XN
0 = xN
0 , the jump rates λN
x = NR,
and the transition probabilities
P N
x,x+z =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
xiρij(F(x), x)
R
if z = 1
N (ej −ei), i, j ∈S, i ̸= j,
1 −

i∈S

j̸=i
xiρij(F(x), x)
R
if z = 0,
0
otherwise.
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
115
3.2. Examples. In economic contexts, revision protocols of the form
(2)
ρij(π, x) = xjrij(π, x)
are called imitative protocols. These protocols can be given a very simple inter-
pretation: when an agent receives a revision opportunity, he chooses an opponent
at random and observes her strategy. If our agent is playing strategy i and the
opponent strategy j, the agent switches from i to j with probability proportional
to rij. Notice that the value of the population share xj is not something the agent
need know; this term in (2) accounts for the agent’s observing a randomly chosen
opponent.
Example
3.2. Suppose that after selecting an opponent, the agent imitates the
opponent only if the opponent’s payoﬀis higher than his own, doing so in this case
with probability proportional to the payoﬀdiﬀerence:
ρij(π, x) = xj[πj −πi]+.
This protocol is known as pairwise proportional imitation; see Helbing (1992) and
Schlag (1998).
♦
Additional references on imitative protocols include Bj¨ornerstedt and Weibull (1996),
Weibull (1995), and Hofbauer (1995).
Protocols of form (2) also appear in biological contexts, starting with the work
of Moran (1962), and revisited more recently by Nowak et al. (2004) among oth-
ers, see Nowak (2006) and Traulsen and Hauert (2009) for further references. In
these cases we refer to (2) as a natural selection protocol. The biological interpreta-
tion of (2) supposes that each agent is programmed to play a single pure strategy.
An agent who receives a revision opportunity dies, and is replaced through asex-
ual reproduction. The reproducing agent is a strategy j player with probability
ρij(π, x) = xj ˆρij(π, x), which is proportional both to the number of strategy j
players and to some function of the prevalences and ﬁtnesses of all strategies. Note
that this interpretation requires the restriction

j∈S
ρij(π, x) ≡1.
Example 3.3. Suppose that payoﬀs are always positive, and let
(3)
ρij(π, x) =
xj πj

k∈S xk πk
.
Understood as a natural selection protocol, (3) says that the probability that the
reproducing agent is a strategy j player is proportional to xjπj, the aggregate ﬁtness
of strategy j players.
In economic contexts, we can interpret (3) as an imitative protocol based on
repeated sampling. When an agent’s clock rings he chooses an opponent at ran-
dom. If the opponent is playing strategy j, the agent imitates him with probability
proportional to πj. If the agent does not imitate this opponent, he draws a new
opponent at random and repeats the procedure.
♦
In the previous examples, only strategies currently in use have any chance
of being chosen by a revising agent (or of being the programmed strategy of the
newborn agent). Under other protocols, agents’ choices are not mediated through
the population’s current behavior, except indirectly via the eﬀect of behavior on
                                                                                                                    
                                                                                                               

116
WILLIAM H. SANDHOLM
payoﬀs. These direct protocols require agents to directly evaluate the payoﬀs of each
strategy, rather than to indirectly evaluate them as under an imitative procedure.
Example 3.4. Suppose that choices are made according to the logit choice rule:
(4)
ρij(π, x) =
exp(η−1πj)

k∈S exp(η−1πk).
The interpretation of this protocol is simple. Revision opportunities arrive at unit
rate. When an opportunity is received by an i player, he switches to strategy j with
probability ρij(π, x), which is proportional to an exponential function of strategy
j’s payoﬀs. The parameter η > 0 is called the noise level. If η is large, choice
probabilities under the logit rule are nearly uniform. But if η is near zero, choices
are optimal with probability close to one, at least when the diﬀerence between the
best and second best payoﬀis not too small.
♦
4. Finite Horizon Deterministic Approximation
4.1. Mean Dynamics. A revision protocol ρ, a population game F, and a
population size N deﬁne a Markov process {XN
t } on the ﬁnite state space X N.
We now derive a deterministic process—the mean dynamic—that describes the
expected motion of {XN
t }. In Section 4.3, we will describe formally the sense in
which this deterministic process provides a very good approximation of the behavior
of the stochastic process {XN
t }, at least over ﬁnite time horizons and for large
population sizes. But having noted this result, we will focus in this section on the
deterministic process itself.
To compute the expected increment of {XN
t } over the next dt time units, recall
ﬁrst that each of the N agents receives revision opportunities via a rate R expo-
nential distribution, and so expects to receive R dt opportunities during the next dt
time units. If the current state is x, the expected number of revision opportunities
received by agents currently playing strategy i is approximately Nxi R dt. Since an
i player who receives a revision opportunity switches to strategy j with probabil-
ity ρij/R, the expected number of such switches during the next dt time units is
approximately Nxi ρij dt. Therefore, the expected change in the number of agents
choosing strategy i during the next dt time units is approximately
(5)
N
⎛
⎝
j∈S
xjρji(F(x), x) −xi

j∈S
ρij(F(x), x)
⎞
⎠dt.
Dividing expression (5) by N and eliminating the time diﬀerential dt yields a diﬀer-
ential equation for the rate of change in the proportion of agents choosing strategy
i:
(M)
˙xi =

j∈S
xjρji(F(x), x) −xi

j∈S
ρij(F(x), x).
Equation (M) is the mean dynamic (or mean ﬁeld) generated by revision pro-
tocol ρ in population game F. The ﬁrst term in (M) captures the inﬂow of agents
to strategy i from other strategies, while the second captures the outﬂow of agents
to other strategies from strategy i.
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
117
4.2. Examples. We now revisit the revision protocols from Section 3.2. To
do so, we let
F(x) =

i∈S
xiFi(x)
denote the average payoﬀobtained by the members of the population, and deﬁne
the excess payoﬀto strategy i,
ˆFi(x) = Fi(x) −F(x),
to be the diﬀerence between strategy i’s payoﬀand the population’s average payoﬀ.
Example 4.1. In Example 3.2, we introduced the pairwise proportional imitation
protocol ρij(π, x) = xj[πj −πi]+. This protocol generates the mean dynamic
(6)
˙xi = xi ˆFi(x).
Equation (6) is the replicator dynamic of Taylor and Jonker (1978), the best-known
dynamic in evolutionary game theory. Under this dynamic, the percentage growth
rate ˙xi/xi of each strategy currently in use is equal to that strategy’s current excess
payoﬀ; unused strategies always remain so. There are a variety of revision protocols
other than pairwise proportional imitation that generate the replicator dynamic as
their mean dynamics; see Bj¨ornerstedt and Weibull (1996) and Hofbauer (1995).
♦
Example
4.2. In Example 3.3, we assumed that payoﬀs are always positive, and
introduced the protocol ρij(π, x) ∝xj πj, which we interpreted both as a model
of biological natural selection and as a model of imitation with repeated sampling.
The resulting mean dynamic,
(7)
˙xi =
xiFi(x)

k∈S xkFk(x) −xi = xi ˆFi(x)
F(x) ,
is the Maynard Smith replicator dynamic, due to Maynard Smith (1982).
This
dynamic only diﬀers from the standard replicator dynamic (6) by a change of speed,
with motion under (7) being relatively fast when average payoﬀs are relatively low.
In multipopulation models, the two dynamics are less similar because the changes
in speed may diﬀer across populations, aﬀecting the direction of motion.
♦
Example
4.3. In Example 3.4 we introduced the logit choice rule ρij(π, x) ∝
exp(η−1πj). The corresponding mean dynamic,
(8)
˙xi =
exp(η−1Fi(x))

k∈S exp(η−1Fk(x)) −xi,
is called the logit dynamic, due to Fudenberg and Levine (1998).
♦
We summarize these and other examples of revision protocols and mean dynamics in
Table 1. Dynamics from the table that have not been mentioned so far include the
best response dynamic of Gilboa and Matsui (1991), the BNN dynamic of Brown
and von Neumann (1950), and the Smith (1984) dynamic. Discussion, examples,
and results concerning these and other deterministic dynamics can be found in J.
Hofbauer’s contribution to this volume.
                                                                                                                    
                                                                                                               

118
WILLIAM H. SANDHOLM
Revision protocol
Mean dynamic
Name
ρij = xj[πj −πi]+
˙xi = xi ˆFi(x)
replicator
ρij =
exp(η−1πj)

k∈S exp(η−1πk)
˙xi =
exp(η−1Fi(x))

k∈S exp(η−1Fk(x)) −xi
logit
ρij = 1{j=argmaxk∈S πk}
˙x ∈BF (x) −x
best response
ρij = [πj −
k∈S xkπk]+
˙xi = [ ˆFi(x)]+ −xi

j∈S
[ ˆFj(x)]+
BNN
ρij = [πj −πi]+
˙xi = 
j∈S
xj[Fi(x) −Fj(x)]+
−xi

j∈S
[Fj(x) −Fi(x)]+
Smith
Table 1. Five basic deterministic dynamics.
4.3. Deterministic Approximation Theorem. In Section 3, we deﬁned
the Markovian evolutionary process {XN
t } from a revision protocol ρ, a population
game F, and a ﬁnite population size N. In Section 4.1, we argued that the expected
motion of this process is captured by the mean dynamic
(M)
˙xi = V F (x) =

j∈S
xjρji(F(x), x) −xi

j∈S
ρij(F(x), x).
The basic link between the Markov process {XN
t } and its mean dynamic (M) is
provided by the following theorem (Kurtz (1970), Sandholm (2003), Bena¨ım and
Weibull (2003)).
Theorem 4.4 (Deterministic Approximation of {XN
t }). Suppose that V F is
Lipschitz continuous. Let the initial conditions XN
0 = xN
0 converge to state x0 ∈X,
and let {xt}t≥0 be the solution to the mean dynamic (M) starting from x0. Then
for all T < ∞and ε > 0,
lim
N→∞P

sup
t∈[0,T ]
XN
t −xt
 < ε

= 1.
Thus, when the population size N is large, nearly all sample paths of the Markov
process {XN
t } stay within ε of a solution of the mean dynamic (M) through time
T. By choosing N large enough, we can ensure that with probability close to one,
XN
t
and xt diﬀer by no more than ε for all t between 0 and T (Figure 1).
The intuition for this result comes from the law of large numbers. At each
revision opportunity, the increment in the process {XN
t } is stochastic. Still, the
expected number of revision opportunities that arrive during the brief time interval
I = [t, t + dt] is large—in particular, of order N dt. Since each opportunity leads to
an increment of the state of size 1
N , the size of the overall change in the state during
time interval I is of order dt. Thus, during this interval there are a large number
of revision opportunities, each following nearly the same transition probabilities,
and hence having nearly the same expected increments. The law of large numbers
therefore suggests that the change in {XN
t } during this interval should be almost
completely determined by the expected motion of {XN
t }, as described by the mean
dynamic (M).
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
119
x0
x1
x2
x3
0
X
1
X
2
X
3
X
=
Figure 1. Deterministic approximation of the Markov process {XN
t }.
It should be emphasized that Theorem 4.4 cannot be extended to an inﬁnite
horizon result. To see why not, consider the logit choice protocol (Example 4.3),
under which switches between all pairs of strategies occur with positive probability
regardless of the current state. As we discuss in Section 5, this property implies
that the induced Markov process {XN
t } is irreducible, and hence that every state
in X N is visited inﬁnitely often with probability one. This fact clearly precludes
an inﬁnite horizon analogue of Theorem 4.4. However, the failure of this result
introduces a new possibility, that of obtaining unique predictions of inﬁnite horizon
behavior. We consider this question in Sections 6 and 7.
4.4. Analysis of Deterministic Dynamics. With this justiﬁcation in hand,
one can use methods from dynamical systems theory to study the behavior of the
mean dynamic (M).
A large literature has considered this question for a wide
range of choices of the revision protocol ρ and the game F, proving a variety
of results about local stability of equilibrium, global convergence to equilibrium,
and nonconvergence.
Other contributions to this volume, in particular those of
J. Hofbauer and R. Cressman, address such results; for general references, see
Hofbauer and Sigmund (1988, 1998, 2003), Weibull (1995), Sandholm (2009), and
chapters 4–9 of Sandholm (2010c).
5. Stationary Distributions
Theorem 4.4 shows that over ﬁnite time spans, the stochastic evolutionary
process {XN
t } follows a nearly deterministic path, closely shadowing a solution
trajectory of the corresponding mean dynamic (M). But if we look at longer time
spans—that is, if we ﬁx the population size N of interest and consider the position of
the process at large values of t—the random nature of the process must assert itself.
If the process is generated by a full support revision protocol, one that always assigns
positive probabilities to transitions to all neighboring states in X N, then {XN
t } must
visit all states in X N inﬁnitely often. Evidently, an inﬁnite horizon approximation
theorem along the lines of Theorem 4.4 cannot hold. To make predictions about play
over very long time spans, we need new techniques for characterizing the inﬁnite
                                                                                                                    
                                                                                                               

120
WILLIAM H. SANDHOLM
horizon behavior of the stochastic evolutionary process. We do so by considering
the stationary distribution μN of the process {XN
t }.
A stationary distribution
is deﬁned by the property that a process whose initial condition is described by
this distribution will continue to be described by this distribution at all future
times. If {XN
t } is generated by a full support revision protocol, then its stationary
distribution μN is not only unique, but also describes the inﬁnite horizon behavior of
{XN
t } regardless of this process’s initial distribution. In principle, this fact allows us
to use the stationary distribution to form predictions about a population’s very long
run behavior that do not depend on its initial behavior. This contrasts sharply with
predictions based on the mean dynamic (M), which generally require knowledge of
the initial state.
5.1. Full Support Revision Protocols. To introduce the possibility of unique
inﬁnite-horizon predictions, we now assume in addition that the conditional switch
rates are bounded away from zero: there is a positive constant R such that
(9)
ρij(F(x), x) ≥R for all i, j ∈S and x ∈X.
We refer to a revision protocol that satisﬁes condition (9) as having full support.
Example 5.1. Best response with mutations. Under best response with mutations
at mutation rate ε > 0, called BRM (ε) for short, a revising agent switches to his
current best response with probability 1 −ε, but chooses a strategy uniformly at
random (or mutates) with probability ε > 0. Thus, if the game has two strategies,
each yielding diﬀerent payoﬀs, a revising agent will choose the optimal strategy
with probability 1 −ε
2 and will choose the suboptimal strategy with probability ε
2.
(Kandori et al. (1993), Young (1993)) ♦
Example 5.2. Logit choice. In Example 3.4 we introduced the logit choice protocol
with noise level η > 0. Here we rewrite this protocol as
(10)
ρij(π) =
exp(η−1(πj −πk∗))

k∈S exp(η−1(πk −πk∗)).
where k∗is an optimal strategy under π. Then as η approaches zero, the denomina-
tor of (10) converges to a constant (namely, the number of optimal strategies under
π), so as η−1 approaches inﬁnity, ρij(π, x) vanishes at exponential rate πk∗−πj.
(Blume (1993, 1997)) ♦
As their noise parameters approach zero, both the BRM and logit protocols
come to resemble an exact best response protocol.
But this similarity masks a
fundamental qualitative diﬀerence between the two protocols. Under best response
with mutations, the probability of choosing a particular suboptimal strategy is
independent of the payoﬀconsequences of doing so: mutations do not favor alter-
native strategies with higher payoﬀs over those with lower payoﬀs. In contrast,
since the logit protocol is deﬁned using payoﬀperturbations that are symmetric
across strategies, more costly “mistakes” are less likely to be made. One might
expect the precise speciﬁcation of mistake probabilities to be of little consequence.
But as we shall see below, predictions of inﬁnite horizon behavior hinge on the
relative probabilities of rare events, so that seemingly minor diﬀerences in choice
probabilities can lead to entirely diﬀerent predictions of behavior.
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
121
5.2. Review: Irreducible Markov Processes. The full support assump-
tion (9) ensures that at each revision opportunity, every strategy in S has a positive
probability of being chosen by the revising agent. Therefore, there is a positive
probability that the process {XN
t } will transit from any given current state x to
any other given state y within a ﬁnite number of periods. A Markov process with
this property is said to be irreducible. Below we review some basic results about
inﬁnite horizon behavior of irreducible Markov processes on a ﬁnite state space; for
details, see e.g. Norris (1997).
Suppose that {Xt}t≥0 is an irreducible Markov process on the ﬁnite state space
X , where the process has equal jump rates λx ≡l and transition matrix P. Then
there is a unique probability vector μ ∈RX
+ satisfying
(11)

x∈X
μxPxy = μy for all y ∈X .
The vector μ is called the stationary distribution of the process {Xt}. Equation
(11) tells us that if we run the process {Xt} from initial distribution μ, then at the
random time of the ﬁrst jump, the distribution of the process is also μ. Moreover,
if we use the notation Pπ(·) to represent {Xt} being run from initial distribution
π, then
(12)
Pμ (Xt = x) = μx for all x ∈X and t ≥0.
In other words, if the process starts oﬀin its stationary distribution, it remains in
this distribution at all subsequent times t.
While equation (12) tells us what happens if {Xt} starts oﬀin its stationary
distribution, our main interest is in what happens to this process in the very long
run if it starts in an arbitrary initial distribution π. Then as t grows large, the time
t distribution of {Xt} converges to μ:
(13)
lim
t→∞Pπ(Xt = x) = μx for all x ∈X .
Thus, looking at the process {Xt} from the ex ante point of view, the probable
locations of the process at suﬃciently distant future times are essentially determined
by μ.
To describe long run behavior from an ex post point of view, we need to consider
the behavior of the process’s sample paths. Here again, the stationary distribution
plays the central role. Then along almost every sample path, the proportion of time
spent at each state in the long run is described by μ:
(14)
Pπ

lim
T →∞
1
T
 T
0
1{Xt=x} dt = μx

= 1 for all x ∈X .
We can also summarize equation (14) by saying that the limiting empirical distri-
bution of {Xt} is almost surely equal to μ.
In general, computing the stationary distribution of a Markov process means
ﬁnding an eigenvector of a matrix, a task that is computationally daunting unless
the state space, and hence the dimension of the matrix, is small. But there is a spe-
cial class of Markov processes whose stationary distributions are easy to compute.
A constant jump rate Markov process {Xt} is said to be reversible if it admits a
reversible distribution: a probability distribution μ on X that satisﬁes the detailed
                                                                                                                    
                                                                                                               

122
WILLIAM H. SANDHOLM
balance conditions:
(15)
μxPxy = μyPyx for all x, y ∈X .
A process satisfying this condition is called reversible because, probabilistically
speaking, it “looks the same” whether time is run forward or backward.
Since
summing the equality in (15) over x yields condition (11), a reversible distribution
is also a stationary distribution.
While in general reversible Markov processes are rather special, we now in-
troduce one important case in which reversibility is ensured.
A constant jump
rate Markov process {XN
t } on the state space X N = {0, 1
N , . . . , 1} is a birth and
death process if the only positive probability transitions move one step to the right,
move one step to the left, or remain still.
This implies that there are vectors
pN, qN ∈RX N with pN
1 = qN
0 = 0 such that the transition matrix of {XN
t } takes
the form
P N
x y ≡
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
pNx
if y = x + 1
N ,
qNx
if y = x −1
N ,
1 −pNx −qNx
if y = x ,
0
otherwise.
Clearly, the process {XN
t } is irreducible if pNx
> 0 for x < 1 and qNx
> 0 for
x > 0, as we henceforth assume. For the transition matrix above, the reversibility
conditions (15) reduce to
μNx qNx = μN
x −1/N pN
x −1/N
for x ∈{ 1
N , . . . , 1}.
Applying this formula inductively, we ﬁnd that the stationary distribution of {XN
t }
satisﬁes
(16)
μNx
μN
0
=
Nx

j=1
pN
(j−1)/N
qN
j/N
for x ∈{ 1
N , . . . , 1},
with μ0 determined by the requirement that the weights in μN must sum to 1.
5.3. Stationary Distributions for Two-Strategy Games. When the pop-
ulation plays a game with just two strategies, the state space X N is a grid in the
simplex in R2. In this case it is convenient to identify state x with the weight
x ≡x1 that it places on strategy 1. Under this notational device, the state space of
the Markov process {XN
t } becomes X N = {0, 1
N , . . . , 1}, a uniformly-spaced grid in
the unit interval. We will also write F(x ) for F(x) and ρ(π,x ) for ρ(π, x) whenever
it is convenient to do so.
Because agents in our model switch strategies sequentially, transitions of the
process {XN
t } are always between adjacent states, implying that {XN
t } is a birth
and death processes. Let us now use formula (16) to compute the stationary distri-
bution of our stochastic evolutionary process, maintaining the assumption that the
process is generated by a full support revision protocol. Referring back to Section
3.1, we ﬁnd that the process {XN
t } has constant jump rates λNx = NR, and that
its upward and downward transition probabilities are given by
pNx = (1 −x ) · 1
Rρ01(F(x ),x ) and
(17)
qNx = x · 1
Rρ10(F(x ),x ).
(18)
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
123
Substituting formulas (17) and (18) into equation (16), we see that for x ∈{ 1
N , 2
N , . . . , 1},
we have
μNx
μN
0
=
Nx

j=1
pN
(j−1)/N
qN
j/N
=
Nx

j=1
(1 −j−1
N )
j
N
·
1
R ρ01(F( j−1
N ), j−1
N )
1
R ρ10(F( j
N ), j
N )
.
Simplifying this expression yields the following result.
Theorem 5.3. Suppose that a population of N agents plays the two-strategy
game F using the full support revision protocol ρ. Then the stationary distribution
for the evolutionary process {XN
t } on X N is given by
μNx
μN
0
=
Nx

j=1
(N −j + 1)
j
· ρ01(F( j−1
N ), j−1
N )
ρ10(F( j
N ), j
N )
for x ∈{ 1
N , 2
N , . . . , 1},
with μN
0 determined by the requirement that 
x ∈X N μNx = 1.
In what follows, we will use Theorem 5.3 to understand the inﬁnite-horizon
behavior of the process {XN
t }, in particular as various parameters are taken to
their limiting values.
5.4. Examples. The power of inﬁnite horizon analysis lies in its ability to
generate unique predictions of play even in games with multiple strict equilibria.
We now illustrate this idea by computing some stationary distributions for two-
strategy coordination games under the BRM and logit rules. In all cases, we ﬁnd
that these distributions place most of their weight near a single equilibrium. But
we also ﬁnd that the two rules need not select the same equilibrium.
Example 5.4. Stag Hunt. The symmetric normal form coordination game
A =

h
h
0
s

with s > h > 0 is known as Stag Hunt. By way of interpretation, we imagine that
each agent in a match must decide whether to hunt for hare or for stag. Hunting
for hare ensures a payoﬀof h regardless of the match partner’s choice. Hunting for
stag can generate a payoﬀof s > h if the opponent does the same, but results in a
zero payoﬀotherwise. Each of the two strategies has distinct merits. Coordinating
on Stag yields higher payoﬀs than coordinating on Hare. But the payoﬀto Hare is
certain, while the payoﬀto Stag depends on the choice of one’s partner.
Suppose that a population of agents is repeatedly matched to play Stag Hunt.
If we let x denote the proportion of agents playing Stag, then with our usual
abuse of notation, the payoﬀs in the resulting population game are FH(x ) = h
and FS(x ) = sx . This population game has three Nash equilibria: the two pure
equilibria, and the mixed equilibrium x ∗= h
s . We henceforth suppose that h = 2
and s = 3, so that the mixed equilibrium places mass x ∗= 2
3 on Stag.
Suppose that agents follow the best response with mutations protocol, with
mutation rate ε = .10. The resulting mean dynamic,
˙x =

ε
2 −x
if x < 2
3,
(1 −ε
2) −x
if x > 2
3,
                                                                                                                    
                                                                                                               

124
WILLIAM H. SANDHOLM
0.0
0.2
0.4
0.6
0.8
1.0
0.00
0.05
0.10
0.15
(i) best response with
mutations (ε = .10)
(ii) logit (η = .25)
Figure 2. Stationary distribution weights μx for Stag Hunt (h = 2, s = 3, N = 100).
has stable rest points at x = .05 and x = .95. The basins of attraction of these
rest points meet at the mixed equilibrium x ∗= 2
3. Note that the rest point that
approximates the all-Hare equilibrium has the larger basin of attraction.
In Figure 2(i), we present this mean dynamic underneath the stationary distri-
bution μN for N = 100, which we computed using the formula derived in Theorem
5.3. While the mean dynamic has two stable equilibria, nearly all of the mass in the
stationary distribution is concentrated at states where between 88 and 100 agents
choose Hare. Thus, while coordinating on Stag is eﬃcient, the “safe” strategy Hare
is selected by the stochastic evolutionary process.
Suppose instead that agents use the logit rule with noise level η = .25. The
mean dynamic is then the logit dynamic,
˙x =
exp(3x η−1)
exp(2η−1) + exp(3x η−1) −x ,
which has stable rest points at x = .0003 and x = .9762, and an unstable rest
point at x = .7650, so that the basin of attraction of the “almost all-Hare” rest
point x = .0003 is even larger than under BRM. Examining the resulting stationary
distribution (Figure 2(ii)), we see that virtually all of its mass is placed on states
where either 99 or 100 agents choose Hare, in rough agreement with the result for
the BRM(.10) rule.
♦
Why does most of the mass in the stationary distribution becomes concentrated
around a single equilibrium? The stochastic evolutionary process {XN
t } typically
moves in the direction indicated by the mean dynamic. If the process begins in
the basin of attraction of a rest point or other attractor of this dynamic, then the
initial period of evolution generally results in convergence to and lingering near this
locally stable set.
However, since BRM and logit choice lead to irreducible evolutionary processes,
this cannot be the end of the story.
Indeed, we know that the process {XN
t }
eventually reaches all states in X N; in fact, it visits all states in X N inﬁnitely
often.
This means that the process at some point must leave the basin of the
stable set visited ﬁrst; it then enters the basin of a new stable set, at which point
it is extremely likely to head directly the set itself. The evolution of the process
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
125
(i) best response with
mutations (ε = .10)
(ii) logit (η = .25)
Figure 3. Stationary distribution weights μx for a nonlinear Stag Hunt
(h = 2, s = 7, N = 100).
continues in this fashion, with long periods of visits to each attractor punctuated
by sudden jumps between the stable set.
Which states are visited most often over the inﬁnite horizon is determined by
the relative unlikelihoods of these rare but inevitable transitions between stable
sets. In the examples above, the transitions from the Stag rest point to the Hare
rest point and from the Hare rest point to the Stag rest point are both very unlikely
events. But for purposes of determining the stationary distribution, what matters
is that in relative terms, the former transitions are much more likely than the latter.
This enables us to conclude that over very long time spans, the evolutionary process
will spend most periods at states where most agents play Hare.
Example 5.5. A nonlinear Stag Hunt. We now consider a version of the Stag Hunt
game in which payoﬀs depend nonlinearly on the population state. With our usual
abuse of notation, we deﬁne payoﬀs in this game by FH(x ) = h and FS(x ) = sx 2,
with x representing the proportion of agents playing Stag. The population game
F has three Nash equilibria: the pure equilibria x = 0 and x = 1, and the mixed
equilibrium x ∗=

h/s. We focus on the case in which h = 2 and s = 7, so that
x ∗=

2/7 ≈.5345.
Suppose ﬁrst that a population of 100 agents play this game using the BRM(.10)
rule. In Figure 3(i) we present the resulting mean dynamic beneath a graph of the
stationary distribution μ100. The mean dynamic has rest points at x = .05, x = .95,
and x ∗≈.5345, so the the “almost all Hare” rest point again has the larger basin of
attraction. As was true in the linear Stag Hunt from Example 5.4, the stationary
distribution generated by the BRM(.10) rule in this nonlinear Stag Hunt places
nearly all of its mass on states where at least 88 agents choose Hare.
Figure 3(ii) presents the mean dynamic and the stationary distribution μ100 for
the logit rule with η = .25. The rest points of the logit(.25) dynamic are x = .0003,
x = 1, and x = .5398, so the “almost all Hare” rest point once again has the larger
basin of attraction. Nevertheless, the stationary distribution μ100 places virtually
all of its mass on the state in which all 100 agents choose Stag.
To summarize, our prediction for very long run behavior under the BRM(.10)
rule is ineﬃcient coordination on Hare, while our prediction under the logit(.25)
rule is eﬃcient coordination on Stag.
♦
                                                                                                                    
                                                                                                               

126
WILLIAM H. SANDHOLM
For the intuition behind this discrepancy in predictions, recall the discussion
from Section 5.1 about the basic distinction between the logit and BRM protocols:
under logit choice, the probability of a “mistake” depends on its payoﬀconse-
quences, while under BRM, it does not. The latter observation implies that under
BRM, the probabilities of escaping from the basins of attraction of stable sets, and
hence the identities of the states that predominate in the very long run, depend only
on the size and the shapes of the basins. In the current one-dimensional example,
these shapes are always line segments, so that only the size of the basins matters;
since the “almost all-Hare” state has the larger basin, it is selected under the BRM
rule.
On the contrary, the probability of escaping a stable equilibrium under logit
choice depends not only on the shape and size of its basin, but also on the payoﬀ
diﬀerences that must be overcome during the journey. In the nonlinear Stag Hunt
game, the basin of the “almost all-Stag” equilibrium is smaller than that of the
all-Hare equilibrium. But because the payoﬀadvantage of Stag over Hare in the
former’s basin tends to be much larger than the payoﬀadvantage of Hare over Stag
in the latter’s, it is more diﬃcult for the population to escape the all-Stag equilib-
rium than the all-Hare equilibrium; as a result, the population spends virtually all
periods coordinating on Stag over the inﬁnite horizon.
We can compare the process of escaping from the basin of a stable rest point
to an attempt to swim upstream. Under BRM, the strength of the stream’s ﬂow is
constant, so the diﬃculty of a given excursion is proportional to distance. Under
logit choice, the strength of the stream’s ﬂow is variable, so the diﬃculty of an ex-
cursion depends on how this strength varies over the distance travelled. In general,
the probability of escaping from a stable set is determined by both the distance
that must be travelled and the strength of the oncoming ﬂow.
To obtain unique predictions of inﬁnite horizon behavior, it is generally enough
either that the population size not be too small, or that the noise level in agents’
choices not be too large. But one can obtain cleaner and more general results by
studying the limiting behavior of the stationary distribution as the population size
approaches inﬁnity, the noise level approaches zero, or both.
This approach to
studying inﬁnite horizon behavior, known as stochastic stability theory.
One diﬃculty that can arise in this setting is that the prediction of inﬁnite
horizon behavior can depend on the identity or on the order in which limits are
taken. Our last example, based on Binmore and Samuelson (1997), illustrates this
point.
Example 5.6. Consider a population of agents who are matched to play the sym-
metric normal form game with strategy set S = {0, 1} and payoﬀmatrix
A =

1
2
3
1

.
The unique Nash equilibrium of the population game F(x) = Ax is the mixed
equilibrium x∗= (x∗
0, x∗
1) = ( 1
3, 2
3). To simplify notation in what follows we allow
self-matching, but the analysis is virtually identical without it.
Suppose that agents employ the following revision protocol, which combines
imitation of successful opponents and mutations:
ρε
ij(π, x) = xjπj + ε.
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
127
(i) N = 100, ε = .1
0.0
0.2
0.4
0.6
0.8
1.0
0.000
0.001
0.002
0.003
0.004
0.005
0.006
(ii) N = 10,000, ε = .1
(iii) N = 100, ε = 10−5
(iv) N = 100, ε = 10−7
Figure 4. Stationary distribution weights μN,ε
x
in an anticoordination game
under an “imitation with mutation” protocol.
The protocol ρε generates the mean dynamic
(19)
˙xi = V ε
i (x) = xi ˆFi(x) + 2ε ( 1
2 −xi),
which is the sum of the replicator dynamic and an order ε term that points toward
the center of the simplex.
When ε = 0, this dynamic is simply the replicator
dynamic: the Nash equilibrium x∗= ( 1
3, 2
3) attracts solutions from all interior
initial conditions, while pure states e0 and e1 are unstable rest points. When ε > 0,
the two boundary rest points disappear, leaving a globally stable rest point that is
near x∗, but slightly closer to the center of the simplex.
Using the formulas from Theorem 5.3, we can compute the stationary distri-
bution μN,ε of the process {XN,ε
t
} generated by F and ρε for any ﬁxed values of N
and ε. Four instances are presented in Figure 4.
Figure 4(i) presents the stationary distribution when ε = .1 and N = 100. This
distribution is drawn above the phase diagram of the mean dynamic (19), whose
global attractor is appears at ˆx ≈.6296. The stationary distribution μN,ε has its
mode at state x = .64, but is dispersed rather broadly about this state.
Figure 4(ii) presents the stationary distribution and mean dynamic when ε = .1
and N = 10,000. Increasing population size moves the mode of the distribution
occurs to state x = .6300, and, more importantly, causes the distribution to exhibit
much less dispersion around the modal state. This numerical analysis suggests that
in the large population limit, the stationary distribution μN,ε will approach a point
mass at ˆx ≈.6296, the global attractor of the relevant mean dynamic.
                                                                                                                    
                                                                                                               

128
WILLIAM H. SANDHOLM
As the noise level ε approaches zero, the rest point of the mean dynamic ap-
proaches the Nash equilibrium x ∗= 2
3. Therefore, if after taking N to inﬁnity we
take ε to zero, we obtain the double limit
(20)
lim
ε→0 lim
N→∞μN,ε = δx ∗,
where the limits refer to weak convergence of probability measures, and δx ∗denotes
the point mass at state x ∗.
The remaining pictures illustrate the eﬀects of setting very small mutation
rates. When N = 100 and ε = 10−5 (Figure 4(iii)) most of the mass in μ100,ε falls
in a bell-shaped distribution centered at state x = .68, but a mass of μ100,ε
1
= .0460
sits in isolation at the boundary state x = 1. When ε is reduced to 10−7 (Figure
4(iv)), this boundary state commands a majority of the weight in the disribution
(μ100,ε
1
= .8286).
This numerical analysis suggests that when the mutation rate approaches zero,
the stationary distribution will approach a point mass at state 1. Increasing the
population size does not alter this result, so for the small noise double limit we
obtain
(21)
lim
N→∞lim
ε→0 μN,ε = δ1,
where δ1 denotes the unit point mass at state 1.
Comparing equations (20) and (21), we conclude that the large population
double limit and the small noise double limit disagree.
♦
In the preceding example, the large population limits agree with the predictions
of the mean dynamic, while the small noise limits do not. Still, the behavior of
the latter limits is easy to explain. Starting from any interior state, and from the
boundary as well when ε > 0, the expected motion of the process {XN,ε
t
} is toward
the interior rest point of the mean dynamic V ε. But when ε is zero, the boundary
states 0 and 1 become rest points of V ε, and are absorbing states of {XN,ε
t
}; in fact,
it is easy to see that they are the only recurrent states of the zero-noise process.
Therefore, when ε = 0, {XN,ε
t
} reaches either state 0 or state 1 in ﬁnite time, and
then remains at that state forever.
If instead ε is positive, the boundary states are no longer absorbing, and they
are far from any rest point of the mean dynamic. But once the process {XN,ε
t
}
reaches such a state, it can only depart by way of a mutation. Thus, if we ﬁx
the population size N and make ε extremely small, then a journey from an interior
state to a boundary state—here a journey against the ﬂow of the mean dynamic—is
“more likely” than an escape from a boundary state by way of a single mutation.
It follows that in the small noise limit, the stationary distribution must become
concentrated on the boundary states regardless of the nature of the mean dynamic.
(In fact, it will typically become concentrated on just one of these states.)
As this discussion indicates, the prediction provided by the small noise limit
does not become a good approximation of behavior at ﬁxed values of N and ε
unless ε is so small that lone mutations are much more rare than excursions from
the interior of X N to the boundary. In Figures 4(iii) and (iv), which consider a
modest population size of N = 100, we see that a mutation rate of ε = 10−5 is
not small enough to yield agreement with the prediction of the small noise limit,
though a mutation rate of ε = 10−7 yields a closer match. With larger population
sizes, the relevant mutation rates would be even smaller.
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
129
This example suggests that in economic contexts, where the probabilities of
“mutations” may not be especially small, the large population limit is more likely
to be the relevant one in cases where the predictions of the two limits disagree.
In biological contexts, where mutation rates may indeed be quite small, the choice
between the limits seems less clear.
6. Asymptotics of the Stationary Distribution and Stochastic Stability
The examples in Section 5 show that even when the underlying game has multi-
ple strict equilibria, the stationary distribution is often concentrated in the vicinity
of just one of them if the noise level η is small or the population size N is large. In
these cases, the population state so selected provides a unique prediction of inﬁnite
horizon play.
In order to obtain clean selection results, we now allow the parameters η and
N to approach their limiting values. While each ﬁxed stationary distribution μN,η
has full support on X N, the limit of a sequence of stationary distributions may
converge to a point mass at a single state; thus, taking limits in η and N allows us to
obtain exact equilibrium selection results. Moreover while computing a particular
stationary distribution requires solving a large collection of linear equalities, the
limiting stationary distribution can often be found without explicitly computing
any of the stationary distributions along the sequence (see Section 8).
Population states that retain mass in a limiting stationary distribution are said
to be stochastically stable. There are a number of diﬀerent deﬁnitions of stochastic
stability, depending on which limits are taken—just η, just N, η followed by N, or
N followed by η—and on what should count as “retaining mass”. Taking only the
small noise limit, or taking this limit ﬁrst, emphasizes the rarity of suboptimal play
as the key force behind equilibrium selection. Taking only the large population
limit, or taking it ﬁrst, emphasizes the eﬀects of large numbers of conditionally
independent decisions in driving equilibrium selection. Since it is not always easy
to know which of these forces should be viewed as the primary one, a important
goal in stochastic stability analysis is to identify settings in which the small noise
and large population limits agree.
Analysis of stochastic stability in games have been carried out under a wide
array of assumptions about the form of the underlying game, the nature of the
revision protocol, the speciﬁcation of the evolutionary process, and the limits taken
to deﬁne stochastic stability—see Section 8 and Sandholm (2009, 2010c) for refer-
ences. In what follows, we focus on an interesting setting in which all calculations
can be carried until their very end, and in which one can obtain precise statements
about inﬁnite horizon behavior and about the agreement of the small noise and
large population limits.
7. Noisy Best Response Protocols in Two-Strategy Games
Here we consider evolution in two-strategy games under a general class of noisy
best response protocols. We introduce the notion of the cost of a suboptimal choice,
which is deﬁned as the rate of decay of the probability of making this choice as the
noise level approaches zero.
Using this notion, we derive simple formulas that
characterize the asymptotics of the stationary distribution under the various limits
in η and N, and oﬀer a necessary and suﬃcient condition for an equilibrium to
be uniquely stochastically stable under every noisy best response protocol. This
                                                                                                                    
                                                                                                               

130
WILLIAM H. SANDHOLM
section follows Sandholm (2010a), which builds on earlier work by Binmore and
Samuelson (1997), Blume (2003), and Sandholm (2007).
7.1. Noisy Best Response Protocols and their Cost Functions. We
consider evolution under noisy best response protocols.
These protocols can be
expressed as
(22)
ρη
ij(π) = ση(πj −πi),
for some function ση : R →(0, 1): when a current strategy i player receives a
revision opportunity, he switches to strategy j ̸= i with a probability that only
depends on the payoﬀadvantage of strategy j over strategy i. To justify its name,
the protocol ση should recommend optimal strategies with high probability when
the noise level is small:
lim
η→0 ση(a) =

1
if a > 0,
0
if a < 0.
To place further structure on the probabilities of suboptimal choices, we impose
restrictions on the rates at which the probabilities ση(a) of choosing a suboptimal
strategy approach zero as η approaches zero.
To do so, we deﬁne the cost of
switching to a strategy with payoﬀdisadvantage d ∈R as
(23)
κ(d) = −lim
η→0 η log ση(−d).
By unpacking this expression, we can write the probability of switching to a strategy
with payoﬀdisadvantage d when the noise level is η as
ση(−d) = exp

−η−1(κ(d) + o(1))

,
where o(1) represents a term that vanishes as η approaches 0. Thus, κ(d) is the ex-
ponential rate of decay of the choice probability ση(−d) as η−1 approaches inﬁnity.
We are now ready to deﬁne the class of protocols we will consider.
Definition. We say that the noisy best response protocol (22) is regular if
(i)
the limit in (23) exists for all d ∈R, with convergence uniform on compact
intervals;
(ii)
κ is nondecreasing;
(iii) κ(d) = 0 whenever d < 0;
(iv) κ(d) > 0 whenever d > 0.
Conditions (ii)-(iv) impose constraints on the rates of decay of switching prob-
abilities. Condition (ii) requires the rate of decay to be nondecreasing in the payoﬀ
disadvantage of the alternative strategy. Condition (iii) requires the switching prob-
ability of an agent currently playing the suboptimal strategy to have rate of decay
zero; the condition is satisﬁed when the probability is bounded away from zero,
although this is not necessary for the condition to hold. Finally, condition (iv)
requires the probability of switching from the optimal strategy to the suboptimal
one to have a positive rate of decay. These conditions are consistent with having
either κ(0) > 0 or κ(0) = 0: thus, when both strategies earn the same payoﬀ, the
probability that a revising agent opts to switch strategies can converge to zero with
a positive rate of decay, as in Example 7.1 below, or can be bounded away from
zero, as in Examples 7.2 and 7.3.
We now present the three leading examples of noisy best response protocols.
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
131
Example 7.1. Best response with mutations. The BRM protocol with noise level
η (= −(log ε)−1), introduced in Example 5.1, is deﬁned by
ση(a) =

1 −exp(−η−1)
if a > 0,
exp(−η−1)
if a ≤0.
In this speciﬁcation, an indiﬀerent agent only switches strategies in the event of a
mutation. Since for d ≥0 we have −η log ση(−d) = 1, protocol ση is regular with
cost function
κ(d) =

1
if d ≥0,
0
if d < 0.
♦
Example 7.2. Logit choice. The logit choice protocol with noise level η > 0, intro-
duced in Examples 3.4 and 5.2, is deﬁned in two-strategy games by
ση(a) =
exp(η−1a)
exp(η−1a) + 1.
For d ≥0, we have that −η log ση(−d) = d + η log(exp(−η−1d) + 1). It follows that
ση is regular with cost function
κ(d) =

d
if d > 0,
0
if d ≤0.
♦
Example 7.3. Probit choice. The logit choice protocol can be derived from a ran-
dom utility model in which the strategies’ payoﬀs are perturbed by i.i.d., double ex-
ponentially distributed random variables (see Hofbauer and Sandholm (2002)). The
probit choice protocol assumes instead that the payoﬀperturbations are i.i.d. normal
random variables with mean 0 and variance η. Thus
ση(a) = P(√η Z + a >√η Z′),
where Z and Z′ are independent and standard normal. It follows easily that
(24)
ση(a) = Φ

a
√
2η

,
where Φ is the standard normal distribution function.
A well-known approximation of Φ tells us that when z < 0,
(25)
Φ(z) = K(z) exp( −z2
2 )
for some K(z) ∈(
−1
√
2π z(1 −
1
z2 ),
−1
√
2π z). By employing this observation, one can
show that ση is regular with cost function
κ(d) =

1
4d2
if d > 0,
0
if d ≤0.
♦
7.2. The (Double) Limit Theorem. Our result on the asymptotics of the
stationary distribution requires a few additional deﬁnitions and assumptions. We
suppose that the sequence of two-strategy games {F N}∞
N=N0 converges uniformly
to a continuous-population game F, where F : [0, 1] →R2 is a continuous function.
We let
FΔ(x ) ≡F1(x ) −F0(x )
denote the payoﬀadvantage of strategy 1 at state x in the limit game.
                                                                                                                    
                                                                                                               

132
WILLIAM H. SANDHOLM
We deﬁne the relative cost function ˜κ: R →R by
˜κ(d) = lim
η→0

−η log ση(−d) + η log ση(d)

= κ(d) −κ(−d).
(26)
Our assumptions on κ imply that ˜κ is nondecreasing, sign preserving (sgn(˜κ(d)) =
sgn(d)), and odd (˜κ(d) = −˜κ(−d)).
We deﬁne the ordinal potential function I : [0, 1] →R by
(27)
I(x ) =
 x
0
˜κ(FΔ(y)) dy,
where the relative cost function ˜κ is deﬁned in equation (26). Observe that by
marginally adjusting the state x so as to increase the mass on the optimal strategy,
we increase the value of I at rate ˜κ(a), where a is the optimal strategy’s payoﬀ
advantage. Thus, the ordinal potential function combines information about payoﬀ
diﬀerences with the costs of the associated suboptimal choices.
Finally, we deﬁne ΔI : [0, 1] →(−∞, 0] by
(28)
ΔI(x ) = I(x ) −max
y∈[0,1] I(y).
Thus, ΔI is obtained from I by shifting its values uniformly, doing so in such a
way that the maximum value of ΔI is zero.
Example 7.4. If ρη represents best response with mutations (Example 7.1), then
the ordinal potential function (27) becomes the signum potential function
Isgn(x ) =
 x
0
sgn

FΔ(y)

dy.
This slope of this function at state x is 1, −1, or 0, according to whether the
optimal strategy at x is strategy 1, strategy 0, or both.
♦
Example 7.5. If ρη represents logit choice (Example 7.2), then (27) becomes the
(standard) potential function
I1(x ) =
 x
0
FΔ(y) dy,
whose slope at state x is just the payoﬀdiﬀerence at x .
♦
Example 7.6. If ρη represents probit choice (Example 7.3), then (27) becomes the
quadratic potential function
I2(x ) =
 x
0
1
4

FΔ(y)
2 dy,
where ⟨a⟩2 = sgn(a) a2 is the signed square function. The values of I2 again depend
on payoﬀdiﬀerences, but relative to the logit case, larger payoﬀdiﬀerences play
a more important role.
This contrast can be traced to the fact that at small
noise levels, the double exponential distribution has fatter tails than the normal
distribution—compare Example 7.3.
♦
Theorem 7.7 shows that whether one takes the small noise limit before the large
population limit, or the large population before the small noise limit, the rates of
decay of the stationary distribution are captured by the ordinal potential function
I. Since the double limits agree, our predictions of inﬁnite horizon behavior under
noisy best response rules do not depend on which force drives the equilibrium
selection results.
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
133
Theorem 7.7. The stationary distributions μN,η satisfy
(i)
lim
N→∞lim
η→0 max
x ∈X N
 η
N log μN,η
x
−ΔI(x )
 = 0 and
(ii)
lim
η→0 lim
N→∞max
x ∈X N
 η
N log μN,η
x
−ΔI(x )
 = 0.
Theorem 7.7 is proved by manipulating the stationary distribution formula from
Theorem 5.3 and applying the dominated convergence theorem.
7.3. Stochastic Stability:
Examples and Analysis. Theorem 7.7 de-
scribes the rate of decay of the stationary distribution weights as η approaches
0 and N approaches inﬁnity. If the main concern is with the states that are likely
to be observed with some frequency over the inﬁnite horizon, then one can focus on
states x ∈[0, 1] with ΔI(x ) = 0, since only neighborhoods of such states receive
nonnegligible mass in μN,η for large N in small η. We therefore call state x weakly
stochastically stable if it maximizes the ordinal potential I on the unit interval, and
we call state x uniquely stochastically stable if it is the unique maximizer of I on
the unit interval. We now investigate in greater detail how a game’s payoﬀfunction
and the revision protocol’s cost function interact to determine the stochastically
stable states.
Stochastic stability analysis is most interesting when it allows us to select among
multiple strict equilibria. For this reason, we focus the analysis to come on coordi-
nation games. The two-strategy population game F : [0, 1] →R2 is a coordination
game if there is a state x ∗∈(0, 1) such that
sgn(FΔ(x )) = sgn(x −x ∗) for all x ̸= x ∗.
Any ordinal potential function I for a coordination game is quasiconvex, with local
maximizers at each boundary state. Because I(0) ≡0 by deﬁnition, Theorem 7.7
implies the following result.
Corollary 7.8. Suppose that the limit game F is a coordination game. Then
state 1 is uniquely stochastically stable in both double limits if I(1) > 0, while state
0 is uniquely stochastically stable in both double limits if I(1) < 0.
The next two examples, which revisit two games introduced in the previous
chapter, show that the identity of the stochastically stable state may or may not
depend on the revision protocol the agents employ.
Example 7.9. Stag Hunt revisited. In Example 5.4, we considered stochastic evo-
lution in the Stag Hunt game
A =

h
h
0
s

,
where s > h > 0. When a continuous population of agents are matched to play
this game, their expected payoﬀs are given by FH(x ) = h and FS(x ) = sx , where
x denotes the proportion of agents playing Stag. This coordination game has two
pure Nash equilibria, as well as a mixed Nash equilibrium that puts weight x ∗= h
s
on Stag.
                                                                                                                    
                                                                                                               

134
WILLIAM H. SANDHOLM
0.0
0.2
0.4
0.6
0.8
1.0
–1.0
–0.8
–0.6
–0.4
–0.2
0.0
(i) h = 2, s = 3
0.0
0.2
0.4
0.6
0.8
1.0
–1.0
–0.8
–0.6
–0.4
–0.2
0.0
(ii) h = 2, s = 5
Figure 5. The ordinal potentials ΔIsgn (solid), ΔI1 (dashed), and ΔI2
(dotted) for Stag Hunt.
The ordinal potentials for the BRM, logit, and probit protocols in this game
are
Isgn(x ) =
x −x ∗ −x ∗,
I1(x) = s
2x 2 −hx , and
I2(x) =

−s2
12x 3 + hs
4 x 2 −h2
4 x
if x ≤x ∗,
s2
12x 3 −hs
4 x 2 + h2
4 x −h3
6s
if x > x ∗.
Figure 5 presents the normalized functions ΔIsgn, ΔI1, and ΔI2 for two speciﬁ-
cations of payoﬀs: h = 2 and s = 3 (in (i)), and h = 2 and s = 5 (in (ii)). For
any choices of s > h > 0, ΔI is symmetric about its minimizer, the mixed Nash
equilibrium x ∗=
h
s . As a result, the three protocols always agree about equi-
librium selection: the all-Hare equilibrium is uniquely stochastically stable when
x ∗> 1
2 (or, equivalently, when 2h > s), while the all-Stag equilibrium is uniquely
stochastically stable when the reverse inequality holds.
♦
Example 7.10. Nonlinear Stag Hunt revisited. In Example 5.5, we introduced the
nonlinear Stag Hunt game with payoﬀfunctions FH(x ) = h and FS(x ) = sx 2,
with x again representing the proportion of agents playing Stag. This game has
two pure Nash equilibria and a mixed equilibrium at x ∗=

h/s. The payoﬀs and
mixed equilibria for h = 2 and various choices of s are graphed in Figure 6.
The ordinal potentials for the BRM, logit, and probit models are given by
Isgn(x ) =
x −x ∗ −x ∗,
I1(x) = s
3x 3 −hx , and
I2(x) =

−s2
20x5 + hs
6 x3 −h2
4 x
if x ≤x ∗,
s2
20x5 −hs
6 x3 + h2
4 x −4h2x ∗
15
if x > x ∗.
Figure 7 presents the functions ΔIsgn, ΔI1, and ΔI2 for h = 2 and for various
choices of s.
When s is at its lowest level of 5, coordination on Stag is at its least appealing.
Since x ∗=

2/5 ≈.6325, the basin of attraction of the all-Hare equilibrium is
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
135
0
.5
1
0
2
4
6
8
Figure 6. Payoﬀs and mixed equilibria in Nonlinear Stag Hunt when h = 2
and s = 5, 5.75, 7, and 8.5.
0.0
0.2
0.4
0.6
0.8
1.0
–1.5
–1.0
–0.5
0.0
(i) h = 2, s = 5
0.0
0.2
0.4
0.6
0.8
1.0
–1.5
–1.0
–0.5
0.0
(ii) h = 2, s = 5.75
0.0
0.2
0.4
0.6
0.8
1.0
–1.5
–1.0
–0.5
0.0
(iii) h = 2, s = 7
0.0
0.2
0.4
0.6
0.8
1.0
–1.5
–1.0
–0.5
0.0
(iv) h = 2, s = 8.5
Figure 7. The ordinal potentials ΔIsgn (solid), ΔI1 (dashed), and ΔI2
(dotted) for Nonlinear Stag Hunt.
                                                                                                                    
                                                                                                               

136
WILLIAM H. SANDHOLM
considerably larger than that of the all-Stag equilibrium. Figure 7(i) illustrates
that coordination on Hare is stochastically stable under all three protocols.
If we make coordination on Stag somewhat more attractive by increasing s
to 5.75, the mixed equilibrium becomes x ∗=

2/5.75 ≈.5898.
The all-Hare
equilibrium remains stochastically stable under the BRM and logit rules, but all-
Stag becomes stochastically stable under the probit rule (Figure 7(ii)).
Increasing s further to 7 shifts the mixed equilibrium closer to the midpoint of
the unit interval (x ∗=

2/7 ≈.5345). The BRM rule continues to select all-Hare,
while the probit and logit rules both select all-Stag (Figure 7(iii)).
Finally, when s = 8.5, the all-Stag equilibrium has the larger basin of attraction
(x ∗=

2/8.5 ≈.4851). At this point, coordination on Stag becomes attractive
enough that all three protocols select the all-Stag equilibrium (Figure 7(iv)).
Why as we increase the value of s does the transition to selecting all-Stag
occur ﬁrst for the probit rule, then for the logit rule, and ﬁnally for the BRM
rule? Examining Figure 6, we see that increasing s not only shifts the mixed Nash
equilibrium to the left, but also markedly increases the payoﬀadvantage of Stag
at states where it is optimal. Since the cost function of the probit rule is the most
sensitive to payoﬀdiﬀerences, its equilibrium selection changes at the lowest level
of s. The next selection to change is that of the (moderately sensitive) logit rule,
and the last is the selection of the (insensitive) BRM rule.
♦
7.4. Risk Dominance, Stochastic Dominance, and Stochastic Stabil-
ity. Building on these examples, we now seek general conditions on payoﬀs that
ensure stochastic stability under all noisy best response protocols.
Example 7.9 showed that in the Stag Hunt game with linear payoﬀs, the noisy
best response rules we considered always selected the equilibrium with the larger
basin of attraction. The reason for this is easy to explain. Linearity of payoﬀs,
along with the fact that the relative cost function ˜κ is sign-preserving and odd (see
equation (26)), implies that the ordinal potential function I is symmetric about the
mixed equilibrium x ∗, where it attains its minimum value. If, for example, x ∗is
less than 1
2, so that pure equilibrium 1 has the larger basin of attraction, then I(1)
exceeds I(0), implying that state 1 is uniquely stochastically stable. Similarly, if
x ∗exceeds 1
2, then I(0) exceeds I(1), and state 0 is uniquely stochastically stable.
With this motivation, we call strategy i strictly risk dominant in the two-
strategy coordination game F if the set of states where it is the unique best response
is larger than the corresponding set for strategy j ̸= i.
Thus, if F has mixed
equilibrium x ∗∈(0, 1), then strategy 0 is strictly risk dominant if x ∗> 1
2, and
strategy 1 is strictly risk dominant if x ∗<
1
2.
If the relevant inequality holds
weakly in either case, we call the strategy in question weakly risk dominant.
The foregoing arguments yield the following result, in which we denote by ei
the state at which all agents play strategy i.
Corollary 7.11. Suppose that the limit game F is a coordination game with
linear payoﬀs. Then
(i)
State ei is weakly stochastically stable under every noisy best response pro-
tocol if and only if strategy i is weakly risk dominant in F.
(ii)
If strategy i is strictly risk dominant in F, then state ei is uniquely stochas-
tically stable under every noisy best response protocol.
                                                                                                                    
                                                                                                               

STOCHASTIC EVOLUTIONARY GAME DYNAMICS
137
Example 7.10 shows that once we turn to games with nonlinear payoﬀs, risk
dominance only characterizes stochastic stability under the BRM rule.
In any
coordination game with mixed equilibrium x ∗, the ordinal potential function for
the BRM rule is Isgn(x ) =
x −x ∗ −x ∗. This function is minimized at x ∗, and
increases at a unit rate as one moves away from x ∗in either direction, reﬂecting the
fact that under the BRM rule, the probability of a suboptimal choice is independent
of its payoﬀconsequences. Clearly, whether Isgn(1) is greater than Isgn(0) depends
only on whether x ∗is less than 1
2. We therefore have
Corollary 7.12. Suppose that the limit game F is a coordination game and
that ση is the BRM rule. Then
(i)
State ei is weakly stochastically stable if and only if strategy i is weakly risk
dominant in F.
(ii)
If strategy i is strictly risk dominant in F, then state ei is uniquely stochas-
tically stable.
Once one moves beyond the BRM rule and linear payoﬀs, risk dominance is no
longer a necessary or suﬃcient condition for stochastic stability. In what follows,
we introduce a natural reﬁnement of risk dominance that serves this role.
To work toward our new deﬁnition, let us ﬁrst observe that any function on the
unit interval [0, 1] can be viewed as a random variable by regarding the interval as a
sample space endowed with Lebesgue measure λ. With this interpretation in mind,
we deﬁne the advantage distribution of strategy i to be the cumulative distribution
function of the payoﬀadvantage of strategy i over the alternative strategy j ̸= i:
Gi(a) = λ({x ∈[0, 1] : Fi(x ) −Fj(x ) ≤a}).
We let ¯Gi denote the corresponding decumulative distribution function:
¯Gi(a) = λ({x ∈[0, 1] : Fi(x ) −Fj(x ) > a}) = 1 −Gi(a).
In words, ¯Gi(a) is the measure of the set of states at which the payoﬀto strategy
i exceeds the payoﬀto strategy j by more than a.
It is easy to restate the deﬁnition of risk dominance in terms of the advantage
distribution.
Observation 7.13. Let F be a coordination game. Then strategy i is weakly
risk dominant if and only if ¯Gi(0) ≥¯Gj(0), and strategy i is strictly risk dominant
if and only if ¯Gi(0) > ¯Gj(0).
To obtain our reﬁnement of risk dominance, we require not only that strategy
i be optimal at a larger set of states than strategy j, but also that strategy i have
a payoﬀadvantage of at least a at a larger set of states than strategy j for every
a ≥0. More precisely, we say that strategy i is weakly stochastically dominant in the
coordination game F if ¯Gi(a) ≥¯Gj(a) for all a ≥0. If in addition ¯Gi(0) > ¯Gj(0),
we say that strategy i is strictly stochastically dominant. The notion of stochastic
dominance for strategies proposed here is obtained by applying the usual deﬁnition
of stochastic dominance from utility theory (see Border (2001)) to the strategies’
advantage distributions.
Theorem 7.14 shows that stochastic dominance is both suﬃcient and necessary
to ensure stochastic stability under every noisy best response rule.
Theorem 7.14. Suppose that the limit game F is a coordination game. Then
                                                                                                                    
                                                                                                               

138
WILLIAM H. SANDHOLM
(i)
State ei is weakly stochastically stable under every noisy best response pro-
tocol if and only if strategy i is weakly stochastically dominant in F.
(ii)
If strategy i is strictly stochastically dominant in F, then state ei is uniquely
stochastically stable under every noisy best response protocol.
The idea behind Theorem 7.14 is simple. The deﬁnitions of I, ˜κ, κ, FΔ, and
Gi imply that
I(1) =
 1
0
˜κ(FΔ(y)) dy
(29)
=
 1
0
κ(F1(y) −F0(y)) dy −
 1
0
κ(F0(y) −F1(y)) dy
=
 ∞
−∞
κ(a) dG1(a) −
 ∞
−∞
κ(a) dG0(a).
As we have seen, whether state e1 or state e0 is stochastically stable depends on
whether I(1) is greater than or less than I(0) = 0. This in turn depends on whether
the value of the ﬁrst integral in the ﬁnal line of (29) exceeds the value of the second
integral. Once we recall that the cost function κ is monotone, Theorem 7.14 reduces
to a variation on the standard characterization of ﬁrst-order stochastic dominance:
namely, that distribution G1 stochastically dominates distribution G0 if and only
if

κ dG1 ≥

κ dG0 for every nondecreasing function κ.
8. Further Developments
The analyses in the previous sections have focused on evolution in two-strategy
games, mostly under noisy best response protocols. Two-strategy games have the
great advantage of generating birth-and-death processes. Because such processes
are reversible, their stationary distributions can be computed explicitly, greatly
simplifying the analysis. Other work in stochastic evolutionary game theory fo-
cusing on birth-and-death chain models includes Binmore and Samuelson (1997),
Maruta (2002), Blume (2003), and Sandholm (2011). The only many-strategy evo-
lutionary game environments known to generate reversible processes are potential
games (Monderer and Shapley (1996); Sandholm (2001)), with agents using either
the standard (Example 5.2) or imitative versions of the logit choice rule; see Blume
(1993, 1997) and Sandholm (2011) for analyses of these models.
Once one moves beyond reversible settings, obtaining exact formulas for the
stationary distribution is generally impossible, and one must attempt to determine
the stochastically stable states by other means. In general, the available techniques
for doing so are descendants of the analyses of sample path large deviations due
to Freidlin and Wentzell (1998), and introduced to evolutionary game theory by
Kandori et al. (1993) and Young (1993).
One portion of the literature considers small noise limits, determining which
states retain mass in the stationary distribution as the amount of noise in agents’
decisions vanishes. The advantage of this approach is that the set of population
states stays ﬁxed and ﬁnite. This makes it possible to use the ideas of Freidlin
and Wentzell (1998) with few technical complications, but also without the com-
putational advantages that a continuous state space can provide.
Many of the
analyses of small noise limits focus on the best response with mutations model
(Example 5.1); see Kandori et al. (1993), Young (1993, 1998), Kandori and Rob
                                                                                                                    
                                                                                                               

REFERENCES
139
(1995, 1998), Ellison (2000), and Beggs (2005). Analyses of other important models
include Myatt and Wallace (2003), Fudenberg and Imhof (2006, 2008), Dokumacı
and Sandholm (2011), and Staudigl (2011).
Alternatively, one can consider large population limits, examining the behavior
of the stationary distribution as the population size approaches inﬁnity. Here, as one
increases the population size, the set of population states becomes an increasingly
ﬁne grid in the simplex X. While this introduces some technical challenges, it also
allows one to use methods from optimal control theory in the analysis of sample
path large deviations. The use of large population limits in stochastic evolutionary
models was ﬁrst proposed by Binmore and Samuelson (1997) and Blume (2003) in
two-strategy settings. Analyses set in more general environments include Bena¨ım
and Weibull (2003) and Bena¨ım and Sandholm (2011), both of which build on
results in Bena¨ım (1998). The analysis of inﬁnite-horizon behavior in the large
population limit is still at an early stage of development, and so oﬀers a promising
avenue for future research.
References
Beckmann, M., McGuire, C. B., and Winsten, C. B. (1956). Studies in the Eco-
nomics of Transportation. Yale University Press, New Haven.
Beggs, A. W. (2005). On the convergence of reinforcement learning. Journal of
Economic Theory, 122:1–36.
Bena¨ım, M. (1998). Recursive algorithms, urn processes, and the chaining number
of chain recurrent sets. Ergodic Theory and Dynamical Systems, 18:53–87.
Bena¨ım, M. and Sandholm, W. H. (2011). Large deviations, reversibility, and equi-
librium selection under evolutionary game dynamics. Unpublished manuscript,
Universit´e de Neuchˆatel and University of Wisconsin.
Bena¨ım, M. and Weibull, J. W. (2003). Deterministic approximation of stochastic
evolution in games. Econometrica, 71:873–903.
Binmore, K. and Samuelson, L. (1997).
Muddling through: Noisy equilibrium
selection. Journal of Economic Theory, 74:235–265.
Bj¨ornerstedt, J. and Weibull, J. W. (1996). Nash equilibrium and evolution by
imitation. In Arrow, K. J. et al., editors, The Rational Foundations of Economic
Behavior, pages 155–181. St. Martin’s Press, New York.
Blume, L. E. (1993). The statistical mechanics of strategic interaction. Games and
Economic Behavior, 5:387–424.
Blume, L. E. (1997). Population games. In Arthur, W. B., Durlauf, S. N., and
Lane, D. A., editors, The Economy as an Evolving Complex System II, pages
425–460. Addison-Wesley, Reading, MA.
Blume, L. E. (2003). How noise matters. Games and Economic Behavior, 44:251–
271.
Border, K. C. (2001). Comparing probability distributions. Unpublished manu-
script, Caltech.
Brown, G. W. and von Neumann, J. (1950). Solutions of games by diﬀerential
equations. In Kuhn, H. W. and Tucker, A. W., editors, Contributions to the
Theory of Games I, volume 24 of Annals of Mathematics Studies, pages 73–79.
Princeton University Press, Princeton.
Dokumacı E. and Sandholm, W. H. (2011).
Large deviations and multinomial
probit choice. Journal of Economic Theory, forthcoming.
                                                                                                                    
                                                                                                               

140
REFERENCES
Ellison, G. (2000). Basins of attraction, long run equilibria, and the speed of step-
by-step evolution. Review of Economic Studies, 67:17–45.
Freidlin, M. I. and Wentzell, A. D. (1998). Random Perturbations of Dynamical
Systems. Springer, New York, second edition.
Fudenberg, D. and Imhof, L. A. (2006). Imitation processes with small mutations.
Journal of Economic Theory, 131:251–262.
Fudenberg, D. and Imhof, L. A. (2008). Monotone imitation dynamics in large
populations. Journal of Economic Theory, 140:229–245.
Fudenberg, D. and Levine, D. K. (1998). The Theory of Learning in Games. MIT
Press, Cambridge.
Gilboa, I. and Matsui, A. (1991). Social stability and equilibrium. Econometrica,
59:859–867.
Helbing, D. (1992).
A mathematical model for behavioral changes by pair in-
teractions. In Haag, G., Mueller, U., and Troitzsch, K. G., editors, Economic
Evolution and Demographic Change: Formal Models in Social Sciences, pages
330–348. Springer, Berlin.
Hofbauer, J. (1995).
Imitation dynamics for games.
Unpublished manuscript,
University of Vienna.
Hofbauer, J. and Sandholm, W. H. (2002). On the global convergence of stochastic
ﬁctitious play. Econometrica, 70:2265–2294.
Hofbauer, J. and Sigmund, K. (1988). Theory of Evolution and Dynamical Systems.
Cambridge University Press, Cambridge.
Hofbauer, J. and Sigmund, K. (1998). Evolutionary Games and Population Dy-
namics. Cambridge University Press, Cambridge.
Hofbauer, J. and Sigmund, K. (2003). Evolutionary game dynamics. Bulletin of
the American Mathematical Society (New Series), 40:479–519.
Kandori, M., Mailath, G. J., and Rob, R. (1993). Learning, mutation, and long
run equilibria in games. Econometrica, 61:29–56.
Kandori, M. and Rob, R. (1995). Evolution of equilibria in the long run: A general
theory and applications. Journal of Economic Theory, 65:383–414.
Kandori, M. and Rob, R. (1998). Bandwagon eﬀects and long run technology choice.
Games and Economic Behavior, 22:84–120.
Kurtz, T. G. (1970). Solutions of ordinary diﬀerential equations as limits of pure
jump Markov processes. Journal of Applied Probability, 7:49–58.
Maruta, T. (2002). Binary games with state dependent stochastic choice. Journal
of Economic Theory, 103:351–376.
Maynard Smith, J. (1982). Evolution and the Theory of Games. Cambridge Uni-
versity Press, Cambridge.
Monderer, D. and Shapley, L. S. (1996). Potential games. Games and Economic
Behavior, 14:124–143.
Moran, P. A. P. (1962). The Statistical Processes of Evolutionary Theory. Clarendon
Press, Oxford.
Myatt, D. P. and Wallace, C. C. (2003). A multinomial probit model of stochastic
evolution. Journal of Economic Theory, 113:286–301.
Norris, J. R. (1997). Markov Chains. Cambridge University Press, Cambridge.
Nowak, M. A. (2006). Evolutionary Dynamics: Exploring the Equations of Life.
Belknap/Harvard, Cambridge.
                                                                                                                    
                                                                                                               

REFERENCES
141
Nowak, M. A., Sasaki, A., Taylor, C., and Fudenberg, D. (2004). Emergence of
cooperation and evolutionary stability in ﬁnite populations. Nature, 428:646–
650.
Sandholm, W. H. (2001). Potential games with continuous player sets. Journal of
Economic Theory, 97:81–108.
Sandholm, W. H. (2003). Evolution and equilibrium under inexact information.
Games and Economic Behavior, 44:343–378.
Sandholm, W. H. (2007). Simple formulas for stationary distributions and stochas-
tically stable states. Games and Economic Behavior, 59:154–162.
Sandholm, W. H. (2009). Evolutionary game theory. In Meyers, R. A., editor,
Encyclopedia of Complexity and Systems Science, pages 3176–3205. Springer,
Heidelberg.
Sandholm, W. H. (2010a). Orders of limits for stationary distributions, stochastic
dominance, and stochastic stability. Theoretical Economics, 5:1–26.
Sandholm, W. H. (2010b). Pairwise comparison dynamics and evolutionary foun-
dations for Nash equilibrium. Games, 1:3–17.
Sandholm, W. H. (2010c). Population Games and Evolutionary Dynamics. MIT
Press, Cambridge.
Sandholm, W. H. (2011).
Stochastic imitative game dynamics with committed
agents. Unpublished manuscript, University of Wisconsin.
Schlag, K. H. (1998). Why imitate, and if so, how? A boundedly rational approach
to multi-armed bandits. Journal of Economic Theory, 78:130–156.
Smith, M. J. (1984). The stability of a dynamic model of traﬃc assignment—an
application of a method of Lyapunov. Transportation Science, 18:245–252.
Staudigl, M. (2011). Stochastic stability in binary choice coordination games. Un-
published manuscript, European University Institute.
Taylor, P. D. and Jonker, L. (1978).
Evolutionarily stable strategies and game
dynamics. Mathematical Biosciences, 40:145–156.
Traulsen, A. and Hauert, C. (2009). Stochastic evolutionary game dynamics. In
Schuster, H. G., editor, Reviews of Nonlinear Dynamics and Complexity, vol-
ume 2, pages 25–61. Wiley, New York.
Weibull, J. W. (1995). Evolutionary Game Theory. MIT Press, Cambridge.
Young, H. P. (1993). The evolution of conventions. Econometrica, 61:57–84.
Young, H. P. (1998). Individual Strategy and Social Structure. Princeton University
Press, Princeton.
Department of Economics, University of Wisconsin, 1180 Observatory Drive, Madi-
son, WI 53706, USA.
E-mail address: whs@ssc.wisc.edu
                                                                                                                    
                                                                                                               

This page intentionally left blank 
                                                                                                                    
                                                                                                               

Proceedings of Symposia in Applied Mathematics
Volume 69, 2011
Evolution of Cooperation in Finite Populations
Sabin Lessard
ABSTRACT. The Iterated Prisoner’s Dilemma with an additive effect on viability selec-
tion as payoff is used to study the evolution of cooperation in ﬁnite populations. A con-
dition for weak selection to favor Tit-for-Tat replacing Always-Defect when introduced as
a single mutant strategy in a well-mixed population is deduced from the sum of all future
expected changes in frequency. It is shown by resorting to coalescent theory that the con-
dition reduces to the one-third law of evolution in the realm of the Kingman coalescent in
the limit of a large population size. The condition proves to be more stringent when the
reproductive success of an individual is a random variable having a highly skewed prob-
ability distribution. An explanation of the one-third law of evolution based on the notion
of projected average excess in payoff is provided. A two-timescale argument is applied
for group-structured populations. The condition is found to be less stringent in the case of
uniform dispersal of offspring followed by interactions within groups. The condition be-
comes even less stringent if dispersal occurs after interactions so that there are differential
contributions of groups in offspring. On the other hand, the condition is strengthened by a
highly skewed probability distribution for the contribution of a group in offspring.
1. Introduction
Although cooperation is widespread in nature, its evolution is difﬁcult to explain. The
main problem is that cooperation did not always exist and before being common in a popu-
lation it must have been rare. But the advantage of cooperation when rare is not obvious. In
order to study the advantage of cooperation and understand its evolution, we will consider
a game-theoretic framework based on pairwise interactions.
In the Prisoner’s Dilemma (PD) two accomplices in committing a crime are arrested
and each one can either defect (D) by testifying against the other or cooperate with the other
(C) by remaining silent. Each of the accomplices receives a light sentence corresponding
to some reward (R) when both cooperate, compared to a heavy sentence corresponding to
a punishment (P) when both defect. When one defects and the other cooperates the defec-
tor receives a lighter sentence represented by some temptation (T), while the cooperator
receives a heavier sentence represented by the sucker’s payoff (S). Therefore, the payoffs
in the PD game satisfy the inequalities T > R > P > S. The situation is summarized in Fig.
1 with some particular values for the different payoffs.
Note that strategy D is the best reply to itself, since the payoff to D against D exceeds
the payoff to C against D. Actually the payoff to C is smaller than the payoff to D whatever
2000 Mathematics Subject Classiﬁcation. Primary 60C05; Secondary 92D15 .
Research supported in part by NSERC Grant 8833.
c
⃝2011 American Mathematical Society
143
                                           
                                                                                                                    
                                                                                                               

144
SABIN LESSARD
against
Defect
Cooperate
Cooperate
Temptation
14
3
5
1
Reward
Defect
Punishment
Sucker’s payoff
FIGURE 1. Payoffs in the PD game with some particular values.
the strategy of the opponent is. If pairwise interactions occur at random in an inﬁnite
population, then the expected payoff to C can only be smaller than the expected payoff
to D. Moreover, if the reproductive success of an individual is an increasing function of
the payoff and true breeding is assumed so that an offspring uses the same strategy as its
parent, then C is not expected to increase in frequency.
In order to ﬁnd conditions that could favor the evolution of cooperation the PD game
is extended by assuming n rounds of the game between the same players. This is known as
the Iterated Prisoner’s Dilemma (IPD). Then two sequential strategies are considered: Tit-
for-Tat, represented by A, and Always-Defect, represented by B. Always-Defect consists
obviously in defecting in every round, while Tit-for-Tat consists in cooperating in the ﬁrst
round and then using the previous move of the opponent in the next rounds. Note that
two players using Tit-for-Tat will always cooperate. Moreover, Tit-for-Tat has proved to
do better than any other sequential strategy in computer experiments. See, e.g., Axelrod
(1984), Hofbauer and Sigmund (1998, Chap. 9), McNamara et al. (2004), and references
therein for more details, variants and historical perspectives.
Let us assume that the payoffs in the different repetitions of the IPD game are additive.
Then the payoffs to A against A, A against B, B against A, and B against B, denoted by a,b,c,
and d, respectively, take the expressions given in Fig. 2. Note that these payoffs satisfy the
inequalities a > c > d > b as soon as the number of repetitions is large enough, that is,
n > T −P
R−P .
(1.1)
This condition guarantees that A is the best reply to itself, since then a > c which means
that the payoff to A against A exceeds the payoff to B against A. Similarly the inequality
d > b means that B is the best reply to itself. This is the situation, for instance, when
n = 10 with the payoffs of the PD game given in Fig. 1. The consequence of this is that
the expected payoff to A will exceed the expected payoff to B in an inﬁnite population with
random pairwise interactions if the frequency of A exceeds some threshold value between
0 and 1.
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
145
against
Always-Defect (B)
Tit-for-Tat (A)
A
c = T +P(n−1)
a = Rn
B
d = Pn
b = S+P(n−1)
41
30
50
28
FIGURE 2. Payoffs in the IPD game with particular values in the case
n = 10 with the numerical payoffs of the PD game given in Fig. 1.
As a matter of fact, if the frequencies of A and B in an inﬁnite population are x and
1−x, respectively, then the expected payoffs to A and B are
wA(x) = ax+b(1−x)
(1.2)
and
wB(x) = cx+d(1−x),
(1.3)
respectively. Therefore, wA(x) > wB(x) if and only if
x >
d −b
a−b−c+d = x∗.
(1.4)
With the expressions of the different payoffs given in Fig. 2, we ﬁnd that
x∗=
P−S
(P−S)+(R−P)

n−T−P
R−P
.
(1.5)
This threshold value for x decreases from 1 to 0 as n increases from (T −P)/(R −P) to
inﬁnity, but remains always positive. This suggests that the frequency of A in an inﬁnite
population can increase, but only if the initial frequency is high enough.
2. Dynamics in an inﬁnite population
Consider an inﬁnite haploid population undergoing discrete, non-overlapping genera-
tions and suppose random pairwise interactions among the offspring of the same genera-
tion. These interactions are assumed to have an additive effect on viability. More precisely
the probability for an individual to survive from conception to maturity, and then to con-
tribute to the next generation, is proportional to some ﬁtness given in the form
ﬁtness = 1+s×payoff.
(2.1)
Here, 1 is an arbitrary reference value and s ≥0 represents an intensity of viability selection
whose coefﬁcient is the payoff to the individual. The intensity of selection will be assumed
small throughout the paper. The case s = 0 corresponds to neutrality.
                                                                                                                    
                                                                                                               

146
SABIN LESSARD
Let x(t) be the frequency of A in generation t before selection. As a result of random
pairwise interactions, the probability for an individual of type A to survive will be 1 +
swA(x(t)) compared to 1+swB(x(t)) for an individual of type B. Then the frequency of A
in generation t after selection will be
˜x(t) = x(t)(1+swA(x(t)))
1+sw(x(t))
,
(2.2)
where
w(x(t)) = x(t)wA(x(t))+(1−x(t))wB(x(t))
(2.3)
is the mean payoff in generation t. After reproduction and in the absence of mutation,
this frequency will be also the frequency of A in the offspring of generation t + 1, that
is, x(t + 1) = ˜x(t). Therefore, the change in the frequency of A before selection from
generation t to generation t +1, represented by Δx(t) = x(t +1)−x(t), will be given by
Δx(t)˜x(t)−x(t) = sx(t)(1−x(t))(wA(x(t))−wB(x(t)))
1+sw(x(t))
,
(2.4)
where
wA(x(t))−wB(x(t)) = (a−b−c+d)(x(t)−x∗).
(2.5)
We conclude that Δx(t) = 0 if and only if x(t) = 0,1 or x∗, which are the stationary states.
Moreover, since a−b−c+d > 0 and 0 < x∗< 1, we have that Δx(t) > 0 if x(t) > x∗, while
Δx(t) < 0 if x(t) < x∗. Therefore, x(t) increases as t →∞if x(0) > x∗, while it decreases if
x(0) < x∗. Actually x(t) increases to 1 in the former case, while x(t) decreases to 0 in the
latter case, since the limit of x(t) as t →∞must be a stationary state by continuity.
Let us summarize.
Proposition 1. Consider the IPD game in Fig. 2 with a number of rounds satisfying
n > (T −P)/(R−P) so that the payoffs to A and B satisfy a > c and d > b, or more gener-
ally any two-player two-strategy matrix game with strategies A and B being the best replies
to themselves. Assume random pairwise interactions in an inﬁnite population undergoing
discrete, non-overlapping generations and viability selection of intensity s with coefﬁcient
given by the payoff. The frequency of A in generation t before selection, x(t), satisﬁes
x(t) ↑1 if x(0) > x⋆and x(t) ↓0 if x(0) < x⋆,
(2.6)
where
x∗=
d −b
a−b−c+d
(2.7)
is a stationary state in (0,1) for the deterministic dynamics.
Proposition 1 means that x∗is an unstable polymorphic equilibrium, while 0 and 1 are
monomorphic stable equilibria. Unfortunately this cannot explain the spread of A from an
initial low frequency following its introduction as a rare mutant strategy.
3. Fixation probability in a ﬁnite population
In a ﬁnite population, random drift that results from sampling effects can ultimately
bring the frequency of A to ﬁxation from any low initial frequency. In this section we
consider the probability of this event.
Each generation starts with N parents labeled from 1 to N. These produce virtually
inﬁnite numbers of offspring identical to themselves in the relative proportions π1,...,πN,
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
147
respectively. The population size N is assumed to be ﬁnite and constant. The proportions
π1,...,πN are exchangeable random variables. This means that the joint distribution is
invariant under any permutation. Furthermore, they satisfy 0 ≤πi ≤1 for i = 1,...,N and
∑N
i=1 πi = 1. In particular this implies that the expected proportion of offspring produced
by each parent is the same. It is given by
E(π1) = N−1
N
∑
i=1
E(πi) = N−1E

N
∑
i=1
πi

= N−1.
(3.1)
Moreover, it is assumed that
cN = NE(π2
1) =
N
∑
i=1
E(π2
i ) →0
(3.2)
as N →∞. This says that the probability for two offspring chosen at random without
replacement to have the same parent tends to 0 as the population size increases.
The Wright-Fisher model (Fisher 1930, Wright 1931) corresponds to the situation
where πi = N−1 for i = 1,...,N. In this case, we have cN = N−1.
A modiﬁed Wright-Fisher model with a skewed distribution of progeny size can be
obtained by allowing for πi = ψ for some i chosen at random and πj = (1−ψ)(N −1)−1
for every j ̸= i, for some ﬁxed 0 < ψ < 1. A combination of this reproduction scheme
with probability N−α and the Wright-Fisher scheme with the complementary probability
1 −N−α, and this for each generation, has been considered and applied to oysters for
instance (Eldon and Wakeley 2006). In this case, we ﬁnd that
cN = 1
N

1−1
Nα

+ 1
Nα

ψ2 + (1−ψ)2
N −1

,
(3.3)
whose leading term is ψ2N−α if 0 < α < 1, but (1+ψ)2N−1 if α = 1, and N−1 if α > 1.
The general situation of exchangeable proportions of offspring produced by the N
parents each generation corresponds to the Cannings model (Cannings 1974).
The frequency of A in the parents of generation t is represented by a random variable
z(t). This random variable can take only the values i/N for i = 0,1,...,N. The frequency
of A in the offspring of generation t is represented by x(t), which has the same expected
value as z(t). This frequency becomes ˜x(t) as deﬁned in the previous section in the adults
of generation t after viability selection as a result of random pairwise interactions among
the offspring. Then N adults are chosen at random to be the parents of the offspring of
generation t +1. The frequency of A in these parents is z(t +1). The conditional distribu-
tion of z(t + 1) given x(t) is the distribution of a binomial random variable of parameters
N and ˜x(t), divided by N. In particular, the conditional expected value of z(t +1) is ˜x(t),
which is the same as the conditional expected value of x(t +1). (See Fig. 3 for a schematic
representation of the life cycle.)
Actually z(t) for t ≥0 is a Markov chain on the ﬁnite state space {i/N : i = 0,1,...,N}
with ﬁxation states 0 and 1, all other states being transient. From any initial state z(0), the
chain will hit 0 or 1 in a ﬁnite time with probability 1 owing to the ergodic theorem.
Actually as t →∞the chain z(t) will converge in probability to a random variable z(∞)
that takes the value 1 with some probability u(s), which is the probability for the chain
to hit 1 before 0, and the value 0 with the complementary probability 1 −u(s). Here, u(s)
represents the probability of ultimate ﬁxation of A as a function of the intensity of selection.
Note that
u(s) = Es[z(∞)],
(3.4)
                                                                                                                    
                                                                                                               

148
SABIN LESSARD
N parents
offspring
adults
N parents
offspring
reproduction
interactions
sampling
reproduction
z(t)
x(t)
˜x(t)
z(t +1)
x(t +1)
FIGURE 3. Life cycle from generation t to generation t +1 and notation
for the frequency of A in the whole population at each step.
where Es denotes expectation as a function of s. Moreover u(0) = z(0), since one of the
offspring in the initial generation will be the ancestor of the whole population in the long
run, and it will be one offspring chosen at random in the initial generation by symmetry if
no selection takes place.
Being uniformly bounded by 1, the chain will also converge in mean. Therefore, we
have
Es[z(∞)]
=
lim
T→∞Es[z(T))]
(3.5)
=
lim
T→∞Es

z(0)+
T
∑
t=0
(z(t +1)−z(t))
	
=
z(0)+ lim
T→∞
T
∑
t=0
Es[z(t +1)−z(t)]
=
z(0)+
∞
∑
t=0
Es[z(t +1)−z(t)].
On the other hand, the tower property of conditional expectation and (2.4) yield
Es[z(t +1)−z(t)]
=
Es[x(t +1)−x(t)]
(3.6)
=
Es

Es[x(t +1)−x(t)|x(t)]

=
Es [˜x(t)−x(t)]
=
s(a−b−c+d)Es
x(t)(1−x(t))(x(t)−x∗)
1+sw(x(t))

=
s(a−b−c+d)E [x(t)(1−x(t))(x(t)−x∗)]+o(s),
where E denotes expectation in the absence of selection, that is, Es when s = 0, while
|o(s)|/s →0 as s →0. This leads to the approximation
u(s) = u(0)+s(a−b−c+d)
∞
∑
t=0
E [x(t)(1−x(t))(x(t)−x∗)]+o(s)
(3.7)
for the probability of ultimate ﬁxation of A under weak selection.
The above approach was suggested in Rousset (2003) and ascertained in Lessard and
Ladret (2007) under mild regularity conditions on the transition probabilities of the Markov
chain. Actually it sufﬁces that these probabilities and their derivatives are continuous func-
tions of s at s = 0, which is the case here.
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
149
The following deﬁnition was introduced in Nowak et al. (2004).
Deﬁnition 1. Selection favors A replacing B if the probability of ultimate ﬁxation of
A is larger in the presence of selection than in the absence of selection.
The inequality u(s) > u(0) for s > 0 small enough guarantees that weak selection favors A
replacing B. This will be the case if u′(0) > 0, where
u′(0) = (a−b−c+d)
∞
∑
t=0
E [x(t)(1−x(t))(x(t)−x∗)]
(3.8)
is the derivative of the ﬁxation probability with respect to the intensity of selection evalu-
ated at s = 0. The condition a−b−c+d > 0 leads to the following conclusion.
Proposition 2. Assume that the offspring of generation t are produced in inﬁnite num-
bers in exchangeable proportions by a ﬁxed ﬁnite number N of adults chosen at random in
the previous generation and that they undergo viability selection according to the game of
Proposition 1. Weak selection favors A replacing B if
x∗< ∑t≥0 E[x(t)2(1−x(t))]
∑t≥0 E[x(t)(1−x(t))] = ˆx,
(3.9)
where x(t) is the frequency of A in the offspring of generation t and E denotes expectation
under neutrality.
Note that the condition for A to be favored for replacement under weak selection is more
stringent if the upper bound ˆx deﬁned in Proposition 2, which satisﬁes 0 < ˆx < 1, is closer
to 0.
4. Generalized one-third law of evolution
In this section we calculate the upper bound ˆx in Proposition 2. This is done under the
assumption that A is initially a single mutant, that is, u(0) = z(0) = N−1. Moreover, all
calculations are made under neutrality.
First note that E[x(t)(1 −x(t))] is the probability for two offspring chosen at random
without replacement in generation t to be of types A and B in this order. This is a con-
sequence of the tower property of conditional expectation. As a matter of fact, using the
indicator random variable ξi(t) = 1 if the i-th offspring chosen at random without replace-
ment in generation t is of type A, and 0 otherwise, for i = 1,2, we have
E [x(t)(1−x(t))] = E

E

ξ1(t)(1−ξ2(t))|x(t)

= E

ξ1(t)(1−ξ2(t))

.
(4.1)
Going backwards in time from generation t to generation 0, we obtain
E

ξ1(t)(1−ξ2(t))

= p22(t +1)
N
,
(4.2)
where p22(t +1) = pt+1
22 is the probability that two offspring chosen at random without re-
placement in generation t descend from two distinct ancestral parents in generation 0, and
1/N the probability that the ancestral parent of the ﬁrst offspring is of type A. Then neces-
sarily the ancestral parent of the second offspring will be of type B, since A is represented
only once in the initial generation. Here, the quantity p22 denotes the probability for two
                                                                                                                    
                                                                                                               

150
SABIN LESSARD
offspring chosen at random without replacement in the same generation to have different
parents. Therefore,
∑
t≥0
E[x(t)(1−x(t))] =
p22
N(1−p22).
(4.3)
Similarly,
E

x(t)2(1−x(t))

= E

ξ1(t)ξ2(t)(1−ξ3(t))

= p32(t +1)
3N
,
(4.4)
where p32(t + 1) represents the probability that three offspring chosen at random without
replacement in generation t descend from two distinct ancestral parents in generation 0 and
1/3 is the conditional probability that it is then the ﬁrst two offspring that descend from
the same ancestral parent (see Fig. 4.). Here,
p32(t +1) =
t
∑
r=0
pt−r
33 p32pr
22 = p32
pt+1
33 −pt+1
22
p33 −p22
,
(4.5)
where
pij =
∑
a1 +···+aj = i
a1,...,aj ≥1
E
 j
∏
r=1
πar
r

(4.6)
represents the probability that i offspring chosen at random without replacement in the
same generation have j distinct parents. This leads to
∑
t≥0
E

x(t)2(1−x(t))

=
p32
3N(1−p22)(1−p33).
(4.7)
Finally we obtain
ˆx =
p32
3p22(1−p33)
(4.8)
for the upper bound in Proposition 2. Note that
p22 = 1−cN →1,
(4.9)
as N →∞, and
p32 ≤p32 + p31 = 1−p3,
(4.10)
which complete the proof of the following statement.
Proposition 3. In the case of a single initial A, the upper bound ˆx in the condition
given in Proposition 2 for weak selection to favor A replacing B satisﬁes
lim
N→∞ˆx = lim
N→∞
p32
3(1−p33) ≤1
3,
(4.11)
where pij is the probability that i offspring chosen at random without replacement in the
same generation have j distinct parents.
An equality on the right-hand side of the equation in Proposition 3 gives the weakest con-
dition for A to be favored for replacement under weak selection. This is known as the
one-third law of evolution (Nowak et al. 2004).
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
151
t
r
0
A
B
A
B
t
t
t
t
A
A
B
A
B
t
t
t
t
t
t
t
FIGURE 4. Lineages of two offspring of types A,B and three offspring
of types A,A,B from generation t to generation 0.
Deﬁnition 2. The one-third law of evolution states that weak selection favors a single
A replacing B in the limit of a large population size if x∗< 1/3.
According to Proposition 3, the one-third law of evolution holds if and only if at most two
lineages out of three coalesce at a time backwards in time with probability 1 in the limit
of a large population size. This is the necessary and sufﬁcient condition for the limiting
backward process of the neutral Cannings model with c−1
N generations as unit of time with
cN deﬁned in (3.2) to be the Kingman coalescent (Kingman 1982, M¨ohle 2000, M¨ohle and
Sagitov 2001).
Let us recall that the number of lineages backwards in time under the Kingman coales-
cent is a death process on the positive integers with death rate from k ≥1 to k−1 given by
λk = k(k−1)/2. This means that each pair of lineages coalesces with rate 1 independently
of each other.
The above conclusion ﬁrst drawn in Lessard and Ladret (2007) shows that the one-
third law of evolution originally deduced for the Moran model (Nowak et al. 2004) and
the Wright-Fisher model (Lessard 2005, Imhof and Nowak 2006) holds for a wide class of
models. Moreover it shows how the one-third law extends beyond this class. Note that the
Moran model (Moran 1958) assumes overlapping generations with one individual replaced
at a time, but such models lead to the same conclusion (Lessard and Ladret 2007, Lessard
2007a).
In the case of the Eldon-Wakeley model with probability N−α for a random parent to
produce a fraction ψ of all offspring (Eldon and Wakeley 2006), the probability p21 that
two offspring have the same parent, which is the same as cN, is given by (3.3), while the
probability for three offspring to have the same parent is
p31 = 1
N2

1−1
Nα

+ 1
Nα

ψ3 + (1−ψ)3
(N −1)2

(4.12)
and the probability for them to have two distinct parents
p32 = 3
N

1−1
N

1−1
Nα

+ 3(1−ψ)
Nα

ψ2 + 1−ψ
N −1 −(1−ψ)2
(N −1)2

.
(4.13)
                                                                                                                    
                                                                                                               

152
SABIN LESSARD
In this case,
lim
N→∞
p32
3(1−p33) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
1
3
if α > 1,
1−ψ
3−2ψ
if α < 1,
1+ψ2(1−ψ)
3+ψ3(3−2ψ)
if α = 1.
(4.14)
The limit is strictly less than 1/3 if and only if α ≤1. This means a more stringent
condition for A to be favored for replacement under weak selection when the distribution
of progeny size is highly skewed.
Note that α ≤1 is the condition for the limit backward process of the neutral Eldon-
Wakeley model with c−1
N
generations as unit of time to be a Λ-coalescent allowing for
multiple mergers involving more than two lineages (Pitman 1999, Sagitov 1999). In the
case α < 1, the rate of an m-merger among k lineages is given by
λk,m =

k
m

ψm−2(1−ψ)k−m,
(4.15)
for m = 2,...,k.
5. Explanation for the one-third law of evolution: projected average excess
The following explanation for the one-third law of evolution in the limit of a large
Moran or Wright-Fisher population has been proposed (Ohtsuki et al. 2007): in an inva-
sion attempt by a single mutant of type A up to extinction or ﬁxation in the absence of
selection, A-players effectively interact on average with B-players twice as often as with
A-players. The argument is based on the mean effective sojourn times in the different pop-
ulation states. These can be obtained exactly for the Moran model and approximated for
the Wright-Fisher model (Fisher 1930, p. 90).
In this section, we propose another explanation based on the notion of projected av-
erage excess (Lessard and Lahaie 2009). This is an extension of the classical notion of
average excess in ﬁtness for a gene substitution (Fisher 1930). Here, we consider the ex-
cess in payoff for a mutant strategy not only in the current generation but also in all future
generations.
First observe that
∑
t≥0
E[x(t)(1−x(t))] = p22E(S2)
N
(5.1)
and
∑
t≥0
E

x(t)2(1−x(t))

= E(S2)−E(S3)
2N
,
(5.2)
where S2 and S3 represent the numbers of generations spent backwards in time with two
and three lineages, respectively, before the ﬁrst coalescence event occurs. As a matter of
fact, we have
E(S2) =
1
1−p22
(5.3)
and
E(S3) =
1
1−p33
,
(5.4)
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
153

PPPPPP
0
t < S3
AF
b
BI
BT
d

PPPP
P
PPPPPP
0
S3 ≤t < S2
AF
a
AI
BT
c
1/2



PPPPPP
0
S3 ≤t < S2
AF
b
BI
BT
d
1/2
FIGURE 5. Average excess in payoff for A in generation t. The indices
F, T and I are used for focal, typical and interacting offspring, respec-
tively. Only typical offspring of type B have to be considered. The coa-
lescence time S2 is for F and T, while S3 is for F, T and I.
so that
E(S2)−E(S3) =
p22 −p33
(1−p22)(1−p33).
(5.5)
Moreover, we have
p22 −p33 = 2p32
3
,
(5.6)
which are two different expressions for the probability that exactly two given offspring
out of three chosen at random without replacement have different parents. Therefore, the
above equalities agree with the corresponding expressions given in the previous section.
On the other hand, the ﬁrst derivative of the probability of ultimate ﬁxation of A with
respect to the intensity of selection evaluated at s = 0 can be written as
u′(0) = (a−c)
∞
∑
t=0
E

x(t)2(1−x(t))

+(b−d)
∞
∑
t=0
E

x(t)(1−x(t))2
,
(5.7)
where
E

x(t)(1−x(t))2
= E [x(t)(1−x(t))]−E

x(t)2(1−x(t))

.
(5.8)
Then, the above equalities and the assumption that cN = 1−p22 →0 as N →∞lead to the
approximation
u′(0) ≈(a−c)
E(S2)−E(S3)
2N

+(b−d)
E(S2)+E(S3)
2N

.
(5.9)
This can be written in the form
u′(0) ≈1
N
a−c+b−d
2

E(S2)−E(S3)

+(b−d)E(S3)

.
(5.10)
The fraction N−1 is the frequency of A in the initial generation, while the expression in
curly brackets represents its projected average excess in payoff. This is the sum of the
differences between the marginal payoff to A and the mean payoff to a competitor in the
same generation over all generations t ≥0 as long as ﬁxation is not reached.
                                                                                                                    
                                                                                                               

154
SABIN LESSARD
The concept of projected average excess in payoff for A can be better understood with
the help of Fig. 5. Consider a focal offspring (F) of type A in generation t ≥0. We want to
compare its marginal payoff to the mean payoff in the same generation. This mean will be
the expected payoff to a typical offspring (T) chosen at random in the same generation. If
this offspring has the same ancestor in generation 0 as the focal offspring, then its marginal
payoff will also be the same. Therefore, it sufﬁces to consider the case of distinct ancestors
for F and T in generation 0. Then a third offspring (I) is chosen at random in the same
generation and it may interact with either F or T.
Let S3 be the number of generations backwards in time for the ﬁrst coalescence event
in the genealogies of the three offspring F, T, and I, and S2 be the corresponding number
for F and T only. If t < S3, the three ancestors in generation 0 are all distinct and therefore
T and I are both of type B. Then the payoff to F would be b compared to d for T. On the
other hand, if S3 ≤t < S2 with F and I having a common ancestor in generation 0, whose
conditional probability is 1/2, then F and I are of type A, while T is of type B. This gives a
payoff a to F compared to c to T. Finally, if S3 ≤t < S2 but with F and I having a common
ancestor in generation 0, whose conditional probability is 1/2, then T and I are of type B,
while F is of type A. In this case, the payoff to F is b compared to d for T. In all other
cases, F and T would be of the same type A, and then they would have the same payoff.
The ﬁnal argument for the interpretation follows from the facts that
∞
∑
t=0
P(S3 > t) = E(S3)
(5.11)
and
∞
∑
t=0
P(S3 ≤t < S2) =
∞
∑
t=0

P(S2 > t)−P(S3 > t)

= E(S2)−E(S3).
(5.12)
Scaled expected times in the limit of a large population size are obtained by multiplying S2
and S3 by cN and by letting N tend to inﬁnity, that is,
μi = lim
N→∞E(cNSi),
(5.13)
for i = 2,3. Then the sign of the ﬁrst derivative of the probability of ultimate ﬁxation of A,
and therefore whether or not weak selection favors A for replacement, is given by the sign
of a scaled projected average excess in ﬁtness.
Let us summarize.
Proposition 4. In the case of a single initial A and in the limit of a large population
size, the condition given in Propositions 2 and 3 for weak selection to favor A replacing B
is equivalent to
aA =
a−c+b−d
2

μ2 −μ3

+(b−d)μ3 > 0,
(5.14)
where μ2 and μ3 designate expected times, in number of c−1
N generations in the limit of a
large population size, with two and three lineages, respectively, and aA represents a scaled
projected average excess in payoff of A.
Note that
μ2 ≥3μ3,
(5.15)
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
155
and the one-third law of evolution
x∗=
d −b
a−b−c+d < 1
3
(5.16)
is obtained when μ2 = 3μ3, which occurs with μ2 = 1 in the case of the Kingman coales-
cent.
6. Island model with dispersal preceding selection
In this section we examine the effect of a group structure on the condition for a single
A to be favored for replacing B. Actually we consider the Wright (1931) island model for a
population subdivided into a ﬁnite number of groups of the same size, assuming a Wright-
Fisher reproduction scheme within groups and partial uniform dispersal of offspring before
selection.
We have D groups of N parents producing virtually inﬁnite numbers of offspring in
equal relative proportions, that is, (ND)−1 for each parent. We suppose that a ﬁxed pro-
portion m of offspring disperse uniformly among all groups, while the complementary
proportion 1 −m stay in their native group. This is followed by random pairwise inter-
actions within groups affecting viability as previously. Finally N parents are sampled at
random in each group to start the next generation.
Under the assumption of a Wright-Fisher reproduction scheme, the frequency of A
in the offspring in group k in generation t before dispersal, for k = 1,...,D and t ≥0, is
the same as the frequency of A in the parents of group k at the beginning of generation t,
denoted by zk(t). Then this frequency becomes
xk(t) = (1−m)zk(t)+mz(t)
(6.1)
in the offspring after dispersal, and
˜xk(t) = xk(t)(1+swA(xk(t)))
1+sw(xk(t))
(6.2)
in the offspring after selection. Here,
z(t) = D−1
D
∑
k=1
zk(t) = D−1
D
∑
k=1
xk(t) = x(t)
(6.3)
is the frequency of A in all parents of generation t, which is the same as the frequency of
A in all their offspring before dispersal as well as after dispersal, but before selection (see
Fig. 6).
Proceeding as previously, we ﬁnd that the probability of ultimate ﬁxation of A is
u(s)
=
Es

z(∞)
	
(6.4)
=
z(0)+
∞
∑
t=0
Es

z(t +1)−z(t)
	
=
u(0)+D−1
∞
∑
t=0
D
∑
k=1
Es

˜xk(t)−xk(t)

,
where
Es

˜xk(t)−xk(t)

= s(a−b−c+d)E

xk(t)(1−xk(t))(xk(t)−x∗)

+o(s).
(6.5)
                                                                                                                    
                                                                                                               

156
SABIN LESSARD
N parents
offspring
offspring
adults
N parents
reproduction
dispersal
interactions
sampling
zk(t)
zk(t)
xk(t)
˜xk(t)
zk(t +1)
FIGURE 6. Life cycle from generation t to generation t +1 and notation
for the frequency of A in group k at each step in the island model with
dispersal before selection.
Actually the derivative evaluated at s = 0 is given by
u′(0)
=
(a−b−c+d)D−1
∞
∑
t=0
D
∑
k=1
E

xk(t)(1−xk(t))(xk(t)−x∗)

.
(6.6)
We conclude that u′(0) > 0 if and only if x∗< ˆx, where
ˆx = ∑t≥0 E[x(t)2(1−x(t))]
∑t≥0 E[x(t)(1−x(t))]
.
(6.7)
Here, we have
x(t)2(1−x(t)) = D−1
D
∑
k=1
xk(t)2(1−xk(t))
(6.8)
and
x(t)(1−x(t)) = D−1
D
∑
k=1
xk(t)(1−xk(t)).
(6.9)
Then the tower property of conditional expectation ascertains the following statement.
Proposition 5. Consider the Wright island model for a ﬁnite number of groups of size
N and assume a Wright-Fisher reproduction scheme followed by uniform dispersal of a
proportion m of offspring and viability selection within groups according to the game of
Proposition 1. Weak selection favors A replacing B if
x∗< ∑t≥0 E[ξ1(t)ξ2(t)(1−ξ3(t))]
∑t≥0 E[ξ1(t)(1−ξ2(t))]
= ˆx,
(6.10)
where ξ1(t),ξ2(t),ξ3(t) are indicator random variables for type A in offspring chosen at
random without replacement in the same group chosen at random in generation t after
dispersal.
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
157
1
2
3
4
5
6




r




r




r




r




r




r




r
r




r
r




r




r
r
r
FIGURE 7. States for the ancestors of three offspring in the island model.
7. Calculation for the island model with dispersal preceding selection
We want to calculate ˆx in Proposition 5 for the island model with dispersal preceding
selection in the limit of a large number of groups and in the case where A is initially a single
mutant. Without loss of generality, suppose z1(0) = N−1 and zk(0) = 0 for k = 2,...,D.
See Ladret and Lessard (2007) for the analysis in the case of a ﬁxed number of groups.
We will have to trace backwards in time the ancestors of two or three offspring after
dispersal. Actually we will just need to know the number of groups d containing at least
one ancestor and the number of groups ni containing i ancestors for i = 1,...,d with 1 ≤
∑d
i=1 ni ≤3. There are six possible states in the form n = (n1,...,nd): (1), (2,0), (3,0,0),
(0,1), (1,1,0), (0,0,1), and they are labeled from 1 to 6 (see Fig. 7).
The state space S is partitioned into two subsets, S1 = {1,2,3} with all ancestors in
different groups and S2 = {4,5,6} with at least two ancestors in the same group. State 1
is absorbing while all other states are transient. As D increases, transitions from the other
states occur according to two different timescales with expected sojourn times in state 4, 5
or 6 becoming negligible compared to expected sojourn times in state 2 or 3.
As shown in Appendix A1, in the limit D →∞with ND generations as unit of time, lin-
eages within the same group either coalesce or migrate instantaneously to different groups,
while each pair of lineages in different groups coalesces at rate f22, which is the probability
for two offspring chosen at random without replacement in the same group after dispersal
to have ultimately two ancestors in different groups in the case of an inﬁnite number of
groups. In other words, after an initial scattering phase during which instantaneous transi-
tions from states in S2 to states in S1 take place, there is a collecting phase during which
transitions within S1 occur according to the Kingman (1982) coalescent but with rate f22
instead of 1.
Let pij(t) be the probability for the chain to be in state j and vij(t) the probability for
the chain to visit state j for the ﬁrst time in the t-th generation backwards in time, given
that the chain is in state i in the current generation. Note that
vij = ∑
t≥1
vij(t)
(7.1)
is the probability for the chain to reach state j from state i for j ̸= i. Moreover,
E(Ti) = (ND)−1 ∑
t≥0
pii(t)
(7.2)
is the expected value of the time Ti spent in state i starting from state i before absorption
into state 1 with ND generations as unit of time. In particular we have (see Appendix A1)
lim
D→∞E(T2) = f −1
22 and lim
D→∞E(T4) = 0,
(7.3)
                                                                                                                    
                                                                                                               

158
SABIN LESSARD
state 2 in generation 0
state 6 in generation t




r




r




r
r
r




r




r
A
A
B
A
B
A
B




r




r




r




r
r
r




r




r




r
A




r
B
A
A
B
A
B
A
B
FIGURE 8. Lineages of three offspring of types A,A,B in the same group
in the island model from generation t to generation 0.
so that only the time spent in state 2 has to be taken into account in the expected time with
two lineages in the limit of a large population size. Moreover,
lim
D→∞v42 = f22 = 1−f21 and lim
D→∞v62 = f32 + f33 = 1−f31,
(7.4)
where fnk represents the probability for n offspring chosen at random without replacement
in the same group after dispersal to have ultimately k ancestors in different groups in the
case of an inﬁnite number of groups.
Considering all possible transitions from state 4 for two offspring chosen at random
without replacement in generation t ≥0 after dispersal to states in generation 0 so that the
two offspring are of types A and B in this order, we obtain
∑
t≥0
E[ξ1(t)(1−ξ2(t))] = (ND)−1 ∑
t≥1
p42(t)+(ND)−1 ∑
t≥1
p44(t),
(7.5)
where
∑
t≥1
p42(t) = ∑
t≥1
t
∑
r=1
v42(r)p22(t −r) = ∑
r≥1∑
t≥0
v42(r)p22(t).
(7.6)
Owing to (7.1), (7.2), (7.3), (7.4), we conclude that
∑
t≥0
E[ξ1(t)(1−ξ2(t))] = v42E(T2)+E(T4)−(ND)−1 →1,
as D →∞.
For three offspring chosen at random without replacement in state 6 in generation t ≥0
after dispersal and of types A, A and B in this order, we obtain in a similar way
∑
t≥0
E[ξ1(t)ξ2(t)(1−ξ3(t))] = (3ND)−1 ∑
t≥1
p62(t)+(3ND)−1 ∑
t≥1
p64(t),
(7.7)
from which
∑
t≥0
E[ξ1(t)ξ2(t)(1−ξ3(t))] = v62
3 E(T2)+ v64
3 E(T4) →
1−f31
3(1−f21),
(7.8)
as D →∞. Here, 1/3 is the probability that two lineages in particular coalesce given that
two lineages out of three coalesce (see Fig. 8).
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
159
Exact expressions of f21 and f31 in terms of m and N are given in Appendix A1. Note
that the inequality f31 < f21 always holds.
It remains to plug the above calculations in the upper bound given in Proposition 5.
The following conclusion ensues.
Proposition 6. In the case of a single initial A, the upper bound ˆx in the condition
given in Proposition 5 for weak selection to favor A replacing B in the island model with
dispersal preceding selection satisﬁes
lim
D→∞ˆx =
1−f31
3(1−f21) > 1
3,
(7.9)
where f21 and f31 are the probabilities that two and three offspring, respectively, chosen at
random without replacement in the same group after dispersal have ultimately a common
ancestor in the case of an inﬁnite number of groups.
Proposition 6 means a less stringent condition for a single A to be favored for replacing B
when the population is subdivided into a large number of small groups.
8. Island model with dispersal following selection
In this section we consider a variant of the previous island model by assuming that
uniform dispersal occurs after selection. The main effect of this assumption is to introduce
differential contributions of groups according to their composition from one generation to
the next.
Here, the frequency of A in the offspring in group k in generation t goes from xk(t) =
zk(t) before selection to
˜xk(t) = xk(t)(1+swA(xk(t)))
1+sw(xk(t))
(8.1)
after selection, and ﬁnally to
˜˜xk(t) = (1−m)xk(t)(1+swA(xk(t))+mD−1∑D
l=1 xl(t)(1+swA(xl(t))
(1−m)(1+sw(xk(t))+mD−1∑D
l=1(1+sw(xl(t))
(8.2)
after selection and dispersal, since the relative size of group k after selection is 1+sw(xk(t)).
(See Fig. 9.)
After some algebraic manipulations, the frequency of A in generation t in the whole
population after selection and dispersal is found to be
D−1
D
∑
k=1
˜˜xk(t)
=
x(t)+s(b−d)x(t)(1−x(t))
(8.3)
+
s(a−b−c+d)x(t)2(1−x(t))
+
sm(2−m)(b+c−2d)(x(t)2 −x(t)
2)
+
sm(2−m)(a−b−c+d)(x(t)3 −x(t) x(t)2)+o(s).
                                                                                                                    
                                                                                                               

160
SABIN LESSARD
N parents
offspring
adults
adults
N parents
reproduction
interactions
dispersal
sampling
zk(t)
xk(t)
˜xk(t)
˜˜xk(t)
zk(t +1)
FIGURE 9. Life cycle from generation t to generation t +1 and notation
for the frequency of A in group k at each step in the island model with
dispersal after selection.
Here, x(t), x(t)(1−x(t)) and x(t)2(1−x(t)) are deﬁned as in Section 6, while
x(t)2 −x(t)
2
=
D−1
D
∑
k=1
xk(t)2 −

D−1
D
∑
k=1
xk(t)
2
=
D−2
D
∑
k=1
D
∑
l=1,l̸=k
xk(t)(1−xl(t))−(1−D−1)x(t)(1−x(t))
(8.4)
and
x(t)3 −x(t) x(t)2
=
D−1
D
∑
k=1
xk(t)3 −

D−1
D
∑
k=1
xk(t)

D−1
D
∑
l=1
xl(t)2

=
D−2
D
∑
k=1
D
∑
l=1,l̸=k
xk(t)2(1−xl(t))−(1−D−1)x(t)2(1−x(t)).
(8.5)
The tower property of conditional expectation yields
E[x(t)(1−x(t))] = E[ζ1(t)(1−ζ2(t))]
(8.6)
and
E[x(t)2(1−x(t))] = E[ζ1(t)ζ2(t)(1−ζ3(t))]
(8.7)
as before, but with ζ1(t),ζ2(t),ζ3(t) being indicator random variables for A in offspring
chosen at random without replacement in generation t in the same group before dispersal.
Proceeding as in the previous section, we ﬁnd that
lim
D→∞∑
t≥0
E[ζ1(t)(1−ζ2(t))] = 1
(8.8)
and
lim
D→∞∑
t≥0
E[ζ1(t)ζ2(t)(1−ζ3(t))] =
1−˜f31
3(1−˜f21),
(8.9)
where
˜fn1 = fn1(1−m)−n
(8.10)
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
161
represents the probability that n offspring chosen at random without replacement in the
same group before dispersal have ultimately a common ancestor in the case of an inﬁnite
number of groups.
On the other hand, we have
E

(D2 −D)−1
D
∑
k=1
D
∑
l=1,l̸=k
xk(t)(1−xl(t))

= E[ζ1(t)(1−η2(t))]
(8.11)
and
E

(D2 −D)−1
D
∑
k=1
D
∑
l=1,l̸=k
xk(t)2(1−xl(t))

= E[ζ1(t)ζ2(t)(1−η3(t))],
(8.12)
where η2(t) and η3(t) are indicator random variables for A in offspring chosen at random
without replacement in generation t before dispersal, but in a different group than the one
for the indicator random variables ζ1(t),ζ2(t),ζ3(t). In this case, we ﬁnd that
lim
D→∞∑
t≥0
E[ζ1(t)(1−η2(t))] =
1
1−˜f21
(8.13)
and
lim
D→∞∑
t≥0
E[ζ1(t)ζ2(t)(1−η3(t))] = 1
3 +
˜f21
1−˜f21
.
(8.14)
These results are obtained by considering all transitions from states 2 and 5, respectively,
for offspring sampled at random without replacement in generation t ≥0 before dispersal
to states in generation 0 that are compatible with the sample conﬁguration.
The probability of ultimate ﬁxation of A as a function of the intensity of selection is
given by
u(s) = u(0)+D−1
∞
∑
t=0
D
∑
k=1
Es

˜˜xk(t)−xk(t)

.
(8.15)
Its derivative evaluated at s = 0 is given by
u′(0)
=
(b−d)∑
t≥0
E[x(t)(1−x(t))]
(8.16)
+
(a−b−c+d)∑
t≥0
E[x(t)2(1−x(t))]
+
m(2−m)(b+c−2d)∑
t≥0
E[x(t)2 −x(t)
2]
+
m(2−m)(a−b−c+d)∑
t≥0
E[x(t)3 −x(t) x(t)2].
In the limit of a large number of groups and after some algebraic manipulations, we ﬁnd
that
lim
D→∞u′(0)
=
(b−d)+(a−b−c+d)1−˜f21 +(1−m)2( ˜f21 −˜f31)
3(1−˜f21)
(8.17)
+
(a−d)m(2−m)
˜f21
1−˜f21
.
                                                                                                                    
                                                                                                               

162
SABIN LESSARD
Using the exact expressions of f21 = (1−m)2 ˜f21 and f31 == (1−m)3 ˜f31 given in Appen-
dix A1, it can be checked that
m(2−m)
˜f21
1−˜f21
=
1
N −1
(8.18)
and
1−˜f21 +(1−m)2( ˜f21 −˜f31)
3(1−˜f21)
>
1−f31
3(1−f21),
(8.19)
as soon as N > 1. Then the condition limD→∞u′(0) > 0 yields the following result.
Proposition 7. In the case of dispersal following selection in the Wright island model
of Proposition 5 in the limit of a large number of groups of ﬁxed size N > 1, weak selection
favors a single A replacing B if
x∗< 1−˜f21 +(1−m)2( ˜f21 −˜f31)
3(1−˜f21)
+
a−d
(N −1)(a−b−c+d),
(8.20)
where ˜f21 and ˜f31 are the probabilities that two and three offspring, respectively, chosen at
random without replacement in the same group before dispersal have ultimately a common
ancestor in the case of an inﬁnite number of groups.
Note that the upper bound for x∗given in Proposition 7 is always larger than the upper
bound given in Proposition 6. This means an even less stringent condition for A to be
favored for replacing B in the Wright island model when dispersal follows selection instead
of preceding it.
9. Modiﬁed island model with skewed contributions of groups preceding selection
In this section we consider the effect of a skewed distribution for the contribution
of a group in offspring in a subdivided population. We assume D groups of size N with
dispersal of offspring preceding selection in each generation as in the island model of
Section 6. However, with a small probability D−β for β < 1, one group chosen at random
provides a proportion χ of all offspring, produced equally by all members of the group,
compared to (1−χ)(D−1)−1 for every other group. With the complementary probability,
the proportion is uniformly the same. In all cases, a proportion m of offspring in each group
disperse and are replaced by as many migrants chosen at random among all migrants.
This is followed by selection and random sampling within each group to start the next
generation. This corresponds to the Eldon-Wakeley model applied to groups instead of
parents.
The conclusion of Proposition 5 still holds. Moreover, a two-timescale argument can
be applied in the limit of a large number of groups as in Section 7, but with NDβ genera-
tions as unit of time (see Appendix A2).
In number of NDβ generations, the expected time spent in state i before absorption
into state 1 is written in the form
E(Ti) = (NDβ)−1 ∑
t≥0
pii(t).
(9.1)
It can be shown that
lim
D→∞E(T2) = λ −1
21 and lim
D→∞E(T4) = 0,
(9.2)
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
163
where λ21 represents the rate of coalescence of two lineages in different groups backwards
in time in the limit of a large number of groups. Moreover, the limiting probabilities of
reaching state 2 from states 4 and 6, respectively, are given by
lim
D→∞v42 = f22 and lim
D→∞v62 = f32 +
f33λ32
λ32 +λ31
,
(9.3)
where λ3i represents the rate of transition from 3 to i lineages, for i = 1,2, in different
groups backwards in time in the limit of a large number of groups.
Assuming a single initial A and using (9.1), (9.2), (9.3), we ﬁnd that
D1−β ∑
t≥0
E[ξ1(t)(1−ξ2(t))]
=
v42E(T2)+E(T4)−(NDβ)−1
(9.4)
→
f22λ −1
21 ,
and
D1−β ∑
t≥0
E[ξ1(t)ξ2(t)(1−ξ3(t))]
=
v62
3 E(T2)+ v64
3 E(T4)
(9.5)
→
f32
3λ21
+
f33λ32
3λ21(λ32 +λ31),
as D →∞. Here, ξ1(t),ξ2(t),ξ3(t) are indicator random variables for type A in offspring
chosen at random without replacement in the same group chosen at random in generation
t after dispersal like in Proposition 5.
This leads to the following result.
Proposition 8. In the case of the Wright island model of Proposition 5 for D groups
with a proportion m of migrant offspring each generation in each group before selection
but a probability Dβ for β < 1 that they come in proportion χ from a same group chosen
at random, weak selection favors a single A replacing B in the limit of a large number of
groups if
x∗<
1−f31 −f33

λ31
λ32+λ31

3(1−f21)
<
1−f31
3(1−f21),
(9.6)
where f21 and f31 are deﬁned as in Proposition 6, while λ31 and λ32 are the rates of tran-
sition from 3 to 1 and from 3 to 2, respectively, for the number lineages in different groups
backwards in time with NDβ generations as unit of time in the limit of a large number of
groups.
Proposition 8 means a more stringent condition for a single A to be favored for replacing B
in an island model with a highly skewed distribution for the contribution of a group in the
limit of a large number of groups.
10. Summary and comments
In conclusion we have shown in this paper that:
• Viability selection determined by the Iterated Prisoner’s Dilemma (IPD) in an
inﬁnite population predicts the increase in frequency of Tit-for-Tat (A) against
Always-Defect (B), and therefore can explain the spread of cooperation, but only
from a frequency x > x⋆, where x⋆is the frequency of A at an unstable polymor-
phic equilibrium.
                                                                                                                    
                                                                                                               

164
SABIN LESSARD
• Weak viability selection determined by the IPD game in a ﬁnite population fa-
vors a single mutant A replacing B, and therefore can explain the advantage for
cooperation to go to ﬁxation from a low frequency, but only under the condition
x⋆< ˆx for some threshold frequency ˆx.
• In the limit of a large population size, we have ˆx ≤1/3. Actually ˆx = 1/3, which
is known as the one-third law of evolution, in a Wright-Fisher model, and more
generally in the domain of application of the Kingman coalescent. On the other
hand, ˆx < 1/3, which leads to a more stringent condition for the evolution of
cooperation, if the contribution of a parent in offspring has a skewed enough
distribution.
• In a group-structured population with uniform dispersal of offspring and weak
viability selection within groups determined by the IPD game in the limit of
a large number of groups of ﬁnite size, we have ˆx > 1/3. This means a less
stringent condition for cooperation to evolve. Moreover, the condition is weaker
if dispersal occurs after selection rather than before selection so that there are
differential contributions of groups according to their composition. On the other
hand, the condition is still weaker but to a lesser extent if the contribution of a
group in offspring has a highly skewed distribution.
• The ﬁrst-order effect of selection on the probability of ﬁxation of a single mutant
strategy is proportional to a projected average excess in payoff. This is the excess
in payoff to the mutant strategy compared to the mean payoff in the population
not only in the current generation but in all future generations as long as ﬁxation
is not reached.
Our results are based on approximations for the probability of ultimate ﬁxation of a
single mutant that are ascertained under the assumption of very weak selection. Actually,
the intensity of selection is assumed to be small compared to the intensity of the other
evolutionary forces. These are random drift, whose intensity is measured by the inverse of
the population size, and dispersal in the case of a group-structured population, whose rate is
supposed to be constant as the population size increases. On the other hand, the approach is
not limited by restrictive assumptions on the production of offspring by parents or groups.
An alternative approach under the assumption that the intensity of selection is of the
same order of magnitude as the other evolutionary forces is a diffusion approximation
(see, e.g., Kimura 1984, Nagylaki 1980, 1997, Lessard 2005, 2007b, 2009). In this case,
however, the contributions of parents and groups in offspring cannot be too highly skewed
in distribution to avoid jump processes.
Our motivation in this paper was the evolution of cooperation and this is the reason
for considering the Prisoner’s Dilemma and its iterated version with Tit-for-Tat (A) and
Always-Defect (B) as strategies. Of course, the approach used to deduce the ﬁrst-order
effect of selection on the probability of ﬁxation of a single mutant is not limited to this
particular game. Indeed, it does not depend on special relationships between the payoffs
a, b, c and d. It holds for any two-player two-strategy matrix game with strategies A and B
being the best replies to themselves or not.
Actually the approach is not limited to a matrix game, or linear expected payoffs wA(x)
and wB(x) to A and B, respectively, with respect to the frequency of A represented by x. It
can be extended to more general cases of frequency dependence with wA(x)−wB(x) being
a polynomial of any degree n with respect to x. Then expected backward times with up to
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
165
n+2 lineages have to be computed to approximate the ﬁxation probability. Moreover, this
can be used to get approximations in the case where the difference wA(x) −wB(x) is any
continuous function of x. (See Lessard and Ladret 2007.)
An approximation for the ﬁxation probability can be obtained also in the case of a
matrix game with any number of strategies. Then the approximation depends on the initial
state of the population. Moreover, it can be expressed in terms of projected average excess
in payoff given any initial frequencies (Lessard and Lahaie 2009).
We have considered pairwise interactions between offspring in inﬁnite numbers. The
case of pairwise interactions between adults in ﬁnite numbers is also of interest and it can
be dealt with in a similar manner (see, e.g., Lessard 2005, Hilbe 2011). The analysis of the
more general case of a multi-player game like the Public goods game is more recondite but
not out of reach (Kurokawa and Ihara 2009, Gokhale and Traulsen 2010, Lessard 2011a).
Finally it can be shown that a matrix game in a ﬁnite group-structured population with
uniform dispersal of offspring, or local extinction and recolonization, and payoff matrix
A within groups is formally equivalent in the limit of a large population size to a matrix
game in a well-mixed population with some effective game matrix A◦(Lessard 2011b).
The entries of this matrix are linear combinations of interaction or competition effects
weighted by coefﬁcients of identity-by-descent in an inﬁnite population in the absence of
selection. Then what is known about matrix games (see, e.g., Lessard 1990, Hofbauer and
Sigmund 1998) can be applied mutatis mutandis.
Appendix A1. Two timescales for the Wright island model
Consider the neutral Wright island model for D groups of size N. In each generation,
inﬁnite numbers of offspring are produced in equal proportions and a fraction m of these
disperse uniformly among all groups. This is followed by random sampling of N offspring
in each group to start the next generation.
The six possible states for the ancestors of three offspring chosen after dispersal are
given in Fig. 7. The transition matrix from one generation to the previous one, whose
entries are represented by pij(1) for i, j in S = {1,...,6}, takes the form
P = R+(ND)−1M(D),
(10.1)
where R is the transition matrix in the case of an inﬁnite number of groups. See Lessard
and Wakeley (2004) for exact expressions of R and M(D).
Since all states in S1 = {1,2,3} are absorbing and all states in S2 = {4,5,6} transient
in the case D = ∞, the ergodic theorem guarantees that
lim
t→∞Rt = H =

I
0
F
0

,
(10.2)
where I designates the 3×3 identity matrix and 0 the 3×3 zero matrix. Moreover,
F =
⎛
⎝
f21
f22
0
0
f21
f22
f31
f32
f33
⎞
⎠,
(10.3)
with fnk denoting the probability for n offspring chosen at random without replacement in
the same group after dispersal to have ultimately k ancestors in different groups in the case
of an inﬁnite number of groups. On the other hand, it can be checked that
lim
D→∞M(D) = M =

M11
M12
M21
M22

,
(10.4)
                                                                                                                    
                                                                                                               

166
SABIN LESSARD
where
M11 =
⎛
⎝
0
0
0
m(2−m)
−Nm(2−m)
0
0
3m(2−m)
−3Nm(2−m)
⎞
⎠
(10.5)
and
M12 =
⎛
⎝
0
0
0
(N −1)m(2−m)
0
0
0
3(N −1)m(2−m)
0
⎞
⎠.
(10.6)
Applying a lemma due to M¨ohle (1998) to the transition matrix from time 0 to time τ in
the past with ND generations as unit of time, we obtain
lim
D→∞P⌊NDτ⌋= HeτHMH =

eτG
0
FeτG
0

= Q(τ),
(10.7)
where ⌊⌋denotes the integer value and
G = M11 +M12F = f22
⎛
⎝
0
0
0
1
−1
0
0
3
−3
⎞
⎠.
(10.8)
This uses the equality
f22 = Nm(2−m)
 1
N +

1−1
N

f21

,
(10.9)
which can deduced from the exact expressions of f21 and f22 = 1−f21 (see below).
The matrix G is the generator of the death process of the Kingman (1982) coalescent
with rate f22 instead of 1. The matrix Q(τ), whose entries are denoted by qij(τ) for i, j in S,
is a transition matrix from time 0 to time τ for a continuous-time Markov chain with initial
instantaneous transitions from states in S2 to states in S1 and generator G for transitions
within S1.
The expected time in state 2 in number of ND generations is
E(T2) = (ND)−1
∞
∑
t=0
p22(t) =
 ∞
0 p22(⌊NDτ⌋)dτ,
(10.10)
from which
lim
D→∞E(T2) =
 ∞
0 q22(τ)dτ = f −1
22 .
(10.11)
This is the case because two lineages coalesce at the rate f22 in the limit of a large number
of groups. Moreover,
p22(⌊NDτ⌋) ≤

1−m(2−m)
ND
⌊NDτ⌋
≤(1−N−1)−1e−m(2−m)τ.
(10.12)
Therefore, the dominated convergence theorem can be applied. Similarly the expected time
in state 4 in number of ND generations is
E(T4) = (ND)−1
∞
∑
t=0
p44(t)
(10.13)
and
lim
D→∞E(T4) =
 ∞
0 q44(τ)dτ = 0,
(10.14)
since q44(τ) = 0 for all τ > 0.
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
167
On the other hand, the vector vT
•2 = (0,1,v32,v42,v52,v62), where vi2 is the probability
of reaching state 2 from state i for i = 3,...,6, satisﬁes the linear system of equations
v•2 = ˜PNDv•2,
(10.15)
where ˜P is the transition matrix on S with state 2 assumed to be absorbing. In this case,
M¨ohle’s (1998) lemma yields
lim
D→∞
˜PND = ˜Q =

e ˜G
0
Fe ˜G
0

,
(10.16)
where
˜G = f22
⎛
⎝
0
0
0
0
0
0
0
3
−3
⎞
⎠.
(10.17)
Therefore,
lim
D→∞v•2 = ˜Q lim
D→∞v•2.
(10.18)
It can be checked directly that the unique solution is
lim
D→∞vT
•2 = (0,1,1, f22,1, f32 + f33).
(10.19)
Finally, f22 = 1−f21 and f32 + f33 = 1−f31, where
f21
=
(1−m)2
 1
N +

1−1
N

f21

,
(10.20)
f31
=
(1−m)3
 1
N2 + 3
N

1−1
N

f21 +

1−1
N

1−2
N

f31

.
(10.21)
This system of linear equations is obtained from a ﬁrst-step analysis. Its solution is given
by
f21
=
(1−m)2
Nm(2−m)+(1−m)2 ,
(10.22)
f31
=
f21

N(1−m)+2(N −1)(1−m)3
N2m(3−3m+m2)+(3N −2)(1−m)3

.
(10.23)
Note that
f32 = 3( f21 −f31).
(10.24)
This is the case because there are 3 possibilities for two offspring out of three to have a
common ancestor.
Similarly the vector v•3 = (0,0,1,v43,v53,v63), where vi3 is the probability of reaching
state 3 from state i for i = 3,...,6, must satisfy
lim
D→∞v•3 = ˜˜Q lim
D→∞v•3,
(10.25)
where
˜˜Q =

I
0
F
0

.
(10.26)
The unique solution is
lim
D→∞vT
•3 = (0,0,1,0, f22, f33).
(10.27)
                                                                                                                    
                                                                                                               

168
SABIN LESSARD
Appendix A2. Two timescales for the modiﬁed Wright island model
Consider the neutral Wright island model for D groups of size N but suppose that, in
each generation and with probability D−β for β < 1, the proportion of offspring produced
equally by all members of a group chosen at random is χ compared to (1 −χ)(D −1)−1
in every other group. With the complementary probability, the proportion is uniformly the
same. In all cases, a proportion m of offspring in each group disperse and they are replaced
by as many migrants chosen at random among all migrants before random sampling of N
offspring to start the next generation.
The transition matrix on the state space for the ancestors of three offspring chosen
after dispersal takes the form
P = R+(NDβ)−1M(D),
(10.28)
where R is the same as in Appendix A1. The entries of the matrix M(D) can be found
explicitly (Lasalle Ialongo 2008). The important point is that
lim
D→∞M(D) = M =

M11
M12
M21
M22

,
(10.29)
where
M11 =
⎛
⎝
0
0
0
(χm)2
−N(χm)2
0
N−1(χm)3
3(χm)2(1−χm)
−3N(χm)2 +2N(χm)3
⎞
⎠
and
M12 =
⎛
⎝
0
0
0
(N −1)(χm)2
0
0
3(1−N−1)(χm)3
3(N −1)(χm)2(1−χm)
(1−N−1)(N −2)(χm)3
⎞
⎠.
In this case, M¨ohle’s (1998) lemma guarantees that
limD →∞P⌊NDβ τ⌋=

eτG
0
FeτG
0

= Q(τ),
(10.30)
where
G = M11 +M12F =
⎛
⎝
0
0
0
λ21
−λ21
0
λ31
λ32
−λ31 −λ32
⎞
⎠.
(10.31)
The parameters λlk for l > k ≥1 are the rates of transition from l to k lineages in different
groups backwards in time with NDβ generations as unit of time as D →∞. We ﬁnd that
λ21
=
N(χm)2
 1
N +

1−1
N

f21

,
(10.32)
λ31
=
N(χm)3
 1
N2 + 3
N

1−1
N

f21 +

1−1
N

1−2
N

f31

,
(10.33)
λ32
=
N(χm)3
 3
N

1−1
N

f22 +

1−1
N

1−2
N

f32

,
(10.34)
+
3N(χm)2(1−χm)
 1
N +

1−1
N

f21

.
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
169
Note that
λlk = N
∑
l≥j≥n≥k−l+ j≥1

l
j

(χm) j(1−χm)l−jpjn fn,k−l+ j,
(10.35)
where pjn is the probability that j offspring chosen at random without replacement in the
same group before dispersal have n parents, and fnk is the probability that n parents chosen
at random without replacement in the same group have ultimately k ancestors in different
groups in the case D = ∞. The relationships between the parameters fnk for 3 ≥n ≥k ≥1
exhibited in Appendix A1 lead to the expressions
λ21
=
N
 χm
1−m
2
f21,
(10.36)
λ31
=
N
 χm
1−m
3
f31,
(10.37)
λ32
=
3N
 χm
1−m
2
f21 −3N
 χm
1−m
3
f31.
(10.38)
Note that
λlk = N
∑
l≥j≥k−l+ j≥1

l
j

(χm) j(1−χm)l−j ˜fn,k−l+ j,
(10.39)
where ˜fnk is the probability that n offspring chosen at random without replacement in the
same group before dispersal have ultimately k ancestors in different groups in the case
D = ∞.
Proceeding as previously, the expected time with two lineages in different groups in
number of NDβ generations before coalescence satisﬁes
E(T2) = (NDβ)−1
∞
∑
t=0
p22(t) →λ −1
21
(10.40)
as D →∞, while the corresponding expected time with two lineages in the same group,
E(T4), tends to 0.
Finally the vector vT
•2 = (0,1,v32,v42,v52,v62), where vi2 is the probability of reaching
state 2 from state i for i = 3,...,6, satisﬁes
lim
D→∞v•2 =

e ˜G
0
Fe ˜G
0

lim
D→∞v•2,
(10.41)
where
˜G =
⎛
⎝
0
0
0
0
0
0
λ31
λ32
−λ31 −λ32
⎞
⎠.
(10.42)
The solution is found to be
lim
D→∞vT
•2 =

0,1,
λ32
λ32 +λ31
, f22, f21 +
f22λ32
λ32 +λ31
, f32 +
f33λ32
λ32 +λ31

.
(10.43)
                                                                                                                    
                                                                                                               

170
SABIN LESSARD
References
[1] AXELROD, R. (1984) The Evolution of Cooperation. New York: Basic Books.
[2] CANNINGS, C. (1974) The latent roots of certain Markov chains arising in genetics: a new approach. I.
Haploid models. Adv. Appl. Prob. 6, 260–290.
[3] ELDON, B. AND WAKELEY, J. (2006) Coalescent processes when the distribution of offspring number
among individuals is highly skewed. Genetics 172, 2621–2633.
[4] FISHER, R. A. (1930) The Genetical Theory of Natural Selection. Oxford: Clarendon.
[5] GOKHALE, C. S. AND TRAULSEN, A. (2010) Evolutionary games in the multiverse. Proc. Natl. Acad. Sci.
USA 107, 5500–5504.
[6] HILBE, C. (2011) Local replicator dynamics: A simple link between deterministic and stochastic models of
evolutionary game theory. Bull. Math. Biol. DOI 10.1007/s11538-010-9608-2.
[7] HOFBAUER, J. AND SIGMUND, K. (1998) Evolutionary Games and Population Dynamics. Cambridge:
Cambridge University Press.
[8] IMHOF, L. A. AND NOWAK, M. A. (2006) Evolutionary game dynamics in a Wright-Fisher process. J.
Math. Biol. 52, 667–681.
[9] KIMURA, M. (1984) Evolution of an altruistic trait through group selection as studied by the diffusion
equation method. IMA J. Math. Appl. Med. Biol. 1, 1–15.
[10] KINGMAN, J. F. C. (1982) The coalescent. Stoch. Proc. Appl. 13, 235–248.
[11] KUROKAWA, S. AND IHARA, Y. (2009) Emergence of cooperation in public goods games. Proc. Roy. Soc.
B 276, 1379–1384.
[12] LADRET, V. AND LESSARD, S. (2007) Fixation probability for a beneﬁcial allele and a mutant strategy in
a linear game under weak selection in a ﬁnite island model. Theor. Pop. Biol. 72, 409–425.
[13] LASALLE IALONGO, D. (2008) Processus de coalescence dans une population subdivis´ee avec possibilit´e
de coalescences multiples. M.Sc. Thesis, Universit´e de Montr´eal.
[14] LESSARD, S. (1990) Evolutionary stability: One concept, several meanings. Theor. Pop. Biol. 37, 159–170.
[15] LESSARD, S. (2005) Long-term stability from ﬁxation probabilities in ﬁnite populations: New perspectives
for ESS theory. Theor. Pop. Biol. 68, 19–27.
[16] LESSARD, S. (2007a) Cooperation is less likely to evolve in a ﬁnite population with a highly skewed
distribution of family size. Proc. Roy. Soc. B 274, 1861–1865.
[17] LESSARD, S. (2007b) An exact sampling formula for the Wright-Fisher model and a conjecture about the
ﬁnite-island model. Genetics 177, 1249-1254.
[18] LESSARD, S. (2009) Diffusion approximations for one-locus multi-allele kin selection, mutation and ran-
dom drift in group-structured populations: a unifying approach to selection models in population genetics.
J. Math. Biol. 59, 659–696.
[19] LESSARD, S. (2011a) On the robustness of the extension of the one-third law of evolution to the multi-player
game. Dyn Games Appl. DOI 10.1007/s13235-011-0010-y.
[20] LESSARD, S. (2011b) Effective game matrix and inclusive payoff in group-structured populations. Dyn
Games Appl. to appear.
[21] LESSARD, S. AND LADRET, V. (2007) The probability of ﬁxation of a single mutant in an exchangeable
selection model. J. Math. Biol. 54, 721–744.
[22] LESSARD, S. AND LAHAIE, P. (2009) Fixation probability with multiple alleles and projected average
allelic effect on selection. Theor. Pop. Biol. 75, 266–277.
[23] LESSARD, S. AND WAKELEY, J. (2004) The two-locus ancestral graph in a subdivided population: conver-
gence as the number of demes grows in the island model. J. Math. Biol. 48, 275–292.
[24] MCNAMARA, J. M., BARTA, Z., HOUSTON, A. I. (2004) Variation in behaviour promotes cooperation in
the Prisoner’s Dilemma game. Nature 428, 747–748.
[25] M ¨OHLE, M. (1998) A convergence theorem for Markov chains arising in population genetics and the coa-
lescent with selﬁng. Adv. Appl. Prob. 30, 493–512.
[26] M ¨OHLE, M. (2000) Total variation distances and rates of convergence for ancestral coalescent processes in
exchangeable population models. Adv. Appl. Prob. 32, 983–993.
[27] M ¨OHLE, M. AND SAGITOV, S. (2001) A classiﬁcation of coalescent processes for haploid exchangable
population models. Ann. Appl. Probab. 29, 1547–1562.
[28] MORAN, P. A. P. (1958) Random processes in genetics. Proc. Camb. Phil. Soc. 54, 60–71.
[29] NAGYLAKI, T. (1980) The strong-migration limit in geographically structured populations. J. Math. Biol.
9, 101–114.
[30] NAGYLAKI, T. (1997) The diffusion model for migration and selection in a plant population. J. Math. Biol.
35, 409–431.
                                                                                                                    
                                                                                                               

EVOLUTION OF COOPERATION IN FINITE POPULATIONS
171
[31] NOWAK, M. A., SASAKI, A., TAYLOR, C. AND FUDENBERG, D. (2004) Emergence of cooperation and
evolutionary stability in ﬁnite populations. Nature 428, 646–650.
[32] OHTSUKI, H., BORDALO, P. AND NOWAK, M. A. 2007 The one-third law of evolutionary dynamics. J.
Theor. Biol. 249, 289–295.
[33] PITMAN, J. (1999). Coalescents with multiple collisions. Annals of Probability 27, 1870–1902.
[34] SAGITOV, S. (1999). The general coalescent with asynchronous mergers of ancestral lines. Journal of Ap-
plied Probability 36, 1116–1125.
[35] ROUSSET, F. (2003) A minimal derivation of convergence stability measures. J. Theor. Biol. 221, 665–668.
[36] WRIGHT, S. (1931) Evolution in Mendelian populations. Genetics 16, 97–159.
D´EPARTEMENT DE MATH´EMATIQUES ET DE STATISTIQUE, UNIVERSIT´E DE MONTR´EAL, C.P. 6128
SUCCURSALE CENTRE-VILLE, MONTR´EAL, QU´EBEC H3C 3J7 CANADA
E-mail address: lessards@dms.umontreal.ca
                                                                                                                    
                                                                                                               

This page intentionally left blank 
                                                                                                                    
                                                                                                               

Index
Λ-coalescent, 152
ω-limit set, 103
adjustment dynamics, 75
advantage distribution, 137
Always-Defect, 144
asymmetric game, 40, 54
asymptotic pseudo-trajectories, 104
asymptotically stable, 16, 104
attracting, 16, 103
attractor, 103
attractor free, 103
average payoﬀ, 117
backward induction, 30
best reply, 3
best reply dynamics, 19, 83
best response dynamic, 63, 117
best response protocol, 120
best response with mutations, 120, 124,
125, 131, 132, 138
bimatrix games, 41, 71
bimatrix replicator dynamics, 41
birth and death process, 122, 138
birth-and-death chain, 138
bistability, 12
BNN dynamic, 117
BRM, 120, 123–125, 131, 134
Brown–von Neumann–Nash dynamics, 66
Buyer-Seller Game, 42
Cannings model, 147
canonical equation of adaptive dynamics,
48, 52, 55
centipede games, 32
Chain Store Game, 30
congestion games, 113
consistency, 93
constant of motion, 14
continuous ﬁctitious play, 83
continuously stable strategy (CSS), 49
convergence stable, 48, 52, 55
cooperation, 143
coordination game, 6, 133, 136, 137
correlated equilibrium distributions, 95
correlated FP, 82
cost, 130
cost function, 131
decision map, 101
detailed balance conditions, 122
deterministic approximation, 118
diﬀerential contributions of groups, 159
diﬀerential inclusion, 19, 83, 103
diﬀusion approximation, 164
direct ESS, 37
direct protocols, 116
discrete deterministic approximation, 86
discrete ﬁctitious play, 82
dispersal following selection, 159
dispersal preceding selection, 155
dominated strategies, 72
Dove, 8
duality gap, 84
eﬀective game matrix, 165
Eldon-Wakeley model, 151
Entry Deterrence Game, 30
equilibrium selection, 129
evolutionarily stable strategy, 17, 61
excess payoﬀ, 117
extensive form games, 28
external regret, 93
external consistency, 94
ﬁctitious play, 81
ﬁrst-order eﬀect of selection, 164
ﬁxation probability, 146
folk theorem, 16
Folk Theorem of Evolutionary Game
Theory, 47
forward precompact, 103
frequency dependence, 164
full support revision protocol, 119, 122
Games with Continuous Strategy Spaces,
47
generalized RSP game, 38
173
                                                                                                                    
                                                                                                               

174
INDEX
Habitat Selection Game, 47
Hannan’s set, 94
Hawk, 8
heteroclinic set, 14
imitation, 126
imitation dynamics, 18
imitative protocols, 115
improvement principle, 91
independent FP, 81
information set, 34
intensity of selection, 145
internal regret, 94
internal consistency, 94
internally chain transitive sets, 105
invariant, 103
island model, 155
Iterated Prisoner’s Dilemma, 144
Kingman coalescent, 151
large population double limit, 128
large population limit, 128, 129, 132, 139
law of large numbers, 118
logit, 123–125, 134
logit choice, 132
logit choice protocol, 120, 131
logit choice rule, 116, 138
logit dynamic, 65, 117
logit rule, 124
Lotka-Volterra equations, 10
Lyapounov stable, 103
Lyapunov function, 14, 104
matrix game, 164
maximin pair, 5
maximin strategy, 5
Maynard Smith replicator dynamic, 117
mean dynamic, 112, 116–118, 124, 127
mean ﬁeld, 117
minimax theorem, 2
mixed strategy, 3
multi-dimensional strategy spaces, 52
multiplicative weight algorithm, 98
mutation rate, 120, 128
mutations, 126
Myopic Adjustment Dynamics, 91
Nash equilibrium, 3, 61, 113
Nash map, 66
natural selection protocol, 115
NE, 30
neighborhood invader strategy (NIS), 53,
57
neighborhood strict NE, 49
neighborhood superiority, 49, 56
noise level, 116
noisy best response protocols, 129
nonconvergence, 77
nonlinear Stag Hunt, 125, 134, 135
normal form game, 113
odd or even, 2
one-third law of evolution, 151
ordinal potential function, 132, 133, 137
ordinal potentials, 134, 135
pairwise comparison dynamics, 68
pairwise interactions, 145
pairwise proportional imitation, 115
Pareto-optimal, 6
partnership games, 17, 75
payoﬀ, 2
payoﬀmatrices, 2
payoﬀmonotonic, 18
payoﬀprojection dynamics, 69
payoﬀs, 144
perfect information game, 28
perturbed solutions, 105
pervasive, 31
population game, 112, 116
population size, 114, 116
population state, 112
potential function, 132
potential games, 75, 88, 138
Prisoner’s Dilemma, 143
Prisoner’s Dilemma game, 6
probit, 134
probit choice, 132
probit choice protocol, 131
projected average excess, 152
quadratic potential function, 132
quotient rule, 9
reduced-strategy normal form, 37
relative cost function, 132
replicator dynamic, 63, 70, 97, 117
replicator equation, 9
rest point, 103
reversible, 121
reversible distribution, 122
revision protocol, 68, 112, 114, 116
risk dominance, 136
risk-dominant, 6
rock–scissors–paper, 2, 62
sample path large deviations, 138, 139
saturated rest point, 15
selection favors A replacing B, 149
Shapley triangle, 19, 65
Shapley’s example, 92
signum potential function, 132
simplex, 3
simultaneity game, 34
sink, 103
skewed contributions of groups, 162
small noise double limit, 128
                                                                                                                    
                                                                                                               

INDEX
175
small noise limit, 128, 129, 132, 138
smooth ﬁctitious play, 95
stable, 16
stable coexistence, 13
stable games, 71
Stag Hunt, 123, 133, 134
stationary distribution, 112, 120–125, 127,
129, 132
stationary distribution weights, 125
stochastic dominance, 136, 137
stochastic stability, 126, 129, 133, 136
stochastically stable, 112, 129, 134, 138
strategies, 2
strategy, 112
strict Nash equilibrium, 4
strictly dominated, 10
strictly risk dominant, 136
strictly stochastically dominant, 137, 138
strongly forward invariant, 103
strongly stable, 17
subgame perfect, 30
supermodular games, 73
support, 3
symmetric games, 7
symmetric Nash equilibrium, 7
symmetric strict Nash equilibrium, 7
symmetrized game, 20
time averages, 10
Tit For Tat, 13, 144
two diﬀerent timescales, 157
two-species ESS, 43
two-strategy games, 122, 129
two-timescale argument, 162
unilateral process, 93, 97
uniquely stochastically stable, 133, 136, 138
updating rule, 101
weakly risk dominant, 136
weakly stochastically dominant, 137, 138
weakly stochastically stable, 133, 136, 138
Wright manifold, 22, 39
Wright-Fisher model, 147
zero-sum game, 4, 72
                                                                                                                    
                                                                                                               

                                                                                                                    
                                                                                                               

