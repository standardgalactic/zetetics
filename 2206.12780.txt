A Pair Measurement Surface Code on Pentagons
Craig Gidney
Google Quantum AI, Santa Barbara, California 93117, USA
June 28, 2022
In this paper, I present a way to compile the surface code into two-body parity mea-
surements ("pair measurements"), where the pair measurements run along the edges of
a Cairo pentagonal tiling. The resulting circuit improves on prior work by Chao et al.
by using fewer pair measurements per four-body stabilizer measurement (5 instead of
6) and fewer time steps per round of stabilizer measurement (6 instead of 10). Using
Monte Carlo sampling, I show that these improvements increase the threshold of the
surface code when compiling into pair measurements from ≈0.2% to ≈0.4%, and
also that they improve the teraquop footprint at a 0.1% physical gate error rate from
≈6000 qubits to ≈3000 qubits. However, I also show that Chao et al’s construction
will have a smaller teraquop footprint for physical gate error rates below ≈0.03% (due
to bidirectional hook errors in my construction). I also compare to the planar honey-
comb code, showing that although this work does noticeably reduce the gap between
the surface code and the honeycomb code (when compiling into pair measurements),
the honeycomb code is still more eﬃcient (threshold ≈0.8%, teraquop footprint at
0.1% of ≈1000).
The source code that was written, the exact noisy circuits that were sampled, and the statistics
that were collected as part of this paper are available at doi.org/10.5281/zenodo.6626417 [10].
1
Introduction
Last year, Hastings and Haah introduced a class of quantum error correcting codes now called
"ﬂoquet codes" [14]. Floquet codes are interesting for a lot of reasons, but from a practical per-
spective they’re interesting because they’re extremely eﬃcient when implemented using two-body
parity measurements (hereafter "pair measurements"). For example, the very ﬁrst ﬂoquet code was
the honeycomb code. When using pair measurements, the honeycomb code has a threshold around
1% and a teraquop footprint of around 1000 qubits at a 0.1% physical gate error rate [2, 12, 16].
(The teraquop footprint is the number of physical qubits required to create a logical qubit reliable
enough to survive one trillion operations.)
From the perspective of hardware designed around pair measurements [16], a notable problem
comes up when attempting to compare ﬂoquet codes to previous work: most previous work wasn’t
done in terms of pair measurements. It’s far more common to see circuits compiled into unitary
interactions, like CNOTs or CZs. Hardware designed around pair measurements may not even be
able to execute those circuits. So, ideally, we’d prefer to compare to previous work that also used
pair measurements.
An example of previous work using pair measurements is Chao et al. [2]. They compiled the
surface code into pair measurements. The surface code is well known as one of the most promising
quantum error correcting codes for building a scalable fault tolerant quantum computer, because
of its high threshold and planar connectivity requirements [8]. This suggests that compiling the
surface code into pair measurements should set a solid baseline that other codes can compare
against. However, ignoring the problems with comparing across gate sets for the moment, Chao et
al. found that the surface code performed much worse when using pair measurements. Their pair
Craig Gidney: craig.gidney@gmail.com
1
arXiv:2206.12780v1  [quant-ph]  26 Jun 2022

Figure 1: Diﬀerent ZX graphs implementing a four-body parity measurement. (a) The ZX graph for the well
known decomposition into four CNOT operations. (b) The ZX graph produced by replacing each CNOT with a
lattice surgery CNOT [15]. (c) The smallest possible ZX graph implementing a four-body parity measurement.
(d) The construction found by Chao et al. using brute force search [2]. (e) The construction presented by this
paper. The graphs are all equal because, when spider fusion is repeatedly applied, they reduce to (c). White
circles are X type nodes. Black circles are Z type nodes. Assuming time moves from left to right, leaf nodes
correspond to single qubit initializations and demolition measurements while vertical edges correspond to CNOT
operations (if the linked nodes have opposite type) or pair measurements (if the linked nodes have the same
type).
measurement surface code’s threshold was roughly 0.2% (as opposed to roughly 0.8% when using
unitary interactions). For comparison, the honeycomb code has a threshold around 1% when using
pair measurements [12, 16].
This paper is my attempt to improve on Chao et al’s result. There are three reasons I thought
it would be possible to do better. First, Chao et al’s work conceptualizes the problem as a quantum
circuit problem. In my experience, the ZX calculus [3] is a better tool for reasoning about and
optimizing measurement based constructions [6]. I suspected that if I modelled the problem as
a ZX graph, I could ﬁnd a better solution. Second, Chao et al. used a union ﬁnd decoder with
unweighted edges [7]. By contrast, estimates of the cost of the honeycomb code have been based
on correlated minimum weight perfect matching [12, 13], which is slower but more accurate. Using
the same decoder for the surface code would make the comparison fairer. Third, Chao et al. only
estimated the threshold. I wanted to check if the loss in threshold was actually matched by an
increase in the teraquop footprint.
The paper is divided as follows. In Section 2, I present the more compact construction that
I found by using the ZX calculus. In Section 3, I present the simulation data obtained using a
correlated minimum weight perfect matching decoder. In Section 4, I discuss conclusions, including
the ironic fact that although my construction has a better threshold, its teraquop footprint even-
tually becomes larger than Chao et al.’s as physical error rate decreases. The paper also includes
Appendix A, which speciﬁes how I computed line ﬁts, Appendix B, which speciﬁes the noise model
I used, Appendix C, which includes an example circuit showing my construction, and Appendix D,
which includes additional data plots.
2
Construction
This paper is not intended to be an introduction to the ZX calculus. I’ll cover barely enough detail
to convey that an eﬃcient graph that implements a four-body X basis parity measurement is going
to look like a Z type core surrounded by X type limbs leading to the data qubits. I recommend
any of [1, 4, 5] as starting points for learning the ZX calculus.
2

Figure 2: Decomposition of a four-body parity measurement into ﬁve pair measurements. This circuit was
derived from graph (e) in Figure 1. Dashed operations are classical Pauli feedback, which can be removed by
folding its eﬀects into the deﬁnitions of detectors and observables used by the surrounding surface code (see
Figure 6). (Click here to open an equivalent circuit in Quirk.)
In the ZX calculus, an X basis parity measurement can be added to a graph by placing an X
type node on each involved qubit and linking all of the nodes to a central Z type node (see graph
(c) of Figure 1). To create other implementations of this parity measurement, rewrite rules can
be applied to the graph [1]. For example, the "spider fusion" rule (called "S1" in [1]) allows any
edge to be contracted if it’s between nodes of the same type. As another example, the "redundant
node" rule (called "S3" in [1]) allows nodes to be added to the middle of edges.
Consider the following circuit decomposition of an X basis parity measurement. First, init
an ancilla in the X basis producing a |+⟩. Then, for each involved data qubit, apply a CNOT
controlled by the ancilla and targeting the data qubit. Finally, demolition measure the ancilla in
the X basis. In the ZX calculus, a CNOT is represented by a Z type node on the control linked to
an X type node on the target. X basis init and demolition measurement are both represented by Z
type leaf nodes (note: it’s intentional that the leaf node’s type is not the same as the init/measure
basis). So, after converting into a ZX graph, the circuit construction would look like a Z type leaf
(the init) leading into a series of Z type nodes linked to X type nodes on the data qubits (the
CNOTs), leading into a Z type leaf (the measurement). This graph is graph (a) in Figure 1. By
repeatedly applying the spider fusion rule, you can contract all of the Z type nodes into a single
node, producing graph (c) in Figure 1, proving the circuit decomposition is correct.
Now consider the second circuit shown in Figure 6 of Chao et al’s paper [2]. This circuit involves
single-qubit non-demolition measurements, X pair measurements, Z pair measurements, and Pauli
feedback. The Pauli feedback is irrelevant in the ZX calculus, and can be added in again later, so
the ﬁrst step to converting the circuit into a ZX graph is to delete all of the feedback. Each X
basis pair measurement becomes a pair of linked X type nodes (normally an X parity measurement
would link the qubits to a central Z type node but, in the speciﬁc case of a pair measurement, that
node can be omitted because of the "redundant node" rule). Similarly, Z basis pair measurements
become a pair of linked Z type nodes. Each single qubit measurement ends up being a leaf node,
via rule B1 from [1], with some of the leaf nodes corresponding to demolition measurements and
some corresponding to inits. The ﬁnal converted graph is graph (d) in Figure 1. You can verify
that it’s correct by repeatedly applying the spider fusion rule.
So far, all of these constructions look like a Z type core with X type limbs leading to each data
qubit. This is not technically required to be true, but because we’re trying to ﬁnd an eﬃcient
construction it’s unlikely that there’s space to do anything else. So let’s explicitly assume that
the optimal construction will have this form. Notice that this implies the graph must have four
core-to-limb transitions, where there is an edge between a Z type node (from the core) and an
X type node (from a limb). Since pair measurements don’t link nodes of opposite type, and the
only interactions we’re using are pair measurements, the core-to-limb transitions have to occur
3

Figure 3: Pair measurement layout for a 5x5 surface code. White circles are data qubits. Black circles are
measurement qubits. Red shapes are the X stabilizers of the surface code. Blue shapes are the Z stabilizers
of the surface code. Red edges are XX pair measurements performed by the circuit. Blue edges are ZZ pair
measurements performed by the circuit. The red and blue edges form a planar graph with pentagonal faces.
along timelike edges (edges that correspond to the worldline of a qubit). Supposing the core was
a single Z type pair measurement, there would be exactly four timelike edges coming out of this
pair measurement (two inputs and two outputs). That’s exactly enough to get the four transitions
that must appear in the graph. Surrounding this central pair measurement with four X type pair
measurements, one leading to each data qubit, is then exactly enough connections to have each
limb reach a data qubit and connect the whole system together. This construction is graph (e)
of Figure 1.
By spider fusion, it implements the desired four-body parity measurement.
The
corresponding circuit (with feedback restored) is shown in Figure 2.
A diﬀerent way to prove that this construction is correct is to show that it has all of the stabilizer
generators of a four-body X basis parity measurement. These generators are X1 →X1, X2 →X2,
X3 →X3, X4 →X4, Z1Z2 →Z1Z2, Z2Z3 →Z2Z3, Z3Z4 →Z3Z4, and X1X2X3X4 →1. Figure 4
gives example visual proofs for three out of these eight rules. Figure 5 then shows how these rules
can be chained together to form detectors.
The four body parity measurement construction that I’ve described uses ﬁve pair measurements.
This is fewer than the six pair measurement construction reported by Chao et al. This is surprising,
because Chao et al. ran a brute force search of the solution space. I see two plausible reasons
that Chao et al. could have missed the ﬁve pair measurement solution, or decided not to report
it. First, this solution involves the data qubits directly interacting with both measurement qubits.
Chao et al. may have been looking for solutions that only directly interacted the data qubits with
one of the measurement qubits (because this has consequences for the topology required of the
hardware). The second reason Chao et al. may not have reported this solution is because it has
bidirectional hook errors.
A hook error is a single physical error that aﬀects multiple data qubits due to the details of how
abstract operations required by an error correcting code are decomposed into an explicit circuit.
Look closely at Figure 2. A hook error equivalent to the data errors ZaZb occurs if the result of
the central measurement (labelled z1) is ﬂipped. A hook error equivalent to the data errors ZbZc
occurs if the two qubit depolarizing noise during the central measurement applies X ⊗X to the
two measurement qubits. These two diﬀerent hook errors are equivalent to pairs of data errors.
These pairs overlap on one data qubit. Therefore, when laid out in 2d, these hook errors will move
in diﬀerent directions. This is a problem because error chains will be able to use these hook errors
to grow at double speed both vertically and horizontally, regardless of the orientation and ordering
used when interleaving parity measurements. This cuts the code distance of the construction in
half.
4

I’d normally consider halving the code distance to be a showstopping problem. In fact, if I’d
realized early on that my construction had bidirectional hook errors, I’d have dropped the whole
project and moved on to something else. But I actually only caught the problem later, when trying
to understand my initial results, and the initial results were promising enough that I decided to
continue despite the problem.
When mapping the circuit shown in Figure 2 into 2d, it’s natural to place the respective
measurement qubits close to the two data qubits they each interact with. The connections required
by the circuit then form a “puckered H" shape. Although there are hook errors in both directions,
it’s still the case that one hook error is more likely than the other. So, it’s still beneﬁcial to orient
the puckered H layouts one way for the Z basis measurements and the other way for the X basis
measurements (because the X and Z boundaries are in diﬀerent directions). Figure 3 shows the
result of applying this to each of the stabilizers of the surface code. Pleasingly, the resulting set of
connections form a pentagonal tiling of the plane known as the Cairo pentagonal tiling [17].
I tried a few possible ways of interleaving the stabilizer measurements. The best interleaving
that I found was to do all of the X basis stabilizer measurements then all of the Z basis stabilizer
measurements. If these two steps are partially pipelined, then only six additional layers of circuit
are needed per round of stabilizer measurements. The exact circuit interleaving, and the resulting
structure of a detector, are shown in Figure 6.
Figure 4: Examples of stabilizer generators satisﬁed by a ZX graph implementing a four-body parity measure-
ment. Red highlight represents X sensitivity. Blue highlight represents Z sensitivity. For highlights to be valid
they must satisfy two rules. First, sensitivity crosses nodes of the same type. All white (black) nodes must have
red (blue) highlight on an even number of adjacent edges. Second, sensitivity broadcasts over nodes of the
opposite type. All black (white) nodes must have red (blue) highlight on all adjacent edges or on no adjacent
edges.
Figure 5: Example of a detector (an internal tautology) in a ZX graph built from four-body parity measurements.
This graph is a simpliﬁed version of what occurs in a surface code (in particular, the detector crosses two parity
measurements in the opposite basis instead of four). When converting into a circuit, the measurements that
deﬁne the detector will correspond to the highlighted pair measurements in the same basis (vertical edges
between white nodes) combined with the highlighted single qubit measurements in the opposite basis (black
leaf nodes pointing rightward).
5

Figure 6: Order of operations used by my construction, and the resulting structure of a detector in the bulk.
In the circuit diagram, the parity of the 14 measurements colored black is always the same under noiseless
execution, forming a detector. Red highlights show where a Z or Y error would cause the detector to produce a
detection event. Gray boxes with dots always correspond to an MXX measurement with a qubit not shown in
the circuit diagram. The top right panel shows the 2d layout of the circuit with layer order annotations. The
top axis indicates the layers of the circuit diagram, corresponding to the layer annotations in the top right panel.
The overlapping layers ("6,0" and "7,1") are due to partial pipelining of the rounds of stabilizer measurements.
The bottom right panel shows where the measurements included in the detector are, in space, with bold lines
for included pair measurements and bold circles for included single qubit measurements.
6

3
Simulation
To quantify how well my construction works I used Stim, Sinter, and an internal correlated mini-
mum weight perfect matching decoder written by Austin Fowler. Stim is a tool for fast simulation
and analysis of stabilizer circuits [9].
Sinter is a tool for bulk sampling and decoding of Stim
circuits using python multiprocessing [11].
I wrote python code to generate Stim circuits representing my construction, for various error
rates and patch sizes.
I also implemented python code to generate Stim circuits reproducing
the best construction described by Chao et al. [2]. I also generated honeycomb code circuits for
comparison, by using the python code attached to [12]. I added noise to these circuits using a
noise model described in detail in Appendix B. I then sampled the logical error rate of each noisy
circuit, targeting a hundred million shots or ten thousand errors (whichever came ﬁrst). Using a
96 core machine, the sampling process took approximately four days.
Figure 7 shows the sampled logical error rate from each construction for various code distances
and physical error rates. It shows that the threshold of my construction is roughly double the
threshold of Chao et al.’s construction. However, if you look closely at the slopes of the curves,
you can see the eﬀects of the bidirectional hook errors: my construction is improving slower than
Chao et al’s as the physical error rate decreases.
Figure 8 shows essentially the same data as Figure 7, but accompanied by line ﬁts that project
the number of qubits required to achieve a given logical error rate. The line ﬁts were computed
using a Bayesian method, with a truncation step to represent systemic uncertainty (see Appendix A
for details).
Figure 9 highlights the intercepts of the line ﬁts from Figure 8 with a target logical error rate
of one in a trillion. In other words, it estimates the number of physical qubits per logical qubit
needed to reach algorithmically relevant logical error rates. In this plot you can clearly see the eﬀect
where my construction is initially better than Chao et al’s construction, but will eventually become
worse as the physical error rate decreases. You can also clearly see that, although my construction
closes some of the gap between the surface code and the honeycomb code when compiling into pair
measurements, the honeycomb code is still more eﬃcient. At a physical error rate of 0.1%, the
honeycomb code has a teraquop footprint of around 1000 qubits, compared to 3000 qubits for my
construction and 6000 qubits for Chao et al’s construction.
7

Figure 7: Physical error rate vs logical error rate for various patch widths. Based on X basis memory experiments;
see Appendix D for Z basis. Note that the honeycomb code uses a qubit patch with a 2:3 aspect ratio (costs
an extra factor of 1.5 in qubit count relative to the surface code constructions) but has no measurement
ancillae (saves a factor of 1.5 in qubit count relative to the surface code constructions), so ultimately all the
constructions have similar qubit counts at a given width. Decoding was done using correlated minimum weight
perfect matching. Shading shows the range of hypothesis probabilities with a likelihood within a factor of 1000
of the max likelihood hypothesis, given the sampled data.
Figure 8: Linear extrapolation of log logical error rate versus square root qubit count, for various physical error
rates. Based on X basis memory experiments; see Appendix D for Z basis. The vertical bar attached to each
point shows the range of hypothesis probabilities with a likelihood within a factor of 1000 of the max likelihood
hypothesis, given the sampled data. See Appendix A for a discussion of how the line ﬁts were computed.
8

Figure 9: Estimated teraquop footprints for error correcting codes compiled into pair measurements. Based
on X basis memory experiments; see Appendix D for Z basis. Derived from the X intercepts at Y = 10−12 in
Figure 8. The teraquop footprint is the number of physical qubits needed to create a logical qubit large enough
to reliably to execute one trillion code distance blocks, which is enough blocks to build classically intractable
instances of textbook algorithms like Shor’s algorithm. See Appendix A for a discussion of how the line ﬁts used
for these data points were computed.
9

4
Conclusion
In this paper, I presented a new way to compile the surface code into pair measurements, found by
using the ZX calculus. Although the resulting construction isn’t as eﬃcient as the honeycomb code,
and although it has bidirectional hook errors that cut its code distance in half, my construction
doubled the threshold and halved the teraquop footprint (at a physical error rate of 0.1%) compared
to previous work compiling the surface code into pair measurements.
One of the striking things I noticed, after compiling the surface code into pair measurements,
is how the compiled circuit looks a lot like the circuit for a ﬂoquet code. The circuit doesn’t really
know about the stabilizers that it came from, it only knows about the pair measurements running
along the pentagonal faces.
Detectors are formed from the edge measurements by combining
them in surprising ways that narrowly avoid anti-commuting with intermediate measurements (see
Figure 6). That’s also what detectors look like in ﬂoquet codes. Particularly notable is that the
compiled circuit is constantly moving the logical observable (for example, see the stim circuit shown
in Appendix C and note the OBSERVABLE_INCLUDE instruction inside the REPEAT block).
Having to constantly move the observable is normally thought of as the deﬁning aspect of a ﬂoquet
code [14], but here the same property is occurring in a compiled surface code circuit.
Another fact about the circuit that surprised me was how well it performs, despite having half
code distance. In the past, I’ve tried very hard to make sure I didn’t accidentally cut the code
distance of a construction in half. In this paper, I instead just accepted the halved code distance,
moved forward, and found that the results were still competitive. This suggests that (at least for
physical error rates above 0.1%), whether the code distance was cut in half isn’t a reliable predictor
of performance.
5
Acknowledgements
Thanks to Hartmut Neven for creating an environment where this work was possible. Thanks to
Austin Fowler for writing the correlated minimum weight perfect matching decoder used by this
paper. Thanks to Oscar Higgott for feedback that improved the paper. Thanks to Matt McEwen
for an enormous amount of feedback that improved the paper.
References
[1] Miriam Backens, Simon Perdrix, and Quanlong Wang. A simpliﬁed stabilizer zx-calculus.
arXiv preprint arXiv:1602.04744, 2016.
[2] Rui Chao, Michael E Beverland, Nicolas Delfosse, and Jeongwan Haah. Optimization of the
surface code design for majorana-based qubits. Quantum, 4:352, 2020. DOI: 10.22331/q-2020-
10-28-352.
[3] Bob Coecke and Ross Duncan.
Interacting quantum observables: categorical algebra and
diagrammatics. New Journal of Physics, 13(4):043016, 2011.
[4] Bob Coecke and Aleks Kissinger. Picturing quantum processes. Cambridge University Press,
2017.
[5] Niel de Beaudrap and Dominic Horsman. The zx calculus is a language for surface code lattice
surgery. arXiv preprint arXiv:1704.08670, 2017.
[6] Niel de Beaudrap and Dominic Horsman. The zx calculus is a language for surface code lattice
surgery. arXiv preprint arXiv:1704.08670, 2017.
[7] Nicolas Delfosse and Naomi H Nickerson. Almost-linear time decoding algorithm for topolog-
ical codes. Quantum, 5:595, 2021. DOI: 10.22331/q-2021-12-02-595.
[8] A. G. Fowler, M. Mariantoni, J. M. Martinis, and A. N. Cleland. Surface codes: Towards prac-
tical large-scale quantum computation. Phys. Rev. A, 86:032324, 2012. DOI: 10.1103/Phys-
RevA.86.032324. arXiv:1208.0928.
[9] Craig Gidney. Stim: a fast stabilizer circuit simulator. Quantum, 5:497, July 2021. ISSN
2521-327X. DOI: 10.22331/q-2021-07-06-497.
[10] Craig Gidney. Data for "A Pair Measurement Surface Code on Pentagons", June 2022. URL
https://doi.org/10.5281/zenodo.6626417.
10

[11] Craig Gidney. Sinter source code on github. https://github.com/quantumlib/Stim/tree/
main/glue/sample, 2022.
[12] Craig Gidney and Michael Newman.
Benchmarking the planar honeycomb code.
arXiv
preprint arXiv:2202.11845, 2022. DOI: 10.48550/arXiv.2202.11845.
[13] Craig Gidney, Michael Newman, Austin Fowler, and Michael Broughton. A fault-tolerant
honeycomb memory. Quantum, 5:605, 2021. DOI: 10.22331/q-2021-12-20-605.
[14] Matthew B Hastings and Jeongwan Haah. Dynamically generated logical qubits. Quantum,
5:564, 2021. DOI: 10.22331/q-2021-10-19-564.
[15] Clare Horsman, Austin G Fowler, Simon Devitt, and Rodney Van Meter.
Surface code
quantum computing by lattice surgery. New Journal of Physics, 14(12):123011, 2012. DOI:
10.1088/1367-2630/14/12/123011.
[16] Adam Paetznick, Christina Knapp, Nicolas Delfosse, Bela Bauer, Jeongwan Haah, Matthew B
Hastings, and Marcus P da Silva. Performance of planar ﬂoquet codes with majorana-based
qubits. arXiv preprint arXiv:2202.11829, 2022. DOI: 10.48550/arXiv.2202.11829.
[17] Wikipedia.
Cairo pentagonal tiling — Wikipedia, the free encyclopedia.
https://en.
wikipedia.org/wiki/Cairo_pentagonal_tiling, 2022. [Online; accessed 4-June-2022].
11

A
Line Fits
The line ﬁts shown in Figure 8 and used for the data points in Figure 9 were computed by using
a Bayesian method, with a truncation step to represent systemic uncertainty.
The line Y = mX + b was deﬁned to be the hypothesis that, for a qubit count q satisfying
√q = X, sampling whether or not a shot had a logical error was equivalent to sampling from a
Bernoulli distribution with parameter p = exp(Y ) = exp(m√q + b).
Given a hypothesis, and the data that was actually sampled, you can compute the probability
that the hypothesis assigned to seeing that data. This is the hypothesis’ likelihood. Note that the
probability of any one speciﬁc thing happening is always tiny, well below the smallest ﬁnite values
that can be presented by ﬂoating point numbers, so likelihood computations have to be done in
log space.
The dashed lines shown in Figure 8 are the max likelihood hypotheses. They are the lines
whose hypotheses assigned the highest probability to the data. The shaded regions are the union
of all the lines whose likelihoods were within a factor of 1000 of the likelihood of the max likelihood
hypothesis.
The beneﬁt of this ﬁtting method, over just performing a typical least squares error line ﬁt to
the points shown in Figure 8 are (1) it doesn’t require discarding data when no errors are observed,
(2) it gives a good sense of the shape of nearby hypotheses, and (3) it naturally accounts for varying
statistical uncertainty in the locations of points being used for the ﬁt.
Unfortunately, although this ﬁtting method is essentially optimal at quantifying which hypothe-
ses are more likely, it doesn’t deal well with systemic error where the true hypothesis is not one
of the hypotheses being considered. For example, by adding more data, you can make the line ﬁts
qualitatively worse. If you keep collecting more and more samples for two points with high logical
error rates, which is cheap to do, you’ll force the line ﬁt to exactly predict those two speciﬁc points.
But the true system doesn’t have logical error rates that lie exactly on a line, so this sacriﬁces the
correspondence to reality of the ﬁt for points at low logical error rates. And our goal is to predict
the behavior at low logical error rates.
To mitigate this problem, I used a hack: I clamped how many errors a point could claim it had.
Any point with s shots and e errors, with e > 10, was instead presented to the ﬁtting function as
a point with ⌈s · 10/e⌉shots and 10 errors. The truncation is supposed to represent the fact that
there is unavoidable uncertainty in how closely the line can estimate the true curve.
Anecdotally, using truncation appears to signiﬁcantly improve the ﬁts. It’s better than treating
all the points equally, which gives low sample count points too much pull (lone "unlucky" errors,
which do appear in the data, shift the prediction too drastically). It’s also better than not clamp-
ing, which gives high sample count points too much pull and produces obviously overconﬁdent
predictions.
Note that Figure 8 doesn’t show data points that had 0 observed errors because, although
the points themselves wouldn’t be visible, their error bars would all overlap and obscure the data
points with less than 5 sampled errors. Although these data points aren’t shown, they were still
included when ﬁtting. The samples they represented correctly aﬀected the likelihoods computed
for each hypothesis.
Note that, before line ﬁtting begins, points with sampled logical error rates above 40% are
discarded (to avoid working with points above threshold).
12

B
Noise Model
The noise model used by this paper treats all measurements the same: single qubit measurements
are treated the same as pair measurements, and demolition measurements are treated the same as
non-demolition measurements. The noise model applies anti-commuting Pauli errors after resets,
depolarization after any other operation (including idling), and it probabilistically ﬂips measure-
ment results. See Table 2.
Note that the model asserts that classically controlled Pauli gates are completely free. Clas-
sically controlled Paulis introduce no noise, they consume no time, they do not prevent other
operations from being applied to the target qubit, they don’t aﬀect whether the target qubit is
idling, and they incur no reaction delay from waiting for measurement results. This is because,
in stabilizer circuits, classically controlled Paulis can always be applied entirely within the control
system and can be deferred until later by conjugating them by upcoming Cliﬀord operations.
Noise channel
Probability distribution of eﬀects
MERR(p)
1 −p →(report previous measurement correctly)
p →(report previous measurement incorrectly; ﬂip its result)
XERR(p)
1 −p →I
p →X
ZERR(p)
1 −p →I
p →Z
DEP1(p)
1 −p →I
p/3 →X
p/3 →Y
p/3 →Z
DEP2(p)
1 −p →I ⊗I
p/15 →I ⊗X
p/15 →I ⊗Y
p/15 →I ⊗Z
p/15 →X ⊗I
p/15 →X ⊗X
p/15 →X ⊗Y
p/15 →X ⊗Z
p/15 →Y ⊗I
p/15 →Y ⊗X
p/15 →Y ⊗Y
p/15 →Y ⊗Z
p/15 →Z ⊗I
p/15 →Z ⊗X
p/15 →Z ⊗Y
p/15 →Z ⊗Z
Table 1: Deﬁnitions of various noise channels. Used by Table 2.
Ideal gate
Noisy gate
(Idling)
DEP1(p)
(Single qubit gate) U1
U1 ◦DEP1(p)
RX
RX ◦ZERR(p)
RY
RY ◦XERR(p)
RZ
RZ ◦XERR(p)
MX
MX ◦MERR(p) ◦DEP1(p)
MY
MY ◦MERR(p) ◦DEP1(p)
MZ
MZ ◦MERR(p) ◦DEP1(p)
MXX
MXX ◦MERR(p) ◦DEP2(p)
MY Y
MY Y ◦MERR(p) ◦DEP2(p)
MZZ
MZZ ◦MERR(p) ◦DEP2(p)
(classically controlled Pauli) P m
(Done in classical control system.)
Table 2: How to turn ideal gates into noisy gates under the "pair measurement depolarizing noise" model used
by this paper. The model is deﬁned by a single parameter p. Note A ◦B = B · A meaning B is applied after
A. Noise channels are deﬁned in Table 1.
13

C
Example Circuit
This stim circuit is a noisy 10-round X-basis d = 3 pentagonal-surface-code memory experiment.
QUBIT_COORDS (-1, 6) 0
QUBIT_COORDS (0, 0) 1
QUBIT_COORDS (0, 4) 2
QUBIT_COORDS (0, 8) 3
QUBIT_COORDS (1, 2) 4
QUBIT_COORDS (2,
-1) 5
QUBIT_COORDS (2, 5) 6
QUBIT_COORDS (2, 7) 7
QUBIT_COORDS (3, 2) 8
QUBIT_COORDS (4, 0) 9
QUBIT_COORDS (4, 4) 10
QUBIT_COORDS (4, 8) 11
QUBIT_COORDS (5, 6) 12
QUBIT_COORDS (6, 1) 13
QUBIT_COORDS (6, 3) 14
QUBIT_COORDS (6, 9) 15
QUBIT_COORDS (7, 6) 16
QUBIT_COORDS (8, 0) 17
QUBIT_COORDS (8, 4) 18
QUBIT_COORDS (8, 8) 19
QUBIT_COORDS (9, 2) 20
RX 1 2 3 9 10 11 17 18 19
R 0 4 8 12 16 20
X_ERROR (0.001) 0 4 8 12 16 20
Z_ERROR (0.001) 1 2 3 9 10 11 17 18 19
DEPOLARIZE1 (0.001) 5 6 7 13 14 15
TICK
MPP (0.001)
X0*X2 X1*X4 X8*X9 X10*X12 X16*X18 X17*X20
DEPOLARIZE2 (0.001) 0 2 1 4 8 9 10 12 16 18 17 20
DEPOLARIZE1 (0.001) 3 5 6 7 11 13 14 15 19
TICK
MPP (0.001)
Z4*Z8 Z12*Z16
DEPOLARIZE2 (0.001) 4 8 12 16
DEPOLARIZE1 (0.001) 0 1 2 3 5 6 7 9 10 11 13 14 15 17 18 19 20
TICK
RX 5 6 7 13 14 15
MPP (0.001)
X0*X3 X2*X4 X8*X10 X11*X12 X16*X19 X18*X20
DEPOLARIZE2 (0.001) 0 3 2 4 8 10 11 12 16 19 18 20
Z_ERROR (0.001) 5 6 7 13 14 15
DEPOLARIZE1 (0.001) 1 9 17
TICK
M(0.001) 0 4 8 12 16 20
MPP (0.001)
Z1*Z5 Z2*Z6 Z3*Z7 Z9*Z13 Z10*Z14 Z11*Z15
DEPOLARIZE1 (0.001) 0 4 8 12 16 20
DEPOLARIZE2 (0.001) 1 5 2 6 3 7 9 13 10 14 11 15
DEPOLARIZE1 (0.001)
17 18 19
TICK
MPP (0.001)
X6*X7 X13*X14
DEPOLARIZE2 (0.001) 6 7 13 14
DEPOLARIZE1 (0.001) 0 1 2 3 4 5 8 9 10 11 12 15 16 17 18 19 20
TICK
R 0 4 8 12 16 20
MPP (0.001)
Z5*Z9 Z6*Z10 Z7*Z11 Z13*Z17 Z14*Z18 Z15*Z19
DEPOLARIZE2 (0.001) 5 9 6 10 7 11 13 17 14 18 15 19
X_ERROR (0.001) 0 4 8 12 16 20
DEPOLARIZE1 (0.001) 1 2 3
TICK
MX (0.001) 5 6 7 13 14 15
MPP (0.001)
X0*X2 X1*X4 X8*X9 X10*X12 X16*X18 X17*X20
DETECTOR (-2, 6, 0) rec [ -46] rec [ -38]
DETECTOR (2, 2, 0) rec [ -45] rec [ -44] rec [ -37] rec [ -36]
DETECTOR (6, 6, 0) rec [ -43] rec [ -42] rec [ -35] rec [ -34]
DETECTOR (10, 2, 0) rec [ -41] rec [ -33]
OBSERVABLE_INCLUDE (0) rec [ -12] rec [-9]
SHIFT_COORDS (0, 0, 1)
DEPOLARIZE1 (0.001) 5 6 7 13 14 15
DEPOLARIZE2 (0.001) 0 2 1 4 8 9 10 12 16 18 17 20
DEPOLARIZE1 (0.001) 3 11 19
TICK
REPEAT 8 {
MPP (0.001)
Z4*Z8 Z12*Z16
DEPOLARIZE2 (0.001) 4 8 12 16
DEPOLARIZE1 (0.001) 0 1 2 3 5 6 7 9 10 11 13 14 15 17 18 19 20
TICK
RX 5 6 7 13 14 15
MPP (0.001)
X0*X3 X2*X4 X8*X10 X11*X12 X16*X19 X18*X20
DEPOLARIZE2 (0.001) 0 3 2 4 8 10 11 12 16 19 18 20
Z_ERROR (0.001) 5 6 7 13 14 15
DEPOLARIZE1 (0.001) 1 9 17
TICK
M(0.001) 0 4 8 12 16 20
MPP (0.001)
Z1*Z5 Z2*Z6 Z3*Z7 Z9*Z13 Z10*Z14 Z11*Z15
DEPOLARIZE1 (0.001) 0 4 8 12 16 20
DEPOLARIZE2 (0.001) 1 5 2 6 3 7 9 13 10 14 11 15
DEPOLARIZE1 (0.001)
17 18 19
TICK
MPP (0.001)
X6*X7 X13*X14
DEPOLARIZE2 (0.001) 6 7 13 14
DEPOLARIZE1 (0.001) 0 1 2 3 4 5 8 9 10 11 12 15 16 17 18 19 20
TICK
R 0 4 8 12 16 20
MPP (0.001)
Z5*Z9 Z6*Z10 Z7*Z11 Z13*Z17 Z14*Z18 Z15*Z19
DEPOLARIZE2 (0.001) 5 9 6 10 7 11 13 17 14 18 15 19
X_ERROR (0.001) 0 4 8 12 16 20
DEPOLARIZE1 (0.001) 1 2 3
TICK
MX (0.001) 5 6 7 13 14 15
MPP (0.001)
X0*X2 X1*X4 X8*X9 X10*X12 X16*X18 X17*X20
DETECTOR (-2, 6, 0) rec [ -86] rec [ -78] rec [ -60] rec [ -46] rec [ -38]
DETECTOR (2,
-2, 0) rec [ -66] rec [ -58] rec [ -40] rec [ -26] rec [ -18]
DETECTOR (2, 2, 0) rec [ -85] rec [ -84] rec [ -77] rec [ -76] rec [ -59] rec [ -52] rec [ -51] rec [ -45] rec [ -44] rec [ -37] rec [ -36]
DETECTOR (2, 6, 0) rec [ -65] rec [ -64] rec [ -57] rec [ -56] rec [ -40] rec [ -32] rec [ -31] rec [ -30] rec [ -29] rec [ -25] rec [ -24] rec [ -17] rec [ -16]
DETECTOR (6, 2, 0) rec [ -63] rec [ -62] rec [ -55] rec [ -54] rec [ -39] rec [ -30] rec [ -27] rec [ -23] rec [ -22] rec [ -15] rec [ -14]
DETECTOR (6, 6, 0) rec [ -83] rec [ -82] rec [ -75] rec [ -74] rec [ -60] rec [ -51] rec [ -50] rec [ -48] rec [ -47] rec [ -43] rec [ -42] rec [ -35] rec [ -34]
DETECTOR (6, 10, 0) rec [ -61] rec [ -53] rec [ -39] rec [ -29] rec [ -28] rec [ -21] rec [ -13]
DETECTOR (10, 2, 0) rec [ -81] rec [ -73] rec [ -59] rec [ -49] rec [ -48] rec [ -41] rec [ -33]
OBSERVABLE_INCLUDE (0) rec [ -12] rec [-9]
SHIFT_COORDS (0, 0, 1)
DEPOLARIZE1 (0.001) 5 6 7 13 14 15
DEPOLARIZE2 (0.001) 0 2 1 4 8 9 10 12 16 18 17 20
DEPOLARIZE1 (0.001) 3 11 19
TICK
}
MPP (0.001)
Z4*Z8 Z12*Z16
DEPOLARIZE2 (0.001) 4 8 12 16
DEPOLARIZE1 (0.001) 0 1 2 3 5 6 7 9 10 11 13 14 15 17 18 19 20
TICK
RX 5 6 7 13 14 15
MPP (0.001)
X0*X3 X2*X4 X8*X10 X11*X12 X16*X19 X18*X20
DEPOLARIZE2 (0.001) 0 3 2 4 8 10 11 12 16 19 18 20
Z_ERROR (0.001) 5 6 7 13 14 15
DEPOLARIZE1 (0.001) 1 9 17
TICK
M(0.001) 0 4 8 12 16 20
MPP (0.001)
Z1*Z5 Z2*Z6 Z3*Z7 Z9*Z13 Z10*Z14 Z11*Z15
DEPOLARIZE1 (0.001) 0 4 8 12 16 20
DEPOLARIZE2 (0.001) 1 5 2 6 3 7 9 13 10 14 11 15
DEPOLARIZE1 (0.001)
17 18 19
TICK
MPP (0.001)
X6*X7 X13*X14
DEPOLARIZE2 (0.001) 6 7 13 14
DEPOLARIZE1 (0.001) 0 1 2 3 4 5 8 9 10 11 12 15 16 17 18 19 20
TICK
MPP (0.001)
Z5*Z9 Z6*Z10 Z7*Z11 Z13*Z17 Z14*Z18 Z15*Z19
DEPOLARIZE2 (0.001) 5 9 6 10 7 11 13 17 14 18 15 19
DEPOLARIZE1 (0.001) 0 1 2 3 4 8 12 16 20
TICK
MX (0.001) 5 6 7 13 14 15
OBSERVABLE_INCLUDE (0) rec [-6] rec [-3]
MX (0.001) 1 2 3 9 10 11 17 18 19
DETECTOR (-2, 6, 0) rec [ -89] rec [ -81] rec [ -63] rec [ -49] rec [ -41]
DETECTOR (2,
-2, 0) rec [ -69] rec [ -61] rec [ -43] rec [ -29] rec [ -21]
DETECTOR (2, 2, 0) rec [ -88] rec [ -87] rec [ -80] rec [ -79] rec [ -62] rec [ -55] rec [ -54] rec [ -48] rec [ -47] rec [ -40] rec [ -39]
DETECTOR (2, 6, 0) rec [ -68] rec [ -67] rec [ -60] rec [ -59] rec [ -43] rec [ -35] rec [ -34] rec [ -33] rec [ -32] rec [ -28] rec [ -27] rec [ -20] rec [ -19]
DETECTOR (6, 2, 0) rec [ -66] rec [ -65] rec [ -58] rec [ -57] rec [ -42] rec [ -33] rec [ -30] rec [ -26] rec [ -25] rec [ -18] rec [ -17]
DETECTOR (6, 6, 0) rec [ -86] rec [ -85] rec [ -78] rec [ -77] rec [ -63] rec [ -54] rec [ -53] rec [ -51] rec [ -50] rec [ -46] rec [ -45] rec [ -38] rec [ -37]
DETECTOR (6, 10, 0) rec [ -64] rec [ -56] rec [ -42] rec [ -32] rec [ -31] rec [ -24] rec [ -16]
DETECTOR (10, 2, 0) rec [ -84] rec [ -76] rec [ -62] rec [ -52] rec [ -51] rec [ -44] rec [ -36]
SHIFT_COORDS (0, 0, 1)
DETECTOR (-2, 6, 0) rec [ -49] rec [ -41] rec [ -23] rec [-8] rec[-7]
DETECTOR (2, 2, 0) rec [ -48] rec [ -47] rec [ -40] rec [ -39] rec [ -22] rec [ -15] rec [ -14] rec [-9] rec [-8] rec [-6] rec [-5]
DETECTOR (6, 6, 0) rec [ -46] rec [ -45] rec [ -38] rec [ -37] rec [ -23] rec [ -14] rec [ -13] rec [ -11] rec [ -10] rec [-5] rec [-4] rec [-2] rec [-1]
DETECTOR (10, 2, 0) rec [ -44] rec [ -36] rec [ -22] rec [ -12] rec [ -11] rec[-3] rec [-2]
OBSERVABLE_INCLUDE (0) rec [-9] rec [-6] rec[-3]
DEPOLARIZE1 (0.001) 5 6 7 13 14 15 1 2 3 9 10 11 17 18 19 0 4 8 12 16 20
14

D
Additional Data
Figure 10: Physical error rate vs logical error rate for various patch widths, based on Z basis memory experiments.
See Figure 7 for X basis. The X and Z basis results are not exactly identical (due to diﬀerences in the layout
of each observable, and due to statistical noise) but are qualitatively identical.
Figure 11: Linear extrapolation of log logical error rate versus square root qubit count, for various physical error
rates. Based on Z basis memory experiments; see Figure 8 for X basis. The X and Z basis results are not exactly
identical (due to diﬀerences in the layout of each observable, and due to statistical noise) but are qualitatively
identical.
15

Figure 12: Estimated teraquop footprints for error correcting codes compiled into pair measurements. Based
on Z basis memory experiments; see Figure 9 for X basis. The X and Z basis results are not exactly identical
(due to diﬀerences in the layout of each observable, and due to statistical noise) but are qualitatively identical.
16

