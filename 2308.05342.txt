Metacognitive Prompting Improves Understanding in Large Language Models
Yuqing Wang1, Yun Zhao2
1University of California, Santa Barbara
2 Meta Platforms, Inc.
https://github.com/EternityYW/Metacognitive-Prompting
Abstract
In Large Language Models (LLMs), there have been consis-
tent advancements in task-specific performance, largely influ-
enced by effective prompt design. While recent research on
prompting has enhanced the reasoning capabilities of LLMs,
a gap remains in further improving their understanding abil-
ities. In this study, we introduce Metacognitive Prompting
(MP), a strategy inspired by human introspective reasoning
processes. Using MP, LLMs undergo a systematic series of
structured, self-aware evaluations, drawing on both their vast
inherent knowledge and new insights. Our experiments in-
volve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5,
and GPT-4, all of which span various general natural lan-
guage understanding (NLU) tasks from the GLUE and Su-
perGLUE benchmarks. Results indicate that, although GPT-4
consistently excels in most tasks, PaLM, when equipped with
MP, approaches its performance level. Furthermore, across
models and datasets, MP consistently outperforms existing
prompting methods, including standard and chain-of-thought
prompting. This study underscores the potential to amplify
the understanding abilities of LLMs and highlights the bene-
fits of mirroring human introspective reasoning in NLU tasks.
Introduction
Large Language Models (LLMs) have made significant ad-
vancements in natural language processing (NLP) in recent
years (Min et al. 2021; Zhao et al. 2023; Wang, Zhao, and
Petzold 2023a). However, as these models progress, sim-
ply increasing their scale does not necessarily enhance their
understanding and reasoning capabilities (Rae et al. 2021).
Delving into the intricacies of prompt design has emerged
as a promising approach; it not only rivals the benefits of ex-
tensive fine-tuning but also offers clear advantages in sample
efficiency (Liu et al. 2023; Kojima et al. 2022).
Several research efforts have extensively explored prompt
design, particularly emphasizing the use of Chain-of-
Thought (CoT) (Wei et al. 2022) approaches to advance
intermediate reasoning steps. This research trajectory has
given rise to variants such as the Least-to-Most (Zhou et al.
2022) and the Self-consistency (Wang et al. 2022a) tech-
niques. While these strategies are effective in designated
contexts, their main objective centers around enhancing ex-
plicit reasoning capacities in areas like arithmetic, common-
sense, and symbolic reasoning, guiding LLMs through a
Preprint. Under Review.
logical progression of thought. However, a limitation arises
in their capacity to deepen understanding. While reasoning
entails methodically connecting concepts, understanding re-
quires an intrinsic grasp of the underlying semantics and the
broader context behind words.
Self-understanding
Reflection
Self-regulation
Text 
Interpretation
Preliminary 
Judgment
Inference 
Evaluation
Final Decision 
and Justification
Confidence 
Assessment
What do I know?
What is my first 
thought?
Is my first thought 
accurate?
How can I justify 
my decision?
How confident am I 
about this decision?
* Recognition of knowledge base
* Assessment of initial 
interpretation
* Re-evaluation of initial 
assessment
* Strategy and decision adjustment 
based on reflections
* Reliability assessment of the 
final decision
Human Metacognition
MP for LLMs
Figure 1: Alignment between human metacognitive pro-
cesses and the stages of metacognitive prompting in LLMs.
While previous research primarily focuses on refining the
logical progression of responses, the concept of metacogni-
tion—often defined as “thinking about thinking”—offers a
unique perspective. Originating from the field of cognitive
psychology (Schwarz 2015), metacognition pertains to an
individual’s awareness and introspection of their cognitive
processes. Informed by this insight, our proposed method,
named Metacognitive Prompting (MP), integrates key as-
pects of human metacognitive processes into LLMs. Fig-
ure 1 illustrates the parallels between human metacognitive
stages and the operational steps of our method in LLMs.
Rather than concentrating solely on the mechanics of “how”
a response is produced, this method delves deeper into the
rationale or “why” behind it. The method proceeds as fol-
lows: 1) the LLM interprets the provided text, a phase rem-
iniscent of human comprehension; 2) the model then forms
an initial judgment, mirroring the stage in which humans
generate judgments based on information; 3) the LLM sub-
arXiv:2308.05342v3  [cs.CL]  17 Aug 2023

jects its preliminary inference to critical evaluation, a step
aligned with the self-reflection that humans engage in during
cognitive processes; 4) after this introspective assessment,
the model finalizes its decision and elucidates its reason-
ing, similar to human decision-making and rationalization;
5) finally, the LLM gauges its confidence in the outcomes,
reflecting how humans evaluate the credibility of their judg-
ments and explanations. This paradigm elevates the model’s
function beyond simple systematic reasoning, compelling it
to participate in introspective evaluations that determine the
depth and relevance of its responses.
We conducted experiments on a range of NLU tasks from
the GLUE (Wang et al. 2019b) and SuperGLUE (Wang et al.
2019a) benchmarks using several leading LLMs, including
Llama2 (Touvron et al. 2023), Vicuna (Chiang et al. 2023),
PaLM (Anil et al. 2023), GPT-3.5, and GPT-4 (OpenAI
2023). Our empirical evaluations underscore the superior-
ity of MP over existing prompting strategies such as stan-
dard and CoT prompting. This work emphasizes the impor-
tance of incorporating human-inspired introspective reason-
ing into LLMs, shedding light on an approach that deepens
their understanding abilities.
In summary, our contributions are threefold:
(1) We introduce metacognitive prompting, a novel prompt-
ing strategy for LLMs rooted in human introspective rea-
soning. This method formalizes the self-aware evaluation
process within the LLM, bridging the gap between mere
task execution and deeper understanding.
(2) Our extensive experiments across various NLU tasks
demonstrate the superiority of MP over existing prompt-
ing paradigms, highlighting its potential to enhance LLM
understanding abilities.
(3) Through error and confidence analysis, we demonstrate
that MP incorporates human-inspired introspection into
LLM comprehension, thereby addressing specific under-
standing challenges and enhancing model reliability.
Related Work
Our proposal for metacognitive prompting is informed by
several foundational trajectories: the evolving paradigms
of prompting within LLMs, advancements in NLU in the
broader NLP domain, and the intricate interplay between
cognitive processes and NLU dynamics.
Prompting Techniques in LLMs
Prompts are essential tools for directing the vast capabil-
ities of LLMs. These specially designed queries or state-
ments direct the model, steering it towards generating ac-
curate outputs or performing specific tasks. While current
research primarily focuses on enhancing the reasoning abili-
ties of LLMs, the main strategies include CoT-related meth-
ods (Wei et al. 2022; Zhou et al. 2022; Kojima et al. 2022;
Zhang et al. 2022) involving multi-step reasoning, and self-
consistency techniques (Wang et al. 2022a; Zheng et al.
2023). In the latter, multiple answers from the LLMs are
considered, and the correct one is determined through ma-
jority voting. However, there still exists a significant gap in
enhancing NLU within LLMs. Inspired by human cognitive
processes, we introduce MP. This approach not only seeks
to bridge the understanding gap but also paves the way for
deeper comprehension and more reliable model outputs.
Natural Language Understanding in NLP
Natural language understanding is a fundamental aspect of
NLP, emphasizing a machine’s capacity to grasp the se-
mantics and nuances of human language. Its applications
span diverse domains such as question answering (Namazi-
far et al. 2021), text classification (Wang et al. 2022b), and
natural language inference (Nie, Zhou, and Bansal 2020),
as well as commercial tools like chatbots (Ait-Mlouk and
Jiang 2020), voice assistants (Bellegarda 2013), and ma-
chine translation. While LLMs have gained remarkable trac-
tion in recent years, with increased efforts dedicated to ex-
panding NLU boundaries, the primary research emphasis
has been on their reasoning abilities (Huang and Chang
2022), ethical use (Weidinger et al. 2021; Zhuo et al. 2023),
and broad applications (Zhao et al. 2021; Surameery and
Shakor 2023; Wang, Zhao, and Petzold 2023b). However,
the NLU competencies of LLMs have remained relatively
underexplored. To address this gap, our study delves into
the understanding capabilities of various LLMs, employing
effective prompting techniques.
Cognitive Processes in NLU
The interplay between cognitive processes and NLU has al-
ways been a central consideration in computational linguis-
tics (Peri˜n´an Pascual and Arcas T´unez 2007; Hausser and
Hausser 2001). Cognitive processes, which encompass ar-
eas like attention, memory, reasoning, and problem-solving,
govern how humans understand, produce, and engage with
language in diverse scenarios. These processes heavily in-
fluence our linguistic abilities (Allen 1995; Cambria and
White 2014). In the domain of NLU, incorporating cog-
nitive insights may offer improvements in model compre-
hension. Recognizing this intrinsic connection, our work is
inspired to employ a metacognition-based prompting tech-
nique, a method rooted in higher-order cognition that reflects
on thinking and decision-making, to bolster the understand-
ing capabilities of LLMs, thereby harmonizing traditional
modeling techniques with cognitive nuances.
Metacognitive Prompting
In the complex terrain of human cognition, metacogni-
tion—our ability to introspect and regulate our thinking pro-
cesses—stands as a keystone for intricate problem-solving
and decision-making. This higher-level cognition underlies
our proficiency in breaking down abstract concepts, criti-
cally evaluating scenarios, and fine-tuning our reasoning.
The principal objective of this work is to imbue language
models with a simulated metacognitive process—a sequen-
tial series of cognitive stages that mirror human “thinking
about thinking”. In doing so, we aim to augment the mod-
els’ capabilities in interpreting and responding to natural
language understanding tasks.
Our proposed method, Metacognitive Prompting (MP),
instills critical elements of human metacognition into LLMs.

Question: For the question pair, Question 1: “What are the most beautiful beaches in the world?” and Question 2: 
“What is the most beautiful beach?”, determine if the two questions are paraphrases of each other. 
Clarify your understanding of both questions.
Make a preliminary identification of similarity 
based on subject matter, context, and semantic 
contents.
Critically assess your preliminary analysis. If you 
are unsure about the initial assessment of the 
questions being paraphrases, try to reassess it. 
Evaluate your confidence (0 – 100%) in your 
analysis and provide an explanation for this 
confidence level.
Confirm your final decision on whether the 
questions are paraphrases or not and provide the 
reasoning for your decision. 
I understand that both questions are asking about the 
most beautiful beaches in the world.
Both questions seem to ask about the beauty of 
beaches, but Question 1 refers to multiple beaches 
while Question 2 asks about a single beach.
Upon reassessment, although Question 1 asks about 
multiple beaches and Question 2 about one, they are 
essentially similar and hence considered paraphrases.
I am 95% confident in my analysis. The change of 
judgment is based on the reassessment of the key 
information both questions seek.
The paraphrase status is paraphrase. Despite a 
difference in quantity, both questions fundamentally 
seek the same type of information.
Metacognitive Stages
Metacognitive Prompting
LLM Responses
Comprehension 
Clarification
Preliminary 
Judgment
Critical 
Evaluation
Decision 
Confirmation
Confidence 
Assessment
LLM
LLM
LLM
LLM
LLM
As you perform this task, follow these steps:
1
2
3
4
5
Figure 2: Our proposed method, metacognitive prompting, emulates certain aspects of human metacognition, consisting of five
stages: 1) understanding the input text, 2) making a preliminary judgment, 3) critically evaluating this preliminary analysis,
4) reaching a final decision accompanied by an explanation of the reasoning, and 5) evaluating the confidence level in the
entire process. By reflecting human self-assessment, these stages guide the LLM, aiding in more accurate text interpretation
and facilitating better judgment formation. The diagram features three columns, from left to right, representing the high-level
metacognitive stages, specific metacognitive prompts fed into the LLM, and the LLM’s corresponding outputs. Prompts in
the middle column are collectively fed into the LLM as a single input during the experiments. The figure illustrates a sample
question chosen from the Quora Question Pair (QQP) dataset.
This approach involves five distinct stages: 1) the LLM be-
gins by deciphering the input text to comprehend its context
and meaning, mirroring the initial comprehension stage in
human thought; 2) it then forms a preliminary interpretation
of the text, a step that reflects judgment formation in hu-
mans; 3) subsequently, the LLM critically evaluates this ini-
tial judgment for accuracy, akin to the self-scrutiny humans
apply during problem-solving; 4) after this evaluation, the
LLM finalizes its decision and offers an explanation for its
reasoning, aligning with the decision-making and rational-
ization phase in human cognition; 5) ultimately, the LLM
assesses its confidence in the outcome of the entire process,
similar to how humans gauge the certainty of their decisions
and explanations. Figure 2 provides a schematic representa-
tion of our MP. It outlines the five sequential metacognitive
stages, the specific prompts directed at the LLM, and the
corresponding outputs from the model.
Table 1 contrasts our MP with prevalent prompting meth-
ods such as standard prompting (SP) and CoT prompting,
highlighting the differences in guidelines and objectives in-
herent to each method. While SP focuses on direct, task-
specific cues, MP uniformly applies the metacognitive pro-
cess, adapting to the unique demands of every task. Contrary
to the sequential progression characteristic of CoT, MP inte-
grates continuous critical evaluations throughout its stages,
enhancing both comprehension and response. For instance,
in a sentiment analysis task, SP might simply request, “Clas-
sify the sentiment of the statement as positive or negative.”
Meanwhile, CoT guides the model through a step-by-step
process, asking, “Identify key emotional words in the state-
ment. Based on these words, would you classify its overall
sentiment as positive or negative?” On the other hand, MP
pushes the model for deeper introspection, suggesting, “Un-
derstand the statement and make a preliminary sentiment
identification. If you are uncertain, reassess. Confirm your
final decision, providing reasoning. Then, evaluate and jus-
tify your confidence (0 - 100%) in this analysis.”
In essence, MP introduces a structured approach that en-
ables LLMs to process tasks, enhancing their contextual
awareness and introspection in responses. By systematically
guiding models through stages that emulate human cogni-
tive processes, this method offers a fresh perspective on ad-

Table 1: Comparison among standard prompting, chain-of-thought prompting, and metacognitive prompting.
Prompting Strategy
Guideline
Purpose
Standard
Provide a straightforward,
succinct prompt tailored
to the task at hand.
To elicit an immediate
and direct answer from
the model.
Chain-of-Thought
Develop a prompt that
leads the model through
progressive stages of reasoning.
To facilitate the model’s
step-by-step logical
engagement with tasks.
Metacognitive
Formulate a prompt that
simulate stages of
human metacognition.
To reproduce human-like
“thinking about thinking”,
deepening the model’s
task comprehension.
Table 2: Overview of general language understanding tasks, datasets, and evaluation metrics. QA stands for question answering,
NLI is natural language inference, WSD is word sense disambiguation, and coref. is coreference resolution.
Task
Dataset
Input
Output
Metric
Sentiment
SST-2
Single sentence
Binary
Accuracy
Similarity
STS-B
Sentence pair
Continuous
Pearson / Spearman Correlation
Paraphrase
QQP
Question pair
Binary
F1 / Accuracy
QA / NLI
QNLI
Question + passage
Binary
Accuracy
NLI
WNLI, RTE, CB
Sentence pair
Binary / Ternary
F1 / Accuracy
WSD
WiC
Sentence pair + target word
Binary
Accuracy
coref.
WSC
Passage + pronouns
Binary
Accuracy
QA
COPA
Question + choices
Binary
Accuracy
dressing complex natural language tasks. It reshapes our per-
ception and utilization of LLMs’ capabilities, ushering in
a paradigm where models not only grasp the intricacies of
given tasks but also critically evaluate and adjust their rea-
soning. This approach lays the groundwork for more effec-
tive and reliable interactions between users and LLMs.
Experiments
We evaluate the effect of MP, in comparison with standard
and chain-of-thoughts prompting, on five leading LLMs us-
ing multiple NLU datasets from GLUE (Wang et al. 2019b)
and SuperGLUE (Wang et al. 2019a) benchmarks. We report
the best result after multiple experimental iterations.
Datasets
We utilize a wide range of general language understand-
ing datasets for our experiments, selected from GLUE and
SuperGLUE benchmarks. These datasets encompass var-
ious tasks, including sentiment analysis (SST-2 (Socher
et al. 2013)), textual similarity (STS-B (Cer et al. 2017)),
question paraphrase (QQP (Shankar, Nikhil, and Kornel
2017)), question-answer entailment (QNLI (Rajpurkar et al.
2016)), textual entailment (WNLI (Levesque, Davis, and
Morgenstern 2012), RTE (Dagan, Glickman, and Magnini
2005), CB (De Marneffe, Simons, and Tonhauser 2019)),
word sense disambiguation (WiC (Pilehvar and Camacho-
Collados 2019)), coreference resolution (WSC (Levesque,
Davis, and Morgenstern 2012)), and question-answering
(COPA (Roemmele, Bejan, and Gordon 2011)). Among
these tasks, STS-B is a regression task, while the rest are
classification tasks. We choose these datasets as they chal-
lenge the general understanding capabilities of language
models. For evaluation purposes, we utilize the development
set corresponding to each task. Table 2 provides an overview
of the tasks and datasets.
Large Language Models
In our evaluation, we consider five popular large lan-
guage models (LLMs): the open-source models Llama-2-
13b-chat (Touvron et al. 2023) and Vicuna-13b-v1.1 (Chi-
ang et al. 2023) , and the closed-source models PaLM-bison-
chat (Anil et al. 2023), GPT-3.5-turbo, and GPT-4 (Ope-
nAI 2023). Each model is employed using its corresponding
API key. For all methods, we apply greedy decoding (i.e.,
temperature = 0) for response generation. Furthermore, we
employ zero-shot and 5-shot settings for each model, with
exemplars in the 5-shot setting randomly chosen from the
training set. Each dataset has its own set of exemplars, where
exemplar answers are obtained through human annotation.
Prompts
We employ three prompting strategies to enhance the perfor-
mance of LLMs on each task: standard prompting (Brown
et al. 2020; Kojima et al. 2022), chain-of-thought (CoT)
prompting (Wei et al. 2022), and our proposed metacogni-
tive prompting. Each prompt is evaluated under both zero-

shot and 5-shot settings. The full set of prompts is provided
in the supplementary materials.
Results
In our empirical evaluations, we compare performance
across all datasets and models, taking into consideration the
different prompting methods used. We also investigate the
efficacy of three prompting strategies, analyze errors associ-
ated with MP, and examine the relationship between confi-
dence scores and accuracy when MP is applied.
Overall Performance Comparison
Table 3 presents a performance comparison of five LLMs
(i.e., Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4) using
three prompting methods (i.e., SP, CoT, and MP) across
various general NLU datasets. The results for each model
and prompting strategy are averaged between zero-shot
and 5-shot settings. Under the 5-shot learning approach,
model performance generally shows improvement across all
tasks when compared to zero-shot learning (see experimen-
tal results in supplementary materials). GPT-4 consistently
achieves the highest scores across nearly all datasets. Al-
though GPT-4 stands out as the dominant model, PaLM
exhibits competitive performance, especially when paired
with MP. This combination closely rivals GPT-4 in specific
datasets, such as COPA and WSC. For traditionally less
competitive LLMs, like Vicuna and GPT-3.5, the adoption
of MP notably improves performance on certain datasets, es-
pecially CB and WSC, when compared to SP and CoT. Fur-
thermore, MP outperforms both SP and CoT across the ma-
jority of datasets. For instance, on the WSC dataset, models
employing MP experience an average accuracy increase of
9.7% and 4.8% compared to those using standard and CoT
prompting, respectively, across all models.
Prompting Strategy Comparison
We evaluate the performance of three prompting strategies
under zero-shot and 5-shot learning settings across all mod-
els and datasets. In the model-level comparison, Figure 3
Standard
CoT
MP
Prompts
75
80
85
90
95
Average Performance (%)
Llama2
Vicuna
PaLM
GPT-3.5
GPT-4
Figure 3: Comparison of average performance for three
prompting methods in both zero-shot and 5-shot learning
scenarios across five models. Performance metrics are aver-
aged over all datasets, treating each dataset and metric with
equal significance and assuming direct comparability. No-
tably, MP consistently surpasses standard and CoT prompt-
ing, with GPT-4 emerging as the top-performing model.
presents an aggregated view of the performance of each
prompting method across all datasets for each model, as-
suming that datasets and evaluation metrics are equally sig-
nificant and directly comparable. MP emerges superior, il-
lustrating a relative performance boost ranging from 4.4% to
6.5% over SP and 2.0% to 4.3% over CoT prompting. This
enhanced performance can be attributed to the unique intro-
spective strategy of MP, which facilitates a deeper under-
standing of tasks by prompting the model to critically eval-
uate, revisit its initial judgments, and refine its responses.
When we shift focus to a data-level comparison in Table 4,
it provides an average performance over five models for each
dataset. The critical reassessment capabilities of MP particu-
larly stand out in datasets like WNLI, WSC, and CB, leading
to marked improvements of 3.7%, 4.7%, and 4.8% over CoT,
respectively. The consistent outstanding performance of MP
underscores its potential in tasks demanding precision, dis-
cernment, and a comprehensive semantic grasp. Meanwhile,
the self-assessment and iterative refinement embedded in
MP give it an advantage in tasks requiring nuanced under-
standing and contextual depth.
Error Analysis
MP has consistently demonstrated proficiency across a range
of NLU tasks. However, upon manual inspection of its incor-
rect predictions, two primary error types associated specifi-
cally with MP were identified. First, the ‘Overthinking er-
rors’ are notably evident in straightforward datasets like
sentiment analysis (SST-2) and question paraphrase (QQP).
In these situations, MP tends to over-complicate the task,
diverging from the correct solution. Conversely, ‘overcor-
rection errors’ predominantly appears in tasks demanding
nuanced interpretation, such as word sense disambiguation
(WiC) and coreference resolution (WSC). As depicted in
Figure 4 with error examples from the WiC dataset, the crit-
ical reassessment stage of MP sometimes strays excessively
from an initially accurate interpretation. To mitigate these
issues, a potential refinement for MP could be the introduc-
tion of a ‘simplification checkpoint’ during the ‘critical as-
sessment’ phase, particularly for tasks where overthinking is
prevalent. For instance, after the model critically reassesses
a sentiment analysis task, the prompt could incorporate an
instruction like: “Re-evaluate the sentiment in its simplest
form. Does this align with your critical assessment?” This
would urge the model to counterbalance its intricate reason-
ing against a rudimentary interpretation. Additionally, for
tasks prone to overcorrection, a potential solution could be
embedding a ‘comparison checkpoint’ within the prompts.
For instance, after the model has critically reassessed its
judgment on a coreference resolution task, the prompt might
guide: “Recall your initial understanding of the reference.
Does this new evaluation maintain the essence of your ini-
tial judgment or does it drastically differ?” By making the
model directly compare its initial thought with the revised
one, we might prevent it from making too many changes and
help it keep a balanced view.
Confidence Analysis
Assessing confidence and uncertainty within the MP frame-
work is instrumental in gauging the reliability of predictions,

Table 3: Performance comparison of five LLMs across diverse natural language understanding datasets using three prompting
strategies. Scores are averaged between zero-shot and 5-shot settings. GPT-4 consistently outperforms other models across
most datasets. Metacognitive prompting notably surpasses both standard and CoT prompting for a majority of the datasets.
Best results for each dataset are highlighted in bold. The presented results are with greedy decoding (i.e., temperature = 0).
Abbreviations: Acc., Pear., Spea. stand for accuracy, Pearson correlation, and Spearman correlation, respectively.
Prompt
Model
Dataset
SST-2
STS-B
QQP
QNLI
WNLI
RTE
CB
WiC
WSC
COPA
Acc.
Pear. / Spea.
F1 / Acc.
Acc.
Acc.
Acc.
F1 / Acc.
Acc.
Acc.
Acc.
Standard
Prompting
Llama2
95.0
56.3 / 52.7
78.5 / 83.8
88.7
67.6
85.4
93.0 / 92.0
76.3
73.1
96.0
Vicuna
96.0
54.9 / 50.9
77.9 / 83.3
88.0
69.7
85.4
87.7 / 85.7
75.1
65.4
96.0
PaLM
96.6
67.6 / 66.2
79.7 / 84.6
89.6
78.9
90.7
93.0 / 92.0
76.8
70.7
98.5
GPT-3.5
96.2
65.9 / 64.2
78.5 / 83.8
89.3
75.4
86.0
90.3 / 88.4
76.3
68.3
99.0
GPT-4
97.3
70.2 / 68.8
84.0 / 88.3
94.5
89.4
94.1
91.9 / 91.1
81.0
85.1
100.0
CoT
Prompting
Llama2
95.9
56.4 / 52.7
79.9 / 84.9
89.6
73.2
86.7
94.4 / 93.8
77.2
75.5
98.5
Vicuna
96.6
56.7 / 54.1
79.4 / 84.4
88.8
71.1
85.8
90.5 / 89.3
76.2
68.8
96.5
PaLM
97.3
70.5 / 69.2
81.0 / 85.6
90.2
83.8
91.5
95.2 / 94.6
77.7
75.0
99.5
GPT-3.5
96.7
65.1 / 63.9
80.1 / 85.1
90.4
83.8
87.0
91.6 / 90.2
77.2
71.7
99.0
GPT-4
98.0
73.6 / 73.1
85.2 / 89.2
95.3
92.3
94.4
95.0 / 94.7
82.2
88.0
100.0
Metacognitive
Prompting
(Ours)
Llama2
98.0
64.3 / 61.9
81.8 / 86.4
91.0
76.8
88.3
97.5 / 97.3
79.2
79.3
99.5
Vicuna
97.2
63.6 / 60.6
80.2 / 85.1
89.6
72.5
86.8
93.6 / 92.3
77.9
72.6
99.0
PaLM
98.2
71.5 / 69.4
82.3 / 86.5
90.7
87.3
93.2
98.4 / 98.2
79.2
78.4
100.0
GPT-3.5
98.3
71.8 / 70.3
81.7 / 86.2
91.2
87.3
88.7
98.4 / 98.2
78.6
75.5
99.5
GPT-4
99.2
85.9 / 85.7
87.0 / 90.4
96.5
95.1
95.5
100.0 / 100.0
83.9
91.4
100.0
Table 4: Comparison of average performance for three
prompting methods across datasets. Performance metrics are
averaged over all models under zero-shot and 5-shot set-
tings. MP consistently achieves superior performance across
a range of NLU tasks.
Dataset
Standard
CoT
MP
SST-2 (Acc.)
96.2
96.9
98.2
STS-B (Pear. / Spea.)
63.0 / 60.6
64.5 / 62.6
71.4 / 69.6
QQP (F1 / Acc.)
79.7 / 84.8
81.1 / 85.8
82.6 / 86.9
QNLI (Acc.)
90.0
90.9
91.8
WNLI (Acc.)
76.2
80.8
83.8
RTE (Acc.)
88.3
89.1
90.5
CB (F1 / Acc.)
91.2 / 89.8
93.3 / 92.5
97.6 / 97.2
WiC (Acc.)
77.1
78.1
79.8
WSC (Acc.)
72.5
75.8
79.4
COPA (Acc.)
97.9
98.7
99.6
particularly when models articulate their confidence levels.
In our analysis, each model operating with MP was evalu-
ated based on its verbalized confidence for every prediction
across the datasets. Scores above 75% are classified as high
confidence; any value below this threshold is considered low
confidence. To illuminate this correlation, we employed a
tailored confusion matrix uniquely adapted for this study.
Within this matrix, the standard terminologies of ‘True Pos-
itive’, ‘False Positive’, ‘True Negative’, and ‘False Negative’
are redefined as follows:
True Positive (TP): Represents instances where the model,
using MP, expressed high confidence and produced a correct
answer. These account for 58.3%.
False Positives (FP): Denotes cases where the model exhib-
ited high confidence but gave an incorrect prediction. These
amount to 27.1%.
True Negatives (TN): Refers to instances where the model
signaled low confidence and its response was indeed incor-
rect. These stand at 8.7%.
False Negatives (FN): Highlights cases where the model in-
dicated low confidence but, surprisingly, delivered a correct
answer. These tally to 5.9%.
These metrics were aggregated across all models and
datasets and then averaged to provide a holistic overview
of the interplay between model confidence using MP and
prediction accuracy. As depicted in Figure 5, MP typically
offers an accurate reflection of its own performance, as evi-
denced by the high TP rate. The relatively low TN rate un-
derscores its reliable self-assessment, suggesting that when
MP has low confidence, it is predominantly correct about
its inaccuracy. However, the considerable FP rate indicates
that, while MP is usually right when confident, it sometimes
makes mistakes despite its high confidence. Moreover, the
FN rate identifies areas where MP might improve its self-
awareness, as there are moments when it might underesti-
mate its accuracy. In summary, the high TP rate and low FN
values underscore MP’s self-awareness, but the FP and TN
values point to potential improvements. Addressing these ar-
eas by emphasizing confidence calibration in future itera-
tions of MP could better align its introspective evaluations
with its actual performance capabilities.
Limitations
While our proposed metacognitive prompting demonstrates
potential by integrating introspective features reminiscent of

Error Type 1: Overthinking
Question:
In two sentences, sentence 1: “Excite the neurons.” 
sentence 2: “The fireworks which opened the festivities 
excited anyone present.”, both contain the target word 
“excite”. Determine if the target word is used with the 
same meaning in both sentences. [MP Instructions]
MP Initial Response (Stage 2):
The target word has the same meaning: False.
MP Final Response (Stage 4 & 5):
The target word has the same meaning: True. Upon re-
evaluation, model hypothesized that both contexts hint 
at triggering a reaction, either biological or emotional, 
leading to the conclusion of similarity. Confidence in 
this revised analysis is 90%.
(a) Overthinking error in model response with MP.
Error Type 2: Overcorrection
Question:
In two sentences, sentence 1: “The very easiness of the 
deed held her back.” sentence 2: “There was an easiness 
between them.”, both contain the target word 
“easiness”. Determine if the target word is used with 
the same meaning in both sentences. [MP Instructions]
MP Initial Response (Stage 2):
The target word has the same meaning: False.
MP Final Response (Stage 4 & 5):
The target word has the same meaning: True. Upon re-
evaluation, ‘easiness’ in both sentences pertains to 
emotional states—implying simplicity-induced 
hesitation in the first and emotional harmony in the 
second. Confidence in this revised analysis is 85%.
(b) Overcorrection error in model response with MP.
Figure 4: Two major error types with MP: overthinking (ex-
cessive analysis) and overcorrection (excessive adjustment).
human cognition into LLMs to enhance their understanding
capacities, our study does have its limitations. We evaluated
the effectiveness of MP using a specific selection of datasets
and models, which may limit the broader applicability of our
findings. Although the verbalized confidence of LLMs of-
fers a window into their perceived certainty levels, it might
not serve as the definitive method for comprehensively gaug-
ing their true confidence. A hybrid approach, such as com-
bining verbalization with self-consistency checks, could of-
fer a more robust method for confidence calibration. Addi-
tionally, our study does not extensively address vital ethical
and legal concerns, such as potential biases, privacy impli-
cations, and fairness challenges. It is imperative that future
research on MP addresses these dimensions to ensure the re-
sponsible and holistic application of LLMs across different
scenarios.
High Confidence
Low Confidence
Correct
Incorrect
58.3%
5.9%
27.1%
8.7%
Figure 5: Relationship between correctness and confidence
levels under MP, averaged over all datasets and models.
Discussion
In this study, we presented Metacognitive Prompting (MP)
to infuse introspective features that mirror human cognition
into LLMs. The MP process involves five distinct stages:
it starts by comprehending the input text, then moves to
formulate an initial judgment. Next, it critically reevaluates
this initial impression, settles on a decision while explain-
ing its rationale, and finally gauges its confidence in the de-
cisions made. Our empirical evaluations spanned a broad
range of NLU datasets from the GLUE and SuperGLUE
benchmarks and several prominent LLMs. The results un-
derscore the potential of our method, demonstrating clear
advantages over existing prompting methods such as stan-
dard and CoT prompting. Through our analysis, specific er-
ror patterns associated with MP were identified, highlighting
nuances in comprehension and judgment stages that warrant
further refinement. While MP provides a structured pathway
for models to introspect, it follows predefined stages, lacking
adaptability based on real-time feedback. The five-stage de-
sign of MP, although foundational, suggests room for more
intricate frameworks that might emulate human-like cogni-
tive feedback loops more authentically.
Looking forward, several areas warrant further explo-
ration. Applying MP to broader datasets, especially those
that are multilingual or domain-specific, is a promising di-
rection. Refining prompting strategies might elicit more
detailed introspective responses from LLMs. Moreover,
our findings suggest that the reliance on verbalized confi-
dence can be augmented by integrating other methods for
a more comprehensive confidence assessment. Additionally,
the broader implications of introducing introspective LLMs,
particularly regarding biases and the reliability of outputs,
require in-depth examination. In essence, our initial venture
with MP lays a solid foundation, but significant opportuni-
ties remain to draw closer parallels between introspection in
LLMs and natural human introspection.

References
Ait-Mlouk, A.; and Jiang, L. 2020.
KBot: a Knowledge
graph based chatBot for natural language understanding
over linked data. IEEE Access, 8: 149220–149230.
Allen, J. 1995. Natural language understanding. Benjamin-
Cummings Publishing Co., Inc.
Anil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin,
D.; Passos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen,
Z.; et al. 2023.
Palm 2 technical report.
arXiv preprint
arXiv:2305.10403.
Bellegarda, J. R. 2013. Spoken language understanding for
natural interaction: The siri experience. Natural Interaction
with Robots, Knowbots and Smartphones: Putting Spoken
Dialog Systems into Practice, 3–14.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Ad-
vances in neural information processing systems, 33: 1877–
1901.
Cambria, E.; and White, B. 2014. Jumping NLP curves: A
review of natural language processing research. IEEE Com-
putational intelligence magazine, 9(2): 48–57.
Cer, D.; Diab, M.; Agirre, E.; Lopez-Gazpio, I.; and Specia,
L. 2017. SemEval-2017 Task 1: Semantic Textual Similarity
Multilingual and Crosslingual Focused Evaluation. In Pro-
ceedings of the 11th International Workshop on Semantic
Evaluation (SemEval-2017), 1–14.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.;
Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; et al.
2023.
Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality. See https://vicuna. lmsys. org
(accessed 14 April 2023).
Dagan, I.; Glickman, O.; and Magnini, B. 2005. The pascal
recognising textual entailment challenge. In Machine learn-
ing challenges workshop, 177–190. Springer.
De Marneffe, M.-C.; Simons, M.; and Tonhauser, J. 2019.
The commitmentbank: Investigating projection in naturally
occurring discourse. In proceedings of Sinn und Bedeutung,
volume 23, 107–124.
Hausser, R.; and Hausser, R. 2001. Foundations of compu-
tational linguistics. Springer.
Huang, J.; and Chang, K. C.-C. 2022.
Towards reason-
ing in large language models: A survey.
arXiv preprint
arXiv:2212.10403.
Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa,
Y. 2022.
Large language models are zero-shot reason-
ers. Advances in neural information processing systems, 35:
22199–22213.
Levesque, H.; Davis, E.; and Morgenstern, L. 2012. The
winograd schema challenge.
In Thirteenth international
conference on the principles of knowledge representation
and reasoning.
Liu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,
G. 2023. Pre-train, prompt, and predict: A systematic survey
of prompting methods in natural language processing. ACM
Computing Surveys, 55(9): 1–35.
Min, B.; Ross, H.; Sulem, E.; Veyseh, A. P. B.; Nguyen,
T. H.; Sainz, O.; Agirre, E.; Heintz, I.; and Roth, D. 2021.
Recent advances in natural language processing via large
pre-trained language models: A survey. ACM Computing
Surveys.
Namazifar, M.; Papangelis, A.; Tur, G.; and Hakkani-T¨ur,
D. 2021. Language model is all you need: Natural language
understanding as question answering. In ICASSP 2021-2021
IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), 7803–7807. IEEE.
Nie, Y.; Zhou, X.; and Bansal, M. 2020. What can we learn
from collective human opinions on natural language infer-
ence data? arXiv preprint arXiv:2010.03532.
OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
Peri˜n´an Pascual, J. C.; and Arcas T´unez, F. 2007. Cogni-
tive modules of an NLP knowledge base for language under-
standing. Procesamiento del Lenguaje Natural, (39): 197–
204.
Pilehvar, M. T.; and Camacho-Collados, J. 2019. WiC: the
Word-in-Context Dataset for Evaluating Context-Sensitive
Meaning Representations. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers), 1267–1273.
Rae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoff-
mann, J.; Song, F.; Aslanides, J.; Henderson, S.; Ring, R.;
Young, S.; et al. 2021. Scaling language models: Methods,
analysis & insights from training gopher.
arXiv preprint
arXiv:2112.11446.
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
Squad: 100,000+ questions for machine comprehension of
text. arXiv preprint arXiv:1606.05250.
Roemmele, M.; Bejan, C. A.; and Gordon, A. S. 2011.
Choice of plausible alternatives: An evaluation of common-
sense causal reasoning. In 2011 AAAI Spring Symposium
Series.
Schwarz, N. 2015. Metacognition.
Shankar, I.; Nikhil, D.; and Kornel, C. 2017.
First
quora dataset release: question pairs (2017).
URL
https://www. quora. com/q/quoradata/First-Quora-Dataset-
Release-Question-Pairs.
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,
C. D.; Ng, A. Y.; and Potts, C. 2013. Recursive deep models
for semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 conference on empirical methods in
natural language processing, 1631–1642.
Surameery, N. M. S.; and Shakor, M. Y. 2023.
Use chat
gpt to solve programming bugs. International Journal of
Information Technology & Computer Engineering (IJITC)
ISSN: 2455-5290, 3(01): 17–22.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; et al. 2023. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288.

Wang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.;
Michael, J.; Hill, F.; Levy, O.; and Bowman, S. 2019a. Su-
perglue: A stickier benchmark for general-purpose language
understanding systems. Advances in neural information pro-
cessing systems, 32.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. R. 2019b. Glue: A multi-task benchmark and
analysis platform for natural language understanding.
In
7th International Conference on Learning Representations,
ICLR 2019.
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q. V.; Chi, E. H.;
Narang, S.; Chowdhery, A.; and Zhou, D. 2022a.
Self-
Consistency Improves Chain of Thought Reasoning in Lan-
guage Models. In The Eleventh International Conference on
Learning Representations.
Wang, Y.; Zhao, Y.; Callcut, R.; and Petzold, L. 2022b. In-
tegrating Physiological Time Series and Clinical Notes with
Transformer for Early Prediction of Sepsis. arXiv preprint
arXiv:2203.14469.
Wang, Y.; Zhao, Y.; and Petzold, L. 2023a.
Are Large
Language Models Ready for Healthcare? A Comparative
Study on Clinical Language Understanding. arXiv preprint
arXiv:2304.05368.
Wang, Y.; Zhao, Y.; and Petzold, L. 2023b.
An empiri-
cal study on the robustness of the segment anything model
(sam). arXiv preprint arXiv:2305.06422.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;
Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022.
Chain-of-
thought prompting elicits reasoning in large language mod-
els. Advances in Neural Information Processing Systems,
35: 24824–24837.
Weidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato, J.;
Huang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh,
A.; et al. 2021. Ethical and social risks of harm from lan-
guage models. arXiv preprint arXiv:2112.04359.
Zhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022. Auto-
matic chain of thought prompting in large language models.
arXiv preprint arXiv:2210.03493.
Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.;
Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; et al.
2023. A survey of large language models. arXiv preprint
arXiv:2303.18223.
Zhao, Y.; Wang, Y.; Liu, J.; Xia, H.; Xu, Z.; Hong, Q.; Zhou,
Z.; and Petzold, L. 2021. Empirical quantitative analysis of
covid-19 forecasting models. In 2021 International Confer-
ence on Data Mining Workshops (ICDMW), 517–526. IEEE.
Zheng, C.; Liu, Z.; Xie, E.; Li, Z.; and Li, Y. 2023.
Progressive-hint prompting improves reasoning in large lan-
guage models. arXiv preprint arXiv:2304.09797.
Zhou, D.; Sch¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,
X.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V.; et al.
2022. Least-to-Most Prompting Enables Complex Reason-
ing in Large Language Models. In The Eleventh Interna-
tional Conference on Learning Representations.
Zhuo, T. Y.; Huang, Y.; Chen, C.; and Xing, Z. 2023. Ex-
ploring ai ethics of chatgpt: A diagnostic analysis. arXiv
preprint arXiv:2301.12867.

