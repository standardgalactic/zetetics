Bayesian Nonparametric Modeling and Theory for
Complex Data
by
Debdeep Pati
Department of Statistical Science
Duke University
Date:
Approved:
David B. Dunson, Advisor
Alan E. Gelfand
Surya T. Tokdar
Lawrence Carin
Dissertation submitted in partial fulﬁllment of the requirements for the degree of
Doctor of Philosophy in the Department of Statistical Science
in the Graduate School of Duke University
2012

Abstract
Bayesian Nonparametric Modeling and Theory for Complex
Data
by
Debdeep Pati
Department of Statistical Science
Duke University
Date:
Approved:
David B. Dunson, Advisor
Alan E. Gelfand
Surya T. Tokdar
Lawrence Carin
An abstract of a dissertation submitted in partial fulﬁllment of the requirements for
the degree of Doctor of Philosophy in the Department of Statistical Science
in the Graduate School of Duke University
2012

Copyright c⃝2012 by Debdeep Pati
All rights reserved except the rights granted by the
Creative Commons Attribution-Noncommercial Licence

Abstract
The dissertation focuses on solving some important theoretical and methodological
problems associated with Bayesian modeling of inﬁnite dimensional ‘objects’, popu-
larly called nonparametric Bayes. The term ‘inﬁnite dimensional object’ can refer to
a density, a conditional density, a regression surface or even a manifold. Although
Bayesian density estimation as well as function estimation are well-justiﬁed in the
existing literature, there has been little or no theory justifying the estimation of more
complex objects (e.g. conditional density, manifold, etc.). Part of this dissertation
focuses on exploring the structure of the spaces on which the priors for conditional
densities and manifolds are supported while studying how the posterior concentrates
as increasing amounts of data are collected.
With the advent of new acquisition devices, there has been a need to model
complex objects associated with complex data-types e.g. millions of genes aﬀect-
ing a bio-marker, 2D pixelated images, a cloud of points in the 3D space, etc.
A signiﬁcant portion of this dissertation has been devoted to developing adaptive
nonparametric Bayes approaches for learning low-dimensional structures underlying
higher-dimensional objects e.g. a high-dimensional regression function supported on
a lower dimensional space, closed curves representing the boundaries of shapes in 2D
images and closed surfaces located on or near the point cloud data. Characterizing
the distribution of these objects has a tremendous impact in several application areas
ranging from tumor tracking for targeted radiation therapy, to classifying cells in the
iv

brain, to model based methods for 3D animation and so on.
The ﬁrst three chapters are devoted to Bayesian nonparametric theory and mod-
eling in unconstrained Euclidean spaces e.g. mean regression and density regression,
the next two focus on Bayesian modeling of manifolds e.g. closed curves and surfaces,
and the ﬁnal one on nonparametric Bayes spatial point pattern data modeling when
the sampling locations are informative of the outcomes.
v

To my family
vi

Contents
Abstract
iv
List of Tables
xiii
List of Figures
xiv
List of Abbreviations and Symbols
xv
Acknowledgements
xvii
1
Introduction
1
1.1
Review of posterior consistency and convergence rate . . . . . . . . .
2
1.2
Review of Gaussian processes
. . . . . . . . . . . . . . . . . . . . . .
6
1.3
Research Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.4
Our contribution
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2
Adaptive dimension reduction with a Gaussian process prior
17
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2
Speciﬁc notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.3
Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
2.3.1
Adaptive estimation of anisotropic functions . . . . . . . . . .
29
2.3.2
Adaptive dimension reduction . . . . . . . . . . . . . . . . . .
30
2.3.3
Connections between cases (i) and (ii)
. . . . . . . . . . . . .
32
2.3.4
Rates of convergence in speciﬁc settings
. . . . . . . . . . . .
33
2.4
Properties of the multi-bandwidth Gaussian process . . . . . . . . . .
35
vii

2.5
Proof of main results . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
2.5.1
Proof of Theorem 10 . . . . . . . . . . . . . . . . . . . . . . .
46
2.5.2
Proof of Theorem 12 . . . . . . . . . . . . . . . . . . . . . . .
50
2.6
Lower bounds on posterior contraction rates . . . . . . . . . . . . . .
51
2.7
Main result
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3
Bayesian nonparametric regression with varying residual density
64
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
3.2
Nonparametric regression modeling . . . . . . . . . . . . . . . . . . .
69
3.2.1
Data Structure and Model . . . . . . . . . . . . . . . . . . . .
69
3.2.2
Prior on the Mean Regression Function . . . . . . . . . . . . .
70
3.2.3
Priors for Residual Distribution . . . . . . . . . . . . . . . . .
71
3.3
Consistency properties . . . . . . . . . . . . . . . . . . . . . . . . . .
76
3.4
Posterior Computation . . . . . . . . . . . . . . . . . . . . . . . . . .
80
3.4.1
Gaussian process regression with t residuals
. . . . . . . . . .
81
3.4.2
Heteroscedastic PSB mixture of normals . . . . . . . . . . . .
82
3.4.3
Heteroscedastic sPSB process location-scale mixture . . . . . .
84
3.5
Measures of Inﬂuence . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
3.6
Simulation studies
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
3.7
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
3.7.1
Boston housing data Application
. . . . . . . . . . . . . . . .
93
3.7.2
Body fat data application
. . . . . . . . . . . . . . . . . . . .
93
3.8
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
4
Posterior consistency in conditional density estimation
96
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
4.2
Conditional density estimation . . . . . . . . . . . . . . . . . . . . . . 100
viii

4.2.1
Predictor dependent mixtures of Gaussian linear regressions
. 101
4.2.2
Gaussian mixtures of ﬁxed-π dependent processes . . . . . . . 102
4.3
Notions of neighborhoods in conditional density estimation . . . . . . 102
4.4
Posterior consistency in MGLRx mixture of Gaussians
. . . . . . . . 104
4.4.1
Kullback-Leibler property
. . . . . . . . . . . . . . . . . . . . 104
4.4.2
Strong Consistency with the q-integrated L1 neighborhood . . 108
4.5
Posterior consistency in mixtures of ﬁxed-π dependent processes . . . 114
4.5.1
Kullback-Leibler property
. . . . . . . . . . . . . . . . . . . . 114
4.5.2
Strong consistency with the q-integrated L1 neighborhood
. . 114
4.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
5
Bayesian shape modeling with closed curves
118
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
5.2
Shape-generating random process . . . . . . . . . . . . . . . . . . . . 121
5.2.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
5.2.2
Roth curve
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
5.2.3
Deforming a Roth curve . . . . . . . . . . . . . . . . . . . . . 122
5.2.4
Vector notation . . . . . . . . . . . . . . . . . . . . . . . . . . 125
5.2.5
Shape-generating Random Process
. . . . . . . . . . . . . . . 126
5.3
Properties of the Prior . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.3.1
Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
5.3.2
Inﬂuence of the control points . . . . . . . . . . . . . . . . . . 132
5.4
Inference from Point Cloud Data
. . . . . . . . . . . . . . . . . . . . 132
5.5
Inference from Pixelated Image Data . . . . . . . . . . . . . . . . . . 135
5.5.1
Modeling surface orientation . . . . . . . . . . . . . . . . . . . 136
5.6
Fitting a collection of curves . . . . . . . . . . . . . . . . . . . . . . . 136
ix

5.7
Posterior computation
. . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.7.1
Conditional posteriors for mk and dprq,k . . . . . . . . . . . . . 139
5.7.2
Derivation of the approximate deformation-orienting matrix
. 141
5.7.3
Conditional posteriors for µr and Σr
. . . . . . . . . . . . . . 142
5.7.4
Gibbs updates for the parameterizations and orientation
. . . 143
5.7.5
Likelihood contribution from surface-normals . . . . . . . . . . 143
5.8
Simulation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
5.9
Brain tumor segmentation study . . . . . . . . . . . . . . . . . . . . . 147
5.10 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
6
Bayesian modeling of closed surfaces through tensor products
151
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.2
Outline of the method
. . . . . . . . . . . . . . . . . . . . . . . . . . 155
6.2.1
Review of Terminology . . . . . . . . . . . . . . . . . . . . . . 155
6.2.2
Choice of the parameterization
. . . . . . . . . . . . . . . . . 156
6.2.3
Closed surface model . . . . . . . . . . . . . . . . . . . . . . . 157
6.2.4
Construction of the cyclic basis . . . . . . . . . . . . . . . . . 158
6.2.5
Model for the control points . . . . . . . . . . . . . . . . . . . 159
6.2.6
Prior realizations . . . . . . . . . . . . . . . . . . . . . . . . . 160
6.3
Support of the prior and posterior convergence rates . . . . . . . . . . 161
6.3.1
Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
6.3.2
Rate of convergence of the posterior . . . . . . . . . . . . . . . 163
6.4
Posterior computation
. . . . . . . . . . . . . . . . . . . . . . . . . . 164
6.4.1
Gibbs sampler for a ﬁxed truncation level
. . . . . . . . . . . 164
6.4.2
Posterior sampling of n and m . . . . . . . . . . . . . . . . . . 165
6.5
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
x

6.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
7
Bayesian geostatistical modeling with informative sampling
174
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
7.2
Model for spatial data with informative sampling
. . . . . . . . . . . 175
7.3
Theoretical properties
. . . . . . . . . . . . . . . . . . . . . . . . . . 177
7.3.1
Weak posterior consistency . . . . . . . . . . . . . . . . . . . . 177
7.3.2
Posterior propriety of a . . . . . . . . . . . . . . . . . . . . . . 178
7.4
Computational details
. . . . . . . . . . . . . . . . . . . . . . . . . . 178
7.5
Simulation study
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
7.6
Analysis of Eastern United States ozone data
. . . . . . . . . . . . . 183
7.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
8
Future works
188
8.1
Latent variable density regression models . . . . . . . . . . . . . . . . 188
8.2
Nonparametric variable selection
. . . . . . . . . . . . . . . . . . . . 190
8.3
Bayesian shape modeling . . . . . . . . . . . . . . . . . . . . . . . . . 191
8.4
Spatial point patterns
. . . . . . . . . . . . . . . . . . . . . . . . . . 191
8.5
Robust Bayesian model based clustering
. . . . . . . . . . . . . . . . 192
8.6
Other directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
A Proofs of some results in Chapter 4
195
A.1 Proof of Lemma 46 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
A.2 A useful lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
A.3 Proof of Theorem 47 . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
A.4 Proof of Theorem 44 . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
A.5 Proof of Theorem 51 . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
A.6 Another useful lemma
. . . . . . . . . . . . . . . . . . . . . . . . . . 206
xi

A.7 Proof of Theorem 57 . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
A.8 Proof of Theorem 58 . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
B Proofs of some results in Chapter 3
214
B.1
Proof of Lemma 34 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
B.2
Proof of Lemma 36 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
B.3
Proof of Theorem thm:ghoshal . . . . . . . . . . . . . . . . . . . . . . 216
C Proofs of some results in Chapter 5
223
C.1 Proof of Lemma 63: . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
C.2 Proof of Theorem 64: . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
D Proofs of some results in Chapter 6
225
D.1 Proofs of Lemma 71
. . . . . . . . . . . . . . . . . . . . . . . . . . . 225
D.2 Proof of Theorem 72 . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
D.3 Proof of Theorem 74 . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
E Proofs of some results in Chapter 7
233
E.1
Proof of Theorem 77 . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
E.2
Proof of Theorem 78 . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
Bibliography
240
Biography
255
xii

List of Tables
3.1
Simulation results for cases (i) and (ii)
. . . . . . . . . . . . . . . . .
89
3.2
Simulation results for cases (iii) and (iv)
. . . . . . . . . . . . . . . .
90
3.3
Simulation results for case (v) . . . . . . . . . . . . . . . . . . . . . .
91
3.4
Boston housing data and body fat data results . . . . . . . . . . . . .
91
6.1
Hausdorﬀdistance between true and ﬁtted surface . . . . . . . . . . . 169
6.2
Posterior summaries of σ and n, m
. . . . . . . . . . . . . . . . . . . 169
7.1
Simulation study results . . . . . . . . . . . . . . . . . . . . . . . . . 182
7.2
Mean and 95% intervals for the ozone data . . . . . . . . . . . . . . . 186
xiii

List of Figures
2.1
Union of rectangles Crθ “ t0 ď a ď rθu . . . . . . . . . . . . . . . . .
43
5.1
Deformation of a Roth curve . . . . . . . . . . . . . . . . . . . . . . . 123
5.2
An illustration of the shape generation process . . . . . . . . . . . . . 127
5.3
Random samples from the shape-generating process . . . . . . . . . . 129
5.4
Inﬂuence of the control points . . . . . . . . . . . . . . . . . . . . . . 132
5.5
Borrowing of information . . . . . . . . . . . . . . . . . . . . . . . . . 146
5.6
Brain tumor application
. . . . . . . . . . . . . . . . . . . . . . . . . 148
6.1
Surface ﬁtted to a pelvic girdle point cloud . . . . . . . . . . . . . . . 151
6.2
Output triangulation from crust algorithm on a point cloud . . . . . . 154
6.3
Parameterization of the human skull and the Beethoven . . . . . . . . 157
6.4
Control points for diﬀerent closed surfaces
. . . . . . . . . . . . . . . 160
6.5
Prior realizations with increasing n and m . . . . . . . . . . . . . . . 161
6.6
Crust on a sparse non-noisy point cloud
. . . . . . . . . . . . . . . . 170
6.7
Tensor-product surface on a sparse non-noisy point cloud . . . . . . . 171
6.8
Crust on a sparse noisy point cloud . . . . . . . . . . . . . . . . . . . 172
6.9
Tensor-product surface on a sparse noisy point cloud
. . . . . . . . . 173
7.1
Plots of the ozone data . . . . . . . . . . . . . . . . . . . . . . . . . . 184
7.2
Posterior mean predicted values of ozone . . . . . . . . . . . . . . . . 185
xiv

List of Abbreviations and Symbols
Symbols
These general symbols apply to all the chapters in this dissertation.
R{ℜ
set of real numbers.
N
set of natural numbers.
Z
set of integers.
λ
Lebesgue measure on ℜor ℜp.
IB
indicator of a set B.
Lppνq
the space of measurable functions with ν-integrable pth absolute
power.
CpX q
the set of continuous functions on X .
CαpX q
the H¨older space of order α, consisting of the functions f P CpX q
that have tαu continuous derivatives with the tαuth derivative
f tαu being Lipshitz continuous of order α ´ tαu.
|| ¨ ||8, || ¨ ||1, || ¨ ||p,ν
supremum, L1 and norm of Lppνq.
Bpx0, r; dq
A ball of radius r with centre x0 relative to the metric d.
suptdpx, yq : x, y P Mu
The diameter of a bounded metric space M relative to a metric
d.
Npǫ, T, dq
covering number of a semi-metric space T relative to the semi-
metric d is the minimal number of balls of radius ǫ needed to
cover T.
log Npǫ, T, dq
ǫ-entropy of the space T with respect to d.
ű
complex line integral.
xv

À
inequality up to a constant multiple
tap1q, ap2q, . . . , apnqu
order statistics of the set tai : ai P R, i “ 1, . . . , nu.
δ0
distribution degenerate at 0
supppνq
support of a measure ν.
Abbreviations
w.r.t
with respect to
RKHS
Reproductive Kernel Hilbert Space
xvi

Acknowledgements
First, I would like to thank my advisor Dr.
David B. Dunson for his profound
ingenuity and indomitable enthusiasm in nurturing within me the roots of Bayesian
nonparametrics. Being trained mostly in Mathematical Statistics, I came to Duke
with almost no knowledge in stochastic modeling. It was him who inculcated within
me the necessity to learn stochastic modeling to be an all-rounded statistician. By
his tremendous work ethics and upfront attitude he has set an example in front of
me to be a dynamic and honest academician.
I also thank my committee members Alan Gelfand, Surya Tokdar and Lawrence
Carin. I admire Alan’s remarkable ability to come up with thought provoking ques-
tions - they helped me understand my work better. I would particularly like to thank
Surya-da for his mentorship in both academics and beyond. Larry is a source of ex-
traordinary energy, his group meetings provided a platform to vent out any random
ideas we have. It was a privilege to work with Brian Reich at NC State - I enjoyed
numerous technical conversations with him. Kelvin Gu is a wonderful friend and
brilliant colleague, I thank him for our enjoyable discussions ranging from the light
hearted ones to the most grueling sessions.
My sincere thanks to Alan Gelfand, Jim Berger, Merlise Clyde, Scott Schmidler,
Robert Wolpert, and Fan Li - who taught me various courses at Duke and provided
helpful feedbacks regarding the work here.
Natesh Pillai is a brilliant mentor, I
thank him for organizing a wonderful visit to Harvard - it was a productive as well
xvii

as fun-ﬁlled experience.
I would like to take this opportunity to thank the friendly and helpful staﬀ-
Karen, Lance, Anne, Nikki and Tameka - I admire your painstaking eﬀort to help
me out with every possible solutions.
I am obliged to the National Science Foundation, National Institutes of Health,
the International Biometric Society, the International Society for Bayesian Analysis,
and the American Statistical Association for ﬁnancing my graduate education and
conference travel.
It has been my privilege to meet some of the brightest and jubilant young friends
while living in Durham. To all of my friends at Duke, it wouldn’t be such an invig-
orating experience without you.
I am especially fortunate to have Anirban as my friend, who has been a tremen-
dous source of encouragement all along this journey right from the undergraduate
days - be it during darkest and the happiest of times to long drawn technical conver-
sations or numerous fun sessions. It was hard to imagine this journey without him,
I thank him for his guidance and support in every sphere of life.
To Avishek, Chiranjit and Anjishnu, life in the United States wouldn’t be so easy
without you - you will continue to be my constant source of inspiration in the years
to come.
Finally, to my parents and my wonderful sister, needless to put an acknowledge-
ment in words, but thank you for all your sacriﬁces, your dedication and your love.
Without you, it would not even be possible to see this day, to you I dedicate this
thesis.
xviii

1
Introduction
During the last decade there has been an immense development in Bayesian nonpara-
metric modeling and theory related to density and function estimation. Bayesian
modeling oﬀers a full probabilistic framework for inference while a nonparametric
approach adds the necessary ﬂexibility. With recent technological developments and
advent of huge amount of data acquisition devices, there is a clear need for mod-
eling and theory beyond traditional density estimation and unconstrained function
estimation. Estimation of a conditional density is one such area where there has
been a signiﬁcant amount of new methodologies developed during the last decade
without being substantiated by suitable theory. Also existing methods for Bayesian
conditional density estimation are black-box, and lack computational simplicity and
interpretability. This motivates developing new classes of ﬂexible, yet computation-
ally tractable and interpretable prior distributions.
The development of next-generation imaging devices enable the scientists to pen-
etrate the deepest parts of anatomical organs producing huge amounts of biologically
rich images and point cloud data. This requires fundamentally new approaches to
model the complicated shapes and manifolds. While there has been signiﬁcant devel-
1

opments for manifold estimation in computer science, most of these methods follow a
sequence of multistage procedures involving noise reduction, outlier removal and sub-
sequent manifold estimation. Multistage procedures fail to capture the uncertainty
in estimation which is crucial to very sensitive analyses like brain tumor detection.
Hence there is a necessity for a fully Bayesian model based approach to manifold
estimation which has received little or no attention over the past decade. A coherent
Bayesian framework also has a huge possibility of being embedded into a population
level analysis with immediate applications to tumor tracking for targeting radiation
therapy, modeling a 3-dimensional forest of morphologically diverse neurons changing
over time in response to chemical and electrical signals and so on.
One of the key interests of this dissertation is understanding and evaluating
inﬁnite-dimensional Bayesian procedures from a frequentist perspective.
The ob-
jective is to ensure that as we get more and more samples from the true ‘parameter’,
the posterior distribution concentrates in an arbitrarily small neighborhood of the
true ‘parameter’, a phenomenon popularly known as posterior consistency. In the
following we provide rigorous deﬁnitions of posterior consistency and posterior con-
vergence rate.
1.1
Review of posterior consistency and convergence rate
Assume Y1, . . . , Yn, . . . is a sequence of independent and identically distributed ran-
dom variables taking values in a complete separable metric space X endowed with
the Borel σ-algebra of subsets BpX q with a common density f0 P F, where F is a
space of densities f : X Ñ R`. The complete separable space F is also endowed
with the Borel σ-algebra of subsets BpFq. It is convenient to think of Y1, Y2, . . . as
the coordinate random variables deﬁned on Ωthe space pX 8, BpX q8q and f 8 as
the i.i.d. product density deﬁned on Ω. We will denote by Ωn “ pX n, BpX qnq and
by f n the n-fold product of f. We will also abbreviate pY1, Y2, ..., Ynq by Y n when
2

convenient.
A Bayesian nonparametric approach to infer f0 involves placing a prior distribu-
tion or a probability measure Πn on pF, BpFqq and computing the posterior distri-
bution of a Borel set B P BpFq according to the Bayes’ rule as
ΠnpB | Y nq “
ş
B
śn
i“1 fpYiqdΠnpfq
ş
F
śn
i“1 fpYiqdΠnpfq.
(1.1)
We must ensure that the expressions in (1.1) are well deﬁned.
In particular, we
assume that the map py, fq ÞÑ fpyq is measurable for the product σ-ﬁeld on X ˆ F.
To deﬁne various notions of posterior consistency, we need diﬀerent notions of
neighborhood which we shall formulate below.
Deﬁnition 1. Wǫpf0q is said to be a sub-basic weak neighborhood of f0 if
Wǫpf0q “
"
f P F :
ˇˇˇˇ
ż
φf ´
ż
φf0
ˇˇˇˇ ă ǫ
*
(1.2)
for a bounded continuous function φ : X Ñ R.
Deﬁnition 2. Sǫpf0q is said to be a strong or L1 neighborhood of f0 if
Sǫpf0q “ tf P F : }f ´ f0}1 ă ǫu.
(1.3)
Deﬁnition 3. KLǫpf0q is said to be a Kullback-Leibler neighborhood of f0 if
KLǫpf0q “
"
f P F :
ż
f0 log f0{f ă ǫ
*
.
(1.4)
Deﬁnition 4. A posterior is said to be weakly or strongly consistent at f0 if for any
ǫ ą 0
ΠnpU | Y nq Ñ 1
rf0s a.s.
(1.5)
where U “ Wǫpf0q or Sǫpf0q respectively.
3

Clearly strong posterior consistency implies weak posterior consistency whereas
the reverse implication is not necessarily true. A special case when this is true is
when X is a countable set.
(1.5) is only an asymptotic evaluation of the performance of the posterior and one
might question the usefulness of such asymptotic justiﬁcation in actual ﬁnite sample
examples. There are several reasons which make the study of posterior consistency
more interesting in nonparametric Bayesian models. First, posterior consistency is
violated more often in the inﬁnite-dimensional models than their parametric coun-
terparts (Diaconis and Freedman, 1986) due to the inability of the data to explain
inﬁnitely many parameters. Second, it turns out that posterior consistency is inti-
mately related to the prior ﬂexibility as well as model identiﬁability which are itself
quite interesting to study. Schwartz (1965a) demonstrated that the two key phe-
nomena governing posterior consistency are i) whether the prior has large support
on the target space, i.e., realizations from the prior can approximate a variety of
objects and ii) the ability of the model space to distinguish two parameters with
respect to the topology concerned. The second aspect is often manifested in the
form of the complexity of the model space. While having a large prior support en-
hances the ability to approximate a wider range of truth, it also decreases the chance
to concentrate near a particular true density given enough samples. Hence there
is always a trade-oﬀbetween (i) and (ii) and a ﬁne balance is required to achieve
consistency. Ghosal et al. (1999) provided two sets of suﬃcient conditions for strong
and weak consistency. For simplicity assume Πn ” Π. Before discussing the suﬃcient
conditions, a notion of prior support is quite important.
Deﬁnition 5. f0 is said to be in the Kullback-Leibler support of the prior Π (write
as f0 P KLpΠq) if ΠpKLǫpf0qq ą 0 for any ǫ ą 0.
Theorem 6.
1. The posterior is weakly consistent at f0 if f0 P KLpΠq for any
4

ǫ ą 0.
2. The posterior is strongly consistent at f0 with f0 satisfying (1) and for every
ǫ ą 0, there exists a δ ă ǫ{4, c1, c2 ą 0, β ă ǫ2{8 and subsets Fn Ă F such that
(a) ΠpF c
nq ă c1 expt´nc2u and
(b) log Npδ, Fn, }¨}1q ă nβ.
The complexity of the model space is measured by the sequence Fn, also referred
to as the sieve. Clearly, the weak topology doesn’t require limiting the complexity
for achieving consistency, having a large prior support alone suﬃces.
Although the idea of consistency is useful, quantifying the speed of convergence
of the posterior is necessary to determine the number of samples required to obtain
a desired accuracy upto constants. The speed or the rate of convergence is deﬁned
in terms of the smallest shrinking neighborhood around the truth that still contains
all the posterior mass asymptotically. It is important to note here that determining
the rate of convergence alone cannot satisfactorily foretell exactly the sample size
required to obtain the desired accuracy as the constant is quite hard to determine
accurately in most cases.
Deﬁnition 7. A posterior Πn is said to concentrate around f0 with rate at least ǫn
in the L1 topology if
ΠnpSMǫnpf0q | Y nq Ñ 1
rf0s a.s.
(1.6)
for some large constant M ą 0.
An accurate calculation of prior concentration and calibration of the model space
(Ghosal et al., 2000; Ghosal and van der Vaart, 2001) allow us to compute posterior
convergence rates.
5

Theorem 8. Suppose that for a sequence ǫn with ǫn Ñ 0 and nǫ2
n Ñ 8, a constant
C and sets Fn Ă F, we have
log Npǫn, Fn, }¨}nq ď nǫ2
n,
(1.7)
ΠnpF c
nq ď expt´nǫ2
npC ` 4qu,
(1.8)
Πn
ˆ
f : ´
ż
f0 log f{f0 ď ǫ2
n,
ż
f0plog f{f0q2 ď ǫ2
n
˙
ě
expp´nǫ2Cq,
(1.9)
then there exists a constant M ą 0 such that
ΠnpSMǫnpf0q | Y nq Ñ 1
rf0s a.s.
The form of the condition (1.9) can be motivated from entropy considerations.
Suppose, we wish to satisfy (1.9) for the minimal ǫn satisfying (1.9) with Fn “ F,
i.e., for the optimal rate of convergence for the model. Furthermore, for the sake of
argument, assume that all the distances are equivalent. Then a minimal ǫn-cover of
F consists of exptnǫ2
nu balls. If the prior Πn would spread its mass uniformly over F,
then every ball would obtain mass approximately expt´Cnǫ2
nu. Hence a rough impli-
cation of the conditions of Theorem 8 is that Πn should spread its mass uniformly in
order for the posterior to attain the optimal rate of convergence. Ghosal et al. (2000);
Ghosal and van der Vaart (2001) also provided several modiﬁcations of Theorem 8.
1.2
Review of Gaussian processes
Another key to this dissertation is the use of conditionally Gaussian processes which
is a powerful tool for function estimation in general. A Borel measurable random
element W with values in a separable Banach space pB, }¨}q (e.g., Cr0, 1s) is called
Gaussian if the random variable b˚W is normally distributed for any element b˚ P B˚,
the dual space of B. Note that under this general deﬁnition
fptq “
Jÿ
j“1
cjBjptq, cj „ Np0, σ2
jq, j “ 1, . . . , J,
(1.10)
6

for a ﬁxed set of basis functions tB1, . . . , BJu, is also Gaussian process conditional
on J and tσ2
j , j “ 1, . . . , Ju. Such a representation covers a wide range of models
and is ﬂexible enough to approximate a smooth function for suﬃciently large J and
thus can be suitably used for function estimation. To characterize the support of
a Gaussian process, one must study the reproducing kernel Hilbert space (RKHS)
associated with a Gaussian process.
The RKHS H attached to a zero-mean Gaussian process W taking values in a
Banach space B is deﬁned as the completion of the linear space of functions t ÞÑ
EWptqH relative to the inner product
xEWp¨qH1; EWp¨qH2yH “ EH1H2,
where H, H1 and H2 are ﬁnite linear combinations of the form ř
i aiWpsiq with
ai P R and si in the index set of W. It is a well-known fact the support of a Gaussian
process is the closure of the RKHS in B. The RKHS plays an important role in
determining the concentration properties of the process around a smooth function.
Refer to van der Vaart and van Zanten (2008b) for further details.
1.3
Research Problems
Our ﬁrst problem is related to variable selection in Bayesian nonparametric regres-
sion.
Theoretical study of variable selection in inﬁnite-dimensional models using
sparsity favoring priors is an important area of modern research which diﬀers signif-
icantly from variable selection in ﬁnite-dimensional models. More speciﬁcally, if the
true regression function is actually lower dimensional, the frequentist minimax rate of
estimating the regression function remains unaltered in a parametric model but can
be improved by a signiﬁcant margin in inﬁnite dimensional models (Barron et al.,
1999a; Kerkyacharian et al., 2001; Hoﬀmann and Lepski, 2002; Klutchnikoﬀ, 2005).
7

Consider the non-parametric mean regression model,
yi “ µpxiq ` ǫi, xi P r0, 1sd, ǫi „ Np0, σ2q,
(1.11)
In a Bayesian context, one would place a Gaussian process prior for µ and a hy-
per prior on the bandwidth parameter and model-average across diﬀerent values of
the bandwidth through the posterior distribution. The parameter a in the squared-
exponential covariance kernel expp´a||s ´ t||2q plays the role of a scaling or inverse
bandwidth. van der Vaart and van Zanten (2009) showed that with a gamma prior
on ad, one obtains the minimax rate of posterior contraction n´α{p2α`dq up to a
logarithmic factor for α-smooth functions adaptively over all α ą 0.
Even with
moderate number of dimensions, the assumption of the true function being in an
isotropic smoothness class characterized by a single smoothness parameter seems
restrictive. Practitioners often use a non-homogeneous variant of the squared ex-
ponential covariance kernel above given by Cps, tq “ expp´ řd
j“1 aj|sj ´ tj|2q. A
separate scaling variable aj for the diﬀerent dimensions incorporates dimension spe-
ciﬁc eﬀects in the covariance kernel, intuitively enabling better approximation of
functions in anisotropic smoothness classes. In particular, one can let a subset of the
covariates to drop out from the covariance kernel by setting some of the scales aj to
zero. Such a model was recently studied in Savitsky et al. (2011); Zou et al. (2010),
who used a point mass mixture prior on the bandwidth. Although this is an at-
tractive scheme for anisotropic modeling and dimension reduction in non-parametric
regression problems with empirical support, there hasn’t been any theoretical study
on this class of models in a Bayesian framework to our knowledge. In particular,
there is an open question whether the rate of posterior contraction can be improved
when the true regression function is supported on a lower dimensional space. We
want to develop a fully adaptive Bayesian nonparametric procedure that achieves
this.
8

Our second problem is regarding robust Bayesian inference on the mean regression
function allowing the residual density to change ﬂexibly with predictors. To simplify
inferences and prior elicitation, it is appealing to separate the mean regression func-
tion from the residual distribution in the speciﬁcation, which is accomplished by
only a few density regression methods. The general framework of separately mod-
eling the mean function and residual distribution nonparametrically was introduced
by Griﬃn and Steel (2010). They allow the residual distribution to change ﬂexibly
with predictors using the order-based Dirichlet process Griﬃn and Steel (2006b).
On the other hand, we want to develop a computationally simpler speciﬁcation with
straightforward prior elicitation. Moreover, existing theory on Gaussian process re-
gression Choi and Schervish (2007a); Choi (2009) ensures consistently estimating the
regression function assuming parametric error distribution. One of the key theoret-
ical problems lies in generalizing their theory to the case of nonparametric error
distribution.
Although there has been a well developed literature in studying posterior consis-
tency and convergence rates in nonparametric Bayes density estimation and mean
regression models, there has been a dearth of such results in density regression
models, particularly since the post Dirichlet process regime has seen the develop-
ment of numerous predictor-dependent random measures for modeling conditional
distributions e.g., the class of dependent random processes (MacEachern, 1999;
De Iorio et al., 2004; Griﬃn and Steel, 2006b; Rodriguez and Dunson, 2011a). Our
third research problem is providing a general theoretical framework for character-
izing the support of priors for conditional distributions. In doing so, a fundamen-
tal technical problem lies in calibrating a large space of conditional densities.
It
has been noted by Wu and Ghosal (2010) that the usual method of constructing
a sieve by controlling prior probabilities is unable to lead to a consistency theo-
rem in the multivariate case. This is because of the explosion of the entropy (ad
n)
9

of t
ş
Npy; µ, IdqdPpµq : Ppp´an, ansdq ą 1 ´ δu with increasing dimension. They
developed a technique speciﬁc to the Dirichlet process in the multivariate case for
showing weak and strong posterior consistency. We would like to develop technique
for constructing a sieve for high-dimensions suited to general mixture models.
We next turn our attention to methodological questions related to Bayesian man-
ifold estimation. With technological advancements, there has been a need to model
complex objects associated with complex data-types, e.g. 2D pixelated images, a
cloud of points in the 3D space, etc. Traditional function estimation methods seem
to be inadequate for this purpose. Boundaries of objects are widely studied across
many disciplines, such as biomedical imaging, cytology and computer vision. In de-
scribing complex boundaries, one can use a parametric curve (2D) or surface (3D),
i.e. Cptq : D1 Ñ R2 or Cptq : D2 Ñ R3 respectively, where D1 Ă R and D2 Ă R2.
Note that this is diﬀerent from a typical function estimation problem because the
independent variable, t, is unknown. Moreover, the curve must be closed to pro-
duce a valid boundary. In many applications featuring low-contrast images or sparse
and noisy point clouds, there is insuﬃcient data to recover local segments of the
boundary in isolation. Thus, it also becomes critical to model the boundary’s global
shape. Multiple related objects may share shape similarities that can be leveraged
for improved inference of boundaries. However, to the best of our knowledge, there
are few curve models which incorporate detailed shape information. Lastly, very few
works have considered integrating both curve ﬁtting and shape analysis. We want to
develop a model based approach for characterizing a population of 2D closed curves
representing the complex boundary of 2D shapes in pixelated images.
Our next intention is to develop models for closed surfaces from point cloud data
because of their usefulness to represent a variety of 3d shapes including human bones,
anatomical organs which are often encountered in practice. In many applications
such as in establishing the target for linac-based radiation therapy, it is necessary
10

to characterize the uncertainty in the 3D tumor contour to compute an expanded
contour, called a planning target volume (PTV). This PTV is used as the target
to which the full radiation dose is delivered. Naturally larger margin expansions
increase the likelihood that the tumor is treated eﬀectively, but also increase the dose
delivered to healthy normal tissues. Bayesian approaches are ideal in characterizing
this uncertainty and calibrating the PTV. The existing literature on closed surface
modeling focuses on frequentist point estimation methods that join surface patches
along the edges leading to heavy geometric constraints. One of the main motivations
is to develop a model for a closed surface which avoids the need for any constraints.
This can improve mixing of the MCMC employed for inferring the posterior surface
and facilitate interpretation of the coeﬃcients. To the best of our knowledge, there
hasn’t been any model-based work on ﬁtting a closed parametric surface to a sparse
and noisy point cloud data, particularly from a Bayesian point of view. Also studying
the structure of the constrained Euclidean spaces on which the prior distribution for
a closed surface is supported is particularly challenging as current Bayesian theory
mostly focus on unconstrained surface estimation.
Our next question is related to a fundamental problem in spatial point pattern
data modeling. Geostatistical models focus on inferring a continuous spatial process
based on data observed at ﬁnitely many locations, with the locations typically as-
sumed to be noninformative. As noted by Diggle et al. (2010), this assumption is
commonly violated for point-referenced spatial data, as it is not unusual to collect
data at locations thought to have a large or small value for the outcome. For exam-
ple, in monitoring of air pollution, one may place more monitors at locations believed
to have a high value of ozone or another pollutant, while in studying distribution
of animal species one may systematically look in locations thought to commonly
contain the species of interest. Diggle et al. (2010) proposed a shared latent pro-
cess model to adjust for bias due to informative sampling locations. Their analysis
11

was implemented using a Monte Carlo approach for maximum likelihood estimation.
However it is not clear whether the data contain information about the informa-
tiveness of the sampling locations, and one may wonder to what extent the prior is
driving the results even in large samples. We would like to develop a Bayesian model
for informative sampling and address these theoretical concerns.
1.4
Our contribution
In Chapter 2, we focus on nonparametric mean regression problem involving multiple
predictors where the interest is in estimating the multivariate regression surface in
the important predictors while discarding the unimportant ones. Our focus is on
deﬁning a Bayesian procedure that leads to the minimax optimal rate of posterior
contraction (up to a log factor) adapting to the unknown dimension and anisotropic
smoothness of the true surface. We propose such an approach based on a Gaussian
process prior with dimension-speciﬁc scalings, which are assigned carefully-chosen
hyperpriors. We obtained fully Bayesian frameworks for the following scenarios.
1. Adaptive estimation over Holder smooth functions that can possibly depend
on fewer coordinates and have isotropic smoothness over the remaining co-
ordinates: Consider a joint prior on pa1, a2, . . . , adq induced through the fol-
lowing hierarchical scheme: (i) draw ˜d according to some prior distribution
(with full support) on t1, . . . , du, (ii) given ˜d, draw a subset S of size ˜d from
t1, . . . , du following some prior distribution assigning positive prior probability
to all
`d
˜d
˘
subsets of size ˜d, (iii) generate a pair of random variables pa, bq with
a ˜d „ gamma and b drawn from any compactly supported density, and ﬁnally,
(iv) let aj “ a for j P S and aj “ b for j R S.
2. Adaptive estimation over anisotropic Holder functions of d arguments: We
propose a joint prior on the bandwidths pa1, a2, . . . , adq induced through the
12

following hierarchical speciﬁcation. Let Θ “ pΘ1, . . . , Θdq denote a random
vector with a density supported on the simplex Sd´1. Given Θ “ θ, we let the
elements of pa1, . . . , adq be conditionally independent, with a
1{θj
j
„ g, where g
is a gamma density. This is a novel generalization of the previous case.
We also demonstrated the necessity of using multiple bandwidths by proving that
the optimal prior choice in the isotropic case leads to a sub-optimal convergence rate
if the true function depends on fewer coordinates.
In Chapter 3, we consider the problem of robust Bayesian inference on the mean
regression function allowing the residual density to change ﬂexibly with predictors.
To accomplish this, we propose to place a Gaussian process prior on the regres-
sion function and to allow the residual density to be unknown through a probit
stick-breaking (PSB) process mixture. Here, we propose four novel variants of PSB
mixtures for the residual distribution. The ﬁrst uses a scale mixture of Gaussians to
obtain a prior with large support on unimodal symmetric distributions. The next is
based on a symmetrised location-scale PSB (sPSB) mixture, which is more ﬂexible
in avoiding the unimodality constraint, while constraining the residual density to be
symmetric and have mean zero. In addition, we show that this prior leads to strong
posterior consistency in estimating the regression function under weak conditions.
To allow the residual density to change ﬂexibly with predictors, we generalize the
above priors through incorporating probit transformations of Gaussian processes in
the weights.
In Chapter 4, deﬁning various topologies on the space of conditional distributions,
we provide suﬃcient conditions for posterior consistency focusing on a broad class of
priors formulated as predictor-dependent mixtures of Gaussian kernels. This theory
is illustrated by showing that the conditions are satisﬁed for a class of generalized
stick-breaking process mixtures in which the stick-breaking lengths are monotone,
13

diﬀerentiable functions of a continuous stochastic process. We also provide a set
of suﬃcient conditions for the case where stick-breaking lengths are predictor inde-
pendent, such as those arising from a ﬁxed Dirichlet process prior. A key technical
contribution of this article is the development of a novel method of constructing a
sieve, suited particularly to multivariate and predictor dependent mixture priors.
We developed a technique suited to general mixture models based on marginalizing
out the random measure P and calibrating the space of inﬁnite mixture models by a
ﬁnite mixture with increasing number of components subject to a tail condition as
follows. Let Fan “
#ż
Npy; µ, 1qdPpµq “
8
ÿ
h“1
πhφpy ´ µh, Idq : ||µh|| ď an, h “ 1, . . . , mn,
8
ÿ
h“mn`1
πh ă ǫ
+
.
for an, mn increasing. The proposed sieve alleviates the curse of dimensional-
ity (Wu and Ghosal, 2010) and can be used to show consistency in a large va-
riety of mixture models for multivariate density estimation (Tokdar, 2011b) and
can be generalized to accommodate predictor dependent mixture models for con-
ditional density estimation (Pati et al., 2012).
Our sieve construction has also
opened up the possibility of studying posterior consistency and convergence rates
in sparse multivariate mixtures of factor analyzers (McLachlan and Peel, 2000;
Canale and Dunson, 2011) and probability tensor decomposed models for categorical
data analysis (Bhattacharya and Dunson, 2011).
In Chapter 5, we proposed a Bayesian hierarchical model for boundaries of 2D
shapes contained in pixelated images.
The model is based on a novel multiscale
deformation process. By relating multiple objects through a hierarchical formulation,
we can successfully recover missing boundaries by borrowing shape information from
similar objects at the appropriate scale. Furthermore, the models latent parameters
help interpret the population, indicating dimensions of signiﬁcant shape variability
14

and also specifying a central curve that summarizes the collection. Often we have
information about surface normals, position of the nucleus and other information
in addition to pixel locations in these images. We incorporate these information to
obtain a better ﬁt without any compromise in computational eﬃciency. Theoretical
properties of our prior are studied in speciﬁc cases and eﬃcient Markov chain Monte
Carlo methods are developed, evaluated through simulation examples and applied to
a brain tumor contour detection problem.
In Chapter 6, we develop a Bayesian model for closed surfaces based on tensor
products of a cyclic basis resulting in inﬁnitely smooth surface realizations avoiding
heavy geometric constraints required to join the surface patches. Theoretical proper-
ties of the support of our proposed prior are studied and it is shown that the posterior
achieves the optimal rate of convergence under reasonable assumptions on the prior.
Chapter 6 laid the foundation for hierarchical modeling of multiple shapes, both 2d
and 3d. It allows the possibility of multitask learning, incorporation of covariates
and do hypothesis testing and dynamic models for closed surfaces.
To address the question associated with preferential sampling in point-pattern
data modeling, we follow a Bayesian approach in which the locations are modeled
using a log Gaussian Cox process (Møller et al., 2001), with the intensity function
included as a spatially-varying predictor in the outcome model, which also includes
spatial random eﬀects drawn from a Gaussian process. We incorporate a sampling
bias term a which causes a tendency to take more observations at spatial locations
having relatively high outcome values. We empirically showed that a joint model
of the responses and the sampling locations can lead to improved prediction and
characterization of uncertainty.
To our knowledge, we are the ﬁrst to develop a
Bayesian approach to the informative locations problem in geostatistical modeling.
A major contribution is studying the theoretical properties of the model. We address
this concern by proving that the posterior is proper under a noninformative prior on
15

a. In addition, one can consistently estimate a, the density of the sampling locations
and the mean function of the outcome process. Ongoing work focuses on quantifying
the increase in the amount of information in the sampling bias term with sample size
by studying the posterior convergence rates of the parameter a.
In Chapter 8, we outline future directions related to the current threads on density
regression models and hierarchical modeling of objects.
16

2
Adaptive dimension reduction with a Gaussian
process prior
2.1
Introduction
Non-parametric function estimation methods have been immensely popular due
to their ability to adapt to a wide variety of function classes with unknown
regularities.
In Bayesian nonparametrics, Gaussian processes (Rasmussen, 2004;
van der Vaart and van Zanten, 2008b) are widely used as priors on functions due
to tractable posterior computation and attractive theoretical properties. The law
of a mean zero Gaussian process Wt is entirely characterized by its covariance
kernel cps, tq “ EpWsWtq.
A squared exponential covariance kernel given by
cps, tq “ expp´a }s ´ t}2q is commonly used in the literature.
It is well established (Stone, 1982) that given n independent observations, the
optimal rate of estimation of a d-variable function that is only known to be α-smooth
is n´α{p2α`dq. The quality of estimation thus improves with increasing smoothness of
the “true” function while it deteriorates with increase in dimensionality. In practice,
the smoothness α is typically unknown and one would thus like to have a uniﬁed
17

estimation procedure that automatically adapts to all possible smoothness levels of
the true function. Accordingly, a lot of eﬀort has been employed to develop adaptive
estimation methods that are rate-optimal for every regularity level of the unknown
function.
The literature on adaptive estimation in a minimax setting was initiated by Lepski
in a series of papers (Lepski, 1990, 1991, 1992); see also Birg´e (2001) for a discussion
on this topic. We also refer the reader to Hoﬀmann and Lepski (2002), which contains
an extensive list of developments in the frequentist literature on adaptive estimation.
There is a growing literature on Bayesian adaptation over the last decade.
Pre-
vious works include Belitser and Ghosal (2003); Ghosal et al. (2003, 2008); Huang
(2004); Rousseau (2010); Kruijer et al. (2010); De Jonge and van Zanten (2010);
Shen and Ghosal (2011).
A key idea in frequentist adaptive estimation is to narrow down the search for an
“optimal” estimator within a class of estimators indexed by a smoothness or band-
width parameter, and make a data-driven choice to select the proper bandwidth.
In a Bayesian context, one would place a prior on the bandwidth parameter and
model-average across diﬀerent values of the bandwidth through the posterior distri-
bution. The parameter a in the squared-exponential covariance kernel c plays the
role of a scaling or inverse bandwidth. van der Vaart and van Zanten (2009) showed
that with a gamma prior on ad, one obtains the minimax rate of posterior contrac-
tion n´α{p2α`dq up to a logarithmic factor for α-smooth functions adaptively over all
α ą 0.
In multivariate problems involving even moderate number of dimensions, the
assumption of the true function being in an isotropic smoothness class character-
ized by a single smoothness parameter seems restrictive.
Practitioners often use
a non-homogeneous variant of the squared exponential covariance kernel given by
cps, tq “ expp´ řd
j“1 aj|sj ´ tj|2q. A separate scaling variable aj for the diﬀerent di-
18

mensions incorporates dimension speciﬁc eﬀects in the covariance kernel, intuitively
enabling better approximation of functions in anisotropic smoothness classes.
In
particular, one can let a subset of the covariates drop out of the covariance ker-
nel by setting some of the scales aj to zero. Such a model was recently studied in
Savitsky et al. (2011), who used a point mass mixture prior on ρj “ ´ log aj P r0, 1s.
Zou et al. (2010) also used a similar model for high-dimensional non-parametric vari-
able selection. Although this is an attractive scheme for anisotropic modeling and
dimension reduction in non-parametric regression problems with encouraging empir-
ical performance, there hasn’t been any theoretical studies of asymptotic properties
in related models in a Bayesian framework.
In the frequentist literature, minimax rates of convergence in anisotropic Sobolev,
Besov and H¨older spaces have been studied in Ibragimov and Khasminski (1981);
Nussbaum (1985); Birg´e (1986), with adaptive estimation procedures developed
in Barron et al. (1999a); Kerkyacharian et al. (2001); Hoﬀmann and Lepski (2002);
Klutchnikoﬀ(2005) among others. The traditional way of dealing with anisotropy is
to employ a separate bandwidth or scaling parameter for the diﬀerent dimensions,
and choose an optimal combination of scales in a data-driven way. However, the mul-
tidimensional nature of the problem makes the optimal bandwidth selection diﬃcult
compared to the isotropic case, as there is no natural ordering among the estimators
with multiple bandwidths (Lepski and Levit, 1999).
It is known (Hoﬀmann and Lepski, 2002) that the minimax rate of convergence
for a function with smoothness αi along the ith dimension is given by n´α0{p2α0`1q,
where α´1
0
“ řd
i“1 α´1
i
is an exponent of global smoothness (Birg´e, 1986). When
αi “ α for all i “ 1, . . . , d, one reduces back to the optimal rate for isotropic classes.
On the contrary, if the true function belongs to an anisotropic class, the assumption
of isotropy would lead to loss of eﬃciency which would be more and more accentu-
ated in higher dimensions. In addition, if the true function depends on a subset of
19

coordinates I “ ti1, . . . , id0u Ă t1, . . . , du for some 1 ď d0 ď d, the minimax rate
would further improve to n´α0I{p2α0I`1q, with α´1
0I “ ř
jPI α´1
j .
The objective of this chapter is to study whether one can fully adapt to this larger
class of functions in a Bayesian framework using dimension speciﬁc rescalings of a
homogenous Gaussian process, referred to as a multi-bandwidth Gaussian process
from now on. We answer the question in the aﬃrmative and develop a class of priors
which lead to the optimal rate n´α0I{p2α0I`1q of posterior contraction (up to a log
term) for any α and I without prior knowledge of either of them.
The general suﬃcient conditions for obtaining posterior rates of convergence
(Ghosal et al., 2000) involve ﬁnding a sequence of compact and increasing sub-
sets of the parameter space, usually referred to as sieves, which are “not to
large” in the sense of metric entropy and yet capture most of the prior mass.
van der Vaart and van Zanten (2008a) developed a general technique for construct-
ing such sieves with Gaussian process priors, which involved subtle manipu-
lations of the reproducing kernel Hilbert space (RKHS) of a Gaussian pro-
cess (van der Vaart and van Zanten, 2008b).
A key technical advancement in
van der Vaart and van Zanten (2009) was to extend the above theoretical framework
to the setting of conditionally Gaussian random ﬁelds. In particular, they exploited
a containment relation among the unit RKHS balls with diﬀerent bandwidths to
construct the sieves Bn in their framework. Their construction can be conceptually
related to the general framework for adaptive estimation developed in Lepski (1990,
1991, 1992), where a natural ordering among kernel estimators with diﬀerent scalar
bandwidths is utilized to compare diﬀerent estimators and balance the bias-variance
trade-oﬀ. However, it gets signiﬁcantly more complicated in situations involving mul-
tiple bandwidths to compare kernel estimators with diﬀerent vectors of bandwidths.
In multi-bandwidth Gaussian processes, a similar problem arises in comparing unit
RKHS balls of Gaussian processes with diﬀerent vectors of bandwidths, and the tech-
20

niques of van der Vaart and van Zanten (2009) cannot be immediately extended to
obtain adaptive posterior contraction rates in this case.
Our main contribution is to address the above issue by a novel prior speciﬁcation
on the vector of bandwidths and a careful construction of the sieves Bn, which can
be used to establish rate adaptiveness of the posterior distribution in a variety of
settings involving a multi-bandwidth Gaussian process. For simplicity of exposition,
we initially study the problem in two parts: (i) adaptive estimation over anisotropic
H¨older functions of d arguments, and (ii) adaptive estimation over functions that can
possibly depend on fewer coordinates and have isotropic H¨older smoothness over the
remaining coordinates. In each of these cases, we propose a joint prior on the band-
widths induced through a hierarchical Bayesian framework. To avoid the problem
of comparing between diﬀerent vectors of scales, we aggregate over a collection of
bandwidth vectors to construct the sets Bn. New results are developed to bound the
metric entropy of such collections of unit RKHS balls. Combining these results, we
balance the metric entropy of the sieve and the prior probability of its complement.
The prior speciﬁcations for the two cases above are easy to interpret intuitively and
can be easily connected to prescribe a uniﬁed prior leading to adaptivity over (i)
and (ii) combined. In particular, our proposed prior has interesting connections to a
class of multiplicity adjusting priors previously studied by Scott and Berger (2010)
in a linear model context.
Although our prior speciﬁcation involving dimension-speciﬁc bandwidth parame-
ters leads to adaptivity, a stronger result is required to conclude that a single band-
width would be inadequate for the above classes of functions. We prove that the
optimal prior choice in the isotropic case leads to a sub-optimal convergence rate if
the true function depends on fewer coordinates by obtaining a lower bound on the
posterior contraction rate. The general suﬃcient conditions for rates of posterior
contraction provide an upper bound on the rate of convergence implying that the
21

posterior contracts at least as fast as the rate obtained. Castillo (2008) studied lower
bounds for posterior contraction rate with a class of Gaussian process priors. We
extend the results of Castillo (2008) to the setting of rescaled Gaussian process pri-
ors. We develop a technique for deriving a sharp lower bound to the concentration
function of a rescaled Gaussian process, which can be used for comparing the pos-
terior convergence rates obtained for diﬀerent prior distributions on the bandwidth
parameter.
The remaining chapter is organized as follows.
In Section 2.2, we introduce
relevant notations. Section 2.3 discusses the main developments with applications to
anisotropic Gaussian process mean regression and logistic Gaussian process density
estimation described in subsection 2.3.4. In Section 2.4, we study various properties
of multi-bandwidth Gaussian processes which are crucially used in the proofs of the
main theorems in Section 2.5 and should also be of independent interest. Section 2.6
establishes the necessity of the multi-bandwidth Gaussian process (GP) by showing
that a single rescaling can lead to sub-optimal rates when the true function is lower-
dimensional.
2.2
Speciﬁc notations
To keep the notation clean, we shall only use boldface for a, b and α to denote
vectors.
We shall make frequent use of the following multi-index notations. For vectors a, b
P Rd, let a. “ řd
j“1 aj, a˚ “ śd
j“1 aj, a! “ śd
j“1 aj!, ¯a “ maxj aj, a “ minj aj, a.{b “
pa1{b1, . . . , ad{bdqT, a ¨ b “ pa1b1, . . . , adbdqT, ab “ śd
j“1 a
bj
j . Denote a ď b if aj ď bj
for all j “ 1, . . . , d. For n “ pn1, . . . , ndq, let Dnf denote the mixed partial derivatives
of order pn1, . . . , ndq of f.
Let Cr0, 1sd and Cβr0, 1sd denote the space of all continuous functions and the
22

H¨older space of β-smooth functions f : r0, 1sd Ñ R respectively, endowed with the
supremum norm }f}8 “ suptPr0,1sd |fptq|.
For β ą 0, the H¨older space Cβr0, 1sd
consists of functions f P Cr0, 1sd that have bounded mixed partial derivatives up
to order tβu, with the partial derivatives of order tβu being Lipschitz continuous of
order β ´ tβu.
Next, we deﬁne an anisotropic H¨older class of functions previously used in
Barron et al. (1999a) and Klutchnikoﬀ(2005).
For a function f
P Cr0, 1sd,
x P r0, 1sd, and 1 ď i ď d, let fip¨ | xq denote the univariate function y ÞÑ
fpx1, . . . , xi´1, y, xi`1, . . . , xdq. For a vector of positive numbers α “ pα1, . . . , αdq,
the anisotropic H¨older space Cαr0, 1sd consists of functions f which satisfy, for some
L ą 0,
max
1ďiďn sup
xPr0,1sd
tαiu
ÿ
j“0
››Djfip¨ | xq
››
8 ď L,
(2.1)
and, for any y P r0, 1s, h small such that y ` h P r0, 1s and for all 1 ď i ď d,
sup
xPr0,1sd
››Dtαiufipy ` h | xq ´ Dtαiufipy | xq
››
8 ď L |h|αi´tαiu .
(2.2)
For t P Rd and a subset I Ă t1, . . . , du of size |I| “ ˜d with 1 ď ˜d ď d, let tI
denote the vector of size ˜d consisting of the coordinates ptj : j P Iq. Let Cr0, 1sI
denote the subset of Cr0, 1sd consisting of functions f such that fptq “ gptIq for some
function g P Cr0, 1s
˜d. Also, let Cαr0, 1sI denote the subset of Cαr0, 1sd consisting
of functions f such that fptq “ gptIq for some function g P CαIr0, 1s ˜d.
The ǫ-covering number Npǫ, S, dq of a semi-metric space S relative to the semi-
metric d is the minimal number of balls of radius ǫ needed to cover S. The logarithm
of the covering number is referred to as the entropy.
We
write
“À”
for
inequality
up
to
a
constant
multiple.
Let
φpxq
“
p2πq´1{2 expp´x2{2q
denote
the
standard
normal
density,
and
23

let
φσpxq
“
p1{σqφpx{σq.
Let
an
asterisk
denote
a
convolution,
e.g.,
pφσ ˚ fqpyq “
ş
φσpy ´ xqfpxqdx.
Let
ˆf denote the Fourier transform of a
function f whenever it is deﬁned. Denote by Sd´1 the d ´ 1-dimensional simplex
consisting of points tx P Rd : xi ě 0, 1 ď i ď d, řd
i“1 xi “ 1u.
2.3
Main results
Let W “ tWt
:
t P r0, 1sdu be a centered homogeneous Gaussian process with
covariance function EpWsWtq “ cps ´ tq. By Bochner’s theorem, there exists a ﬁnite
positive measure ν on Rd, called the spectral measure of W, such that
cptq “
ż
Rd e´ipλ,tqνpdλq,
where for u, v
P
Cd,
pu, vq denotes the complex inner product.
As in
van der Vaart and van Zanten (2009), we shall restrict ourselves to processes with
spectral measure ν having sub-exponential tails, i.e., for some δ ą 0,
ż
eδ}λ}νpdλq ă 8.
(2.3)
The spectral measure ν of a squared exponential covariance kernel with cptq “
expp´ }t}2q has a density w.r.t.
the Lebesgue measure given by fpλq
“
1{p2dπd{2q expp´ }λ}2 {4q which clearly satisﬁes (2.3).
Rates of posterior contraction with Gaussian process priors were ﬁrst studied
by van der Vaart and van Zanten (2008a), who gave suﬃcient conditions in terms
of the concentration function of a Gaussian random element for optimal rate of
convergence in a variety of statistical problems including density estimation using
the logistic Gaussian process (Lenk, 1988, 1991), Gaussian process mean regression,
latent Gaussian process regression (e.g., in logit, probit models), binary classiﬁcation,
etc. As indicated in the introduction, one needs to build appropriate sieves in the
space of continuous functions to get a handle on the posterior rates of convergence
24

in such models. van der Vaart and van Zanten (2008a) constructed the sieves as a
collection of continuous functions within a small (sup-norm) neighborhood of a norm-
bounded subset of the RKHS. Sharp bounds on the complement probability of such
sets can be obtained using Borell’s inequality (Borell, 1975), and the metric entropy
can also be appropriately controlled exploiting the fact that the RKHS consists of
smooth functions if the covariance kernel is smooth.
It is important to mention
here that a similar strategy involving a subset of continuous functions bounded in
sup-norm doesn’t work beyond the uni-dimensional case (Tokdar and Ghosh, 2007).
A process W with inﬁnitely smooth sample paths is not suitable for modeling
less smooth functions. Rescaling the sample paths of an inﬁnitely smooth Gaus-
sian process is a powerful technique to improve the approximation of α-H¨older
functions from the RKHS of the scaled process tW A
t
“ WAt :
t P r0, 1sdu
with A ą 0.
Intuitively, for large values of A, the scaled process traverses the
sample path of an unscaled process on the larger interval r0, Asd, thereby in-
corporating more “roughness”.
In the context of univariate function estimation,
van der Vaart and van Zanten (2007) had previously shown that a rescaled Gaus-
sian process W an with a deterministic scaling an “ n1{p2α`1q logκ n leads to the
minimax optimal rate for α-smooth functions up to a log factor.
This speciﬁca-
tion requires knowledge of the true smoothness to obtain the minimax rate. Since
the true smoothness is essentially always unknown, one would ideally employ a ran-
dom rescaling, i.e., place a prior on the scale. van der Vaart and van Zanten (2009)
studied rescaled Gaussian processes W A “ tWAt
: t P r0, 1sdu for a real positive
random variable A stochastically independent of W, extending the framework of
van der Vaart and van Zanten (2008a) to the setting of conditionally Gaussian ran-
dom elements (see also De Jonge and van Zanten (2010) for a diﬀerent class of con-
ditionally Gaussian processes). van der Vaart and van Zanten (2009) showed that
with a Gamma prior on Ad, one obtains the minimax-optimal rate of convergence
25

n´α{p2α`dq (up to a logarithmic factor) for α-smooth functions.
Since their prior
speciﬁcation does not involve the unknown smoothness α, the procedure is fully
adaptive.
The key result of van der Vaart and van Zanten (2009) was to construct the sieves
Bn Ă Cr0, 1sd so that given α ą 0, a function w0 P Cαr0, 1sd, and a constant C ą 1,
there exists a constant D ą 0 such that, for every suﬃciently large n,
log Np¯ǫn, Bn, }¨}8q ď Dn¯ǫ2
n,
(2.4)
PpW A R Bnq ď e´Cnǫ2
n,
(2.5)
Pp
››W A ´ w0
››
8 ď ǫnq ě e´nǫ2
n,
(2.6)
with ǫn “ n´α{p2α`1qplog nqκ1, ¯ǫn “ n´α{p2α`1qplog nqκ2 for constants κ1, κ2 ą 0.
There is a deep connection between the above measure theoretic result involv-
ing the concentration probability and complexity of the support of the conditional
Gaussian process W A and rates of posterior contraction with Gaussian process pri-
ors. van der Vaart and van Zanten (2008a) mention that the conditions (2.4) - (2.6)
have a one-to-one correspondence with the general suﬃcient conditions for rates of
posterior contraction (Theorem 2.1 of Ghosal et al. (2000)). In a speciﬁc statistical
setting involving Gaussian process priors on some function, sieves in the parameter
space of interest can be easily obtained by restricting the unknown function to such
sets Bn. It only remains to appropriately relate the norm of discrepancy speciﬁc to
the problem (e.g., Hellinger norm for density estimation) to the Banach space norm
(sup-norm in this case) of the Gaussian random element to conclude that maxtǫn, ¯ǫnu
is the rate of posterior contraction; refer to the discussion following Theorem 3.1 in
van der Vaart and van Zanten (2009).
In this chapter, we shall consider two function classes deﬁned in Section 2.2,
(i) H¨older class of functions Cαr0, 1sd with anisotropic smoothness (α P Rd
`), and
(ii) H¨older class of functions Cαr0, 1sI with isotropic smoothness that can possibly
26

depend on fewer dimensions (α ą 0 and I Ă t1, . . . , du). We shall study multi-
bandwidth Gaussian processes of the form tWa
t “ Wa¨t : t P r0, 1sdu for a vector of
rescalings (or inverse-bandwidths) a “ pa1, . . . , adqT with aj ą 0 for all j “ 1, . . . , d.
For a continuous function in the support of a Gaussian process, the probability
assigned to a sup-norm neighborhood of the function is controlled by the centered
small ball probability and how well the function can be approximated from the
RKHS of the process (Section 5 of van der Vaart and van Zanten (2008b)). With
the target class of functions as in (i) or (ii), a single scaling seems inadequate and it
is intuitively appealing to introduce multiple bandwidth parameters to enlarge the
RKHS and facilitate improved approximation from the RKHS.
As in van der Vaart and van Zanten (2007), we shall ﬁrst consider minimax
estimation with deterministic scalings an.
van der Vaart and van Zanten (2008a)
showed that the rate of posterior contraction with a Gaussian process prior W is
determined by the behavior of the concentration function φw0pǫq for ǫ close to zero,
where
φw0pǫq “
inf
h:H:}h´w0}8ďǫ }h}2
H ´ log Pp}W}8 ď ǫq,
(2.7)
and H is the RKHS of W. (We tacitly assume that there is a given statistical problem
where the true parameter f0 is a known function of w0.) Based on their result, with
a multi-bandwidth Gaussian process prior Wan, the posterior distribution would
asymptotically accumulate all of its mass on an Opǫnq ball around the true parameter,
where ǫn is the smallest possible solution to
φan
w0 pǫnq À nǫ2
n,
(2.8)
with φan
w0 pǫnq denoting the concentration function of the scaled process Wan.
In
the following Theorem 9, we state choices of the bandwidth parameters speciﬁc to
(i) and (ii) that lead to minimax rates of convergence. The proof follows from the
27

properties of multi-bandwidth GPs developed in Lemma 16–19 and hence is not
provided separately.
Theorem 9.
1. Suppose w0 P Cαr0, 1sd for some α P Rd
` and let α´1
0
“
řd
i“1 α´1
i . Let an “ pa1n, . . . , adnqT, where,
ajn “
“
n1{p2α0`1q‰α0{αi.
(2.9)
Then, with ǫn “ n´α0{p2α0`1q logκ1 n for some constant κ1, φan
w0 pǫnq À nǫ2
n.
2. Suppose w0 P Cαr0, 1sI for some α ą 0 and I Ă t1, . . . , du with |I| “ d˚. Let
an “ pa1n, . . . , adnqT, where,
ajn “
#“
n1{p2α`d˚q‰1{d˚
if j P I,
1
if j R I.
(2.10)
Then, with ǫn “ n´α{p2α0`d˚q logκ2 n for some constant κ2, φan
w0 pǫnq À nǫ2
n.
Theorem 9 coupled with van der Vaart and van Zanten (2008a) implies that a
multi-bandwidth Gaussian process Wan with an as in (2.9) and (2.10) leads to the
minimax optimal rate of convergence in cases (i) and (ii) respectively.
Theorem 9 requires knowledge of the true smoothness levels or the true dimen-
sionality for minimax estimation. This is clearly unappealing and one would instead
like to devise priors on a that lead to minimax rates for all smoothness levels. We
propose a novel class of joint priors on the rescaling vector a that leads to adaptation
over function classes (i) and (ii) in Section 2.3.1 and 2.3.2 respectively. Connections
between the two prior choices are discussed and a uniﬁed framework is prescribed
for the function class
␣
Cαr0, 1sI : α P Rd
`, I Ă t1, . . . , du
(
combining (i) and (ii).
The main technical challenge for adaptation is to ﬁnd sets Bn so that (2.4)–
(2.6) are satisﬁed with w0 in the above function classes and ǫn being the optimal
rate of convergence for the same. With such sets Bn, one can use standard results
to establish adaptive minimax rate of convergence in various statistical settings.
Applications to some speciﬁc statistical problems are described in Section 2.3.4.
28

2.3.1
Adaptive estimation of anisotropic functions
Let A = pA1, . . . AdqT be a random vector in Rd with each Aj a non-negative random
variable stochastically independent of W. We can then deﬁne a scaled process WA “
tWA¨t : t P r0, 1sdu, to be interpreted as a Borel measurable map in Cr0, 1sd equipped
with the sup-norm }¨}8. The basic idea here is to stretch or shrink the diﬀerent
dimensions by diﬀerent amounts so that the resulting process becomes suitable for
approximating functions having diﬀerential smoothness along the diﬀerent coordinate
axes.
We shall deﬁne a joint distribution on A induced through the following hier-
archical speciﬁcation. Let Θ “ pΘ1, . . . , Θdq denote a random vector with a den-
sity supported on the simplex Sd´1. In the subsequent analysis, we shall assume
Θ „ Dirpβ1, . . . , βdq for some β “ pβ1, . . . , βdq. Given Θ “ θ, we let the elements of
A be conditionally independent, with A
1{θj
j
„ g, where g is a density on the positive
real line satisfying,
C1xp expp´D1x logq xq ď gpxq ď C2xp expp´D2x logq xq,
for positive constants C1, C2, D1, D2 and every suﬃciently large x ą 0.
In particular, the conditions in the above display are satisﬁed with q “ 0 if A
1{θj
j
follows a gamma distribution. For notational simplicity, we shall assume g to be a
gamma density from now on, noting that the main results would all hold for the
general form of g above.
Let πA denote the induced joint prior on A, so that πApaq “
ş śd
j“1 πpaj |
θjqdπpθq. We now state our main theorem for the anisotropic smoothness class in
(i), with a detailed proof provided in Section 2.5.
Theorem 10. Let W be a centered homogeneous Gaussian random ﬁeld on Rd with
spectral measure ν that satisﬁes (2.3) and let WA denote the multi-bandwidth process
29

with A „ πA as above. Let α “ pα1, . . . , αdq be a vector of positive numbers and
α0 “ přd
i“1 α´1
i q´1. Suppose w0 belongs to the anisotropic H¨older space Cαr0, 1sd.
Then for every constant C ą 1, there exist Borel measurable subsets Bn of Cr0, 1sd
and a constant D ą 0 such that, for every suﬃciently large n, the conditions (2.4)–
(2.6) are satisﬁed by WA with ǫn “ n´α0{p2α0`1qplog nqκ1, ¯ǫn “ n´α0{p2α0`1qplog nqκ2
for constants κ1, κ2 ą 0.
2.3.2
Adaptive dimension reduction
We next consider the smoothness class in (ii), namely Cαr0, 1sI for I Ă t1, . . . , du and
α ą 0. If the true function has isotropic smoothness on the dimensions it depends
on, it is intuitively clear that one doesn’t need a separate scaling for each of the
dimensions. Indeed, had we known the true coordinates I Ă t1, . . . , du, we could
have only scaled the dimensions in I by a positive random variable A, and a slight
modiﬁcation of the results in van der Vaart and van Zanten (2009) would imply that
a gamma prior on A|I| would lead to adaptation.
Without knowledge of I, it is natural to consider mixture priors of the form Aj „
pA`p1´pqB, where A and B are positive random variables and 0 ď p ď 1, so that a
subset of the dimensions are scaled by A and the remaining by B. Assume a gamma
prior on Ad and B any ﬁxed compactly supported density. We ﬁrst construct a sample
size dependent prior πn
A for A through the following deterministic speciﬁcation for
p “ pn assuming knowledge of |I| and the true smoothness level α.
Aj „ pnA ` p1 ´ pnqB, j “ 1, . . . , d
pd
n “ 1 ´ expp´cnq, cn “ n´d˚{p2α`d˚q,
where d˚ “ |I|. The following theorem is a result on partial adaptive estimation,
where we can adapt to the positions in I using πn
A assuming only the knowledge of
|I| and α.
30

Theorem 11. Let W be a centered homogeneous Gaussian random ﬁeld on Rd with
spectral measure ν that satisﬁes (2.3) and let WA denote the multi-bandwidth process
with A „ πn
A as above. Suppose w0 P Cαr0, 1sI and let I Ă t1, . . . , du with |I| “
d˚.Then for every constant C ą 1, there exist Borel measurable subsets Bn of Cr0, 1sd
and a constant D ą 0 such that, for every suﬃciently large n, the conditions (2.4)–
(2.6) are satisﬁed by WA with ǫn “ n´α{p2α`d˚qplog nqκ1, ¯ǫn “ n´α{p2α`d˚qplog nqκ2
for constants κ1, κ2 ą 0.
As in the previous sub-section, our ultimate aim is to propose a joint prior on A so
that the rescaled process WA satisﬁes conditions (2.4)–(2.6) without the knowledge
of α or I. We describe such a prior speciﬁcation below.
Consider a joint prior πA on A induced through the following hierarchical scheme:
(i) draw ˜d according to some prior distribution (with full support) on t1, . . . , du, (ii)
given ˜d, draw a subset S of size ˜d from t1, . . . , du following some prior distribution
assigning positive prior probability to all
`d
˜d
˘
subsets of size ˜d, (iii) generate a pair
of random variables pA, Bq with A
˜d „ gamma and B drawn from a ﬁxed compactly
supported density, and ﬁnally, (iv) let Aj “ A for j P S and Aj “ B for j R S.
We next state our main result on adaptive dimension reduction. The proof of
the following Theorem 12 has elements in common with the proof of the previous
theorem, and hence only a sketch of the proof is provided in Section 2.5. Theorem
11 can be proved along similar lines.
Theorem 12. Let W be a centered homogeneous Gaussian random ﬁeld on Rd with
spectral measure ν that satisﬁes (2.3) and let WA denote the multi-bandwidth pro-
cess with A „ πA as above. Suppose w0 belongs to the H¨older space Cαr0, 1sI for
some subset I of t1, . . . , du and α ą 0.
Then for every constant C ą 1, there
exist Borel measurable subsets Bn of Cr0, 1sd and a constant D ą 0 such that,
for every suﬃciently large n, the conditions (2.4)–(2.6) are satisﬁed by WA with
31

ǫn “ n´α{p2α`d0qplog nqκ1, ¯ǫn “ n´α{p2α`d0qplog nqκ2 for constants κ1, κ2 ą 0 and
d0 “ |I|.
Remark 13. A salient feature of our hierarchical prior formulation is that the tail
heaviness of A is related to the size of the subset S, i.e., the number of dimensions
that are scaled by the non-compact random variable A. For larger subsets S, the
tails of A get lighter, inducing a bigger penalty for large values of A. In the previous
mixture speciﬁcation Aj „ πnA`p1´πnqB, we believe that we needed the information
of α and d0 in the weights πn since the interplay between the size of S and the tail
heaviness of A was missing.
2.3.3
Connections between cases (i) and (ii)
The joint distributions on A speciﬁed in Section 2.3.1 and 2.3.2 are closely connected.
To begin with, note that if we set Aj “ A and θj “ 1{d for all j, one obtains a
gamma prior on Ad which was previously suggested by van der Vaart and van Zanten
(2009). In the general anisotropic case, the joint distribution can be motivated as
follows. Recall that the purpose of rescaling is to traverse the sample paths of an
inﬁnite smooth stochastic process on a larger domain to make it more suitable for
less smooth functions.
If the true function has anisotropic smoothness, then we
would like to stretch those directions more where the function is less smooth. Now
note that for smaller values of θj, the marginal distribution of aj has lighter tails
compared to larger values of θj. We would thus like θj to assume smaller values for
the directions j where the function is more smooth and larger values corresponding
to the less smooth directions. Without further constraints on θ, it is not possible
to separate the scale of A from θ. This motivates us to constrain θ to the simplex
which serves as a weak identiﬁability condition.
In the limit as θj Ñ 0, the distribution of aj converges to a point mass at zero.
Accordingly, if the true function doesn’t depend on a set of pd ´ d˚q dimensions, we
32

would set θj “ 0 for those dimensions and choose the remaining θj’s from a d˚ ´ 1
dimensional simplex. In particular, if the function has isotropic smoothness in the
remaining d˚ coordinates, one can simply choose θj “ 1{d˚ for those dimensions.
This explains our choice of letting ad˚ follow a gamma distribution in Section 2.3.2.
Based on the above discussion, we combine the results in Section 2.3.1 and 2.3.2
to prescribe a uniﬁed framework for adaptively estimating functions which possibly
depend on fewer coordinates and have anisotropic smoothness in the remaining ones,
i.e., functions in Cαr0, 1sI for α P Rd
` and I Ă t1, . . . , du.
2.3.4
Rates of convergence in speciﬁc settings
The
above
two
theorems
are
in
the
same
spirit
as
Theorem
3.1
of
van der Vaart and van Zanten (2009) and Theorem 2.2 of De Jonge and van Zanten
(2010) and can be used to derive rates of posterior contraction in a variety of sta-
tistical problems involving Gaussian random ﬁelds. We shall consider a couple of
speciﬁc problems with the message that similar results can be obtained for a large
class of problems involving rescaled Gaussian random ﬁelds.
We ﬁrst consider a regression problem where given independent response variable
yi and covariates xi P r0, 1sd, the response is modeled as random perturbations around
a smooth regression surface, i.e., yi “ µpxiq ` ǫi. We assume ǫi „ Np0, σ2q with a
prior on σ supported on some interval ra, bs Ă r0, 8q.
As motivated before, the regression surface might depend only on a subset of
variables in r0, 1sd and have anisotropic smoothness in the remaining variables. It is
thus appealing to place a Gaussian process prior with dimension speciﬁc rescalings on
µ as follows. Let W denote a Gaussian process with squared exponential covariance
kernel cptq “ expp´ }t}2q and A “ pA1, . . . , AdqT be a vector of positive random
variables stochastically independent of W. We use the conditionally Gaussian process
WA “ tWA¨t : t P r0, 1sdu as a prior for µ, with a joint prior on A induced through
33

the following hierarchical speciﬁcation: (i) draw ˜d uniformly on t1, . . . , du, (ii) given
˜d, draw a subset S “ ti1, . . . , i ˜du of size ˜d uniformly from t1, . . . , du, (iii) draw
θ “ pθ1, . . . , θ ˜dq from the ˜d ´ 1-dimensional simplex S ˜d´1, (iv) let A
1{θj
j
„ gamma for
j P S, and set the remaining Aj’s to zero.
We denote the posterior distribution by Πp¨
|
y1, . . . , ynq.
Let }µ}2
n
“
n´1 řn
i“1 µ2pxiq denote the L2 norm corresponding to the empirical distribution of
the design points. Let the true value σ0 of σ be contained in the interval ra, bs. The
posterior is said to contract at a rate ǫn, if for every suﬃciently large M,
Eµ0,σ0Π
“
pµ, σq : }µ ´ µ0}n ` |σ ´ σ0| ą Mǫn | y1, . . . , yn
‰
Ñ 0.
Theorem 14. Let α “ pα1, . . . , αdq be a vector of positive numbers and I be a
subset of t1, . . . , du. If w0 P Cαr0, 1sI, then the posterior contracts at the rate ǫn “
n´α0I{p2α0I`1q logκ n, where α´1
0I “ ř
jPI α´1
j .
Thus, one obtains the minimax optimal rate up to a log factor adapting to the
unknown dimensionality and anisotropic smoothness.
A similar result holds for density estimation using the logistic Gaussian process.
Suppose X1, . . . , Xn are drawn i.i.d. from a continuous, everywhere positive density
f0 on the hypercube r0, 1sd. Suppose one uses a multi-bandwidth Gaussian process
exponentiated and re-normalized to integrate to one as the prior on the unknown
density f, so that
fptq “
eWA
t
ş
r0,1sd eWA
s ds
.
Theorem 15. Let α “ pα1, . . . , αdq be a vector of positive numbers and I be a subset
of t1, . . . , du. If w0 “ log f0 P Cαr0, 1sI, then the posterior contracts at the rate ǫn “
n´α0I{p2α0I`1q logκ n with respect to the Hellinger distance, where α´1
0I “ ř
jPI α´1
j .
34

The proofs of the above Theorems 14 and 15 follow in a straightforward manner
from our main results in Theorem 10 and 12. We don’t provide a proof here since the
steps are very similar to those in Section 3 of van der Vaart and van Zanten (2008a).
2.4
Properties of the multi-bandwidth Gaussian process
We now summarize some properties of the RKHS of the scaled process Wa
for a ﬁxed vector of scales a, which shall be crucially used to prove our
main theorems.
The ﬁrst ﬁve lemmas generalize the results in section 4 of
van der Vaart and van Zanten (2009) from a single scaling to a vector of scales. A
key idea in van der Vaart and van Zanten (2009) to construct the sieves Bn was to
exploit a containment relation among the unit balls of the RKHS with diﬀerent
amounts of scaling. Such a result suﬃced in the single rescaling framework exploit-
ing the ordering in elements of R`. However, the result can only be generalized with
respect to the partial order on Rd
` which is not suﬃcient for our purpose. We develop
a technique to circumvent this curse of dimensionality by precisely calculating the
metric entropy of a collection of unit RKHS balls.
Assume that the spectral measure ν of W has a spectral density f. For a P Rd
`, the
rescaled proces Wa has a spectral measure νa given by νapBq “ νpB.{aq. Further,
νa admits a spectral density fa, with fapλq “ a´1fpλ.{aq. For w0 P Cr0, 1sd, deﬁne
φa
w0pǫq to be the concentration function of the rescaled Gaussian process Wa.
As
a
straightforward
extension
of
Lemma
4.1
and
4.2
in
van der Vaart and van Zanten (2009), it turns out that the RKHS of the pro-
cess Wa can be characterized as below.
Lemma 16. The RKHS Ha of the process tWa
t
: t P r0, 1sdu consists of real parts
of the functions
t ÞÑ
ż
eipλ,tqgpλqνapdλq,
35

where g runs over the complex Hilbert space L2pνaq. Further, the RKHS norm of the
element in the above display is given by }g}L2pνaq.
Lemma 4.3 of van der Vaart and van Zanten (2009) shows that for any isotropic
H¨older smooth function w, convolutions with an appropriately chosen class of higher
order kernels indexed by the scaling parameter a belong to the RKHS. This suggests
that driving the bandwidth 1{a to zero, one can obtain improved approximations to
any H¨older smooth function. The following Lemma 17 illustrates the usefulness of
using separate bandwidths for each dimension for approximating anisotropic H¨older
functions from the RKHS.
Lemma 17. Assume ν has a density with respect to the Lebesgue measure which
is bounded away from zero on a neighborhood of the origin. Let α P Rd
` be given.
Then, for any subset I of t1, . . . , du and w P Cαr0, 1sI, there exists constants C and
D depending only on ν and w such that, for a large enough,
inft}h}2
Ha : }h ´ w}8 ď C
ÿ
iPI
a´αi
i
u ď Da˚.
Proof. We shall prove the result for w P Cαr0, 1sd and sketch an argument for
extending the proof to any w P Cαr0, 1sI.
Let ψj, j “ 1, . . . , d, be a set of higher order kernels as in the proof of Lemma 4.3 of
van der Vaart and van Zanten (2009), which satisfy
ş
ψjptjqdtj “ 1,
ş
tk
jψjptjqdtj “
0 for any positive integer k and
ş
|tj|αj|ψjptjq|dtj ď 1.
Deﬁne ψ : Rd Ñ C by
ψptq “ ψ1pt1q . . . ψdptdq so that one has
ş
Rd ψptqdt “ 1,
ş
Rd tkψptqdt “ 0 for any non-
zero multi-index k “ pk1, . . . , kdq, and the functions | ˆψ|{f and | ˆψ|2{f are uniformly
bounded, where ˆψ denotes the Fourier transform of ψ.
For a vector of positive numbers a “ pa1, . . . , adq, let ψaptq “ a˚ψpa ¨ tq, where
a˚ “ śd
j“1 aj. By Whitney’s theorem, w can be extended to a function w : Rd Ñ R
with compact support and }w}α ă 8. Working with this extension, we shall ﬁrst
36

show that the convolution ψa ˚ w is contained in the RKHS Ha. To that end, note
that,
1
p2πqdpψa ˚ wqptq “
ż
e´ipt,λq ˆwpλq ˆψapλqdλ “
ż
eipt,λq ˆwp´λq ˆψapλq
fapλq
νapdλq.
Thus, following Lemma 16, we need to show that ˆwp´λq ˆψapλq{fapλq P L2pνaq to
conclude that ψa ˚ w belongs to Ha. Since ˆ
ψapλq “ ˆψpλ.{aq, one has
ż ˇˇˇˇˇ
ˆwp´λq ˆψapλq
fapλq
ˇˇˇˇˇ
2
νapdλq ď a˚
›››››
| ˆψ|2
f
›››››
8
ż
| ˆwpλq|2 dλ.
The above assertion is thus proved by noting that | ˆψ|2{f is uniformly bounded by
construction and p2πqd ş
| ˆw2pλq|dλ “
ş
|wptq|2dt ă 8. Also, the squared RKHS norm
of ψa ˚w is bounded by Da˚, with D depending only on ν and w. Thus, the proof of
Lemma 17 would be completed if we can show that }ψa ˚ w ´ w}8 ď C řd
j“1 a
´αj
j
.
We have, for any t P Rd,
ψa ˚ wptq ´ wptq “
ż
ψpsqtwpt ´ s.{aq ´ wptquds.
For 1 ď j ď d ´ 1, let upjq denote the vector in Rd with upjq
i
“ 0 for i “ 1, . . . , j and
upjq
i
“ 1 for i “ j ` 1, . . . , d. For any two vectors x, y P Rd, we can navigate from x
to y in a piecewise linear fashion traveling parallel to one of the coordinate axes at a
time. The vertices of the path will be given by xp0q “ x, xpjq “ upjq ¨ x ` p1 ´ upjqq ¨ y
for j “ 1, . . . , d ´ 1 and xpdq “ y.
A multivariate Taylor expansion of wpt´s.{aq around wptq cannot take advantage
of the anisotropic smoothness of w across diﬀerent coordinate axes. Letting x “ t, y “
t ´ s.{a and xpjq, j “ 0, 1, . . . , d as above, let us write wpyq ´ wpxq in the following
telescoping form,
wpyq ´ wpxq “
dÿ
j“1
wpxpjqq ´ wpxpj´1qq “
dÿ
j“1
wjptj ´ sj{aj | xpjqq ´ wjptj | xpjqq,
37

where the functions wj are as deﬁned in Section 2.2, with
wjpt | xq “ wpx1, . . . , xj´1, t, xj`1, . . . , xdq for any t P R and x P Rd.
Thus,
wpt ´ s.{aq ´ wptq “
dÿ
j“1
„ tαju
ÿ
i“1
Diwjptj | xpjqqp´sj{ajqi
i!
` Sjptj, ´sj{ajq

,
where |Sjptj, ´sj{ajq| ď Ks
αj
j a
´αj
j
by (2.2), for a constant K depending on ν and w
but not on t and s. Combining the above, we have
ˇˇˇˇ
ż
ψpsqtwpt ´ s.{aq ´ wptqu
ˇˇˇˇ “
ˇˇˇˇˇ
dÿ
j“1
ż
Sjptj, ´sj{ajqdtj
ˇˇˇˇˇ ď C
dÿ
j“1
a
´αj
j
.
If, w P Cαr0, 1sI for some subset I of t1, . . . , du with |I| “ ˜d, so that wptq “ w0ptIq
for some w0 P CαIr0, 1s
˜d, then the conclusion follows trivially follows from the
observation ψa ˚ w “ ψaI ˚ w0.
We next study the metric entropy of the unit ball of the RKHS and the centered
small ball probability of the rescaled process. Let Ha
1 denote the unit ball in the
RKHS of Wa.
Lemma 18. There exists a constant K, depending only on ν and d, such that, for
ǫ ă 1{2,
log Npǫ, Ha
1 , }¨}8q ď Ka˚
ˆ
log 1
ǫ
˙d`1
.
Proof. By Lemma 16, an element of Ha
1 can be written as the real part of the function
h : r0, 1sd Ñ C given by
hptq “
ż
eipλ,tqgpλqνapdλq
(2.11)
38

for g : Rd Ñ C a function with
ş
|gpλq|2 νapdλq ď 1.
Viewing h as a function of it, we would like to exploit the sub-exponential tails
of ν as in (2.3) to extend h analytically over a larger domain in Cd. For z P Cd,
we shall continue to denote the function z ÞÑ
ş
epλ,zqψpλqνapdλq by h. Using the
Cauchy-Schwartz inequality and the change of variable theorem,
|hpzq|2 ď
ż
epλ,2a¨Repzqqνpdλq,
(2.12)
where Repzq denotes the vector whose jth element is the real part of zj for j “
1, . . . , d, and a ¨ Repzq “ pa1Repz1q, . . . , adRepzdqqT. From (2.12) and the dominated
convergence theorem, any h P Ha
1 can be analytically extended to Γ “ tz P Cd :
}2a ¨ Repzq}2 ă δu. Clearly, Γ contains a strip Ωin Cd given by Ω“ tz P Cd :
|Repzjq| ď Rj, j “ 1, . . . , du with Rj “ δ{p6aj
?
dq. Also, for every z P Ω, h satisﬁes
the uniform bound |hpzq|2 ď
ş
eδ}λ}νpdλq “ C2.
The analytic extension of h to a strip containing the product of the imaginary
axes allows us to precisely estimate the error term of a k-order Taylor expansion of
hptq. For t P r0, 1sd, Let C1, . . . , Cd denote circles of radius R1, . . . , Rd in the complex
plane around the coordinates it1, . . . , itd of it respectively. Using the Cauchy integral
formula,
ˇˇˇˇ
Dnhptq
n!
ˇˇˇˇ “
ˇˇˇˇˇˇ
1
p2πiqd
¿
C1
¨ ¨ ¨
¿
Cd
hpzq
pz ´ tqn`1dz1 ¨ ¨ ¨ dzd
ˇˇˇˇˇˇ
ď C
Rn.,
where Dn denotes the partial derivative of order n “ pn1, . . . , ndq. This suggests
using a net of piecewise polynomials for approximating the elements of Ha
1 . One
can discretize the coeﬃcients and centers of the piecewise polynomials to obtain a
ﬁnite set of functions that approximate the leading terms of a Taylor expansion of
a function in Ha
1 and the remainder terms can be controlled using the bound in the
above display.
39

To elaborate, let R “ pR1, . . . , RdqT.
Partition T “ r0, 1sd into rectangles
Γ1, . . . , Γm with centers tt1, t2, . . . , tmu such that given any z P T, there exists Γj
with center tj “ ptj1, . . . , tjdqT with |zi ´ tji| ď Ri{4, i “ 1, . . . , d.
Consider the
piecewise polynomials P “ řm
j“1 Pj,γj1Γj with
Pj,γjptq “
ÿ
n.ďk
γj,npt ´ tjqn.
We obtain a ﬁnite set of functions Pa by discretizing the coeﬃcients γj,n for each
j and n over a grid of mesh width ǫ{Rn in the interval r´C{Rn, C{Rns, with Rn “
Rn1
1 . . . Rnd
d and C deﬁned as above. As in van der Vaart and van Zanten (2009), the
log cardinality of the set is bounded above by
log
˜ m
ź
j“1
ź
n:n.ďk
#γj,n
¸
ď mkd log
ˆ2C
ǫ
˙
.
(2.13)
We can choose m À 1{R˚.
The proof is complete if we show that the result-
ing set of functions is a Kǫ-net for constants C and K depending on ν and
k À logp1{ǫq.
The rest of the proof follows exactly as in the proof for Lemma
4.5 in van der Vaart and van Zanten (2009) by showing that
ˇˇˇˇˇ
ÿ
n.ąk
Dnhψptiq
n!
pz ´ tiqn
ˇˇˇˇˇ ď
ÿ
n.ąk
C
RnpR{2qn ď KC
ˆ2
3
˙k
(2.14)
and
ˇˇˇˇˇ
ÿ
n.ďk
Dnhψptiq
n!
pz ´ tiqn ´ Pi,γipzq
ˇˇˇˇˇ ď Kǫ.
(2.15)
The proof is completed by choosing k large enough such that p2{3qk ď Kǫ.
Lemma 19. For any a0 positive, there exists constants C and ǫ0 ą 0 such that for
a ě a0 and ǫ ă ǫ0,
´ log P
` ››Wa››
8 ď ǫ
˘
ď Ca˚
ˆ
log ¯a
ǫ
˙d`1
.
40

Proof. This follows from Theorem 2 in Kuelbs and Li (1993) and Lemma 4.6
in
van der Vaart and van Zanten
(2009).
Proceeding
as in
Lemma
4.6
in
van der Vaart and van Zanten (2009) and Lemma 18, we obtain
φapǫq ` log 0.5 ď K1a˚
ˆ
log φa
0 pǫq
ǫ
˙1`d
.
(2.16)
for some constant K1 ą 0. Note that with L “ r0, a1s ˆ ¨ ¨ ¨ ˆ r0, ads,
φa
0 pǫq “ ´ log Pp
››Wa››
8 ď ǫq
“
´ log Ppsup
tPL
|Wt| ď ǫq
(2.17)
ď
´ log Pp sup
tPr0, ¯asd |Wt| ď ǫq
(2.18)
ď
K2
ˆ¯a
ǫ
˙τ
,
(2.19)
for some constant K2 and τ ą 0, where the last inequality follows from the proof of
Lemma 4.6 in van der Vaart and van Zanten (2009). Inserting this bound in (2.16),
we obtain
´ log P
` ››Wa››
8 ď ǫ
˘
ď Ca˚
ˆ
log ¯a
ǫ
˙d`1
(2.20)
for some constant C ą 0.
We next state a nesting property of the unit ball Ha
1 of the RKHS of Wa for dif-
ferent values of a, generalizing Lemma 4.7 of van der Vaart and van Zanten (2009).
Lemma 20. Assume the spectral measure ν satisﬁes (2.3) and has a density f with
respect to the Lebesgue measure on Rd which satisﬁes fpt.{aq ď fpt.{bq for any
a ď b. Then,
?a1 . . . ad Ha
1 Ă
a
b1 . . . bd Hb
1 .
41

Proof. Let h P Ha
1 .
Following Lemma 16, hptq “
ş
eipλ,tqψpλqνapdλq.
Since
}h}2
Ha
“
}ψ}2
L2pνaq,
it follows that
ş
|ψpλq|2 fapλqdλ
ď
1.
Now,
hptq
“
ş
eipλ,tqtψpλqfapλq{fbpλquνbpdλq. The conclusion follows since,
}h}2
Hb “
ż
|ψpλq|2
"fapλq
fbpλq
*2
νbpdλq ď
››››
fapλq
fbpλq
››››
8
ż
|ψpλq|2 νapdλq ď a˚
b˚,
using the fact that fapλq{fbpλq “ pb˚{a˚qfpλ.{aq{fpλ.{bq ď pb˚{a˚q by assumption.
van der Vaart and van Zanten (2009) crucially used the above containment re-
lation among the RKHS unit balls in the single bandwidth case to conclude that
pr{δqd{2Hr
1 contains Ha
1 for all a in the interval rδ, rs. Combining this fact with the
observation that for very small values of a, the sample paths of W a behave like a
constant function, they could construct the sieves Bn containing MHa
1 ` ǫB1 for all
a P r0, rs without increasing the entropy from that of MHr
1 ` ǫB1. The complement
probability of Bn under the law of the rescaled process could also be appropriately
controlled by choosing r large enough so that PpA ą rq is small enough. However,
one doesn’t obtain a straightforward generalization of the above scheme to the multi-
bandwidth case since the entropy of the sieve blows up in trying to control the joint
probability of the rescaling vector a outside a hyper-rectangle in Rd
`.
The problem mentioned above is fundamentally due to the curse of dimensionality
and one needs a more careful construction of the sieve to avoid this problem. The
next three lemmas are crucially used in our treatment of the multi-bandwidth case.
In the proof of Lemma 18, a collection of piece-wise polynomials is used to cover
the unit RKHS ball Ha
1 . The main idea in the next set of lemmas is to exploit the
fact that the same set of piecewise polynomials can also be used to cover Hb
1 for b
suﬃciently close to a. Further, we shall carefully choose a compact subset Q of Rd
`
that balances the metric entropy of the collection of unit RKHS balls Ha
1 with a P Q
42

and the complement probability of Q under the joint prior on a.
Let Sp0q
d´1 denote the interior of Sd´1, i.e., all vectors θ P Rd
` with řd
j“1 θj “ 1 and
θj ą 0 for all j “ 1, . . . , d. For u P Rd
`, let Cu denote the rectangle in the positive
quadrant given by a ď u, i.e., 0 ď aj ď uj for all j “ 1, . . . , d. For a ﬁxed r ą 0,
let Q “ Qprq consist of vectors a with aj ď rθj for some θ P Sp0q
d´1. Clearly, Q is a
union of rectangles Crθ over θ P Sp0q
d´1. Clearly, the volume of each such rectangle Crθ
is r and the outer boundary of Q consists of points a with aj ď r for all j “ 1, . . . , d
and a˚ “ r (ﬁgure 2.1). By Lemma 18, for any such a in the outer boundary of
Q, the metric entropy of Ha
1 is bounded by a constant multiple of r logd`1p1{ǫq. In
the following, we show in Lemma 21 that the metric entropy of the collection of
unit RKHS balls with a varying over the outer boundary of Q is still of the order
of r logd`1p1{ǫq. Lemma 22 - 23 establish a stronger result which states that the
entropy remains of the same order even if the union is considered over all of Q.
(a) For ﬁxed r ą 1, rectangles Crθ “ t0 ď a ď
rθu for diﬀerent values of θ P Sp0q
d´1
(b) The region Q (shaded) resulting from the union
of all such rectangles
Figure 2.1: Union of rectangles Crθ “ t0 ď a ď rθu for diﬀerent θ
43

Lemma 21. For a positive number r ą 1 and θ P Sp0q
d´1, let Hr,θ
1
denote the unit ball
of the RKHS of Wa with aj “ rθj for 1 ď j ď d. Then, there exists a constant K1,
depending only on ν and d, such that, for ǫ ă 1{2,
log N
ˆ
ǫ,
ď
θPSp0q
d´1
Hr,θ
1 , }¨}8
˙
ď K1r
ˆ
log 1
ǫ
˙d`1
.
Proof. Let Q “ ta P Rd
` :
1 ď aj ď r @j “ 1, . . . , d, a˚ “ ru denote the outer
boundary of Q deﬁned above. Clearly,
ď
θPSp0q
d´1
Hr,θ
1
“
ď
aPQ
Ha
1 .
For a, b P Q, the idea of the proof is to show that the piecewise polynomials Pa that
form a Kǫ-net for Ha
1 in the proof of Lemma 18 are also a Kǫ-net for Hb
1 if b is
“close enough” to a.
Fix a P Q. Let Ωa “ tz P Cd : |Repzjq| ď Rj, j “ 1, . . . , du with Rj “ δ{p6aj
?
dq
denoting the strip in Cd on which every h P Ha
1 can be analytically extended. Let
b P Q satisfy maxj |aj ´ bj| ď 1. We shall show that any h P Hb
1 can also be extended
analytically to the same strip Ωa by showing that }2b ¨ Repzq}2 ă δ on Ωa. To that
end, for z P Ωa,
}2b ¨ Repzq}2 ď }2a ¨ Repzq}2 ` }2pb ´ aq ¨ Repzq}2
ď 2 }2a ¨ Repzq}2 ď 2δ{3.
where the penultimate inequality uses |bj ´ aj| ď 1 ď aj for all j “ 1, . . . , d.
Clearly, the same tail estimate as in (2.14) works for any h P Hb
1 . From (2.15),
it thus follows that the set of functions Pa form a Kǫ-net for Hb
1 . Let A be a set of
points in Q such that for any b P Q, there exists a P A such that maxj |aj ´ bj| ď 1.
One can clearly ﬁnd an A with |A| ď rd. The proof is completed by observing that
YaPAPa form a Kǫ net for YθPSd´1Hr,θ
1 .
44

Lemma 22. For u P Rd
`, let Cu denote the subset of Rd
` consisting of all vectors
a ď u, i.e., aj ď uj for all j “ 1, . . . , d. Then, there exists a constant K2, depending
only on ν and d, such that, for ǫ ă 1{2,
log N
ˆ
ǫ,
ď
aPCu
Ha
1 , }¨}8
˙
ď K2u˚
ˆ
log 1
ǫ
˙d`1
.
Proof. The idea of the proof is similar to Lemma 21 in that we partition the space Cr
into ﬁnitely many sets and cover the collection of unit RKHS balls with the scaling
vector varying over one of these sets by a single collection of piecewise polynomials.
We only sketch the partitioning scheme here and the rest of the proof is similar to
Lemma 21.
For a subset I of t1, . . . , du, let CIu denote the subset of Cu consisting of vectors
a ď u with aj ď 1 for all j P I and aj ą 1 for all j R I. Then, clearly Cu can be
written as the following disjoint union,
Cu “
dď
l“0
ď
I:|I|“l
CIu.
Fix 0 ď l ď d and a subset I of t1, . . . , du with |I| “ l. It suﬃces to prove the desired
entropy bound for CIu. We shall slightly modify the complex strip from the proof
of 18 to exploit that for any a P CIu, the values of aj for the coordinates j in I are
smaller than one.
Fix a P CIu. Let Ωa “ tz P Cd : |Repzjq| ď Rj, j “ 1, . . . , du with Rj “ δ{p6aj
?
dq
if j R I and Rj “ δ{p6
?
dq if j P I. Since }2a ¨ Repzq} ă δ for any z P Ωa, it follows
from the proof of Lemma 18 that any function h P Ha
1 has an analytic extension
to Ω. Let b P CIu satisfy maxj |aj ´ bj| ď 0.5. Then one can prove along the lines
of 21 that any h P Hb
1 can also be extended analytically to Ωa. The remainder of
the proof follows similarly as Lemma 22, where the net for CIu is constructed as the
union of the set of piecewise polynomials Pa covering Ha
1 , with a varying over a
45

ﬁnite subset of CIu with cardinality Opu˚q.
The following Lemma 23 follows along similar lines as the previous two lemmas.
Lemma 23. Let ν satisfy (2.3).
Fix r ě 1.
Then, there exists a constant K2
depending on ν and d only, so that, for ǫ ă 1{2,
log N
ˆ
ǫ,
ď
aPQprq
Ha
1 , }¨}8
˙
“ log N
ˆ
ǫ,
ď
θPSp0q
d´1
ď
aďrθ
Ha
1 , }¨}8
˙
ď K2r
ˆ
log 1
ǫ
˙d`1
.
2.5
Proof of main results
We shall only provide a detailed proof of Theorem 10 and sketch the main steps in
the proof of Theorem 12.
2.5.1
Proof of Theorem 10
Let us begin by observing that,
P
ˆ ›››WA ´ w0
›››
8 ď 2ǫ
˙
“
ż
Pp
››Wa ´ w0
››
8 ď 2ǫqπApdaq
“
ż " ż
Pp
››Wa ´ w0
››
8 ď 2ǫqπpa | θqda
*
πpθqdθ.
As in van der Vaart and van Zanten (2009), we ﬁrst derive bounds on the non-
centered small ball probability for a ﬁxed rescaling a, and then integrate over the
distribution of a to derive the same for WA.
Given a P Rd
`, recall the deﬁnition of the centered and non-centered concentration
functions of the process Wa,
φa
0 pǫq “ ´ log Pp
››Wa››
8 ď ǫq,
φa
w0pǫq “
inf
hPHa
1 :}h´w0}8ďǫ
}h}2
Ha ´ log Pp
››Wa››
8 ď ǫq.
(2.21)
46

For a ﬁxed a, the non-centered small ball probability of Wa can be bound in terms
of the concentration function as follows (van der Vaart and van Zanten, 2008b),
Pp
››Wa ´ w0
››
8 ď 2ǫq ě e´φa
w0pǫq.
Now, suppose that w0 P Cαr0, 1sd for some α P Rd
`. From Lemma 17 and 19,
it follows that for every a0 ą 0, there exist positive constants ǫ0 ă 1{2, C, D and E
that depend only on w0 and ν such that, for a ą a0, ǫ ă ǫ0 and C řd
i“1 a´αi
i
ă ǫ,
φa
w0pǫq ď Da˚ ` Ea˚
ˆ
log ¯a
ǫ
˙1`d
ď K1a˚
ˆ
log ¯a
ǫ
˙1`d
,
with K1 depending only on a0, ν and d. Thus, for ǫ ă mintǫ0, C1a´¯α
0 u, by (2.21), for
constants K2, . . . , K6 ą 0 and C2, . . . , C6 ą 0,
P
ˆ ›››WA ´ w0
›››
8 ď 2ǫ
˙
ě
ż
θ
" ż
e´φa
w0pǫqπpa | θqda
*
πpθqdθ
ě
ż
θ
" ż 2pC1{ǫq1{α1
a1“pC1{ǫq1{α1
¨ ¨ ¨
ż 2pC1{ǫq1{αd
ad“pC1{ǫq1{αd
e´K1a˚ log1`dp¯a{ǫqπpa | θqda
*
πpθqdθ
ě C2e´K2p1{ǫq1{α0 log1`dp1{ǫq
ż
θ
" ż 2pC1{ǫq1{α1
a1“pC1{ǫq1{α1
¨ ¨ ¨
ż 2pC1{ǫq1{αd
ad“pC1{ǫq1{αd
πpa | θqda
*
πpθqdθ.
Let Γ denote the region in the simplex Sd´1 given by Γ “ tθ P Sd´1 : τ ă θj ´ α0
α1 ă
2τ, j “ 1, . . . , d ´ 1u. Since řd
j“1 α0{αj “ 1, we can choose τ ą 0 small enough
to guarantee that any θ satisfying the set of inequalities lies inside the simplex.
Moreover, with θd “ 1 ´ řd´1
j“1 θj, one has pd ´ 1qτ ă θd ă 2pd ´ 1qτ. Choosing
τ “ C3{ logp1{ǫq, one can show that řd
j“1p1{ǫq1{pαjθjq ď C4p1{ǫq1{α0 for any θ P Γ.
47

Now,
ż " ż 2pC1{ǫq1{α1
a1“pC1{ǫq1{α1
¨ ¨ ¨
ż 2pC1{ǫq1{αd
ad“pC1{ǫq1{αd
πpa | θqda
*
πpθqdθ
ě
ż " ż 2pC1{ǫq1{α1
a1“pC1{ǫq1{α1
¨ ¨ ¨
ż 2pC1{ǫq1{αd
ad“pC1{ǫq1{αd
e´ řd
j“1 a
1{θj
j
da
*
πpθqdθ
ě
ż
e´K3
řd
j“1p1{ǫq1{αj θj πpθqdθ
ě
ż
θPΓ
e´K4p1{ǫq1{α0πpθqdθ ě C5e´K5p1{ǫq1{α0.
The last inequality in the above display uses that
ş
θPΓ πpθqdθ “
ż α0{α1´τ
θ1“α0{α1´2τ
¨ ¨ ¨
ż α0{αd´1´τ
θd´1“α0{αd´1´2τ
θβ1´1
1
. . . θβd´1´1
d´1
p1 ´
d´1
ÿ
j“1
θjqβd´1dθ1 . . . dθd´1
can be bounded below by a polynomial in τ91{ logp1{ǫq. Hence,
P
ˆ ›››WA ´ w0
›››
8 ď 2ǫ
˙
ě C6e´K6p1{ǫq1{α0 log1`dp1{ǫq.
(2.22)
Let B1 denote the unit sup-norm ball of Cr0, 1sd. For a vector θ P Sd´1 and
positive constants M, r, ǫ, let Bθ “ BθpM, r, ǫq denote the set,
Bθ “
ď
aďrθ
pMHa
1 q ` ǫB1,
where rθ denotes the vector whose jth element is rθj. We further let,
B “
ď
θPSd´1
ď
aďrθ
pMHa
1 q ` ǫB1.
Let us ﬁrst calculate the probability PpWA R Bθ | θq. Note that,
PpWa R Bθ | θq “
ż
PpW θ R Bθqπpa | θqda
ď
ż
aďrθ PpWa R Bθqπpa | θqda ` PpA ę rθ | θq,
48

where PpWA ę r | θq is a shorthand notation for Ppat least one Aj ą rθj | θq.
To tackle the ﬁrst term in the last display, note that Bθ contains the set MHa
1 `
ǫB1 for any a ď rθ by deﬁnition. Hence, for any a ď rθ, by Borell’s inequality,
PpWa R Bθq ď PpWa R MHa
1 ` ǫB1q
ď 1 ´ Φ
"
M ` Φ´1
ˆ
e´φa
0 pǫq
˙*
ď 1 ´ Φ
"
M ` Φ´1
ˆ
e´φrθ
0 pǫq
˙*
ď e´φrθ
0 pǫq,
if M ě ´2Φ´1`
e´φrθ
0 pǫq˘
, where the penultimate inequality follows from the fact that,
with T “ r0, 1sd,
e´φa
0 pǫq “ Pp sup
tPa¨T
|Wt| ď ǫq ě Pp sup
tPrθ¨T
|Wt| ď ǫq “ e´φrθ
0 pǫq.
By Lemma 4.10 of van der Vaart and van Zanten (2009), Φ´1puq ě ´t2 logp1{uqu1{2
for u P p0, 1q. Hence, the last inequality in the above display remains valid if we
choose
M ě 4
b
φrθ
0 pǫq.
Since A
1{θj
j
follows a gamma distribution given θj, in view of Lemma 4.9 of
van der Vaart and van Zanten (2009), for r larger than a positive constant depending
only on the parameters of the gamma distribution,
PpAj ą rθj | θq ď C1rD1e´D2r.
Combining the above, since B contains Bθ for every θ P Sd´1,
PpWA R Bq “
ż
θ
" ż
PpWa R B | θqgpa | θq
*
ď
ż
θ
" ż
PpWa R Bθ | θqgpa | θq
*
ď C2rD1e´D2r ` e´D3r logpr{ǫqd`1.
(2.23)
49

From Lemma 23, the entropy of B can be estimated as,
log Np2ǫ, B, }¨}8q ď log Npǫ,
ď
θPSd´1
ď
aďrθ
pMHa
1 q, }¨}8q
ď r log
ˆM
ǫ
˙d`1
.
(2.24)
Thus (2.22), (2.23) and (2.24) can be simultaneously satisﬁed if we choose, for con-
stants κ, κ1, κ2 ą 0,
ǫn “ n´α0{p2α0`1q logκpnq,
rn “ n1{p2α0`1q logκ1pnq,
Mn “ rn logκ2pnq.
2.5.2
Proof of Theorem 12
For ease of notation, we shall make the simplifying assumption that the random
variable B is degenerate at 1. For a ą 0 and S Ă t1, . . . , du, let Ha,S denote the
RKHS of Wa, where aj “ a for j P S and aj “ 1 for j R S.
For a subset S Ă t1, . . . , du with |S| “ ˜d, and given positive constants M, r, ξ, ǫ,
let
BS “ BSpM, r, ξ, ǫq
“
„
M
`r
ξ
˘ ˜dHr,S
1
` ǫB1
ď „ ď
aăξ
`
MHa,S
1
˘
` ǫB1

.
Since, given S, A
˜d „ gamma, it can be shown that, for some constant C1 ą 0,
PpWA R BS | Sq À e´C1rd˚
.
The dominating term in the ǫ entropy of BS is bounded by
C2rd˚ log1`d
ˆC3M
ǫ
˙
.
50

While calculating the concentration probability around w0 P Cαr0, 1sI, simply use
the fact that prpS “ Iq ą 0.
Combining the above, the sieves Bn are constructed as,
Bn “
dď
˜d“1
ď
S:|S|“ ˜d
BSpMS
n , rS
n, ξn, ǫnq,
where, for constants κ, κ1 ą 0,
ǫn “ n´α{p2α`d0q logκ n,
rS
n “
ˆ
n
d0
2α`d0
˙1{|S|
logκ1pnq,
pMS
n q2 “ prS
nq
˜d logprS
n{ǫnq.
2.6
Lower bounds on posterior contraction rates
In this section, we will demonstrate that when the true density is dependent on a
smaller number of variables, a Gaussian process prior with a single bandwidth leads
to a sub-optimal rate of convergence. To illustrate this, we will focus on the example
of density estimation using the logistic Gaussian process prior. We will show that
the posterior contraction rate using a single bandwidth logistic Gaussian process
with respect to the sup-norm topology is bounded below by n´α{p2α`dq when the true
density is
f0px1, . . . , xdq “ Ce|x1´0.5|1.5, x “ px1, . . . , xdq
T P r0, 1sd.
(2.25)
This shows the necessity of using an inhomogeneous Gaussian process in high-
dimensional density estimation when the true density is actually lower dimensional.
Although lower bounds on the posterior contraction rates in Gaussian process set-
tings have been previously addressed by Castillo (2008), the literature is restricted
to series expansion priors and the Riemann-Liouville process priors. In this section,
51

we have extended the results to Gaussian process with exponential covariance kernel
having a single bandwidth. In particular, we have derived a lower bound to the
concentration function around w0px1, . . . , xdq “ |x1 ´ 0.5|1.5 using a single inverse-
gamma bandwidth.
In the following, we shall consider a rescaled Gaussian process W A for a positive
random variable A stochastically independent of W. Recall that the logistic Gaussian
process prior for a density f on r0, 1sd is given by
fpxq “
exptW Apxqu
ş
r0,1sd exptW Aptqudt, x P r0, 1sd.
(2.26)
We shall consider a prior distribution on A speciﬁed by Ad˚ „ g, where g is the
gamma density and d˚ P t1, . . . , du. Recall that a gamma prior on Ad results in
the minimax rate of contraction adaptively over log f being an isotropic α-H¨older
function of d variables for any α ą 0. We shall show below that the above spec-
iﬁcation involving a single bandwidth leads to sub-optimal rate for any choice of
d˚ P t1, . . . , du if log f0 depends on fewer coordinates.
We will start with a few auxiliary lemmas which enable us to provide an lower
bound to the concentration function of the Gaussian process W A. First we derive a
lower bound to the concentration function φapǫq for a ﬁxed a and then marginalize
with respect to the prior for a. The lower bound coupled with the ability of the
model (2.26) to identify the Gaussian process term W A from w0 results in a lower
bound to the posterior concentration rate. The key to obtaining a lower bound for
the concentration function φapǫq is to ﬁnd a lower bound to ´ log Pp}W a}8 ď ǫq.
However, it is important to note here that one can’t just obtain a lower bound to
the marginalized concentration function by marginalizing over ´ log Pp}W a}8 ď ǫq.
It becomes necessary to carefully characterize the domain of a in terms of the ǫ for
which there exists an element in Ha in an ǫ-sup-norm neighborhood of w0. Lemma
52

25-27 serve to ﬁnd this domain by searching for the best approximator of w0 in
Ha. In conjunction with our intuition, the obtained domain is rC0ǫ´1{α, 8q for some
global constant C0.
This fact immediately provides a sharp lower bound to the
marginalized concentration function which turns out to be of the same order as the
upper bound up to a log-factor. Thus it is of no surprise that one can only achieve a
sub-optimal rate of posterior convergence using a single bandwidth logistic Gaussian
process prior.
Denote by Ha the reproducing kernel Hilbert space of the Gaussian process
W a.
In the following, we deﬁne a Gaussian based higher order kernel as in
Wand and Schucany (1990).
For r ě 1, let Q2r´2 be the polynomial given by
Q2r´2pxq “ řr´1
i“0 c2ix2i where
c2i “
p´1qi2i´2r`1p2rq!
r!p2i ` 1q!pr ´ i ´ 1q!.
Wand and Schucany (1990) showed that Q2r´2 is the unique polynomial of degree
ď 2r ´ 2 for which G2r ” Q2r´2φ is a 2r order kernel. It is easy to see that r “ 1
corresponds to the standard Gaussian kernel. For r ą 1 and any 1 ď j ď r ´ 1,
ş
R x2jG2rpxq “ 0.
For x P Rd, deﬁne ψ2rpxq “ G2rpx1q . . . G2rpxdq and for a ą 0, let ψ2r
a pxq “
adψ2rpaxq.
In the following Lemma 24, we calculate the Fourier transform of ψ2rptq.
Lemma 24. ˆψ2rpλq “ e´}λ}2{2 śd
j“1
„ řr´1
s“0
λ2s
j
2ss!

.
53

Proof.
ˆψ2rpλq
“
ż
eipλ,tqψ2rptqdt
“
ż
eipλ,tqG2rpt1q ¨ ¨ ¨ G2rptdqdt
“
d
ź
j“1
ż
eipλj,tjqG2rptjqdtj
“
d
ź
j“1
e´λ2
j{2
r´1
ÿ
s“0
λ2s
j
2ss!
“
e´}λ}2{2
d
ź
j“1
r´1
ÿ
s“0
λ2s
j
2ss!
where the penultimate identity follows from Wand and Schucany (1990).
Lemma 4.1 of van der Vaart and van Zanten (2009) gives a nice characterization
of Ha in view of the isometry with the space L2pνaq. In the following Lemma 25,
we express each element of Ha as a convolution of ψ2r
a
with a function in CpRdq
for any given r ě 1. In other words, every element of Ha arises as a convolution
of a higher order kernel with a function in CpRdq showing that the search for the
best approximator of a Cαr0, 1s function in the space Ha can be restricted to only
convolutions of continuous functions with a higher order kernel.
Lemma 25. Given any h P Ha and r ě 1, there exists w P CpRdq such that h “
ψ2r
2a ˚ w.
Proof. By Lemma 4.1 of van der Vaart and van Zanten (2009), we obtain that any
h P Ha can be written as
t Ñ
ż
eipλ,tqgpλqfapλqdλ,
(2.27)
54

where
ş
gpλq2fapλqdλ ă 8.
By change of variable,
hptq “
ż
e´ipλ,tqgp´λqfapλqdλ,
(2.28)
with
ş
gp´λq2fapλqdλ ă 8. Then ˆhpλq “ p2πqdgp´λqfapλq. Now observe that ˆψ2rpλq
is real and positive for all values of t and ˆψ2rpλq ą e´}λ}2{2. Also note that ˆψ2r
2apλq “
ˆψ2rpλ{2aq. Hence setting ˆwpλq “
ˆhpλq
ˆψ2r
2apλq, we obtain
ˆwpλq “
gp´λqπd{2 expt´ }λ}2 {4a2u
expt´ }λ}2 {8a2u śd
j“1
řr´1
s“0
λ2s
j
p2aq2s2ss!
.
Thus | ˆwpλq| ď expt´ }λ}2 {8a2u |gp´λq| and
" ż
| ˆwpλq| dλ
*2
ď
" ż
expt´ }λ}2 {8a2u |gp´λq|dλ
*2
ď
ż
expt´ }λ}2 {4a2u |gp´λq|2 dλ
ă
8.
As ˆw belongs to L1, and ˆh “ ˆ
ψ2r
2a ˆw, we immediately have h “ ψ2r
2a ˚w for a continuous
function w given by
wptq
“
1
p2πqd
ż
e´ipλ,tq ˆwpλqdλ
“
1
p2πqd
ż
e´ipλ,tq
gp´λqπd{2 expt´ }λ}2 {4a2u
expt´ }λ}2 {8a2u śd
j“1
řr´1
s“0
λ2s
j
p2aq2s2ss!
dλ.
The following Lemma 26 says that ψ2r
a ˚ w0 can better approximate w0 P CpRdq
compared to ψ2r
a ˚ w for any w ‰ w0. Lemma 26 further restricts the search for
55

the best approximator of a CpRdq function to only convolutions of the higher order
kernel ψ2r
a with the function w0 itself.
Lemma 26. Given any w0 P CpRdq compactly supported and r ě 1,
››w0 ´ ψ2r
a ˚ w0
››
8 ď
››w0 ´ ψ2r
a ˚ w
››
8
for suﬃciently large a ą 0 and for any w P CpRdq compactly supported with
}w ´ w0} ą δ for some δ ą 0.
Proof. Note that
››ψ2r
a ˚ w ´ w0
››
8 ě }w ´ w0}8 ´
››φ2r
a ˚ w ´ w
››
8 .
Since w is compactly supported, there exists a0 ą 0 such that for a ą a0,
}φ2r
a ˚ w ´ w}8 ă δ{2. The conclusion of the lemma follows by observing that for
a ą a0, }ψ2r
a ˚ w ´ w0}8 ą δ{2.
The following Lemma 27 provides a lower bound to the approximation error for
w0px1, . . . , xdq “ |x1 ´ 0.5|1.5 , px1, . . . , xdq P r0, 1sd with ψ2
a ˚ w0.
Lemma 27. For w0px1, . . . , xdq “ |x1 ´ 0.5|1.5,
››w0 ´ ψ2
2a ˚ w0
››
8 ě C0a´1.5
(2.29)
for some global constant C0 ą 0.
Proof. Since w0 P C1.5r0, 1sd, by Whitney’s theorem we can extend it to Rd so that
w0 has a compact support with }w0}1.5 ă 8. Without loss of generality, assume w0
is non-negative and the support of w0 is r´L, Lsd for some large L. Observe that
ψ2
2a ˚ w0p1{2q ´ w0p1{2q
“
ż
ψ2psqw0p1{2 ´ s{p2aqqds
56

Now since w0p1{2 ´ s{p2aqq “ 0 if |1{2 ´ s{p2aq| ą L, so for a ą 1{2, ts :
|1{2 ´ s{p2aq| ď Lu Ą r´2L ` 1, 2L ` 1sd. Thus
ż
ψ2psqw0p1{2 ´ s{p2aqqds
ě
ż
r´2L`1,2L`1sd ψ2psqw0p1{2 ´ s{p2aqqds
“
1{p2aq1.5
ż
r´2L`1,2L`1sd ψ2psq |s1|1.5 ds.
This shows that }w0 ´ ψ2
2a ˚ w0}8 ě C0a´1.5 where
C0 “
1
21.5
ż
r´2L`1,2L`1sd ψ2psq |s1|1.5 ds.
Also it follows from the last part of Lemma 4.3 of van der Vaart and van Zanten
(2009) that ψ2
2a ˚ w0 P Ha since p ˆψ2
2aq2pλq “ fapλq.
Note that the lower bound obtained is same as the upper bound to the approxi-
mation error of any C1.5r0, 1s function using ψ2
2a ˚ w upto constants.
The following Lemma 28 is crucial to the derivation of a lower bound
to the concentration function φapw0q.
Lemma 28 complements Lemma 4.6
of van der Vaart and van Zanten (2009) and is an application of Theorem 2 of
Kuelbs and Li (1993).
Lemma 28. There exists ǫ0 ą 0, possibly depending on a, such that for all ǫ ă ǫ0,
´ log Pp}W a}8 ă ǫq Á ad log
˜
|log ǫ|1{2
ǫ
¸d`1
.
(2.30)
Proof. Obtaining a lower bound is a simple application of Lemma 4.5 of
van der Vaart and van Zanten (2009) and Theorem 2 of Kuelbs and Li (1993). The
proof of Lemma 4.3 of van der Vaart and van Zanten (2009) shows that
log Npǫ, Ha
1, }¨}8q « ad
ˆ
log 1
ǫ
˙d`1
.
57

If we deﬁne gapxq “ ad`
log 1
x
˘d`1, it is easy to observe that g is a slowly varying
function. Then by Theorem 2 of Kuelbs and Li (1993), we obtain
φa
0pǫq ě C1ga
ˆ
ǫ
a
φa
0pǫq
˙
“ ad
ˆ
log
a
φa
0pǫq
ǫ
˙d`1
.
(2.31)
Below we show that we only need to ﬁnd a crude lower bound to φa
0pǫq to obtain the
required bound. Observe that
φa
0pǫq “ ´ log Pp}W a}8 ď ǫq ě ´ log Pp
ˇˇW 0ˇˇ ď ǫq.
(2.32)
Note that W 0 „ Np0, 1q and hence Pp|W 0| ď ǫq “ t2Φpǫq ´ 1u « 1 ` |log ǫ| as ǫ Ñ 0.
Hence we obtain for suﬃciently small ǫ,
φa
0pǫq Á |log ǫ| .
(2.33)
Plugging in the bound (2.33) in (2.31), we obtain
φa
0pǫq Á ad log
˜
|log ǫ|1{2
ǫ
¸d`1
.
(2.34)
Note that the lower bound in Lemma 28 diﬀers from the upper bound in Lemma
4.6 of van der Vaart and van Zanten (2009) only by a logarithmic factor suggesting
that the lower bound obtained is reasonably tight.
Finally, we calculate the tail probability of the supremum of the Gaussian process
W A which will be crucially used to derive a lower bound to the posterior concentra-
tion rate. Although this is an application of Borell’s Inequality, we will provide an
independent proof to carefully identify the role of the prior for the bandwidth.
58

Lemma 29. For r ą 1,
P
` ››W A››
8 ą M
˘
ď
PpA ą rq ` 2paMqd exp
„
´ 1
2M2 ` Ctplog rq1{2 ` plog Mq1{2u

for some constant C ą 0.
Proof. From Theorem 5.2 of Adler (1990) it follows that if X is a centered Gaussian
process on a compact set T Ă Rd and σ2
T is the maximum variance attained by the
Gaussian process on T, then for large M,
Pp}X}8 ą Mq ď 2Np1{M, T, }¨}q exp
„
´
1
2σ2
T
tM ´ νpMqu2

,
where νpMq “ C1
ş1{M
0
tlog Np1{M, T, }¨}qu1{2dp1{Mq for some constant C1 ą 0. Ob-
serve that W a is rescaled to T “ r0, asd and the maximum variance attained by W a
is 1. Note that Np1{M, T, }¨}q “ paMqd. Now
νpMq
ď
C2
ż 1{M
0
td logpaMqu1{2dp1{Mq
ď
C3
ż 1{M
0
tplog aq1{2 ` plog Mq1{2udp1{Mq
ď
C3
1
M tplog aq1{2 ` plog Mq1{2u
for some constants C2, C3 ą 0. Using W a in place of X, we obtain,
Pp}W a}8 ą Mq ď 2paMqd exp
„
´ 1
2M2 ` C3tplog aq1{2 ` plog Mq1{2u

The conclusion of the lemma follows immediately.
59

2.7
Main result
Below we state the main theorem on obtaining a lower bound to the posterior con-
centration rate using a logistic Gaussian process prior when the true density is given
by (2.25). Since w0 is a C1.5r0, 1sd function, the best obtainable upper bound to
the posterior rate of convergence using a single bandwidth logistic Gaussian pro-
cess prior is n´1.5{p3`dq “ n´3{p6`2dq upto a log factor (van der Vaart and van Zanten,
2009). In the following Theorem 30, we show that the lower bound using the sup-
norm topology is also of the same order if we use a single bandwidth. In other words,
it is impossible for a single bandwidth Gaussian process to optimally learn the lower
dimensional density.
Theorem 30. If f0 is given by (2.25) and the prior for a density f on r0, 1sd is given
as in (2.26) for any d˚ P t1, . . . , du, then
Pp}f ´ f0}8 ď n´3{p6`2dq logt0 n | Y1, . . . , Ynq Ñ 0
(2.35)
a.s. as n Ñ 8 for some constant t0 ą 0.
Proof. To obtain the lower bound, we will verify the conditions of Lemma 1 in Castillo
(2008) with Bn “ tf : }f ´ f0}8 ď ξnu for ξn “ n´3{p6`2dq logt0 n for some constant
t0 chosen appropriately in the subsequent analysis. From the proof of Lemma 5 in
Castillo (2008) it follows that for ck “ kdξn, k “ ´N, . . . , N and N the smallest
integer larger than C?n,
P
`
}f ´ f0}8 ď ξn
˘
ď
N
ÿ
k“´N
P
` ››W A ´ w0 ´ ck
››
8 ď 2dξn
˘
` P
` ››W A››
8 ą C?nξn
˘
.
(2.36)
60

An application of Lemma 29 with M2
n, rd˚
n “ Opnξ2
nq yields
P
` ››W A››
8 ą C?nξn
˘
ď
PpA ą rnq ` expt´K1M2
nu
ď
expt´rd˚
n u ` expt´K1M2
nu
ď
expt´K2nξ2
nu,
(2.37)
for some constants K1, K2 ą 0.
Lemma 25-27 and the observation that w0 R C1.5`δr0, 1sd for any δ ą 0 together
imply that given any ǫ ą 0, there does not exist any element in Ha for a ă C0ǫ´1{α
such that for each k “ ´N, . . . , N,
}w0 ´ h ´ ck}8 ă ǫ,
where w0 is given by w0px1, . . . , xdq “ |x1 ´ 0.5|1.5. From Lemma 28, if a ą C0ǫ´1{α,
φa
w0`ckpǫq
ě
inf
hPHa:}h´w0´ck}8ăǫ
1
2 }h}2
H ` ad log
ˆ|log ǫ|1{2
ǫ
˙d`1
ě
ad log
ˆ|log ǫ|1{2
ǫ
˙d`1
.
Hence for k “ ´N, . . . , N,
P
ˆ ››W A ´ w0 ´ ck
››
8 ă ǫ
˙
ď
ż 8
a“C0ǫ´1{α exp
"
´ ad log
ˆ|log ǫ|1{2
ǫ
˙d`1*
da.
Using the inequality
ż 8
v
expt´trudt ď 2r´1v1´r expt´vru,
we obtain that
P
ˆ ››W A ´ w0 ´ ck
››
8 ă ǫ
˙
ď C1 expt´C2ǫ´d{α |log ǫ|d`1u,
61

for some constants C1, C2 ą 0. Thus, from (2.37) and (2.36),
P
`
}f ´ f0}8 ď ξn
˘
ď C3N expt´C4ξ´d{α
n
u,
(2.38)
for some constant C3 ą 0. From van der Vaart and van Zanten (2009) it also follows
that
PpBKLpf0, ξnqq ě e´C5nξ2
n,
(2.39)
for some constant t0 ą 0 and C5 ą 0 where
BKLpf0, ǫq “
"
f :
ż
f0 log f0
f ă ǫ2,
ż
f0
ˆ
log f0
f
˙2
ă ǫ2
*
.
(2.40)
By adjusting t0, C4 and C5, we have from (2.38) and (2.39)
P
`
}f ´ f0}8 ď ξn
˘
PpBKLpf0, ξnqq
ď expt´2nξ2
nu,
which proves the assertion of the theorem by Lemma 1 of Castillo (2008).
Remark 31. Note that the lower bound n´3{p6`2dq logt0 n for d ą 1 is only a sub-
optimal rate for estimating w0, the optimal rate being given by n´3{8 which is actually
achieved by a multi-bandwidth Gaussian process prior.
Refer to Theorem 15 for
details.
Remark 32. Note that we have derived a lower bound to the posterior contraction
rate only for this special choice of f0 given in 2.25. The choice is motivated by the fact
that it is easy to ﬁnd a lower bound to the best approximation error of this function
within the class Ha. More generally one might be interested in ﬁnding a subset of
Cαr0, 1sd for a ﬁxed α ą 0 such that we can characterize both the best approximator
and a lower bound to the approximation error for each of the elements in the subset.
This would require a diﬀerent version of Lemma 27 in each of the cases. However
the general recipe provided in Lemma 25–27 remains the same.
62

Remark 33. One can also obtain a lower bound to the posterior concentration rate
in other statistical settings, e.g., the Gaussian process mean regression using the
same technique. This would need careful characterization of the upper bound to the
concentration probability of the induced density around the truth i.e., Pp}f ´ f0}8 ă
ξnq in terms of the concentration probability of the Gaussian process W A around w0
similar to that for the logistic Gaussian process in Theorem 30. Interested readers
might ﬁnd an outline of such an exercise in Section 7.7 of Ghosal and van der Vaart
(2007a).
63

3
Bayesian nonparametric regression with varying
residual density
3.1
Introduction
Nonparametric regression oﬀers a more ﬂexible way of modeling the eﬀect of co-
variates on the response compared to parametric models having restrictive assump-
tions on the mean function and the residual distribution.
Here we consider a
fully Bayesian approach. The response y P Y corresponding to a set of covariates
x “ px1, x2, . . . , xpq1 P X can be expressed as
y “ ηpxq ` ǫ
(3.1)
where ηpxq “ Epy | xq is the mean regression function under the assumption that
the residual density has mean zero, i.e., Epǫ | xq “ 0 for all x P X . Our focus is on
obtaining a robust estimate of η while allowing heavy tails to down-weight inﬂuential
observations. We propose a class of models that allows the residual density to change
nonparametrically with predictors x, with homoscedasticity arising as a special case.
There is a substantial literature proposing priors for ﬂexible estimation of the
mean function, typically using basis function representations such as splines or
64

wavelets (Denison et al., 2002). Most of this literature assumes a constant resid-
ual density, possibly up to a scale factor allowing heteroscedasticity. Yau and Kohn
(2003) allow the mean and variance to change with predictors using thin plate splines.
In certain applications, this structure may be overly restrictive due to the speciﬁc
splines used and the normality assumption. Chan et al. (2006) also used splines for
heteroscedastic regression, but with locally adaptive estimation of the residual vari-
ance and allowance for uncertainty in variable selection. Nott (2006) considered the
problem of simultaneous estimation of the mean and variance function by using pe-
nalized splines for possibly non Gaussian data. Due to the lack of conjugacy, these
methods rely on involved sampling techniques using Metropolis Hastings, requiring
proposal distributions to be chosen that may not be eﬃcient in all cases. The residual
density is assumed to have a known parametric form and heavy-tailed distributions
have not been considered. In addition, since basis function selection for multiple
predictors is highly computationally demanding, additive assumptions are typically
made that rule out interactions.
Gaussian process (GP) regression (Adler, 1990; Ghoshal and Roy, 2006; Neal,
1998) is an increasingly popular choice, which avoids the need to explicitly choose
the basis functions, while having many appealing computational and theoretical
properties. For articles describing some of these properties, refer to Adler (1990),
Cram´er and Leadbetter (1967) and van der Vaart and Wellner (1996). A wide vari-
ety of functions can arise as the sample paths of the Gaussian process. GP priors can
be chosen that have support on the space of all smooth functions while facilitating
Bayes computation through conjugacy properties. In particular, the GP realizations
at the data points are simply multivariate Gaussian. As shown by Choi and Schervish
(2007b), GP priors also lead to consistent estimation of the regression function under
normality assumptions on the residuals. Recently, Choi (2009) extended their results
to allow for non-Gaussian symmetric residual distributions (for example, the Laplace
65

distribution) which satisfy certain regularity conditions and the induced conditional
density belongs to a location-scale family. Although they require mild assumptions
on the parametric scale family, the results depend heavily on parametric assump-
tions. In particular, their theory of posterior consistency is not applicable to an
inﬁnite mixture prior on the residual density. We extend their result allowing a rich
class of residual distributions through PSB mixtures of Gaussians in Section 3.3.
There is a rich literature on Bayesian methods for density estimation using mix-
ture models of the form
yi „ fpθiq,
θi „ P,
P „ Π,
(3.2)
where fp¨q is a parametric density and P is an unknown mixing distribution as-
signed a prior Π. The most common choice of Π is the Dirichlet process (Ferguson,
1973b, 1974b). Lo (1984) showed that Dirichlet process mixtures of normals have
dense support on the space of densities with respect to Lesbesgue measure, while
Escobar and West (1995) developed methods for posterior computation and infer-
ence. James et al. (2005) considered a broader class of normalized random measures
for Π.
In order to combine methods for Bayesian nonparametric regression with meth-
ods for Bayesian density estimation, one can potentially use mixture model (3.2) for
the residual density in (8.1). A number of authors have considered nonparametric
priors for the residual distribution in regression. For example, Kottas and Gelfand
(2001) proposed mixture models for the error distributions in median regression mod-
els. To ensure identiﬁability of the regression coeﬃcients, the residual distribution
is constrained to have median zero.
Their approach is very ﬂexible but has the
unappealing property of producing a residual density that is discontinuous at zero.
In addition, the approach of mixing uniforms leads to blocky looking estimates of
the residual density particularly for sparse data. Lavine and Mockus (2005) allow
66

both a regression function for a single predictor and the residual distribution to be
unknown subject to a monotonicity constraint.
A number of recent papers have
focused on generalizing model (3.2) to the density regression setting in which the
entire conditional distribution of y given x changes ﬂexibly with predictors. Refer,
for example, to M¨uller et al. (1996); Griﬃn and Steel (2006b, 2010); Dunson et al.
(2007b) and Dunson and Park (2008b) among others. Bush and MacEachern (1996)
is contemporary with M¨uller et al. (1996) and is concerned with nonparametrically
estimating the random block eﬀects in an anova-type mean linear-regression model
with a t-residual density rather than density regression.
Although these approaches are clearly highly ﬂexible, there are several issues that
provide motivation for this article. First, to simplify inferences and prior elicitation,
it is appealing to separate the mean function ηpxq from the residual distribution in
the speciﬁcation, which is accomplished by only a few density regression methods.
The general framework of separately modeling the mean function and residual dis-
tribution nonparametrically was introduced by Griﬃn and Steel (2010). They allow
the residual distribution to change ﬂexibly with predictors using the order-based
Dirichlet process (Griﬃn and Steel, 2006b). On the other hand, we want to able to
have a computationally simpler speciﬁcation with straightforward prior elicitation.
Chib and Greenberg (2010) develops a nonparametric model jointly for continuous
and categorical responses where they model the mean of the link function and resid-
ual density separately. The mean is modeled using ﬂexible additive splines and the
residual density is modeled using a DP scale mixture of normals.
However they
didn’t allow the residual distribution to change ﬂexibly with the predictors. Often
we have strong prior information regarding the form of the regression function. Most
of the current density regression models do not allow the incorporation of prior in-
formation without being overparametrized. Second, in many applications, the main
interest is in inference on η or in prediction, and the residual distribution can be con-
67

sidered as a nuisance. Third, we would like to be able to provide a speciﬁcation with
theoretical support. By placing some constraints on the support we may achieve a
gain in eﬃciency in estimating the regression function, since density regression pro-
cedures are almost too ﬂexible. In particular, it would be appealing to show strong
posterior consistency in estimating η without requiring restrictive assumptions on η
or the residual distribution. Current density regression models lack such theoretical
support. In addition, computation for density regression can be quite involved, par-
ticularly in cases involving more than a few predictors, and one encounters the curse
of dimensionality in that the speciﬁcations are almost too ﬂexible. Our goal is to ob-
tain a computationally convenient speciﬁcation that allows consistent estimation of
the regression function, while being ﬂexible in the residual distribution speciﬁcation
to obtain robust estimates.
To accomplish this, we propose to place a Gaussian process prior on η and to allow
the residual density to be unknown through a probit stick-breaking (PSB) process
mixture. The basic PSB process speciﬁcation was proposed by Chung and Dunson
(2009) in developing a density regression approach that allows variable selection.
On the other hand we are concerned with robust estimation of the mean regression
function allowing the residual distribution to change ﬂexibly with predictors. While
we want to model the mean regression function nonparametrically, we also want to
be able to incorporate our prior knowledge for the regression function quite easily as
opposed to density regression models which are often a black box. Here, we propose
four novel variants of PSB mixtures for the residual distribution. The ﬁrst uses a scale
mixture of Gaussians to obtain a prior with large support on unimodal symmetric
distributions. The next is based on a symmetrized location-scale PSB mixture, which
is more ﬂexible in avoiding the unimodality constraint, while constraining the residual
density to be symmetric and have mean zero. In addition, we show that this prior
leads to strong posterior consistency in estimating η under weak conditions. To allow
68

the residual density to change ﬂexibly with predictors, we generalize the above priors
through incorporating probit transformations of Gaussian processes in the weights.
The last two prior speciﬁcations allow changing residual variances and tail heav-
iness with predictors, leading to a highly robust speciﬁcation which is shown to
have better performance in simulation studies and out of sample prediction. It will
be shown in some small sample simulated examples that the heteroscedastic sym-
metrized location-scale PSB mixture leads to even more robust inference than the
heteroscedastic scale PSB mixture without compromising out of sample predictive
performance.
Section 3.2 proposes the class of models under consideration. Section 3.3 shows
consistency properties. Section 3.4 develops eﬃcient posterior computation through
an exact block Gibbs sampler. Section 3.5 describes measures of inﬂuence to study
robustness properties of our proposed methods. Section 3.6 contains simulation study
results, Section 3.7 applies the methods to the Boston housing data and body fat
data, and Section 3.8 discusses the results. Proofs are included in the Appendix.
3.2
Nonparametric regression modeling
3.2.1
Data Structure and Model
Consider n observations with the ith observation recorded in response to the covariate
xi “ pxi1, xi2, . . . , xipq1.
Let X “ px1, . . . , xnq1 be the predictor matrix for all n
subjects. The regression model can be expressed as
yi “ ηpxiq ` ǫi,
ǫi „ fxi, i “ 1, . . . , n.
We assume that the response y P Y is continuous and x P X where X Ă Rp is
compact. Also, the residuals ǫi are sampled independently, with fx denoting the
residual density speciﬁc to predictor value xi “ x. We focus initially on the case in
which the covariate space X is continuous, with the covariates arising from a ﬁxed,
69

non-random design or consisting of i.i.d realizations of a random variable. We choose
a prior on the regression function ηpxq that has support on a large subset of C8pX q,
the space of smooth real valued X Ñ R functions. The priors proposed for tfx, x P
X u will be chosen to have large support so that heavy-tailed distributions and outliers
will automatically be accommodated, with inﬂuential observations downweighted in
estimating η.
3.2.2
Prior on the Mean Regression Function
We assume that η P F “ tg : X Ñ R is a continuous functionu, with η as-
signed a Gaussian process (GP) prior, η „ GPpµ, cq, where µ is the mean func-
tion and c is the covariance kernel.
A Gaussian process is a stochastic process
tηpxq : x P X u such that any ﬁnite dimensional distribution is multivariate normal,
i.e., for any n and x1, . . . , xn, ηpXq :“ pηpx1q, . . . , ηpxnqq1 „ NpµpXq, Σηq, where
µpXq “ pµpx1q, . . . , µpxnqq1 and Ση
ij “ cpxi, xjq.
Naturally the covariance kernel
cp¨, ¨q must satisfy, for each n and x1, . . . , xn, that the matrix Ση is positive deﬁnite.
The smoothness of the covariance kernel essentially controls the smoothness of the
sample paths of tηpxq : x P X u. For an appropriate choice of c, a Gaussian process
has large support in the space of all smooth functions. More precisely, the support
of a Gaussian process is the closure of the reproducing kernel Hilbert space gener-
ated by the covariance kernel with a shift by the mean function (Ghoshal and Roy,
2006). For example, when X Ă R, the eigenfunctions of the univariate covariance
kernel, cpx, x1q “
1
τ e´κpx´x1q2, span C8pX q if κ is allowed to vary freely over R`.
Thus we can see that the Gaussian process prior has a rich class of functions as its
support and hence is appealing as a prior on the mean regression function. Refer
to Rasmussen and Williams (2005) and van der Vaart and van Zanten (2008b) as an
introductory textbook on Gaussian processes.
We follow common practice in choosing the mean function in the GP prior to
70

correspond to a linear regression, µpXq “ Xβ, with β denoting unknown regression
coeﬃcients. As a commonly used covariance kernel, we took the Gaussian kernel
cpx, x1q “ 1
τ e´κ||x´x1||2, where τ and κ are unknown hyperparameters, with κ control-
ling the local smoothness of the sample paths of ηpxq. Smoother sample paths imply
more borrowing of information from neighboring x values.
3.2.3
Priors for Residual Distribution
Motivated by the problem of robust estimation of the regression function η, we
consider ﬁve diﬀerent types of priors for the residual distributions tfx, x P X u
as enumerated below.
The ﬁrst prior corresponds to the t distribution, which is
widely used for robust modeling of residual distributions (West, 1984; Lange et al.,
1989; Fonseca et al., 2008), while the remaining priors are ﬂexible nonparametric
speciﬁcations.
1.
Heavy tailed parametric error distribution: Following many previous
authors, we ﬁrst consider the case in which the residual distributions follow a
homoscedastic Student-t distribution with unknown degrees of freedom.
As the
Student-t with low degrees of freedom is heavy tailed, outliers are allowed.
By
placing a hyperprior on the degrees of freedom, νσ „ Gapaν, bνq, with Gapa, bq
denoting the Gamma distribution with mean a{b, one can obtain a data adaptive
approach to down-weighting outliers in estimating the mean regression function.
However, note that this speciﬁcation assumes that the same degrees of freedom and
tail-heaviness holds for all x P X . Following West (1987), we express the Student-t
distribution as a scale mixture of normals for ease in computation. In addition, we
allow an unknown scale parameter, letting ǫi „ Np0, σ2{φiq, with φi „ Gapνσ{2, νσ{2q.
2.
Nonparametric error distribution: Let Y “ ℜbe the response space and
X be the covariate space which is a compact subset of ℜp. Let F denotes the space
71

of densities on X ˆ Y w.r.t the Lebesgue measure and Fd denotes the space of all
conditional densities subject to mean zero,
Fd “
"
g : X ˆ Y Ñ p0, 8q,
ż
Y
gpx, yqdy “ 1,
ż
Y
ygpx, yqdy “ 0 @ x P X
*
.
We propose to induce a prior on the space of mean zero conditional densities through
a prior for a collection of mixing measures tPx, x P X u using the following predictor-
dependent mixture of kernels.
Px “
8
ÿ
h“1
πhpxqδtµhpxq,σhu,
µh „ P0, σh „ P0,σ
(3.3)
where πhpxq ě 0 are random functions of x such that ř8
h“1 πhpxq “ 1 a.s. for each
ﬁxed x P X . tµhpxq, x P Xu8
h“1 are iid realizations of a real valued stochastic pro-
cess, i.e., P0 is a probability distributions over a function space FX. Here P0,σ is a
probability distribution on ℜ`. Hence for each x P X , Px is a random probability
measure over the measurable Polish space pℜˆ ℜ`, Bpℜˆ ℜ`qq. Before propos-
ing the prior, we ﬁrst review the probit stick breaking process speciﬁcation and its
relationship to the Dirichlet process. Rodriguez and Dunson (2011a) introduce the
probit stick-breaking process in greater details and discuss some to its theoretical
smoothness and clustering properties. A probability measure P P P on pY, BpYqq
follows a probit stick-breaking process with base measure P0 if it has a representation
of the form
Pp¨q “
8
ÿ
h“1
πhδθhp¨q,
θh „ P0,
(3.4)
where the atoms tθhu8
h“1 are independent and identically distributed from P0 and
the random weights are deﬁned as πh “ Φpαhq ś
lăht1 ´ Φpαlqu, αh „ Npµα, σ2
αq, h “
1, . . . , 8. Here Φp¨q denotes the cumulative distribution function for the standard
normal distribution.
Note that expression (3.4) is identical to the stick-breaking
72

representation (Sethuraman, 1994) of the Dirichlet process (DP), but the DP is ob-
tained by replacing the stick-breaking weight Φpαhq with a beta(1, α) distributed
random variable. Hence, the PSB process diﬀers from the DP in using probit trans-
formations of Gaussian random variables instead of betas for the stick lengths, with
the two speciﬁcations being identical in the special case in which µα “ 0, σα “ 1
and the DP precision parameter is α “ 1. Rodriguez and Dunson (2011a) also men-
tioned the possibility of constructing a variety of predictor dependent models e.g.,
latent Markov random ﬁelds, spatio-temporal processes, etc by using probit trans-
formation latent Gaussian processes. Such latent Gaussian processes can be updated
using data augmentation Gibbs sampling as in continuation-ratio probit models for
survival analysis (Albert and Chib, 2001). While we follow similar computational
strategies as in Rodriguez and Dunson (2011a), they didn’t consider robust regres-
sion using predictor dependent residual density.
Under the symmetric about zero assumption, we propose two nonparametric pri-
ors for the residual density fx for all x P X . The ﬁrst prior is a predictor dependent
PSB scale mixture of Gaussians which enforces symmetry about zero and unimodal-
ity, and the next is a symmetrized location-scale PSB mixture of Gaussians, which
we develop to satisfy the symmetric about zero assumption while allowing multi-
modality.
2a.
Heteroscedastic scale PSB mixtures: To allow the residual density to change
ﬂexibly with predictors, while maintaining the constraint that each of the predictor-
dependent residual distributions is unimodal and symmetric about zero, we propose
the following speciﬁcation
fp¨q “
ż
Np¨ ; 0, τ ´1qPxpdτq,
Px “
8
ÿ
h“1
πhpxqδτh,
τh „ Gapατ, βτq,
(3.5)
where πhpxq “ Φtαhpxqu ś
lăhr1 ´ Φtαlpxqus is the predictor-dependent probability
73

weight on the hth mixture component, and the αh’s are drawn independently from
zero mean Gaussian processes having covariance kernel cαpx, x1q “
1
τα e´κα||x´x1||2.
This implies fxp¨q “ ř8
h“1 πhpxqNp¨ ; 0, τ ´1
h q and is a highly-ﬂexible speciﬁcation that
enforces smoothly changing mixture weights across the predictor space, so that the
residual densities at x and x1 will tend to be similar if x is located close to x1, as
measured by κα||x ´ x1||2.
Clearly, the speciﬁcation allows the residual variance to change ﬂexibly with
predictors, as we have varpǫ | xq “ ř8
h“1 πhpxqτ ´1
h . However, unlike the previously
proposed methods for heteroscedastic nonlinear regression, we do not just allow the
variances to vary, but allow any aspect of the density to vary, including the heaviness
of the tails. This allows locally adaptive downweighting of outliers in estimating
the mean function. Previous methods, which instead assume a single heavy-tailed
residual distribution, such as a t-distribution, can lead to a lack of robustness due to
global estimation of a single degree of freedom parameter. In addition, due to the
form of our speciﬁcation, posterior computation becomes very straightforward using
a data augmentation Gibbs sampler, which involves simple steps for sampling from
conjugate full conditional distributions.
Even under the assumption of Gaussian
residual distributions, posterior computation for heteroscedastic models tends to be
complex, with Gibbs sampling typically not possible due to the lack of conditional
conjugacy.
2b.
Heteroscedastic symmetric PSB (sPSB) location-scale mixtures: The PSB scale
mixture in (3.5) restricts the residual density to be unimodal.
As this is a very
restrictive assumption, it is appealing to deﬁne a prior with larger support that allows
multimodal residual densities, while enforcing the symmetric about zero assumption
so that the residual density is constrained to have mean zero. To accomplish this,
we propose a novel symmetrized PSB process speciﬁcation, which is related to the
74

symmetrized Dirichlet process proposed by Tokdar (2006b). We deﬁne
fp¨q
“
ş
Np¨ ; µ, τ ´1qdP s
xpµ, τq,
dP s
xpµ, τq “ 1
2dPxp´µ, τq ` 1
2dPxpµ, τq,
(3.6)
where the atoms pµh, τhq are drawn independently from P0 a priori, with P0 cho-
sen as a product of a Npµ0, σ2
0q and Gapατ, βτq measure. The diﬀerence between
the sPSB process prior and the PSB process prior is that instead of just plac-
ing probability weight πh on atom pµh, τhq, we place probability πh{2 on p´µh, τhq
and pµh, τhq.
The resulting residual density under (3.6) has the form fp¨q “
ř8
h“1
πhpxq
2
tNp¨., ; ´µh, τ ´1
h q ` Np¨ ; µh, τ ´1
h qu.
Clearly, each of the realizations cor-
responds to a mixture of Gaussians that is constrained to be symmetric about zero.
The same comments made for the heteroscedastic scale PSB mixture apply here, but
(3.6) is more ﬂexible in allowing multi-modal residual distributions, with modality
changing ﬂexibly with predictors. Posterior computation is again straightforward, as
will be shown later.
2c.
Homoscedastic scale PSB process mixture of Gaussians. A simpler homoscedas-
tic version of 3.5 is to consider
fp¨q “
ż
Np¨ ; 0, τ ´1qPpdτq,
P “
8
ÿ
h“1
πhδτh,
τh „ Gapατ, βτq,
(3.7)
where the weights tπhu are speciﬁed as in
πh “ νh
ź
lăh
p1 ´ νhq, νh “ Φpαhq, αh „ Npµα, σ2
αq.
(3.8)
This implies that fp¨q “ ř8
h“1 πhNp¨ ; 0, τ ´1
h q, so that the unknown density of the
residuals is expressed as a countable mixture of Gaussians centered at zero but with
varying variances.
Observations will be automatically allocated to clusters, with
outlying clusters corresponding to components having large variance (low τh). By
choosing a hyperprior on µα while letting σα “ 1, we allow the data to inform more
75

strongly about the posterior distribution on the number, sizes and allocation to clus-
ters.
2d.
Location-scale symmetrized PSB (sPSB) mixture of Gaussians. A homoscedas-
tic version of 3.6 is the following.
fp¨q “
ż
Np¨ ; µ, τ ´1qdP spµ, τq,
dP spµ, τq “ 1
2dPp´µ, τq ` 1
2dPpµ, τq,
P “
8
ÿ
h“1
πhδpµh,τhq,
pµh, τhq „ P0,
(3.9)
where the prior on the weights πh are given by (3.8) and the prior for pµh, τhq are
exactly as in 2b.
3.3
Consistency properties
Let f „ Πu and f „ Πs denote the priors for the unknown residual density deﬁned
in expressions (3.7) and (3.9) respectively. It is appealing for Πu and Πs to have
support on a large subset of Su and Ss respectively, where Ss denotes the set of
densities on R with respect to Lebesgue measure that are symmetric about zero and
Su Ă Ss is the subset of Ss corresponding to unimodal densities. We characterize
the weak support of Πu, denoted by wkpΠuq Ă Su, in the following lemma.
Lemma 34. wkpΠuq “ Cm, where Cm “ tf : f P Su, hpxq “ fp?xq, x ą
0 is a completely monotone functionu.
A function hpxq on p0, 8q is completely monotone in x if it is inﬁnitely diﬀer-
entiable and p´1qm dm
dxmhpxq ě 0 for all x and for all m P t1, 2, . . . , 8u. Chu (1973)
proved that if f is a density on R which is symmetric about zero and unimodal, it
can be written as a scale mixture of normals,
fpxq “
ż
σ´1φpσ´1xqgpσqdσ
76

for some density g on R, if and only if hpxq “ fp?xq, x ą 0, is a completely monotone
function, where φ is the standard normal pdf. This restriction places a smoothness
constraint on fpxq, but still allows a broad variety of densities.
Deﬁnition 35. Letting f „ Π, f0 is in the Kullback-Leibler(KL) support of Π if
Π
ˆ
f :
ż
f0pyq log f0pyq
fpyq dy ă ǫ
˙
ą 0,
@
ǫ ą 0
The set of densities f in the Kullback-Leibler support of Π is denoted by KLpΠq.
Let ˜Ss denote the subset of Ss corresponding to densities satisfying the following
regularity conditions.
1. f is nowhere zero and bounded by M ă 8
2.
ˇˇ ş
ℜfpyq log fpyqdy
ˇˇ ă 8
3.
ˇˇ ş
ℜfpyq log fpyq
ψ1pyqdy
ˇˇ ă 8, where ψ1pyq “ inftPry´1,y`1s fptq
4. there exists ψ ą 0 such that
ş
ℜ|y|2p1`ψqfpyqdy ă 8
Lemma 36. KLpΠsq Ě ˜Ss.
Remark 37. The above assumptions on f are standard regularity conditions intro-
duced by Tokdar (2006b) and Wu and Ghoshal (2008) to prove that f P KLpΠq,
where Π is a general stick breaking prior which has all compactly supported probabil-
ity distributions as its support. (1) is usually satisﬁed by common densities arising
in practice. (4) imposes a minor tail restriction e.g., t-density with p2 ` δq degrees
of freedom for some δ ą 0 satisﬁes (4). (1)-(4) are satisﬁed by a ﬁnite mixture of
t-densities or even by an inﬁnite mixture of t-densites with p2`δq degrees of freedom
for some δ ą 0 and bounded component speciﬁc means and variances.
77

From Lemma 36, it follows that the sPSB location-scale mixture has KL-support
on a large subset of the set of densities symmetric about zero. These conditions
are important in verifying that the priors are ﬂexible enough to approximate any
density subject to the noted restrictions.
We provide fairly general suﬃcient conditions to ensure strong and weak poste-
rior consistency in estimating the mean regression function and the residual density,
respectively. We focus on the case in which a GP prior is chosen for η and an sPSB
location-scale mixture of Gaussians is chosen for the residual density as in (3.9).
Similar results can be obtained for the homoscedastic scale PSB process mixture
under stronger restrictions on the true residual density. Although showing consis-
tency results using predictor dependent mixtures of normals as the prior for the
residual density in (3.5) and (3.6) is a challenging task, one can anticipate such re-
sults given the theory in Pati et al. (2010) and Norets and Pelenis (2010). Indeed
Pelenis and Norets (2011) showed posterior consistency of the regression coeﬃcients
in a mean linear regression model with covariate dependent nonparametric residuals
using the kernel stick-breaking process Dunson and Park (2008a). However, showing
posterior consistency of the mean regression when we have a Gaussian process prior
on the regression function and predictor dependent residuals is quite challenging and
is a topic of future research.
For this section, we assume xi’s are non random and arising from a ﬁxed design,
though the proofs are easily modiﬁed for random xi’s. When the covariate values
are ﬁxed in advance, we consider the neighborhood based on the empirical measure
of the design points. Let Qn be the empirical probability measure of the design
points, Qnpxq “ 1
n
řn
i“1 Ixipxq. Based on Qn, we deﬁne a strong L1 neighborhood
of radius ∆ą 0 as in Choi (2005) around the true regression function η0. Letting
78

||η ´ η0||1,n “
ş
xPX |ηpxq ´ η0pxq|dQnpxq set,
Snpη0; ∆q “
␣
η : ||η ´ η0||1,n ă ∆
(
(3.10)
We introduce the following notation. Let f0 denote an arbitrary ﬁxed density in
˜Ss, η0 denote an arbitrary ﬁxed regression function in F, and
f0i “ f0py ´ η0pxiqq
fηi “ fpy ´ ηpxiqq.
For any two densities f and g, let
Kpf, gq “
ż
R
fpyq logtfpyq{gpyqudy,
V pf, gq “
ż
R
fpyq
“
log`tfpyq{gpyqu
‰2dy,
where log` x “ maxplog x, 0q. Set Kipf, ηq “ Kpf0i, fηiq and Vipf, ηq “ V pf0i, fηiq
for i “ 1, . . . , n.
For technical simplicity assume X “ r0, 1sp, τ “ 1 and µ ” 0. Denote a mean
zero Gaussian process tWx : x P r0, 1spu with covariance kernel cpx, x1q “ e´||x´x1||2
by W. Rescaling the sample paths of an inﬁnitely smooth Gaussian process is a
powerful technique to improve the approximation of α-H¨older functions from the
RKHS of the scaled process tW κ
x “ W?κx :
x P r0, 1sdu with κ ą 0.
Intu-
itively, for large values of κ, the scaled process traverses the sample path of an
unscaled process on the larger interval r0, ?κsp, thereby incorporating more “rough-
ness”.
van der Vaart and van Zanten (2009) studied rescaled Gaussian processes
W κ “ tW?κx
:
x P r0, 1spu for a positive random variable κ stochastically in-
dependent of W and showed that with a Gamma prior on κp{2, one obtains the
minimax-optimal rate of convergence for arbitrary smooth functions.
Assumption 1: η „ W κ with the density g of ?κ on the positive real line satisfying
C1xp expp´D1x logq xq ď gpxq ď C2xp expp´D2x logq xq,
for positive constants C1, C2, D1, D2 and every suﬃciently large x ą 0. Next we state
the lemma on prior positivity due to van der Vaart and van Zanten (2009).
79

Lemma 38. If η satisﬁes Assumption 1 then Pp||η ´ η0||8 ă ǫq ą 0 @ ǫ ą 0, if η0 is
continuous.
In order to prove posterior consistency for our proposed model, we rely on a
theorem of Amewou-Atisso et al. (2003), which is a modiﬁcation of the celebrated
Schwartz (1965a) theorem to accommodate independent but not identically dis-
tributed data.
Theorem 39. Suppose η as in Assumption 1 with q ě p ` 2 and f „ Πs, with
Πs deﬁned in (3.9). In addition, assume the data are drawn from the true density
f0pyi ´ η0pxiqq, with txiuﬁxed and non-random, f0 P ˜Ss, η0 P F and f0 following the
additional regularity conditions,
1.
ş
y4f0pyqdy ă 8 and
ş
f0pyq| log f0pyq|2dy ă 8.
2.
ş
R f0pyq
ˇˇ log f0pyq
ψ1pyq
ˇˇ2dy ă 8, where ψ1pyq “ inftPry´1,y`1s f0ptq.
Let U be a weak neighborhood of f0 and Wn “ U ˆ Snpη0; ∆q, with Wn Ă ˜Ss ˆ F.
Then the posterior probability
pΠs ˆ W κqpWn|y1, . . . , yn, x1, . . . , xnq “
ş
Wn
śn
i“1 fηipyiqdΠspfqdW κpηq
ş
˜SsˆF
śn
i“1 fηipyiqdΠspfqdW κpηq Ñ 1 a.s.
Theorem 39 ensures weak consistency of the posterior of the residual density and
strong consistency of the posterior of the regression function η.
3.4
Posterior Computation
We provide details for posterior computation separately for the most important mod-
els. We ﬁrst describe the choice of hyperparameters of the prior on the regression
function.
80

Choice of hyperpriors: We choose the typical conjugate prior for the regression
coeﬃcients in the mean of the GP, β „ Npβ0, Σ0q, where β0 “ 0 and Σ0 “ cI is
a common choice corresponding to a ridge regression shrinkage prior. The prior on
τ is given by τ „ Gap ντ
2 , ντ
2 q. We let κ „ Gapακ, βκq with small βκ and large ακ.
Normalizing the predictors prior to analysis, we ﬁnd that the data are quite informa-
tive about κ under these priors, so as long as the priors are not overly informative,
inferences are robust. The parameter τ controls the heaviness of the tails of the prior
for the regression function. In fact, choosing a Gapντ{2, ντ{2q prior induces a heavy
tailed t-process with ντ degrees of freedom as a prior for the regression function. In
all the examples, we have ﬁxed ντ “ 3. κ controls the correlation of the Gaussian
process at two points in the covariate space similar to a spatial decay parameter in
a spatial random eﬀects model. Although a discrete uniform prior for κ is computa-
tionally eﬃcient in leading to a griddy Gibbs update step, there can be sensitivity to
the choice of grid. A gamma prior for κ eliminates such sensitivity at some associated
computational price in terms of requiring a Metropolis-Hastings update that tends
to mix slowly. Since the responses are normalized and the covariates are scaled to lie
in the interval r0, 1s, using a single decay parameter appears to be reasonable. We
choose the parameters ακ and βκ so that the mean correlation is 0.1 for two points
separated by a distance ?p in the covariate space. νσ controls the tail-heaviness of
the prior for the scaling φ. Since we would like to accommodate outliers with the
mean being ﬁxed at 1, we assume φi „ Gapνσ{2, νσ{2q with νσ „ Gap3, 1q. a and b
are ﬁxed at 3{2 to resemble a t-distribution with 3 degrees of freedom without the
scaling φ.
3.4.1
Gaussian process regression with t residuals
Let Y “ py1, . . . , ynq1, η “ pηpx1q, ηpx2q, . . . , ηpxnqq1 and deﬁne a matrix T such that
Tij “ e´κ||xi´xj||2. Hence Ση “
1
τ T. Assume Ω“ diagp1{φi : i “ 1, . . . , nq and
81

φ “ pφ1, . . . , φnq1. Then we have
Y|η „ Npη, σ2Ωq, η|β, τ, κ „ NpXβ, τ ´1Tq, β „ Npβ0, Σ0q
φi „ Ga
`νσ
2 , νσ
2
˘
, νσ „ Gapαν, βνq, σ´2 „ Gapa, bq
κ „ Gapακ, βκq, τ „ Ga
`ντ
2 , ντ
2
˘
.
Next we provide the full conditional distributions needed for Gibbs sampling. Due
to conjugacy, η, β, σ´2, φ and τ have closed form full conditional distributions, while
νσ and κ are updated by using Metropolis Hastings steps within the Gibbs sampler.
Let Vη “ pτT´1 ` σ´2Ω´1q´1 and Vβ “ pτX1T´1X ` Σ´1
0 q´1.
η|Y, β, σ´2, τ, κ, νσ, φ „ N
`
VηpτT´1Xβ ` σ´2Ω´1Yq, Vη
˘
β|Y, η, σ´2, τ, κ, νσ, φ „ N
`
VβpτX1T´1η ` Σ´1
0 β0q, Vβ
˘
σ´2|Y, η, β, τ, κ, νσ, φ „ Ga
˜
n
2 ` a, 1
2
nÿ
i“1
φipyi ´ ηiq2 ` b
¸
τ|Y, η, β, σ´2, κ, νσ, φ „ Ga
ˆn ` ντ
2
, 1
2
␣
pη ´ Xβq1T´1pη ´ Xβq ` ντ
(˙
φi|Y, η, β, σ´2, κ, νσ „ Ga
ˆνσ ` 1
2
, 1
2tσ´2pyi ´ ηiq2 ` νσu
˙
.
3.4.2
Heteroscedastic PSB mixture of normals
First we need to describe the choice of hyperparameters in this case.
Choice of hyperparameters: We assume κα „ Gapγκ, βκq and τα „ Gap να
2 , να
2 q.
If the data yi are normalized, we can expect the overall variance to be close to one,
so the variance of the residuals, V arpǫq “ ř8
h“1 πhτ ´1
h , should be less than one. We
set ατ “ 1 and choose a hyperprior on βτ, βτ „ Gap1, k0q with k0 ą 1 so that the
prior mean of τh is signiﬁcantly less than one. Diﬀerent values of k0 are tried out to
assess robustness of the posteriors.
For
posterior
computation,
we
propose
a
Markov
chain
Monte
Carlo
82

algorithm,
which
is
a
hybrid
of
data
augmentation,
the
exact
block
Gibbs sampler of Papaspiliopoulos (2008) and Metropolis Hastings sampling.
Papaspiliopoulos (2008) proposed the exact block Gibbs sampler as an eﬃcient ap-
proach to posterior computation in Dirichlet process mixture models, modifying the
block Gibbs sampler of Ishwaran and James (2001) to avoid truncation approxima-
tions.
The exact block Gibbs sampler combines characteristics of the retrospec-
tive sampler (Papaspiliopoulos and Roberts, 2008) and the slice sampler (Walker,
2007; Kalli et al., 2010).
We included the label switching moves introduced by
Papaspiliopoulos and Roberts (2008) for better mixing. Introduce γ1, . . . , γn such
that πhpxiq “ Ppγi “ hq, h “ 1, 2, . . . , 8. Then
γi „
8
ÿ
h“1
πhpxiqδh “
8
ÿ
h“1
1pui ă πhpxiqqδh
where ui „ Up0, 1q. The MCMC steps are given below.
1.
Update ui’s and stick breaking random variables: Generate
ui|´ „ Up0, πγipxiqq
where πhpxiq “ Φtαhpxiqu ś
lăhr1 ´ Φtαlpxiqus.
For i “ 1, . . . , n, introduce la-
tent variables Zhpxiq, h “ 1, 2, . . . such that Zhpxiq „ Npαhpxiq, 1q. Thus πhpxiq “
PpZhpxiq ą 0, Zlpxiq ă 0 for l ă hq. Then
Zhpxiq|´ „
#
Npαhpxiq, 1qIR`, h “ γi
Npαhpxiq, 1qIR´, h ă γi.
Let Zh “ pZhpx1q, . . . , Zhpxnqq1 and αh “ pαhpx1q, . . . , αhpxnqq1. Letting
`
Σα
˘
ij “
e´κα||xi´xj||, Zh „ Npαh, Iq and αh „ Np0, 1
ταΣαq,
αh|´ „ N
`
pταΣ´1
α ` Inq´1Zh, pταΣ´1
α ` Inq´1˘
83

Continue up to h “ 1, . . . , h˚ “ maxth˚
1, . . . , h˚
nu, where h˚
i is the minimum integer
satisfying řh˚
i
l“1 πlpxiq ą 1 ´ mintu1, . . . , unu, i “ 1, . . . , n. Now
τα|´ „ Ga
ˆ1
2
`
nh˚ ` να
˘
, 1
2
ˆ h˚
ÿ
l“1
α1
kΣ´1
α αk ` να
˙˙
,
while kα is updated using a Metropolis Hastings step.
2.
Update allocation to atoms: Update pγ1, . . . , γnq|´ as multinomial random
variables with probabilities
Ppγi “ hq9Npyi; ηpxiq, τ ´1
h qIpui ă πhpxiqq, h “ 1, . . . , h˚.
3.
Update component-speciﬁc locations and precisions: Letting nl “ #ti :
γi “ lu, l “ 1, 2, . . . , h˚,
τl|´ „ Ga
ˆnl
2 ` ατ, βτ `
ÿ
i:γi“l
pyi ´ ηpxiqq2
˙
, l “ 1, 2, . . . , h˚
βτ|´ „ Ga
ˆ
1,
k˚
ÿ
l“1
τl ` k0
˙
.
4.
Update the mean regression function: Letting Λ “ diagpτ ´1
γ1 , . . . , τ ´1
γn q,
η|´ „ NppτT´1 ` Λ´1q´1pτT´1Xβ ` Λ´1Yq, pτT´1 ` Λ´1q´1q
β|´ „ N
`
pτX1T´1X ` τΣ´1
0 q´1pτX1T´1η ` Σ´1
0 β0q, pτX1T´1X ` Σ´1
0 q´1˘
τ|´ „ Ga
ˆn ` ντ
2
, 1
2
␣
pη ´ Xβq1T´1pη ´ Xβq1 ` ντ
(˙
.
5.
Update κ in a Metropolis Hastings step.
3.4.3
Heteroscedastic sPSB process location-scale mixture
We will need the following changes in the updating steps from the previous case.
1.
Update allocation to atoms: Update pγ1, . . . , γnq|´ as multinomial random
84

variables with probabilities
Ppγi “ hq91
2
␣
Npyi; ηpxiq ` µh, τ ´1
h q ` Npyi; ηpxiq ´ µh, τ ´1
h q
(
Ipui ă πhpxiqq,
h “ 1, . . . , h˚.
3.
Component-speciﬁc locations and precisions: Let nl “ #ti : γi “ lu, l “
1, 2, . . . , h˚ and ml “ ř
i:γi“lpyi ´ ηiq. The atoms of the base measure location is
updated from a mixture of normals as
µl|´ „ plN
ˆµ0σ´2
0
` τlml
σ´2
0
` nlτl
,
1
σ´2
0
` nlτl
˙
` p1 ´ plqN
ˆµ0σ´2
0
´ τlml
σ´2
0
` nlτl
,
1
σ´2
0
` nlτl
˙
,
where pl9 exp
"
1
2
ˆ
µ0σ´2
0 `τlml
σ´2
0
`nlτl
˙*
.
τl|´ „ plGa
ˆnl
2 ` ατ, βτ `
ÿ
i:γi“l
tyi ´ ηpxiq ´ µlu2
˙
`
p1 ´ plqGa
ˆnl
2 ` ατ, βτ `
ÿ
i:γi“l
tyi ´ ηpxiq ` µlu2
˙
,
where pl9
"
1
`
βτ ` 1
2
ř
i:γi“ltyi´ηpxiq´µlu2˘
* nl
2 `α
.
4.
Update the mean regression function: Let Λ “ diagpτ ´1
γ1 , . . . , τ ´1
γn q, µ˚ “
pµγ1, µγ2, . . . , µγnq and W “
`
τT´1 ` Λ´1˘´1. Hence
η|´pN
ˆ
η; WtτT´1Xβ ` Λ´1pY ´ µ˚qu, W
˙
`
p1 ´ pqN
ˆ
η; WtτT´1Xβ ` Λ´1pY ` µ˚qu, W
˙
where p9 exp
“ 1
2
␣
pτT´1Xβ`Λ´1pY´µ˚qq1WXβ`Λ´1pY´µ˚q´pY´µ˚q1Λ´1pY´
µ˚q
(‰
.
85

3.5
Measures of Inﬂuence
There has been limited work on sensitivity of the posterior distribution to perturba-
tions of the data and outliers. Arellano-Vallea et al. (2000) use deletion diagnostics
to assess sensitivity, but their methods are computationally expensive in requiring
posterior computation with and without data deleted. Weiss (1996) proposed an
alternative that perturbs the posterior instead of the likelihood, and only requires
samples from the full posterior. Following Weiss (1996), let fpyi|˜Θ, xiq denote the
likelihood of the data yi, deﬁne
δ˚
i p˜Θq “ fpyi ` δ|˜Θ, xiq
fpyi|˜Θ, xiq
,
for some small δ ą 0 and let pip˜Θ|Yq denote a new perturbed posterior,
pip˜Θ|Yq “ pp˜Θ|Yqδ˚
i p˜Θq
Epδ˚
i p˜Θq|Yq
.
Denote by Li the inﬂuence measure, which is a divergence measure between the
unperturbed posterior pp˜Θ|Yq and the perturbed posterior pip˜Θ|Yq,
Li “ 1
2
ż
|pp˜Θ|Yq ´ pip˜Θ|Yq|d˜Θ.
Li is bounded and takes values in r0, 1s. When pp˜Θ|Yq “ pip˜Θ|Yq, Li “ 0 indicating
that the perturbation δ˚
i has no inﬂuence. On the other hand, if Li “ 1, the supports
of pp˜Θ|Yq and pip˜Θ|Yq are disjoint indicating maximum inﬂuence. We can deﬁne
an inﬂuence measure as L “
1
n
řn
i“1 Li. Clearly L also takes values in r0, 1s with
L “ 0 ñ Li “ 0 @ i “ 1, 2, . . . , n. Also L “ 1 ñ Li “ 1 @ i “ 1, 2, . . . , n. Weiss
(1996) provided a sample version of Li, i “ 1, . . . , n.
Letting ˜Θ1, . . . , ˜ΘM be the
posterior samples with B the burn-in,
ˆLi “
1
M ´ B
M
ÿ
k“B`1
1
2
ˇˇˇˇ
δ˚
i p˜Θkq
ˆEpδ˚
i p˜Θqq
´ 1
ˇˇˇˇ,
86

where ˆEtδ˚
i p˜Θqu “
1
M´B
řM
k“B`1 δ˚
i p˜Θkq. Our estimated inﬂuence measure is ˆL “
1
n
řn
i“1 ˆLi. We will calculate the inﬂuence measure for our proposed methods and
compare their sensitivity.
3.6
Simulation studies
To assess the performance of our proposed approaches, we consider a number of sim-
ulation examples, (i) linear model, homoscedastic error with no outliers, (ii) linear
model, homoscedastic error with outliers (iii) linear model, heteroscedastic errors
and outliers, (iv) non-linear model (a), heteroscedastic errors and outliers and (v)
non-linear model (b), heteroscedastic errors and outliers. We let the heaviness of the
tails and error variance change with x in cases (iii), (iv) and (v). We considered the
following methods of assessing the performance, namely, mean squared prediction
error (MSPE), coverage of 95% prediction intervals, mean integrated squared error
(MISE) in estimating the regression function at the points for which we have data,
point wise coverage of 95% credible intervals for the regression function and the in-
ﬂuence measure (ˆL) as described in Section 3.5. We also consider a variety of sample
sizes in the simulation, n=30, 60, 80 and simulate 10 covariates independently from
Up0, 1q. Let z be 10-dim vector of i.i.d Up0, 1q random variables independent of the
covariates.
Generation of errors in heteroscedastic case and outliers: Let fxipǫiq “
pxiNpǫi; 0, 1q ` qxiNpǫi; 0, 5q where pxi “ Φpx1
izq. The outliers are simulated from the
model with error distribution fo
xip¨q, which is a mixture of truncated normal distri-
butions as follows. In the heteroscedastic case, fo
xipǫiq “ pxiTNp´8,3qYp3,8qpǫi; 0, 1q `
qxiTNp´8,´3
?
5qYp3
?
5,8qpǫi; 0, 5q where TNRp¨ ; µ, σ2q denotes a truncated normal dis-
tribution with mean µ and standard deviation σ over the region R. We consider the
following ﬁve cases.
87

1. Case (i): yi “ 2.3 ` 5.7x1i ` ǫi, ǫi „ Np0, 1q with no outliers.
2. Case (ii): yi “ 2.3 ` 5.7x1i ` ǫi, ǫi „ 0.95Np0, 1q ` 0.05Np0, 10q.
3. Case (iii): yi “ 1.2 ` 5.7x1i ` 4.7x2i ` 0.12x3i ´ 8.9x4i ` 2.4x5i ` 3.1x6i `
0.01x7i ` ǫi, ǫi „ fxi, with 5% outliers generated from fo
xipǫiq.
4. Case (iv): yi “ 1.2 ` 5.7x1i ` 3.4x2
1i ` 4.7xi2 ` 0.89x2
i2 ` 0.12xi3 ´ 8.9xi4xi8 `
2.4xi5xi9 ` 3.1xi6 ` x2
i6 ` 0.01xi7 ` ǫi, ǫi „ fxi with 5% outliers generated from
fo
xipǫiq.
5. Case (v): yi “ 1.2 ` 5.7 sin x1i ` 3.4 exppx2iq ` 4.7 log |xi3| ` ǫi, ǫi „ fxi with
5% outliers generated from fo
xipǫiq.
For each of the cases and for each sample size n, we took the ﬁrst
n
2 samples as
the training set and the next n
2 samples as the test set. The hyperparameters are
speciﬁed as follows.
1. Heavy tailed parametric error distribution: We described the choice of
the hyperparameters in Section 3.5. We took β0 “ 0, Σ0 “ 5I2, αν “ 1, βν “ 1,
a “ 0.5, b “ 0.5, ατ “ 5 and βτ “ 1.
2. Heteroscedastic PSB or sPSB process scale mixture on the residual
density: β0 “ 0, Σ0 “ 5I2, αν “ 1, βν “ 1, a “ 0.5, b “ 0.5, ατ “ 5, βτ “ 1,
γκ “ 5, βκ “ 1, να “ 1 and k0 “ 10.
We also compare the MSPE of the proposed methods with Lasso (Tibshirani, 1996),
Bayesian additive regression trees (Chipman et al., 2010), and Treed Gaussian pro-
cesses (Gramacy and Lee, 2008). The MCMC algorithms described in Section 3.5 are
used to obtain samples from the posterior distribution. The results for model 1 given
here are based on 20,000 samples obtained after a burn-in period of 3,000. The re-
sults for Model 2 and 3 are based on 20,000 samples obtained after a period of 7,000.
88

Table 3.1: Simulation results under homoscedastic residuals (Cases (i) and (ii))
n=40
Case (i)
Case (ii)
MSPE
cov(y)a
MISE
cov(η)b
L
MSPE
cov(y)
MISE
cov(η)
L
Method 1c
0.2997
1
0.0248
1
0.0017
0.6043
1
0.0232
1
0.0027
Method 2d
0.2821
0.9980
0.0141
1
0.0015
0.5983
0.9740
0.0173
1
0.0019
Method 3e
0.2798
1
0.0144
1
0.0015
0.5987
0.9745
0.0169
1
0.0017
Lasso
0.4651
0.1934
0.6410
0.1080
BART
0.3510
0.6866
0.0714
0.7051
0.7845
0.0950
Treed GP
0.3042
0.9134
0.0256
0.6968
0.9365
0.0803
n=60
MSPE
cov(y)
MISE
cov(η)
L
MSPE
cov(y)
MISE
cov(η)
L
Method 1
0.2990
1
0.0246
1
0.0019
0.5776
1
0.0242
1
0.0023
Method 2
0.2769
0.9947
0.0103
1
0.0017
0.5471
0.95
0.0143
0.97
0.0016
Method 3
0.2752
0.9963
0.0104
1
0.0016
0.5541
0.95
0.0141
0.98
0.0016
Lasso
0.4715
0.1974
0.6702
0.1194
BART
0.3314
0.6753
0.0539
0.6725
0.7777
0.1098
Treed GP
0.3000
0.9193
0.0218
0.6880
0.9301
0.1198
n=80
MSPE
cov(y)
MISE
cov(η)
L
MSPE
cov(y)
MISE
cov(η)
L
Method 1
0.2913
1
0.0252
1
0.0021
0.5583
1
0.0172
1
0.0022
Method 2
0.2592
0.9940
0.0086
1
0.0021
0.4989
0.97
0.0050
1
0.0014
Method 3
0.2574
0.9956
0.0069
1
0.002
0.4898
0.98
0.0067
1
0.0010
Lasso
0.4318
0.1756
0.6569
0.1150
BART
0.3128
0.6525
0.0437
0.6509
0.7815
0.1098
Treed GP
0.2886
0.9301
0.0175
0.6532
0.9224
0.1031
a cov(y) denotes the coverage of the 95% predictive intervals of the test cases
b cov(η) denotes the coverage of the 95% credible intervals of the mean regression function
c GP on mean and t residual distribution
d GP on mean and heteroscedastic PSB process scale mixtures as residual distribution
e GP on mean and heteroscedastic sPSB process mixtures as residual distribution
Rapid convergence was observed based on diagnostic tests of Geweke (1992) and
Raftery and Lewis (1992). In addition, the mixing was very good for model 1. For
models 2 and 3, we use the label switching moves by Papaspiliopoulos and Roberts
(2008), which lead to adequate mixing. Tables 6.1, 6.2 and 3.3 summarize the per-
formance of all the methods based on 50 replicated datasets.
Tables 6.1, 6.2 and 3.3 clearly show that in small samples both of the heteroscedas-
tic methods (2 and 3) have substantially reduced MSPE and MISE relative to the
heavy tailed parametric error model in most of the cases, interestingly even in the
89

Table 3.2: Simulation results under heteroscedastic residuals (Cases (iii) and (iv))
n=40
Case (iii)
Case (iv)
MSPE
cov(y)
MISE
cov(η)
L
MSPE
cov(y)
MISE
cov(η)
L
Method 1
0.4833
1
0.3612
1
0.0027
0.4416
1
0.3274
1
0.0029
Method 2
0.2570
0.9990
0.1394
1
0.0025
0.2783
0.9923
0.1583
0.98
0.0023
Method 3
0.2586
0.9990
0.1298
1
0.0025
0.2712
0.9867
0.1501
0.97
0.0017
Lasso
0.3219
0.1970
0.3140
0.1863
BART
0.4639
0.8444
0.3413
0.4103
0.8833
0.2675
Treed GP
0.3320
0.7834
0.1979
0.3548
0.8268
0.2108
n=60
MSPE
cov(y)
MISE
cov(η)
L
MSPE
cov(y)
MISE
cov(η)
L
Method 1
0.2254
1
0.1154
1
0.0023
0.2367
1
0.1067
1
0.0021
Method 2
0.1744
0.9973
0.0572
1
0.0020
0.2178
1
0.0562
0.97
0.0019
Method 3
0.1712
0.9878
0.0567
1
0.0016
0.2099
1
0.0656
0.98
0.0017
Lasso
0.2958
0.1830
0.3025
0.1543
BART
0.3429
0.8546
0.2217
0.3385
0.9122
0.1799
Treed GP
0.2047
0.8349
0.0779
0.2611
0.8867
0.0899
n=80
MSPE
cov(y)
MISE
cov(η)
L
MSPE
cov(y)
MISE
cov(η)
L
Method 1
0.1636
1
0.0454
1
0.0018
0.1855
1
0.0346
1
0.0019
Method 2
0.1509
0.9976
0.0373
0.95
0.0015
0.1653
1
0.0321
0.9952
0.0014
Method 3
0.1578
0.9931
0.0324
1
0.0013
0.1614
1
0.0312
0.9932
0.0010
Lasso
0.2592
0.1437
0.2798
0.1373
BART
0.2284
0.9265
0.1098
0.2491
0.9490
0.1083
Treed GP
0.1655
0.8876
0.0427
0.2022
0.8923
0.0548
homoscedastic cases. This may be because discrete mixture of Gaussians better ap-
proximate a single normal than a t-distribution in small samples. Methods 2 and 3
also did a better job than method 1 in allowing uncertainty in estimating the mean
regression and predicting the test sample observations. In some cases, the heavy
tailed t-residual distribution results in overly conservative predictive and credible
intervals. As seen from the value of the inﬂuence statistic, the heteroscedatic PSB
process mixtures result in more robust inference compared to the parametric error
model, the sPSB process mixture of normals being more robust than the symmetric
and unimodal version. As the sample size increases, the diﬀerence in the predictive
performances between the parametric and the nonparametric models is reduced and
in some cases the parametric error model performs as well as the nonparametric
approaches, which is as expected given the Central Limit Theorem.
90

Table 3.3: Simulation results under heteroscedastic residuals (Case (v))
n=40
MSPE
cov(y)
MISE
cov(η)
L
Method 1
0.6666
0.9800
0.5856
1
0.0033
Method 2
0.5233
0.9770
0.3980
0.9812
0.0025
Method 3
0.5231
0.9854
0.3745
0.9765
0.0019
Lasso
0.3713
0.2871
BART
0.4956
0.8980
0.4013
Treed GP
0.7224
0.8123
0.6132
n=60
MSPE
cov(y)
MISE
cov(η)
L
Method 1
0.3828
1
0.2911
0.9985
0.0031
Method 2
0.3745
0.9832
0.2617
0.9840
0.0022
Method 3
0.3767
0.9812
0.2601
0.9867
0.0020
Lasso
0.3532
0.2616
BART
0.3930
0.9313
0.2668
Treed GP
0.4225
0.9023
0.3217
n=80
MSPE
cov(y)
MISE
cov(η)
L
Method 1
0.3599
0.9901
0.2759
0.9998
0.0029
Method 2
0.3503
0.9762
0.2582
0.9765
0.0022
Method 3
0.3519
0.9712
0.2545
0.9715
0.0019
Lasso
0.4505
0.2751
BART
0.3594
0.9442
0.2867
Treed GP
0.4489
0.9125
0.3509
Table 3.4: Boston housing data and body fat data results
Boston housing data
body fat data
Methods
MSPE
cov(y)
L
corr(Ytest, Ypred)a
MSPE
cov(y)
L
corr(Ytest, Ypred)
Method 1
0.0012
0.99
0.0034
0.9894
0.0055
1
0.0020
0.9972
Method 2
0.0013
0.99
0.0027
0.9901
0.0031
1
0.0017
0.9984
Method 3
0.0016
0.99
0.0020
0.9863
0.0029
1
0.0017
0.9989
Lasso
0.0015
0.9909
0.0184
0.9909
BART
0.0024
0.92
0.9836
0.0355
0.95
0.9655
Treed GP
0.0053
0.91
0.9524
0.1526
0.98
0.9250
a corr(Ytest, Ypred) denotes the sample correlation between the test and predicted y
91

Table 6.1 shows that, in the simple linear model with normal homoscedastic
errors, all the models perform similarly in terms of mean squared prediction error,
though the methods 2 and 3 are somewhat better than the rest. Also, in estimating
the mean regression function in case (i), methods 2 and 3 performed better than all
the other methods. In case (ii)(Table 6.1), methods 2 and 3 are most robust in terms
of estimation and prediction in presence of outliers. In cases (iii) and (iv), when the
residual distribution is heteroscedastic, our methods 2 and 3 perform signiﬁcantly
better than the parametric model 1 in both estimation and prediction, since the
heteroscedastic PSB mixture is very ﬂexible in modeling the residual distribution.
This is quite evident from the MSPE values under cases (iii) and (iv) in Table 2.
Lasso did a poor job in estimating the mean regression function and also in prediction
particularly in cases (iii) and (iv) when the underlying mean function is actually non-
linear. Also BART failed to perform well in estimating the mean function in small
samples in these cases. On the other hand, GP based approaches perform quite well
in these cases in estimating the regression function with methods 2 and 3 performing
better than the rest.
Treed GP performed close to method 1 in estimation and
prediction as both the methods are based on GP priors on the mean function and
have a parametric error distribution. In not allowing heteroscedastic error variance,
BART and Treed GP under-estimates uncertainty in prediction, leading to overly
narrow predictive intervals.
In case (v)(Table 3.3), where the true model is generated using comparatively
less number of true signals, Lasso and BART performed slightly better in terms of
prediction than the methods 4 and 5 in small samples. This may be due to the fact
that Lasso can pick up the true signals quite eﬃciently in an overly parsimonious
model. However, as the sample size increased, Lasso performed poorly while the GP
prior on the mean can accommodate the non-linearity resulting in substantially good
predictive performances.
92

3.7
Applications
3.7.1
Boston housing data Application
To compare our proposed approaches to alternatives, we applied the methods to a
commonly used data set from the literature, the Boston housing data. The response
is the median value of the owner-occupied homes (measured in 1000$) in 506 census
tracts in the Boston area, and there are 13 predictors (12 continuous, 1 binary) that
might help to explain the variation in the median value across tracts. We predict
the median value of the owner occupied homes of which the ﬁrst 253 is taken as
the training set and the remaining 253 as the test set. Out of sample predictive
performance of our three methods is compared to competitors in Table 3.4. The
parametric model 1, the heteroscedastic PSB process mixture models 2 and 3 and
the Lasso perform very closely to each other in terms of prediction and did better than
BART and Treed GP. Methods 1 and 2 even perform slightly better than method 3
and Lasso. As in the simulation examples, BART and Treed GP underestimates the
uncertainty in prediction. On the other hand, the predictive intervals of the methods
1, 2 and 3 are more conservative and accommodate uncertainty in predicting regions
with outliers quite ﬂexibly. Also the model 3 appears to be more robust compared
to models 1 and 2 in terms of the inﬂuence measure.
3.7.2
Body fat data application
With the increasing trend in obesity and concerns about associated adverse health
eﬀects, such as heart disease and diabetes, it has become even more important to
obtain accurate estimates of body fat percentage. It is well known that body mass
index, which is calculated based only on weight and height, can produce a misleading
measure of adiposity as it does not take into account muscle mass or variability in
frame size. As a gold standard for measuring percentage of body fat, one can rely
93

on under water weighing techniques, and age and body circumference measurements
have also been widely used as additional predictors. We consider a commonly-used
data set from Statlib (http://lib.stat.cmu.edu/datasets/bodyfat), which contains the
following 15 variables; percentage of body fat(%), body density from underwater
weighing (gm{cm3), age (year), weight (lbs.), height (inches), and ten body circum-
ferences (neck, chest, abdomen, hip, thigh, knee, ankle, biceps, forearm, wrist, all in
cm). Percentage of body fat is given from Siri’s (1956) equation:
Percentage of body Fat “
495
Density ´ 450
We predict the percentage of body fat(%) taking the ﬁrst 126 as the training set
and the remaining 126 as the test set. We summarize the predictive performances in
Table 3.4.
Table 3.4 suggests that the nonparametric regression procedures with het-
eroscedastic residual distribution 2 and 3 perform better than the parametric model
1, BART, Lasso and Treed GP in predicting the percentage of body fat.
3.8
Discussion
We have developed a novel regression model that can accommodate a large range
of non linearity in the mean function and at the same time can ﬂexibly deal with
outliers and heteroscedasticity. Based on preliminary simulation results, it appears
that our method can outperform contemporary nonparametric regression methods,
such as BART and treed Gaussian processes, with the performance also better than
Lasso in certain linear regression settings. We also provide theoretical support for
the proposed methodology when both the mean and the residuals are modeled non-
parametrically.
One possible future direction is to relax the symmetry assumption on the residual
distribution and introduce a model for median regression based on conditional PSB
94

mixtures for allowing possibly asymmetric residual densities constrained to have
zero median. Conditional DP mixtures are well known in the literature (Doss, 1985;
Burr and Doss, 2005) and it is certainly interesting to extend our approach via a
conditional PSB. In that way we can hope to obtain a more robust estimate of the
regression function. It is challenging to extend our theoretical results to conditional
PSB and develop a fast algorithm for computation. Another possible theoretical
direction is to prove posterior consistency using heteroscedastic mixtures. Currently
we only have results for the homoscedastic PSBP mixture.
95

4
Posterior consistency in conditional density
estimation
4.1
Introduction
There is a rich literature on Bayesian methods for density estimation using mixture
models of the form
yi „ fpθiq,
θi „ P,
P „ Π,
(4.1)
where fp¨q is a parametric density and P is an unknown mixing distribution as-
signed a prior Π. The most common choice of Π is the Dirichlet process prior, ﬁrst
introduced by Ferguson (1973a, 1974a). Barron et al. (1999b); Ghosal et al. (1999)
used upper bracketing and L1-metric entropy bounds respectively to derive suﬃcient
conditions on the prior on f and the true data generating f for obtaining strong pos-
terior consistency in Bayesian density estimation. Ghosal et al. (1999) also provided
suﬃcient conditions for posterior consistency in univariate density estimation using
Dirichlet process location mixtures of normals. Tokdar (2006a) signiﬁcantly relaxed
their conditions in a Dirichlet process location-scale mixture of normals setting, re-
quiring existence of only weak moments of the true f. Ghosal and van der Vaart
96

(2001, 2007b) provided rates of convergence for Bayesian univariate density estima-
tion using a Dirichlet process mixture of normals. Bhattacharya and Dunson (2010)
provided conditions for strong consistency of kernel mixture priors for densities on
compact metric spaces and manifolds.
Recent literature has focused on generalizing model (4.1) to the density regression
setting in which the entire conditional distribution of y given x changes ﬂexibly with
predictors. Bayesian density regression views the entire conditional density fpy | xq
as a function valued parameter and allows its center, spread, skewness, modality and
other such features to vary with x. For data tpyi, xiq, i “ 1, . . . , nu let
yi | xi „ fp¨ | xiq,
tfp¨ | xq, x P Xu „ ΠX,
(4.2)
where X is the predictor space and ΠX is a prior for the class of conditional densities
tfx, x P X u indexed by the predictors. Refer, for example, to M¨uller et al. (1996);
Griﬃn and Steel (2006a, 2010); Dunson et al. (2007a); Dunson and Park (2008a);
Chung and Dunson (2009) and Tokdar et al. (2010a) among others.
The primary focus of this recent development has been mixture models of the
form
fpy | xq “
8
ÿ
h“1
πhpxqφ
"y ´ µhpxq
σh
*
,
(4.3)
wmhere φ is the standard normal density, tπhpxq, h “ 1, 2, . . .u are predictor-
dependent probability weights that sum to one almost surely for each x P X , and
pµh, σhq „ G0 independently, with G0 a base probability measure on FX ˆ ℜ`,
FX Ă X ℜ, the space of all X Ñ ℜfunctions. However, there is a dearth of results on
support properties of prior distributions for conditional distributions and on general
theorems providing conditions for weak and strong posterior consistency. To our
knowledge, only Barrientos et al. (2011) have considered formalizing the notions of
97

weak and KL-support for dependent stick-breaking processes. We focus on a broad
class of generalized stick-breaking processes, which express the probability weights
πhpxq in stick-breaking form, with the stick lengths constructed through mapping
continuous stochastic processes to the unit interval using a monotone diﬀerentiable
link function. This class includes dependent Dirichlet processes (MacEachern, 1999)
as a special case.
To our knowledge, only a few papers have considered posterior consistency in con-
ditional density estimation. Tokdar et al. (2010a) considers posterior consistency in
estimating conditional distributions focusing exclusively on logistic Gaussian process
priors (Tokdar and Ghosh, 2007). Such priors have beautiful theoretical properties
but lack the computational simplicity of the countable mixture priors in (4.3). In ad-
dition, (4.3) has the appealing side eﬀect of inducing predictor-dependent clustering,
which is often of interest in itself and is an aid to interpretation and inferences. Yoon
(2009) considers posterior consistency in conditional distribution estimation through
a limited information approach by approximating the likelihood by the quantiles of
the true distribution. Tang and Ghosal (2007a,b) provide suﬃcient conditions for
showing posterior consistency in estimating an autoregressive conditional density
and a transition density rather than regression with respect to another covariate.
In this chapter, focusing on model (4.3), we initially provide suﬃcient conditions
on the prior and true data-generating model under which the prior leads to weak and
various types of strong posterior consistency. In this context, we ﬁrst deﬁne notions
of weak and L1-integrated neighborhoods. We then show that the suﬃcient condi-
tions are satisﬁed for a novel class of generalized stick-breaking priors that construct
the stick-breaking lengths through mapping continuous stochastic processes to the
unit interval using a monotone diﬀerentiable link function. The theory is illustrated
through application to a model relying on probit transformations of Gaussian pro-
cesses, an approach related to the probit stick-breaking process of Chung and Dunson
98

(2009) and Rodriguez and Dunson (2011b). We also considered Gaussian mixtures
of ﬁxed-π dependent processes (MacEachern, 1999; De Iorio et al., 2004).
Norets and Pelenis (2010) showed posterior consistency in conditional density
estimation using kernel stick breaking process mixtures of Gaussians in a very recent
unpublished article. They approximated a conditional density by a smooth mixture
of linear regressions as in Norets (2010) to demonstrate the KL property. In this
chapter, we have shown KL support using a more direct approach of approximating
the true density by a kernel mixture of a compactly supported conditional measure.
The fundamental contribution of this chapter is developing a novel class of
prior distributions which has large support in the space of conditional densities
and also leads to a consistent posterior.
In doing so, a key technical contribu-
tion is the development of a novel method of constructing a sieve for the pro-
posed class of priors. It has been noted by Wu and Ghosal (2010) that the usual
method of constructing a sieve by controlling prior probabilities is unable to lead
to a consistency theorem in the multivariate case.
This is because of the explo-
sion of the L1-metric entropy with increasing dimension. They developed a tech-
nique speciﬁc to the Dirichlet process in the multivariate case for showing weak
and strong posterior consistency. The proposed sieve1 avoids the pitfall mentioned
by Wu and Ghosal (2010) in showing consistency using multivariate mixtures. Our
sieve construction has been applied to a variety of settings for studying posterior
consistency and convergence rates - adaptive Bayesian multivariate density estima-
tion Shen and Ghosal (2011); Tokdar (2011b), sparse multivariate mixtures of factor
analyzers McLachlan and Peel (2000) and probability tensor decomposed models for
categorical data analysis Bhattacharya and Dunson (2011).
1 A similar sieve appears in Norets and Pelenis (2010) with a citation to an earlier draft of our
paper.
99

4.2
Conditional density estimation
In this section, we will deﬁne the space of conditional densities and construct a prior
on this space. It is ﬁrst necessary to generalize the topologies to allow appropriate
neighborhoods to be constructed around an uncountable collection of conditional
densities indexed by predictors. With such neighborhoods in place, we then state
our main theorems providing suﬃcient conditions under which various modes of pos-
terior consistency hold for a broad class of predictor-dependent mixtures of Gaussian
kernels.
Let Y “ ℜbe the response space and X be the covariate space which is a compact
subset of ℜp. Unless otherwise stated, we will assume X “ r0, 1sp without loss of
generality. Let F denote the space of densities on X ˆY w.r.t. the Lebesgue measure
and Fd denote a subset of the space of conditional densities satisfying,
Fd “
"
g : X ˆ Y Ñ p0, 8q,
ż
Y
gpx, yqdy “ 1 @ x P X , x ÞÑ gpx, ¨q
continuous as a function from X Ñ L1pλ, Yq
*
.
Suppose yi is observed independently given the covariates xi, i “ 1, 2, . . . which are
drawn independently from a probability distribution Q on X . Assume that Q admits
a density q with respect to the Lebesgue measure.
If we deﬁne hpx, yq “ qpxqfpy | xq and h0px, yq “ qpxqf0py | xq then h, h0 P
F. Throughout the chapter, h0 is assumed to be a ﬁxed density in F which we
alternatively refer to as the true data generating density and tf0p¨ | xq, x P X u is
referred to as the true conditional density. The density qpxq will be needed only for
theoretical investigation. In practice, we do not need to know it or learn it from the
data.
We propose to induce a prior ΠX on the space of conditional densities through a
100

prior PX for a collection of mixing measures GX “ tGx, x P X u using the following
predictor-dependent mixture of kernels
fpy | xq “
ż 1
σφ
ˆy ´ µ
σ
˙
dGxpψq,
(4.4)
where ψ “ pµ, σq, and
Gx “
8
ÿ
h“1
πhpxqδtµhpxq,σhu,
pµh, σhq „ G0,
(4.5)
where πhpxq ě 0 are random functions of x such that ř8
h“1 πhpxq “ 1 a.s. for each
ﬁxed x P X . tµhpxq, x P Xu8
h“1 are i.i.d. realizations of a real valued stochastic
process, i.e., G0 is a probability distribution over FX ˆ ℜ`, where FX Ă X ℜ, X ℜ
being the space of functions from X to ℜ. Hence for each x P X , Gx is a random
probability measure over the measurable Polish space pℜˆ ℜ`, Bpℜˆ ℜ`qq. We are
interested in Bayesian posterior consistency for a broad class of predictor-dependent
stick-breaking mixtures including the following two important special cases.
4.2.1
Predictor dependent mixtures of Gaussian linear regressions
We deﬁne the predictor dependent countable mixtures of Gaussian linear regressions
(MGLRx) as
fpy | xq “
ż 1
σφ
ˆy ´ x1β
σ
˙
dGxpβ, σq,
and
Gx “
8
ÿ
h“1
πhpxqδpβh,σhq,
pβh, σhq „ G0
(4.6)
where πhpxq ě 0 are random functions of x such that ř8
h“1 πhpxq “ 1 a.s. for each
ﬁxed x P X and G0 “ G0,β ˆG0,σ is a probability distribution on ℜpˆℜ` where G0,β
101

and G0,σ are probability distributions on ℜp and ℜ` respectively. For a particular
choice of πhpxq’s, we obtain the probit stick-breaking mixtures of Gaussians which
have been previously applied by (Chung and Dunson, 2009; Rodriguez and Dunson,
2011b; Pati and Dunson, 2010). The latter two articles considered probit transfor-
mations of Gaussian processes in constructing the stick-breaking weights.
4.2.2
Gaussian mixtures of ﬁxed-π dependent processes
In (4.4), set Gx as in (4.5) with πhpxq ” πh for all x P X where πh ě 0 are random
probability weights ř8
h“1 πh “ 1 a.s. and tµhpxq, x P Xu8
h“1 are as in (4.5). Examples
include ﬁxed-π dependent Dirichlet process mixtures of Gaussians (MacEachern,
1999). Versions of the ﬁxed π-DDP have been applied to ANOVA (De Iorio et al.,
2004), survival analysis (De Iorio et al., 2009; Jara et al., 2010), spatial modeling
(Gelfand et al., 2005), and many more.
4.3
Notions of neighborhoods in conditional density estimation
We deﬁne the weak and ν-integrated L1 neighborhoods of the collection of conditional
densities tf0p¨ | xq, x P X u in the following. A sub-base of a weak neighborhood is
deﬁned as
Wǫ,gpf0q “
"
f : f P Fd,
ˇˇˇˇ
ż
XˆY
gh ´
ż
XˆY
gh0
ˇˇˇˇ ă ǫ
*
,
(4.7)
for a bounded continuous function g : Y ˆ X Ñ ℜ. A weak neighborhood base
is formed by ﬁnite intersections of neighborhoods of the type (4.7).
Deﬁne a ν-
integrated L1 neighborhood
Sǫpf0; νq “
"
f : f P Fd,
ż
}fp¨ | xq ´ f0p¨ | xq}1 νpxqdx ă ǫ
*
(4.8)
for any measure ν with supppνq Ă X . Observe that under the topology in (4.8), Fd
can be identiﬁed to a closed subset of L1pλ ˆ ν, Y ˆ supppνqq making it a complete
102

separable metric space. Thus measurability issues won’t arise with these topologies.
Although the choice of ν-integrated L1 topology might not seem to be a good choice
of metric for the conditional densities in the onset, we refer the reader to two ob-
servations mentioned in Section 4 in Tokdar (2011a) to point out why this is not a
terrible thing to do.
In the following, we deﬁne the Kullback-Leibler (KL) property of ΠX at a given
f0 P Fd.
Note that we deﬁne a KL-type neighborhood around the collection of
conditional densities f0 through deﬁning a KL neighborhood around the joint density
h0, while keeping Q ﬁxed at its true unknown value.
Deﬁnition 40. For any f0 P Fd, such that h0px, yq “ qpxqf0py | xq is the true joint
data-generating density, we deﬁne an ǫ-sized KL neighborhood around f0 as
Kǫpf0q “ tf : f P Fd, KLph0, hq ă ǫ, hpx, yq “ qpxqfpy | xq @y P Y, x P X u,
where KLph0, hq “
ş
h0 logph0{hq. Then, ΠX is said to have KL property at f0 P Fd,
denoted f0 P KLpΠXq, if ΠXtKǫpf0qu ą 0 for any ǫ ą 0.
We recall the deﬁnitions of various modes of posterior consistency through yn “
py1, . . . , ynq and xn “ px1, . . . , xnq.
Deﬁnition 41. The posterior ΠX
`
¨ | yn, xn˘
is consistent weakly or strongly in the
ν-integrated L1 topology at tf0p¨ | xq, x P X u if ΠX
`
Uc | yn, xn˘
Ñ 0 a.s. for any
ǫ ą 0 with U “ Wǫpf0q and Sǫpf0; νq respectively.
Here a.s. consistency at tf0p¨ | xq, x P X u means that the posterior distribution
concentrates around a neighborhood of tf0p¨ | xq, x P X u for almost every sequence
tyi, xiu8
i“1 generated by i.i.d. sampling from the joint density qpxqf0py | xq.
Another deﬁnition we would require for showing the KL support is the notion of
weak neighborhood of a collection of mixing measures GX “ tGx, x P X u where Gx
103

is a probability measure on S ˆ ℜ` for each x P X . Here S “ ℜp or ℜdepending
on the cases considered above. We formulate the notion of a sub-base of the weak
neighborhood of GX “ tGx, x P X u below.
Deﬁnition 42. For a bounded continuous function g : S ˆ ℜ` ˆ X Ñ ℜand ǫ ą 0,
a sub-base of the weak neighborhood of a conditional probability measure tFx, x P X u
is deﬁned as
"
tGx, x P X u :
ˇˇˇˇ
ż
Sˆℜ`ˆX
gps, σ, xqdGxps, σqqpxqdx ´
gps, σ, xqdFxps, σqqpxqdx
ˇˇˇˇ ă ǫ
*
(4.9)
A conditional probability measure tGx, x P X u lies in the weak support of PX if
PX assigns positive probability to every basic neighborhood generated by the sub-
base of the type (4.9). In the sequel, we will also consider a neighborhood of the
form
"
tGx, x P X u : sup
xPX
ˇˇˇˇ
ż
Sˆℜ`
␣
gps, σqdGxps, σq ´ gps, σqdFxps, σq
(ˇˇˇˇ
ă ǫ
*
.
(4.10)
for a bounded continuous function g : S ˆ ℜ` Ñ ℜ.
4.4
Posterior consistency in MGLRx mixture of Gaussians
4.4.1
Kullback-Leibler property
We will work with a speciﬁc choice of PX motivated by the probit stick breaking
process construction in Chung and Dunson (2009) but using Gaussian process trans-
forms instead of Gaussian transforms. Let
πhpxq “ Φtαhpxqu
ź
lăh
r1 ´ Φtαlpxqus ,
(4.11)
104

where αh „ GPp0, chq, for h “ 1, 2, . . . , 8. Assume the following holds.
S1. ch is chosen so that αh „ GPp0, chq has continuous path realizations and
S2. for any continuous function g : X ÞÑ ℜ,
PX
"
sup
xPX
|αhpxq ´ gpxq| ă ǫ
*
ą 0
h “ 1, . . . , 8 and for any ǫ ą 0.
S3. G0 is absolutely continuous with respect to λpℜp ˆ ℜ`q.
Consider the subset F ˚
d Ă Fd satisfying the following conditions.
A1. f is nowhere zero and bounded by M ă 8.
A2. |
ş
X
ş
Y fpy | xq log fpy | xqdyqpxqdx| ă 8.
A3. |
ş
X
ş
Y fpy | xq log fpy|xq
ψxpyq dyqpxqdx| ă 8, where ψxpyq “ inftPry´1,y`1s fpt | xq.
A4. D η ą 0 such that
ş
X
ş
Y |y|2p1`ηq fpy | xqdyqpxqdx ă 8.
A5. px, yq ÞÑ fpy | xq is jointly continuous.
Remark 43. A1 is usually satisﬁed by common densities arising in practice. A4
imposes a minor tail restriction; e.g., a mean regression model with continuous mean
function and a heavy-tailed t residual density with 4 degrees of freedom satisﬁes A4.
Conditions A2 and A3 are more subtle, but are also mild. A ﬂexible class of models
which satisﬁes A1-A5 is as follows. Let yi “ µpxiq ` ǫi, with µ : X Ñ ℜcontinuous
and ǫi „ fxi, where fxpǫq “ řH
h“1 πhpxqψpǫ; µh, σ2
hq for some H ě 1, řH
h“1 πhpxq “ 1,
πh : X Ñ r0, 1s continuous and ψ is Gaussian or t with greater than 2 degrees of
freedom.
105

The following theorem characterizes the subset of Fd for which ΠX has the KL
property. The proof of Theorem 44 is provided in Appendix A.
Theorem 44. f0 P KLpΠXq for each f0 in F ˚
d if PX satisﬁes S1-S3.
Remark 45. The conditions are satisﬁed for a class of generalized stick-breaking
process mixtures in which the stick-breaking lengths are constructed through mapping
continuous stochastic processes to the unit interval using a monotone diﬀerentiable
link function.
To prove Theorem 44, we need several auxiliary results related to the support
of the prior PX which might be of independent interest. The key idea for showing
that the true f0 satisﬁes ΠXtKǫpf0qu ą 0 for any ǫ ą 0 is to impose certain tail
conditions on f0py | xq and approximate it by ˜fpy | xq “
ş 1
σφ
`y´x1β
σ
˘
d ˜Gxpβ, σq,
where t ˜Gx, x P X u is compactly supported. Observe that,
KLph0, hq “
ż
X
ż
Y
f0py | xq log f0py | xq
˜fpy | xq
dyqpxqdx `
(4.12)
ż
X
ż
Y
f0py | xq log
˜fpy | xq
fpy | xqdyqpxqdx.
We construct
such
an
˜f
in
Theorem
44
which
makes
the ﬁrst
term
in
the
right
hand
side
of
(4.12)
suﬃciently
small.
The
following
lemma
(which
is
similar
to
Lemma
3.1
in
Tokdar
(2006a)
and
Theorem
3
in
Ghosal et al. (1999)) guarantees that the second term in the right hand side of (4.12)
is also suﬃciently small if tGx, x P X u lies inside a ﬁnite intersection of neighbor-
hoods of t ˜Gx, x P X u of the type (4.10).
Lemma 46. Assume that f0 P Fd satisﬁes
ş
X
ş
Y y2f0py | xqdyqpxqdx ă 8. Suppose
˜fpy | xq “
ş 1
σφ
` y´x1β
σ
˘
d ˜Gxpβ, σq, where D a ą 0 and 0 ă σ ă σ such that
˜Gx
`
r´a, asp ˆ pσ, σq
˘
“ 1 @ x P X ,
(4.13)
106

so that ˜Gx has compact support for each x P X . Then given any ǫ ą 0, D a ﬁnite
intersection W of neighborhoods of t ˜Gx, x P X u of the type (4.10) such that for any
conditional density fpy | xq “
ş 1
σφ
`y´x1β
σ
˘
dGxpβ, σq, x P X , with tGx, x P X u P W,
ż
X
ż
Y
f0py | xq log
˜fpy | xq
fpy | xqdyqpxqdx ă ǫ.
(4.14)
The proof of Lemma 46 is provided in Appendix A. In order to ensure that
the weak support of ΠX is suﬃciently large to contain all densities ˜f satisfying the
assumptions of Lemma 46, we deﬁne a collection of ﬁxed conditional probability
measures on pℜp ˆ ℜ`, Bpℜp ˆ ℜ`qq denoted by G˚
X satisfying
1. x ÞÑ FxpBq is a continuous function of x P X @ B P Bpℜp ˆ ℜ`q.
2. For any sequence of sets An Ă ℜp ˆ ℜ` Ó H, supxPX FxpAnq Ó 0.
Next we state the theorem characterizing the weak support of PX which will be
proved in Appendix A.
Theorem 47. If PX satisﬁes S1-S3, then any tFx, x P X u P G˚
X lies in the weak
support of PX.
Corollary 48. Assume S1-S3 hold and assume Fx P G˚
X is compactly supported,
i.e., there exists a, σ, σ ą 0 such that Fxpr´a, asp ˆ rσ, σsq “ 1. Then for a bounded
uniformly continuous function g : ℜp ˆ ℜ` Ñ r0, 1s satisfying gpβ, σq Ñ 0 as }β} Ñ
8, σ Ñ 8,
PX
"
tGx, x P X u : sup
xPX
ˇˇˇˇ
ż
ℜpˆℜ`
␣
gpβ, σqdGxpβ, σq ´ gpβ, σqdFxpβ, σq
(ˇˇˇˇ
ă ǫ
*
ą 0.
(4.15)
107

Proof. The proof is similar to Theorem 47 with the L1 convergence in (A.6) replaced
by convergence uniformly in x. This is because under the assumptions of Corol-
lary 48, the uniformly continuous sequence of functions řn
k“1 gp ˜βk,n, ˜σk,nqFxpAk,nq
on X monotonically decreases to
ş
C gpβ, σqdFxpβ, σq as n Ñ 8 where C is given by
r´a, asp ˆ rσ, σs.
The proof of the following corollary is along the lines of the proof of Theorem 47
and is omitted here.
Corollary 49. Under the assumptions of Corollary 48 for any k0 ě 1,
PX
"
Xk0
j“1 Uj
*
ą 0,
(4.16)
where Uj’s are neighborhoods of the type (4.15).
4.4.2
Strong Consistency with the q-integrated L1 neighborhood
To obtain strong consistency in the q-integrated L1 topology, we need a very straight-
forward extension of Theorem 2 of Ghosal et al. (1999) below.
Theorem 50. Suppose f0 P KLpΠXq and there exists subsets Fn Ă Fd with
1. log Npǫ, Fn, }¨}1q “ opnq,
2. ΠXpF c
nq ď c2e´nβ2 for some c2, β2 ą 0,
then the posterior is strongly consistent with respect to the q-integrated L1 neighbor-
hood.
Before stating the main theorem on strong consistency, we consider a hierarchical
extension of MGLRx where the bandwidths are taken to be random.
We deﬁne
a sequence of random inverse-bandwidths Ah of the Gaussian process αh, h ě 1
each having ℜ` as its support. Since the ﬁrst few atoms suﬃce to explain most
108

of the dependence of y on x, we expect that the variability due to the covariate in
the stochastic process Φtαhu decreases as h increases. This is achieved through a
carefully chosen prior for the covariance kernel ch of the Gaussian process αh.
Let α0 denote the base Gaussian process on r0, 1sp with covariance kernel
c0px, x1q “ τ 2e´||x´x1||2. Then αhpxq “ α0pA1{2
h xq for each x P X . The variability
of αh with respect to the covariate is shrunk or stretched to the rectangle r0, A1{2
h sp
as Ah decreases or increases. Ah’s are constructed to be stochastically decreasing
to δ0 in the following manner.
We assume that there exist η, η0 ą 0 and a se-
quence δn “ Opplog nq2{n5{2q such that PpAh ą δnq ď expt´n´η0hpη0`2q{η log hu
for each h ě 1.
Also assume that there exists a sequence rn Ò 8 such that
rp
nnηplog nqp`1 “ opnq and PpAh ą rnq ď e´n. We will discuss how to construct
such a sequence of random variables in the Remark 53 following Theorem 51.
The following theorem provides suﬃcient conditions for strong posterior consis-
tency in the q-integrated L1 topology. The proof is provided in Appendix A.
Theorem 51. Let πh’s satisfy (4.11) with αh „ GPp0, chq where chpx, x1q “
τ 2e´Ah}x´x1}2, h ě 1, τ 2 ą 0 ﬁxed.
C1. There exists sequences an, hn Ò 8, ln Ó 0 with
an
ln “ Opnq, hn
ln “ Openq, and
constants d1, d2 ą 0 such that G0tBp0; anq ˆ rln, hnsuc ă d1e´d2n for some
d1, d2 ą 0.
C2. Ah’s are constructed as in the last paragraph before Lemma 56.
then f0 P KLpΠXq implies that ΠX achieves strong posterior consistency in q-
integrated L1 topology at f0.
Remark 52. Veriﬁcation of condition C1 of Theorem 51 is particularly simple. For
example, if G0 is a product of multivariate normals on β and an inverse Gamma
prior on σ2, the condition C1 is satisﬁed with an “ Op?nq, hn “ en, ln “ Op 1
?nq. It
109

follows from van der Vaart and van Zanten (2009) that f0 P KLpΠXq is still satisﬁed
when we have the additional assumptions C1-C2 together with S1-S3 on the prior ΠX.
Remark 53. Since we need rp
nnηplog nqp`1 “ opnq, rp
n can be chosen to be Opnη1q
for some 0 ă η1 ă 1. Let d be such that dη1{p ě 1 and set η0 “ 3d. Let Ah “
chBh, where Bd
h „ Exppλq and ch “ php3d`2q{η log hq´1{d for any 0 ă η ă 1. Then
PpAh ą nη1{pq ď PpBh ą nη1{pq ď e´ndη1{p ď e´n and PpAh ą plog nq2n´5{2q ď
expt´n´3dhp3d`2q{η log hu.
Remark 54. The theory of strong posterior consistency can be generalized to an
arbitrary monotone diﬀerentiable link function L : ℜÞÑ r0, 1s which is Lipschitz,
i.e., there exists a constant K ą 0 such that |Lpxq ´ Lpx1q| ď K |x ´ x1| for all
x, x1 P X .
Below we will develop several auxiliary results required to prove Theorem 51.
They are stated below as some of them might be of independent interest.
Let
φβ,σpx, yq :“
1
σφ
`y´x1β
σ
˘
for y P Y and x P X . From Tokdar (2006a), we obtain
for σ2 ą σ1 ą σ2
2 and for each x P X ,
ż
Y
|φβ1,σ1px, yq ´ φβ2,σ2px, yq| dy ď
ˆ2
π
˙1{2}β2 ´ β1} ?p
σ2
` 3pσ2 ´ σ1q
σ1
Construct a sieve for pβ, σq as
Θa,h,l “
␣
φβ,σ : }β} ď a, l ď σ ď h
(
.
(4.17)
In the following Lemma, we provide an upper bound to NpΘa,h,l, ǫ, dSSq. The proof
is omitted as it follows trivially from Lemma 4.1 in Tokdar (2006a).
Lemma 55. There exists constants d1, d2 ą 0 such that NpΘa,h,l, ǫ, dSSq ď d1
` a
l
˘p `
d2 log h
l ` 1.
110

In the proof of Theorem 51, we will verify the suﬃcient conditions of Theo-
rem 50.
We calibrate Fd by a carefully chosen sequence of subsets Fn Ă Fd.
The fundamental problem with mixture models
ş
Npy; µ, σ2IpqdPpµq in estimat-
ing a multivariate density lies in attempting to compactify the model space by
t
ş
Npy; µ, σ2IpqdPpµq : Ppp´an, anspq ą 1 ´ δu for each σ leading to an en-
tropy ap
n growing exponentially with the dimension p.
Here we marginalize P
in
ş
Npy; µ, σ2IpqdPpµq to yield the following construction třmn
h“1 πhNpy; µh, σ2Ipq :
||µh|| ď an, h “ 1, . . . , mn, ř8
h“mn`1 πh ă ǫu leading to an entropy mn log an where
mn is related to the tail-decay of Ppř8
h“mn`1 πh ą ǫq. With this idea in place, we
extend the construction of Fn for conditional densities below.
Assume ǫ ą 0 is given. Let Ha
1 denote a unit ball in the RKHS of the covariance
kernel τ 2e´a}x´x1}2 and B1 is a unit ball in Cr0, 1sp. For numbers M, m, r, δ, construct
a sequence of subsets tBh, h “ 1, . . . , mu of Cr0, 1sp as follows.
Bh “
#`
M
a
r{δHr
1 `
ǫ
m2B1
˘
Y
`
Yaăδ MHa
1 `
ǫ
m2B1
˘
, if h “ 1, . . . , mη
YaăδnMnHa
1 `
ǫ
m2B1, if h “ mη ` 1, . . . , m.
The idea is to construct
Fn
“
"
f : fpy | xq “
8
ÿ
h“1
πhpxq 1
σh
φ
ˆy ´ x1βh
σh
˙
, tφβh,σhumn
h“1
P Θan,hn,ln, αh P Bh,n, h “ 1, . . . , mn, sup
xPX
ÿ
hěmn`1
πhpxq ď ǫ
*
.
(4.18)
for appropriate sequences am, ln, hn, Mn, mn, rn, δn to be chosen in the proof of The-
orem 51.
The following lemma is also crucial to the proof of Theorem 51 which allows us
to calculate the rate of decay of PpsupxPX πhpxq ą ǫq with mn.
Lemma 56. Let πh’s satisfy (4.11) with αh
„ GPp0, chq where chpx, x1q “
111

τ 2e´Ah}x´x1}2, h ě 1, τ 2 ą 0 ﬁxed. Then for some constant C7 ą 0,
ΠX
˜›››››
8
ÿ
h“mn`1
πh
›››››
8
ą ǫ
¸
ď e´C7mn log mn `
mn
ÿ
h“mη
n`1
PpAh ą δnq.
(4.19)
Proof. Let Wh “ ´ logr1´Φtα1
hus where α1
h “ infxPX αhpxq, Zh „ Gap1, γ0q. We will
choose an appropriate value for γ0 in the sequel. Let t0 “ ´ log ǫ ą 0. Observe that
ΠX
˜›››››
8
ÿ
h“mn`1
πh
›››››
8
ą ǫ
¸
“ ΠX
ˆ
sup
xPX
mn
ź
h“1
r1 ´ Φtαhpxqus ą ǫ
˙
ď ΠX
ˆ
mn
ź
h“mη
n`1
t1 ´ Φpα1
hqu ą ǫ
˙
“ ΠX
ˆ
´
mn
ÿ
h“mη
n`1
logt1 ´ Φpα1
hqu ă t0
˙
.
Note that if we had αhpxq ” αh „ Np0, 1q, then the right hand side above equals
ΠX
ˆ
´
mn
ÿ
h“1
logt1 ´ Φpαhqu ă t0
˙
“ ΠXpΛh ă t0q
where Λh „ Gapmn, 1q.
Then its easy to show that ΠXpΛh ă t0q À e´mn log mn.
However, the calculation gets complicated when αh’s are i.i.d realizations of a zero
mean Gaussian process. The proof relies on the fact that the supremum of Gaussian
processes has sub-Gaussian tails.
Below we calculate the rate of decay of ΠX
ˆ ››ř8
h“mn`1 πh
››
8 ą ǫ
˙
with mn. We
will show that there exists γ0, depending on ǫ and τ but not depending on n, such
that
ΠX
ˆ
mn
ÿ
h“mη
n`1
Wh ă t0
˙
ď ξpδnqmn´mη
nΠX
ˆ
mn
ÿ
h“mη
n`1
Zh ă t0
˙
`
mn
ÿ
h“mη
n`1
PpAh ą δnq.
(4.20)
where there exists a constant C5 ą 0 such that ξpxq “ C5xp{2 for x ą 0. Observe that
112

ΠX
ˆ řmn
h“mη
n`1 Wh ă t0
˙
ď ΠX
ˆ řmn
h“mη
n`1 Wh ă t0, Ah ď δn, h “ mη
n ` 1, . . . , mn
˙
`
řmn
h“mη
n`1 PpAh ą δnq.
Since ΠX
ˆ řmn
h“mη
n`1 Wh ă t0
˙
“ ΠX
ˆ řmn
h“mη
n`1pτ 1{τqWh ă τ 1t0{τ
˙
for some
τ 1 ă 1, we can re-parameterize t0 as τ 1t0{τ and τ as τ 1.
Hence without loss of
generality we assume τ ă 1.
Deﬁne g : r0, t0s Ñ ℜ, t ÞÑ ´Φ´1p1 ´ e´tq.
It holds that g is a continuous
function on p0, t0s. Assume α0 „ GPp0, c0q where c0px, x1q “ τ 2e´}x´x1}2. For h “
mη
n ` 1, . . . , mn,
Ppsup
xPX
αhpxq ě λ, Ah ď δnq ď Pp sup
xP?δnX
α0pxq ě λq.
Below we estimate PpsupxP?δnX α0pxq ě λq for large enough λ following Theorem
5.2 of Adler (1990). However extra care is required to identify the role of δn. Since
Npǫ, ?δnX , }¨}q ď C1p?δn{ǫqp,
ż ǫ
0
tlog Npǫ,
a
δnX , }¨}qu1{2dǫ ď C2ǫt1 `
a
logp1{ǫqu.
for some constant C2 ą 0. Hence
Pp sup
xPrnX
α0pxq ě λq ď C3p
a
δnλqp expr´1{2tλ ´ C2{λp1 `
a
log λqu2{τ 2s
ď C3δp{2
n λp`2t1 ´ Φpλ{τ 2qu ď C4δp{2
n t1 ´ Φpλqu.
for constants C3, C4 ą 0. The last inequality holds for all large λ because τ ă 1.
Hence there exists t1 P p0, t0q suﬃciently small and independent of n such that for
all t P p0, t1q, ΠXtsupxP?δnX α0pxq ě gptqu ď C4δp{2
n Φt´gptqu. Observe that
ΠXt sup
xP?δnX
α0pxq ě gptqu ď C4δp{2
n Φt´gptqu “ C4δp{2
n p1 ´ e´tq
ă C5δp{2
n p1 ´ e´γ0tq,
for any γ0 ą 1. Further choose γ0 large enough such that 2p1´e´γ0tq ą 1 @ t P rt1, t0s.
Hence PpWh ď t, Ah ď δnq ď ξpδnqPpZh ă tq @ t P p0, t0s where ξpδnq “ C5δp{2
n , with
113

C5 “ maxt2, C4u. Applying Lemma 80, we conclude (4.20) by induction. Lemma
80 is proved in Appendix A. As řmn
h“1 Zh „ Gapmn, γ0q, ΠX
ˆ řmn
h“1 Zh ă t0
˙
ď
e´C6mn log mn for some constant C6 ą 0. Since ξpδnqmn´mη
nΠX
ˆ řmn
h“1 Zh ă t0
˙
ď
pe´C7mn log mnq for some constant C7 ą 0, the result follows immediately.
4.5
Posterior consistency in mixtures of ﬁxed-π dependent processes
4.5.1
Kullback-Leibler property
The following theorem veriﬁes that ΠX has KL property at f0 P F ˚
d . The proof of
Theorem 57 is somewhat similar to that of Theorem 44 and can be found in Appendix
A.
Theorem 57. f0 P KLpΠXq for each f0 in F ˚
d if PX satisﬁes
T1. G0 is speciﬁed by µh „ GPpµ, cq, σh „ G0,σ where c is chosen so that GPp0, cq
has continuous path realizations and Πσ is absolutely continuous w.r.t. Lebesgue
measure on ℜ`.
T2. For every k ě 2, pπ1, . . . , πkq is absolutely continuous w.r.t. to the Lebesgue
measure on Sk´1.
T3. For any continuous function g : X ÞÑ ℜ,
PX
"
sup
xPX
|µhpxq ´ gpxq| ă ǫ
*
ą 0
h “ 1, . . . , 8 and for any ǫ ą 0.
4.5.2
Strong consistency with the q-integrated L1 neighborhood
Next we summarize the consistency theorem with respect to the q-integrated L1
topology. The proof of Theorem 58 is also similar to that of Theorem 51 and is
provided in Appendix A.
114

Theorem 58. Let µhpxq “ x1βh ` ηhpxq, βh „ Gβ and ηh „ GPp0, cq, h “ 1, . . . , 8
where cpx, x1q “ τ 2e´A}x´x1}2, App1`η2q{η2 „ Gapa, bq for some η2 ą 0.
F1. There exists sequences an, hn Ò 8, ln Ó 0 with
an
ln
“ Opnq, hn
ln
“ Openq,
and constants d1, d2, d3 and d4 ą 0 such that GβtBp0; anquc ă d1e´d2n and
G0,σtrln, hnsuc ď d3e´d4n.
F2. Ppř8
h“n πh ą ǫq À Ope´n1`η2plog nqpp`1qq.
then f0 P KLpΠXq implies that ΠX achieves strong posterior consistency at f0 with
respect to the q-integrated L1 topology.
Remark 59. F2 is satisﬁed if πh’s are made to decay more rapidly than the usual
Betap1, αq stick-breaking random variables, e.g, if πh “ νh
ś
lăhp1 ´ νhq and if νh „
Betap1, αhq where αh “ h1`η2plog hqp`1α0 for some α0 ą 0, then F2 is satisﬁed. Large
value of αh for the higher indexed weights favors smaller number of components.
4.6
Discussion
We have provided suﬃcient conditions to show posterior consistency in estimating
the conditional density via predictor dependent mixtures of Gaussians which include
probit stick-breaking mixtures of Gaussians and the ﬁxed-π dependent processes as
special cases. The problem is of interest, providing a more ﬂexible and informative
alternative to the usual mean regression. For both the models, we need the same set
of tail conditions (mentioned in F ˚
d ) on f0 for KL support. Although the ﬁrst prior is
ﬂexible in the weights and the second one in the atoms through their corresponding
GP terms, S1, S2, T1 and T3 show that veriﬁcation of KL property only requires
that both the GP terms have continuous path realizations and desired approximation
property. Moreover, for the second prior, any set of weights summing to one a.s. (T2)
suﬃces for showing KL property. Careful investigations of the prior for the GP kernel
for the ﬁrst model and the probability weights for the second one are required for
strong consistency. For the ﬁrst one we need the covariate dependence of the higher
115

indexed GP terms in the weights to fade oﬀ. On the other hand, for the second model,
the atoms can be i.i.d. realizations of a GP with Gaussian covariance kernel with
inverse-Gamma bandwidth while limiting the model complexity through a sequence
of probability weights which are allowed to decay rapidly. This suggests that full
ﬂexibility in the weights should be down-weighted by an appropriately chosen prior
while full ﬂexibility in the atoms should be accompanied by a restriction imposing
fewer number of components.
One alternative possibility is to specify a prior for the joint density hpx, yq “
qpxqfpy | xq, to induce a prior on the conditional fpy | xq, where qpxq denotes the joint
density of the covariates. Using such an approach, which was originally proposed by
M¨uller et al. (1996) using Dirichlet process mixtures of multivariate Gaussians, one
can potentially rely on the theory of large support and posterior consistency for i.i.d.
realizations from a multivariate distribution; for example, refer to Wu and Ghosal
(2010); Norets and Pelenis (2009). Unfortunately, such an approach has clear dis-
advantages. When interest focuses on the conditional distribution of fpy | xq it is
very appealing to avoid needing to model the joint density of the predictors, qpxq,
which will be multivariate in typical applications. In addition, standard models for
the joint distribution relying on multivariate Dirichlet process mixtures (refer also
to Shahbaba and Neal (2009); Park and Dunson (2009)), can have relatively poor
performance, because many mixture components may be introduced primarily to
provide a good ﬁt to the marginal qpxq, potentially leading to degradation of perfor-
mance in estimating fpy | xq for all x P X . The MGLRx and the Gaussian mixture
of ﬁxed-π dependent processes are examples of priors directly on the conditional
densities.
The q-integrated L1 topology concerns average accuracy for prediction of future
y values when the future x values are drawn from the same covariate distribution
Q that generate the data x’s. It is preferable to use a topology that can lead to
average accuracy guarantees when the future x’s are generated from any distribution
ν whose support is a subset of the support of Q. To accomplish this, we propose to
focus on a topology based on the supremum of L1 neighborhoods of the true density
116

in our future research.
Although, a more reasonable way of evaluating a Bayes procedure is to study the
posterior convergence rates, deriving the rates of convergence in our case substan-
tially complicates the analysis and is a topic of future research. Of course our sieve
construction can be used to derive the rates, while being more careful in estimat-
ing the concentration of the prior around the true density, the rates of decay of the
complement of the sieve and calculating the entropy.
117

5
Bayesian shape modeling with closed curves
5.1
Introduction
Boundaries of objects are widely studied across many disciplines, such as biomedical
imaging, cytology and computer vision. In describing complex boundaries, one can
use a parametric curve (2D) or surface (3D), i.e. Cptq : D1 Ñ R2 or Cptq : D2 Ñ R3
respectively, where D1 Ă R and D2 Ă R2. Note that this is diﬀerent from a typical
function estimation problem because the independent variable, t, is unknown. And
furthermore, the curve must be closed to produce a valid boundary.
A collection of introductory work on curve and surface modeling can be found
in Su and Liu (1989) and subsequent developments in Muller (2005). Popular repre-
sentations include Bezier curves, splines, and principal curves (Hastie and Stuetzle,
1989), the ﬁnal one being a nonlinear generalization of principal components involv-
ing smooth curves which pass through the middle of a data cloud. Su et al. (2011)
dealt with curve modeling based on stochastic processes when the observations are
given as a set of time-indexed points on manifolds. Kurtek et al. (2011) developed
an elegant theoretical framework for comparing and analyzing curves once the ﬁtted
118

curves are obtained.
Nonparametric representations of parametric curves and surfaces are widely used
(Barnhill, 1985; Lang and R¨oschel, 1992; Hagen and Santarelli, 1992; Aziz et al.,
2002), because they provide a ﬂexible model for a broad range of objects e.g. cells,
pollen grains, protein molecules, machine parts, etc.
Although there is a vast literature on estimating curves and surfaces, the majority
of this work focuses on estimating unrestricted functions. However, the boundary of
a simply-connected object must be a closed curve, which is a restriction on the curve
representation. Estimating a closed surface or curve involves a diﬀerent modeling
strategy and there has been little work in this regime, particularly from a Bayesian
point of view. To our knowledge, only Pati and Dunson (2011) developed a Bayesian
approach for ﬁtting a closed surface, using tensor-products.
In many applications featuring low-contrast images or sparse and noisy point
clouds, there is insuﬃcient data to recover local segments of the boundary in isola-
tion. Thus, it becomes critical to model the boundary’s global shape. Furthermore,
multiple related objects may share shape similarities that can be leveraged for im-
proved inference of boundaries. However, to the best of our knowledge, there are few
curve models which incorporate detailed shape information.
One strategy for analyzing complex curves is to refactor them in a multiscale fash-
ion, as done by Fourier and wavelet descriptors (Whitney, 1937; Zahn and Roskies,
1972; Mortenson, 1985; Persoon and Fu, 1977). These approaches decompose a curve
into components of diﬀerent scales, so that the coarsest scale components carry the
global approximation information while the ﬁner scale components contain the lo-
cal detailed information.
Mokhtarian and Mackworth (1992), D´esid´eri and Janka
(2004) and D´esid´eri et al. (2007) also proposed multiscale curves. Such multiscale
transforms make it easier to compare objects that share the same coarse shape, but
diﬀer on ﬁner details, or vice versa. The ﬁner scale components can also be discarded
119

to yield a ﬁnite and low-dimensional representation. However, none of these methods
are model-based.
In this chapter, we propose a Bayesian hierarchical model for object boundaries,
which addresses all of the aforementioned problems: 1) guaranteeing valid boundaries
through closed curves, 2) enabling borrowing of information when ﬁtting multiple
similar objects, and 3) employing a multiscale representation suitable for shape anal-
ysis.
The key innovation in our model is a curve-generating random process which can
approximate the whole range of simply connected 2D shapes. It is based on applying
a sequence of multiscale deformations to a novel type of closed curve R´oth et al.
(2009). Because the model is multiscale, it is able to detect and borrow inter-object
similarities at a particular resolution even if similarities are not present at other res-
olutions. This process also yields a ‘central curve’ that summarizes multiple objects.
Dryden and Mardia (1998) discussed a related concept of mean shape, shape vari-
ability and various methods of estimating them in the context of landmark-based
analysis.
En route, we solve several important sub-problems that may be generally useful in
the study of curve and surface ﬁtting. First, we develop a model-based approach for
parameterizing point cloud data. Second, we show how fully Bayesian joint modeling
can be used to incorporate several pieces of auxiliary information in the process of
curve-ﬁtting, such as when a surface orientation is reported for each point within a
point cloud. Lastly, the concept of multi-scale deformation can be generalized to 3D
surfaces in a straightforward manner.
120

5.2
Shape-generating random process
5.2.1
Overview
Our shape-generating random process starts with a closed curve and performs a
sequence of multiscale deformations to generate a ﬁnal curve. In §5.2.2, we introduce
the Roth curve developed by R´oth et al. (2009), which will be used to represent the
object boundary.
Then, in §5.2.3, we demonstrate how to deform a Roth curve
at multiple scales to produce any simply-connected shape. Using the mechanisms
developed in §5.2.2 and §5.2.3, we present the full random process in §5.2.5. In §5.4,
we use this as a prior distribution for curve-ﬁtting.
5.2.2
Roth curve
A Roth curve is a closed parametric curve, C : r´π, πs Ñ R2, deﬁned by a set of 2n`1
points in R2, tcj, j “ 1, . . . , 2n ` 1u (also known as control points), where n is the
degree of the curve and we may choose it to be any positive integer. For convenience,
we will refer to the total number of control points as J, where Jpnq “ 2n ` 1. For
notational simplicity, we will drop the dependence of n in Jpnq. As a function of t,
the curve can be viewed as the trajectory of a particle over time. At every time t,
the particle’s location is deﬁned as some convex combination of all control points.
The weight accorded to each control point in this convex combination varies with
time according to a set of basis functions, tBn
j ptq, j “ 1, . . . , Ju, where Bn
j ptq ą 0
and řJ
j“1 Bn
j ptq “ 1 for all t.
Cptq
“
Jÿ
j“1
cjBn
j ptq, t P r´π, πs ,
(5.1)
Bn
j ptq
“
hn
2n
"
1 ` cos
ˆ
t ` 2πpj ´ 1q
2n ` 1
˙*n
, hn “ p2nn!q2
p2n ` 1q! ,
(5.2)
121

where cj “ rcj,x cj,ys1 speciﬁes the location of the jth control point and Bn
j : r´π, πs Ñ
r0, 1s is the jth basis function. For simplicity, we omit the superscript n denoting a
basis function’s degree, unless it requires special attention. This representation is a
type of Bezier curve. The Roth curve has several appealing properties:
1. It is fully deﬁned by a ﬁnite set of control points, despite being an inﬁnite
dimensional curve.
2. It is always closed, i.e. Cp´πq “ Cpπq. This is necessary to represent the
boundary of an object.
3. All basis functions are nonlinear translates of each other, and are evenly spaced
over the interval r´π, πs. They can be cyclically permuted without altering the
curve. This implies that each control point exerts the same ‘inﬂuence’ over the
curve.
4. A degree 1 Roth curve having 3 control points is always a circle or ellipse.
5. Any closed curve can be approximated arbitrarily well by a Roth curve, for
some large degree n. This is because the Roth basis, for a given n, spans the
vector space of trigonometric polynomials of degree n and as n Ñ 8, the basis
functions span the vector space of Fourier series. We elaborate on this in §5.3.
6. Roth curves are inﬁnitely diﬀerentiable (C8).
5.2.3
Deforming a Roth curve
A Roth curve can be deformed simply by translating some of its control points. We
now formally deﬁne deformation and illustrate it in Figure 5.1.
Deﬁnition 60. Suppose we are given two Roth curves,
Cptq “
Jÿ
j“1
cjBjptq,
rCptq “
Jÿ
j“1
rcjBjptq,
(5.3)
122

Figure 5.1: Deformation of a Roth curve
where for each j, rcj “ cj ` Rjdj, dj P R2 and Rj is a rotation matrix. Then, we say
that Cptq is deformed into rCptq by the deformation vectors tdj, j “ 1, . . . , Ju.
Each Rj orients the deformation vector dj relative to the original curve’s surface.
As a result, positive values for the y-component of dj always correspond to outward
deformation, negative values always correspond to inward deformation, and dj’s x-
component corresponds to deformation parallel to the surface. We will call Rj a
deformation-orienting matrix. In precise terms,
Rj
“
„
cospθjq
´ sinpθjq
sinpθjq
cospθjq

,
(5.4)
where θj is the angle of the curve’s tangent line at qj “ ´2πpj´1q
2n`1
, the point where the
control point cj has the strongest inﬂuence: qj “ arg max
tPr´π,πs Bjptq. θj can be obtained
by computing the ﬁrst-derivative of the Roth curve, also known as its hodograph.
Deﬁnition 61. The hodograph of a Roth curve is given by:
Hptq
“
Jÿ
j“1
cj
d
dtBjptq,
(5.5)
123

where
d
dtBjptq is given by
´
2
p2n ` 1q
`2n
n
˘
Jÿ
j“1
cj
n´1
ÿ
k“0
ˆ2n
k
˙
pn ´ kq sin
ˆ
pn ´ kqt ` 2pn ´ kqpj ´ 1qπ
2n ` 1
˙
,
(5.6)
where t P r´π, πs. If we view Cptq as the trajectory of a particle, Hptq intuitively
gives the velocity of the particle at point t.
We can use simple trigonometry to determine that
θj “ arctan
ˆHypqjq
Hxpqjq
˙
.
(5.7)
Note that Rj is ultimately just a function of tcj P R2, j “ 1, . . . , Ju.
Next, we show how to alter the scale of deformation, using an important concept
called degree elevation.
Deﬁnition 62. Given any Roth curve, we can use degree elevation to re-express
the same curve using a larger number of control points (a higher degree).
More
precisely, if we are given a curve of degree n, Cptq “ ř2n`1
j“1 cjBn
j ptq, we can elevate
its degree by any positive integer v, to obtain a new degree elevated curve: pCptq “
ř2pn`vq`1
j“1
pcjBn`v
j
ptq such that Cptq “ pCptq for all t P r´π, πs. In pCptq, each new
degree-elevated control point, pcj, can be deﬁned in terms of the original control points,
tci, i “ 1, . . . , 2n ` 1u:
1
2n ` 1
2n`1
ÿ
i“1
ci `
`2pn`vq
n`v
˘
hn
22n´1
n´1
ÿ
k“0
`2n
k
˘
`2pn`vq
v`k
˘
2n`1
ÿ
i“1
cospξpk, n, iqqci,
where ξpk, n, iq “
´
pn ´ kq
´
´2pi´1qπ
2pn`vq`1
¯
` 2pn´kqpi´1qπ
2n`1
¯
.
Although daunting to read, the only crucial points to note about this relationship
are that pcj is linear in ci’s, i “ 1, . . . , 2n`1 and that the ‘inﬂuence’ of a single control
124

point shrinks after degree elevation. This is because the curve is now shared by a
greater total number of control points. This implies that after degree-elevation, the
translation of any single control point will cause a smaller, ﬁner-scale deformation
to the curve’s shape.
Thus, degree elevation can be used to adjust the scale of
deformation. We exploit this strategy in the random process proposed in §5.2.5.
To that end, we ﬁrst rewrite all of the concepts described above in more com-
pact vector notation. Note that the formulas for degree elevation, deformation, the
hodograph and the curve itself all simply involve linear operations on the control
points.
5.2.4
Vector notation
Rewrite the control points in a ‘stacked’ vector of length 2J,
c “ pc1,x, c1,y, c2,x, c2,y, . . . , cJ,x, cJ,yq1.
(5.8)
The formula for a Roth curve given in (5.1) can be rewritten as
Cptq
“
Xptqc
(5.9)
Xptq
“
„
B1ptq
0
B2ptq
0
¨ ¨ ¨
BJptq
0
0
B1ptq
0
B2ptq
¨ ¨ ¨
0
BJptq

.
(5.10)
The formula for the hodograph given in (5.5) is rewritten as
Hptq “
9
Xptqc,
9Xptq “ d
dtXptq.
(5.11)
Deformation can be written as
rc “ c ` Tpcqd, d “ pd1,x, d1,y, d2,x, d2,y, . . . , dJ,x, dJ,yq1,
(5.12)
Tpcq “ blockpR1, R2, . . . , RJq,
(5.13)
where blockpA1, . . . , Aqq is a pq ˆ pq block diagonal matrix using p ˆ p matrices
Ai, i “ 1, . . . , q. We call T the stacked deformation-orientating matrix. Note that T
125

is a function of c, because each Rj depends on c. Degree elevation can be written as
the linear operator, E:
pc “ Ec,
E “ pEi,jqn`v,n
i“1,j“1.
where
Ei,j “
1
2n ` 1 `
`2pn`vq
n`v
˘
hn
22n´1
n´1
ÿ
k“0
`2n
k
˘
`2pn`vq
v`k
˘ cospξpk, n, iqq.
We will maintain this vector notation throughout the rest of the chapter.
5.2.5
Shape-generating Random Process
The random process starts with some initial Roth curve, speciﬁed by an initial set
of control points, cp0q. From here on, we will refer to all curves by the stacked vector
of their control points, c. Then, drawing on the deformation and degree-elevation
operations deﬁned earlier, we repeatedly apply the following recursive operation R
times:
pcpr´1q “ Ercpr´1q,
dprq „ Npµr, Σrq,
cprq “ pcpr´1q ` Trpcpr´1qqdprq
(5.14)
resulting in a ﬁnal curve cpRq. In other words, (i) degree elevate the current curve,
(ii) randomly deform it, and repeat a total of R times. This random process speciﬁes
a probability distribution over cpRq.
We now elaborate on the details of this recursive process. The parameters of the
process are
1. R P Z, the number of steps in the process.
2. nr P Z, the degree of the curve cprq, for each r “ 0, . . . , R.
The sequence
of tnruR
0 must be strictly monotonically increasing. For convenience, we will
denote the number of control points at a certain step r to be Jr “ 2nr ` 1.
126

Figure 5.2: An illustration of the shape generation process. From left to right:
1) initial curve speciﬁed by three control points, 2) the same curve after degree
elevation, 3) deformation, 4) degree elevation again, 5) deformation again. Dark
lines indicate the curve, pale dots indicate the curve’s control points, and pale lines
connect the control points in order.
3. µr P R2Jr, the average set of deformations applied at step r “ 0, . . . , R. Note
that this vector contains a stack of deformations, not just one.
4. Σr P R2Jrˆ2Jr, the covariance in the set of deformations applied at step r “
0, . . . , R.
For these parameters, Er is the degree-elevation matrix going from degree nr´1to nr,
Np¨, ¨q is a 2Jr-variate normal distribution and Tr is the stacked deformation orienting
matrix.
We take special care in deﬁning the initial curve, cp0q. We choose cp0q to be degree
n0 “ 1, which guarantees that it is an ellipse. For j “ 1, 2, 3, we deﬁne each control
127

point as
cp0q
j
“
p0, 0q1 ` Rθjdp0q
j ,
(5.15)
Rθj
“
rotation matrix where θj “ 2πj
3 ,
(5.16)
and where each dp0q
j
P R2 is a random deformation vector. In words: we start with
a curve that is just a point at the origin, Cptq ” p0, 0q, and apply three random
deformations which are rotated by a radially symmetric amount: 0˝, 120˝ and 240˝
(note that the ﬁnal deformations are not radially symmetric, since each dj is randomly
drawn). We will write this in vector notation as
dp0q
„
Npµ0, Σ0q,
cp0q
“
0 ` T0dp0q.
The deformations essentially ‘inﬂate’ the curve into some ellipse. This completes our
deﬁnition of the random process.
We now give some intuition about the process and each of its parameters, and
deﬁne several additional concepts which make the process easier to interpret. The
random process gives a multiscale representation, because each step in the process
produces increasingly ﬁne-scale deformations, through degree-elevation.
R is then the number of scales or ‘resolutions’ captured by the process. Each
nr speciﬁes the number of control points at resolution r. We will use Sr to denote
the class of curves that can be exactly represented by a degree nr Roth curve. If
tnruR
1 is monotonically increasing, then S1 Ă S2 Ă . . . Ă SR. Thus, the deformations
dprq roughly describe the additional details gained going from Sr´1 to Sr. Modeling
multiple resolutions allows better ‘borrowing of information’ between subjects. For
example, we may wish to model a human body before and after it has lost weight.
The two shapes will diﬀer in their coarse outline, but share the same ﬁne-scale
128

features (a nose, ears, etc.). If one object is missing a large part of its boundary,
we may borrow ﬁne-scale features without incorrectly importing the coarse outline.
Thus, resolutions should be chosen to reﬂect the levels at which shapes are similar.
It is crucial that we deﬁne each resolution relative to the surface orientation of the
previous resolution. For example, if two human bodies only diﬀer by the tilted angle
of their head, it should be possible to observe that the facial features are identical,
once diﬀerences in the coarser level head-orientation have been removed.
µr is the mean deformation at level r. Based on tµr, r “ 0, . . . , Ru, we deﬁne the
‘central curve’ of the random process, cµ as:
cµ
:“
cpRq
µ
cprq
µ
“
Ercpr´1q
µ
` Trpcpr´1q
µ
qµr
Note that c˚ is simply the deterministic result of the random process if each
dprq “ µr, rather than being drawn from a distribution centered on µr. Thus, all
shapes generated by the process tend to be deformed versions of the central curve.
We illustrate this in Figure 5.3. If the random process is used to describe a collection
of objects, the central curve provides a good summary.
(a)
(b)
(c)
Figure 5.3: Random samples from the shape-generating process (red: the cen-
tral curve, blue: random samples). (a) A moon-shaped collection, (b) star-shaped
collection, (c) high-variance but symmetry-constrained collection.
Σr determines the covariance of the deformations at level r. This naturally con-
129

trols the variability among shapes generated by the process. If the variance is very
small, all curves will be very similar to the central curve. Σr can also be chosen to
induce correlation between deformation vectors at the same resolution, in the typical
way that correlation is induced between dimensions of a multivariate normal distri-
bution. This allows us to incorporate higher-level assumptions about shape, such as
reﬂected or radial symmetry. For example, if R “ 2, n1 “ 1 and n2 “ 2, we can
specify perfect correlation in Σ2, such that dp2q
1
“ dp2q
4
and dp2q
2
“ dp2q
3 . The resulting
curves are guaranteed to be symmetrical along an axis of reﬂection.
In the subsequent sections 5.4 and 5.5, we show how to use our random process
to guide curve-ﬁtting for point clouds and image data.
5.3
Properties of the Prior
5.3.1
Support
Let the H¨older class of periodic functions on r´π, πs of order α be denoted by
Cαpr´π, πsq. Deﬁne the class of closed parametric curves SCpα1, α2q having diﬀerent
smoothness along diﬀerent coordinates as
SCpα1, α2q :“ tS “ pS1, S2q : r´π, πs Ñ R2, Si P Cαipr´π, πsq, i “ 1, 2u.
(5.17)
Consider for simplicity a single resolution Roth curve with control points tcj, j “
0, . . . , 2nu. Assume we have independent Gaussian priors on each of the two coordi-
nates of cj for j “ 0, . . . , 2n, i.e., Cptq “ ř2n
j“0 cjBn
j ptq, cj „ N2p0, σ2
jI2q, j “ 0, . . . , 2n.
Denote the prior for C by ΠCn. ΠCn deﬁnes an independent Gaussian process for
each of the components of C. Technically speaking, the support of a prior is deﬁned
as the smallest closed set with probability one.
Intuitively, the support charac-
terizes the variety of prior realizations along with those which are in their limit.
We construct a prior distribution to have large support so that the prior realiza-
tions are ﬂexible enough to approximate the true underlying target object. As re-
130

viewed in van der Vaart and van Zanten (2008b), the support of a Gaussian process
(in our case ΠCn) is the closure of the corresponding reproducing kernel Hilbert space
(RKHS). The following Lemma 63 describes the RKHS of ΠCn, which is a special
case of Lemma 2 in Pati and Dunson (2011). Refer to Appendix C for the proofs.
Lemma 63. The RKHS Hn of ΠCn consists of all functions h : r´π, πs Ñ R2 of the
form
hptq “
2n
ÿ
j“0
cjBn
j ptq
(5.18)
where the weights cj range over R2. The RKHS norm is given by
||h||2
Hn “
2n
ÿ
j“0
||cj||2{σ2
j .
(5.19)
The following theorem describes how well an arbitrary closed parametric surface
S0 P SCpα1, α2q can be approximated by the elements of Hn for each n.
Theorem 64. For any ﬁxed S0 P SCpα1, α2q, there exists h P Hn with ||h||2
Hn ď
K1
ř2n
j“0 1{σ2
j such that
||S0 ´ h||8 ď K2n´αp1q log n
(5.20)
for some constants K1, K2 ą 0 independent of n.
This shows that the Roth basis expansion is suﬃciently ﬂexible to approximate
any closed curve arbitrarily well. Although we have only shown large support of
the prior under independent Gaussian priors on the control points, the multiscale
structure should be even more ﬂexible and hence rich enough to characterize any
closed curve. We can also expect minimax optimal posterior contraction rates using
the prior ΠCn similar to Theorem 2 in Pati and Dunson (2011) for suitable choices
of prior distributions on n.
131

Figure 5.4: Inﬂuence of the control points on the Roth curve
5.3.2
Inﬂuence of the control points
The unique maximum of basis function Bn
j ptq deﬁned in (5.1) is at t “ ´2πpj ´1q{J,
therefore the control point cj has the most signiﬁcant eﬀect on the shape of the
curve in the neighborhood of the point Cp´2πpj ´ 1q{Jq. Note that Bn
j ptq vanishes
at t “ π´2πpj ´1q{J, thus cj has no eﬀect on the corresponding point i.e., the point
of the curve is invariant under the modication of cj. The control point cj aﬀects all
other points of the curve, i.e. the curve is globally controlled. These properties are
illustrated in Figure 5.4.
However, we emphasize following Proposition 5 in R´oth et al. (2009) that while
control points have a global eﬀect on the shape, this inuence tends to be local and
dramatically decreases on further parts of the curve, especially for higher values of
n.
5.4
Inference from Point Cloud Data
We now demonstrate how our multiscale closed curve process can be used as a prior
distribution for ﬁtting a curve to a 2D point cloud. Data examples are given in §5.8.
As a byproduct of ﬁtting, we also obtain an intuitive description of the shape in
terms of deformation vectors.
Assume that the data consist of points tpi P R2, i “ 1, . . . , Nu concentrated near
132

a 2D closed curve. Since a Roth curve can be thought of as a function expressing the
trajectory of a particle over time, we view each data point, pi, as a noisy observation
of the particle’s location at a given time ti,
pi “ Cptiq ` ǫi,
ǫi „ N2p0, σ2I2q.
(5.21)
(5.21) shares a similar form to nonlinear factor models, where ti is the latent factor
score. We assume that the noise variance σ2 is known, but if not, one can easily
place a conjugate inverse Gamma prior on it. First, we will rewrite the point cloud
model in stacked vector notation. Deﬁning
p “ pp1,x, p1,y, . . . , pN,x, pN,yq1,
ǫ “ pǫ1,x, ǫ1,y, . . . , ǫN,x, ǫN,yq1
t “ pt1,x, t1,y, . . . , tN,x, tN,yq1,
Xptq1 “ rXpt1q1Xpt2q1 . . . XptNq1s
we have
p “ Xptqc ` ǫ,
ǫ „ N2Np0, σ2I2Nq
(5.22)
where Xptiq is as deﬁned in (5.11).
To ﬁt a Roth curve through the data, we want to infer Ppc | pq, the posterior
distribution over control points c, given the data points p. To compute this, we must
specify Ppp | cq, the likelihood, and Ppcq, the prior distribution over Roth curves. We
choose Ppcq to be the probability distribution induced by the shape-generating ran-
dom process speciﬁed in §5.2.5. From (5.22), we can specify the likelihood function
as,
PptpiuN
1 | tciuJ
1q
“
N
ź
i“1
N2
ˆ
pi;
Jÿ
j“1
cjBjptiq, σ2I2
˙
,
(5.23)
Ppp | cq
“
N2Npp; Xptqc, σ2I2Nq.
(5.24)
This completes the Bayesian formulation for inferring c, given p and t. In §5.7, we
describe the exact method for performing Bayesian inference.
133

In many applications, ti is not known and can be treated as a latent variable. We
propose a prior for ti conditionally on c, which is designed to be uniform over the
curve’s arc-length. This prior is motivated by the frequentist literature on arc-length
parameterizations Madi (2004), but instead of assigning the values tti P r´π, πsu
in a deterministic preliminary step prior to statistical analysis, we use a Bayesian
approach to formally accommodate uncertainty in parameterization of the points.
Deﬁne the arc-length function A : r´π, πs ÞÑ R`
Apuq :“ Apu; pc0, . . . , c2nqq “
ż u
´π
||Hptq||dt.
(5.25)
Note that A is monotonically increasing and satisﬁes Ap´πq
“
0, Apπq
“
Lpc0, . . . , c2nq where Lpc0, . . . , c2nq is the length of the curve conditional on the control
points pc0, . . . , c2nq and is given by
şπ
´π ||Hptq||dt.
Given pc0, . . . , c2nq, we draw li „ Unifp0, Lpc0, . . . , c2nqq and set ti “ A´1pliq.
Thus we obtain a prior for the ti’s which is uniform along the length of the curve
and is given by
pptq “
||Hptq||
şπ
´π ||Hptq||dt.
Thus the high velocity regions on the curve are penalized more and the low velocity
regions are penalized less to enable uniform arc-length parameterizations.
Uniform arc-length parametrization is extremely important for two reasons. First,
it ensures that the control points are well distributed along the entire object bound-
ary. This means that a roughly equal amount of ”detail” is given to describing any
given length of the curve. Second, it standardizes parametrization among multiple
curves to make them directly comparable.
We will discuss a griddy Gibbs algorithm for implementing the arc-length
parametrization in a fully Bayesian framework in §5.7.
134

5.5
Inference from Pixelated Image Data
In this section, we show how to model image data by converting it to point cloud
data. We also show how image data gives a bonus estimate for the object’s surface
orientation, ωi at each point pi. We incorporate this extra information into our model
to improve ﬁtting, with essentially no sacriﬁce in computational eﬃciency.
A grayscale image can be treated as a function Z : R2 Ñ R. The gradient of this
function, ∇Z : R2 Ñ R2 is a vector ﬁeld, where ∇Zpx, yq is a vector pointing in the
direction of steepest ascent at px, yq. In computer vision, it is well known that the
gradient norm of the image, ||∇Z||2 : R2 Ñ R approximates a ‘line-drawing’ of all
the high-contrast edges in the image. Our goal is to ﬁt the edges in the image with
our model.
In practice, an image is discretized into pixels tza,b | a “ 1, . . . , X, b “ 1, . . . , Y u
but a discrete version of the gradient can still be computed by taking the diﬀerence
between neighboring pixels, such that one gradient vector, ga,b is computed at each
pixel.
The image’s gradient norm is then just another image, where each pixel
ma,b “ ||ga,b||2.
Finally, we extract a point cloud: tpa, bq | ma,b ą M, a “ 1, . . . , X, b “ 1, . . . , Y u
where M is some user-speciﬁed threshold. Each point pa, bq can still be matched
to a gradient vector ga,b. For convenience, we will re-index them as pi and gi. The
gradient vector points in the direction of steepest change in contrast, i.e. it points
across the edge of the object, approximating the object’s surface normal. The surface
orientation is then just ωi “ arctanp gi,y
gi,xq.
In the following, we describe a model relating a Roth curve to each ωi. This
model can be used together with the model we speciﬁed earlier for the pi.
135

5.5.1
Modeling surface orientation
Denote by vi “ pHxptiq, Hyptiqq P R2 the velocity vector of the curve Cptq at the
parameterization location ti, i “ 1, . . . , N. Note that vi is always tangent to the
curve. Since each ωi points roughly normal to the curve, we can rotate all of them
by 90 degrees, θi “ ωi ` π
2, and treat each θi as a noisy estimate of vi’s orientation.
Note that we cannot rotate the vector gi by 90 degrees and directly treat it as a
noisy observation of vi. In particular, gi ’s magnitude bears no relationship to the
magnitude of vi: ||gi|| is the rate of change in image brightness when crossing the
edge of the object, while ||vi|| describes the speed at which the curve passes through
pi.
Suppose we did have some noisy observation of vi, denoted ui. Then, we could
have speciﬁed the following linear model relating the curve tcj, j “ 1, . . . , Ju to the
ui’s:
ui
“
vi ` δi
(5.26)
“
Jÿ
j“1
cj
d
dtBjptiq ` δi
(5.27)
for i “ 1, . . . , N where δi „ N2p0, τ 2I2q. Instead, we only know the angle of ui, θi.
In §5.7, we show that using this model, we can still write the likelihood for θi, by
marginalizing out the unknown magnitude of ui. The resulting likelihood still results
in conditional conjugacy of the control points.
5.6
Fitting a collection of curves
We can extend our methodology in the previous section to simultaneously ﬁt and
characterize a collection of K separate point clouds, via hierarchical modeling. In the
previous section, we used the random shape process as a prior with ﬁxed parameters.
136

Now, we will instead treat the random process as a latent mechanism which generated
all K objects. The inferred parameters of the latent process then characterize the
collection of shapes.
As a reminder, the parameters of the random process and their interpretations
are deﬁned in §5.2.5. Rather than ﬁxing their values, we will treat µr and Σr for
r “ 0, . . . , R as unknowns and place the following priors on them:
µr
„
N2Jrpµµr, Σµrq,
Σ´1
r
“
diag
ˆ”
τ prq
1,x, τ prq
1,y, τ prq
2,x, τ prq
2,y, ¨ ¨ ¨ , τ prq
Jr,x, τ prq
Jr,y
ı1˙
,
τ prq
u
„
Gamma pατ, βτq
for u “ t1, . . . Jru ˆ tx, yu,
where diag pvq takes any vector v P Rd and produces the diagonal matrix V P
Rdˆd with elements of v along the diagonal.
We will continue to treat R and
tnr | r “ 0, . . . , Ru as ﬁxed, although we plan to consider inferring these quanti-
ties in future work.
Note that the prior on Σr only permits diagonal covariance
structure, assuming independence between deformations (and between the x/y com-
ponents of each deformation). In future work, it will be interesting to remove this
simplifying assumption and characterize inter-deformation correlation. Nonetheless,
the inferred values of
!
τ prq
u
| u “ t1, . . . Jru ˆ tx, yu, r “ 0, . . . , R
)
can still be usefully
interpreted. For example, a high value for τ pRq
2,y indicates that there is a high-level
of variability in the second deformation vector at resolution R (implying a fairly
ﬁne-scale deformation). Since it is the y-component, this corresponds to variability
normal to the object surface.
We now formalize the concept that all K objects are generated from a single
random process. Denote the kth point cloud as pk for k “ 1, . . . K. Each pk is ﬁt by
a curve ck, which is composed from the deformations
␣
dprq,k | r “ 0, . . . , R
(
and each
dprq,k „ N pµr, Σrq. This hierarchical structure also induces dependence between the
137

K curves, enabling them to borrow information from each other during ﬁtting.
An additional challenge that arises from modeling multiple objects is inter-object
alignment (also known as registration). This typically involves removing diﬀerences
in object position, orientation and scale.
Here, we only deal with position and
orientation. According to (5.15), our random process generates shapes centered at
p0, 0q and rotated to a ﬁxed angle. However, in an actual collection of shapes, each
object is rotated to a diﬀerent angle and centered at a diﬀerent location. We can
modify (5.15) to account for this simply by adding latent variables for the position,
mk P R2, and orientation, φk P r´π, πs, of each object k:
cp0q,k
j
“
mk ` RφkRθjdp0q,k
j
,
where Rφkis a rotation matrix. We place a uniform prior on φk and a normal prior
on mk. It can be desirable to put a more sophisticated spatial prior on mk, but our
focus here is on modeling object boundaries, not their location.
The orientation of level r “ 0 orients all subsequent levels. This is suﬃcient to
align the entire collection and make the deformation vectors of each shape directly
comparable.
We also note that the deﬁnition of φk has an important anchoring eﬀect on tk.
The prior for tk is conditional on the curve ck. Since the random process prior for
the curve is oriented by φk, the prior for tk favors parametrizations that conform to
this orientation.
5.7
Posterior computation
In §5.6, we presented a model for characterizing and ﬁtting a collection of K closed
curves, with unknown underlying parametrization. We now present an MCMC algo-
rithm for sampling from the joint posterior of this model. This involves deriving the
conditional posteriors of mk, dprq,k, µr, Σr and tk for r “ 0, . . . , R and k “ 1, . . . , K.
138

5.7.1
Conditional posteriors for mk and dprq,k
The conditional posteriors for mk and dprq,k are the most challenging to sample
from, because our model’s likelihood function is nonlinear in these terms, preventing
conditional conjugacy. To overcome this, we derive a linear approximation to the true
likelihood function, which does yield conditional conjugacy, and then use samples
from the approximate conditional posterior as proposals in a Metropolis-Hastings
step.
We ﬁrst present the source of nonlinearity in the likelihood function. Recall from
§5.4 that:
Pppk | cpRq,kq “ N2Nppk; XptkqcpRq,k, σ2I2Nq.
From §5.2.5, we note that cpRq,k is the result of combining the deformations tdprq,k |
r “ 0, . . . , Ru through the following recursive relation, with mk appearing in the base
case:
cprq,k
“
Ercpr´1q,k ` Tr
`
cpr´1q,k˘
dprq,k
(5.28)
cp0q,k
“
mk ` TφkT0dp0q,k
(5.29)
At each step r of the recursive process, the deformation-orienting matrix Tr is a
nonlinear function of the previous cpr´1q,k. As a result, cpRq,k is nonlinear in dprq,k for
r “ 0, . . . , R ´ 1. For any given step of the process, we can replace the true recursive
relation with a linear approximation. In particular, we will substitute Trpcpr´1q,kqdprq,k
with ˆTr,kcpr´1q,k, where ˆTr,k will be derived shortly. The new approximate step is then
cprq,k
«
´
Er ` ˆTr,k
¯
cpr´1q,k.
(5.30)
If we wish to write cpRq,k linearly in terms of cprq,k for any r “ 0, . . . , R, we can
replace every recursive step from r to R with the approximate step given in (5.30).
139

We emphasize that steps 0, . . . r ´1 follow the original recursive relation. This yields
the following approximation:
cpRq,k « ΩR
r`1cprq,k,
Ωb
a “
bź
ρ“a
´
Eρ ` ˆTρ,k
¯
.
(5.31)
Now, by combining (5.28) and (5.31), we have that:
cpRq,k
«
#
ΩR
r`1
“
Ercpr´1q,k ` Tr
`
cpr´1q,k˘
dprq,k‰
r ą 0
ΩR
1
`
mk ` TφkT0dp0q,k˘
r “ 0
Thus, the approximation of cpRq,k can be written linearly in terms of any dprq,k
or mk. Note that it is still nonlinear in dpρq,k for any ρ ‰ r. However, for MCMC
sampling, we only need one dprq,k to be linear at a time, holding all others ﬁxed.
Lastly, we note that the approximation becomes increasingly good as r approaches
R, because the number of approximate steps (contained in Ωb
a) decrease.
We are now ready to derive the approximate conditional posteriors for mk and
dprq,k. First, we claim that these posteriors can all be written in the following form
for generic ‘x’, ‘y’ and ‘z’.
Ppx | ´q
9
N py; Qx, Σyq N px; z, Σxq
(5.32)
Ppx | ´q
“
N
´
ˆµ, ˆΣ
¯
,
ˆ
Σ´1 “ Σ´1
x `
ÿ
k
Q1Σ´1
y Q,
(5.33)
ˆµ
“
ˆΣ
ˆ
Σ´1
x z `
ÿ
k
Q1Σ´1
y y
˙
.
(5.34)
Note that each approximate conditional posterior is simply a multivariate normal.
We now show that each approximate posterior can be rearranged to match the form
140

of (5.32) - (5.34). Papproxpmk | ´q
9
N
`
pk; XptkqcpRq,k, σ2I2Nk
˘
Npmk; µm, Σmq
9
N
´
pk; XptkqΩR,k
1
`
mk ` T0dp0q,k˘
, σ2I2Nk
¯
Npmk; µm, Σmq
9
N
´
pk ´ XptkqΩR,k
1
T0dp0q,k; XptkqΩR,k
1
mk, σ2I2Nk
¯
Npmk; µm, Σmq
Papproxpdprq,k | ´q
9
N
`
pk; XptkqcpRq,k, σ2I2Nk
˘
Npdprq,k; µr, Σrq
9
N
`
pk; XptkqΩR
r`1
“
Ercpr´1q,k ` Tr
`
cpr´1q,k˘
dprq,k‰
, σ2I2Nk
˘
Npdprq,k; µr, Σrq
9
N
`
pk ´ XptkqΩR
r`1Ercpr´1q,k; XptkqΩR
r`1Tr
`
cpr´1q,k˘
dprq,k, σ2I2Nk
˘
Npdprq,k; µr, Σrq
We then use Papproxpmk | ´q and Papproxpdprq,k | ´q as M-H proposal distributions to
sample from their true counterparts. Both are multivariate normals and if necessary,
their variance parameters may be tuned to improve sampling eﬃciency.
5.7.2
Derivation of the approximate deformation-orienting matrix
For visual clarity of the derivation, we will temporarily drop superscripts denoting
the resolution r and object index k of each variable. First, we recall from §5.2 that
Tpcq is a block diagonal matrix with blocks consisting of the rotation matrices Rj, for
j “ 1, . . . , J (where J is the total number of control points at the particular resolution
r). Each Rj rotates its corresponding deformation, dj, by θj “ arctan
´
Hypqjq
Hxpqjq
¯
. Now,
using the identities
cosparctanpx{yqq
“
x
a
x2 ` y2,
sinparctanpx{yqq “
y
a
x2 ` y2,
we can write Rj as:
Rj
“
1
sjpcq
„
Hxpqjq
´Hypqjq
Hypqjq
Hxpqjq

141

where sjpcq “
a
Hxpqjq2 ` Hypqjq2. This term intuitively represents the “speed” of
the curve at parametric position qj. It is reasonable to think that the curve’s speed
does not vary greatly among samples in the posterior, because in §5.4 we imposed
a prior that encourages arc-length uniform parametrization, and because the total
arc-length of the curve is not expected to vary greatly. Therefore, we approximate
this term with the ﬁxed constant Sj “ sjpcprevq, where cprev is just the curve sampled
in the previous iteration of the M-H sampler.
Lastly note that the hodograph,
H ptq “ 9Xptqc, is a linear function of c. So, we can now approximate Rj as a linear
function of c:
Rj
«
1
Sj
„ 9Xxpqj,qc
´ 9Xypqjqc
9Xypqjqc
9Xxpqjqc

.
Then, we can write Rjdj as:
Rjdj
«
ˆ
Rjc,
ˆ
Rj “ 1
Sj
»
–
´
9Xxpqj,qdj,x ´ 9Xypqjqdj,y
¯
´
9Xypqjqdj,x ` 9Xxpqjqdj,y
¯
ﬁ
ﬂ,
and ﬁnally we deﬁne ˆT “ block
´
ˆR1, . . . , ˆRJ
¯
.
5.7.3
Conditional posteriors for µr and Σr
P pµr | ´q
“
N
´
ˆµr, ˆΣµr
¯
ˆΣ´1
µr
“
Σ´1
µr ` KΣ´1
r
ˆµr
“
ˆΣµr
˜
Σ´1
µr µµr ` Σ´1
r
K
ÿ
k“1
dprq,k
¸
Ppτ prq
j,x | ´q
“
Ga
´
ˆαprq
j,x, ˆβprq
j,x
¯
ˆαprq
j,x “ ατ ` K,
ˆβprq
j,x “ βτ `
K
ÿ
k“1
´
dprq,k
j,x
´ µr,j,x
¯2
2
142

5.7.4
Gibbs updates for the parameterizations and orientation
We discretize the possible values of tk
i P r´π, πs to obtain a discrete approximation
of its conditional posterior:
P
`
tk
i | ´
˘
„
Nppk
i ; XptiqcpRq,k, σ2I2qPptk
i | cpRq,kq
ř
τPr´π,πs Nppk
i ; XpτqcpRq,k, σ2I2qPptk
i | cpRq,kq
We can make this arbitrarily accurate, by making a ﬁner summation over τ. To
achieve quick burn-in, we initialize ti using polar-coordinate parametrization, where
p1
i “ pi ´ ¯p,
ti “ t
tan´1 ´ p1
i,y
p1
i,x
¯
` φ
2π
u ´ π.
The point ¯p is the average of tpi, i “ 1, . . . , Nu and φ is the orientation variable
deﬁned in §5.6.
We discretize the possible value of φk P r´π, πs in a similar manner.
5.7.5
Likelihood contribution from surface-normals
Deﬁne
9Xxptiq
“
„dBn1
0 ptiq
dt
, 0, dBn1
1 ptiq
dt
, 0, ¨ ¨ ¨ , dBn1
2n1ptiq
dt
, 0

(5.35)
9Xyptiq
“
„
0, dBn1
0 ptiq
dt
, 0, dBn1
1 ptiq
dt
, ¨ ¨ ¨ , 0, dBn1
2n1ptiq
dt

(5.36)
Proposition 65. The likelihood contribution of the tangent directions θk
i , i “
1, . . . , Nk ensures conjugate updates of the control points for a multivariate normal
prior.
Proof. Recall the noisy tangent direction vectors uk
i ’s and vk
i ’s in (5.26). Using a
simple reparameterization
uk
i “ pek
i , ek
i tan θk
i q
143

where only θ1
is are observed and ei’s aren’t. Observe that
vk
i “ pHxptiq, Hyptiqq “ p 9Xxptk
i qcp3q,k, 9Xyptk
i qcp3q,kq.
(5.37)
Assuming a uniform prior for the ek
i ’s on R, the marginal likelihood of the tangent
direction θk
i given τ 2 and the parameterization tk
i is given by
lpθk
i q “
1
2πτ 2
ż 8
´8
exp
„
´ 1
2τ 2tpek
i ´ 9Xxptk
i qcp3q,kq2 ` pek
i tanpθiq ´ 9Xyptk
i qcp3q,kq2u

dek
i
It turns out the above expression has a closed form given by
lpθk
i q “
1
2πτ 2
?
2πτ 2
?
1`tan2pθk
i q ˆ
exp
”
´ 1
2τ 2
!
p 9Xxptk
i qcp3q,kq2 ` p 9Xyptk
i qcp3q,kq2 ´ p 9Xxptk
i qcp3q,k` 9Xyptk
i qcp3q,k tanpθiqq2
1`tan2pθk
i q
)ı
.
The likelihood for the tθk
i , i “ 1, . . . , Nku is given by
Lpθk
1, . . . , θk
Nkq9 1
τ Nk exp
«
´ 1
2τ 2pcp3q,kq1
# Nÿ
i“1
pSk
i q1Γk
i Sk
i
+
cp3q,k
ﬀ
where
Γk
i “
¨
˝
tan2pθk
i q
1`tan2pθk
i q
´ tanpθk
i q
1`tan2pθk
i q
´ tanpθk
i q
1`tan2pθk
i q
1
1`tan2pθk
i q
˛
‚
and Sk
i “ rp 9Xxptk
i qq1
p 9Xyptk
i q1s is a 2p2n3 ` 1q ˆ 2 matrix.
Clearly, an inverse-
Gamma for τ 2 and a multivariate normal prior for the control points are conjugate
choices.
5.8
Simulation Study
We evaluate our method by deﬁning a true underlying curve, c˚, and checking to
see how accurately this curve can be recovered by our model.
In particular, we
144

place the true curve in nine diﬀerent orientations and positions, sparsely sampling to
generate nine point clouds. Then, we use our model to recover a full boundary for
each cloud, gaining high accuracy by borrowing information between objects. This
scenario is similar to real-world applications where a single object has been observed
in multiple poses, or a collection of similar objects have been observed. We compare
our model against a simpler version of our model which does not allow borrowing of
information, and against principal curves, in which each point cloud is ﬁt separately.
It was not apparent how to achieve borrowing of information using principal
curves. One strategy would be to align the separate point clouds ﬁrst, then treat them
as a single cloud. However, the sparsity of each cloud makes alignment extremely
diﬃcult, as there are no clear features present across all nine clouds. For our method,
it is only necessary to initialize the parametrization of each point cloud (via polar-
coordinate parametrization) such that the orientation of each cloud is roughly correct.
The model is robust to small errors in initial parametrization.
We deﬁne several concepts to help interpret our results. Given some curve c,
let Bpcq “ tXptqc | t P r´π, πsu (the set of all points along the curve), and let Apcq
denote the interior region enclosed by the curve. For a given distribution over curves,
Ppcq, we deﬁne its boundary heatmap, MB
P pcq : R2 Ñ R, and its region heatmap,
MA
P pcq : R2 Ñ R, as:
MA
P pcq px, yq
“
P ppx, yq P A pcqq ,
MB px, yq dxdy
“
P ppx ` dx, y ` dyq X Bpcqq .
Given a set of samples from the distribution Ppcq, tcs | s “ 1, . . . , Su, we can dis-
cretely approximate MB
P pcq and MA
P pcq as:
MB
P pcq px, yq « 1
S
Sÿ
s“1
1 rW px, yq X B pcsqs,
MA
P pcq px, yq « 1
S
Sÿ
s“1
1 rpx, yq P A pcsqs ,
145

where W px, yq “
tpx1, y1q | px1, y1q P rx div ∆x, x div ∆x ` ∆xs ˆ ry div ∆y, y div ∆y ` ∆ysu
Here div denotes integer division. The function W simply maps values in R2 to a reg-
ular grid of bins with width ∆x and height ∆y. Lastly, the mean can be approximated
by ˆc “ 1
S
Sÿ
s“1
cs.
The following Figure 5.5 illustrates borrowing of information across 16 diﬀerent
point clouds generated from a single curve having missing chunks in diﬀerent regions.
The hyperparameters were set to: r1 “ 1, r2 “ 4, r3 “ 22, Σ1 “ 100 IJr1, Σ2 “
70 IJr2, Σ3 “ 70 IJr3, µµ1 “
“
1
1
1
‰1 b
“
0
10
‰1 , µµ2 “ 0Jr2, µµ3 “ 0Jr3, α “ 1, β “
1, σp “ 1{100.
Figure 5.5: (left) Using a simpliﬁed model ﬁtting each point cloud independently
i,e., µr and Σr are ﬁxed rather than inferred. Note the large margin of uncertainty
for gaps in the point cloud. (Right) full hierarchical model resulting in much tighter
ﬁt. Parametrization was achieved by the arc-length model given in §5.4. For the sim-
pliﬁed model, insuﬃcient data was present for arc-length parametrization. Instead,
a ﬁxed polar-coordinate parametrization was used, producing artifacts in the ﬁt.
Convergence was monitored using the Raftery & Lewis diagnostic test as well as
trace plots of the deviance parameters. Also, we get essentially identical posterior
146

summaries with diﬀerent MCMC starting points and moderate changes to hyperpa-
rameters.
5.9
Brain tumor segmentation study
In brain tumor diagnosis and therapy, it is very important to account for uncertainty
in the tumor’s outline. This information is crucial for assessing whether a tumor
has grown/regressed over time, and even more important if a surgeon must target
the tumor for excision or radiation therapy.
In that situation, there is a critical
tradeoﬀbetween false positives (targeting healthy tissue) and extremely undesirable
false negatives (missing the tumor). Furthermore, tumor outlines are notoriously
hard to determine. Error stems from the poor contrast between tumor and healthy
tissue in magnetic resonance imaging (MRI), the prevalent modality for diagnosis.
Even seasoned experts diﬀer greatly when tracing an estimate.
We use our model to intelligently combine the input traces of multiple experts
(Figure 5.6), by treating each trace as a point cloud drawn from the same random
process.
We can then interpret P pcµ | tpkuq as the posterior distribution of the
tumor, fully describing the variability and uncertainty among the experts. One might
also run additional tumor segmentation algorithms, and combine their outputs using
the same approach. In this setting, the region heatmap of the posterior, MA
P pcµ|tpkuq
(shortened to MA), is especially informative. For every point x, MApxq gives the
probability that it is part of the tumor. This enables a neurosurgeon to manage the
tradeoﬀbetween false positives/negatives in a principled manner. Let the true tumor
region be Xtumor Ă R2 and Xc
tumor its complement. Then, deﬁne the loss function
for targeting a region X to be
LpXq
“ λ`Area pX X Xc
tumorq
`λ´Area pXc X Xtumorq .
Depending on the ratio of the penalties λ` and λ´, the surgeon can minimize L
147

simply by cutting along a level set of MA.
Figure 5.6: (top left) raw MRI image; (bottom) MA discretized into 3 colored
regions (red:ą 0.95, orange:ą 0.5, yellow:ą 0.01), the traces provided by 4 experts
are overlaid on top; (top right) the raw brain image with MA overlaid, and the trace
from one expert overlaid for reference.
We can also allow for experts to express varying conﬁdence in diﬀerent portions of
their trace. This is desirable, because certain boundaries of the tumor will have high
contrast with the surrounding tissue while other parts won’t, and the expert should
not be forced to make an equal opinion on both. We can achieve this by slightly
modifying the point cloud model given in (5.21). There, we assumed that each point
pi was generated with ﬁxed variance σ2
p. Instead, we can let σ2
pi “ σ2
p{κi, where κi is
148

the expert’s conﬁdence in that point. Furthermore, if the expert has no conﬁdence
at all, they can simply leave a gap in their trace. The model automatically closes the
gap, as shown in simulation examples. Lastly, it is also easy to compute the posterior
distribution for quantities such as the size of the tumor, simply by computing the
size of each sample.
5.10
Discussion
We have developed a fully Bayesian hierarchical model based on multiscale deforma-
tions for modeling a collection of 2D closed curves. Although we have characterized
a collection of curves using our model, comparing the shapes of objects in a rigorous
Riemannian framework will involve further work, such as deﬁning a loss function
involving an appropriate metric between shapes. We propose to address this issue in
future.
In deﬁning the multiscale process, we would like to have a more automatic way of
choosing the diﬀerent resolutions. It is clear that the highest resolution is obtained by
maximizing the ﬁt subject to minimizing the Bayesian penalty for model complexity.
In our future research we would like to have a more informed way of selecting the
lower resolutions.
Our multiscale model diﬀers in its purpose from other multiscale methods such as
the wavelet transform. With wavelet methods, the goal is often to compress the data,
whereas our goal is to deﬁne levels that isolate dimensions of similarity or variability
within a collection of shapes.
In our current methodology, we have assumed that all K shapes are generated
from the same random shape process. However, in future applications, it may be
useful to model the collection as a mixture of multiple random shape processes,
resulting in a clustering method. For example, in analyzing a blood sample featuring
sickle cell anemia, it is useful to assume that the population was generated by two
149

random shape processes: one healthy and one “sickle-shaped”.
Finally, we would like to extend our multiscale random shape process to the 3D
case using the tensor product approach Pati and Dunson (2011) which has potential
applications in modeling animated characters or tracking 3D lung tumors for targeted
radiation therapy and so on.
150

6
Bayesian modeling of closed surfaces through
tensor products
6.1
Introduction
Surface reconstruction can be viewed as an algorithm that takes as an input an unor-
ganized set of points tp1, . . . , pnu P R3 on or near the unknown manifold M embed-
ded in R3 and produces a surface that approximates M. Free-form surface modeling
from massive data points is becoming an important area of research in commercial
computer aided design and development of manufacturing software (Barnhill, 1985;
Lang and R¨oschel, 1992; Hagen and Santarelli, 1992; Aziz et al., 2002). A collection
of introductory works on surface modeling can be found in Su and Liu (1989) and
the subsequent developments in Muller (2005).
Figure 6.1: Scattered data from a pelvic girdle and the ﬁtted surface
151

Common surface reconstruction algorithms in the computer science literature usu-
ally follow a sequential multistage process which includes scanning, outlier removal,
denoising and input normal estimation to generate a simplicial surface. The Poisson
surface reconstruction method (Kazhdan et al., 2006) solves for an approximate indi-
cator function of the inferred surface, whose gradient best matches the input normals.
The output scalar function, represented in an adaptive octree (Whang et al., 2002), is
then iso-contoured using an adaptive marching cubes algorithm (Lorensen and Cline,
1987). An illustration of scattered data from a pelvic girdle and the ﬁtted surface
using the Poisson surface reconstruction method is provided in Figure 6.1. Cgal
surface mesh generator (Rineau and Yvinec, 2007) implements a variant of this al-
gorithm which solves for a piecewise linear function on a 3D Delaunay triangulation
instead of an adaptive octree. Hoppe et al. (1992); Boissonnat and Oudot (2005)
developed a two stage surface reconstruction algorithm by ﬁrst estimating M by the
implicit surface Zpfq “ ty : fpyq “ 0u of a suitable function f : R3 Ñ R and then
using a contouring algorithm to approximate Zpfq by a simplicial surface.
There is a rich literature on estimation of surfaces using tensor products of bases
(Fowler, 1992; Goshtasby, 1992; Mann and DeRose, 1995; Johnstone and Sloan,
1995). Tensor product surfaces provide a ﬂexible representation of a surface em-
bedded in an arbitrary Euclidean space. However, there is a limited literature on
Bayesian modeling of free-form surfaces (Cunningham et al., 1999) and closed sur-
faces (Soussen and Mohammad-Djafari, 2002). While frequentist surface estimation
using tensor products has been widely studied, Bayesian estimation has received
almost no consideration. A notable exception is the approach of Smith and Kohn
(1997) for Bayesian estimation of bivariate regression surfaces using tensor products.
Modeling of closed surfaces is a primary focus in application areas such as com-
puter vision, as closed surfaces provide an adequate geometric model of a wide range
of objects ranging from human faces to brains and other organs. In this ﬁeld, stan-
152

dard practice involves restrictive parametric shapes depending on a few parameters
(Cinquin et al., 1982; Amenta et al., 1998; Rossi and Willsky, 2003). Although such
models can describe many common surfaces, the variety of generated shapes is lim-
ited. More ﬂexible models for closed surfaces can be deﬁned through carefully spec-
iﬁed linear combinations of basis functions. Soussen and Mohammad-Djafari (2002)
developed the notion of global harmonic surfaces, which yield a simple procedure to
reconstruct coarse surfaces. Shen and Makedon (2006); Chung et al. (2008) devel-
oped a novel method based on general and weighted spherical harmonics to model
closed sphere-like objects, such as the cortical surface. However the variety of shapes
generated by spherical harmonics are somewhat limited to sphere-like or convex ob-
jects although weighted spherical harmonics can capture local features like cortical
folds quite well.
Amenta et al. (1998) developed a surface reconstruction algorithm called the
Crust algorithm based on the three-dimensional Voronoi diagram to model closed
surfaces from a data cloud in R3. The algorithm generates a regular surface and
the output mesh interpolates, rather than approximates, the input points. How-
ever, the algorithm is not probabilistic and does not allow uncertainty in estimat-
ing the surface. Moreover, the algorithm requires a dense collection of data points
for a reasonably good reconstruction indicating slow convergence.
Some illustra-
tions of the Crust algorithm are provided in Figure 6.2. In computer aided design,
closed surface modeling is often aided by combining several B´ezier or spline surface
patches by endpoint interpolation (Gordon and Riesenfeld, 1974; Piegl, 1986; Casale,
1987; Szeliski and Tonnesen, 1992; Hoppe et al., 1992; Yang and Lee, 1999; Li et al.,
2007). In a frequentist analysis such endpoint restrictions are incorporated through
constrained optimization. In the Bayesian paradigm, these restrictions lead to mix-
ing problems in the posterior analysis. Furthermore, these restrictions can make the
resulting surface non-diﬀerentiable along the edges joining the patches.
153

−5
0
5
10
−5
0
5
10
−10
−5
0
5
Points Cloud
−5
0
5
10
−5
0
5
10
−10
−5
0
5
Output Triangulation
−4
−2
0
2
4
−6
−4
−2
0
2
4
6
−3
−2
−1
0
1
2
3
Points Cloud
−4
−2
0
2
4
−6
−4
−2
0
2
4
6
−3
−2
−1
0
1
2
3
Output Triangulation
Figure 6.2: Output triangulation from crust algorithm on a point cloud
Instead we use a cyclic basis developed by R´oth et al. (2009) to accommodate
restrictions without parameter constraints and give rise to an inﬁnitely smooth sur-
face. In this chapter we propose a Bayesian hierarchical model of a closed surface
embedded in R3 using tensor products of cyclic bases with a carefully-chosen shrink-
age prior placed on the tensor of basis coeﬃcients. In particular, motivated by the
decreasing impact of the higher indexed basis functions in the B´ezier surface repre-
sentation, we increase the shrinkage as the index increases. The speciﬁcation leads
to a highly eﬃcient algorithm for posterior computation that allows uncertainty in
the number of bases. In addition, the proposed prior is shown to have large support
and to lead to a posterior with the optimal rate of convergence up to a log factor.
154

6.2
Outline of the method
6.2.1
Review of Terminology
Assume a data cloud tpi P R3, i “ 1, . . . , Nu is given.
Our aim is to obtain a
posterior distribution for a smooth closed surface about which these data points
are concentrated. Before going into the details of our model, we start with a few
deﬁnitions.
Deﬁnition 66. A closed surface is a compact two dimensional closed manifold, which
does not have a boundary. Examples are spaces like the sphere, the torus, and the
Klein bottle.
Deﬁnition 67. A parametric surface is a surface in R3 which is deﬁned by a para-
metric equation with two parameters u and v. Mathematically, a parametric surface
is an injective map from R2 to R3 deﬁned by S : ra, bs2 Ñ R3, pu, vq ÞÑ Spu, vq. See
the Purdue University thesis Sederberg (1983) for a detailed description of parametric
surfaces.
Deﬁnition 68. Parametrization is an algorithm to ﬁnd the coordinate pui, viq cor-
responding to the observed data point pi for each i “ 1, . . . , N such that there ex-
ists a parametric surface S so that pi is regarded as an error-prone realization of
Spui, viq, i “ 1, . . . , N. The coordinate chart tpui, viq, i “ 1, . . . , Nu is alternatively
termed as the associated parameter values.
Deﬁnition 69. A tensor product surface is formed by taking a tensor product of
bases
Sn,mpu, vq “
kn
ÿ
j“0
km
ÿ
k“0
djkBn
j puqBm
k pvq,
(6.1)
where pu, vq P ra, bs2, S is a parametric surface, tdjk P R3, j “ 0, . . . , km, k “
155

0, . . . , knu are control points and tBkn
l puq, u P r0, 1s, l “ 0, . . . , knu are basis func-
tions.
Here kn “ n or 2n depending on whether the bases span the algebraic or the
trigonometric polynomials having maximum degree n. An example of a tensor prod-
uct surface is the B´ezier surface (Farin, 2002) in which Bn
j puq “
`n
j
˘
ujp1´uqn´j, j “
1, . . . , n, u P r0, 1s. B´ezier surfaces are an extension of the idea of B´ezier curves, and
share many of their properties.
6.2.2
Choice of the parameterization
Since we intend to ﬁt a parametric surface, we have to ﬁnd the coordinate chart
tpui, viq P ra, bs2u corresponding to the points tpi P R3, i “ 1, . . . , Nu. Closed sur-
faces can be achieved by parameterizations on the sphere or the torus.
The pa-
rameterizations are typically estimated from the data by, for example, projecting
the points tpiu onto a suitably chosen plane. Spherical harmonics were originally
used as a type of parametric surface representation for radial or steller surfaces
Spu, vq, 0 ă u ă 2π, 0 ă v ă π (Brechb¨uhler et al., 1995; Shen and Makedon, 2006).
The idea is to project the data on the sphere by constrained optimization and then
recover the surface by ﬁtting Spu, vq to pi, i “ 1, . . . , N.
Parameterization with
the torus topology has the advantage of encompassing a wider range of closed sur-
faces compared to spherical harmonic functions which can only model sphere-like or
convex surfaces. The torus topology ensures that the cross sections along the axes
of the closed surface are closed curves, thus allowing more general closed surfaces.
As discussed by Staib and Duncan (1992), the torus can be deformed into a tube by
squeezing the torus cross section to a thin ribbon and closed surfaces are obtained by
considering tubes whose ends meet up to a point. Brechb¨uhler et al. (1995) discuss
some of the practical disadvantages of their method. Instead, we use the relational
perspective map developed by Li (2004) to project the 3-d point cloud onto a torus
156

and then scale down to r´π, πs2. Then we can use a tensor product of cyclic bases
on r´π, πs2 devoid of any constraints to develop a ﬂexible model for closed surfaces.
Applying the relational perspective map to the point cloud in Figure 2, we obtain
the points in the r´π, πs2 square shown in Figure 6.3.
Figure 6.3: Parameterization of the human skull and the Beethoven
6.2.3
Closed surface model
We assume that the data tpi “ pp1
i , p2
i , p3
i qT, i “ 1, . . . , Nu arise as a random additive
perturbation from the closed parametric surface Spu, vq, pu, vq P r´π, πs2, as follows,
pi “ Spui, viq ` ei,
ei „ Np0, σ2I3q,
i “ 1, . . . , N,
(6.2)
where pui, viq are coordinates in r´π, πs2 corresponding to point pi P R3,
Spui, viq “ tS1pui, viq, S2pui, viq, S3pui, viquT is the ﬁtted surface at coordinates
pui, viq, and ei P R3 is a measurement error. Let P, S and E denote the corresponding
N ˆ 3 matrix representations with rows tpT
i , i “ 1, . . . , Nu, tSpui, viqT, i “ 1, . . . , Nu
and teT
i , i “ 1, . . . , Nu respectively. Assume σ´2 „ Gapaσ, bσq. We follow a ten-
sor product surface representation (6.1) to model the closed parametric surface
Spu, vq, pu, vq P r´π, πs2.
157

6.2.4
Construction of the cyclic basis
Using the tensor product speciﬁcation in (6.1) for the closed surface Spu, vq, we
propose to use the cyclic basis developed by R´oth et al. (2009); R´oth and Juh´asz
(2010). These bases have a cyclic symmetry that eliminates the need for constraints
on the control points, while also leading to surfaces that are inﬁnitely smooth in
the sense that the realizations are inﬁnitely diﬀerentiable (C8). Assuming S P C8
is appealing in avoiding the need for geometric constraints and surfaces in C8 can
approximate any parametric closed surface arbitrarily well preserving local features.
In addition, S can be characterized as a single coherent surface dependent on only
the positions of the control points. In contrast, most methods characterize S by
piecing together local surfaces with heavy constraints needed for continuity along
the joints of the patches.
R´oth et al. (2009) devised a basis for the vector space
Vn “ x1, cospuq, sinpuq, . . . , cospnuq, sinpnuqy
of trigonometric polynomials of degree at most n, i.e., of truncated Fourier series.
Let
Bn
j puq “ cn
2n
"
1 ` cos
ˆ
u `
2πj
2n ` 1
˙*n
,
pj “ 0, 1, . . . , 2nq, u P r´π, πs,
(6.3)
where cn “
p2nn!q2
p2n`1q!.
The following lemma from R´oth et al. (2009) demonstrates
that any truncated Fourier series can be expressed as a linear combination of the
elements of Vn for some large n. This implies that any reasonable closed curve can
be approximated arbitrarily well by the linear combination of the elements of Vn for
some n. This concept is formalized in §6.3 in discussing posterior convergence.
Lemma 70. The functions tBn
j puq, i “ 0, 1, . . . , 2n, u P r´π, πsu form a basis of the
vector space Vn.
158

Using basis functions (6.3), we can deﬁne the tensor product of surfaces of degree
pn, mqpn ě 1, m ě 1q by Sn,mpu, vq with kn “ 2n in (6.1).
6.2.5
Model for the control points
Let T2n`1,2m`1pRpq denote the space of tensors of order p2n ` 1q ˆ p2m ` 1q ˆ p.
Deﬁne Dn,m “ rdjks2m,2n
j“0,k“0. Clearly Dn,m P T2n`1,2m`1pR3q for all m ě 1, n ě 1.
R´oth et al. (2009) remarked that although the control points have a global eﬀect
on the shape, this inﬂuence dramatically decreases on further parts of the surface,
especially for higher value of n and m. They provide several test examples to show
that the decrease of the inﬂuence is fast. This observation is the key to the choice
of sparsity favoring priors for Dn,m.
Because the elements of Dn,m are expected to have an increasingly localized
inﬂuence on the shape of the surface Spu, vq as the index on the control points
increases, we choose a shrinkage prior that favors smaller values for djk as j and
k increases. Here we use a double shrinkage prior to facilitate a sparseness of the
tensors Dn,m.
djk
„
N3p0, φ´1
jk I3q, φjk “ τjξk, τj „ Gapαn, βq, ξk „ Gapαm, βq,
(6.4)
where αn is an increasing sequence of positive integers.
The prior for S induced from (6.1), (6.3) and (6.4), denoted S „ ΠSn,m, is deﬁned
conditionally on n and m. If n and m are chosen to be too small, the prior ΠSn,m
will not support a sizable subset of surfaces in Cp8q. As an alternative to choosing
n and m to be extremely large or even inﬁnite to obtain large support, we propose to
choose a prior for n and m, which allows one to adaptively learn and model average
over the unknown dimensions of the control point tensor Dn,m. Let pn, mq „ Πn,m
denote this prior, with Πn,m a distribution over t1, . . . , 8u2, such as independent
truncated Poissons, and let S „ ΠS denote the resulting prior for S marginalizing
159

out n and m. This approach is related to the literature on Bayesian adaptive splines
(Denison et al., 1998), though we will bypass the need to implement the standard
reversible jump Markov chain Monte Carlo and describe a computationally eﬃcient
approach in §3.
6.2.6
Prior realizations
Since Bn
j puq ą 0, j “ 0, . . . , 2n, u P r´π, πs and ř2n
j“0 Bn
j puq “ 1, the closed surface
Sn,mpu, vq “ ř2n
j“1
ř2m
k“0 djkBn
j puqBm
k pvq, ru, vs P T2 lies in the convex-hull of its con-
trol points Dn,m. We can achieve a variety of closed surfaces through speciﬁc choices
of the control points as shown below in Figure 6.4. To demonstrate the nature of
(a) A sphere with its control points
(b) A closed surface with n “ 7, m “ 9
Figure 6.4: (a) A sphere with its control points (b) A closed surface with n “ 7,
m “ 9
the prior realizations with increase in n and m, consider ﬁrst the case n “ m “ 1.
For a ﬁxed v “ v0, the v0-section of the surface S1,1pu, v0q, u P r´π, πs is a closed
curve of degree p1, 1q. Similarly any u0-section is also a closed curve of degree p1, 1q.
Thus S is a closed surface whose cross-sections parallel to the axes are closed curves.
For general n and m, the v0-section Sn,mpu, v0q is just a linear combination of closed
curves, thus producing a rich class of closed curves. Hence the variety of shapes
generated increases with increase in n and m which is shown in Figure 6.5. Figure
6.5 also demonstrates that the inﬂuence of the control points is increasingly localized
for large values of n and m.
160

Figure 6.5: Prior realizations with increasing n and m
6.3
Support of the prior and posterior convergence rates
6.3.1
Support
Let T2 denote the 2-dimensional torus represented by the square r´π, πs2. Let the
H¨older class of bivariate periodic functions on T2 of order α be denoted by CαpT2q.
Deﬁne a class of closed parametric surfaces SCpα1, α2, α3q having diﬀerent smoothness
along diﬀerent coordinates as
SCpα1, α2, α3q :“ tS “ pS1, S2, S3q : T2 Ñ R3, Si P CαipT2q, i “ 1, 2, 3u.
(6.5)
For ﬁxed n and m, deﬁne the stochastic process S „ ΠSn,m. To characterize the
support of our prior, we ﬁrst recall the deﬁnition of the RKHS of a multivariate
Gaussian process prior. van der Vaart and van Zanten (2008b) review facts that are
relevant to the present setting. A Borel measurable random element W with values
in a separable Banach space pB, || ¨ ||q is called Gaussian if the random variable b˚W
is normally distributed for any element b˚ P B˚, the dual space of B. In our case,
161

the Banach space B is CpT2; R3q, the space of continuous functions from T2 to R3.
The reproducing kernel Hilbert space (RKHS) H attached to a zero-mean Gaussian
process W is deﬁned as the completion of the range MB˚ of the map M : B˚ Ñ B
deﬁned by Mb˚ “ EWb˚pWq relative to the inner product
xMb˚
1, Mb˚
2yH “ Eb˚
1pWqb˚
2pWq.
The following lemma describes the RKHS of the Gaussian process ΠSn,m given
tφjk, j “ 0, . . . , 2n, k “ 0, . . . , 2mu. Refer to Appendix D for a proof.
Lemma 71. Given tφjk, j “ 0, . . . , 2n, k “ 0, . . . , 2mu, the RKHS Hn,m of ΠSn,m
consists of all functions h : T Ñ R3 of the form
hpu, vq “
2n
ÿ
j“0
2m
ÿ
k“0
cjkBn
j puqBm
k pvq,
(6.6)
where the weights cjk range over R3. The RKHS norm is given by
||h||2
Hn,m “
2n
ÿ
j“0
2m
ÿ
k“0
||cjk||2φjk.
(6.7)
The following theorem describes how well an arbitrary closed parametric surface
S0 P SCpα1, α2, α3q can be approximated by the elements of Hn,m for each n and m
given tφjk, j “ 0, . . . , 2n, k “ 0, . . . , 2mu. Refer to Appendix D for a proof.
Theorem 72. For any ﬁxed S0 P SCpα1, α2, α3q, there exists h P Hn,m with
||h||2
Hn,m ď K1
ř2n
j“0
ř2m
j“0 φjk such that
||S0 ´ h||8 ď K2pn ^ mq´αp1q log n log m
(6.8)
for some constants K1, K2 ą 0 independent of n and m.
162

6.3.2
Rate of convergence of the posterior
The parameter space is CpT2; R3q ˆ r0, 8q and ΠS ˆ Πσ is the prior on CpT2; R3q ˆ
r0, 8q where Πσ denotes a general prior for σ which is compactly supported on
r0, Ls for some L ą 0. Assume that the density of Πσ with respect to the Lebesgue
measure on the compact interval is bounded away from zero. The inverse gamma
prior truncated to the interval r0, Ls provides an example.
Deﬁnition 73. For a given sequence ǫN Ó 0, the posterior is said to contract around
the true parameter value pS0, σ0q P CpT2; R3qˆr0, 8q at a rate ǫN if for L suﬃciently
large,
Π
"
pS, σq : 1
N
Nÿ
i“1
||Spui, viq ´ S0pui, viq||2 ` |σ ´ σ0|2 ą L2ǫ2
N
ˇˇˇˇ tpi, pui, viquN
i“1
*
PS0,σ0
Ñ
0 as N Ñ 8. (6.9)
The proof of the following Theorem 74 is provided in Appendix D.
Theorem 74. If pS0, σ0q P SCpα1, α2, α3q ˆ r0, Ls, an « Oplog nq3 and expt´pnr `
msqu ď Πn,m ď pnmq´3, n, m ě 1 for some r, s ą 0, ǫN « N
´
αp1q
2αp1q`2 logt N, where t
is a known constant.
The assumption on Πn,m ensures that the prior probability is not too small on
smaller values of n and m so that the prior favors relatively simple representations of
the surface. The assumption is satisﬁed by a product of independent Poissons. Also
the shape parameter of the Gamma distribution for τj and ξk should be increased
depending on the values of n and m to guarantee an optimal rate of convergence. The
increase in shape parameter with n and m corresponds to a greater shrinkage of the
higher indexed control points. To estimate a real valued d-variate function in CαpX q,
the minimax optimal rate of convergence is n´α{p2α`dq. One can anticipate that for
163

vector valued functions with smoothness αj, j “ 1, 2, 3 in the coordinates, with the
loss function deﬁned by the sum of the individual loss across the coordinates, the rate
of convergence cannot be improved beyond n´αp1q{p2αp1q`dq. Theorem 74 ensures that
the posterior will converge to the true surface at this rate which is oﬀset slightly by
a logarithmic factor as expected for Bayesian procedures (De Jonge and van Zanten,
2010; van der Vaart and van Zanten, 2009).
6.4
Posterior computation
6.4.1
Gibbs sampler for a ﬁxed truncation level
For a ﬁxed n and m, the full conditional distributions of all the unknown variables
are conjugate and we can do Gibbs sampling. Since we only require αn to grow slowly
at Oplog nq3 to achieve the optimal rate of convergence, we will assume αn “ α. The
sampler cycles through the following steps.
Step 1.
Deﬁne X
to be the N ˆ p2n ` 1qp2m ` 1q matrix with rows
tBn
0 puiq, Bn
1 puiq, . . . , Bn
2npuiqub tBm
0 pviq, Bm
1 pviq, . . . , Bm
2mpviqu, i “ 1, . . . , N. Also let
D be the p2n ` 1qp2m ` 1q ˆ 3 coeﬃcient matrix with rows dT
jk, j “ 0, 1, . . . , 2n, k “
0, 1, . . . , 2m.
Recall that the density of a matrix-normal random variable Z „
MNpM, Ω, Σq with mean M having dimension n ˆ p is given by
fpz | M, Ω, Σq9 expr´0¨5trtΩ´1pz ´ Mq
TΣ´1pz ´ Mqus,
(6.10)
for positive deﬁnite matrices Ωand Σ of order p ˆ p and n ˆ n. Then
D | ´
„
MNp2n`1qp2m`1qˆ3
"
X
TP, I3,
`
X
TX ` Λ´1˘´1
*
(6.11)
vecpDq | ´
„
N3p2n`1qp2m`1q
"
vecpX
TPq, I3 b
`
X
TX ` Λ´1˘´1
*
.
(6.12)
164

Here Λ´1 “ diagtτjξk, j “ 0, . . . , 2n, k “ 0, . . . , 2mu
Step 2.
σ´2 | ´ „ Ga
ˆ
aσ ` 3N{2, bσ ` 0¨5
N
ÿ
i“1
||pi ´ Spui, viq||2
˙
.
(6.13)
Step 3. For j “ 0, . . . , 2n and k “ 0, . . . , 2m,
τj | ´ „ Ga
ˆ
α ` 3p2m ` 1q{2, β ` 0¨5
2m
ÿ
k“0
ξk||djk||2
˙
.
(6.14)
ξk | ´ „ Ga
ˆ
α ` 3p2n ` 1q{2, β ` 0¨5
2n
ÿ
j“0
τj||djk||2
˙
.
(6.15)
6.4.2
Posterior sampling of n and m
The conditional likelihood of n, m, tpdjk, φjkq, j “ 0, . . . , 2n, k “ 0, . . . , 2mu given
tpi, pui, viq, i “ 1, . . . , Nu is proportional to
exp
"
´
1
2σ2
Nÿ
i“1
||pi ´ Sn,mpui, viq||2
*
Πn,m
2n
ź
j“0
2m
ź
k“0
ppdjk | φjkqppφjkq.
In this case, we take advantage of the partial analytic structure (Godsill, 2001) in
the models as pn, mq changes and rather than proposing an entirely new parameter
vector, the form of reversible jump MCMC for n and m becomes relatively straight-
forward. The common parameters σ´2 and tdjk, j “ 0, . . . , 2n, k “ 0, . . . , 2mu as
the order of the model changes are updated using a within model Gibbs move as in
§6.4.1. Consider a proposal qpn, m | n0, m0q “ qpn | n0qqpm | m0q with qp1 | 0q “ 1
and qpk1 | kq “ 1{2 for all |k´k1| “ 1. Suppose the chain is at pn0, m0q and a proposal
is made to go to state pn0 ` 1, m0 ` 1q, we employ a step-wise sampler as in (Godsill,
165

2001). We sample pd1
2n0`1,2m0`1, φ1
2n0`1,2m0`1q and pd1
2n0`2,2m0`2, φ1
2n0`2,2m0`2q from a
kernel kerpdjk, φjkq and the move is accepted with probability mint1, αu, where
α “ exp
␣
´
1
2σ2
řN
i“1 ||pi ´ Sn0`1,m0`1pui, viq||2(
Πn0`1,m0`1
exp
␣
´
1
2σ2
řN
i“1 ||pi ´ Sn0,m0pui, viq||2(
Πn0,m0
ˆ
qpn0 ` 1, m0 ` 1 | n0, m0q
qpn0, m0 | n0 ` 1, m0 ` 1q ś2
j“1 ppd1
2n0`j,2m0`j, φ1
2n0`j,2m0`jq
.
(6.16)
We take kerpdjk, φjkq “ ppdjk, φjkq.
The proposal probabilities for the moves
pn0, m0q Ñ pn0, m0 ˘1q, pn0, m0q Ñ pn0 ˘1, m0q and pn0, m0q Ñ pn0 ˘1, m0 ˘1q can
be derived similarly. The shrinkage prior on the φjk’s gives rise to highly eﬃcient
RJMCMC moves which converge to the appropriate values of n and m rapidly in
most cases we have observed.
6.5
Applications
We analyzed the skull and Beethoven data shown in Figure 6.1 using our proposed
method. As all reasonable methods will do a good job at surface estimation based on
a large number of points located very close to the surface of interest, we simulated
diﬀerent levels of sparse and noisy data by sampling a subset of the points in the
original data sets and adding diﬀerent levels of Gaussian measurement errors. In
many other applications, sparse and noisy data are routinely collected but focusing
on two dense, low measurement error data sets allows careful study of the impact
of sample size and measurement error on the performance of our proposed Bayesian
approach relative to the state-of-the-art Crust algorithm.
First we reconstruct the surface from non-noisy sparse data by taking random
subsamples of 390 points from the skull and Beethoven point clouds. The results for
Crust are shown in Figure 6.6, while the results for our proposed Bayesian approach
are shown in Figure 6.7. In each case, we generated 5000 samples and discarded the
166

ﬁrst 2000 as burn-in. Convergence was monitored using trace plots of the deviance as
well as several parameters. Also we get essentially identical posterior modes of n and
m with diﬀerent MCMC starting points and moderate changes to hyperparameters.
In many applications, the features of the data acquisition device can dictate the
amount of noise incorporated. Choosing an informative prior for the noise variance
can help in the ability to pick up local features. The hyperparameters in the priors
for τj and ξk play a key role in controlling the smoothness of the surface. An increase
in αn corresponds to a decrease in the values of τj and ξk leading to over-smoothing.
However one needs to carefully control αn to prevent over-shrinkage leading to over-
smoothing. In the applications below αn “ 3{2 and β “ 3{2. Estimation of noise
variance and the surface is robust to moderate changes in hyperparameters as the
sample size increases.
Our method performs closely to Crust for non-noisy data. As we add Gaussian
noise to the points, the performance of Crust deteriorates (Figure 6.8) while the
tensor product surface (Figure 6.9) is quite robust to the addition of noise as it
takes into account the uncertainty in estimating the surface.
In Figure 6.8, we
notice some parts from the skull and the Beethoven’s head jutting out owing to poor
characterization of the noise.
To compare the performance of our method with existing competitors, we com-
pute the Hausdorﬀdistance between the true surface and the ﬁtted surface as de-
scribed below. Let S1 and S2 be two manifolds embedded in R3. Then the Hausdorﬀ
distance is deﬁned by
hDpS1, S2q “ max
"
sup
xPS1
inf
yPS2 dpx, yq, sup
xPS2
inf
yPS1 dpx, yq
*
(6.17)
where d is any distance in R3. It can be shown that hDpS1, S2q “ 0 if and only if S1
and S2 have the same closure. For the tensor product approach we estimate hDpS, ˆSq
167

by
max
␣
sup
i
inf
j dppi, ˆpjq, sup
j
inf
i dppi, ˆpjq
(
where tˆpi, i “ 1, . . . , Nu is a Bayes estimate of tpi, i “ 1, . . . , Nu where d is the
standard Euclidean distance. For the Crust algorithm, we estimate hDpS, ˆSq by
max
␣
sup
i inf
j dppi, tjq, sup
j inf
i dppi, tjq
(
where tti : i “ 1, . . . , Mu is a dense grid of points on the resulting simplicial surface.
We summarize the performances of the Crust algorithm and the tensor product
approach in Table 6.1 for a variety of choices of the sample size and noise variance
(σ2). We observe that for non-noisy data Crust performs closely and slightly bet-
ter than the tensor-product surface for large sample sizes while the tensor product
outperforms the Crust as the noise variance increases. As the sample size increases,
the tensor product surface ﬁt becomes better even when the noise variance is large.
However, the performance of the Crust improves with sample size only when the
noise variance is very small. Posterior summaries of the noise variance and the basis
function truncation levels n and m are provided in Table 6.2. The noise variance is
not well-estimated for small sample sizes and smaller value of the true noise variance.
However, estimation becomes better for larger sample sizes consistent with the pos-
terior convergence results. Also, one can estimate larger variances well compared to
smaller ones for reasons discussed earlier. As the sample size increases, the posterior
mode of pn, mq tend to increase slightly when the noise variance is small in order to
capture local features. When the noise variance is large, the global features dominate
and the posterior modes of n and m remain constant at the smaller values.
168

Table 6.1: Hausdorﬀdistance between true and ﬁtted surface using tensor product
method and Crust
Skull
Beethoven
σ
N=390
N=690
N=990
N=390
N=690
N=990
0¨05
(2¨123, 2¨045)
(2¨008, 1¨971)
(1¨981, 1¨791)
(1¨528, 1¨557)
(1¨510, 1¨527)
(1¨411,1¨397)
0¨1
(2¨561, 2¨671)
(2¨345, 2¨682)
(2¨311, 2¨677)
(1¨589, 1¨679)
(1¨524, 1¨560)
(1¨579, 1¨730)
0¨2
(2¨711, 3¨134)
(2¨697, 3¨225)
(2¨523, 3¨435)
(1¨812, 2¨146)
(1¨796 ,1¨874)
(1¨657, 2¨334)
Table 6.2: Posterior summaries of σ and n, m (posterior mean of σ, 95% credible
intervals for σ, posterior mode of pn, mq)
σ
N=390
N=690
N=990
Skull
0¨05
0¨075, [0¨065, 0¨093], (3,4)
0¨064, [0¨056, 0¨077], (4,4)
0¨056, [0¨047, 0¨062], (4,4)
0¨1
0¨194, [0¨124, 0¨265], (4,4)
0¨154 [0¨096, 0¨213], (4,4)
0¨120, [0¨081, 0¨156], (3,4)
0¨2
0¨220, [0¨127, 0¨314], (3,4)
0¨210 [0¨136, 0¨279], (3,4)
0¨196 [0¨139, 0¨253], (3,4)
Beethoven
0¨05
0¨090, [0¨081 0¨109], (5,6)
0¨061, [0¨041, 0¨081], (6,6)
0¨537, [0¨039, 0¨067], (6,7)
0¨1
0¨220, [0¨191, 0¨261], (5,6)
0¨171,[0¨143, 0¨191], (5,6)
0¨167, [0¨091, 0¨159], (6,6)
0¨2
0¨228, [0¨166, 0¨291], (5,6)
0¨214, [0¨161, 0¨267], (5,6)
0¨203 [0¨161, 0¨246], (5,6)
6.6
Discussion
This chapter develops a novel Bayesian hierarchical model for a closed surface, al-
lowing full posterior inferences via an eﬃcient Markov chain Monte Carlo algorithm.
Consistent with our theory results on optimal rates of posterior contraction, we ﬁnd
that the methodology does a good job in reconstructing a closed surface from sparse
and noisy 3d point cloud data yielding improved performance over state-of-the-art
computer science algorithms. Although modern sensing technology, such as com-
puted tomography or magnetic resonance imaging, enables us to make detailed scans
of complex objects generating point cloud data consisting of millions of points, the
data acquired is usually distorted by noise arising out of various physical measure-
ment processes and limitations of the acquisition technology. Most of these points
are typically discarded after taking into account acquisition eﬀects leading to a sparse
noisy point cloud. The resolution speciﬁcs of these acquisition devices provide infor-
mation on the magnitude of the measurement error variance. An appealing feature of
169

−5
0
5
10
−5
0
5
10
−10
−5
0
5
Sparse non−noisy points Cloud
−5
0
5
10
−5
0
5
10
−10
−5
0
5
Output Triangulation
−4
−2
0
2
4
−6
−4
−2
0
2
4
−4
−3
−2
−1
0
1
2
3
4
Sparse non−noisy points Cloud
−4
−2
0
2
4
−6
−4
−2
0
2
4
−4
−3
−2
−1
0
1
2
3
4
Output Triangulation
Figure 6.6: Output triangulation using Crust on a sparse (390 points) non-noisy
point cloud
our Bayesian approach is that we obtain a full posterior for the surface allowing un-
certainty. Visualizing this uncertainty is an interesting challenge for future research,
but one can produce interior and exterior pointwise 95% credible surfaces and even
movies of surface realizations from the posterior. In addition, when there is interest
in surface features, such as the interior volume, surface area, or the number of holes,
one can obtain posterior summaries of the feature of interest.
Our proposed approach represents an initial step in a line of research related to
Bayesian modeling of 3-d closed surfaces. There are several important next steps.
It is commonly the case that each subject has their own surface and interest focuses
on modeling a collection of dependent surfaces across subjects, while incorporating
subject-speciﬁc predictors, using the surface to predict a response variable, and test-
ing diﬀerences in distributions of surfaces between groups. In such settings, it is
necessary to align the surfaces for the diﬀerent subjects, which can potentially be ac-
170

−5
0
5
10
−5
0
5
10
−10
−5
0
5
Sparse non−noisy Points Cloud
−5
0
5
10
−5
0
5
10
−15
−10
−5
0
5
Output Tensor product surface
−4
−2
0
2
4
−6
−4
−2
0
2
4
−3
−2
−1
0
1
2
3
Sparse non−noisy Points Cloud
−4
−2
0
2
4
−6
−4
−2
0
2
4
−3
−2
−1
0
1
2
3
Output Tensor product surface
Figure 6.7: Output tensor product surface on a sparse (390 points) non-noisy point
cloud
complished in a Bayesian probabilistic framework. Another ongoing problem relates
to surfaces that change dynamically over time within a subject. In addition, it is
common for the data to not consist simply of a 3-d point cloud but instead to have
pixelated data in which the surface(s) of interest are embedded in an blurry image
containing other objects.
As in other functional data modeling settings, the smoothness and local features
of the surfaces being estimated can be somewhat sensitive to the basis functions
being used. We have focused on tensor products of truncated Fourier series, which
lead to obtain rates of posterior contraction and have good practical performance
in reconstructing inﬁnitely smooth surfaces that have cross sections that are closed
curves. There are settings in which the objects being modeled may have interesting
local features, such as spikes, that may be smoothed out with our proposed bases
and shrinkage priors in the absence of abundant data.
171

−5
0
5
10
−5
0
5
10
−10
−5
0
5
Sparse noisy points Cloud
−5
0
5
10
−5
0
5
10
−10
−8
−6
−4
−2
0
2
4
6
8
Output Triangulation
−4
−2
0
2
4
−6
−4
−2
0
2
4
−3
−2
−1
0
1
2
3
Sparse noisy points Cloud
−4
−2
0
2
−6
−4
−2
0
2
4
−2
−1
0
1
2
3
Output Triangulation
Figure 6.8:
Output triangulation using Crust on a sparse (390 points) noisy
(std=0.2) point cloud
172

−5
0
5
10
−5
0
5
10
−10
−5
0
5
Sparse noisy Points Cloud
−5
0
5
10
−5
0
5
10
−10
−5
0
5
Output Tensor product surface
−4
−2
0
2
4
−6
−4
−2
0
2
4
−3
−2
−1
0
1
2
3
Sparse noisy (std =0.2) Points Cloud
−4
−2
0
2
4
−5
0
5
−3
−2
−1
0
1
2
3
Output Tensor product surface
Figure 6.9: Output tensor product surface on a sparse (390 points) noisy (std=0.2)
point cloud
173

7
Bayesian geostatistical modeling with informative
sampling
7.1
Introduction
Geostatistical models focus on inferring a continuous spatial process based on data
observed at ﬁnitely many locations, with the locations typically assumed to be non-
informative. As noted by Diggle et al. (2010), this assumption is commonly violated
for point-referenced spatial data, as it is not unusual to collect data at locations
thought to have a large or small value for the outcome. For example, in monitoring
of air pollution, one may place more monitors at locations believed to have a high
value of ozone or another pollutant, while in studying distribution of animal species
one may systematically look in locations thought to commonly contain the species
of interest. Diggle et al. (2010) proposed a shared latent process model to adjust for
bias due to informative sampling locations. Their analysis was implemented using a
Monte Carlo approach for maximum likelihood estimation.
We follow a Bayesian approach using a model related to those described by
Menezes (2005), Ho and Stoyan (2008) and Diggle et al. (2010). The locations are
174

modeled using a log Gaussian Cox process (Møller et al., 2001), with the intensity
function included as a spatially-varying predictor in the outcome model, which also
includes spatial random eﬀects drawn from a Gaussian process. A parameter a con-
trols the degree of informative sampling, and the sampling locations are ignorable
in the special case in which a “ 0, while a ą 0 implies a tendency to take more
observations at spatial locations having relatively high outcome values. This model
modiﬁes shared random eﬀects models for joint modeling of longitudinal and event
time data (Radcliﬀe et al., 2004) and for accommodating informative missingness
(Wu and Follmann, 1999).
To our knowledge, we are the ﬁrst to develop a Bayesian approach to the in-
formative locations problem in geostatistical modeling. However, adapting recently
proposed models to the Bayesian paradigm is relatively straightforward, and our pri-
mary contribution is studying the theoretical properties of the model. In particular,
it is not obvious that the data contain information about the informativeness of the
sampling locations, and one may wonder to what extent the prior is driving the re-
sults even in large samples. We address this concern by proving that the posterior is
proper under a noninformative prior on a. In addition, one can consistently estimate
a, the density of the sampling locations and the mean function of the outcome pro-
cess. This later result extends recent work showing posterior consistency in Gaussian
process regression models (Choi and Schervish, 2007b; Choi, 2007).
7.2
Model for spatial data with informative sampling
Our objective is to estimate the spatial surface µpsq P R for all s P D Ă R2 based on
observations y1, . . . , yn at locations s1, . . . , sn P D. We propose the following joint
model
yi | si
„
Ntηpsiq ` aξpsiq, σ2u,
ppsiq “
exptξpsiqu
ş
D exptξpsquds
pi “ 1, . . . , nq,(7.1)
175

where the observations are independent across locations si given ξpsq and ηpsq, and
ppsq is the location density. Assuming the locations are a realization of an inhomo-
geneous Poisson process with log intensity ξpsq, the mean surface is characterized
as µpsq “ ηpsq ` aξpsq, where ηpsq is a baseline surface and aξpsq is an adjustment
due to informative sampling.
Letting xpsq denote a vector of spatial covariates,
ξpsq “ xpsqTβξ ` ξrpsq and ηpsq “ xpsqTβη ` ηrpsq, where βξ and βη are regression
coeﬃcients and ξrpsq and ηrpsq are mean zero residual processes.
The log sampling density is treated as a latent covariate to adjust for informative
sampling, with a ą 0 implying that samples are more likely to be taken in areas with
a large response. Setting the coeﬃcient in βξ corresponding to the intercept to zero
for identiﬁability,
Epyi | siq “ xpsiq
Tβ˚ ` aξrpsiq ` ηrpsiq
pi “ 1, . . . , nq,
(7.2)
where β˚ “ aβξ`βη. Therefore, accounting for informative sampling is only necessary
when there is an association between the spatial surface of interest and the sampling
density that cannot be explained by the shared spatial covariates xpsq.
The residuals ξrpsq „ Πξr and ηrpsq „ Πηr are assigned independent mean zero
Gaussian process priors with Mat´ern covariance functions (Stein, 1999),
cph | ψq “
τ 2
2ν´1Γpνq
ˆ2ν1{2h
ρ
˙ν
Kν
ˆ2ν1{2h
ρ
˙
,
h “ ||s ´ s1||,
(7.3)
where ψ “ pτ 2, ρ, νq and K is the modiﬁed Bessel function of the second kind. The
Mat´ern covariance has three parameters: τ 2 ą 0 controls the variance, ρ ą 0 controls
the spatial range of the correlation, and ν ą 0 controls the smoothness of the process.
Special cases include the exponential cph | ψq “ τ 2 expp´21{2h{ρq with ν “ 1{2, and
the squared exponential cph | ψq “ τ 2 expp´2h2{ρ2q with ν “ 8.
176

7.3
Theoretical properties
7.3.1
Weak posterior consistency
In this section we obtain posterior consistency of the parameters of our model with
respect to ﬁxed-domain asymptotics. Consider the joint model deﬁned in §2, with
D “ r0, 1s2 without loss of generality and Πξr, Πηr Gaussian processes on CpDq,
the space of continuous functions on D. Letting cph | ψξq and cph | ψηq denote
the covariance functions for ξr and ηr, respectively, we choose independent bounded
hyperpriors for τ 2
ξ , τ 2
η, νξ and νη while letting ρξ „ πξ and ρη „ πη, where the supports
of both πη and πξ are R`. We choose a proper prior on R for a, βξ „ Npβ0ξ, Σ0ξq,
βη „ Npβ0η, Σ0ηq and σ2 „ Inv-Gapασ, βσq.
Assumption 75. The prior ζ „ Π satisﬁes the prior positivity condition Πpζ :
||ζ ´ ζ0||8 ă ǫq ą 0 for all ǫ ą 0 and for any ζ0 P CpDq.
van der Vaart and van Zanten (2009) showed that Assumption 1 holds for Gaus-
sian process priors with squared exponential covariance under mild conditions, and
Choi (2005) provided a set of suﬃcient conditions on the Mat´ern covariance kernel
for the same.
Assumption 76. The covariates are uniformly bounded, so there exists an M ą 0
such that ||xpsq|| ď M for all s P D.
Theorem 77. Under model (7.1)–(7.2) with priors chosen as described in §3 and As-
sumptions 1–2, the posterior distribution Πpξr, ηr, a, βξ, βη, σ | tpyi, siq, i “ 1, . . . , nuq,
is weakly consistent.
Theorem 77 does not imply that the hyperparameters in the covariance kernel
are consistently estimated, though we do take into account uncertainty in these
parameters and do not assume that the priors are well speciﬁed. It is typically not
177

possible to consistently estimate all the parameters in the Mat´ern covariance (Zhang,
2004).
7.3.2
Posterior propriety of a
Under model (7.1)–(7.2), the parameter a controls the degree of informative sampling.
The uniform improper prior, πapaq91, provides a noninformative choice. Theorem
78 shows that this prior leads to a proper posterior, implying that the data are
informative about a.
Letting s “ ps1, s2, . . . , snq, y “ py1, y2, . . . , ynqT, ξn
r “
␣
ξrps1q, ξrps2q, . . . , ξrpsnq
(T
and ηn
r “
␣
ηrps1q, ηrps2q, . . . , ηrpsnq
(T, we have ξn
r „ Np0, Σn
ξ q and ηn
r „ Np0, Σn
ηq,
where Σn
ξ ps, s1q “ cp||s ´ s1|| | ψξq and Σn
ηps, s1q “ cp||s ´ s1|| | ψηq for s, s1 P D. Let
cph | ψξq “ τ 2
ξ expp´21{2hp{ρξq and cph | ψηq “ τ 2
η expp´21{2hp{ρηq for 0 ă p ď 2. We
assume independent bounded priors on τξ and τη and independent discrete uniform
priors on ρξ and ρη.
Also, βξ „ Npβ0ξ, Σ0ξq, βη „ Npβ0η, Σ0ηq and σ2 „ πpσ2q.
Here we focus on powered exponential covariance functions rather than Mat´ern to
simplify calculations. A similar result should hold for Mat´ern covariance functions
if the priors on the hyperparameters have a bounded support.
Theorem 78. With the above prior speciﬁcations, the marginal posterior distribution
of a, ppa | y, sq, is proper provided n ě 2 and Eπpσq ă 8.
When the conditions of Theorem 78 are satisﬁed, the joint posterior is also proper.
Proofs are provided in Appendix E.
7.4
Computational details
The exact density for the sample locations in (7.1) is not available analytically, so
approximation is required. In point process modeling the integral is often approx-
imated as the sum over a ﬁne grid. Letting t1, . . . , tM P D be a rectangular grid
178

covering D with cell area ∆,
ż
D
exptξpsquds « ∆
M
ÿ
j“1
exptξptjqu.
(7.4)
This approximation yields a tractable posterior, but requires computationally expen-
sive matrix inversions, which we limit using a kernel convolution approximation to
f.
Let δpsq be a mean zero Gaussian process with covariance cph | ψq. A process
convolution (Higdon, 2002) lets
δpsq “
ż
D
Kψps ´ uqdWpuq,
(7.5)
where W is Brownian motion and Kψ is a kernel with parameters ψ. The kernel
corresponding to the Mat´ern covariance is
Kψpuq “ τ
Γpν ` 1q1{2νν{4`1{4|u|ν{2´1{2
π1{2Γpν{2 ` 1{2qΓpνq1{2ρν{2`1{2Kν{2`1{2
ˆ2ν1{2|u|
ρ
˙
.
The kernel convolution representation of the Gaussian process in (7.5) is often used
to motivate dimension reduction for the spatial process. Let φ1, . . . , φN be a grid of
spatial knots. Then for large N
δpsq «
N
ÿ
j“1
Kψps ´ φjqwj,
(7.6)
where wj „ Np0, 1q. Applying kernel convolution to ξpsq and ηpsq yields
yi | si
„
N
"
xpsiq
Tβ˚ `
Nÿ
j“1
Kψηpsi ´ φjquj ` a
N
ÿ
j“1
Kψξpsi ´ φjqvj, σ2
*
, (7.7)
ppsiq
“
exp
!
xpsiqTβξ ` řN
j“1 Kψξpsi ´ φjqvj
)
řM
l“1 exp
!
xptlqTβξ ` řN
j“1 Kψξptl ´ φjqvj
),
179

where uj, vj „ Np0, 1q.
Selecting the number of grid points M and knots N is
discussed in §7.5 & 7.6.
We use a combination of Gibbs and Metropolis sampling for posterior computa-
tion. Assuming conjugate normal and inverse gamma priors, and reparameterization
so that uj „ Np0, τ 2
ηq and vj „ Np0, τξ2q, the full conditionals for β˚, a, τ 2
η , τ 2
ξ and
the vector pu1, . . . , uNqT are conjugate and we use Gibbs sampling. The correlation
parameters ρη and ρξ and the smoothness parameters νη and νξ are updated with
Metropolis sampling, tuned to have acceptance ratio near 0¨4. The sampling density
parameters vj are updated using blocked Metropolis sampling to account for poste-
rior correlation between coeﬃcients for nearby knots. We used ten blocks, with knots
allocated to blocks using k-means clustering implemented by the kmeans package in
R. For the simulation study in §7.5 we generated 5,000 samples and discarded the
ﬁrst 1,000 as burn-in. For the analysis of the ozone data in §7.6 we generated 20,000
samples and discarded the ﬁrst 5,000. Convergence was monitored using trace plots
of the deviance as well as several representative parameters.
7.5
Simulation study
We conduct a simulation study to illustrate the eﬀect of failing to account for infor-
mative sampling on spatial interpolation, and determine the amount of data need to
reliably identify informative sampling. We assume D “ r0, 1s2 and no spatial covari-
ates, xpsq “ 1 for all s. We generate data using model (7.7) with an equally-spaced
grid of N “ 225 knots on r-0¨2,1¨2s2 and a Mat´ern kernel. We generate S “ 50 data
sets from each of four simulation scenarios: (1) n “ 250, a “ 0, ρ “0¨2; (2) n “ 250,
a “ 1, ρ “0¨2; (3) n “ 250, a “ 1, ρ “0¨5; and (4) n “ 500, a “ 1, ρ “0¨2, with
σ “ 1, Etµpsqu “ 0, ν “2¨0, and τ “0¨1 under all scenarios. For each simulated data
set we ﬁt the following three models. The noninformative sampling (NIS) model
sets a “ 0, the plug-in model sets ξpsq “ ˆξpsq to account for informative locations,
180

and the full model implements the approach of §4. In the plug-in analysis the loca-
tion density is estimated using kernel density estimation in R’s KernSur function in
the GenKern package with default settings. GenKern gives a bivariate kernel density
estimate that uses Gaussian kernels with bandwidth chosen using a direct plug-in
approach to approximate the asymptotically optimal bandwidth.
We use the same grid of N “ 225 knots used to generate the data in the kernel
convolution model, and approximate the integral using a square grid of M “ 900
points t1, . . . , tM covering r0, 1s. Motivated by Rodrigues and Diggle (2010), we used
an equally spaced grid of 225 knots on r´0.2, 1.2s2. Simulation study results show
that irrespective of the number and position of the sampling locations, the Gaussian
process can be well approximated with 225 knots. Following Lee et al. (2005), the
grid spacings are chosen to be no larger than the standard deviation of the kernel in
the convolution representation. We use diﬀuse normal priors for β˚ and a and the
covariance parameters have priors σ2, τ 2
ξ , τ 2
η „ Inv-Gap0¨01, 0¨01q, ρ2
ξ, ρ2
η „ Up0, 2q,
and ν2
ξ , ν2
η „ Up0, 30q.
Table 1 reports bias, mean squared error (MSE), mean absolute deviation (MAD)
and coverage probability (CP), each averaged over the grid of M spatial locations
t1, . . . , tM. The coverage probability is the proportion of the M grid locations for
which the posterior 95% interval for µptjq covers the true value. For the plug-in
model and the full model we also report the power for a in Table 7.1 which is deﬁned
to be the proportion of data sets for which the posterior 95% credible interval for a
excludes zero.
All three methods perform similarly when sampling is not informative. In this
case, the informative sampling methods rarely identify a as signiﬁcant and reduce
to the usual geostatistical model.
The noninformative sampling model has high
mean squared error and negative bias in the remaining designs with informative
sampling. The two methods that allow for informative sampling reduce mean squared
181

Table 7.1: Simulation study results
Design
Model
MSE(ˆ102)
MAD(ˆ102)
Bias(ˆ102)
CP(ˆ102)
Power for a(ˆ102)
1
NIS
33¨1 (2¨8)
41¨3 (0¨6)
2¨0 (1¨3)
93¨0 (1¨0)
–
Plug-in
32¨2 (1¨7)
41¨3 (0¨)
2¨5 (1¨3)
93¨0 (1¨0)
10¨0
Full
31¨9 (1¨2)
41¨5 (0¨7)
2¨5 (1¨3)
93¨0 (1¨0)
10¨0
2
NIS
49¨4 (5¨0)
50¨0 (1¨1)
´25¨8 (1¨3)
90¨0 (1¨0)
–
Plug-in
39¨2 (5¨5)
44¨8 (0¨9)
´13¨9 (1¨3)
91¨0 (1¨0)
74¨0
Full
32¨9 (2¨8)
43¨2 (0¨8)
´7¨5 (1¨6)
93¨0 (1¨0)
80¨0
3
NIS
13¨2 (1¨1)
28¨1 (1¨8)
´8¨3 (1¨4)
94¨0 (1¨0)
–
Plug-in
12¨1 (0¨8)
27¨1 (1¨8)
´3¨1 (1¨4)
94¨0 (1¨0)
40¨0
Full
10¨8 (0¨7)
25¨3 (1¨4)
´2¨0 (1¨3)
95¨0 (1¨0)
50¨0
4
NIS
25¨6 (1¨1)
36¨9 (0¨7)
´15¨3 (1¨2)
92¨0 (1¨0)
–
Plug-in
20¨9 (0¨8)
33¨9 (0¨5)
´7¨2 (1¨1)
92¨0 (1¨0)
88¨0
Full
19¨1 (0¨6)
32¨6 (0¨4)
´0¨8 (1¨0)
94¨0 (1¨0)
98¨0
error compared to the noninformative sampling model. The informative sampling
models also reduce bias, although some bias remains, especially for design 2. In all
cases the full model improves on the plug-in approach. The relative mean squared
error of the noninformative sampling model to the full model is smaller for design 3
(0¨132/0¨108 = 1¨222) with large spatial range and design 4 (0¨256/0¨190=1¨347) with
large sample size than for design 2 (0¨494/0¨329 = 1¨502), so it seems that accounting
for informative sampling is most important for small data sets with considerable
spatial variation.
To analyze sensitivity to the prior for a, we redid simulation design 2 with a “ 1
and ρ=0¨2 and used four diﬀerent priors for a: Np1, 1q, Np0, 1q, Np0, 102q and an
improper prior. In summary, mean squared prediction error and predictive coverage
are insensitive to the hyperparameters of the prior on a for n “ 150 and n “ 200.
Even for a sample size as small as n “ 50, diﬀerences are small for diﬀerent priors.
However, the Np0, 102q prior and the informative prior Np1, 1q lead to a better power
for a than the others when n “ 50 and 100. The minimum sample size needed to
swamp out the prior for a is around 150 in this example.
182

7.6
Analysis of Eastern United States ozone data
With the increasing concern about air pollution and climate change, building predic-
tive models for ozone is an important area. It is often the case that the monitoring
locations are informative about the ozone surface and hence it is important to ac-
count for informative sampling. We analyze the median daily ozone for June-August
2007 for n “ 631 observations in the Eastern United States. The data are plotted
in Fig. 7.1(a). There is a clear association between the sampling density and the
response, as there are more monitors placed in areas with high ozone, such as At-
lanta and New England, than areas with low ozone, such as Mississippi and West
Virginia. We ﬁt a generalized additive model to the median ozone values and the
kernel density estimate of the log sampling density using locally weighted scatterplot
smoothing in Fig. 7.1(b). The linear ﬁt is entirely contained within the generalized
additive model 95% conﬁdence intervals for all values of the log sampling density
estimate, supporting the log linear model in (7.1).
To apply a stationary spatial model we ﬁrst project the spatial locations to a
two-dimensional surface using the Mercator projection, and then scale them to the
unit square coordinate-wise by subtracting the minimum and dividing by the range
of the observation locations. We ﬁt the informative sampling model with a 30 ˆ 30
grid of knots on r´0¨2, 1¨2s2 in the kernel convolution approximation in (7.6) and
a 50 ˆ 50 grid of points on r0, 1s2 in the integral approximation in the sampling
density (7.4). Points outside the convex hull of the observation locations or outside
the continental United States were discarded from integral approximation to the
sampling density, leaving M “ 1077. Kernel convolution knots not within 0¨1 of an
integral approximation knot were discarded, leaving N “ 490.
We include a second-order spatial trend as predictors in xpsq, that is, linear
and quadratic terms for re-scaled latitude and longitude, and their interaction. We
183

−90
−85
−80
−75
−70
30
35
40
45
60
70
80
90
100
(a) Median ozone
(b) Log sampling density versus median ozone
(circles), gamﬁt with 95% intervals (dashed),
linear ﬁt (solid)
Figure 7.1: Plots of the ozone data. Panel (a) plots the ozone data (ppb; color)
and monitor locations (points), Panel (b) plots the estimated log sampling density
against the response.
compare the noninformative sampling, plug-in and full models described in §7.5. The
posteriors for several parameters are summarized in Table 7.2. The spatial process
for both the mean process and sampling density are fairly smooth. The posterior
95% intervals for νξ and νη exclude the exponential covariance (ν “ 0¨5) for all the
three models.
The 95% interval of a for both the plug-in model (2¨16, 6¨46) and fully Bayesian
model (2¨12, 4¨25) excludes zero, indicating an informative sampling scheme. The
scale of a’s posterior is not comparable between the two models since the plug-in
density estimate has been standardized to have mean zero and variance one. The
eﬀect of accounting for informative sampling is illustrated in Fig. 7.2. The diﬀerence
in predicted values between the noninformative sampling and full model in Fig.
7.2(c) is the largest in Northern Pennsylvania and West Virginia. These areas have
184

−85
−80
−75
−70
30
35
40
70
75
80
85
90
(a) Posterior mean predicted values from full model
−85
−80
−75
−70
30
35
40
−8.0
−7.5
−7.0
−6.5
−6.0
−5.5
−5.0
(b) Log sampling density from the full model
−85
−80
−75
−70
30
35
40
−4
−2
0
2
4
(c) Posterior mean predicted values (NIS - full)
−85
−80
−75
−70
30
35
40
−4
−2
0
2
4
(d) Posterior mean predicted values (NIS - plug-in)
Figure 7.2: Posterior mean predicted values of ozone
relatively few monitors and are near areas with high ozone. The diﬀerence between
the noninformative sampling and plug-in predictions in Fig. 7.2(d) are also positive in
these areas though the diﬀerences are not nearly as large in the plug-in analysis. This
may be because the plug-in estimates do not appropriately account for uncertainty
185

Table 7.2: Mean and 95% intervals for the ozone data
Parameters
NIS
Plug-in
Full
a
–
4¨43 (2¨16, 6¨46)
3¨21 (2¨12, 4¨25)
σ
4¨68 (4¨37, 5¨03)
4¨70 (4¨38, 5¨04)
4.78 (4.47, 5.12)
τg
0¨17 (0¨14, 0¨27)
0¨15 (0¨13, 0¨19)
0¨17 (0¨13, 0¨21)
ρg
0¨06 (0¨05, 0¨16)
0¨06 (0¨04, 0¨10)
0¨06 (0¨05, 0¨10)
νg
3¨95 (0¨92, 6¨42)
3¨46 (1¨53, 5¨52)
12¨6 (0¨74, 28¨8)
τf
–
–
0¨05 (0¨04, 0¨06)
ρf
–
–
0¨07 (0¨04, 0¨13)
νf
–
–
10¨7 (0¨74, 28¨77)
in estimation, and hence may lead to some attenuation of the estimated surface.
Finally, we reﬁt the model with diﬀerent priors and diﬀerent knot locations to
test for sensitivity to these assumptions. We ﬁt the model with 20ˆ20 and 40ˆ40
initial grids of knots in the kernel convolution approximation. After removing knots
outside the domain of interest, this gave N “ 206 and N “ 876 knots, respectively.
The results were fairly similar to the original 30ˆ30 grid. In all cases the posterior of
a was separated from zero, the posterior median being 3¨31 and 2¨85 for N “ 206 and
N “ 876 knots, respectively, and the largest diﬀerence between the noninformative
sampling and full model was in the Northern Pennsylvania and West Virginia.
7.7
Discussion
We have focused on a simple model for informative locations, which assumes that
the outcomes are conditionally independent of the locations given the mean process
µpsq and the spatial location density ppsq. In addition, we include a single param-
eter a controlling the informativeness of the sampling process. These simplifying
assumptions certainly make the theory and computation more tractable. However,
to more realistically characterize data from a broader variety of applications, it may
186

be necessary to generalize the models. There are several interesting directions in
this regard. First, it is straightforward conceptually to replace the constant a with
a spatially-varying coeﬃcient apsq, which is assigned a Gaussian process prior. This
generalization allows the informativeness of the sampling locations to vary spatially;
for example, in certain regions, say near cities, monitors may be placed without re-
gard to the outcome, while in other regions, say in the rural areas, monitors may be
placed at sites likely to have high values of ozone. It is an open question whether
one can consistently estimate apsq in this extended model without very restrictive
assumptions. However, a simple adjustment for informative sampling may be prefer-
able to more complicated models that require rich datasets for reliable estimation.
187

8
Future works
8.1
Latent variable density regression models
Current density regression models focusing on mixture models are often a black-box
in terms of realistic applications which require needing to center the model on a sim-
ple parametric family without sacriﬁcing computational eﬃciency. Stephen Walker
pointed out in one of the ISBA bulletins “Current density regression models are too
big, too non-identiﬁable and I doubt whether they would survive the test of time.”
Lenk (1988, 1991); Tokdar et al. (2010b) proposed a logistic Gaussian process prior
which also allows convenient prior centering in density regression models, however
computationally quite challenging. Somewhat discontented with the discrete mixture
formulation of the existing density estimation and density regression models which
doesn’t allow for convenient prior centering, we turned our attention to latent vari-
able models which have become increasingly popular as a dimension reduction tool
in machine learning applications. Although latent variable models are widely used
in machine learning community, it was only recently realized (Kundu and Dunson,
2011) that they are also suitable for density estimation. Kundu and Dunson (2011)
188

developed a density estimation model where unobserved Up0, 1q latent variables are
related to the response variables via a random non-linear regression with an additive
error. This allows convenient prior centering, avoids the mixture formulation and
enables eﬃcient computation through a griddy Gibbs algorithm Kundu and Dunson
(2011). However, there has been little study on theoretical properties of these mod-
els. In particular does it share the same appealing support and optimal convergence
properties of some of the existing methods? Can it achieve faster convergence rates
if the prior centering is appropriate? In an ongoing paper Pati et al. (2011b), we
answered these questions in the aﬃrmative by characterizing the space of densities
induced by the above model as kernel convolutions with a general class of continuous
mixing measures. Our paper Pati et al. (2011b) leads to the following simple density
regression formulation. Consider the following non-linear latent variable model,
yi “ µpηi, xiq ` ǫi, ηi „ Up0, 1q, ǫi „ Np0, σ2q
(8.1)
Integrating out ηi,
fpy | xq “
ż
φσpy ´ µpt, xqqdt
The above model can approximate a large collection of conditional densities tf0py |
xqu by letting µpt, xq concentrate around the conditional quantile functions F ´1
0 py | xq
by assigning a Gaussian process prior. A couple of advantages of this formulation
is the feasibility of an eﬃcient posterior computation based on an uni-dimensional
griddy Gibbs algorithm and the ability to center the model on a prior paramet-
ric guess which are not both shared by any of the prevalent density regression ap-
proaches. Studying rates of convergence in density regression models becomes more
challenging as we need to assume mixed smoothness in y and x. Although poste-
rior contraction rates are studied widely in mean regression, logistic regression and
density estimation models, results on convergence rates for density regression models
189

are lacking. In the ongoing work Pati et al. (2011a), we study posterior convergence
rates of the density regression model (8.1) by assuming the true conditional density
has diﬀerent smoothness across y and x. Assuming f0py | xq to be compact and twice
and thrice continuously diﬀerentiable in y and x, we obtain a rate of n´1{3plog nqt2
using a Gaussian process prior for µ having a single inverse-Gamma bandwidth across
diﬀerent dimensions. The optimal rate in such a mixed smoothness class is n´6{17.
The slight slow rate of n´1{3 is the drawback of using an isotropic Gaussian process
used for modeling an anisotropic function. Current research focuses on improving
the rate of convergence using an anisotropic Gaussian process with diﬀerent scaling
across diﬀerent dimensions.
8.2
Nonparametric variable selection
Although Pati et al. (2011a) and Chapter 2 deal with the optimal convergence
rate in estimating the true regression function, it might be of importance to
study the behavior of the marginal posterior inclusion probabilities of individ-
ual variables to actually study consistency and rates of convergence of vari-
able selection.
Alternatively, let µi
“
tµpxi1q, . . . , µpxipqu and let µp´jq
i
“
tµpxi1q, . . . , µpxij´1q, µpx0jq, µpxij`1q, . . . , µpxipqu, where x0j is some reference point
which is ﬁxed across subjects. Then, under the hypothesis H0j that the jth variable
has no impact on the regression function, it would seem that µi & µp´jq
i
will be very
close asymptotically. We are interested in providing suﬃcient conditions for Bayes
factor consistency in testing
Hpnq
0j : ||µ ´ µp´jq||2,n ă ǫn, Hpnq
1j : ||µ ´ µp´jq||2,n ą ǫn
(8.2)
where || ¨ ||2,n denotes the L2pPnq norm, Pn being the empirical distribution. Hence
ǫn Ó 0 determines the convergence rate of the variable selection.
190

8.3
Bayesian shape modeling
Object segmentation and surface ﬁtting from pixelated volume data is widely used
in bio-medical applications as a part of pre-therapeutic diagnosis. We plan to de-
velop a novel Bayesian method for inferring the surface of a 3-dimensional object
by modeling the intensity of the pixels with a mixture of normals with the weights
depending ﬂexibly on the pixel coordinates. We want our approach to yield smooth,
closed surface estimates that can prove highly useful in medical diagnosis and other
general imaging applications. Lung tumors moving during respiration are particu-
larly challenging. Because of irregular breathing and imaging limitations it can be
hard to characterize and predict how a tumor will move during treatment. There has
been essentially no work on Bayesian hierarchical modeling of 3d surfaces evolving
over time (e.g, tumor image stacks of diﬀerent individual at several time points).
Our objective is to provide a joint framework for registering and modeling multiple
3d shapes. Instead of registering the entire volumetric data, we want to be able
to develop a computationally eﬃcient selective registration scheme which takes into
account only the tumor region. One of the long-term goals is to develop an algorithm
which uses the real-time 3D images acquired during treatment to adapt the radiation
beam to optimize the dose being delivered to the target based on the updated im-
ages. Finally, working with volumetric data poses several computational bottlenecks.
We propose to overcome these computational challenges via an adaptive partitioning
scheme, and random projection-based techniques.
8.4
Spatial point patterns
The log Gaussian Cox process model which we have exploited in Chapter 7 in the
context of preferential sampling is useful in a variety of other spatial settings. More
commonly it is used to model spatial point pattern data such as the locations of for-
191

est ﬁres or earthquakes. Here the parametric gps; θq could capture eﬀects of spatial
covariates, e.q., gps; θq “ exppdistance from location s to a fault ˚ θq. Sampling for
these types of models is highly challenging, particularly in presence of information
on numerous spatially varying covariates where we are also interested in in selecting
the important variables. The approach that is usually taken is to approximate the
denominator of the likelihood of the log-Gaussian Cox process by a ﬁnite Riemann
sum. However, since the exponentiated Gaussian process is not an inﬁnitely divisi-
ble process, there has been a debate whether the approximated posterior converges
to the true posterior. Stephen Walker recently developed an exactly sampling al-
gorithm relying on the introduction of latent variables which removes any integrals
associated with the inaccessibility of the normalizing constant. This would open up
the possibility of exact sampling for a wide range of models (e.g., the log Gaussian
Cox process for the sampling distribution in our informative sampling model) which
would circumvent the existing criticisms.
8.5
Robust Bayesian model based clustering
Model-based clustering based on mixtures of parametric kernels is a substantially
popular tool for separating heterogeneous collection of items into homogeneous sub-
sets. However, accurate estimation of the number of clusters as well as the cluster
speciﬁc densities is highly sensitive to the choice of the kernels. To address this issue,
we propose to develop a novel Bayesian hierarchical clustering model based on mix-
tures of constrained unimodal kernels where we potentially cluster based on modes
without restricting the form of the kernel other than assuming it to be unimodal.
We plan to explore theoretical directions like consistency of the number of clusters
and the cluster speciﬁc densities.
192

8.6
Other directions
Another interesting direction is when the true regression function is supported on
a smaller dimensional linear subspace and it is of importance to estimate the min-
imal subspace, popularly termed as suﬃcient dimension reduction. Although there
has been some works on Bayesian suﬃcient dimension reduction Reich et al. (2010);
Tokdar et al. (2010a), accurate calibration of the posterior contraction rate in such
settings is still an open area of research.
In high dimensional small sample size scenario e.g. gene expression data, it be-
comes necessary to come up with simple parametric procedures with an accurate
calibration for the prior that automatically adjusts for multiplicity. I would particu-
larly like to explore variable selection consistency and convergence rates in Bayesian
models where the number of predictors are increasing faster than the sample size.
There has been a recent surge of interest in the frequentist literature in working out
minimax rates for estimating high-dimensional covariance matrices where the dimen-
sionality increases with the sample size and the truth lies in some sparsity class, but
almost no work from a factor model type representation which are more commonly
used in Bayesian factor models for learning covariance matrices. A particularly in-
teresting direction is to consider estimation of covariance matrices by assuming a low
rank decomposition which arises from factor models.
Apart from these speciﬁc directions, one of my long-term research goals is to
provide a non-asymptotic theoretical framework for comparison of frequentist and
Bayesian procedures. Although, in most cases, Bayesian procedures behave as well
as the frequentist procedures asymptotically, empirical evidence often suggests that
the Bayesian procedures are superior to the frequentist counterparts in speciﬁc prob-
lems, particularly in a high-dimensional sparse data setting. A rigorous theoretical
framework is necessary to validate and promote the use of Bayesian procedures. An-
193

other interesting direction is to theoretically compare diﬀerent nonparametric Bayes
models and evaluate the coverage probability of the parameter of interest in the light
of Knapik et al. (2011).
194

Appendix A
Proofs of some results in Chapter 4
A.1
Proof of Lemma 46
The
proof
proceeds
similarly
to
that
for
Theorem
3
in
Ghosal et al. (1999). Note that t ˜Gx, x P X u P G˚
X. Let B “
`
r´a, asp ˆ pσ, σq
˘
.
Choose k ą pa ` σ such that
ż
X
ż
|y|ąk
f0py | xq
"|y| ` pa
2σ2
*2
dyqpxqdx ă ǫ
2.
Take V “ ttGx | x P X u : infxPX GxpBq ą σ
σu. By approximating 1B by a bounded
continuous function, we can show that V contains a neighborhood V 1 of t ˜Gx | x P X u
of the type (4.15). For any density f P Fd, fpy | xq “
ş 1
σφ
`y´x1β
σ
˘
dGxpβ, σq, x P X ,
195

with tGx | x P X u P V 1,
ż
X
ż
|y|ąk
f0py | xq log
" ˜fpy | xq
fpy | xq
*
dyqpxqdx
ď
ż
X
ż
|y|ąk
f0py | xq log
ş
B
1
σφ
` y´x1β
σ
˘
d ˜Gxpβ, σq
ş
B
1
σφ
` y´x1β
σ
˘
dGxpβ, σq
dyqpxqdx
ď
ż
X
ż
|y|ąk
f0py | xq log
1
σφ
`|y|´pa
σ
˘
1
σφ
` |y|`pa
σ
˘
GxpBq
dyqpxqdx
ď
ż
X
ż
|y|ąk
f0py | xq
"|y| ` pa
2σ2
*2
dyqpxqdx ă ǫ
2.
Let infty:|y|ďkuˆX infpβ,σqPB
1
σφ
` y´x1β
σ
˘
“ c. Consider the uniformly equi-continuous
family of functions
"
gy,x : gy,x : B Ñ ℜ, pβ, σq ÞÑ 1
σφ
ˆy ´ x1β
σ
˙
, py, xq P r´k, ks ˆ X
*
.
By the Arzela-Ascoli theorem, given δ ą 0, there exists ﬁnitely many points tpyi, xiq P
r´k, ks ˆ X , i “ 1, . . . , mu such that for any py, xq P r´k, ks ˆ X , D i such that
sup
pβ,σqPB
|gy,xpβ, σq ´ gyi,xipβ, σq| ă cδ.
Let gyi,xi ˚ Gx “
ş 1
σ
ş
φ
ˆ
yi´x1
iβ
σ
˙
dGxpβ, σq.
E “
"
tGx | x P X u : sup
xPX
ˇˇˇgyi,xi ˚ Gx ´ gyi,xi ˚ ˜Gx
ˇˇˇ ă cδ, i “ 1, 2, . . . , m
*
.
It holds that E is a neighborhood of t ˜Gx | x P X u formed by ﬁnite intersections of
sets of the type (4.15) and for tGx | x P X u P E and py, xq P r´k, ks ˆ X ,
ˇˇˇˇˇ
ş 1
σφ
` y´x1β
σ
˘
d ˜Gxpβ, σq
ş 1
σφ
` y´x1β
σ
˘
dGxpβ, σq
´ 1
ˇˇˇˇˇ ă
3δ
1 ´ 3δ.
196

for δ ă 1
3. Thus given any ǫ ą 0, there exists a neighborhood E of t ˜Gx | x P X u such
that for tGx | x P X u P E with fpy | xq “
ş 1
σφ
`y´x1β
σ
˘
dGxpβ, σq,
ż
X
ż
y:|y|ďk
f0py | xq log
" ˜fpy | xq
fpy | xq
*
dyqpxqdx ă ǫ
2.
(A.1)
Taking W “ V 1 X E and since W is a ﬁnite intersection of neighborhoods of ˜Gx of
the type (4.15), the result follows immediately.
A.2
A useful lemma
Lemma 79. If {πhpxq, h “ 1, . . . , 8} constructed as in (4.11) satisﬁes S1 and S2
then
PX
"
sup
xPX
|π1pxq ´ FxpA1q| ă ǫ1, . . . , sup
xPX
|πkpxq ´ FxpAkq| ă ǫk
*
ą 0.
(A.2)
for a measurable partition tAi, i “ 1, . . . , ku of ℜp ˆ ℜ`, ǫi ą 0 and a conditional cdf
tFx, x P X u.
Proof. Without loss of generality, let 0 ă FxpAiq ă 1, i “ 1, . . . , k @ x P X . We want
to show that for any ǫi ą 0, i “ 1, . . . , k, (A.2) holds. Construct continuous functions
gi : X ÞÑ ℜ, 0 ă gipxq ă 1 @x P X , i “ 1, . . . , k ´ 1 such that
g1pxq “ FxpA1q, gipxq
ź
lăi
t1 ´ glpxqu “ FxpAiq, 2 ď i ď k ´ 1, gkpxq “ 1 @x.
(A.3)
As 0 ă FxpAiq ă 1, i “ 1, . . . , k @ x P X , it is trivial to ﬁnd gi, i “ 1, . . . , k satisfying
(A.3) since one can solve back for the gi’s from (A.3). řk
i“1 FxpAiq “ 1 enforces
gk ” 1. Since Φ is a continuous function, for any ǫi ą 0, i “ 1, . . . , k ´ 1,
PX
"
sup
xPX
|Φtαipxqu ´ gipxq| ă ǫi
*
ą 0
(A.4)
197

and for i “ k,
PX
"
sup
xPX
|Φtαkpxqu ´ 1| ă ǫk
*
“ PX
"
inf
xPX αkpxq ą Φ´1p1 ´ ǫkq
*
.
(A.5)
Choose M ą Φ´1p1 ´ ǫkq ` ǫk. We have 0 ă M ă 1 and
"
sup
xPX
|αkpxq ´ M| ă ǫk
*
Ă
"
inf
xPX αkpxq ą Φ´1p1 ´ ǫkq
*
.
Hence by assumption, PX
"
infxPX αkpxq ą Φ´1p1 ´ ǫkq
*
ą 0.
Let Sk´1 denote
the k-dimensional simplex. For notational simplicity let pipxq “ Φtαipxqu, gipxq “
FxpAiq, i “ 1, . . . , k ´ 1 and gkpxq “ 1. Let z “ pz1, . . . , zpq1, fi : Sk´1 Ñ ℜ, z ÞÑ
zi
ś
lăip1 ´ zlq, i “ 2, . . . , k and f1pzq “ z1.
Let ppxq “ pp1pxq, . . . , pkpxqq and
gpxq “ pg1pxq, . . . , gkpxqq. Then we need to show that
PXt}f1ppq ´ f1pgq}8 ă ǫ1, . . . , }fk´1ppq ´ fk´1pgq}8 ă ǫk´1,
}fkppq ´ 1}8 ă ǫku ą 0.
Note that for 2 ď i ď k,
}fippq ´ fipgq}8 “
›››››pi
␣
1 ´
ÿ
lăi
flppq
(
´ gi
␣
1 ´
ÿ
lăi
flpgq
(
›››››
8
ď pi ´ 1q }pi ´ gi}8 `
ÿ
lăi
}flppq ´ flpgq}8 .
Thus one can get ǫ˚
i ą 0, i “ 1, . . . , k, such that
t}pi ´ gi}8 ă ǫ˚
i , i “ 1, . . . , ku Ă t}f1ppq ´ f1pgq}8 ă ǫ1, . . . ,
}fk´1ppq ´ fk´1pgq}8 ă ǫk´1, }fkppq ´ 1}8 ă ǫku.
But since PXt}pi ´ gi}8 ă ǫ˚
i , i “ 1, . . . , ku “ śk
i“1 PXt}pi ´ gi}8 ă ǫ˚
i u, the result
follows immediately.
198

A.3
Proof of Theorem 47
Fix tFx, x P X u P G˚
X. Without loss of generality it is enough to show that for a
uniformly continuous function g : ℜp ˆ ℜ` ˆ X Ñ r0, 1s and ǫ ą 0,
PX
"
tGx, x P X u :
ˇˇˇˇ
ż
ℜpˆℜ`ˆX
␣
gpβ, σ, xqdGxpβ, σqqpxqdx ´
gpβ, σ, xqdFxpβ, σqqpxqdx
(ˇˇˇˇ ă ǫ
*
ą 0.
Furthermore, it suﬃces to assume gpβ, σ, xq Ñ 0 uniformly in x P X as }β} Ñ
8, σ Ñ 8.
Fix ǫ ą 0, there exists a, σ, σ ą 0 not depending on x such that Fxpr´a, asp ˆ
rσ, σsq ą 1 ´ ǫ for all x P X . Let C “ r´a, asp ˆ rσ, σs.
ż
ℜpˆℜ`X
␣
gpβ, σ, xqdGxpβ, σq ´ gpβ, σ, xqdFxpβ, σq
(
qpxqdx ď
ż
X
" 8
ÿ
h“1
πhpxqgpβh, σh, xq ´
ż
C
gpβ, σ, xqdFxpβ, σq
*
qpxqdx ` ǫ.
where πh’s are speciﬁed by 4.11 with ch satisfying S1 and S2 and pβh, σhq „ G0. Now
for each x P X , construct a Riemann sum approximation of
ż
C
gpβ, σ, xqdFxpβ, σq.
Let tAk,n, k “ 1, . . . , nu be sequence of partitions of C with increasing reﬁnement as
n increases. Assume max1ďkďn diampAk,nq Ñ 0 as n Ò 8. Fix p ˜βk,n, ˜σk,nq P Ak,n, k “
1, . . . , n. Then by DCT as n Ñ 8,
ż
X
"
nÿ
k“1
gp ˜βk,n, ˜σk,n, xqFxpAk,nquqpxqdx Ñ
ż
X
ż
C
gpβ, σ, xqdFxpβ, σqqpxqdx.
(A.6)
199

Hence there exists n1 such that for n ě n1
ˇˇˇˇ
ż
ℜpˆℜ`X
␣
gpβ, σ, xqdGxpβ, σq ´ gpβ, σ, xqdFxpβ, σq
(
qpxqdx
ˇˇˇˇ ď
ˇˇˇˇˇ
ż
X
" 8
ÿ
h“1
πhpxqgpβh, σh, xq ´
nÿ
k“1
gp ˜βk,n, ˜σk,n, xqFxpAk,nq
*
qpxqdx
ˇˇˇˇˇ ` 2ǫ.
Consider the set
Ω1 “
"
pπh, h “ 1, . . . , 8q : sup
xPX
|π1pxq ´ FxpA1,n1q| ă ǫ
n1
, . . . ,
sup
xPX
|πn1pxq ´ FxpAn1,n1q| ă ǫ
n1
*
.
By Lemma 79 which is proved in A.2, PXpΩ1q ą 0.
Since ř8
h“1 πhpxq “ 1 a.s.
there D Ωwith PXpΩq “ 1, such that for each ω “ tπh, h “ 1, . . . , 8u P Ω, gnpxq “
řn
h“1 πhpxq Ñ 1 as n Ñ 8 for each x in X . Note that this convergence is uniform
since, gnp¨q, n ě 1 are continuous functions deﬁned on a compact set monotonically
increasing to a continuous function identically equal to 1.
Hence for each ω “
tπh, h “ 1, . . . , 8u P Ω, gnpxq Ñ 1 uniformly in x. By Egoroﬀ’s theorem, there
exists a measurable subset Ω2 of Ω1 with PXpΩ2q ą 0 such that within this subset
gnpxq Ñ 1 uniformly in x and uniformly in ω in Ω2. Thus there exists a positive
integer nǫ ě n1 not depending on x and ω, such that ř8
h“nǫ`1 πhpxq ă ǫ on Ω2.
Moreover, one can ﬁnd a K ą 0 independent of x such that gpβ, σ, xq ă ǫ if }β} ą K
and σ ą K. Let A1 “ tpβ, σq : }β} ą K, σ ą Ku. Let Ω3 “ Ω2 X tpβn1`1, σn1`1q P
A1, . . . , pβnǫ´1, σnǫ´1q P A1u. For ω P Ω3,
ˇˇˇˇ
ż
ℜpˆℜ`X
␣
gpβ, σ, xqdGxpβ, σq ´ gpβ, σ, xqdFxpβ, σq
(
qpxqdx
ˇˇˇˇ ď
ż
X
" n1
ÿ
k“1
ˇˇˇπkpxqgpβk, σk, xq ´ gp ˜βk,n, ˜σk,n, xqFxpAk,n1q
ˇˇˇ
*
qpxqdx ` 4ǫ
200

and
ż
X
" n1
ÿ
k“1
ˇˇˇπkpxqgpβk, σk, xq ´ gp ˜βk,n, ˜σk,n, xqFxpAk,n1q
ˇˇˇ
*
qpxqdx
ď
n1
ÿ
k“1
ż
X
"
πkpxq
ˇˇˇgpβk, σk, xq ´ gp ˜βk,n, ˜σk,n, xq
ˇˇˇ ` |πkpxq ´ FxpAk,n1q|
*
qpxqdx
ď
n1
ÿ
k“1
ż
X
πkpxq
ˇˇˇgpβk, σk, xq ´ gp ˜βk,n, ˜σk,n, xq
ˇˇˇ qpxqdx ` ǫ.
There exists sets Bk, k “ 1, . . . , n1 depending on n1 but independent of x such that if
pβk, σkq P Bk,
ˇˇˇgpβk, σk, xq ´ gp ˜βk,n1, ˜σk,n1, xq
ˇˇˇ ă ǫ. So for ω P Ω4 “ Ω3 X tpβ1, σ1q P
B1, . . . , pβn1, σn1q P Bn1u,
ˇˇˇˇ
ż
ℜpˆℜ`X
␣
gpβ, σ, xqdGxpβ, σq ´ gpβ, σ, xqdFxpβ, σq
(
qpxqdx
ˇˇˇˇ ă 5ǫ.
Now since PXpΩ2q ą 0 and the sets tpβn1`1, σn1`1q P A1, . . . , pβnǫ´1, σnǫ´1q P A1u
and tpβ1, σ1q P B1, . . . , pβn1, σn1q P Bn1u are independent from Ω2 and have positive
probability, it follows that PXpΩ4q ą 0.
A.4
Proof of Theorem 44
Without loss of generality, assume that the covariate space X is rζ, 1sp for some
0 ă ζ ă 1. The proof is essentially along the lines of Theorem 3.2 of Tokdar (2006a).
The ˜f in (4.12) will be constructed so as to satisfy the assumptions of Lemma 46 and
such that
ş
X
ş
Y f0py | xq log f0py|xq
˜fpy|xq dyqpxqdx ă ǫ
2 for any ǫ ą 0. Deﬁne a sequence of
conditional densities fnpy | xq “
ş 1
σφp y´x1β
σ
qd ˜Gn,xpβ, σq, n ě 1 where for σn “ n´η,
dGn,xpβ, σq “
Iβ1Pr´n,nsf0px1β | xq śp
j“2 δ0pβjqδσnpσq
şn
´n f0px1β1 | xqdβ1
.
(A.7)
201

Deﬁne
fnpy | xq “
şnx1
´nx1
1
σnφp y´t
σn qf0pt | xqdt
şnx1
´nx1 f0pt | xqdt
.
(A.8)
Proceeding as in Theorem 3.2 of Tokdar (2006a), an application of DCT using the
conditions A1-A5 yields
ż
X
ż
Y
f0py | xq log f0py | xq
fnpy | xqdyqpxqdx Ñ 0 as n Ñ 8.
Therefore one can simply choose ˜f “ fn0 for suﬃciently large n0.
fn0 satisﬁes
the assumptions of Lemma 46 since tGn0,x, x P X u is compactly supported. Also
tGn0,x, x P X u P G˚
X as x Ñ Gn0,xpAq is continuous. Hence there exists a ﬁnite
intersection W of neighborhoods of tGn0,x, x P X u the type (4.15) such that for any
tGx, x P X u P W, the second term of (4.12) is arbitrarily small. The conclusion of
the theorem follows immediately from Corollary 49.
A.5
Proof of Theorem 51
Consider the sequence of sieves deﬁned by (4.18) for given ǫ ą 0 and for sequences
an, hn, ln, Mn, mn, rn to be chosen later with δn “ K1ǫ{pMnm2
nq for some constant
K1. We will ﬁrst show that given ξ ą 0, there exists c1, c2 ą 0 and sequences mn
and Mn, such that ΠX
`
F c
n
˘
ď c1e´nc2 and log Npδ, Fn, }¨}q ă nξ.
202

For f1, f2 P Fn, we have for each x P X ,
}f1p¨ | xq ´ f2p¨ | xq}1 ď
ż
Y
mn
ÿ
h“1
πp1q
h pxq
ˇˇˇφβp1q
h ,σp1q
h px, yq ´ φβp2q
h ,σp2q
h px, yq
ˇˇˇ dy
`
ż
Y
mn
ÿ
h“1
ˇˇˇπp1q
h pxq ´ πp2q
h pxq
ˇˇˇ φβp2q
h ,σp2q
h px, yqdy
`
8
ÿ
h“mn`1
␣
πp1q
h pxq ` πp2q
h pxq
(
ď
mn
ÿ
h“1
πhpxq
$
&
%
ˆ2
π
˙1{2
›››βp2q
h ´ βp1q
h
››› ?p
σp2q
h
` 3pσp2q
h ´ σp1q
h q
σp1q
h
,
.
-
`
mn
ÿ
h“1
›››πp1q
h
´ πp2q
h
›››
8 ` 2ǫ.
Let Θπ,n “ tπmn “ pπ1, π2, . . . , πmnq : αh P Bh,n, h “ 1, . . . , mnu. Fix πmn
1 , πmn
2
P
Θπ,n. Note that since |Φpx1q ´ Φpx2q| ă K2 |x1 ´ x2| for a global constant K2 ą 0,
we have
}Φpαh,1q ´ Φpαh,2q}8 ď K2 }αh,1 ´ αh,2}8 .
The above fact together with the proof of Lemma 79 show that if we can make
}αh,1 ´ αh,2}8 ă
ǫ
m2n, h “ 1, . . . , mn, we would have řmn
h“1
›››πp1q
h ´ πp2q
h
›››
8 ă ǫ. From
the proof of Theorem 3.1 in van der Vaart and van Zanten (2009) it follows that for
h “ 1, . . . , mη
n and for suﬃciently large Mn, rn,
log Np2ǫ{m2
n, Bh,n, }¨}8q ď K3rp
n log
˜
Mnm2
n
a
rn{δn
ǫ
¸p`1
`
2 log K4Mnm2
n
ǫ
.
(A.9)
for global constants K3, K4 ą 0. For M2
n ą 16K5rp
nplogprn{ǫqq1`p, rn ą 1 we have for
h “ 1, . . . , mη
n,
Ppαh R Bh,nq ď PpAh ą rnq ` e´M2
n{2.
(A.10)
203

Hence for suﬃciently large Mn, we have for h “ mη
n ` 1, . . . , mn,
log Np3ǫ{m2
n, Bh,n, }¨}8q ď 2 log K4Mnm2
n
ǫ
.
(A.11)
For h “ mη
n ` 1, . . . , mn,
Ppαh R Bh,nq ď PpAh ą δnq `
ż δn
a“0
Ppαh R Bh,n | Ah “ aqgAhpaqda
ď PpAh ą δnq `
ż δn
a“0
Ppαh R MnHa
1 ` ǫB1 | Ah “ aqgAhpaqda
ď PpAh ą δnq ` p1 ´ ΦpΦ´1pe´φδn
0 pǫ{m2
nqq ` Mnqq.
where φκ
0pǫq denotes the concentration function of the Gaussian process with covari-
ance kernel cpx, x1q “ τ 2e´κ}x´x1}2. Now
φδn
0 pǫ{m2
nq ď ´ log Pp|W0| ď ǫ{m2
nq “ K6
ˇˇlogpǫ{m2
nq
ˇˇ
for
some
constant
K6
ą
0.
Hence
if
Mn
ě
K7 |logpǫ{m2
nq|
for
some
K7
ą
0,
then
it
follows
from
the
proof
of
Theorem
3.1
in
van der Vaart and van Zanten (2009) that
Ppαh R Bh,nq ď PpAh ą δnq ` e´M2
n{2.
(A.12)
From (A.9) and (A.11),
logpNpǫ, B1,n ˆ ¨ ¨ ¨ ˆ Bmn,n, }¨}8q ď2mn log K4Mnm2
n
ǫ
`
mη
nrp
n log
˜
Mnm2
n
a
rn{δn
ǫ
¸p`1
.
(A.13)
Also from (A.10) and (A.12),
mn
ÿ
h“1
Ppαh R Bh,nq ď mne´M2
n{2 `
mη
n
ÿ
h“1
PpAh ą rnq `
mn
ÿ
h“mη
n`1
PpAh ą δnq.
204

We will show that with mn “ Op
n
log nq, ΠXpF c
nq ă e´nξ0 for some ξ0. By assump-
tion C1, we have
ΠXpΘc
an,hn,lnq À mnOpe´nq À Ope´nq.
(A.14)
With mn “ Opn{ log nq, řmη
n
h“1 PpAh ą rnq ď mη
ne´n À e´n, řmn
h“mη
n`1 PpAh ą
δnq ď pmn ´ mη
nqe´n´η0mη0`2
n
log mn À e´mn log mn.
With mn “
n
log n, mn log mn ą n
2 for large enough n and it follows from Lemma
56 that
ΠX
ˆ
sup
xPX
8
ÿ
h“mn`1
πhpxq ą ǫ
˙
À Ope´n{2q.
(A.15)
Thus with Mn “ Opn1{2q,
mn
ÿ
h“1
Ppαh R Bh,nq À e´n.
(A.16)
(A.14), (A.15) and (A.16) together imply that ΠXpF c
nq À Ope´nq.
Also mη
nrp
n log
ˆ
Mn?
rn{δn
ǫ
˙p`1
“ opnq for the choice of the sequence rn. With
mn “ n{pC log nq for some large C ą 0, one can make
logpNpǫ, B1,n ˆ ¨ ¨ ¨ ˆ Bmn,n, }¨}8q ă nξ
(A.17)
for any ξ ą 0. Also from Lemma 55,
mn log NpΘan,hn,ln, ǫ, }¨}8q ď mn log
"
d1
ˆan
ln
˙p
` d2 log hn
ln
` 1
*
ă nξ
(A.18)
for any ξ ą 0. Combining (A.17) and (A.18), log NpFn, 4ǫ, }¨}1q ă nξ for any ξ ą
0.
205

A.6
Another useful lemma
Lemma 80. For non-negative r.v.s Ai, Bi, if PpAi ď uq ď CiPpBi ď uq for u P
p0, t0q, t0 ą 0, i “ 1, 2, PpA1 ` A2 ď t0q ď C1C2PpB1 ` B2 ď t0q.
Proof. Denote by f the corresponding density functions.
PpA1 ` A2 ď t0q “
ż t0
0
fA1puqPpA2 ď t0 ´ uq ď C2
ż t0
0
fA1puqPpB2 ď t0 ´ uq
“ C2PpA1 ` B2 ď t0q “ C2
ż t0
0
fB2puqPpA1 ď t0 ´ uq
ď C1C2
ż t0
0
fB2puqPpB1 ď t0 ´ uq “ C1C2PpB1 ` B2 ď t0q.
A.7
Proof of Theorem 57
Proof. Once again we approximate f0py | xq by ˜fpy | xq “
ş 1
σφ
`y´µ
σ
˘
d ˜Gxpµ, σq, so
that the ﬁrst term of 4.12 is arbitrarily small. We construct such an ˜f analogous to
that in Theorem 44. Lemma 81 is a variant of Lemma 46 which ensures that the
second term in (4.12) is also suﬃciently small. Before that we need a diﬀerent notion
of neighborhood of tFx, x P X u which we formulate below.
"
tGx, x P X u : sup
xPX
ˇˇˇˇ
ż
ℜˆℜ`
␣
gpµ, σqdGxpµ, σq ´ gpµ, σqdFxpµ, σq
(ˇˇˇˇ
ă ǫ
*
.
(A.19)
Lemma 81. Assume that f0 P Fd satisﬁes
ş
X
ş
Y y2f0py | xqdyqpxqdx ă 8. Suppose
˜fpy | xq “
ş 1
σφ
` y´µ
σ
˘
d ˜Gxpµ, σq, where D a ą 0 and 0 ă σ ă σ such that
˜Gx
`
r´a, as ˆ pσ, σq
˘
“ 1 @ x P X ,
(A.20)
206

so that ˜Gx has compact support for each x P X .
Then given any ǫ ą 0, D a
neighborhood W of t ˜Gx, x P X u which is a ﬁnite intersection of neighborhoods of the
type (A.19) such that for any conditional density fpy | xq “
ş 1
σφ
` y´µ
σ
˘
dGxpµ, σq, x P
X , with tGx, x P X u P W,
ż
X
ż
Y
f0py | xq log
˜fpy | xq
fpy | xqdyqpxqdx ă ǫ.
(A.21)
The proof of Lemma 81 is similar to that of Lemma 46 and is omitted here.
To characterize the support of PX, we deﬁne a collection of ﬁxed conditional prob-
ability measures tFx, x P X u on pℜˆ ℜ`, Bpℜˆ ℜ`qq denoted by G˚˚
X
satisfying
x ÞÑ
ş
ℜˆℜ` gpµ, σqdFxpµq is a continuous function of x for all bounded uniformly
continuous functions g : ℜˆ ℜ` Ñ r0, 1s.
Theorem 82. Assume the following holds.
T1. G0 is speciﬁed by µh „ GPpµ, cq, σh „ G0,σ where c is chosen so that GPp0, cq
has continuous path realizations and Πσ is absolutely continuous w.r.t. Lebesgue
measure on ℜ`.
T2. For every k ě 2, pπ1, . . . , πkq is absolutely continuous w.r.t. to the Lebesgue
measure on Sk´1.
T3. For any continuous function g : X ÞÑ ℜ,
PX
"
sup
xPX
|µhpxq ´ gpxq| ă ǫ
*
ą 0
h “ 1, . . . , 8 and for any ǫ ą 0.
Then for a bounded uniformly continuous function g : ℜˆ ℜ` : r0, 1s satisfying
207

gpµ, σq Ñ 0 as |µ| Ñ 8, σ Ñ 8,
PX
"
tGx, x P X u : sup
xPX
ˇˇˇˇ
ż
ℜˆℜ`
␣
gpµ, σqdGxpµ, σq ´ gpµ, σqdFxpµ, σq
(ˇˇˇˇ
ă ǫ
*
ą 0.
(A.22)
Proof. It suﬃces to assume that g is is coordinatewise monotonically increasing on
ℜˆ ℜ`. Let ǫ ą 0 be given and ψpxq “
ş
ℜˆℜ` gpµ, σqdFxpµ, σq. Let nǫ be such that
PXpΩ1q ą 0 where Ω1 “ tř8
h“nǫ`1 πh ă ǫu. Then in Ω1,
ˇˇˇˇ
ż
ℜˆℜ`
␣
gpµ, σqdGxpµ, σq ´ ψpxq
(ˇˇˇˇ ď
nǫ
ÿ
k“1
πk |gpµkpxq, σkq ´ ψpxq| ` ǫ.
Deﬁne Ω2
“
tsupxPX |gpµkpxq, σkq ´ ψpxq|
ă
ǫ, k
“
1, . . . , nǫu.
For a
ﬁxed σk,
there exists a δ such that supxPX |gpµkpxq, σkq ´ ψpxq|
ă
ǫ{2 if
supxPX
ˇˇµkpxq ´ g´1
σk ψpxq
ˇˇ ă δ where g´1
σk denotes the inverse of gp¨, σkq for ﬁxed
σk.
Hence there exists a neighborhood Bk of σk such that for σk P Bk and
supxPX
ˇˇµkpxq ´ g´1
σk ψpxq
ˇˇ ă δ, we have supxPX |gpµkpxq, σkq ´ ψpxq| ă ǫ. Since for
each k “ 1, . . . , nǫ, PX
␣
σk P Bk, supxPX
ˇˇµkpxq ´ g´1
σk ψpxq
ˇˇ ă δ
(
“
ż
σkPBk
PX
␣
sup
xPX
ˇˇµkpxq ´ g´1
σk ψpxq
ˇˇ ă δ
(
dG0,σpσkq ą 0,
PXpΩ2q ą 0. The conclusion of the theorem follows from the independence of Ω1
and Ω2.
˜f in (4.12) will be constructed so as to satisfy the assumptions of Lemma 81 and
such that
ş
X
ş
Y f0py | xq log f0py|xq
˜fpy|xq dyqpxqdx ă ǫ
2 for any ǫ ą 0. Deﬁne a sequence of
conditional densities fnpy | xq “
ş 1
σφp y´µ
σ qd ˜Gn,xpµ, σq, n ě 1 where for σn “ n´η,
dGn,xpµ, σq “ IµPr´n,nsf0pµ | xqδσnpσq
şn
´n f0pµ | xq
.
(A.23)
208

As before deﬁne the approximator
fnpy | xq “
şn
´n
1
σnφp y´t
σn qf0pt | xqdt
şn
´n f0pt | xqdt
.
(A.24)
˜f will be chosen to be fn0 for some large n0. fn0 satisﬁes the assumptions of Lemma
81 since tGn0,x, x P X u is compactly supported. Moreover tGn0,x, x P X u P G˚˚
X as
x Ñ
ş
ℜˆℜ` gpµ, σqdGn0,xpµ, σq is continuous function of x for all bounded uniformly
continuous function g. Hence there exists a ﬁnite intersection W of neighborhoods of
tGn0,x, x P X u the type (A.19) such that for any tGx, x P X u P W, the second term
of (4.12) is arbitrarily small. The conclusion of the theorem follows immediately
from a variant of Corollary 49 applied to neighborhoods of the type (A.19).
A.8
Proof of Theorem 58
Proof. As before we establish q-integrated L1 consistency of Gaussian mixtures
of ﬁxed-π dependent processes by verifying the conditions of Theorem 50.
Let
φµ,σpx, yq :“ 1
σφ
`y´µpxq
σ
˘
for y P Y and x P X . From Lemma 4.1 of Tokdar (2006a),
we obtain for σ2 ą σ1 ą σ2
2 and for each x P X ,
ż
Y
|φµ1,σ1px, yq ´ φµ2,σ2px, yq|dy ď
ˆ2
π
˙1{2}µ1 ´ µ2}8
σ2
` 3pσ2 ´ σ1q
σ1
Let µhpxq “ x1βh ` ηhpxq, h “ 1, 2, . . ., βh „ Gβ where Gβ is a probability distribu-
tion on ℜp. Let ηh „ GPp0, cq independently where cpx, x1q “ τ 2e´A}x´x1}2, where
A is a distributed with support ℜ` and τ 2 is ﬁxed. Assume that σh „ G0,σ where
G0,σ is a distribution on ℜ`. Here G0x is a distribution on ℜˆ ℜ` induced from the
distribution of pµhpxq, σ2
hq. For any pair µ1, µ2,
}µ1 ´ µ2}8 ď
ˆ2
π
˙1{2}β1 ´ β2} ?p ` }η1 ´ η2}8
σ2
.
209

As before, let Ha
1 denote a unit ball in the RKHS of the covariance kernel τ 2e´a}x´x1}2
and B1 is a unit ball in Cr0, 1sp.
For sequences Mn Ò 8, ln Ó 0, rn Ò 8 to be
determined later and given ǫ ą 0 construct Bn as
Bn “
ˆ
Mn
crn
δn
Hrn
1 ` ǫln
?π
4
?
2 B1
˙
Y
ˆ
Yaăδn MnHa
1 ` ǫln
?π
4
?
2 B1
˙
.
with δn “ K1ǫln
Mn
for some constant K1 ą 0. Let
Θn “
␣
φµ,σ : }β} ď an, η P Bn, ln ď σ ď hn
(
.
(A.25)
In the following Lemma, we provide an upper bound to NpΘn, ǫ, }¨}1q.
Lemma 83. There exists constants d1, d2, K2 and K3 ą 0 such that for Mn
a
rn{δn ą
2ǫ and for suﬃciently large rn
log NpΘn, ǫ, dSSq
ď K2rp
n
"
log
ˆ
8
?
2Mn?
rn{δn
ǫ?πln
˙*p`1
` log K3Mn
ǫln
`
log
"
d1
ˆ
an
ln
˙p
` d2 log hn
ln ` 1
*
.
Proof. We have Θn Ă
␣
φµ,σ | β P
`
´an?p, an?p
‰p , η P Bn, hn ď σ ď ln
(
. Let
κ ă minp ǫ
6, 1q and σm “ lnp1 ` κqm, m ě 0. Let m0 be the smallest integer such
that σm0 “ lnp1 ` κqm0 ą h. This implies m0 ď p1 ` κq´1 log hn
ln ` 1. By the choice
of σm, m ě 1, 3pσm´σm´1q
σm´1
ă
ǫ
2. Let Nj “ r
`128
π
˘1{2 anp?p
σj´1ǫ s. For each 1 ď j ď m0,
construct a ǫ?πσj´1
4
?
2
-covering tAkj, k “ 1, . . . , Mju of Bn with
Mj “ Npǫ?πσj´1
4
?
2
, Bn, }¨}8q ď exp
«
K2rp
n
"
log
ˆ8
?
2Mn
a
rn{δn
ǫ?πσj´1
˙*p`1
log K3Mn
ǫln
ﬀ
for some constants K2, K3 ą 0. For 1 ď i ď Nj, 1 ď k ď Mj & 1 ď j ď m0, deﬁne
Eikj “
ˆ
´a1
n ` 2a1
npi ´ 1q
Nj
, ´a1
n ` 2a1
ni
Nj
p
ˆ Akj ˆ pσj´1, σjs
(A.26)
210

where a1
n “ an?p. We have for pβ, η, σq, pβ1, η1, σ1q P Eikj and for each x P X ,
}φµ,σpx, ¨q ´ φµ1,σ1px, ¨q}1 ď
ˆ 2
π
˙1{2}β ´ β1} ?p ` }η1 ´ η2}8
σ2
` ǫ
2
ď
ˆ 2
π
˙1{2ˆ2anp?p
σj´1Nj
` ǫ?π
4
?
2
˙
` ǫ
2 ď ǫ.
Thus
NpΘn, ǫ, }¨}q ď
m0
ÿ
j“1
"ˆ128
π
˙1{2anp?p
σj´1ǫ ` 1
*p
ˆ
exp
#
K2rp
n log
ˆ8
?
2Mn
a
rn{δn
ǫ?πσj´1
˙p`1
log K3Mn
ǫln
+
ď exp
«
K2rp
n
"
log
ˆ8
?
2Mn
a
rn{δn
ǫ?πln
˙p`1*
log K3Mn
ǫln
ﬀ
ˆ
"
d1
ˆan
ln
˙p
` d2 log hn
ln
` 1
*
.
The rest of the proof follows similar to that of Theorem 51. Consider the sequence
of sieves deﬁned by
Fn “
"
f : fpy | xq “
8
ÿ
h“1
πh
1
σh
φ
ˆy ´ µhpxq
σh
˙
, tφµh,σhumn
h“1 P Θn,
sup
xPX
ÿ
hěmn`1
πh ď ǫ
*
.
We will show that given any ξ ą 0, there exists a c1, c2 ą 0 such that ΠX
`
F c
n
˘
ď
211

c1e´nc2 and logpδ, Fn, }¨}q ă nξ. For f1, f2 P Fn, we have
}f1p¨ | xq ´ f2p¨ | xq}1 ď
ż
Y
8
ÿ
h“1
ˇˇˇπp1q
h φµp1q
h ,σp1q
h px, yq ´ πp2q
h φµp2q
h ,σp2q
h px, yq
ˇˇˇ dy
ď
mn
ÿ
h“1
πp1q
h
ż
Y
ˇˇˇφµp1q
h ,σp1q
h px, yq ´ φµp2q
h ,σp2q
h px, yq
ˇˇˇ dy
`
mn
ÿ
h“1
ˇˇˇπp1q
h ´ πp2q
h
ˇˇˇ ` 2ǫ.
Let Θπ,n “ tπmn “ pπ1, π2, . . . , πmnq : νh, h “ 1, . . . , mn P r0, 1su. Fix πmn
1 , πmn
2
P
Θπ,n. It is easy to see that if we can make |νh,1 ´ νh,2| ă
ǫ
m2n, h “ 1, . . . , mn, we would
have řmn
h“1
ˇˇˇπp1q
h
´ πp2q
h
ˇˇˇ ă ǫ. Since νh,1, νh,2 P r0, 1s, the number of balls required to
cover Θπ,n so that řmn
h“1
ˇˇˇπp1q
h ´ πp2q
h
ˇˇˇ ă ǫ is K4pm2
n{ǫqmn for some constant K4 ą 0.
Hence
log NpFn, 4ǫ, }¨}q ď K2mnrp
n
"
log
ˆ8
?
2Mn
a
rn{δn
ǫ?πln
˙*p`1
` mn log K4m2
n
ǫ
mn log K3Mn
ǫln
` mn log
"
d1
ˆan
ln
˙p
` d2 log hn
ln
` 1
*
(A.27)
Note that ΠXpF c
nq ď mnPpΘc
nq ` Ppř8
h“mn πh ą ǫq and PpΘc
nq ď
␣
Pp}β} ą
anq ` Ppσ P rln, hnscq ` Ppη P Bc
nq
(
.
It follows from the proof of Theorem 3.1
of van der Vaart and van Zanten (2009) that
Ppη P Bc
nq ď PpA ą rnq ` e´M2
n{2
if M2
n ą rp
n
"
log
ˆ
8
?
2Mn?
rn{δn
ǫ?πln
˙
.
Since App1`η2q{η2 „ Gapa, bq, Lemma 4.9 of
van der Vaart and van Zanten (2009) indicates that PpA ą rnq À expt´rpp1`η2q{η2
n
u.
Hence with Mn “ Opn1{2q, mn “ Otn{plog nqp`1u1{p1`η2q and rp
n “ Otnη2{p1`η2qu,
212

PpΘc
nq À e´n and
Pp
8
ÿ
h“mn
πh ą ǫq À expt´m1`η2
n
plog mnqpp`1qu À e´n.
(A.28)
Also, the ﬁrst term in the right hand side of (A.27) can be made smaller than nξ
since mnrp
n “ Opn{plog nqp`1q. Also by F1, the last two terms of the right hand side
of (A.27) can be made to grow at opnq.
213

Appendix B
Proofs of some results in Chapter 3
B.1
Proof of Lemma 34
It follows from Chu (1973) that
f P Cm ô fpxq “
ż
σ´1φpσ´1xqgpσqdσ
for some density g on R`. Recall from Ongaro and Cattaneo (2004) that a collection
of random weights tπhu8
h“1 with ř8
h“1 πh “ 1 a.s. is said to have a full support if
for any m ě 1, pπ1, . . . , πmq admits a positive joint density with respect to Lebesgue
measure on the simplex tpp1, . . . , pmq : řm
i“1 pi ď 1u. Ongaro and Cattaneo (2004)
showed that if πh’s have a full support, the weak support of
P “
8
ÿ
h“1
πhδθh, θh „ G0
is the set of all probability measures whose support is contained the support of G0.
Since
pπ1, . . . , πmq
d“
ˆ
Φpα1q, Φpα2qt1´Φpα1qu, . . . , Φpαmq
m´1
ź
i“1
t1´Φpαiqu
˙
, αi „ Npµα, σ2
αq,
214

πh’s have a full support and hence the weak support of P “ ř8
h“1 πhδτh deﬁned
in (3.7) is all probability measures on R`. It follows that the weak support of the
induced prior Πu on Su, denoted by wkpΠuq, is precisely Cm.
B.2
Proof of Lemma 36
It follows from Tokdar (2006b) that if we can show that the weak support of Πs
contains all probability measures symmetric about zero and having compact support,
then f P ˜Ss ñ f P KLpΠsq. The argument given in Lemma 34 shows that the weak
support of the PSB prior in (3.4) is the set of all probability measures on R ˆ R`.
Now we will show that an arbitrary ˜P s is in a weak neighborhood of P s if ˜P is in a
weak neighborhood of P. We state a lemma to prove our claim.
Lemma 84. Let ˜Pn be a sequence of probability measures and ˜P be a ﬁxed probability
measure. Then p ˜Pn ñ ˜Pq ñ p ˜P s
n ñ ˜P sq, with ˜P s
n and ˜P s the symmetrised versions
of ˜Pn and ˜P, respectively, where the symmetrizing operation is as deﬁned in (3.9).
Proof. Assume ˜Pn ñ ˜P.
We have to show that for any bounded function φ on
R ˆ R`,
ż
φpt, τqd ˜P s
npt, τq Ñ
ż
φpt, τqd ˜P spt, τq as n Ñ 8.
Now,
ż
φpt, τqd ˜P s
npt, τq “ 1
2
ż
φpt, τqd ˜Pnpt, τq ` 1
2
ż
φpt, τqd ˜Pnp´t, τq
“
ż 1
2
␣
φpt, τq ` φp´t, τq
(
d ˜Pnpt, τq.
Since ψpt, τq “
1
2
␣
φpt, τq ` φp´t, τq
(
is also a bounded continuous function and
˜Pn ñ ˜P,
ż 1
2
␣
φpt, τq ` φp´t, τq
(
d ˜Pnpt, τq Ñ
ż 1
2
␣
φpt, τq ` φp´t, τq
(
d ˜Ppt, τq “
ż
φpt, τqd ˜P spt, τq
215

as n Ñ 8. This completes the proof of Lemma 84.
Lemma 84 in fact shows that the weak support of Πs contains all probability
measures symmetric about zero. With an appeal to Tokdar (2006b), f P ˜Ss ñ f P
KLpΠsq.
B.3
Proof of Theorem thm:ghoshal
In order to prove the theorem we need the following variant of Theorem 2.1 of
Amewou-Atisso et al. (2003) and Theorem 1 of Choi and Schervish (2007b) which
we state as Lemma 86. Existence of exponentially consistent tests is a typical tool
in showing strong consistency.
Deﬁnition 85. Let W Ă ˜Ss ˆF. A sequence of test functions Φn
`
tyi, xiun
i“1
˘
is said
to be exponentially consistent for testing
H0 : pf, ηq “ pf0, η0q against H1 : pf, ηq P Wn
if there exists constants C1, C2, C ą 0 such that
1. Eśn
i“1 f0ipΦnq ď C1e´nC,
2. infpf,ηqPWn Eśn
i“1 fηipΦnq ě 1 ´ C2e´nC.
Lemma 86. Let ˜Π “ pΠsˆπq be the prior on ˜SsˆF. Let Un be a sequence of subsets
of ˜Ss ˆ F. Suppose that there exists test functions tΦnu8
n“1, sets Θn Ă ˜Ss ˆ F, n ě 1
and constants C1, C2, c1, c2 ą 0 such that
1. ř8
n“1 Eśn
i“1 f0iΦn ă 8.
2. suppf,ηqPUcnXΘn Eśn
i“1 fηip1 ´ Φnq ď C1e´c1n.
3. ˜ΠpΘc
nq ď C2e´c2n.
216

4. For all δ ą 0 and for almost every data sequence tyi, xiu8
i“1,
˜Π
"
pf, ηq : Kipf, ηq ă δ @i,
8
ÿ
i“1
Vipf, ηq
i2
ă 8
*
ą 0.
Then ˜Πtpf, ηq P Uc
n | pY1, x1q, . . . , pYn, xnqu Ñ 0 a.s.rPf0,η0s.
In
this
case
Un
“
Wn
“
U ˆ Snpf0, ∆q @ n
ě
1.
As
in
van der Vaart and van Zanten (2009), we construct Θn “ F ˆ Θ1n where Θ1n “
YaărnMnHa
1 ` ǫB1 where H1 and B1 are unit ball of the RKHS of W a and unit ball
of the Banach space of Cr0, 1sp respectively, rn, Mn are increasing sequences to be
chosen later.
The nth test is constructed by combining a collection of tests one
for each of the ﬁnitely many elements of Θn. It follows from the proof of Theorem
3.1 in van der Vaart and van Zanten (2009) that under Assumption 1, there exists
constants d1, d2, K ą 0 such that
1. ˜ΠpΘc
nq ď expt´d1rp
n logqprnqu ` expt´M2
n{8u.
2. log Npǫ, Θ1n, || ¨ ||8q ď Krp
n
ˆ
log Mn
ǫ
˙p`1
.
Choosing Mn “ Opn1{2q, rp
n “ Opn{plog nqp`2q, we observe that
1. ˜ΠpΘc
nq ď expt´d2nu.
2. log Npǫ, Θ1n, || ¨ ||8q “ opnq.
for some constant d2 ą 0.
In order to verify 1 and 2 of Lemma 86, we will write Wn as a disjoint union of
two easily tractable regions. The particular form of Wn that is of interest to us is
W1n Y W2n, where for any ∆ą 0,
W1n “ Uc ˆ
␣
η : ||η ´ η||1,n ď ∆
(
W2n “
␣
pf, ηq : ||η ´ η||1,n ą ∆
*
.
217

We will establish the existence of a consistent sequence of tests for each of these
regions by considering the following variants of Proposition 3.1 and Proposition 3.3 of
Amewou-Atisso et al. (2003).
Proposition 87. There exists an exponentially consistent sequence of tests for
H0 : pf, ηq “ pf0, η0q against H1 : pf, ηq P W2n X Θn.
Proof. Let 0 ă t ă ∆{2 and assume Nt “ Npt, Θ1n, || ¨ ||8q.
Let η1, . . . , ηNt P
Θ1n be such that for each η P Θ1n there exists j such that ||η ´ ηj||8 ă t.
If
||η´η0||1,n ą ∆, ||ηj´η0||1,n ą ∆{2. It follows from Lemma 3.2 Amewou-Atisso et al.
(2003) that there exists a set Aj
i and a constant C ą 0 depending on f0 such that
αj
i :“ Pf0ipAj
iq ď 1
2 ´C|ηjpxiq´η0pxiq|. and γj
i :“ PfηjipAiq ě 1
2. If i ď n and i R Kn,
set Ai “ R, so that αj
i “ γj
i “ 1. Thus
lim inf
nÑ8
1
n
nÿ
i“1
pγj
i ´ αj
iq ě C∆{2
From Lemma 3.1 and Lemma 3.2 of Amewou-Atisso et al. (2003), it follows that there
exist test functions Φj
n based on tIAj
i, i “ 1, . . . , nu such that Eśn
i“1 f0iΦj
n ă e´nC1 and
Eśn
i“1 fηjip1 ´ Φj
nq ă e´nC2 for constants C1, C2 ą 0 Now deﬁne Φn “ max1ďjďNt Φj
n.
Then
Eśn
i“1 f0iΦn ď
Nt
ÿ
j“1
Eśn
i“1 f0iΦj
n ď
Nt
ÿ
j“1
e´nC1 ď Nte´nC1 ď e´nC3.
for some constant C3 ą 0. Clearly ř8
n“1 Eśn
i“1 f0iΦn ă 8.
Next we consider the type II error probability. The type II error probability of Φn
is no larger than the type II error probability of any of the tΦj
n, j “ 1, . . . , Ntu and
hence exponentially small.
Proposition 88. There exists an exponentially consistent sequence of tests for
H0 : pf, ηq “ pf0, η0q against H1 : pf, ηq P W1n
218

Proof. Without loss of generality take
U “
"
f :
ż
Φpyqfpyqdy ´
ż
Φpyqf0pyqdy ă ǫ
*
where 0 ď Φ ď 1 and Φ is Lipschitz continuous. Hence there exists M ą 0 such that
|Φpy1q ´ Φpy2q| ă M|y1 ´ y2|. Set ˜Φipyq “ Φty ´ η0pxiqu. Notice that Ef0i ˜Φi “ Ef0Φ.
Now
Efηi ˜Φi “
ż
˜Φipyqfηipyqdy “
ż
Φpyqfry ´ tηpxiq ´ η0pxiqus
ě
ż
Φry ´ tηpxiq ´ η0pxiqusfry ´ tηpxiq ´ η0pxiqusdy
´
ż ˇˇˇˇΦpyq ´ Φry ´ tηpxiq ´ η0pxiqus
ˇˇˇˇfry ´ tηpxiq ´ η0pxiqusdy
ě
ż
Φpyqfpyqdy ´ M|ηpxiq ´ η0pxiq|
ě Ef0Φ ` ǫ ´ M|ηpxiq ´ η0pxiq|
Hence 1{n řn
i“1 Efηi ˜Φi ě Ef0Φ ` ǫ ´ M∆for any f P Uc. Now choosing ∆ă ǫ{M
and applying Lemma 3.1 of Amewou-Atisso et al. (2003) we complete the proof.
It remains to verify the second suﬃcient condition of Theorem 39. Under the
assumptions, it follows from Lemma 36 that f0 P KLpΠsq.
We will present an
important lemma which is similar to Lemma 5.1 of Tokdar (2006b). It guarantees
that Kpf0, fθq and V pf0, fθq are continuous at θ “ 0. First we state and prove some
properties of the prior Πs described in (3.9) which will be used to prove the lemma.
Lemma 89. If Πs is the prior described in (3.9) and P0pt, τq “ Npt; µ0, σ2
0q ˆ
Gapτ; ατ, βτq, with ατ ą 0 and βτ ą 0. Then,
ż
τdP spt, τq ă 8 a.s.,
ż
t2dP spt, τq ă 8 a.s.,
ż
τt2dP spt, τq ă 8 a.s., ´8 ă
ż
plog τqdP spt, τq ă 8 a.s.
(B.1)
219

Proof.
ż ż
τą0,tPR
τdP spt, τqdP “
ż
τą0,tPR
τ
ż
dP spt, τqdP
“ 1
2
ż
τą0,tPR
τNpt; µ0, σ2
0qGapτ; ατ, βτqdtdτ ` 1
2
ż
τą0,tPR
τNpt; ´µ0, σ2
0qGapτ; ατ, βτqdtdτ
“
ż
τą0
τGapτ; ατ, βτqdτ ă 8.
The proofs of
ş
t2dP spt, τq ă 8 a.s. and
ş
τt2dP spt, τq ă 8 a.s. are similar. Since
ατ ą 0, choose an integer m large enough such that ατ ą 1
m.
ż ż
τą0,tPR
plog τqdP spt, τqdP “
ż
τą0
plog τqGapτ; ατ, βτqdτ
“ C
ż
τą0
plog τqτ ατ ´1e´βτ τdτ “ C
ż
τą0
pτ 1{m log τqτ ατ ´ 1
m´1e´βττdτ ą ´8
since
τ 1{m log τ
is
bounded
in
r0, 1s.
Also
ş
τą0plog τqτ ατ ´1e´βτ τdτ
ď
ş
τą0 ττ ατ ´1e´βτ τdτ ă 8.
Lemma 90. Under the conditions of the Theorem 39, if fp¨q “
ş
Np¨ ; t, τ ´1qdP spt, τq
and fθpyq “ fpy ´ θq, then
1. limθÑ0
ş
f0pyq log f0pyq
fθpyqdy “
ş
f0pyq log f0pyq
fpyq dy.
2. limθÑ0
ş
f0pyq
`
log`
f0pyq
fθpyq
˘2dy “
ş
f0pyq
`
log`
f0pyq
fpyq
˘2dy.
Proof. Clearly τφ
␣
τpy ´ θ ´ tqu Ñ τφ
␣
τpy ´ tq
(
as θ Ñ 0. Since
ş
τφ
␣
τpy ´ θ ´
tq
(
dP spt, τq ď
1
?
2π
ş
τdP spt, τq ă 8, so by DCT fθpyq Ñ fpyq as θ Ñ 0. Hence
log f0pyq
ftpyq Ñ log f0pyq
fpyq as t Ñ 0
ˆ
log`
f0pyq
ftpyq
˙2
Ñ
ˆ
log`
f0pyq
fpyq
˙2
as t Ñ 0.
220

To apply DCT again, we have to bound the function | log fθpyq|by an integrable
function.
| log fθpyq| ď log
?
2π `
ˇˇˇˇ log
ż
τe´ τ
2 py´t´θq2dP spt, τq
ˇˇˇˇ.
Let c “
ş
τdP spt, τq ă 8. Then
ˇˇˇˇ log
ż
τe´ τ
2 py´t´θq2dP spt, τq
ˇˇˇˇ ď | log c| `
ˇˇˇˇ log
ż τ
c e´ τ
2 py´t´θq2dP spt, τq
ˇˇˇˇ.
Now since
ş
τe´ τ
2 py´t´θq2dP spt, τq ď c,
ˇˇ log
ş τ
ce´ τ
2 py´t´θq2dP spt, τq
ˇˇ
“ ´ log
ş τ
ce´ τ
2 py´t´θq2dP spt, τq. Hence, by Jensen’s inequality applied to ´ log x, we
get,
´ log
ż τ
c e´ τ
2 py´t´θq2dP spt, τq ď log c ´
ż
plog τqdP spt, τq ` 1
2
ż
τpy ´ t ´ θq2dP spt, τq.
Now since θ Ñ 0, w.l.o.g assume |θ| ď 1. Hence
ż
τpy ´ t ´ θq2dP spt, τq ď 4
ˆ
y2
ż
τdP spt, τq `
ż
τt2dP spt, τq ` 1
˙
ñ | log fθpyq| ď log
?
2π ` | log c| ` log c ´
ż
plog τqdP spt, τq ` 2
ˆ
y2
ż
τdP spt, τq`
ż
τt2dP spt, τq ` 1
˙
which is clearly f0-integrable according to the assumptions of the lemma and from
the properties of Πs proved in Lemma 89. Similarly | log fθpyq|2 can be bounded
by an f0-integrable function. The conclusion of the lemma follows from a simple
application of DCT.
Lemma 36 together with the assumption (2) of the Theorem 39 guarantees Π
"
f :
Kpf0, fq ă δ, V pf0, fq ă 8
*
ą 0 for all δ ą 0. Since (B.1) holds, we may assume
ΠpUq ą 0, where U “
"
f : Kpf0, fq ă δ, V pf0, fq ă 8, pB.1q holds
*
.
(B.2)
221

Now for every fp¨q “
ş
Np¨ ; t, τ ´1qdP spt, τq P U, using Lemma 93, choose δf such
that for |θ| ă δf,
Kpf0, fθq ă 2Kpf0, fq, V pf, fθq ă 2V pf0, fq.
Now if ||η ´ η0|| ă δf, |ηpxiq ´ η0pxiq| ă δf, for i “ 1, . . . , n. So if f P U and
||η ´ η0|| ă δf, we have
Kipf, ηq “
ż
f0i log f0i
fηi
“
ż
f0 log
f0
fpη´η0qi
ă 2Kpf0, fq,
Vipf, ηq “
ż
f0i
`
log`
f0i
fηi
˘2 “
ż
f0
`
log`
f0
fpη´η0qi
˘2 ă 2V pf0, fq.
From (B.2) and Lemma 38 we have,
Π
"
pf, ηq : f P U, ||η ´ η0|| ă δf
*
ą 0.
Hence
Π
"
pf, ηq : Kipf, ηq ă 2δ @ i,
8
ÿ
i“1
Vipf, ηq
i2
ă 8
*
ą 0.
This ensures weak consistency of the posterior of the residual density and strong
consistency of the posterior of the regression function η.
222

Appendix C
Proofs of some results in Chapter 5
C.1
Proof of Lemma 63:
The Gaussian process prior ΠCn given tσj, j “ 0, . . . , 2nu has the following represen-
tation.
Cnptq “
2n
ÿ
j“0
cjBn
j ptq, cj „ N2p0, σ2
jI2q, ptq P r´π, πs.
(C.1)
Since cj „ N2p0, σ2
jI2q, ΠCn can be written as
Cnptq “
2n
ÿ
j“0
c˚
j σjBn
j ptq.
(C.2)
where c˚
j „ N2p0, I2q.
Hence from Proposition 1 in Pati and Dunson (2011), Hn
consists of h : r´π, πs Ñ R2 such that
hptq “
2n
ÿ
j“0
cjBn
j ptq,
(C.3)
where cj P R2. The RKHS norm of h in (C.3) is given by ||h||2
Hn “ ř2n
j“0 ||cj||2{σ2
j .
223

C.2
Proof of Theorem 64:
From Stepanets (1974) and observing that the basis functions tBn
j , j “ 0, . . . , 2nu
span the vector space of trigonometric polynomials of degree at most n, it follows that
given any Si
0 P Cαipr´π, πsq, there exists hipuq “ ř2n
j“0 ci
jBn
j puq, hi : r´π, πs Ñ R
with |ci
j| ď Mi, such that ||hi ´ Si
0||8 ď Kin´αi log n for some constants Mi, Ki ą
0, i “ 1, 2. Setting hpuq “ ř2n
j“0pc1
j, c2
jq1Bn
j puq, we have
||h ´ S0||8 ď Mn´αp1q log n
with ||h||2
H ď K ř2n
j“0 φj where M “ Mp2q, K “ Kp2q.
224

Appendix D
Proofs of some results in Chapter 6
D.1
Proofs of Lemma 71
The Gaussian process prior ΠSn,m given tφjk, j “ 0, . . . , 2n, k “ 0, . . . , 2mu has the
following representation.
Sn,mpu, vq “
2n
ÿ
j“0
2m
ÿ
k“0
djkBn
j puqBm
k pvq, djk „ N3p0, φ´1
jk I3q, pu, vq P T2.
(D.1)
To characterize the RKHS of Sn,mpu, vq, we need the following generalization of
Theorem 4.2 of van der Vaart and van Zanten (2008b) to the multivariate case.
Proposition 91. Let phiq be a sequence of elements in a separable Banach space B
such that ř8
i“1 wihi “ 0 for a sequence w P ℓ2pR3q, where the convergence is in B,
implying that w “ 0. Let Zi “ pZi1, Zi2, Zi3qT „ N3p0, I3q, and assume that the series
W “ ř8
i“1 Zihi converges almost surely in B3. Then the RKHS of W as a map in
B3 is given by H “ tř8
i“1 wihi : w P ℓ2pR3qu with squared norm || ř8
i“1 wihi||2
H “
ř8
i“1 ||wi||2.
Proof. The almost sure convergence of the series W “ ř8
i“1 Zihi P B3 implies al-
225

most sure convergence of the series b ˚ W for any b˚ P pB3q˚. Now any b˚ P pB3q˚
can be written as b˚ “ α1b˚
1 ` α2b˚
2 ` α3b˚
3 for αi P R, b˚
i P B˚.
Hence b˚W “
ř3
j“1 αj
ř8
i“1 Zijb˚
j phiq. Since the partial sums of the last series are zero mean Gaus-
sian, the series also converges in L2pΩ, U, Pq. Hence for b˚, b˚ P pB3q˚,
Eb˚Wb˚W “
3ÿ
j“1
αjαj
8
ÿ
i“1
b˚
j hib˚
j hi.
For w P ℓ2pR3q and natural numbers m ă n, by the Hahn-Banach theorem and the
Cauchy-Schwartz inequality, we have
||
ÿ
mďiďn
wihi||2
“
sup
||b˚||ď1
||
3ÿ
j“1
αj
8
ÿ
mďiďn
wijb˚
j phiq||2
ď
3 sup
||b˚||ď1
3ÿ
j“1
α2
j
ÿ
mďiďn
w2
ij
ÿ
mďiďn
b˚
j phiq2
ď
3p
3ÿ
j“1
ÿ
mďiďn
w2
ijq sup
||b˚||ď1
3ÿ
j“1
α2
j
ÿ
mďiďn
b˚
j phiq2
As m, n Ñ 0, the ﬁrst term on the far right converges to zero as w P ℓ2pR3q. By
the ﬁrst paragraph the second factor is bounded by sup||b˚||ď1 Epb˚Wq2 ď E||W||2.
Hence the partial sums of the series ř
i wihi form a Cauchy sequence in B3 and hence
it converges.
Because ř8
i“1pb˚
j hiq2 was seen to converge for each j
“ 1, 2, 3, it follows
that ř8
i“1pb˚
j hiqhi converge in B, and hence b˚ ř8
i“1pα1b˚
1hi, α2b˚
2hi, α3b˚
3hiqThi “
ř3
j“1 αjαj
ř8
i“1 b˚
j hib˚
j hi
“ Eb˚Wb˚W, for any b˚ P pB3q˚.
This shows that
Mb˚ “ ř8
i“1pα1b˚
1hi, α2b˚
2hi, α3b˚
3hiqThi and the RKHS is not bigger than this
space. Also ||Mb˚||2
H “ ř3
j“1 α2
j
ř8
i“1pb˚
j hiq2. Thus the RKHS consists of elements
ř8
i“1 wihi “ ř8
i“1 wihi where wi P ℓ2pR3q and || ř8
i“1 wihi||2
H “ ř8
i“1 ||wi||2.
226

The space would have been smaller than claimed if there existed w P ℓ2pR3q that
is not in the closure of the linear span of the elements pb˚hiq of ℓ2pR3q when b˚ ranges
over pB˚q3. Without loss of generality, we can take this w to be orthogonal to the
later collection, i.e., ř3
j“1 αj
ř
i wijb˚
j hi “ 0 for every b˚ P pB˚q3. This is equivalent
to ř
i wijhi “ 0 for j “ 1, 2, 3 which implies w “ 0.
Since djk „ N3p0, φ´1
jk I3q, ΠSn,m can be written as
Sn,mpu, vq “
2n
ÿ
j“0
2m
ÿ
k“0
b˚
jkφ´1{2
jk
Bn
j puqBm
k pvq.
(D.2)
where b˚
jk „ N3p0, I3q. Hence Hn,m consists of h : T2 Ñ R3 such that
hpu, vq “
2n
ÿ
j“0
2m
ÿ
k“0
cjkBn
j puqBm
k pvq,
(D.3)
where cjk
P
R3.
The RKHS norm of h in (D.3) is given by ||h||2
Hn,m
“
ř2n
j“0
ř2m
k“0 φjk||cjk||2.
D.2
Proof of Theorem 72
From Stepanets (1974) and observing that the basis functions tBn
j , j “ 0, . . . , 2nu
span the vector space of trigonometric polynomials of degree at most n, it follows that
given any Si
0 P CαipT2q, there exists hipu, vq “ ř2n
j“0
ř2m
k“0 ci
jkBn
j puqBm
k pvq, hi : T2 Ñ
R with |ci
jk| ď Mi, such that ||hi´Si
0||8 ď Kipn^mq´αi log n log m for some constants
Mi, Ki ą 0, i “ 1, 2, 3. Setting hpu, vq “ ř2n
j“0
ř2m
k“0pc1
jk, c2
jk, c3
jkqTBn
j puqBm
k pvq, we
have
||h ´ S0||8 ď Mpn ^ mq´αp1q log n log m
with ||h||2
H ď K ř2n
j“0
ř2m
k“0 φjk where M “ Mp3q, K “ Kp3q.
227

D.3
Proof of Theorem 74
. It is enough to verify the following along the lines of De Jonge and van Zanten
(2010). We will show that if S0 P SCpα1, α2, α3q there exists for every constant C ą 1
measurable subsets BN of CpT2; R3q such that for N large enough,
log Np¯ǫN, BN, || ¨ ||8q
ď
DN¯ǫ2
N
(D.4)
PpS R BNq
ď
e´CNǫ2
N
(D.5)
Pp sup
pu,vqPT2 ||Spu, vq ´ S0pu, vq|| ď ǫNq
ě
e´Nǫ2
N
(D.6)
with ǫN “ N´αp1q{p2αp1q`2q logt1 N and ¯ǫN “ N´αp1q{p2αp1q`2q logt2 N for some global
constants t1, t2 ą 0.
To ﬁnd an upper bound to the metric entropy of the unit ball of Hn,m, we embed
it in an appropriate space of functions for which the upper bound is known. The
function h is in fact well deﬁned on Ap1q “ tz P C2 : |Impzjq| ď 1, j “ 1, 2u, is
analytic on this set and takes real values in R2. By the Cauchy-Schwartz inequality,
it follows that with φ1,n,m “ mintφjk, j “ 0, . . . , 2n, k “ 0, . . . , 2mu,
|hpzq|2
ď ř2n
j“0
ř2m
k“0 ||cjk||2φjk
ř2n
j“0
ř2m
k“0p1{φjkqBn
j pz1q2Bm
k pz2q2,
ď ||h||2
Hn,mp1{φ1,n,mq.
(D.7)
for every z P Ap1q. Let Spφ, ψq denote the set of all analytic functions on Apψq,
uniformly bounded by φ´0¨5. (D.7) shows that Hn,m
1
Ă Spφ1,n,m, 1q.
Next we characterize the metric entropy of Spφ, ψq for any φ ą 0 in Proposition
92.
Proposition 92. There exist ǫ0, φ0 ą 0 such that
logpǫ, Spφ, ψq, || ¨ ||8q ď K1
1
ψ2
ˆ
log K2
φ0¨5ǫ
˙3
(D.8)
for φ P p0, φ0q and ǫ P p0, ǫ0q.
228

Proof. The proof proceeds similarly to van der Vaart and van Zanten (2009). How-
ever, extra care is needed to identify the role of φ and ψ. Let M “ φ´0¨5. h is an
analytic function h : C2 Ñ C, |hpzq| ď M for all z P Ω“ tz P C2 : |Repz1q| ď
ψ, |Repz2q| ď ψu and hence admits a Taylor series expansion on Ω.
Let tt1, . . . , tmu be an ψ{2-net of T2 for sup norm, let T2 “ Ym
i“1Bi be a partition of
T2 into sets B1, . . . , Bm obtained by assigning every t P T to a closest ti P tt1, . . . , tmu.
Consider P “ řm
i“1 Pi,aiIBi for Pi,aiptq “ ř
n.ďk ai,npt ´ tiqn where the sum ranges
over n “ pn1, n2q P pN Y t0uq2 with n. “ n1 ` n2 ď k and xn is deﬁned as xn1
1 xn2
2 .
Obtain a ﬁnite set of functions by discretizing ai,n for each i and n over a mesh of
ǫ{ψn.-net of the interval r´M{ψn., M{ψn.s. Then
log
ˆ ź
i
ź
n:n.ďk
#ai,n
˙
ď t3{pψ{2qu2k2 log
ˆ2M
ǫ
˙
.
By the Cauchy formula (2 applications of the formula in one dimension suﬃce),
for C1, C2 circles of radius ψ in the complex plane around the coordinates ti1, ti2 of
ti, and with Dn the partial derivative of orders n “ pn1, n2q and n! “ n1!n2!,
ˇˇˇˇ
Dnhptiq
n!
ˇˇˇˇ “
ˇˇˇˇ
1
p2πiq2
¿
C1
¿
C2
hpzq
pz ´ tiqn`1dz1dz2
ˇˇˇˇ ď M
ψn..
Consequently for any z P Bi, a universal constant K, an appropriately chosen ai and
for k ą log KM
ǫ ,
ˇˇˇˇ
ÿ
n.ąk
Dnhptiq
n!
pz ´ tiqn
ˇˇˇˇ ď
ÿ
n.ąk
M
ψn.pψ{2qn. ď M
8
ÿ
l“k`1
l
2l ď KM
ˆ2
3
˙k
ď ǫ,
ˇˇˇˇ
ÿ
n.ďk
Dnhptiq
n!
pz ´ tiqn ´ Pi,aipzq
ˇˇˇˇ ď
ÿ
n.ąk
ǫ
ψn.pψ{2qn. ď ǫ
kÿ
l“1
l
2l ď Kǫ.
Hence logpǫ, SpM, ψq, || ¨ ||8q ď K1
1
ψ2
`
log K2M
ǫ
˘3.
229

We return to verifying (D.4), (D.5) and (D.6). First we will verify (D.6). By
Lemma 5.3 of van der Vaart and van Zanten (2008b), we have for S0 P SCpα1, α2, α3q,
the inequality
´ log Pp||S ´ S0||8 ă ǫq ď ψn,m
S0 pǫq,
with ψn,m
S0
the so-called concentration function, deﬁned as follows:
ψn,m
S0 pǫq “
inf
hPHn,m:||h´S0||8ăǫ ||h||2
Hn,m ´ log Pp||Sn,m|| ă ǫq.
We can provide a lower bound to ´ log Pp||Sn,m|| ă ǫq using Proposition 92. Observe
that
Pp||S ´ S0|| ď ǫNq “
8
ÿ
n,m“1
Πn,m
ż
Pp||Sn,m ´ S0|| ď ǫNq
2n,2m
ź
j,k“0
ppφjkqdφjk.
From Theorem 72 we obtain Pp||S ´ S0|| ď ǫNq ě
8
ÿ
n,měp1{ǫNq1{αp1q
Πn,m exp
"
´ Mp2n ` 1qp2m ` 1qαmαn
β2
` K1
ˆ
log K3
ǫN
˙3*
for some constant K3 ą 0.
Next we will verify (D.5). Deﬁne RN to be the region tφjk ě tN, j “ 0, . . . , n, k “
0, . . . , m; n, m “ 1, . . . , rNu.
Let B1 denote the unit ball in the Banach space
CpT2; R3q. Deﬁne
BN “ LNSptN, 1q ` ǫNB1.
230

Then by Borel’s inequality (van der Vaart and van Zanten, 2008b)
PpS R BNq
“
8
ÿ
n“1
8
ÿ
m“1
Πn,m
ż
PpSn,m R BNq
2n,2m
ź
j,k“0
ppφjkqdφjk
ď
rN
ÿ
n“1
rN
ÿ
m“1
Πn,m
ż
PpSn,m R LNHn,m
1
` ǫNB1q
2n,2m
ź
j,k“0
ppφjkqdφjk
`
Ppn ą rN, m ą rNq
ď
rN
ÿ
n“1
rN
ÿ
m“1
Πn,m
ż
RN
PpSn,m R LNHn,m ` ǫNB1q
2n,2m
ź
j,k“0
ppφjkqdφjk
`
Ppφ1,rN,rN ď tNq ` Ppn ą rN, m ą rNq.
From van der Vaart and van Zanten (2009), the ﬁrst term on the right hand side
of the previous inequality is bounded as follows.
PpSn,m R LNHn,m ` ǫNB1q ď 1 ´ ΦrΦ´1tPp||Sn,m||8 ď ǫNqu ` LNs.
For ǫN small enough and since Φ´1pyq ě ´tp5{2q logp1{yqu0¨5 for y P p0, 0¨5q, it
follows that
PpSn,m R LNHn,m ` ǫNB1q ď 1 ´ Φ
„
LN ´
"
p5{2qK1
ˆ
log
K2
t0¨5
N ǫN
˙3*0¨5
.
for LN ě
"
p5{2qK1
ˆ
log
K2
t0¨5
N ǫN
˙3*0¨5
and for tφjk, j “ 0, . . . , 2rN, k “ 0, . . . , 2rNu in
RN.
Let τN “ mintτi, 0 ď i ď 2rNu, ˜τN „ GatαrN, p2rN ` 1qβu and κN “ mintκi, 0 ď
i ď 2rNu, ˜κN „ GatαrN, p2rN ` 1qβu, ˜τN and ˜κN are independent.
Observe that
Ppφ1,rN,rN ď tNq
ď
PpτNκN ď tNq ď Pp˜τN ˜κN ď tNq
ď
ż e´1
0
Pp˜τN ď tN{yqf˜κNpyqdy ` Pp˜τN ď etNq
231

Now Pp˜τN ď etNq À exprαrN ` αrN logtp2rN ` 1qtNβu ´ αrN log αrNs and f˜κNpyq À
expp´αrNq for y P p0, e´1q. Thus
Ppφ1,rN,rN ď tNq À exprαrN ` αrN logtp2rN ` 1qtNβu ´ αrN log αrNs ` expp´αrNq.
Finally we will verify (D.4). For ¯ǫN ě ǫN,
Np2¯ǫN, BN, || ¨ ||8q
ď
Np¯ǫN{LN, SptN, 1q, || ¨ ||8q
ď
K1
ˆ
log
K2
t0¨5
N ¯ǫN{LN
˙3
Letting αN
“
Oplog Nq3, rN
“
O
„
exp
"
N
2
3p2αp1q`2q
*
, tN
“
O
„
exp
"
´
N
2
3p2αp1q`2q
*
such that p2rN ` 1qtNβ is a global constant, LN “ N2{p2αp1q`2q, we can
verify that (D.4), (D.5) and (D.6) are satisﬁed with ǫN “ N´αp1q{p2αp1q`2q logt1 N and
¯ǫN “ N´αp1q{p2αp1q`2q logt2 N for some global constants t1, t2 ą 0. Ppn ą rN, m ą rNq
is guaranteed to be O
„
exp
"
´N
2
2αp1q`2
*
from the tail condition in the assumption.
232

Appendix E
Proofs of some results in Chapter 7
E.1
Proof of Theorem 77
Let φ “ pξr, ηr, βξ, βη, a, σq and φ0 “ pξ0r, η0r, βξ0, βη0, a0, σ0q be a ﬁxed set of param-
eters in CpDq ˆ CpDq ˆ R ˆ R`. Clearly pyi, siq „ fpy, s | φq, where fpy, s | φq “
fpy | s, φqpps | φq “
1
?
2πσ2 exp
„
´ ty ´ µpsqu2
2σ2

exptxpsqTβξ ` ξrpsqu
ş
D exptxpsqTβξ ` ξrpsquds.
Here µpsq “ xpsqTpaβξ`βηq`aξrpsq`ηrpsq. Let µ0psq “ xpsqTpa0βξ0`βη0q`a0ξ0rpsq`
η0rpsq. Deﬁne Λpφ0, φq “ log
␣
fpy, s | φ0q{fpy, s | φqu and Kpφ0, φq “ Eφ0tΛpφ0, φqu.
Then following Schwartz (1965b), its enough to show that for all ǫ ą 0,
pΠξr ˆ Πηr ˆ πβξ ˆ πβη ˆ πσ ˆ πaq
␣
φ : Kpφ0, φq ă ǫ
(
ą 0.
233

We calculate Kpφ0, φq below.
Kpφ0, φq
“
Eφ0tΛpφ0, φqu “ Eφ0
"
log fpy, s | φ0q
fpy, s | φq
*
“
1
2 log σ2
σ2
0
` Eφ0
„
´ty ´ µ0psqu2
2σ2
0

´ Eφ0
„
´ty ´ µpsqu2
2σ2

´
Eφ0
␣
xpsq
Tpβξ ´ βξ0q ` ξrpsq ´ ξ0rpsq
(
` log
„ ş
D exp
␣
xpsqTβξ ` ξrpsq
(
ds
ş
D exp
␣
xpsqTβξ0 ` ξ0rpsq
(
ds

“
1
2 log σ2
σ2
0
´ 1
2
ˆ
1 ´ σ2
0
σ2
˙
`
1
2σ2
ż
D
tµ0psq ´ µpsqu2ppsqds
`
ż
D
txpsq
Tpβξ ´ βξ0q ` ξrpsq ´ ξ0rpsquppsqds
` log
„ ş
D exp
␣
xpsqTβξ ` ξrpsq
(
ds
ş
D exp
␣
xpsqTβξ0 ` ξ0rpsq
(
ds

.
For each δ ą 0, deﬁne Bδ “
␣
φ : ||ξr ´ ξ0r||8 ă δ, ||ηr ´ η0r||8 ă δ, ||βξ ´ βf0|| ă δ, ||βg ´ βg0|| ă δ, |a ´ a0| ă δ,
|σ{σ0 ´ 1| ă δ
(
.
Take b1 “ ||µ0´µ||8 and b2 “ σ{σ0. Let g1pb1, b2q “ log b2´pb2
2´1q{p2b2
2q`b2
1{p2σ2
0b2
2q.
Clearly g1pb1, b2q is continuous at b1 “ 0 and b2 “ 1 and g1p0, 1q “ 0. We have
b1 ď M||paβξ ` βηq ´ pa0βξ0 ` βη0q|| ` ||taξrpsq ` ηrpsqu ´ ta0ξ0rpsq ` η0rpsqu||
and
Kpφ0, φq
ď
g1pb1, b2q `
ż
D
␣
xpsq
Tpβξ ´ βξ0q ` ξrpsq ´ ξ0rpsq
(
ppsqds
`
log
„ ş
D exp
␣
xpsqTβξ ` ξrpsq
(
ds
ş
D exp
␣
xpsqTβξ0 ` ξ0rpsq
(
ds

.
234

For ǫ ą 0, there exists a δ1 ą 0 such that for all φ P Bδ1,
1
2 log σ2
σ2
0
´ 1
2
ˆ
1 ´ σ2
0
σ2
˙
`
1
2σ2
ż
D
tµ0psq ´ µpsqu2ppsqds ă ǫ
3.
Also there exists δ2 ą 0 such that for all φ P Bδ2,
␣
xpsqTpβξ ´ βξ0q ` ξrpsq ´ ξ0rpsq
(
ă
ǫ{3 uniformly for all s P D.
If we deﬁne hφpsq “ exp
␣
xpsqTβξ ` ξrpsq
(
, then
φ ÞÑ
ş
D hφpsqds is a continuous function and hence φ ÞÑ log
␣ş
D hφpsqds
(
is also
a continuous function. So there exists a δ3 ą 0 such that
φ P Bδ3 ñ log
"ż
D
hφpsqds
*
´ log
"ż
D
hφ0psqds
*
ă ǫ
3.
Choosing δ “ mintδ1, δ2, δ3u, φ P Bδ implies Kpφ0, φq ă ǫ. From Choi (2005), it
follows that with the priors speciﬁed in §7.3.1
pΠξr ˆ Πηr ˆ πβξ ˆ πβη ˆ πσ ˆ πaqpBδq ą 0.
Hence,
pΠξr ˆ Πηr ˆ πβξ ˆ πβη ˆ πσ ˆ πaq
␣
φ : Kpφ0, φq ă ǫ
(
ą 0.
E.2
Proof of Theorem 78
The prior speciﬁcations on ρξ, ρη, τξ and τη enable one to bound any quadratic forms
and determinants involving Σn
ξ and Σn
η by ﬁxed quantities. Hence, in showing that
the posterior ppa | y, sq is proper, its enough to treat ρξ, ρη, τξ and τη as constants.
Without loss of generality we can work with D “ r0, 1s2 by the projection argument
described in §7.6. Following Benes et al. (2003), we consider the grid approximation
of the inﬁnite dimensional Gaussian process tξrpsq : s P Du, denoted by ξr. Let
D “ ŤJ
j“1 Ij, with tIju denoting a segmentation of D into contiguous regions of
235

equal area ∆“ J´1 ş
D ds. Choose J suﬃciently large such that at most one si lies
within any Ij. The inﬁnite-dimensional Gaussian process, ξr, can be approximated
by a ﬁnite dimensional vector ξJ
r “ pξ˚1
r , . . . , ξ˚J
r qT, corresponding to the choice of
arbitrary points s˚
1, . . . , s˚
J within I1, . . . , IJ, respectively such that ξrpsiq “ ξ˚j
r
if
si P Ij.
Thus ξJ
r „ Np0, Σ˚J
ξ q, where pΣ˚J
ξ qij “ cp||s˚
i ´ s˚
j || | ψq.
Deﬁne the
true posterior ptruepξn
r | sq and the approximated posterior pJpξn
r | sq as follows.
ptruepξn
r | sq9ptruepξn
r , sq “
E
" ż
exp
␣
xpsq
Tβξ ` ξrpsq
(
ds | ξn
r
*´n
exp
␣
´ 0¨5
`
ξn
r
˘T`
Σn
ξ
˘´1`
ξn
r
˘T(
and pJpξn
r | sq9pJpξn
r , sq “
„
∆
Jÿ
j“1
exp
␣
xps˚
j q
Tβξ ` ξrps˚
j q
(´n
exp
␣
´ 0¨5
`
ξJ
r
˘T`
Σ˚J
ξ
˘´1`
ξJ
r
˘T(
.
Marginalizing out ηn
r , we have y | s, ξr, a, σ2, βη, βξ „ NpXβ˚`aξn
r , σ2In`Σn
ηq, where
X T “ txps1q ¨ ¨ ¨xpsnqu. The true posterior of (ξn
r , a, σ2, βξ, βη) is
ptruepξn
r , a, βξ, βη, σ2 | y, sq9ppy | s, ξr, a, σ2, βξ, βηqptruepξn
r , sqπpσ2qπpβξqπpβηq.
Benes et al. (2003) showed that, under these assumptions, for a ﬁxed s P Dn, the
expectation of any bounded function with respect to pJpξn
r | sq converges to the corre-
sponding expectation with respect to ptruepξn
r | sq as J tends to inﬁnity. Hence there
exists a J such that the expectation of the bounded function with respect to pJpξn
r | sq
is greater than the corresponding expectation with respect to p1{2qptruepξn
r | sq.
Thus, in order to show propriety of the true posterior of (ξn
r , a, σ2, βξ, βg), which
involves ptruepξn
r | sq, its enough to show the propriety of the approximated pos-
terior pJpξn
r , a, βξ, βη, σ2 | y, sq. The approximated posterior of (ξn
r , a, σ2, βξ, βη) is
pJpξn
r , a, βξ, βη, σ2 | y, sq “
C exp
␣
´ 0¨5
`
Y ´ Xβ˚ ´ aξn
r
˘T`
σ2In ` Σn
η
˘´1`
Y ´ Xβ˚ ´ aξn
r
˘(
ˆ
236

exp
␣
´ 0¨5
`
ξJ
r
˘T`
Σ˚J
ξ
˘´1`
ξJ
r
˘(
πpβξqπpβηq ˆ πpσ2q
exp
␣řn
i“1 xpsiqTβξ ` ξrpsiq
(
∆n“ řJ
j“1 exp
␣
xps˚
j qTβξ ` ξ˚j
r
(‰n,
where C is a constant. Since exp
␣
xpsiqTβξ ` ξrpsiq
(
ă řJ
j“1 exp
␣
xps˚
j qTβξ ` ξ˚j
r
(
for
all i “ 1, . . . , n,
exp
␣řn
i“1 xpsiqTβξ ` ξrpsiq
(
“ řJ
j“1 exp
␣
xps˚
j qTβξ ` ξ˚j
r
(‰n ă 1.
After integrating out ξJ
r excluding ξn
r we are left with ppξn
r , a, βξ, βη, σ2 | Y, sq ď
C1 exp
␣
´ 0¨5
`
Y ´ Xβ˚ ´ aξn
r
˘T`
σ2In ` Σn
η
˘´1`
Y ´ Xβ˚ ´ aξn
r
˘(
ˆ
exp
␣
´ 0¨5
`
ξn
r
˘T`
Σn
f
˘´1`
ξn
r
˘(
πpβξqπpβηqπpσ2q,
where C1 ą 0 is a constant and Σn
ξ is the variance-covariance matrix of ξn
r constructed
out of Σ˚J
ξ . Setting Z “
`
y´Xβ˚˘
{a, Σ “
`
Σn
η`σ2In
˘
{a2 and Ωη “
␣
pΣn
ξ q´1`Σ´1(´1
and completing quadratic forms yield
ppξn
r , a, βξ, βη, σ2 | y, sq ď C2 exp
␣
´ 0¨5
`
ξn
r ´ ΩηΣ´1Z
˘TΩ´1
η
`
ξn
r ´ ΩηΣ´1Z
˘(
ˆ
exp
␣
´ 0¨5
`
Z
TΣ´1Z ´ Z
TΣ´1ΩηΣ´1Z
˘(
πpβξqπpβηqπpσ2q,
where C2 ą 0 is another constant. Next we state a useful lemma from matrix algebra.
Lemma 93. If A and B are positive deﬁnite square matrices so is A´ApA`Bq´1A.
Proof. We have
A ´ ApA ` Bq´1A “ ApA ` Bq´1B “ tB´1pA ` BqA´1u´1 “ pB´1 ` A´1q´1.
The conclusion follows from the fact that the sum and inverses of positive deﬁnite
matrices of the same dimension are also positive deﬁnite.
237

From Lemma 93, we have
`
ZTΣ´1Z ´ ZTΣ´1ΩηΣ´1Z
˘
ě 0, so that
ppξn
r , a, βξ, βη, σ2 | y, sq ď C2 exp
␣
´ 0¨5
`
ξn
r ´ ΩηΣ´1Z
˘TΩ´1
η
`
ξn
r ´ ΩηΣ´1Z
˘(
ˆ
πpβξqπpβηqπpσ2q.
Integrating out ξn
r ﬁrst and then βξ and βη,
ppa, σ2 | y, sq ď C3
ˇˇ`
Σn
ξ
˘´1 ` a2`
Σn
η ` σ2In
˘´1ˇˇ´p1{2q.
Call Σn
ξ “ A and Σn
η “ B. Hence
ˇˇA´1 ` a2pB ` σ2Iq´1ˇˇ “ |I ` a2ApB ` σ2Iq´1|
|A|
“ |a2A ` σ2I ` B|
|σ2I ` B||A|
.
Now we state a useful result from matrix algebra.
Proposition 94. If A and B and non-negative deﬁnite matrices, then
ˇˇA ` B
ˇˇ ě
ˇˇA
ˇˇ `
ˇˇB
ˇˇ with strict inequality holding in case of positive deﬁnite matrices.
Using Proposition 94, we get
ˆ|a2A ` σ2I ` B|
|σ2I ` B|
˙´p1{2q
ď
ˆ|a2A| ` |σ2I ` B|
|σ2I ` B|
˙´p1{2q
“
"
1 `
a2n|A|
śn
i“1pσ2 ` biq
*´p1{2q
ď
"
1 `
a2n|A|
pσ2 ` bnqn
*´p1{2q
,
where 0 ă b1 ď b2 ď ¨ ¨ ¨ ď bn are the eigen values of B. By Minkowski’s inequality
we get
"
1 `
a2n|A|
pσ2 ` bnqn
*´p1{2q
ď
pσ2 ` bnqn{2
cn pa2|A|p1{nq ` σ2 ` bnqn{2.
238

Set |A|1{n “ k1 and bn “ k2. We assume n ě 2. Then ignoring constants
ż 8
´8
pσ2 ` bnqn{2
`
a2|A|1{n ` σ2 ` bn
˘n{2da
“
ż 8
´8
1
␣
1 ` pa2k1q{pσ2 ` k2q
(n{2da
ď
ż 8
´8
1
␣
1 ` pa2k1q{pσ2 ` k2q
(da “ π
ˆσ2 ` k2
k1
˙p1{2q
.
Now since Eπpσq ă 8,
ż 8
0
ˆσ2 ` k2
k1
˙p1{2q
πpdσ2q ă 8.
By Fubini’s Theorem, ppa | Y, sq is integrable.
239

Bibliography
Adler, R. (1990), An introduction to continuity, extrema, and related topics for gen-
eral Gaussian processes, vol. 12, Institute of Mathematical Statistics.
Albert, J. and Chib, S. (2001), “Sequential ordinal modeling with applications to
survival data,” Biometrics, 57, 829–836.
Amenta, N., Bern, M., and Kamvysselis, M. (1998), “A new Voronoi-based surface
reconstruction algorithm,” in Proceedings of the 25th annual conference on Com-
puter graphics and interactive techniques, pp. 415–421, ACM.
Amewou-Atisso, M., Ghoshal, S., Ghosh, J. K., and Ramamoorthi, R. V. (2003),
“Posterior consistency for semi-parametric regression problems,” Bernoulli, 9, 291–
312.
Arellano-Vallea, R. B., Galea-Rojasb, M., and Zuazola, P. I. (2000), “Bayesian sensi-
tivity analysis in elliptical linear regression models,” Journal of Statistical Planning
and Inference, 86, 175–199.
Aziz, N., Bata, R., and Bhat, S. (2002), “Bezier surface/surface intersection,” Com-
puter Graphics and Applications, IEEE, 10, 50–58.
Barnhill, R. (1985), “Surfaces in computer aided geometric design: A survey with
new results,” Computer Aided Geometric Design, 2, 1–17.
Barrientos, F., Jara, A., and Quintana, F. (2011), “On the support of MacEacherns
dependent Dirichlet processes,” Unpublished manuscript, University of Chile.
Barron, A., Birg´e, L., and Massart, P. (1999a), “Risk bounds for model selection via
penalization,” Probability theory and related ﬁelds, 113, 301–413.
Barron, A., Schervish, M., and Wasserman, L. (1999b), “The consistency of posterior
distributions in nonparametric problems,” The Annals of Statistics, 27, 536–561.
Belitser, E. and Ghosal, S. (2003), “Adaptive Bayesian inference on the mean of an
inﬁnite-dimensional normal distribution,” The Annals of Statistics, 31, 536–559.
240

Benes, V., Bodl´ak, K., Møller, J., and Waagepetersen, R. P. (2003), “Bayesian anal-
ysis of log Gaussian Cox process models for disease mapping,” in The ISI In-
ternational Conference on Environmental Statistics and Health, Univ Santiago de
Compostela, pp. 95–105.
Bhattacharya, A. and Dunson, D. (2010), “Strong consistency of nonparametric
Bayes density estimation on complex Watson kernels,” Duke University, DSS dis-
cussion series.
Bhattacharya, A. and Dunson, D. (2011), “Posterior rates of contraction in proba-
bility tensor decomposed latent variable models,” (in progress).
Birg´e, L. (1986), “On estimating a density using Hellinger distance and some other
strange facts,” Probability theory and related ﬁelds, 71, 271–291.
Birg´e, L. (2001), “An alternative point of view on Lepski’s method,” Lecture Notes-
Monograph Series, pp. 113–133.
Boissonnat, J. and Oudot, S. (2005), “Provably good sampling and meshing of sur-
faces,” Graphical Models, 67, 405–451.
Borell, C. (1975), “The Brunn-Minkowski inequality in gauss space,” Inventiones
Mathematicae, 30, 207–216.
Brechb¨uhler, C., Gerig, G., and K¨ubler, O. (1995), “Parametrization of closed sur-
faces for 3-D shape description,” Computer vision and image understanding, 61,
154–170.
Burr, D. and Doss, H. (2005), “A Bayesian Semiparametric Model for Random-
Eﬀects Meta-Analysis,” Journal of the American Statistical Association, 100, 242–
251.
Bush, C. and MacEachern, S. (1996), “A semiparametric Bayesian model for ran-
domised block designs,” Biometrika, 83, 275.
Canale, A. and Dunson, D. (2011), “Bayesian multivariate mixed scale density esti-
mation,” Arxiv preprint arXiv:1110.1265.
Casale, M. (1987), “Free-form solid modeling with trimmed surface patches,” IEEE
Computer Graphics and Applications, pp. 33–43.
Castillo, I. (2008), “Lower bounds for posterior rates with Gaussian process priors,”
Electronic Journal of Statistics, 2, 1281–1299.
Chan, D., Kohn, R., Nott, D., and Kirby, C. (2006), “Locally adaptive semiparamet-
ric estimation of the mean and variance functions in regression models,” Journal
of Computational and Graphical Statistics, 15, 915–936.
241

Chib, S. and Greenberg, E. (2010), “Additive cubic spline regression with Dirichlet
process mixture errors,” Journal of Econometrics, 156, 322–336.
Chipman, H. A., George, E. I., and Mcculloch, R. E. (2010), “BART: Bayesian
additive regression trees,” The Annals of Applied Statistics, 4, 266–298.
Choi, T. (2005), “Posterior Consistency in Nonparametric Regression problems in
Gaussian Process Priors,” Ph.D. thesis, Department of Statistics, Carnegie Mellon
University.
Choi, T. (2007), “Alternative posterior consistency results in nonparametric binary
regression using Gaussian process priors,” Journal of Statistical Planning and In-
ference, 137, 2975–2983.
Choi, T. (2009), “Asymptotic properties of posterior distributions in nonparametric
regression with non-Gaussian errors,” Annals of the Institute of Statistical Math-
ematics, 61, 835–859.
Choi, T. and Schervish, M. (2007a), “On posterior consistency in nonparametric
regression problems,” Journal of Multivariate Analysis, 98, 1969–1987.
Choi, T. and Schervish, M. (2007b), “On posterior consistency in nonparametric
regression problems,” Journal of Multivariate Analysis, 98, 1969–1987.
Chu, K. C. (1973), “Estimation and detection in linear systems with elliptical errors.”
IEEE Trans. Auto. Control, 18, 499–505.
Chung, M., Dalton, K., and Davidson, R. (2008), “Tensor-based cortical surface
morphometry via weighted spherical harmonic representation,” IEEE Transactions
on Medical Imaging, 27, 1143–1151.
Chung, Y. and Dunson, D. (2009), “Nonparametric Bayes conditional distribution
modeling with variable selection,” Journal of the American Statistical Association,
104, 1646–1660.
Cinquin, P., Chalmond, B., and Berard, D. (1982), “Hip prosthesis design,” Lecture
Notes in Medical Informatics, 16, 195–200.
Cram´er, H. and Leadbetter, M. R. (1967), Stationary and related stochastic processes,
sample function properties and their applications, John Wiley & Sons, New York.
Cunningham, G., Lehovich, A., and Hanson, K. (1999), “Bayesian estimation of
regularization parameters for deformable surface models (Proceedings Paper),” .
De Iorio, M., Mueller, P., Rosner, G., and MacEachern, S. (2004), “An ANOVA
model for dependent random measures,” Journal of the American Statistical As-
sociation, 99, 205–215.
242

De Iorio, M., Johnson, W., M¨uller, P., and Rosner, G. (2009), “Bayesian nonpara-
metric nonproportional hazards survival modeling,” Biometrics, 65, 762–771.
De Jonge, R. and van Zanten, J. (2010), “Adaptive nonparametric bayesian inference
using location-scale mixture priors,” The Annals of Statistics, 38, 3300–3320.
Denison, D., Mallick, B., and Smith, A. (1998), “Bayesian MARS,” Statistics and
Computing, 8, 337–346.
Denison, D., Holmes, C., Mallick, B., and Smith, A. F. M. (2002), Bayesian methods
for nonlinear classiﬁcation and regression, Wiley & Sons, London.
D´esid´eri,
J. and Janka,
A. (2004),
“Multilevel
shape parameterization for
aerodynamic optimization–application to drag and noise reduction of tran-
sonic/supersonic business jet,” in European Congress on Computational Methods
in Applied Sciences and Engineering (ECCOMAS 2004), E. Heikkola et al eds.,
Jyv¨askyla, pp. 24–28.
D´esid´eri, J., Abou El Majd, B., and Janka, A. (2007), “Nested and self-adaptive
B´ezier parameterizations for shape optimization,” Journal of Computational
Physics, 224, 117–131.
Diaconis, P. and Freedman, D. (1986), “On the consistency of Bayes estimates,” The
Annals of Statistics, pp. 1–26.
Diggle, P., Menezes, R., and Su, T. (2010), “Geostatistical inference under preferen-
tial sampling (with discussion),” Journal of the Royal Statistical Society: Series C
(Applied Statistics), 59, 191–232.
Doss, H. (1985), “Bayesian nonparametric Estimation of the median; Part I: Com-
putation of the estimates,” The Annals of Statistics, 13, 1432–1444.
Dryden, I. and Mardia, K. (1998), Statistical shape analysis, vol. 4, John Wiley &
Sons New York.
Dunson, D. and Park, J. (2008a), “Kernel stick-breaking processes,” Biometrika, 95,
307–323.
Dunson, D., Pillai, N., and Park, J. (2007a), “Bayesian density regression,” Journal
of the Royal Statistical Society, Series B, 69, 163–183.
Dunson,
D. B. and Park,
J.-H. (2008b),
“Kernel stick-breaking processes,”
Biometrika, 95, 307–323.
Dunson, D. B., Pillai, N., and Park, J.-H. (2007b), “Bayesian density regression,”
Journal of the Royal Statistical Society, Series B, 69, 163–183.
243

Escobar, M. D. and West, M. (1995), “Bayesian Density Estimation and Inference
Using Mixtures,” Journal of the American Statistical Association, 90, 577–588.
Farin, G. (2002), Curves and surfaces for CAGD: a practical guide, Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA, 5th edn.
Ferguson, T. (1973a), “A Bayesian analysis of some nonparametric problems,” The
Annals of Statistics, 1, 209–230.
Ferguson, T. (1974a), “Prior distributions on spaces of probability measures,” The
Annals of Statistics, 2, 615–629.
Ferguson, T. S. (1973b), “A Bayesian Analysis of Some Nonparametric Problems,”
The Annals of Statistics, 1, 209–230.
Ferguson, T. S. (1974b), “Prior Distributions on Spaces of Probability Measures,”
The Annals of Statistics, 2, 615–629.
Fonseca, T. C. O., Ferreira, M. A. R., and Migon, H. S. (2008), “Objective Bayesian
analysis for the Student-t regression model,” Biometrika, 95, 325–333.
Fowler, B. (1992), “Geometric manipulation of tensor product surfaces,” in Proceed-
ings of the 1992 symposium on Interactive 3D graphics, pp. 101–108, ACM.
Gelfand, A., Kottas, A., and MacEachern, S. (2005), “Bayesian nonparametric spa-
tial modeling with Dirichlet process mixing,” Journal of the American Statistical
Association, 100, 1021–1035.
Geweke, J. (1992), “Evaluating the Accuracy of Sampling-Based Approaches to the
Calculation of Posterior Moments,” Bayesian Statistics, 4, 169–194.
Ghosal, S. and van der Vaart, A. (2001), “Entropies and rates of convergence for
maximum likelihood and Bayes estimation for mixtures of normal densities,” The
Annals of Statistics, 29, 1233–1263.
Ghosal, S. and van der Vaart, A. (2007a), “Convergence rates of posterior distribu-
tions for noniid observations,” The Annals of Statistics, 35, 192–223.
Ghosal, S. and van der Vaart, A. (2007b), “Posterior convergence rates of Dirichlet
mixtures at smooth densities,” The Annals of Statistics, 35, 697–723.
Ghosal, S., Ghosh, J., and Ramamoorthi, R. (1999), “Posterior consistency of Dirich-
let mixtures in density estimation,” The Annals of Statistics, 27, 143–158.
Ghosal, S., Ghosh, J., and van der Vaart, A. (2000), “Convergence rates of posterior
distributions,” Annals of Statistics, 28, 500–531.
244

Ghosal, S., Lember, J., and Van Der Vaart, A. (2003), “On Bayesian adaptation,”
Acta Applicandae Mathematicae, 79, 165–175.
Ghosal, S., Lember, J., and Van Der Vaart, A. (2008), “Nonparametric Bayesian
model selection and averaging,” Electronic Journal of Statistics, 2, 63–89.
Ghoshal, S. and Roy, A. (2006), “Posterior consistency of Gaussian process prior in
nonparametric binary regression,” The Annals of Statistics, 34, 2413–2429.
Godsill, S. (2001), “On the relationship between Markov chain Monte Carlo methods
for model uncertainty,” Journal of Computational and Graphical Statistics, 10,
230–248.
Gordon, W. and Riesenfeld, R. (1974), “Bernstein-B´ezier methods for the computer-
aided design of free-form curves and surfaces,” Journal of the ACM (JACM), 21,
293–310.
Goshtasby, A. (1992), “Surface reconstruction from scattered measurements,” SPIE.
Gramacy, R. and Lee, H. (2008), “Bayesian treed Gaussian process models with an
application to computer modeling,” Journal of the American Statistical Associa-
tion, 103, 1119–1130.
Griﬃn, J. and Steel, M. (2006a), “Order-based dependent Dirichlet processes,” Jour-
nal of The American Statistical Association, 101, 179–194.
Griﬃn, J. and Steel, M. (2010), “Bayesian nonparametric modelling with the Dirich-
let process regression smoother,” Statistica Sinica, 20, 1507–1527.
Griﬃn, J. and Steel, M. F. J. (2006b), “Order-Based Dependent Dirichlet Processes,”
Journal of the American Statistical Association, Theory and Methods, 101, 179–
194.
Hagen, H. and Santarelli, P. (1992), “Variational design of smooth B-spline surfaces,”
in Topics in surface modeling, pp. 85–92, Society for Industrial and Applied Math-
ematics.
Hastie, T. and Stuetzle, W. (1989), “Principal curves,” Journal of the American
Statistical Association, pp. 502–516.
Higdon, D. (2002), “Space and space-time modeling using process convolutions,”
Quantitative methods for current environmental issues, pp. 37–56.
Ho, L. and Stoyan, D. (2008), “Modeling marked point patterns by intensity-marked
Cox processes,” Statistics and Probability Letters, 78, 1194–1199.
Hoﬀmann, M. and Lepski, O. (2002), “Random rates in anisotropic regression,”
Annals of statistics, pp. 325–358.
245

Hoppe, H., DeRose, T., Duchamp, T., McDonald, J., and Stuetzle, W. (1992), “Sur-
face reconstruction from unorganized points,” Computer Graphics, 26, 71–71.
Huang, T. (2004), “Convergence rates for posterior distributions and adaptive esti-
mation,” The Annals of Statistics, 32, 1556–1593.
Ibragimov, I. and Khasminski, R. (1981), Statistical estimation–asymptotic theory,
vol. 16, Springer.
Ishwaran, H. and James, L. (2001), “Gibbs Sampling Methods for Stick-Breaking
Priors,” Journal of the American Statistical Association, 96, 161–173.
James, L. F., Lijoi, A., and Pr¨unster, I. (2005), “Bayesian nonparametric inference
via classes of normalized random measures,” Tech. rep., ICER Applied Mathemat-
ics Working Papers Series 5/2005.
Jara, A., Lesaﬀre, E., De Iorio, M., and Quintana, F. (2010), “Bayesian semipara-
metric inference for multivariate doubly-interval-censored data,” The Annals of
Applied Statistics, 4, 2126–2149.
Johnstone, J. and Sloan, K. (1995), “Tensor product surfaces guided by minimal
surface area triangulations,” in Visualization, p. 254, Published by the IEEE Com-
puter Society.
Kalli, M., Griﬃn, J., and Walker, S. (2010), “Slice sampling mixture models,” Statis-
tics and computing, pp. 1–13.
Kazhdan, M., Bolitho, M., and Hoppe, H. (2006), “Poisson surface reconstruction,”
in Proceedings of the fourth Eurographics symposium on Geometry processing, pp.
61–70, Eurographics Association.
Kerkyacharian, G., Lepski, O., and Picard, D. (2001), “Nonlinear estimation in
anisotropic multi-index denoising,” Probability theory and related ﬁelds, 121, 137–
170.
Klutchnikoﬀ, N. (2005), “On the adaptive estimation of anisotropic functions,” Ph.D.
thesis, Ph. D. thesis, Univ. Aix–Marseille I.
Knapik, B., van der Vaart, A., and van Zanten, J. (2011), “Bayesian inverse prob-
lems,” Arxiv preprint arXiv:1103.2692.
Kottas, A. and Gelfand, A. E. (2001), “Bayesian Semiparametric Median Regression
Modeling,” Journal of the American Statistical Association, 96, 1458–1468.
Kruijer, W., Rousseau, J., and van der Vaart, A. (2010), “Adaptive Bayesian density
estimation with location-scale mixtures,” Electronic Journal of Statistics, 4, 1225–
1257.
246

Kuelbs, J. and Li, W. (1993), “Metric entropy and the small ball problem for Gaus-
sian measures,” J. Funct. Anal, 116, 133–157.
Kundu, S. and Dunson, D. (2011), “Single Factor Transformation Priors for Density
Regression,” DSS Discussion Series.
Kurtek, S., Srivastava, A., Klassen, E., and Ding, Z. (2011), “Statistical Modeling
of Curves Using Shapes and Related Features,” Journal of American Statistical
Association, (in revision).
Lang, J. and R¨oschel, O. (1992), “Developable (1, n)-B´ezier surfaces,” Computer
Aided Geometric Design, 9, 291–298.
Lange, K., Little, R. J. A., and Taylor, J. M. G. (1989), “Robust statistical modelling
using the T distribution.” Journal of the American Statistical Association, 84, 881–
896.
Lavine, M. and Mockus, A. (2005), “A nonparametric Bayes method for isotonic
regression,” Journal of Statistical Planning and Inference, 46, 235–248.
Lee, H., Higdon, D., Calder, C., and Holloman, C. (2005), “Eﬃcient models for
correlated data via convolutions of intrinsic processes,” Statistical Modelling, 5,
53–74.
Lenk, P. (1988), “The logistic normal distribution for Bayesian, nonparametric, pre-
dictive densities,” Journal of the American Statistical Association, 83, 509–516.
Lenk, P. (1991), “Towards a practicable Bayesian nonparametric density estimator,”
Biometrika, 78, 531.
Lepski, O. (1990), “A problem of adaptive estimation in Gaussian white noise,”
Teoriya Veroyatnostei i ee Primeneniya, 35, 459–470.
Lepski, O. (1991), “Asymptotic minimax adaptive estimation. —. Upper bounds.”
Theory Probab. Appl, 36, 645–659.
Lepski, O. (1992), “Asymptotic minimax adaptive estimation. 2.— Statistical model
without optimal adaptation. Adaptive estimators,” Theory Probab. Appl, 37, 468–
481.
Lepski, O. and Levit, B. (1999), “Adaptive nonparametric estimation of smooth
multivariate functions,” Mathematical Methods of Statistics, 8, 344–370.
Li, J. (2004), “Visualization of high-dimensional data with relational perspective
map,” Information Visualization, 3, 49–59.
Li, R., Li, G., and Wang, Y. (2007), “Closed surface modeling with helical line
measurement data,” Frontiers of Mechanical Engineering in China, 2, 72–76.
247

Lo, A. Y. (1984), “On a class of Bayesian nonparametric estimates. I: Density esti-
mates,” The Annals of Statistics, 12, 351–357.
Lorensen, W. and Cline, H. (1987), “Marching cubes: A high resolution 3D surface
construction algorithm,” in Proceedings of the 14th annual conference on Computer
graphics and interactive techniques, pp. 163–169, ACM.
MacEachern, S. (1999), “Dependent nonparametric processes,” in Proceedings of the
Section on Bayesian Statistical Science, pp. 50–55.
Madi, M. (2004), “Closed-form expressions for the approximation of arclength pa-
rameterization for Bezier curves,” International journal of applied mathematics
and computer science, 14, 33–42.
Mann, S. and DeRose, T. (1995), “Computing values and derivatives of B´ezier and
B-spline tensor products,” Computer Aided Geometric Design, 12, 107–110.
McLachlan, G. and Peel, D. (2000), “Mixtures of factor analyzers,” in In Proceedings
of the Seventeenth International Conference on Machine Learning, Citeseer.
Menezes, R. (2005), “Assessing spatial dependency under non-standard sampling,”
Ph.D. thesis, Universidad de Santiago de Compostela, Santiago de Compostela,
Spain.
Mokhtarian, F. and Mackworth, A. (1992), “A theory of multiscale, curvature-based
shape representation for planar curves,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, 14, 789–805.
Møller, J., Syversveen, A., and Waagepetersen, R. (2001), “Log Gaussian Cox pro-
cesses,” Scandinavian Journal of Statistics, 25, 451–482.
Mortenson, M. (1985), Geometrie modeling, John Wiley, New York.
Muller, H. (2005), “Surface reconstruction-an introduction,” in Scientiﬁc Visualiza-
tion Conference, 1997, p. 239, IEEE.
M¨uller, P., Erkanli, A., and West, M. (1996), “Bayesian curve ﬁtting using multi-
variate normal mixtures,” Biometrika, 83, 67–79.
M¨uller, P., Erkanli, A., and West, M. (1996), “Bayesian curve ﬁtting using multi-
variate normal mixtures,” Biometrika, 83, 67–79.
Neal, R. J. (1998), “Regression and Classiﬁcation using Gaussian process Priors,”
Bayesian Statistics, 6, 475–501.
Norets, A. (2010), “Approximation of conditional densities by smooth mixtures of
regressions,” The Annals of Statistics, 38, 1733–1766.
248

Norets, A. and Pelenis, J. (2009), “Bayesian modeling of joint and conditional dis-
tributions,” Unpublished manuscript, Princeton Univ.
Norets, A. and Pelenis, J. (2010), “Posterior consistency in conditional distribution
estimation by covariate dependent mixtures,” Unpublished manuscript, Princeton
Univ.
Nott, D. (2006), “Semiparametric estimation of mean and variance functions for
non-Gaussian data,” Computational Statistics, 21, 603–620.
Nussbaum, M. (1985), “Spline smoothing in regression models and asymptotic eﬃ-
ciency in L2,” The Annals of Statistics, pp. 984–997.
Ongaro, A. and Cattaneo, C. (2004), “Discrete random probability measures: a
general framework for nonparametric Bayesian inference,” Statistics & Probability
Letters, 67, 33–45.
Papaspiliopoulos, O. (2008), “A note on posterior sampling from Dirichlet mixture
models,” Tech. rep.
Papaspiliopoulos, O. and Roberts, G. . (2008), “Retrospective Markov chain Monte
Carlo methods for Dirichlet process hierarchical models,” Biometrika, 95, 169–183.
Park, B. and Dunson, D. (2009), “Bayesian generalized product partition model,”
Statistica Sinica, (to appear).
Pati, D. and Dunson, D. (2010), “Bayesian nonparametric regression with varying
residual density,” Annals of the Institute for Statistical Mathematics.
Pati, D. and Dunson, D. (2011), “Bayesian modeling of closed surfaces through tensor
products,” (submitted to Biometrika).
Pati, D., Dunson, D., and Tokdar, S. (2010), “Posterior consistency in conditional
distribution estimation,” Duke University, Dept. of Statistics.
Pati, D., Bhattacharya, A., and Dunson, D. (2011a), “Posterior convergence rates in
latent variable density regression models,” (in progress).
Pati, D., Bhattacharya, A., and Dunson, D. (2011b), “Posterior convergence rates in
non-linear latent variable models,” Arxiv preprint arXiv:1109.5000, (submitted).
Pati, D., Dunson, D., and Tokdar, S. (2012), “Posterior consistency in conditional
distribution estimation,” Journal of Multivariate Analysis, (submitted).
Pelenis, J. and Norets, A. (2011), “Bayesian semi-parametric regression,” Tech. rep.,
Tech. rep., Princeton University, Economics Department.
249

Persoon, E. and Fu, K. (1977), “Shape discrimination using Fourier descriptors,”
Systems, Man and Cybernetics, IEEE Transactions on, 7, 170–179.
Piegl, L. (1986), “The sphere as a rational Bezier surface,” Computer Aided Geomet-
ric Design, 3, 45–52.
Radcliﬀe, S. J., Guo, W., and Ten Have, T. (2004), “Joint modeling of longitudinal
and survival data via a common frailty,” Biometrics, 60, 892–899.
Raftery, A. E. and Lewis, S. (1992), “How Many Iterations in the Gibbs Sampler?”
Bayesian Statistics, 4, 763–773.
Rasmussen, C. (2004), “Gaussian processes in machine learning,” Advanced Lectures
on Machine Learning, pp. 63–71.
Rasmussen, C. and Williams, C. (2005), “Gaussian Processes for Machine Learning
(Adaptive Computation and Machine Learning),” .
Reich, B., Bondell, H., and Li, L. (2010), “Suﬃcient Dimension Reduction via
Bayesian Mixture Modeling,” Biometrics.
Rineau, L. and Yvinec, M. (2007), “A generic software design for Delaunay reﬁne-
ment meshing,” Computational Geometry, 38, 100–110.
Rodrigues, A. and Diggle, P. (2010), “A Class of Convolution-Based Models for
Spatio-Temporal Processes with Non-Separable Covariance Structure,” Scandina-
vian Journal of Statistics, (to appear).
Rodriguez, A. and Dunson, D. (2011a), “Nonparametric Bayesian models through
probit stick-breaking processes,” Bayesian Analysis, 6, 145–178.
Rodriguez, A. and Dunson, D. (2011b), “Nonparametric Bayesian models through
probit stick-breaking processes,” Bayesian Analysis, 6, 145–178.
Rossi, D. and Willsky, A. (2003), “Reconstruction from projections based on detec-
tion and estimation of objects–Parts I and II: Performance analysis and robust-
ness analysis,” Acoustics, Speech and Signal Processing, IEEE Transactions on,
32, 886–906.
R´oth, ´A. and Juh´asz, I. (2010), “Control point based exact description of a class of
closed curves and surfaces,” Computer Aided Geometric Design, 27, 179–201.
R´oth, ´A., Juh´asz, I., Schicho, J., and Hoﬀmann, M. (2009), “A cyclic basis for closed
curve and surface modeling,” Computer Aided Geometric Design, 26, 528–546.
Rousseau, J. (2010), “Rates of convergence for the posterior distributions of mixtures
of betas and adaptive nonparametric estimation of the density,” The Annals of
Statistics, 38, 146–180.
250

Savitsky, T., Vannucci, M., and Sha, N. (2011), “Variable selection for nonparametric
Gaussian process priors: Models and computational strategies,” Statistical Science,
26, 130–149.
Schwartz, L. (1965a), “On Bayes procedures,” Z. Wahrsch. Verw. Gebiete, 4, 10–26.
Schwartz, L. (1965b), “On Bayes procedures,” Z. Wahrsch. Verw. Gebiete, 4, 10–26.
Scott, J. and Berger, J. (2010), “Bayes and empirical-Bayes multiplicity adjustment
in the variable-selection problem,” The Annals of Statistics, 38, 2587–2619.
Sederberg, T. (1983), “Implicit and parametric curves and surfaces for computer
aided geometric design,” ETD Collection for Purdue University.
Sethuraman, J. (1994), “A Constructive Deﬁnition of Dirichlet Priors,” Statistica
Sinica, 4, 639–650.
Shahbaba, B. and Neal, R. (2009), “Nonlinear models using dirichlet process mix-
tures,” Journal of Machine Learning Research, 10, 1829–1850.
Shen, L. and Makedon, F. (2006), “Spherical mapping for processing of 3D closed
surfaces,” Image and vision computing, 24, 743–761.
Shen, W. and Ghosal, S. (2011), “Adaptive Bayesian multivariate density estimation
with Dirichlet mixtures,” Arxiv preprint arXiv:1109.6406.
Smith, M. and Kohn, R. (1997), “A Bayesian approach to nonparametric bivariate
regression,” Journal of the American Statistical Association, 92, 1522–1535.
Soussen, C. and Mohammad-Djafari, A. (2002), “Closed surface reconstruction in
X-ray tomography,” in Image Processing, 2001. Proceedings. 2001 International
Conference on, vol. 1, pp. 718–721, IEEE.
Staib, L. and Duncan, J. (1992), “Deformable Fourier models for surface ﬁnding in
3-D images,” in Proceedings:SPIE-International Society for Optical Engineering,
pp. 90–90, Citeseer.
Stein, M. L. (1999), Interpolation of Spatial Data: Some Theory for Kriging, Springer
Series in Statistics, New York.
Stepanets, A. (1974), “The approximation of certain classes of diﬀerentiable periodic
functions of two variables by Fourier sums,” Ukrainian Mathematical Journal, 25,
498–506.
Stone, C. (1982), “Optimal global rates of convergence for nonparametric regression,”
The Annals of Statistics, pp. 1040–1053.
251

Su, B. and Liu, D. (1989), Computational geometry: curve and surface modeling,
Academic Press Professional, Inc. San Diego, CA, USA.
Su, J., Dryden, I., Klassen, E., Le, H., and Srivastava, A. (2011), “Fitting Optimal
Curves to Time-Indexed, Noisy Observations of Stochastic Processes on Nonlinear
Manifolds,” Journal of Image and Vision Computing.
Szeliski, R. and Tonnesen, D. (1992), “Surface modeling with oriented particle sys-
tems,” ACM SIGGRAPH Computer Graphics, 26, 185–194.
Tang, Y. and Ghosal, S. (2007a), “A consistent nonparametric Bayesian procedure
for estimating autoregressive conditional densities,” Computational Statistics &
Data Analysis, 51, 4424–4437.
Tang, Y. and Ghosal, S. (2007b), “Posterior consistency of Dirichlet mixtures for
estimating a transition density,” Journal of Statistical Planning and Inference,
137, 1711–1726.
Tibshirani, R. (1996), “Regression Shrinkage and Selection via the Lasso,” Journal
of the Royal Statistical Society Series B, 58, 267–288.
Tokdar, S. (2006a), “Posterior consistency of Dirichlet location-scale mixture of nor-
mals in density estimation and regression,” Sankhy¯a: The Indian Journal of Statis-
tics, 67, 90–110.
Tokdar, S. (2011a), “Dimension adaptability of Gaussian process models with vari-
able selection and projection,” Arxiv preprint arXiv:1112.0716.
Tokdar, S. (2011b), “Posterior rates of contraction in Dirichlet process mixtures of
multivariate normals,” Arxiv preprint arXiv:1111.4148.
Tokdar, S. and Ghosh, J. (2007), “Posterior consistency of logistic Gaussian process
priors in density estimation,” Journal of Statistical Planning and Inference, 137,
34–42.
Tokdar, S., Zhu, Y., and Ghosh, J. (2010a), “Bayesian Density Regression with
Logistic Gaussian Process and Subspace Projection,” Bayesian Analysis, 5, 1–26.
Tokdar, S., Zhu, Y., and Ghosh, J. (2010b), “Bayesian Density Regression with
Logistic Gaussian Process and Subspace Projection,” Bayesian Analysis, 5, 1–26.
Tokdar, S. T. (2006b), “Posterior Consistency of Dirichlet Location-scale Mixture of
Normals in Density Estimation and Regression,” Sankhy¯a, 68, 90–110.
van der Vaart, A. and van Zanten, J. (2007), “Bayesian inference with rescaled
Gaussian process priors,” Electronic Journal of Statistics, 1, 433–448.
252

van der Vaart, A. and van Zanten, J. (2008a), “Rates of contraction of posterior
distributions based on Gaussian process priors,” The Annals of Statistics, 36, 1435–
1463.
van der Vaart, A. and van Zanten, J. (2008b), “Reproducing kernel Hilbert spaces
of Gaussian priors,” IMS Collections, 3, 200–222.
van der Vaart, A. and van Zanten, J. (2009), “Adaptive Bayesian estimation using a
Gaussian random ﬁeld with inverse Gamma bandwidth,” The Annals of Statistics,
37, 2655–2675.
van der Vaart, A. W. and Wellner, J. A. (1996), Weak Convergence and Empirical
Processes, Springer-Verlag, New York.
Walker, S. G. (2007), “Sampling the Dirichlet Mixture Model with Slices,” Commu-
nications in Statistics-Simulation and Computation, 36, 45–54.
Wand, M. and Schucany, W. (1990), “Gaussian-based kernels,” Canadian Journal of
Statistics, 18, 197–204.
Weiss, R. (1996), “An Approach to Bayesian Sensitivity Analysis,” Journal of the
Royal Statistical Society. Series B (Methodological), 58, 739–750.
West, M. (1984), “Outlier models and prior distributions in Bayesian linear regres-
sion,” Journal of the Royal Statistical Society Series B, 46, 431–439.
West, M. (1987), “On scale mixtures of normal distributions,” Biometrika, 74, 646–
648.
Whang, K., Song, J., Chang, J., Kim, J., Cho, W., Park, C., and Song, I. (2002),
“Octree-R: An adaptive octree for eﬃcient ray tracing,” Visualization and Com-
puter Graphics, IEEE Transactions on, 1, 343–349.
Whitney, H. (1937), “On regular closed curves in the plane,” Compositio Math, 4,
276–284.
Wu, M. and Follmann, D. (1999), “Use of summary measures to adjust for informa-
tive missingness in repeated measures data with random eﬀects,” Biometrics, 55,
75–84.
Wu, Y. and Ghosal, S. (2010), “L1-Consistency of Dirichlet Mixtures in Multivariate
Bayesian Density Estimation,” Journal of Multivariate Analysis, (to appear).
Wu, Y. and Ghoshal, S. (2008), “Kullback Leibler property of kernel mixture priors
in Bayesian density estimation,” Electronic Journal of Statistics, 2, 298–331.
Yang, M. and Lee, E. (1999), “Segmentation of measured point data using a para-
metric quadric surface approximation,” Computer Aided Design, 31, 449–457.
253

Yau, P. and Kohn, R. (2003), “Estimation and variable selection in nonparametric
heteroscedastic regression,” Statistics and Computing, 13, 191–208.
Yoon, J. (2009), “Bayesian analysis of conditional density functions: a limited infor-
mation approach,” Unpublished manuscript, Claremont Mckenna College.
Zahn, C. and Roskies, R. (1972), “Fourier descriptors for plane closed curves,” Com-
puters, IEEE Transactions on, 100, 269–281.
Zhang, H. (2004), “Inconsistent Estimation and Asymptotically Equal Interpolations
in Model-Based Geostatistics,” Journal of the American Statistical Association, 99,
250–261.
Zou, F., Huang, H., Lee, S., and Hoeschele, I. (2010), “Nonparametric Bayesian
Variable Selection With Applications to Multiple Quantitative Trait Loci Mapping
With Epistasis and Gene–Environment Interaction,” Genetics, 186, 385.
254

Biography
Debdeep Pati was born on March 12, 1985 in Kolkata, India. He received his Bach-
elors degree in Statistics from the Indian Statistical Institute in 2006. He continued
there for a masters degree and graduated in 2008 specializing in Mathematical Statis-
tics and Probability. In August 2008, Debdeep moved to the United States to pursue
a Ph.D. in Statistical Science at Duke University, Durham, NC. In 2010, he earned
a Masters degree en route to his Ph.D. He graduated with a Doctor of Philosophy
under the supervision of Professor David B. Dunson in May 2012. From Fall 2012,
he will be an Assistant Professor at the Department of Statistics, Florida State Uni-
versity. His research interests center around nonparametric Bayesian foundational
theory and methodology in a broad range of areas including density estimation, high-
dimensional density regression and variable selection, shape reconstruction, imaging
and hierarchical modeling of shapes.
255

