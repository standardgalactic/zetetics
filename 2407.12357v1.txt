Evaluating graph-based explanations for AI-based recommender
systems
Simon Delarue
simon.delarue@telecom-paris.fr
LTCI, Télécom Paris,
Institut Polytechnique de Paris
France
Astrid Bertrand
LTCI, Télécom Paris,
Institut Polytechnique de Paris
France
Tiphaine Viard
i3, Télécom Paris,
Institut Polytechnique de Paris
France
ABSTRACT
Recent years have witnessed a rapid growth of recommender sys-
tems, providing suggestions in numerous applications with poten-
tially high social impact, such as health or justice. Meanwhile, in Eu-
rope, the upcoming AI Act mentions transparency as a requirement
for critical AI systems in order to “mitigate the risks to fundamental
rights”. Post-hoc explanations seamlessly align with this goal and
extensive literature on the subject produced several forms of such
objects, graphs being one of them. Early studies in visualization
demonstrated the graphs’ ability to improve user understanding,
positioning them as potentially ideal explanations. However, it
remains unclear how graph-based explanations compare to other
explanation designs. In this work, we aim to determine the effective-
ness of graph-based explanations in improving users’ perception
of AI-based recommendations using a mixed-methods approach.
We first conduct a qualitative study to collect users’ requirements
for graph explanations. We then run a larger quantitative study in
which we evaluate the influence of various explanation designs,
including enhanced graph-based ones, on aspects such as under-
standing, usability and curiosity toward the AI system. We find
that users perceive graph-based explanations as more usable than
designs involving feature importance. However, we also reveal that
textual explanations lead to higher objective understanding than
graph-based designs. Most importantly, we highlight the strong
contrast between participants’ expressed preferences for graph de-
sign and their actual ratings using it, which are lower compared
to textual design. These findings imply that meeting stakeholders’
expressed preferences might not alone guarantee “good” explana-
tions. Therefore, crafting hybrid designs successfully balancing
social expectations with downstream performance emerges as a
significant challenge.
1
INTRODUCTION
Recommender systems have emerged as fundamental tools for
delivering personalized services to users. These frameworks find
application across diverse domains, ranging from commonplace
ones like online library and e-shopping, to more contentious ones
such as finance, law, education or health. In cases where recommen-
dations carry significant implications for users, it becomes essential
to provide additional explanatory mechanisms. In Europe, ongoing
legal discussions highlight the likelihood of transparency becoming
a requirement for high-risk Artificial Intelligence (AI) systems [14].
From the academic perspective, several works underscore a positive
correlation between users’ understanding of the model and their
trust in such systems [26, 47, 49, 54, 58].
Explanations aim to address the “why” question in relation to
a recommender system or a specific prediction [58]. Emerging
from two primary research fields, namely Computer Science (CS)
and Human Computer Interaction (HCI), various forms of expla-
nations, such as text [52], charts [18, 24], matrices [8], hybrid de-
signs [4, 9, 28] or graphs [41, 53, 55], have been proposed to enhance
user satisfaction toward AI systems. Nevertheless, determining
whether an explanation qualifies as a “good” one is not straightfor-
ward. On the one hand, Miller [31] argues that “good” explanations
should be (i) contrastive, highlighting contrast with alternatives, (ii)
selected, recognizing that people seldom expect the complete cause
of an event and (iii) preferring causal over probabilistic reasoning.
Meanwhile, authors in [26] suggest that it is essential to ensure
that explanations align with stakeholders’ desiderata, sometimes
referred to as social expectations [20].
We hypothesize that graphs, as objects that can model selected
relational data and exhibit causality, appears to align intuitively
with Miller’s criteria and may have the ability to fulfill various
users’ requirements. More specifically, the recommendation task
can be naturally framed as a link prediction task in a bipartite graph
(see Figure 1) that includes both users and items. The relevance
of graph structures in problem-solving is not novel, as its roots
trace back hundreds of years to Euler’s solution to the Königsberg
bridges problem [13]. More recently, within the HCI field, several
works have explored the impact of graph visualization through ex-
tended user studies [6, 19]. However, only a few works [15, 24] have
explored the application of such designs in the explanatory context
of recommender systems. In the CS community, when graphs are
considered as explanations, they are often evaluated from an algo-
rithmic perspective, also referred to as functionally-grounded evalu-
ation [10]. This setup involves the use of machine learning-oriented
proxy metrics that operate without human oversight, contradicting
the criteria outlined in [20, 26].
In this work, we seek to bridge the gap between the HCI field,
where graph designs are seldom examined in the context of explain-
ability, and the CS field, where graphs are ubiquitous in explainable
recommender systems – either as components of models or expla-
nation designs – but are primarily evaluated from an algorithmic
standpoint. To achieve this, we conduct a qualitative user study to
characterize users’ needs in terms of graph-based designs. Specif-
ically, we gather requirements from users with varying levels of
expertise in AI systems. Leveraging this knowledge, we develop
an enhanced graph-based design and compare it to two commonly
used forms of explanations – textual and SHAP [28]-based expla-
nations – through a quantitative user study. Through these studies,
our goal is to answer the following research questions:
arXiv:2407.12357v1  [cs.AI]  17 Jul 2024

• RQ1: What are the different stakeholders’ desiderata regarding
graph-based explanations for recommender systems?
• RQ2: How does enhanced graph-based explanation design
influence users of AI recommendations?
• RQ3: How does enhanced graph-based explanation design
compare to other explanation designs?
• RQ4: How does the user’s expertise level toward AI systems
affect their explanation design preferences?
Our contributions encompass the following key aspects. Firstly,
we investigate stakeholders’ desiderata for graph-based explana-
tions through a qualitative user study involving participants with
diverse expertise levels, highlighting the preference for item-based
design over user-based design. Secondly, drawing upon these in-
sights, we build an enhanced graph-based explanation using item-
oriented graph projection. Lastly, we conduct a larger scale quanti-
tative study to discuss the impact of visual design on understanding,
curiosity and usability. Consequently, we emphasize the contrast
between users’ expressed preference, both qualitatively and quan-
titatively, for graph-based explanations and their higher ratings
when using dialogic explanations.
2
RELATED WORK
2.1
Explainable AI for recommender systems
Explainable recommendations offer user personalized item sug-
gestions while explicitly clarifying why a particular item is being
proposed. The design of explanations for recommendations has
been studied for a long time [18, 47]. Numerous studies emphasized
the correlation between understanding the system and placing trust
in it [26, 47, 49, 54, 58], amplifying the significance of such expla-
nations. While recommender systems can be crafted to naturally
include interpretable elements (model-intrinsic) [59], our focus is
on approaches involving post-hoc explanations (model-agnostic),
also known as post-hoc interpretability [27, 31].
Designing such explanation is a task situated at the intersec-
tion of two research fields, namely Computer Science (CS) and
Human Computer Interaction (HCI), each with its distinct focus
and requirements.
2.2
Explainable AI in Computer Science
From the CS perspective, building explainable recommendations
involves a variety of techniques and explanation types. This in-
cludes rule mining [40], approximation using simple models as
with LIME [43], feature importance methods such as SHAP [28],
attention maps [44] or graphs [41, 53, 55]. Concerning graphs, au-
thors often highlight their natural ability to reveal user-item con-
nectivity [53] or extract small subgraphs containing elements most
influential for predictions [55]. In this context, justifications for
using graphs as explanations for recommendations seem to align
seamlessly with Miller’s description of good explanations [31].
However, the evaluation of such explanations in the CS field heav-
ily relies on functionally-grounded approaches [10], i.e. algorithmic-
oriented techniques that measure proxy metrics such as fidelity
or correctedness [34], but lack human control. While such an eval-
uation approach offers advantages by saving time and avoiding
ethical concerns potentially associated with human participation,
authors in [10, 32] emphasize the limitations of these approaches
in terms of “real-world impact” and advocate for their use solely
after conducting user studies. To tackle this issue, recent work inte-
grated both graph-based explanations and an evaluation framework
through a user study [57]. However, the authors restricted their
comparison to graph designs among themselves only, preventing
a comprehensive assessment of the validity of graph explanations
against alternative designs.
In this work, we go beyond the conventional algorithmic-oriented
evaluation typically employed in the CS field and introduce both
qualitative and quantitative user studies to evaluate the validity of
graph-based explanations compared to other designs.
2.3
Explainable AI in Human Computer
Interaction
It is acknowledged that visualization can provide cognitive support
through various mechanisms, e.g. helping in pattern discovery, sum-
marizing large volumes of data, or reducing search time [50]. The
HCI field extensively studies the influence of design choices, with
several works emphasizing their significance in impacting users’
understanding and ability to contextualize problems. In particular,
numerous studies have explored the impact of graph visualizations
and showed the influence of the overall setting on their perfor-
mance. For instance, several works demonstrated that node-link
representations were indeed advantageous over matrix represen-
tations, when faced with small graphs and when path-oriented
tasks were involved [16, 17, 23, 37, 38]. For other tasks, such as
weighted graph comparison [3] or suspicious node detection [29],
matrix representations have proven to be a superior choice over
node-link diagrams. More recently, authors in [9] analyzed hybrid
visualizations that combine node-link and matrix representations.
Their goal was to provide a comprehensive tool for analyzing real-
world networks that are globally sparse but locally dense. They
showed that in such configurations, their mixed model overcomes
the limitations of using the node-link diagram alone. While all these
approaches systematically involve user study evaluations, they are
not specific to the explanatory context of recommender systems.
Few works from the HCI field address both graphs and their eval-
uation as explanations. In an early study, authors in [36] carried out
a user study to assess the impact of various interactive graph-based
representations of a recommender system. They showed that 78%
of users “felt that the system provided a good explanation of collabo-
rative filtering”. However, the graph representation itself was not
challenged and the study only focused on the layout of the system
(profile-based or not) and its interactivity. More recently, authors
in [24] involved graphs to highlight user preferences for item-based
explanations. However, their graphs consist in concentric circle
diagrams or pathways between columns, which we argue do not
fully leverage the capabilities of graphs. We draw a similar con-
clusion for [15], where authors show participants’ preference for
action-oriented over connection-oriented explanations, but where
graphs are limited to the paths they can exhibit.
In this work, similar to the aforementioned studies, we maintain
the evaluation process that involves user studies to assess design
performance. However, we deviate from them by integrating our
analysis directly into the explanatory context.
2

3
STUDY 1: QUALITATIVE EXPLORATION OF
STAKEHOLDERS’ EXPECTATIONS
REGARDING GRAPH-BASED
EXPLANATIONS
We seek to answer RQ1 by exploring stakeholders’ expectations
regarding graph-based explanations for AI-based recommender
systems. To achieve this, we conduct a qualitative study involving
participants with varying expertise levels in AI systems, employing
a think-aloud case study to collect insights and remarks.
3.1
Study design
We interviewed 12 participants. All participants were volunteers,
recruited through an email campaign within the university with
which the authors are affiliated. We conducted a 30-minute inter-
view with each participant. To ensure data privacy, participants
signed a consent form designed and approved in collaboration with
the Data Protection Officer. Each interview was then divided into
three parts.
In the first part of the session, we assessed the perceived ex-
pertise level of participants regarding AI-based systems in general
and recommender systems specifically, using a preliminary ques-
tionnaire. From these answers, we obtained three distinct groups.
The first group consists of Experts (4 participants); people who con-
sider themselves really familiar with AI-based systems and either
have a precise idea of what recommender systems are, or devel-
oped such algorithms. On the other side of the spectrum, another
group includes non-Experts (2 participants); these users consider
themselves slightly familiar with AI-based systems and are only
interacting with recommender systems as users, or have a general
idea of how these systems work. The last group lies in-between
these two groups and includes Insiders (6 participants); people who
are familiar with AI-based systems and have a general idea of how
recommender systems work.
During the second part of the interview, we evaluated partic-
ipants’ perceived understanding of AI-based systems. They an-
swered questions about their anticipated risks for recommender
systems, their comprehension of the recommendations when they
receive some, and their understanding of the criteria influencing
these recommendations.
Lastly, we introduced a task-oriented think-aloud scenario. In
this scenario, we presented participants with an AI-based book
recommendation. To increase participant engagement, we collected
their book preferences prior to the interview and used this infor-
mation to create a personalized context incorporated into the AI
system, thus making the experiment more realistic. The system
recommendation was presented within a graph-based explanation,
featuring a subset of the participants’ previously read books along-
side other users and their readings. This explanation was displayed
as a bipartite graph, where users were connected to the books they
had read, with a distinct link indicating the system recommenda-
tion. Lastly, we provided information about book preferences using
link weighting (see Figure 1 for an illustration). We did not disclose
to the participants the details of the recommendation algorithm or
how we had chosen the other visible users. We asked the partici-
pants to discuss the relevance of the recommendation to them, their
understanding of the elements that led to this recommendation,
Figure 1: Bipartite graph-based explanation containing two
sets of distinct nodes. Blue nodes represent users (along with
their identifier) and red nodes represent books (along with
their title). A user is linked to a book if the former has read
and rated the latter. Thin links denote low ratings, while
thick links denote high ratings. The blue link corresponds
to the system recommendation.
and what kind of information was missing to better understand the
recommendation.
3.2
Results
We extracted key insights from the interviews, focusing on par-
ticipants’ understanding of the recommendation through the lens
of graph-based explanation. Additionally, we performed an induc-
tive content analysis [12] of the notes taken during the interviews,
identifying main themes related to participants’ desiderata toward
graph-based explanation. These results are summarized in Table 1.
3.2.1
A high level of perceived understanding, based on three main
criteria. To the question “To what extent do you understand the sug-
gestions made to you by AI-based recommendation systems?”, the
participants, whatever their level of expertise, said they under-
stood the recommendations in most of the cases. In the few cases
when participants find the recommendations unclear, they suggest
that their profile diverges from a hypothetical “basic” user, or they
propose that the algorithm’s designers intentionally introduced a
mechanism to propose a “percentage of new things” unknown to the
users. When questioned about their understanding of the factors
influencing a system to recommend a specific item, participants
identified three main criteria. Both Insider and non-Expert partici-
pants reported that the recommendations are primarily built based
on user characteristics, such as “the person’s gender, age” or “the
person’s interests”. Some Experts and a few Insiders believed that
the “similarity” between their profile and the profiles of other users
takes precedence in the recommendation algorithm’s decision. Ac-
cording to this group, the cross-knowledge of a large number of
users and their preferences enables the system to suggest new items.
Lastly, some participants considered that a recommendation is pri-
marily influenced by their personal history of interactions with the
platform. These participants mentioned factors such as “clicking
on an ad”, the “frequency of viewing”, or the “purchase history” as
elements at the core of the system’s suggestions.
3

3.2.2
Strong link between context knowledge and recommendation
understanding. When asked “Do you think this recommendation is
relevant? and why?”, participants unfamiliar with the item sug-
gested by the system either expressed a lack of understanding of
the recommendation (“I do not know this book therefore I don’t know
if the recommendation is valuable.”) or asked for additional informa-
tion about the book’s characteristics (e.g. author or literary genre)
before answering. Conversely, a participant who is familiar with
the recommendation or, after reviewing its related characteristics
identifies familiar elements (other books by the same author), will
immediately deem the recommendation relevant and give it credi-
bility: “The recommendation seems relevant [...] because I know the
author, and it makes me want to read the book.”. In one case, the
participant was familiar with the recommendation, had already
read the suggested book, yet found it less relevant. This judgment
was based on the perception that the recommendation did not align
with the literary genre they prefer. In summary, regardless of their
level of expertise, participants tend to draw parallels between the
predicted book and their reading history, considering factors such
as literary genre, period, or author. The analysis of similarities with
other users is only taken into consideration at a later stage and
appears to be optional for the positive or negative judgment of
the recommendation. In one instance, the recommendation aligns
with the participant’s literary preferences (known and appreciated
author) and is described as relevant, even if the participant does not
see the connection with the elements provided in the explanation.
3.2.3
Graphs as objects modeling both similarities and popular-
ity. Participants’ utilization of the graph-based explanation can be
characterized along two dimensions. Firstly, Insiders and Experts
follow the links between users and books to highlight similarities
among users. These similarities are described through the sharing
of literary tastes and represent the manifestation of their a priori
understanding of how a recommender system operates. In such
cases, the weight (thickness) associated with the links, especially
when positive, plays an important role: “It visually strikes me a bit”.
Secondly, the graph-based explanation leads to an understanding
of predictions in terms of popularity, i.e. participants focus on the
quantity of users who have liked the recommendation. In these
cases, the recommendation is seen as a consequence of the enthu-
siasm surrounding it, rather than its relevance to the individual
user.
A few remarks about the use of graphs by participants should be
noted. The conclusions drawn regarding user similarities through
graph analysis require time (a few seconds), even for Experts who,
at the end of the interview, considered the graph easy to use. To
derive insights about the mechanisms of the system, non-Experts
tend not to analyze the graph through its links and users, but rather
to focus on already-known item characteristics (genre or author).
Finally, none of the participants mention graph-specific notions,
such as clique, i.e. fully connected subset elements in a graph or
density. Intuitively, such elements could have been used to deeply
understand subgroups of users that led to the recommendation.
3.2.4
Graph-based explanations are considered interpretable but
should contain item information. Participants, regardless of their
level of expertise, all agree that it would be useful to have more
information about the characteristics of the suggested item, such
as literary genre, period, etc. This would enable the creation of
“connections between different books” and provide a “general context”
to the decision: “What I miss is the tool’s knowledge about the world.”,
“I lack an idea of which types of books are similar to each other.”. To
address this, a participant suggests using colors to highlight the
proximity between groups of books. Furthermore, we noticed that
the needs for additional item information align perfectly with the
understanding of the recommendation; more than any other feature,
participants require item characteristics that they can rely on to
judge the relevance of a given recommendation.
An Expert and an Insider discussed the value of links representing
a weak attraction to items, mentioning that they helped establish a
“contrast” with other users, thereby enhancing their understanding
of “similarities in their profile with other users.”. Some Insiders evoked
adding numerical information on links, rather than playing on
thickness. However, the relevance of information about disliked
items, i.e. thin links in the graph, is not shared by all users: “I don’t
really get the thin links in this graph.”, “The fact that we have a thick
line and a very thin one [...], side by side, confuses me a bit.”.
Experts find graph explanations “very easily interpretable” or
“sufficient” and do not see the need for them to be “transformed
into natural language.”. One of them interprets them as the visual
counterpart to the classic formula found on platforms using recom-
mendation: “Other users than you, who liked similar things to you,
also liked...”. Moreover, they “do not expect to have an exhaustive
representation” of the context of the recommendation. Yet, even
Experts can have trouble distinguishing clear relations with other
users; “I don’t see clearly whether I am close or not to other users. [...]
I don’t fully see the connection between me and other users.”.
4
STUDY 2: INFLUENCE OF EXPLANATION
DESIGN
To address RQ2, RQ3 and RQ4, we conduct a qualitative user study
in which we investigate the influence of explanation design, among
them a graph-based explanation, on various AI-based recommender
system users. An example of our interface is shown in Figure 5 in
the Appendix.
4.1
Study design
4.1.1
Recommender system. For this purpose, we use the setup
introduced in our qualitative experiment (see Section 3): we build
an AI-based book recommender system which goal is to provide
the participant with a book suggestion given pre-selected read-
ing preferences. We did not share the functioning details of the
recommender system with the participants, but specified that the
recommendation they were given was AI-based.
4.1.2
Explanation design. We introduced three different explana-
tion designs, that were detailed to each participant through a small
paragraph, as well as a reading key. We used the well-known
SHAP [28] approach to design a feature importance oriented ex-
planation design. This choice stems from the popularity of this
method in the Explainable AI (XAI) community as well as for its
tight bounds with the users’ expectations obtained from our first
study, i.e. book characteristics are considered important for the
quality of the explanation. We also presented a Text explanation,
4

Table 1: Main themes on graph-based explanation design’s desiderata emerging from interviews.
Explanation
Experts
Insiders
non-Experts
Global design choice
Natural language explanation,
no need to transform it into nat-
ural language, not structured,
contrastive (with link thickness),
not exhaustive, [graphs] are re-
ally not bad, very easily inter-
pretable, interactions between
groups, I don’t fully see the con-
nection between me and other
users
Links between books, themes of
books, adding numbered infor-
mation to links, The fact that we
have a thick line and a very thin
one [...], side by side, confuses
me a bit.
I don’t see what [other users]
liked about [an item]
Item-based vs. user-based
You like this book and there is
this other similar book, connec-
tions between different books vs.
I don’t see if I’m close to other
users, I miss visualization of my
similarity with other users
How to find the book character-
istics?, I lack an idea of which
types of books are similar to
each other, thicker links if books
are of the same kind, links be-
tween books and themes vs. both
this user and I did not like this
book
[About books] Do you consider
classics? Where do authors come
from? Did the books receive
prizes?
containing concise item-based sentences stating why this recom-
mendation was made. This choice was influenced by recent studies
indicating that text explanations, specifically with item-based word-
ing, were perceived more persuasive than other visual formats [25]
and were effective in providing the user with personalized feel-
ing, which is correlated with trust in the system [56]. Finally, we
improved the Graph design introduced in Figure 1 based on partici-
pants’ feedback from our qualitative study. Specifically, we focused
on item characteristics by projecting the initial bipartite graph
into an item-oriented one. In this representation, each book is a
node, and edges connect books that share common readers in our
database. Additionally, we incorporated information about book
characteristics (literary genre, author, publication year, etc) through
node colors. Consequently, this final graph representation aligns
more closely with users’ expectations. We illustrate these designs
in Figure 2.
4.1.3
Experimental conditions. Overall, 66 participants were re-
cruited through two channels: (i) students, doctoral students and
researchers from the author’s university and affiliated laboratories,
and (ii) users of the Reddit forum “SampleSize”1 dedicated to user
studies. All participants were asked to fill out an online form divided
into two parts. First, as in the previous study, participants answered
preliminary questions about their perceived expertise level toward
AI-based systems in general and AI-based recommender systems
in particular. We ended up with 11 non-Experts (16.7%), 32 Insid-
ers (48.5%) and 23 Experts (34.8%). We then asked each participant
to choose their favorite book selection from among 5 sets of 10
books and used our AI-based recommender system to suggest a
new book. Each participant was shown all explanation designs for
this recommendation (in a random order), and for each design, they
were asked to answer a set of questions related to our evaluation
1https://www.reddit.com/r/SampleSize/
measures. Finally, we collected participants’ design preferences by
explicitly asking them to rank the three designs. The whole study
lasted around 10 minutes.
In summary, this study is constructed using a Mixed Factorial
design, involving the participants’ expertise level as a between-
subject variable and the explanation design as a within subject
variable. Therefore, in the following analysis, we assess three effects:
(i) the influence of the within-subject variable on the measures, (ii)
the influence of the between-subject variable on the measures, and
(iii) the effect of the interaction between the two factors on the
measures.
4.1.4
Evaluation constructs. The same set of questions was used
for each explanation design. These questions are built according
to previous literature on explanation evaluation within AI-based
frameworks [4, 21, 39, 42] and are shaped to fit our specific exper-
imental design. All questions have the same structure; given an
explanation design, we ask the participant “Please evaluate your
level of agreement with the following sentences.”. Each participant
answers using a 5-point Likert scale ranging from Strongly disagree
to Strongly agree. We organized the study around four global con-
structs (detailed below). We verified the internal validity of the
questionnaire by computing Pearson correlations between answers
within each construct and the total of each construct (at this stage
we discarded results from one question), and its internal reliability
using McDonald’s 𝜔[30]. Detailed questions and corresponding
categories are provided in Table 2.
Subjective understanding. We measured subjective under-
standing by asking participants if they understood the recommen-
dation made by the system or if they were able to derive insights
about its internal mechanics.
5

(a) Text-based explanation. Item-based wording is chosen accord-
ingly with results from [25].
(b) Graph-based explanation. Nodes represent the books read by the user,
except for the black-circled node which corresponds to the recommendation.
Node colors denote similarities between book characteristics. A link between
two books exists if they both have been read by some user. Thickness of
links stands for the number of common readers for two books.
(c) SHAP-based explanation [28]. Recommendation is explained through feature importance.
Figure 2: Explanation designs used in the second user study.
Objective understanding. We measured objective understand-
ing of the recommendations by asking participants about the fea-
tures used by the system to provide the recommendation. These
questions were specifically designed for the study but encompass
different recommender systems elements that have been empha-
sized in our first study as well as in previous studies from the
literature [4, 25], e.g. item-based influence, user-based influence or
self-historical information influence.
Usability. Usability was measured by asking participants how
difficult the explanation was to read and understand. We also asked
participants about how easily other people would learn to read this
explanation [2].
Curiosity. We measured the curiosity induced by the explana-
tion design using questions adapted from [21, 39]. We asked the
participants if they were curious to know why the recommender
system did not provide another suggestion, and if the recommen-
dation incited curiosity.
4.2
Results
We used a repeated-measure analysis of variance (RM-ANOVA)
to analyze the collected measures for each participant. Each mea-
sure consisted in the average of the ratings for the corresponding
questions. All measures passed the sphericity assumption (con-
stant variance across repeated measure), either using the traditional
Mauchly’s test or after applying Greenhouse-Geisser correction.
We confirmed the homogeneity of variance for all levels of the re-
peated measures using Levene’s test. When significant effects were
observed, we conducted a post-hoc Bonferroni test for pairwise
comparisons. Results are summarized in Figure 3.
4.2.1
Text rather than graph design for higher objective understand-
ing. We found a statistically significant main effect of explanation
design on participants’ objective understanding (𝑝= .043). Post-
hoc analysis with a Bonferroni adjustment revealed that text-based
design led to significantly higher objective understanding than
graph-based design (𝑝= .046) (see Figure 3a), but that there was no
statistically significant difference for the levels of objective under-
standing between graph and SHAP-based designs, nor between text
and SHAP-based designs. The main effect of the expertise level on
objective understanding was not statistically significant (𝑝= .424),
i.e., if we ignore the explanation design being evaluated, Experts,
Insiders and non-Experts gave similar ratings considering the ob-
jective understanding measure. However, within the Expert group,
objective understanding was statistically significantly higher for
text-based design than for graph-based design (𝑝= .022).
4.2.2
Graph and text designs increase usability. There was a sta-
tistically significant main effect of the explanation design on us-
ability (𝑝< .001), but no statistically significant influence of the
level of expertise on this measure (𝑝= .398). Post-hoc tests with a
6

Table 2: Questions asked to participants for each explanation design, with the 𝑝-value corresponding to the validity test (Person’s
correlation between each question and its corresponding global construct) and the McDonald’s 𝜔score.
Construct
Reliability 𝜔
Validity
Question
Subjective understanding
84.4
𝑝< .001
I understand why this recommendation was made to me.
𝑝< .001
I can figure out the internal mechanics of the recommender system.
Objective understanding
63.5
𝑝< .001
I believe this book was recommended to me because it is similar (literary genre,
author, etc.) to the books I chose previously.
𝑝< .001
The fact that I share preferences with other readers only counted a little by the
algorithm.
𝑝< .001
I believe that this book was recommended to me due to its popularity among
other readers.
Usability
48.2
𝑝< .001
I would imagine that most people would learn to read this explanation very
quickly.
𝑝< .001
I think the recommender system is interpretable.
Usability (not used)
-
𝑝> .05
I found it difficult to read and understand the recommendation context.
Curiosity (0.510)
60.5
𝑝< .001
I am curious about why the recommender system did not make other decisions.
𝑝< .001
The recommended book incited my curiosity.
*
*
(a) Objective understanding
n.s
(b) Subjective understanding
*
*
(c) Curiosity
***
**
**
*
**
(d) Usability
Figure 3: Results for quantitative study. Vertical bars are confidence intervals at 95%. Significance levels are reported as follows:
∗∗∗= 𝑝≤.001, ∗∗= 𝑝≤.01, ∗= 𝑝≤.05, · = 𝑝≤.07 and n.s. non significant. Reading key: Objective understanding (Figure 3a) was
statistically significantly higher (𝑝≤.05) when participants used textual explanation rather than graph explanation.
7

Bonferroni adjustment revealed that SHAP-based design led to sta-
tistically significantly lower usability compared to graph (𝑝= .043)
or text-based (𝑝< .001) designs (see Figure 3d). However, we did
not find any statistically significant difference between graph and
text-based designs (𝑝= .418). Within expertise levels, SHAP-based
design led to statistically significantly lower level of usability than
text-based design, for both Insiders (𝑝= .007) and Experts (𝑝= .006).
Among Experts, usability was also statistically significantly higher
for text-based design than for graph-based design (𝑝= .008).
4.2.3
Higher expertise increases curiosity for graph and text designs.
Our analysis determined that there was no statistically significant
effect of explanation design on participants’ curiosity (𝑝= .514).
However, the expertise level had a statistically significant influence
on curiosity (𝑝= .016). Post-hoc tests with a Bonferroni adjustment
revealed that being Insider led to significantly lower levels of cu-
riosity compared to being Expert (𝑝= .02). Specifically, when faced
with graph-based designs, Experts showed statistically significantly
higher levels of curiosity than Insiders (𝑝= .041). Similarly, when
faced with text-based designs, Experts were significantly more curi-
ous than Insiders (𝑝= .032) (see Figure 3c). However, we did not find
any statistically significant difference in curiosity levels between
non-Experts and other users. Furthermore, we did not find any statis-
tically significant difference in ratings between participants when
using SHAP-based design.
4.2.4
No effect of explanation design or expertise level on subjec-
tive understanding. We did not observe any statistically significant
influence of explanation design on participants’ subjective under-
standing (𝑝= .104). Similarly, the expertise level of participants did
not have any statistically significant influence on their subjective
understanding (𝑝= .185) (see Figure 3b).
4.2.5
Graph-based design is preferred, regardless of the expertise
level. At the end of the questionnaire, participants were asked to
rank explanation designs according to their preferences. A Chi-
Square Goodness of Fit Test was performed to determine whether
the design preferences were equally distributed across the 6 possi-
ble outcomes. Our results revealed that the obtained proportions
significantly differed (𝑝= .015). We display in Figure 4 the number
of occurrences of each design at each ranking position. We show
that participants preferred graph-based design more frequently
than other designs. When graph-based design was not ranked in
first position, it was ranked in second position more often than
in third position. The second best-ranked design was text-based
explanation. SHAP-based explanation was ranked in last position
more than 35 times. Finally, we explored the relationship between
participants’ expertise level and design preference. We ran a Chi-
Square test and did not find any statistically significant evidence of
correlation between the two variables (𝑝= .841).
5
DISCUSSION
5.1
Graph vs. textual explanations
According to Miller [31], a “good” explanation should be contrastive,
selected, dialogic and causal. We argued that graphs inherently
model causal and contrastive relationships between entities through
their node-link structure. They achieve the selected criterion by
Rank #1
Rank #2
Rank #3
0
5
10
15
20
25
30
35
Count
Design
Graph
SHAP
Text
Figure 4: Number of occurrences of each design at each
ranking position. Reading key: Graph-based design has been
ranked 29 times in #1 position, 24 times in #2 position and 13
times in #3 position.
potentially representing a subset of the original data, e.g. in the
vicinity of a recommender system’s prediction. Finally, the node or
link attributes they may possess contribute to them being consid-
ered dialogic. Consequently, we aligned with previous studies that
acknowledged the strong expressive power of graphs and the fact
that they were especially well-suited for applications like recom-
mender systems [1].
Initial findings from our qualitative study affirmed these state-
ments and emphasized the potential effectiveness of graph-based ex-
planations. With the exception of one Expert who preferred textual
explanations, participants found graph-based design “very easily
interpretable” and highlighted the importance of showcasing “rela-
tions between groups”. This was further confirmed quantitatively;
when asked for their preferred explanation type, the majority of par-
ticipants in our second study favored graph-based explanation over
text or SHAP designs (see Figure 4). Specifically, users expressed a
preference for item-centric graph explanations over user-centric
ones, aligning with results from Kouki et al.’s work [24, 25], which
compared textual and visual explanations, although not specifically
graphs.
However, despite their expressed preferences for graph-based de-
sign, our second study revealed that participants had significantly
higher levels of objective understanding and usability when us-
ing textual designs. Such contrast between expressed desiderata
and measured performance aligns with previous research [5, 48]
where authors emphasized users’ preference for visual explanations
(though not specifically graphs) over texts, despite their poorer per-
formance when using the former kind. A plausible explanation
for this tendency may be grounded in the findings of a recent
perception study [33] in which authors showed that interest and
attractiveness of a stimuli can be well predicted by the complex-
ity of this stimuli; in our case, the visual complexity of our graph
explanation, compared to textual explanation could encourage par-
ticipants to prefer this design over others. More recent research [7]
explored this direction in the context of graph designs and further
confirmed the close relationship between complex visual structure
and interest.
Overall, our findings suggest that depending solely on stake-
holders’ expressed desiderata for crafting graph-based explanation
designs may not be sufficient. Considering both their preferences
and measured performance when using such explanations could
8

provide a more comprehensive understanding of what constitutes
a good explanation.
5.2
Impact of expertise level
Despite its enhanced design tailored to the needs of users with
various levels of expertise, our graph explanation did not lead to
significantly higher ratings on the different measures. Nevertheless,
noteworthy insights can be derived from this visual design. For
example, we showed that high expertise level positively influenced
participants’ objective understanding when facing textual expla-
nation vs. graph explanation, but we did not reveal any difference
in ratings for non-Experts. This contrasts with the findings in [48]
showing how lay users performed better with textual explanations
vs. visual explanations. While authors evoke the plausible effects of
confirmation bias, i.e. favoring elements confirming preconceived
ideas, on lay users to explain their results, we did not notice such
behavior in our qualitative study. An interpretation of our results
could be that Experts may be more inclined than non-Experts to
have preconceived notions, or false narratives about the internal
functioning of the system, a behavior that has been particularly
observed for experienced users by previous works [22, 51]. These a
priori may be more contradicted by complex designs, such as SHAP
or graph, compared to relatively vague explanations like the textual
design we proposed.
We also demonstrated higher curiosity levels for Experts com-
pared to Insiders, when using graph and text designs, which aligns
with previous research [35] stating that curiosity is strongly as-
sociated with intrinsic interest and motivation. We hypothesize
that Expert participants are more engaged in the proposed tasks
due to their interest in the topic. However, this tendency is not ob-
served with SHAP-based explanation, warranting further analysis
for comprehensive understanding.
Our analysis finally revealed that both Insiders and Experts con-
sidered text-based design more usable than SHAP-based design, but
only the most experienced participants found text-based explana-
tions more usable than graph-based ones. This could be attributed
to users with moderate expertise being more persuadable by graph-
ical design and experiencing some kind of over-trust [11]. Or it
may be the result of Experts being more inclined to over-analyze
graph explanations to align with their knowledge of recommender
systems and network layouts. Additionally, we did not find any
statistically significant difference between levels of expertise when
measuring graph usability. This observation aligns with previous
research in the biological field [46], showing that novice users could
create and evaluate graph layouts as effectively as domain experts.
In summary, we highlighted how graph-based explanations were
subject to different cognitive biases according to the epxertise level
of the end-users. To address this limit, future work could involve
further inclusion of cognitive factors [45] in explanations to further
enhance perceived designs.
5.3
Limitations
In our quantitative study, we compelled participants to choose from
pre-defined user profiles based on a selection of representative
books. This action further determined the algorithm recommen-
dation and consequently the content of each explanation. While
this approach allows us to use pre-computed answers, which sig-
nificantly simplifies the questionnaire procedure, we acknowledge
that it might hinder participants from fully recognizing their tastes
within the recommender system’s choices, potentially influencing
their design preferences. To address this concern, we verified that
the user profile chosen by participants was not correlated with
their design preferences. For this purpose, we ran a Chi-Square test
between the two variables and the results revealed that there is not
enough evidence to suggest an association between the selected
user profile and design preferences (𝑝= .549).
In this work, we did not impose any time constraint on partici-
pants during their analysis of design, and the only time indication
provided was that the questionnaire should take around 10 min-
utes to complete. Consequently, participants had sufficient time
to explore and analyze each recommendation, which may not en-
tirely replicate a real-world scenario where end users require an
explanation to make a decision. Future research aimed at assessing
the generalizability of our findings could involve time-constrained
tasks tailored for specific contexts.
More generally, the results presented in this work are derived
from data related to book recommendations. This simplistic use
case facilitated the building of the study since it mitigated potential
ethical concerns. While our results may have applicability in other
domains, additional experiments would be necessary to confirm
this.
6
CONCLUSION
In this work, we have conducted a qualitative study to understand
the needs for graph-based explanation designs expressed by users
with various levels of expertise. From these results, we built an en-
hanced graph-based design emphasizing several key recommenda-
tions such as a content-based approach obtained through bipartite
graph projection or the focus on item similarity rather than user-
similarity. We then conducted a quantitative study in which we
investigated the influence of SHAP, textual and graph-based expla-
nation designs regarding four constructs: objective understanding,
subjective understanding, curiosity and usability. Our results re-
vealed that text-based explanations significantly improved objective
understanding to graph-based explanation, and that it was specif-
ically marked for users with a higher level of expertise. On the
other hand, we did not find any statistically significant evidence of
influence of explanation design on subjective understanding. We
also showed that both graph and text explanations were considered
more usable than SHAP-based design, and that it was particularly
true for users with middle to high level of expertise. Moreover, we
highlighted the influence of expertise level on curiosity by showing
how expert users were more curious than insiders toward graph
and textual explanations. Lastly, we emphasized the discrepancy
between participants’ expressed preferences for graph-based ex-
planations during qualitative interviews and further confirmed
quantitatively, and their higher ratings regarding understanding
or usability for textual designs. This outcome suggests that solely
fulfilling stakeholders’ desiderata may not suffice to achieve “good”
explanations. Crafting hybrid designs achieving a balance between
social expectation and effective downstream performance emerges
as a significant challenge.
9

7
ETHICAL CONCERNS
To ensure participants’ privacy, the qualitative study was designed
and approved in collaboration with the Data Protection Officer
(DPO) of the authors’ affiliated school. Anonymized versions of
interview notes for data analysis are securely stored, with access
restricted to the authors. We did not collected any personal infor-
mation for the quantitative study.
ACKNOWLEDGMENTS
REFERENCES
[1] Darius Afchar, Alessandro Melchiorre, Markus Schedl, Romain Hennequin, Elena
Epure, and Manuel Moussallam. 2022. Explainability in music recommender
systems. AI Magazine 43, 2 (2022), 190–208.
[2] Rasheed Omobolaji Alabi, Alhadi Almangush, Mohammed Elmusrati, Ilmo Leivo,
and Antti Mäkitie. 2022. Measuring the usability and quality of explanations
of a machine learning web-based tool for Oral Tongue Cancer Prognostication.
International Journal of Environmental Research and Public Health 19, 14 (2022),
8366.
[3] Basak Alper, Benjamin Bach, Nathalie Henry Riche, Tobias Isenberg, and Jean-
Daniel Fekete. 2013. Weighted graph comparison techniques for brain connec-
tivity analysis. In Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems. ACM, 483–492.
[4] Astrid Bertrand, James R Eagan, and Winston Maxwell. 2023. Questioning the
ability of feature-based explanations to empower non-experts in robo-advised
financial decision-making. In Proceedings of the 2023 ACM Conference on Fairness,
Accountability, and Transparency. 943–958.
[5] Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z Gajos. 2021. To trust or to
think: cognitive forcing functions can reduce overreliance on AI in AI-assisted
decision-making. Proceedings of the ACM on Human-Computer Interaction 5,
CSCW1 (2021), 1–21.
[6] Michael Burch, Weidong Huang, Mathew Wakefield, Helen C Purchase, Daniel
Weiskopf, and Jie Hua. 2020. The state of the art in empirical user evaluation of
graph visualizations. IEEE Access 9 (2020), 4173–4198.
[7] Claus-Christian Carbon, Tamara Mchedlidze, Marius Hans Raab, and Hannes
Wächter. 2018. The power of shape: How shape of node-link diagrams im-
pacts aesthetic appreciation and triggers interest.
i-Perception 9, 5 (2018),
2041669518796851.
[8] Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and
Hongyuan Zha. 2018. Sequential recommendation with user memory networks.
In Proceedings of the eleventh ACM international conference on web search and
data mining. 108–116.
[9] Emilio Di Giacomo, Walter Didimo, Fabrizio Montecchiani, and Alessandra Tap-
pini. 2021. A user study on hybrid graph visualizations. In International Sympo-
sium on Graph Drawing and Network Visualization. Springer, 21–38.
[10] Finale Doshi-Velez and Been Kim. 2018. Considerations for evaluation and
generalization in interpretable machine learning. Explainable and interpretable
models in computer vision and machine learning (2018), 3–17.
[11] Malin Eiband, Daniel Buschek, Alexander Kremer, and Heinrich Hussmann. 2019.
The impact of placebic explanations on trust in intelligent systems. In Extended
abstracts of the 2019 CHI conference on human factors in computing systems. 1–6.
[12] Satu Elo and Helvi Kyngäs. 2008. The qualitative content analysis process. Journal
of advanced nursing 62, 1 (2008), 107–115.
[13] Leonhard Euler. 1741. Solutio problematis ad geometriam situs pertinentis. In
MAA Euler Archive.
[14] European Commission. 21/04/2021. Proposal for a REGULATION OF THE EURO-
PEAN PARLIAMENT AND OF THE COUNCIL LAYING DOWN HARMONISED
RULES ON ARTIFICIAL INTELLIGENCE (ARTIFICIAL INTELLIGENCE ACT)
AND AMENDING CERTAIN UNION LEGISLATIVE ACTS. COM/2021/206 final
(21/04/2021).
[15] Azin Ghazimatin, Oana Balalau, Rishiraj Saha Roy, and Gerhard Weikum. 2020.
PRINCE: Provider-side Interpretability with Counterfactual Explanations in Rec-
ommender Systems. In Proceedings of the 13th International Conference on Web
Search and Data Mining. ACM, 196–204.
[16] Mohammad Ghoniem, J-D Fekete, and Philippe Castagliola. 2004. A comparison
of the readability of graphs using node-link and matrix-based representations. In
IEEE symposium on information visualization. IEEE, 17–24.
[17] Mohammad Ghoniem, Jean-Daniel Fekete, and Philippe Castagliola. 2005. On
the readability of graphs using node-link and matrix-based representations: a
controlled experiment and statistical analysis. Information visualization 4, 2
(2005), 114–135.
[18] Jonathan L. Herlocker, Joseph A. Konstan, and John Riedl. 2000. Explaining col-
laborative filtering recommendations. In Proceedings of the 2000 ACM conference
on Computer supported cooperative work. ACM, 241–250.
[19] Ivan Herman, Guy Melançon, and M Scott Marshall. 2000. Graph visualization
and navigation in information visualization: A survey. IEEE Transactions on
visualization and computer graphics 6, 1 (2000), 24–43.
[20] Denis J Hilton. 1990. Conversational processes and causal explanation. Psycho-
logical Bulletin 107, 1 (1990), 65.
[21] Robert R Hoffman, Shane T Mueller, Gary Klein, and Jordan Litman. 2018. Metrics
for explainable AI: Challenges and prospects. arXiv preprint arXiv:1812.04608
(2018).
[22] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wal-
lach, and Jennifer Wortman Vaughan. 2020. Interpreting interpretability: under-
standing data scientists’ use of interpretability tools for machine learning. In
Proceedings of the 2020 CHI conference on human factors in computing systems.
1–14.
[23] René Keller, Claudia M Eckert, and P John Clarkson. 2006. Matrices or node-
link diagrams: which visual representation is better for visualising connectivity
models? Information Visualization 5, 1 (2006), 62–76.
[24] Pigi Kouki, James Schaffer, Jay Pujara, John O’Donovan, and Lise Getoor. 2017.
User Preferences for Hybrid Explanations. In Proceedings of the Eleventh ACM
Conference on Recommender Systems. ACM, 84–88.
[25] Pigi Kouki, James Schaffer, Jay Pujara, John O’Donovan, and Lise Getoor. 2019.
Personalized explanations for hybrid recommender systems. In Proceedings of
the 24th International Conference on Intelligent User Interfaces. 379–390.
[26] Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kästner,
Eva Schmidt, Andreas Sesing, and Kevin Baum. 2021. What Do We Want From
Explainable Artificial Intelligence (XAI)?– A Stakeholder Perspective on XAI and
a Conceptual Model Guiding Interdisciplinary XAI Research. Artificial Intelligence
296 (2021), 103473.
[27] Zachary C Lipton. 2018. The Mythos of Model Interpretability: In machine
learning, the concept of interpretability is both important and slippery. Queue
16, 3 (2018), 31–57.
[28] Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model
predictions. Advances in neural information processing systems 30 (2017).
[29] Michael McBride and Michael Caldara. 2013. The efficacy of tables versus graphs
in disrupting dark networks: An experimental study. Social Networks 35, 3 (2013),
406–422.
[30] Roderick P McDonald. 2013. Test theory: A unified treatment. psychology press.
[31] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social
sciences. Artificial intelligence 267 (2019), 1–38.
[32] Tim Miller, Piers Howe, and Liz Sonenberg. 2017. Explainable AI: Beware of
inmates running the asylum or: How I learnt to stop worrying and love the social
and behavioural sciences. arXiv preprint arXiv:1712.00547 (2017).
[33] Claudia Muth, Marius H Raab, and Claus-Christian Carbon. 2015. The stream of
experience when watching artistic movies. Dynamic aesthetic effects revealed
by the Continuous Evaluation Procedure (CEP). Frontiers in Psychology 6 (2015),
365.
[34] Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin
Schmitt, Jörg Schlötterer, Maurice van Keulen, and Christin Seifert. 2023. From
anecdotal evidence to quantitative evaluation methods: A systematic review on
evaluating explainable ai. Comput. Surveys 55, 13s (2023), 1–42.
[35] Xi Niu and Ahmad Al-Doulat. 2021. LuckyFind: Leveraging surprise to improve
user satisfaction and inspire curiosity in a recommender system. In Proceedings
of the 2021 Conference on Human Information Interaction and Retrieval. 163–172.
[36] John O’Donovan, Barry Smyth, Brynjar Gretarsson, Svetlin Bostandjiev, and
Tobias Höllerer. 2008. PeerChooser: visual interactive recommendation. In Pro-
ceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1085–
1088.
[37] Mershack Okoe and Radu Jianu. 2015. Graphunit: Evaluating interactive graph
visualizations using crowdsourcing. In Computer Graphics Forum, Vol. 34. Wiley
Online Library, 451–460.
[38] Mershack Okoe, Radu Jianu, and Stephen Kobourov. 2018. Node-link or adjacency
matrices: Old question, new insights. IEEE transactions on visualization and
computer graphics 25, 10 (2018), 2940–2952.
[39] Heather O’Brien and Paul Cairns. 2015. An empirical evaluation of the User
Engagement Scale (UES) in online news environments. Information Processing &
Management 51, 4 (2015), 413–427.
[40] Georgina Peake and Jun Wang. 2018. Explanation mining: Post hoc interpretabil-
ity of latent factor models for recommendation systems. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
2060–2069.
[41] Phillip E. Pope, Soheil Kolouri, Mohammad Rostami, Charles E. Martin, and Heiko
Hoffmann. 2019. Explainability Methods for Graph Convolutional Neural Net-
works. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR). 10764–10773.
[42] Erasmo Purificato, Baalakrishnan Aiyer Manikandan, Prasanth Vaidya Karanam,
Mahantesh Vishvanath Pattadkal, and Ernesto William De Luca. 2021. Evaluating
Explainable Interfaces for a Knowledge Graph-Based Recommender System.. In
IntRS@ RecSys. 73–88.
10

[43] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. " Why should i
trust you?" Explaining the predictions of any classifier. In Proceedings of the 22nd
ACM SIGKDD international conference on knowledge discovery and data mining.
1135–1144.
[44] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedan-
tam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from
deep networks via gradient-based localization. In Proceedings of the IEEE interna-
tional conference on computer vision. 618–626.
[45] Dajung Diane Shin and Sung-il Kim. 2019. Homo curious: Curious or interested?
Educational Psychology Review 31 (2019), 853–874.
[46] Divit P Singh, Lee Lisle, TM Murali, and Kurt Luther. 2018. Crowdlayout: Crowd-
sourced design and evaluation of biological network visualizations. In Proceedings
of the 2018 CHI Conference on Human Factors in Computing Systems. 1–14.
[47] Rashmi Sinha and Kirsten Swearingen. 2002. The role of transparency in rec-
ommender systems. In CHI’02 extended abstracts on Human factors in computing
systems. 830–831.
[48] Maxwell Szymanski, Martijn Millecamp, and Katrien Verbert. 2021. Visual, tex-
tual or hybrid: the effect of user expertise on different explanations. In 26th
International Conference on Intelligent User Interfaces. 109–119.
[49] Nava Tintarev. 2007. Explanations of recommendations. In Proceedings of the
2007 ACM conference on Recommender systems. 203–206.
[50] M. Tory and T. Moller. 2004. Human factors in visualization research. IEEE
transactions on visualization and computer graphics 10 (2004), 72–84.
[51] Amos Tversky and Daniel Kahneman. 1974. Judgment under Uncertainty: Heuris-
tics and Biases: Biases in judgments reveal some heuristics of thinking under
uncertainty. science 185, 4157 (1974), 1124–1131.
[52] Peng Wang, Renqin Cai, and Hongning Wang. 2022. Graph-based Extractive
Explainer for Recommendations. In Proceedings of the ACM Web Conference 2022.
ACM, 2163–2171.
[53] Xiang Wang, Dingxian Wang, Canran Xu, Xiangnan He, Yixin Cao, and Tat-Seng
Chua. 2019. Explainable reasoning over knowledge graphs for recommendation.
In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 5329–5336.
[54] Daniel A Wilkenfeld. 2014. Functional explaining: a new approach to the philos-
ophy of explanation. Synthese 191, 14 (2014), 3367–3391.
[55] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
2019. Gnnexplainer: Generating explanations for graph neural networks. Ad-
vances in neural information processing systems 32 (2019).
[56] Jingjing Zhang and Shawn P Curley. 2018. Exploring explanation effects on
consumers’ trust in online recommender agents. International Journal of Human–
Computer Interaction 34, 5 (2018), 421–432.
[57] Shichang Zhang, Jiani Zhang, Xiang Song, Soji Adeshina, Da Zheng, Christos
Faloutsos, and Yizhou Sun. 2023. PaGE-Link: Path-based Graph Neural Network
Explanation for Heterogeneous Link Prediction. In Proceedings of the ACM Web
Conference 2023. ACM, 3784–3793.
[58] Yongfeng Zhang and Xu Chen. 2020. Explainable Recommendation: A Survey
and New Perspectives. Foundations and Trends® in Information Retrieval 14, 1
(2020), 1–101.
[59] Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang, Yiqun Liu, and Shaoping
Ma. 2014. Explicit factor models for explainable recommendation based on
phrase-level sentiment analysis. In Proceedings of the 37th international ACM
SIGIR conference on Research & development in information retrieval. 83–92.
11

(a) Participants are asked to select a user profile based on a selection of
books.
(b) Participants are reminded of their profile choice and are recom-
mended a new book.
(c) Example of explanation design description (here graph-based design.)
Figure 5: Example of a participant’s progression through the questionnaire.
12

