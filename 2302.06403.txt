Sources of Richness and Ineﬀability for
Phenomenally Conscious States
Xu Ji∗1,2, Eric Elmoznino∗1,2,
George Deane2, Axel Constant3, Guillaume Dumas2, Guillaume
Lajoie1,2, Jonathan Simon2, and Yoshua Bengio1,2,4
1Mila
2University of Montreal
3University of Sussex
4CIFAR Fellow
February 14, 2023
Abstract
Conscious states—states that there is something it is like to be in—
seem both rich or full of detail, and ineﬀable or hard to fully describe or
recall. The problem of ineﬀability, in particular, is a longstanding issue
in philosophy that partly motivates the explanatory gap: the belief that
consciousness cannot be reduced to underlying physical processes. Here,
we provide an information theoretic dynamical systems perspective on the
richness and ineﬀability of consciousness. In our framework, the richness
of conscious experience corresponds to the amount of information in a
conscious state and ineﬀability corresponds to the amount of information
lost at diﬀerent stages of processing. We describe how attractor dynamics
in working memory would induce impoverished recollections of our original
experiences, how the discrete symbolic nature of language is insuﬃcient for
describing the rich and high-dimensional structure of experiences, and how
similarity in the cognitive function of two individuals relates to improved
communicability of their experiences to each other. While our model may
not settle all questions relating to the explanatory gap, it makes progress
toward a fully physicalist explanation of the richness and ineﬀability of
conscious experience—two important aspects that seem to be part of what
makes qualitative character so puzzling.
∗Equal contribution.
1
arXiv:2302.06403v1  [q-bio.NC]  13 Feb 2023

Contents
1
Introduction
3
2
Preliminaries: Computation through neural dynamics
5
2.1
Neural activation state space
. . . . . . . . . . . . . . . . . . . .
5
2.2
Neural dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.3
State attractors . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.3.1
Attractors are mutually exclusive: contractive dynamics
discretize the state . . . . . . . . . . . . . . . . . . . . . .
10
2.3.2
Emergent attractors in task-optimized networks . . . . . .
10
3
A dynamical systems model of conscious experience
12
3.1
Motivating attractor dynamics as a model for conscious experience 12
3.1.1
Working memory . . . . . . . . . . . . . . . . . . . . . . .
12
3.1.2
Stability and robustness of conscious states . . . . . . . .
13
3.2
Richness and ineﬀability . . . . . . . . . . . . . . . . . . . . . . .
14
3.3
Intra-personal ineﬀability
. . . . . . . . . . . . . . . . . . . . . .
19
3.3.1
Information loss from attractor dynamics
. . . . . . . . .
20
3.3.2
Information loss at verbal report . . . . . . . . . . . . . .
22
3.3.3
Hierarchical attractor dynamics . . . . . . . . . . . . . . .
23
3.4
Inter-personal ineﬀability
. . . . . . . . . . . . . . . . . . . . . .
25
3.4.1
A blank-slate listener
. . . . . . . . . . . . . . . . . . . .
26
3.4.2
A typical listener . . . . . . . . . . . . . . . . . . . . . . .
28
3.5
Phenomenal and access consciousness
. . . . . . . . . . . . . . .
31
3.5.1
Eﬀability, accessibility, reportability
. . . . . . . . . . . .
31
3.5.2
Existence and report of phenomenal experience . . . . . .
31
4
Conclusion
33
2

1
Introduction
Conscious states—states that there is something it is like to be in (Nagel,
1974)—present many apparent contradictions. On the one hand, every time
we have a thought, look out at the world, or feel an emotion, we have a rich
experience that seems impossible to fully describe. At the same time, conscious
experiences are conceptualizable, with similar properties across individuals, and
can often be communicated with a degree of ﬁdelity.
This paper provides an information theoretic dynamical systems perspective
on how and why consciousness may appear to us the way it does, namely as
both rich or full of detail, and ineﬀable or hard to fully describe or recall—in
other words, why it seems that an experience is “worth a thousand words”.
In addition, a dynamical systems model for consciousness oﬀers an explanation
for why much of the conscious content that is reportable has a discrete nature
that can be expressed with words. Our key contention is that these aspects of
consciousness are implicated by a dynamical systems model of neural process-
ing, in particular by “attractors”: patterns of joint neural activity that remain
relatively stable over short timescales and yield a discrete partition over neural
states. Importantly, interpreting cognitive processing through the lenses of dy-
namical systems and information theory will give us the ability to reason about
richness, ineﬀability, and communicability in general terms, without relying on
implementation details of the neural processes that may give rise to conscious-
ness. Broadly, the suggestion is that the rather abstract level of explanation
aﬀorded by dynamic systems theory is the commensurate level of explanation
for some key questions about richness and ineﬀability.
By “consciousness” we mean phenomenal consciousness, i.e. the felt or sub-
jective quality of experience. A state is phenomenally conscious when, in the
words of Nagel (1974), there is something it is like to be in that state. Phenom-
enal consciousness is the form of consciousness that gives rise to what Joseph
Levine calls the “explanatory gap” (Levine, 1993) and what David Chalmers
calls the “hard problem of consciousness” (Chalmers, 1996): the problem of
showing that phenomenal consciousness can be explained in terms of, or re-
duced to, underlying physical processes.
The explanatory gap is one of the
central problems in the philosophy of mind, and it relies heavily on the in-
tuition that “physicalist theories leave out [phenomenal consciousness] in the
epistemological sense, because they reveal our inability to explain qualitative
character in terms of the physical properties of sensory states.” (Levine, 1993).
Here, we address one aspect of this problem by developing a structural/mech-
anistic explanation of the richness and ineﬀability of conscious experience, one
that is given entirely in terms of information processing in a dynamical system
such as the brain. Our model assumes that conscious experiences are derived
from neural processes according to known physical laws, and can therefore be
understood using the standard methods of cognitive neuroscience. While our
model may not settle all questions relating to the explanatory gap, it will make
progress toward a fully physicalist explanation of the richness and ineﬀability
of conscious experience—two important aspects that seem to be part of what
3

makes qualitative character so puzzling. Richness and ineﬀability ﬁgure in sev-
eral important live debates about consciousness in the philosophical literature.
Here we summarize two: the illusionism debate and the overﬂow debate.
Illusionists argue that consciousness is an illusion, while realists deny this
(Frankish, 2016).
Illusionists generally argue that our expectations for con-
sciousness are too high: that the job of describing a conscious experience is too
demanding for any physical process to fulﬁll, and that (rather than rejecting
physicalism) we should conclude that there is no such thing as consciousness (or
at least, make do with a diminished conception of it) (Dennett, 1993; Graziano
et al., 2020; Humphrey, 2020). Daniel Dennett famously lists ineﬀability as one
of the hard-to-fulﬁll conditions that should lead one to illusionism: the prospect
that conscious contents somehow escape our attempts to fully describe them is,
for Dennett, a sign that consciousness is chimerical (Dennett, 1993). Notably,
illusionists acknowledge that something gives rise to the relevant illusions: there
must be an explanation of why it seems plausible to us, on introspection, that
we are the subjects of (ineﬀable) conscious states. Qualia realists, in contrast,
see conscious experience as the subjective viewpoint from which all else is ob-
served or known, and therefore consider it to be an explanandum that cannot
be discarded (Chalmers, 2010; Descartes, 1986; Tononi and Edelman, 1998).
The overﬂow debate is between those who hold that consciousness is indeed
rich and ineﬀable, and those who deny it (while still maintaining that conscious-
ness exists). Richness is a relative term, and one contender for a reference object
that justiﬁes the characterization of consciousness as rich is the accessible con-
tent of working memory. Empirically there appears to be a clear bandwidth
limitation on the latter (Cohen et al., 2016; Miller and Buschman, 2015; Sper-
ling, 1960), which is what makes it diﬃcult, for example, to remember all of the
names of the people you meet at a party or all of the digits of a phone number.
Proponents of overﬂow say that consciousness is considerably richer than this
sort of working memory and includes ineﬀable content unavailable for report
(Block, 2007; Bronfman et al., 2019; Lamme, 2007; Vandenbroucke et al., 2012),
while the staunchest opponents of overﬂow will maintain that consciousness is
no richer than the bandwidth-restricted content of working memory, generally
because they take consciousness to just be working memory or a supporting
system for it (Cohen and Dennett, 2011; Naccache, 2018; Phillips, 2016; Ward,
2018).
We thus have two important debates where those on both sides may beneﬁt
from a formal model of ineﬀability: illusionists and realists who deny overﬂow
may beneﬁt from a general model of why it seems to us that we are the sub-
jects of rich and ineﬀable experiences, while realists who accept overﬂow may
beneﬁt from a characterization of how it emerges (that is independent of the
interpretation of contentious cases such as Sperling 1960).
The aim of this paper is to propose and justify a formal description of how
neural dynamics could give rise to the ordinary sense of richness and ineﬀability
in the brain. We provide a framework in which brain dynamics are cast as infor-
mation processing functions, where richness of conscious experience corresponds
to the amount of information in a conscious state and ineﬀability corresponds
4

to the amount of information lost at diﬀerent stages of processing. We describe
how attractor dynamics in working memory would induce impoverished recollec-
tions of our original experiences, how the discrete symbolic nature of language
is insuﬃcient for describing the rich and high-dimensional structure of experi-
ences, and how similarity in the cognitive function of two individuals relates to
improved communicability of their experiences to each other.
The paper is structured as follows. In Section 2, we introduce key concepts
on computation and neural dynamics, in particular the role of attractor states
that can be used for computations involving short-term memory and have a
dual discrete and high-dimensional nature. We present our dynamical systems
model of conscious experience in Section 3, beginning with Section 3.1 which
motivates the use of attractor dynamics for modeling conscious processing using
prior arguments from the literature that are independent of our own, includ-
ing evidence for the Global Workspace Theory (Baars, 1993, 2005; Dehaene
et al., 1998). Section 3.2 formalizes the notions of richness and ineﬀability using
both Shannon information theory (Shannon, 1948) and Kolmogorov complexity
(Kolmogorov, 1965), which play a central role in making our later arguments
precise. Core contributions are presented in Sections 3.3 and 3.4, which discuss
various sources of ineﬀability in conscious experience and explain the conditions
under which these experiences can be partially communicated to others. We
then brieﬂy discuss the implications of our model on the debate surrounding
‘phenomenal’ vs. ‘access’ consciousness (Block, 1995), before concluding with a
high-level discussion in Section 4.
2
Preliminaries:
Computation through neural
dynamics
In Section 3, we will argue that we can account for the richness and ineﬀability
of experience by modeling conscious states as neural trajectories in a high-
dimensional dynamical system with attractors. To do so, we will now provide a
brief overview of the essential concepts needed to understand the model.
First, we will introduce the notion of a neural activation space, in which
temporally evolving states of neural activity follow trajectories governed by
recurrent dynamics in the brain. Next, we will explain how state attractors,
which are emergent properties of dynamical systems, can allow neural networks
to solve computational problems that require some form of persistent memory.
Along the way, we will highlight key examples from the computational neuro-
science literature where this dynamical systems framework was used to explain
how populations of neurons solve perceptual and cognitive tasks.
2.1
Neural activation state space
At any given moment, every neuron in the brain has some level of activity, and
this activity can be numerically quantiﬁed in several diﬀerent ways (e.g., ﬁring
or not, ﬁring rate over some time window, membrane voltage, etc.), which we
5

illustrate in Fig. 1a. Together, this instantaneous pattern of activity deﬁnes
the brain’s current state, which may be compactly represented as a vector in
an N-dimensional state space, where N is the number of neurons in the brain
(or in the subpopulation of interest). In such a representation, each index in
this vector identiﬁes a particular neuron, and the value of a particular index
corresponds to that neuron’s current level of activity (Fig. 1b). We reason at
the level of neuronal activity for clarity, but strictly our framework makes no
assumptions about the appropriate level of granularity: where these make direct
contributions to cognitive information processing, other cells such as astrocytes
or cell components such as dendrites may be state-space parameters in their
own right (Godfrey-Smith, 2016).
A beneﬁt of describing neural activity in this manner is that it allows us
to draw on the mathematical framework of dynamical systems theory to reason
about mental states. For example, we can now talk about what a pattern of neu-
ral activity represents by projecting the state onto lower-dimensional subspaces
that encode some meaningful feature. To explain this by example, it might be
the case that when perceiving an object certain dimensions of the elicited state
represent its color, others represent its shape, yet others represent its function,
etc. In addition, given a probabilistic transition model for states that accounts
for noise in neural activity and other sources of uncertainty, we can measure
quantities such as the likelihood and information content of a state. We can
also quantify the similarities between states according to some distance metric
between their vectors.
Figure 1: Visualization of neural state space. A. The activity trace for multiple
neurons, where activity can be quantiﬁed in several diﬀerent ways (e.g., ﬁring or not,
ﬁring rate over some time window, membrane voltage, etc.). Colored boxes denote
joint activity patterns across all neurons at speciﬁc timepoints. B. At any particular
timepoint, the joint activity pattern across N diﬀerent neurons can be expressed as a
vector in an N-dimensional state space.
2.2
Neural dynamics
While neural states can be used to represent an instantaneous pattern of activity,
the brain is a complex dynamical system and must ultimately be understood in
6

terms of how neural activity unfolds in time. The temporal evolution of neural
activity—and any other dynamical system—is governed by two factors.
First, neurons in the brain have a large number of synapses that form recur-
rent loops. Recurrency means that even in the absence of any sensory input,
brain states will evolve dynamically; the activity of one neuron at a particular
time will inﬂuence the future activity of surrounding neurons, which may in
turn inﬂuence the original neuron’s activity at a later time in a causal loop.
The dynamics governing these neural state trajectories are deﬁned by the joint
synaptic connectivity proﬁle between all neurons in the brain. Any given con-
nectivity proﬁle results in a set of rules for how each state transitions to the
next. This can be visually illustrated for the entire system using a vector ﬁeld
as shown in Fig. 2a: each vector indicates how a state at that location would
evolve in the next instant in the absence of noise, and where the size of the
vector denotes the speed of the change. Intuitively, one can understand the
dynamics of the system by starting oﬀat an initial point in neural state space
and tracing a trajectory that follows the vector ﬁeld at each point in time. A
diﬀerent connectivity proﬁle would yield diﬀerent transition dynamics (i.e., a
diﬀerent vector ﬁeld), and therefore the same initial neural state would follow a
diﬀerent trajectory.
Another factor that governs neural dynamics is the input to the system,
which may itself evolve over time. The dynamics of a sub-population of neurons
(e.g., a particular brain region) are modulated extrinsically by signals from
surrounding neurons that synapse onto the population, including information
from the stream of sensory signals entering the brain. Illustrated visually in
Fig. 2b, this means that inputs warp the vector ﬁeld that deﬁne transitions from
the current state to the next, ultimately resulting in potentially very diﬀerent
trajectories from those that would have occurred given other inputs.
Figure 2: Neural dynamics and trajectories in activation space. A. A dynam-
ical system whose behavior is depicted using vector ﬁelds and example trajectories.
B. External inputs can modulate the behavior of a dynamical system (compare vec-
tor ﬁelds and trajectories with those in Panel A). C. An example of neural dynamics
empirically observed in the primate motor cortex. As the neural dynamics are high-
dimensional, jPCA was used to reduce their dimensionality for visualization.
The
ﬁgure was reproduced with permissions from Churchland et al. (2012).
Much of the ﬁeld of computational neuroscience is concerned with under-
7

standing neural population coding through the lens of dynamical systems, thanks
to their rich theoretical underpinnings and the mechanistic models they provide
(Favela, 2021). Historically, this approach has been particularly fruitful in two
systems: sensory integration (Burak, 2014; Zhang, 1996) and motor control
(Churchland et al., 2012; Michaels et al., 2016; Shenoy et al., 2013). For ex-
ample, Churchland et al. (2012) recorded from a population of neurons in the
primate motor cortex and found that they exhibited rotational dynamics during
a simple reaching task (Fig. 2c). While this was initially surprising because the
movement itself was not rhythmic, the authors proposed a theory that mus-
cle activity is constructed from an oscillatory basis, which was later supported
by additional experiments. The neural dynamics, then, can be understood as
pattern generators that generate sequences of muscle activity optimized for pro-
ducing natural movements.
Despite the success of this framework in sensory and motor domains, much
less is understood about the dynamical underpinnings of higher-level cognition,
although such dynamical systems are also implemented with neural substrates
and would presumably share similar mechanisms. A novel contribution of our
work, then, is the application of dynamical systems to high-level conscious cog-
nition and the implications for explaining the richness and ineﬀability of expe-
rience.
2.3
State attractors
When neural dynamics are used to solve computational tasks, it is often the
case that the solutions require some form of persistent memory, meaning that
at least some projections of the neural activity must be self-sustaining. A dy-
namical system can implement this behavior by forming regions in its state
space where states are drawn towards steady states (Fig. 3a). These regions
are called “basins of attraction” because any state trajectory that enters them
would progress towards the steady state in the absence of noise or changes in
the dynamics (e.g. due to external inputs). By steady states, we mean regions
within the basins that deterministic trajectories eventually converge to. More
generally, these sets of states are called “attractors” because neural activity
trajectories that have reached them remain there until some external input or
intrinsic noise in neural activity nudges the state suﬃciently to escape the at-
tractor basin. In general, dynamical systems can produce attractors that have
complex and high-dimensional structure within the basin (e.g. manifold, frac-
tal structure) and can exhibit their own internal dynamics, as in is the case
of chaotic attractors (also called ”strange”). Other common attractors contain
fewer points, such as stable periodic orbits, or stable ﬁxed points—single state
points that do not change in time. In this section we will focus on ﬁxed point
attractors for simplicity, but arguments in subsequent sections apply to the
general case of attractor subspaces. The important aspect of attractors for our
purposes is that they are distinct and have non-overlapping basins of attraction.
Since trajectories that have converged to attractors have a tendency to re-
main there in the absence of strong external inputs, attractors can endow a
8

dynamical system with a form of self-sustaining memory over short timescales
that is useful for performing many computations essential to real-world tasks.
Attractor dynamics can also be used for eﬃcient long-term memory, without
the brain having to directly store the high-dimensional vectors of the attractors
in state space. As we will explain in Section 2.3.1, attractors are mutually ex-
clusive and thus have a discrete structure; they can be identiﬁed with symbols
(e.g., words) that label which attractor the system is in without describing the
attractor’s location in state space. The system could thus store a concise symbol
in long-term memory rather than a high-dimensional vector. Afterwards, the
memory could be retrieved by using the symbol as an input ‘key’ that drives the
state to any location in the basin of the attractor, at which point the dynamics of
the system will cause the trajectory to converge to the attractor. For example,
to memorize an image of a face (represented by a high-dimensional vector) and
associate it with a discrete entity like the name of a person, a learning process
could update the parameters of the dynamical system, so that the image vector
is an attractor state and system enters its basin of attraction when the name
(or rather a neural code for it) is provided as an input.
It is important to emphasize that the existence of these attractors and the
particular properties they have (e.g., cardinality, location, shape, etc.)
are
purely functions of the internal dynamics of the system.
Neural networks
are therefore particularly well-suited for implementing diverse computations
through dynamical systems since they are composed of simple units whose con-
nectivity can be ﬂexibly tuned to achieve many possible complex attractor con-
ﬁgurations, with the capacity for universal function approximation in the limit
of large networks (Sch¨afer and Zimmermann, 2007).
A dynamical system can be modulated by external inputs, therefore the na-
ture of its attractors can also be driven by contextual signals. In the human
brain, for example, this context could include both external sensory input and
the content of short- and long-term memory. In particular, the previous con-
tent of working memory (which is a part of short term memory) might have a
strong inﬂuence, so that our thoughts form coherent sequences and so that we
can alternate between mutually exclusive interpretations of the world that are
compatible with the context (e.g., ﬂipping between diﬀerent interpretations of
the Necker cube—an ambiguous 2D line drawing of a cube that can be in one
of two possible 3D orientations).
As was summarized in review articles by Rolls (2010) and Khona and Fiete
(2022), the framework of attractor dynamics has been used to mechanistically
explain the neural computations underlying decision-making (Wang, 2002, 2008;
Wong and Wang, 2006), long-term memory (Chaudhuri and Fiete, 2019; Hop-
ﬁeld, 1982; Ramsauer et al., 2020), working memory (Barak and Tsodyks, 2014;
Curtis and D’Esposito, 2003; Deco and Rolls, 2003; Durstewitz et al., 2000;
Seeholzer et al., 2019), and the performance of simple cognitive tasks (Driscoll
et al., 2022).
Attractors have also been observed empirically across several
experiments investigating decision-making (Kurt et al., 2008; Lin et al., 2014;
Stevens, 2015) and working memory (Constantinidis et al., 2001; Curtis and
D’Esposito, 2003; Gnadt and Andersen, 1988).
9

2.3.1
Attractors are mutually exclusive: contractive dynamics dis-
cretize the state
An important property of attractors is that they are mutually exclusive: each
attractor a is associated with a basin of attraction B(a), which is the region
in state-space such that any state x in B(a) will necessarily converge through
the dynamics into a, in the absence of noise or external perturbations. This
division into mutually exclusive basins of attraction thus creates a partition of
the state space: one can associate to any state x the attractor a corresponding
to the basin of attraction B(a) in which x falls.
As a consequence of this mutual exclusivity, any attractor a has a dual
discrete and continuous nature: the symbol or composition of symbols i(a) (e.g.,
words or sentences) that identify a among all the other possible attractors in
the current dynamics is discrete, while a ﬁxed point a is associated with a real-
valued vector (also called embedding (Bengio et al., 2000; Morin and Bengio,
2005; Roweis and Saul, 2000) in the deep learning literature) corresponding to
the state of the system at that ﬁxed point. If the dynamics are not attractive
over all dimensions, the same statement can be made for the subspace that
is attractive, which means that this discretization eﬀect need not cover every
possible dimension and non-discretized dimensions may represent values in a
continuous space.
Note that introducing randomness in the dynamics makes it possible to
sample one of the attractors that may be reachable from the current state when
that noise is taken into consideration. For example, if the state x is close to the
boundary between basins of attraction of attractors A and B, a small amount
of additive noise would suﬃce to stochastically sample one destination or the
other, with probabilities that would vary depending on how far x is from the
boundary and the speciﬁc dynamics in its area (for instance, basin depth or
slope).
2.3.2
Emergent attractors in task-optimized networks
To demonstrate how attractors naturally emerge as solutions to cognitive tasks,
we brieﬂy summarize relevant results from Sussillo and Barak (2013), where an
artiﬁcial recurrent neural network (RNN) was trained to solve a simple mem-
ory task. An RNN is a network of artiﬁcial neurons which can be connected
through recurrent feedback loops. Neurons can also form connections to spe-
cial input and output units, which allow the network to interface with a task.
The connection strength between each directed pair of neurons is parameterized
using a scalar weight that modulates the degree to which activity in the ﬁrst
neuron drives future activity in the second, and these weights are optimized
in order to minimize error on the task. Like the brain, RNNs have recurrent
connections between neurons that deﬁne a dynamical system optimized to per-
form some computation, and are therefore useful models for studying emergent
neural dynamics.
10

Sussillo and Barak (2013) train an RNN on the 3-bit ﬂip-ﬂop task (Fig. 3b),
in which the network must learn to continuously output the sign (+1 or −1) of
the last binary spike across 3 input channels (which we can call the “red”,
“green”, and “blue” channels).
For instance, following the input sequence
[red=+1, green=-1, blue=+1], the correct output should be the vector {red=+1,
green=-1, blue=+1}. If the next input spike was red=-1, the new output would
change to {red=-1, green=-1, blue=+1}. Importantly, while each input spike
only has a short duration, the network must continuously output the value of
each channel’s last spike, which imposes a memory demand.
When Sussillo and Barak (2013) inspected the learned dynamics of the RNN,
they found that it solved the task through the use of ﬁxed point attractors. Since
the number of possible outputs is 23 = 8, the model represented each of these
using an attractor.
Due to their stability, the model was then able to con-
tinuously read out from whichever attractor the trajectory had most recently
converged to. Whenever a new spike appeared in one of the input channels (with
a value diﬀerent from that channel’s previous spike), the state escaped the cur-
rent basin of attraction and followed transient dynamics towards the attractor
for the new output. This simple task demonstrated how attractor dynamics can
naturally emerge in neural networks and implement nontrivial computations,
such as those involving transitions between discrete memory states.
Figure 3: Attractor dynamics in neural networks. A. Attractors in a 2D state
space.
When a trajectory enters an attractor’s basin, it begins to converge to the
attractor and remains there until suﬃcient external input or intrinsic noise allows it
to escape. B. Sussillo and Barak (2013) train an artiﬁcial recurrent neural network
to solve the 3-bit ﬂip-ﬂip memory task. In this task, the model must continuously
output the sign of the most recent binary spike on 3 separate input channels.
C.
Fixed point attractors emerge in the learned dynamics of a recurrent neural network
(RNN) as a solution to the task. Each of the attractors corresponds to one of the 8 (23)
possible bit conﬁgurations, providing a stable memory state from which the output
can be continuously read out. The plot shows a trajectory in the RNN’s state-space
for changing inputs, where points along the trajectory are colored according to the
correct output. The dimensionality of the state space was reduced using Principal
Component Analysis (PCA) for visualization.
11

3
A dynamical systems model of conscious ex-
perience
The main contribution of our paper will be to argue that a dynamical systems
model of consciousness with state attractors can account for the communicable,
rich, and ineﬀable aspects of experience that we discussed in Section 1. Through-
out this section, we will use an information theoretic perspective to characterize
richness as information, ineﬀability as information loss, and communicability as
information retention. We will deploy both the notions of Shannon information
and Kolmogorov complexity (Kolmogorov, 1965) (though for reasons we explain,
we will make heavier use of the latter than the former). We will illustrate how
information loss arises from dimensionality reduction implemented by attractor
dynamics. We show how our model links the problem of accounting for ineﬀa-
bility to the Global Workspace Theory, which predicts that only representations
with suﬃcient ampliﬁcation and temporal duration (i.e., attractor states) can
be broadcast to the rest of the brain for downstream verbal-behavioral report-
ing. Finally, we will generalize the notion of ineﬀability by discussing multiple
forms of information loss in intra-personal and inter-personal communication
pathways, going beyond the speciﬁc case of information loss between working
memory and verbal-behavioral reporting.
To contextualize our argument, we begin by drawing on existing work to
highlight several connections between state attractor models and conscious ex-
perience.
3.1
Motivating attractor dynamics as a model for con-
scious experience
3.1.1
Working memory
The contents of working memory are typically considered to be the attended
contents of short term memory: a projection of short term representations held
in the brain given context from task information or other executive functioning
objectives (Cowan, 2008; Engle, 2002). A central claim in many leading theo-
ries of consciousness is that what we are consciously aware of is the contents
of working memory. For example, the Global Workspace Theory (Baars, 1993,
2005) and its neuronal extension (Dehaene et al., 1998) state that information
becomes conscious by gaining entry into a limited workspace that serves as a
bottleneck for the distributed activity present across the brain. Pairs of brain
regions are largely isolated from each other and arbitrary point-to-point com-
munication is only possible via the workspace, which itself can both receive and
broadcast information globally. The workspace, then, serves as a hub capable
of coordinating brain-wide activity for centralized control and decision-making.
It is easy to see the connection between the concepts of a global workspace and
working memory (attentional selectivity, inﬂuence on executive decision-making,
availability to verbal and behavioral reporting processes, limited capacity, ar-
bitrary modalities) and there is little distinction between them in the Global
12

Workspace Theory (Dehaene and Naccache, 2001). Similarly, the notion of “ac-
cess consciousness” introduced in Block (1995) can be framed through the lens
of a working memory whose contents are globally accessible across the brain.
The link between working memory and attractor dynamics, in turn, is well
established. Empirical studies have demonstrated that attractor dynamics are
ubiquitous in the brain, both across species and levels in the brain’s hierar-
chy (Khona and Fiete, 2022; Rolls, 2010).
The attractor model for working
memory postulates that working memory emerges from recurrently connected
cortical neural networks that allow representations to be maintained in the short
term (on the order of seconds) by self-generated positive feedback (Barak and
Tsodyks, 2014; Curtis and D’Esposito, 2003; Deco and Rolls, 2003; Durstewitz
et al., 2000; Seeholzer et al., 2019). Attractor dynamics can support both sup-
pression of inputs, for example in decision making where the brain state ﬂows
rapidly towards a discrete attractor and subsequent inputs or perturbations are
discounted, as well as integration over inputs, where the incremental response to
inputs causes reversible ﬂow along continuous attractor manifolds (Khona and
Fiete, 2022; Redish et al., 1996; Wang, 2008). Neural winner-take-all (WTA)
models implement hybrid analog-discrete computation (Wang, 2008; Wong and
Wang, 2006). Robustness, discreteness, and temporal integration of information
are all traits apparent in working memory (Khona and Fiete, 2022).
3.1.2
Stability and robustness of conscious states
As a model of conscious processing, discrete attractor dynamics predict that
our experience consists of a sequence of relatively stable states that transition
swiftly from one to another. Such types of sequential dynamics have been hy-
pothesized to be a key component of conscious thought and perception (James,
1892; Rabinovich et al., 2008; Tsuda, 2015; Varela, 1999). Empirically, one of
the characteristics that distinguishes conscious vs. unconscious neural represen-
tations in psychophysics tasks is that they are signiﬁcantly more stable in the
“aware” condition (Schurger et al., 2015).
Qualitatively, subjects commonly report on the emergence of stable discrete
”choices” within conscious perception. For instance, when looking at the Necker
cube, subjects only perceive one single interpretation of its structure and orien-
tation rather than a mixture of both possibilities. Occasionally, this interpreta-
tion will change to the alternative one, but the change will happen rapidly as
an abrupt transition. Similarly, in the case of binocular rivalry, only a single
image presented to one of the eyes will be consciously perceived rather than
a mixture of the two, and which image is consciously perceived will abruptly
change at random times. Such cases are characterizable by attractor dynamics
that converge to one attractor and remain stable until suﬃcient input change
or noise result in a rapid transition to another attractor.
Input change or noise may also result in basin transitions that occur without
complete convergence to attractors. This is familiar in the cases of thought and
speech. One common example is thought-disruptive external stimuli, in which
external stimuli distract or interrupt one’s chain of thought. A less well-known
13

but equally important example is the role of internal time-saving mechanisms.
These are active in cases where one does not need to spell something out in
full detail. For example, in speech production, phonemes are often not fully
articulated: this may be understood by noting that once one has arrived at an
attractor basin it is disambiguated which point one converges toward (Roessig
et al., 2019). A similar mechanism may explain the utility of verbal or symbolic
thought, where the key may serve as synecdoche for the value.
Schurger et al. (2010) suggested that conscious states were associated with
increased robustness to noise in psychophysics experiments. A signature of neu-
ral representations in the “conscious” condition was that they were highly re-
producible; given the same stimulus presentation across diﬀerent trials, patterns
of neural activity were similar, so long as the subjects reported awareness of the
stimulus. In contrast, patterns of activity during the “nonconscious” condition
in which subjects were unaware of the stimulus exhibited greater variability.
Both robustness to noise and reproducibility of states, in turn, are core proper-
ties accommodated by attractor dynamics.
3.2
Richness and ineﬀability
Box 1. Notation
Let lower case x denote an instance of random variable X, X denote the set of
possible states for X with probability distribution P(X), P
x∈X P(X = x) = 1,
p(x) denote P(X = x), expectation Ep(x)[f(x)] denote P
x∈X p(x)f(x), and
likewise for other variables. We consider discretized variables, including ﬂoating
point representations. [n] denotes the list of natural numbers 1, . . . , n.
What is meant by the richness of experience? Intuitively, whilst we ﬁnd it
easy to communicate certain aspects of our mental state, we struggle to convey
their full content or meaning. One can consider color as an example. We are
tempted to think of color space as a simple 3-dimensional surface, on the basis of
perceptual similarity judgments that people tend to make. However, there is a
far richer and higher dimensional structure to experiencing color. For instance,
most people would describe the color “red” as warm, aggressive, and perhaps
tasty.
There are myriad associations that we make with various colors that
are not functions of their nominal deﬁnitions, and all of these associations as a
whole contribute to the richness of the experience (Chalmers, 2010).
Broadly, richness means having a lot; the condition of being “well supplied or
endowed” (Merriam Webster Dictionary, 2023). In the context of mental state
attribution, richness gauges the amount of speciﬁcity - detail, texture, nuance or
informational content - contained by a mental state. It is a common principle in
aesthetics that experience is rich (a picture speaks a thousand words), and many
philosophers acknowledge that conscious states at least appear to be highly
detailed, nuanced and contentful (Block, 1995; Chuard, 2007; Tye, 2006) though
some take this appearance to be ultimately illusory (Cohen et al., 2016; Dennett,
1993).
14

This conception of richness corresponds well to the mathematical notion
formalized by Shannon (Shannon, 1948), where richness of a random variable X
is given by its entropy H(X). Here, a random variable would represent a state
type such as, e.g., experience of some face or other. To say that such a variable
is high in entropy is to say that the number of values it could take (the number of
possible states the system could be in, e.g., the diﬀerent experiences of faces one
could possibly have) is relatively large, and that the probability distribution over
these is relatively ﬂat (and uniform in the extreme case of maximal entropy): it
isn’t predictable in advance which face will be seen. Strictly, Shannon entropy
measures the average number of bits (answers to yes-or-no questions) that it
takes to specify which value the relevant variable takes on (i.e., which face one
sees); a useful measure of informational content.
The notion of ineﬀability is closely related. In popular usage, ineﬀable can
be deﬁned as “too great for words” (Oxford English Dictionary, 2023). The
concept is often used in theological contexts but since at least (Dennett, 1993)
it has been applied to descriptions of qualitative experience. Given the term’s
theological associations, the claim that experience is ineﬀable might sound like
a profession of dualism: consciousness is something magic that no physicalist
theory can understand. However, strictly speaking, to claim that experience is
ineﬀable is simply to claim that its informational content exceeds what we can
remember or report. Much hinges on what exactly we mean by “can remember
or report”. Of course, one can say a thousand words, so the fact that a picture
speaks that many words does not make a picture ineﬀable. On the other hand,
in the time it takes to say a thousand words, one will have seen many more
pictures.
Thus verbally expressing all of the symbolic content of all of the
pictures one sees is ineﬀable in one sense, but not in another. Below, we will
develop tools to allow us to precisely reﬁne the senses of ineﬀability at issue,
and we will see that experience is ineﬀable in multiple senses (though none of
them need involve magic or anything anathema to physicalist theories).
We propose that ineﬀability corresponds to the mathematical notion of infor-
mation loss when trying to express a conscious state in words. Given a function
that processes an input variable X and produces an output variable Y , infor-
mation loss of the input incurred by the output is measurable by conditional
entropy H(X|Y ), or entropy of the input variable given the output variable. In-
tuitively, conditional entropy H(X|Y ) measures how well Y describes X: how
much uncertainty remains about what value X might take, once you know what
value Y takes. Strictly, conditional entropy H(X|Y ) is mathematically equiva-
lent to the entropy of the input X minus the mutual information between input
and output, H(X|Y ) = H(X) −I(X; Y ), where the latter is a measure of infor-
mation shared between them; the amount of information about the state of one
variable obtained by observing the state of the other (note the diﬀerence be-
tween conditional entropy and mutual information: mutual information is how
much uncertainty one random variable removes from another, while conditional
entropy tells you how much uncertainty remains in the ﬁrst variable after you
know the value of the second. Later we will also make use of joint entropy,
H(X, Y ), which is the amount of information needed on average to specify the
15

values of x and y from X and Y, as a function of their joint distribution).
Usefully, thinking about things in this way allows us to oﬀer a precise deﬁ-
nition of eﬀability as the negation of ineﬀability. Where ineﬀability is given by
H(X|Y ), negating ineﬀability gives eﬀability: −H(X|Y ) = I(X; Y ) −H(X).
Recalling that entropy is a measure of uncertainty or spread in a probability
distribution, the smaller H(X|Y ) is, the less uncertain X is given Y , the less
information is lost, and the more eﬀable or communicable X is via Y . We may
say that given a variable X with entropy H(X), its eﬀability to variable Y scales
with the amount of shared information I(X; Y ). Finally, since entropy H(X) is
recoverable as H(X) = H(X|C) for any constant variable C, richness may be
considered the special case of ineﬀability where the output state is a constant.
In the foregoing we draw on the framework of Shannon information. But
there are advantages, for our purposes, to using the Kolmogorov framework
(Kolmogorov, 1965) as an alternative way to characterize richness and ineﬀabil-
ity. For our purposes the most important distinction between these frameworks
is that Shannon entropy is deﬁned for a random variable given its distribu-
tion (i.e., a class of possible states and the probability of winding up in each),
measuring the average information carried by a given state in the class, while
Kolmogorov complexity is a measure of the information carried by an individ-
ual state x, independently of probability distributions associated with random
variables ranging over it.
In this formalism, richness of a state x corresponds to its Kolmogorov com-
plexity K(x), which is the length in bits of the shortest program written in a
general programming language that outputs x and halts. Ineﬀability then cor-
responds to conditional Kolmogorov complexity of an input x given an output
y, K(x|y), the length of the shortest program needed to produce x if y is given,
or intuitively the complexity of x minus the number of bits that can be saved
from knowing y, which is the Kolmogorov analog of Shannon information loss
as conditional entropy or entropy minus mutual information.
Shannon entropy and Kolmogorov complexity are closely related metrics of
richness, and are described in more detail in Box 2 and Fig. 4. If the prob-
ability distribution over states is given, taking an expectation over the distri-
bution on Kolmogorov complexity of its states allows Shannon entropy to be
approximately recovered (Gr¨unwald and Vit´anyi, 2004). Under either frame-
work, richness is characterizable as information measured in bits, ineﬀability
as information loss or richness reduction, and communicability and ineﬀability
are neither separate nor boolean traits, but direct opposites of each other and
varying on a scale.
However, Shannon entropy has several drawbacks for characterizing ineﬀabil-
ity. First, a major diﬀerence between entropy and Kolmogorov complexity is the
former is deﬁned given a probability distribution over variable states, whereas
the latter is deﬁned on individual states without assuming a given probability
distribution. Knowledge of the distribution over a variable’s states is generally a
non-trivial assumption. The distribution may be undeﬁned or highly privileged
information in itself (that is, the meta-distribution over the distribution’s pa-
rameters is rich). Consider, for example, measuring the amount of information
16

in a book by considering the set of all possible books and the distribution over
them (Gr¨unwald and Vit´anyi, 2003) or the information in a temporal snapshot
of a high-dimensional brain state by considering the distribution over all possi-
ble states. In these cases, we want a way to measure informational content that
does not require knowledge of a hard-to-specify distribution. This is especially
salient for us where inter-personal ineﬀability is concerned. Even if we assume
that a brain’s parameters fully determine the distribution over its own states
(and so in some sense individuals have direct access to their own distributions),
still individuals cannot have this level of knowledge of the distributions of their
interlocutors’ brains. Explicitly allowing the communicator’s distributional pa-
rameters to be unknown is therefore convenient for characterizing inter-personal
ineﬀability from the perspective of the listener.
A second drawback of Shannon’s framework is that entropy is a measure of
statistical determinability of states as opposed to diﬀerence in absolute states;
information is fully determined by the probability distribution on states and un-
related to the meaning, structure or content of individual states (Gr¨unwald and
Vit´anyi, 2003). For example, consider again a case where we want to measure
inter-personal ineﬀability, as a relationship between a communicator’s expe-
rience and a listener’s. Conditional entropy of the communicator’s experience
given the listener’s experience is low if the pairing is statistically unique, regard-
less of the semantic correspondence between experiences, whereas conditional
Kolmogorov complexity is concerned with the diﬃculty of reconstructing the
communicator’s experience given the listener’s experience, i.e. absolute diﬀer-
ence, which corresponds more closely to the lay deﬁnition of ineﬀability. For
example, it might just happen to turn out that whenever Alice thinks and talks
about tennis, Bob almost always thinks about Beethoven. In this case, condi-
tional entropy will be low, but conditional Kolmogorov complexity will be high,
and therefore suited to capture the absolute diﬀerence between their experi-
ences. For these reasons, we argue that particularly in the case of inter-personal
communication, Kolmogorov complexity should be used to characterize rich-
ness and ineﬀability of experiences. However, Shannon entropy is functionally
equivalent if the distribution is given, and we will refer to both frameworks.
Box 2. Metrics for richness and ineﬀability
Shannon entropy is given by
H(X) = E
p(x)[−log p(x)].
(1)
If variable Y is produced by processing X, y = f(x), with joint distribution
denoted by p(x, y) and f stochastic in the general case, then information loss
from X to Y is given by conditional entropy H(X|Y ) = H(X)−I(X; Y ), where
I(X; Y ) denotes Shannon mutual information between variables,
I(X; Y ) =
E
p(x,y)

log p(x, y)
p(x)p(y)

,
(2)
17

Figure 4: Illustrating Shannon entropy and Kolmogorov complexity for discrete color
distributions. Entropy (Eq. (1)) involves an expectation over the states of stochastic
variable X whereas Kolmogorov complexity (Eq. (4)) is deﬁned for an instance of state,
x. The distribution on the left has non-zero mass in one state and is the minimum
entropy distribution; the distribution on the right is uniform over states and is the
maximum entropy distribution for 8 states. Assume a universal RGB representation
for colors where each RGB component ranges between 1 and 256. Without assumptions
on the distribution over colors, the Kolmogorov complexity of each state is no greater
than 24 (excluding program overheads) since color can be represented with 3 8-bit
binary sequences, but may be lowered for smaller RGB values that do not require 8
bits if an optimized number encoding scheme is used (Gr¨unwald and Vit´anyi, 2003).
Whereas entropy is the same for the same probability distribution over any states,
Kolmogorov complexity would increase for states whose values are algorithmically
more diﬃcult to construct.
and H(X|Y ) is given by
H(X|Y ) =
E
p(x,y)[−log p(x|y)].
(3)
The Kolmogorov complexity of a state x, K(x), is the length l(z) in bits of
the shortest binary program z that prints x and halts. Speciﬁcally, let U be a
reference preﬁx universal machine. The preﬁx Kolmogorov complexity of x is
K(x) = min
z {l(z) : U(z) = x, z ∈{0, 1}∗}
(4)
Conditional Kolmogorov complexity is the length of the shortest program that
takes y as an input, prints x and halts. It is given by
K(x) −I(x : y) = K(x|y∗)
+= K(x|y, K(y))
log
= K(x|y)
(5)
where I(x : y) denotes Kolmogorov mutual information between states, y∗
denotes the shortest program that produces y and halts, standard notation
+= and
log
= are used to denote equality up to constant and logarithmic factors
respectively (Gr¨unwald and Vit´anyi, 2004; Li et al., 2008). As Eq. (5) shows,
K(x|y∗) and K(x|y) are comparable and either may be used to characterize
information loss; in subsequent sections we will generally refer to K(x|y).
18

Kolmogorov complexity has several intuitive properties and similarities with
Shannon entropy. Conditioning on more data can only decrease information
loss, K(x|y) ≤K(x), as y is utilized if it allows for a shorter program and
otherwise ignored. If y merely copies x there is no information loss, K(x|y)
+= 0.
Under the Shannon framework, H(X|Y ) = 0 if X = Y but also more generally
if each state y corresponds to a unique x. Figure 4 illustrates Shannon entropy
and Kolmogorov complexity for a toy example. Shannon entropy is concerned
with statistical determinability of a random variable given knowledge of its
probability distribution, whereas Kolmogorov complexity can be considered as
a more tabula rasa (not knowing the distribution) measure of richness of a
particular value of this variable. Shannon entropy and Kolmogorov complexity
are related by the following constraints (Gr¨unwald and Vit´anyi, 2004):
0 ≤(Ep(x)[K(x)]) −H(X)
+
≤K(p),
(6)
I(X; Y )
+= Ep(x,y)[I(x : y|p)],
(7)
I(X; Y ) −K(p)
+< Ep(x,y)[I(x : y)]
+< I(X; Y ) + 2K(p),
(8)
which conveys how Kolmogorov complexity pays a penalty for not assuming
knowledge of the distribution, since it must be encoded within the program.
3.3
Intra-personal ineﬀability
Figure 5:
A model of intra-personal ineﬀability.
Information is channelled
through the stages of input (d), subconscious processing (v), working memory (x, a),
conscious experience (s) and verbal report (m). A trajectory x in the state-space of
working memory follows attractor dynamics, converging near an attractor a. Each step
transforming one variable to another is executed by the dynamics of the individual’s
brain, which is determined by parameters φ. Conscious experience s is a function of
the subject’s cognitive parameters φ and working memory trajectories x, and encodes
the experience’s meaning.
In this section we will develop our model of intra-personal ineﬀability, that
is, ineﬀability between stages of processing within a single experiencer. We will
be concerned with the following variables: Let X (with value x) be a trajectory
19

of neural activities that determine working memory content and conscious expe-
rience, and let it consist of a sequence of transient working memory states Xt for
t ∈[T], where length T is ﬁxed and suﬃciently large such that all trajectories
terminate near an attractor state. Let A (with value a) denote the terminating
attractor state, S (with value s) denote the conscious experience, D (with value
d) denote external input datum, let V (with value v) denote a list of N subpro-
cess states Vn for n ∈[N] and ﬁxed N that comprise subconscious computation
aﬀecting transient working memory trajectory X, and let M (with value m)
denote the verbal report or output message of the individual. In addition, let
φ denote the brain’s synaptic weights that parametrize its dynamics. These
variables are connected by a computation graph of functions (Fig. 5), given by
v = f V
φ (d), x = f X
φ (v), a = f A
φ (x), s = f S
φ (x) and m = f M
φ (a). The func-
tions f A
φ (returns ﬁnal attractor state), f S
φ (outputs conscious experience that
is fully determined by x) and f M
φ
(verbal projection) are deterministic while
f V
φ and f X
φ
are generally stochastic, meaning outputs may be dependent on
hidden stochastic variables within the function that encode historical states or
neural processing noise. Subscripting with φ denotes that function behavior is
determined by cognitive parameters φ. The computation graph deﬁnes a joint
probability pφ(d, x, s, a, m), from which conditional and marginal probability
distributions on individual variables may be obtained. Entropy Hφ is also pa-
rameterized since it depends on pφ. Finally, denote the transient state by ¯X,
where pφ(¯x) = 1
T
P
t∈[T ] Pφ(Xt = ¯x) is the probability that any transient state
takes the value ¯x.
Our model remains neutral about whether conscious states can be iden-
tical to transient states or may only be identical to attractor states.
Since
s = f S
φ (x), conscious experience is not restricted to be identical to transient
working memory states or attractor states, but is more generally the output of
a deterministic function that depends on the trajectory through working mem-
ory states and cognitive parameters, i.e. a projection of working memory and
states approaching it, given cognitive parameters φ. While we will not focus on
the implementation details of how conscious experiences might relate to neural
processes, intuitively s can be thought of as a vector of real numbers represent-
ing one point in an abstract space of possible experiences, and its role in our
framework is to provide a semantic grounding for conscious state through de-
pendencies on working memory and parameters φ that deﬁne the entire neural
dynamics of the brain. Subsequently, information theory gives us the ability
to reason about the relative richness and ineﬀability of conscious experience
based on the computation graph, without needing implementation details of
the functions.
3.3.1
Information loss from attractor dynamics
The relation of time-varying states x to a smaller subset a of attractor states is
a deﬁning characteristic of attractor dynamics, whether the subset consists of a
discrete number of ﬁxed points or a set of states that trace out a complex shape
20

such as a curved manifold. In this section, we argue that the presence of attrac-
tor dynamics decreases the richness of working memory states and conscious
experience. We will identify two related eﬀects. First, at the level of compari-
son between systems, the greater the percentage of a system’s trajectories are
at attractors, the less rich each trajectory is than those of a comparable system.
Second, at the level of comparison between states, trajectories that are not yet
at attractors are richer than trajectories at attractors.
Since dynamics are characterized by the ﬂow of transient states towards
an attractor in A followed by persistent membership in A, and attractors A
typically constitute a signiﬁcantly smaller subset of all possible transient states
¯
X (Khona and Fiete, 2022), the presence of attractors decreases the (Shannon)
richness of transient states Hφ( ¯X). Since entropy is a measure of distributional
spread, dynamics with larger non-attractor spaces ¯
X \ A (the class of possible
system trajectories that are not in attractors) and with more time spent in
such states yield richer distributions over transient states Pφ( ¯X); conversely,
faster convergence to attractors and more time spent at attractors yields lower
Hφ( ¯X). In turn, reducing the richness of transient states limits the richness of
full trajectories and conscious experience (Box 3).
Box 3. Implications of reducing transient state richness
Reducing the richness of transient states Hφ( ¯X) also reduces a ceiling on
the richness of full trajectories Hφ(X), since Hφ(X) = Hφ(X1 . . . HT ) ≤
P
t∈[T ] Hφ(Xt) ≤T(Hφ( ¯X)+C) by the addition rule of entropy, where constant
C = maxt∈[T ](Hφ(Xt) −Hφ( ¯X)) limits the maximum deviation of entropy be-
tween individual timesteps and the temporal average. This in turn reduces a ceil-
ing on the richness of conscious experience as Hφ(S) ≤Hφ(X). The latter is de-
rived as follows: the joint entropy Hφ(S, X) = Hφ(X)+Hφ(S|X) = Hφ(X) since
f S
φ is deterministic, i.e., Hφ(S|X) = 0. Hφ(X) = Hφ(S, X) = Hφ(S)+Hφ(X|S)
and Shannon entropy is non-negative, thus Hφ(S) ≤Hφ(X).
This might seem to be an artefact of the Shannon approach, which directly
concerns features of the distribution.
One might suppose otherwise for the
Kolmogorov approach: why should the computational complexity of a speciﬁc
trajectory depend on global features of the distribution? But in fact the same
reasoning applies under Kolmogorov’s formalism if the probability distribution
is known, because it turns out that knowing the distribution gives the encoder
a short-cut: expected Kolmogorov complexity Epφ(¯x)K(¯x|pφ) is equivalent to
entropy Hφ( ¯X) up to an additive constant if the distribution is given (Eq. (6)).
Intuitively, this is because the shortest lossless descriptor of ¯x, given knowledge
of the distribution Pφ( ¯X) and thus support ¯
X, has length −log p(¯x) under Shan-
non’s noiseless coding theorem (Gr¨unwald and Vit´anyi, 2004). Given knowledge
of Pφ( ¯X), −log p(¯x) bits are all that is additionally needed to determine the
state using a descriptionally simple (but not necessarily computationally short)
computer program.
Thus far we have described how the presence of attractors can decrease
21

the richness of transient states overall, i.e., as a matter of comparing between
systems (e.g., two brains). We turn now to a second way in which attractors
reduce richness, but this time as a matter of comparison between states in a
given system.
Global Workspace Theory postulates that the access of representations from
working memory by diverse processes across the brain depends on the represen-
tations being ampliﬁed and maintained over a suﬃcient duration, for instance
for a minimum of approximately 100ms (Dehaene and Naccache, 2001). In the
language of the attractor framework, this amounts to the claim that the vari-
able released to downstream processes such as verbal-behavioral reporting and
long-term memory is A, not X. Crucially, as we explain below, it is provable
that attractor states are strictly less rich than trajectory states Hφ(A) < Hφ(X)
(Box 4). Thus selective release of attractor working memory states to down-
stream processing implements an information bottleneck that limits the rich-
ness of downstream inputs. This constitutes an important source of ineﬀability,
where our in-the-moment experiences S seem (and perhaps are) richer than our
later recollections, since richness of experience is upper bounded by the richness
of trajectories (i.e. Hφ(X) ≥Hφ(S), Box 3), so the higher the richness of tra-
jectories, the higher the ceiling on information loss to the attractor state and
downstream variables. This will be highly relevant to our discussion of phenom-
enal overﬂow (Block, 2007) below. In practice one would expect the magnitude
of information loss from trajectory X to working memory output A to be sig-
niﬁcantly large, since trajectories are sequences of brain states specifying the
activity of billions of neurons, whereas working memory appears to be limited
to representing a handful of items, which gives us a clue to the magnitude of
the bottleneck. (Sperling, 1960).
Box 4. Richness of attractors strictly less than richness of trajectories
On the one hand, f A
φ is a deterministic function: the full trajectory deter-
mines the attractor it terminates in. It follows that Hφ(A|X) = 0. On the
other hand, we also know that Hφ(X|A) > 0 since multiple possible trajecto-
ries terminate in the same attractor state. Our result follows from this asym-
metry. By the general relationship between joint and conditional entropy we
have Hφ(X, A) = Hφ(X) + Hφ(A|X). But since Hφ(A|X) = 0 we therefore
have Hφ(X, A) = Hφ(X). Re-applying the general relation between joint and
conditional probability we also have Hφ(X, A) = Hφ(A) + Hφ(X|A).
From
these observations together we know that H(A) + H(X|A) = H(X). But since
Hφ(X|A) > 0, this yields our conclusion: Hφ(A) < Hφ(X).
3.3.2
Information loss at verbal report
The ineﬀability of our experience is perhaps most obvious when we attempt
to put it into words, due to the highly compressed nature of language (Kirby
et al., 2015).
From the computation graph, we can say that ineﬀability or
22

information loss from conscious experience to verbal report is at least as great
as information loss from conscious experience to working memory attractor (Box
5). Additionally, it would be reasonable to assume information losses Hφ(A|M)
and Hφ(S|M) are strictly positive (i.e., Hφ(A|M) > 0 and Hφ(S|M) > 0) if
message M is a low-dimensional symbolic variable (such as a few words) whereas
A and S are snapshots of cortical states. While it might appear that language is
rich, note that n characters with an alphabet of 256 possible characters require
no more than 8n bits to represent, whereas neural state is determined by the
activity of up to approximately 100 billion neurons (Herculano-Houzel, 2009).
Information loss from attractor A or conscious experience S to verbal mes-
sage M means the latter do not fully identify the former, and instead divide
the space of attractors and conscious experiences more coarsely. For instance,
saying that one “saw a fat cat” leaves out signiﬁcant details about the speciﬁc
composition of attractors that generated the message, which would be diﬃcult
to communicate fully (e.g., the cat’s color, size, pose, the surrounding environ-
ment, etc.). Positive information loss Hφ(S|M) implies it is generally impossible
to perfectly recover the conscious experience from the verbal message. Note that
as long as Hφ(A|M) is strictly positive, this means that conscious experience
is somewhat ineﬀable to verbal report even if we identify conscious experience
with working memory (i.e. attractor) states.
Box 5. Ineﬀability of conscious experience to verbal report
From the computation graph, S −X −A −M form a Markov chain, thus
S −A −M is also a Markov chain and S is conditionally independent of M if
A is given. Thus Iφ(S; A) ≥Iφ(S; M) from the data processing inequality (a
theorem of information theory), implying Hφ(S)−Hφ(S|A) ≥Hφ(S)−Hφ(S|M)
and Hφ(S|M) ≥Hφ(S|A).
An additional source of ineﬀability is that attractors can have more complex
and high-dimensional structure than simple ﬁxed points, which is common in
high-dimensional systems. Such a system would exhibit increased richness of
attractor state Hφ(A) and increased ineﬀability, as the same richness of mes-
sages Hφ(M) and an increase in joint entropy Hφ(A, M) implies an increase in
information loss Hφ(A|M), since Hφ(A, M) = Hφ(M) + Hφ(A|M).
3.3.3
Hierarchical attractor dynamics
The brain is hierarchical in nature with many levels of spatial and temporal or-
ganization that can be studied, ranging from molecular and synaptic activity to
local networks and large-scale networks (Changeux and Dehaene, 1989). Attrac-
tor dynamics appear to be ubiquitous across organizational levels and cortical
regions of the brain, with processing in the neocortex hypothesized to support
many attractor networks each concerned with a diﬀerent type of processing (ex-
ecutive function and working memory, general short-term memory, long-term
memory etc.) (Khona and Fiete, 2022; Rolls, 2007b, 2010). The presence of
23

multiple weakly coupled neocortical attractor networks yields beneﬁts including
specialization and increased memory capacity, and in addition has ramiﬁcations
for understanding conscious experience.
Anatomically, the inferior temporal cortex is an example of a sensory process-
ing area that responds discriminatively to novel stimuli, whereas the prefrontal
cortex is implicated in maintaining attention-modulated projections of such rep-
resentations in working memory (Miller et al., 1993; Renart et al., 1999; Rolls,
2007b). Neural activity in both regions maintains persistence over time and ex-
hibits attractor dynamics, but the content of sensory memory is akin to the state
of a worker subprocess whereas the content of working memory corresponds to
the state of executive control; working memory representations exhibit increased
temporal stability, persisting for longer durations of up to several seconds, and
provide top-down feedback to diverse regions of the brain, including the inferior
temporal cortex (Bushnell et al., 1981; Chelazzi, 1999; Rolls, 2010). The ability
of the prefrontal attractor to be stimulated into activity by signals from sensory
processes before stabilizing in its high ﬁring rate attractor state is attributable
to positive feedback from strong internal recurrent connections that suppress
incoming stimuli (Renart et al., 1999). The need to maintain information in
working memory during periods where new stimuli may be perceived exempli-
ﬁes why working memory and subprocess memory necessitate distinct attractor
networks (Rolls, 2007b, 2010).
Without coming down on how they relate to conscious experience, we can
note that the well-known Sperling experiments (Sperling, 1960) illustrate dif-
ferent dynamics in working memory and sensory memory processes, notably in
terms of duration (a few seconds or less after the brief visual presentation of
an array of letters, only letters that have been consciously attended to remain
reportable) and capacity (sensory memory is capable of holding rich information
pertaining to all digits whereas the number of reportable items was limited to
approximately 4). Numerous studies have demonstrated the short-lived nature
of representations in sensory memory and the importance of top-down feedback,
as backprojected attention appears necessary to avoid exponential decay in sen-
sory memory representations (Cohen and Dehaene, 1998; Rolls, 2010; Rolls and
Tovee, 1994; Tiitinen et al., 1994).
The limits imposed on the richness of working memory state by subconscious
subprocess memory states may be illustrated in an information theoretic manner
by considering that the latter is an input to the former (Box 6).
Box 6.
Richness of subconscious states constrains richness of con-
scious experience
Subprocesses state V is considered subconscious as it is computed upstream of
conscious experience S in the computational graph. Extracting the stochastic-
ity in f X
φ into an input variable ϵ, the richness of X is bounded as Hφ(X) ≤
Hφ(V1, . . . , VN, ϵ) ≤P
n∈[N] Hφ(Vn) + Hφ(ϵ) due to the deterministic data pro-
cessing and addition rule of entropy. That is, given a limit on the richness of
24

noise Hφ(ϵ), a ceiling on the richness of working memory trajectories Hφ(X)
scales with the richness of the subconscious states that constitute its inputs.
In turn this restricts ceilings on the richness of downstream variables such as
conscious experience and working memory attractors (Box 3).
3.4
Inter-personal ineﬀability
Communication channels are not limited to personal sensory processes and ver-
bal or behavioral reporting processes but extend to channels between individ-
uals. In this section, we will consider communication between two individuals
using the model summarized in Fig. 6 in which a speaker, Alice, wishes to
communicate her experience to a listener, Bob. We use the same variables as
as in prior sections, but denote Bob’s variables using a “∼” (e.g., ˜s denotes
Bob’s conscious experience). Again we assume a computational chain of states
x →a →m →˜x →˜a that elicit an experience ˜s = f ˜φ(˜x) in Bob. In prior
sections, we have already considered sources of ineﬀability up to Hφ(s|m) and
K(s|m, pφ) in this chain. What remains is to identify additional sources of inef-
fability after the message is transmitted. In this section we use the Kolmogorov
formalism, since we assume the parameters φ of Alice’s brain are not available
to Bob.
Figure 6: A model of inter-personal ineﬀability. We model the communication
pipeline between a speaker Alice and a listener Bob. A trajectory x in Alice’s state-
space of working memory follows attractor dynamics, converging near an attractor
a. Alice then attempts to communicate the experience with a message m. On Bob’s
end, the message is decoded and inﬂuences his working memory trajectory ˜x, which in
turn converges near an attractor ˜a. Each step transforming one variable to another is
executed by the dynamics of the subject’s brain, denoted by φ for Alice and ˜φ for Bob.
Conscious experiences s and ˜s are functions of the subject’s cognitive parameters φ and
˜φ and working memory trajectories x and ˜x respectively, and encode the experience’s
meaning. We are interested in the ineﬀability K(s|˜s) of Alice’s conscious experience s
given the experience ˜s that was elicited in Bob.
25

3.4.1
A blank-slate listener
Before considering the case in which Bob is a typical human listener, we begin
with a discussion of ineﬀability when Bob is a blank-slate (setting ˜φ = ∅, ˜x = ∅,
˜s = ∅, ˜a = ∅, where ∅denotes the null value). In this case the chain of communi-
cation ends at m, thus we are interested in the ineﬀability K(s|m). Intuitively
what this quantity refers to is the intrinsic ineﬀability of an experience given
its message, without conditioning on extra information such as cognitive pa-
rameters φ or ˜φ. We have Epφ(s|m)K(s|m) ≥Epφ(s|m)K(s|m, pφ) trivially since
conditioning on more information cannot increase the length of the shortest pro-
gram that outputs s, but it is important to note that one would additionally ex-
pect the reduction to be signiﬁcant, i.e., Epφ(s|m)K(s|m) ≫Epφ(s|m)K(s|m, pφ).
This is because under Shannon’s noiseless coding theorem, knowledge of Alice’s
state distribution pφ reduces the problem of describing s in the general space of
high-dimensional vectors to the problem of describing its index amongst the set
of all possible conscious experiences associated with m for a brain parametrized
by φ.
The inequality Epφ(s|m)K(s|m) ≫Epφ(s|m)K(s|m, pφ) relates to an obser-
vation at the core of the philosophical debate on ineﬀability: our descriptions
of our experiences never seem to come close to capturing their full richness.
The gap is so signiﬁcant that it has at times led some philosophers, scientists
and laypersons to the dualistic conclusion that conscious experiences are intrin-
sically indescribable, such that there is something more to their content than
physically-embodied information encoded in neural activity.
For example, in Frank Jackson’s famous thought experiment, Mary the color
scientist has lived her entire life in a black and white room, but she knows
everything there is to know about the science of color and color perception. By
extension, she knows everything that anyone could possibly tell her about the
experience of seeing something red. Jackson argues that when she ﬁnally sees
something red, she nevertheless learns something new (“what it is like to see
red”). But she already knew all of the physical facts, so what she learned must
have been a non-physical fact (Chalmers, 2010; Jackson, 1986).
Many philosophers have responded to this argument, developing diﬀerent
conceptions of how what Mary learns might be physical after all (Alter and
Walter, 2006). Our model can be understood as informing this conversation,
and oﬀering support to these physicalist accounts.
In general, what one gains in such cases is (physical) information about the
distributions of one’s interlocutors’ cognitive parameters (and perhaps about
one’s own). In the special case of Mary, what she learns is a speciﬁc formatting
of this information (though by Jackson’s stipulation she already possesses it in
some other form). This is not to come down on whether the relevant acquisition
on Mary’s part should be classiﬁed, strictly speaking, as a new ability (Lewis,
1990), a new mode of presentation (Loar, 1990), a new relation of acquaintance
(Conee, 1985) or rather just a reminder of something that in principle she must
have had access to all along, if we accept Jackson’s stipulations about the case
(Dennett, 2006; Rabin, 2011). Instead it is to oﬀer a more precise conception
26

in information processing terms of the speciﬁc content of the acquisition.
It is worth noting that what we lack when we lack knowledge of other minds
(Russell, 1992) may also be construed in the terms of our model. Ignorance over
whether “your red” is “my green” is ignorance over one another’s distributions.
We return to these points in 3.4.2 below.
The increase in ineﬀability from not conditioning on pφ also applies to the
problem
of
describing
the
attractor
state,
i.e.,
Epφ(a|m)K(a|m)
≫
Epφ(a|m)K(a|m, pφ), due to a being a high dimensional vector that represents
the output of working memory and m being a relatively low dimensional vector
representing a sentence. Note that ineﬀability of the attractor imposes a lower
bound on the ineﬀability of the conscious experience under mild assumptions,
thus if the former is large, so is the latter (Box 7). While the representation of m
is shared amongst individuals who speak the same language, the representation
of a is unique to communicator Alice. Therefore, under the Kolmogorov for-
malism, there is complexity or information content in a that requires adopting
Alice’s representation space to reconstruct.
An analogy can be made with word symbols and word embeddings (or rep-
resentation vectors) in deep learning models of natural language, as initially
proposed by Bengio et al. (2000) and the earlier ideas on distributed repre-
sentations of symbols (Hinton et al., 1986) (now ubiquitous in leading NLP
applications including word2vec and large language models). Essentially, every
word in the system is associated with an arbitrary unique integer (the symbol)
as well as a learnable vector (the embedding). As shown by Bengio et al. (2000),
word embeddings can be used to represent semantics in a shared space, and can
therefore help a model generalize to new sentences from training data compris-
ing only a small subset of all possible sentences. Importantly, because the word
symbols are arbitrary, they contain no information about the embeddings. In a
similar vein, when communicating using a message that simply conveys an at-
tractor using a symbolic description m = f M
φ (a), we lose the rich representation
of a that provides information on Alice’s subjective experience.
The signiﬁcant magnitude of Epφ(s|m)K(s|m) and Epφ(a|m)K(a|m) captures
the blank-slate or tabula rasa case of the problem of ineﬀability: without assum-
ing knowledge of the parameters of Alice’s brain, experiences are highly ineﬀable
using low dimensional descriptions such as typical verbal messages. Nonethe-
less, K(s|m) ≤K(s) < ∞; our experiences are describable in principle, even
to a blank slate observer where no additional information is assumed. Using
a numerical scale to quantify ineﬀability allows us to convey the dual sense in
which our experiences are, to varying degrees, both communicable and ineﬀable.
Box 7. Triangle inequalities for Kolmogorov complexity
We have that K(a|m∗)
+<
K(a|s∗) + K(s|m∗) (Gr¨unwald and Vit´anyi,
2004,
Theorem
4.1)
where
m∗
is
the
shortest
preﬁx
program
that
outputs
m
and
halts,
and
likewise
for
the
other
variables.
Thus
K(a|m∗) −K(s|m∗)
+< K(a|s∗).
From a similar application of the tri-
27

angle inequality, we have K(s|m∗) −K(a|m∗)
+< K(s|a∗).
Assuming the
complexity of conscious experience is at least as great as the complexity of the
working memory attractor, K(s) ≥K(a), we obtain K(s|a∗) ≥K(a|s∗) from
I(a : s) = K(a) −K(a|s∗) = K(s) −K(s|a∗). Therefore we have that the
ceiling on relative ineﬀability of conscious experience s is equal or higher than
for working memory attractor a.
3.4.2
A typical listener
Cognitive similarity and eﬀability.
In a realistic communication scenario,
the cognitive parameters of listener Bob ˜φ are given by a high-dimensional vec-
tor that provides information about Alice’s parameters φ within the generic
space of high-dimensional vectors, due to shared physical environment (includ-
ing cultural experience) and shared evolutionary background, and thus may be
used to reduce the description length of pφ. Trivially, we have that the expected
ineﬀability of Alice’s conscious experience can only improve by conditioning on
Bob’s parameters Epφ(s|m)K(s|m) ≥Epφ(s|m)K(s|m, p ˜φ). However, we also ob-
tain that a ceiling on the disadvantage of using Bob’s parameters compared to
Alice’s parameters scales with the distance between them (Box 8).
Box 8. Cognitive dissimilarity and ineﬀability
From Gr¨unwald and Vit´anyi (2004, Theorem 2.10) we obtain for given m, pφ, p ˜φ
that 0 ≤Epφ(s|m)[K(s|m, p ˜φ)]−Hφ(S|m) ≤K(pφ(·|m)|p ˜φ, m)+c ≤K(pφ|p ˜φ)+
c and 0 ≤Epφ(s|m)[K(s|m, pφ)] −Hφ(S|m) ≤K(pφ(·|m)|pφ, m) + c = c.
Note Hφ(S|m) ≥Hφ(S|m, p ˜φ) where the underlying joint distribution includes
the meta-distribution over p ˜φ, and likewise Hφ(S|m) ≥Hφ(S|m, pφ).
Then
Epφ(s|m)[K(s|m, p ˜φ)] ≤Hφ(S|m) + K(pφ|p ˜φ) + c and Epφ(s|m)[K(s|m, pφ)] ≤
Hφ(S|m) + c. The diﬀerence between upper bounds on ineﬀability is K(pφ|p ˜φ).
The mismatch between Alice and Bob’s parameters, which is formalized by
K(pφ|p ˜φ) or the minimum number of bits required to encode a program that
produces Alice’s parameters from Bob’s, loosely corresponds to the diﬀerence
between Bob and Alice’s cognitive function (Box 9), which depends on the ex-
tent to which they diﬀer in genetic biases and lived experiences. This result
supports the common intuition that our experiences are more eﬀable or com-
municable to people who are similar to ourselves. It also resonates with the
empirical observation of greater inter-brain synchronization in related individu-
als (Goldstein et al., 2018) and how the brain anatomical structure (i.e. φ and
˜φ) aﬀects the propensity to communicate at the inter-personal level (Dumas
et al., 2012). When neural architectures are distinct, neural networks that are
optimized for solving similar tasks yield analogous attractor structures (Mah-
eswaranathan et al., 2019).
28

Consider a prototypical example of inter-personal ineﬀability, in which Bob
has been blind from birth and Alice is attempting to convey her experience of
seeing the color red. In this case, Bob’s brain might be so diﬀerent from Alice’s
that the distance between their cognitive parameters K(pφ|p ˜φ) is suﬃciently
high that the beneﬁt of conditioning on his own parameters is negligible. In
other words, since Epφ(s|m)K(s|m, p ˜φ) ≤K(pφ|p ˜φ) + c + Hφ(S|m) (Box 8),
if K(pφ|p ˜φ) is large, then the ceiling on Epφ(s|m)K(s|m, p ˜φ), the ineﬀability
of Alice’s conscious experience given the message from Bob’s perspective, is
also large.
Intuitively, when K(pφ|p ˜φ) is small, the information required to
communicate the functions f M
φ , f A
φ and f S
φ in order to reconstruct s from m
is oﬄoaded to p ˜φ, which is given, thus reducing a ceiling on expected program
length Epφ(s|m)K(s|m, p ˜φ).
The cognitive dissimilarity factor K(pφ|p ˜φ) is also implicated in the thought
experiment of color scientist Mary (see 3.4.1 above), who has lived her whole
life in an entirely black and white room and has learned exhaustive knowledge
about the process of color perception, but nonetheless possesses a brain that
is incapable of understanding the experience of color (i.e., she does not know
what it is like to see red) (Alter and Walter, 2006; Jackson, 1986). Our model
highlights how the ineﬀability Epφ(s|m)K(s|m, p ˜φ) of Alice describing her expe-
rience of color to Mary (who is playing the role of Bob), may be explained in
part by the diﬀerence in their cognitive function. In other words, the ability
to empathize with another person from a verbal report of their experience is
aided by cognitive similarity or ease of reconstructing their cognitive function
based on knowledge of one’s own cognitive function, but simply memorizing a
description of how the brain behaves in response to color does not imply one’s
brain is capable of responding in that manner upon being exposed to it, and it
is similarity in cognitive behavior that is implicated in K(pφ|p ˜φ).
This sheds further light on how precisely to understand the information that
Mary acquires (or, if Dennett (2006) is correct: the information she may not
have realized she had). The standard format by which we encode our knowledge
of the distributions of others’ cognitive parameters (and by extension their joint
distributions with our cognitive parameters) involves actually resembling those
others. Even if Mary already knew all of the relevant physical facts, she certainly
did not know them in that form.
Box 9. Diﬀerence in functionality and diﬀerence in parameters
For a scalar valued function h with bounded gradient magnitude, we have
h(x, ˜θ)
=
h(x, θ) + (˜θ −θ)⊺∇θh(x, θ) + O(∥˜θ −θ∥2)
≤
h(x, θ) + ∥˜θ −
θ∥∥∇θh(x, θ)∥+ O(∥˜θ −θ∥2) by the Taylor expansion. Assuming ﬁrst order
gradients are bounded by positive constant C, then we have |h(x, ˜θ)−h(x, θ)| ≤
C∥˜θ −θ∥+O(∥˜θ −θ∥2), i.e. an upper bound on the mismatch in functional out-
put given parameterizations θ and ˜θ scales with the Euclidian distance between
them.
29

Theory of mind.
Evolution has optimized human beings to be skilled at in-
ferring the thoughts of others, an ability termed “Theory of Mind” (Graziano
and Kastner, 2011; Graziano and Webb, 2015; Kelly et al., 2014; Premack and
Woodruﬀ, 1978). In our model, there is a link between theory of mind and
ineﬀability. If cognitive functions f X
˜φ and f S
˜φ that produce Bob’s conscious ex-
perience ˜s are optimized for decoding m into Alice’s conscious experience s, then
ineﬀability is reduced compared to reconstructing Alice’s conscious experience
from the raw message, K(s|m, ˜φ) ≥K(s|˜s, ˜φ), because part of the computa-
tion of reconstructing s is executed during inference of ˜s, meaning that the
smallest program from ˜s and ˜φ to s would make use of ˜s to reduce its residual
work, shortening the descriptive length of the program. In the extreme case, if
K(s|˜s, ˜φ) = 0, then by deﬁnition Bob’s cognitive function is optimal for inferring
Alice’s conscious experience, since no extra information is required to determine
s.
In turn, if Alice’s parameters φ contain information about Bob’s cognitive
function or parameters ˜φ, she is capable of producing her message m in a way
that maximises eﬀability and minimizes K(s|˜s, ˜φ), since her cognitive function-
ality, including verbal reporting function f M
φ , depend on φ.
The grounding problem.
Two individuals will generally understand the
same word or sentence in diﬀerent ways. For example, if a social group gen-
erally associates cats with femininity and dogs with masculinity, these associ-
ations may be inverted for someone who has a male cat and female dog. A
reasonable model for ineﬀability would account for such diﬀerences in their ex-
periences, regardless of whether the individuals detect such inter-personal dis-
crepancies in their conscious thoughts or verbally express such thoughts. This
is taken into account in 2 ways by our model. First, analogously to the case of
Epφ(s|m)K(s|m, ˜φ), the ineﬀability of Alice’s conscious experience given Bob’s
conscious experience Epφ(s|m)K(s|˜s, ˜φ) pays a penalty that scales with K(pφ|p ˜φ),
which measures a mismatch between p ˜φ and pφ where the latter includes all the
parameters in Alice’s computation graph, including those that parameterize
functions on input data D. This grounds Alice’s φ in a representation that is
shared with Bob’s ˜φ; intuitively, if Bob’s parameters implement a function that
operates diﬀerently on inputs than Alice’s, they do not inform on the latter
and the ceiling on ineﬀability is increased via K(pφ|p ˜φ). In other words, the
objective meaning of s is largely determined by how φ relates s and the input
d: for Bob to understand m well requires him to know something about that
relationship in Alice’s brain, which is given by her parameters φ. Second, con-
scious experience s depends on φ, which includes Alice’s long-term knowledge,
therefore s is capable of containing information about the associations Alice
makes in the process of generating her thoughts, and thus the latter may also
be included in the reconstruction target of K(s|˜s, ˜φ).
30

3.5
Phenomenal and access consciousness
Having provided an information theoretic dynamical systems perspective on
richness and ineﬀability, we now turn explicitly to the question of whether rich
phenomenal experience exists and why we self-report that it does.
We ﬁrst
highlight ambiguities in the meaning of access before contrasting two hypotheses
for explaining the report of phenomenal experience.
3.5.1
Eﬀability, accessibility, reportability
Notions such as ‘accessible’, ‘reportable’ and perhaps ‘eﬀable’ are somewhat
ambiguous. A beneﬁt of our framework is that it allows us to distinguish between
(at least) three distinct notions in the vicinity.
First, the notion we call ‘eﬀability’. As we have presented it above, eﬀability
refers to the ability to accurately describe one variable by another, which implies
it can be formalized using mutual information (Section 3.2).
Second, there is the notion of a variable X being a direct input to a transfor-
mation or process g: we suggest that this is what is often meant by ‘accessiblity’
(of X by g). Crucially, a process that has no (direct) access to X may still have
access to its information via inputs. For instance, given M = g(A), process
g has access to information about X if I(A; X) > 0. Thus a variable may be
eﬀable to a process (to a degree) without being accessible, strictly speaking.
Third, while a reporting process is in general a process or transformation
that outputs to another process, we suggest that in the context of the relevant
debates about consciousness ‘reporting process’ may be understood to refer
speciﬁcally to those that output to processes outside the cortex, such as cortical
processes that encode speech or motor movements. We may then say that a
variable is reportable if it is accessible by a reporting process (in this restricted
sense), where the report corresponds to the output of the reporting process.
Note that so construed, we may dissociate all three notions. Variable X
might be eﬀable to process g but not strictly accessible by it, and variable
X might be strictly accessible by g but not reportable, if g does not output
to processes outside of the cortex. These distinctions will be helpful in what
follows.
3.5.2
Existence and report of phenomenal experience
According to the Global Workspace Theory, information from diverse brain re-
gions corresponding to a variety of perceptual or cognitive processes is selected
for inclusion in the contents of a centralized processing workspace associated
with working memory that coordinates and communicates with multiple sub-
systems, resulting in a rich space of “highly diﬀerentiated” states with “high
complexity” (Tononi and Edelman, 1998).
The features of this global workspace system make it suitable as a frame-
work for an analysis of consciousness (i.e., phenomenal consciousness), even if
we deny that only items in workspace are conscious. The features of the global
31

workspace system also make it a suitable target for modelling in terms of attrac-
tor dynamics, since by their nature, states ampliﬁed and sustained in a central
processing workspace are attractors. Thus, our model allows for the reﬁnement
of theses concerning the relationship of consciousness to the global workspace.
Global workspace models of consciousness (Dehaene and Naccache, 2001)gen-
erally divide representations into three classes:
1. Those not directly accessible to working memory processes (unconscious).
2. Those mobilized in the workspace via ampliﬁcation and made accessible
to downstream processing (conscious).
3. Those accessible by working memory processes but not suﬃciently ampli-
ﬁed or attended to be released by the workspace.
The latter may include non-attractor transient states in an attractor model
of working memory, and being rich and unreportable, are a clear candidate for
the basis of phenomenal experience (Dehaene and Naccache, 2001). It is a point
of debate between adherents of the global workspace framework, whether or not
items from the third class are indeed conscious. Some say no (Cohen et al.,
2016; Naccache, 2018), others say yes (Prinz, 2012).
By allowing f S
φ to be abstract, our model only speciﬁes that S is a deter-
ministic projection of φ, X and A, and therefore is compatible with both views.
However, our model informs the issue, and oﬀers some support for the more
permissive position.
If one assumes that attractor states are included in the content of con-
sciousness and that the physical basis of transient states and attractor states in
working memory is the same (i.e. they are diﬀerentiated by duration of atten-
tional ampliﬁcation, not location of neural circuitry), it would be reasonable to
believe that transient states are also included in conscious awareness. If this is
the case, then transient states are rich states that are consciously experienced
but not directly accessible or reportable by downstream processes, while being
partially verbally eﬀable because of shared information with attractors which
are reportable. In this paradigm, the ﬂeeting nature of transient states impacts
their reportability but not their inclusion in conscious experience. This is an
attractive position partly because it takes phenomenology seriously - people
report their experience being much richer than they are able to articulate.
One challenge for those who hold the more permissive view (allowing some
items from class three to be conscious) is how exactly to delineate the boundary
between the conscious and the unconscious. Some such as (Prinz, 2012) appeal
to the notion of ‘attendedness’ to do so. However, it is not obvious how to
understand the relevant notion of attendedness. Our model may inform this
debate, by oﬀering another way of thinking about the relevant processes (e.g.
as transient states within attractors basins but not yet at attractors), which in
turn may be useful in reﬁning relevant conceptions of attention.
Another set of questions that arise for proponents of the global workspace
framework is how to generally delineate the class of working memory processes,
32

and accordingly, how to delineate the conscious from the unconscious (processes
from class one). For example, it is often debated whether the fragile short-term
iconic representations of a Sperling grid are conscious, and often taken as given
that these do not ﬁgure in working memory.
But we may be able to make
progress on this issue by formulating a more precise criterion for being a working
memory process as a constraint on attractor structure.
Regardless of whether transient states are included in the contents of con-
sciousness, the attractor model for working memory suggests a second expla-
nation for the self-report of phenomenal experience: an attractor state may
encode information about its basin of attraction. For example, point attractor
states may include dimensions whose values estimate the size of its local basin,
which is a measure of the information loss when going from transient states
in trajectories within that basin to the attractor state itself. This posits that
rich phenomenal experience exists, whether inside or outside the delimitation
of working memory processes, and its properties - such as richness - would be
reportable, even if the transient states that support them are not. It is plau-
sible that conscious awareness of abstract attributes of transient states such as
richness would be advantageous, for instance when reasoning about one’s uncer-
tainty, including for the purpose of anticipating the listener’s uncertainty when
engaging in theory of mind to minimize ineﬀability (Section 3.4.2).
These arguments suggest that Block’s distinction between phenomenal and
access consciousness is not due to a categorical diﬀerence between fundamen-
tally diﬀerent kinds of processing (Block, 1995) but rather to a diﬀerence in the
representational stage of the same information processing function (Dehaene
and Naccache, 2001), and that the existence of a rich phenomenological experi-
ence that exceeds our reporting abilities (Sperling, 1960) is both justiﬁable and
veridically reportable. Unpacking the implications of the model is an important
task for future work.
4
Conclusion
This paper characterizes the rich and ineﬀable nature of conscious experience
from an information theoretic perspective. It connects the ordinary notion of
ineﬀability with mathematical formalisms of information loss, describing how
the latter arises as a result of computation in cognitive processing, how it is
implemented by an attractor model for working memory, and how it may be
increased by the compressed nature of language as well as diﬀerences in the
cognitive processing functions of individuals.
Attractor dynamics may be considered an attentional process: out of many,
one or a few states are selected. This connects our work not only to Global
Workspace Theory but more broadly to research in machine learning on atten-
tion mechanisms. We generally observe that attention, e.g., as introduced in
deep learning by (Bahdanau et al., 2014), may be used to name any function
that incurs signiﬁcant information loss and is present in both artiﬁcial and bio-
logical cognitive systems, where it is – at present – commonly modelled by the
33

family of attention-based and transformer architectures (Bahdanau et al., 2014;
Chorowski et al., 2015; Devlin et al., 2018; Khan et al., 2022) and dynamical
systems (Khona and Fiete, 2022; Rolls, 2007a) respectively.
In this work we use a simple model to reason about emitter-receptor com-
munication, where the past is conditioned on implicitly via parameters φ and
stochasticity in dynamics. An alternative would be to model more complex com-
munication patterns explicitly. We have also not considered learning objectives
for function parameters. Doing so would enable a discussion on the generaliza-
tion beneﬁts of the inductive bias (Goyal and Bengio, 2022) giving rise to this
information loss: intuitively, how simpler representations support the successful
extrapolation of behavior beyond previously seen inputs. Information bottle-
necks are a popular training regularizer in machine learning (Alemi et al., 2016;
Tishby et al., 2000) but are understudied in the context of biologically plausible
models, despite generalization ability being a key diﬀerence between humans
and current artiﬁcial learning systems. Considering the beneﬁts of information
loss may allow us to understand ineﬀability more deeply; not just how it arises,
but why.
References
Alemi, A. A., Fischer, I., Dillon, J. V., and Murphy, K. (2016). Deep variational
information bottleneck. arXiv preprint arXiv:1612.00410.
Alter, T. and Walter, S. (2006). Phenomenal Concepts and Phenomenal Knowl-
edge: New Essays on Consciousness and Physicalism.
Oxford University
Press.
Baars, B. J. (1993). A cognitive theory of consciousness. Cambridge University
Press.
Baars, B. J. (2005). Global workspace theory of consciousness: toward a cogni-
tive neuroscience of human experience. In Laureys, S., editor, The Boundaries
of Consciousness: Neurobiology and Neuropathology, volume 150 of Progress
in Brain Research, pages 45–53. Elsevier.
Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by
jointly learning to align and translate. arXiv:1409.0473, AISTATS’2015.
Barak, O. and Tsodyks, M. (2014). Working models of working memory. Current
opinion in neurobiology, 25:20–24.
Bengio, Y., Ducharme, R., and Vincent, P. (2000). A neural probabilistic lan-
guage model. Advances in neural information processing systems, 13.
Block, N. (1995). On a confusion about a function of consciousness. Behavioral
and Brain Sciences, 18(2):227–247.
34

Block, N. (2007). Overﬂow, access, and attention. Behavioral and Brain Sci-
ences, 30(5-6):530–548.
Bronfman, Z. Z., Jacobson, H., and Usher, M. (2019). Impoverished or rich con-
sciousness outside attentional focus: Recent data tip the balance for overﬂow.
Mind & Language, 34(4):423–444.
Burak, Y. (2014). Spatial coding and attractor dynamics of grid cells in the
entorhinal cortex. Current opinion in neurobiology, 25:169–175.
Bushnell, M. C., Goldberg, M. E., and Robinson, D. L. (1981).
Behavioral
enhancement of visual responses in monkey cerebral cortex. i. modulation
in posterior parietal cortex related to selective visual attention. journal of
Neurophysiology, 46(4):755–772.
Chalmers, D. J. (1996).
The Conscious Mind: In Search of a Fundamental
Theory. Oxford University Press.
Chalmers, D. J. (2010).
The character of consciousness.
Oxford University
Press.
Changeux, J.-P. and Dehaene, S. (1989). Neuronal models of cognitive functions.
Cognition, 33(1-2):63–109.
Chaudhuri, R. and Fiete, I. (2019). Bipartite expander hopﬁeld networks as self-
decoding high-capacity error correcting codes. Advances in neural information
processing systems, 32.
Chelazzi, L. (1999). Serial attention mechanisms in visual search: a critical look
at the evidence. Psychological research, 62(2):195–219.
Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., and Bengio, Y. (2015).
Attention-based models for speech recognition. Advances in neural informa-
tion processing systems, 28.
Chuard, P. (2007). The riches of experience. Journal of Consciousness Studies,
14(9-10):20–42.
Churchland, M. M., Cunningham, J. P., Kaufman, M. T., Foster, J. D., Nuyu-
jukian, P., Ryu, S. I., and Shenoy, K. V. (2012). Neural population dynamics
during reaching. Nature, 487(7405):51–56.
Cohen, L. and Dehaene, S. (1998).
Competition between past and present.
assessment and interpretation of verbal perseverations. Brain: a journal of
neurology, 121(9):1641–1659.
Cohen, M. A. and Dennett, D. C. (2011). Consciousness cannot be separated
from function. Trends in cognitive sciences, 15(8):358–364.
Cohen, M. A., Dennett, D. C., and Kanwisher, N. (2016). What is the bandwidth
of perceptual experience? Trends in Cognitive Sciences, 20(5):324–335.
35

Conee, E. (1985). Physicalism and phenomenal properties. Philosophical Quar-
terly, 35(July):296–302.
Constantinidis, C., Franowicz, M. N., and Goldman-Rakic, P. S. (2001). Coding
speciﬁcity in cortical microcircuits: a multiple-electrode analysis of primate
prefrontal cortex. Journal of Neuroscience, 21(10):3646–3655.
Cowan, N. (2008). What are the diﬀerences between long-term, short-term, and
working memory? Progress in brain research, 169:323–338.
Curtis, C. E. and D’Esposito, M. (2003). Persistent activity in the prefrontal
cortex during working memory. Trends in cognitive sciences, 7(9):415–423.
Deco, G. and Rolls, E. T. (2003). Attention and working memory: a dynamical
model of neuronal activity in the prefrontal cortex.
European Journal of
Neuroscience, 18(8):2374–2390.
Dehaene, S., Kerszberg, M., and Changeux, J.-P. (1998). A neuronal model of
a global workspace in eﬀortful cognitive tasks. Proceedings of the national
Academy of Sciences, 95(24):14529–14534.
Dehaene, S. and Naccache, L. (2001).
Towards a cognitive neuroscience of
consciousness: basic evidence and a workspace framework. Cognition, 79(1-
2):1–37.
Dennett, D. (2006). What robomary knows. In Alter, T. and Walter, S., ed-
itors, Phenomenal Concepts and Phenomenal Knowledge: New Essays on
Consciousness and Physicalism. Oxford University Press.
Dennett, D. C. (1993). Consciousness explained. Penguin uk.
Descartes, Ren´e, .-. (1986). Discourse on method. New York : Macmillan ; Lon-
don : Collier Macmillan, 1986. Originally published: Indianapolis : Bobbs-
Merrill, 1950.;Translation of Discours de la m´ethode.;Bibliography: page xxii.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training
of deep bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805.
Driscoll, L., Shenoy, K., and Sussillo, D. (2022). Flexible multitask computation
in recurrent networks utilizes shared dynamical motifs. bioRxiv.
Dumas, G., Chavez, M., Nadel, J., and Martinerie, J. (2012).
Anatomical
connectivity inﬂuences both intra-and inter-brain synchronizations. PloS one,
7(5):e36414.
Durstewitz, D., Seamans, J. K., and Sejnowski, T. J. (2000). Neurocomputa-
tional models of working memory. Nature neuroscience, 3(11):1184–1191.
Engle, R. W. (2002). Working memory capacity as executive attention. Current
directions in psychological science, 11(1):19–23.
36

Favela, L. H. (2021). The dynamical renaissance in neuroscience. Synthese,
199(1):2103–2127.
Frankish, K. (2016). Illusionism as a theory of consciousness. Journal of Con-
sciousness Studies, 23(11-12):11–39.
Gnadt, J. W. and Andersen, R. A. (1988). Memory related motor planning
activity in posterior parietal cortex of macaque. Experimental brain research,
70(1):216–220.
Godfrey-Smith, P. (2016). Mind, matter, and metabolism. Journal of Philoso-
phy, 113(10):481–506.
Goldstein, P., Weissman-Fogel, I., Dumas, G., and Shamay-Tsoory, S. G. (2018).
Brain-to-brain coupling during handholding is associated with pain reduction.
Proceedings of the national academy of sciences, 115(11):E2528–E2537.
Goyal, A. and Bengio, Y. (2022). Inductive biases for deep learning of higher-
level cognition. Proceedings of the Royal Society A: Mathematical, Physical
and Engineering Sciences, 478(2266):20210068.
Graziano, M. S., Guterstam, A., Bio, B. J., and Wilterson, A. I. (2020). To-
ward a standard model of consciousness: Reconciling the attention schema,
global workspace, higher-order thought, and illusionist theories. Cognitive
Neuropsychology, 37(3-4):155–172.
Graziano, M. S. and Kastner, S. (2011). Human consciousness and its rela-
tionship to social neuroscience: A novel hypothesis. Cognitive neuroscience,
2(2):98–113.
Graziano, M. S. and Webb, T. W. (2015). The attention schema theory: a
mechanistic account of subjective awareness. Frontiers in psychology, page
500.
Gr¨unwald, P. and Vit´anyi, P. M. B. (2004).
Shannon information and kol-
mogorov complexity. CoRR, cs.IT/0410002.
Gr¨unwald, P. D. and Vit´anyi, P. (2003). Kolmogorov complexity and informa-
tion theory. with an interpretation in terms of questions and answers. Journal
of Logic, Language and Information, 12(4):497–529.
Herculano-Houzel, S. (2009). The human brain in numbers: a linearly scaled-up
primate brain. Frontiers in human neuroscience, page 31.
Hinton, G. E., McClelland, J. L., and Rumelhart, D. E. (1986). Distributed
representations. In Rumelhart, D. E. and McClelland, J. L., editors, Paral-
lel Distributed Processing: Explorations in the Microstructure of Cognition.
Volume 1: Foundations, pages 77–109. MIT Press, Cambridge, MA.
37

Hopﬁeld, J. J. (1982). Neural networks and physical systems with emergent
collective computational abilities.
Proceedings of the national academy of
sciences, 79(8):2554–2558.
Humphrey, N. (2020). The invention of consciousness. Topoi, 39(1):13–21.
Jackson, F. (1986).
What mary didn’t know.
The Journal of Philosophy,
83(5):291–295.
James, W. (1892). The stream of consciousness. In James, W., editor, Psychol-
ogy, pages 71–82. MIT Press.
Kelly, Y. T., Webb, T. W., Meier, J. D., Arcaro, M. J., and Graziano, M. S.
(2014). Attributing awareness to oneself and to others. Proceedings of the
National Academy of Sciences, 111(13):5012–5017.
Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., and Shah, M.
(2022). Transformers in vision: A survey. ACM computing surveys (CSUR),
54(10s):1–41.
Khona, M. and Fiete, I. R. (2022). Attractor and integrator networks in the
brain. Nature Reviews Neuroscience, pages 1–23.
Kirby, S., Tamariz, M., Cornish, H., and Smith, K. (2015). Compression and
communication in the cultural evolution of linguistic structure. Cognition,
141:87–102.
Kolmogorov, A. N. (1965).
Three approaches to the quantitative deﬁnition
oﬁnformation’. Problems of information transmission, 1(1):1–7.
Kurt, S., Deutscher, A., Crook, J. M., Ohl, F. W., Budinger, E., Moeller, C. K.,
Scheich, H., and Schulze, H. (2008). Auditory cortical contrast enhancing by
global winner-take-all inhibitory interactions. PLoS One, 3(3):e1735.
Lamme, V. A. (2007). Sue ned block!: Making a better case for p-consciousness.
Behavioral and Brain Sciences, 30(5-6):511–512.
Levine, J. (1993). On leaving out what it’s like. na.
Lewis, D. K. (1990). What experience teaches. In Lycan, W. G., editor, Mind
and Cognition, pages 29–57. Blackwell.
Li, M., Vit´anyi, P., et al. (2008). An introduction to Kolmogorov complexity and
its applications, volume 3. Springer.
Lin, A. C., Bygrave, A. M., De Calignon, A., Lee, T., and Miesenb¨ock, G.
(2014). Sparse, decorrelated odor coding in the mushroom body enhances
learned odor discrimination. Nature neuroscience, 17(4):559–568.
Loar, B. (1990). Phenomenal states. Philosophical Perspectives, 4:81–108.
38

Maheswaranathan, N., Williams, A., Golub, M., Ganguli, S., and Sussillo, D.
(2019). Universality and individuality in neural dynamics across large pop-
ulations of recurrent networks. Advances in neural information processing
systems, 32.
Merriam Webster Dictionary (2023). Rich. https://www.merriam-webster.
com/dictionary/rich. Accessed: 2023-01-30.
Michaels, J. A., Dann, B., and Scherberger, H. (2016). Neural population dy-
namics during reaching are better explained by a dynamical system than
representational tuning. PLoS computational biology, 12(11):e1005175.
Miller, E. and Buschman, T. (2015). Working memory capacity: Limits on the
bandwidth of cognition. Daedalus, 144.
Miller, E. K., Li, L., and Desimone, R. (1993). Activity of neurons in ante-
rior inferior temporal cortex during a short-term memory task. Journal of
Neuroscience, 13(4):1460–1478.
Morin, F. and Bengio, Y. (2005). Hierarchical probabilistic neural network lan-
guage model. In International workshop on artiﬁcial intelligence and statis-
tics, pages 246–252. PMLR.
Naccache, L. (2018). Why and how access consciousness can account for phe-
nomenal consciousness. Philosophical Transactions of the Royal Society B:
Biological Sciences, 373(1755):20170357.
Nagel, T. (1974).
What is it like to be a bat?
The philosophical review,
83(4):435–450.
Oxford English Dictionary (2023). “ineﬀable”. https://www.oed.com/view/
Entry/94904?redirectedFrom=ineffable#eid. Accessed: 2023-01-30.
Phillips, I. (2016). No watershed for overﬂow: Recent work on the richness of
consciousness. Philosophical Psychology, 29(2):236–249.
Premack, D. and Woodruﬀ, G. (1978). Does the chimpanzee have a theory of
mind? Behavioral and brain sciences, 1(4):515–526.
Prinz, J. (2012). The Conscious Brain: How Attention Engenders Experience.
Oup Usa.
Rabin, G. (2011). Conceptual mastery and the knowledge argument. Philosoph-
ical Studies, 154(1):125–147.
Rabinovich, M. I., Huerta, R., Varona, P., and Afraimovich, V. S. (2008). Tran-
sient Cognitive Dynamics, Metastability, and Decision Making. PLOS Com-
putational Biology, 4(5):e1000072. Publisher: Public Library of Science.
39

Ramsauer, H., Sch¨aﬂ, B., Lehner, J., Seidl, P., Widrich, M., Adler, T., Gruber,
L., Holzleitner, M., Pavlovi´c, M., Sandve, G. K., et al. (2020).
Hopﬁeld
networks is all you need. arXiv preprint arXiv:2008.02217.
Redish, A. D., Elga, A. N., and Touretzky, D. S. (1996). A coupled attractor
model of the rodent head direction system. Network: computation in neural
systems, 7(4):671.
Renart, A., Parga, N., and Rolls, E. (1999). A recurrent model of the interaction
between prefrontal and inferotemporal cortex in delay tasks.
Advances in
Neural Information Processing Systems, 12.
Roessig, S., M¨ucke, D., and Grice, M. (2019).
The dynamics of intonation:
Categorical and continuous variation in an attractor-based model. PloS one,
14(5).
Rolls, E. T. (2007a). An attractor network in the hippocampus: theory and
neurophysiology. Learning & memory, 14(11):714–731.
Rolls, E. T. (2007b). Memory, Attention, and Decision-Making: A Unifying
Computational Neuroscience Approach. Oxford University Press.
Rolls, E. T. (2010). Attractor networks. Wiley Interdisciplinary Reviews: Cog-
nitive Science, 1(1):119–134.
Rolls, E. T. and Tovee, M. J. (1994). Processing speed in the cerebral cortex
and the neurophysiology of visual masking. Proceedings of the Royal Society
of London. Series B: Biological Sciences, 257(1348):9–15.
Roweis, S. T. and Saul, L. K. (2000). Nonlinear dimensionality reduction by
locally linear embedding. science, 290(5500):2323–2326.
Russell, B. (1992). Human Knowledge: Its Scope and Limits. New York, USA:
Simon and Schuster.
Sch¨afer, A. M. and Zimmermann, H.-G. (2007). Recurrent neural networks are
universal approximators. International journal of neural systems, 17(04):253–
263.
Schurger, A., Pereira, F., Treisman, A., and Cohen, J. D. (2010). Reproducibil-
ity distinguishes conscious from nonconscious neural representations. Science,
327(5961):97–99.
Schurger, A., Sarigiannidis, I., Naccache, L., Sitt, J. D., and Dehaene, S. (2015).
Cortical activity is more stable when sensory stimuli are consciously perceived.
Proceedings of the National Academy of Sciences, 112(16):E2083–E2092.
Seeholzer, A., Deger, M., and Gerstner, W. (2019). Stability of working memory
in continuous attractor networks under the control of short-term plasticity.
PLoS computational biology, 15(4):e1006928.
40

Shannon, C. E. (1948). A mathematical theory of communication. The Bell
system technical journal, 27(3):379–423.
Shenoy, K. V., Sahani, M., Churchland, M. M., et al. (2013). Cortical control
of arm movements: a dynamical systems perspective. Annu Rev Neurosci,
36(1):337–359.
Sperling, G. (1960).
The information available in brief visual presentations.
Psychological monographs: General and applied, 74(11):1.
Stevens, C. F. (2015). What the ﬂy’s nose tells the ﬂy’s brain. Proceedings of
the National Academy of Sciences, 112(30):9460–9465.
Sussillo, D. and Barak, O. (2013). Opening the black box: low-dimensional
dynamics in high-dimensional recurrent neural networks. Neural computation,
25(3):626–649.
Tiitinen, H., May, P., Reinikainen, K., and N¨a¨at¨anen, R. (1994).
Attentive
novelty detection in humans is governed by pre-attentive sensory memory.
Nature, 372(6501):90–92.
Tishby, N., Pereira, F. C., and Bialek, W. (2000). The information bottleneck
method. arXiv preprint physics/0004057.
Tononi, G. and Edelman, G. M. (1998). Consciousness and complexity. science,
282(5395):1846–1851.
Tsuda, I. (2015). Chaotic itinerancy and its roles in cognitive neurodynamics.
Current Opinion in Neurobiology, 31:67–71.
Tye, M. (2006).
Nonconceptual content, richness, and ﬁneness of grain.
In
Gendler, T. S. and Hawthorne, J., editors, Perceptual Experience, pages 504–
30. Oxford University Press.
Vandenbroucke, A. R., Sligte, I. G., Fahrenfort, J. J., Ambroziak, K. B., and
Lamme, V. A. (2012). Non-attended representations are perceptual rather
than unconscious in nature. PLoS One, 7(11):e50042.
Varela, F. (1999). The specious present: A neurophenomenology of time con-
sciousness. In Petitot, J., Varela, F. J., Pacoud, B., and Roy, J., editors,
Naturalizing Phenomenology, pages 266–314. Stanford University Press.
Wang, X.-J. (2002).
Probabilistic decision making by slow reverberation in
cortical circuits. Neuron, 36(5):955–968.
Wang, X.-J. (2008). Decision making in recurrent neuronal circuits. Neuron,
60(2):215–234.
Ward, E. J. (2018). Downgraded phenomenology: how conscious overﬂow lost
its richness.
Philosophical Transactions of the Royal Society B: Biological
Sciences, 373(1755):20170355.
41

Wong, K.-F. and Wang, X.-J. (2006). A recurrent network mechanism of time
integration in perceptual decisions. Journal of Neuroscience, 26(4):1314–1328.
Zhang, K. (1996). Representation of spatial orientation by the intrinsic dynam-
ics of the head-direction cell ensemble: a theory. Journal of Neuroscience,
16(6):2112–2126.
42

