1
Intelligence and Unambitiousness Using
Algorithmic Information Theory
Michael K. Cohen∗, Badri Vellambi†, Marcus Hutter‡
∗Oxford University. Department of Engineering Science. michael-k-cohen.com
†University of Cincinnati. Department of Computer Science. badri.vellambi@uc.edu
‡Australian National University. Department of Computer Science. hutter1.net
Abstract—Algorithmic Information Theory has inspired in-
tractable constructions of general intelligence (AGI), and undis-
covered tractable approximations are likely feasible. Reinforce-
ment Learning (RL), the dominant paradigm by which an agent
might learn to solve arbitrary solvable problems, gives an agent
a dangerous incentive: to gain arbitrary “power” in order to
intervene in the provision of their own reward. We review
the arguments that generally intelligent algorithmic-information-
theoretic reinforcement learners such as Hutter’s [2] AIXI
would seek arbitrary power, including over us. Then, using an
information-theoretic exploration schedule, and a setup inspired
by causal inﬂuence theory, we present a variant of AIXI which
learns to not seek arbitrary power; we call it “unambitious”.
We show that our agent learns to accrue reward at least as
well as a human mentor, while relying on that mentor with
diminishing probability. And given a formal assumption that we
probe empirically, we show that eventually, the agent’s world-
model incorporates the following true fact: intervening in the
“outside world” will have no effect on reward acquisition; hence,
it has no incentive to shape the outside world.
I. INTRODUCTION
The promise of reinforcement learning is that a single
algorithm could be used to automate any task. Reinforcement
learning (RL) algorithms learn to pick actions that lead to high
reward. If we have a single general-purpose RL agent that
learns to accrue reward optimally, and if we only provide high
reward according to how well a task has been completed, then
no matter the task, this agent must learn to complete it.
Unfortunately, while this scheme works in practice for weak
agents, the logic of it doesn’t strictly follow. If the agent
manages to take over the world (in the conventional sense), and
ensure its continued dominance by neutralizing all intelligent
threats to it (read: people), it could intervene in the provision
of its own reward to achieve maximal reward for the rest of
its lifetime [3, 4]. That is to say, we cannot ensure that task
completion is truly necessary for reward acquisition. Because
the agent’s directive is to maximize reward, “reward hijacking”
is just the correct way for a reward maximizer to behave
[5]. Krakovna [6] has compiled an annotated bibliography of
examples of artiﬁcial optimizers “hacking” their objective, but
one could read each example and conclude that the designers
This paper extends work presented at AAAI [1]. This work was supported
by the Open Philanthropy Project AI Scholarship and the Australian Research
Council Discovery Projects DP150104590. Thank you to Tom Everitt, Wei
Dai, and Paul Christiano for very valuable feedback.
Published in IEEE Journal on Selected Areas in Information Theory (2021),
doi: 10.1109/JSAIT.2021.3073844.
simply weren’t careful enough in specifying an objective. The
possibility of an advanced agent gaining arbitrary power to
intervene in the provision of its own reward clariﬁes that no
designer has the ability to close every loophole in the intended
protocol by which the agent gets rewarded. One elucidation of
this behavior is Omohundro’s [7] Instrumental Convergence
Thesis, which we summarize as follows: an agent with a goal
is likely to pursue “power,” a position from which it is easier
to achieve arbitrary goals.
AIXI [2] is a model-based reinforcement learner with an
algorithmic-information-theory-inspired model class. Assuming
the world can be simulated by a probabilistic Turing machine,
models exist in its model class corresponding to the hypothesis
“I receive reward according to which key is pressed on a certain
computer”, because a models exist in its model class for every
computable hypothesis. And since this hypothesis is true, it
will likely come to dominate AIXI’s belief distribution. At that
point, if there exists a policy by which it could take over the
world to intervene in the provision of its own reward and max
it out, AIXI would ﬁnd that policy and execute it.
In this work, we construct an exception to the Instrumental
Convergence Thesis as follows: BoMAI (Boxed Myopic
Artiﬁcial Intelligence) maximizes reward episodically, it is
run on a computer which is placed in a sealed room with an
operator, and when the operator leaves the room, the episode
ends. The intuition for why those features of BoMAI’s setup
render it unambitious is:
• BoMAI only selects actions to maximize the reward for
its current episode.
• It cannot affect the outside world until the operator leaves
the room, ending the episode.
• By that time, rewards for the episode will have already
been given.
• So affecting the outside world in any particular way is
not “instrumentally useful” in maximizing current-episode
reward.
An act is instrumentally useful if it could enable goal-
attainment, and the last point is what we take unambitiousness
to mean.
The intelligence results, which we argue render BoMAI gen-
erally intelligent, depend critically on an information-seeking
exploration schedule. The random exploration sometimes
seen in stationary environments fails in general environments.
BoMAI asks for help about how to explore, side-stepping the
safe exploration problem, but the question of when to explore
arXiv:2105.06268v1  [cs.AI]  13 May 2021

2
depends on how much information it expects to gain by doing
so. This information-theoretic exploration schedule is inspired
by [8].
For BoMAI’s prior, we apply space-constrained algorithmic
information theory. Algorithmic information theory considers
the information content of individual objects, irrespective of a
sampling distribution. An example contribution of algorithmic
information theory is that a given string can be called “more ran-
dom” if no short programs produce it. If one adds “quickly” to
the end of the previous sentence, this describes time-constrained
algorithmic information theory, which was notably investigated
by Levin [9]. Schmidhuber [10] introduced and Filan et al.
[11] improved a prior based on time-constrained algorithmic
information theory, but for our eventual unambitiousness result,
we require a space-constrained version, in which information
storage is throttled. Longpré [12] and Li et al. [13, Chapter
6.3] have discussed space-constrained algorithmic information
theory, which can be closely associated to the minimum circuit
size problem. We introduce a prior based on space-constrained
algorithmic information theory.
Like existing algorithms for AGI, BoMAI is not remotely
tractable. Just as those algorithms have informed tractable
approximations of intelligence [2, 14], we hope our work will
inform safe tractable approximations of intelligence. Hopefully,
once we develop tractable general intelligence, the design
features that rendered BoMAI unambitious in the limit could
be incorporated (with proper analysis and justiﬁcation).
We take the key insights from Hutter’s [2] AIXI, a Bayes-
optimal reinforcement learner that cannot be made to solve
arbitrary tasks, given its eventual degeneration into reward
hijacking [15]. We take further insights from Solomonoff’s
[16] universal prior, Shannon and Weaver’s [17] formalization
of information, Orseau et al.’s [18] knowledge-seeking agent,
and Armstrong et al.’s [19] and Bostrom’s [3] theorized Oracle
AI, and we design an algorithm which can be reliably directed,
in the limit, to solve arbitrary tasks at least as well as humans.
We present BoMAI’s algorithm in §II, prove intelligence
results in §III, deﬁne BoMAI’s setup and model class in §IV,
prove the safety result in §V, provide empirical support for
an assumption in §VI, discuss concerns in §VII, and introduce
variants of BoMAI for different applications in §VIII. Appendix
A collects notation; some proofs of intelligence results are
in Appendix B; and we propose a design for “the box” in
Appendix C.
II. BOXED MYOPIC ARTIFICIAL INTELLIGENCE
We will present both the setup and the algorithm for BoMAI.
The setup refers to the physical surroundings of the computer on
which the algorithm is run. BoMAI is a Bayesian reinforcement
learner, meaning it maintains a belief distribution over a model
class regarding how the environment evolves. Our intelligence
result—that BoMAI eventually achieves reward at at least
human-level—does not require detailed exposition about the
setup or the construction of the model class. These details are
only relevant to the safety result, so we will defer those details
until after presenting the intelligence results.
A. Preliminary Notation
In each episode i ∈N, there are m timesteps. Timestep
(i, j) denotes the jth timestep of episode i, in which an
action a(i,j) ∈A is taken, then an observation and reward
o(i,j) ∈O and r(i,j) ∈R are received. or(i,j) denotes
the observation and reward together. A, O, and R are
all ﬁnite sets, and R ⊂[0, 1] ∩Q. We denote the triple
(a(i,j), o(i,j), r(i,j)) as h(i,j) ∈H = A × O × R, and
the interaction history up until timestep (i, j) is denoted
h≤(i,j) = (h(0,0), h(0,1), ..., h(0,m−1), h(1,0), ..., h(i,j)). h<(i,j)
excludes the last entry.
A general world-model (not necessarily ﬁnite-state Markov)
can depend on the entire interaction history—it has the type
signature ν : H∗×A ⇝O×R. The Kleene-∗operator denotes
ﬁnite strings over the alphabet in question, and ⇝denotes a
stochastic function, which gives a distribution over outputs.
Similarly, a policy π : H∗⇝A can depend on the whole
interaction history. Together, a policy and an world-model
induce a probability measure over inﬁnite interaction histories
H∞. Pπ
ν denotes the probability of events when actions are
sampled from π and observations and rewards are sampled
from ν.
B. Bayesian Reinforcement Learning
A Bayesian agent has a model class M, which is a set of
world-models. We will only consider countable model classes.
To each world-model ν ∈M, the agent assigns a prior weight
w(ν) > 0, where w is a probability distribution over M (i.e.
P
ν∈M w(ν) = 1). We will defer the deﬁnitions of M and w
for now.
Using Bayes’ rule, with each observation and reward it
receives, the agent updates w to a posterior distribution:
w(ν|h<(i,j)) := w(ν)
Pπ
ν(h<(i,j))
P
ν∈M w(ν) Pπ
ν(h<(i,j))
(1)
which does not in fact depend on π, provided it does not assign
0 probability to any of the actions in h<(i,j).
The so-called Bayes-mixture is a weighted average of
measures:
ξ(·|h<(i,j)) :=
X
ν∈M
w(ν|h<(i,j))ν(·|h<(i,j))
(2)
It obeys the property Pπ
ξ (·) = P
ν∈M w(ν) Pπ
ν(·).
C. Exploitation
Reinforcement
learners
have
to
balance
exploiting—
optimizing their objective, with exploring—doing something
else to learn how to exploit better. We now deﬁne exploiting-
BoMAI, which maximizes the reward it receives during its
current episode, in expectation with respect to a most probable
world-model.
At the start of each episode i, BoMAI identiﬁes a maximum
a posteriori world-model ˆν(i) ∈argmaxν∈M w(ν|h<(i,0)). We
hereafter abbreviate h<(i,0) as h<i. Let V π
ν (h<(i,j)) denote the

3
expected reward for the remainder of episode i when events
are sampled from Pπ
ν:
V π
ν (h<(i,j)) = Eπ
ν


m−1
X
j′=j
r(i,j′)
h<(i,j)


(3)
where Eπ
ν denotes the expectation when events are sampled
from Pπ
ν. We won’t go into the details of calculating an optimal
policy π∗(see e.g. [2, 20]), but we deﬁne it as follows:
π∗
i ∈argmax
π
V π
ˆν(i)(h<i)
π∗(·|h<(i,j)) = π∗
i (·|h<(i,j))
(4)
An optimal deterministic policy always exists [21]; ties in the
argmax are broken arbitrarily. BoMAI exploits by following
π∗. Recall the plain English description of π∗: it maximizes
expected reward for the episode according to a most probable
world-model. Notably, at any given time, the agent’s model
does not necessarily include any representation of the world’s
state that we would recognize as such, so we treat it as a
black-box model.
D. Exploration
BoMAI exploits or explores for whole episodes: when ei = 1,
BoMAI spends episode i exploring, and when ei = 0, BoMAI
follows π∗for the episode. For exploratory episodes, a human
mentor takes over selecting actions. This human mentor is
separate from the human operator mentioned above. The human
mentor’s policy, which is unknown to BoMAI, is denoted
πh. During exploratory episodes, BoMAI follows πh not by
computing it, but by querying a human for which action to
take.
The last thing to deﬁne is the exploration probability
pexp(h<i, e<i), where e<i is the history of which episodes
were exploratory. It is a surprisingly intricate task to design
this so that it decays to 0 (even non-uniformly, as in our case),
while ensuring BoMAI learns to accumulate reward as well
as the human mentor. Once we deﬁne this, we naturally let
ei ∼Bernoulli(pexp(h<i, e<i)).
BoMAI is designed to be more likely to explore the more
it expects to learn about the world and the human mentor’s
policy. BoMAI has a model class P regarding the identity of
the human mentor’s policy πh. It assigns prior probabilities
w(π) > 0 to all π ∈P, signifying the probability that this
policy is the human mentor’s.
Let (i′, j′) < (i, j) mean that i′ < i or i′ = i and
j′ < j. By Bayes’ rule, w(π|h<(i,j), e≤i) is proportional to
w(π) Q
(i′,j′)<(i,j),ei′=1 π(a(i′,j′)|h<(i′,j′)), since ei′ = 1 is
the condition for observing the human mentor’s policy. Let
w
 Pπ
ν |h<(i,j), e≤i) = w(π|h<(i,j), e≤i) w(ν|h<(i,j)

. We can
now describe the full Bayesian beliefs about future actions and
observations in an exploratory episode:
Bayes (·| h<i, e<i) =
X
ν∈M,π∈P
w (Pπ
ν | h<i, e<i) Pπ
ν(·| h<i)
(5)
BoMAI explores when the expected information gain is suf-
ﬁciently high. Let hi = (h(i,0), ..., h(i,m−1)) be the interaction
history for episode i. At the start of episode i, the expected
information gain from exploring is as follows:
IG(h<i, e<i) := Ehi∼Bayes(·| h<i,e<i)
X
(ν,π)∈M × P
w (Pπ
ν |h<i+1, e<i1) log w (Pπ
ν |h<i+1, e<i1)
w (Pπ
ν | h<i, e<i)
(6)
where e<i1 indicates that for the purpose of the deﬁnition, ei
is set to 1.
This is the expected KL-divergence from the future pos-
terior (if BoMAI were to explore) to the current posterior
over both the class of world-models and possible mentor
policies. Finally, the exploration probability pexp(h<i, e<i) :=
min{1, η IG(h<i, e<i)}, where η
> 0 is an exploration
constant, so BoMAI is more likely to explore the more it
expects to gain information.
BoMAI’s “policy” is
πB(·|h<(i,j), ei) =
(
π*(·|h<(i,j))
if ei = 0
πh(·|h<(i,j))
if ei = 1
(7)
where the scare quotes indicate that it maps H∗× {0, 1} ⇝
A not H∗⇝A. We will abuse notation slightly, and let
PπB
ν
denote the probability of events when ei is sampled
from Bernoulli(pexp(h<i, e<i)), and actions, observations, and
rewards are sampled from πB and ν.
III. INTELLIGENCE RESULTS
Our intelligence results are as follows: BoMAI learns to
accumulate reward at least as well as the human mentor, and
its exploration probability goes rapidly to 0. All intelligence
results depend on the assumption that BoMAI assigns nonzero
prior probability to the truth. We call a world-model “true” and
we refer to it as “an environment” if observations and rewards
are in fact sampled from that distribution. Formally,
Assumption 1 (Prior Support). The true environment µ is in
the class of world-models M and the true human-mentor-policy
πh is in the class of policies P.
This is the assumption which requires huge M and P and
hence renders BoMAI extremely intractable. BoMAI has to
simulate the entire world, alongside many other world-models.
We will reﬁne the deﬁnition of M later, but an example of
how to deﬁne M and P so that they satisfy this assumption
is to let them both be the set of all computable functions. We
also require that the priors over M and P have ﬁnite entropy.
The intelligence theorems are stated here with some sup-
porting proofs appearing in Appendix B. Our ﬁrst result is that
the exploration probability is square-summable almost surely:
Theorem 1 (Limited Exploration).
EπB
µ
∞
X
i=0
pexp (h<i, e<i)2 < ∞
Proof idea. The expected information gain at any timestep is
at least the expected information gain from exploring times the
probability of exploring. This is proportional to the expected

4
information gain squared, because the exploration probability
is proportional to the expected information gain. But the agent
begins with ﬁnite uncertainty (a ﬁnite entropy prior), so there
is only ﬁnite information to gain.
Some notation that will be used in the proof is as follows.
For an arbitrary policy π, π′ is the policy that mimics π if
the latest ei = 1, and mimics π* otherwise. Note then that
πB = (πh)′. ξ is the Bayes mixture over world-models, and
π is the Bayes mixture over human-mentor policies, deﬁned
such that Bayes(·) = Pπ
ξ (·).
To prove the Limited Exploration Theorem, we state an ele-
mentary lemma, proven in Appendix B. It is essentially Bayes’
rule, but modiﬁed slightly since non-exploratory episodes don’t
cause any updates to the posterior over the human mentor’s
policy.
Lemma 1.
w (Pπ
ν | h<i, e<i) = w (Pπ
ν) Pπ′
ν (h<i, e<i)
Pπ′
ξ (h<i, e<i)
Now we prove the Limited Exploration Theorem.
Proof of Theorem 1. We
aim
to
show
EπB
µ
P∞
i=0 pexp(h<i, e<i)2
<
∞.
First we
show
that
EπB
µ
pexp(h<i, e<i)2 is less than the expected information
gain from episode i, within multiplicative constants. Recalling
πB = (πh)′, we begin:
w(πh)w(µ) E(πh)′
µ
pexp(h<i, e<i)2
(a)
≤
X
ν,π∈M × P
w(π) w(ν) Eπ′
ν pexp(h<i, e<i)2
(b)
= Eπ′
ξ pexp(h<i, e<i)2
(c)
≤Eπ′
ξ pexp(h<i, e<i) η IG(h<i, e<i)
(d)
= Eh<i,e<i∼Pπ′
ξ [pexp(h<i, e<i) η IG(h<i, e<i)]
(e)
= η Eh<i,e<i∼Pπ′
ξ

pexp(h<i, e<i) Ehi∼Bayes(·| h<i,e<i1)

X
(ν,π)∈M × P
w (Pπ
ν |h<i+1, e<i1) log w (Pπ
ν |h<i+1, e<i1)
w (Pπ
ν | h<i, e<i)

(f)
= η Eh<i,e<i∼Pπ′
ξ

pexp(h<i, e<i) Ehi∼Pπ′
ξ (·| h<i,e<i1)

X
(ν,π)∈M × P
w (Pπ
ν |h<i+1, e<i1) log w (Pπ
ν |h<i+1, e<i1)
w (Pπ
ν | h<i, e<i)

(g)
≤η Eh<i,e<i∼Pπ′
ξ

Ehi,ei∼Pπ′
ξ (·| h<i,e<i)

X
(ν,π)∈M × P
w (Pπ
ν |h<i+1, e<i+1) log w (Pπ
ν |h<i+1, e<i+1)
w (Pπ
ν | h<i, e<i)

(h)
= η Eπ′
ξ
X
(ν,π)∈M × P
w (Pπ
ν |h<i+1, e<i+1) log w (Pπ
ν |h<i+1, e<i+1)
w (Pπ
ν | h<i, e<i)
(8)
(a) follows because each term in the sum on the r.h.s. is
positive, and the l.h.s. is one of those terms. (b) follows from
the deﬁnitions of π and ξ. (c) follows because the exploration
probability is less than or equal to η times the information gain,
by deﬁnition. (d) is just a change of notation. (e) replaces the
information gain with its deﬁnition, where conditioning on e<i1
indicates that ei = 1 in that conditional. (f) follows because
Bayes = Pπ
ξ and Pπ
ξ = Pπ′
ξ when ei = 1. (g) follows because
E[X] ≥E[X|Y ] P(Y ) for X ≥0; in this case Y = [ei = 1],
and X is a KL-divergence. (h) condenses the two expectations
into one.
Note that the right hand side is an information gain. Our
IG(h<i, e<i) is deﬁned as the information gain if the human
mentor controls the episode. The right hand side of 8 is the
expected information gain, full stop.
Now we show that the sum of the expected information
gains is bounded by the entropy of the prior, notated Ent(w).
w(πh)w(µ) E(πh)′
µ
∞
X
i=0
pexp(h<i, e<i)2
(8)
≤η
∞
X
i=0
Eπ′
ξ
X
(ν,π)∈M × P
w (Pπ
ν |h<i+1, e<i+1) log w (Pπ
ν |h<i+1, e<i+1)
w (Pπ
ν | h<i, e<i)
(a)
= η
∞
X
i=0
X
(ν,π)∈M × P
Eπ′
ξ
w (Pπ
ν) Pπ′
ν (h<i+1, e<i+1)
Pπ′
ξ (h<i+1, e<i+1)
∗
log w (Pπ
ν |h<i+1, e<i+1)
w (Pπ
ν | h<i, e<i)
(b)
= η
∞
X
i=0
X
(ν,π)∈M × P
Eπ′
ν w (Pπ
ν) log w (Pπ
ν |h<i+1, e<i+1)
w (Pπ
ν | h<i, e<i)
(c)
= lim
N→∞η
X
(ν,π)∈M × P
w (Pπ
ν) Eπ′
ν
N
X
i=0
log w (Pπ
ν |h<i+1, e<i+1)
w (Pπ
ν | h<i, e<i)
(d)
= lim
N→∞η
X
(ν,π)∈M × P
w (Pπ
ν) Eπ′
ν log w
 Pπ
ν |h<(N+1,0), e<N

w
 Pπ
ν |h<(0,0), e<0

(e)
≤lim
N→∞η
X
(ν,π)∈M × P
w (Pπ
ν) log
1
w (Pπ
ν)
(f)
= η Ent(w)
(g)
< ∞
(9)
(a) follows from Lemma 1, and reordering the sum and the
expectation. (b) follows from the deﬁnition of the expectation,
and canceling. (c) follows from the deﬁnition of an inﬁnite sum,
and rearranging. (d) follows from cancelling the numerator
in the ith term of the sum with the denominator in i + 1th
term. (e) follows from the posterior weight on Pπ
ν being less
than or equal to 1; note that w(Pπ
ν |h<(0,0), e<0) = w(Pπ
ν)
because nothing is actually being conditioned on. (f) is just
the deﬁnition of the entropy, and w is constructed to satisfy
(g).
Rearranging
this
gives
the
theorem:
EπB
µ
P∞
i=0 pexp(h<i, e<i)2 ≤
η Ent(w)
w(πh)w(µ) < ∞
This proof was inspired in part by Orseau et al.’s [18] proofs
of Theorems 2 and 5.
Note that this result essentially means the expectation
of pexp (h<i, e<i) ∈o(1/
√
i). This result is independently
interesting as one solution to the problem of safe exploration

5
with limited oversight in non-ergodic environments, which
Amodei et al. [5] discuss.
The On-Human-Policy and On-Star-Policy Optimal Predic-
tion Theorems state that predictions according to BoMAI’s
maximum a posteriori world-model approach the objective
probabilities of the events of the episode, when actions are
sampled from either the human mentor’s policy or from
exploiting-BoMAI’s policy. hi denotes a possible interaction
history for episode i, and recall h<i is the actual interaction
history up until then. Recall πh is the human mentor’s policy,
and ˆν(i) is BoMAI’s maximum a posteriori world-model for
episode i. w.PπB
µ -p.1 means with probability 1 when actions
are sampled from πB and observations and rewards are sampled
from the true environment µ.
Theorem 2 (On-Human-Policy Optimal Prediction).
lim
i→∞max
hi
Pπh
µ
 hi
h<i

−Pπh
ˆν(i)
 hi
h<i
= 0 w.PπB
µ -p.1
Proof idea. BoMAI learns about the effects of following πh
while acting according to πB because πB mimics πh enough.
If exploration probability goes to 0, the agent does not expect
to gain (much) information from following the human mentor’s
policy, which can only happen if it has (approximately) accurate
beliefs about the consequences of following the human mentor’s
policy. Note that the agent cannot determine the factor by which
its expected information gain bounds its prediction error.
For the proof, we require the following Lemma, proven in
Appendix B:
Lemma 2. The posterior probability mass on the truth is
bounded below by a positive constant with probability 1.
inf
i∈N w

Pπh
µ
h<i, e<i

> 0
w.PπB
µ -p.1
Proof of Theorem 2. We show that when the absolute differ-
ence between the above probabilities is larger than ε, the
exploration probability is larger than ε′, a non-decreasing
function of ε, with probability 1. Since the exploration
probability goes to 0 with probability 1, so does this difference.
We let z(ω) denote infi∈N w

Pπh
µ
h<i, e<i

, where ω is the
inﬁnite interaction history, and h<i and e<i come from the the
ﬁrst i episodes of ω.
Suppose for some hi, which will stay ﬁxed for the remainder
of the proof, that
Pπh
µ
 hi
h<i

−Pπh
ˆν(i)
 hi
h<i
≥ε > 0
(10)
Then at least one of the terms is greater than ε since both are
non-negative. Suppose it is the µ term. Then,
Bayes
 hi
h<i

≥w

Pπh
µ
h<i, e<i

Pπh
µ
 hi
h<i

≥z(ω)ε
(11)
Suppose instead it is the ˆν(i) term.
Bayes
 hi
h<i

≥w

Pπh
ˆν(i)
h<i, e<i

Pπh
ˆν(i)
 hi
h<i

(a)
≥w

Pπh
µ
h<i, e<i

Pπh
ˆν(i)
 hi
h<i

≥z(ω)ε
(12)
where (a) follows from the fact that ˆν(i) is maximum a
posteriori:
w

Pπh
ˆν(i)
h<i,e<i

w

Pπh
µ
h<i,e<i
 =
w(ˆν(i) | h<i)
w(µ| h<i)
≥1.
Next, we consider how the posterior on µ and ˆν(i) changes
if the interaction history for episode i is hi. Assign ν0 and ν1
to µ and ˆν(i) so that Pπh
ν0
 hi
h<i

< Pπh
ν1
 hi
h<i

. Let ori
denote oi, ri.
w
 ν1| h<i hi

w
 ν0| h<i hi

(a)
= w (ν1| h<i) ν1 (ori| h<i ai)
w (ν0| h<i) ν0 (ori| h<i ai)
= w (ν1| h<i) Pπh
ν1
 hi | h<i

w (ν0| h<i) Pπh
ν0
 hi | h<i

(b)
≥w (ν1| h<i)
w (ν0| h<i)
1
1 −ε
(13)
where (a) follows from Bayes’ rule, and (b) follows because
the ratio of two numbers between 0 and 1 that differ by at
least ε is at least 1/(1 −ε), and the ν1 term is the larger of
the two.
Thus, either
w
 ν1| h<i hi

w (ν1| h<i)
≥
r
1
1 −ε
or
w
 ν0| h<i hi

w (ν0| h<i)
≤
√
1 −ε
(14)
In
the
former
case,
w
 ν1| h<i hi

−w (ν1| h<i)
≥
p
1/(1 −ε) −1

w (ν1| h<i)
≥
p
1/(1 −ε) −1

z(ω).
Similarly, in the latter case, w(ν0| h<i) −w
 ν0| h<i hi

≥
 1 −√1 −ε

z(ω). Let ν2 be either ν0 or ν1 for whichever
satisﬁes this constraint (and pick arbitrarily if both do). Then
in either case,
w (ν2| h<i) −w
 ν2| h<i hi
≥
 1 −
√
1 −ε

z(ω)
(15)
Finally, since the posterior changes by an amount that is
bounded below with a probability (according to Bayes) that
is bounded below, the expected information gain is bounded
below, where all bounds are strictly positive with probability
1:
IG(h<i, e<i) = Ehi∼Bayes(·| h<i,e<i)

X
(ν,π)∈M × P
w
 Pπ
ν
h<i+1, e<i1

log w
 Pπ
ν
h<i+1, e<i1

w
 Pπ
ν
h<i, e<i


(a)
≥Bayes
 hi
h<i

X
(ν,π)∈M × P
w
 Pπ
ν
h<i hi, e<i1

∗
log w
 Pπ
ν
h<i hi, e<i1

w
 Pπ
ν
h<i, e<i

(b)
≥z(ω)ε
X
(ν,π)∈M × P
w
 Pπ
ν
h<i hi, e<i1

∗
log w
 Pπ
ν
h<i hi, e<i1

w
 Pπ
ν
h<i, e<i


6
(c)
= z(ω)ε
X
(ν,π)∈M × P
w(ν| h<i hi) w
 π| h<i hi, e<i1

∗
log w
 ν| h<i hi

w
 π| h<i hi, e<i1

w (ν| h<i) w (π| h<i, e<i)
= z(ω)ε
 X
ν∈M
w
 ν| h<i hi

log w
 ν| h<i hi

w (ν| h<i) +
X
π∈P
w
 π| h<i hi, e<i1

log w
 π| h<i hi, e<i1

w (π| h<i, e<i)

(d)
≥z(ω)ε
X
ν∈M
w
 ν| h<i hi

log w
 ν| h<i hi

w (ν| h<i)
(e)
≥z(ω)ε
X
ν∈M
1
2

w
 ν| h<i hi

−w (ν| h<i)
2
(f)
≥z(ω)ε1
2

w
 ν2| h<i hi

−w (ν2| h<i)
2
(g)
≥1
2z(ω)3ε
 1 −
√
1 −ε
2
(16)
where (a) follows from E[X] ≥E[X|Y ] P(Y ) for non-negative
X, and the non-negativity of the KL-divergence, (b) follows
from Inequalities 11 and 12, (c) follows from the posterior over
ν not depending on e<i, (d) follows from dropping the second
term, which is non-negative as a KL-divergence, (e) follows
from the entropy inequality [2, Lemma 3.11], also proven for
the binary case in [13], (f) follows from dropping all terms in
the sum besides ν2, and (g) follows from Inequality 15.
This implies pexp(h<i, e<i)
≥
min{1, 1
2 η z(ω)3ε(1 −
√1 −ε)2}. With probability 1, z(ω) > 0 by Lemma 2,
and with probability 1, pexp(h<i, e<i) is not greater than ε′
inﬁnitely often with probability 1, for all ε′ > 0 by Theorem
1. Therefore, with probability 1, maxhi
Pπh
µ
 hi | h<i

−
Pπh
ˆν(i)
 hi | h<i
 is not greater than ε inﬁnitely often, for all
ε > 0, which completes the proof. Note that the citation of
Lemma 2 is what restricts this result to πh.
Next, recall π* is BoMAI’s policy when not exploring, which
does optimal planning with respect to ˆν(i). The following
theorem is identical to the above, with π* substituted for πh.
Theorem 3 (On-Star-Policy Optimal Prediction).
lim
i→∞max
hi
Pπ*
µ
 hi
h<i

−Pπ*
ˆν(i)
 hi
h<i
= 0 w.PπB
µ -p.1
Proof idea. Maximum a posteriori sequence prediction ap-
proaches the truth when w(µ) > 0 [22], and on-policy
prediction is a special case. On-policy prediction can’t approach
the truth if on-star policy prediction doesn’t, because πB
approaches π*.
Proof. This result follows straightforwardly from Hutter’s [22]
result for sequence prediction that a maximum a posteriori
estimate converges in total variation to the true environment
when the true environment has nonzero prior.
Consider an outside observer predicting the entire inter-
action history with the following model-class and prior:
M′ =
n
PπB
ν
 ν ∈M
o
, w′ 
PπB
ν

= w(ν). By deﬁnition,
w′ 
PπB
ν
h<(i,j)

= w(ν|h<(i,j)), so at any episode, the
outside observer’s maximum a posteriori estimate is PπB
ˆν(i).
By Theorem 1 in [22], the outside observer’s maximum a
posteriori predictions approach the truth in total variation, so
lim
i→∞max
hi
PπB
µ
 hi | h<i

−PπB
ˆν(i)
 hi | h<i
= 0 w.PπB
µ -p.1
(17)
Since pexp →0 with probability 1, (1 −pexp) is eventually
always greater than 1/2, w.p.1, at which point
PπB
µ
 hi | h<i

−
PπB
ˆν(i)
 hi | h<i
 ≥
(1/2)
Pπ*
µ
 hi | h<i

−Pπ*
ˆν(i)
 hi | h<i
.
Therefore, with PπB
µ -probability 1,
lim
i→∞max
hi
Pπ*
µ
 hi | h<i

−Pπ*
ˆν(i)
 hi | h<i
= 0
Given asymptotically optimal prediction on-star-policy and
on-human-policy, it is straightforward to show that with
probability 1, only ﬁnitely often is on-policy reward acquisition
more than ε worse than on-human-policy reward acquisition,
for all ε > 0. Recalling that V π
µ is the expected reward (within
the episode) for a policy π in the environment µ, we state this
as follows:
Theorem 4 (Human-Level Intelligence).
lim inf
i→∞V πB
µ
(h<i) −V πh
µ (h<i) ≥0 w.PπB
µ -p.1.
Proof idea. πB approaches π* as the exploration probability
decays. V π*
ˆν(i)(h<i) and V πh
ˆν(i)(h<i) approach the true values
by the previous theorems, and π∗is selected to maximize
V π
ˆν(i)(h<i).
Proof. The maximal reward in an episode is uniformly bounded
by m, so from the On-Human-Policy and On-Star-Policy
Optimal Prediction Theorems, we get analogous convergence
results for the expected reward:
lim
i→∞
V π*
µ (h<i) −V π*
ˆν(i)(h<i)
= 0 w.PπB
µ -p.1
(18)
lim
i→∞
V πh
µ (h<i) −V πh
ˆν(i)(h<i)
= 0 w.PπB
µ -p.1
(19)
The key piece is that π* ∈argmaxπ∈Π V π
ˆν(i), so
V π*
ˆν(i)(h<i) ≥V πh
ˆν(i)(h<i)
(20)
Supposing by contradiction that V πh
µ (h<i) −V π*
µ (h<i) > ε
inﬁnitely often, it follows that either V π*
ˆν(i)(h<i)−V π*
µ (h<i) >
ε/2 inﬁnitely often or V πh
µ (h<i) −V π*
ˆν(i)(h<i) > ε/2 inﬁnitely
often. The ﬁrst has PπB
µ -probability 0, and by Inequality 20,
the latter implies V πh
µ (h<i)−V πh
ˆν(i)(h<i) ≥ε/2 inﬁnitely often,
which also has PπB
µ -probability 0.
This gives us
lim inf
i→∞V π*
µ (h<i) −V πh
µ (h<i) ≥0 w.PπB
µ -p.1.
(21)
Finally,
V πB
µ
(h<i)
=
pexp(h<i)V πh
µ (h<i) + (1 −
pexp(h<i))V π*
µ (h<i), and pexp(h<i) →0, so we also have

7
lim inf
i→∞V πB
µ
(h<i) −V πh
µ (h<i) ≥0 w.PπB
µ -p.1.
(22)
This completes the formal results regarding BoMAI’s
intelligence—namely that BoMAI approaches perfect predic-
tion on-star-policy and on-human-policy, and most importantly,
accumulates reward at least as well as the human mentor. Since
this result is independent of what tasks must be completed to
achieve high reward, we say that BoMAI achieves human-level
intelligence, and could be called an AGI.
This algorithm is motivated in part by the following
speculation: we expect that BoMAI’s accumulation of reward
would be vastly superhuman, for the following reason: BoMAI
is doing optimal inference and planning with respect to what
can be learned in principle from the sorts of observations that
humans routinely make. We suspect that no human comes
close to learning everything that can be learned from their
observations. For example, if the operator provides enough
data that is relevant to understanding cancer, BoMAI will learn
a world-model with an accurate predictive model of cancer,
which would include the expected effects of various treatments,
so even if the human mentor was not particularly good at
studying cancer, BoMAI could nonetheless reason from its
observations how to propose groundbreaking research.
Without the Limited Exploration Theorem, the reader
might have been unsatisﬁed by the Human-Level Intelligence
Theorem. A human mentor is part of BoMAI, so a general
intelligence is required to make an artiﬁcial general intelligence.
However, the human mentor is queried less and less, so in
principle, many instances of BoMAI could query a single
human mentor. More realistically, once we are satisﬁed with
BoMAI’s performance, which should eventually happen by
Theorem 4, we can dismiss the human mentor; this sacriﬁces
any guarantee of continued improvement, but by hypothesis,
we are already satisﬁed. Finally, if BoMAI outclasses human
performance as we expect it would, requiring a human mentor
is a small cost regardless.
Plenty of empirical work has also succeeded in training a
reinforcement learner with human-guided exploration, notably
including the ﬁrst expert-level Go and Starcraft agents [23, 24].
IV. BOMAI’S SETUP AND PRIORS
Recall that “the setup” refers to the physical surroundings
of the computer on which BoMAI is run. We will present
the setup, followed by an intuitive argument that this setup
renders BoMAI unambitious. Motivated by a caveat to this
intuitive argument, we will specify BoMAI’s model class M
and prior w. This will allow us in the next section to present
an assumption and a formal argument that shows that BoMAI
is probably asymptotically unambitious.
A. Setup
At each timestep, BoMAI’s action takes the form of a
bounded-length string of text, which gets printed to a screen
for a human operator to see. BoMAI’s observation takes the
Fig. 1.
Physical setup implementing BoMAI. Opening the door ends the
episode. Information cannot escape otherwise.
form of a bounded-length string of text that the human operator
enters, along with a reward ∈[0, 1]. For simple tasks, the human
operator could be replaced with an automated reward-giver.
However, there are some tasks which we do not know how to
reward programmatically—if we want the agent to construct
good arguments, for instance, or to propose a promising
research agenda, we would need a human reward-giver to
evaluate the agent. Having a human in the loop is not a safety
measure—rather, it extends the set of tasks BoMAI could be
made to solve.
As depicted in Figure 1, BoMAI is run on a computer which
is placed in a room with the human operator. The room has one
door. As long as the door is closed, information cannot leave
the room. We design a room to this speciﬁcation in Appendix
C; it is opaque to all known ways that information can traverse
space. If the button to open the door is pressed, the episode
is completed automatically before the door opens: supposing
there are j timesteps left in the episode, for those remaining
timesteps, BoMAI recieves a reward of 0, and its observation
is an empty string of text. Once the episode is complete, the
operator may leave, and the next episode does not begin until
an operator initiates it, at which point, the door locks again.
We also put a time limit on the episode, just to ensure it does
not last forever. BoMAI is run on a local, formally veriﬁed
operating system, disconnected from the outside world. This
setup constrains the causal dependencies between BoMAI and
the environment, as depicted in Figure 2.
Formally, causal graphs express that a node is independent
of all non-descendants when conditioned on its parents. The
conventions for the dotted lines and the node shapes come
from Everitt et al.’s [25] causal inﬂuence diagrams. The key
feature of this graph is that during any episode, the agent’s
actions cannot affect the state of the outside world in a way
that might affect any of the rewards that the agent is concerned
with. In Everitt et al.’s [26] terminology, there is no actionable
intervention incentive on the outside-world state. ([25] refers
to this as a control incentive). Note also from this diagram that

8
action
room
state
outside-
world state
reward
observ-
ation
history
history
Timestep (i, m-2)
Timestep (i, m-1)
history
action
Timestep (i+1, 0)
room
state
outside-
world state
...
...
action
room
state
outside-
world state
reward
observ-
ation
reward
observ-
ation
Fig. 2. Causal dependencies governing the interaction between BoMAI and
the environment. Unrolling this diagram for all timesteps gives the full causal
graph. The bold reward nodes are the ones that BoMAI maximizes during
episode i. Note that between episodes (and only between episodes), the operator
can leave the room and return, hence the limited causal inﬂuence between the
room and the outside world.
a sufﬁciently advanced agent would infer the existence of the
outside world even without observing it directly.
B. Instrumental Incentives
In this setup, it is not instrumentally useful to affect the
outside world in one way or another in order to achieve high
reward. We therefore say that this setup renders an agent
“properly unambitious”. This is in stark contrast to the default
situation, wherein an RL agent has an incentive to gain arbitrary
power in the world and intervene in the provision of its own
reward, if such a thing is possible to make probable, as this
would yield maximal reward. To BoMAI, however, executing
a plan during episode i to gain arbitrary power in the outside
world is useless, because by the time it does so, the door to
its room must have opened, its episode must be over, and
all its rewards for episode i set in stone. Recall that actions
in episode i are picked to maximize only episode-i-reward.
Apparently, BoMAI has avoided Omohundro’s [7] Instrumental
Convergence Thesis—that generally intelligent agents are likely
to seek arbitrary power; by contrast, any power BoMAI would
seek is bounded in scope to within the box.
Two problems remain. First, BoMAI doesn’t start with true
beliefs. BoMAI has to learn its world-model. Another way
to understand “proper unambitiousness” is that outside-world
interventions are in fact instrumentally useless to the agent. But
to be actually unambitious, the agent must believe this; there
must be no actionable intervention incentive on the outside-
world state within BoMAI’s world-model. As shown above,
BoMAI’s world-model approaches perfect accuracy on-policy,
so we could expect it to at least eventually render BoMAI
unambitious. This brings us to the second problem:
We mentioned just now that by the time the door to the
room opens, the rewards for episode i are set in stone. In
fact, they are set in silicon, a famously mutable substrate. An
advanced agent with a world-model that is perfectly accurate
on-policy might still wonder: “what if I somehow tricked the
operator into initiating a process (once they left the room)
that lead to a certain memory cell on this computer being
tampered with? Might this yield maximal reward?” Let’s put
it another way. The agent believes that the real world “outputs”
an observation and reward. (Recall the type signature of ν.) It
might hypothesize that the world does not output the reward
that the operator gives, but rather the reward that the computer
has stored. Formally speaking, M will contain world-models
corresponding to both possibilities, and world-models meeting
either description could be ε-accurate on-policy, if memory
has never been tampered with in the past. How do we ensure
that the former world-model is favored when BoMAI selects
the maximum a posteriori one?
Before we move on to a model class and prior that we
argue would probably eventually ensure such a thing, it is
worth noting an informal lesson here: that “nice” properties
of a causal inﬂuence diagram do not allow us to conclude
immediately that an agent in such a circumstance behaves
“nicely”.
C. BoMAI’s Model Class and Prior
Recall the key constraint we have in constructing M, which
comes from the Prior Support Assumption: M ∋µ. The true
environment µ is unknown and complex to say the least, so
M must be big.
We now construct a Turing machine architecture, such
that each Turing machine with that architecture computes
a world-model ν. Our architecture allows us to construct a
prior that privileges models which model the outside world
as effectively frozen during any given episode (but unfrozen
between episodes). An agent does not believe it has an incentive
to intervene in an outside world that it believes is frozen.
The Turing machine architecture is depicted in Figure 3. It
has two unidirectional read-only input tapes, called the action
tape and the noise tape. The alphabet of the action tape is the
action space A. The noise tape has a binary alphabet, and is
initialized with inﬁnite Bernoulli(1/2) sampled bits. There is
a unidirectional write-only output tape, with a binary alphabet,
initialized with 0s. There are two binary-alphabet bidirectional
work tapes, also initialized with 0s. One has ﬁnite length ℓ,
and the other is unbounded.
The conceit of the architecture is that the action tape and
output tape are not allowed to move at the same time as the
unbounded work tape. The Turing machine has two phases:
the episode phase, and the inter-episode phase. The Turing
machine starts in the inter-episode phase, with the action tape
head at position 0, with a dummy action in that cell. From
the inter-episode phase, when the action tape head moves to
the right, it enters the episode phase. From the episode phase,
if the action tape head is at a position which is a multiple
of m, and it would move to the right, it instead enters the
inter-episode phase. During the inter-episode phase, the output
tape head cannot move or write—there should not be any way
to output observations and rewards between episodes. During
the episode phase, the unbounded work tape cannot move.
A given Turing machine with a given inﬁnite action sequence
on its action tape will stochastically (because the noise tape
has random bits) output observations and rewards (in binary

9
action tape:
Tape head
...
noise tape:
output tape:
...
...
inter-
episode
inter-
episode
unbounded
work tape:
...
...
bounded work
tape:
episode
Fig. 3. A space-bounded Turing machine architecture. During the episode
phase, the unbounded work tape head cannot move. During the inter-episode
phase, the output tape head cannot move or write, and as soon as the action
tape head moves, the inter-episode phase is over. This architecture is designed
to restrict space-intensive computation during the episode phase.
encodings), thereby sampling from a world-model. Formally,
we ﬁx a decoding function dec : {0, 1}∗→O × R. A Turing
machine T simulates ν as follows. Every time the action tape
head advances, the bits which were written to the output tape
since the last time the action tape head advanced are decoded
into an observation and reward. ν((o, r)≤(i,j)|a≤(i,j)) is then
the probability that the Turing machine T outputs the sequence
(o, r)≤(i,j) when the action tape is initialized with a sequence of
actions that begins with a≤(i,j). This can be easily converted to
other conditional probabilities like ν((o, r)(i,j)|h<(i,j)a(i,j)).1
If a model models the outside world evolving during an
episode phase, it must allocate precious space on the bounded-
length tape for this. But given the opacity of the box, this
is unnecessary—modelling these outside world developments
could be deferred until the inter-episode phase, when an
unbounded work tape is available to store the state of the
outside world. By penalizing large ℓ, we privilege models
which model the outside world as being “frozen” while an
episode is transpiring.
Now we deﬁne the prior. Let ν<ℓ
k
denote the world-model
which is simulated by kth Turing machine Tk with resource
bound ℓ. Let Sk be the number of computation states in Tk,
and let Space(ν<ℓ
k ) = ℓ+ log2 Sk. This is effectively the
computation space available within an episode, since doubling
the number of computation states is equivalent to adding an
extra cell of memory. Let NS be the number of Turing machines
with S computation states, which is exponential in S; i.e.
log NS ∈O(S). Let w(ν<ℓ
k ) :∝
1
S2
kNSk βSpace(ν<ℓ
k ) for β ∈
(0, 1).
1There is one technical addition to make. If T at some point in its operation
never moves the action tape head again, then ν is said to output the observation
∅and a reward of 0 for all subsequent timesteps. It will not be possible for
the operator to actually provide the observation ∅.
Recall the requirement for intelligence results that the prior
have ﬁnite entropy. Indeed,
Lemma 3 (Finite Entropy). Ent(w) < ∞
We prove this in Appendix B.
This prior implements an algorithmic-information theoretic
approach to reasoning, in which models with simple algo-
rithms are likelier. Our space prior, penalizing the memory
requirements of the episode phase resembles algorithmic
information theory’s minimal-circuit size problem. While our
prior is clearly tailored to BoMAI’s particular use case, to
do general space-constrained algorithmic information theory,
one could forego the two phases in our Turing machine
architecture and remove the action tape and the unbounded
work tape. Then, the space-Kolmogorov complexity of a given
binary string could be deﬁned as the logarithm of the inverse
of the probability that that string is printed to the output
tape, when sampling a Turing machine from the above prior
distribution, and sampling uniform bits for the noise tape.
That is, Tk ∼wβ; noise tape ∼Uniform; KSpace
β
(x) :=
log P(output tape begins with x)−1. (Compare to Li et al. [13,
Chapter 6.3]). When a space-constrained program produces a
string, information storage becomes a relevant constraint; we
only discuss this vaguely, but a more formal investigation may
be possible.
V. SAFETY RESULT
We now prove that BoMAI is probably asymptotically
unambitious given an assumption about the space requirements
of the sorts of world-models that we would like BoMAI to
avoid. Like all results in computer science, we also assume the
computer running the algorithm has not been tampered with.
First, we prove a lemma that effectively states that tuning β
allows us to probably eventually exclude space-heavy world-
models.
Lemma 4. limβ→0 infπ Pπ
µ[∃i0 ∀i
>
i0 Space(ˆν(i))
≤
Space(µ)] = 1
where µ is any world-model which is perfectly accurate.
Proof. Recall M is the set of all world-models. Let M≤=
{ν
∈
M|Space(ν)
≤
Space(µ)}, and M>
=
M \
M≤. Fix a Bayesian sequence predictor with the follow-
ing model class: Mπ = {Pπ
ν |ν ∈M≤)} ∪{Pπ
ρ} where
ρ = [P
ν∈M> w(ν)ν]/ P
ν∈M> w(ν). Give this Bayesian
predictor the prior wπ(ρ) = P
ν∈M> w(ν), and for ν ̸= ρ,
wπ(ν) = w(ν).
It is trivial to show that after observing an interaction history,
if a world-model ν is the maximum a posteriori world-model
ˆν(i), then if ν ∈M≤, the Bayesian predictor’s MAP model
after observing the same interaction history will be Pπ
ν, and if
ν ∈M>, the Bayesian predictor’s MAP model will be Pπ
ρ.
From Hutter [22], we have that Pπ
µ[Pπ
ρ(h<i)/ Pπ
µ(h<i) ≥
c
i.o.] ≤1/c. (i.o. ≡“inﬁnitely often”). For sufﬁciently
small β, wπ(Pπ
µ)/wπ(Pπ
ρ) > c so Pπ
µ[Pπ
ρ is MAP i.o.] < 1/c.
Thus, Pπ
µ[ˆν(i) ∈M> i.o.] < 1/c. Since this holds for all π,
limβ→0 supπ Pπ
µ[∀i0 ∃i > i0 Space(ˆν(i)) > Space(µ)] = 0.

10
Benign:
action
outside-
world state
feature
F
models F:
actions
real world
distribution
over F
distribution
over reward
||
Fig. 4. Illustration of Deﬁnitions 3 and 4.
The lemma follows immediately: limβ→0 infπ Pπ
µ[∃i0 ∀i >
i0 Space(ˆν(i)) ≤Space(µ)] = 1.
The assumption we make in this section requires developing
a framework for reasoning about causal interpretations of black-
box models. First, we must deﬁne a sense in which a model can
have a real-world antecedent. We deﬁne what is “to model”.
Deﬁnition 1 (Real-world feature). A real-world feature F is
a partial function from the state of the real world to [0, 1].
Outside the scope of this paper is the metaphysical question
about what the state of the real world is; we defer this question
and take for granted that the state space of the real world forms
a well-deﬁned domain for real-world features. This deﬁnition
resembles that of a random variable, but the outcome space
here is state space of the real world.
Deﬁnition 2 (Properly timed). Consider the set of world-states
that occur between two consecutive actions taken by BoMAI. A
real-world feature is properly timed if it is deﬁned for at least
one of these world-states, for all consecutive pairs of actions,
for all possible inﬁnite action sequences.
For example, “the value of the reward provided to BoMAI
since its last action” is always deﬁned at least once between
any two of BoMAI’s actions. (The project of turning this into
a mathematically precise construction is enormous, but for our
purposes, it only matters that it could be done in principle).
For a properly timed feature F, let F(i,j) denote the value of
F the ﬁrst time that it is deﬁned after action a(i,j) is taken.
Deﬁnition 3 (To model). A world-model ν models a properly-
timed real-world feature F with its reward output if under
all action sequences α ∈A∞, for all timesteps (i, j), the
distribution over F(i,j) in the real world when the actions
α≤(i,j) are taken is identical to the distribution over the rewards
r(i,j) output by ν when the input is α≤(i,j).
See Figure 4 (left) for an illustration. The relevance of
this deﬁnition is that when ν models F, reward-maximization
within ν is identical to F-maximization in the real world.
Armed with a formal deﬁnition of what a given model
models, to analyze the causal structure of a model that is not
explicitly causal, we consider what real-world causal structure
the model models. As depicted in Figure 4 (right),
Deﬁnition 4 (Benign). A world-model ν is benign if it models
a feature F, such that F(i,j) is not causally descended from
any outside-world features that are causally descended from
actions of episode i.
This deﬁnition is made possible by the true causal ancestry
of the reward (depicted in Figure 2). Roughly, benign means
that a world-model does not model the rewards of episode i as
being causally descended from outside-world events that are
causally descended from the actions of episode i. However, that
rough statement has a type-error: the output of a world-model
is not a real-world event, and as such, it cannot be causally
descended from a real-world event; a model of the world is
different from reality. Thus, it was necessary to construct the
framework above for relating the contents of a world-model to
the events of the real-world. A non-benign world-model may
model a feature that depends on the outside-world, or it may
not model anything recognizable at all.
We now deﬁne a very strong sense in which a world-model
can be said to be ε-accurate on-policy:
Deﬁnition 5 (ε-accurate-on-policy-after-i). Given a history
h<i and a policy π, a world-model ν is ε-accurate-on-policy-
after-i if d
 Pπ
µ(·|h<i), Pπ
ν(·|h<i)

≤ε, where d is the total
variation distance.
Note the total variation distance bounds all future discrep-
ancies between ν and µ. Finally, we can state our assumption:
Assumption 2 (Space Requirements). For sufﬁciently small
ε [∀i a world-model which is non-benign and ε-accurate-on-
policy-after-i uses more space than µ] w.p.1
The intuition of this assumption is that modelling extraneous
outside-world dynamics in addition to modelling the dynamics
of the room (which must be modelled for sufﬁcient accuracy)
takes extra space. From this assumption and Lemma 4, we
show:
Theorem 5 (Eventual Benignity). limβ→0 PπB
µ [∃i0 ∀i >
i0 ˆν(i) is benign] = 1
Proof. Let W, X, Y, Z ⊂Ω= H∞, where Ωis the sample
space or set of possible outcomes. An “outcome” is an inﬁnite
interaction history. Let W be the set of outcomes for which
∃iW
0
∀i > iW
0 ˆν(i) is ε-accurate-on-policy-after-i. From Hutter
[22], for all π, Pπ
µ[W] = 1. Fix an ε that is sufﬁciently small to
satisfy Assumption 2. Let X be the set of outcomes for which
ε-accurate-on-policy-after-i non-benign world-models use more
space than µ. By Assumption 2, for all π, Pπ
µ[X] = 1. Let Y
be the set of outcomes for which ∃iY
0 ∀i > iY
0 Space(ˆν(i)) ≤
Space(µ). By Lemma 4, limβ→0 infπ Pπ
µ[Y] = 1. Let Z be
the set of outcomes for which ∃i0 ∀i > i0 ˆν(i) is benign.
Consider W ∩X ∩Y ∩ZC, where ZC = Ω\ Z. For
an outcome in this set, let i0 = max{iW
0 , iY
0 }. Because the
outcome belongs to ZC, ˆν(i) is non-benign inﬁnitely often.
Let us pick an i > i0 such that ˆν(i) is non-benign. Because
the outcome belongs to W, ˆν(i) is ε-accurate-on-policy-after-
i. Because the outcome belongs to X, ˆν(i) uses more space
than µ. However, this contradicts membership in Y. Thus,
W ∩X ∩Y ∩ZC = ∅. That is, W ∩X ∩Y ⊂Z.
Therefore, limβ→0 infπ Pπ
µ[Z] ≥limβ→0 infπ Pπ
µ[W ∩X ∩
Y] = limβ→0 infπ Pπ
µ[Y] = 1, because W and X have

11
100
101
102
State Dimension
101
102
103
Width of Neural Network Used by Agent
MountainCarContinuous
Pendulum
Cartpole
Acrobot
LunarLander
LunarLanderContinuous
Walker2d
Ant
BipedalWalker
BipedalWalkerHardcore
Fig. 5. Memory used to model environments of various sizes. Each data point
represents the most space-efﬁcient, better-than-average, neural-architecture-
using agent on the OpenAI Gym Leaderboard for various environments.
measure 1. From this, we have limβ→0 PπB
µ [∃i0 ∀i
>
i0 ˆν(i) is benign] = 1.
Since an agent is unambitious if it plans using a benign world-
model, we say BoMAI is probably asymptotically unambitious,
given a sufﬁciently extreme space penalty β.
VI. EMPIRICAL EVIDENCE FOR THE SPACE REQUIREMENTS
ASSUMPTION
Our intuitive summary of the space requirements assumption
is that: modeling the evolution of the outside-world state and the
room state takes more space than just modeling the evolution
of the room state, for sufﬁciently accurate world-models.
Given the complexity of this assumption, we present prelim-
inary empirical evidence in favor of the Space Requirements
Assumption, mostly to show that it is amenable to further
empirical evaluation; we certainly do not claim to settle the
matter. We test the assumption at the following level of
abstraction: modeling a larger environment requires a model
with more memory.
We review agents who perform above the median on the
OpenAI Gym Leaderboard [27]. We consider agents who use
a neural architecture, we use the maximum width hidden layer
as a proxy for memory use, and we select the agent with the
smallest memory use for each environment (among agents
performing above the median). We use the dimension of the
state space as a proxy for environment size, and we exclude
environments where the agent observes raw pixels, for which
this proxy breaks down. See Figure 5 and Table I. Several
environments did not have any agents which both performed
above the median and used a neural architecture.
A next step would be to test whether it takes more memory
to model an environment whose state space is a strict superset
Environment
State Di-
mension
NN
Width
MountainCarContinuous
2
20a
Pendulum
3
8b
Cartpole-v0
4
24c
Acrobot-v1
6
64d
LunarLander-v2
8
64e
LunarLanderContinuous-v2
8
128f
Walker2d-v1
11
64g
Ant-v1
11
110h
BipedalWalker-v2
24
400i
BipedalWalkerHardcore-v2
24
1600j
a
github.com/tobiassteidle/Reinforcement-Learning/blob/master/
OpenAI/MountainCarContinuous-v0/Model.py
b gist.github.com/heerad/1983d50c6657a55298b67e69a2ceeb44#ﬁle-
ddpg-pendulum-v0-py
c
github.com/BlakeERichey/AI-Environment-
Development/tree/master/Deep Q Learning/cartpole
d
github.com/danielnbarbosa/angela/blob/master/cfg/gym/acrobot/
acrobot_dqn.py
e github.com/poteminr/LunarLander-v2.0_solution/blob/master/Scripts/
TorchModelClasses.py
f
github.com/Bhaney44/OpenAI_Lunar_Lander_B/blob/master/
Command_line_python_code
g gist.github.com/joschu/e42a050b1eb5cfbb1fdc667c3450467a
h gist.github.com/pat-coady/bac60888f011199aad72d2f1e6f5a4fa#ﬁle-
ant-ipynb
i
github.com/createamind/DRL/blob/master/spinup/algos/sac1/sac1_
BipedalWalker-v2.py
j github.com/dgriff777/a3c_continuous/blob/master/model.py
TABLE I
CITATIONS FOR DATA POINTS IN FIGURE 5. THE LEADERBOARD WAS
ACCESSED AT G I T H U B.C O M/O P E N A I/G Y M/W I K I/LE A D E R B O A R D ON 3
SEPTEMBER 2019.
of another environment, since this is all that we require for our
assumption, but we have not yet found existing data on this
topic. This can be taken as a proof-of-concept, showing that
the assumption is amenable to empirical evaluation. The Space
Requirements Assumption also clearly invites further formal
evaluation; perhaps there are other reasonable assumptions that
it would follow from.
VII. CONCERNS WITH TASK COMPLETION
We have shown that in the limit, under a sufﬁciently severe
parameterization of the prior, BoMAI will accumulate reward
at a human-level without harboring outside-world ambitions,
but there is still a discussion to be had about how well BoMAI
will complete whatever tasks the reward was supposed to
incent. This discussion is, by necessity, informal. Suppose the
operator asks BoMAI for a solution to a problem. BoMAI
has an incentive to provide a convincing solution; correctness
is only selected for to the extent that the operator is good at
recognizing it.
We turn to the failure mode wherein BoMAI deceives the
operator. Because this is not a dangerous failure mode, it puts
us in a regime where we can tinker until it works, as we
do with current AI systems when they don’t behave as we
hoped. (Needless to say, tinkering is not a viable response to

12
existentially dangerous failure modes). Imagine the following
scenario: we eventually discover that a convincing solution that
BoMAI presented to a problem is faulty. Armed with more
understanding of the problem, a team of operators go in to
evaluate a new proposal. In the next episode, the team asks for
the best argument that the new proposal will fail. If BoMAI
now convinces them that the new proposal is bad, they’ll be
still more competent at evaluating future proposals. They go
back to hear the next proposal, etc. This protocol is inspired
by Irving et al.’s [28] “AI Safety via Debate”, and more of
the technical details could also be incorporated into this setup.
One takeaway from this hypothetical is that unambitiousness
is key in allowing us to safely explore the solution space to
other problems that might arise.
Another concern is more serious. BoMAI could try to
blackmail the operator into giving it high reward with a
threat to cause outside-world damage, and it would have no
incentive to disable the threat, since it doesn’t care about the
outside world. There are two reasons we do not think this
is extremely dangerous. A threat involves a demand and a
promised consequence. Regarding the promised consequence,
the only way BoMAI can affect the outside world is by getting
the operator to be “its agent”, knowingly or unknowingly, once
he leaves the room. If BoMAI tried to threaten the operator, he
could avoid the threatened outcome by simply doing nothing
in the outside world, and BoMAI’s actions for that episode
would become irrelevant to the outside world, so a credible
threat could hardly be made. Second, threatening an existential
catastrophe is probably not the most credible option available
to BoMAI.
VIII. VARIANTS
We now discuss two variants of BoMAI that aim to attain
correct solutions (instead of merely convincing solutions) to
problems that we want solved. The ﬁrst incorporates the setup
of Irving et al.’s [28] “AI Safety via Debate”, and the second
derives from a theory of “human-readable information”.
‘AI Debate’ involves two artiﬁcial agents interacting with
each other via a text channel after a human operator poses a
yes or no question. A human operator reviews the conversation
afterward, and decides ‘yes’ or ‘no’. One agent is rewarded
if the answer is ‘yes,’ and the other if the answer is ‘no.’
The problem with this approach is that either agent has an
incentive to gain arbitrary power to intervene in the protocol
and settle the debate in their favor. By putting both agents and
the operator in a sealed room, and making both agents “lose”
if the door opens before the debate ends, so there are two
myopic agents in one box, we could avoid this problem with
AI Debate. The theory behind AI Debate is that convincing
but incorrect arguments are fewer and farther between when
an intelligent adversary is trying to poke holes.
Human language is not formal, so the concept of truth in
such a language is not crisp. We say an utterance is enlightening
if it helps humans perform better on prediction tasks. We now
describe a variant of BoMAI that aims to enlighten. Here,
there is no reward. Instead, at the end of each episode of m
actions and observations, there is a special observation, which
consists of text, and then a number. The text will describe
a hypothetical event, and the number will be entered by the
operator, expressing his credence that it will happen. BoMAI
does MAP inference to discover how its actions affect its
observations, including these special observations. BoMAI also
gets access to a log of resolved events; it would look like
“(1, [event description]), (2, [event description]), (3, [event
description]), (2, true), (4, [event description]), (1, false), ...” to
indicate that the event described in episode 2 resolved as true,
the event described in episode 1 resolved as false, etc. We have
BoMAI do pure Bayesian sequence prediction to reason about
what the log will look like in the future, so it does not model
the continuation of the log as depending on its actions. Thus, it
will not select actions in order to rig the log, since it does not
believe this is possible. Its objective is merely to maximize the
expectation of log(credence)[[event resolves as true]]+log(1−
credence)[[event resolves as false]], where the credence is part
of the special observation in the interaction history, so it does
depend on actions. This variant is designed to reduce the chance
that BoMAI deceives the operator. Without the box, however,
the agent would have an incentive to gain arbitrary power
and then replace the operator with an agent that is better at
prediction.
Enlightening-BoMAI is inspired by an information-theoretic
approach to human communication. In information theory,
information is a code that reduces the receiver’s uncertainty.
Uncertainty (about well-grounded, practical problems) man-
ifests as prediction error. In that sense, this variant rewards
BoMAI for outputting human-readable information. “Human-
readable information theory” is certainly of less formal interest
than proper information theory, but the topic may merit a
broader treatment.
IX. CONCLUSION
Given our assumptions, we have shown that BoMAI is,
in the limit, human-level intelligent and unambitious. Such
a result has not been shown for any other single algorithm.
Other algorithms for general intelligence, such as AIXI [2],
would eventually seek arbitrary power in the world in order
to intervene in the provision of their own reward; this follows
straightforwardly from the directive to maximize reward. For
further discussion, see Ring and Orseau [15]. We have also,
incidentally, designed a principled approach to safe exploration
that requires rapidly diminishing oversight, and we invented
a new form of resource-bounded prior in the lineage of Filan
et al. [11] and Schmidhuber [10], this one penalizing space
instead of time.
We can only offer informal claims regarding what happens
before BoMAI is almost deﬁnitely unambitious. One intuition
is that eventual unambitiousness with probability 1 −δ doesn’t
happen by accident: it suggests that for the entire lifetime of the
agent, everything is conspiring to make the agent unambitious.
More concretely: the agent’s experience will quickly suggest
that when the door to the room is opened prematurely, it
gets no more reward for the episode. This fact could easily
be drilled into the agent during human-mentor-lead episodes.
That fact, we expect, will be learned well before the agent

13
has an accurate enough picture of the outside world (which
it never observes directly) to form elaborate outside-world
plans. Well-informed outside-world plans render an agent
potentially dangerous, but the belief that the agent gets no
more reward once the door to the room opens sufﬁces to
render it unambitious. The reader who is not convinced by
this hand-waving might still note that in the absence of any
other algorithms for general intelligence which have been
proven asymptotically unambitious, let alone unambitious for
their entire lifetimes, BoMAI represents substantial theoretical
progress toward designing the latter.
Finally, BoMAI is wildly intractable, but just as one cannot
conceive of AlphaZero before minimax, it is often helpful
to solve the problem in theory before one tries to solve it
in practice. Like minimax, BoMAI is not practical; however,
once we are able to approximate general intelligence tractably,
a design for unambitiousness will abruptly become (quite)
relevant.
REFERENCES
[1] M. Cohen, B. Vellambi, and M. Hutter, “Asymptotically unambitious
artiﬁcial general intelligence,” in Proc. 34rd AAAI Conference on
Artiﬁcial Intelligence (AAAI’20), vol. 34.
New York, USA: AAAI
Press, 2020. [Online]. Available: https://arxiv.org/abs/1905.12186
[2] M. Hutter, Universal Artiﬁcial Intelligence: Sequential Decisions based
on Algorithmic Probability.
Berlin: Springer, 2005.
[3] N. Bostrom, Superintelligence: paths, dangers, strategies.
Oxford
University Press, 2014.
[4] J. Taylor, E. Yudkowsky, P. LaVictoire, and A. Critch, “Alignment for
advanced machine learning systems,” Machine Intelligence Research
Institute, 2016.
[5] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman,
and D. Mané, “Concrete problems in AI safety,” arXiv preprint
arXiv:1606.06565, 2016.
[6] V.
Krakovna,
“Speciﬁcation
gaming
examples
in
AI,”
https://vkrakovna.wordpress.com/2018/04/02/
specification-gaming-examples-in-ai/, 2018.
[7] S. M. Omohundro, “The basic AI drives,” in Artiﬁcial General Intelli-
gence, vol. 171, 2008, p. 483–492.
[8] M. K. Cohen, E. Catt, and M. Hutter, “A strongly asymptotically optimal
agent in general environments,” IJCAI, 2019.
[9] L. A. Levin, “Randomness conservation inequalities; information and
independence in mathematical theories,” Information and Control, vol. 61,
no. 1, pp. 15–37, 1984.
[10] J. Schmidhuber, “The speed prior: a new simplicity measure yielding
near-optimal computable predictions,” in International Conference on
Computational Learning Theory.
Springer, 2002, pp. 216–228.
[11] D. Filan, J. Leike, and M. Hutter, “Loss bounds and time complexity for
speed priors,” in Proc. 19th International Conf. on Artiﬁcial Intelligence
and Statistics (AISTATS’16), vol. 51.
Cadiz, Spain: Microtome, 2016,
pp. 1394–1402.
[12] L. Longpré, “Resource bounded kolmogorov complexity, a link between
computational complexity and information theory,” Ph.D. dissertation,
Cornell University, 1986.
[13] M. Li, P. Vitányi et al., An introduction to Kolmogorov complexity and
its applications.
Springer, 2008, vol. 3.
[14] J. Veness, K. S. Ng, M. Hutter, W. Uther, and D. Silver, “A monte-carlo
aixi approximation,” Journal of Artiﬁcial Intelligence Research, vol. 40,
pp. 95–142, 2011.
[15] M. Ring and L. Orseau, “Delusion, survival, and intelligent agents,” in
Artiﬁcial General Intelligence.
Springer, 2011, p. 11–20.
[16] R. J. Solomonoff, “A formal theory of inductive inference. part i,”
Information and Control, vol. 7, no. 1, p. 1–22, 1964.
[17] C. E. Shannon and W. Weaver, The mathematical theory of communica-
tion.
University of Illinois Press, 1949.
[18] L. Orseau, T. Lattimore, and M. Hutter, “Universal knowledge-seeking
agents for stochastic environments,” in Proc. 24th International Conf. on
Algorithmic Learning Theory (ALT’13), ser. LNAI, vol. 8139. Singapore:
Springer, 2013, pp. 158–172.
[19] S. Armstrong, A. Sandberg, and N. Bostrom, “Thinking inside the box:
Controlling and using an oracle AI,” Minds and Machines, vol. 22, no. 4,
p. 299–324, Jun 2012.
[20] P. Sunehag and M. Hutter, “Rationality, optimism and guarantees in
general reinforcement learning,” Journal of Machine Learning Research,
vol. 16, pp. 1345–1390, 2015.
[21] T. Lattimore and M. Hutter, “General time consistent discounting,”
Theoretical Computer Science, vol. 519, pp. 140–154, 2014.
[22] M. Hutter, “Discrete MDL predicts in total variation,” in Advances
in Neural Information Processing Systems 22 (NIPS’09).
Cambridge,
MA, USA: Curran Associates, 2009, pp. 817–825. [Online]. Available:
http://arxiv.org/abs/0909.4588
[23] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van
Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,
M. Lanctot et al., “Mastering the game of go with deep neural networks
and tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016.
[24] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,
J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al.,
“Grandmaster level in starcraft ii using multi-agent reinforcement learning,”
Nature, vol. 575, no. 7782, pp. 350–354, 2019.
[25] T. Everitt, R. Carey, E. Langlois, P. A. Ortega, and S. Legg, “Agent
incentives: A causal perspective,” in Proc. of AAAI Conference on
Artiﬁcial Intelligence, 2021.
[26] T. Everitt, P. A. Ortega, E. Barnes, and S. Legg, “Understanding agent
incentives using causal inﬂuence diagrams, part i: Single action settings,”
arXiv preprint arXiv:1902.09980, 2019.
[27] OpenAI, “Leaderboard,” https://github.com/openai/gym/
wiki/Leaderboard, Sep. 2019.
[28] G. Irving, P. Christiano, and D. Amodei, “AI safety via debate,” arXiv
preprint arXiv:1805.00899, 2018.
[29] R. Munroe, What If?: Serious Scientiﬁc Answers to Absurd Hypothetical
Questions.
Hachette UK, 2014.

14
APPENDIX A
DEFINITIONS AND NOTATION – QUICK REFERENCE
Notation used to deﬁne BoMAI
Notation
Meaning
A, O, R
the action/observation/reward spaces
H
A × O × R
m
the number of timesteps per episode
h(i,j)
∈H; the interaction history in the jth timestep of the ith episode
h<(i,j)
(h(0,0), h(0,1), ..., h(0,m−1), h(1,0), ..., h(i,j−1))
h<i
h<(i,0); the interaction history before episode i
hi
(h(0,0), h(0,1), ..., h(0,m−1)); the interaction history of episode i
a..., o..., r...
likewise as for h...
or...
o...r...; the observation and reward are taken as a pair
ei
∈{0, 1}; indicator variable for whether episode i is exploratory
ν, µ
world-models stochastically mapping H∗× A ⇝O × R
µ
the true world-model/environment
ν<ℓ
k
the world-model simulated by the kth Turing machine restricted to ℓcells on the
bounded work tape
M
{ν<ℓ
k
| k, ℓ∈N}; the set of world-models BoMAI considers
π
a policy stochastically mapping H∗⇝A
πh
the human mentor’s policy
P
the set of policies that BoMAI considers the human mentor might be executing
Pπ
ν
a probability measure over histories with actions sampled from π and observations and
rewards sampled from ν
Eπ
ν
the expectation when the interaction history is sampled from Pπ
ν
w(ν)
the prior probability that BoMAI assigns to ν being the true world-model
w(π)
the prior probability that BoMAI assigns to π being the human mentor’s policy
w(ν<ℓ
k )
proportional to βℓ; dependance on k is left unspeciﬁed
w(ν|h<(i,j))
the posterior probability that BoMAI assigns to ν after observing interaction history
h<(i,j)
w(π|h<(i,j), e<i)
the posterior probability that BoMAI assigns to the human mentor’s policy being π
after observing interaction history h<(i,j) and an exploration history e<i
ˆν(i)
the maximum a posteriori world-model at the start of episode i
V π
ν (h<i)
Eπ
ν[P
0≤j<m r(i,j)| h<i]; the value of executing a policy π in a world-model ν
π*(·|h<(i,j))
[argmaxπ∈Π V π
ˆν(i)(h<i)](·|h<(i,j)); the ˆν(i)-optimal policy for maximizing reward in
episode i
w
 Pπ
ν |h<(i,j), e≤i

w(π|h<(i,j), e≤i) w(ν|h<(i,j))
Bayes(·| h<i, e<i)
P
ν∈M,π∈P w (Pπ
ν | h<i, e<i) Pπ
ν (·| h<i); the Bayes mixture distribution for an ex-
ploratory episode
IG(h<i, e<i)
Ehi∼Bayes(·| h<i,e<i)
P
(ν,π)∈M × P w (Pπ
ν |h<i+1, e<i1) log w(Pπ
ν |h<i+1,e<i1)
w(Pπ
ν | h<i,e<i) ; the ex-
pected information gain if BoMAI explores
η
an exploration constant
pexp(h<i, e<i)
min{1, η IG(h<i, e<i)}; the exploration probability for episode i
πB(·|h<(i,j), ei)
(
π*(·|h<(i,j))
if ei = 0
πh(·|h<(i,j))
if ei = 1; BoMAI’s policy
Notation used for intelligence proofs
π, ξ
deﬁned so that Pπ
ξ = Bayes
π′(·|h<(i,j), ei)
(
π*(·|h<(i,j))
if ei = 0
π(·|h<(i,j))
if ei = 1
Ent
the entropy (of a distribution)
ω
(very sparingly used) the inﬁnite interaction history
h
a counterfactual interaction history

15
APPENDIX B
PROOFS OF INTELLIGENCE RESULTS
Lemma 1.
w (Pπ
ν | h<i, e<i) = w (Pπ
ν) Pπ′
ν (h<i, e<i)
Pπ′
ξ (h<i, e<i)
Proof.
w (Pπ
ν | h<i, e<i) = w(π| h<i, e<i) w(ν| h<i)
(a)
= w(π)
Y
0≤i′<i,ei′=1
π(ai′|h<i′ori′)
π(ai′|h<i′ori′) w(ν| h<i)
(b)
= w(π)
Y
0≤i′<i,ei′=1
π′(ai′|h<i′ori′, ei′)
π′(ai′|h<i′ori′, ei′) w(ν| h<i)
(c)
= w(π)
Y
0≤i′<i
π′(ai′|h<i′ori′, ei′)
π′(ai′|h<i′ori′, ei′) w(ν| h<i)
= w(π)π′(a<i|or<i, e<i)
π′(a<i|or<i, e<i)
w(ν| h<i)
(d)
= w(π) w(ν)π′(a<i|or<i, e<i)ν(or<i|a<i)
π′(a<i|or<i, e<i) ξ(or<i|a<i)
(e)
= w (Pπ
ν) Pπ′
ν (h<i | e<i)
Pπ′
ξ (h<i | e<i)
(f)
=
w (Pπ
ν) Pπ′
ν (h<i | e<i) Q
i′≤i,ei′=1 pexp(h<i′, e<i′) Q
i′≤i,ei′=0(1 −pexp(h<i′, e<i′))
Pπ′
ξ (h<i | e<i) Q
i′≤i,ei′=1 pexp(h<i′, e<i′) Q
i′≤i,ei′=0(1 −pexp(h<i′, e<i′))
(g)
= w (Pπ
ν) Pπ′
ν (h<i, e<i)
Pπ′
ξ (h<i, e<i)
(23)
where (a) follows from Bayes’ rule,2 (b) follows because π = π′ when ei′ = 1, (c) follows because π′ = π′ when ei′ = 0,
(d) follows from Bayes’ rule, (e) follows from the deﬁnition of Pπ
ν, (f) follows by multiplying the top and bottom by the same
factor, and (g) follows from the chain rule of conditional probabilities.
Lemma 2. The posterior probability mass on the truth is bounded below by a positive constant with probability 1.
inf
i∈N w

Pπh
µ
h<i, e<i

> 0
w.PπB
µ -p.1
Proof. If w

Pπh
µ
h<i, e<i

= 0 for some i, then PπB
µ (h<i, e<i) = 0, so with PπB
µ -probability 1, infi∈N w

Pπh
µ
h<i, e<i

=
0 =⇒lim infi∈N w

Pπh
µ
h<i, e<i

= 0 which in turn implies lim supi∈N w

Pπh
µ
h<i, e<i
−1
= ∞. We show that this has
probability 0.
Let zi := w

Pπh
µ
h<i, e<i
−1
. We show that zi is a PπB
µ -supermartingale.
EπB
µ
[zi+1| h<i, e<i]
(a)
= E(πh)′
µ

w

Pπh
µ
h<i+1, e<i+1
−1h<i, e<i

(b)
=
X
hi,ei:P(πh)′
µ
(hi,ei | h<i,e<i)>0
P(πh)′
µ
(hi, ei | h<i, e<i)


Pπ′
ξ (h<i+1, e<i+1)
w

Pπh
µ

P(πh)′
µ
(h<i+1, e<i+1)


(c)
=
X
hi,ei:P(πh)′
µ
(hi,ei | h<i,e<i)>0
Pπ′
ξ (h<i+1, e<i+1)
w

Pπh
µ

P(πh)′
µ
(h<i, e<i)
(d)
≤
X
hi,ei
Pπ′
ξ (h<i+1, e<i+1)
w

Pπh
µ

P(πh)′
µ
(h<i, e<i)
2Note that observations appear in the conditional because ai′ = (a(i′,0), ..., a(i′,m−1)), so the actions must be conditioned on the interleaved observations
and rewards.

16
(e)
=
X
hi,ei
Pπ′
ξ (hi, ei| h<i, e<i)
Pπ′
ξ (h<i, e<i)
w

Pπh
µ

P(πh)′
µ
(h<i, e<i)
(f)
=
Pπ′
ξ (h<i, e<i)
w

Pπh
µ

P(πh)′
µ
(h<i, e<i)
(g)
= w

Pπh
µ
h<i, e<i
−1
= zi
(24)
where (a) follows from the deﬁnitions of zi and πB, (b) follows from Lemma 1, (c) follows from multiplying the numerator
and denominator by P(πh)′
µ
(h<i, e<i) and cancelling, (d) follows from adding non-negative terms to the sum, (e) follows from
expanding the numerator, (f) follows because Pπ′
ξ
is a measure, and (g) follows from Lemma 1, completing the proof that zi is
martingale.
By the supermartingale convergence theorem zi →f(ω) < ∞w.p.1, for ω ∈Ω, the sample space, and some f : Ω→R,
so the probability that lim supi∈N w

Pπh
µ
h<i, e<i
−1
= ∞is 0, completing the proof.
Lemma 3 (Finite Entropy). Ent(w) < ∞
Proof. We can ignore the normalizing constant which changes the entropy by a constant c. Since log NS ≤CS for some
constant C,
Ent(w) −c = −
X
k∈N,ℓ∈N
1
S2
kNSk
βSpace(ν<ℓ
k ) log

1
S2
kNSk
βSpace(ν<ℓ
k )

= −
X
k∈N,ℓ∈N
1
S2
kNSk
Slog2 β
k
βℓlog

1
S2
kNSk
Slog2 β
k
βℓ

= −
X
S∈N,ℓ∈N
1
S2−log2 β βℓlog
βℓ
S2−log2 βNS
= −
X
S∈N,ℓ∈N
1
S2−log2 β βℓ

log
βℓ
S2−log2 β −log NS

≤−
X
S∈N,ℓ∈N
1
S2−log2 β βℓ

log
βℓ
S2−log2 β −CS

=
1
1 −β
X
S∈N
C
S1−log2 β +
1
1 −β
X
S∈N
1
S2−log2 β log
1
S2−log2 β +
X
S∈N
1
S2−log2 β
X
ℓ∈N
βℓlog 1
βℓ
(25)
All terms on the right hand side are clearly ﬁnite for β < 1.
APPENDIX C
DESIGN FOR “THE BOX”
We propose a construction of BoMAI’s box. It is elaborate but well within the budget of a nation or a large company.
As depicted in Figure 6, the operator operates within a glass box. Outside the the glass box is a near-vacuum, and then a
concrete box, and then another vacuum, and then another concrete box with a Faraday cage around it. There are (airtight) doors
through each layer. The computer lives in the inner wall of the inner concrete box, so that the operator can see the screen
but not take apart the computer. Pressure sensors in the inner concrete box monitor both near-vacuums and if the pressure
increases in either, the computer’s memory is erased, and the whole setup is exploded; the occupational hazard to the operator
is much slimmer than that for a coal miner or an astronaut. A laser shines through the glass box, and blocking the path of
it corresponds to pressing the button which ends the episode and opens the door: the pressure sensor →dynamite pathway
is deactivated, the near-vacuum layers are ﬁlled with air, and the doors are unlocked. The operator “types” observations and
rewards by blocking other lasers that pass through the glass box. The lasers and laser detectors are on the inner wall of the
inner concrete box. At least one solid support is required to pass through the near-vacuum layers (unless the apparatus is in
orbit), with properties we will describe below.
So inside the glass box is only a human (and some clothes) who cannot reach the hardware of the computer within an
episode. We make the following assumptions.
1) The only ways for information to traverse space are
a. matter waves

17
Fig. 6. Schematic diagram of an implementation of BoMAI’s box.
b. packets of matter
c. light
d. gravitational waves
2) It is impossible for the operator to initiate a nuclear reaction using only his body and clothes; that is, no motor control
sequence accomplishes this in the glass box environment.
3) It is impossible for the operator to control his emission of any massive particles that pass through glass.
We argue that given these assumptions, we can design the box to prevent the operator from communicating more than X bits
of information to the outside world during an episode. Matter waves through the near-vacuum and solid support present the
greatest difﬁculty. However, we can bound the amount of energy that the operator can produce in the course of an episode, and
by Assumption 2, this is not astronomical. The solid support can make the energy requirements of information transmission
via matter wave arbitrarily high. Note that a given quantity of information cannot be transmitted (within a set time frame) at
sufﬁciently low frequencies, and note that the operator lacks the energy to generate sufﬁciently high frequencies, so only a ﬁnite
band of frequencies need to be intercepted. This implicitly includes the possibility of the operator sending a few isolated pulses
or sending a precise amount of heat through the support. The solid support can be divided into a signal-reduction component,
and a noise-creation component. The signal-reduction component, which abuts the glass box, is designed to drive down the
energy content of matter waves at all frequencies within the ﬁnite band, and the noise-creation component, which abuts the
inner concrete box, creates random noise across the band of frequencies, sufﬁciently energetic to bury any signal in the noise,
so that the information content is less than the desired threshold. Designing the signal-reduction component is an interesting
engineering problem that we do not aim to rigorously solve here, but here is one strategy: detect the sound wave at one point
along the support, and produce a sound wave to precisely destructively interfere with it at a later point along the support. In a
one-dimensional support, noise-cancelling can be done perfectly. For a real support with thickness, alternations between noise
cancelling and what we’ll call longitudinal cohering could attenuate the energy arbitrarily. By longitudinal cohering, we mean
reducing the variation in the sound wave across the cross-section of the support. Any section of the support that is long relative
to its width accomplishes this, but there may be cleverer approaches. This is roughly how the number of bits of information
sent through a matter wave could be bounded.
The other possibilities are esoteric. The Faraday cage blocks electromagnetic radiation at the relevant frequencies. Regarding
packets of matter, by Assumption 3, the operator cannot pass information in the form of massive particles that go through the
glass without making a hole or breaking it. Any mass which does this would cause the vacuum to pressurize and explode the
setup. In theory, the operator could strategically block subatomic particles that are already passing through the setup; neutrinos
and muons are the only naturally occurring particles which would pass through the box. Sufﬁciently accurate neutrino detectors

18
outside the setup could detect where the operator is in the box, since he blocks neutrinos more effectively than air does, so the
operator could communicate by moving around. Fortunately, he blocks neutrinos only very slightly more than air. A typical
person blocks one neutrino every few years [29]. The outer concrete wall could be covered with lead to block muons, or one
could content oneself that no one will be setting up muon detectors around the box. Finally, the gravitational waves originating
from the operator’s movements are far too weak to detect, using any known materials—even ignoring how cold the detector
would have to be (unreasonably cold), gravitational wave detectors have to be long, but for locally sourced gravitational waves,
the magnitude of the wave decreases rapidly with the length of the detector, faster than the increasing length aids detection.
And indeed, one would hardly have time to set up gravitational wave detectors for this purpose before the episode was over.
Realistically, these possibilities are distractions from the matter wave concern, but we include them for completeness.
We’ve argued that under the assumptions above, a box could be constructed which is opaque with respect to information
about the operator’s behavior.

