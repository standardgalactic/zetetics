Can Large Language Models Reason and Plan?
Subbarao Kambhampati
School of Computing & Augmented Intelligence
Arizona State University
email: rao@asu.edu
A version appears in the Annals of The New York Academy of Sciences:
https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.15125
Large Language Models (LLMs), essentially n-gram mod-
els on steroids* that have been trained on web-scale lan-
guage corpora (or, effectively, our civilizational knowl-
edge), have caught our collective imagination with linguis-
tic behaviors that no one expected text completion systems
to possess3. By training and operation, LLMs are perhaps
best seen as giant non-veridical memories akin to an ex-
ternal System 12 for us all (see Figure 1). Their seem-
ing versatility has however led many researchers to wonder
whether they can also do well on planning and reasoning
tasks typically associated with System 2 competency.
Nothing in the training and use of LLMs would seem
to suggest remotely that they can do any type of princi-
pled reasoning (which, as we know, often involves com-
putationally hard inference/search). What LLMs are good
at is a form of universal approximate retrieval.
Unlike
databases that index and retrieve data exactly, LLMs, as n-
gram models, probabilistically reconstruct completions for
the prompt word by word–a process we shall refer to as
approximate retrieval. This means that LLMs can’t even
guarantee memorizing complete answers, something that
is the flip side of their appeal about constructing “novel”
prompt completions on the fly. The boon (“creativity”) and
bane (“hallucination”) of LLMs is that n-gram models will
naturally mix and match–and have almost as much trouble
strictly memorizing as we do. It is indeed the very basis of
their appeal.
*LLMs are trained to predict the distribution of the n-th to-
ken given n-1 previous tokens. GPT3.5 that powered the origi-
nal ChatGPT is, for example, a roughly 3001-gram model of lan-
guage.
Figure 1. An informal account of viewing LLM as a giant external
non-veridical memory that acts as a pseudo System 1
Despite this, the “Large Language Models are Zero-Shot
⟨insert-your-reasoning-task ⟩” has almost become a meme
paper title! At some level, this trend is perhaps inevitable as
in the era of LLMs, AI has become a form of ersatz natural
science5–driven by observational studies of capabilities of
these behemoth systems.
So, are these n-gram models on steroids really capable of
planning and reasoning? In the summer of 2022, when my
research group wanted to better answer this question, most
reasoning claims were still somewhat anecdotal. So, we set
out to evaluate GPT3 on a set of planning instances derived
from the domains typically used in the International Plan-
ning Competition (IPC) –including the well known Blocks
World†. Our results13 were contrary to the anecdotal claims
about the planning abilities of LLMs, and when we made
them public, received significant attention in the AI circles.
By the beginning of 2023, with the wide-spread public re-
†https://en.wikipedia.org/wiki/Blocks world
1
arXiv:2403.04121v2  [cs.AI]  8 Mar 2024

lease of ChatGPT, and later, GPT4, there were a slew of ad-
ditional claims, including in refereed papers, about LLM’s
abilities to reason and plan. So we decided to repeat our
tests on both GPT3.5 and GPT412. Initial results showed
that there was some improvement in the accuracy of gen-
erated plans from GPT3 to GPT3.5 to GPT4, with GPT4
reaching 30% empirical accuracy in the Blocks World (al-
beit still lower in other domains). We then wanted to know
whether the modest improvement is because of the im-
proved approximate retrieval abilities or whether GPT4 is
actually doing/searching for plans.
Let us pause to note that my interest here is not whether
LLMs can fake reasoning (by giving correct answers to
reasoning tasks from memory and pattern finding), but
whether they can actually do principled reasoning.
Of
course, seeing patterns in reasoning problems is not any-
thing to be sneezed at. After all, our interest in master-
ing it is what is behind much of “street fighting” math
(e.g. George P´olya’s ”How to Solve it”). But finding ap-
proximate shortcuts over provably correct reasoning proce-
dures is obviously not equivalent to doing reasoning–unless
you have an ability to establish from first principles that
your hunch is actually correct. It is challenging to decide
whether a system (or a human, for that matter) is memoriz-
ing or solving a problem from scratch–especially as the sys-
tems (or humans) get trained on larger and larger “question
banks.” This is a challenge that most instructors and inter-
viewers are acutely aware of. Think of that infamous “Why
are manhole covers round?” interview question. While it
may well have given the interviewer an insight into the
candidate’s analytical reasoning skills the very first time it
was asked, all it does with high probability now is to con-
firm whether the candidate trained on the interview ques-
tion banks!
Considering that the LLMs don’t suffer some of the nor-
mal limitations of humans–such as having a life on the
side, and thus not having the time or inclination to focus
exclusively on the test/interview preparation for long peri-
ods, they can support approximate retrieval over webscale
corpora. My research group wanted to check if the im-
proved performance of GPT4 is because of approximate re-
trieval from a larger training corpus, or really comes from
its ability to plan. One way of checking this for planning
tasks is to reduce the effectiveness of approximate retrieval
by obfuscating the names of the actions and objects in the
planning problem. When we did this for test domains12,11,
GPT4’s empirical performance plummeted precipitously,
despite the fact that none of the standard off-the-shelf AI
planners have any trouble with such obfuscation.‡
Perhaps they can’t do planning autonomously straight out
of the box, but can they do it with a little nudge? There
are broadly two popular techniques for such nudging. The
first, called “fine tuning,” is rather straightforward: take a
general LLM and fine tune it on planning problems (i.e.,
instances and their solutions), with the hope that they will
subsequently make better guesses (see the left-hand side
of Figure 1). While our own limited experiments didn’t
show any significant improvement through fine tuning, it
is possible that with even more fine tuning data and ef-
fort, the quality of LLM guesses may well improve. But
all that such fine tuning is doing is converting the planning
task into a memory-based approximate retrieval (akin to the
memorization/compilation from System 2 to System 1; see
Figure 1). It doesn’t prove that LLMs are able to plan.
The second way to improve planning (and reasoning) per-
formance is to prompt an LLM back with hints/suggestions
about how it can improve its initial plan guess. The crucial
questions here are (a) whether this back prompting is man-
ual or automated (b) who is certifying the correctness of the
final answer and (c) whether the prompts inject additional
problem knowledge or are just merely exhorting the LLM
to try again.
The cleanest approach–one we advocate12,6–is to let an ex-
ternal model-based plan verifier do the back prompting and
to certify the correctness of the final solution. In general,
such LLM-Modulo frameworks6 can gainfully leverage the
amazing idea generation capabilities of LLMs with sound
external verifiers in a generate-test-critique framework with
gurantees.
In contrast, by far the more popular methodology is to
have the human in the loop prompt the LLM iteratively.
The problem with this is that it is highly susceptible to the
‡As these results came about at the height of sparks of
AGI/existential risk angst, we couldn’t resist the tongue-in-cheek
editorializing that if GPT4 ever goes rogue, you can stymie it by
throwing a simple planning problem at it! Humor aside, nothing
in our studies showed that GPT4 is capable of generating exe-
cutable plans autonomously.
2

Figure 2. Claimed reasoning capabilities of LLMs are sometimes
due to the subconscious helpful iterative prompting by the humans
in the loop (graphic adapted from https://xkcd.com/2347/ under
Creative Commons License)
Clever Hans effect§, where the LLM is merely generating
guesses, and it is the human in the loop, with the knowledge
of right vs. wrong solutions, who is steering the LLM–even
if they didn’t set out to do so deliberately. The credit and
blame for the ensuring accuracy, if any, falls squarely on
the human in the loop. The relevance of such a frame-
work becomes questionable when the human-in-the-loop
doesn’t know (or is unable to verify) the answer to the rea-
soning/planning problem themselves. Thus the tongue-in-
cheek characterization of LLM reasoning abilities in Fig-
ure 2.
A variation on the second approach is to have the LLM it-
self “critique” the guesses it generates and iteratively self-
improve. Although some papers seem to swear by such a
“self-improvement” capability of LLMs, the plausibility of
such a claim hinges on the belief that the LLMs are bet-
ter at verifying their solutions than they are at generating
them. While never explicitly justified, the assumption rests
§https://en.wikipedia.org/wiki/Clever Hans
on either analogies to humans or indirect nods to compu-
tational complexity arguments. While humans sometimes
do show the capability of correcting their own erroneous
guesses with self-critiquing, there seems to be no basis for
that assumption in the case of LLMs. And while for many
computational tasks (e.g. those in class NP¶), the verifica-
tion is often of lower complexity than generation, that fact
doesn’t seem particularly relevant for LLMs which are gen-
erating (approximately retrieving) guesses, rather than ac-
tually solving the problem with guarantees. Indeed two re-
cent studies from my lab–one on plan verification10 and the
other on constraint verification9–seem to throw cold water
on this optimism by showing that with “self-verification”
performance actually worsens. This is because LLMs hal-
lucinate both false positives and false negatives while ver-
ifying the solutions they generate. One reason this is not
recognized in earlier literature is that there self-verification
claims are often made in the context of tacit knowledge
tasks for which there is little possibility of a verifier (e.g.
writing/improving essays), making it harder to evaluate
whether LLM’s critiquing actually helped. Paradoxically,
the fact that it is infeasible to write sound verifiers for tacit
knowledge tasks also makes it easier to mistake LLMs for
being as reasonable a critic as any!|| In other cases, an ex-
ternal simulator winds up playing the role of sound verifi-
cation.
While the foregoing questions the claims that LLMs are
capable of planning/reasoning, it is not meant to imply that
LLMs don’t have any constructive roles to play in solving
planning/reasoning tasks. In particular, their uncanny abil-
ity to generate ideas/potential candidate solutions–albeit
with no guarantees about those guesses–can still be valu-
able in the “LLM-Modulo” setups6, in conjunction with
either model-based verifiers or expert humans in the loop.
The trick to avoiding ascribing autonomous reasoning ca-
pabilities to LLMs is to recognize that LLMs are generating
potential answers that still need to be checked by external
verifiers.
The skeptical reader might now ask: But what about all
¶NP stands for nondeterministic polynomial, and covers the
class of computational problems whose solutions can be verified
in polynomial time.
||In other words, LLMs can be as good as that Peloton instruc-
tor in confidently critiquing Christopher Nolan movies.
3

those papers at high profile AI conferences that claim to
show planning abilities of LLMs? To analyze those claims,
we need to first understand that solving planning tasks re-
quires (a) having the necessary planning domain knowl-
edge–the actions and their preconditions, effects; the stan-
dard hierarchical recipes (e.g. task reduction schemas in
Hierarchical Task Network planning), past cases/plans etc.
and (b) being able to assemble this knowledge into an ex-
ecutable plan that takes care of any subgoal/resource inter-
actions. The first can be called the knowledge acquisition
part, and the second reasoning/planning part. Many of the
papers claiming planning abilities of LLMs, on closer ex-
amination, wind up confusing general planning knowledge
extracted from the LLMs for executable plans. When all we
are looking for are abstract plans, such as “wedding plans,”
with no intention of actually executing said plans directly,
it is easy to confuse them for complete executable plans.
Indeed, our close examination of several papers claiming
planning capabilities7 for LLMs suggests that they either
are evaluating in domains/tasks where subgoal interactions
can be safely ignored, or delegating the interaction resolu-
tion (reasoning) to the humans in the loop (who, through
repeated prompting, have to “correct” the plan). Some-
times, in common sense domains, or with enough fine tun-
ing, the “assembling” part may also be obviated by having
seen a case that pretty much corresponds to the problem
that needs to be solved. Without these assumptions or mit-
igations, the plans that come out of LLMs may look rea-
sonable to the lay user, but lead to execution time interac-
tions and errors. These issues are illustrated in part by a
recent news story about the proliferation of travel planning
books8, mostly auto-extracted from LLMs, and the ensu-
ing disappointment of the unsuspecting end users who buy
them mistaking them for usable plans!
The fact that LLMs are often good at extracting planning
knowledge can indeed be gainfully leveraged. As we have
argued in recent work1, LLMs can be a rich source of
approximate models of world/domain dynamics and user
preferences, as long as the humans (and any specialized
critics) in the loop verify and refine the models, and give
them over to model-based solvers. This way of using LLMs
has the advantage that the humans need only be present
when the dynamics/preference model is being teased out
and refined, with the actual planning after that being left to
sound frameworks with correctness guarantees6.
Figure 3. Viewing LLMs as an approximate knowledge source
trained over civilizational knowledge
Such a framework has striking similarities to knowledge-
based AI systems of yore, with LLMs effectively replac-
ing the “knowledge engineer” (Figure 3). Given the rather
quixotic and dogmatic shift of AI away from approaches
that accept domain knowledge from human experts, some-
thing I bemoaned in “P´olanyi’s revenge and AI’s new ro-
mance with tacit knowledge”4 this new trend of using
LLMs as knowledge sources can be viewed as a form of
avenging Polanyi’s revenge (by bringing explicit knowl-
edge back to AI systems, if only as gleaned from LLMs).**
Indeed, LLMs make it easy to get problem-specific knowl-
edge, as long as we are willing to relax correctness require-
ments of that knowledge. In contrast to the old knowledge
engineering approaches, LLMs offer this without making it
look like we are inconveniencing any specific human (we
are, instead, just leveraging everything humans have told
each other!). So the million dollar question for reasoning
tasks becomes: “how would you do planning if you have
some doddering know-it-all ready to give you any kind of
knowledge?” The LLM-Modulo framework6 is a princi-
pled method for tackling this challenge.
To summarize, nothing that I have read, verified, or done
gives me any compelling reason to believe that LLMs do
reasoning/planning, as normally understood. What they do
instead, armed with web-scale training, is a form of uni-
versal approximate retrieval, which, as I have argued, can
**There is rich irony here: If you give what you know about
a toy world to the computer, and have it solve new instances, it
would be derisively called “Good Old Fashioned AI,” but if you
capture all that the humanity knows about everything (as exported
to the Internet), train your LLM on it, and then ask it to provide
approximate task relevant knowledge, then it becomes “modern
AI.”
4

sometimes be mistaken for reasoning capabilities. LLMs
do excel in idea generation for any task–including those
involving reasoning, and as I pointed out, this can be ef-
fectively leveraged to support reasoning/planning in LLM-
Modulo frameworks6. In other words, LLMs already have
enough amazing approximate retrieval abilities that can be
gainfully leveraged, that we don’t need to ascribe question-
able reasoning/planning capabilities to them.††
References
[1] Lin Guan, Karthik Valmeekam, Sarath Sreedharan,
and Subbarao Kambhampati. Leveraging pre-trained
large language models to construct and utilize world
models for model-based task planning.
In Thirty-
seventh Conference on Neural Information Process-
ing Systems, 2023.
[2] Daniel Kahneman. Thinking, fast and slow. macmil-
lan, 2011.
[3] Subbarao Kambhampati. Language imitation games
and the arrival of broad and shallow AI. CACM Blog,
2021.
[4] Subbarao Kambhampati. Polanyi’s revenge and AI’s
new romance with tacit knowledge. Communications
of the ACM, 64(2):31–32, 2021.
[5] Subbarao Kambhampati. AI as (an ersatz) natural sci-
ence? Communications of the ACM, 65(9):8–9, 2022.
[6] Subbarao Kambhampati, Karthik Valmeekam, Lin
Guan, Kaya Stechly, Mudit Verma, Siddhant Bham-
bri, Lucas Saldyt, and Anil Murthy. LLMs can’t plan,
but can help planning in LLM-Modulo frameworks.
arXiv preprint 2402.01817, 2024.
[7] Subbarao.
Kambhampati,
Karthik.
Valmeekam,
Matthew. Marquez, and Lin. Guan.
On the role
††Although we focused on planning and reasoning tasks, the
discussion here has implications to LLM-based code generation.
There too, while LLMs can help as “co-pilots” to human pro-
grammers, there is never any guarantee that the code they gen-
erate is correct. The fact that code generation capabilities come
from github data, that is of higher quality than the general web
data, and the fact that incremental interpreters in the loop can pin-
point any syntax errors in the generated code, helps the utility of
the suggested code snippets for human programmers.
of large language models in planning, July 2023.
Tutorial presented at the International Conference
on Automated Planning and Scheduling (ICAPS),
Prague.
[8] Seth Kugel and Stephen Hiltner. A new frontier for
travel scammers: A.I.-Generated Guidebooks. New
York Times, August 2023.
[9] Kaya Stechly, Matthew Marquez, and Subbarao
Kambhampati. GPT-4 Doesn’t Know It’s Wrong: An
Analysis of Iterative Prompting for Reasoning Prob-
lems. In NeurIPS 2023 Foundation Models for Deci-
sion Making Workshop, 2023.
[10] Karthik Valmeekam, Matthew Marquez, and Sub-
barao Kambhampati. Can large language models re-
ally improve by self-critiquing their own plans?
In
NeurIPS 2023 Foundation Models for Decision Mak-
ing Workshop, 2023.
[11] Karthik Valmeekam, Matthew Marquez, Alberto
Olmo, Sarath Sreedharan, and Subbarao Kambham-
pati. Planbench: An extensible benchmark for evalu-
ating large language models on planning and reason-
ing about change. In Thirty-seventh Conference on
Neural Information Processing Systems Datasets and
Benchmarks Track, 2023.
[12] Karthik Valmeekam,
Matthew Marquez,
Sarath
Sreedharan, and Subbarao Kambhampati.
On the
planning abilities of large language models - a crit-
ical investigation.
In Thirty-seventh Conference on
Neural Information Processing Systems, 2023.
[13] Karthik Valmeekam, Alberto Olmo, Sarath Sreedha-
ran, and Subbarao Kambhampati.
Large language
models still can’t plan (a benchmark for llms on plan-
ning and reasoning about change).
arXiv preprint
arXiv:2206.10498, 2022.
5

