4 Preliminaries and prior work on framing
intelligence
The metaphor of a ladder in attempts to explicate “intelligence” is popular and
put to work in the next subchapters. We came across the Ladder of Being from
Christianity already in Chapter 1 and here, we will see three more: the Cognitive
Ladder, the Ladder of Prediction, and the Causal Ladder. The latter proves to be
eminently fruitful for our ambitions in Chapter 5. With that in mind, while, as
noticed, many researchers in the AI community do not think deeply about the
concept of intelligence, this Part II is ironically, thoroughly and unashamedly
influenced by one of its finest and sharpest thinkers, namely Judea Pearl.
4.1 The cognitive ladder
Let us inspect the first ladder and examine the distinction between intelligence
and close cognates such as knowledge, understanding and wisdom. To capture
them in a single framework, the notion of the cognitive ladder has been inaugu-
rated (Hoffmann, 2019b; Vallor, 2017) and it is plotted in Figure 17 where the
cognates are envisioned as successive rungs.
For each of those notions, one could fill libraries with books about their re-
spective nature, and I have no pretensions to a comprehensive philosophical
account. My modest objectives in this subchapter are to “fence off” those diver-
gent concepts grouped around the term “intelligence” to then gain some broad
guidelines for our own proposal in Chapter 5 by elucidating their respective po-
sition on the ladder.
Climbing the cognitive ladder starts at the lowest rung with knowledge,
which is traditionally conceived of as true, justified belief. In this respect, I know
that there is the Mont Salève in the agglomeration of Geneva because the state-
ment is true and I have good reasons (for example, because I see the mountain
from the window) to believe it is true. Even if not every case of knowing that p
implies believing that p (as the tripartite conception of knowledge would vouch-
safe), the pair knowledge/belief comes as a double-pack (Glock, 2018: 92). The
hurdle for knowledge is not particularly high for us humans – even fools know
something –, yet for other creatures it might be a different story: To what extent
can animals and even more machines be said to have beliefs or reasons at all? In
retrospect to Chapter 2, we are positive about the faculty of some animals, espe-
cially corvids (cf. also Glock, 2018). In hindsight to Chapter 3, we recapitulate
that first and second wave AI, strictly speaking, do not have beliefs (cf. also
https://doi.org/10.1515/9783110756166-007

Dretske, 1988: 48), but that, by adopting an intentional stance (Dennett, 1989),
AI can be considered to still “know” that, for instance, a cat is in a picture be-
cause it has been trained with millions of cat pictures.
In tune with Anderson (1993) and Ryle (1949), a discrimination can be
made between declarative or content knowledge (“knowing that”) and proce-
dural or process knowledge “knowing how”. The former is factual knowledge
about the meaning or perceptual characteristics of things, from anecdotal mem-
ories to highly organized conceptual knowledge of some subject matter like a
theorem of geometry (Lohman, 2000: 289). The value of content knowledge has
been shrinking in the machine age because questions, problems, tasks targeting
knowledge in this sense can be and have been outsourced to computers: I do not
need to know by heart where the adage “Rome wasn’t built in a day” comes from
historically, I can plainly google it. This shift might explain why we are inclined
to ascribe knowledge to computers and AI or, conversely, regard our minds as
extended (Vold, 2018; Clark & Chalmers, 1998). “Knowing how”, by contrast, is
Figure 17: The four-rung cognitive ladder. In accordance with: Vallor, 2017: 163;
Hoffmann, 2019b.
100
4 Preliminaries and prior work on framing intelligence

knowledge of how to do something, from pronouncing a word to riding skis. Like
this, it is closely intertwined with intelligence, the second rung: In fact, there may
not be a big difference between knowing how to ski well and bodily-kinesthetic
intelligence in this respect of skiing.
According to Vallor (2017) and Hoffmann (2019b), intelligence requires the
distinctive, sometimes narrowly compartmentalized competence of applying
knowledge in some relevant context(s). This renders intelligence practical (“prac-
tical intelligence”) and is noteworthy because “[m]ost conceptions of intelligence
have focused on traits or processes to the neglect of knowledge” (Wagner, 2000:
392). I display (logical-mathematical) intelligence when I apply knowledge from
one of the seminars of philosophy I attended by designating a certain kind of logi-
cal argument as syllogism; or when I use my experience from interacting with peo-
ple to comfort the anxious neighbor (emotional intelligence). Here on the second
rung, too, AIs already often captivate; for example, when an AI “uses its knowl-
edge/training” to identify a cat in a new image. In computer science, McCarthy
was among the first to appreciate that to track intelligent behavior we needed to
study the knowledge that behavior depends on, and how this knowledge might
be applied in adjudicating how to behave (Levesque, 2018: 30), which is unequiv-
ocally on the same trajectory for making sense of intelligence. Or in the words of
Zenon Pylyshyn (1999), intelligent behavior is cognitively penetrable: the deci-
sions you make about what actions to pick is penetrated by what you believe or
know. Philosophically, Vallor’s (2017) and Hoffmann’s (2019b) reading of intelli-
gence suggests that intelligence is a second order mental property, a property
that consists in having first order mental states – beliefs, desires, etc. This read-
ing can be criticized or not (Block, 1981), but the question whether to endorse or
to discard it is ultimately immaterial for the endeavor at hand.
What AI currently lacks is an integrated mastery of at least one domain of
practical or theoretical cognition, which would set the central requirement for
understanding as a general, not specific, third-level competence. Accordingly,
our AI trained with cat pictures may not be poised to recognize the lion with his
mane in a picture as a cat, i.e., to apply other domains of knowledge (from biol-
ogy, for example) to the problem. We can accept this reading and exemplification
of understanding albeit, in this context, we made “comprehension” part of the
sphere of intelligence by acknowledging intelligent behavior with and without
comprehension. What is to be applauded though is that a strong common denom-
inator is given by not treating comprehension as the source or antecedent of com-
petence. Instead, both this book and Hoffmann (2019b)/Vallor (2017) contend that
competence comes first (on a lower rung to dwell upon the picture), that compre-
hension is both composed of competences and representing as well as promoting a
general and higher competence. In addition, Dennett (2017: 95) accentuates that
4.1 The cognitive ladder
101

the distinction between comprehension and incomprehension is not only momen-
tous, but it is also non-Boolean. The notion might come, following the well-tested
Darwinian perspective of gradualism, in different stages: “At one extreme, we
have the bacterium’s sorta comprehension of the quorum-sensing signals it re-
sponds to [. . .] and the computer’s sorta comprehension of the “ADD” instruction.
At the other extreme we have Jane Austen’s comprehension of the interplay of
personal and social forces in the emotional states of people and Einstein’s compre-
hension of relativity. But even at the highest levels of competence, comprehension
is never absolute. There are always ungrasped implications and unrecognized pre-
suppositions in any mind’s mastery of a concept or topic.”
If knowledge, intelligence and understanding are envisioned as progres-
sively advanced steps, we encounter wisdom on the fourth and final rung of
the fictional cognitive ladder. In contrast to understanding, it is characterized,
following Hoffmann (2019b), by holistic value judgments, which dovetails an
evaluation and questioning not only of the means to achieve a goal, but also of
the goals themselves. While, speaking for the running example of the AI and
the cat pictures, the means at the level of narrow intelligence are the ascer-
tained patterns from the training data, which are then enriched at the level of
understanding by insights from other fields of knowledge, the supreme disci-
pline of wisdom would encapsulate the evaluation of the extent to which it
would be wise or moral to evaluate the images at all. To put it bluntly, there is
no clear developmental trajectory on which AI could attain anything like wis-
dom. For the foreseeable future, intelligent machines simply lack the distinctive
social and psychological conditions that make wisdom possible in the first
place. In fact, this supposed lack is at the same time the core of their strength
and commercial appeal. Wisdom in AI is at least for the predictable future not
only technologically impossible, but neither imperative nor desirable.
To draw the demarcation line, Vallor (2017: 164) adds that wisdom is unlike
the first three rungs which “are internal cognitive states of a person; they align
epistemically with some domain of the world, but do not necessarily alter or
affect that domain. Intelligence, for example, presupposes an ability to success-
fully deploy knowledge in action, but once acquired, this ability could in principle
remain forever latent in any given agent. Though often helpful, it is not always
necessary to exercise one’s intelligence to retain it. Contrast this with [. . . active]
expertise, [an essential ingredient of wisdom] that tend[s] to refer to an agent’s
actual practices of employing intelligence [and understanding] in skillful and
world-altering ways.” Taking this negative and positive characterization of wis-
dom as a plea, it bears some resemblance with Karl Marx’s quote carved in stone
in the main building of the Humboldt University: “The philosophers [meaning
102
4 Preliminaries and prior work on framing intelligence

“lovers of wisdom”, C.H.] have only interpreted the world, in various ways. The
point, however, is to change it.”
For the sake of the present treatment, we note that wisdom is a special case
of a special kind of intelligence, namely practical intelligence (Wagner, 2000) as
rudimentarily portrayed above on rung two. One possibly can discuss intelligence
without dealing with wisdom – a path we indeed tread in the remainder –, but
one cannot adequately discuss wisdom without dealing with intelligence, which
is why the order of rungs is the way it is in Figure 17. The most conventional view
is that wisdom (just as creativity, cf. Sternberg & O’Hara, 2000) and intelligence
are overlapping sets: Intelligence and wisdom (creativity) overlap in some re-
spects, but not in others. Some people like Bill Gates or Ruth Bader Ginsburg (ar-
guably) radiate both intelligence and wisdom, others are just intelligent like
Wolfgang Amadeus Mozart or intelligent in a (very) myopic sense like Wladimir
Iljitsch Lenin without being wise, some are only wise (often the elderly in a fam-
ily), and many people are neither nor; a colorful palette. Content-wise, neither
Hoffmann’s (2019b) nor Vallor’s (2017) interpretation of wisdom is compelling for
us. The former in terms of questioning goals of others swiftly unfolds itself as infa-
mous: Who are we to disparage Mozart as unwise by judging the genius who
passed away “too early” and his eccentric or hedonistic lifestyle? The latter, albeit
making a good point about practical wisdom, is redundant since in the course of
this work we do not always insulate intelligence as a corpus of abilities (which is
in the foreground) from intelligent behavior (which is more in the background;
see key takeaways from Part I). Intelligence which remains forever latent is
worthless!
Reply to the overall framework: At first glance, Vallor’s (2017) and Hoff-
mann’s (2019b) thrust seems plausible as a loose metaphor at least; it might also
be stimulating further thought and discussion. As an explanatory scheme for illu-
minating “intelligence” where the latter, according to the authors, simply repre-
sents a self-contained section within a canon of (only) four cognitive abilities
(see Figure 17), however, it does not do justice to such a itself multi-layered and
rich concept. In a critical appraisal, a repertory of just four cognitive traits (on
what grounds by the way?) displeases. For example, how would we make sense of
another aspect of mental life such as learning in this scheme, which is also second
order in the sense that there can be no learning without memory?
Grasping the gist of the matter, the approach by Vallor (2017) and Hoffmann
(2019b) diminishes, first, intelligence to practical intelligence, and, second, factors
out the issue of how much and what kind of knowledge is needed to voice intelli-
gence. To the extent, for instance, that scientific as well as naïve theories are an
articulation of highly developed intelligence that often remain agnostic about pro-
cesses of transmission, Schulz et al. (2007) note that Newton propounded his
4.1 The cognitive ladder
103

theory of gravitation without knowing any mechanism that might enable masses
to attract one another or Darwin originated his theory of evolution without know-
ing any mechanism that might make variation in the species heritable, etc. Hence,
this cognitive ladder ultimately proves to be deficient for enlightening us about
the nature and role of intelligence.
4.2 The ladder of prediction
How can a thoroughly physical being think, dream, and feel? How can such a
being act in ways that mirror what it knows and that serve its ever-changing
needs? At the busy intersection of psychology, neuroscience, philosophy and
AI (where we with this book gather too), philosopher and cognitive scientist
Andy Clark (2016) feeds answers by scouting brains like ours as prediction ma-
chines. Devices thus construed do not wait passively for sensory stimulations
to arrive, but instantiate morphing, buzzing, dynamical systems forever reconfi-
guring themselves so as better to anticipate the incoming sensory barrage
(ibid.: 167). These are the brains of active agents, gifted to structure their own
worlds, constructing and re-constructing them in ways that alter the very things
their brains must engage and predict. What crops up is Clark’s (2016: 295) unified
overarching vision of the brain as an engine of multilevel probabilistic prediction
in terms of hierarchical predictive coding or Predictive Processing which dove-
tails with work on the embodied and environmentally situated mind. His pro-
posal serves as a cogent illustration of how to tackle a wide range of issues,
shedding light on perception, action, reason, experience, emotion, understand-
ing other agents, and the nature and origins of various pathologies and break-
downs (ibid.: 10).
According to Clark (2016: 204), we need, if you will, to begin to recognize
ourselves in the swirl of ongoing, multilevel prediction, which can be depicted
by another ladder, the ladder of prediction in Figure 18, that in opposition to
the one in 4.1. is equipped with only three rungs, but sketches a dynamic situa-
tion. Even though he does not suggest the image of a ladder himself along
which to organize his whole treatment, it can very well be deployed: His book
is composed of three distinct parts, and Part I corresponding to the first rung
deals with the entire bedrock story about perception and learning; Part II, i.e.,
the second rung, is about the neat ploy of bringing the sensory signal progres-
sively in tune with some special subset of the agent’s own sensory predictions,
morphing some of our sensory predictions into self-fulfilling prophecies; and
finally Part III representing the highest rung completes the picture with im-
mersing in agents’ capacity to alter the long-term structure of their own social
104
4 Preliminaries and prior work on framing intelligence

and material environment, “so as to inhabit a world in which the ‘energetic in-
puts that matter’ are more reliably served up as and when required” (ibid.: 7).
Taking a fictional tabula rasa agent, the core idea is to endow her with passive
perception on the first rung. To perceive the world is generally to meet the sensory
signal with an apt stream of multilevel predictions (Clark, 2016: 6): “Those predic-
tions aim to construct the incoming sensory signal ‘from the top down’ using
stored knowledge about interacting distal causes. To accommodate the incoming
sensory signal in this way is already to understand quite a lot about the world.
Creatures deploying this kind of strategy learn to become knowledgeable consum-
ers of their own sensory stimulations. They come to know about their world, and
about the kinds of entity and event that populate it. Creatures deploying this strat-
egy, when they see the grass twitch in just that certain way, are already expecting
to see the tasty prey emerge, and already expecting to feel the sensations of their
own muscles tensing to pounce. An animal, or machine, that has that kind of grip
on its world is already deep into the business of understanding that world.”
Yet, something crucial is missing from this neat picture of passive percep-
tion, something crucial for a system to qualify as an agent in the first place.
Figure 18: The three-rung ladder of prediction. In accordance with Clark (2016) who, however,
does not envision a ladder.
4.2 The ladder of prediction
105

What is prerequisite is action, and action or, more precisely, performing action
that makes our predictions come true (ibid.: 121), changes everything. Our massed
recurrent neuronal ensembles are not just buzzing away constantly attempting to
predict the sensory stream. Rather, our brains are constantly bringing about the
sensory stream by causing bodily movements that selectively harvest new sen-
sory stimulations (ibid.: 7). Perception (first rung) and action (second rung) work
seamlessly in concert in the normal course of events, they are locked in a type of
endless circular embrace. The upshot is that “the perceptual and motor systems
should not be regarded as separate but instead as a single active inference ma-
chine that tries to predict its sensory input in all domains: visual, auditory, so-
matosensory, interoceptive and, in the case of the motor system, proprioceptive”
(Adams et al., 2013: 614). Or in Clark’s (2016: 176) words, perception and action
are both co-determined and co-determining. In these broader terms, what we do
hinges on what we perceive, and what we perceive is throughout conditioned by
what we do. Creating and maintaining such perception-action cycles that reflect
organismic needs and environmental opportunities results in the rather specific
forms of circular causality (described in his Chapter 2.6 and 4): “[H]igh-level pre-
dictions entertain actions that both test and confirm the predictions, and that
help sculpt the sensory flows that recruit new high-level predictions (and so on,
in a rolling cycle of expectation, sensory stimulation, and action)” (ibid.: 176).
We are not yet on the top. On the highest rung, we must specify a kind of
lifestyle and habitat for our agent; otherwise, “we have no sense of what
might constitute apt action in response to the sensory inputs” (ibid.: 7). Such
world-structuring, repeated time and time again, generation by generation,
also facilitates beings like us to build better and better worlds to think in, en-
abling impinging energies to steer ever-more-complex forms of behavior as well
as allowing thought and reason to penetrate domains which were previously
“off-limits” (ibid.). The grip, in the somewhat special case of the human mind –
recall that most, if not all, of our faculties might not be unprecedented in the
animal kingdom as seen in Part I, but we are still special (and the backside of
the coin says that other animals are special in other regards) – is further en-
riched and transformed by layer upon layer of socio-cultural structures and
practices (ibid.: 294). Steeped in such practices, our predictive neuronal ensem-
bles are empowered to redeploy their basic skills in new and transformative
ways (ibid.). Comprehending the consequent interplay of action, culture, tech-
nology, and cascading neural prediction is surely one of the principal quests
confronting 21st century cognitive science (ibid.).
Critical appraisal: In his superb book, Clark (2016) expertly wields the hier-
archical predictive coding or Predictive Processing account and applies it in an
engaging fashion to a high constellation of cases to appreciate how brain, body
106
4 Preliminaries and prior work on framing intelligence

and world interact with each other to create our lived experience. Tailored to the
subject line of the present treatment, we also endorse his three-tier conception of
the ladder of prediction for illuminating intelligence. This stems from chiefly
three reasons: Firstly, Clark (2016) integrates a number of pertinent perspectives
covering neuroscience, psychology, philosophy and AI, and, thereby, erects a
scaffold which is not static in contrast to the ladder of cognition. It thus lends
itself to supposedly better capturing factually dynamic intelligence (e.g., in the
sense that cognitive abilities evolve over time). Secondly, his approach does not
end with incorporating simple, linear dynamics, but he harbors the kind of dy-
namics which can lead to cycles and circular causality, paving the ground for
strange loops which Hofstadter (1979: 35) ultimately views as the signature of in-
telligence. We revisit this point in Chapter 5.1.3.
Thirdly, Clark’s first rung corresponds well with the basic form of intelli-
gence we painted in 3.2. following the simple economics of AI according to
which some, i.e., rung one intelligence befits agents that passively observe to
make predictions. Rung two clarifies what tags higher intelligence, namely the
ability of acting upon the world for causal learning or, as we will restate it fol-
lowing Pearl (2018a) in 4.3., intervention. Only Clark’s (2016) rung three does
not fit into the conceptual fabric. Neither is the demarcation between his rung
two and three compelling within his framework – e.g., rung one and two stand
for two distinct abilities of an agent, rung three does not –; nor will it be revital-
ized for our own foray in Chapter 5. On the contrary, we are more in consonance
with Pearl (2018a) who in turn is d’accord with Clark’s proposal on rung one
and two, but deviates from his track for rung three, which we scrutinize in the
last subchapter.
4.3 The ladder of causation
The researcher had never personally destroyed one of his animals before, always leaving
the task to assistants. As the unsuspecting chimpanzee Martha [who is connected to some
electronic equipment so as to monitor the neural activity of her speech center and translate
it into English words] placed the poisoned gift into her mouth and bit, Belinsky [the re-
searcher] conceived of an experiment he had never before considered. He turned on the
switch. “Candy Candy Thank you Belinsky Happy Happy Martha.” Then her voice stopped
of its own accord. [. . .] But brain death is not immediate. The final sensory discharge of
some circuit within her inert body triggered a brief burst of neural pulsations decoded as
“Hurt Martha Hurt Martha.” [. . . And finally, one last pulsating signal was sent to the
world of men:] “Why Why Why Why –“ [emphasis added by C.H.] A soft electrical click
stopped the testimony.
Miedaner, 1982b: 105f.
4.3 The ladder of causation
107

The pathetic forlorn cry of the dying chimp evokes in us powerful sympathy –
we can identify so easily with this innocent and enchanting creature; partly
through her charming simple-minded syntax, which might make us feel protec-
tive of her as we would of a baby or small child, but also, and this is the chief
point of this subchapter, because she prominently raises the question of why,
once reckoned as marking us as uniquely human.
The exhilarating, cogent and enthralling book by Judea Pearl (2018a) The
Book of Why: The New Science of Cause and Effect takes exactly that equivocal
term, why, as a starting and focal point. Its main ambiguity is disclosed by a
familiar pair of substitute phrases: what for? and how come? (Dennett, 2017:
38). As we see in this subchapter, Pearl (2018a) conceives of the “Why?” ques-
tion as a counterfactual question in disguise. The subtitle of his book putting
the limelight on “cause” or “causality” is not exempt from equivocality either,
as Aristotle taught us already. He identified four questions we might wish to
ask about anything:
1)
What is it made of, or its material cause?
2)
What is its structure, or its formal cause?
3)
How did it get started, or what is its efficient cause?
4)
What is its purpose, or its final, or telic, cause (also often translated with
reason; cf. Dennett, 2017: 40; we can say that reasons are causes, Davidson,
1963/1980)?
On top of that, we can discern between internal and external causes (Dretske,
1988: 1): The distinction between Christian’s losing his boring job as assistant
professor of finance (something that happens to him) and his quitting his job
(something he opted for) resides in the locus – in Christian or in the university –
of the cause of termination.
And the list with complications emanating from attempts of clarifying and
systematizing the Babylonian confusion of tongues around “cause” or “causality”
goes on, exemplified by further questions such as: Are causes events (Lewis,
1986)? Facts (Mellor, 1995)? Composita (Menzies, 1989)? With or without back-
ground conditions (Kim, 1976)? Is causality, the relationship of cause and effect, to
be understood in probabilistic terms (Reichenbach, 1956; Suppes, 1970; Hoffmann,
2021c)? And so on (for a general overview, cf. Schaffer, 2016).
To put it bluntly, I have no philosophical interest in playing umpire in
these disputes, no interest in agonizing over settling specific questions about
what is and what is not causality, what are and what are not causes. My interest
centers on how intelligence can be recurred to causality, precisely to different
forms of causal learning, no matter the exact localizations and tenets on the
108
4 Preliminaries and prior work on framing intelligence

higher level of metaphysics of causation. An indispensable stepping stone to-
wards this goal was laid by Pearl (2018a).
In analogy to Clark (2016) from 4.2., there are two ways we can get (first-
hand) evidence about an event: We can perceive the event happen, or we can
make the event happen. These two ways of receiving data – seeing and doing –
can lead to radically divergent conclusions in terms of learning, even when the
evidence itself is otherwise identical (Schulz et al., 2007: 77). What you can
learn depends not only on what you know already (contra the stance in the pre-
vious subchapter 4.1.), but also on how you know it (ibid.). In this spirit, Pearl
(2000: 421) stated that “[s]cientific activity, as we know it, consists of two basic
components: Observations and interventions. The combination of the two is
what we call a laboratory.” In (Pearl, 2018a), he abstracts from science, adds
a third layer, et voilà, we gain the intriguing product of his so-called ladder of
causation, which, contrasted with the two previous ladders, proves to be a more
sophisticated proposal for, how he calls it, what makes us uniquely human: we
recognize human reasoning “through words such as ‘preventing,’ ‘cause,’ ‘attrib-
uted to,’ ‘discrimination,’ and ‘should I’” (Pearl, 2018b: 3). Accordingly, a causal
learner must master three distinct levels of cognitive ability: seeing, doing, imag-
ining; and, as his argument runs, only us humans bestride all three sectors of the
causal ladder (which is depicted in Figure 19 on the next page).
Rung one of the ladder
The first rung, seeing or observing or, as we will also denote it, identifying, entails
the detection of regularities, patterns and associations in our environment and is
shared, according to Pearl (2018a: 27), by most animals, modern and prehistoric
humans, but also by our present-day learning machines. All these creatures learn
from associations. This rung calls for predictions predicated on passive observa-
tions or, better, regularities in observations. This is what a dog does when observ-
ing how a cat moves and figuring out where it is likely to be a moment after, and
it is what the computer program AlphaGo by DeepMind Technologies (Alphabet/
Google) does when it studies a database of millions of Go games so that it can de-
rive which moves are geared with a higher percentage of wins.
We say that one event is associated with another if observing one changes
the likelihood of observing the other. In statistics, a thriving, but after all causal-
ity-free enterprise, this type of relationship is called correlation, thereby reducing
a large body of data. But, data per se is profoundly dumb. Naked data can tell
you that this author is more into philosophy (books) than economics (books), but
they cannot tell you why. In many situations more like this, in everyday life,
4.3 The ladder of causation
109

Figure 19: The three-rung causal ladder. Pearl (2018a) is the only exception where a ladder is
actually painted compared to the two other pieces of prior work where this did not happen.
Source: Pearl, 2018a: 28.
110
4 Preliminaries and prior work on framing intelligence

science or business, we witness that mere data is not enough. No system, human,
animal or machine, can determine what is going on in the world merely by “look-
ing out” and seeing or sampling it (Cantwell Smith, 2019: 14), drastically limiting
the applicability of Brooks’ famous maxim that “the world is its own best model”
(Brooks, 1991): The maxim roughly says that in lieu of being plagued by the prob-
lem of updating, searching, and otherwise manipulating the symbolic worlds in-
side our AIs, in nuce, the problem of modeling the outside world, nouvelle AI can
get along without the modeling part by processing the external world information
it needs from the senses when it is required. [Pearl ignores Granger causality, a
statistical hypothesis test procedure (Granger, 1969), and cognates, but I would
contend that it is still licit to label statistics a causality-free enterprise; a) the rela-
tively little significance and impact of Granger causality (few people use it), b) in-
voked criticisms, e.g., cf. Grassmann, 2020 or He & Maekawa, 2001 besides others
(does Granger not flatly deal with correlation?).]
This first rung of the ladder is characterized by the question “What if I per-
ceive/see . . . ?”. For example, linking back to Chapter 2, we find on this rung
one a special form of learning by conditioning or reinforcement which is called
passive discrimination or classical conditioning. In these procedures, an animal
(like Pavlovian dogs) learns to identify a condition C, or at least to distinguish
(discriminate) C from other conditions, by having particular responses to C –
and, indeed, the widespread preference for simplicity has unjustifiably biased
animal cognition researchers in favor of such associative models instead of more
quantitative cognitive models (Mikhalevich, 2018).
What makes this case then passive or classical is that the test subject, let us
say a dog, learns an association between two events that are both outside his
control – e.g., an association between the ringing of a bell and the provision of
food and the dog responds to the former by salivation because he expects the
latter. Or couched in the terms of the key question of rung one: If I perceive the
signal of a ringing bell, food is served. The dog is thus in the position of learn-
ing through passive observation rather than active action, and what is learned
is that one stimulus predicts another, where this predictive relationship may or
may not reflect the fact that the first stimulus causes the second (Woodward,
2007: 26).
Linking back to Chapter 3, we also find passive reinforcement or deep
learning in the machine kingdom. We came across a machine learning program
processing digitized images and which associates certain features in the images
(like a certain uniform color and brightness or an area where there is a distinc-
tive change in brightness and color corresponding to an edge) with the feature
of being a cat. Deep learning thus far cannot inherently distinguish causation
from correlation: “Roughly speaking, deep learning learns complex correlations
4.3 The ladder of causation
111

between input and output features, but with no inherent representation of cau-
sality” (Marcus, 2018: 12f.).
Such organisms that are incapable of acting on the world, but can only pas-
sively scout associations outside their control would have no need for a notion
of causation or cause-like representations, conceived along interventionist
lines that we encounter on rung two (Menzies & Price, 1993: 195). A paucity of
flexibility and adaptability is inevitable in any system that works at the first
level of the ladder of causation (cf. Camp & Shupe, 2018, who make a case for
instrumental reasoning on rung two as an important mark of cognitive flexibil-
ity). Therefore, let us now explore the epistemically significant disparities be-
tween observing and intervening.
Rung two of the ladder
We step up the next level of causal queries when we begin to change the world
by taking actions. In other words, a new kind of knowledge, absent from data,
which we find at rung two of the ladder, is needed and consists of doing or in-
tervention (which we wish to perform mentally before we decide whether and
how to do it in real life). Intervention ranks higher than association because it
encapsulates not just seeing but amending what is and entails planning as well
as predicting the effect(s) of deliberate alterations to produce a desired out-
come. Pearl spots tool users, such as early humans (e.g., recall Figure 3), on
the second rung if they act by planning and not merely by imitation. An effect
of such tool use is a sharpened “understanding of causality of events and of the
self as one of the drivers of the causality” (Hiraiwa-Hasegawa, 2019: 171).
Or as Pearl (2018a: 31) puts it neatly: “Seeing smoke tells us a totally different
story about the likelihood of fire than making smoke”. We cannot answer ques-
tions about interventions, generally “What would Y be if I do X?” or “How can
we . . . ?”, with passively collected data, no matter how big the data set is. A very
direct way to learn about the results of an intervention is to use experiments – be
it under prudently controlled conditions in the lab or simply by trial and error,
which presumably is how babies acquire much of their causal knowledge – to
predict the effects of interventions.
We perform interventions all the time in our daily life. For instance, when
we are in a bad mood and watch a comedy to lighten it up, we are intervening on
one variable (distraction or the quantity of smiles, jokes or laughter around us) in
order to affect another one (our status of wellbeing). If we are correct in our causal
belief about the comedy, the “outcome” variable will respond by turning from
“bad mood” to “good mood”. The faculty to move smoothly from claims about
112
4 Preliminaries and prior work on framing intelligence

causal structure that follow from information about the results of intervention to
claims about causal structure that are countenanced by observations and vice
versa is, as Woodward (2007: 23f.) argues, one of the distinctive features of human
causal cognition.
In contrast to observations, however, interventions do not lend positive or
negative diagnostic evidence to the causes of the event on which we intervened.
Whereas the perception of events allows us to reason diagnostically about their
causes, interventions make the occurrence of events independent of their typical
causes (Hagmayer et al., 2007: 87): “For example, forcing somebody to eat 50
(and only 50) grams of fat per day fixes that intake independent of the presence
or absence of other factors normally affecting diet” (ibid.).
An expedient illustration of the difference between rung 1 and rung 2 is the
difference between Pavlovian conditioning (seen above) and instrumental/operant
conditioning (cf. e.g., Skinnerian rat experiments). In the latter, what is learned is
an association between some behavior produced by the subject and an outcome,
as when dogs learn an association between pressing a lever in response to hearing
the bell, which in contrast to passive salivation is an active action, and the provi-
sion of food pellets. From an interventionist perspective, “instrumental learning
has a ‘causelike’ flavor” (Woodward, 2007: 26). But neither the bell does (suffi-
ciently) cause the dog’s active reaction (the bell could ring, but the dog does not
have any desire for the reward because he is not hungry), nor does his intervention
(sufficiently) cause the receipt of a food reward (the experimenter could forget
about it).
Woodward (2003: 28; 2009: 234) suggests that causal language can capture
facts about manipulability that are beyond the reach of any non-causal or mere
statistical claim, in the sense that facts about manipulability and intervention are
not reducible to statistical facts. Various other philosophers before him argued
that the core notion of causation involves human intervention (Collingwood,
1940; von Wright, 1971; Hart & Honoré, 1983). It is through our actions and ma-
nipulations of the environment around us that we acquire our basic sense of
causality. Even though more contemporary theories of causality dispense with
its anthropomorphic connotations, they maintain the notion of intervention as a
central concept (Spirtes et al., 1993; Pearl, 2000; Glymour, 2001; Woodward,
2003). What is key for the purposes of causal learning is that an intervention can
act as a quasi-experiment, one that eliminates (or reduces) confounds and assists
in establishing the existence of a causal relation between the intervened-on vari-
able and its effects (Lagnado et al., 2007: 161).
While reasoning about interventions is a vital step on the causal ladder, it
is not sufficient to answer all causal questions of interest. We might wonder,
my bad mood is gone, but why? Was it the comedy I watched? The birds that I
4.3 The ladder of causation
113

have heard singing during a warm and sunny spring day? The phone call I re-
ceived from a good friend of mine? These queries take us to the top rung of the
ladder of causation, the level of counterfactuals, because to answer them we
must go back in time, change history, and ask: “What would have happened if
I had not watched the comedy?” Or more generally speaking: “Was it X that
caused Y? What if X had not occurred? What if I had acted differently?” No ex-
periment in the world can deny the effect of a measure that has already been
taken and compare the two outcomes. Therefore, we have to import a whole
new kind of knowledge.
Rung three of the ladder
Good predictions crop up in tandem with good explanations (Toulmin, 1963).
We often desire something more than mere prediction. We need to have infor-
mation about the underlying mechanisms in order to make accurate and robust
predictions about what will happen when the system we are in breaks down or
modifies itself in various ways; instrumentalist theories provide no such informa-
tion to get there. To embark on this arduous journey, counterfactual learners, on
the final rung, can imagine worlds that do not exist and infer reasons for ob-
served phenomena. “As-if,” exploratory modes and imagination should not be
frowned upon by reason, but rather treated as an integral substrate of it (Camp &
Shupe, 2018: 106).
Counterfactual reasoning is retrospective reasoning; an ability that accord-
ing to Pearl most distinguishes humans from animal intelligence, as well as
from model-blind versions of AI and machine learning. It tells us what would
have happened if events other than the ones we are currently observing had
happened.
(A note of warning for the philosophically well-versed reader: In the following
setting to briefly illustrate counterfactual reasoning for causal queries, we use
rather loose speech about causes and effects of behavior; for a more accurate
account cf. Dretske, 1988: 37f., who addresses an analogous course of events:)
If we are currently observing that both event A (e.g. my mug shatters on the
floor) and B (I knock over my cup of tea in reaching for a book) are present,
then we can ask ourselves if B would still be present if we had intervened on A
and provoked its absence. If we know that B is the actual cause of A, then we
should infer that the absence of A (there is no broken mug on the floor) makes no
difference to the presence of B because effects do not perforce affect their causes
(e.g. the mug can land on my legs because I sit at my desk writing a book on intel-
ligence). By contrast, if I had intervened and stopped the mug from falling in the
114
4 Preliminaries and prior work on framing intelligence

first place (i.e., ¬B), then we should infer that (ceteris paribus) A also would not
occur.
The example evidences that counterfactual reasoning combines observa-
tional and interventional reasoning: “[I]nstantiating a counterfactual event is
causally equivalent to an imaginary intervention on a causal model in which all
variables that are not affected by the intervention are assumed to [abide] at cur-
rently observed levels” (Hagmayer et al., 2007: 87). In other words, counterfac-
tuals are placed at the top of the hierarchy since they subsume interventional
and associational questions (Pearl, 2018b: 2). If we have a model that can an-
swer counterfactual queries, we can also answer questions about interventions
and observations (ibid.). For instance, the interventional question, “What will
happen if I knock over my mug?” can be answered by asking the counterfactual
question: “What would happen had the mug been knocked over?” Likewise, as-
sociational queries can be covered once we can address interventional queries;
we plainly ignore the action part and let observations take over (ibid.). (And, as
seen, the translation does not work in the opposite direction.)
Counterfactuals possess an eminently problematic relationship with data
since data are, by definition, facts (Pearl, 2018a: 33). They cannot tell us what
would happen in a counterfactual or imaginary world where some known facts
are bluntly negated. Or how Hofstadter (1979: 638) phrases it in his vivid, elo-
quent, and congenial way:
That is what Contrafactus is all about. In everyday thought, we are constantly manufactur-
ing mental variants on situations we face, ideas we have, or events that happen, and we let
some features stay exactly the same while others “slip”. What features do we let slip? What
ones do we not even consider letting slip? What events are perceived on some deep intui-
tive level as being close relatives of ones which really happened? What do we think “al-
most” happened or “could have” happened, even though it unambiguously did not? What
alternative versions of events pop without any conscious thought into our minds when we
hear a story? Why do some counterfactuals strike us as “less counterfactual” than other
counterfactuals? After all, it is obvious that anything that didn’t happen didn’t happen.
There aren’t degrees of “didn’t-happen-ness”. And the same goes for “almost” situations.
There are times when one plaintively says, “It almost happened”, and other times when one
says the same thing, full of relief. But the “almost” lies in the mind, not in the external facts.
As seen in Chapter 1, anthropologists like Harari (2014) have general sympathies
that the decisive ingredient that empowered our Homo sapiens ancestors to achieve
global dominion, about 40,000 years ago, was their ability to choreograph a mental
representation, a blue-print of their environment, interrogate that representation,
distort it by mental acts of imagination and finally answer “What if?” kind of
questions (Pearl, 2018b: 1). And Steiner (1975: 227) sings a counterfactual hymn to
4.3 The ladder of causation
115

counter-factuality: “It is unlikely that man, as we know him, would have survived
without the fictive, counter-factual, anti-determinist means of language, without
the semantic capacity, generated and stored in the superfluous, zones of the cor-
tex, to conceive of, to articulate possibilities beyond the treadmill of organic decay
and death.”
Discussion: Pearl’s (2018a) approach bears fruits when we are talking about
causality since it bypasses long and unproductive disputes of what exactly cau-
sality is and focuses instead on the concrete and answerable question “What can
a causal reasoner do?” (Pearl, 2018a: 27). Or more precisely, “what can an organ-
ism processing a causal model compute that one lacking such a model cannot?”
(ibid.). Pearl has been absorbed in game-changing work about causality (for both
philosophy and computer science) since long (e.g., cf. his magnum opus in
2000) – game-changing because he converted formerly unsolvable metaphysical
questions into decidable mathematical ones. (He even spoke of a causal revolu-
tion recently, cf. Pearl, 2020.) With the present treatment, we wish to demon-
strate that, given a few amendments and modifications, his work is exceptionally
fertile and path-breaking for understanding intelligence too. Therefore, it is need-
less to say that we attribute grand merits to his ladder of causation.
Yet, on the flipside, the weak points of his proposal, as sketched in this
subchapter, have to be fixed prior to harvesting the fruits. The enumeration of
weaknesses in the following thus delineates our agenda in Chapter 5 to come.
–
Pearl’s (2018a) contention about animals’ and machines’ rank or position in
his causal ladder are simplistic and empirically wrong; possibly/fatally, af-
fecting all three levels. We address this point in 5.1.
–
A further strength of Pearl’s (2018a) conception is that each layer in the hier-
archy has a syntactic signature that characterizes the sentences admitted
into that layer (Pearl, 2018b building on Pearl, 2000). (E.g., on the interven-
tional layer, we find conditional probability sentences like P (y | do(x), z)
stating that: The probability of event Y = y given that we intervene and set
the value of X to x and subsequently observe event Z = z is equal to p. More
on this in Chapter 6.) For the sake of capturing intelligence though, the
three rungs are not fine-grained enough. We address this point in 5.1.
–
For the same purpose, capturing intelligence, the causal ladder remains
under-complex and neglects material aspects and forms of intelligence. For
instance, a corollary of the current version would be that some present-day
AI ranks as high as Einstein on rung three, which is inacceptable (not least
for Pearl himself who sees us humans accommodated at the lonely pinnacle
of causal reasoning): “A neurosymbolic or purely symbolic [machine learn-
ing] system should be capable of satisfying the requirements of all three of
Pearl’s levels, e.g. by mapping the neural networks onto symbolic descriptions”
116
4 Preliminaries and prior work on framing intelligence

(Garcez & Lamb, 2020: 8). “[S]ymbolic machine learning [. . .] is unequivocally
not confined to association rules” (ibid.: 19). We shall revert to this point and
give a concrete example of such an AI system in 5.2.
–
The image of a ladder is inappropriate. Not only proves a three-rung ladder to
be insufficient for reaching the top (AD 5.1.), but it is indeterminate what wall
to lean it against, given how multidirectional intelligence is in different ani-
mals (both within and between species). We address this latter point in 5.2.
–
The model needs to be more dynamic. Imbued by the excursion in 4.2., we
address this point in 5.3.
4.3 The ladder of causation
117

