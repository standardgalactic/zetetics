arXiv:2301.01379v1  [cs.AI]  3 Jan 2023
A Succinct Summary of Reinforcement Learning
Sanjeevan Ahilan∗
Abstract
This document is a concise summary of many key results in single-
agent reinforcement learning (RL). The intended audience are those who
already have some familiarity with RL and are looking to review, reference
and/or remind themselves of important ideas in the ﬁeld.
Contents
1
Acknowledgements
2
2
Fundamentals
2
2.1
The RL paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
2.2
Agent and environment
. . . . . . . . . . . . . . . . . . . . . . .
2
2.3
Observability . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.4
Markov processes and Markov reward processes . . . . . . . . . .
3
2.5
Markov decision processes . . . . . . . . . . . . . . . . . . . . . .
3
2.6
Policies, values and models
. . . . . . . . . . . . . . . . . . . . .
3
2.7
Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . .
5
3
Model-free approaches
6
3.1
Prediction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3.2
Control with action-value functions . . . . . . . . . . . . . . . . .
8
3.3
Value function approximation . . . . . . . . . . . . . . . . . . . .
9
3.4
Policy gradient methods . . . . . . . . . . . . . . . . . . . . . . .
10
3.5
Baselines
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
3.6
Compatible function approximation . . . . . . . . . . . . . . . . .
11
3.7
Deterministic policy gradients . . . . . . . . . . . . . . . . . . . .
12
4
Model-based Approaches
12
4.1
Model Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
4.2
Combining model-free and model-based approaches . . . . . . . .
13
5
Latent variables and partial observability
14
5.1
Latent variable models . . . . . . . . . . . . . . . . . . . . . . . .
14
5.2
Partially observable Markov decision processes
. . . . . . . . . .
14
6
Deep reinforcement learning
15
6.1
Experience replay . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
6.2
Target networks . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
∗sanjeevanahilan@gmail.com. Much of this work was done at the Gatsby Unit, UCL.
1

1
Acknowledgements
I would like to thank Peter Dayan, David Silver, Chris Watkins and ChatGPT
for helpful feedback. Much of this work was drawn from David Silver’s UCL
course1 and Sutton and Barto’s textbook (Sutton and Barto, 2018) and formed
the introductory chapter of my PhD thesis (Ahilan, 2021).
2
Fundamentals
2.1
The RL paradigm
The ﬁeld of reinforcement learning (RL) (Sutton and Barto, 2018) concerns it-
self with the computational principles underlying goal-directed learning through
interaction. Although primarily seen as a ﬁeld of machine learning, it has a rich
history spanning multiple ﬁelds. In psychology it can be used to model classi-
cal (Pavlovian) and operant (instrumental) conditioning. In neuroscience it has
been used to model the dopamine system of the brain (Schultz et al., 1997). In
economics, it relates to ﬁelds such as bounded rationality, and in engineering
it has extensive overlap with the ﬁeld of optimal control (Bellman, 1957). In
mathematics, investigation has continued under the guise of operations research.
The plethora of perspectives ensures that RL continues to be an exciting and
extraordinarily interdisciplinary ﬁeld.
2.2
Agent and environment
RL problems typically draw a separation between the agent and the environ-
ment. The agent receives observation ot and scalar reward rt from the environ-
ment and emits action at, where t indicates the time step. The environment
receives action at from the agent and then emits a reward rt+1 and an ob-
servation ot+1. The cycle then begins again with the agent emitting its next
action.
How the environment responds to the agent’s action is determined by the
environment state st, which is updated at every time step. The conditional
distribution for the next environment state depends only on the present state
and action and therefore satisﬁes the Markov property:
P(st+1|st, at) = P(st+1|s1, . . . , st, a1, . . . , at)
(1)
The environment state is in general private from the agent, which only re-
ceives observations and rewards. The conditional distribution for the next ob-
servation given the current observation is not in general Markov, and so it may
be beneﬁcial for an agent to construct its own notion of state sα
t , which it uses
to determine its next action. This can be deﬁned as sα
t = f(ht), where ht is the
history of the agent’s sequence of observations, actions and rewards:
ht = a1, o1, r1, . . . , at, ot, rt
(2)
1https://www.davidsilver.uk/teaching/
2

2.3
Observability
A special case exists when the observation received by the agent ot is identical
to the environment state st (such that there is no need to distinguish between
the two). This is the assumption underlying the formalism of Markov decision
processes covered in the next section. An environment is partially observable
if the agent cannot observe the full environment state, meaning that the con-
ditional distribution for its next observation given its current observation does
not satisfy the Markov property. This assumption underlies the formalism of a
partially observable Markov decision process which we describe in Section 5.2.
2.4
Markov processes and Markov reward processes
A Markov process (or Markov chain) is a sequence of random states with the
Markov property. It is deﬁned in terms of the tuple ⟨S, P⟩where S is a ﬁnite
set of states and P : S × S →[0, 1] is the state transition probability kernel.
A Markov Reward Process (MRP) ⟨S, P, r, γ⟩extends the Markov process
by including a reward function r : S × S →R for each state transition and a
discount factor γ. The immediate expected reward in a given state is deﬁned
as: r(s) = P
s′ P(s, s′)r(s, s′).
The discount factor γ ∈[0, 1] is used to determine the present value of future
rewards. Conventionally, a reward received k steps into the future is of worth
γk times what it would be worth if received immediately. As we will shortly
see, the cumulative sum of discounted rewards is a quantity RL agents often
seek to maximise, and so γ < 1 ensures that this sum is bounded (assuming r
is bounded).
2.5
Markov decision processes
Single-agent RL can be formalised in terms of Markov decision processes (MDPs).
The idea of an MDP is to capture the key components available to the learning
agent; the agent’s sensation of the state of its environment, the actions it takes
which can aﬀect the state, and the rewards associated with states and actions.
An MDP extends the formalism of an MRP to include a ﬁnite set of actions on
which both P and r depend. Discrete-time, inﬁnite-horizon MDPs are described
in terms of the 5-tuple ⟨S, A, P, r, γ⟩where S is the set of states, A is the
set of actions, P : S × A × S →[0, 1] is the state transition probability kernel,
r : S×A×S →R is the immediate reward function and γ ∈[0, 1) is the discount
factor. The expected immediate reward for a given state and action is deﬁned
as r(s, a) = P
s′ P(s, a, s′)r(s, a, s′), which we use for convenience subsequently.
2.6
Policies, values and models
Common components of a reinforcement learning agent are a policy, value func-
tion and a model. The policy π : S ×A →[0, 1] is the agent’s behaviour function
which denotes the probability of taking action a in state s. Agents may also act
according to a deterministic policy µ : S →A. We will assume that policies are
stochastic unless otherwise noted.
3

Given an MDP and a policy π, the observed state sequence is a Markov
process ⟨S, Pπ⟩.
Pπ(s, s′) =
X
a∈A
π(s, a)P(s, a, s′)
(3)
Similarly, the state and reward sequence is a MRP ⟨S, Pπ, rπ, γ⟩in which:
rπ(s) =
X
a∈A
π(s, a)r(s, a)
(4)
Starting from any particular state s at time step t = 0, the value function
vπ(s) is a prediction of the expected discounted future reward given that the
agent starts in state s and follows policy π:
vπ(s) = Eπ
" ∞
X
t=0
γtrt+1|s0 = s
#
(5)
where rt+1 = r(st, at, st+1)
which is the solution of an associated Bellman expectation equation:
vπ(s) =
X
a∈A
π(s, a)
"
r(s, a) + γ
X
s′∈S
P(s, a, s′)vπ(s′)
#
(6)
In matrix form the Bellman expectation equation can be expressed in terms
of the induced MRP:
vπ = rπ + γPπvπ = (I −γPπ)−1rπ
(7)
where vπ ∈R|S| and rπ ∈R|S| are the vector of values and expected imme-
diate rewards respectively for each state under policy π. We can also deﬁne a
Bellman expectation backup operator:
T π(v) = rπ + γPπv
(8)
which has a ﬁxed point of vπ.
An action-value for a policy π can also be deﬁned, which is the expected dis-
counted future reward for executing action a and subsequently following policy
π.
qπ(s, a) = r(s, a) + γ
X
s′∈S
P(s, a, s′)vπ(s′)
= r(s, a) + γ
X
s′∈S
P(s, a, s′)
X
a′∈A
π(s′, a′)qπ(s′, a′)
(9)
The process of estimating vπ or qπ is known as policy evaluation. Policies
can be evaluated without directly knowing or estimating a model, using instead
the directly sampled experience of the environment, an approach which is known
as ‘model-free’. However a ‘model-based’ approach is also possible in which a
model is used to predict what the environment will do next. A key component
of a model is an estimate of P(s, a, s′), the probability of the next state given
the current state and action. Another is an estimate of r(s, a), the expected
immediate reward.
4

Policy evaluation enables a value function to be learned for a given policy.
However, we often wish to learn the best possible policy. The value function for
this is known as the optimal value function and corresponds to the maximum
value function over all policies:
v∗(s) = max
π
vπ(s)
(10)
The deﬁnition of the optimal action-value function (which evaluates the im-
mediate action a in state s) is similarly:
q∗(s, a) = max
π
qπ(s, a)
(11)
A partial ordering over policies can be deﬁned according to:
π ≥π′ if vπ(s) ≥vπ′(s), ∀s
(12)
For any MDP there exists an optimal policy π∗that is better than or equal
to all other policies. All optimal policies achieve the optimal value function and
optimal action-value function and there is always a deterministic optimal policy
for any MDP. The latter is achieved by selecting:
a = arg max
a∈A
q∗(s, a)
(13)
If there are many possible actions which satisfy this, any of these may be
chosen to constitute an optimal policy (of which there may be many).
The
optimal value and state-value functions satisfy Bellman optimality equations:
v∗(s) = max
a∈A q∗(s, a)
v∗(s) = max
a∈A
h
r(s, a) + γ
X
s′∈S
P(s, a, s′)v∗(s′)
i
q∗(s, a) = r(s, a) + γ
X
s′∈S
P(s, a, s′)max
a′
q∗(s′, a′)
(14)
The Bellman optimality equation is non-linear with no closed form solution
(in general). Solving it therefore requires iterative solution methods.
2.7
Dynamic programming
Dynamic programming (DP) (Bertsekas et al., 1995) refers to a collection of
algorithms that can be used to compute optimal policies given a perfect model
of the environment as an MDP. In general, DP solves complex problems by
breaking them down into subproblems and then combining the solutions. It is
particularly useful for overlapping subproblems, the solutions to which reoccur
many times when solving the overall problem, making it more computationally
eﬃcient to cache and reuse them.
When applied to MDPs, the recursive decomposition of DP corresponds to
the Bellman equation and the cached solution to the value function. DP assumes
that the MDP is fully known and therefore does not address the full RL problem
but instead addresses the problem of planning. By planning, the prediction
problem can be addressed by ﬁnding the value function vπ of a given policy
5

π. This can be evaluated by iterative application of the Bellman Expectation
Backup (Equation 8).
This leads to convergence to a unique ﬁxed point vπ, which can be shown
using the contraction mapping theorem (also known as the Banach ﬁxed-point
theorem) (Banach, 1922). When a Bellman expectation backup operator T π
is applied to two value functions u and v over states, we ﬁnd that it is a γ-
contraction:
||T π(u) −T π(v)||∞= ||(rπ + γPπu) −(rπ + γPπv)||∞
= ||γPπ(u −v)||∞
≤||γPπ1||u −v||∞||∞
≤γ||u −v||∞
(15)
where 1 is a vector of ones and the inﬁnity norm of a vector a is denoted
||a||∞and is deﬁned as the maximum value of its components. This contraction
ensures that both u and v converge to the unique ﬁxed point of T π which is vπ.
For control, DP can be used to ﬁnd the optimal value function v∗and in turn
the optimal policy π∗. One possibility is policy iteration in which the current
policy π is ﬁrst evaluated as described and then subsequently improved to π′
such that:
π′(s) = arg max
a∈A
qπ(s, a)
(16)
This improves the value from any state s over one step:
qπ(s, π′(s)) = max
a∈A qπ(s, a) ≥
X
a∈A
π(s, a)qπ(s, a) = vπ(s)
(17)
It can be shown that this improves the value function such that that vπ′(s) ≥
vπ(s) (Silver, 2015). This process is then repeated, with improvements ending
when the Bellman optimality equation (14) has been satisﬁed and convergence
to π∗achieved. A generalisation of policy iteration is also possible in which,
instead of waiting for policy evaluation to converge, only n steps of evaluation
are taken before policy improvement occurs and the process is repeated. If n = 1
this is known as value iteration, as the policy is no longer explicit (being a direct
consequence of the value function). Like policy iteration, value iteration is also
guaranteed to converge to the optimal value function and policy. This can be
demonstrated using the contraction mapping theorem.
3
Model-free approaches
3.1
Prediction
As has been outlined, dynamic programming can be used to solve known MDPs
enabling optimal value functions and policies to be found. However, in many
cases the MDP is not directly known - instead an agent taking actions in the
MDP must learn directly from its experiences, as it transitions from state to
state and receives rewards accordingly. One approach, known as ‘model-free’,
seeks to solve MDPs without learning transitions or rewards. For prediction, a
6

key quantity to estimate in this setting is the expected discounted future reward.
A sampled estimate of this, starting from state st, is known as the return:
Rt = rt+1 + γrt+2 + γ2rt+3 + ... =
∞
X
k=0
γkrt+k+1
(18)
which depends on the actions sampled from the policy, and states from
transitions.
Monte-Carlo (MC) methods seek to estimate this directly using complete
episodes of experience. Introducing a learning rate αt, the agent’s value function
can therefore be updated according to2:
v(st) ←v(st) + αt

Rt −v(st)

(19)
The value function updated in this way will converge to a solution with min-
imum mean-square error (best ﬁt to the observed returns), assuming a suitable
sequential decrease in the learning rate.
Temporal-diﬀerence (TD) learning methods learn from incomplete episodes
by bootstrapping. For example, if learning occurs after a single step, this is
known as TD(0), which has the following update:
v(st) ←v(st) + αt

rt+1 + γv(st+1) −v(st)

(20)
where rt+1 + γv(st+1) is known as the target. This approximates the full-width
Bellman expectation backup (Equation 8) in which every successor state and
action is considered, with experiences instead being sampled. TD(0) will con-
verge to the solution of the maximum likelihood Markov model which best ﬁts
the data (again assuming a suitable sequential decrease in the learning rate).
This solution may be diﬀerent from the minimum mean-square error solution of
MC methods, which do not assume the Markov property.
Unlike MC methods, TD methods introduce bias into the estimated return
as the currently estimated value function may be diﬀerent from the true value
function. However, they generally have reduced variance relative to MC meth-
ods, as in MC the estimated return depends on a potentially long sequence of
random actions, transitions and rewards.
The distinction between MC and TD methods can be blurred by considering
multi-step TD methods (rather than only TD(0)), in which rewards are sampled
for a number of steps before the value function is used to compute an estimate
of future rewards. The n-step return is deﬁned as:
R(n)
t
= rt+1 + γrt+2 + ... + γn−1rt+n + γnv(st+n)
(21)
As n →∞it tends towards the unbiased MC return. An algorithm may
seek to ﬁnd a good bias-variance tradeoﬀby estimating a weighted combination
of n-step returns; one popular method to do this is known as TD(λ):
Rλ
t = (1 −λ)
∞
X
n=1
λn−1R(n)
t
(22)
where λ ∈[0, 1].
2assuming a table-based representation rather than use of a function approximator
7

3.2
Control with action-value functions
Model free control concerns itself with optimising rather than evaluating the
RL objective. Policies may be evaluated according to various objectives. In
the case of continuing environments, the objective can be the average value or
the average reward per time-step. We focus instead on episodic environments,
assuming an initial distribution over starting states p0(s) : S →[0, 1]. The
objective is thus:
J(π) = Eπ
" ∞
X
t=0
γtrt+1|p0(s)
#
(23)
Note that if the domain of the starting state distribution is only over a single
starting state, the objective is simply the value function (Equation 5) in that
starting state. This objective can equivalently be expressed as:
J(π) = Es∼ρπ,a∼π[r(s, a)]
(24)
where:
ρπ(s) :=
X
s′
∞
X
t=0
γtp(st = s|s′, π)p0(s′)
(25)
is the improper discounted state distribution induced by policy π starting from
an initial state distribution p0(s′). In Section 3.4 we describe policy gradient
methods which seek to optimise this objective directly.
However, we ﬁrst consider model-free approaches which rely on an action-
value function q(s, a) to achieve control (a value function v(s) alone is insuﬃcient
for model-free control).
The optimal action-value function q∗(s, a) must be
learned, with MC and TD methods both viable.
Once it has been learned,
an optimal policy may be achieved by selecting the best action in each state
(Equation 13).
However, unlike dynamic programming, full-width backups are not used and
so if actions are selected greedily (meaning those with highest action-values are
always chosen) then certain states and actions may never be correctly evalu-
ated. Model-free RL methods must therefore allow for enough exploration dur-
ing learning before ultimately exploiting this learning to achieve near-optimal
cumulative reward.
One simple approach, known as ǫ-greedy is to take a random action with
probability ǫ but otherwise act greedily according to the current estimate of
the action-value function. The value of ǫ can be decreased with the number of
episodes. This can satisfy a condition known as greedy in the limit of inﬁnite
exploration in which all state-action pairs are explored inﬁnitely many times
and the policy converges to the greedy policy.
One popular algorithm for model-free control is known as Q-learning (Watkins and Dayan,
1992), which seeks to learn the optimal action-value function whilst using a pol-
icy which also takes exploratory actions (such as epsilon greedy). This learning
is termed oﬀ-policy as the policy used to sample experience is diﬀerent from the
policy being learned (the optimal policy). The resulting update is:
q(st, at) ←q(st, at) + α

rt+1 + γmax
a′∈A q(st+1, a′) −q(st, at)

(26)
8

An alternative to oﬀ-policy Q-learning is on-policy SARSA (Rummery and Niranjan,
1994). This uses the sampled sampled state st, action at, reward rt+1, next state
st+1, and next action at+1 for updates3:
q(st, at) ←q(st, at) + α(rt+1 + γq(st+1, at+1) −q(st, at))
(27)
3.3
Value function approximation
So far we have assumed a tabular representation of states and actions such
that each state is separately updated. However, in practice we would like value
functions and policies to generalise to new states and actions, and so it is ben-
eﬁcial to use function approximators such as deep neural networks. A common
approach is to approximate the value function or action-value function:
vw(s) = ˆv(s; w) ≈vπ(s)
qw(s, a) = ˆq(s, a; w) ≈qπ(s, a)
(28)
where w are the parameters we wish to learn.
If we start by assuming we
know the true value function vπ, we can deﬁne a mean square error between the
approximate value function and the true function:
L(w) = Eπ[(vπ(s) −vw(s))2]
(29)
Given a distribution of states s ∼p(s)4, we can minimise this iteratively
using stochastic gradient descent:
w ←w + α(vπ(st) −vw(st))∇wvw(st)
(30)
In reality we can only use a better estimate of vπ provided by the sampled
reward(s). For example, if we use the TD(0) target the update is:
w ←w + α(rt+1 + γvw(st+1) −vw(st))∇wvw(st)
(31)
Updates like this are known as ‘semi-gradient’ as the gradient of the value
function used to deﬁne the target is ignored.
If we use a linear function approximator vw(s) = x(s)T w (where features
x(s) and w are vectors), then we ﬁnd:
w ←w + α(rt+1 + γvw(st+1) −vw(st))x(st)
(32)
indicating that the linear weights are updated in proportion to the activity
of their corresponding features. Non-linear function approximators can also be
used, but typically have weaker convergence guarantees than linear function
approximators. Nevertheless, due to their ﬂexibility such approximators have
enabled impressive performance in a number of challenging domains, such as
Atari games (Mnih et al., 2015) and Go (Silver et al., 2016).
3and also gives SARSA its name
4we later discuss a method for sampling states
9

3.4
Policy gradient methods
Parameterised stochastic policies πθ may be improved using the policy gradient
theorem (Sutton et al., 2000). This can be derived for any of the common RL
objectives. To demonstrate a derivation of this result we use a starting state
objective J(θ) = vπθ(s0) with a single starting state s0:
∇θJ(θ) = ∇θvπ(s0)
= ∇θ
X
a
π(s0, a)qπ(s0, a)
=
X
a
∇θπ(s0, a)qπ(s0, a) + π(s0, a)∇θqπ(s0, a)
=
X
a
∇θπ(s0, a)qπ(s0, a) + π(s0, a)∇θ
h
r(s0, a) +
X
s′
γP(s0, a, s′)vπ(s′)
i
=
X
a
∇θπ(s0, a)qπ(s0, a) + π(s0, a)
X
s′
γP(s0, a, s′)∇θvπ(s′)
(33)
We note that we could continue to unroll ∇θvπ(s′) on the R.H.S in the same
way as we have already done. Considering now transitions from starting state
s0 to arbitrary state s we therefore ﬁnd:
∇θvπ(s0) =
X
s
∞
X
t=0
γtp(st = s|s0, π)
X
a
∇θπ(s, a)qπ(s, a)
(34)
where P∞
t=0 γtp(st = s|s0, π) is the discounted state distribution ρπ(s) from
a ﬁxed starting state s0 (Equation 25). This derivation holds even when there
is a distribution over starting states, and gives us the policy gradient theorem:
∇θJ(θ) =
X
s
ρπ(s)
X
a
∇θπ(s, a)qπ(s, a)
(35)
Using the likelihood ratio trick:
∇θπ(s, a) = π(s, a)∇θπ(s, a)
π(s, a)
= π(s, a)∇θ log π(s, a)
(36)
this can be equivalently expressed as:
∇θJ(θ) =
X
s
ρπ(s)
X
a
π(s, a)qπ(s, a)∇θ log π(s, a)
= Eπ[qπ(s, a)∇θ log π(s, a)]
(37)
The policy gradient theorem result enables model-free learning as gradi-
ents need only be determined for the policy rather than for properties of the
environment. There are a variety of approaches for determining qπ. If qπ is ap-
proximated using the sample return (Equation 18), this leads to the algorithm
known as REINFORCE (Williams, 1992):
θ ←θ + αRt∇θ log π(st, at)
(38)
10

As there is no bootstrapping here, this is also known as MC policy gradient.
An alternative approach is to separately approximate qπ with a ‘critic’ qw giving
rise to what are commonly known as ‘actor-critic’ methods. These introduce two
sets of parameter updates; the critic parameters w are updated to approximate
qπ, and the policy (actor) parameters θ are updated according to the policy
gradient as indicated by the critic. The critic itself can be updated according
to the TD error. An example of this approach is SARSA actor-critic:
w ←w + α1(rt+1 + γqw(st+1, at+1) −qw(st, at))∇wqw(st, at)
θ ←θ + α2qw(st, at)∇θ log π(st, at)
(39)
where diﬀerent learning rates α1 and α2 may be used for the actor and the
critic.
3.5
Baselines
Whether we use REINFORCE or an actor-critic based approach to policy gradi-
ents, it is possible to reduce the variance further by the introduction of baselines.
If this baseline depends only on the state s, then we ﬁnd it introduces no bias:
X
s
ρπ(s)
X
a
∇θπ(s, a)b(s) =
X
s
ρπ(s)b(s)∇θ
X
a
π(s, a)
=
X
s
ρπ(s)b(s)∇θ1
= 0
(40)
A natural choice for the state-dependent baseline is the value function:
∇θJ(θ) = Eπ[(qπ(s, a) −vπ(s))∇θ log π(s, a)]
= Eπ[Aπ(s, a)∇θ log π(s, a)]
(41)
where Aπ is known as the advantage, which may in some algorithms be approx-
imated directly (rather than approximating both qπ and vπ).
3.6
Compatible function approximation
In the general case, our choice to approximate qπ with qw introduces bias such
that there are no guarantees of convergence to a local optimum. However, in the
special case of a compatible function approximator we can introduce no bias and
take steps in the direction of the true policy gradient. This becomes possible
when the critic’s function approximator reaches a minimum in the mean-squared
error:
0 = Eπ[∇w(qπ(s, a) −qw(s, a))2]
= Eπ[(qπ(s, a) −qw(s, a))∇wqw(s, a)]
(42)
If we choose qw(s, a) such that ∇wqw(s, a) = ∇θ log π(s, a) we ﬁnd:
Eπ[qπ(s, a)∇θ log π(s, a)] = Eπ[qw(s, a)∇θ log π(s, a)]
(43)
11

where the L.H.S is equal to the true policy gradient and so our function ap-
proximation has introduced no bias. For example, if the policy is a Boltzmann
policy with a linear combination of features, of the form:
π(s, a) =
eθT φ(s,a)
P
a′ eθT φ(s,a′)
(44)
then a compatible value function must be linear in the same features as the
policy except normalised to zero mean for each state using a subtractive baseline
(Sutton et al., 2000).
qw(s, a) = wT [φ(s, a) −
X
a′
φ(s, a′)π(s, a′)]
(45)
3.7
Deterministic policy gradients
Rather than have a policy specify a probability for certain actions in certain
states we can instead have it simply be a function mapping states to actions
µθ : S →A and, in the case of continuous actions, seek to ﬁnd the gradient
of the objective with respect to the policy parameters. An example of an al-
gorithm which uses such an approach is Deterministic Policy Gradients (DPG)
(Silver et al., 2014).
The DPG algorithm builds on the deterministic policy
gradient theorem:
∇θJ(θ) = Es∼ρµ[∇θµθ(s)∇aqµ(s, a)|a=µθ(s)].
(46)
where the parameters of the policy are adjusted in an oﬀ-policy fashion using
an exploratory behavioural policy (which is a noisy version of the deterministic
policy). In practice qµ is approximated by the critic qw, which is diﬀerentiable
in the action and updated using Q-learning:
δt = rt+1 + γqw(st+1, µθ(st+1)) −qw(st, at)
w ←w + α1δt∇wqw(st, at)
The parameters of the policy are then updated according to:
θ ←θ + α2∇θµθ(st)∇aqw(st, at)|a=µθ(st)
(47)
4
Model-based Approaches
In model-free RL agents learn to take actions directly from experiences, without
ever modelling transitions in the environment or reward functions, whereas in
model-based RL the agent attempts to learn these. The key beneﬁt is that if
the agent can perfectly predict the environment ‘in its head’, then it no longer
needs to interact directly with the environment in order to learn an optimal
policy.
4.1
Model Learning
Recall that MDPs are deﬁned in terms of the 5-tuple ⟨S, A, P, r, γ⟩. Although
models can be predictions about anything, a natural starting point is to ap-
proximate the state transition function Pη ≈P and immediate reward function
12

rη ≈r. We can then use dynamic programming to learn the optimal policy for
an approximate MDP ⟨S, A, Pη, rη, γ⟩, the performance of which may be worse
than for the true MDP.
Given a ﬁxed set of experiences, a model can be learned using supervised
methods. For predicting immediate expected scalar rewards, this is a regression
problem whereas for predicting the distribution over next states this a density
estimation problem. Given the simplicity of this framing, a range of function
approximators may be employed, including neural networks and Gaussian pro-
cesses.
4.2
Combining model-free and model-based approaches
Once a model is learned it can be used for planning. However, in many situ-
ations it is computationally infeasible to do the full-width backups of dynamic
programming as the state space is too large. Instead, experiences can be sam-
pled from the model and used as data by a model-free algorithm.
A well known architecture which combines model-based and model-free RL is
the Dyna architecture (Sutton, 1991). Dyna treats samples of simulated and real
experience similarly, using both to learn a value function. Simulated experience
is generated by the model which is itself learned from real experience. In Dyna,
model-free based updates depend on the state the agent is currently in, whereas
for the model-based component starting states can be sampled randomly and
then rolled forwards using the model to update the value function using e.g.
TD learning.
One potential disadvantage of Dyna is that it does not preferentially treat
the state the agent is currently in.
In many cases, such as deciding on the
next move in chess, it is useful to start all rollouts from the current state (the
board position) when choosing the next move. This is known as forward search,
where a search tree is built with the current state as the root. Forward based
search often uses sample based rollouts rather than full-width ones so as to be
computationally tractable and this is known as simulation-based search.
An eﬀective algorithm for simulation-based search is Monte-Carlo Tree search
(Coulom, 2007). It uses the MC return to estimate the action-value function for
all nodes in the search tree using the current policy. It then improves the pol-
icy, for example by being ǫ-greedy with respect to the new action-value function
(or more commonly handling exploration-exploitation using Upper Conﬁdence
Trees, see Kocsis and Szepesv´ari (2006) for a more detailed discussion). MC
Tree Search is equivalent to MC control applied to simulated experience and
therefore is guaranteed to converge on the optimal search tree. Instead of using
MC control for search it is also possible to use TD-based control, which will
increase bias but reduce variance.
Model-based RL is a highly active area of research. Recent advances include
MuZero (Schrittwieser et al., 2020), which extends model-based predictions to
value functions and policies, and Dreamer which plans using latent variable
models (Hafner et al., 2019).
13

5
Latent variables and partial observability
5.1
Latent variable models
Hidden or ‘latent’ variables correspond to variables which are not directly ob-
served but nevertheless inﬂuence observed variables and thus may be inferred
from observation. In reinforcement learning, it can be beneﬁcial for agents to
infer latent variables as these often provide a simpler and more parsimonious
description of the world, enabling better predictions of future states and thus
more eﬀective control.
Latent variable models are common in the ﬁeld of unsupervised learning.
Given data p(x) we may describe a probability distribution over x according to:
p(x; θx|z, θz) =
Z
dz p(x|z; θx|z)p(z; θz)
(48)
where θx|z parameterises the conditional distribution x|z and θz parame-
terises the distribution over z.
Key aims in unsupervised learning include capturing high-dimensional cor-
relations with fewer parameters (as in probabilistic principal components analy-
sis), generating samples from a data distribution, describing an underlying gen-
erative process z which describes causes of x, and ﬂexibly modelling complex
distributions even when the underlying components are simple (e.g. belonging
to an exponential family).
5.2
Partially observable Markov decision processes
A partially observable Markov decision process (POMDP) (Kaelbling et al.,
1998) is a generalisation of an MDP in which the agent cannot directly ob-
serve the true state of the system, the dynamics of which is determined by an
MDP. Formally, a POMDP is a 7-tuple ⟨S, A, P, r, O, Ω, γ⟩where S is the set
of states, A is the set of actions, P : S × A × S →[0, 1] is the state transition
probability kernel, r : S × A × S →R is the reward function, O is the set of
observations, Ω: S × A × O →[0, 1] is the observation probability kernel and
γ ∈[0, 1) is the discount factor. As with MDPs, agents in POMDPs seek to
learn a policy π(sα
t ) which maximises some notion of cumulative reward, com-
monly Eπ[P∞
t=0 γtrt+1]. This policy depends on the agent’s representation of
state sα
t = f(ht), which is a function of its history.
One approach to solving POMDPs is by maintaining a belief state over the
latent environment state - transitions for which satisfy the Markov property.
Maintaining a belief over states only requires knowledge of the previous belief
state, the action taken and the current observation. Beliefs may then be updated
according to:
b′(s′) = ηΩ(o′|s′, a)
X
s∈S
P(s′|s, a)b(s)
(49)
where η = 1/ P
s′ Ω(o′|s′, a) P
s∈S P(s′|s, a)b(s) is a normalising constant.
A Markovian belief state allows a POMDP to be formulated as an MDP
where every belief is a state. However, in practice, maintaining belief states in
POMDPs will be computationally intractable for any reasonably sized problem.
In order to address this, approximate solutions may be used.
Alternatively,
14

agents learning using function approximators which condition on the past can
construct their own state representations, which may in turn enable relevant
aspects of the state to be approximately Markov.
6
Deep reinforcement learning
The policies and value functions used in reinforcement learning can be learned
using artiﬁcial neural network function approximators. When such networks
have many layers they are conventionally denoted as ‘deep’, and are typically
trained on large amounts of data using stochastic gradient descent (LeCun et al.,
2015). The application of deep networks in model-free reinforcement learning
garnered extensive attention when they were successfully used to learn a variety
of Atari games from scratch (Mnih et al., 2013). For the particular problem
of learning from pixels a convolutional neural network architecture was used
(LeCun et al., 1998), which are highly eﬀective at extracting useful features
from images. They have been extensively used on supervised image classiﬁcation
tasks due to their ability to scale to large and complex datasets (LeCun et al.,
2015).
A deep analysis of deep reinforcement learning (DRL) is beyond the scope
of this summary. However we review two key techniques used to overcome the
technical challenge of stabilising training.
6.1
Experience replay
As an agent interacts with its environment it receives experiences that can be
used for learning. However, rather than using those experiences immediately, it
is possible to store such experience in a ‘replay buﬀer’ and sample them at a later
point in time for learning. The beneﬁts of such an approach were introduced by
Mnih et al. (2013) for their ‘deep Q-learning’ algorithm. At each timestep, this
method stores experiences et = (st, at, rt+1, st+1) in a replay buﬀer over many
episodes. After suﬃcient experience has been collected, Q-learning updates are
then applied to randomly sampled experiences from the buﬀer. This breaks the
correlation between samples, reducing the variance of updates and the potential
to overﬁt to recent experience. Further improvements to the method can be
made by prioritised (as opposed to random) sampling of experiences according to
their importance, determined using the temporal-diﬀerence error (Schaul et al.,
2015).
6.2
Target networks
When using temporal diﬀerence learning with deep function approximators a
common challenge is stability of learning. A source of instability arises when
the same function approximator is used to evaluate both the value of the current
state and the value of the target state for the temporal diﬀerence update. After
such updates, the approximated value of both current and target state change
(unlike tabular methods), which can lead to a runaway target. To address this,
deep RL algorithms often make use of a separate target network that remains
stable even whilst the standard network is updated. As it is not desirable for
the target network to diverge too far from the standard network’s improved
15

predictions, at ﬁxed intervals the parameters of the standard network can be
copied to the target network. Alternatively, this transition is made more slowly
using Polyak averaging:
φtarg ←ρφtarg + (1 −ρ)φ
(50)
where φ are the parameters of the standard network and ρ is a hyperparameter
typically close to 1.
16

References
Sanjeevan Ahilan. Structures for Sophisticated Behaviour: Feudal Hierarchies
and World Models. PhD thesis, UCL (University College London), 2021.
Stefan Banach. Sur les op´erations dans les ensembles abstraits et leur application
aux ´equations int´egrales. Fund. math, 3(1):133–181, 1922.
Richard Bellman. A markovian decision process. Journal of mathematics and
mechanics, pages 679–684, 1957.
Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P
Bertsekas. Dynamic programming and optimal control, volume 1. Athena
scientiﬁc Belmont, MA, 1995.
R´emi Coulom. Eﬃcient selectivity and backup operators in monte-carlo tree
search. In International conference on computers and games, pages 72–83.
Springer, 2007.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.
Dream to control: Learning behaviors by latent imagination. arXiv preprint
arXiv:1912.01603, 2019.
Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning
and acting in partially observable stochastic domains. Artiﬁcial intelligence,
101(1-2):99–134, 1998.
Levente Kocsis and Csaba Szepesv´ari. Bandit based monte-carlo planning. In
European conference on machine learning, pages 282–293. Springer, 2006.
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haﬀner. Gradient-based
learning applied to document recognition. Proceedings of the IEEE, 86(11):
2278–2324, 1998.
Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. nature, 521
(7553):436–444, 2015.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep
reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Ve-
ness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. Human-level control through deep reinforcement
learning. Nature, 518(7540):529, 2015.
Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connec-
tionist systems, volume 37. University of Cambridge, Department of Engi-
neering Cambridge, UK, 1994.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
Prioritized
experience replay. arXiv preprint arXiv:1511.05952, 2015.
17

Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hass-
abis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning
with a learned model. Nature, 588(7839):604–609, 2020.
Wolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of
prediction and reward. Science, 275(5306):1593–1599, 1997.
David Silver. Lecture 3: Planning by dynamic programming. Google DeepMind,
2015.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
Martin Riedmiller. Deterministic policy gradient algorithms. In ICML, 2014.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George
Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneer-
shelvam, Marc Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. nature, 529(7587):484–489, 2016.
Richard S Sutton. Dyna, an integrated architecture for learning, planning, and
reacting. ACM Sigart Bulletin, 2(4):160–163, 1991.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduc-
tion. 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour.
Policy gradient methods for reinforcement learning with function approxima-
tion. In Advances in neural information processing systems, pages 1057–1063,
2000.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8
(3-4):279–292, 1992.
Ronald J Williams. Simple statistical gradient-following algorithms for connec-
tionist reinforcement learning. Machine learning, 8(3-4):229–256, 1992.
18

