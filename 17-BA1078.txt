Bayesian Analysis (2018)
13, Number 3, pp. 767–796
Bayesian Community Detection
S. L. van der Pas‡∗and A. W. van der Vaart§†
Abstract.
We introduce a Bayesian estimator of the underlying class structure
in the stochastic block model, when the number of classes is known. The estimator
is the posterior mode corresponding to a Dirichlet prior on the class proportions,
a generalized Bernoulli prior on the class labels, and a beta prior on the edge
probabilities. We show that this estimator is strongly consistent when the expected
degree is at least of order log2 n, where n is the number of nodes in the network.
MSC 2010 subject classiﬁcations: Primary 62F15, 90B15.
Keywords: stochastic block model, community detection, networks, consistency,
Bayesian inference, modularities, MAP estimation.
1
Introduction
The stochastic block model (SBM) (Holland et al., 1983) is a model for network data
in which individual nodes are considered members of classes or communities, and the
probability of a connection occurring between two individuals depends solely on their
class membership. It has been applied to social, biological and communication networks,
for example in Park and Bader (2012), Bickel and Chen (2009) and Snijders and Now-
icki (1997) amongst many others. There are many extensions of the SBM for various
applications, including the degree-corrected SBM (Karrer and Newman, 2011; Zhao
et al., 2012) which accounts for possible heterogeneity among nodes within the same
class, and the mixed-membership SBM (Airoldi et al., 2008), in which the assumption
that the classes are disjoint is removed. These extensions allow for additional modelling
ﬂexibility.
Two main SBM research directions are the recovery of the class labels (community
detection) and recovery of the remaining model parameters, consisting of the probability
vector generating the class labels, and the class-dependent probabilities of creating an
edge between nodes. In this paper, we focus on community detection, noting that once
strong consistency of a community detection method has been established, consistency
of the natural plug-in estimators for the remaining parameters follows directly by results
in (Channarond et al., 2012).
A large number of methods for recovering the class labels has been proposed. Those
most closely related to this work are the modularities. Newman and Girvan (2004)
introduced the term modularity for ‘a measure of the quality of a particular division
of a network’. They described one such measure for models in which edges are more
∗Research supported by Netherlands Organization for Scientiﬁc Research NWO.
†The research leading to these results has received funding from the European Research Council
under ERC Grant Agreement 320637.
‡Mathematical Institute, Leiden University, svdpas@math.leidenuniv.nl
§Mathematical Institute, Leiden University, avdvaart@math.leidenuniv.nl
c⃝2018 International Society for Bayesian Analysis
https://doi.org/10.1214/17-BA1078

768
Bayesian Community Detection
likely to occur within classes than between classes, in which case there is a community
structure in the colloquial sense, although the SBM does not require this assumption.
Bickel and Chen (2009) studied more general modularities, deﬁning them as functions
of the number of connections between all combinations of classes and the proportion
of nodes placed in each class. They introduced the likelihood modularity, and provided
general conditions under which modularities are consistent. Their method and theory
was extended to the degree-corrected SBM by Zhao et al. (2012).
Spectral methods for community detection have gained in popularity, and reﬁned
results on error bounds are now available for the SBM and extensions of the SBM,
as evidenced in Rohe et al. (2011), Jin (2015), Sarkar and Bickel (2015) and Lei and
Rinaldo (2015) for example. Many other algorithms have been introduced, most of them
currently lacking formal proofs of consistency. A notable exception is the Largest Gaps
algorithm (Channarond et al., 2012), which only takes the degree of each node as its
input, and is strongly consistent under a separability condition.
A Bayesian approach towards recovering the class assignments in the SBM was
ﬁrst suggested by Snijders and Nowicki (1997), motivated by computational advantages
of Gibbs sampling over maximum likelihood estimation. They considered two classes
and proposed uniform priors on the class proportions and the edge probabilities. This
approach was extended in (Nowicki and Snijders, 2001) to allow for more classes, with
a Dirichlet prior on the class proportions and beta priors on the edge probabilities.
Hofman and Wiggins (2008) described a similar Bayesian approach for a special case
of the SBM and suggested a variational approach to overcome the computational issues
associated with maximizing over all possible class assignments.
Bayesian methods for the SBM have barely been studied from a theoretical point of
view, although recent results for parameter recovery by Pati and Bhattacharya (2015),
for detecting the number of communites by Hayashi et al. (2016) and for an empirical
Bayes approach to community detection by Suwan et al. (2016) are encouraging. In
this work, we provide theoretical results on community detection, establishing that the
Bayesian posterior mode is strongly consistent for the class labels if the expected degree
is at least of order log2 n, where n is the number of nodes. This is proven by relating
the posterior mode to the maximizer of the likelihood modularity of Bickel and Chen
(2009). The likelihood modularity has been claimed to be strongly consistent under the
weaker assumption that the expected degree is of larger order than log n (Bickel and
Chen, 2009; Zhao et al., 2012; Bickel et al., 2015). However, their proof assumes that
the likelihood modularity is globally Lipschitz, while it is only locally so. The Bayesian
method is based on a combination of likelihood and prior, and for this reason the proof
of our main theorem, Theorem 1, runs into a similar problem. We were able to resolve
this only under the slightly stronger assumption that the expected degree is of larger
order than (log n)2. The literature on other methods for community detection shows
that the order log n is suﬃcient for consistent detection. However, these results are
usually obtained under additional assumptions such as a restriction to two classes or
an ordering of the connection probabilities, and their implications for the likelihood
or Bayesian modularities is unclear. We discuss this and the relevant literature further
following the statement of our main result in Section 3.5.

S. L. van der Pas and A. W. van der Vaart
769
The main result of the present paper is that the posterior mode is strongly consistent
in the frequentist setup, a property that it shares with the maximizer of the likelihood
modularity. As the number of parameters of the model (“labellings”) increases rapidly
with n, this result is certainly not covered by standard theory for parametric models,
and in fact we shall see that the prior on the labellings plays a special role for consis-
tency. That the posterior mode behaves well in terms of consistency is encouraging, and
makes one hope that other aspects of the posterior distribution will also be useful for
inference. The present paper may be considered a ﬁrst step and further study of such
aspects is desirable. One possible research direction would be to use the full posterior
distribution on the labels to quantify uncertainty in the estimate of the class labels. A
second issue that may be resolved by the Bayesian approach is the question of estimating
the number of classes, K. This remains an important open question, as noted by Bickel
and Chen (2009), despite recent attempts (e.g. Saldana et al. (2014), Chen and Lei
(2014) and Wang and Bickel (2015)). By introducing a prior on K, such as the Poisson-
prior suggested by McDaid et al. (2013), the number of communities K can be detected
by the posterior. A third open question is whether the Bayesian estimator can be im-
proved by incorporating prior knowledge of the community structure. Recent work on
incorporating prior information in Gaussian graphical models (Kpogbezan et al., 2016)
is encouraging, and has not been translated to the SBM yet.
This paper is organized as follows. We introduce the SBM and the associated nota-
tion in Section 2. Our main results are in Section 3, where we describe the prior and
the link with the likelihood modularity, present the consistency results and discuss the
underlying assumptions, especially those on the expected degree. After an illustration
of the method on a data set in Section 4, we conclude with the proofs, ﬁrst of weak
consistency in Section 5 and ﬁnally of strong consistency of the Bayesian modularity in
Section 6.
1.1
Notation
For a vector v we denote by Diag(v) the diagonal matrix with diagonal v, and for a
matrix M we denote its diagonal by diag (M).
The ∥.∥1-norm of a matrix M is the sum of the absolute values of all entries of M.
We write f(n) = O(g(n)) as n →∞if there exist C, n0 > 0 such that |f(n)| ≤
C|g(n)| for all n > n0.
2
The stochastic block model
We introduce the notation and generative model for the SBM with K ∈{1, 2, . . .}
classes. Consider an undirected random graph with n nodes, numbered 1, 2, . . . , n, and
edges encoded by the n × n symmetric adjacency matrix (Aij), with entries in {0, 1}.
Thus Aij = Aji is equal to 1 or 0 if the nodes i and j are or are not connected by an
edge, respectively. Self-loops are not allowed, so Aii = 0 for i = 1, . . . , n. The generative
model for the random graph is:

770
Bayesian Community Detection
1. The nodes are randomly labeled with i.i.d. variables Z1, . . . , Zn, taking values in
a ﬁnite set {1, . . . , K}, according to probabilities π = (π1, . . . , πK).
2. Given Z = (Z1, . . . , Zn)T , the edges are independently generated as Bernoulli
variables with P(Aij = 1 | Z) = PZi,Zj, for i < j, for a given K × K symmetric
matrix P = (Pab).
The probability vector π is considered ﬁxed, but unknown. Although this is not visible in
the notation, the matrix P may change with n, a case of particular interest being that P
tends to zero, which gives a sparse graph. The order of magnitude of ∥P∥∞= maxa,b Pab
is the same as the order of magnitude of ρn = 
a,b πaπbPab, the probability of there
being an edge between two randomly selected nodes. The expected degree of a randomly
selected node is λn = (n −1)ρn, and twice the expected total number of edges in the
network is μn = n(n −1)ρn.
The likelihood for the model is given by

i<j
P Aij
ZiZj(1 −PZiZj)1−Aij 
i
πZi =

a≤b
P Oab(Z)
ab
(1 −Pab)nab(Z)−Oab(Z) 
a
πna(Z)
a
, (1)
where Oab(Z) is the number of edges between nodes labelled a and b by the labelling Z,
nab(Z) is the maximum number of edges that can be created between nodes labelled a
and b, and na(Z) is the number of nodes labelled a, and a and b range over {1, 2, . . . , K}.
More formally, for a given labelling e = (e1, . . . , en)T ∈{1, . . . , K}n of nodes, and
class labels a, b ∈{1, . . . , K}, we deﬁne
Oab(e) =

i,j Aij1{ei=a,ej=b},
a ̸= b,

i<j Aij1{ei=a,ej=b},
a = b,
nab(e) =

na(e)nb(e),
a ̸= b,
1
2na(e)(na(e) −1),
a = b,
na(e) =
n

i=1
1{ei=a}.
Since the matrix A is symmetric with zero diagonal by assumption, for a ̸= b the variable
Oab(e) can also be written as 
i<j Aij[1{ei=a,ej=b} + 1{ej=a,ei=b}], which explains the
diﬀerent appearances of the diagonal and oﬀ-diagonal entries. The numbers nab(e) are
equal to the numbers Oab(e) when all Aij are equal to 1. We collect the variables Oab(e)
and nab(e) in K × K matrices O(e) and n(e).
Now consider the K × K probability matrix R(e, c) and K probability vector f(e)
with entries
Rab(e, c) = 1
n
n

i=1
1{ei=a,ci=b},
fa(e) = na(e)
n
.
(2)
The row sums of R(e, c) are equal to R(e, c)1 = f(e), while the column sums are equal to
1T R(e, c) = f(c)T . Thus, the matrix R(e, c) can be seen as a coupling of the marginal

S. L. van der Pas and A. W. van der Vaart
771
probability vectors f(e) and f(c). If e = c, then it is diagonal with diagonal f(c) =
f(e). More generally, the matrix can be viewed as measuring the discrepancy between
labellings e and c. This can be precisely measured as half the L1-distance of R(e, c) to
its diagonal, as evidenced by Lemma 1, which is noted in Bickel and Chen (2009).
Recall that by ∥M∥1 we denote the sum of the absolute values of all entries of a
matrix M.
Lemma 1. For every labelling c, e in the K-class stochastic block model:
1
n
n

i=1
1{ci̸=ei} = 1
2∥Diag(f(c)) −R(e, c)∥1.
Proof. The diagonal of R(e, c) gives the fractions of labels on which c and e agree. Hence
the left side of the lemma is 1 −
a Raa(e, c) = 
a(fa(c) −Raa(e, c)). The elements of
both K × K matrices Diag(f(c)) and R(e, c) can be viewed as probabilities that add
up to 1. Thus the sum of the diﬀerences of the diagonal elements is minus the sum of
the diﬀerences of the oﬀ-diagonal elements. Because fa(c) ≥Raa(e, c) for every a, we
have 
a(fa(c)−Raa(e, c)) = 
a |fa(c)−Raa(e, c)|. Similarly the oﬀ-diagonal elements
of Diag(f(c)), which are zero, are smaller than the oﬀ-diagonal elements of R(e, c) and
hence we can add absolute values. Thus the sum over the diagonal is half the sum of
the absolute values of all terms in Diag(f(c)) −R(e, c).
3
Bayesian approach to community detection
Our main results are presented in this section. We ﬁrst discuss the choice of prior in
Section 3.1, and deﬁne the estimator, in Section 3.2. The resulting Bayesian modularity
is closely related to the likelihood modularity of Bickel and Chen (2009). The relation-
ship is clariﬁed in Section 3.3. We brieﬂy consider the issue of identiﬁability in the SBM
in Section 3.4, and conclude with our main theorem on the strong consistency of the
Bayesian modularity in Section 3.5.
3.1
The prior
We adopt the Bayesian approach of Nowicki and Snijders (2001). We put prior dis-
tributions on the parameters of the stochastic block model with K known, the vector
π and the matrix P, yielding a joint probability distribution of (A, Z, π, P). Next we
marginalize over π and P as in McDaid et al. (2013), leading to a joint distribution of
(A, Z). Finally we “estimate” the unobserved vector Z by the posterior mode of the
conditional distribution of Z given A. From a frequentist point of view this means that
Z is treated as a parameter of the problem, equipped with a hierarchical prior that
chooses ﬁrst π and then Z. Accordingly we shall change notation from Z to e, reserving
Z for the frequentist description of the stochastic block model in Section 2.
The prior on π is a Dirichlet, and independently the Pab for a ≤b receive independent
beta priors:

772
Bayesian Community Detection
π ⊥(PAb),
π ∼Dir(α, . . . , α),
Pab
i.i.d.
∼Beta(β1, β2),
1 ≤a ≤b ≤K.
This is essentially the same set-up as in Nowicki and Snijders (2001) and McDaid et al.
(2013), except that we use a more ﬂexible Beta(β1, β2) instead of a uniform prior on
the Pab. We assume α, β1, β2 > 0.
We complete the Bayesian model by specifying class labels e = (e1, . . . , en) and
edges A = (Aij : i < j) through
ei | π, P
i.i.d.
∼π,
1 ≤i ≤n,
Aij | π, P, e
ind.
∼Bernoulli(Pei,ej),
1 ≤i < j ≤n.
Abusing notation we write p(e), p(A | e) and p(e | A) for marginal and conditional
probability density functions.
3.2
The Bayesian modularity
The Bayesian estimator of the class labels will be the posterior mode, that is:
e = argmax
e
p(e | A).
The posterior mode can be interpreted as a modularity-based estimator in the sense
of Bickel and Chen (2009), in that it maximizes a function that only depends on the
Oab(e) and the na(e). This can be seen from the joint density of (A, e), which is found by
marginalizing the likelihood (1) over π and P. The conjugacy between the multinomial
and Dirichlet distributions gives the marginal density of the class assignment e as:
p(e) =

SK

a
πna(e)
a

a πα−1
a
D(α)
dπ =
Γ(αK)
Γ(α)KΓ(n + αK)

a
Γ(na(e) + α).
(3)
Here the integral is relative to the Lebesgue measure on the K-dimensional unit simplex
and D(α) = Γ(α)K/Γ(Kα) is the norming constant for the Dirichlet density. Similarly
the conjugacy between the Bernoulli and Beta distributions gives the marginal condi-
tional density of A given e as:
p(A | e) =

[0,1]K(K+1)/2

a≤b
P Oab(e)
ab
(1 −Pab)nab(e)−Oab(e) 
a≤b
P β1−1
ab
(1 −Pab)β2−1
B(β1, β2)
dP
=

a≤b
1
B(β1, β2)B(Oab(e) + β1, nab(e) −Oab(e) + β2),
(4)
where B(x, y) = Γ(x)Γ(y)/Γ(x + y) is the beta-function. The joint density of A and e
is given by the product of (3) and (4), and n−2 times its logarithm is up to a constant
that is free of e equal to

S. L. van der Pas and A. W. van der Vaart
773
QB(e) = 1
n2

1≤a≤b≤K
log B(Oab(e) + β1, nab(e) −Oab(e) + β2) + 1
n2
K

a=1
log Γ(na(e) + α).
(5)
This is a modularity in the sense of Bickel and Chen (2009), which we deﬁne as the
Bayesian modularity. As p(e | A) is proportional to p(e, A), the posterior mode is
equal to the class assignment that maximizes the Bayesian modularity, so the Bayesian
estimator is equal to:
e = argmax
e
QB(e).
(6)
3.3
Similarity to the likelihood modularity
The Bayesian modularity QB(e) consists of two parts, originating from the likelihood
and the prior on the classiﬁcation, respectively. The ﬁrst part is close to the likelihood
modularity given by
QML(e) = 1
n2

1≤a≤b≤K
nab(e) τ
	Oab(e)
nab(e)

,
where τ(x) = x log x + (1 −x) log(1 −x). This criterion, obtained in Bickel and Chen
(2009), results from replacing in the log conditional likelihood of A given e (the logarithm
of (1) with Z replaced by e and discarding the term involving the parameters πa) the
parameters Pab by their maximum likelihood estimators ˆPab = Oab(e)/nab(e). In other
words, the parameters are proﬁled out rather than integrated out as for the Bayesian
modularity. The corresponding estimator
eML = argmax
e
QML(e)
is consistent, and hence one may hope that the Bayesian estimator can be proved con-
sistent by showing that the Bayesian and likelihood modularities are close. This will
indeed be our line of approach, but we shall see that the proximity of the two criteria
is not close enough to explain the strong consistency of the two methods. In particular,
the second, prior part of the Bayesian modularity, resulting from the prior density (3)
over the labels, does play a role in the proof of strong consistency. We discuss this in
more detail at the end of Section 3.5.
The following lemma links the Bayesian and likelihood modularities. The ﬁnal as-
sertion shows that they are at most of the order log n/n apart, which will be seen to be
enough in the proof of weak consistency. For the proof of strong consistency we shall
need the ﬁrst assertion of the lemma, which makes the discrepancy between the two
modularities explicit up to order log n/n2.
Lemma 2. There exists a constant C such that, for E = {1, . . . , K}n the set of all
possible labellings:
max
e∈E
QB(e) −QML(e) −QP (e)
 ≤C log n
n2
,

774
Bayesian Community Detection
for
QP (e) = 1
n2

a:na+⌊α⌋≥2
na(e) log(na(e)) −1
n.
Consequently maxe∈E |QB(e) −QML(e)| = O(log n/n) as n →∞.
3.4
Identiﬁability and consistency
A classiﬁcation e is said to be weakly consistent if the fraction of misclassiﬁed nodes tends
to zero (partial recovery), and strongly consistent if the probability of misclassifying any
of the nodes tends to zero (exact recovery). In deﬁning consistency in a precise manner,
the complication of the possible unidentiﬁability of the labels needs to be dealt with.
From the observed data A we can at best recover the partition of the n nodes in the
K classes with equal labels Zi, but not the values Z1, . . . , Zn of the labels, in the set
{1, 2, . . . , K}, attached to the classes. Thus consistency will be up to a permutation of
labels.
To make this precise deﬁne, for a given permutation (1, . . . , K) →(σ(1), . . . , σ(K)),
the permutation matrix Pσ as the matrix with rows
eT
σ(1)
...
eT
σ(K),
for e1, . . . , eK the unit vectors in RK. Then pre-multiplication of a matrix by Pσ per-
mutes the rows, and post-multiplication by P T
σ the columns: PσR is the matrix with
jth row equal to the σ(j)th row of R, and RP T
σ is the matrix with jth column the
σ(j)th column of R. Thus PσR(e, Z) = R(Pσe, Z) is the matrix that would result
if we would permute the labels of the classes of the assignment e, and PσPP T
σ and
PσR(e, Z)P T
σ = R(Pσe, PσZ) are the matrices that would result if we would relabel the
classes throughout. Since we cannot recover the labels, the matrix PσR(e, Z) is just as
good or bad as R(e, Z) for measuring discrepancy between a labelling e and the true
labelling Z; furthermore, nothing should change if we choose diﬀerent names for the
classes.
Thus, taking into account the unidentiﬁability of the labels, by Lemma 1, we deﬁne
an estimator e to be weakly consistent if
∥PσR(e, Z) −Diag(f(Z))∥1 →0,
for some permutation matrix Pσ. We say the classiﬁcation e is strongly consistent if
P(PσR(e, Z) = Diag(f(Z))) →1,
for some permutation matrix Pσ.
The following lemma shows that the permutation matrix Pσ is for large n uniquely
deﬁned, unless there are empty classes.

S. L. van der Pas and A. W. van der Vaart
775
Lemma 3. If for a given vector π and matrix R, there exist permutation matrices Pσ
and Qσ such that both ∥PσR −Diag(π)∥1 ≤mina πa and ∥QσR −Diag(π)∥1 ≤mina πa,
then Pσ = Qσ.
Proof. Because
the
L1-norm
is
invariant
under
permutations,
we
have
∥R −Pσ
−1Diag(π)∥1 ≤mina πa, and similarly for Qσ. Therefore ∥P −1
σ Diag(π) −
Q−1
σ Diag(π)∥1 ≤2 mina πa, by the triangle inequality. Again by invariance, the left
side of this inequality is equal to ∥(QσP −1
σ )Diag(π) −Diag(π)∥1, which is at least two
times the sum of the two smallest coordinates of π if QσP −1
σ
is not equal to the identity
matrix.
A necessary requirement for consistency is that the classes can be recovered from
the likelihood, i.e. the model parameters must be identiﬁable. If π has strictly positive
coordinates, so that all labels will appear in the data eventually, then as explained in
Bickel and Chen (2009) an appropriate condition is that P does not have two identical
rows. If πa = 0 for some a, then class a will never be consumed; the identiﬁability
condition should then be imposed after deleting the ath column from P. Thus, we call
the pair (P, π) identiﬁable if the rows of P are diﬀerent after removing the columns
corresponding to zero coordinates of π. Throughout we assume that P is symmetric.
3.5
Consistency results and assumptions
We are now ready to present our results on consistency for the Bayesian maximum a
posteriori (MAP) estimator (6). Recall that ρn = 
a,b πaπbPab is the probability of a
new edge, and λn = (n−1)ρn is the expected degree of a node. Theorem 1 shows strong
consistency of the Bayesian estimator if λn ≫(log n)2. The proof rests on a proof of
weak consistency under similar conditions, stated in Section 5 as Theorem 2.
Theorem 1 (strong consistency). If P = ρnS, where either ρn = 1 is ﬁxed or ρn →0,
and (S, π) is ﬁxed and identiﬁable with all entries of P strictly smaller than 1 and
all entries of S being strictly positive, then the MAP classiﬁer e = arg maxe QB(e) is
strongly consistent if ρn ≫(log n)2/n.
The theorem is proven in two steps: ﬁrst for the dense case, where ρn is ﬁxed, and
then for the sparse case, where ρn goes to zero. The second is the most interesting of
the two, as it touches on the question how much information is required to recover the
underlying community structure. Much recent research eﬀort has gone into determining
detection and computational boundaries, in particular for special cases of the SBM with
K = 2 (see e.g. Mossel et al. (2012), Chen and Xu (2014), Abbe et al. (2014) and Zhang
and Zhou (2015)).
Weakly consistent estimation of the class labels for an arbitrary, but known, number
of classes is possible by some method under the assumption λn ≫log n, as this was
shown to hold for spectral clustering by Lei and Rinaldo (2015). Strong consistency
of maximum likelihood was shown to hold in the special cases of planted bisection
(K = 2 and equal community sizes) and planted clustering (equal community sizes and

776
Bayesian Community Detection
Pab can take two values) by Abbe et al. (2014); Chen and Xu (2014), again under the
assumption λn ≫log n. Gao et al. (2015) and Gao et al. (2016) achieve optimality
in diﬀerent senses, under assumptions on the average within-community and between-
community edge probabilities; Gao et al. (2015) introduce a two-stage procedure which
achieves the optimal proportion of misclassiﬁed nodes in a special case where Pab can
only take two values, while Gao et al. (2016) obtain minimax rates for the proportion
of misclassiﬁed nodes in the degree corrected SBM.
Strong consistency of the likelihood modularity for an arbitrary number of classes K
has been claimed under the same assumption λn ≫log n (Bickel and Chen, 2009; Bickel
et al., 2015), and those results have been extended to the degree-corrected SBM (Zhao
et al., 2012). However, these results were obtained by application of an abstract theorem
to the special case of the likelihood modularity, which would require the function τ(x) =
x log x + (1 −x) log(1 −x), or the function σ(x) = x log x, to be globally Lipschitz. As
τ and σ are only locally Lipschitz, it is still unclear whether λn ≫log n is a suﬃcient
condition for either weakly or strongly consistent estimation by maximum likelihood.
From our proof of Theorem 1, which proceeds by comparing the Bayesian modularity
and the likelihood modularity, it follows that λn ≫(log n)2 is certainly suﬃcient. Given
weak consistency the problem can be reduced to a neighbourhood of the true parameter
on which the Lipschitz condition is satisﬁed. However, it is precisely our proof of weak
consistency that needs the additional log n factor.
The Largest Gaps algorithm of Channarond et al. (2012) is strongly consistent pro-
vided that mina̸=b | K
k=1 αk(Pak −Pbk)| is at least of order

log n/n, implying that at
least one of the Pab is of the same order, and thus λn ≫√n log n. This much stronger
condition is not surprising, as the Largest Gaps algorithm only uses the degree of a
node and does not take into account any ﬁner information on the group structure, such
as the information contained in the Oab.
To the best of our knowledge, for K > 2, it remains to be shown that λn ≫log n
is suﬃcient for strong consistency of any community detection method for the general
SBM. For the minimax rate for the proportion of misclustered nodes in community
detection, when only classes of sizes proportional to n are considered, a phase transition
when going from the case K = 2 to K ≥3 was observed by Zhang and Zhou (2015).
Their results show that if K = 2, communities of the same size are most diﬃcult to
distinguish, while if K ≥3, small communities are harder to discover. This shift in the
nature of the communities that are harder to detect may be what has been preventing
a general strong consistency result under the assumption λn ≫log n so far.
While the prior on the class assignment plays a negligible role in the proof of
weak consistency, our argument for strong consistency requires that the prior does
not vary too much in a neighborhood of the truth. To be precise, denote by QB,2(e) =
n−2 K
a=1 log Γ(na(e) + α) the second part of the Bayesian modularity (5), and let Z
be the true labelling. Then we need that for any e that diﬀers from Z by at most m
nodes, the distance |QB,2(e) −QB,2(Z)| is of smaller order than m/n. We thus ﬁnd
that a variation on general posterior contraction results (e.g. Ghosal et al. (2000)) holds
for the SBM as well, namely that the prior mass should be spread homogeneously in a
neighborhood of the truth.

S. L. van der Pas and A. W. van der Vaart
777
The number K of classes is held ﬁxed in the preceding theorem. Our proofs suggest
that consistency is retained if K = Kn →∞and ρn ≫K4
n(log Kn)n−1(log(n/ log Kn))2,
provided the model is asymptotically identiﬁable in a suitable sense. In Theorem 1 iden-
tiﬁability in the case of ﬁxed K is described as a property of the pair (S, π). If Kn →∞,
then the dimensions of these objects tend to inﬁnity and identiﬁability must be deﬁned
in a diﬀerent way. Our proofs suggest that a crucial quality is 
a πaK0(Sab′∥Sa,b),
where K0(s∥s′) is the Kullback–Leibler divergence between two Poisson distributions
with means s and s′. As seen in the proof of Lemma 11, this quantity drives local iden-
tiﬁability. It seems a reasonable assumption that this number be bounded away from
zero, but any type of behaviour is possible as the matrix S will grow in dimension.
A reasonable global identiﬁability condition might be that the left side of Lemma 11
is bounded below by this number, and then the preceding bound on ρn is valid. See
Remark 1 for further discussion.
4
Application
Some options for implementing the Bayesian modularity are given in Section 4.1, after
which the results of applying the Bayesian and likelihood modularities to the well-
studied karate club data of Zachary (1977) are discussed in Section 4.2.
4.1
Implementation
The Bayesian modularity, like the likelihood modularity, requires maximization over all
possible labellings. This is computationally feasible even in large networks, as shown
in two recent works on implementing Bayesian methods for the SBM. McDaid et al.
(2013) followed the approach of Nowicki and Snijders (2001) and added a Poisson prior
on K. After marginalizing over π and P, they employ an allocation sampler to sample
from the joint density of K and z given A, and use the posterior mode to estimate K.
Their algorithm gives access to the full posterior distribution on the node labels and can
scale to networks with approximately ten thousand nodes and ten million edges. Cˆome
and Latouche (2014), claiming that the algorithm of McDaid et al. (2013) suﬀers from
poor mixing properties, propose a greedy inference algorithm for the same problem.
They demonstrate their algorithm on networks ranging in size from one hundred to ten
thousand nodes, and compare the results to a range of other methods, including spectral
clustering.
For the karate club data in Section 4.2, the network was small enough that a tabu
search (Glover, 1989), run for a number of diﬀerent initial conﬁgurations, yielded good
results. This takes a similar amount of time as a tabu search in combination with the
likelihood modularity, as in Bickel and Chen (2009). Although tabu search has been
implemented on large networks consisting of approximately 1000 nodes for the degree-
corrected version of the likelihood modularity (Zhao et al., 2012), we recommend the
use of the methods designed for the stochastic block model proposed by McDaid et al.
(2013) or Cˆome and Latouche (2014) for networks of medium and large sizes.

778
Bayesian Community Detection
Figure 1: Communities detected by the Bayesian modularity when K = 2 (left) and
K = 4 (right), with α = β1 = β2 = 1/2. The polygons contain the two groups the
karate club was split into; the left one is Mr. Hi’s club, the right one is the Oﬃcers’
club. The shapes of the nodes represent the communities selected by the modularities.
Figure made using the igraph package (Csardi and Nepusz, 2006).
4.2
Karate club
Zachary (1977) described a karate club which split into two clubs after a conﬂict over
the price of the karate lessons. The new club was led by Mr. Hi, the karate teacher of
the original club, while the remainder of the old club stayed under the former Oﬃcers’
rule. The data consists of an adjacency matrix for those 34 individuals who interacted
with other club members outside club meetings and classes. Each of these individuals’
aﬃliations after the conﬂict is known.
We used α = 1/2 for the Dirichlet prior, and β1 = β2 = 1/2 for the beta prior. The
communities selected by the Bayesian modularity for K = 2 and K = 4 are given in
Figure 1. In both instances, the tabu search led to nearly the same solution for both
the Bayesian and likelihood modularities, only diﬀering at one node for K = 4, which
is not surprising in light of Lemma 2. For K = 2, the results of Bickel and Chen (2009)
for this data set are recovered. For K = 4, the partition in Figure 1 yields a higher
value of the likelihood modularity than the partition into four classes found by Bickel
and Chen (2009), and an even higher value is obtained by switching club member 20
to the second-largest class. This discrepancy is likely due to the heuristic nature of the
tabu search algorithm, and for the same reason, it may be the case that improvement
over the partitions found by the Bayesian modularity in Figure 1 are possible.
For K = 2, the communities found by the algorithms do not correspond in the
slightest to the two karate clubs, instead grouping the nodes with the highest degrees,
corresponding to Mr. Hi, the president of the original club, and their closest supporters,

S. L. van der Pas and A. W. van der Vaart
779
together. Incidentally, this partition is the same as the one returned by the Largest
Gaps algorithm of Channarond et al. (2012), which solely uses the degrees of the nodes
and discards all other information.
These bad results are no reason to shelve the Bayesian and likelihood modularities,
as there is no reason to believe that the two karate clubs form communities in the
sense of the stochastic block model. Mr. Hi and the club’s president are clear outliers
within their groups, and neither of the algorithms were designed to be robust to such
a phenomenon. The communities selected by the modularities are communities in the
sense that they form connections within and between the groups in a similar fashion.
This sense does not correspond to the social notion of a community in this setting.
The results for four classes unify the social and stochastic senses of community.
The prominent members of each of the new clubs are placed into two separate, small,
communities. The other members are classiﬁed nearly perfectly, with two exceptions.
However, one of those exceptional individuals is the only person described by Zachary
(1977) as being a supporter of the club’s president before the split, who joined Mr.
Hi’s club, making this person’s aﬃliation up for debate. The second is described as
only a weak supporter of Mr. Hi. The increased number of communities allows for some
outliers within the social communities, and leads to a more detailed understanding of
the dynamics within both of the groups. We essentially recover the two communities,
each with a core that is more connective than the remainder of the nodes.
5
Weak consistency
The proof of Theorem 1 is built on our proof of weak consistency of the Bayesian
modularity, which we present here. The following quantities will be used in the course
of multiple proofs. The function HP , with domain K × K probability matrices, is given
by, for τ(u) = u log u + (1 −u) log(1 −u),
HP (R) = 1
2

a,b
(R1)a(R1)b τ
 (RPRT )ab
(R1)a(R1)b

.
(7)
For τ0(u) = u log(u) −u, deﬁne
GP (R) = 1
2

a,b
(R1)a(R1)b τ0
	 (RPRT )ab
(R1)a(R1)b

.
The sums deﬁning these functions are over all pairs (a, b) with 1 ≤a, b ≤K, unlike the
sums deﬁning the modularities QB and QML, which are restricted to a ≤b.
We write diag (P) for the diagonal of P if P is a matrix, and Diag(f) for the diagonal
matrix with diagonal f if f is a vector.
Theorem 2 (weak consistency). If P = ρnS where either ρn = 1 is ﬁxed or ρn →0, and
(S, π) is ﬁxed and identiﬁable, then the MAP classiﬁer e = arg maxz QB(e) is weakly
consistent provided nρn ≫(log n)2.

780
Bayesian Community Detection
Proof. By Lemma 2 the Bayesian modularity QB is equivalent to the likelihood mod-
ularity QML up to order (log n)/n. With the notation Oab(e) = Oab(e) if a ̸= b, and
Oab(e) = 2Oab(e) if a = b, the likelihood modularity is in turn equivalent up to the
same order to
L(e) =
1
2n2

a,b
na(e)nb(e) τ
	
Oab(e)
na(e)nb(e)

.
(8)
Indeed the terms of QML(e) for a < b are identical to the sums of the terms of L(e)
for a < b and a > b, while for a = b the terms of QML(e) and L(e) diﬀer only subtly:
the ﬁrst uses naa(e) =
1
2na(e)(na(e) −1), where the second uses
1
2na(e)2. Thus the
diﬀerence is bounded in absolute value by the sum over a of (where e is suppressed from
the notation)
 n2
a
2n2 τ
	 Oaa
n2a

−na

na −1)
2n2
τ
	
Oaa
na(na −1)

 ≤1
2n∥τ∥∞+ n2
a
2n2 l
	
Oaa
n2a(na −1)

,
where l(x) = 2x(1 ∨log(1/x)), in view of Lemma 6. We now use that nal(u/na) ≲
log na ≤log n, for 0 ≤u ≤1.
Combining the preceding, we conclude that
ηn,1 := max
e
|L(e) −QB(e)| = O
log n
n

.
Since QB(e) ≥QB(Z), by the deﬁnition of e, it follows that L(e)−L(Z) ≥−2ηn,1. The
next step is to replace L in this equality by an asymptotic value.
For x equal to a big multiple of (∥P∥1/2
∞∨n−1/2)/n1/2, the right side of Lemma 4
tends to zero and hence maxe ∥O(e) −E( O(e) | Z)∥∞/n2 is of this order in probability.
We also have, by Lemma 5:
max
e
 1
n2 E
 O(e) | Z

−R(e, Z)PR(e, Z)T 
∞= max
e
1
n
Diag(R(e, Z) diag (P))

∞
= O
	ρn
n

,
as the row sums of the matrix R(e, Z) are bounded above by one. By Lemma 6,
|vτ(x/v) −vτ(y/v)| ≤l(|x −y|), uniformly in v ∈[0, 1], where l(x) = 2x(1 ∨log(1/x)).
It follows that
ηn,2 := max
e
L(e) −L(e)
 = oP
	
l
	∥P∥1/2
∞∨n−1/2
n1/2


,
for
L(e) = 1
2

a,b
fa(e)fb(e) τ
	(R(e, Z)PR(e, Z)T )ab
fa(e)fb(e)

.
Combining this with the preceding paragraph, we conclude that L(e) ≥L(Z)−2(ηn,1 +
ηn,2). Since L(e) = HP (R(e, Z)) for every e and HP as deﬁned in (7), and R(Z, Z) =
Diag(f(Z)) = Diag(R(e, Z)T 1), this can be translated into
HP (Diag(R(e, Z)T 1)) −HP (R(e, Z)) ≤2(ηn,1 + ηn,2).
(9)
We complete the proof separately for the cases that ρn is ﬁxed or tends to zero.

S. L. van der Pas and A. W. van der Vaart
781
For given δ > 0, let Rδ be the set of all probability matrices R with
min
Pσ
PσR −Diag(RT 1)

1 ≥δ,
and
min
a:πa>0(RT 1)a ≥δ.
Here the minimum is taken over the (ﬁnite) set of all permutation matrices Pσ on K
labels. Furthermore, set
η := inf
R∈Rδ

HP

Diag(RT 1)

−HP (R)

.
Because Rδ is compact and the maps R →HP (R) and R →Diag(RT 1) are continuous,
the inﬁmum in the display is assumed for some R ∈Rδ. Because no R ∈Rδ can
be transformed into a diagonal element by permuting rows and every R ∈Rδ has
a nonzero element in every column a with πa > 0, Lemma 7 shows that η > 0. If
2(ηn,1+ηn,2) is smaller than η, then it follows from (9) that R(e, Z) cannot be contained
in Rδ. Since R(e, Z)T 1 = f(Z)
P→π, by the law of large numbers, for suﬃciently
small δ > 0 this must be because R(e, Z) fails the ﬁrst requirement deﬁning Rδ. That
is, ∥PσR(e, Z) −Diag(f(Z))∥1 ≤δ for some permutation matrix Pσ. As this is true
eventually for any δ > 0, it follows that minPσ ∥PσR(e, Z) −Diag(π)∥1
P→0.
Finally we consider the case where ρn →0. In view of Lemma 8, the number η = ηn,
which now depends on n, is now bounded below by ρn times a positive number that
depends on (S, π). The preceding argument goes through provided ηn,1+ηn,2 is of smaller
order than ηn. This leads to l(

ρn/n)+log(n)/n ≪ρn, or (ρn/n) log2(n/(ρn∥S∥∞)) ≪
ρ2
n.
Remark 1. If Kn →∞, then the numbers ηn,2 in the preceding proof need to be
adapted to ηn,2 ≍K2
nl(xn + ρn/n), for xn a big multiple of (log Kn/n)1/2(ρ1/2
n
∨
(log Kn/n)1/2). Equation (9) remains valid. Rather than referring to the identiﬁability
lemma, Lemma 8, we would now wish to lower bound the left side of (9) by a multiple
of n−1 n
i=1 1ˆei̸=Zi. The proof of Lemma 11 combined with Lemma 1 shows that lo-
cally the left side of (9) is bounded below by a multiple of ρn

a πaK0(Sab′∥Sab)n−1 ×
n
i=1 1ˆei̸=Zi. If this is also globally true, then we obtain consistency as announced at
the end of Section 3.5.
Lemmas 4–8 are more precise, or, in case of Lemma 7, corrected versions of lemmas
from Bickel and Chen (2009); Zhao et al. (2012); Bickel et al. (2015), supporting the
weak consistency theorem.
Lemma 4. Let Oab(e) = Oab(e) if a ̸= b, and Oab(e) = 2Oab(e) if a = b. For any
x > 0,
P
	
max
e
 O(e) −E
 O(e) | Z

∞> xn2
≤2Kn+2e−x2n2/(8∥P ∥∞+4x/3).
Proof. This Lemma is adapted from Lemma 1.1 in Bickel and Chen (2009). There are
Kn possible values of e and ∥· ∥∞is the maximum of the K2 entries in the matrix.

782
Bayesian Community Detection
We use the union bound to pull these maxima out of the probability, giving the factor
Kn+2 on the right. Next it suﬃces to bound the tail probability of each variable
Oab(e) −E
 Oab(e) | Z

=

i,j

Aij −E(Aij | Z)

(1{ei = a, ej = b} + 1{ei = b, ej = a}).
The nab(e) variables in this sum are conditionally independent given Z, take values in
[−2, 2], and have conditional mean zero given Z and conditional variance bounded by
4 var(Aij | Z) ≤4PZiZj(1 −PZiZj) ≤4∥P∥∞. Thus we can apply Bernstein’s inequality
to ﬁnd that
P
	 Oab(e) −E
 Oab(e) | Z
 > xn2
≤2e−x2n4/(8nab(e)∥P ∥∞+4xn2/3).
Finally we use the crude bound nab(e) ≤n2 and cancel one factor n2.
Lemma 5. Deﬁne Oab(e) = Oab(e) if a ̸= b, and Oab(e) = 2Oab(e) if a = b. Then, for
R(e, Z) as deﬁned in (2),
E( Oab | Z) = n2R(e, Z)PR(e, Z)T −nDiag(R(e, Z) diag (P)).
Proof. A similar expression, not taking into account the absence of self-loops, appears
in Bickel and Chen (2009). The relevant computation for our situation is as follows:
E( Oab(e) | Z = c) =

i̸=j
Pcicj1{ei = a, ej = b}
=

a′,b′
Pa′b′

i̸=j
1{ci = a′, cj = b′}1{ei = a, ej = b}
=

a′,b′
Pa′b′

i,j
1{ci = a′, cj = b′}1{ei = a, ej = b}
−δab

i

a′
Pa′a′1{ci = a′}1{ei = a}
= n2 
a′,b′
Pa′b′Raa′(e, c)Rbb′(e, c) −δabn

a′
Pa′a′Raa′(e, c).
Lemma 6. The function τ : [0, 1] →R satisﬁes |τ(x) −τ(y)| ≤l(|x −y|), for l(x) =
2x(1 ∨log(1/x)).
Proof. Write the diﬀerence between x log x and y log y as |
 y
x (1+log s) ds|. The function
s →1 + log s is strictly increasing on [0, 1] from −∞to 1 and changes sign at s = e−1.
Therefore the absolute integral is bounded above by the maximum of
−
 |x−y|∧e−1
0
(1 + log s) ds = −(|x −y| ∧e−1) log

|x −y| ∧e−1
and
 1
1−|x−y|∨e−1(1 + log s) ds ≤|x −y|.

S. L. van der Pas and A. W. van der Vaart
783
Lemma 7. For any probability matrix R,
HP (R) ≤HP (Diag(RT 1)

.
(10)
Furthermore, if (P, π) is identiﬁable and the columns of R corresponding to positive
coordinates of π are not identically zero, then the inequality is strict unless PσR is a
diagonal matrix for some permutation matrix Pσ.
Proof. This Lemma is related to the proof that the likelihood modularity is consistent
given in Bickel and Chen (2009). This proof however rests on their incorrect Lemma
3.1, and thus we provide full details on how the argument can be adapted to avoid the
use of their Lemma 3.1 altogether.
For R a diagonal matrix the numbers (RPRT )ab/(R1)a(R1)b reduce to Pab. Conse-
quently, by the deﬁnition of HP ,
HP

Diag(f)

=

a,b
fafb τ(Pab).
(11)
For a general matrix R, by inserting the deﬁnition of τ,
HP (R) =

a,b
(RPRT )ab log (RPRT )ab
(R1)a(R1)b
+

a,b

(R1)a(R1)b −(RPRT )ab

log
	
1 −(RPRT )ab
(R1)a(R1)b

.
Because (R1)a(R1)b −(RPRT )ab = (R(1 −P)RT )ab, with 1 the (K × K)-matrix with
all coordinates equal to 1, we can rewrite this as

a,b

a′,b′
Raa′Rbb′

Pa′b′ log (RPRT )ab
(R1)a(R1)b
+ (1 −Pa′b′) log
	
1 −(RPRT )ab
(R1)a(R1)b


.
By the information inequality for two-point measures, the expressions in square brackets
become bigger when (RPRT )ab/(R1)a(R1)b is replaced by Pa′b′, with a strict increase
unless these two numbers are equal. After making this substitution the term in square
brackets becomes τ(Pa′b′), and we can exchange the order of the two (double) sums and
perform the sum on (a, b) to write the resulting expression as

a′,b′
(RT 1)a′(RT 1)b′τ(Pa′b′) = HP

Diag(RT 1)

.
This proves the ﬁrst assertion (10) of the lemma.
If R attains equality, then also for every permutation matrix Pσ, by the equality
HP (PσR) = HP (R) and the fact that (PσR)T 1 = RT 1, we have
HP (PσR) = HP

Diag((PσR)T 1)

.
(12)

784
Bayesian Community Detection
We shall show that if R satisﬁes this equality and PσR has a positive diagonal, then
PσR is in fact diagonal. Furthermore, we shall show that there exists Pσ such that PσR
has a positive diagonal.
Fix some (Pσ)m that maximizes the number of positive diagonal elements of PσR
over all permutation matrices Pσ, and denote ¯R = (Pσ)mR. Because the information
inequality is strict, the preceding argument shows that (12) can be true for Pσ = (Pσ)m
(giving PσR = ¯R) only if
Pa′b′ = ( ¯RP ¯RT )ab
( ¯R1)a( ¯R1)b
,
whenever ¯Raa′ ¯Rbb′ > 0.
(13)
Denote the matrix on the right of the equality by Q.
If ¯R has a completely positive diagonal, then we can choose a = a′ and b = b′ and
ﬁnd from (13), that Pab = Qab, for every a, b. If also ¯Raa′ > 0, then we can also choose
b = b′ and ﬁnd that Pa′b = Qab, for every b. Thus the ath and a′th rows of P are
identical. Since all rows of P are diﬀerent by assumption, it follows that no a ̸= a′ with
¯Raa′ > 0 exists.
If ¯R does not have a fully positive diagonal, then the submatrix of ¯R obtained by
deleting the rows and columns corresponding to positive diagonal elements must be
the zero matrix, since otherwise we might permute the remaining rows and create an
additional nonzero diagonal element, contradicting that (Pσ)m already maximized this
number. If I and Ic are the sets of indices of zero and nonzero diagonal elements, then
the preceding observation is that ¯Rij is zero for every i, j ∈I. If π > 0, then we need to
consider only R with nonzero columns. For i ∈I a nonzero element in the ith column
of ¯R must be located in the rows with label in Ic: for every i ∈I there exists ki ∈Ic
with ¯Rkii > 0. Then, for i, j ∈I,
(1) for a = ki, b = kj, a′ = i, b′ = j, (13) implies Qkikj = Pij.
(2) for a = ki, b ∈Ic, a′ = i, b′ = b, (13) implies Qkib = Pib.
(3) for a = ki, b ∈Ic, a′ = ki, b′ = b, (13) implies Qkib = Pkib.
We combine these three assertions to conclude that, for a, i ∈I and b ∈Ic,
Pai = Pia
(1)
= Qkika
(2)
= Pika = Pkai,
Pab
(2)
= Qkab
(3)
= Pkab.
Together these imply that the ath and the kath row of P are equal. Since by assumption
they are not (if π > 0), this case can actually not exist (i.e. k = 0).
Finally if πa = 0 for some a, then we follow the same argument, but we match only
every column i ∈I with πi > 0 to a row ki ∈Ic. By the assumption on R such ki exist,
and the construction results in two rows of P that are identical in the coordinates with
πa > 0.

S. L. van der Pas and A. W. van der Vaart
785
Lemma 8. For any ﬁxed (K × K)-matrix P with elements in [0, 1], uniformly in prob-
ability matrices R, as ρn →0,
1
ρn
	
HρnP (Diag(RT 1)

−HρnP (R)

→GP (Diag(RT 1)

−GP (R).
(14)
Furthermore, if (P, π) is identiﬁable and the columns of R corresponding to positive
coordinates of π are not identically zero, then the right side is strictly positive unless
SR is a diagonal matrix for some permutation matrix S.
Proof. From the fact that |(1 −u) log(1 −u) + u| ≤u2, for 0 ≤u ≤1, it can be veriﬁed
that, |ρ−1
n τ(ρnu) −(u log ρn + τ0(u))| ≤ρn →0, uniformly in 0 ≤u ≤1. It follows that,
uniformly in R,
1
ρn
HρnP (R) = log ρn

a,b
(RPRT )ab +

a,b
(R1)a(R1)bτ0
	 (RPRT )ab
(R1)a(R1)b

+ O(ρn).
The ﬁrst term on the right is equal to log ρn(RT 1)T P(RT 1), and hence is the same for
R and Diag(RT 1). Thus this term cancels on taking the diﬀerence to form the left side
of (14), and hence (14) follows.
The right side of (14) is nonnegative, because the left side is, by Lemma 7. This fact
can also be proved directly along the lines of the proof of Lemma 7, as follows. Write
GP (R) =

a,b

a′,b′
Raa′Rbb′

Pa′b′ log (RPRT )ab
(R1)a(R1)b
−(RPRT )ab
(R1)a(R1)b

.
By the information inequality for two Poisson distributions the term in square brack-
ets becomes bigger if (RPRT )ab/(R1)a(R1)b is replaced by Pa′b′. It then becomes
τ0(Pa′b′) and the double sum on (a, b) can be executed to see that the resulting bound is
GP (Diag(RT 1)). Furthermore, the inequality is strictly unless (13) holds, with ¯R = R.
Since also GP (PσR) = GP (R), for every permutation matrix Pσ, the ﬁnal assertion of
the lemma is proved by copying the proof of Lemma 7.
Proof of Lemma 2
Proof. The second assertion of the lemma follows from the ﬁrst and the fact that
maxe QP (e) ≲(log n)/n. It suﬃces to prove the ﬁrst assertion.
Recall that the Bayesian modularity is given by n−2 times
n2QB(e) =

a≤b
log B

Oab(e) + 1
2, nab(e) −Oab(e) + 1
2

+

a
log Γ(na(e) + α).
(15)
We shall show that the ﬁrst sum on the right is equivalent to QML(e), and the second
sum is equivalent to QP (e). We show this by comparing the sums deﬁning the vari-
ous modularities term by term. For clarity we shall suppress the argument e. We will
repeatedly use the following bound from (Robbins, 1955): for n ∈N≥1,
Γ(n + 1) =
√
2πnn+1/2e−nean,
(16)

786
Bayesian Community Detection
with (12n+1)−1 ≤an ≤(12n)−1, as well as the fact that Γ(s) is monotone increasing for
s ≥3/2. In addition, we will bound remainder terms by using the inequality x log((x +
c)/x) ≤c for c ≥0 and the fact that x log((x −1)/x) is bounded for x > 1.
First sum of (15)
Upper bound, case 1: Oab ̸= 0 and nab ̸= Oab. We apply (16):
logB(Oab + β1, nab −Oab + β2) ≤log Γ(Oab + ⌊β1⌋+ 1)Γ(nab −Oab + ⌊β2⌋+ 1)
Γ(nab + ⌊β1 + β2⌋)
= Oab log

Oab + ⌊β1⌋
nab + ⌊β1 + β2⌋−1

+ (nab −Oab) log
 nab −Oab + ⌊β2⌋
nab + ⌊β1 + β2⌋−1

+ (⌊β1⌋+ 1/2) log(Oab + ⌊β1⌋) + (⌊β2⌋+ 1/2) log(nab −Oab + ⌊β2⌋)
−(⌊β1 + β2⌋−1/2) log(nab + ⌊β1 + β2⌋−1) + log
√
2π −⌊β1⌋−⌊β2⌋
+ ⌊β1 + β2⌋−1 + αab + βab −γab,
where αab, βab and γab are bounded by constants. By the inequality x log((x+c)/x) ≤c
for c ≥0, and the fact that x log((x −1)/x) is bounded for x > 1, we ﬁnd the upper
bound:
log B(Oab + β1, nab −Oab + β2) ≤nabτ
Oab
nab

+ O(log nab).
Upper bound, case 2: nab = 1 and Oab = 0 or nab = Oab, or nab = 0. In both cases,
the corresponding term of the likelihood modularity vanishes, whereas the contribution
of the Bayesian modularity is either log B(1 + β1, β2), log(β1, 1 + β2), or log B(β1, β2).
Upper bound, case 3: nab ≥2 and Oab = 0 or nab = Oab. Again, the corresponding
term of the likelihood modularity vanishes. We show the computations for the case
nab = Oab; for the case Oab = 0, switch β1 and β2. By (16):
logB(Oab + β1, nab −Oab + β2) = log B(nab + β1, β2) ≤log Γ(nab + ⌊β1⌋+ 1)Γ(β2)
Γ(nab + ⌊β1 + β2⌋)
= (nab + ⌊β1⌋) log

nab + ⌊β1⌋
nab + ⌊β1 + β2⌋

+ (1/2) log(nab + ⌊β1⌋)
−(⌊β1 + β2⌋+ 1/2) log(nab + ⌊β1 + β2⌋) + log Γ(β2) + ⌊β1 + β2⌋−1 + δab −ϵab,
where δab and ϵab are bounded by constants. Arguing as before, the ﬁrst term is bounded,
while the remainder is of order log(nab). A lower bound is found analogously.
Lower bound. The computations for the lower bound are completely analogous, ex-
cept that we require Oab + β1 ≥2 and nab −Oab + β2 ≥2. We study four cases. The
cases (1) Oab ≥2 and nab −Oab ≥2, (2) nab = 0 and (3) nab > 0 and nab = Oab or
Oab = 0 are similar to cases 1, 2 and 3 respectively of the upper bound. The fourth case
is nab −Oab = 1 and Oab ≥2, or Oab = 1 and nab −Oab ≥1. In both instances, the like-
lihood modularity is equality to a bounded term minus log nab. By similar calculations
as before, the Bayesian modularity is of the order log nab as well.

S. L. van der Pas and A. W. van der Vaart
787
Conclusion. We ﬁnd:

a≤b
log B(Oab + β1, nab −Oab + β2) =

a≤b
nabτ
Oab
nab

+ O(log n).
Second sum of (15)
We consider three cases. If na + ⌊α⌋= 0, then α > 0, implies na = 0, in which
case log Γ(na + α) = log Γ(α), which is bounded. In case na + ⌊α⌋= 1, the term
log Γ(na + α) is equal to either log Γ(1 + α) or log Γ(α) and thus bounded as well. For
the case na + ⌊α⌋≥2, we study the upper bound Γ(na + α) ≤Γ(na + ⌊α⌋+ 1) and the
lower bound Γ(na + α) ≥Γ(na + ⌊α⌋). By applying (16) in both cases, we conclude:

a
log Γ(na + α) =

a:na+⌊α⌋≥2
na log na −n + O(log n).
6
Strong consistency
We build upon the foundations from the previous Section to prove Theorem 1. We need
slightly adapted versions of the function HP , given by, with δab equal to 1 or 0 if a = b
or not,
HP,n(R) = 1
2

a,b
(R1)a

(R1)b −δab/n

τ
	(RPRT )ab −δab

k PkkRka/n
(R1)a

(R1)b −δab/n


.
(17)
For given functions tab : [0, 1] →R, let X(e) be the K × K matrix with entries
Xab(e) = tab
	 Oab(e)
n2

−tab
	E( Oab(e) | Z)
n2

.
(18)
Proof of Theorem 1 [strong consistency]
Proof. We ﬁrst prove the statement in case ρn is ﬁxed. By Theorem 2, e is weakly con-
sistent, and hence with probability tending to one it belongs to the set of classiﬁcations e
such that the fractions f(e) are close to π, and the matrices R(e, Z) are close to Diag(π)
after the appropriate permutation of the labels (that is, of rows of R(e, Z)). Therefore, it
is no loss of generality to assume that e is restricted to this set. By Lemmas 4 and 5, the
matrices O(e)/n2 are then close to R(e, Z)PR(e, Z)T →Diag(π)PDiag(π), and hence
are bounded away from zero and one if P has this property.
If e and Z diﬀer at m nodes, then e belongs to the set of e with ∥R(Z, Z)−R(e, Z)∥1 =
m(2/n), by Lemma 1. In that case QB(e) ≥QB(Z), for some e in this set, and hence by
Lemma 2 QML(e) −QML(Z) + QP (e) −QP (Z) ≥−ηn, for some ηn of order (log n)/n2.
It follows that:

788
Bayesian Community Detection

QML(e) −HP,n

R(e, Z)

−

QML(Z) −HP,n

R(Z, Z)

≥HP,n

R(Z, Z)

−HP,n

R(e, Z)

−|QP (e) −QP (Z)| −ηn.
(19)
The ﬁrst term on the right is bounded below by a multiple of m/n, by Lemmas 9
and 1. Because (x + α) log x −(y + α) log y =
 y
x (log s + (s + α)/s) ds is bounded in
absolute value by a multiple of |x −y| log(x ∨y), if α ≥0 and x, y > 0, the second
term −|QP (e) −QP (Z)| is bounded below by a multiple of m(log n)/n2, which is of
smaller order than m/n. We conclude that the left side of (19) is bounded below by
C1m/n. The left side is 
a,b(Xab(e)−Xab(Z)), for X deﬁned in (18) and t the function
with coordinates tab(o) = fa(e)(fb(e) −δab/n)τ(o/fa(e)(fb(e) −δab/n)). Because we
restrict e to classiﬁcations such that Oab(e)/nab(e) and fa(e)fb(e) are bounded away
from zero and one, only the values of the function τ on an open interval strictly within
(0, 1) matter. On any such interval τ has uniformly bounded derivatives, and hence the
bound of Lemma 12 is valid. Thus we ﬁnd that
Pr

#(i : ei ̸= Zi) = m

≤Pr
	
sup
e:#(i:ei̸=Zi)≤m
X(e) −X(Z)

∞≥C1m
n

≤C2Km
n
m

e−cm2/(m∥P ∥∞/n+m/n)
≤C2em log(Kne/m)−c1mn.
The sum of the right side over m = 1, . . . , n tends to zero.
In case ρn →0, we follow the some proof, but in (19) use that HP,n(R(Z, Z)) −
HP,n(R(e, Z)) ≥ρnC∥R(Z, Z) −R(e, Z)∥1 ≥ρnC2m/n, by Lemma 11. Since ρn ≫
(log n)/n by assumption, we have that the contribution m(log n)/n2 of QP (e) −QP (Z)
is still negligible and hence ρnC2m/n is a lower bound for the left side of (19). As a
bound on the left side of the preceding display, we then obtain
n

m=1
Km
n
m

e−c2ρ2
nm2/(mρn/n+ρnm/n) ≤
n

m=1
em log(Kne/m)−c3ρnmn.
This sum tends to zero provided that nρn ≫log n.
Lemmas 9–11 are explicit veriﬁcations of versions of condition IIIc of Bickel and
Chen (2009).
Lemma 9. If P is ﬁxed and symmetric, (P, π) is identiﬁable and 0 < P < 1, then, for
suﬃciently small δ > 0,
lim inf
n→∞
inf
0<∥R−Diag(π)∥<δ
HP,n

Diag(RT 1)

−HP,n(R)
∥Diag(RT 1) −R∥
> 0.
(20)
Proof. We can reparametrize the K ×K matrices R by the pairs (RT 1, R−Diag(RT 1)),
consisting of the K vector f = RT 1 and the K × K matrix R −Diag(RT 1). The latter
matrix is characterized by having nonnegative oﬀ-diagonal elements and zero column

S. L. van der Pas and A. W. van der Vaart
789
sums, and can be represented in the basis consisting of all K × K matrices Δbb′, for
b ̸= b′, deﬁned by: (Δbb′)b′b′ = −1, (Δbb′)bb′ = 1 and (Δbb′)aa′ = 0, for all other entries
(a, a′), i.e. the b′th column of Δbb′ has a 1 in the bth coordinate and a −1 on the b′th
coordinate and all its other columns are zero. Given any matrix R ≥0 the matrix
R −Diag(RT 1) can be decomposed as
R −Diag(RT 1) =

b̸=b′
λbb′Δbb′,
for λbb′ = Rbb′ ≥0. Since every Δbb′ has exactly one nonzero oﬀ-diagonal element,
which is equal to 1, and in a diﬀerent location for each b ̸= b, the sum of the oﬀ-diagonal
elements of the matrix on the right side is 
b,b′ λbb′. Because the sum of all its elements
is zero, it follows that its sum of absolute elements is given by ∥R −Diag(RT 1)∥1 =
2 
b̸=b′ λbb′.
Thus we obtain a further reparametrization R ↔(f, λ), in which R = Diag(f) +

b̸=b′ λbb′Δbb′. Here the vector f is a probability vector, and all λbb′ are nonnegative (as
Rbb′ ≥0). The nonnegativity of the diagonal elements of R gives the further restrictions
that 
b̸=a λba ≤fa, for every a; in particular λba is zero for every b and a such that
fa = 0. Other restrictions on the λbb′ follow from the fact that R ≤1, but since we shall
be interested in λbb′ close to zero, these restrictions will not be active.
For given P, f and n, deﬁne the function
G(λ) = HP,n
	
Diag(f) +

b̸=b′
λbb′Δbb′

.
Then we would like to show that there exists C such that
HP,n(Diag(RT 1)) −HP,n(R)
∥R −Diag(RT 1)∥1
= G(0) −G(λ)
2 
b̸=b′ λbb′ ≥C > 0,
for every f in a neighbourhood of π, λ in a neighbourhood of 0 intersected with {λ :
λ ≥0} and ∩a{λ : 
b̸a λba ≤fa}, and every suﬃciently large n. The numerator in the
quotient is g(0)−g(1) for the function g(s) = G(sλ). Writing this diﬀerence in the form
−g′(0) −
 1
0 (g′(s) −g′(0)) ds gives that the numerator is equal to
−∇G(0)T λ −
 1
0

∇G(sλ) −∇G(0)
T ds λ.
(21)
Here ∇G is the gradient of G, where we only include partial derivatives with respect
to coordinates λbb′ that vary freely, i.e. not the coordinates λba for which fa = 0. It
suﬃces to show that the ﬁrst term is bounded below by a multiple of ∥λ∥1 and that the
second is negligible relative to the ﬁrst, as n →∞, uniformly in f in a neighbourhood
of π and λ in a neighbourhood of 0 intersected with {λ : λ ≥0}. Thus it is suﬃcient to
show ﬁrst that for every coordinate λbb′ of λ minus the partial derivative of G at λ = 0
with respect to λbb′ is bounded away from 0, as n →∞uniformly in f, and second that
every partial derivative is equicontinuous at λ = 0 uniformly in f and large n.

790
Bayesian Community Detection
We have
G(λ) = 1
2

a,a′
fa(λ)

fa′(λ) −δaa′/n

τ
	
R(λ)PR(λ)T 
aa′ −δaa′ea(λ)/n
fa(λ)

fa′(λ) −δaa′/n


,
(22)
for
f(λ) = f +

b̸=b′
λbb′(Δbb′1),
R(λ) = Diag(f) +

b̸=b′
λbb′Δbb′,
ea(λ) =

k
PkkRak(λ) = Paafa +

b̸=b′
Pb′b′λbb′(δab −δab′).
By a lengthy calculation, given in Lemma 10,
∂
∂λbb′ G(λ)|λ=0 = −

a
faK(Pab′∥Pab) + 1
2nK(Pb′b′∥Pbb),
(23)
for K(p∥q) = p log(p/q) + (1 −p) log((1 −p)/(1 −q)) the Kullback–Leibler divergence
between the Bernoulli distributions with success probabilities p and q. For f suﬃciently
close to π the numbers fa such that πa > 0 are bounded away from zero, and hence

a faK(Pab′∥Pab) > 0, by identiﬁability of (P, π), since it suﬃces that just one of
the terms of the sum is nonzero. The whole expression is bounded below by the min-
imum over (b, b′) of these numbers minus (2n)−1 times the maximum of the numbers
K(Pb′b′∥Pbb), and hence is positive and bounded away from zero for suﬃciently large n.
To verify the equicontinuity in f of the partial derivatives, we can compute these
explicitly at λ and take their limit as n →∞. We omit the details of this calculation.
However, we note that every term of G(λ) is a ﬁxed function of the quadratic forms in λ

fa +

b̸=b′
λbb′(Δbb′1)a

fa′ +

b̸=b′
λbb′(Δbb′1)a′ −δaa′/n

,
(24)
	
Diag(f) +

b̸=b′
λbb′Δbb′
P

Diag(f) +

b̸=b′
λbb′ΔT
bb′

aa′
−δaa′
n

Paafa +

b̸=b′
Pb′b′λbb′(δab −δab′)

.
(25)
These forms are obviously smooth in λ, and their dependence and that of their deriva-
tives on n is seen to vanish as n →∞. For f and λ restricted to neighbourhoods of π
and 0, the values of the quadratic forms are restricted to a domain in which the trans-
formation that maps them into G(λ) is continuously diﬀerentiable. Thus the desired
equicontinuity follows by the chain rule.
Lemma 10. The partial derivatives of the function G at 0 deﬁned by (22) are given by
(23).
Proof. For given diﬀerentiable functions u and v the map ϵ →u(ϵ)τ(v(ϵ)/u(ϵ)) has
derivative v′ log(v/(u −v)) −u′ log(u/(u −v)). We apply this for every given pair (a, a′)

S. L. van der Pas and A. W. van der Vaart
791
to the functions u and v obtained by taking λbb′ in (24) and (25) equal to ϵ and all
other coordinates of λ equal to zero. Then
u(0) = fa(fa′ −δaa′/n),
v(0) = fa(fa′ −δaa′/n)Paa′,
u′(0) = (Δbb′1)a(fa′ −δaa′/n) + fa(Δbb′1)a′,
v′(0) = (Δbb′P)aa′fa′ + fa(Δbb′P)a′a −(δaa′/n)Pb′b′(δab −δab′).
It follows that v(0)/(u(0) −v(0))
=
Paa′/(1 −Paa′), and u(0)/(u(0) −v(0))
=
1/(1 −Paa′). Hence in view of (17) the partial derivative in (23) is equal to
1
2

a,a′

v′(0) log
Paa′
1 −Paa′ −u′(0) log
1
1 −Paa′

.
We combine this with the equalities
(Δbb′1)a =
⎧
⎪
⎨
⎪
⎩
0
if a /∈{b, b′},
−1
if a = b′,
1
if a = b,
(Δbb′P)aa′ =
⎧
⎪
⎨
⎪
⎩
0
if a /∈{b, b′},
−Pb′a′
if a = b′,
Pb′a′
if a = b.
If fa or fa′ are zero, then the method to obtain the values found for v(0)/(u(0) −v(0))
and u(0)/(u(0) −v(0)) in the preceding (substituting the given values of v(0) and u(0))
breaks down as we obtain a quotient of zeros. However, the values obtained are still
correct when interpreted as the limits from the right at 0. In (21) and (23) the gradient
∇G(0) and derivative at λ = 0 may also be interpreted as limits from the right as λ ↓0
of the gradient. With this substitution the arguments go through. If both fa and fa′
are zero, the term involving (a, a′) disappears completely from the analysis.
Lemma 11. If S is ﬁxed and symmetric, (S, π) is identiﬁable and S > 0 coordinatewise,
then there exists C > 0 such that, for suﬃciently small δ > 0 and any ρn ↓0,
lim inf
n→∞
inf
0<∥R−Diag(π)∥<δ
HρnS,n

Diag(RT 1)

−HρnS,n(R)
ρn∥Diag(RT 1) −R∥
≥C.
Proof. In the notation of the proof of Lemma 9 we must now show that G(0) −G(λ) ≥
Cρn∥λ∥1, as n →∞, uniformly in f in a neighbourhood of π, and λ in a positive
neighbourhood of 0. As in that proof we write G(0) −G(λ) in the form (21) and see
that it suﬃces that the partial derivatives of G at 0 divided by ρn tend to negative limits,
and that ∥∇G(λ) −∇G(0)∥/ρn becomes uniformly small as λ is close enough to zero.
The partial derivative at 0 with respect to λbb′ is given in (23), where we must replace
P by ρnS. Since the scaled Kullback–Leibler divergence ρ−1
n K(ρns∥ρnt) of two Bernoulli
laws converges to the Kullback–Leibler divergence K0(s∥t) = s log(s/t) + t −s between
two Poisson laws of means s and t, as ρn →0, it follows that for ρn →0, uniformly in f,
1
ρn
∂
∂λbb′ G(λ)|λ=0 →−

a
faK0(Sab′∥Sab).
The right side is strictly negative for f close to π, by the assumption of identiﬁability
of (S, π).

792
Bayesian Community Detection
If P = ρnS, then the function λ →v(λ) given in (25) takes the form v = ρnvS, for
vS deﬁned in the same way but with S replacing P. The function u given in (24) does
not depend on P or S. Using again that the derivative of the map ϵ →u(ϵ)τ(v(ϵ)/u(ϵ))
is given by v′ log(v/(u −v)) −u′ log(u/(u −v)), we see that the partial derivative with
respect to λbb′ of the (a, a′) term in the sum deﬁning G takes the form
ρnv′
S log
ρnvS
u −ρvS
−u′ log
u
u −ρnvS
= ρnv′
S log ρn −ρnv′
S log(vS/u) −(ρnv′
S −u′) log(1 −ρnvS/u).
Here u and vS are as in (24) and (25) (with P replaced by S), and depend on (a, a′). From
the fact that the column sums of the matrices R(λ) do not depend on λ, we have that

a,a′

(R(λ)SR(λ)T )aa′ −δaa′
n

k
PkkR(λ)ak

= R(λ)T 1SR(λ)T 1 −

k
Pkk

a
R(λ)ak
is constant in λ. This shows that 
a,a′ v′
S = 0 and hence the contribution of the term
ρnv′
S log ρn to the partial derivatives of G vanishes. The term −(ρnv′
S −u′) log(1 −
ρnvS/u) can be expanded as (ρnv′
S −u′)ρnvS/u up to O(ρ2
n), uniformly in f and λ.
Since these are equicontinuous functions of λ, it follows that ρ−1
n (∇G(λ) −∇G(0))
becomes arbitrarily small if λ varies in a suﬃciently small neighbourhood of 0.
Lemma 12 shows that in a neighborhood of the truth, there is not much variation in
the diﬀerences between the observed modularity and the modularity evaluated on the
expected number of connections between classes given the true labelling.
Lemma 12. There exists a constant c > 0 such that for X(e) as in (18), for every twice
diﬀerentiable function ta,b : [0, 1] →R with ∥t′
a,b∥∞∨∥t′′
a,b∥∞≤1, and every x > 0,
Pr
	
max
e:#(ei̸=Zi)≤m
X(e) −X(Z)

∞> x

≤6
n
m

Km+2e−
cx2n2
m∥P ∥∞/n+x .
Proof. Given Z there are at most
 n
m

groups of m candidate nodes that can be assigned
to have ei ̸= Zi, and the label of each node can be chosen in at most K −1 ways. Thus
conditioning the probability on Z, we can use the union bound to pull out the maximum
over e, giving a sum of fewer than
 n
m

Km terms. Next we pull out the norm giving
another factor K2. It suﬃces to combine this with a tail bound for a single variable
Xa,b(e) −Xa,b(Z). Write t for ta,b.
Assume for simplicity of notation that ei = Zi, for i > m, and decompose
1
n2 Oab(e) = 1
n2


i≤m or j≤m
Aij1ei=a,ej=b +

i>m and j>m
Aij1ei=a,ej=b

=: S1 + S2.
Let Oab(Z)/n2 =: S′
1 + S2, with the same variable S2, be the corresponding decompo-
sition if e is changed to Z, and then decompose, where the expectation signs E denote
conditional expectations given Z,

S. L. van der Pas and A. W. van der Vaart
793
Xab(e) −Xab(Z)
=

t(S1 + S2) −t(ES1 + ES2)

−

t(S′
1 + S2) −t(ES′
1 + ES2)

= t(S1 + S2) −t(ES1 + S2)
+

t(ES1 + S2) −t(ES1 + ES2)

−

t(ES′
1 + S2) −t(ES′
1 + ES2)

+ t(ES′
1 + S2) −t(S′
1 + S2).
The ﬁrst and third terms on the far right can be bounded above in absolute value by
∥t′∥∞times the increment. To estimate the second term we write it as
(S2 −ES2)(ES1 −ES′
1)
 1
0
 1
0
t′′
uS2 + (1 −u)ES2 + vES1 + (1 −v)ES′
1

du dv.
Since the ﬁrst and second derivatives of t are uniformly bounded by 1, it follows that
Xab(e) −Xab(Z)
 ≤|S1 −ES1| + |S2 −ES2| |ES1 −ES′
1| + |S′
1 −ES′
1|.
The variable S1 −ES1 is a sum of fewer than 2mn independent variables, each with con-
ditional mean zero, bounded above by 1/n2 and of variance bounded above by ∥P∥∞/n4.
Therefore Bernstein’s inequality gives that
P

|S1 −ES1| > x

≤e−1
2 x2/(2mn∥P ∥∞/n4+x/(3n2)).
This is as the exponential factor in the bound given by the lemma, for appropriate c.
The variable S′
1 −ES′
1 can be bounded similarly. Furthermore |ES1 −ES′
1| ≤4mn/n2 =
4m/n, and S2 −ES2 is the sum of fewer than n2 variables as before, so that
P

|S2 −ES2| |ES1 −ES′
1| > x

≤e−1
2 (xn/(4m))2/(n2∥P ∥∞/n4+xn/(12mn2)).
The exponent has a similar form as before, except for an additional factor n/m ≥1.
References
Abbe, E., Bandeira, A. S., and Hall, G. (2014). “Exact Recovery in the Stochas-
tic Block Model.” ArXiv:1405.3267v4. MR3447993. doi: https://doi.org/10.1109/
TIT.2015.2490670.
775, 776
Airoldi, E. M., Blei, D. M., Fienberg, S. E., and Xing, E. P. (2008). “Mixed Membership
Stochastic Blockmodels.” Journal of Machine Learning Research, 9: 1981–2014.
767
Bickel, P. J. and Chen, A. (2009). “A Nonparametric View of Network Models and
Newman-Girvan and Other Modularities.” Proceedings of the National Academy of
Sciences of the United States of America, 106(50): 21068–21073.
767, 768, 769, 771,
772, 773, 775, 776, 777, 778, 781, 782, 783, 788
Bickel, P. J., Chen, A., Zhao, Y., Levina, E., and Zhu, J. (2015). “Correction to the Proof
of Consistency of Community Detection.” The Annals of Statistics, 43(1): 462–466.
MR3311866. doi: https://doi.org/10.1214/14-AOS1271.
768, 776, 781

794
Bayesian Community Detection
Channarond, A., Daudin, J.-J., and Robin, S. (2012). “Classiﬁcation and Estimation
in the Stochastic Blockmodel Based on the Empirical Degrees.” Electronic Journal
of Statistics, 6: 2574–2601. MR3020277. doi: https://doi.org/10.1214/12-EJS753.
767, 768, 776, 779
Chen, K. and Lei, J. (2014). “Network Cross-Validation for Determining the Number
of Communities in Network Data.” ArXiv:1411.1715v1.
769
Chen, Y. and Xu, J. (2014). “Statistical-Computational Tradeoﬀs in Planted Problems
and Submatrix Localization with a Growing Number of Clusters and Submatrices.”
ArXiv:1402.1267v2. MR3491121.
775, 776
Cˆome, E. and Latouche, P. (2014). “Model Selection and Clustering in Stochastic Block
Models with the Exact Integrated Complete Data Likelihood.” ArXiv:1303.2962.
MR3441229. doi: https://doi.org/10.1177/1471082X15577017.
777
Csardi, G. and Nepusz, T. (2006). “The igraph Software Package for Complex Network
Research.” InterJournal Complex Systems, 1695.
778
Gao, C., Ma, Z., Zhang, A. Y., and Zhou, H. H. (2015). “Achieving Optimal Misclassi-
ﬁcation Proportion in Stochastic Block Model.” ArXiv:1505.03772v5.
776
Gao, C., Ma, Z., Zhang, A. Y., and Zhou, H. H. (2016). “Community Detection in
Degree-Corrected Block Models.” ArXiv:1607.06993.
776
Ghosal, S., Ghosh, J. K., and van der Vaart, A. W. (2000). “Convergence rates
of posterior distributions.” The Annals of Statistics, 28(2): 500–531. MR1790007.
doi: https://doi.org/10.1214/aos/1016218228.
776
Glover, F. (1989). “Tabu Search – Part I.” ORSA Journal on Computing, 1(3): 190–206.
777
Hayashi, K., Konishi, T., and Kawamoto, T. (2016). “A Tractable Fully Bayesian
Method for the Stochastic Block Model.” ArXiv:1602.02256v1.
768
Hofman, J. M. and Wiggins, C. H. (2008). “Bayesian Approach to Network Modularity.”
Physical Review Letters, 100: 258701.
768
Holland, P. W., Laskey, K. B., and Leinhardt, S. (1983). “Stochastic Blockmod-
els: First Steps.” Social Networks, 5: 109–137. MR0718088. doi: https://doi.org/
10.1016/0378-8733(83)90021-7.
767
Jin, J. (2015). “Fast Community Detection by SCORE.” The Annals of Statistics, 43(1):
57–89. MR3285600. doi: https://doi.org/10.1214/14-AOS1265.
768
Karrer, B. and Newman, M. E. J. (2011). “Stochastic Blockmodels and Com-
munity Structure in Networks.” Physical Review E, 83: 016107. MR2788206.
doi: https://doi.org/10.1103/PhysRevE.83.016107.
767
Kpogbezan, G. B., van der Vaart, A. W., van Wieringen, W. N., Leday, G. G. R., and
van de Wiel, M. A. (2016). “An empirical Bayes approach to network recovery using
external knowledge.” ArXiv:1605.07514.
769

S. L. van der Pas and A. W. van der Vaart
795
Lei, J. and Rinaldo, A. (2015). “Consistency of Spectral Clustering in Stochas-
tic
Block
Models.”
The
Annals
of
Statistics,
43(1):
215–237.
MR3285605.
doi: https://doi.org/10.1214/14-AOS1274.
768, 775
McDaid, A. F., Brendan Murphy, T., Friel, N., and Hurley, N. J. (2013). “Im-
proved Bayesian Inference for the Stochastic Block Model with Application to Large
Networks.” Computational Statistics and Data Analysis, 60: 12–31. MR3007016.
doi: https://doi.org/10.1016/j.csda.2012.10.021.
769, 771, 772, 777
Mossel,
E.,
Neeman,
J.,
and
Sly,
A.
(2012).
“Reconstruction
and
Esti-
mation
in
the
Planted
Partition
Model.”
ArXiv:11202.1499v4.
MR3383334.
doi: https://doi.org/10.1007/s00440-014-0576-6.
775
Newman, M. and Girvan, M. (2004). “Finding and Evaluating Community Structure
in Networks.” Physical Review E, 69: 026113. MR1975193. doi: https://doi.org/
10.1103/PhysRevE.67.026126.
767
Nowicki, K. and Snijders, T. A. B. (2001). “Estimation and Prediction for Stochastic
Blockstructures.” Journal of the American Statistical Association, 96(455): 1077–
1087.
MR1947255.
doi:
https://doi.org/10.1198/016214501753208735.
768,
771, 772, 777
Park, Y. and Bader, J. S. (2012). “How Networks Change with Time.” Bioinformatics,
28(12): i40–i48.
767
Pati, D. and Bhattacharya, A. (2015). “Optimal Bayesian Estimation in Stochastic
Block Models.” ArXiv:1505.06794.
768
Robbins, H. (1955). “A Remark on Stirling’s Formula.” The American Mathematical
Monthly, 62(1): 26–29. MR0069328. doi: https://doi.org/10.2307/2308012.
785
Rohe, K., Chatterjee, S., and Yu, B. (2011). “Spectral Clustering and the High-
Dimensional Stochastic Blockmodel.” The Annals of Statistics, 39(4): 1878–1915.
MR2893856. doi: https://doi.org/10.1214/11-AOS887.
768
Saldana, D. F., Yu, Y., and Feng, Y. (2014). “How Many Communities Are There?”
ArXiv:1412.1684v1.
MR3610418.
doi:
https://doi.org/10.1080/10618600.
2015.1096790.
769
Sarkar, P. and Bickel, P. J. (2015). “Role of Normalization in Spectral Clustering
for Stochastic Blockmodels.” The Annals of Statistics, 43(3): 962–990. MR3346694.
doi: https://doi.org/10.1214/14-AOS1285.
768
Snijders, T. A. and Nowicki, K. (1997). “Estimation and Prediction for Stochastic Block-
models for Graphs with Latent Block Structure.” Journal of Classiﬁcation, 14: 75–
100. MR1449742. doi: https://doi.org/10.1007/s003579900004.
767, 768
Suwan, S., Lee, D. S., Tang, R., Sussman, D. L., Tang, M., and Priebe, C. E. (2016).
“Empirical Bayes estimation for the stochastic blockmodel.” Electronic Journal of
Statistics, 10: 761–782. MR3477741. doi: https://doi.org/10.1214/16-EJS1115.
768

796
Bayesian Community Detection
Wang, Y. X. R. and Bickel, P. J. (2015). “Likelihood-Based Model Selection for
Stochastic Block Models.” ArXiv:1502.02069v1. MR3196592. doi: https://doi.org/
10.1080/15598608.2013.771546.
769
Zachary, W. W. (1977). “An Information Flow Model for Conﬂict and Fission in Small
Groups.” Journal of Anthropological Research, 33(4): 452–473.
777, 778, 779
Zhang, A. Y. and Zhou, H. H. (2015). “Minimax Rates of Community Detection in
Stochastic Block Models.” Preprint available at http://www.stat.yale.edu/~hz68/
CommunityDetection.pdf.
MR3546450.
doi:
https://doi.org/10.1214/15-
AOS1428.
775, 776
Zhao, Y., Levina, E., and Zhu, J. (2012). “Consistency of Community Detection in Net-
works under Degree-Corrected Stochastic Block Models.” The Annals of Statistics,
40(4): 2266–2292. MR3059083. doi: https://doi.org/10.1214/12-AOS1036.
767,
768, 776, 777, 781

