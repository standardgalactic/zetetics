Bayesian Analysis (2018)
13, Number 3, pp. 767‚Äì796
Bayesian Community Detection
S. L. van der Pas‚Ä°‚àóand A. W. van der Vaart¬ß‚Ä†
Abstract.
We introduce a Bayesian estimator of the underlying class structure
in the stochastic block model, when the number of classes is known. The estimator
is the posterior mode corresponding to a Dirichlet prior on the class proportions,
a generalized Bernoulli prior on the class labels, and a beta prior on the edge
probabilities. We show that this estimator is strongly consistent when the expected
degree is at least of order log2 n, where n is the number of nodes in the network.
MSC 2010 subject classiÔ¨Åcations: Primary 62F15, 90B15.
Keywords: stochastic block model, community detection, networks, consistency,
Bayesian inference, modularities, MAP estimation.
1
Introduction
The stochastic block model (SBM) (Holland et al., 1983) is a model for network data
in which individual nodes are considered members of classes or communities, and the
probability of a connection occurring between two individuals depends solely on their
class membership. It has been applied to social, biological and communication networks,
for example in Park and Bader (2012), Bickel and Chen (2009) and Snijders and Now-
icki (1997) amongst many others. There are many extensions of the SBM for various
applications, including the degree-corrected SBM (Karrer and Newman, 2011; Zhao
et al., 2012) which accounts for possible heterogeneity among nodes within the same
class, and the mixed-membership SBM (Airoldi et al., 2008), in which the assumption
that the classes are disjoint is removed. These extensions allow for additional modelling
Ô¨Çexibility.
Two main SBM research directions are the recovery of the class labels (community
detection) and recovery of the remaining model parameters, consisting of the probability
vector generating the class labels, and the class-dependent probabilities of creating an
edge between nodes. In this paper, we focus on community detection, noting that once
strong consistency of a community detection method has been established, consistency
of the natural plug-in estimators for the remaining parameters follows directly by results
in (Channarond et al., 2012).
A large number of methods for recovering the class labels has been proposed. Those
most closely related to this work are the modularities. Newman and Girvan (2004)
introduced the term modularity for ‚Äòa measure of the quality of a particular division
of a network‚Äô. They described one such measure for models in which edges are more
‚àóResearch supported by Netherlands Organization for ScientiÔ¨Åc Research NWO.
‚Ä†The research leading to these results has received funding from the European Research Council
under ERC Grant Agreement 320637.
‚Ä°Mathematical Institute, Leiden University, svdpas@math.leidenuniv.nl
¬ßMathematical Institute, Leiden University, avdvaart@math.leidenuniv.nl
c‚Éù2018 International Society for Bayesian Analysis
https://doi.org/10.1214/17-BA1078

768
Bayesian Community Detection
likely to occur within classes than between classes, in which case there is a community
structure in the colloquial sense, although the SBM does not require this assumption.
Bickel and Chen (2009) studied more general modularities, deÔ¨Åning them as functions
of the number of connections between all combinations of classes and the proportion
of nodes placed in each class. They introduced the likelihood modularity, and provided
general conditions under which modularities are consistent. Their method and theory
was extended to the degree-corrected SBM by Zhao et al. (2012).
Spectral methods for community detection have gained in popularity, and reÔ¨Åned
results on error bounds are now available for the SBM and extensions of the SBM,
as evidenced in Rohe et al. (2011), Jin (2015), Sarkar and Bickel (2015) and Lei and
Rinaldo (2015) for example. Many other algorithms have been introduced, most of them
currently lacking formal proofs of consistency. A notable exception is the Largest Gaps
algorithm (Channarond et al., 2012), which only takes the degree of each node as its
input, and is strongly consistent under a separability condition.
A Bayesian approach towards recovering the class assignments in the SBM was
Ô¨Årst suggested by Snijders and Nowicki (1997), motivated by computational advantages
of Gibbs sampling over maximum likelihood estimation. They considered two classes
and proposed uniform priors on the class proportions and the edge probabilities. This
approach was extended in (Nowicki and Snijders, 2001) to allow for more classes, with
a Dirichlet prior on the class proportions and beta priors on the edge probabilities.
Hofman and Wiggins (2008) described a similar Bayesian approach for a special case
of the SBM and suggested a variational approach to overcome the computational issues
associated with maximizing over all possible class assignments.
Bayesian methods for the SBM have barely been studied from a theoretical point of
view, although recent results for parameter recovery by Pati and Bhattacharya (2015),
for detecting the number of communites by Hayashi et al. (2016) and for an empirical
Bayes approach to community detection by Suwan et al. (2016) are encouraging. In
this work, we provide theoretical results on community detection, establishing that the
Bayesian posterior mode is strongly consistent for the class labels if the expected degree
is at least of order log2 n, where n is the number of nodes. This is proven by relating
the posterior mode to the maximizer of the likelihood modularity of Bickel and Chen
(2009). The likelihood modularity has been claimed to be strongly consistent under the
weaker assumption that the expected degree is of larger order than log n (Bickel and
Chen, 2009; Zhao et al., 2012; Bickel et al., 2015). However, their proof assumes that
the likelihood modularity is globally Lipschitz, while it is only locally so. The Bayesian
method is based on a combination of likelihood and prior, and for this reason the proof
of our main theorem, Theorem 1, runs into a similar problem. We were able to resolve
this only under the slightly stronger assumption that the expected degree is of larger
order than (log n)2. The literature on other methods for community detection shows
that the order log n is suÔ¨Écient for consistent detection. However, these results are
usually obtained under additional assumptions such as a restriction to two classes or
an ordering of the connection probabilities, and their implications for the likelihood
or Bayesian modularities is unclear. We discuss this and the relevant literature further
following the statement of our main result in Section 3.5.

S. L. van der Pas and A. W. van der Vaart
769
The main result of the present paper is that the posterior mode is strongly consistent
in the frequentist setup, a property that it shares with the maximizer of the likelihood
modularity. As the number of parameters of the model (‚Äúlabellings‚Äù) increases rapidly
with n, this result is certainly not covered by standard theory for parametric models,
and in fact we shall see that the prior on the labellings plays a special role for consis-
tency. That the posterior mode behaves well in terms of consistency is encouraging, and
makes one hope that other aspects of the posterior distribution will also be useful for
inference. The present paper may be considered a Ô¨Årst step and further study of such
aspects is desirable. One possible research direction would be to use the full posterior
distribution on the labels to quantify uncertainty in the estimate of the class labels. A
second issue that may be resolved by the Bayesian approach is the question of estimating
the number of classes, K. This remains an important open question, as noted by Bickel
and Chen (2009), despite recent attempts (e.g. Saldana et al. (2014), Chen and Lei
(2014) and Wang and Bickel (2015)). By introducing a prior on K, such as the Poisson-
prior suggested by McDaid et al. (2013), the number of communities K can be detected
by the posterior. A third open question is whether the Bayesian estimator can be im-
proved by incorporating prior knowledge of the community structure. Recent work on
incorporating prior information in Gaussian graphical models (Kpogbezan et al., 2016)
is encouraging, and has not been translated to the SBM yet.
This paper is organized as follows. We introduce the SBM and the associated nota-
tion in Section 2. Our main results are in Section 3, where we describe the prior and
the link with the likelihood modularity, present the consistency results and discuss the
underlying assumptions, especially those on the expected degree. After an illustration
of the method on a data set in Section 4, we conclude with the proofs, Ô¨Årst of weak
consistency in Section 5 and Ô¨Ånally of strong consistency of the Bayesian modularity in
Section 6.
1.1
Notation
For a vector v we denote by Diag(v) the diagonal matrix with diagonal v, and for a
matrix M we denote its diagonal by diag (M).
The ‚à•.‚à•1-norm of a matrix M is the sum of the absolute values of all entries of M.
We write f(n) = O(g(n)) as n ‚Üí‚àûif there exist C, n0 > 0 such that |f(n)| ‚â§
C|g(n)| for all n > n0.
2
The stochastic block model
We introduce the notation and generative model for the SBM with K ‚àà{1, 2, . . .}
classes. Consider an undirected random graph with n nodes, numbered 1, 2, . . . , n, and
edges encoded by the n √ó n symmetric adjacency matrix (Aij), with entries in {0, 1}.
Thus Aij = Aji is equal to 1 or 0 if the nodes i and j are or are not connected by an
edge, respectively. Self-loops are not allowed, so Aii = 0 for i = 1, . . . , n. The generative
model for the random graph is:

770
Bayesian Community Detection
1. The nodes are randomly labeled with i.i.d. variables Z1, . . . , Zn, taking values in
a Ô¨Ånite set {1, . . . , K}, according to probabilities œÄ = (œÄ1, . . . , œÄK).
2. Given Z = (Z1, . . . , Zn)T , the edges are independently generated as Bernoulli
variables with P(Aij = 1 | Z) = PZi,Zj, for i < j, for a given K √ó K symmetric
matrix P = (Pab).
The probability vector œÄ is considered Ô¨Åxed, but unknown. Although this is not visible in
the notation, the matrix P may change with n, a case of particular interest being that P
tends to zero, which gives a sparse graph. The order of magnitude of ‚à•P‚à•‚àû= maxa,b Pab
is the same as the order of magnitude of œÅn = 
a,b œÄaœÄbPab, the probability of there
being an edge between two randomly selected nodes. The expected degree of a randomly
selected node is Œªn = (n ‚àí1)œÅn, and twice the expected total number of edges in the
network is Œºn = n(n ‚àí1)œÅn.
The likelihood for the model is given by

i<j
P Aij
ZiZj(1 ‚àíPZiZj)1‚àíAij 
i
œÄZi =

a‚â§b
P Oab(Z)
ab
(1 ‚àíPab)nab(Z)‚àíOab(Z) 
a
œÄna(Z)
a
, (1)
where Oab(Z) is the number of edges between nodes labelled a and b by the labelling Z,
nab(Z) is the maximum number of edges that can be created between nodes labelled a
and b, and na(Z) is the number of nodes labelled a, and a and b range over {1, 2, . . . , K}.
More formally, for a given labelling e = (e1, . . . , en)T ‚àà{1, . . . , K}n of nodes, and
class labels a, b ‚àà{1, . . . , K}, we deÔ¨Åne
Oab(e) =

i,j Aij1{ei=a,ej=b},
a Ã∏= b,

i<j Aij1{ei=a,ej=b},
a = b,
nab(e) =

na(e)nb(e),
a Ã∏= b,
1
2na(e)(na(e) ‚àí1),
a = b,
na(e) =
n

i=1
1{ei=a}.
Since the matrix A is symmetric with zero diagonal by assumption, for a Ã∏= b the variable
Oab(e) can also be written as 
i<j Aij[1{ei=a,ej=b} + 1{ej=a,ei=b}], which explains the
diÔ¨Äerent appearances of the diagonal and oÔ¨Ä-diagonal entries. The numbers nab(e) are
equal to the numbers Oab(e) when all Aij are equal to 1. We collect the variables Oab(e)
and nab(e) in K √ó K matrices O(e) and n(e).
Now consider the K √ó K probability matrix R(e, c) and K probability vector f(e)
with entries
Rab(e, c) = 1
n
n

i=1
1{ei=a,ci=b},
fa(e) = na(e)
n
.
(2)
The row sums of R(e, c) are equal to R(e, c)1 = f(e), while the column sums are equal to
1T R(e, c) = f(c)T . Thus, the matrix R(e, c) can be seen as a coupling of the marginal

S. L. van der Pas and A. W. van der Vaart
771
probability vectors f(e) and f(c). If e = c, then it is diagonal with diagonal f(c) =
f(e). More generally, the matrix can be viewed as measuring the discrepancy between
labellings e and c. This can be precisely measured as half the L1-distance of R(e, c) to
its diagonal, as evidenced by Lemma 1, which is noted in Bickel and Chen (2009).
Recall that by ‚à•M‚à•1 we denote the sum of the absolute values of all entries of a
matrix M.
Lemma 1. For every labelling c, e in the K-class stochastic block model:
1
n
n

i=1
1{ciÃ∏=ei} = 1
2‚à•Diag(f(c)) ‚àíR(e, c)‚à•1.
Proof. The diagonal of R(e, c) gives the fractions of labels on which c and e agree. Hence
the left side of the lemma is 1 ‚àí
a Raa(e, c) = 
a(fa(c) ‚àíRaa(e, c)). The elements of
both K √ó K matrices Diag(f(c)) and R(e, c) can be viewed as probabilities that add
up to 1. Thus the sum of the diÔ¨Äerences of the diagonal elements is minus the sum of
the diÔ¨Äerences of the oÔ¨Ä-diagonal elements. Because fa(c) ‚â•Raa(e, c) for every a, we
have 
a(fa(c)‚àíRaa(e, c)) = 
a |fa(c)‚àíRaa(e, c)|. Similarly the oÔ¨Ä-diagonal elements
of Diag(f(c)), which are zero, are smaller than the oÔ¨Ä-diagonal elements of R(e, c) and
hence we can add absolute values. Thus the sum over the diagonal is half the sum of
the absolute values of all terms in Diag(f(c)) ‚àíR(e, c).
3
Bayesian approach to community detection
Our main results are presented in this section. We Ô¨Årst discuss the choice of prior in
Section 3.1, and deÔ¨Åne the estimator, in Section 3.2. The resulting Bayesian modularity
is closely related to the likelihood modularity of Bickel and Chen (2009). The relation-
ship is clariÔ¨Åed in Section 3.3. We brieÔ¨Çy consider the issue of identiÔ¨Åability in the SBM
in Section 3.4, and conclude with our main theorem on the strong consistency of the
Bayesian modularity in Section 3.5.
3.1
The prior
We adopt the Bayesian approach of Nowicki and Snijders (2001). We put prior dis-
tributions on the parameters of the stochastic block model with K known, the vector
œÄ and the matrix P, yielding a joint probability distribution of (A, Z, œÄ, P). Next we
marginalize over œÄ and P as in McDaid et al. (2013), leading to a joint distribution of
(A, Z). Finally we ‚Äúestimate‚Äù the unobserved vector Z by the posterior mode of the
conditional distribution of Z given A. From a frequentist point of view this means that
Z is treated as a parameter of the problem, equipped with a hierarchical prior that
chooses Ô¨Årst œÄ and then Z. Accordingly we shall change notation from Z to e, reserving
Z for the frequentist description of the stochastic block model in Section 2.
The prior on œÄ is a Dirichlet, and independently the Pab for a ‚â§b receive independent
beta priors:

772
Bayesian Community Detection
œÄ ‚ä•(PAb),
œÄ ‚àºDir(Œ±, . . . , Œ±),
Pab
i.i.d.
‚àºBeta(Œ≤1, Œ≤2),
1 ‚â§a ‚â§b ‚â§K.
This is essentially the same set-up as in Nowicki and Snijders (2001) and McDaid et al.
(2013), except that we use a more Ô¨Çexible Beta(Œ≤1, Œ≤2) instead of a uniform prior on
the Pab. We assume Œ±, Œ≤1, Œ≤2 > 0.
We complete the Bayesian model by specifying class labels e = (e1, . . . , en) and
edges A = (Aij : i < j) through
ei | œÄ, P
i.i.d.
‚àºœÄ,
1 ‚â§i ‚â§n,
Aij | œÄ, P, e
ind.
‚àºBernoulli(Pei,ej),
1 ‚â§i < j ‚â§n.
Abusing notation we write p(e), p(A | e) and p(e | A) for marginal and conditional
probability density functions.
3.2
The Bayesian modularity
The Bayesian estimator of the class labels will be the posterior mode, that is:
e = argmax
e
p(e | A).
The posterior mode can be interpreted as a modularity-based estimator in the sense
of Bickel and Chen (2009), in that it maximizes a function that only depends on the
Oab(e) and the na(e). This can be seen from the joint density of (A, e), which is found by
marginalizing the likelihood (1) over œÄ and P. The conjugacy between the multinomial
and Dirichlet distributions gives the marginal density of the class assignment e as:
p(e) =

SK

a
œÄna(e)
a

a œÄŒ±‚àí1
a
D(Œ±)
dœÄ =
Œì(Œ±K)
Œì(Œ±)KŒì(n + Œ±K)

a
Œì(na(e) + Œ±).
(3)
Here the integral is relative to the Lebesgue measure on the K-dimensional unit simplex
and D(Œ±) = Œì(Œ±)K/Œì(KŒ±) is the norming constant for the Dirichlet density. Similarly
the conjugacy between the Bernoulli and Beta distributions gives the marginal condi-
tional density of A given e as:
p(A | e) =

[0,1]K(K+1)/2

a‚â§b
P Oab(e)
ab
(1 ‚àíPab)nab(e)‚àíOab(e) 
a‚â§b
P Œ≤1‚àí1
ab
(1 ‚àíPab)Œ≤2‚àí1
B(Œ≤1, Œ≤2)
dP
=

a‚â§b
1
B(Œ≤1, Œ≤2)B(Oab(e) + Œ≤1, nab(e) ‚àíOab(e) + Œ≤2),
(4)
where B(x, y) = Œì(x)Œì(y)/Œì(x + y) is the beta-function. The joint density of A and e
is given by the product of (3) and (4), and n‚àí2 times its logarithm is up to a constant
that is free of e equal to

S. L. van der Pas and A. W. van der Vaart
773
QB(e) = 1
n2

1‚â§a‚â§b‚â§K
log B(Oab(e) + Œ≤1, nab(e) ‚àíOab(e) + Œ≤2) + 1
n2
K

a=1
log Œì(na(e) + Œ±).
(5)
This is a modularity in the sense of Bickel and Chen (2009), which we deÔ¨Åne as the
Bayesian modularity. As p(e | A) is proportional to p(e, A), the posterior mode is
equal to the class assignment that maximizes the Bayesian modularity, so the Bayesian
estimator is equal to:
e = argmax
e
QB(e).
(6)
3.3
Similarity to the likelihood modularity
The Bayesian modularity QB(e) consists of two parts, originating from the likelihood
and the prior on the classiÔ¨Åcation, respectively. The Ô¨Årst part is close to the likelihood
modularity given by
QML(e) = 1
n2

1‚â§a‚â§b‚â§K
nab(e) œÑ
	Oab(e)
nab(e)

,
where œÑ(x) = x log x + (1 ‚àíx) log(1 ‚àíx). This criterion, obtained in Bickel and Chen
(2009), results from replacing in the log conditional likelihood of A given e (the logarithm
of (1) with Z replaced by e and discarding the term involving the parameters œÄa) the
parameters Pab by their maximum likelihood estimators ÀÜPab = Oab(e)/nab(e). In other
words, the parameters are proÔ¨Åled out rather than integrated out as for the Bayesian
modularity. The corresponding estimator
eML = argmax
e
QML(e)
is consistent, and hence one may hope that the Bayesian estimator can be proved con-
sistent by showing that the Bayesian and likelihood modularities are close. This will
indeed be our line of approach, but we shall see that the proximity of the two criteria
is not close enough to explain the strong consistency of the two methods. In particular,
the second, prior part of the Bayesian modularity, resulting from the prior density (3)
over the labels, does play a role in the proof of strong consistency. We discuss this in
more detail at the end of Section 3.5.
The following lemma links the Bayesian and likelihood modularities. The Ô¨Ånal as-
sertion shows that they are at most of the order log n/n apart, which will be seen to be
enough in the proof of weak consistency. For the proof of strong consistency we shall
need the Ô¨Årst assertion of the lemma, which makes the discrepancy between the two
modularities explicit up to order log n/n2.
Lemma 2. There exists a constant C such that, for E = {1, . . . , K}n the set of all
possible labellings:
max
e‚ààE
QB(e) ‚àíQML(e) ‚àíQP (e)
 ‚â§C log n
n2
,

774
Bayesian Community Detection
for
QP (e) = 1
n2

a:na+‚åäŒ±‚åã‚â•2
na(e) log(na(e)) ‚àí1
n.
Consequently maxe‚ààE |QB(e) ‚àíQML(e)| = O(log n/n) as n ‚Üí‚àû.
3.4
IdentiÔ¨Åability and consistency
A classiÔ¨Åcation e is said to be weakly consistent if the fraction of misclassiÔ¨Åed nodes tends
to zero (partial recovery), and strongly consistent if the probability of misclassifying any
of the nodes tends to zero (exact recovery). In deÔ¨Åning consistency in a precise manner,
the complication of the possible unidentiÔ¨Åability of the labels needs to be dealt with.
From the observed data A we can at best recover the partition of the n nodes in the
K classes with equal labels Zi, but not the values Z1, . . . , Zn of the labels, in the set
{1, 2, . . . , K}, attached to the classes. Thus consistency will be up to a permutation of
labels.
To make this precise deÔ¨Åne, for a given permutation (1, . . . , K) ‚Üí(œÉ(1), . . . , œÉ(K)),
the permutation matrix PœÉ as the matrix with rows
eT
œÉ(1)
...
eT
œÉ(K),
for e1, . . . , eK the unit vectors in RK. Then pre-multiplication of a matrix by PœÉ per-
mutes the rows, and post-multiplication by P T
œÉ the columns: PœÉR is the matrix with
jth row equal to the œÉ(j)th row of R, and RP T
œÉ is the matrix with jth column the
œÉ(j)th column of R. Thus PœÉR(e, Z) = R(PœÉe, Z) is the matrix that would result
if we would permute the labels of the classes of the assignment e, and PœÉPP T
œÉ and
PœÉR(e, Z)P T
œÉ = R(PœÉe, PœÉZ) are the matrices that would result if we would relabel the
classes throughout. Since we cannot recover the labels, the matrix PœÉR(e, Z) is just as
good or bad as R(e, Z) for measuring discrepancy between a labelling e and the true
labelling Z; furthermore, nothing should change if we choose diÔ¨Äerent names for the
classes.
Thus, taking into account the unidentiÔ¨Åability of the labels, by Lemma 1, we deÔ¨Åne
an estimator e to be weakly consistent if
‚à•PœÉR(e, Z) ‚àíDiag(f(Z))‚à•1 ‚Üí0,
for some permutation matrix PœÉ. We say the classiÔ¨Åcation e is strongly consistent if
P(PœÉR(e, Z) = Diag(f(Z))) ‚Üí1,
for some permutation matrix PœÉ.
The following lemma shows that the permutation matrix PœÉ is for large n uniquely
deÔ¨Åned, unless there are empty classes.

S. L. van der Pas and A. W. van der Vaart
775
Lemma 3. If for a given vector œÄ and matrix R, there exist permutation matrices PœÉ
and QœÉ such that both ‚à•PœÉR ‚àíDiag(œÄ)‚à•1 ‚â§mina œÄa and ‚à•QœÉR ‚àíDiag(œÄ)‚à•1 ‚â§mina œÄa,
then PœÉ = QœÉ.
Proof. Because
the
L1-norm
is
invariant
under
permutations,
we
have
‚à•R ‚àíPœÉ
‚àí1Diag(œÄ)‚à•1 ‚â§mina œÄa, and similarly for QœÉ. Therefore ‚à•P ‚àí1
œÉ Diag(œÄ) ‚àí
Q‚àí1
œÉ Diag(œÄ)‚à•1 ‚â§2 mina œÄa, by the triangle inequality. Again by invariance, the left
side of this inequality is equal to ‚à•(QœÉP ‚àí1
œÉ )Diag(œÄ) ‚àíDiag(œÄ)‚à•1, which is at least two
times the sum of the two smallest coordinates of œÄ if QœÉP ‚àí1
œÉ
is not equal to the identity
matrix.
A necessary requirement for consistency is that the classes can be recovered from
the likelihood, i.e. the model parameters must be identiÔ¨Åable. If œÄ has strictly positive
coordinates, so that all labels will appear in the data eventually, then as explained in
Bickel and Chen (2009) an appropriate condition is that P does not have two identical
rows. If œÄa = 0 for some a, then class a will never be consumed; the identiÔ¨Åability
condition should then be imposed after deleting the ath column from P. Thus, we call
the pair (P, œÄ) identiÔ¨Åable if the rows of P are diÔ¨Äerent after removing the columns
corresponding to zero coordinates of œÄ. Throughout we assume that P is symmetric.
3.5
Consistency results and assumptions
We are now ready to present our results on consistency for the Bayesian maximum a
posteriori (MAP) estimator (6). Recall that œÅn = 
a,b œÄaœÄbPab is the probability of a
new edge, and Œªn = (n‚àí1)œÅn is the expected degree of a node. Theorem 1 shows strong
consistency of the Bayesian estimator if Œªn ‚â´(log n)2. The proof rests on a proof of
weak consistency under similar conditions, stated in Section 5 as Theorem 2.
Theorem 1 (strong consistency). If P = œÅnS, where either œÅn = 1 is Ô¨Åxed or œÅn ‚Üí0,
and (S, œÄ) is Ô¨Åxed and identiÔ¨Åable with all entries of P strictly smaller than 1 and
all entries of S being strictly positive, then the MAP classiÔ¨Åer e = arg maxe QB(e) is
strongly consistent if œÅn ‚â´(log n)2/n.
The theorem is proven in two steps: Ô¨Årst for the dense case, where œÅn is Ô¨Åxed, and
then for the sparse case, where œÅn goes to zero. The second is the most interesting of
the two, as it touches on the question how much information is required to recover the
underlying community structure. Much recent research eÔ¨Äort has gone into determining
detection and computational boundaries, in particular for special cases of the SBM with
K = 2 (see e.g. Mossel et al. (2012), Chen and Xu (2014), Abbe et al. (2014) and Zhang
and Zhou (2015)).
Weakly consistent estimation of the class labels for an arbitrary, but known, number
of classes is possible by some method under the assumption Œªn ‚â´log n, as this was
shown to hold for spectral clustering by Lei and Rinaldo (2015). Strong consistency
of maximum likelihood was shown to hold in the special cases of planted bisection
(K = 2 and equal community sizes) and planted clustering (equal community sizes and

776
Bayesian Community Detection
Pab can take two values) by Abbe et al. (2014); Chen and Xu (2014), again under the
assumption Œªn ‚â´log n. Gao et al. (2015) and Gao et al. (2016) achieve optimality
in diÔ¨Äerent senses, under assumptions on the average within-community and between-
community edge probabilities; Gao et al. (2015) introduce a two-stage procedure which
achieves the optimal proportion of misclassiÔ¨Åed nodes in a special case where Pab can
only take two values, while Gao et al. (2016) obtain minimax rates for the proportion
of misclassiÔ¨Åed nodes in the degree corrected SBM.
Strong consistency of the likelihood modularity for an arbitrary number of classes K
has been claimed under the same assumption Œªn ‚â´log n (Bickel and Chen, 2009; Bickel
et al., 2015), and those results have been extended to the degree-corrected SBM (Zhao
et al., 2012). However, these results were obtained by application of an abstract theorem
to the special case of the likelihood modularity, which would require the function œÑ(x) =
x log x + (1 ‚àíx) log(1 ‚àíx), or the function œÉ(x) = x log x, to be globally Lipschitz. As
œÑ and œÉ are only locally Lipschitz, it is still unclear whether Œªn ‚â´log n is a suÔ¨Écient
condition for either weakly or strongly consistent estimation by maximum likelihood.
From our proof of Theorem 1, which proceeds by comparing the Bayesian modularity
and the likelihood modularity, it follows that Œªn ‚â´(log n)2 is certainly suÔ¨Écient. Given
weak consistency the problem can be reduced to a neighbourhood of the true parameter
on which the Lipschitz condition is satisÔ¨Åed. However, it is precisely our proof of weak
consistency that needs the additional log n factor.
The Largest Gaps algorithm of Channarond et al. (2012) is strongly consistent pro-
vided that minaÃ∏=b | K
k=1 Œ±k(Pak ‚àíPbk)| is at least of order

log n/n, implying that at
least one of the Pab is of the same order, and thus Œªn ‚â´‚àön log n. This much stronger
condition is not surprising, as the Largest Gaps algorithm only uses the degree of a
node and does not take into account any Ô¨Åner information on the group structure, such
as the information contained in the Oab.
To the best of our knowledge, for K > 2, it remains to be shown that Œªn ‚â´log n
is suÔ¨Écient for strong consistency of any community detection method for the general
SBM. For the minimax rate for the proportion of misclustered nodes in community
detection, when only classes of sizes proportional to n are considered, a phase transition
when going from the case K = 2 to K ‚â•3 was observed by Zhang and Zhou (2015).
Their results show that if K = 2, communities of the same size are most diÔ¨Écult to
distinguish, while if K ‚â•3, small communities are harder to discover. This shift in the
nature of the communities that are harder to detect may be what has been preventing
a general strong consistency result under the assumption Œªn ‚â´log n so far.
While the prior on the class assignment plays a negligible role in the proof of
weak consistency, our argument for strong consistency requires that the prior does
not vary too much in a neighborhood of the truth. To be precise, denote by QB,2(e) =
n‚àí2 K
a=1 log Œì(na(e) + Œ±) the second part of the Bayesian modularity (5), and let Z
be the true labelling. Then we need that for any e that diÔ¨Äers from Z by at most m
nodes, the distance |QB,2(e) ‚àíQB,2(Z)| is of smaller order than m/n. We thus Ô¨Ånd
that a variation on general posterior contraction results (e.g. Ghosal et al. (2000)) holds
for the SBM as well, namely that the prior mass should be spread homogeneously in a
neighborhood of the truth.

S. L. van der Pas and A. W. van der Vaart
777
The number K of classes is held Ô¨Åxed in the preceding theorem. Our proofs suggest
that consistency is retained if K = Kn ‚Üí‚àûand œÅn ‚â´K4
n(log Kn)n‚àí1(log(n/ log Kn))2,
provided the model is asymptotically identiÔ¨Åable in a suitable sense. In Theorem 1 iden-
tiÔ¨Åability in the case of Ô¨Åxed K is described as a property of the pair (S, œÄ). If Kn ‚Üí‚àû,
then the dimensions of these objects tend to inÔ¨Ånity and identiÔ¨Åability must be deÔ¨Åned
in a diÔ¨Äerent way. Our proofs suggest that a crucial quality is 
a œÄaK0(Sab‚Ä≤‚à•Sa,b),
where K0(s‚à•s‚Ä≤) is the Kullback‚ÄìLeibler divergence between two Poisson distributions
with means s and s‚Ä≤. As seen in the proof of Lemma 11, this quantity drives local iden-
tiÔ¨Åability. It seems a reasonable assumption that this number be bounded away from
zero, but any type of behaviour is possible as the matrix S will grow in dimension.
A reasonable global identiÔ¨Åability condition might be that the left side of Lemma 11
is bounded below by this number, and then the preceding bound on œÅn is valid. See
Remark 1 for further discussion.
4
Application
Some options for implementing the Bayesian modularity are given in Section 4.1, after
which the results of applying the Bayesian and likelihood modularities to the well-
studied karate club data of Zachary (1977) are discussed in Section 4.2.
4.1
Implementation
The Bayesian modularity, like the likelihood modularity, requires maximization over all
possible labellings. This is computationally feasible even in large networks, as shown
in two recent works on implementing Bayesian methods for the SBM. McDaid et al.
(2013) followed the approach of Nowicki and Snijders (2001) and added a Poisson prior
on K. After marginalizing over œÄ and P, they employ an allocation sampler to sample
from the joint density of K and z given A, and use the posterior mode to estimate K.
Their algorithm gives access to the full posterior distribution on the node labels and can
scale to networks with approximately ten thousand nodes and ten million edges. CÀÜome
and Latouche (2014), claiming that the algorithm of McDaid et al. (2013) suÔ¨Äers from
poor mixing properties, propose a greedy inference algorithm for the same problem.
They demonstrate their algorithm on networks ranging in size from one hundred to ten
thousand nodes, and compare the results to a range of other methods, including spectral
clustering.
For the karate club data in Section 4.2, the network was small enough that a tabu
search (Glover, 1989), run for a number of diÔ¨Äerent initial conÔ¨Ågurations, yielded good
results. This takes a similar amount of time as a tabu search in combination with the
likelihood modularity, as in Bickel and Chen (2009). Although tabu search has been
implemented on large networks consisting of approximately 1000 nodes for the degree-
corrected version of the likelihood modularity (Zhao et al., 2012), we recommend the
use of the methods designed for the stochastic block model proposed by McDaid et al.
(2013) or CÀÜome and Latouche (2014) for networks of medium and large sizes.

778
Bayesian Community Detection
Figure 1: Communities detected by the Bayesian modularity when K = 2 (left) and
K = 4 (right), with Œ± = Œ≤1 = Œ≤2 = 1/2. The polygons contain the two groups the
karate club was split into; the left one is Mr. Hi‚Äôs club, the right one is the OÔ¨Écers‚Äô
club. The shapes of the nodes represent the communities selected by the modularities.
Figure made using the igraph package (Csardi and Nepusz, 2006).
4.2
Karate club
Zachary (1977) described a karate club which split into two clubs after a conÔ¨Çict over
the price of the karate lessons. The new club was led by Mr. Hi, the karate teacher of
the original club, while the remainder of the old club stayed under the former OÔ¨Écers‚Äô
rule. The data consists of an adjacency matrix for those 34 individuals who interacted
with other club members outside club meetings and classes. Each of these individuals‚Äô
aÔ¨Éliations after the conÔ¨Çict is known.
We used Œ± = 1/2 for the Dirichlet prior, and Œ≤1 = Œ≤2 = 1/2 for the beta prior. The
communities selected by the Bayesian modularity for K = 2 and K = 4 are given in
Figure 1. In both instances, the tabu search led to nearly the same solution for both
the Bayesian and likelihood modularities, only diÔ¨Äering at one node for K = 4, which
is not surprising in light of Lemma 2. For K = 2, the results of Bickel and Chen (2009)
for this data set are recovered. For K = 4, the partition in Figure 1 yields a higher
value of the likelihood modularity than the partition into four classes found by Bickel
and Chen (2009), and an even higher value is obtained by switching club member 20
to the second-largest class. This discrepancy is likely due to the heuristic nature of the
tabu search algorithm, and for the same reason, it may be the case that improvement
over the partitions found by the Bayesian modularity in Figure 1 are possible.
For K = 2, the communities found by the algorithms do not correspond in the
slightest to the two karate clubs, instead grouping the nodes with the highest degrees,
corresponding to Mr. Hi, the president of the original club, and their closest supporters,

S. L. van der Pas and A. W. van der Vaart
779
together. Incidentally, this partition is the same as the one returned by the Largest
Gaps algorithm of Channarond et al. (2012), which solely uses the degrees of the nodes
and discards all other information.
These bad results are no reason to shelve the Bayesian and likelihood modularities,
as there is no reason to believe that the two karate clubs form communities in the
sense of the stochastic block model. Mr. Hi and the club‚Äôs president are clear outliers
within their groups, and neither of the algorithms were designed to be robust to such
a phenomenon. The communities selected by the modularities are communities in the
sense that they form connections within and between the groups in a similar fashion.
This sense does not correspond to the social notion of a community in this setting.
The results for four classes unify the social and stochastic senses of community.
The prominent members of each of the new clubs are placed into two separate, small,
communities. The other members are classiÔ¨Åed nearly perfectly, with two exceptions.
However, one of those exceptional individuals is the only person described by Zachary
(1977) as being a supporter of the club‚Äôs president before the split, who joined Mr.
Hi‚Äôs club, making this person‚Äôs aÔ¨Éliation up for debate. The second is described as
only a weak supporter of Mr. Hi. The increased number of communities allows for some
outliers within the social communities, and leads to a more detailed understanding of
the dynamics within both of the groups. We essentially recover the two communities,
each with a core that is more connective than the remainder of the nodes.
5
Weak consistency
The proof of Theorem 1 is built on our proof of weak consistency of the Bayesian
modularity, which we present here. The following quantities will be used in the course
of multiple proofs. The function HP , with domain K √ó K probability matrices, is given
by, for œÑ(u) = u log u + (1 ‚àíu) log(1 ‚àíu),
HP (R) = 1
2

a,b
(R1)a(R1)b œÑ
 (RPRT )ab
(R1)a(R1)b

.
(7)
For œÑ0(u) = u log(u) ‚àíu, deÔ¨Åne
GP (R) = 1
2

a,b
(R1)a(R1)b œÑ0
	 (RPRT )ab
(R1)a(R1)b

.
The sums deÔ¨Åning these functions are over all pairs (a, b) with 1 ‚â§a, b ‚â§K, unlike the
sums deÔ¨Åning the modularities QB and QML, which are restricted to a ‚â§b.
We write diag (P) for the diagonal of P if P is a matrix, and Diag(f) for the diagonal
matrix with diagonal f if f is a vector.
Theorem 2 (weak consistency). If P = œÅnS where either œÅn = 1 is Ô¨Åxed or œÅn ‚Üí0, and
(S, œÄ) is Ô¨Åxed and identiÔ¨Åable, then the MAP classiÔ¨Åer e = arg maxz QB(e) is weakly
consistent provided nœÅn ‚â´(log n)2.

780
Bayesian Community Detection
Proof. By Lemma 2 the Bayesian modularity QB is equivalent to the likelihood mod-
ularity QML up to order (log n)/n. With the notation Oab(e) = Oab(e) if a Ã∏= b, and
Oab(e) = 2Oab(e) if a = b, the likelihood modularity is in turn equivalent up to the
same order to
L(e) =
1
2n2

a,b
na(e)nb(e) œÑ
	
Oab(e)
na(e)nb(e)

.
(8)
Indeed the terms of QML(e) for a < b are identical to the sums of the terms of L(e)
for a < b and a > b, while for a = b the terms of QML(e) and L(e) diÔ¨Äer only subtly:
the Ô¨Årst uses naa(e) =
1
2na(e)(na(e) ‚àí1), where the second uses
1
2na(e)2. Thus the
diÔ¨Äerence is bounded in absolute value by the sum over a of (where e is suppressed from
the notation)
 n2
a
2n2 œÑ
	 Oaa
n2a

‚àína

na ‚àí1)
2n2
œÑ
	
Oaa
na(na ‚àí1)

 ‚â§1
2n‚à•œÑ‚à•‚àû+ n2
a
2n2 l
	
Oaa
n2a(na ‚àí1)

,
where l(x) = 2x(1 ‚à®log(1/x)), in view of Lemma 6. We now use that nal(u/na) ‚â≤
log na ‚â§log n, for 0 ‚â§u ‚â§1.
Combining the preceding, we conclude that
Œ∑n,1 := max
e
|L(e) ‚àíQB(e)| = O
log n
n

.
Since QB(e) ‚â•QB(Z), by the deÔ¨Ånition of e, it follows that L(e)‚àíL(Z) ‚â•‚àí2Œ∑n,1. The
next step is to replace L in this equality by an asymptotic value.
For x equal to a big multiple of (‚à•P‚à•1/2
‚àû‚à®n‚àí1/2)/n1/2, the right side of Lemma 4
tends to zero and hence maxe ‚à•O(e) ‚àíE( O(e) | Z)‚à•‚àû/n2 is of this order in probability.
We also have, by Lemma 5:
max
e
 1
n2 E
 O(e) | Z

‚àíR(e, Z)PR(e, Z)T 
‚àû= max
e
1
n
Diag(R(e, Z) diag (P))

‚àû
= O
	œÅn
n

,
as the row sums of the matrix R(e, Z) are bounded above by one. By Lemma 6,
|vœÑ(x/v) ‚àívœÑ(y/v)| ‚â§l(|x ‚àíy|), uniformly in v ‚àà[0, 1], where l(x) = 2x(1 ‚à®log(1/x)).
It follows that
Œ∑n,2 := max
e
L(e) ‚àíL(e)
 = oP
	
l
	‚à•P‚à•1/2
‚àû‚à®n‚àí1/2
n1/2


,
for
L(e) = 1
2

a,b
fa(e)fb(e) œÑ
	(R(e, Z)PR(e, Z)T )ab
fa(e)fb(e)

.
Combining this with the preceding paragraph, we conclude that L(e) ‚â•L(Z)‚àí2(Œ∑n,1 +
Œ∑n,2). Since L(e) = HP (R(e, Z)) for every e and HP as deÔ¨Åned in (7), and R(Z, Z) =
Diag(f(Z)) = Diag(R(e, Z)T 1), this can be translated into
HP (Diag(R(e, Z)T 1)) ‚àíHP (R(e, Z)) ‚â§2(Œ∑n,1 + Œ∑n,2).
(9)
We complete the proof separately for the cases that œÅn is Ô¨Åxed or tends to zero.

S. L. van der Pas and A. W. van der Vaart
781
For given Œ¥ > 0, let RŒ¥ be the set of all probability matrices R with
min
PœÉ
PœÉR ‚àíDiag(RT 1)

1 ‚â•Œ¥,
and
min
a:œÄa>0(RT 1)a ‚â•Œ¥.
Here the minimum is taken over the (Ô¨Ånite) set of all permutation matrices PœÉ on K
labels. Furthermore, set
Œ∑ := inf
R‚ààRŒ¥

HP

Diag(RT 1)

‚àíHP (R)

.
Because RŒ¥ is compact and the maps R ‚ÜíHP (R) and R ‚ÜíDiag(RT 1) are continuous,
the inÔ¨Åmum in the display is assumed for some R ‚ààRŒ¥. Because no R ‚ààRŒ¥ can
be transformed into a diagonal element by permuting rows and every R ‚ààRŒ¥ has
a nonzero element in every column a with œÄa > 0, Lemma 7 shows that Œ∑ > 0. If
2(Œ∑n,1+Œ∑n,2) is smaller than Œ∑, then it follows from (9) that R(e, Z) cannot be contained
in RŒ¥. Since R(e, Z)T 1 = f(Z)
P‚ÜíœÄ, by the law of large numbers, for suÔ¨Éciently
small Œ¥ > 0 this must be because R(e, Z) fails the Ô¨Årst requirement deÔ¨Åning RŒ¥. That
is, ‚à•PœÉR(e, Z) ‚àíDiag(f(Z))‚à•1 ‚â§Œ¥ for some permutation matrix PœÉ. As this is true
eventually for any Œ¥ > 0, it follows that minPœÉ ‚à•PœÉR(e, Z) ‚àíDiag(œÄ)‚à•1
P‚Üí0.
Finally we consider the case where œÅn ‚Üí0. In view of Lemma 8, the number Œ∑ = Œ∑n,
which now depends on n, is now bounded below by œÅn times a positive number that
depends on (S, œÄ). The preceding argument goes through provided Œ∑n,1+Œ∑n,2 is of smaller
order than Œ∑n. This leads to l(

œÅn/n)+log(n)/n ‚â™œÅn, or (œÅn/n) log2(n/(œÅn‚à•S‚à•‚àû)) ‚â™
œÅ2
n.
Remark 1. If Kn ‚Üí‚àû, then the numbers Œ∑n,2 in the preceding proof need to be
adapted to Œ∑n,2 ‚âçK2
nl(xn + œÅn/n), for xn a big multiple of (log Kn/n)1/2(œÅ1/2
n
‚à®
(log Kn/n)1/2). Equation (9) remains valid. Rather than referring to the identiÔ¨Åability
lemma, Lemma 8, we would now wish to lower bound the left side of (9) by a multiple
of n‚àí1 n
i=1 1ÀÜeiÃ∏=Zi. The proof of Lemma 11 combined with Lemma 1 shows that lo-
cally the left side of (9) is bounded below by a multiple of œÅn

a œÄaK0(Sab‚Ä≤‚à•Sab)n‚àí1 √ó
n
i=1 1ÀÜeiÃ∏=Zi. If this is also globally true, then we obtain consistency as announced at
the end of Section 3.5.
Lemmas 4‚Äì8 are more precise, or, in case of Lemma 7, corrected versions of lemmas
from Bickel and Chen (2009); Zhao et al. (2012); Bickel et al. (2015), supporting the
weak consistency theorem.
Lemma 4. Let Oab(e) = Oab(e) if a Ã∏= b, and Oab(e) = 2Oab(e) if a = b. For any
x > 0,
P
	
max
e
 O(e) ‚àíE
 O(e) | Z

‚àû> xn2
‚â§2Kn+2e‚àíx2n2/(8‚à•P ‚à•‚àû+4x/3).
Proof. This Lemma is adapted from Lemma 1.1 in Bickel and Chen (2009). There are
Kn possible values of e and ‚à•¬∑ ‚à•‚àûis the maximum of the K2 entries in the matrix.

782
Bayesian Community Detection
We use the union bound to pull these maxima out of the probability, giving the factor
Kn+2 on the right. Next it suÔ¨Éces to bound the tail probability of each variable
Oab(e) ‚àíE
 Oab(e) | Z

=

i,j

Aij ‚àíE(Aij | Z)

(1{ei = a, ej = b} + 1{ei = b, ej = a}).
The nab(e) variables in this sum are conditionally independent given Z, take values in
[‚àí2, 2], and have conditional mean zero given Z and conditional variance bounded by
4 var(Aij | Z) ‚â§4PZiZj(1 ‚àíPZiZj) ‚â§4‚à•P‚à•‚àû. Thus we can apply Bernstein‚Äôs inequality
to Ô¨Ånd that
P
	 Oab(e) ‚àíE
 Oab(e) | Z
 > xn2
‚â§2e‚àíx2n4/(8nab(e)‚à•P ‚à•‚àû+4xn2/3).
Finally we use the crude bound nab(e) ‚â§n2 and cancel one factor n2.
Lemma 5. DeÔ¨Åne Oab(e) = Oab(e) if a Ã∏= b, and Oab(e) = 2Oab(e) if a = b. Then, for
R(e, Z) as deÔ¨Åned in (2),
E( Oab | Z) = n2R(e, Z)PR(e, Z)T ‚àínDiag(R(e, Z) diag (P)).
Proof. A similar expression, not taking into account the absence of self-loops, appears
in Bickel and Chen (2009). The relevant computation for our situation is as follows:
E( Oab(e) | Z = c) =

iÃ∏=j
Pcicj1{ei = a, ej = b}
=

a‚Ä≤,b‚Ä≤
Pa‚Ä≤b‚Ä≤

iÃ∏=j
1{ci = a‚Ä≤, cj = b‚Ä≤}1{ei = a, ej = b}
=

a‚Ä≤,b‚Ä≤
Pa‚Ä≤b‚Ä≤

i,j
1{ci = a‚Ä≤, cj = b‚Ä≤}1{ei = a, ej = b}
‚àíŒ¥ab

i

a‚Ä≤
Pa‚Ä≤a‚Ä≤1{ci = a‚Ä≤}1{ei = a}
= n2 
a‚Ä≤,b‚Ä≤
Pa‚Ä≤b‚Ä≤Raa‚Ä≤(e, c)Rbb‚Ä≤(e, c) ‚àíŒ¥abn

a‚Ä≤
Pa‚Ä≤a‚Ä≤Raa‚Ä≤(e, c).
Lemma 6. The function œÑ : [0, 1] ‚ÜíR satisÔ¨Åes |œÑ(x) ‚àíœÑ(y)| ‚â§l(|x ‚àíy|), for l(x) =
2x(1 ‚à®log(1/x)).
Proof. Write the diÔ¨Äerence between x log x and y log y as |
 y
x (1+log s) ds|. The function
s ‚Üí1 + log s is strictly increasing on [0, 1] from ‚àí‚àûto 1 and changes sign at s = e‚àí1.
Therefore the absolute integral is bounded above by the maximum of
‚àí
 |x‚àíy|‚àße‚àí1
0
(1 + log s) ds = ‚àí(|x ‚àíy| ‚àße‚àí1) log

|x ‚àíy| ‚àße‚àí1
and
 1
1‚àí|x‚àíy|‚à®e‚àí1(1 + log s) ds ‚â§|x ‚àíy|.

S. L. van der Pas and A. W. van der Vaart
783
Lemma 7. For any probability matrix R,
HP (R) ‚â§HP (Diag(RT 1)

.
(10)
Furthermore, if (P, œÄ) is identiÔ¨Åable and the columns of R corresponding to positive
coordinates of œÄ are not identically zero, then the inequality is strict unless PœÉR is a
diagonal matrix for some permutation matrix PœÉ.
Proof. This Lemma is related to the proof that the likelihood modularity is consistent
given in Bickel and Chen (2009). This proof however rests on their incorrect Lemma
3.1, and thus we provide full details on how the argument can be adapted to avoid the
use of their Lemma 3.1 altogether.
For R a diagonal matrix the numbers (RPRT )ab/(R1)a(R1)b reduce to Pab. Conse-
quently, by the deÔ¨Ånition of HP ,
HP

Diag(f)

=

a,b
fafb œÑ(Pab).
(11)
For a general matrix R, by inserting the deÔ¨Ånition of œÑ,
HP (R) =

a,b
(RPRT )ab log (RPRT )ab
(R1)a(R1)b
+

a,b

(R1)a(R1)b ‚àí(RPRT )ab

log
	
1 ‚àí(RPRT )ab
(R1)a(R1)b

.
Because (R1)a(R1)b ‚àí(RPRT )ab = (R(1 ‚àíP)RT )ab, with 1 the (K √ó K)-matrix with
all coordinates equal to 1, we can rewrite this as

a,b

a‚Ä≤,b‚Ä≤
Raa‚Ä≤Rbb‚Ä≤

Pa‚Ä≤b‚Ä≤ log (RPRT )ab
(R1)a(R1)b
+ (1 ‚àíPa‚Ä≤b‚Ä≤) log
	
1 ‚àí(RPRT )ab
(R1)a(R1)b


.
By the information inequality for two-point measures, the expressions in square brackets
become bigger when (RPRT )ab/(R1)a(R1)b is replaced by Pa‚Ä≤b‚Ä≤, with a strict increase
unless these two numbers are equal. After making this substitution the term in square
brackets becomes œÑ(Pa‚Ä≤b‚Ä≤), and we can exchange the order of the two (double) sums and
perform the sum on (a, b) to write the resulting expression as

a‚Ä≤,b‚Ä≤
(RT 1)a‚Ä≤(RT 1)b‚Ä≤œÑ(Pa‚Ä≤b‚Ä≤) = HP

Diag(RT 1)

.
This proves the Ô¨Årst assertion (10) of the lemma.
If R attains equality, then also for every permutation matrix PœÉ, by the equality
HP (PœÉR) = HP (R) and the fact that (PœÉR)T 1 = RT 1, we have
HP (PœÉR) = HP

Diag((PœÉR)T 1)

.
(12)

784
Bayesian Community Detection
We shall show that if R satisÔ¨Åes this equality and PœÉR has a positive diagonal, then
PœÉR is in fact diagonal. Furthermore, we shall show that there exists PœÉ such that PœÉR
has a positive diagonal.
Fix some (PœÉ)m that maximizes the number of positive diagonal elements of PœÉR
over all permutation matrices PœÉ, and denote ¬ØR = (PœÉ)mR. Because the information
inequality is strict, the preceding argument shows that (12) can be true for PœÉ = (PœÉ)m
(giving PœÉR = ¬ØR) only if
Pa‚Ä≤b‚Ä≤ = ( ¬ØRP ¬ØRT )ab
( ¬ØR1)a( ¬ØR1)b
,
whenever ¬ØRaa‚Ä≤ ¬ØRbb‚Ä≤ > 0.
(13)
Denote the matrix on the right of the equality by Q.
If ¬ØR has a completely positive diagonal, then we can choose a = a‚Ä≤ and b = b‚Ä≤ and
Ô¨Ånd from (13), that Pab = Qab, for every a, b. If also ¬ØRaa‚Ä≤ > 0, then we can also choose
b = b‚Ä≤ and Ô¨Ånd that Pa‚Ä≤b = Qab, for every b. Thus the ath and a‚Ä≤th rows of P are
identical. Since all rows of P are diÔ¨Äerent by assumption, it follows that no a Ã∏= a‚Ä≤ with
¬ØRaa‚Ä≤ > 0 exists.
If ¬ØR does not have a fully positive diagonal, then the submatrix of ¬ØR obtained by
deleting the rows and columns corresponding to positive diagonal elements must be
the zero matrix, since otherwise we might permute the remaining rows and create an
additional nonzero diagonal element, contradicting that (PœÉ)m already maximized this
number. If I and Ic are the sets of indices of zero and nonzero diagonal elements, then
the preceding observation is that ¬ØRij is zero for every i, j ‚ààI. If œÄ > 0, then we need to
consider only R with nonzero columns. For i ‚ààI a nonzero element in the ith column
of ¬ØR must be located in the rows with label in Ic: for every i ‚ààI there exists ki ‚ààIc
with ¬ØRkii > 0. Then, for i, j ‚ààI,
(1) for a = ki, b = kj, a‚Ä≤ = i, b‚Ä≤ = j, (13) implies Qkikj = Pij.
(2) for a = ki, b ‚ààIc, a‚Ä≤ = i, b‚Ä≤ = b, (13) implies Qkib = Pib.
(3) for a = ki, b ‚ààIc, a‚Ä≤ = ki, b‚Ä≤ = b, (13) implies Qkib = Pkib.
We combine these three assertions to conclude that, for a, i ‚ààI and b ‚ààIc,
Pai = Pia
(1)
= Qkika
(2)
= Pika = Pkai,
Pab
(2)
= Qkab
(3)
= Pkab.
Together these imply that the ath and the kath row of P are equal. Since by assumption
they are not (if œÄ > 0), this case can actually not exist (i.e. k = 0).
Finally if œÄa = 0 for some a, then we follow the same argument, but we match only
every column i ‚ààI with œÄi > 0 to a row ki ‚ààIc. By the assumption on R such ki exist,
and the construction results in two rows of P that are identical in the coordinates with
œÄa > 0.

S. L. van der Pas and A. W. van der Vaart
785
Lemma 8. For any Ô¨Åxed (K √ó K)-matrix P with elements in [0, 1], uniformly in prob-
ability matrices R, as œÅn ‚Üí0,
1
œÅn
	
HœÅnP (Diag(RT 1)

‚àíHœÅnP (R)

‚ÜíGP (Diag(RT 1)

‚àíGP (R).
(14)
Furthermore, if (P, œÄ) is identiÔ¨Åable and the columns of R corresponding to positive
coordinates of œÄ are not identically zero, then the right side is strictly positive unless
SR is a diagonal matrix for some permutation matrix S.
Proof. From the fact that |(1 ‚àíu) log(1 ‚àíu) + u| ‚â§u2, for 0 ‚â§u ‚â§1, it can be veriÔ¨Åed
that, |œÅ‚àí1
n œÑ(œÅnu) ‚àí(u log œÅn + œÑ0(u))| ‚â§œÅn ‚Üí0, uniformly in 0 ‚â§u ‚â§1. It follows that,
uniformly in R,
1
œÅn
HœÅnP (R) = log œÅn

a,b
(RPRT )ab +

a,b
(R1)a(R1)bœÑ0
	 (RPRT )ab
(R1)a(R1)b

+ O(œÅn).
The Ô¨Årst term on the right is equal to log œÅn(RT 1)T P(RT 1), and hence is the same for
R and Diag(RT 1). Thus this term cancels on taking the diÔ¨Äerence to form the left side
of (14), and hence (14) follows.
The right side of (14) is nonnegative, because the left side is, by Lemma 7. This fact
can also be proved directly along the lines of the proof of Lemma 7, as follows. Write
GP (R) =

a,b

a‚Ä≤,b‚Ä≤
Raa‚Ä≤Rbb‚Ä≤

Pa‚Ä≤b‚Ä≤ log (RPRT )ab
(R1)a(R1)b
‚àí(RPRT )ab
(R1)a(R1)b

.
By the information inequality for two Poisson distributions the term in square brack-
ets becomes bigger if (RPRT )ab/(R1)a(R1)b is replaced by Pa‚Ä≤b‚Ä≤. It then becomes
œÑ0(Pa‚Ä≤b‚Ä≤) and the double sum on (a, b) can be executed to see that the resulting bound is
GP (Diag(RT 1)). Furthermore, the inequality is strictly unless (13) holds, with ¬ØR = R.
Since also GP (PœÉR) = GP (R), for every permutation matrix PœÉ, the Ô¨Ånal assertion of
the lemma is proved by copying the proof of Lemma 7.
Proof of Lemma 2
Proof. The second assertion of the lemma follows from the Ô¨Årst and the fact that
maxe QP (e) ‚â≤(log n)/n. It suÔ¨Éces to prove the Ô¨Årst assertion.
Recall that the Bayesian modularity is given by n‚àí2 times
n2QB(e) =

a‚â§b
log B

Oab(e) + 1
2, nab(e) ‚àíOab(e) + 1
2

+

a
log Œì(na(e) + Œ±).
(15)
We shall show that the Ô¨Årst sum on the right is equivalent to QML(e), and the second
sum is equivalent to QP (e). We show this by comparing the sums deÔ¨Åning the vari-
ous modularities term by term. For clarity we shall suppress the argument e. We will
repeatedly use the following bound from (Robbins, 1955): for n ‚ààN‚â•1,
Œì(n + 1) =
‚àö
2œÄnn+1/2e‚àínean,
(16)

786
Bayesian Community Detection
with (12n+1)‚àí1 ‚â§an ‚â§(12n)‚àí1, as well as the fact that Œì(s) is monotone increasing for
s ‚â•3/2. In addition, we will bound remainder terms by using the inequality x log((x +
c)/x) ‚â§c for c ‚â•0 and the fact that x log((x ‚àí1)/x) is bounded for x > 1.
First sum of (15)
Upper bound, case 1: Oab Ã∏= 0 and nab Ã∏= Oab. We apply (16):
logB(Oab + Œ≤1, nab ‚àíOab + Œ≤2) ‚â§log Œì(Oab + ‚åäŒ≤1‚åã+ 1)Œì(nab ‚àíOab + ‚åäŒ≤2‚åã+ 1)
Œì(nab + ‚åäŒ≤1 + Œ≤2‚åã)
= Oab log

Oab + ‚åäŒ≤1‚åã
nab + ‚åäŒ≤1 + Œ≤2‚åã‚àí1

+ (nab ‚àíOab) log
 nab ‚àíOab + ‚åäŒ≤2‚åã
nab + ‚åäŒ≤1 + Œ≤2‚åã‚àí1

+ (‚åäŒ≤1‚åã+ 1/2) log(Oab + ‚åäŒ≤1‚åã) + (‚åäŒ≤2‚åã+ 1/2) log(nab ‚àíOab + ‚åäŒ≤2‚åã)
‚àí(‚åäŒ≤1 + Œ≤2‚åã‚àí1/2) log(nab + ‚åäŒ≤1 + Œ≤2‚åã‚àí1) + log
‚àö
2œÄ ‚àí‚åäŒ≤1‚åã‚àí‚åäŒ≤2‚åã
+ ‚åäŒ≤1 + Œ≤2‚åã‚àí1 + Œ±ab + Œ≤ab ‚àíŒ≥ab,
where Œ±ab, Œ≤ab and Œ≥ab are bounded by constants. By the inequality x log((x+c)/x) ‚â§c
for c ‚â•0, and the fact that x log((x ‚àí1)/x) is bounded for x > 1, we Ô¨Ånd the upper
bound:
log B(Oab + Œ≤1, nab ‚àíOab + Œ≤2) ‚â§nabœÑ
Oab
nab

+ O(log nab).
Upper bound, case 2: nab = 1 and Oab = 0 or nab = Oab, or nab = 0. In both cases,
the corresponding term of the likelihood modularity vanishes, whereas the contribution
of the Bayesian modularity is either log B(1 + Œ≤1, Œ≤2), log(Œ≤1, 1 + Œ≤2), or log B(Œ≤1, Œ≤2).
Upper bound, case 3: nab ‚â•2 and Oab = 0 or nab = Oab. Again, the corresponding
term of the likelihood modularity vanishes. We show the computations for the case
nab = Oab; for the case Oab = 0, switch Œ≤1 and Œ≤2. By (16):
logB(Oab + Œ≤1, nab ‚àíOab + Œ≤2) = log B(nab + Œ≤1, Œ≤2) ‚â§log Œì(nab + ‚åäŒ≤1‚åã+ 1)Œì(Œ≤2)
Œì(nab + ‚åäŒ≤1 + Œ≤2‚åã)
= (nab + ‚åäŒ≤1‚åã) log

nab + ‚åäŒ≤1‚åã
nab + ‚åäŒ≤1 + Œ≤2‚åã

+ (1/2) log(nab + ‚åäŒ≤1‚åã)
‚àí(‚åäŒ≤1 + Œ≤2‚åã+ 1/2) log(nab + ‚åäŒ≤1 + Œ≤2‚åã) + log Œì(Œ≤2) + ‚åäŒ≤1 + Œ≤2‚åã‚àí1 + Œ¥ab ‚àíœµab,
where Œ¥ab and œµab are bounded by constants. Arguing as before, the Ô¨Årst term is bounded,
while the remainder is of order log(nab). A lower bound is found analogously.
Lower bound. The computations for the lower bound are completely analogous, ex-
cept that we require Oab + Œ≤1 ‚â•2 and nab ‚àíOab + Œ≤2 ‚â•2. We study four cases. The
cases (1) Oab ‚â•2 and nab ‚àíOab ‚â•2, (2) nab = 0 and (3) nab > 0 and nab = Oab or
Oab = 0 are similar to cases 1, 2 and 3 respectively of the upper bound. The fourth case
is nab ‚àíOab = 1 and Oab ‚â•2, or Oab = 1 and nab ‚àíOab ‚â•1. In both instances, the like-
lihood modularity is equality to a bounded term minus log nab. By similar calculations
as before, the Bayesian modularity is of the order log nab as well.

S. L. van der Pas and A. W. van der Vaart
787
Conclusion. We Ô¨Ånd:

a‚â§b
log B(Oab + Œ≤1, nab ‚àíOab + Œ≤2) =

a‚â§b
nabœÑ
Oab
nab

+ O(log n).
Second sum of (15)
We consider three cases. If na + ‚åäŒ±‚åã= 0, then Œ± > 0, implies na = 0, in which
case log Œì(na + Œ±) = log Œì(Œ±), which is bounded. In case na + ‚åäŒ±‚åã= 1, the term
log Œì(na + Œ±) is equal to either log Œì(1 + Œ±) or log Œì(Œ±) and thus bounded as well. For
the case na + ‚åäŒ±‚åã‚â•2, we study the upper bound Œì(na + Œ±) ‚â§Œì(na + ‚åäŒ±‚åã+ 1) and the
lower bound Œì(na + Œ±) ‚â•Œì(na + ‚åäŒ±‚åã). By applying (16) in both cases, we conclude:

a
log Œì(na + Œ±) =

a:na+‚åäŒ±‚åã‚â•2
na log na ‚àín + O(log n).
6
Strong consistency
We build upon the foundations from the previous Section to prove Theorem 1. We need
slightly adapted versions of the function HP , given by, with Œ¥ab equal to 1 or 0 if a = b
or not,
HP,n(R) = 1
2

a,b
(R1)a

(R1)b ‚àíŒ¥ab/n

œÑ
	(RPRT )ab ‚àíŒ¥ab

k PkkRka/n
(R1)a

(R1)b ‚àíŒ¥ab/n


.
(17)
For given functions tab : [0, 1] ‚ÜíR, let X(e) be the K √ó K matrix with entries
Xab(e) = tab
	 Oab(e)
n2

‚àítab
	E( Oab(e) | Z)
n2

.
(18)
Proof of Theorem 1 [strong consistency]
Proof. We Ô¨Årst prove the statement in case œÅn is Ô¨Åxed. By Theorem 2, e is weakly con-
sistent, and hence with probability tending to one it belongs to the set of classiÔ¨Åcations e
such that the fractions f(e) are close to œÄ, and the matrices R(e, Z) are close to Diag(œÄ)
after the appropriate permutation of the labels (that is, of rows of R(e, Z)). Therefore, it
is no loss of generality to assume that e is restricted to this set. By Lemmas 4 and 5, the
matrices O(e)/n2 are then close to R(e, Z)PR(e, Z)T ‚ÜíDiag(œÄ)PDiag(œÄ), and hence
are bounded away from zero and one if P has this property.
If e and Z diÔ¨Äer at m nodes, then e belongs to the set of e with ‚à•R(Z, Z)‚àíR(e, Z)‚à•1 =
m(2/n), by Lemma 1. In that case QB(e) ‚â•QB(Z), for some e in this set, and hence by
Lemma 2 QML(e) ‚àíQML(Z) + QP (e) ‚àíQP (Z) ‚â•‚àíŒ∑n, for some Œ∑n of order (log n)/n2.
It follows that:

788
Bayesian Community Detection

QML(e) ‚àíHP,n

R(e, Z)

‚àí

QML(Z) ‚àíHP,n

R(Z, Z)

‚â•HP,n

R(Z, Z)

‚àíHP,n

R(e, Z)

‚àí|QP (e) ‚àíQP (Z)| ‚àíŒ∑n.
(19)
The Ô¨Årst term on the right is bounded below by a multiple of m/n, by Lemmas 9
and 1. Because (x + Œ±) log x ‚àí(y + Œ±) log y =
 y
x (log s + (s + Œ±)/s) ds is bounded in
absolute value by a multiple of |x ‚àíy| log(x ‚à®y), if Œ± ‚â•0 and x, y > 0, the second
term ‚àí|QP (e) ‚àíQP (Z)| is bounded below by a multiple of m(log n)/n2, which is of
smaller order than m/n. We conclude that the left side of (19) is bounded below by
C1m/n. The left side is 
a,b(Xab(e)‚àíXab(Z)), for X deÔ¨Åned in (18) and t the function
with coordinates tab(o) = fa(e)(fb(e) ‚àíŒ¥ab/n)œÑ(o/fa(e)(fb(e) ‚àíŒ¥ab/n)). Because we
restrict e to classiÔ¨Åcations such that Oab(e)/nab(e) and fa(e)fb(e) are bounded away
from zero and one, only the values of the function œÑ on an open interval strictly within
(0, 1) matter. On any such interval œÑ has uniformly bounded derivatives, and hence the
bound of Lemma 12 is valid. Thus we Ô¨Ånd that
Pr

#(i : ei Ã∏= Zi) = m

‚â§Pr
	
sup
e:#(i:eiÃ∏=Zi)‚â§m
X(e) ‚àíX(Z)

‚àû‚â•C1m
n

‚â§C2Km
n
m

e‚àícm2/(m‚à•P ‚à•‚àû/n+m/n)
‚â§C2em log(Kne/m)‚àíc1mn.
The sum of the right side over m = 1, . . . , n tends to zero.
In case œÅn ‚Üí0, we follow the some proof, but in (19) use that HP,n(R(Z, Z)) ‚àí
HP,n(R(e, Z)) ‚â•œÅnC‚à•R(Z, Z) ‚àíR(e, Z)‚à•1 ‚â•œÅnC2m/n, by Lemma 11. Since œÅn ‚â´
(log n)/n by assumption, we have that the contribution m(log n)/n2 of QP (e) ‚àíQP (Z)
is still negligible and hence œÅnC2m/n is a lower bound for the left side of (19). As a
bound on the left side of the preceding display, we then obtain
n

m=1
Km
n
m

e‚àíc2œÅ2
nm2/(mœÅn/n+œÅnm/n) ‚â§
n

m=1
em log(Kne/m)‚àíc3œÅnmn.
This sum tends to zero provided that nœÅn ‚â´log n.
Lemmas 9‚Äì11 are explicit veriÔ¨Åcations of versions of condition IIIc of Bickel and
Chen (2009).
Lemma 9. If P is Ô¨Åxed and symmetric, (P, œÄ) is identiÔ¨Åable and 0 < P < 1, then, for
suÔ¨Éciently small Œ¥ > 0,
lim inf
n‚Üí‚àû
inf
0<‚à•R‚àíDiag(œÄ)‚à•<Œ¥
HP,n

Diag(RT 1)

‚àíHP,n(R)
‚à•Diag(RT 1) ‚àíR‚à•
> 0.
(20)
Proof. We can reparametrize the K √óK matrices R by the pairs (RT 1, R‚àíDiag(RT 1)),
consisting of the K vector f = RT 1 and the K √ó K matrix R ‚àíDiag(RT 1). The latter
matrix is characterized by having nonnegative oÔ¨Ä-diagonal elements and zero column

S. L. van der Pas and A. W. van der Vaart
789
sums, and can be represented in the basis consisting of all K √ó K matrices Œîbb‚Ä≤, for
b Ã∏= b‚Ä≤, deÔ¨Åned by: (Œîbb‚Ä≤)b‚Ä≤b‚Ä≤ = ‚àí1, (Œîbb‚Ä≤)bb‚Ä≤ = 1 and (Œîbb‚Ä≤)aa‚Ä≤ = 0, for all other entries
(a, a‚Ä≤), i.e. the b‚Ä≤th column of Œîbb‚Ä≤ has a 1 in the bth coordinate and a ‚àí1 on the b‚Ä≤th
coordinate and all its other columns are zero. Given any matrix R ‚â•0 the matrix
R ‚àíDiag(RT 1) can be decomposed as
R ‚àíDiag(RT 1) =

bÃ∏=b‚Ä≤
Œªbb‚Ä≤Œîbb‚Ä≤,
for Œªbb‚Ä≤ = Rbb‚Ä≤ ‚â•0. Since every Œîbb‚Ä≤ has exactly one nonzero oÔ¨Ä-diagonal element,
which is equal to 1, and in a diÔ¨Äerent location for each b Ã∏= b, the sum of the oÔ¨Ä-diagonal
elements of the matrix on the right side is 
b,b‚Ä≤ Œªbb‚Ä≤. Because the sum of all its elements
is zero, it follows that its sum of absolute elements is given by ‚à•R ‚àíDiag(RT 1)‚à•1 =
2 
bÃ∏=b‚Ä≤ Œªbb‚Ä≤.
Thus we obtain a further reparametrization R ‚Üî(f, Œª), in which R = Diag(f) +

bÃ∏=b‚Ä≤ Œªbb‚Ä≤Œîbb‚Ä≤. Here the vector f is a probability vector, and all Œªbb‚Ä≤ are nonnegative (as
Rbb‚Ä≤ ‚â•0). The nonnegativity of the diagonal elements of R gives the further restrictions
that 
bÃ∏=a Œªba ‚â§fa, for every a; in particular Œªba is zero for every b and a such that
fa = 0. Other restrictions on the Œªbb‚Ä≤ follow from the fact that R ‚â§1, but since we shall
be interested in Œªbb‚Ä≤ close to zero, these restrictions will not be active.
For given P, f and n, deÔ¨Åne the function
G(Œª) = HP,n
	
Diag(f) +

bÃ∏=b‚Ä≤
Œªbb‚Ä≤Œîbb‚Ä≤

.
Then we would like to show that there exists C such that
HP,n(Diag(RT 1)) ‚àíHP,n(R)
‚à•R ‚àíDiag(RT 1)‚à•1
= G(0) ‚àíG(Œª)
2 
bÃ∏=b‚Ä≤ Œªbb‚Ä≤ ‚â•C > 0,
for every f in a neighbourhood of œÄ, Œª in a neighbourhood of 0 intersected with {Œª :
Œª ‚â•0} and ‚à©a{Œª : 
bÃ∏a Œªba ‚â§fa}, and every suÔ¨Éciently large n. The numerator in the
quotient is g(0)‚àíg(1) for the function g(s) = G(sŒª). Writing this diÔ¨Äerence in the form
‚àíg‚Ä≤(0) ‚àí
 1
0 (g‚Ä≤(s) ‚àíg‚Ä≤(0)) ds gives that the numerator is equal to
‚àí‚àáG(0)T Œª ‚àí
 1
0

‚àáG(sŒª) ‚àí‚àáG(0)
T ds Œª.
(21)
Here ‚àáG is the gradient of G, where we only include partial derivatives with respect
to coordinates Œªbb‚Ä≤ that vary freely, i.e. not the coordinates Œªba for which fa = 0. It
suÔ¨Éces to show that the Ô¨Årst term is bounded below by a multiple of ‚à•Œª‚à•1 and that the
second is negligible relative to the Ô¨Årst, as n ‚Üí‚àû, uniformly in f in a neighbourhood
of œÄ and Œª in a neighbourhood of 0 intersected with {Œª : Œª ‚â•0}. Thus it is suÔ¨Écient to
show Ô¨Årst that for every coordinate Œªbb‚Ä≤ of Œª minus the partial derivative of G at Œª = 0
with respect to Œªbb‚Ä≤ is bounded away from 0, as n ‚Üí‚àûuniformly in f, and second that
every partial derivative is equicontinuous at Œª = 0 uniformly in f and large n.

790
Bayesian Community Detection
We have
G(Œª) = 1
2

a,a‚Ä≤
fa(Œª)

fa‚Ä≤(Œª) ‚àíŒ¥aa‚Ä≤/n

œÑ
	
R(Œª)PR(Œª)T 
aa‚Ä≤ ‚àíŒ¥aa‚Ä≤ea(Œª)/n
fa(Œª)

fa‚Ä≤(Œª) ‚àíŒ¥aa‚Ä≤/n


,
(22)
for
f(Œª) = f +

bÃ∏=b‚Ä≤
Œªbb‚Ä≤(Œîbb‚Ä≤1),
R(Œª) = Diag(f) +

bÃ∏=b‚Ä≤
Œªbb‚Ä≤Œîbb‚Ä≤,
ea(Œª) =

k
PkkRak(Œª) = Paafa +

bÃ∏=b‚Ä≤
Pb‚Ä≤b‚Ä≤Œªbb‚Ä≤(Œ¥ab ‚àíŒ¥ab‚Ä≤).
By a lengthy calculation, given in Lemma 10,
‚àÇ
‚àÇŒªbb‚Ä≤ G(Œª)|Œª=0 = ‚àí

a
faK(Pab‚Ä≤‚à•Pab) + 1
2nK(Pb‚Ä≤b‚Ä≤‚à•Pbb),
(23)
for K(p‚à•q) = p log(p/q) + (1 ‚àíp) log((1 ‚àíp)/(1 ‚àíq)) the Kullback‚ÄìLeibler divergence
between the Bernoulli distributions with success probabilities p and q. For f suÔ¨Éciently
close to œÄ the numbers fa such that œÄa > 0 are bounded away from zero, and hence

a faK(Pab‚Ä≤‚à•Pab) > 0, by identiÔ¨Åability of (P, œÄ), since it suÔ¨Éces that just one of
the terms of the sum is nonzero. The whole expression is bounded below by the min-
imum over (b, b‚Ä≤) of these numbers minus (2n)‚àí1 times the maximum of the numbers
K(Pb‚Ä≤b‚Ä≤‚à•Pbb), and hence is positive and bounded away from zero for suÔ¨Éciently large n.
To verify the equicontinuity in f of the partial derivatives, we can compute these
explicitly at Œª and take their limit as n ‚Üí‚àû. We omit the details of this calculation.
However, we note that every term of G(Œª) is a Ô¨Åxed function of the quadratic forms in Œª

fa +

bÃ∏=b‚Ä≤
Œªbb‚Ä≤(Œîbb‚Ä≤1)a

fa‚Ä≤ +

bÃ∏=b‚Ä≤
Œªbb‚Ä≤(Œîbb‚Ä≤1)a‚Ä≤ ‚àíŒ¥aa‚Ä≤/n

,
(24)
	
Diag(f) +

bÃ∏=b‚Ä≤
Œªbb‚Ä≤Œîbb‚Ä≤
P

Diag(f) +

bÃ∏=b‚Ä≤
Œªbb‚Ä≤ŒîT
bb‚Ä≤

aa‚Ä≤
‚àíŒ¥aa‚Ä≤
n

Paafa +

bÃ∏=b‚Ä≤
Pb‚Ä≤b‚Ä≤Œªbb‚Ä≤(Œ¥ab ‚àíŒ¥ab‚Ä≤)

.
(25)
These forms are obviously smooth in Œª, and their dependence and that of their deriva-
tives on n is seen to vanish as n ‚Üí‚àû. For f and Œª restricted to neighbourhoods of œÄ
and 0, the values of the quadratic forms are restricted to a domain in which the trans-
formation that maps them into G(Œª) is continuously diÔ¨Äerentiable. Thus the desired
equicontinuity follows by the chain rule.
Lemma 10. The partial derivatives of the function G at 0 deÔ¨Åned by (22) are given by
(23).
Proof. For given diÔ¨Äerentiable functions u and v the map œµ ‚Üíu(œµ)œÑ(v(œµ)/u(œµ)) has
derivative v‚Ä≤ log(v/(u ‚àív)) ‚àíu‚Ä≤ log(u/(u ‚àív)). We apply this for every given pair (a, a‚Ä≤)

S. L. van der Pas and A. W. van der Vaart
791
to the functions u and v obtained by taking Œªbb‚Ä≤ in (24) and (25) equal to œµ and all
other coordinates of Œª equal to zero. Then
u(0) = fa(fa‚Ä≤ ‚àíŒ¥aa‚Ä≤/n),
v(0) = fa(fa‚Ä≤ ‚àíŒ¥aa‚Ä≤/n)Paa‚Ä≤,
u‚Ä≤(0) = (Œîbb‚Ä≤1)a(fa‚Ä≤ ‚àíŒ¥aa‚Ä≤/n) + fa(Œîbb‚Ä≤1)a‚Ä≤,
v‚Ä≤(0) = (Œîbb‚Ä≤P)aa‚Ä≤fa‚Ä≤ + fa(Œîbb‚Ä≤P)a‚Ä≤a ‚àí(Œ¥aa‚Ä≤/n)Pb‚Ä≤b‚Ä≤(Œ¥ab ‚àíŒ¥ab‚Ä≤).
It follows that v(0)/(u(0) ‚àív(0))
=
Paa‚Ä≤/(1 ‚àíPaa‚Ä≤), and u(0)/(u(0) ‚àív(0))
=
1/(1 ‚àíPaa‚Ä≤). Hence in view of (17) the partial derivative in (23) is equal to
1
2

a,a‚Ä≤

v‚Ä≤(0) log
Paa‚Ä≤
1 ‚àíPaa‚Ä≤ ‚àíu‚Ä≤(0) log
1
1 ‚àíPaa‚Ä≤

.
We combine this with the equalities
(Œîbb‚Ä≤1)a =
‚éß
‚é™
‚é®
‚é™
‚é©
0
if a /‚àà{b, b‚Ä≤},
‚àí1
if a = b‚Ä≤,
1
if a = b,
(Œîbb‚Ä≤P)aa‚Ä≤ =
‚éß
‚é™
‚é®
‚é™
‚é©
0
if a /‚àà{b, b‚Ä≤},
‚àíPb‚Ä≤a‚Ä≤
if a = b‚Ä≤,
Pb‚Ä≤a‚Ä≤
if a = b.
If fa or fa‚Ä≤ are zero, then the method to obtain the values found for v(0)/(u(0) ‚àív(0))
and u(0)/(u(0) ‚àív(0)) in the preceding (substituting the given values of v(0) and u(0))
breaks down as we obtain a quotient of zeros. However, the values obtained are still
correct when interpreted as the limits from the right at 0. In (21) and (23) the gradient
‚àáG(0) and derivative at Œª = 0 may also be interpreted as limits from the right as Œª ‚Üì0
of the gradient. With this substitution the arguments go through. If both fa and fa‚Ä≤
are zero, the term involving (a, a‚Ä≤) disappears completely from the analysis.
Lemma 11. If S is Ô¨Åxed and symmetric, (S, œÄ) is identiÔ¨Åable and S > 0 coordinatewise,
then there exists C > 0 such that, for suÔ¨Éciently small Œ¥ > 0 and any œÅn ‚Üì0,
lim inf
n‚Üí‚àû
inf
0<‚à•R‚àíDiag(œÄ)‚à•<Œ¥
HœÅnS,n

Diag(RT 1)

‚àíHœÅnS,n(R)
œÅn‚à•Diag(RT 1) ‚àíR‚à•
‚â•C.
Proof. In the notation of the proof of Lemma 9 we must now show that G(0) ‚àíG(Œª) ‚â•
CœÅn‚à•Œª‚à•1, as n ‚Üí‚àû, uniformly in f in a neighbourhood of œÄ, and Œª in a positive
neighbourhood of 0. As in that proof we write G(0) ‚àíG(Œª) in the form (21) and see
that it suÔ¨Éces that the partial derivatives of G at 0 divided by œÅn tend to negative limits,
and that ‚à•‚àáG(Œª) ‚àí‚àáG(0)‚à•/œÅn becomes uniformly small as Œª is close enough to zero.
The partial derivative at 0 with respect to Œªbb‚Ä≤ is given in (23), where we must replace
P by œÅnS. Since the scaled Kullback‚ÄìLeibler divergence œÅ‚àí1
n K(œÅns‚à•œÅnt) of two Bernoulli
laws converges to the Kullback‚ÄìLeibler divergence K0(s‚à•t) = s log(s/t) + t ‚àís between
two Poisson laws of means s and t, as œÅn ‚Üí0, it follows that for œÅn ‚Üí0, uniformly in f,
1
œÅn
‚àÇ
‚àÇŒªbb‚Ä≤ G(Œª)|Œª=0 ‚Üí‚àí

a
faK0(Sab‚Ä≤‚à•Sab).
The right side is strictly negative for f close to œÄ, by the assumption of identiÔ¨Åability
of (S, œÄ).

792
Bayesian Community Detection
If P = œÅnS, then the function Œª ‚Üív(Œª) given in (25) takes the form v = œÅnvS, for
vS deÔ¨Åned in the same way but with S replacing P. The function u given in (24) does
not depend on P or S. Using again that the derivative of the map œµ ‚Üíu(œµ)œÑ(v(œµ)/u(œµ))
is given by v‚Ä≤ log(v/(u ‚àív)) ‚àíu‚Ä≤ log(u/(u ‚àív)), we see that the partial derivative with
respect to Œªbb‚Ä≤ of the (a, a‚Ä≤) term in the sum deÔ¨Åning G takes the form
œÅnv‚Ä≤
S log
œÅnvS
u ‚àíœÅvS
‚àíu‚Ä≤ log
u
u ‚àíœÅnvS
= œÅnv‚Ä≤
S log œÅn ‚àíœÅnv‚Ä≤
S log(vS/u) ‚àí(œÅnv‚Ä≤
S ‚àíu‚Ä≤) log(1 ‚àíœÅnvS/u).
Here u and vS are as in (24) and (25) (with P replaced by S), and depend on (a, a‚Ä≤). From
the fact that the column sums of the matrices R(Œª) do not depend on Œª, we have that

a,a‚Ä≤

(R(Œª)SR(Œª)T )aa‚Ä≤ ‚àíŒ¥aa‚Ä≤
n

k
PkkR(Œª)ak

= R(Œª)T 1SR(Œª)T 1 ‚àí

k
Pkk

a
R(Œª)ak
is constant in Œª. This shows that 
a,a‚Ä≤ v‚Ä≤
S = 0 and hence the contribution of the term
œÅnv‚Ä≤
S log œÅn to the partial derivatives of G vanishes. The term ‚àí(œÅnv‚Ä≤
S ‚àíu‚Ä≤) log(1 ‚àí
œÅnvS/u) can be expanded as (œÅnv‚Ä≤
S ‚àíu‚Ä≤)œÅnvS/u up to O(œÅ2
n), uniformly in f and Œª.
Since these are equicontinuous functions of Œª, it follows that œÅ‚àí1
n (‚àáG(Œª) ‚àí‚àáG(0))
becomes arbitrarily small if Œª varies in a suÔ¨Éciently small neighbourhood of 0.
Lemma 12 shows that in a neighborhood of the truth, there is not much variation in
the diÔ¨Äerences between the observed modularity and the modularity evaluated on the
expected number of connections between classes given the true labelling.
Lemma 12. There exists a constant c > 0 such that for X(e) as in (18), for every twice
diÔ¨Äerentiable function ta,b : [0, 1] ‚ÜíR with ‚à•t‚Ä≤
a,b‚à•‚àû‚à®‚à•t‚Ä≤‚Ä≤
a,b‚à•‚àû‚â§1, and every x > 0,
Pr
	
max
e:#(eiÃ∏=Zi)‚â§m
X(e) ‚àíX(Z)

‚àû> x

‚â§6
n
m

Km+2e‚àí
cx2n2
m‚à•P ‚à•‚àû/n+x .
Proof. Given Z there are at most
 n
m

groups of m candidate nodes that can be assigned
to have ei Ã∏= Zi, and the label of each node can be chosen in at most K ‚àí1 ways. Thus
conditioning the probability on Z, we can use the union bound to pull out the maximum
over e, giving a sum of fewer than
 n
m

Km terms. Next we pull out the norm giving
another factor K2. It suÔ¨Éces to combine this with a tail bound for a single variable
Xa,b(e) ‚àíXa,b(Z). Write t for ta,b.
Assume for simplicity of notation that ei = Zi, for i > m, and decompose
1
n2 Oab(e) = 1
n2


i‚â§m or j‚â§m
Aij1ei=a,ej=b +

i>m and j>m
Aij1ei=a,ej=b

=: S1 + S2.
Let Oab(Z)/n2 =: S‚Ä≤
1 + S2, with the same variable S2, be the corresponding decompo-
sition if e is changed to Z, and then decompose, where the expectation signs E denote
conditional expectations given Z,

S. L. van der Pas and A. W. van der Vaart
793
Xab(e) ‚àíXab(Z)
=

t(S1 + S2) ‚àít(ES1 + ES2)

‚àí

t(S‚Ä≤
1 + S2) ‚àít(ES‚Ä≤
1 + ES2)

= t(S1 + S2) ‚àít(ES1 + S2)
+

t(ES1 + S2) ‚àít(ES1 + ES2)

‚àí

t(ES‚Ä≤
1 + S2) ‚àít(ES‚Ä≤
1 + ES2)

+ t(ES‚Ä≤
1 + S2) ‚àít(S‚Ä≤
1 + S2).
The Ô¨Årst and third terms on the far right can be bounded above in absolute value by
‚à•t‚Ä≤‚à•‚àûtimes the increment. To estimate the second term we write it as
(S2 ‚àíES2)(ES1 ‚àíES‚Ä≤
1)
 1
0
 1
0
t‚Ä≤‚Ä≤
uS2 + (1 ‚àíu)ES2 + vES1 + (1 ‚àív)ES‚Ä≤
1

du dv.
Since the Ô¨Årst and second derivatives of t are uniformly bounded by 1, it follows that
Xab(e) ‚àíXab(Z)
 ‚â§|S1 ‚àíES1| + |S2 ‚àíES2| |ES1 ‚àíES‚Ä≤
1| + |S‚Ä≤
1 ‚àíES‚Ä≤
1|.
The variable S1 ‚àíES1 is a sum of fewer than 2mn independent variables, each with con-
ditional mean zero, bounded above by 1/n2 and of variance bounded above by ‚à•P‚à•‚àû/n4.
Therefore Bernstein‚Äôs inequality gives that
P

|S1 ‚àíES1| > x

‚â§e‚àí1
2 x2/(2mn‚à•P ‚à•‚àû/n4+x/(3n2)).
This is as the exponential factor in the bound given by the lemma, for appropriate c.
The variable S‚Ä≤
1 ‚àíES‚Ä≤
1 can be bounded similarly. Furthermore |ES1 ‚àíES‚Ä≤
1| ‚â§4mn/n2 =
4m/n, and S2 ‚àíES2 is the sum of fewer than n2 variables as before, so that
P

|S2 ‚àíES2| |ES1 ‚àíES‚Ä≤
1| > x

‚â§e‚àí1
2 (xn/(4m))2/(n2‚à•P ‚à•‚àû/n4+xn/(12mn2)).
The exponent has a similar form as before, except for an additional factor n/m ‚â•1.
References
Abbe, E., Bandeira, A. S., and Hall, G. (2014). ‚ÄúExact Recovery in the Stochas-
tic Block Model.‚Äù ArXiv:1405.3267v4. MR3447993. doi: https://doi.org/10.1109/
TIT.2015.2490670.
775, 776
Airoldi, E. M., Blei, D. M., Fienberg, S. E., and Xing, E. P. (2008). ‚ÄúMixed Membership
Stochastic Blockmodels.‚Äù Journal of Machine Learning Research, 9: 1981‚Äì2014.
767
Bickel, P. J. and Chen, A. (2009). ‚ÄúA Nonparametric View of Network Models and
Newman-Girvan and Other Modularities.‚Äù Proceedings of the National Academy of
Sciences of the United States of America, 106(50): 21068‚Äì21073.
767, 768, 769, 771,
772, 773, 775, 776, 777, 778, 781, 782, 783, 788
Bickel, P. J., Chen, A., Zhao, Y., Levina, E., and Zhu, J. (2015). ‚ÄúCorrection to the Proof
of Consistency of Community Detection.‚Äù The Annals of Statistics, 43(1): 462‚Äì466.
MR3311866. doi: https://doi.org/10.1214/14-AOS1271.
768, 776, 781

794
Bayesian Community Detection
Channarond, A., Daudin, J.-J., and Robin, S. (2012). ‚ÄúClassiÔ¨Åcation and Estimation
in the Stochastic Blockmodel Based on the Empirical Degrees.‚Äù Electronic Journal
of Statistics, 6: 2574‚Äì2601. MR3020277. doi: https://doi.org/10.1214/12-EJS753.
767, 768, 776, 779
Chen, K. and Lei, J. (2014). ‚ÄúNetwork Cross-Validation for Determining the Number
of Communities in Network Data.‚Äù ArXiv:1411.1715v1.
769
Chen, Y. and Xu, J. (2014). ‚ÄúStatistical-Computational TradeoÔ¨Äs in Planted Problems
and Submatrix Localization with a Growing Number of Clusters and Submatrices.‚Äù
ArXiv:1402.1267v2. MR3491121.
775, 776
CÀÜome, E. and Latouche, P. (2014). ‚ÄúModel Selection and Clustering in Stochastic Block
Models with the Exact Integrated Complete Data Likelihood.‚Äù ArXiv:1303.2962.
MR3441229. doi: https://doi.org/10.1177/1471082X15577017.
777
Csardi, G. and Nepusz, T. (2006). ‚ÄúThe igraph Software Package for Complex Network
Research.‚Äù InterJournal Complex Systems, 1695.
778
Gao, C., Ma, Z., Zhang, A. Y., and Zhou, H. H. (2015). ‚ÄúAchieving Optimal Misclassi-
Ô¨Åcation Proportion in Stochastic Block Model.‚Äù ArXiv:1505.03772v5.
776
Gao, C., Ma, Z., Zhang, A. Y., and Zhou, H. H. (2016). ‚ÄúCommunity Detection in
Degree-Corrected Block Models.‚Äù ArXiv:1607.06993.
776
Ghosal, S., Ghosh, J. K., and van der Vaart, A. W. (2000). ‚ÄúConvergence rates
of posterior distributions.‚Äù The Annals of Statistics, 28(2): 500‚Äì531. MR1790007.
doi: https://doi.org/10.1214/aos/1016218228.
776
Glover, F. (1989). ‚ÄúTabu Search ‚Äì Part I.‚Äù ORSA Journal on Computing, 1(3): 190‚Äì206.
777
Hayashi, K., Konishi, T., and Kawamoto, T. (2016). ‚ÄúA Tractable Fully Bayesian
Method for the Stochastic Block Model.‚Äù ArXiv:1602.02256v1.
768
Hofman, J. M. and Wiggins, C. H. (2008). ‚ÄúBayesian Approach to Network Modularity.‚Äù
Physical Review Letters, 100: 258701.
768
Holland, P. W., Laskey, K. B., and Leinhardt, S. (1983). ‚ÄúStochastic Blockmod-
els: First Steps.‚Äù Social Networks, 5: 109‚Äì137. MR0718088. doi: https://doi.org/
10.1016/0378-8733(83)90021-7.
767
Jin, J. (2015). ‚ÄúFast Community Detection by SCORE.‚Äù The Annals of Statistics, 43(1):
57‚Äì89. MR3285600. doi: https://doi.org/10.1214/14-AOS1265.
768
Karrer, B. and Newman, M. E. J. (2011). ‚ÄúStochastic Blockmodels and Com-
munity Structure in Networks.‚Äù Physical Review E, 83: 016107. MR2788206.
doi: https://doi.org/10.1103/PhysRevE.83.016107.
767
Kpogbezan, G. B., van der Vaart, A. W., van Wieringen, W. N., Leday, G. G. R., and
van de Wiel, M. A. (2016). ‚ÄúAn empirical Bayes approach to network recovery using
external knowledge.‚Äù ArXiv:1605.07514.
769

S. L. van der Pas and A. W. van der Vaart
795
Lei, J. and Rinaldo, A. (2015). ‚ÄúConsistency of Spectral Clustering in Stochas-
tic
Block
Models.‚Äù
The
Annals
of
Statistics,
43(1):
215‚Äì237.
MR3285605.
doi: https://doi.org/10.1214/14-AOS1274.
768, 775
McDaid, A. F., Brendan Murphy, T., Friel, N., and Hurley, N. J. (2013). ‚ÄúIm-
proved Bayesian Inference for the Stochastic Block Model with Application to Large
Networks.‚Äù Computational Statistics and Data Analysis, 60: 12‚Äì31. MR3007016.
doi: https://doi.org/10.1016/j.csda.2012.10.021.
769, 771, 772, 777
Mossel,
E.,
Neeman,
J.,
and
Sly,
A.
(2012).
‚ÄúReconstruction
and
Esti-
mation
in
the
Planted
Partition
Model.‚Äù
ArXiv:11202.1499v4.
MR3383334.
doi: https://doi.org/10.1007/s00440-014-0576-6.
775
Newman, M. and Girvan, M. (2004). ‚ÄúFinding and Evaluating Community Structure
in Networks.‚Äù Physical Review E, 69: 026113. MR1975193. doi: https://doi.org/
10.1103/PhysRevE.67.026126.
767
Nowicki, K. and Snijders, T. A. B. (2001). ‚ÄúEstimation and Prediction for Stochastic
Blockstructures.‚Äù Journal of the American Statistical Association, 96(455): 1077‚Äì
1087.
MR1947255.
doi:
https://doi.org/10.1198/016214501753208735.
768,
771, 772, 777
Park, Y. and Bader, J. S. (2012). ‚ÄúHow Networks Change with Time.‚Äù Bioinformatics,
28(12): i40‚Äìi48.
767
Pati, D. and Bhattacharya, A. (2015). ‚ÄúOptimal Bayesian Estimation in Stochastic
Block Models.‚Äù ArXiv:1505.06794.
768
Robbins, H. (1955). ‚ÄúA Remark on Stirling‚Äôs Formula.‚Äù The American Mathematical
Monthly, 62(1): 26‚Äì29. MR0069328. doi: https://doi.org/10.2307/2308012.
785
Rohe, K., Chatterjee, S., and Yu, B. (2011). ‚ÄúSpectral Clustering and the High-
Dimensional Stochastic Blockmodel.‚Äù The Annals of Statistics, 39(4): 1878‚Äì1915.
MR2893856. doi: https://doi.org/10.1214/11-AOS887.
768
Saldana, D. F., Yu, Y., and Feng, Y. (2014). ‚ÄúHow Many Communities Are There?‚Äù
ArXiv:1412.1684v1.
MR3610418.
doi:
https://doi.org/10.1080/10618600.
2015.1096790.
769
Sarkar, P. and Bickel, P. J. (2015). ‚ÄúRole of Normalization in Spectral Clustering
for Stochastic Blockmodels.‚Äù The Annals of Statistics, 43(3): 962‚Äì990. MR3346694.
doi: https://doi.org/10.1214/14-AOS1285.
768
Snijders, T. A. and Nowicki, K. (1997). ‚ÄúEstimation and Prediction for Stochastic Block-
models for Graphs with Latent Block Structure.‚Äù Journal of ClassiÔ¨Åcation, 14: 75‚Äì
100. MR1449742. doi: https://doi.org/10.1007/s003579900004.
767, 768
Suwan, S., Lee, D. S., Tang, R., Sussman, D. L., Tang, M., and Priebe, C. E. (2016).
‚ÄúEmpirical Bayes estimation for the stochastic blockmodel.‚Äù Electronic Journal of
Statistics, 10: 761‚Äì782. MR3477741. doi: https://doi.org/10.1214/16-EJS1115.
768

796
Bayesian Community Detection
Wang, Y. X. R. and Bickel, P. J. (2015). ‚ÄúLikelihood-Based Model Selection for
Stochastic Block Models.‚Äù ArXiv:1502.02069v1. MR3196592. doi: https://doi.org/
10.1080/15598608.2013.771546.
769
Zachary, W. W. (1977). ‚ÄúAn Information Flow Model for ConÔ¨Çict and Fission in Small
Groups.‚Äù Journal of Anthropological Research, 33(4): 452‚Äì473.
777, 778, 779
Zhang, A. Y. and Zhou, H. H. (2015). ‚ÄúMinimax Rates of Community Detection in
Stochastic Block Models.‚Äù Preprint available at http://www.stat.yale.edu/~hz68/
CommunityDetection.pdf.
MR3546450.
doi:
https://doi.org/10.1214/15-
AOS1428.
775, 776
Zhao, Y., Levina, E., and Zhu, J. (2012). ‚ÄúConsistency of Community Detection in Net-
works under Degree-Corrected Stochastic Block Models.‚Äù The Annals of Statistics,
40(4): 2266‚Äì2292. MR3059083. doi: https://doi.org/10.1214/12-AOS1036.
767,
768, 776, 777, 781

