1
Quantum-Inspired Machine Learning: a Survey
Larry Huynh, Jin Hong, Ajmal Mian, Hajime Suzuki, Yanqiu Wu, and Seyit Camtepe
Abstract—Quantum-inspired Machine Learning (QiML) is a burgeoning field, receiving global attention from researchers for its
potential to leverage principles of quantum mechanics within classical computational frameworks. However, current review literature
often presents a superficial exploration of QiML, focusing instead on the broader Quantum Machine Learning (QML) field. In response
to this gap, this survey provides an integrated and comprehensive examination of QiML, exploring QiML’s diverse research domains
including tensor network simulations, dequantized algorithms, and others, showcasing recent advancements, practical applications,
and illuminating potential future research avenues. Further, a concrete definition of QiML is established by analyzing various prior
interpretations of the term and their inherent ambiguities. As QiML continues to evolve, we anticipate a wealth of future developments
drawing from quantum mechanics, quantum computing, and classical machine learning, enriching the field further. This survey serves
as a guide for researchers and practitioners alike, providing a holistic understanding of QiML’s current landscape and future directions.
Index Terms—Quantum-inspired Machine Learning, Quantum Machine Learning, Quantum Computing, Quantum Algorithms,
Machine Learning, Tensor Networks, Dequantized Algorithms, Quantum Circuit Simulation
✦
1
INTRODUCTION
T
HE
field
of
Quantum-Inspired
Machine
Learning
(QiML) has seen substantial growth, garnering interest
from researchers globally. A specialized subset of Quantum
Machine Learning (QML), QiML focuses on developing
classical machine learning algorithms inspired by principles
of quantum mechanics within a classical computational
framework, commonly referenced as the “classical-classical”
quadrant of QML categorization as shown in Figure 1. QiML
represents a multifaceted research domain, with investi-
gations pushing to exceed conventional, classical state-of-
the-art results, or exploring the expressivity provided by
quantum formulations.
To situate QiML within the context of QML, we briefly
expound upon the latter. QML, more broadly, sits at the
fascinating intersection of quantum computing and ma-
chine learning. The dominant research field concerns the
“classical-quantum” domain, and explores the use of quan-
tum hardware to accelerate and enhance machine learning
strategies. Here, two challenges present in classical machine
learning are addressed. First, the increasing size and com-
plexity of datasets in many fields have created computa-
tional challenges that classical machine learning struggles
to manage efficiently. Secondly, quantum computing offers
the potential to solve complex problems that are currently
infeasible with classical computation methods [1]. Practical
evaluation of QML algorithms on actual quantum hard-
ware, however, is currently limited by factors such as the
limited number of qubits, high error rates in quantum gates,
difficulty in maintaining quantum states (decoherence), and
challenges associated with quantum error correction [2]. As
a result, the QML landscape has been primarily shaped
by theoretical considerations, with recent advancements in
noisy-intermediate scale quantum (NISQ) devices providing
an early, empirical glimpse into the potential of full-scale
L. Huynh, J. Hong, and A. Mian are with the University of Western Australia.
Emails: {larry.huynh, jin.hong, ajmal.mian}@uwa.edu.au.
H.
Suzuki,
Y.
Wu,
and
S.
Camtepe
are
with
CSIRO’s
Data61,
Marsfield,
NSW,
Australia.
Emails:
{hajime.suzuki,
yanqiu.wu,
seyit.camtepe}@data61.csiro.au.
Fig. 1. Four approaches to QML [4], based on whether quantum or
classical data/processing is used. QiML methods describe the “CC”
mode (blue shading).
quantum computing [3]. As such, the true extent and impact
of QML on the machine learning landscape remains an
ongoing research topics.
QiML has evolved in tandem with QML research. In-
stances of often cited research domains include tensor net-
work quantum simulations and dequantized algorithms [4],
[5]. However, in contrast with QML, discoveries in QiML
are frequently backed by numerical evidence, facilitated by
the absence of quantum hardware requirements, thereby
enabling easier quantitative evaluations compared to other
QML subsets. While QiML research is flourishing, current
survey literature often neglects this field, with a larger
focus given to QML as a whole. Often, QiML is only
briefly mentioned or treated superficially [5], [6], [7], [8],
arXiv:2308.11269v1  [cs.LG]  22 Aug 2023

2
[9], [10]. Practical use cases of QiML, their applications, and
comparative analyses with standard classical benchmarks
often remain unexplored. This points to a crucial need for a
standalone, in-depth review of QiML as a distinct field.
Responding to this literature gap, our survey aims to
provide a comprehensive, integrated discussion on the var-
ious facets of QiML. We aim to provide an accessible and
comprehensive overview of how QiML is used in practice,
detailing its recent advancements and giving readers an
understanding of the field’s progression. The reader should
note that while exploring QiML methods from the lens of
quantum mechanics and categorizing methods based on
sources of inspiration would be of interest, this survey
approaches the field from an applications perspective. The
contributions of this survey are to provide an overview of
the progression of QiML and its research directions in recent
years, and to identify the future directions of QiML research.
Specifically, they are:
•
To highlight and classify existing QiML methods;
•
To establish a concrete definition of QiML, account-
ing for its multi-directional research trends;
•
To discuss the practical applications of these meth-
ods, specifically identifying the tasks to which QiML
techniques have currently been applied;
•
To discuss the limiting factors of QiML in practice,
and;
•
To explore and discuss the potential future directions
of QiML research.
2
QUANTUM-INSPIRED MACHINE LEARNING
In this section, we dissect the QiML term into its constituent
parts — “quantum-inspired” and “machine learning” — for
a comprehensive understanding. Following this, we unpack
the QiML term itself, integrating our analysis of its indi-
vidual components. Our goal is to address inconsistencies
in past literature, identify common threads, and propose
a precise definition for QiML. This serves as a reliable
compass, guiding future research in this dynamic field.
2.1
“Quantum-Inspired”
The term “quantum-inspired” was introduced by Moore
and Narayanan [11] in the context of computing for the first
time in 1995. The term was used to differentiate between
two types of computational methods: “pure” quantum com-
putation and quantum-inspired computation. The former
is firmly rooted in quantum mechanical concepts, such as
standing waves, interference, and coherence, and can only
be executed on a quantum computer. On the other hand,
“quantum-inspired” computing refers to practical methods
that have been derived from these concepts. These meth-
ods do not require a quantum computer, but rather utilize
classical computers or algorithms to simulate quantum ef-
fects and achieve computational advantages. The catego-
rization of these two types of methods was significant at
the time, since quantum computing methods were not yet
practically realizable due to the technological inability of
implementing stable quantum systems with robust error
correction [12]. The potential of quantum computing was
known and acknowledged, as well as the challenges of
practical implementation; many pure quantum algorithms
still currently operate at a theoretical level [2]. This led
to research efforts in exploring the utilization of quantum
mechanics in classical computing.
Han and Kim were among the pioneers giving name to
quantum-inspired algorithms, and proposed the “quantum-
inspired evolutionary algorithm” (QIEA). Extending upon
prior works [13], [14], QIEAs describe evolutionary algo-
rithms that are inspired by quantum mechanical concepts,
specifically employing “Q-bits” and “Q-gates” to model
populations and evolutionary processes [15]. These are the
classical analogues of quantum qubits and quantum gates;
Q-bits serve as probabilistic representations that maintain
population diversity among individuals through the linear
superposition of states, while Q-gates act as variational
operators, driving individuals towards an optimal solution
by modifying the probability distributions associated with
Q-bits. Since these operate on classical computers, quantum
phenomena is not observed, e.g. state collapse does not
occur when the Q-bit state is “measured”. Nevertheless,
significant performance improvements were observed when
compared to its classical counterpart, highlighting the bene-
fits of using quantum-inspired methods.
QIEAs form a subset of quantum-inspired metaheuris-
tics, which are optimization techniques developed for find-
ing approximate or near-optimal solutions inspired by
quantum mechanics principles but are implemented on
classical computers. This research area has experienced sig-
nificant success in recent decades with various gains in per-
formances [16], [17], [18], [19]. Quantum-inspired Genetic
Algorithms (QGA) [13], [20] also belong to this category,
employing Q-bits for probabilistic solution encoding and
Q-gates for genetic operations like crossover and mutation.
These features enable QGAs to maintain greater population
diversity [15]. Quantum-inspired Particle Swarm Optimiza-
tion (QPSO) [21] represents particles as Q-bits, allowing for
a probabilistic representation of the solution space instead
of fixed positions and velocities. By maintaining particles
in a superposition of states, the search space diversity and
exploration capabilities are enhanced. Similarly, quantum-
inspired ant colony optimization (QACO) [22] utilizes Q-
bits and Q-gates to improve the classical ACO’s pheromone
update mechanism. The traversal of the solution space via
these mechanisms has been shown to augment exploration
and exploitation capabilities [23], [24]. Quantum Simulated
Annealing (QSA) [25] employs Monte Carlo methods to
efficiently simulate classical annealing processes. Additional
quantum concepts are also utilized, such as quantum ran-
dom walks for efficient exploration of the energy landscape,
quantum phase estimation for optimizing the annealing
schedule, and quantum tunneling to escape local minima.
Artificial intelligence methods have also benefited from
quantum concepts. Menneer et al. [26] investigated a neural
network training method inspired by the “many universes”
interpretation of quantum behaviour. The authors trained
multiple single-layer networks on individual patterns and
derived a quantum network with weights calculated as
a superposition of the individual network weights. This
technique demonstrated increased efficiency compared to
classical models and potentially provided a more accurate
representation of human cognitive processes.

3
It is important to note that the research goals in devel-
oping quantum-inspired methods can vary. Typically, the
objectives include achieving faster and more stable conver-
gence, enhancing the effectiveness of the solution search, or
a combination of both [27].
Some works also introduce unique additions to their
specification of “quantum-inspired”. Moore and Narayanan
[11] further characterised quantum-inspired computing al-
gorithms by stipulating that their output need only be
verified by classical computing methods, and not requiring
a quantum computer for the task. Manju and Nigam [17]
constrain quantum-inspired computing to methods for solv-
ing engineering problems with cyclic or recurrent behavior.
Further, some studies emphasize the use of quantum bits or
Q-bits as the defining aspect of being quantum-inspired [27],
[28], while others characterise different quantum concepts
to be the core underpinning of the “quantum inspired”
definition.
These minor distinctions in defining “quantum inspired”
do not significantly impact the overall understanding of
the term, as the core concept remains consistent with the
generalization outlined earlier.
2.2
“Machine Learning”
Machine learning, as a field, has evolved significantly since
its inception, with its definition and scope adapting to reflect
advancements in techniques and computational capabili-
ties. Initially conceptualized in the 1950s as the concept
of “programming computers to learn from experience” by
Arthur Samuel [29], machine learning has witnessed several
transformative milestones. Throughout these developments,
the core idea of learning from data to make predictions
or decisions has persisted, although the specific models,
techniques, and learning paradigms have diversified over
time. Several foundational definitions of machine learning
have been offered, each emphasizing various aspects of
what constitutes the field. For example, [30] highlights
the importance of learning from experience and improving
performance over time, thus underscoring the iterative and
adaptive nature of machine learning. Goodfellow et al. [31]
position machine learning as a sub-field of artificial intel-
ligence, concerned with building algorithms that rely on a
collection of examples of some phenomenon to be useful.
A few authors define machine learning rather as a broad
collection of algorithms that learn patterns over feature
spaces [32], [33]. Clear commonalities have been agreed
upon, such as the importance of learning from patterns
inherent within data [30], [31] via automatic processes [32],
[34] without explicit programming [35], and the ability to
improve performance based on the experience or data it is
exposed to. While the specific techniques and approaches
within machine learning have evolved and diversified, its
fundamental definition has remained consistently cohesive
and integral to the field.
2.3
“Quantum-Inspired Machine Learning”
Both quantum computing and machine learning have
gained tremendous popularity in the past couple of decades
with significant advances made compared to when they
were first introduced. More recently, researchers have
turned their attention to an inner subset at the intersection of
these fields; “quantum-inspired machine learning” (QiML).
The interpretation of this term by researchers, however, has
varied significantly in the literature. Hence, this subsection
aims to explore different perspectives and approaches taken
by researchers in their attempts to define “quantum-inspired
machine learning,” shedding light on the nuances and chal-
lenges involved in characterizing this rapidly developing
field. We will thus also argue for a concrete description of
the quantum-inspired machine learning term, which will
better promote clarity, and assist in characterizing method-
ologies within this, and related domains.
The terms “quantum-inspired”, or “quantum-like” ma-
chine learning have, in early reviews, focused on describing
optimization techniques inspired by quantum phenomena,
and run on classical computers [33], likely in the absence
of parameterized, iterative pattern recognition models more
akin to classical machine learning methods. Other authors
have corroborated this idea, with more explicit mention
of machine learning rather than optimization [6], [7], [9],
[10]. These definitions are consistent with the expected
combination of the individual terms “quantum-inspired”
and “machine learning”. In recent years, the umbrella of
QiML has been extended to include tensor network ma-
chine learning models that parallel classical methodologies
[36], [37], as well as ”dequantized” algorithms, which aim
to develop classical analogs of quantum algorithms with
comparable computational advantages [4]. Although ten-
sor network techniques agree with the concept of quan-
tum inspiration (given that tensor decompositions aspire to
model complex quantum wave functions), the classification
of dequantized algorithms as ”quantum-inspired” may not
wholly align with prevailing definitions of QiML. In the
case of dequantized algorithms, the ”inspiration” derives
not from quantum mechanics itself but from the scrutiny of
claims of quantum supremacy, or rather by the quantum
algorithms themselves. In this sense, the relationship to
quantum mechanics is indirect; the focus is on understand-
ing the potential of quantum algorithms in classical settings.
While the prevailing definitions of QiML offer some flex-
ibility and accommodate a variety of quantum applications
to machine learning, they may also inadvertently lead to
imprecise categorizations. Notably, certain techniques might
be labeled as QiML when they may not accurately belong
to this domain. Consider, for example, a quantum kernel
used in a quantum support vector machine or a quantum
variational circuit, methods typically relying on quantum
circuit implementation, that has been simulated on classical
hardware, as in [38], [39] using tools like Qiskit [40] or
Pennylane [41]. These algorithms are now implemented on
classical hardware and do not necessarily require quantum
hardware, yet are often considered part of the broader
quantum machine learning (QML) context, not QiML. The
main reason for this distinction is not immediately clear, but
may be related to the nature of the algorithms themselves,
and that quantum circuit implementations have been col-
loquial considered as QML, regardless of implementation
device. Another explanation could be tied to the efficiency of
implementation: if the translation of a quantum computing
process to a classical setting is not intrinsically efficient,
regardless of the circuit or input size, and does not scale

4
beyond the limits of classical computation, the technique
could be more appropriately classified as QML.
Addressing these inconsistencies is crucial for a more ac-
curate understanding of the evolving landscape of QiML, as
it will allow researchers to effectively build upon previous
work, avoid confusion, and foster more precise communi-
cation within the academic community. We now attempt
to pin down an appropriate definition of QiML. In doing
so, the terms QML and QiML warrant clarification to better
understand their roles in the interdisciplinary area between
quantum computing and machine learning. Broadly, QML
represents the integration of quantum computing and ma-
chine learning principles, forming an umbrella term that
includes related concepts within this intersection, one of
which is QiML. To gain a deeper understanding of the
broader QML landscape, we examine a typology introduced
by Schuld and Petruccione [4], depicted in Figure 1. This
typology categorizes QML based on whether the data source
is a classical (C) or quantum (Q) system and whether the
data processing device is classical (C) or quantum (Q). Four
distinct categories emerge, each highlighting a different
aspect of the interplay between quantum computing and
machine learning:
•
CC: Classical data and classical processing. This cat-
egory is where QiML primarily resides. Methods in
this category are inspired by quantum mechanics but
still use classical data and processing.
•
QC: Quantum data and classical processing. In this
category, machine learning techniques are used to
analyze quantum data or measurement outcomes
from quantum systems and experiments.
•
CQ: Classical data and quantum processing. Quan-
tum computing is utilized to process conventional
data, often with the objective of developing quantum
algorithms for data mining.
•
QQ: Quantum data and quantum processing. This
category explores processing quantum data with
quantum devices, either by inputting experimental
measurements into a quantum computer or using
a quantum computer to simulate and subsequently
analyze the behavior of quantum systems.
These four categories have been corroborated and uti-
lized in various studies within the field of quantum ma-
chine learning, as demonstrated in the literature [2], [10],
[42]. Some researchers have also proposed a contemporary
categorization scheme that reflects the evolving landscape
of QML [7], [10]. Here, QML is divided into three distinct
categories:
•
Quantum Machine Learning: all quantum adap-
tations of classical ML algorithms that necessitate
quantum computation for their execution.
•
Quantum-inspired Machine Learning: the integra-
tion of quantum computing concepts to enhance
traditional machine learning algorithms, without re-
quiring actual quantum computation.
•
Hybrid Classical-Quantum Machine Learning: the
fusion of classical and quantum algorithms, aim-
ing to optimize performance and minimize learning
costs by exploiting the strengths of both approaches.
These perspectives are consistent with prior QiML defi-
nitions, while also specifying classical computation as a cru-
cial component. To consolidate this aspect with conventional
understanding, and also accommodate for the existence of
dequantized algorithms, we now propose a concrete defini-
tion of “quantum-inspired machine learning”.
Definition
1:
Quantum-inspired
machine
learning
(QiML) refers to machine learning algorithms that draw
inspiration from principles of quantum mechanics or quan-
tum computing constructs, but do not necessitate quantum
processing and can be executed on classical hardware.
This definition encapsulates the following pivotal as-
pects:
•
The foundational principles of machine learning;
•
Inspiration from quantum phenomena or quantum
computing, including quantum algorithms, thus ac-
knowledging the significance of dequantized algo-
rithms within QiML;
•
The capacity for problem representation and compu-
tation on classical hardware, thereby incorporating
the ability to simulate quantum hardware.
Our aim is to provide clarity and guidance for fu-
ture research in this rapidly evolving field. This definition
acknowledges the growing intersection between quantum
computing and machine learning and fosters a more focused
and constructive discourse within the QiML landscape.
3
SELECTION CRITERIA
This review aims to collate and analyze recent advance-
ments in the rapidly evolving field of quantum-inspired
machine learning. We focus on contemporary studies; only
studies published in the recent years are considered (2017-
2023). A systematic search strategy was employed to re-
trieve literature from multiple databases, including Google
Scholar, and other academic search engines. The search
was performed using the specific key phrases, in order to
refine the search space, as these research areas have been
considered QiML in the literature:
•
“quantum-inspired”,
•
“dequantized algorithms”,
•
“tensor networks”,
•
“variational quantum algorithms”
We combine each of these terms with “machine learning”
to ensure the retrieval of publications specifically relevant
to QiML. We also exclude terms that may capture works
rooted in combinatorial optimization, using the key phrase
“-‘combinatorial optimization’” and other related search
terms such as “-‘heuristic algorithms’”, “-‘optimization al-
gorithms”, “-‘combinatorial algorithms”, with additional
manual vetting of papers that bypass this filter — in this
review, we will consider such methods disjoint from ma-
chine learning. Initial search returned 2,300 results. We
also use the key phrases “dequantized algorithms”, “tensor
networks” and “variational quantum algorithms” in order
to refine the search space, as these research areas have been
considered QiML in the literature.
Categorizing the selected studies was based on the
following criteria. First, papers are categorized based on

5
Fig. 2. Yearly plot displaying the frequency of works found via the
search term “‘quantum-inspired’ AND ‘machine learning’ -‘combinatorial
optimization’”
the types of techniques involved in accomplishing machine
learning tasks. It focused on the various quantum-inspired
algorithms, methodologies, and models used in these stud-
ies and their unique aspects that contribute to the advance-
ment of machine learning tasks. Secondly, the applicability
and practical implementation of these quantum-inspired
methods is of importance. As a fast-growing field, it is essen-
tial to discern not only the theoretical advancements but also
where these methods have been applied in empirical exper-
imentation. Identifying such applications provides insights
into the current state of quantum-inspired machine learning
and its potential in solving complex problems across various
domains; works that demonstrate these real-world applica-
tions and emphasize the practical benefits and challenges
of quantum-inspired techniques were given priority. Works
that introduced novel methods, algorithms, or theoretical
insights into QiML were also particularly valued.
4
CURRENT QIML TECHNIQUES
We classify works in QiML based on their underlying
methodologies and purposes. Three overarching categories
have been identified: “Dequantized Algorithms” (Section
4.1), “Tensor Networks” (Section 4.2), and “Quantum Vari-
ational Algorithm Simulation” (Section 4.3). Methods that
do not fail into these categories are group and labeled
as “Other QiML Methods” (Section 4.4). Within each cate-
gory, methods can be further grouped by their application
domains, with the exception of Dequantized Algorithms,
which are grouped via method due to lack of practical
application domains. Figure 3 presents these categorizations
in an organizational chart.
4.1
Dequantized Algorithms
In
recent
years,
algorithms
termed
as
“dequantized”
have been developed. These algorithms aim to determine
whether the speedups claimed by quantum machine learn-
ing algorithms are genuinely attributed to the inherent
power of quantum computation or are merely a byproduct
of strong assumptions regarding input and output encod-
ing/decoding (i.e. via state preparation). By scrutinizing
these assumptions and their implications, researchers can
more accurately assess the practicality of quantum algo-
rithms and identify potential drawbacks that may impede
their real-world applications. This line of inquiry has lead to
the identification of classical counterparts to quantum-based
machine learning methods, which can be implemented on
classical hardware and achieve performance levels compa-
rable to their quantum analogs. By designing algorithms
that can be efficiently executed on classical resources, the
costly and arguably impractical requirements for quantum
state preparation and hardware can both be circumvented
[43].
Ewin Tang presented the seminal work in this area in
2019 [44]. In an attempt to prove that no classical algorithm
could match the runtime of the quantum recommendation
system developed by Kerenidis and Prakash [45], the au-
thor was instead able to devise a classical algorithm that
matched the fast runtime. The classical algorithm ran in
time O(poly(k)log(mn)), only polynomially slower than the
quantum algorithm’s O(poly(k)polylog(mn)), thus there is
no quantum advantage observed. This was a significant
result, as this quantum algorithm was once thought to be
one of the most promising candidates for demonstrably
exponential improvements in quantum machine learning
[3]. The key observation is that the exponential speedup
achieved by the quantum algorithm relies on specific in-
put assumptions about the user-product preference matrix.
These were, in general, the prevailing assumptions for
many quantum machine learning algorithms at the time;
that either computing the corresponding quantum state |v⟩
from some input vector v is arbitrarily fast, or that the
necessary quantum states come into the system already
prepared. However, the author emphasizes that the cost of
state preparation is nontrivial; if state preparation cannot
be performed in poly-logarithmic time, then the claimed
exponential speedup of the associated quantum algorithm
can not be realized in practice, as it does not account for the
time constraints imposed by state preparation.
The quantum recommendation system by Kerenidis and
Prakash [45] describes an explicit data structure used to
quickly prepare quantum states, as seen in Figure 4. This
is implemented via a set of binary search trees, one for
each row (user) in the preference matrix. Quantum states
are encoded using the square amplitudes of the coeffi-
cients of the given row vector |v⟩in a hierarchical manner:
|v⟩= Pn
i=1 vi |i⟩for v ∈Cn. The amplitudes and signs
are stored as leaf nodes in the tree; the root node contains
∥v∥2. This structure can be seen as a classical analogue to
QRAM 1. Sampling from this requires randomly traversing
down through each subsequent child node with probability
proportional to its weight. This data structure allows for
O(log(mn)) query access, as well as time O(log2(mn)) for
online updates to preference matrix elements Aij. However,
the model assumes quantum access to this data structure
with prepared quantum states. Tang demonstrated that
this data structure employed to meet the state preparation
assumptions can also fulfill classical L2-norm sampling
assumptions, allowing for “sample and query” access (SQ
1. We disregard the exact notion of QRAM for quantum settings,
as the data structure is only interfaced with classically; no quantum
operations are involved.

6
Fig. 3. Categories of QiML Methods
access) to the data. As a result, a classical algorithm aiming
to “equal” the performance of the quantum algorithm can
take advantage of these assumptions. In this way, more
reasonable comparisons can be made between QML al-
gorithms with state preparation assumptions and classical
counterparts with sampling assumptions.
Specifically, SQ access to x — SQ(x) is possible if, in
time O(T), the given data structure supports the following
operations:
•
Sample: sample the i-th entry from x with probability
x2
i /∥x∥
•
Query: output entries xi (i ∈[n]) of x, and;
•
Norm: determine the L2-norm ∥x∥.
A data structure only supporting query operations over x
will be denoted as Q(x). These assumptions serve as the
classical analogue to quantum state preparation, and are
often easier to satisfy.
This work also introduces three dequantized linear alge-
bra protocols, which have been widely used by subsequent
studies to dequantize various machine learning techniques.
Protocol 1: Inner Product Estimation (from Proposition
4.2 in [44]): For x, y ∈Cn given SQ(x) and Q(y), the
inner product ⟨x|y⟩can be approximated with an additive
error of ∥x∥∥y∥ε, and a probability of at least 1 −δ, using
O(1/ε2log(1/δ)) queries and samples.
Protocol 2: “Thin-Matrix” Multiplication (from Propo-
sition 4.3 in [44]): For w ∈Ck and V ∈Cn×k, given SQ(V †)2
(sample and query access to columns of V ) and Q(w),
sampling from the linear combination of V ’s columns V w
can be done in O(k2C(V, w)) query and time complexity,
where
C(V, w) :=
Pk
i=1 |wiV (i)|2
|V w|2
(1)
The cancellation measure C(V, w) quantifies the extent of
cancellation in the matrix-vector product V w, with a value
of 1 indicating no cancellation in orthogonal columns and
undefined values in cases of maximum cancellation, such as
linearly dependent columns.
2. The † symbol denotes the complex conjugate transpose: A† :=
(A⊺)∗= (A∗)⊺

7
Fig. 4. QRAM-like Data Structure for a matrix A ∈C2×4 [46]. Sampling from this structure involves traversing down the tree based on L2-norm
probability. The terminating nodes contain the sign of each element A(i, j). An example is shown for A(2, 3) (blue highlighting), sampled with
probability ∥a2∥2/∥A∥2
F × |A(2, 3)|2/∥a2∥2.
Protocol 3: Low-Rank Approximation (from Theorem
4.4 in [44]): For matrix A ∈C(m × n), a threshold k,
and an error parameter ε, a low-rank approximation of
A can be described with probability of at least 1 −δ, in
O(poly(k, 1/ε)) query and time complexity. Specifically in
[44], the low-rank description is SQ(S, ˆU, ˆΣ) for matrices
S ∈Cℓ×n, ˆU ∈Cℓ×k, and ˆΣ ∈Ck×k (with ℓ= poly(k, 1/ε)).
S is a normalized sub-matrix of A, while ˆU, and ˆΣ result
from the singular value decomposition of S. This implicitly
describes the low-rank approximation of A, denoted by D:
D := A ˆV ˆV ⊺= A(S† ˆU ˆΣ−1)(S† ˆU ˆΣ−1)†.
(2)
4.1.1
Recommendation Systems
We now briefly describe the dequantized recommendation
systems approach in [44]. Like its quantum version, the al-
gorithm finds a low rank approximation of the hidden pref-
erence matrix A, from which the preferences for a user i are
sampled. Specifically, given sample and query access SQ(A)
to A in the data structure, the algorithm leverages L2-
norm sampling to obtain a succinct description of matrix A:
SQ(S, ˆU, ˆΣ) (Protocol 3). SQ access is shown to be available
to these components; ˆV := S⊺ˆU ˆΣ−1 is obtained implicitly
(the approximate largest k right singular vectors of A). The
low-rank approximation of A is thus D, the projection of A
onto the subspace of the right singular vectors ˆV , satisfying
an acceptable error bound. Given, SQ(S, ˆU, ˆΣ), SQ(D) and
thus SQ(Di) is available. Protocol 1 is used to approximate
Ai ˆV using k inner product estimations. Rejection sampling
then allows for finding Di = Ai ˆV ˆV ⊺in time independent
of dimensions (Protocol 2). The algorithm’s quality bounds
on recommendations match those of the quantum version.
Further, this regime produces an exponential improvement
over the next best classical recommendations systems algo-
rithm, with a running time of ˜O(∥A∥24
F /σ24ε12)3.
Finally, the author provides a resulting guideline for
performing any comparisons between QML and classical
algorithms:
When QML algorithms are compared to classical ML
algorithms in the context of finding speedups, any state
preparation assumptions in the QML model should be
matched with L2-norm sampling assumptions in the
classical ML model [44] .
Improvements over this work in the recommendation
systems task come after several advancements in the de-
quantized algorithms field, particularly with the advent
of the dequantized Quantum Singular Value Transform
(QSVT) (Section 4.1.6). Chia et al. [46] used their dequan-
tized QSVT framework to obtain a complexity bound of
˜O(∥A∥6
F ∥A∥10/σ16ε6), which introduces a dependence on
∥A∥, but otherwise substantially improves upon [44]. Chep-
urko et al. [47] presents an algorithm that achieves a lower
bound run-time complexity of Ω(k3/ε6) for generating a
low-rank k approximation of an input matrix M, guarantee-
ing that ||A−M||2
F ≤(1+ε)||A−Ak||2
F . Their methodology
employs ‘λ-importance’ sampling sketches that over sample
ridge leverage score sketching, which relies on certain as-
sumptions regarding the size of the Frobenius norm of A,
||A||F , and the residual ||A −Ak||F . The authors note that
this is directly comparable to [44], which requires a run-time
that is polynomially large in k, κ, and ε−1. While providing
relative error bounds for low-rank approximations, their
method has an additive error, with a bound more like
||A −Ak||F + ε||A||F .
3. The notation ˜O(·) is used to suppress poly-logarithmic factors in
variables.

8
Bakshi and Tang [48] similarly employed their dequan-
tized QSVT framework, and achieved a complexity of
˜O(∥A∥4
F /σ9ε2). The authors note that direct comparison
with [47] is difficult due to the additional assumptions on
the size of ∥Ak∥F and the residual ∥A −Ak∥F required
by ridge leverage score sketching. Introducing the bound
k ≤∥A∥2
F /σ2 converts these error bounds to be more
“QSVT-like”, which elucidates the run-time in [47] to be
˜O(∥A∥6
F /σ6ε6), which is improved upon in terms of ∥A∥F
and ε, yet loses a factor of σ3.
4.1.2
Clustering and Dimensionality Reduction
Following their work in [44], Tang further went on to
dequantize both quantum supervised clustering and prin-
ciple component analysis (PCA) [49]. In that work, the
dequantization model (SQ access input model with L2-
norm sampling assumptions) is formalised, which directly
comes from the prior work on the recommendation systems
problem:
A quantum protocol’s S : O(T)-time state preparation of
|ϕ1⟩, . . . , |ϕc⟩→|ψ⟩is “dequantized” if a classical algorithm
of the form CS : O(T)-time SQ(ϕ1, . . . , ϕc) →SQv(ψ) can be
described with similar guarantees to S up to polynomial slowdown
[49].
Under this framework, only a quadratic speedup (due
to amplitude amplification) is observed by the quantum
nearest-centroid supervised clustering method proposed in
[50], which aims to find the distance from a point u ∈Cn
to the centroid of a cluster of points given by V ∈Cmxn
(and let ¯V be V with unit normalized rows). Assuming SQ
access to both u and rows of V , the problem reduces to
approximating ∥Mw∥, where:
M :=


u/∥u∥
1
√n ¯V

and w :=

∥u∥−
1
√n ¯V

(3)
At this point, the quantum version will perform a swap test
to construct |wM⟩. This can be effectively dequantized by
reformulating ∥Mw∥into w†M †Mw, which can be given
as the inner product ⟨a|b⟩of two tensors a, b ∈Cm×n×n
constructed from M and w:
a :=
d
X
i=1
n+1
X
j=1
n+1
X
k=1
Mji∥Mk,∗∥|i⟩|j⟩|k⟩;
(4)
b :=
d
X
i=1
n+1
X
j=1
n+1
X
k=1
w†
jwkMki
∥Mk,∗∥|i⟩|j⟩|k⟩.
(5)
Then, given SQ(a) and using Protocol 1, the desired
approximation is achieved in O(T(∥w∥2/ε2)log(1/δ)) time.
The Quantum Principle Component Analysis (QPCA)
method [51] was dequantized via a similar methodology
to that in [44], since the low rank approximation effec-
tively produces a dimensionality reduction of the given
matrix. In QPCA, the algorithm outputs estimates for both
the top-k singular values ˆσ2
1, . . . , ˆσ2
k and singular vectors
|ˆv2
1⟩, . . . , |ˆv2
k⟩, using density matrix exponentiation to find
these principal components. The dequantized version uses
Protocol 3 to find SQ(S, ˆU, ˆΣ) and produces the approx-
imate large singular vectors ˆV := S† ˆU ˆΣ−1. The σi’s are
taken from Σ, and the vi’s are given implicitly: ˆvi
=
S† ˆU∗,i/ˆσi.
In the QSVT dequantization method of [46], the bounds
in [49] for supervised clustering were reproduced, and
improved for PCA from
˜O(∥X∥36
F /∥X∥12λ12
k η6ε12) to
˜O(∥X∥6
F /∥X∥2λ2
kη6ε6), a substantial improvement in al-
most all parameters.
4.1.3
Matrix Inversion and Solving Linear Systems
The task of matrix inversion is an essential subroutine in
many machine learning optimization operations. Solving
the linear system Ax = b for some A ∈Rm×n typically
requires solving the pseudoinverse of A: ¯x = A+b (finding
an ¯x minimizing ∥Ax −b∥when A is not invertible). In
classical settings, computing the pseudoinverse can be com-
putationally difficult, especially for large or ill-conditioned
matrices. Known methods of doing so, such as the Moore-
Penrose pseudoinverse, may take, at worst, close to O(n3)
time (where m ≈n, m < n) [52]; a severe bottleneck
in large data applications [53]. In the quantum setting,
matrix inversion is performed using the famed HHL algo-
rithm [54]. Dequantizing this in the sparse matrix inversion
setting is generally difficult due to its BQP completeness
[54]. However, many machine learning contexts operate in
low rank settings, wherein which dequantization becomes
possible, as demonstrated by the low-rank matrix inversion
algorithms independently proposed by Gily´en et al. [55]
and Chia et al. [56]. These algorithms exploit the low-
rank structure of the input matrix to perform the inversion
efficiently using classical techniques. In [55] an exponential
speed-up was shown to be possible for classical low-rank
matrix inversion. Given SQ(A) and SQ(b), both in constant
time, SQ(A+b) is approximated in time O(polylog(mn)),
equivalent to the quantum version presented in [54], thus
showing no quantum advantage. Specifically, the approxi-
mation to ¯x is given by:
˜x ≈
k
X
ℓ=1
1
˜σ4
ℓ
|R†w(ℓ)⟩⟨R†w(ℓ)| A†b
≈
k
X
ℓ=1
1
˜σ2
ℓ
|˜v(ℓ)
A ⟩⟨˜v(ℓ)
A | A†b ≈(A†A)+A†b = A+b
(6)
SQ(A) allows for L2-norm sampling of row indices r
from A ∈Cm×n; s ∈[r] indices are then sampled from
uniformly, from which the column indices c are L2-norm
sampled to find C (Protocol 3); R ∈Cr×n is implicitly
defined from the r rows. The SVD of C is then computed
to obtain its left singular vectors u1, . . . , uk and singular
values eσ1, . . . , eσk, which are shown to be good approxi-
mations for the right singular vectors of R, and implicitly
A. Specifically, this approximation is good with probabil-
ity at least 1 −η given r ≥
4ln(2n/η)
ε2
. The left singular
vectors ev(ℓ)
A
:=
Pr
s=1 R†
isu(ℓ)
s /eσℓ

of A are then approxi-
mated by the projection (⟨˜v(ℓ)
A | A†/eσℓ). Then an estimate of
⟨˜v(ℓ)
A | A† |b⟩is obtained via Protocol 1 to additive error, since
⟨˜v(ℓ)
A | A† |b⟩= Tr
h
|A†b⟩⟨˜v(ℓ)
A |
i
is an inner product. Protocol

9
2 produces samples from the linear combinations of these to
achieve the approximate solution ¯x.
Chia et al. [56] present a similar method. The objective is
to approximate A†A from the singular values and singular
vectors of a small sub-matrix of A, from which the solution
A−1b can then be sampled from in sub-linear time, given
SQ(A). The left singular values and vectors are produced
from A using a similar sub-sampling technique in [55] to ar-
rive at A ∈Cm×n →R ∈Cp×n →C ∈Cp×p →W ∈Ck×k
where W’s u1, . . . , uk left singular vectors and ˆσ1, . . . , ˆσk,
singular values are good approximations of A’s. V ∈Cn×k
is then formed from column vectors S†
ˆσi ˆui, with D ∈Ck×k
being a diagonal matrix containing the ˆσi’s. This allows for
the description of ˆA∼2 = V D−2V †, the approximation of
the square matrix A†A in the normal equation (A†A)+A†b.
The problem is now to find ¯x = V D−2V †A+b. An ex-
tension of Protocols 1 and 2 is proposed that shows that
approximating x†Ay up to additive error is possible given
SQ(A), SQ(x) and SQ(y). This is used to find w ∈Ck =
V †A+b. The authors then show that both querying an entry
in Vi,∗D−2w (via Protocol 1) and sampling the solution
V D−2w (via Protocol 2) can be done in sub-linear time.
The works by [55] and [56] achieve running times of
˜O(k6∥A∥6
F ∥A∥16/σ22ε6) (where k ≤∥A∥2
F
σ2
is rank(A)) and
˜O(∥A∥6
F ∥A∥22/σ28ε6) respectively. The dependence on k in
the former makes direct comparison between these com-
plexities difficult, though the algorithm in [55] operates in
the more restricted setting where A is strictly rank k. Chia et
al. go on to employ their dequantized QSVT [46] (see Section
4.1.6) to re-derive an algorithm for the task that runs in the
same time as [56], in a more general setting where σ can
be chosen as an arbitrary threshold, and not necessarily the
minimum singular value of A.
Several subsequent works have produced quantum-
inspired classical low-rank linear regression algorithms that
improve on these complexity bounds. Gily´en et al. [57], in
the same setting, provide two algorithms; one that outputs a
measurement of |xi| in the computational basis and another
that outputs an entry of x in ˜O
 ∥A∥6
F ∥A∥6/σ12ε4
and
˜O
 ∥A∥6
F ∥A∥2/σ8ε4
time, respectively, a large improve-
ment on all prior methods. The problem considered is
extended to the ridge regression setting: finding a solution x
given f(x) := 1
2
 ∥Ax −b∥2 + λ∥x∥2
. The key contribution
is the construction of a sparse description of the solution
vector b, after-which an SGD optimization process exploits
sample-query accesses SQ(A) and Q(b) to find x. This
avoids explicitly computing the matrix pseudo-inverse, re-
sulting in large improvements in running time. The authors
note that this method is viable only when b is sparse, but
show that via matrix sketching techniques, this requirement
can be bounded up to a certain degree. This work highlights
the advantages of using iterative approaches in conjunction
with quantum-inspired linear algebra techniques.
Chepurko et al. [47] proposed an algorithms for the
ridge regression problem using alternative techniques from
randomized numerical linear algebra. A key distinction is
the use of Projection-Cost Preserving (PCP) sketches, with
analysis via ridge leverage score sampling techniques, as op-
posed to the L2-norm sampling common in prior quantum-
inspired algorithms. The authors argue that sampling ac-
cording to the squared row norms of a matrix A is akin
to sampling from a distribution close to the leverage score
distribution. Consequently, by oversampling by a factor
of the square of the condition number (κ2(A)), the same
guarantees can be achieved as with leverage score sam-
pling. PCP sketches produces the low-rank approximation
of A, which is then processed through the SVD, and a
QR factorization to produce an approximation of A. This
small, sub-matrix decomposition solves a linear system of
equations using a conjugate gradient method to find the
solution, as opposed to inverting the matrix directly. A
running time of ˜O(∥A∥4
F log(c)/σ8ε4) is achieved, where σ
is the minimum singular value of A, c is the number of
columns in A, and assuming some sizable regularization
parameter λ = Θ(∥A∥2
2)4, a roughly ∥A∥2
F
∥A∥2 improvement on
[57].
Shao and Montanaro [58] propose two algorithms for
solving linear systems: One based on the randomized Kacz-
marz method, and one based on the randomized coordinate
descent method. Both are iterative methods. The authors
explore their algorithms under various settings of A, such
as when it is dense, sparse, or semi-positive definite (SPD),
and note that their latter algorithm only operates in the SPD
setting. The former algorithm focuses on the dual form of
the Kaczmarz method, specifically iterating on y, obtained
by introducing y such that x = A⊺y for consistent linear
systems. The goal is to find a sparse approximate solution
for y, which, when plugged back into x = A⊺y provides an
approximate solution for the original linear system, similar
to [57]. This sparsity, guaranteed by the iterative method,
simplifies querying an entry of x, and an entry of x can be
sampled using rejection sampling. The resulting algorithm
has a complexity bound of ˜O(∥A∥6
F ∥A∥2/σ8ε2), which im-
proves in ε over [47], but suffers in ∥A∥6
F . In [57], the authors
recognize the roughly
∥A∥2
F
σ4ε2
improvement on their work,
when λ = 0 and Ax∗= b exactly (without any regression
parameter). The authors note the similarity between this
method and the ones introduced in [58]; the randomized
Kaczmarz update is a variant of SGD, and by replacing
their stochastic gradient with the Kaczmarz update (and
with accommodating minor adjustments to their sketch), a
similar running time to [58] is achieved for the no regression
setting.
Finally, Bakshi and Tang [48] achieve a running time
of ˜O(∥A∥4
F /σ11ε2) with their method and note that this is
comparable with, or improves upon several, prior works
over the matrix inversion and linear systems tasks. When
contrasted with [47], their work exhibits better ε dependence
and equivalent ∥A∥F dependence. Notably a superior σ
dependence in low-error settings is achieved, although in
situations where a higher error is permissible, the approach
in [47] may be more efficient. Relative to [58], the authors
match the dependence in ε, excels in σ-dependence, but
falls short in Frobenius norm dependence. Additionally, the
authors note their approach is more general than in [58],
which requires b to be in the column space of A.
4. The symbol Θ denotes that the behavior of λ is tightly bound to
the squared 2-norm of matrix A, neither growing much faster nor much
slower than that term.

10
4.1.4
Support Vector Machine
The seminal quantum SVM algorithm presented in [59]
finds a solution to a set of linear equations — the Least
Squares SVM (LS-SVM); a variant of the SVM optimization
problem. LS-SVM attempts to label points in Rm as +1 or
-1. Given input data points xj for j = 1, ..., m ∈Rn and
their corresponding labels y in ±1m, the goal is to find a
hyperplane, specified by w ∈Rn and b ∈R, that separates
these points. Since it may not be possible to perfectly sepa-
rate all points, a slack vector e ∈Rm is introduced, where
e(j) ≥0 for all j ∈[m]. The objective is to minimize the
squared norm of the residuals, which is a combination of
the squared norm of w and e, given by:
min
w,b
∥w∥2
2
+ γ
2 ∥e∥2
(7)
s.t.
yi(w⊺∗xi + b) = 1 −ei, ∀i ∈[m].
(8)
In the dual problem, LS-SVM tries to find a classification
for the data points based on their projection on the optimal
hyperplane. This is achieved by solving for the α values in
the equation:
(X⊺X + γ−1I)α = y,
(9)
where α = ukyk; this term quantifies the weight of each
data point in defining the classification hyperplane. The
equation for the hyperplane thus becomes x⊺Xα = 0. For
new data points x, the LS-SVM performs the classification
by evaluating sgn(x⊺Xα).
In the quantum approach, labelled data vectors xj for
j = 1, ..., m are transformed into quantum vectors |xj⟩=
1
∥xj∥
P
k(xj)k |k⟩via QRAM. The kernel matrix is then as-
sembled by leveraging quantum inner product estimations.
The solution is obtained by solving a system of linear equa-
tions related to the quadratic programming problem of the
SVM, facilitated by the HHL algorithm. A dequantized LS-
SVM algorithm was presented in [60]. Here, the data vectors
are stored in classical QRAM [45], allowing for SQ(x).
Although the L2 norm sub-sampling methods from [55], [56]
could be used to inverse (X⊺X + γ−1I), they would not be
able to produce the inverse in the reported logarithmic time,
since directly computing X⊺X is done in polynomial time.
Instead, the authors devise an indirect sampling technique
to perform the inversion, given only SQ(X). This process
involves two primary stages: column and row sub-sampling
from the matrix X, forming matrices X′ and X′′ and their
squares A′ = X′⊺X′ and A′′ = X′′⊺X′′ respectively. Here,
it is the spectral decomposition of A′′ = V ′′Σ2V ′′⊺that
yields the approximate eigenvalues and eigenvectors of X.
The elements of the vector ˜λl = ˜V ⊺
l y are estimated, using
the approximated eigenvectors and the data labels. Then,
a vector u is derived as a weighted sum of the columns
from the spectral decomposition. The algorithm constructs
˜α, an approximation of the solution to the LS-SVM problem
obtained by finding query access of ˜α = ˜R⊺u, where ˜R
represents a sampled part of matrix R = X′⊺X. The final
classification of any new data point x is determined by
calculating its projection onto the classification hyperplane,
given by x⊺X ˜α, with the sign of this projection determining
the class of x.
4.1.5
Semidefinite Programming
Semidefinite programming (SDP) is a subfield of convex
optimization concerned with the optimization of a linear ob-
jective function over the intersection of the cone of positive
semidefinite matrices with an affine space. While traditional,
classical SDP solvers assume entry-wise access to matrices,
quantum SDP solvers make use of oracle access to specially
constructed data structures, allowing for sub-linear access
of matrix elements in superposition. Such quantum SDP
algorithms have been explored in the sparse setting to
achieve exponential speeds. With respect to this paradigm,
[61] propose a dequantized SDP solver on the back of Tang’s
methodology [44], since such access can be analogously
described classically via the sample and query framework.
In [61] the normalized SDP feasibility problem variant of
SDP is explored, which involves finding, for a given ε > 0,
a positive semidefinite matrix X with trace 1 that satisfies
Tr[AiX] ≤ai + ε for a set of given Hermitian matrices Ai
and real numbers ai, or determining that no such matrix ex-
ists. By leveraging binary search, the ε-approximation of the
SDP can be simplified into a sequence of feasibility tests, as
per the above definition. The Matrix Multiplicative Weight
(MMW) method is used as the solver, which is a game-
theoretic, iterative process that seeks an approximate fea-
sible solution. In each iteration, the first player attempts to
find a feasible solution X ∈Sε, which is updated based on
any violation j ∈[m] by any proposed X of the constraints
found by the second player, i.e., Tr[AjX] > aj + ε. Each
round t has the update Xt+1 ←exp[−(Aji + . . . + Ajt)] for
the next round. The solution is arrived at in the equilibrium
of this process.
The approach here is to sample from an approximate
description of the matrix exponentiations exp[−(Aji + . . . +
Ajt)], using the approximate spectral decomposition A :=
Pt
r=1 Ajτ as A ≈ˆV ˆD ˆV † to achieve e−A ≈ˆV e−D ˆV †, where
ˆV ∈Cn×r and ˆD ∈Rn×n are diagonal. This work devises
two methods to address a few encountered challenges. First,
the dynamic nature of the matrix A throughout the MMW
method makes the assumption SQ(A) infeasible. This prob-
lem is circumvented by devising a weighted sampling
procedure that provides a succinct description of a low-
rank approximation of A by sampling each individual Ajτ:
A := Pt
r=1 Ajτ. Secondly, standard sampling procedures
yield an approximation V D2V † ≈A†A instead of a spectral
decomposition ˆV ˆD ˆV † ≈A, even if A is Hermitian. This
discrepancy is problematic for matrix exponentiation, as the
singular values disregard the signs of the eigenvalues, lead-
ing to significant errors when approximating e−A by na¨ıvely
exponentiating the SVD. The resolution is a novel approxi-
mation procedure called symmetric approximation, which
can calculate and diagonalize the small matrix ( ˆV †A ˆV ),
yielding approximate eigenvalues of A and a desired de-
scription of its spectral decomposition in logarithmic time
in dimension. Evaluating the solution via the constraints
can be done with the dequantized protocols, e.g. Tr[AjX]
via Protocol 1.
4.1.6
Quantum Singular Value Transform
Recent research by Gily´en et al. [62] has proposed that
a broad array of quantum algorithms may be manifesta-

11
tions of a singular underlying principle, encapsulated by
the Quantum Singular Value Transformation (QSVT). The
QSVT can be understood as a process that, given a matrix
A ∈Cm×n and a function f : [0, ∞) →R, implements a
unitary U that represents the polynomial transformation of
A to the matrix f(A), defined via its singular value decom-
position. The QSVT is uniquely characterized by its para-
metric nature, defined by phase angles ϕ(k) which can be
efficiently and stably computed in a classical manner given
a desired polynomial transformation [63]. The flexibility of
this parameterization imbues the QSVT with remarkable
power and adaptability; many quantum algorithms can be
phrased under the QSVT framework, so long as there can
be found a specific parameterization for the task. As such,
the QSVT has been touted as a “unifying framework” for
quantum algorithms [63]; its power lies in its capacity to
encapsulate a broad range of quantum algorithms within a
coherent theoretical structure.
The QSVT extends upon Quantum Signal Processing and
Qubitization [64], applicable to an entire vector space. It can
apply polynomial transformations to all the eigenvalues of
a given Hamiltonian H that has been block-encoded into
a larger unitary matrix U – the Quantum Eigenvalue Trans-
form (QET). This can be generalized for non-square matrices
to its singular values – the QSVT. Given the singular value
decomposition of a matrix A:
A = UΣΣV †
Σ =
r
X
k=1
σk |uk⟩⟨vk| ,
(10)
where UΣ, VΣ are unitaries and Σ is a diagonal matrix
with non-negative, real singular values σ1, . . . , σk along the
diagonal, up to the k-th (r = rank(A)) singular value. UΣ
and VΣ span orthonormal bases, denoted by {|uk|} (the
left singular vector space) and {|vk|} (the right singular
vector space) respectively. These spaces can be defined by
the projectors ˜Π := P
k |uk⟩⟨uk| and Π := P
k |vk⟩⟨vk|. The
block-encoding of A within a unitary U can thus be given
by:
U =


Π
˜Π
A
·
·
·

.
(11)
˜Π and Π locate A within U, such that they form the projected
unitary encoding A := ˜ΠUΠ. The QSVT is then realised
with the projector-controlled phase-shift operations of ˜Π
and Π to approximate the degree-d polynomial transforma-
tions:
U−
→
ϕ =


Π
˜Π f (SV)(A)
·
·
·

,
(12)
where f (SV)(A) is defined for an odd polynomial as:
f (SV)(A) :=
r
X
k=1
f (SV)(σk) |uk⟩⟨vk| ,
(13)
which applies the polynomial transformation to the singular
values of A. A similar result is obtained for even polynomial
transformations, where only the right singular vector projec-
tions are involved. Of importance is the compositional prop-
erties of block-encoded matrices. Many quantum algorithms
and QML applications can be reformulated via this frame-
work, given a satisfactory approximation can be achieved
by carefully considered polynomial transformations.
A natural question now is whether the QSVT can be
successfully dequantized. Several independent efforts have
been made on this front. Chia et al. [46] note that algo-
rithms framed under QSVT fall roughly into two categories
based on their assumptions on input data A: those with
sparsity assumptions and those with low stable rank and
under QRAM assumptions (which inform how the block-
encodings in [62] can be obtained). These settings allow
for efficient block-encodings, outside of which they are
generally not efficiently computable. It may not be possible
to dequantize algorithms under sparsity assumptions, since
this would imply dequantizing the HHL algorithm, which
is recoverable from the QSVT framework; HHL is known
to be BQP-complete for sparse matrices, even for constant
precision [54]. For block-encoding arithmetic operations
over algorithms using QRAM-based data structures, this
category shares efficient operations with classical algorithms
that have oversampling and query access available. From this
insight, the authors go on to implement a dequantized
QSVT framework applicable to the prior tasks examined un-
der the dequantized lens, showing improvements in query
and time complexity over several established methods.
These results suggest that dequantization techniques can be
generalized to most, if not all QRAM-based QSVT models,
and give strong evidence that these models admit no ex-
ponential speedup in the bounded Frobenius norm regime.
In particular, the framework extends upon the notion of
SQ access for vectors SQ(v) and matrices SQ(A), and
formalizes ϕ-oversampling and query access to vectors and
matrices SQϕ(v) and SQϕ(A) respectively. ϕ-oversampling
and query access to a vector v is available if we have Q(v)
and sample and query access SQ(˜v) to a vector ˜v, which
serves as an “element-wise upper bound” of v and complies
with the following properties: ∥˜v∥2 = ϕ∥v∥2 and |˜vi| ≥|vi|
for every index i. The definition can be extended similarly
for a matrix. The factor ϕ can be interpreted as a kind of
computational surplus incurred in the execution of algo-
rithms: Via rejection sampling, SQ(˜v) is capable of perform-
ing approximations of all the queries of SQ(v), albeit with
an additional cost denoted by the factor ϕ. This cost roughly
corresponds to overheads in post-selection in the quantum
setting. The oversampling and query input model has clo-
sure properties very similar to that of block-encodings, that
allow for the chained composition of complex arithmetic
operations, and can be achieved whenever quantum states
and generic block-encodings can be prepared efficiently.
Specifically, these closure properties allow for access-
ing the approximation of f(A†A) for a smooth, bounded

12
Lipschitz-continuous function f in time independent of
dimension, given a generic matrix A. f(A†A) can be shown
to, via various matrix sketching techniques and the affore-
mentioned closure properties, approximate an RUR decom-
position R† ¯f(CC†)R, where ¯f(x := f(x)/x), and R and C
contain the normalized columns of A and R respectively.
This expresses a desired matrix as a linear combination of
r outer products of rows of the input matrix, for which
oversampling and query access is available, since having
SQ(A) implies SQϕ(R†UR), where U := ¯f(CC†). We can
then achieve an approximation for f (SV)(A) up to some error,
from which we can sample from some solution vector v :=
|f (SV)(A)b⟩such that ∥f(A)b −v∥≤ε in ˜O(d22∥A∥6
F /ε6)
time; the dequantized generic singular value transform for
degree-d polynomials. Significantly, the authors argue for
the generality of their framework, which allows for approx-
imately low rank matrices, rather than strictly low rank
matrices, as input, which has been stipulated in prior works.
This framework is then applied to re-derive several existing
dequantized algorithms, showing improvements in com-
plexity bounds in most cases, and some relaxation of input
constraints in many, as seen in Section 4.1.9. Further, the
framework was applied to two tasks that had not yet been
explored under the dequantization framework: Hamiltonian
simulation and Discriminant Analysis.
In a parallel work, Jethwani et al. [65] also proposed their
version of the dequantized QSVT. The authors similarly
define the function f as smooth, Lipschitz-continuous, such
that: f (SV)(A†) = h(A†A)A† for h(x) = f(√x)/√x. The
authors provide a sketch of A down to a smaller sub-
matrix W from successive samples of rows and columns of
A. Then a similar result to [46] is obtained by computing
the SVD of W to find U := W +f(W †)W +(W †)+. The
solution is then sampled from R†URA†b ≈f(A†)b. The
complexity achieved is ˜O(∥A∥6
F κ20(d2 + κ)/ε6), where κ is
the condition number of A. The authors require that A be
a well-conditioned matrix, such that all σk of A lie within
[1/κ, 1]. The conditional number dependence makes direct
comparison with the results from [46] difficult, however,
[46] surmises that for the SVT in [65], the typical cases of
f require A to be strictly low-rank.
Gharibian and Le Gall [66] explored dequantizing QSVT
for sparse matrices, noted to be a difficult task. This is the
primary setting relevant to quantum chemistry and quan-
tum PCP applications. They derive a dequantized QSVT
algorithm for s-sparse Hamiltonians, where s denotes the
number of non-zero entries per row or column. This al-
gorithm is then considered for the task of Guilded Local
Hamiltonian estimation (GLH): estimating the ground state
energy of a local Hamiltonian when given a state suffi-
ciently close to the ground state. Two key results follow:
(1) showing that QSVT in sparse settings provided with SQ
access can be efficiently dequantized and computed classi-
cally given constant-degree polynomials and with constant
precision, and; (2) this problem becomes BQP-complete for
input representations in a “semi-classical state”, which are
unit-norm vectors represented as a uniform superposition
of basis states within a polynomial size subset, with poly-
nomial precision. This is distinguished from other efforts
in dequantizing the QSVT, which only consider QSVT cir-
cuits in the QRAM setting. This work highlights the idea
that dequantizing quantum algorithms is not a uniquely
ML-driven focus; other fields may benefit from efforts in
dequantizing algorithms for their respective applications.
Finally, the most recent entry into this line of work is
presented by Bakshi and Tang [48], who focus on improving
the complexity bounds of the dequantized QSVT algorithms
presented earlier. The authors suggest that a significant cost
of performing these algorithms is due to the computation
of the singular value decomposition of matrix A, which
directly incurs the ∥A∥6
F /ε6 cost in [46]. A key insight
is to additionally use iterative algorithms on top of the
sketches of A and b to approximate matrix polynomials.
Bakshi and Tang’s work [48] combines sketches of matrices
A and b with Clenshaw recurrence, an iterative algorithm, to
approximate matrix polynomials more efficiently than pre-
vious approaches. They introduce the Bilinear Entry Sam-
pling Transform (BEST) for matrix sparsification, which op-
timizes ε-dependence without requiring full spectral norm
bounds. This technique streamlines the error analysis and
achieves a notable reduction in complexity bounds for
dequantized QSVT. Consequently, their method presents
substantial improvements in terms of time and resource
consumption; computing v only requires ˜O(d11∥A∥4
F /ε2)
time, and without the condition number dependence in [65].
This complexity reduction brings the time required within
the scope suggested as indicative of quantum advantage
[67], thus providing robust evidence countering the notion
of quantum supremacy. The framework was then used to
find improvements in complexity bounds for the tasks of
recommendations systems, linear regression, and Hamilto-
nian simulation.
4.1.7
Discriminant Analysis
This subsection leads on from the work of [46]. The dequan-
tized QSVT framework is used to dequantize the quantum
Fisher’s Linear Discriminant Analysis (LDA) algorithm, pre-
sented by Cong and Duan [68]. The LDA problem aims
to project classified data onto a subspace that maximizes
between-class variance while minimizing within-class vari-
ance. Given M input data points {xi ∈RN : 1 ≤i ≤M}
belonging to k classes, we define between-class scatter
matrix SB and within-class scatter matrix SW . The orig-
inal goal is to solve the generalized eigenvalue problem
SBvi = λiSW vi, but this may not be feasible in cases
where SW is not full-rank. Consequently, Cong and Duan
consider a relaxation where small eigenvalues of SW and
SB are ignored, leading to an approximation using inexact
eigenvalues, which can be applied to the quantum context.
Dequantizing this involves finding an approximate
isometry
U
and
a
diagonal
matrix
D,
such
that
S
1
2 BS−1
W S
1
2 BU ≈UD, which finds the approximate eigen-
values and eigenvectors of S+
W SB. Given SQ(B, W) ∈
Cm×n, with SW
≈W †W and SB
≈B†B, the Even
Singular Value Transform in [46] approximates
√
W †W ≈
R†
W UW RW and B−†B ≈R†
BUBRB through RUR decom-
positions. Matrix sketching techniques then yield an approx-
imate RUR decomposition of the target matrix, R†
W URW ,
where U = UW RW R†
BUBRBR†
W UW , from which the ap-
proximate eigenvalues and eigenvectors can be extracted.

13
4.1.8
Hamiltonian Simulation
The Hamiltonian Simulation problem, rooted in the original
motivation for quantum computers proposed by Feynman
[69], aims to simulate the dynamics of quantum systems.
Given a Hamiltonian H, a quantum state |ψ⟩, a desired
error ε > 0, and time t > 0, the objective is to prepare
a quantum state |ψt⟩such that |||ψt⟩−eiHt|ψ|| ≤ε. With
wide applications in quantum physics and chemistry, the
rich literature on quantum algorithms for Hamiltonian sim-
ulation includes optimal algorithms for simulating sparse
Hamiltonians. In [46] the dequantized QSVT framework is
applied for Hamiltonian simulation that operate in different
regimes, both for low-rank H and for arbitrary H. Authors
consider a Hermitian matrix H ∈Cn×n, a unit vector
b ∈Cn, and error parameters ε, δ > 0. Given SQ(H) and
SQ(b), the task is to output SQϕ(ˆb) with probability ≥1−δ
for some ˆb ∈Cn satisfying ||ˆb −eiHb|| ≤ε. Formulating the
problem as a decomposition of a generic function f(x) into
sine and cosine functions results in the expression eiHb =
fcos(H†H)b + fsinc(H†H)Hb, where sinc(x) = sin(x)/x, to
which an RUR decomposition can be achieved. The authors
obtain a running time of ˜O
 ∥H∥6
F ∥H∥16/ε6
in the low-
rank regime. The authors note that their algorithm for the
Hamiltonian Simulation task is comparatively slower when
compared with similar classical techniques in randomized
linear algebra that consider sparsity in H and b [70], and
posit their framework exposes this trade-off between spar-
sity and speed.
In [48], the authors improve upon the complexity in
[46] using their classical algorithm employing Clenshaw
recurrence in the same setting. The authors here obtain a
description that is O(ε)-close to eiHt. Sampling from this
can be done in ˜O(∥H∥4
F ∥H∥9/ε2) time, an improvement
over [46] in all parameters.
4.1.9
Complexity Comparisons
A critical motivator in the dequantized algorithms space is
in analysing the nature of “quantum advantage” in QML
settings, when presented with their classical counterparts.
We adopt Chia et al.’s [46] Figure 1 (Table 1 in this review),
which presents the time complexities of the dequantized
algorithms discussed, and the quantum algorithms they are
based on. We extend this table with the complexities of
subsequent works in the field. All complexities are given
as poly-logarithmic in the input size. The dequantized
algorithms are presented loosely in order of decreasing
time complexity; the current gap between the QML and
classical algorithms can be observed. Significant progress
has been made with successive advancements, bringing us
closer towards the QML benchmark, particularly seen in
the matrix inversion and QSVT tasks. Targeted research
efforts in these areas are justified due to the central role
of matrix inversion as a fundamental subroutine in various
linear algebra settings, and the generalization of numerous
tasks under the QSVT framework. Further, the relaxation
of parameters and data requirements can be observed in
some successive works, showing an increase in generality
of algorithms over time.
4.1.10
Critical Views on the SQ Access Model
Much of the work in dequantizing QML algorithms relies
on the QRAM-like classical data structure introduced by
Kerenidis and Prakash [45]. While this model has been
successfully applied in the literature, there may exist al-
ternative methods of representing the input data take can
inform classical algorithms. In the quantum domain, input
models that store data as entries in density matrices and
simulating them as Hamiltonians is common, especially
in QML applications [51], [59]. Sparse representations on
quantum circuits are another a form that has already been
briefly discussed.
Zhao et al. [71] address inefficiencies in the standard
input access model for quantum machine learning due
to the need for pre-computation and data storage. They
introduce a flexible model enabling entry-wise data ac-
cess, particularly useful when applying varying functions
or requiring matrix row entries. The authors show that
quantum amplitude encoding and classical L2-sampling can
be conducted cost-effectively, even with moderately noisy
input data, suggesting that quantum state preparation can
be efficient despite initial state preparation challenges.
Cotler et al. [72] discuss the appropriateness of SQ access
as a classical analog of quantum state inputs. SQ access, in
its current form, allows classical algorithms to manipulate
data that is exponentially difficult to extract from quan-
tum states, thus artificially ascribing excessive power to
dequantized algorithms when compared to their quantum
counterparts. The authors suggest that the definition of SQ
access needs to be revised. One plausible approach is to
limit classical algorithms to accessing data obtained through
measurements of quantum states inputs. This modification
would preserve significant computational power for exist-
ing dequantized algorithms, while describing an oracle that
is at most as powerful as inputs given to the quantum algo-
rithms. Subsequent analysis notes that Quantum PCA under
measurement data access retains its exponential quantum
advantage [73].
Alternative input models may thus need to be con-
sidered, and could provide a more accurate comparison
between classical and quantum algorithms and better reflect
the true potential of quantum computation.
4.2
Tensor Networks
A large focus in QiML research in the current day has been
on the use of tensor networks (TNs) as a machine learning
models. QiML-based tensor network research benefits from
a rich body of prior knowledge5 in the classical context, both
in and out of the machine learning field; theory and practical
applications show a useful degree of transferability into the
quantum domain [81], [82]. A key motivator in the use of
TNs is their ability to classically simulate the many-body
quantum wave function efficiently, which has been shown to
be consistent with higher-order tensor representations [77].
Hamiltonians of many physically realistic systems tend to
exhibit strong locality; the interactions between constituent
5. It is worth noting that TNs have been widely discussed and
analyzed in the literature outside of the machine learning context,
with a multitude of informative, introductory materials available on
the topic [75], [76], [77], [78], [79], [80].

14
TABLE 1
Comparison of time complexities for dequantized algorithms across specific tasks. In each row, the references given in the leftmost column
correspond to each successive item from left to right.
Quantum Algorithm
Dequantized Algorithms
Rec.
Systems
[45], [44], [46],
[47], [48]
∥A∥F
σ
∥A∥24
F
σ24ε12 ,
∥A∥6
F ∥A∥10
σ16ε6
,
∥A∥6
F
σ6ε6 ,
∥A∥4
F
σ9ε2
Supervised
Clustering
[50], [49], [46]
∥M∥2
F ∥w∥2
ε
∥M∥4
F ∥w∥4
ε2
,
∥M∥4
F ∥w∥4
ε2
PCA
[51], [49], [46]
∥X∥F ∥X∥
λkε
∥X∥36
F
∥X∥12λ12
k η6ε12 ,
∥X∥6
F
∥X∥2λ2
kη6ε6
Matrix
Inversion
[62], [55], [46],
[57], [47], [58]
[48]
∥A∥F
σ
k6∥A∥6
F ∥A∥16
σ22ε6
,
∥A∥6
F ∥A∥22
σ28ε6
,
∥A∥6
F ∥A∥6
σ12ε4
, ∥A∥4
F log(c)
σ8ε4
, ∥A∥6
F ∥A∥2
σ8ε2
, ∥A∥4
F
σ11ε2
SVM
[59], [60], [46]
1
λ3ε3
poly
 1
λ, 1
ε

,
1
λ28ε6
SDP
[74], [61], [46]
∥A(·)∥7
F
ε7.5
+
√m∥A(·)∥2
F
ε4
mk57
ε92 ,
∥A(·)∥22
F
ε46
+
√m∥A(·)∥14
F
ε28
QSVT
[62], [46], [65],
[48]
d∥A∥F ∥b∥
p(QV )(A)b
d22∥A∥6
F
ε6
,
∥A∥6
F κ20(d2 + κ)
ε6
,
d11∥A∥4
F
ε2
HS
[62], [46], [48]
∥H∥F
∥H∥6
F ∥H∥16
ε6
,
∥H∥4
F ∥H∥9
ε2
DA
[68], [46]
∥B∥7
F
ε3σ7 + ∥W∥7
F
ε3σ7
∥B∥6
F ∥B∥4
ε6σ10
+ ∥W∥7
F ∥W∥10
ε6σ16
particles are limited to next or nearest neighbors [83]. In the
case of gapped, local Hamiltonians, the exponentially large
and intractable Hilbert space is constrained to low energy
states bounded by the entanglement area-law, i.e., these
states cannot be highly entangled [83]. This constricts the
exploration space to only a relevant fraction of the Hilbert
space. When modelled by tensor networks, approximating
this subset can be done in polynomial time [75].
Further, tensor networks form a bridge between classical
neural network methods and quantum computing. Several
works have identified the natural corollary of tensor net-
works to quantum circuits, where many tensor networks
have a direct quantum circuit translation [77], [81]. Com-
plex tensors under tensor network decomposition are rep-
resented as unitary gates; the bond dimension connecting
two nodes of the tensor network is determined by number
of qubits connecting two sequential unitaries in the circuit.
It is shown that qubit-efficient tree tensor network (TTN)
and matrix product state (MPS) models can be devised,
with logarithmic scaling in the number of physical (O(1),
independent of the input data size) and ancilla (O(log2 D) in
bond dimension D) qubits required [84]. This allows tensor
networks to express higher order feature spaces in classical
settings, and act as classical simulators for quantum circuits.
Several works have since successfully implemented tensor
networks as parameterized quantum circuits on small, near-
term quantum devices for machine learning tasks, with
many of the advancements in the classical setting being
readily transferred to the quantum domain [84], [85], [86],
[87].
The many-body quantum system of N particles can be
described as follows:
|Ψ⟩=
X
s1s2···sN
Ψs1s2···sN |s1s2 · · · sN⟩
(14)
where, in quantum computing, the quantum state of N
qubits |Ψ⟩with amplitude Ψs1s2s3···sN is a composition
of the sN single-qubit basis states. This can be effectively
considered as one, large tensor. TNs aim to find alternative
representations of this computationally-inefficient descrip-
tion by reducing the complexity of the system. This is
achieved by decomposing the large tensor into a network
of many smaller tensors of smaller rank. The total num-
ber of parameters in the final representation scales sub-
exponentially with the number of composite tensors and
the bond dimension (the dimension of the largest contracted

15
index within the tensor network) between them, which al-
lows for the classical computation of expectation values [75].
For example, a tensor with N indices, each of dimension d,
must generally be specified by dN parameters. In contrast,
the MPS representation of such a tensor with bond dimen-
sion m only requires Ndm2 parameters, which now scales
linearly with N. Various such methods of tensor network
decomposition exist, which depend on the properties of the
original tensor and the desired resulting network. In further
sections, we discuss relevant tensor network methods and
their application as QiML techniques.
Fig. 5. Common Tensor Network Decompositions. (a): Matrix Product
State (MPS), (b): Project Entangled-Pair States (PEPS), (c): Tree Tensor
Network (TTN), (d): Multi-scale Entanglement Renormalization Ansatz
(MERA) [88]
4.2.1
Matrix Product States
Matrix Product States (MPS), or Tensor Train Networks, are
widely used for the efficient representation of 1D gapped
quantum systems with low energy states [89]. In fact, any
such quantum state can be exactly represented by the MPS
structure efficiently [90]. The MPS is achieved through
the decomposition of the quantum state into a product of
smaller matrices which represent the constituent tensors.
A general process involves successively partitioning away
individual tensors from the rest of the system via the sin-
gular value decomposition (SVD) [78]; several other related
methods have been also been proposed for performing this
decomposition in a practical manner [91]. By considering
only relevant states with bounded entanglement, a com-
pressed form is obtainable by truncating the least relevant
singular values. This low rank approximation is what drives
the efficient representation of the quantum state by the MPS,
which would otherwise be described exponentially with an
n-qubit state.
Supervised Tensor Network Modeling: The first notable
instance of tensor networks being used as machine learning
models utilize the MPS decomposition. Stoudenmire and
Schwab [92] demonstrated their capabilties in parameter-
ized supervised learning. They note that handling high-
dimensional vectors is also a critical challenge in non-linear
kernel learning. Traditional approaches often rely on the
“kernel trick” [93] to solve the dual representation; instead
the authors advocated for applying tensor network de-
compositions. In their proposed model, the kernel learning
problem is expressed as:
f l(x) = W l · Φ(x).
(15)
Here, vectors x are mapped to a high-dimensional feature
space through a non-linear mapping Φ(x), with W l as the
weight matrix; l is a tensor index for the function f l(x) that
maps every x to the space of classification labels. Both W
and Φ(x) could be considerably large, which presents large
computational bottleneck scaling exponentially with their
size; i.e. the resulting Hilbert space of Φ(x) is 2N, and neces-
sitates a compatibly sized W. An MPS tensor decomposition
is thus leveraged to reduce this computational complexity.
By representing W via an MPS and optimizing it directly,
the model scales linearly with the training set size. An MPS
decomposition can derived from W l:
W l
s1s2...sN =
X
{a}
Aα1
s1 Aα1α2
s2
· · · Al:αjαj+1
sj
· · · AαN−1
sN
,
(16)
where index αj is the bond dimension between sites. Φ(x)
is obtained via an embedding scheme that converts classical
input data into a linear combination of quantum states in
an orthogonal basis. In [92], the quantum mapping function
ϕ(xj) is used to embed every j-th gray-scale pixel value
ϕ(xj) =
h
cos
π
2 xj

, sin
π
2 xj
i
(17)
into the L2-normalized trigonometric basis. A full image is
thus the tensor product of ϕ(xj) individual local embed-
dings over all N pixels:
Φ(x) = ϕ (x1) ⊗ϕ (x2) ⊗· · · ⊗ϕ (xN) .
(18)
Words and sentences within natural language documents
can be associated with quantum systems by building their
vector and tensor space representations [94], [95], [96]. Each
word wi is mapped to an m-dimensional vector space, with
each dimension representing a different semantic meaning.
A word can thus be represented as a linear combination of
these orthogonal semantic bases:
wi =
m
X
i=1
αi |ei⟩,
(19)
where αi is the coefficient of the i-th base vector ei. A
sentence s = (wi, . . . , wn) or length n can then be modeled
as a tensor product of the word vectors:
s = w1 ⊗· · · ⊗wn
=
m
X
i,...,n=1
Ai,...,n(ei ⊗· · · ⊗en),
(20)
with A being the mn-dimensional coefficient tensor of basis
states.
A few other local embedding methods have been dis-
cussed in the literature, including the polynomial basis [97]:
ϕ(xj) = [1, xj] ,
(21)

16
which enables a transformed feature space capturing inter-
actions within categorical data, and simplifying the interpre-
tation of the resulting model as a high-degree polynomial.
Other kernels that are the product of some N local kernels
could also potentially be used [92].
A quadratic cost function:
L = 1
2
NT
X
n=1
X
l
(yℓ
n −f ℓ(xn))2,
(22)
is optimized via a “sweeping” algorithm, inspired by the
density matrix renormalization group (DMRG) algorithm
successfully used in physics applications [98]. This process
essentially involves ”sweeping” across the MPS (Matrix
Product State), where only two adjacent MPS tensors are
varied at a time. The tensors at sites j and j+1 are combined
into a single bond tensor Bℓ, followed by the calculation of
the derivative of the cost function with respect to the bond
tensor for a gradient descent step. The gradient update to
the tensor Bℓcan be computed as:
∆Bℓ= −∂L
∂Bℓ=
NT
X
n=1

yℓ
n −f ℓ(xn)

˜Φn,
(23)
where ˜Φn is the projection of the input via the contrac-
tion of the “outer ends” of the MPS that do not include
Bℓ. Bℓis then replaced by the updated bond tensor:
B
′ℓ= Bℓ+ α∆Bℓ, where α is a scalar value that con-
trols convergence. This can then be decomposed back into
separate MPS tensors using a singular value decomposition
(SVD), which assists in adapting the MPS bond dimension.
The singular value matrix S can then be absorbed into the
right singular vector matrix, resulting in the updated sites
A′
sj = Usj and A′ℓ
sj+1 = SV ℓ
sj+1. The MPS form is restored,
with the ℓindexing moving to the j + 1 site. The process
then iteratively continues to the next j + 1 and j + 2 tensors
and returning through the opposite direction when an end
node is reached for a predetermined number of “sweeps”. A
major advantage to this is that the resulting bond dimension
can be chosen adaptively based on number of large singular
values. This flexibility allows the MPS form of W to undergo
maximum possible compression; the degree of compression
can vary for each bond, while still ensuring an optimal
decision function. Inference is performed by successively
contracting the network until the value f l(x) is obtained,
where the classification l is determined by the largest |f l(x)|.
While the DMRG sweeping method has seen success
in several works, gradient descent based methods can be
used to directly optimize tensor networks [97], [99], [100],
[101]. This is first noted by Novikov et al. [97] who de-
veloped parameterizing supervised learning models with
MPS in parallel with the work in [92]. Gradient descent
based approaches for minimizing the cost function incurs
an increased bond dimension after each iteration, which
[92] handles via successive SVD operations to reduce the
rank. To avoid the bond dimension growth, a stochastic
Riemannian optimization procedure is used instead, with
the addition of a bond dimension regularisation term in the
cost function. The polynomial basis (Equation 21) was used
as the local feature mapping.
Fig. 6. DMRG Sweeping Algorithm [92]: (a) Formation of the bond tensor
Bℓat bond j; (b) Projection of a training input into the “MPS basis” ˜Φn at
bond j; (c) Computation of the decision function and the gradient update
∆Bℓ; (d) Restoration of the MPS form by the SVD.
Efthymiou et al. [99] similarly provide an implemen-
tation using automatic gradients for optimization, rather
than DMRG. The results show strong performance over
benchmark image classification tasks.
Araz and Spannowsky [102] perform experiments com-
paring the effectiveness of DRMG and SGD approaches in
high-energy physics applications, as well as investigating
training methods that combined both SGD and DRMG
techniques, previously noted to be speculatively compatible
[92]. At each epoch, the first batch has the DMRG applied
for a certain number of sweeps, after-which SGD takes over
for the remaining batches. This method showed comparable
performance with MPS training solely on DMRG or SGD,
however the authors note the conflicting aims of the algo-
rithms; DMRG attempts to reduce degrees of freedom of
node, while the SGD tries to increase it.
Unsupervised Tensor Network Modeling: The second
prominent machine learning formalism for tensor networks
is in probabilistic generative modeling. Generative model-
ing approaches learn the underlying probability distribution
that describes a set of data, from which new data instances
can be either generated or inferred from the distribution.
Quantum states inherently possess a probabilistic interpre-
tation, in which the squared norm of a quantum state’s am-
plitudes gives rise to the probabilities of different outcomes.
This connection can be traced back to Born’s rule in quan-
tum mechanics [103]. Thus, Han et al. [104] first proposed
the use of tensor networks for generative modeling. Their
model, dubbed by contemporaries as the “Tensor Network
Born Machine” (TNBM) [105], produces random samples by
first encoding probability distributions into quantum states,
which are represented by a tensor network. A given dataset
x is modelled using a quantum state described by a real-

17
valued wave function Ψ(x), which could be some quantum
state embedding kernel such as in Equation 15. This in turn
forms the model probability distribution:
P(x) = |Ψ(x)|2
Z
,
(24)
normalized by a partition function Z = P |Ψ(x)|2; |Ψ(x)|2
is the energy function of x. This form allows for the rep-
resentation of Ψ(x) via a tensor network. Similar to the
supervised setting, both DMRG-like, and gradient descent
algorithms can be used for optimization [104]. The loss
function used is the negative log-likelihood (NLL):
L = −1
NT
NT
X
n=1
ln P(xn).
(25)
While the TNBM formalism has been predominately used in
unsupervised contexts, it has also been adapted for super-
vised learning [100], where classification involves finding
the maximum fidelity between a given test example and the
learned quantum states argmaxc|v†Ψc|, for c ∈1, . . . , K,
where K is the total number of classes. The work in [100]
demonstrates that the mapping of images into Hilbert space
produces natural clustering patterns with and between
classes, which was only previously assumed. This explicitly
shows advantages in solving machine learning tasks in the
many-body Hilbert space as opposed to data-driven feature
spaces.
Bit-string classification over parity datasets have sug-
gested that the highly expressive probability distributions
encoded by generative MPS networks lead to strong pre-
dictive outcomes [106]. Further, MPS decompositions have
been shown to handle tasks that deep learning methods
cannot. Bradley et al. [107] note that standard models like
Restricted Boltzmann Machines (RBMs) often struggle while
learning high-length parity datasets, despite their catego-
rization as universal approximators, theoretically endowed
with the necessary expressive capabilities. Results show that
MPS models can excel at such tasks, lending further weight
to the unique inductive bias provided by tensor networks,
echoing findings from other studies such as [100].
4.2.2
Tree-Tensor Networks/MERA
Tree Tensor Networks (TTN) (also called the Hierarchical
Tucker decomposition [79]) are a tensor network structure
where the tensors are arranged hierarchically, often forming
a binary tree structure [108]. TTNs exhibit advantages in
computational efficiency compared with other tensor net-
work forms, which stems from the tree structure that avoids
loops in the network, enabling efficient and exact contrac-
tion. The Multi-scale Entanglement Renormalization Ansatz
(MERA) is a particular type of TTN, which is specifically
designed to efficiently represent quantum states with long-
range correlations [109] due to its incorporation of disentan-
glers and isomorphisms in its network structure, accounting
for and managing entanglement at different length scales.
This makes it particularly suitable for representing ground
states of critical systems or systems with power-law decay
of correlations. While not as general as PEPS or MPS, TTNs
and MERA offer advantages in specific contexts where their
structure aligns with the physical system.
The mathematical form of the TTN varies based on
the compositional structure, dependant on the number of
layers, the number of tensors, and the contraction of those
tensors within those layers. The contractions are often de-
fined recursively. An example is given in Equation 26 for
the construction of a binary TTN, with each tensor node
connected to two children:
W =
X
{α}
A[1]
α2,α3
N
Y
n=2
A[n]
αn,α2n,α2n+1,
(26)
where tensor A[1] is the root tensor, with subsequent A[n]
children over N total layers [110]. Similar to the MPS,
the TTN can be trained via DMRG and gradient-based
optimization. Predicting an output is then given by the
contraction:
| ˜pn⟩= W † · Φ(xn).
(27)
In addition to gradient-descent based optimization used
in the MPS, a MERA-like training process has also been
proposed [111]. The cost function to be minimized is chosen
as:
f = −
N
X
n=1
⟨pn| ˜pn⟩,
(28)
with N as the total number of samples. This cost can
be reduced by imposing unitary constraints on all tensors
A of the TTN such that A†A = I, inducing the whole
transformation as a unitary: W †W = I. Thus the simplified
cost function becomes:
f =
N
X
n=1

⟨Φ(xn)| WW † |Φ(xn)⟩−2 ⟨Φ(xn)| W |pn⟩+ 1

=
N
X
n=1
⟨Φ(xn)| W |pn⟩,
(29)
and is shown to reduce the complexity of optimization. Over
the task of Modified National Institute of Standards and
Technology (MNIST) [112] image classification in [111], this
learning method exhibited relatively small entanglement be-
tween classification states, meaning that the TTN efficiently
represents the MNIST dataset; a conjecture that may extend
over classical images in general.
2D hierarchical structures have also been proposed for
generative modeling as a direct extension of the MPS ver-
sion in [104], where the modeling of 2D images can be
directly achieved [110]. This was shown to overcome the
issue of exponential decay of correlations in MPS, making
it more effective in capturing long-range correlations and
perform better for large-size images.
TTNs have been used as a means of coarse-grained
unsupervised feature extraction in [88]. The model prepares
quantum states from classical input and feeds them into a
1D TTN. Optimal weights W are computed from its left sin-
gular vectors Ws = P
n βnU †n
s . The basis U † diagonalizes

18
the feature space covariance matrix ρs′
s
= P
n U s′
n PnU †n
s .
Since direct diagonalization of ρ is not feasible, the DMRG-
like algorithm is employed to iteratively produce and diago-
nalize reduced density matrices. This procedure is repeated
log(N) times to produce a suitable approximation, leading
to the diagonalizing U of isometry tensors, approximated
as a layered TTN. To produce a classification output, the
reduced feature set is subsequently used in a supervised
context, where the layers of U are fixed and the top tensor
is replaced with an MPS decomposition, optimized via
DMRG. The authors note that the method is akin to a direct
computation of kernel PCA in feature space.
4.2.3
Projected Entangled-Pair States
Projected Entangled-Pair States (PEPS) are a natural exten-
sion of MPS to higher-dimensional systems [113]. Just as
MPS provide efficient descriptions of 1D quantum systems,
PEPS have been shown to efficiently represent ground states
of gapped 2D local Hamiltonians [75]. The tensors in PEPS
are arranged in a grid-like structure, which mirrors the
geometry of physical systems. This structure allows the
PEPS to effectively capture long-range quantum correlations
[114]. The PEPS decomposition has a polynomial correlation
decay with respect to the separation distance between parts
of the network, whereas the MPS decomposition shows
an exponential decay [79]. Like MPS, the bond dimension
of PEPS determines the maximum entanglement entropy
across any cut of the tensor network. However, contracting
the PEPS networks is computationally more challenging
than MPS due to the increased tensor connectivity. As
such, exact calculations for PEPS are practically infeasible
for larger systems, and approximation methods are usually
employed.
The weight matrix W can be modelled as PEPS decom-
position of tensors on an L × L grid, mapped to some
x ∈RL×L input feature tensor:
W l
s1s2...sN =
X
{a}
Aα1,α2
s1
Aα3,α4,α5
s2
· · ·
(30)
Al:αk,αk+1,αk+2,αk+3
sj
· · · AαK−1,αK
sN
,
(31)
where K is the number of bonds in the lattice, and each
tensor has a ”physical” index sj connected to the input
vector, along with ”virtual” indices αk for contraction with
adjacent tensors. A special tensor in the center also has a
”label” index l to generate the output vector [115].
In unsupervised generative modeling, the PEPS model
can capture the probability distribution P(x) as a decom-
posed sum of individual distributions:
P(x) = N1
N P1(x) + . . . + Nm
N Pm(x),
(32)
each representing different labels or categories over data,
in which the total wavefunction seen as a superposition of
these distributions with smaller entanglement [116]. Each
Pi is weighted by the fraction of its category within the
total training set Ni/N, with the categorisation determined
by labels, if present, allowing for supervised generative
modeling, or some agnostic clustering algorithm.
Efficient contractions are possible when both image size
L and bond dimension D are small; for larger representa-
tions, an approximate contraction method is used, which
treats the bottom row tensors as an MPS and the rest of row
tensors as the operators applied on the MPS, over which the
DMRG-like truncating algorithm can operate [115].
The PEPS decomposition has seen success in image
modeling. Cheng et al. [115] note that the method better
captures structural and spatial information in images when
compared to MPS and TTNs [115]. Each image pixel is
mapped to each PEPS tensor without the need for flattening,
such as with MPS decompositions. The grid structure of
the PEPS allowed for the exploration of additional feature
maps; two are explored, the trigonometric feature map
(Equation 17), and an adaptive feature map that leverages
convolutional kernels, allowing the PEPS to accept a feature
tensor as input. The use of convolutional feature extractors
proved more effective in experiments over MNIST and
Fashion-MNIST [117] datasets. Vieijra et al. [116] explore
PEPS for unsupervised generative modeling, which showed
greater performance then that of existing MPS and TTN
models. The results suggest the enhanced capability of
multi-dimensional tensor network structures in unsuper-
vised generative modeling for image classification, with
[116] stating that tensor networks perform better when the
network mimics the local structure of the data.
4.2.4
Matrix Product Operators
The Matrix Product Operator (MPO) is an extension of the
MPS concept to describe quantum operators, especially in
the context of 1D quantum systems [118]. An MPO is ex-
pressed as a collection of matrices arranged in a chain, with
each matrix indexed by physical indices that account for the
ingoing and outgoing states of the operator. This structure
allows for a compact and efficient representation of com-
plex quantum operators that would otherwise require an
exponentially large amount of information. Further, MPOs
can be compressed to approximate a given operator with
smaller matrix dimensions. Similar to the MPS, techniques
such as SVD can be applied, reducing the bond dimensions
while maintaining a controlled approximation error.
An MPO can be expressed in the form:
M s1,...,sN
s′
1,...,s′
N =
X
{a}
As1α1
s′
1
As2α2
α1s′
2 · · · Asj+1αj+1
αjs′
j+1
· · · AsN
αN−1s′
N ,
(33)
in traditional mathematical notation, or diagrammatically as
shown in Figure 7 [119].
Fig. 7. Diagrammatic Notation of the Matrix Product Operator (MPO)
with six Sites [119]
The ability for compressed representation has allowed
for the use of MPOs in classical neural network structures.

19
Research has shown that replacing one, or several dense
hidden layers in the neural network with a parameterized
MPO can allow for significantly decreases the number of
parameters involved, effectively compressing the network
[120], [121]. This transformation maintains the network’s
ability to express complex relationships and functions, thus
preserving its overall performance while also improving
the training efficiency and memory consumption. In [121],
a slight improvement is seen in test accuracies in state-
of-the-art models that incorporate the MPO compression,
compared to their standard parameterization. The authors
suggest that by reducing in the number of parameters, local
correlations in input signals are emphasized, and the risk
of overfitting is reduced by constraining the linear trans-
formation matrix, which helps avoid trapping the training
data in local minima. Patel et al. [122] extend this approach
for solving differentiable equations involving in portfolio
optimization, to similar results.
Wang et al. [123] explore the use of MPOs as sparse rep-
resentations of operators, in parameterizing a linear trans-
formation over an exponentially large space with respect to
the number of features. The authors highlight the suitability
of the model for anomaly detection, as linear models utiliz-
ing MPOs can provide control and manage behavior over
the entire input space, even when it’s vastly imbalanced
in terms of inliers and outliers. In the anomaly detection
model, data is first embedded into a high-dimensional vec-
tor space V using a fixed feature map Φ. “Normal” instances
undergoing the transformation would be projected close
to the surface of the hypersphere W, whereas anomalous
instances are mapped close to its center.
4.2.5
Other Tensor Network Methods
Several other tensor network decompositions have been
suggested for use in probabilistic modeling, particularly
over natural language modeling tasks. The Canonical
Polyadic (CP) and the Tucker decompositions, similar to
decompositions previously mentioned, have been used for
their capacity for producing low-rank approximations of
high dimensional tensors. Several authors have demon-
strated their suitability in modelling the sequential and
polysemic natural of words. Hierarchial MPS models have
also been proposed, providing a structured way to compute
exact normalized probabilities and perform unbiased direct
sampling, allowing for efficient training through gradient-
based procedures and demonstrating competitive perfor-
mance on tasks such as image classification with reduced
computational resources. Finally, we touch on the use of
tensor networks embedded within neural network struc-
tures, in which tensor networks have demonstrated their
capabilities in model compression.
Canonical Polyadic Decomposition: In Zhang et al. [94],
a weighted form of the CP decomposition is used, which
factorizes a higher-order tensor into a sum of rank-one
tensors, expressed as:
W =
R
X
r=1
λrar,1 ⊗ar,2 ⊗. . . ⊗ar,N
(34)
where λr are scalar weights and ar,i are unit column vec-
tors of M-dimension: (ar,i,1, . . . , ar,i,M)⊺, where M is the
dimension of the corresponding mode of W. R denotes the
minimum possible number of rank-one tensors. The authors
propose an approach based on the CP decomposition to
account for interaction among polysemic words, creating
new compound meanings by combining the different basis
vectors of the words. The global representation of all possi-
ble compound meanings is captured in the quantum many-
body wave function of Equation 19. Solving the resulting
high-dimensional tensor is made feasible through the use
of tensor decomposition, which then enables the projection
of a global semantic space onto a local one for a particular
sequence of words.
Recurrent Methods: Zhang et al. [95] propose a Re-
cursive Tensor Decomposition, inspired by the MPS and
Tucker decompositions. The decomposition of a tensor T
is expressed as:
T =
r
X
i=1
λiS(n),i ⊗ui
(35)
S(n),k =
r
X
i=1
Wk,iS(n−1),i ⊗ui
(36)
where S(1) = 1 ∈Rr. Here, an n-order tensor T ∈Rm×...×m
is decomposed into an n-order tensor S(n) ∈Rm×...×r,
a diagonal matrix Λ ∈Rr×r, and a matrix U ∈Rr×m:
λi and ui are the i-th singular values and left singular
vectors resulting from each successive factorization. The
parameter r (with r ≤m) denotes the rank of the tensor
decomposition, and the decomposition can be viewed as a
matrix SVD after tensor matricization or flattening by one
mode. The method reduces the parameters from O(mn)
to O(m × r), effectively capturing the main features of
the tensor T while significantly reducing the complexity.
The decomposition can be used to calculate the conditional
probability p(wt|w1
t−1) of sequential words via the softmax
function over the inner product of two tensors T and A,
where At−1 is the input of (t −1) words α1, . . . , α(t−1).
Intermediate variables ht are recursively calculated using:
ht = Wht−1 ◦Uαt
where ◦denotes element-wise multiplication, and W and U
are matrices decomposed from the tensor T.
Miller et al. [96] propose the uniform Matrix Product
State (u-MPS) model as a type of recurrent tensor network
used for processing sequential data. In the u-MPS, all cores
of the MPS are identical tensors A with shape (D, d, D).
The model’s recurrent nature allows it to generate n-th
order tensors Tn ∈Rdn for any natural number n, en-
abling its application to sequential data. For a sequence of
arbitrary length n over an alphabet Σ of size d, a u-MPS
can map the sequence to the index of an n-th order tensor
Tn, defining a scalar-valued function fA(s) = α⊺A(s)ω,
where A(s) = A(s1)A(s2) . . . A(sn) represents the compo-
sitional matrix product of the sequence, and α and ω are
D-dimensional vectors that serve as boundary conditions
terminating the initial and final bond dimensions of the
network. Parallel evaluation of fA(s) is efficient, carried
out in O(log n) time, and enables u-MPS to be used in
various tasks such as probabilistic sequence modeling. The
application of a unitary MPS (u-MPS) model has shown

20
unique generative properties and successful extrapolation
of non-local correlations, indicating potential scalability to
real-world sequence modeling tasks. These findings under-
line the suitability of MPS and tensor networks in handling
complex linguistic phenomena.
Generalized Tensor Networks: Glasser, Pancotti and
Cirac [101] explored the connection between tensor net-
works and probabilistic graphical models, from which “gen-
eralized tensor networks” are proposed. These allow for
input tensors to be copied and reused in other parts of
the network, allowing for greater computational efficiency
while also involving fewer parameters, and greater expres-
sivity over new types of variational wavefunctions when
compared with regular tensor networks. Several generalized
tensor network structures are proposed, the discriminative
string-bond states (SBS) and entangled plaquette state (EPS)
models utilizing tensor copy operations, and are tested
in supervised contexts in both image and environmental
sound classification tasks. The method showed superior
performance to other tensor network learning models; the
authors note that these results are achieved over very small
bond dimensions compared to previous works.
Hierarchical
MPS: Hierarchical MPS models have
emerged as a powerful approach for addressing complex
machine learning tasks. They are characterized by a tiered
structure that emphasizes the flexibility of representation
and computational efficiency.
Liu, Zhang and Zhang [124] noted the generally poor
performance of prior works in generative tensor network
models relative to standard methods. To address this, an
autoregressive MPS (AMPS) is proposed, where the joint
probability distribution P(x) is not represented by a tensor
network directly, but the factorization of it as a product
of conditional probabilities; an idea which stems from au-
toregressive modeling in ML. The model constructs a 2D
hierarchical tensor network representation using separate
MPS decompositions for individual conditional probabili-
ties. These individual MPS tensor elements can be trained
and parameterized through a gradient-based Negative Log-
Likelihood (NLL) minimization process, demonstrating a
significant theoretical expressive power that exceeds previ-
ous tensor network models.
Selvan and Dam [125] introduce the Locally Orderless
Tensor Network (LoTeNet) model using the theory of lo-
cally orderless images that allows it to handle larger im-
ages without sacrificing global structure, a deviation from
prior models that flatten entire 2D images [92], [99], [104].
LoTeNet begins with a squeeze operation, rearranging local
k × k image patches and stacking them along the feature
dimension, with the stride of the kernel k determining the
reduction in spatial dimensions. The squeezed images with
an inflated feature dimension of C · k2 are then flattened
from 2D to 1D and processed through MPS blocks, embed-
ded into the joint feature space, and contracted to output a
vector with dimension ν. The output vectors from all MPS
blocks are reshaped back into 2D space and passed through
subsequent layers of the model. After L layers, the final MPS
block performs a decision contraction, resulting in predic-
tions for the M classes. In evaluation, this model required
fewer hyperparameters and significantly less GPU memory
than standard deep learning models. This is primarily due
to its design of successively contracting input into smaller
tensors, thereby avoiding escalating memory consumption
with larger images and batch sizes.
Tensor Networks in Neural Network Architectures. In
4.2.4, the benefits of using MPO tensor networks as neural
network layers as methods of neural network compression
is discussed. Other decompositions have also been sug-
gested for this purpose, such as in [126], who integrate the
Tucker decomposition into a neural network architecture
for performing voxel-wise processing for fully automated
semantic segmentation of brain and liver volume scans. Us-
ing a tensor representation for the high-dimensional weight
vector is shown to enhance network operations and extracts
critical semantic information, while also facilitating faster
convergence and improving precision.
Tensor networks have also been used as feature ex-
tractors for neural networks, which is a natural extension
given their kernel-based feature map representations. The
resulting low-rank tensors in [94] are used as trainable
kernel weights fed into a convolutional layer in a classical
CNN model.
4.2.6
Differences in Optimization Methods
Tensor networks are most commonly optimized using either
DMRG sweeping or gradient descent, particularly SGD. A
comparison of these methods reveals various strengths and
weaknesses:
Performance: As evidenced in Tables 3 and 4, gradient
descent-based optimization generally outperforms DMRG
on both supervised Fashion-MNIST and unsupervised bi-
nary MNIST. An exception can be found in the method used
in [92] on the supervised MNIST task, which is the second-
best performing model. However, the study by [102] ob-
served that SGD and DMRG produced similar classification
results over various settings. These findings do not entirely
reflect the performances in Tables 3 and 4, however this may
be due to other factors pertaining to the models in question.
Understanding the nature of these observed inconsistencies
might call for additional examination or targeted studies.
Efficiency and Usability: Gradient descent optimization
is often more efficient than DMRG, especially when stochas-
tic gradients are considered [88], [102]. DMRG is known
to exhibit large computational complexity, especially for
decompositions outside of boundary MPS [115]. This may
limit the applicability of the algorithm; we note that DMRG
is mostly used in MPS and TTN settings from Tables 3 and 4,
or on small-sized PEPS [115]. Additionally, the widespread
use of GD in machine learning, as well as its suitability
for parallelization [99] makes it readily adoptable by ML
practitioners.
Simplicity and Interpretability: DMRG provides a sim-
pler learning structure and can adapt the network architec-
ture based on the complexity of the problem, reducing the
need for hyperparameter optimization [99]. Furthermore, it
provides more interpretable results than SGD, emphasizing
inherent data structures for a better understanding of the
learned representations [102].
Information Capture: In image recognition, SGD and
DMRG produce similar classification results, but SGD is
shown to capture more information due to a larger entropy,
potentially outperforming DMRG in scenarios with large

21
data fluctuations. DMRG focuses more on the image’s cen-
tral part and maintains a similar entropy distribution per
site [102].
4.2.7
Enhancing Tensor Network Methods
Image recognition tasks have served as an experimental
ground for enhancing tensor network methods, especially
in optimizing computational efficiency, understanding the
exploitation of Hilbert feature spaces, and optimizing model
selection based on entanglement scaling analysis.
Liu et al. [127] propose using quantum entanglement in-
formation to guide the learning of MPS architectures for im-
age recognition tasks. By converting images into frequency
space using a direct cosine transformation, natural local
correlations in 1D space are enhanced, and entanglement
structures are shown to prioritize low-frequency data. These
factors are captured in lower entanglement entropy across
the system. An MPS is used to model these correlations.
A MERA-based learning method is used, which minimizes
the bipartite entanglement entropy (BEE) by rearranging the
MPS contraction path to align the single-site entanglement
entropy values (SEE) in descending order. Low SEE sites
are discarded. This approach has demonstrated solid accu-
racy with relatively small bond dimensions on the MNIST
dataset. Furthermore, the authors argue that their method
reduces the number of qubits needed for quantum models,
in contrast to methods where qubit count scales linearly
with image size.
A few works also tackle the question of selecting the
optimal tensor network decomposition for a given task.
Convy et al. [128] applied entanglement scaling analysis
from quantum physics to classical ML data. For a quantum
system represented as a tensor network, the entanglement
entropy of the system is bounded by its maximum bond
dimension m and n connecting indices. Assuming a fixed
m (typically a hyperparameter), the entanglement scaling
differs between tensor networks due to n, which depends on
the network geometry. Consequently, the foremost objective
when deploying tensor network ansatze is to align the
entanglement scaling dictated by the data, or the quantum
state, with the inherent entanglement scaling of the network.
The authors proposed the Mutual Information (MI) score to
analyze entanglement scaling on classical data. They inves-
tigated MI scaling patterns in the MNIST and grayscaled
Tiny Images [129] datasets. Results suggested that Tiny
Images’ MI follows a boundary law, while the findings were
less conclusive for MNIST. These insights could guide the
selection of tensor networks, with 2D geometries like PEPS
being suitable for datasets obeying a boundary law.
Hashemizadeh et al. [130] propose a greedy algorithm
for tensor network structure learning, aimed at efficiently
traversing the space of tensor network structures for com-
mon tasks like decomposition, completion, and model com-
pression. They introduce a novel tensor optimization prob-
lem that seeks to minimize a loss across diverse tensor
network structures with a parameter count constraint. This
bi-level optimization problem uniquely involves discrete
optimization over tensor network structures at the upper
level and continuous optimization of a specific loss function
at the lower level. Their approach entails a greedy algorithm
to address the upper-level problem, which is then combined
with continuous optimization techniques to solve the lower-
level problem. Starting with a rank one initialization, the
algorithm successively identifies the most promising edge
in a tensor network for a rank increment. This allows for
the adaptive identification of the optimal tensor network
structure for a given task, directly from the data, evidenced
by experimental results over image reconstruction.
4.2.8
Understanding Neural Networks via Tensor Networks
Aside from the use of tensor networks for learning tasks, the
application of these methods to machine learning can help
us understand the machinery of neural networks in new
ways.
Cohen et al. [131] provides comprehensive examina-
tion of the representational capabilities of diverse tensor-
network architectures in the context of probabilistic model-
ing, including structures like non-negative matrix product
states and Born machines. This research underscored the
fundamental advantage of deep networks over their shallow
counterparts, demonstrating that functions that could be
efficiently represented by a deep CNN of polynomial size
would necessitate an exponential size for approximation
by a shallow CNN, which was previously a conjectural
theorem. Moreover, the study highlighted the extraordinary
power of neural network depth in exponentially reducing
the need for breadth with each additional layer, although
the exact class of expressible functions remains a subject of
ongoing exploration. Subsequent work further extends this
analysis to recurrent neural networks, showcasing their nat-
ural correlation with MPSs and validating Cohen’s theorem
in this context [132].
Gao and Duan [133] show that efficient Deep Boltzmann
Machine representations can be efficiently constructed from
any tensor network state, including PEPS and MERA, pro-
vided a deep enough neural network. In essence, assuming
quantum state presentation is a P/poly class problem, deep
neural networks possess the capability of representing the
majority of physical states efficiently, including the ground
states originating from many-body Hamiltonians and states
that emerge from quantum dynamics. Chen et al. [134] show
that under certain conditions, the reverse is also true — TN
states can be converted into RBMs if they describe a non-
entangled quantum system. Levine et al. [135] furthers the
work in [133] and extends the proof to CNNs and RNNs
— when considered under TN representations, these neural
networks can efficiently represent entangled quantum sys-
tems.
4.3
Quantum Variational Algorithm Simulation
In view of the categorization of QiML within the “classical-
classical” (CC) mode of QML, as indicated in Figure 1, one
could interpret QiML as the application of classical data
to quantum circuits, all simulated on classical hardware, to
process machine learning tasks. This category also overlaps
with the “classical-quantum” (CQ) aspect of QML, which
encompasses concepts such as variational quantum circuits
where optimization of quantum parameters is offloaded to
classical computation methods. This research area delves
into the potential capabilities of quantum computing in
anticipation of the realization of quantum hardware. It

22
is worth noting that despite the common disjunction be-
tween references to QiML and classical-quantum based ML
in the literature, the common denominator remains that
classically simulated quantum circuits are indeed run on
classical hardware. Considering this perspective, the task
becomes discerning those studies that not only investigate
practical machine learning applications but also develop
techniques particularly intended for classical hardware sim-
ulation. These constitute a minor subset within the vast
body of QML literature, where the CQ paradigm takes
center stage. Moreover, using naive keyword search terms
such as ”quantum simulation” may not yield desired results,
as this term already denotes an established research domain
[136], [137].
Our survey into this area is thus informed by both our
own keyword searches, and the recent reviews of other
authors exposing the interested subset, such as [138]. We
target works that use quantum simulation and attempt to
further machine learning tasks of interest by some met-
ric (performance, speed, resource consumption, etc.), over
purely classical implementations.
In the following section, we highlight recent advance-
ments in simulating quantum computing, present a concise
overview of QML learning frameworks, and discuss recent
practical applications in this field. For a more thorough
understanding, we direct interested readers towards com-
prehensive resources such as [4], [37], [138], [139], [140],
[141], [142].
4.3.1
Frontiers of Classical Simulation
The successful emulation of quantum computations on
classical hardware hinges on the capability to simulate
qubits and their potential for exponential information stor-
age effectively. Determining the boundary between classical
and quantum computation elicits a discussion on quan-
tum supremacy [143]; finding the exact crossover point,
beyond which a quantum system becomes infeasible for
classical computer simulation, remains a complex issue
[144]. Classical resources needed for such simulations scale
exponentially with the number of qubits and the depth of
the quantum circuit [144], marking an exponential cost in
their classical parameterization.
Despite these challenges, researchers have pushed the
boundaries of classical hardware capabilities. Strategies
such as data compression [145], optimized circuit parti-
tioning [146], and large-scale batching methods [147] have
enabled the simulation of many tens of qubits. However,
these frontiers are largely restricted to supercomputing, or
high-performance platforms. For users without access to
such architectures, the possibilities are considerably more
limited; a PC equipped with 16GB of GPU memory can
simulate approximately 30 qubits [148]. Given this limita-
tion, many practical applications of QML operate within
this smaller qubit range. As such, we will focus on studies
that have achieved promising results on these smaller-scale,
more accessible devices.
For a comprehensive exploration of the challenges in-
volved in the practical simulation of quantum computers,
we refer the interested reader to the work by Xu et al. [148].
4.3.2
Encoding Classical Data
Classical data must be processed through an encod-
ing mechanism for quantum settings, wherein an m-
dimensional classical dataset is mapped onto a quantum
state vector within Hilbert space. This procedure permits
us to leverage the vast feature space in quantum systems,
thereby offering superior representational power in com-
parison to classical feature spaces. We outline common
encoding schemes below:
1)
Basis Encoding: Also known as computational basis
encoding, it is the simplest way of encoding classical
data. Given a classical vector x = (x1, x2, ..., xn),
where xi ∈0, 1, each classical bit xi is encoded onto
the state of the i-th qubit. An n-bit classical string is
directly encoded into a quantum state of n qubits:
|x⟩= |x1, x2, ..., xn⟩.
(37)
2)
Amplitude Encoding: This method allows efficient
encoding of classical data, taking advantage of the
exponentially large Hilbert space. In amplitude en-
coding, an n-dimensional normalized real vector
x = (x1, x2, ..., xn) such that Pn
i=1 |xi|2 is encoded
into the amplitudes of a quantum state. This re-
quires at least log2(n) qubits, where n is the dimen-
sion of the classical vector. The encoded state is:
|x⟩=
n
X
i=1
xi |i⟩.
(38)
Note that this is the encoding predominantly used
in the dequantized algorithms context.
3)
Angle Encoding: In angle encoding, data is encoded
into the angles of rotational gates. Given a classical
vector x = (x1, x2, ..., xn), each value xi is used as
a parameter in a rotation gate applied to the i-th
qubit. For example, with Ry rotations, the encoded
state is:
|x⟩= Ry(x1) ⊗Ry(x2) ⊗... ⊗Ry(xn) |0⟩⊗n . (39)
Note the similarities between this and Equations 15
and 17; a possible Ry gate rotation could produce
the embedding:
|x⟩=
n−1
O
i=0
Ry(xi) |0⟩⊗n
=
n−1
O
i=0
cos
π
2 xj

|0⟩+ sin
π
2 xj

|1⟩.
(40)
In each of these methods, classical data is encoded into the
quantum state space, and these encoded states are then used
as inputs to quantum circuits. Different encoding methods
can lead to different computational advantages, and the
choice of encoding is often problem-specific.
Once classical data is embedded into the quantum space,
it can be manipulated via various QML algorithms. These
algorithms are diverse and span a broad range of types,
stemming from various mathematical bases [138], [142].

23
Some algorithms, such as Quantum Boltzmann Machines,
are known to be BQP complete, implying they cannot be ef-
fectively simulated on classical computers [149]. Conversely,
there are QML algorithms that remain within the realm of
classical simulation, up to classical computing limits. These
typically include quantum kernel and quantum variational
methods [138].
4.3.3
Quantum Kernel Methods
Quantum kernel methods utilize quantum devices to com-
pute kernel functions, thus capturing the similarity between
data points in a feature space. Notably, these methods have
the potential to provide exponential speedups for specific
problems while still being classically tractable. A frequently
employed quantum kernel method is the Quantum Kernel
Estimator (QKE) [150]. This method involves defining a
quantum feature map, ϕ(x), that transforms classical data,
x, into quantum states via unitary operations on a quantum
circuit Uϕ(x). This is performed by applying the circuit to
an initial state, commonly chosen as |0⟩⊗n:
|Φ(x)⟩= Uϕ(x) |0⟩⊗n ,
(41)
where Uϕ(x) can be considered as a unitary that pro-
duces quantum states |Φ(x)⟩based on a chosen feature
mapping ϕ, and n is the number of qubits. For any two
data points, xi and xj in the dataset D, their correspond-
ing encoded states are Φ(xi) and Φ(xj). The kernel entry
between xi and xj is given by:
κ(xi, xj) = | ⟨Φ(xj)|Φ(xi)⟩|2
(42)
= | ⟨0|⊗n U †
ϕ(x)Uϕ(x) |0⟩⊗n |2,
(43)
representing the inner product of the two feature vectors
in the quantum state space. The computation is achieved by
approximation in computing the overlap of quantum states
Φ(xi) and Φ(xj). The quantum kernel can then be used to
construct a Quantum Support Vector Machine (QSVM) that
integrates the quantum kernel (constructed via QKE) with a
classical SVM, replacing the kernel K(xi, xj) in Equation 92
[150]. This method essentially processes data classically and
uses the quantum state space as feature space, enabling the
use of high-dimensional, non-linear feature mappings that
are difficult to compute classically.
The optimal choice of Uϕ(x) is largely an unsolved re-
search problem, especially in the context of classical simula-
tion. A common kernel is the UΦ(x) = ZΦ(x)H⊗nZΦ(x)H⊗n,
where H is the Hadamard gate, and Z is a diagonal unitary
in the Pauli-Z basis [150]. However, this kernel has been
known to be hard to implement classically. There may
be opportunity to explore quantum kernels that are more
amenable to classical settings.
The QSVM has been tested over various applications in
simulated settings. Sim˜oes et al. [151] empirically assess the
QSVM in terms of accuracy against classical SVMs. Experi-
ments showed that the quantum models outperformed clas-
sical solutions whilst being comparatively less complex over
relatively small datasets, such as the Iris and Rain datasets.
Payares and Martinez-Santos [152] achieved similar results
in detecting distributed denial of service (DDoS) attacks.
The QSVM achieved high accuracy in binary classification
(99.6%), but the authors note its substantial computational
demand. Masun et al. [153] evaluated the performance of
QSVM for malware detection and source code vulnerability
analysis, utilizing the ClaMP and Reveal datasets respec-
tively. The QSVM showed relatively comparable perfor-
mance compared to classical SVMs over both tasks, however
exhibited significantly extended execution times.
4.3.4
Variational Quantum Circuits
Of the various variational quantum algorithms devised for
quantum-based optimization, the most representative class
of algorithms that capture the essence of machine learning is
the variational quantum circuit (VQC). VQCs use a hybrid
quantum-classical approach to solve complex problems. A
classical optimizer adjusts the parameters of a parameter-
ized quantum circuit (PQC), so that the output of the quan-
tum circuit approaches an optimal solution. VQCs serve as
quantum analogues of neural networks, with the capability
to encode classical data into quantum states and harness the
power of quantum computing to minimize cost functions.
These cost functions often represent the expectation value of
some operator, such as a Hamiltonian in quantum physics
or a measurement operator in machine learning tasks.
VQCs have been devised in response to current limita-
tions in implementing quantum algorithms on true quan-
tum computers. By employing a hybrid quantum-classical
framework, VQCs utilize classical optimization techniques
to fine-tune the parameters of quantum circuits [154]. This
combination allows for the exploitation of quantum paral-
lelism and interference in problem-solving while mitigating
some of the challenges faced by fully quantum algorithms,
such as error rates and limited coherence times. The classical
optimizer guides the training process, iteratively updating
the quantum circuit’s parameters based on the outcomes
of quantum measurements. This approach leverages the
power of quantum computing while accommodating the
constraints of near-term quantum devices, thereby facilitat-
ing the development of quantum applications that would be
infeasible with solely quantum-based methods.
A VQC workflow typically comprises three steps:
•
Quantum Feature Map: Classical data x, is en-
coded into the quantum state space using a non-
linear feature map (ϕ). The encoding circuit defined
in Equation 41 is similarly used, where ϕ may be
some chosen encoding method. This process can be
repeated or interleaved with the variational circuit,
depending on the problem at hand.
•
Variational Circuit: A short-depth parameterized
quantum circuit, U(θ), is applied to the quantum
state obtained from the feature map. This circuit
consists of layers of quantum gates parameterized
by θ. Learning the parameters θ can be seen as an
objective minimization task over some loss function
L(θ) with respect to the circuit expectation values
Mk, similar to classical machine learning routines.
Thus the use of classical routines such as gradient-
descent have seen success in application. Offloading
the computation to classical machines reduces the
number of quantum resources required, allowing for

24
Fig. 8. Overview of the Variational Quantum Classifier (VQC) [154]. The state preparation circuit UΦ(x) encodes classical data into a quantum
state using the function ϕ(x). The variational circuit Uθ, parameterized by θ, acts on this prepared state and possibly on additional ancilla qubits.
The framework outputs observable quantities {Mk}K
k=1, which are measured and mapped to the output space through a classical post-processing
function f to predict a result. Classical methods optimize the parameters θ.
feasilbility on NISQ hardware implementation. Anal-
ogous to neural networks, variational circuits have
been shown to approximate any target function, up
to arbitrary error [154], making them viable learning
models. The choice of ansatz, or circuit design, in
this stage is critical and can significantly influence
the performance of the VQC.
•
Measurement: A measurement is performed on the
final quantum state, resulting in a bit string z ∈0, 1n,
which is then mapped to a label. By running this
circuit multiple times, the probability of observing z
can be estimated:
f(x, θ) = ⟨Φ(x)| U(x, θ)†(θ)MkU(x, θ) |Φ(x)⟩(44)
This quantity is computed for each of the different
classes k ∈{1, . . . , K} using the measurement oper-
ator Mk and can be interpreted as the prediction of x
by the circuit.
Early works have demonstrated the predictive capabilities
of the VQC in simulation. Schuld et al [155] propose a
VQC that is both low-depth and highly expressive through
the use . The proposed circuit geometry uses systemati-
cally entangled gates, allowing for circuits that scale poly-
logarithmically with the dataset size and representative
principle components. However, the authors note that this
approach limits the set of applicable datasets. Simulated
results over benchmark handwritten digit and tabular classi-
fication tasks showed superior performance compared with
classical multilayer perceptrons (MLPs) and SVMs, while
exhibiting far lower parameter counts. Similar comparative
studies for generic VQC models have been conducted across
several domains, such as cybersecurity [156], finance [157],
and physics [158], [159]. Such works highlight the enhanced
performance of VQCs over several classical methods, in-
cluding DNN, SVM, k-nearest neighbors (KNNs), Naive
Bayes, and Decision Trees, provided low qubit count, low
circuit depth, and relatively small datasets. Further, the
resource demand for quantum simulation with larger data is
high, increasing exponentially with the number of variables
[157], [158].
Blance and Spannowsky [160] proposed a combination
of quantum and classical gradient descent methods for pa-
rameter optimization. The optimization process employs a
forward pass to calculate the MSE loss function, followed by
a backpropagation procedure to update the trainable param-
eters. Quantum gradient descent, based on the Fubini-Study
metric [161], is employed to optimize the quantum weight
parameters w providing an advantage over the Euclidean-
based gradient descent by taking into account the geometry
of the parameter space of quantum states. Vanilla gradient
descent is used to optimize the classical bias term b. The
combined optimization algorithm is given by:
θw
t+1 = θw
t −ηg+∇wL(θt),
θb
t+1 = θb
t −η∇bL(θt),
(45)
where g+ is the pseudoinverse of the Fubini-Study met-
ric. The benefits of the quantum method are seen in simu-
lation results, where faster learning and convergence rates
are observed in comparison with classical neural networks
using standard steepest descent.
Various extensions to this formulation have also been
proposed, which aim to mirror classical neural network
frameworks. These VQC are typically characterized by their
circuit architecture, choice of quantum gates, and overall
compositional structure. The pipeline of feature mapping,
circuit construction, and measurement persists within all
these algorithms. We briefly discuss such methods in the
sections below.
4.3.5
Quantum Convolutional Neural Networks
The quantum convolutional neural network (QCNN) em-
ploys VQCs to perform convolutional operations that mimic
the functionality of their classical counterparts but in the
quantum domain. The QCNN architecture consists of quan-
tum convolution layers represented by quasilocal unitary
operations, pooling layers achieved by measuring qubits
and applying subsequent unitary operations, and fully con-
nected layers, implemented by specific unitary transforma-
tions.
Various frameworks for the QCNN have been offered,
exhibiting differing approaches and techniques. In Cong et

25
al. [162], the quantum convolutional layer is characterized
by a single quasilocal unitary operation (denoted by Ui).
Pooling is carried out by measuring some qubits and using
the results to define unitary operations (denoted by Vj) on
nearby qubits. When the remaining qubit count is manage-
able, a unitary operation F acts as a fully connected layer
before the final measurement. During the training phase, the
unitaries are optimized with O(log(N)) parameters, where
N is the input qubit count. Li et al. [163] introduced the
Quantum Deep Convolutional Neural Network (QDCNN)
that uses a layered sequence of VQCs as the convolutional
filters, with a final VQC producing the classification result.
The implementation, however, relies on efficient QRAM for
input state preparation. Henderson et al. [164] proposed the
Quanvolutional Neural Network, a model that integrates
random quantum circuits as filter layers within a traditional
CNN structure for feature extraction in image classification
tasks. These quanvolutional filters can be stacked by insert-
ing a classical pooling layer in between. In particular, Riaz
et al. [39] empirically showed that increasing the number
of quanvolutional layers enhances performance. Addition-
ally, the use of strongly entangled quantum circuits instead
of random quantum circuits as transformation layers fur-
ther improved performance. Numerical simulation of these
methods performed over benchmark image datasets have
showed superior performance to a classical CNNs with
similar structure, often with faster convergence [162], [163],
[164], [165].
4.3.6
Quantum Autoencoders
The quantum autoencoder (QAE) aims to compress quan-
tum data into a smaller dimension, preserving essential
information while reducing the number of qubits required
to describe the data. It consists of two main parts: the
encoder and the decoder. The encoder maps the original
data into a compressed space by applying a VQC, reducing
the dimensionality of the data. The decoder reverses this
process, attempting to reconstruct the original data from the
compressed representation. The aim is to minimize the dif-
ference between the input and the reconstructed states. This
is typically achieved by optimizing the parameters of the
encoding and decoding circuits using a cost function, often
the fidelity between the original and reconstructed states.
Several different methods have been explored. A single
unitary can be used that acts as both encoder and decoder.
The unitary evolves an input state |ϕ⟩to a latent state |χ⟩
using an encoder circuit U(θ), and learns to reconstruct this
state using its Hermitian conjugate |ϕ⟩= U †(θ) |χ⟩[166].
Alternatively, two unitaries can be learned with individual
parameterizations, each acting as the encoder and decoder
respectively [167]. The size of the latent dimension can be
fixed by discarding intermediate qubits that would feed into
the decoder circuit.
4.3.7
Quantum Generative Adversarial Networks
Quantum Generative Adversarial Networks (QGANs) [168],
[169] are composed of two main quantum circuits, the
generator and the discriminator. The generator is a VQC
controlled by a set of parameters ΘG, and is responsible for
transforming random quantum noise into quantum states
that resemble the real data distribution. The discriminator
is another VQC controlled by parameters ΘD tasked with
distinguishing between real quantum states from the target
distribution and the fake ones generated by the generator. In
the hybrid quantum-classical setting, the discriminator is a
classical neural network [170]. During the training process,
the objective is to simultaneously train the generator to
produce indistinguishable states from the real data and the
discriminator to efficiently differentiate between the real
and generated states. The loss function is typically designed
as a two-player min-max game, and the parameters are
iteratively updated through gradient-based optimization
methods, aiming to find the equilibrium of this adversarial
game.
4.3.8
Quantum Natural Language Modeling
Quantum natural language modeling has scarcely left the
theoretical realm, with descriptions of prospective frame-
works being offered in the literature. Research has fo-
cused on the distributional-compositional-categorical model
of meaning (DisCoCat) [171], which combines linguistic
meaning and structure into a single model via via tensor
product composition. Subsequent works show that when
semantic representations are modeled in this way can be
interpreted in terms of quantum processes, over which
quantum computation can readily handle the resulting high
dimensional tensor product spaces [172], [173], [174]. Var-
ious ansatze and encoding schemes have been suggested
for NLP computation over quantum hardware; a few works
have implemented these ideas on classical simulators.
Kartsaklis et al. [175] developed the lambeq Python li-
brary that allows for the conversion of sentences into quan-
tum circuits, providing the tools for implementing exper-
imental quantum NLP pipelines, following the methodol-
ogy in [174] who first present a quantum pipeline for the
DisCoCat methodology. In brief, the pipeline initiates with
the generation of a syntax tree from a sentence using a
statistical Combinatory Categorial Grammar (CCG) parser,
delineating the sentence’s grammatical structure. This tree
is then translated into a string diagram, refined using
rewriting rules to streamline the computation process, po-
tentially omitting redundant word interactions. Finally, the
adjusted diagram is transformed into a concrete quantum
circuit or tensor network ansatz, trained via standard ML
optimisation backends such as PyTorch and JAX. Experi-
ments using the Qiskit Aer cloud quantum simulator were
performed using a simple binary meaning classification
dataset of 130 sentences created using a simple context-
free grammar. When compared against a classical pipeline,
where the sentences are encoded as tensor networks, similar
testing accuracies were achiever, albeit with fluctuation and
instability at the early stages of training. These results were
corroborated by Lorenz et al. [176], where practical simula-
tions of the DisCoCat compositional model were compared
against quantum-friendly versions of the word-sequence
and bag-of-words models. The latter methods being rep-
resented by simple tensor compositions of semantic bases.
The compositional model showed superior results on both
classical and quantum hardware.
Li et al. [177] propose a Quantum Self-Attention Neural
Network (QSANN) for text classification, noting that the
DisCoCat compositional model requires heavy syntactic

26
pre-processing and a syntax-dependent network architec-
ture, limiting its scalability to larger datasets. The self-
attention mechanism that has seen large success in classical
NLP is introduced into the quantum setting; the key com-
ponent vectors of classical self-attention: queries, keys and
values are modelled and trained using quantum ansatze,
with an additional projection onto 1D space and Gaus-
sian function applied to handle long distance correlations
induced by inner-product self-attention. Numerical results
against both classical self-attention neural networks and the
DisCoCat model [174], and showed superior results to both
in terms of predictive performance while requiring fewer
parameters.
4.4
Other QiML Methods
Works in this section do not necessarily fall into the afore-
mentioned categories, but are more intrinsic to early def-
initions of QiML — methods that incorporate and adapt
quantum phenomena in classical settings.
4.4.1
Quantum-Inspired Nearest Mean Classifiers
A line of work in QiML research, commenced by Sergioli
et al. [178] has explored a so-called “new approach” to
QiML [179] which explores supervised binary classification
using quantum concepts. In [178], the authors developed
a quantum-inspired version of the nearest mean classifier
(QNMC). The NMC problem finds an average ‘centroid’ ui
for each class Ci in the training dataset:
ui = 1
ni
X
x∈Ci
x,
(46)
where ni is the number of data points belonging to
the class Ci. New instances are assigned labels based on
proximity to these centroids, using some distance metric.
The introduction of the quantum-inspired NMC (QNMC)
stems from the observation that any real, two-feature pat-
tern x corresponds exactly with some quantum pure density
operator ρ, obtained by the stereographic projection of x
onto the Bloch Sphere representation. From these encodings,
the quantum centroid is defined as
ρQC = 1
n
n
X
i=1
ρi.
(47)
The normalized trace is used as the distance metric due
to its ability to preserve the order of distances between arbi-
trary density patterns. The authors show that the QNMC
potentially outperforms the classical NMC on synthetic,
non-linear data, particularly on datasets with high data
dispersion or mixed class distributions. Improvements to
this model have come from subsequent works. In [180],
the correspondence between real and quantum objects is
extended to an arbitrary n feature patterns, allowing for
experimentation on more complex, real-world datasets. The
QNMC again shows considerably enhanced performance
over classical NMC. In [181], Helstrom’s distance is used
instead, producing a model coined as the Helstrom Quan-
tum Classifier (HQC). This allowed for the use of multiple
copies of quantum states, bolstering their informational
content, leading to empirically enhanced performance over
benchmark tasks. This model was extended to the multi-
class context in [182] by leveraging the pretty-good measure-
ment (PGM) measurement technique from quantum state
discrimination; a minimum-error discrimination that dis-
cerns between multiple unknown quantum states with high
success probabilities. These models have also seen success
in practical implementation over biomedical contexts [183],
[184]. Leporini and Pastorello [185] consider a geometric
construction of the classifier, and discusses a method to
encode real feature vectors into the amplitudes of pure
quantum states using Bloch vectors. Bloch vectors represent
the density operators, and the centroids of data classes are
directly calculated based on these vectors. The obtained
Bloch vector is rescaled into a real sphere to identify the
centroid as a proper density operator, since the mean of
a set of Bloch vectors is not typically a Bloch vector. This
representation allows for data compression by eliminating
null and repeated components, allows for the implemen-
tation of feature maps while saving space and time re-
sources without compromising performance. This method
showed similar, and sometimes improved accuracies over
benchmark datasets, when compared with similar proposed
classifiers. Bertini et al. [186] later propose a KNN version
of the Bloch vector-based classifier by executing the method
over a local neighborhood of training data points.
4.4.2
Density Matrix-Based Feature Representation
A central source of inspiration for QiML is derived from the
probabilistic interpretation of quantum mechanics, known
as the quantum probabilistic framework. Unlike classi-
cal probability, quantum probability encompasses complex-
valued probability amplitudes and allows for phenomena
such as superposition and entanglement.
The quantum probabilistic framework introduces math-
ematical structures that can be applied to classical machine
learning, specifically through the use of density matrices
and Hilbert spaces. A density matrix, represented as ρ,
describes the statistical state of a quantum system, allowing
for a mixture of pure states, and can be expressed as:
ρ =
X
i
pi |ψi⟩⟨ψi| ,
(48)
where |ψi⟩are the pure states of the system, and pi are
the classical probabilities for each state.
In the context of machine learning, this framework can
be used to represent complex relationships and dependen-
cies within data. The density matrix can reflect the covari-
ance among different embedding dimensions, representing
how scattered words are in the embedded space [187].
Quantum entanglement can also be leveraged to describe
intricate correlations between features, providing a more
expressive model [188].
The use of density matrices is prevalent in quantum-
inspired NLP methods, used to capture probabilities and
semantic subspaces of the individual words in the sentence
[189], [190]. Once classical data is encoded into quantum
states, the density matrix representation of these states can
be computed via several methods. One method is to com-
pute ρ directly, where a sentence or document corresponds

27
to a mixed state represented by a density matrix of individ-
ual semantic spaces [188], [191]:
ρ =
X
i
pi |wi⟩⟨wi| ,
(49)
where pi is the relative importance of word wi within
the sentence, satisfying P
i pi = 1. The assigned pi can
be captured in various ways, such as uniformly [190], by
number of occurrences [191], or by a softmax function [192].
This representation extends to multi-modal settings by con-
sidering non-textual media (images, videos) as “textual”
features. For instance, the SIFT algorithm [193] can be used
to detect and describe local features in images as vectors,
which can be clustered, with cluster centers considered
as “visual word” vectors [189] and used directly in the
calculation of the density matrix to produce a compositional
Hilbert Space [188], [194]:
ρ =
X
i
pi(|wm
i ⟩⊗· · · ⊗|wM
i ⟩)(⟨wm
i | ⊗· · · ⊗⟨wM
i |)
(50)
=
X
i
pi(ρm
i ⊗· · · ⊗ρM
i ),
(51)
This method assumes the importances pi are accurately
known or can be reliably computed. An alternative method
is to construct projectors in semantic space from word
features, from which ρ can be learned [189], [190]. Each
projector can be described as:
Πi = |wi⟩⟨wi| ,
(52)
where Πi describes the semantic space of a normalized
word vector |wi⟩. Such vectors may be obtained via various
real-valued embedding schemes, such as by GloVe [195] or
BERT [196] word embeddings. A document is then consid-
ered as a sequence of projectors, P = {Π1, Π2, . . . , Πn},
where n is the number of terms in the document. A ran-
domly initialised density matrix is trained and iteratively
updated based on this sequence of projectors via a globally
convergent Maximum Likelihood Estimation algorithm un-
til a convergence threshold is reached, using the objective
function:
F(ρ) ≡max
ρ
X
i
log (tr (Πiρ)) ,
(53)
s.t.
tr(ρ) = 1,
(54)
ρ ≥0.
(55)
The final density matrix encapsulates the semantic de-
pendencies and distributional information of the terms in
the document. In multi-modal settings, image and video
feature vectors are extracted, which which projectors can
be constructed via Equation 52 and included in the set P. A
multi-modal fusion method inspired by quantum interfer-
ence has been proposed [189], [191]. Each mode has its own
classifier that takes in only input density matrices for that
mode. Then, the individual inferences on a document with
M modes produces M predictions, thus the overall senti-
ment of the document could be uncertain. This sentiment
can be modeled as a quantum wavefunction, analogized
as a combination of the modal sentiment components. For
example, the combination of the sentiment of a document
containing text and the image components is described by:
ψ(xi) = αψ(xtext
i
) + βψ(ximage
i
).
(56)
Thus the overall sentiment score can be represented by a
probability distribution, measured as:
P(xi) = α2Pt + β2Pi + I
(57)
I = 2αβ
p
PtPi cos θ
(58)
such that P(xi) = |ψ(xi)|2 describes the probability of the
document’s sentiment score. Pt = |ψ(xtext
i
)|2 and Pi =
|ψ(ximage
i
)|2 are the probabilities governing the sentiment
scores of the text and image respectively. α, β and θ are
learnable parameters. The interference term I reflects the
degree of conflict in local decisions; if both modalities agree
in sentiment, there is a constructive interference, resulting
in a strongly positive or negative sentiment.
In question and answering tasks, the joint representation
for a question-answer pair can be viewed as their multi-
plicative interaction [187]:
ρQA = ρQρA.
(59)
The spectral decomposition of ρQA exposes the joint
eigenspaces, overwhich the trace inner product reveals the
similarity between the question and answer. From this, [187]
propose two methods of constructing the feature set for each
document. The first uses the trace and diagonal elements of
ρQA: [tr(ρQρA); diag(ρQρA)], where the former captures the
semantic overlaps between the question-answer pair, and
the latter accounts for the varying degrees of importance
for similarity measurement. The second uses 2D convo-
lutions to scan the density matrices, resulting in feature
maps which are processed using row-wise and column-wise
max-pooling to generate feature vectors, aiming to capture
a more nuanced and complex understanding of similarity
between question and answer pairs.
Learning and Classification: Several classical ML tech-
niques have been used to learn a performant parameteriza-
tion over the density matrix features, including SVMs and
Random Forests [190], as well as deep learning structures
such as RNNs [191], [197], and CNNs [187]. Back propaga-
tion is often used as the optimization method.
In addition, quantum measurement-based procedures
can also be used to extract predictions. Once the set of
states {ρt
c} is obtained that represent the data, a global
observable O is introduced, uniquely represented by a set of
eigenvalues λ and corresponding eigenstates |e⟩, expressed
as O = P
i λi |ei⟩⟨ei|. These eigenvalues and eigenstates
correspond to some outcome representation of interest, such
as sentiment-related aspects [188] or emotional states [194].
The measurement process leads to a collapse of the state
onto one of the eigenstates, and a probability distribution
over the eigenstates is calculated as pi = ⟨ei| ρt
c |ei⟩, where
ρt
c is the state at time t. This probability distribution can then
be used to perform predictions over the data.
Complex-Valued Density Matrix Features: Recently, re-
searchers have explored the effect of using complex-valued

28
word embeddings in tandem with considering local word
correlations, noting in using only real-valued vectors, the
full probabilistic properties of a density matrices and their
complex formulations are ignored [192].
The formulation of words via Equation 19 is preserved,
except with the inclusion of complex components. This
complex-valued approach is inspired by the representation
of words as a superposition of semantic units, where each
word |w⟩is defined as a unit-length vector on H:
|w⟩=
n
X
j=1
rjeiϕj|ej⟩,
(60)
where i is the imaginary unit, and the rj and ϕj are non-
negative real-valued amplitudes and corresponding com-
plex phases, satisfying Pn
j=1 r2
j = 1, and ϕj ∈[−π, π]
respectively. The inclusion of complex components in the
word embeddings allows for a richer representation of
semantic information by leveraging both the magnitude and
phase of complex numbers. The magnitudes rj describe the
importance of each semantic base in the composition of the
word, while the phases ϕj capture the subtle interrelations
between different semantic units. This leads to a more
expressive and nuanced modeling of word semantics [192].
Complex-valued representations have also been used
over multi-modal tasks. In emotion recognition, Li et al.
[194] consider each utterance is as a mixture of unimodal
states whose features are recast as pure states creating a
multimodal mixed state representation. Then, a procedure
inspired by quantum evolution is employed to track the
dynamics of emotional states in a conversation. A quantum-
like recurrent neural network tracks the evolving emotional
states during a conversation, considering the uncertainties
in the conversational context and efficiently memorizing the
context information due to unitary transformation, which
ensures zero information loss. The ”measurement and col-
lapse” phase introduces a global observable to measure the
emotional state of each utterance, calculating a probabil-
ity distribution that corresponds to the likelihood of the
state collapsing onto specific eigenstates. The result is then
mapped to emotion labels using a neural network with a
single hidden layer.
Shi et al. [197] similarly propose complex-valued word
embeddings which likens words to quantum particles exist-
ing in multiple states, representing polysemy. The method
corresponds a word with multiple meanings to a quan-
tum particle that can exist in several states. Additionally,
sentences are likened to quantum systems where these
particles (or words) interact or interfere with each other,
just as quantum particles can interact in a quantum system.
Complex-valued word embeddings can be formed from
amplitude word vectors and phase vectors, capturing rich
semantic and positional information with greater alignment
with quantum concepts. These embeddings are used in text
classification models utilizing GRU gated recurrent units
and self-attentive layers to extract more semantic features.
An extended model is also presented that applies a convo-
lutional layer on the projected word embeddings matrix to
capture local textual features. This is inspired by the quan-
tum theory concept of ‘entanglement’, where the state of one
particle is connected to the state of another, no matter the
distance between them. In a similar vein, the convolutional
layer captures dependencies between different parts of the
text, or ‘local features’, that might otherwise be missed.
This quantum probabilistic formulation of density ma-
trices not only serves as an effective representation for sen-
tences or documents in NLP tasks but also offers a versatile
framework that may extend to other contexts where data
can be captured as probabilistic events.
4.4.3
Quantum Formalisms Applied to Neural Networks
Exploring neural network representations through the lens
of quantum mechanics has been a long explored topic.
Such methods aim to improve the robustness of classical
neural networks by formulating quantum-based activation
operators or utilizing quantum feature spaces [198], [199],
[200]. In this subsection we present a few recent, practical
works in this area.
Patel et al. [201] introduce the Quantum-inspired Fuzzy
based Neural Network (Q-FNN), a three-layer neural net-
work that employs Fuzzy c-Means (FCM) clustering to fine-
tune connection weights and decide the number of neurons
in the hidden layer. The fuzziness parameter m, which man-
ages the overlap among samples from different classes, takes
on a qubit representation, which enlarges the search space
for the selection of an appropriate fuzziness parameter. The
final cluster centroids, found after numerous iterations of
fuzzy clustering, serve as the final connection weights of
the hidden layer. This model has been proven effective in
dealing with two-class classification problems.
Sagheer et al. [202] replace the classical perceptron
within the neural network model with a quantum-inspired
version: the autonomous perceptron model (APM). In the
APM, a feature vector x ∈Rn is replaced by a quantum
state vector |ψ⟩∈Cn which can be represented as a complex
linear combination of the basis vectors in the n-dimensional
complex vector space. Instead of real-valued weights used
in a classical perceptron, quantum weights are introduced as
normalized complex numbers ωi = exp(iθi) where θi is the
phase of the i-th weight. The activation function is replaced
by a measurement operation, which projects the state of the
system onto one of the basis vectors. The output of the APM
is the expectation value of this measurement, which can be
calculated as ⟨ψ|M|ψ⟩, where M is the Pauli-Z measure-
ment operator in the computational basis. The measurement
outcome is then compared with a threshold value to make
binary decisions, akin to classical perceptrons. Results over
the UC Irvine (UCI) Machine Learning Repository bench-
mark classification datasets [203] showed the APM-based
model outperformed 15 standard classifiers models when
subjected to the same experimental conditions.
Konar et al. [204] developed the quantum-inspired self-
supervised network (QIS-Net) architecture for the automatic
segmentation of brain magnetic resonance (MR) images. The
model is composed of layers of classically implemented
quantum neurons arranged. Each neuron is depicted as a
qubit using matrix notation. The intra-connection weights
among neurons within the same layer are set to π/2, to emu-
late a quantum state. The input layer deals with qubit infor-
mation from connected neighborhood subsets of each seed
neuron, which is then accumulated at the central neuron of
the intermediate layer through interconnections. Image data

29
by feeding image pixels to the input layer as quantum bits,
which then propagate to the intermediate and output layers,
which are updated through rotation gates determined by
the relative quantum fuzzy measures of pixel intensity at
the constituent quantum neurons between layers. A novel
quantum-inspired multi-level sigmoidal (QMSig) activation
function is integrated into the QIS-Net model to handle the
complexity of multi-intensity grayscale values in brain MR
images.
QMSig =
1
λω + e−ν(x−η) .
(61)
The function adjusts its activation based on qubits,
improving the model’s accuracy in segmenting complex
images. When tested on Dynamic Susceptibility Contrast
(DSC) brain MR images for tumor detection, QIS-Net
demonstrated superior performance compared to classical
self-supervised network models commonly used in MR
image segmentation, whilst requiring less computational
overhead with respect to time and resources.
Zhang et al. [205] introduce a method that leverages
quantum entanglement to calculate joint probabilities be-
tween features and labels. The maximally entangled Bell
state system of two qubits |Φ+⟩is defined, where one qubit
is described as the feature and the other as the label. The ob-
servables and measurement operators of the entangled sys-
tem are defined with specific spectral decompositions. The
positive and negative measurement operators for the entan-
gled system consists of the n-th attribute and the label, and
are given by M±
n (θn, ϕn) = P +
n (θn, ϕn) ⊗P ±
l , where polar
and azimuth angles θn and ϕn, are the arbitrary real param-
eters. By applying these operators to the entangled system,
probability values for both positive and negative examples
are obtained: p±
n (θn, ϕn) = ⟨|Φ+⟩|M±
n (θn, ϕn)| |Φ+⟩⟩. This
formalism enables the calculation of quantum joint proba-
bilities, and is integrated into a classical MLP by replacing
hidden layer neurons in the MLP with the measurement
process. Model optimization uses the cross-entropy loss
function with the Adam optimizer to ensure smooth param-
eter changes.
4.4.4
Miscellaneous Quantum Mechanics-Based Classi-
fiers
Various other classification methods have also been pro-
posed that incorporate quantum phenomena into their clas-
sical learning model. In the following works, the main
concepts of encoding classical data into some quantum
state representation, and discrimination of those states via
a measurement process are highlighted.
Tiwari and Melucci [206] explore the prospect of classi-
fication inspired by quantum signal detection theory, which
aims to decide between two different hypotheses — the
presence or absence of a signal. A codification process
converts the signal into a particle state, then measured,
much like the classical signal detection framework. When
considered in a classification context, the two hypotheses
subjected to decision become two class labels, represented
by distinct density operators derived from data features,
and characterize the system state associated with each class.
Outcomes are decided via projections corresponding to the
density operators. Using this knowledge, the authors devise
a binary classifier over vectorized documents based on
frequency of distinct elements. Density operators for each
class are estimated using training samples:
ρc =
|vc⟩⟨vc|
tr(|vc⟩⟨vc|),
(62)
for each class c ∈{0, 1}, from which an optimal projection
operator can be calculated:
Λ =
X
l:el≥0
|el⟩⟨el|
(63)
from eigenstates el of the operator ρ1 −λρ0. A test sample is
then classified based on the result of a projection operation
involving the sample’s feature vector and the projection
operator. If the result is greater or equal to 0.5, the sample
is assigned to the class, otherwise, it is not. The model has
been tested over image and textual datasets [207], showing
superior performance in recall, and comparable precision
and F-measures across varying feature ranges compared to
baseline models.
Zhang et al. [208] proposed a novel method for data
classification using principles of quantum open system the-
ory, termed the Interaction-based Quantum Classifier (IQC)
that models the classification process as a quantum system’s
evolution. The interaction between the target system (qubit)
and the environment (input data) is characterized by the
Hamiltonian Hint = −˜gσQ ⊗σE, where ˜g is the coupling
constant whose magnitude reflects the strength of the inter-
action, leading to the unitary evolution U(τ) = eiσQ⊗σE(τ),
where σQ = σx + σy + σz. Both systems are initialized as
equal probability superpositions, and the composite system
evolves according to the unitary operator. Measurement of
the evolved state determines probabilities used for clas-
sification. The two-category classification task is defined
by a unitary operator involving input and weight vectors,
U(xi) = eiσQ⊗σE(xi), and a gradient descent update rule
for the weights, wi = wi −η(zi −yi)(1 −p1
2)xi, is applied
to optimize the classification.
5
QIML IN PRACTICE
In this section, we delve into the practical applications of
Quantum-inspired Machine Learning, showcasing where
these techniques have been employed and evaluated empiri-
cally. We present an exploration of several sectors, including
medical, financial, physics, and more, detailing how QiML
has been utilised, and discuss these in terms of the QiML
methods presented (dequantized algorithms, TNs, QVAS,
and others). To aid this discussion, we present a compilation
of relevant works in Table 2, offering a quick reference
to the practical applications of QiML. Each perspective is
treated as a subsection, allowing readers the flexibility to
navigate the section that resonates with their interest, be it
an understanding of QiML methods or their domain-specific
applications.

30
TABLE 2
Practical Application Domains of QiML: A summary of works that report experimental results and their availability of source code (* indicates
source code availability).
Dequantized
Algorithms
TNs
QVAS
Other QiML Methods
Image Modeling
Classification
-
[92]*, [111]*, [88], [99]*,
[100]*, [101], [115]
[155], [163], [164], [39]
[206], [209]
Generative Modeling
-
[104], [110], [116], [124]*
-
-
Natural Language Processing
Language Modeling
-
[95], [96]*
-
-
Sentiment Analysis
-
-
-
[189], [190], [188], [191],
Question-Answering
-
[94]
-
[187], [192]
Text Classification
-
-
[175]*, [177], [176]*
[207], [197]
Emotion Recognition
-
-
-
[194]*
Medical
Disease Classification
-
[125]*, [210]*
[211], [212]
[183], [184]*
Image Segmentation
-
[126]*
-
[204]
Finance
Options Pricing
-
[122]
[213]*
-
Portfolio Optimization
[214]*
[215]
-
-
Time-Series Forecasting
-
-
[157]
-
Physics Modeling
Event Classification
& Reconstruction
-
[102]*
[158], [160], [216], [217],
[159], [218]
-
Cybersecurity
Attack Detection
-
-
[156], [219], [152]*, [153]
-
Intrusion Detection
-
-
[220]
-
Fraud Detection
-
-
[221]
-
Automatic Speech
Recognition
-
-
[222]*
-
Other Tasks and Applications
Generic Classification
[214]*, [60]*, [47]
[97]*
[155], [151]
[178], [180], [181], [182],
[185]*, [186]*, [208], [205],
[201], [202]
Recommendation Systems
[214]* [47]
[97]*
-
-
Bit-string Classification
-
[106]*, [107]*
-
-
Generic PDE Solvers
-
[223]*
-
-
Generic Anomaly Detection
-
[123]
-
-

31
5.1
Image Modeling
Both image classification and generative image modeling
have seen a wealth of implementation using tensor net-
work methods. Particularly, the MNIST and Fashion-MNIST
datasets have provided a relatively simple, low dimensional
test bed for the development of TN methodologies. Ta-
bles 3 and 4 outline these numerical results and presents
key improvements in TN performance over the benchmark
datasets. The current, classical, state-of-the-art benchmark
for the given task is also included, indicated by the asterisk
(*). The study conducted by Han et al. [104] is not featured in
Table 4 due to the lack of experimental results from training
on the full MNIST dataset. In general, TN learning mod-
els have shown competitive, but not superior performance
when compared to classical benchmarks.
TABLE 3
Supervised Tensor Network Performance, compared with Classical
Benchmarks (the classical benchmark is indicated by *)
Task
Method
Test
Acc.
Optimization
MNIST
MPS [92]
99.03%
DMRG
MPS + TTN [88]
98.11%
DMRG
TTN [111]
95%
MERA
MPS [99]
98%
SGD+Adam
GMPSC [100]
98.2%
SGD+Adam
PEPS [115]
99.31%
SGD+Adam
CNN-
Snake-SBS
[101]
99%
SGD
Ensemble CNN* [224]
99.91%
-
Fashion-
MNIST
MPS [99]
88%
GD
MPS + TTN [88]
88.97%
DMRG
CNN-PEPS [115]
91.2%
SGD+Adam
CNN-
Snake-SBS
[101]
92.3%
SGD
Fine-Tuned
DARTS*
[225]
96.91%
-
Similarly, variational quantum algorithms have used
image datasets to test various introduced methods [155],
[163], [164], and explore their capabilities [39]. In particular,
DQCNN [163] and Quanvolutional Neural Networks [164]
have been introduced that incorporate quantum filters and
operations that mirror classical convolution techniques, and
have shown enhanced performance over classical methods
with comparable architectures.
Other methods in QiML have shown promise in image
classification tasks [206], [209]. The extra flexibility provided
by superposition is cited to contribute to better decision-
TABLE 4
Unsupervised Tensor Network Performance, compared with Classical
Benchmarks (the classical benchmark is indicated by *)
Task
Method
Test
NLL
Optimization
Binarized-
MNIST
MPS [110]
101.5
DMRG
TTN 1D [110]
96.9
DMRG
TTN 2D [110]
94.3
DMRG
PEPS (D = 4) [116]
91.2
SGD+Adam
AMPS [124]
84.1
GD
Deep-AMPS [124]
81.8
GD
CR-NVAE [226]
76.93
-
making in these tasks [206]. Applications to real-world data
is also seen in medical imaging [184].
Further applications of such methods in this domain
may be expected in future.
5.2
Natural Language Processing
Numerous natural language processing (NLP) tasks have
been explored in QiML literature, including sentiment
analysis, question-answering, and text classification. These
methods typically extract word embeddings and project
them into higher dimensional space, before producing ei-
ther a tensorial representation by the summation of basis
states describing individual semantic elements in the full
vocabulary [94], [95] or a full density matrix feature repre-
sentation [189], [190], [207]. Success is seen in performances
over high dimensional word embeddings, with GloVe em-
beddings used at a 100-dimensional level to semantic suf-
ficiency, although computation complexity concerns have
been cited [189], [190], [197]. The dimensions of complex
valued word embeddings has varied where used [188],
[192], [194], scaling with the vocabulary size. Where noted,
the computational time of the QiML method is typically
much longer than traditional methods [189], [190], where the
discrepancy is due to the matrix representation of samples.
It is not clear which input preparation methods are best in
representing natural language. Several works have utilized
density matrices, or have offered physical interpretations
for complex-valued embedding schemes, where different
vector components are argued to encode low and high
level semantic aspects [192]. Further research may inquire
into the transferability or suitability of methods to different
language tasks.
Concerning quantum variational methods, quantum
NLP remains a largely theoretical field; advancements cen-
tered around the distributional-compositional-categorical
model of meaning (DisCoCat) which integrates linguistic se-
mantics and structure through tensor product composition
[173]. Research indicates that when semantic interpretations
are framed in this manner, quantum processes can man-

32
age the resulting high-dimensional tensor product spaces.
Experimental results on this approach, using quantum
simulators, align with classical tensor network outcomes,
emphasizing the potential of quantum methods in NLP.
Outside of the DisCoCat model, efforts to enhance classical
NLP architectures using quantum components have seen
the implementation of the Quantum Self-Attention Neural
Network (QSANN) model [177]. In general, while quantum
variational methods in NLP are still in nascent stages,
there is a growing interest and understanding that they can
offer computational benefits and efficiency, particularly in
handling high-dimensional spaces and potentially reducing
model parameters.
5.3
Medical
Quantum Nearest Mean Classifier models have shown some
promising results [183], [184] in the medical field. Models
like the HQC [184] have been successful, primarily due to
their invariance to re-scaling, with the inclusion of the free
re-scaling parameter appearing to be a key factor in their
performance. In [183], the authors emphasized the impor-
tance of incorporating qualitative features, often prevalent
in biomedical contexts but neglected in their study, sug-
gesting that more advanced modeling methods need to be
investigated. The authors also underlined the criticality of
identifying optimal encoding methods that can accurately
represent the given dataset, a challenge that persists in both
QiML and traditional ML fields.
Tensor networks have also been used in medical con-
texts, in binary classification over metastasis detection from
histopathologic scans, detection of nodules in thoracic com-
puted tomography (CT) scans [125], [210], and 3D magnetic
resonance imaging (MRI) scans [210]. The models showed
strong area-under-the-curve (AUC) performance compared
with classical baselines, while using only a fraction of the
GPU memory. However when modeling 3D images, the
approach seemed to require a high number of parameters
when compared with CNN baselines. The same was not
true for model in [126], where tensor network compression
was shown to give large savings in parameters.
VQCs have been used in medical contexts over both
image [211] and audio-based [212] datasets. Azevedo et al.
[211] propose a transfer learning approach, where classi-
cal networks pretrained on ImageNet are used to extract
features for a quantum circuit, the DressedQuantumNet,
which performs the final classification. This circuit, attached
to the final linear layer of a pretrained model (Resnet18),
takes 512 real values, and performs an angle encoding to
construct quantum states, before being passed through to
several variational layers. Notably, the quantum classifier is
the only trainable part of the network, with weights updated
during training via techniques like cross-entropy loss and
the Adam optimizer. The model achieved an accuracy of
84%, outperforming the classical standalone ResNet model,
which achieved a maximum accuracy of 67%. Esposito et
al. [212] applied the Quanvolutional Neural Network to
detect COVID-19 from cough audio using DiCOVA and
COUGHVID datasets. They integrated quantum circuits
as quanvolutional layers into Recurrent and Convolutional
Neural Networks (RNN, CNN) using the PennyLane library,
with feature extraction via two- and four-qubit quantum
circuits. Test accuracies for classical RNN and CNN were
79.4% and 73.0% respectively, while QNNs achieved 74.6%-
78.8% with no noise. The results are thus comparable to
classical methods, however the quantum simulations were
observed to necessitate extended training duration.
5.4
Finance
Finance modeling has seen the implementation of QiML
chiefly in portfolio optimization. The implementation in
[214] serves as a proof of concept for dequantized matrix
inversion on large datasets with intrinsic large-scale matrix
calculations, with the goal of identifying practical bottle-
necks, rather than achieving high performance.
Tensor network structures used here have shown effec-
tiveness in optimization. In [223], a classically simulated
quantum register encoded via an MPS is proposed for multi-
variate calculus computation, capitalizing on the low entan-
glement between states for smooth functions with bounded
derivatives. This representation enables the efficient stor-
age of an exponential amount of weights and proves the-
oretically amenable to operations such as Fourier analy-
sis, derivatives approximation, and interpolation methods.
Mugel et al. [215] implemented an MPS for dynamic port-
folio optimization, which showed impressive performance
when compared to D-wave hybrid-quantum annealing and
quantum variational circuits in terms of Sharpe ratios and
ability to achieve global minimums reliably. The method,
however, suffers greatly in computation time when com-
pared with quantum methods. Patel et. al [122] integrate
MPO structures into neural network layers to reduce the
number of model parameters. The authors show the con-
sequently leading to the model size reduction and is also
shown to lead to faster convergence in some cases. This
network compression method was able to well approximate
the original weight matrices with many fewer parameters,
whilst exhibiting minimal loss in performance. However,
in the presented works, the nature of the dataset used for
experiments is either random, citing difficulties in scaling to
large, real-world datasets [215], or not exposed or elucidated
[122].
Emmanoulopoulos and Dimoska [157] note that VQCs
could match the performance of long short term memory
(LSTM) models over time-series forecasting, even exhibiting
slight superiority with high noise coefficients data due to the
alignment of trigonometric functions in quantum circuits
with the nature of time series signals. However, the au-
thors acknowledged that the practical application of VQCs
are currently constrained by their inability to handle large
datasets.
5.5
Physics
QiML has seen wide use in high-energy physics (HEP) ap-
plications. A common task is discriminating between signal
and background events in the context of the Standard Model
of physics.
Araz and Spannowsky [102] utilized MPS tensor net-
works for top versus quantum chromodynamics (QCD) jet
discrimination in physics modeling using a combined SGD
and DMRG optimization method; applying DMRG in the

33
first batch of each epoch and SGD thereafter. Despite slightly
weaker performance compared to CNN models, the MPS
method provided a higher degree of interpretability.
Variational quantum methods have also seen such use.
Terashi et al. [158] applied variational quantum algorithms
for signal event classification in HEP data analysis using su-
persymmetry. Two implementations were tested: one using
RY and RZ gates with an Ising model Hamiltonian, and the
other using Hadamard and RZ gates with Hadamard and
CNOT for entanglement. The study compared these meth-
ods against traditional Boosted Decision Tree (BDT) and
DNN algorithms, finding comparable discriminating power
for small training sets (10,000 events or fewer). Simulations
were run on Qulacs and IBMQ QASM simulators. Resource
demand for quantum simulation was high and increased
exponentially with the number of variables used, making
extended iterations impractical.
For data analysis of t¯tH (Higgs coupling to top quark
pairs), both the quantum variational classifier [216] and the
quantum kernel estimator [217] methods were employed
using IBM quantum simulators. Results on the quantum
simulators using 10-20 qubits show that these quantum
machine learning methods perform comparably to SVM and
BDT classical algorithms, with both achieving reasonable
AUC scores, indicating good classification performance.
These results maintained over various quantum simulators,
including Google Quantum [227], IBM Quantum [228], and
Amazon Braket [229].
In Gianelle et al. [159], VQCs were used for β-jet charge
identification over Large Hadron Collider data. Both am-
plitude and angle encoding schemes were assessed, along-
side classical deep neural networks (DNNs). Results found
DNNs to slightly outperform angle encoding VQCs, be-
ing compatible within a 2σ range, suggesting similar per-
formance levels. Amplitude encoding VQCs consistently
under-performed in comparison with angle encoding, but
generally took less time to train due to being less complex
in layer depth. The authors note that the number of layers
is a parameter to be optimised, and show that increasing
layer depth did not necessarily result in improved perfor-
mance. The study also highlighted the resilience of quantum
algorithms, with the angle embedding model maintaining
efficacy with fewer training events, a potential advantage
over classical ML methods. However, increases in model
complexity and training time present challenges, with accu-
racy improvements saturating beyond five layers and longer
training times for quantum models. The DNN also showed
superior performance when a large number of features is
employed.
In Ngairangbam et al. [218], a quantum autoencoder
(QAE) is used for the task of distinguishing signal events
from background events, following an anomaly detection
approach. Approximately 30,000 background and 15,000
signal events are generated. Anomaly detection then con-
siders that the compression and subsequent reconstruction
of data will work poorly on data with different characteris-
tics to the background. Performance is compared against a
classical autoencoder network (CAE); the QAE maintained
higher classification performance than the CAE across a
range of latent dimensions. Quantum gradient descent is
used for faster convergence in optimization; the study finds
that using this method allows the QAE to efficiently learn
from as little as ten sample events, demonstrating the model
is much less dependant on the number of training samples.
This suggests that QAEs show better learning capabilities
from small data samples compared to CAEs, particularly
relevant to Large Hadron Collider searches where the back-
ground cross section is small. The authors hypothesize this
could be due to the uncertainty of quantum measurements
enhancing statistics and the relatively simple circuits em-
ployed in QAEs.
Preliminary findings suggest that quantum approaches
can achieve comparable results to classical algorithms, es-
pecially for smaller training sets [158], [218]. Despite these
promising outcomes, challenges such as increased train-
ing time and model complexity remain. Overall, quantum
variational methods offer promising avenues for HEP data
analysis, but their scalability and efficiency in comparison
to classical techniques need further exploration.
5.6
Cybersecurity
Quantum variational methods have been employed across a
broad range of cybersecurity applications.
Payares and Martinez-Santos [152] achieved similar re-
sults in detecting distributed denial of service (DDoS) at-
tacks. The QSVM achieved high accuracy in binary classifi-
cation (99.6%), but the authors note its substantial computa-
tional demand.
Masun et al. [153] evaluated the performance of QSVM
for malware detection and source code vulnerability anal-
ysis, utilizing the ClaMP and Reveal datasets respectively.
The QSVM showed relatively comparable performance
compared to classical SVMs over both tasks, however ex-
hibited significantly extended execution times.
Suryotrisongko and Musashi [219] investigated the effect
of adding a quantum circuit as a hidden layer in a classical
neural network for domain generation algorithms (DGA)-
based botnet detection. The classical model employs a stan-
dard deep learning architecture with 2 hidden layers (dense-
dropout-dense), with the quantum layer inserted between
the dense layers after dropout. Six combinations of ansatze
were evaluated, using various embedding and entangling
strategies made available by the Pennylane software frame-
work. No single combination seemed to outperform all
others across all settings, suggesting that quantum circuit ar-
chitecture plays a significant role in determining the model’s
accuracy. The hybrid models performed slightly better than
their classical counterparts under certain conditions. For
instance, with the combination of Angle Embedding and
Strongly Entangling Layers, the accuracy reached 94.7% for
100 random samples. However, on average, the classical
models outperformed the hybrid models.
Herr et al. [221] explored a variant of QGANs by adopt-
ing the AnoGan [230] generative adversarial network struc-
ture, with the generative network portion replaced with
a hybrid quantum-classical neural network. Specifically, a
short state preparation layer encodes N uniform latent vari-
ables as quantum states, which are fed into a parameterized
quantum circuit of N qubits. Measurement is performed
over all qubits in the Z basis, from which the now classical
output is up-scaled via a classical dense network into a

34
higher dimensional feature space. The intuition in using the
VQC in the generator is in their ability to more efficiently
sample from distributions that are hard to sample from
classically. This is seen in experimental results over a credit
card fraud dataset; the quantum AnoGAN method showed
comparable F1 scores to variety of classical architectures
and system sizes, whilst staying robust to changes in the
dimension of the latent space.
Yang et al. [222] proposed a novel decentralized fea-
ture extraction approach for speech recognition to address
privacy-preservation issues. The framework is built upon a
quantum convolutional neural network (QCNN), consisting
of a quantum circuit encoder for feature extraction and a
recurrent neural network (RNN) based end-to-end acous-
tic model (AM). This decentralized architecture enhances
model parameter protection by first up-streaming an input
speech to a quantum computing server to extract Mel-
spectrogram feature vectors, encoding the corresponding
convolutional features using a quantum circuit algorithm
with random parameters, and then down-streaming the
encoded features to the local RNN model for the final
recognition. The authors test this approach on the Google
Speech Commands dataset, attaining an accuracy of 95.12%,
showing competitive recognition results for spoken-term
recognition when compared with classical DNN based AM
models with the same convolutional kernel size.
Other subfields within QiML have yet to see many works
in the cybersecurity domain, with the anomaly detection
work by Wang et al. [123] cited to have potential applica-
tions in fraud prevention and network security, among other
applicable domains.
5.7
Other Tasks and Applications
Concerning dequantized algorithms, in general, while many
works present the theoretical application of these methods
to various ML tasks, few provide experimental analysis
of the methods on data. Arrazola et al. [214] both imple-
mented, and performed analysis on the quantum-inspired
algorithms for linear systems [62] and recommendation
systems [44]. In implementation, the former was applied
to portfolio optimization on stocks from the S&P 500,
and latter to a dataset of movies; significantly faster run-
times were observed than what their complexity bounds
would suggest. Analysis showed that when the rank and
condition number are small, the dequantized algorithms
provided good estimates in reasonable time, even for high-
dimensional problems. However, outside this specification,
the dequantized algorithms performed poorly in terms of
both run time and estimation quality relative to direct com-
putation of the solution on practical datasets. The authors
note a threshold for improved relative performance when
the matrix size is larger than 106. Ding, Bao and Huang [60]
test their quantum-inspired LS-SVM on low-rank, and low
approximated rank synthetic data, and analysed the per-
formance against the classical counterpart LIBSVM. Their
results show that their model outperformed LIBSVM by
5% on average, and noted greater performance in low rank
settings. However, the running times for both models are
omitted. Chepurko et al. [47] similarly provided analysis
of their implemented algorithms. For the recommendation
systems task, their algorithm operates in a similar setting
to [214]. The results indicated a notable six-fold speed in-
crease, and demonstrated superior performance over direct
computation methods. However, this enhancement was ac-
companied by a slight uptick in error. Comparable outcomes
were found in their work on the ridge regression task. The
observed improvements seem to stem from a more efficient
implementation than [214], coupled with an algorithm pos-
sessing a superior asymptotic runtime. While the field has
progressed far beyond these benchmarks, and given their
inconclusive nature, the applicability of dequantized algo-
rithms to practical data remains an open question, pending
further investigation.
QiML has seen implementation over an assortment of
ML tasks, typically over benchmark generic datasets, such
as the UCI and PMLB [231] datasets. Works involving
the Quantum Nearest Mean Classifiers have used these
datasets extensively to assess the model’s capabilities, often
as validation before moving to real-world contexts (Section
5.3). In this setting, these works commonly cite improved
performance over other baseline models. The models are
also shown to be able to learn complex distribution, typi-
cally challenging for classical Nearest Mean Classifiers [180].
However, a caveat of these methods is their much longer
training and inference time compared with classical meth-
ods [179]. Further, these benchmark datasets are typically
small-sized. QMNCs have yet to see use over large-scale
data. The HQC, despite it’s prowess, admits a roadblock
to this, as increasing the number of copies of samples in-
troduces a non-negilible computation cost [181]. The works
inspired by quantum interference [208] and quantum corre-
lation [205] show promise over these benchmark datasets,
however also note computation costs to be a detriment
to the applicability of these methods. These works defer
improvements in training speed to the promise of realisable
quantum computers.
In tensor network modeling, early works with MPS sug-
gests the applicability of encoding features as polynomial
functions [97], showing success over generic classification
tasks from the UCI dataset, recommendation systems, and
synthetic data. Results show that inference time is com-
petitive with baseline classical models, however training
time suffers significantly, scaling with the chosen bond
dimension of the MPS. Over bit-string classification, the
works presented cite their primary goal to be assessing the
feasibility of their models, asserting the settings and datasets
for which tensor network models show stronger learning
capabilities than, for instance, generative neural networks
[106], [232]. Establishing performance for challenging real-
world tasks presents as potential future work.
5.8
Limiting Factors on the Use of QiML in Practice
5.8.1
Dequantized Algorithms
A few key limitations to the applicability of these algorithms
have been discussed in the literature. First, the requirements
for the input matrices are often strict. The matrices must be
of low stable rank [44], [57], have a small condition number
[65], or be relatively sparse [48]. These requirements are
generally not conducive to the needs of real-world datasets,

35
though these conditions have progressively become more
lax with advancements in the field.
Secondly, the need for an input model that provides
SQ access may not be readily amenable to current ML
implementations. Performing the necessary pre-processing
for adapting datasets to this structure may be reasonably
assumed to be expensive and detrimental to computational
efficiency, limiting the applicability of the these algorithms
to existing systems [43].
Thirdly, many QML algorithms (and hence, often, the
resulting dequantized algorithms) are tailored to solve tasks
that deviate from what is conventionally addressed in the
classical literature. For instance, while classical approaches
to the recommendation systems problem typically employ
low-rank matrix completion, the quantum algorithm in-
stead executes sampling over a low-rank approximation of
the input matrix [45]. In [60], a simplified version of the
least squares SVM problem is considered by assuming data
points are equally distributed across hyperplanes. Another
example is the algorithm presented by [47] which sees no
classical counterpart. As such, many works do not remark
on the performance of dequantized algorithms in compari-
son with other, more traditional classical algorithms for their
examined tasks.
Lastly, Chia et al. [46] argues that dequantized algo-
rithms operate under more restrictive and ostensibly weaker
computation parameters compared to classical randomized
numerical linear algebra algorithms. Dequantized algo-
rithms assume the ability to efficiently measure quantum
states related to the input data and aim to provide quick
algorithms with dimension-independent runtime. However,
this model is intrinsically weaker than its standard counter-
part. In essence, a dequantized algorithm, with a runtime
of O(T), translates to a standard algorithm with a run time
of O(nnz(A) + T) dependent on both T and the number
of non-zero entries in the input matrix. Although this may
lead to under-performance in typical sketching contexts, it
broadens the range of problems where quantum speedup
may not exponentially surpass classical solutions. Therefore,
dequantized algorithms, in spite of their theoretical promise
of exponentially improved runtime, may not perform as
well as conventional sketching algorithms.
As such, the particular nature of the gap between clas-
sical and quantum ML algotithms remains an open ques-
tion. In general, QML applications operate in either the
low-rank or the high-rank datasetting. The dequantiza-
tion formalism suggests that most quantum linear algebra
tasks over low-dimensional data can likely be dequantized
into a classical variant, provided SQ access is available to
that data. In contrast, evidence suggests that dequantizing
high-dimensional problems incurs must greater difficultly.
Several high-dimensional problems cannot be successfully
dequantized despite SQ access. For example, the Fourier
Sampling Problem is solved by randomized linear algebra
techniques (i.e., SQ access) in exponential time, whereas the
quantum version can find a solution in O(1) [233]. Further-
more, high-rank data frequently necessitates the use of the
HHL algorithm or its variants, which have been discussed
to be BQP-complete [54]. Another example is with quantum
Boltzmann machine training, noted in [49], which cannot be
dequantized in full unless BQP = BPP. This presents a sig-
nificant impediment for classical algorithms trying to match
the performance of their quantum counterparts. As such,
QML algorithms can potentially extend their advantage in
the high-rank setting by making assumptions such as taking
sparse matrices as input or utilizing other high-rank quan-
tum operations that can be efficiently implemented, such as
the Quantum Fourier Transform [46]. Additionally, dequan-
tized algorithms hinge on cost-effective access to classically
analogous QRAM. As of now, the quantum version of this
input model has not been practically realized. However,
should an efficient quantum input model be developed, one
that eschews the need for expensive computations or classi-
cal interfacing, it could potentially prompt a reevaluation of
the supposed advantages of dequantization methods.
In light of this, evidence has shown that there is a
strong opportunity for classical algorithms to compete with
quantum in the low-rank setting. In early works, for many
tasks, the quantum algorithm still admitted a strong poly-
nomial advantage. This restricted the applicability of several
dequantized algorithms; even matrices with very low-rank
could not be practically computed [234]. As noted by several
authors, the cost of computation is dominated by the SVD
computation that occurs after sampling down to the low-
rank approximation [44], [47], [48]. New techniques have
since been developed that bypass this computation, with
Bakshi and Tang’s method most recently demonstrating
that low-degree QSVT circuits do not exhibit exponential
advantage [48]. It should be noted that, at present, quantum
algorithms polynomially surpass their dequantized coun-
terparts. As both quantum and dequantized algorithms
continue to improve their relative complexity bounds, it re-
mains to be seen whether there is a limit to the performance
of dequantized algorithms. An insurmountable threshold
may exist that definitively ascribes quantum supremacy, or
a classical regime might be discovered that denounces the
advantage entirely.
5.8.2
Tensor Networks
Tensor networks learning models have scarcely stepped
outside of a few, benchmark datasets, such as MNIST and
Fashion-MNIST, due to difficulties in extending the tensor
network model to larger inputs and higher dimensional
feature spaces. In [128], the Tiny Image dataset was used,
however the images were cropped to a 28 × 28 resolution
and converted to gray scale, matching the context of MNIST
images. For MPS structures, accommodating for images of
larger resolution is difficult due to the inherent exponential
loss of correlation across the network, which can also be
exacerbated by common encoding methods. Images are
typically flattened and encoded into high dimensional space
[92], [99]; for small images, the pixel correlations are largely
preserved, however they are lost when considering high
resolution images [125]. This issue is less prevalent in higher
order decompositions such as PEPS [115] and TTNs [110].
However, several works have noted that the advantage of
neural-network-based models over these tensor network
methods lies in the better priors for images, made possible
by the use of convolution. Tensor network methods that
incorporate convolution are nearing the performance of
traditional neural networks [124]. The potential for discov-
ering approaches within tensor network methods that could

36
surpass neural networks is an area of interest and highlights
a promising direction for future research.
Tensor network algorithms demand a high cost in both
the bond dimension, a user-chosen free parameter, and the
number of components contained after each local feature
mapping, determined by choice of ϕ in Equation 18. Tensor
network machine learning methods currently admit cubic,
or even higher polynomial dependence on these parameters
[92], [110], despite only scaling linearly with the number of
input components (e.g., the number of pixels in an input
image). Thus there appears to be a a trade-off in the greater
expressivity afforded by increasing these parameters and
computation time.
The challenge of high dimensionality is especially preva-
lent in language modeling, as the semantic spaces of word
vectors can be inherently large. This is in contrast to com-
monly used image modeling datasets, where pixel values
can be represented in a low-dimensional format. The tensor
products of such word vectors can become computationally
challenging, maintaining high complexity even after tensor
decompositions are applied [95]. This complexity may ex-
plain the limited research in the field of tensor networks for
language modeling. In [96], tensor network evaluation was
performed on a context-free language task using relatively
simple, synthetic data. The field has yet to see robust,
performant methods for complex language modeling tasks
involving tensor networks.
These observations are supported in existing tensor
network literature, stemming from the fact that classical
tensor networks are only able to represent low-entangled,
low-complexity states [80]. However this stipulation is less
relevant outside of the quantum setting, where less complex
classical input data is concerned which is inherently non-
entangled. Further research may be necessary to under-
stand what types and volumes of data become prohibitive
in learning, and how to best utilize tensor networks for
working with such data.
5.8.3
Quantum Variational Algorithm Simulation
The hybrid quantum-classical variational methods pre-
sented have seen success in performing computations over
relatively small datasets, and using small-scale quantum
circuits. Few methods have been present outside this set-
ting, due to the exponential limitation in simulating larger
circuits with more qubits. For instance, methods using basis
encoding require qubits that scale linearly with the number
of representative features [155]. This restricts the number of
features that can be used for learning. As such, when eval-
uating relative performance against classical counterparts,
many works will apply constraints to classical methods
in order to provide fair comparisons. This may involve
severely condense the number of features [152], [216], or
using a heavily reduced dataset size [219], for amenability
with current-day quantum simulator architectures. Such
limitations imply that comparisons between quantum sim-
ulation and classical methods in their fully-optimized, unre-
stricted settings may be inherently challenging, pending the
development of more performant quantum algorithms.
The training time for simulating quantum algorithms is
frequently noted to be more prolonged than their classical
counterparts due to the inherent complexities of emulating
quantum systems on traditional hardware [153], [159], [212],
[212], [235]. In [160], the speedup over classical methods
is owed to the use of quantum gradient descent used in
the VQCs, allowing for faster convergence than classical
neural networks using traditional gradient descent. Such
optimizations could inform methods of decreasing training
time in classical settings, for acceptable loss thresholds.
For classical simulation of QML in the noisy setting, such
algorithms naturally inherit the limitations of QML, such as
the need for robust gate error correction, and degradation of
performance due to decoherence after prolonged training.
Noiseless simulations are free from such issues, although
they may not accurately represent the conditional settings of
quantum hardware execution [148]. Despite this advantage
for classical implementation purposes, most of the works
in this domain are forward-facing, developing methods
and frameworks for true quantum computation, and assess
their robustness with added noise and constraints. Very
few studies have concentrated on the specific context where
computation on classical machines is the primary objective
of the proposed methodology.
5.8.4
Other QiML Methods
The range of QiML methods discussed demonstrate a di-
verse application of quantum theory to facilitate machine
learning tasks. Generally, the introduction of quantum phe-
nomena into these models is observed to enhance their
expressive power compared to their classical counterparts,
often resulting in improved performance [94], [179], [208].
However, a commonly reported drawback among many of
these methods is their high computational time complexity
when compared with classical techniques. This issue pre-
dominantly affects models that rely heavily on quantum
formalisms requiring computationally intensive operations,
such as tensor products and complex number manipulations
[179], [208]. This is especially noted in methods that require
matrix operations over density operators [178], [180], [190],
or require making tensor copies of quantum patterns in
producing classification [181], [182], [184].
In essence, there is an apparent trade-off between per-
formance and computational speed, by way of simulating
quantum operations via computationally heavy mathemat-
ical objects to incorporate greater expressivity in QiML
models.
6
PARALLELS
BETWEEN
QIML
MODELS
AND
CONVENTIONAL CLASSICAL MODELS
Efforts in QiML have applied quantum mechanics to en-
hance machine learning routines. Some of these efforts yield
outcomes that bear resemblance to conventional classical
ML models. In this subsection, we explore several of these
parallels, shedding light on the relationships and distinc-
tions between QiML techniques and their classical counter-
parts. By doing so, we aim to provide a familiar framework
for understanding and interpreting QiML approaches. This
approach could make the field more accessible to machine
learning practitioners and researchers who are venturing
into quantum information theory for the first time.

37
6.1
Tensor Networks
Previous studies have explored the relationship between
tensor networks and neural network models [236]. As men-
tioned in Section 4.2.8, several tensor network decomposi-
tions exhibit parallels between deep learning architectures,
such as CNNs [131], RNNs [132], and RBMs [133], [134].
Non-negative Matrix Product States (MPS) have been es-
tablished as having a correspondence with Hidden Markov
Models (HMM). Specifically, they can factorize probability
mass functions of HMM into tensor network representa-
tions, capturing the essential stochastic relationships be-
tween hidden and observed variables [237].
Tensor networks employ kernel learning approaches,
similar to SVMs, in which samples are mapped to a higher
dimensional feature space for improved separability [100].
The ability of tensor networks to model the joint dis-
tribution of variables, as seen in Tensor Network Born
Machines (TNBMs), draws parallels with generative models
like Variational Autoencoders (VAEs) and Generative Ad-
versarial Networks (GANs) that parameterize conditional
probabilities via deep neural networks [124]. In fact, several
works have shown that many-body quantum states can also
be efficiently represented by neural network structures, such
as DBMs [238] and shallow fully-connected neural networks
[239], provided a simplified Hamiltonian ground state; sim-
ilar conditions in which tensor networks see success.
6.2
Quantum Variational Algorithm Simulation
Variational quantum algorithms draw obvious parallels be-
tween classical machine learning methods. Quantum kernel
methods, similar to SVMs, map data to a high-dimensional
Hilbert space where they become linearly separable [155].
The computation of quantum kernel is effectively measuring
the inner product in this Hilbert space, analogous to the
operation of the kernel function in SVMs. VQCs combine
classical optimization methods with a variational quantum
circuit to learn a parameterized quantum state. This learning
mechanism bears a strong resemblance to the operational
principle of classical neural networks, which adjust weights
and biases through an iterative optimization process to learn
a function that can accurately classify data. Similarly, in
a VQC, parameters of the quantum circuit are iteratively
updated, effectively optimizing the quantum state to classify
quantum data [240]. As such, performance comparisons
are often made between devised quantum algorithms and
classical models of similar scale, i.e., by scaling down the
number of available parameters in a classical neural net-
work [155]
The relationship between tensor networks and quantum
circuits has also been explored in the literature, where tensor
networks are seen to admit quantum circuits [81]. As such,
tensor network decompositions have inspired quantum
variational methods. Hierarchical ansatz layouts mirroring
tree-tensor and MERA networks have been shown to enable
the classification of highly entangled states through greater
expressive power [85], [241]. Quantum Circuit Born Ma-
chines (QCBM) perform generative modeling by exploiting
the inherent probabilistic interpretation of quantum wave-
functions [242]. This can be seen as analogous to the Tensor
Network Born Machine; generative quantum circuits have
been proposed based on the MPS [243]. The similarities of
TNBMs to classical generative models naturally extend to
the QCBM.
6.3
Dequantized Algorithms
Regarding dequantized algorithms, extracting such correla-
tions may initially seem an unassuming task. As noted in
Section 5.8.1, the precise task that dequantized algorithms
solve can be different to what is conventionally tackled.
However, drawing face-value parallels between dequan-
tized algorithms and classical machine learning methods
seems intuitive and practical, since their underlying objec-
tives are largely congruent. For instance, quantum-inspired
SVMs aim to establish hyperplanes for classification, and
quantum-inspired supervised clustering also engages in
nearest centroid discrimination, mirroring their classical
counterparts.
6.4
Other QiML Methods
Several QiML methods outside the afforementioned subsets
incorporate quantum theory into existing classical struc-
tures. These include [189] and [190], where density matrix
projections are processed and feed into traditional classifiers
such as CNNs, LSTMs and SVMs. In [201], a traditional
fully-connected neural network structure is realized using
a fuzzy c-means-based learning process, where learning
parameters are represented via quantum bits and quantum
rotational gate operations. In [197], complex-valued word
embedding are constructed via gate recurrent units (GRU)
and scaled dot-product self-attention. Features are then ex-
tracted from projected density matrices via convolutional
and max-pooling layers.
Other works have produced methods that use quantum
operations to mirror neural network behavior, such as in
[194] where a parameterized update function is used to
evolve a “hidden” density matrix, which is updated at
each time step based on the current input quantum state
and the previous hidden density matrix, mirroring the key
operational dynamics of an RNN.
Methods such as the QMNC and HQC [180], [181],
[185] exhibit obvious similarities to classical nearest mean
classification, as they rely on distance-based evaluation to
a constructed centroid object. Methods involving quantum
signal detection theory [206], [207], [209] share similarities
to the Naive Bayes classification, where probabilities are
calculated based on the frequencies of features.
Some methods, like the Interactive Quantum Classi-
fier (IQC) [208], seem entirely quantum mechanical. The
IQC interprets the classification process as an interaction
between a physical target system and its environment,
using quantum-inspired unitary transformations to adjust
probability amplitudes and phases. Although the method
involves common machine learning components, such as
feature modeling and gradient-based updates, the core ar-
chitecture of IQC is heavily rooted in quantum theory.
As the field continues to evolve, we anticipate the emer-
gence of other such models that deeply integrate quan-
tum theory while still leveraging classical machine learning
strategies to varying extents.

38
6.5
Levels of Quantum Mechanics Integration
Several sources of quantum inspiration have driven QiML
learning methods. As briefly discussed in Section 6.4, the
various QiML methods vary in how much quantum me-
chanics they integrate, leading to both opportunities and
challenges. On one hand, principles from quantum mechan-
ics such as superposition and entanglement provide rich
inspiration, allowing the development of innovative algo-
rithms and encoding strategies beyond classical paradigms.
On the other hand, implementing these quantum-infused
methods in classical computing settings can be difficult.
Quantum mechanics often exhibits complex correlations
and dynamics that are hard to simulate classically, leading
to potential exponential slowdowns or the need for approx-
imations that may sacrifice quantum advantages.
Figure 9 illustrates a qualitative view of QiML methods
based on the extent of quantum involvement. We sur-
mise that methods incorporating more, or deeper levels
of quantum mechanics tend to face greater challenges or
limitations when attempting to simulate or implement them
using classical computing resources. At the far right end
of the spectrum, just before QML, lie variational quantum
algorithms. Quantum circuit and quantum kernel learn-
ing methods heavily incorporate quantum aspects such as
qubits, quantum gates, superposition, and entanglement.
The classical-quantum hybridization of these techniques
that rely on classical optimization allow for easier simula-
tion on classical devices, however factors such as the circuit
depth, width (i.e., number of qubits used), choice of gates,
and encoding method can have a dramatic effect on this ease
of simulation, as discussed in Section 5.8.3. In other words,
increasing the quantum-based complexity of these methods
reduces the ability to classically simulate them.
QiML methods that project classical data into Hilbert
feature spaces also exhibit varying levels of classical sim-
ulatability, typically tied to the nature of the projection
and the dimensionality of the Hilbert space. Tensor net-
work methods adapt techniques from quantum many-body
physics, with network decompositions easing the computa-
tional burden by reducing the scope of the feature space.
However, challenges still arise in their computation, as
discussed in Section 5.8. Additionally, higher-order tensor
network methods such as PEPS often resort to approxima-
tion techniques instead of exact computation. In a machine
learning context, these approximations may be sufficiently
representative, as generalizations are usually more ben-
eficial than precise exactitude. Furthermore, the need to
operate over density matrices presents a common source of
computational difficulty. Methods that rely on potentially
large density matrices as features typically incur greater
time complexity compared to classical ML methods using
vector-based features, representing a trade-off for increased
expressive power.
At the classical end of the spectrum, dequantized al-
gorithms attempt to match QML methods using classical
techniques. Although devoid of explicit quantum compo-
nents, these methods can incur substantial computational
costs in line with the matrix dimension and norms. De-
spite the intention to make them more classically amenable,
dequantized algorithms may still be relatively slow, high-
lighting the intrinsic complexity and potential inefficiency
of translating quantum-inspired techniques into classical
paradigms. This mirrors the challenges found in more
quantum-intensive techniques, revealing that the integra-
tion of quantum insights in classical contexts is a nuanced
and demanding endeavor.
While the primary focus of this analysis has been on
methods that trade computational capacity for quantum
inspiration, it’s worth noting a caveat: some QiML methods,
including compression techniques, can actually experience
speed-ups compared to their classical counterparts. This em-
phasizes the diverse potential of QiML, extending beyond
mere computational trade-offs to offer tangible benefits and
enhancements to classical ML.
7
AVAILABLE RESOURCES FOR QIML IMPLEMEN-
TATION
We discuss the available resources allowing for the develop-
ment and exploration of QiML methods.
7.1
Research Implementations
While many of the explored works operate as standalone
research models, not all perform empirical evaluations of
their models using real data. Furthermore, only a subset
of these works offer accessible code repositories that allow
others to reproduce their results, with many authors stipu-
lating that their code is available upon request. We highlight
works that include accessible code in Table 2, indicated by
the asterisk (*). The ability to reproduce presented methods
is an essential part of scientific inquiry. However, the quality
and accessibility of these repositories can vary considerably,
often due to varying levels of documentation and the spe-
cific requirements of certain computational environments
and packages. This variation makes the replication process,
and the adaptation of methods to wider domains and tasks
challenging. Custom implementations of presented methods
are less prevalent in tensor network research, thanks in part
to the availability of well-developed toolboxes and libraries,
as we discuss in Section 7.2.
We advocate for more uniformity in the way code is
shared in the QiML research community. Greater trans-
parency, along with increased adoption of best practices for
documentation and repository organization, will enhance
the accessibility of these implementations and enable more
robust scientific discourse, especially in an emerging field.
7.2
Toolboxes
Various toolboxes and libraries have been developed for
tensor network operations and quantum circuit simulation.
Table 5 outlines a few commonly used, available packages
for QiML purposes.
Basic tensor operations are supported by implemen-
tations for various languages. Most prominently, Python
has the TT-Toolbox [246], TorchMPS [251] and Scikit-TT
[249] frameworks, which all provide MPS solvers with sup-
port for DMRG optimization. tntorch [258] facilitates auto
differentiation-based optimization for MPS. Tensorly [247]
and TensorNetwork [250] Python libraries offer more gen-
eralized functions, allowing for additional decomposition

39
Fig. 9. QiML methods represented on a 1D spectrum, illustrating their relative positioning based on the level of quantum inspiration, as compared
to purely classical and purely quantum machine learning. The positioning is not determined by a strict quantitative measure but rather reflects an
informal assessment of the underlying principles and techniques. Citations given are representative works.
formats with support for various Python backends, such
as PyTorch or JAX which provide the machine learning
functionality. Non-specialised, generic Python libraries such
as Numpy [259], which provides a base for many tensor
network libraries, have also been successfully used on a
standalone basis [100], [124]. MATLAB, C++ and Julia also
see a host of supporting tensor network libraries. Psarras et
al. [260] provides a comprehensive survey of existing tensor
network software. Wang et al. [79] categorizes tensor net-
work toolboxes based on their functionality and application
areas. We collate the ones that have been used by researchers
in the QiML context in Table 5. For works that did not
explicitly mention what packages were used, we discovered
them by inspecting noted code repositories.
While these toolboxes have been used to much success, a
few limitations present themselves, namely the lack of both
predefined models for higher-order tensor decompositions
(such as PEPS and TTN), and input embedding pipelines.
This restricts the ability for users to freely produce and
develop new models.
Several libraries have been developed to facilitate quan-
tum circuit construction and simulation. Pennylane [41]
and TensorFlow Quantum [255] are heavily focused on
the integration of quantum computing with classical ma-
chine learning frameworks, such as PyTorch and Tensor-
Flow (predominantly in the latter), with back-end support
for various quantum simulation platforms. Qulacs [254]
and Qibo [256] provide efficient and flexible standalone
quantum simulators operable on personal computing de-
vices. Intel Quantum Simulator [252] offers similar support,
with adaptations for high-performance computing environ-
ments. Qiskit [40] provides general comprehensive quantum
software development kits that supports a wide range of
quantum computing workflows, including local simulation,
circuit optimization, and execution on IBM’s cloud-based
quantum hardware. Cirq [248] allows user to build quan-
tum circuits for near-term quantum hardware and NISQ
(Noisy Intermediate-Scale Quantum) devices, giving fine-
tuned control over quantum program execution.
The Quantum Nearest Mean Classifier sees a package for
its implementation, which allows for application over cus-
tom data, and offers parallel computing capabilities [181].
Methods for input modeling and encoding are not provided
in this repository.
In contrast, there is a sparsity in toolboxes that facilitate
the implementation of practical models for dequantized
algorithms. This may be primarily due to the theoretical
nature of the research, the level of specificity necessary
for adapting general ideas to particular tasks, and the
(im)maturity of the field. As highlighted in [214], claimed
complexities may not always be indicative of real-world
application scenarios. The development and introduction
of frameworks could potentially provide researchers with
deeper insights into these issues. Additionally, they could
serve as a tools for validating proposed methods through
comparative or ablative studies. Similar observations apply
to the various other QiML methods. However, given that
these methods have seen practical implementations with di-
rect application of the proposed methods, we can anticipate
the development of dedicated toolboxes for them in the near
future.
7.3
Commercial Applications
Several noisy intermediate-scale quantum computing archi-
tectures have been developed and commercialized and used
for various applications. Cloud-based compute for quan-
tum simulation has been employed extensively in practical
research, particularly for applications requiring computa-
tional resources beyond the scope of personal computing.
As outlined in Table 6, common cloud-computing back-ends
employed in literature, offering high-performance simula-
tion capabilities. IBM Quantum [228] provides the QasmSim-
ulator and StatevectorSimulator backends through Qiskit Aer:
QasmSimulator allows for multi-shot execution of circuits,
while StatevectorSimulator also returns the final statevector of
the simulator after application. Both simulate up to 32 qubits
in both noisy and noise-free settings. Google’s qsim [227]
provides a full wavefunction quantum circuit simulator that
leverages vectorized optimization and multi-threading, ca-
pable of simulating up to 40 qubits. Microsoft’s Azure Quan-
tum platform [261] offers three back-end simulators, from
the IonQ, Quantuum and Rigetti providers. IonQ provides
a GPU-accelerated idealized simulator supporting up to 29
qubits, Quantuum provides emulators of real physical quan-
tum models supporting up to 32 qubits, and Rigetti provides
a cloud service simulator for Quil, a quantum instruction
set language, supporting up to 30 qubits. Amazon Braket
[229] provides on-demand state vector, tensor network and
density matrix simulators, similar to IBM Quantum, with a
current qubit limit of 34. Quantum hardware accessibility is
offloaded to third-party providers.
A few quantum-inspired-related frameworks are avail-
able such as the Fujitsu [262], NEC [263] and D-Wave
quantum-inspired annealing services6, with the latter being
6. https://docs.ocean.dwavesys.com/en/stable/

40
TABLE 5
QiML Toolboxes
Toolbox
Mode
Functionality
Languages
Research Group
LIQUi|⟩(2014) [244]
VQAS
comprehensive framework for quan-
tum programming with three built-in
classes of simulators
F#
Microsoft Research
NCON (2014) [245]
TN
functions that facilitate tensor network
contractions, which are integral to sev-
eral other tensor network toolboxes
MATLAB
Perimeter Institute for
Theoretical Physics
TT-Toolbox (2014) [246]
TN
basic tensor arithmetic, contractions,
and routines involving MPS
MATLAB,
Fortran,
Python
Institute of Numerical
Mathematics RAS
Tensorly (2016) [247]
TN
tensor methods and deep tensorized
neural networks via several Python
backends
Python
Imperial College Lon-
don
Qiskit (2017) [40]
VQAS
software development framework for
modeling
circuits,
algorithms,
and
hardware
Python
IBM
Cirq (2018) [248]
VQAS
quantum
programming
library
for
NISQ hardware control and simulation
Python
Google, Open-source
Pennylane (2018) [41]
VQAS
open-source quantum software library
for quantum machine learning tasks
with support for various quantum
computing platforms
Python
Xanadu AI
Scikit-TT (2018) [249]
TN
MPS methods for representing and
solving linear systems
Python
Freie Universit¨at Berlin
HQC (2019) [181]
Other
(QMNC)
probabilisitc classification with paral-
lelization available
Python
University of Cagliari
TensorNetwork (2019) [250]
TN
defining and manipulating general ten-
sor network models
Python
Alphabet (Google) X
TorchMPS (2019) [251]
TN
MPS modeling with DMRG support
via PyTorch backend
Python
Universit´e de Montr´eal
Intel Quantum Simulator (2020) [252]
VQAS
quantum circuit simulator with high-
performance computing capabilities
C++, Python
Intel Labs
PastaQ (2020) [253]
TN,
VQAS
various
quantum
circuit
simulation
methods using tensor-network repre-
sentations
Julia
Flatiron Institute
Qulacs (2020) [254]
VQAS
fast, low-scale quantum circuit simula-
tor that provides a wide range of built-
in quantum gates and operations
Python, C++
QunaSys, Osaka Uni-
versity, NTT, Fujitsu
TensorFlow Quantum (2020) [255]
VQAS
provides tools and frameworks for
building
hybrid
quantum-classical
models
Python
Google
lambeq (2021) [175]
VQAS
library for end-to-end quantum NLP
pipeline development
Python
Cambridge
Quantum
Computing
Qibo (2021) [256]
VQAS
builds and runs quantum circuits, sup-
porting GPU, multi-GPU, and multi-
threaded CPU.
Python
Quantum
Research
Center (QRC)
ITensor (2022) [257]
TN
tensor
arithmetic,
contractions,
and
support for MPS and MPO decompo-
sitions
C++, Julia
Flatiron Institute
tntorch (2022) [258]
TN
supports tensor factorizations, includ-
ing CP, Tucker, and MPS, and offers
autodifferentiation optimization
Python
IE University, Madrid

41
TABLE 6
Various Quantum Computing Platforms
Platform
No. Sim. Qubits
Noisy/Noiseless
Simulation
Languages
Integration with:
Quantum
Hardware
Accessibility
IBM Quantum [228]
32
Yes/Yes
Python, Swift
Qiskit
Yes
Google
Quantum
AI [227]
40
Yes/Yes
Python, C++
TensorFlow Quantum
Yes
Microsoft
Azure
Quantum
Cloud
Service [261]
29-32
Yes/Yes
Q#
.NET
Yes
Amazon
Braket
[229]
34
Yes/Yes
Python
Amazon Web Services
Yes
applied to a few, small-scale quantum-inspired [215] and
quantum-assisted [264] ML applications. Outside of these
services, dedicated QiML architectures have yet to see such
production and adoption.
In general, there are ample toolbox options available for
TN and QVAS methods, with commercial platform-as-a-
service providers supplying compute power for quantum
simulation. However there is a lack of available tooling
outside of these areas, which limits the scope of choices
for ML practitioners in exploring QiML solutions. The gap
presents ample opportunities for the development, and po-
tential commercialization, of such frameworks, which could
in turn catalyze further research in the field.
8
OPEN ISSUES
QiML research is subject to the numerous challenges of an
emerging discipline. In the practical setting, these challenges
largely revolve around how QiML can be used for a broader
range of applications, and in more performant ways. We
identify potential issues in furthering this goal.
1)
Speed and Performance: A significant challenge
in present QiML methods is the dichotomy be-
tween the complexity of the methods and their
performance. Works have shown the performance
of QiML methods in general is worse than that
of contemporary classical methods. Where they do
show competitive results, this is typically caveated
by slower runtimes, larger model sizes or greater
incurred error [43], [124], [179]. In the case of VQAS
models, comparable performance is often only ob-
served when classical models are deliberately scaled
back in terms of architecture size, the number of
parameters and/or number of input features. A
few exceptions to this caveat have been presented,
particularly in methods designed with parameter
reduction in mind [122], [126]. In general, however,
there is a clear necessity for research that improves
the speed and performance of QiML, striking a
balance between complexity and effectiveness.
2)
Constraints on Input Data:
Dequantized algorithms and tensor network learn-
ing methods are predicated on the assumption that
input data exhibits low rank characteristics: low
linear algebraic rank and low bond dimension,
respectively. However, such assumptions may not
hold true in real-world scenarios, where data can
often be high-dimensional and complex. While there
are claims advocating for the broad applicability
of QiML methods [49], these assertions have yet
to be effectively demonstrated on large, real-world
datasets. Therefore, addressing the challenge of
high-dimensional data representation in quantum
machine learning remains a key area for future
research and development.
3)
Alternative Input and Embedding Methods:
Currently, methods for transforming classical data
to be compatible with QiML methods are largely
under-explored. Input modeling typically adheres
to a few established embedding schemes, for ex-
ample Equation 18 for tensor networks, or den-
sity matrix-based representations commonly seen
in QiML subsets. Though these appear to work in
generalised settings, authors have noted potential
adaptations, such as having independent mappings
per feature, or promoting the mapping space to
higher dimensions [92] for TNs. Analysing and
suggesting appropriate access models for adequate
comparisons between dequantized and quantum
algorithms is also an emerging area of concern. As
the embedding method itself often directly depends
on the nature of the underlying data, investigation
of the effects of different embedding schemes on fac-
tors such as performance and information entropy,
researchers can gain insights into optimal methods
for specific data types and tasks.
4)
The Need for Comprehensive Tooling:
There is a significant need for comprehensive, user-
friendly tooling in the field of QiML. Existing tool-
boxes, while useful, do not fully meet the needs
of researchers and developers. They often lack the
extensive array of tools and capabilities required
to effectively develop new models, re-implement
existing ones, or explore innovative research av-
enues, especially in the areas of tensor networks
and other QiML methods. The development of more
robust toolsets could greatly accelerate the pace of
advancement in this promising field.

42
5)
Effective Quantum Formalisms in QiML
In the current literature, several quantum for-
malisms that inspire QiML have demonstrated con-
siderable success, especially ones that contribute
towards building and utilizing quantum feature
spaces. However, there remains a lack of systematic
documentation identifying which quantum prin-
ciples are adaptable to classical ML, the reasons
for their success or potential, and indeed, which
quantum concepts may not translate well or at
all. The exploration of novel quantum mechanics
adaptations within classical ML is an ongoing area
of research. Consequently, the QiML field would
significantly benefit from a methodical analysis de-
lineating which approaches are effective and which
are not.
9
CONCLUSION
Quantum-inspired Machine Learning has seen a rapid ex-
pansion in recent years, diversifying into numerous research
directions, such as tensor network simulations, dequantized
algorithms and other such methods that draw inspiration
from quantum physics. Prior surveys have alluded to QiML,
often presenting it as a facet of QML or concentrating on
specific QiML subsets. In contrast, this survey provides a
comprehensive examination, bringing these emergent fields
together under the QiML umbrella. We have explored re-
cent work across these areas, highlighting their practical
applications, offering readers an understanding of where
and how QiML has been used. This insight can potentially
guide readers in exploring QiML for their specific use cases.
Significantly, we strive to pin down a more precise defini-
tion of QiML, addressing the issue of vague and generic
descriptions prevalent in previous work. Furthermore, this
review illuminates crucial open issues in QiML, particularly
pertaining to its current level of practical applicability. As
we move forward, we anticipate the emergence of new
QiML methods. Quantum mechanics, quantum computing
and classical machine learning are expansive fields with
a constant influx of emerging knowledge and techniques.
The untapped potential of these fields presents a vast reser-
voir of methods and approaches that could further enrich
QiML. The continuous evolution of these fields presents a
fertile ground for novel perspectives and cross-pollination,
promising to stimulate the growth and diversification of
QiML.
ACKNOWLEDGMENTS
This work was supported by CSIRO’s Quantum Technolo-
gies Future Science Platform. Ajmal Mian is the recipient of
an Australian Research Council Future Fellowship Award
(project number FT210100268) funded by the Australian
Government.
REFERENCES
[1]
C. Ciliberto, M. Herbster, A. D. Ialongo, M. Pontil, A. Rocchetto,
S. Severini, and L. Wossnig, “Quantum machine learning: a clas-
sical perspective,” Proceedings of the Royal Society A: Mathematical,
Physical and Engineering Sciences, vol. 474, no. 2209, p. 20170551,
2018.
[2]
M. Cerezo, G. Verdon, H.-Y. Huang, L. Cincio, and P. J. Coles,
“Challenges and opportunities in quantum machine learning,”
Nature Computational Science, vol. 2, no. 9, pp. 567–576, 2022.
[3]
J. Preskill, “Quantum computing in the nisq era and beyond,”
Quantum, vol. 2, p. 79, 2018.
[4]
M. Schuld and F. Petruccione, Machine learning with quantum
computers.
Springer, 2021.
[5]
T. M. Khan and A. Robles-Kelly, “Machine learning: Quantum vs
classical,” IEEE Access, vol. 8, pp. 219 275–219 294, 2020.
[6]
S. Garg and G. Ramakrishnan, “Advances in quantum deep
learning: An overview,” arXiv preprint arXiv:2005.04316, 2020.
[7]
Z. Abohashima, M. Elhosen, E. H. Houssein, and W. M. Mo-
hamed, “Classification with quantum machine learning: A sur-
vey,” 2020.
[8]
N. Mishra, M. Kapil, H. Rakesh, A. Anand, N. Mishra, A. Warke,
S. Sarkar, S. Dutta, S. Gupta, A. Prasad Dash et al., “Quantum ma-
chine learning: A review and current status,” Data Management,
Analytics and Innovation: Proceedings of ICDMAI 2020, Volume 2,
pp. 101–145, 2021.
[9]
E. H. Houssein, Z. Abohashima, M. Elhoseny, and W. M. Mo-
hamed, “Machine learning in the quantum realm: The state-
of-the-art, challenges, and future vision,” Expert Systems with
Applications, p. 116512, 2022.
[10]
A. Zeguendry, Z. Jarir, and M. Quafafou, “Quantum machine
learning: A review and case studies,” Entropy, vol. 25, no. 2, p.
287, 2023.
[11]
M. Moore and A. Narayanan, “Quantum-inspired computing,”
Dept. Comput. Sci., Univ. Exeter, Exeter, UK, 1995.
[12]
A. Steane, “Quantum computing,” Reports on Progress in Physics,
vol. 61, no. 2, p. 117, 1998.
[13]
A. Narayanan and M. Moore, “Quantum-inspired genetic algo-
rithms,” in Proceedings of IEEE international conference on evolution-
ary computation.
IEEE, 1996, pp. 61–66.
[14]
K.-H. Han and J.-H. Kim, “Genetic quantum algorithm and its
application to combinatorial optimization problem,” in Proceed-
ings of the 2000 congress on evolutionary computation. CEC00 (Cat.
No. 00TH8512), vol. 2.
IEEE, 2000, pp. 1354–1360.
[15]
——, “Quantum-inspired evolutionary algorithm for a class of
combinatorial optimization,” IEEE transactions on evolutionary
computation, vol. 6, no. 6, pp. 580–593, 2002.
[16]
G. Zhang, “Quantum-inspired evolutionary algorithms: a survey
and empirical study,” Journal of Heuristics, vol. 17, no. 3, pp. 303–
351, 2011.
[17]
A. Manju and M. J. Nigam, “Applications of quantum inspired
computational intelligence: a survey,” Artificial Intelligence Review,
vol. 42, pp. 79–156, 2014.
[18]
C. Varmantchaonala Moudina, J. L. Fendji Kedieng Ebongue, and
M. Atemkeng, “Quantum algorithms for combinatorial optimiza-
tion problems: A comprehensive survey from 2000 to 2022,” Jean
Louis and Atemkeng, Marcel, Quantum Algorithms for Combinatorial
Optimization Problems: A Comprehensive Survey from, 2022.
[19]
O. H. M. Ross, “A review of quantum-inspired metaheuristics:
Going from classical computers to real quantum computers,” Ieee
Access, vol. 8, pp. 814–838, 2019.
[20]
K.-H. Han, K.-H. Park, C.-H. Lee, and J.-H. Kim, “Parallel
quantum-inspired genetic algorithm for combinatorial optimiza-
tion problem,” in Proceedings of the 2001 congress on evolutionary
computation (IEEE Cat. No. 01TH8546), vol. 2.
IEEE, 2001, pp.
1422–1429.
[21]
J. Sun, B. Feng, and W. Xu, “Particle swarm optimization with
particles having quantum behavior,” in Proceedings of the 2004
congress on evolutionary computation (IEEE Cat. No. 04TH8753),
vol. 1.
IEEE, 2004, pp. 325–331.
[22]
L. Wang, Q. Niu, and M. Fei, “A novel quantum ant colony
optimization algorithm,” in Bio-Inspired Computational Intelligence
and Applications: International Conference on Life System Modeling
and Simulation, LSMS 2007, Shanghai, China, September 14-17, 2007.
Proceedings.
Springer, 2007, pp. 277–286.
[23]
D. Yongjun and L. Jiying, “The application of quantum-inspired
ant colony algorithm in automatic segmentation of tomato im-
age,” in 2017 2nd International Conference on Image, Vision and
Computing (ICIVC).
IEEE, 2017, pp. 341–345.
[24]
M. Das, A. Roy, S. Maity, and S. Kar, “A quantum-inspired ant
colony optimization for solving a sustainable four-dimensional
traveling salesman problem under type-2 fuzzy variable,” Ad-
vanced Engineering Informatics, vol. 55, p. 101816, 2023.

43
[25]
R. Somma, S. Boixo, and H. Barnum, “Quantum simulated an-
nealing,” 2007.
[26]
T. Menneer and A. Narayanan, “Quantum-inspired neural net-
works,” Tech. Rep. R329, 1995.
[27]
F. S. Gharehchopogh, “Quantum-inspired metaheuristic algo-
rithms: Comprehensive survey and classification,” Artificial In-
telligence Review, pp. 1–65, 2022.
[28]
A. dos Santos Nicolau, R. Schirru, and A. M. M. de Lima, “Nu-
clear reactor reload using quantum inspired algorithm,” Progress
in Nuclear Energy, vol. 55, pp. 40–48, 2012.
[29]
A. L. Samuel, “Machine learning,” The Technology Review, vol. 62,
no. 1, pp. 42–45, 1959.
[30]
T. M. Mitchell et al., Machine learning.
McGraw-hill New York,
2007, vol. 1.
[31]
I. Goodfellow, Y. Bengio, and A. Courville, Deep learning.
MIT
press, 2016.
[32]
K. P. Murphy, Machine learning: a probabilistic perspective.
MIT
press, 2012.
[33]
P. Wittek, Quantum machine learning: what quantum computing
means to data mining.
Academic Press, 2014.
[34]
P. Domingos, “A few useful things to know about machine
learning,” Communications of the ACM, vol. 55, no. 10, pp. 78–87,
2012.
[35]
B. Mahesh, “Machine learning algorithms-a review,” International
Journal of Science and Research (IJSR).[Internet], vol. 9, pp. 381–386,
2020.
[36]
J. Carrasquilla, “Machine learning for quantum matter,” Advances
in Physics: X, vol. 5, no. 1, p. 1797528, 2020.
[37]
L. Alchieri, D. Badalotti, P. Bonardi, and S. Bianco, “An intro-
duction to quantum machine learning: from quantum logic to
quantum deep learning,” Quantum Machine Intelligence, vol. 3,
pp. 1–30, 2021.
[38]
S. L. Wu, S. Sun, W. Guan, C. Zhou, J. Chan, C. L. Cheng, T. Pham,
Y. Qian, A. Z. Wang, R. Zhang et al., “Application of quantum
machine learning using the quantum kernel algorithm on high
energy physics analysis at the lhc,” Physical Review Research,
vol. 3, no. 3, p. 033221, 2021.
[39]
F. Riaz, S. Abdulla, H. Suzuki, S. Ganguly, R. C. Deo, and S. Hop-
kins, “Accurate image multi-class classification neural network
model with quantum entanglement approach,” Sensors, vol. 23,
no. 5, p. 2753, 2023.
[40]
Qiskit contributors, “Qiskit: An open-source framework for
quantum computing,” 2023.
[41]
V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, S. Ahmed, V. Ajith,
M. S. Alam, G. Alonso-Linaje, B. AkashNarayanan, A. Asadi
et al., “Pennylane: Automatic differentiation of hybrid quantum-
classical computations,” arXiv preprint arXiv:1811.04968, 2018.
[42]
E. A¨ımeur, G. Brassard, and S. Gambs, “Machine learning in a
quantum world,” in Advances in Artificial Intelligence: 19th Confer-
ence of the Canadian Society for Computational Studies of Intelligence,
Canadian AI 2006, Qu´ebec City, Qu´ebec, Canada, June 7-9, 2006.
Proceedings 19.
Springer, 2006, pp. 431–442.
[43]
E.
Tang,
“An
overview
of
quantum-inspired
classical
sampling,”
https://ewintang.com/blog/2019/01/28/
an-overview-of-quantum-inspired-sampling/,
2019,
accessed:
2023-04-30.
[44]
——, “A quantum-inspired classical algorithm for recommen-
dation systems,” in Proceedings of the 51st annual ACM SIGACT
symposium on theory of computing, 2019, pp. 217–228.
[45]
I. Kerenidis and A. Prakash, “Quantum recommendation sys-
tems,” arXiv preprint arXiv:1603.08675, 2016.
[46]
N.-H.
Chia,
A.
P.
Gily´en,
T.
Li,
H.-H.
Lin,
E.
Tang,
and C. Wang, “Sampling-based sublinear low-rank matrix
arithmetic
framework
for
dequantizing
quantum
machine
learning,” J. ACM, vol. 69, no. 5, oct 2022. [Online]. Available:
https://doi.org/10.1145/3549524
[47]
N. Chepurko, K. Clarkson, L. Horesh, H. Lin, and D. Woodruff,
“Quantum-inspired algorithms from randomized numerical lin-
ear algebra,” in International Conference on Machine Learning.
PMLR, 2022, pp. 3879–3900.
[48]
A. Bakshi and E. Tang, “An improved classical singular value
transformation for quantum machine learning,” arXiv preprint
arXiv:2303.01492, 2023.
[49]
E. Tang, “Quantum principal component analysis only achieves
an exponential speedup because of its state preparation assump-
tions,” Physical Review Letters, vol. 127, no. 6, p. 060503, 2021.
[50]
S. Lloyd, M. Mohseni, and P. Rebentrost, “Quantum algo-
rithms for supervised and unsupervised machine learning,”
arXiv preprint arXiv:1307.0411, 2013.
[51]
——, “Quantum principal component analysis,” Nature Physics,
vol. 10, no. 9, pp. 631–633, 2014.
[52]
P. Courrieu, “Fast computation of moore-penrose inverse matri-
ces,” arXiv preprint arXiv:0804.4809, 2008.
[53]
A. G´eron, Hands-on machine learning with Scikit-Learn, Keras, and
TensorFlow.
” O’Reilly Media, Inc.”, 2022.
[54]
A. W. Harrow, A. Hassidim, and S. Lloyd, “Quantum algorithm
for linear systems of equations,” Physical review letters, vol. 103,
no. 15, p. 150502, 2009.
[55]
A. Gily´en, S. Lloyd, and E. Tang, “Quantum-inspired low-rank
stochastic regression with logarithmic dependence on the dimen-
sion,” arXiv preprint arXiv:1811.04909, 2018.
[56]
N.-H. Chia, H.-H. Lin, and C. Wang, “Quantum-inspired sub-
linear classical algorithms for solving low-rank linear systems,”
arXiv preprint arXiv:1811.04852, 2018.
[57]
A. Gily´en, Z. Song, and E. Tang, “An improved quantum-inspired
algorithm for linear regression,” Quantum, vol. 6, p. 754, 2022.
[58]
C. Shao and A. Montanaro, “Faster quantum-inspired algorithms
for solving linear systems,” ACM Transactions on Quantum Com-
puting, vol. 3, no. 4, pp. 1–23, 2022.
[59]
P. Rebentrost, M. Mohseni, and S. Lloyd, “Quantum support
vector machine for big data classification,” Physical review letters,
vol. 113, no. 13, p. 130503, 2014.
[60]
C. Ding, T.-Y. Bao, and H.-L. Huang, “Quantum-inspired sup-
port vector machine,” IEEE Transactions on Neural Networks and
Learning Systems, vol. 33, no. 12, pp. 7210–7222, 2021.
[61]
N.-H. Chia, T. Li, H.-H. Lin, and C. Wang, “Quantum-inspired
sublinear algorithm for solving low-rank semidefinite program-
ming,” arXiv preprint arXiv:1901.03254, 2019.
[62]
A. Gily´en, Y. Su, G. H. Low, and N. Wiebe, “Quantum singular
value transformation and beyond: exponential improvements for
quantum matrix arithmetics,” in Proceedings of the 51st Annual
ACM SIGACT Symposium on Theory of Computing, 2019, pp. 193–
204.
[63]
J. M. Martyn, Z. M. Rossi, A. K. Tan, and I. L. Chuang, “Grand
unification of quantum algorithms,” PRX Quantum, vol. 2, no. 4,
p. 040203, 2021.
[64]
G. H. Low and I. L. Chuang, “Hamiltonian simulation by qubiti-
zation,” Quantum, vol. 3, p. 163, 2019.
[65]
D. Jethwani, F. L. Gall, and S. K. Singh, “Quantum-inspired
classical algorithms for singular value transformation,” arXiv
preprint arXiv:1910.05699, 2019.
[66]
S. Gharibian and F. Le Gall, “Dequantizing the quantum singular
value transformation: Hardness and applications to quantum
chemistry and the quantum pcp conjecture,” in Proceedings of the
54th Annual ACM SIGACT Symposium on Theory of Computing,
2022, pp. 19–32.
[67]
R. Babbush, J. R. McClean, M. Newman, C. Gidney, S. Boixo, and
H. Neven, “Focus beyond quadratic speedups for error-corrected
quantum advantage,” PRX Quantum, vol. 2, no. 1, p. 010103, 2021.
[68]
I. Cong and L. Duan, “Quantum discriminant analysis for di-
mensionality reduction and classification,” New Journal of Physics,
vol. 18, no. 7, p. 073011, 2016.
[69]
R. P. Feynman, “Simulating physics with computers,” in Feynman
and computation.
CRC Press, 2018, pp. 133–153.
[70]
A. Rudi, L. Wossnig, C. Ciliberto, A. Rocchetto, M. Pontil, and
S. Severini, “Approximating hamiltonian dynamics with the
nystr¨om method,” Quantum, vol. 4, p. 234, 2020.
[71]
Z. Zhao, J. K. Fitzsimons, P. Rebentrost, V. Dunjko, and J. F. Fitzsi-
mons, “Smooth input preparation for quantum and quantum-
inspired machine learning,” Quantum Machine Intelligence, vol. 3,
no. 1, p. 14, 2021.
[72]
J. Cotler, H.-Y. Huang, and J. R. McClean, “Revisiting dequanti-
zation and quantum advantage in learning tasks,” arXiv preprint
arXiv:2112.00811, 2021.
[73]
H.-Y. Huang, M. Broughton, J. Cotler, S. Chen, J. Li, M. Mohseni,
H. Neven, R. Babbush, R. Kueng, J. Preskill et al., “Quantum
advantage in learning from experiments,” Science, vol. 376, no.
6598, pp. 1182–1186, 2022.
[74]
J. van Apeldoorn and A. Gily´en, “Improvements in quantum
sdp-solving with applications,” arXiv preprint arXiv:1804.05058,
2018.

44
[75]
R. Or´us, “A practical introduction to tensor networks: Matrix
product states and projected entangled pair states,” Annals of
physics, vol. 349, pp. 117–158, 2014.
[76]
S. Montangero, E. Montangero, and Evenson, Introduction to
tensor network methods.
Springer, 2018.
[77]
J. Biamonte and V. Bergholm, “Tensor networks in a nutshell,”
arXiv preprint arXiv:1708.00006, 2017.
[78]
J. C. Bridgeman and C. T. Chubb, “Hand-waving and interpretive
dance: an introductory course on tensor networks,” Journal of
physics A: Mathematical and theoretical, vol. 50, no. 22, p. 223001,
2017.
[79]
M. Wang, Y. Pan, X. Yang, G. Li, and Z. Xu, “Tensor networks
meet neural networks: A survey,” arXiv preprint arXiv:2302.09019,
2023.
[80]
H.-M. Rieser, F. K¨oster, and A. P. Raulf, “Tensor networks
for quantum machine learning,” arXiv preprint arXiv:2303.11735,
2023.
[81]
W. Huggins, P. Patil, B. Mitchell, K. B. Whaley, and E. M.
Stoudenmire, “Towards quantum machine learning with tensor
networks,” Quantum Science and technology, vol. 4, no. 2, p. 024001,
2019.
[82]
J. Y. Araz and M. Spannowsky, “Classical versus quantum: com-
paring tensor network-based quantum circuits on lhc data,” arXiv
preprint arXiv:2202.10471, 2022.
[83]
J. Eisert, M. Cramer, and M. B. Plenio, “Colloquium: Area laws
for the entanglement entropy,” Reviews of modern physics, vol. 82,
no. 1, p. 277, 2010.
[84]
M. L. Wall, M. R. Abernathy, and G. Quiroz, “Generative machine
learning with tensor networks: Benchmarks on near-term quan-
tum computers,” Physical Review Research, vol. 3, no. 2, p. 023010,
2021.
[85]
E. Grant, M. Benedetti, S. Cao, A. Hallam, J. Lockhart, V. Stojevic,
A. G. Green, and S. Severini, “Hierarchical quantum classifiers,”
npj Quantum Information, vol. 4, no. 1, p. 65, 2018.
[86]
M. L. Wall and G. D’Aguanno, “Tree-tensor-network classifiers
for machine learning: From quantum inspired to quantum as-
sisted,” Physical Review A, vol. 104, no. 4, p. 042408, 2021.
[87]
J. Dborin, F. Barratt, V. Wimalaweera, L. Wright, and A. G.
Green, “Matrix product state pre-training for quantum machine
learning,” Quantum Science and Technology, vol. 7, no. 3, p. 035014,
2022.
[88]
E. M. Stoudenmire, “Learning relevant features of data with
multi-scale tensor networks,” Quantum Science and Technology,
vol. 3, no. 3, p. 034003, 2018.
[89]
D. Perez-Garcia, F. Verstraete, M. M. Wolf, and J. I. Cirac, “Matrix
product state representations,” arXiv preprint quant-ph/0608197,
2006.
[90]
M. B. Hastings, “An area law for one-dimensional quantum
systems,” Journal of statistical mechanics: theory and experiment, vol.
2007, no. 08, p. P08024, 2007.
[91]
S. Paeckel, T. K¨ohler, A. Swoboda, S. R. Manmana, U. Schollw¨ock,
and C. Hubig, “Time-evolution methods for matrix-product
states,” Annals of Physics, vol. 411, p. 167998, 2019.
[92]
E. M. Stoudenmire and D. J. Schwab, “Supervised learning with
quantum-inspired tensor networks,” 2017.
[93]
B. Scholkopf and A. J. Smola, Learning with kernels: support vector
machines, regularization, optimization, and beyond. MIT press, 2018.
[94]
P. Zhang, Z. Su, L. Zhang, B. Wang, and D. Song, “A quan-
tum many-body wave function inspired language modeling ap-
proach,” in Proceedings of the 27th ACM International Conference on
Information and Knowledge Management, 2018, pp. 1303–1312.
[95]
L. Zhang, P. Zhang, X. Ma, S. Gu, Z. Su, and D. Song, “A
generalized language model in tensor space,” in Proceedings of
the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019,
pp. 7450–7458.
[96]
J. Miller, G. Rabusseau, and J. Terilla, “Tensor networks for
probabilistic sequence modeling,” in International Conference on
Artificial Intelligence and Statistics.
PMLR, 2021, pp. 3079–3087.
[97]
A. Novikov, M. Trofimov, and I. Oseledets, “Exponential ma-
chines,” Bulletin of the Polish Academy of Sciences Technical Sciences,
pp. 789–797, 2018.
[98]
U. Schollw¨ock, “The density-matrix renormalization group in the
age of matrix product states,” Annals of physics, vol. 326, no. 1, pp.
96–192, 2011.
[99]
S. Efthymiou, J. Hidary, and S. Leichenauer, “Tensornetwork for
machine learning,” arXiv preprint arXiv:1906.06329, 2019.
[100] Z.-Z. Sun, C. Peng, D. Liu, S.-J. Ran, and G. Su, “Genera-
tive tensor network classification model for supervised machine
learning,” Physical Review B, vol. 101, no. 7, p. 075135, 2020.
[101] I. Glasser, N. Pancotti, and J. I. Cirac, “From probabilistic graph-
ical models to generalized tensor networks for supervised learn-
ing,” IEEE Access, vol. 8, pp. 68 169–68 182, 2020.
[102] J. Y. Araz and M. Spannowsky, “Quantum-inspired event recon-
struction with tensor networks: Matrix product states,” Journal of
High Energy Physics, vol. 2021, no. 8, pp. 1–28, 2021.
[103] N. Zettili, “Quantum mechanics: concepts and applications,”
2009.
[104] Z.-Y. Han, J. Wang, H. Fan, L. Wang, and P. Zhang, “Unsuper-
vised generative modeling using matrix product states,” Physical
Review X, vol. 8, no. 3, p. 031012, 2018.
[105] J. Alcazar, V. Leyton-Ortega, and A. Perdomo-Ortiz, “Classical
versus quantum models in machine learning: insights from a
finance application,” Machine Learning: Science and Technology,
vol. 1, no. 3, p. 035003, 2020.
[106] J. Stokes and J. Terilla, “Probabilistic modeling with matrix prod-
uct states,” Entropy, vol. 21, no. 12, p. 1236, 2019.
[107] T.-D. Bradley, E. M. Stoudenmire, and J. Terilla, “Modeling se-
quences with quantum states: a look under the hood,” Machine
Learning: Science and Technology, vol. 1, no. 3, p. 035008, 2020.
[108] Y.-Y. Shi, L.-M. Duan, and G. Vidal, “Classical simulation of quan-
tum many-body systems with a tree tensor network,” Physical
review a, vol. 74, no. 2, p. 022320, 2006.
[109] G. Vidal, “Entanglement renormalization,” Physical review letters,
vol. 99, no. 22, p. 220405, 2007.
[110] S. Cheng, L. Wang, T. Xiang, and P. Zhang, “Tree tensor networks
for generative modeling,” Physical Review B, vol. 99, no. 15, p.
155131, 2019.
[111] D. Liu, S.-J. Ran, P. Wittek, C. Peng, R. B. Garc´ıa, G. Su, and
M. Lewenstein, “Machine learning by unitary tensor network of
hierarchical tree structure,” New Journal of Physics, vol. 21, no. 7,
p. 073059, 2019.
[112] Y.
LeCun,
C.
Cortes,
and
C.
Burges,
“Mnist
hand-
written
digit
database,”
ATT
Labs
[Online].
Available:
http://yann.lecun.com/exdb/mnist, vol. 2, 2010.
[113] F. Verstraete and J. I. Cirac, “Renormalization algorithms for
quantum-many body systems in two and higher dimensions,”
arXiv preprint cond-mat/0407066, 2004.
[114] J. I. Cirac, D. Perez-Garcia, N. Schuch, and F. Verstraete, “Matrix
product states and projected entangled pair states: Concepts,
symmetries, theorems,” Reviews of Modern Physics, vol. 93, no. 4,
p. 045003, 2021.
[115] S. Cheng, L. Wang, and P. Zhang, “Supervised learning with
projected entangled pair states,” Physical Review B, vol. 103,
no. 12, p. 125117, 2021.
[116] T. Vieijra, L. Vanderstraeten, and F. Verstraete, “Generative
modeling with projected entangled-pair states,” arXiv preprint
arXiv:2202.08177, 2022.
[117] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image
dataset for benchmarking machine learning algorithms,” arXiv
preprint arXiv:1708.07747, 2017.
[118] C. Hubig, I. McCulloch, and U. Schollw¨ock, “Generic construc-
tion of efficient matrix product operators,” Physical Review B,
vol. 95, no. 3, p. 035129, 2017.
[119] “Tensor network,” tensornetwork.org, accessed: 2023-07-27.
[120] A. Novikov, D. Podoprikhin, A. Osokin, and D. P. Vetrov, “Ten-
sorizing neural networks,” Advances in neural information process-
ing systems, vol. 28, 2015.
[121] Z.-F. Gao, S. Cheng, R.-Q. He, Z.-Y. Xie, H.-H. Zhao, Z.-Y. Lu, and
T. Xiang, “Compressing deep neural networks by matrix product
operators,” Physical Review Research, vol. 2, no. 2, p. 023300, 2020.
[122] R. Patel, C.-W. Hsing, S. Sahin, S. S. Jahromi, S. Palmer, S. Sharma,
C. Michel, V. Porte, M. Abid, S. Aubert et al., “Quantum-inspired
tensor neural networks for partial differential equations,” arXiv
preprint arXiv:2208.02235, 2022.
[123] J. Wang, C. Roberts, G. Vidal, and S. Leichenauer, “Anomaly
detection with tensor networks,” arXiv preprint arXiv:2006.02516,
2020.
[124] J. Liu, S. Li, J. Zhang, and P. Zhang, “Tensor networks for unsu-
pervised machine learning,” Physical Review E, vol. 107, no. 1, p.
L012103, 2023.
[125] R. Selvan and E. B. Dam, “Tensor networks for medical image
classification,” in Medical Imaging with Deep Learning.
PMLR,
2020, pp. 721–732.

45
[126] D. Konar, S. Bhattacharyya, T. K. Gandhi, B. K. Panigrahi, and
R. Jiang, “3-d quantum-inspired self-supervised tensor network
for volumetric segmentation of medical images,” IEEE Transac-
tions on Neural Networks and Learning Systems, 2023.
[127] Y. Liu, X. Zhang, M. Lewenstein, and S.-J. Ran, “Entanglement-
guided architectures of machine learning by quantum tensor
network,” arXiv preprint arXiv:1803.09111, 2018.
[128] I. Convy, W. Huggins, H. Liao, and K. B. Whaley, “Mutual in-
formation scaling for tensor network machine learning,” Machine
learning: science and technology, vol. 3, no. 1, p. 015017, 2022.
[129] A. Torralba, R. Fergus, and W. T. Freeman, “80 million tiny
images: A large data set for nonparametric object and scene
recognition,” IEEE transactions on pattern analysis and machine
intelligence, vol. 30, no. 11, pp. 1958–1970, 2008.
[130] M. Hashemizadeh, M. Liu, J. Miller, and G. Rabusseau, “Adap-
tive learning of tensor network structures,” arXiv preprint
arXiv:2008.05437, 2020.
[131] N. Cohen, O. Sharir, and A. Shashua, “On the expressive power
of deep learning: A tensor analysis,” in Conference on learning
theory.
PMLR, 2016, pp. 698–728.
[132] V. Khrulkov, A. Novikov, and I. Oseledets, “Expressive power of
recurrent neural networks,” arXiv preprint arXiv:1711.00811, 2017.
[133] X. Gao and L.-M. Duan, “Efficient representation of quantum
many-body states with deep neural networks,” Nature commu-
nications, vol. 8, no. 1, p. 662, 2017.
[134] J. Chen, S. Cheng, H. Xie, L. Wang, and T. Xiang, “Equivalence
of restricted boltzmann machines and tensor network states,”
Physical Review B, vol. 97, no. 8, p. 085104, 2018.
[135] Y. Levine, O. Sharir, N. Cohen, and A. Shashua, “Quantum en-
tanglement in deep learning architectures,” Physical review letters,
vol. 122, no. 6, p. 065301, 2019.
[136] I. M. Georgescu, S. Ashhab, and F. Nori, “Quantum simulation,”
Reviews of Modern Physics, vol. 86, no. 1, p. 153, 2014.
[137] A. J. Daley, I. Bloch, C. Kokail, S. Flannigan, N. Pearson,
M. Troyer, and P. Zoller, “Practical quantum advantage in quan-
tum simulation,” Nature, vol. 607, no. 7920, pp. 667–676, 2022.
[138] Y. Gujju, A. Matsuo, and R. Raymond, “Quantum machine learn-
ing on near-term quantum devices: Current state of supervised
and unsupervised techniques for real-world applications,” arXiv
preprint arXiv:2307.00908, 2023.
[139] Y. Zhang and Q. Ni, “Recent advances in quantum machine
learning,” Quantum Engineering, vol. 2, no. 1, p. e34, 2020.
[140] M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo,
K. Fujii, J. R. McClean, K. Mitarai, X. Yuan, L. Cincio et al.,
“Variational quantum algorithms,” Nature Reviews Physics, vol. 3,
no. 9, pp. 625–644, 2021.
[141] G. De Luca, “A survey of nisq era hybrid quantum-classical
machine learning research,” Journal of Artificial Intelligence and
Technology, vol. 2, no. 1, pp. 9–15, 2022.
[142] D. P. Garc´ıa, J. Cruz-Benito, and F. J. Garc´ıa-Pe˜nalvo, “Systematic
literature review: Quantum machine learning and its applica-
tions,” arXiv preprint arXiv:2201.04093, 2022.
[143] J. Preskill, “Quantum computing and the entanglement frontier,”
arXiv preprint arXiv:1203.5813, 2012.
[144] Y. Zhou, E. M. Stoudenmire, and X. Waintal, “What limits the
simulation of quantum computers?” Physical Review X, vol. 10,
no. 4, p. 041038, 2020.
[145] X.-C. Wu, S. Di, E. M. Dasgupta, F. Cappello, H. Finkel, Y. Alex-
eev, and F. T. Chong, “Full-state quantum circuit simulation
by using data compression,” in Proceedings of the International
Conference for High Performance Computing, Networking, Storage and
Analysis, 2019, pp. 1–24.
[146] Z.-Y. Chen, Q. Zhou, C. Xue, X. Yang, G.-C. Guo, and G.-P. Guo,
“64-qubit quantum circuit simulation,” Science Bulletin, vol. 63,
no. 15, pp. 964–971, 2018.
[147] F. Pan and P. Zhang, “Simulation of quantum circuits using the
big-batch tensor network method,” Physical Review Letters, vol.
128, no. 3, p. 030501, 2022.
[148] X. Xu, S. Benjamin, J. Sun, X. Yuan, and P. Zhang, “A herculean
task: Classical simulation of quantum computers,” arXiv preprint
arXiv:2302.08880, 2023.
[149] M. H. Amin, E. Andriyash, J. Rolfe, B. Kulchytskyy, and R. Melko,
“Quantum boltzmann machine,” Physical Review X, vol. 8, no. 2,
p. 021050, 2018.
[150] V. Havl´ıˇcek, A. D. C´orcoles, K. Temme, A. W. Harrow, A. Kan-
dala, J. M. Chow, and J. M. Gambetta, “Supervised learning with
quantum-enhanced feature spaces,” Nature, vol. 567, no. 7747, pp.
209–212, 2019.
[151] R. D. M. Sim˜oes, P. Huber, N. Meier, N. Smailov, R. M. F¨uchslin,
and K. Stockinger, “Experimental evaluation of quantum ma-
chine learning algorithms,” IEEE Access, vol. 11, pp. 6197–6208,
2023.
[152] E. Payares and J. C. Mart´ınez-Santos, “Quantum machine learn-
ing for intrusion detection of distributed denial of service attacks:
a comparative overview,” Quantum Computing, Communication,
and Simulation, vol. 11699, pp. 35–43, 2021.
[153] M. Masum, M. Nazim, M. J. H. Faruk, H. Shahriar, M. Valero,
M. A. H. Khan, G. Uddin, S. Barzanjeh, E. Saglamyurek, A. Rah-
man et al., “Quantum machine learning for software supply chain
attacks: How far can we go?” in 2022 IEEE 46th Annual Computers,
Software, and Applications Conference (COMPSAC). IEEE, 2022, pp.
530–538.
[154] M. Benedetti, E. Lloyd, S. Sack, and M. Fiorentini, “Parameterized
quantum circuits as machine learning models,” Quantum Science
and Technology, vol. 4, no. 4, p. 043001, 2019.
[155] M. Schuld, A. Bocharov, K. M. Svore, and N. Wiebe, “Circuit-
centric quantum classifiers,” Physical Review A, vol. 101, no. 3, p.
032308, 2020.
[156] M. Islam, M. Chowdhury, Z. Khan, and S. M. Khan, “Hybrid
quantum-classical neural network for cloud-supported in-vehicle
cyberattack detection,” IEEE Sensors Letters, vol. 6, no. 4, pp. 1–4,
2022.
[157] D. Emmanoulopoulos and S. Dimoska, “Quantum machine
learning in finance: Time series forecasting,” arXiv preprint
arXiv:2202.00599, 2022.
[158] K. Terashi, M. Kaneda, T. Kishimoto, M. Saito, R. Sawada, and
J. Tanaka, “Event classification with quantum machine learning
in high-energy physics,” Computing and Software for Big Science,
vol. 5, pp. 1–11, 2021.
[159] A. Gianelle, P. Koppenburg, D. Lucchesi, D. Nicotra, E. Ro-
drigues, L. Sestini, J. de Vries, and D. Zuliani, “Quantum machine
learning for b-jet charge identification,” Journal of High Energy
Physics, vol. 2022, no. 8, pp. 1–24, 2022.
[160] A. Blance and M. Spannowsky, “Quantum machine learning for
particle physics using a variational quantum classifier,” Journal of
High Energy Physics, vol. 2021, no. 2, pp. 1–20, 2021.
[161] J. Stokes, J. Izaac, N. Killoran, and G. Carleo, “Quantum natural
gradient,” Quantum, vol. 4, p. 269, 2020.
[162] I. Cong, S. Choi, and M. D. Lukin, “Quantum convolutional
neural networks,” Nature Physics, vol. 15, no. 12, pp. 1273–1278,
2019.
[163] Y. Li, R.-G. Zhou, R. Xu, J. Luo, and W. Hu, “A quantum deep
convolutional neural network for image recognition,” Quantum
Science and Technology, vol. 5, no. 4, p. 044003, 2020.
[164] M. Henderson, S. Shakya, S. Pradhan, and T. Cook, “Quan-
volutional neural networks: powering image recognition with
quantum circuits,” Quantum Machine Intelligence, vol. 2, no. 1,
p. 2, 2020.
[165] T. Hur, L. Kim, and D. K. Park, “Quantum convolutional neural
network for classical data classification,” Quantum Machine Intel-
ligence, vol. 4, no. 1, p. 3, 2022.
[166] J. Romero, J. P. Olson, and A. Aspuru-Guzik, “Quantum au-
toencoders for efficient compression of quantum data,” Quantum
Science and Technology, vol. 2, no. 4, p. 045001, 2017.
[167] M. Srikumar, C. D. Hill, and L. C. Hollenberg, “Clustering and
enhanced classification using a hybrid quantum autoencoder,”
Quantum Science and Technology, vol. 7, no. 1, p. 015020, 2021.
[168] S. Lloyd and C. Weedbrook, “Quantum generative adversarial
learning,” Physical review letters, vol. 121, no. 4, p. 040502, 2018.
[169] P.-L. Dallaire-Demers and N. Killoran, “Quantum generative
adversarial networks,” Physical Review A, vol. 98, no. 1, p. 012324,
2018.
[170] J. Zeng, Y. Wu, J.-G. Liu, L. Wang, and J. Hu, “Learning and
inference on generative adversarial quantum circuits,” Physical
Review A, vol. 99, no. 5, p. 052306, 2019.
[171] B. Coecke, M. Sadrzadeh, and S. Clark, “Mathematical founda-
tions for a compositional distributional model of meaning,” arXiv
preprint arXiv:1003.4394, 2010.
[172] W. Zeng and B. Coecke, “Quantum algorithms for compositional
natural language processing,” arXiv preprint arXiv:1608.01406,
2016.

46
[173] B. Coecke, G. de Felice, K. Meichanetzidis, and A. Toumi, “Foun-
dations for near-term quantum natural language processing,”
arXiv preprint arXiv:2012.03755, 2020.
[174] K. Meichanetzidis, S. Gogioso, G. De Felice, N. Chiappori,
A. Toumi, and B. Coecke, “Quantum natural language pro-
cessing
on
near-term
quantum
computers,”
arXiv
preprint
arXiv:2005.04147, 2020.
[175] D. Kartsaklis, I. Fan, R. Yeung, A. Pearson, R. Lorenz, A. Toumi,
G. de Felice, K. Meichanetzidis, S. Clark, and B. Coecke, “lambeq:
An efficient high-level python library for quantum nlp,” arXiv
preprint arXiv:2110.04236, 2021.
[176] R. Lorenz, A. Pearson, K. Meichanetzidis, D. Kartsaklis, and
B. Coecke, “Qnlp in practice: Running compositional models of
meaning on a quantum computer,” Journal of Artificial Intelligence
Research, vol. 76, pp. 1305–1342, 2023.
[177] G. Li, X. Zhao, and X. Wang, “Quantum self-attention neural
networks for text classification,” arXiv preprint arXiv:2205.05625,
2022.
[178] G. Sergioli, E. Santucci, L. Didaci, J. A. Miszczak, and R. Giuntini,
“A quantum-inspired version of the nearest mean classifier,” Soft
Computing, vol. 22, pp. 691–705, 2018.
[179] G. Sergioli, “Quantum and quantum-like machine learning: a
note on differences and similarities,” Soft Computing, vol. 24,
no. 14, pp. 10 247–10 255, 2020.
[180] G. Sergioli, G. M. Bosyk, E. Santucci, and R. Giuntini, “A
quantum-inspired version of the classification problem,” Inter-
national Journal of Theoretical Physics, vol. 56, pp. 3880–3888, 2017.
[181] G. Sergioli, R. Giuntini, and H. Freytes, “A new quantum
approach to binary classification,” PloS one, vol. 14, no. 5, p.
e0216224, 2019.
[182] R. Giuntini, F. Holik, D. K. Park, H. Freytes, C. Blank, and
G. Sergioli, “Quantum-inspired algorithm for direct multi-class
classification,” Applied Soft Computing, vol. 134, p. 109956, 2023.
[183] G. Sergioli, G. Russo, E. Santucci, A. Stefano, S. E. Torrisi,
S. Palmucci, C. Vancheri, and R. Giuntini, “Quantum-inspired
minimum distance classification in a biomedical context,” Inter-
national Journal of Quantum Information, vol. 16, no. 08, p. 1840011,
2018.
[184] G. Sergioli, C. Militello, L. Rundo, L. Minafra, F. Torrisi, G. Russo,
K. L. Chow, and R. Giuntini, “A quantum-inspired classifier for
clonogenic assay evaluations,” Scientific Reports, vol. 11, no. 1, p.
2830, 2021.
[185] R. Leporini and D. Pastorello, “An efficient geometric approach
to quantum-inspired classifications,” Scientific Reports, vol. 12,
no. 1, p. 8781, 2022.
[186] C. Bertini and R. Leporini, “Quantum-inspired applications for
classification problems,” Entropy, vol. 25, no. 3, p. 404, 2023.
[187] P. Zhang, J. Niu, Z. Su, B. Wang, L. Ma, and D. Song, “End-to-
end quantum-like language models with application to question
answering,” in Proceedings of the AAAI Conference on Artificial
Intelligence, vol. 32, no. 1, 2018.
[188] Q. Li, D. Gkoumas, C. Lioma, and M. Melucci, “Quantum-
inspired multimodal fusion for video sentiment analysis,” Infor-
mation Fusion, vol. 65, pp. 58–71, 2021.
[189] Y. Zhang, D. Song, P. Zhang, P. Wang, J. Li, X. Li, and B. Wang, “A
quantum-inspired multimodal sentiment analysis framework,”
Theoretical Computer Science, vol. 752, pp. 21–40, 2018.
[190] Y. Zhang, D. Song, P. Zhang, X. Li, and P. Wang, “A quantum-
inspired sentiment representation model for twitter sentiment
analysis,” Applied Intelligence, vol. 49, pp. 3093–3108, 2019.
[191] Y. Zhang, D. Song, X. Li, P. Zhang, P. Wang, L. Rong, G. Yu,
and B. Wang, “A quantum-like multimodal network framework
for modeling interaction dynamics in multiparty conversational
sentiment analysis,” Information Fusion, vol. 62, pp. 14–31, 2020.
[192] Q. Li, B. Wang, and M. Melucci, “Cnm: An interpretable complex-
valued network for matching,” arXiv preprint arXiv:1904.05298,
2019.
[193] D. G. Lowe, “Distinctive image features from scale-invariant
keypoints,” International journal of computer vision, vol. 60, pp. 91–
110, 2004.
[194] Q. Li, D. Gkoumas, A. Sordoni, J.-Y. Nie, and M. Melucci,
“Quantum-inspired neural network for conversational emotion
recognition,” in Proceedings of the AAAI Conference on Artificial
Intelligence, vol. 35, no. 15, 2021, pp. 13 270–13 278.
[195] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vec-
tors for word representation,” in Proceedings of the 2014 conference
on empirical methods in natural language processing (EMNLP), 2014,
pp. 1532–1543.
[196] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” arXiv preprint arXiv:1810.04805, 2018.
[197] J. Shi, Z. Li, W. Lai, F. Li, R. Shi, Y. Feng, and S. Zhang, “Two
end-to-end quantum-inspired deep neural networks for text clas-
sification,” IEEE Transactions on Knowledge and Data Engineering,
2021.
[198] M. Altaisky, “Quantum neural network,” arXiv preprint quant-
ph/0107012, 2001.
[199] R. Zhou, L. Qin, and N. Jiang, “Quantum perceptron network,”
in Artificial Neural Networks–ICANN 2006: 16th International Con-
ference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16.
Springer, 2006, pp. 651–657.
[200] M. Schuld, I. Sinayskiy, and F. Petruccione, “The quest for a quan-
tum neural network,” Quantum Information Processing, vol. 13, pp.
2567–2586, 2014.
[201] O. P. Patel, N. Bharill, A. Tiwari, and M. Prasad, “A novel
quantum-inspired fuzzy based neural network for data classifi-
cation,” IEEE Transactions on Emerging Topics in Computing, vol. 9,
no. 2, pp. 1031–1044, 2019.
[202] A. Sagheer, M. Zidan, and M. M. Abdelsamea, “A novel au-
tonomous perceptron model for pattern classification applica-
tions,” Entropy, vol. 21, no. 8, p. 763, 2019.
[203] K. Markelle, R. Longjohn, and K. Nottingham, “Uci machine
learning repository.” [Online]. Available: http://archive.ics.uci.
edu/ml
[204] D. Konar, S. Bhattacharyya, T. K. Gandhi, and B. K. Panigrahi,
“A quantum-inspired self-supervised network model for auto-
matic segmentation of brain mr images,” Applied Soft Computing,
vol. 93, p. 106348, 2020.
[205] J. Zhang, Z. Li, J. Wang, Y. Wang, S. Hu, J. Xiao, and Z. Li,
“Quantum entanglement inspired correlation learning for clas-
sification,” in Advances in Knowledge Discovery and Data Mining:
26th Pacific-Asia Conference, PAKDD 2022, Chengdu, China, May
16–19, 2022, Proceedings, Part II.
Springer, 2022, pp. 58–70.
[206] P. Tiwari and M. Melucci, “Towards a quantum-inspired binary
classifier,” IEEE Access, vol. 7, pp. 42 354–42 372, 2019.
[207] ——, “Towards a quantum-inspired framework for binary classi-
fication,” in Proceedings of the 27th ACM international conference on
information and knowledge management, 2018, pp. 1815–1818.
[208] J. Zhang, Z. Li, R. He, J. Zhang, B. Wang, Z. Li, and T. Niu,
“Interactive quantum classifier inspired by quantum open system
theory,” in 2021 International Joint Conference on Neural Networks
(IJCNN).
IEEE, 2021, pp. 1–7.
[209] P. Tiwari and M. Melucci, “Binary classifier inspired by quantum
theory,” in Proceedings of the AAAI conference on artificial intelli-
gence, vol. 33, no. 01, 2019, pp. 10 051–10 052.
[210] R. Selvan, S. Ørting, and E. B. Dam, “Locally orderless tensor
networks for classifying two-and three-dimensional medical im-
ages,” arXiv preprint arXiv:2009.12280, 2020.
[211] V. Azevedo, C. Silva, and I. Dutra, “Quantum transfer learning
for breast cancer detection,” Quantum Machine Intelligence, vol. 4,
no. 1, p. 5, 2022.
[212] M. Esposito, G. Uehara, and A. Spanias, “Quantum machine
learning for audio classification with applications to healthcare,”
in 2022 13th International Conference on Information, Intelligence,
Systems & Applications (IISA).
IEEE, 2022, pp. 1–4.
[213] T. Sakuma, “Application of deep quantum neural networks to
finance,” arXiv preprint arXiv:2011.07319, 2020.
[214] J. M. Arrazola, A. Delgado, B. R. Bardhan, and S. Lloyd,
“Quantum-inspired
algorithms
in
practice,”
arXiv
preprint
arXiv:1905.10415, 2019.
[215] S. Mugel, C. Kuchkovsky, E. Sanchez, S. Fernandez-Lorenzo,
J. Luis-Hita, E. Lizaso, and R. Orus, “Dynamic portfolio optimiza-
tion with real datasets using quantum processors and quantum-
inspired tensor networks,” Physical Review Research, vol. 4, no. 1,
p. 013006, 2022.
[216] S. L. Wu, J. Chan, W. Guan, S. Sun, A. Wang, C. Zhou, M. Livny,
F. Carminati, A. Di Meglio, A. C. Li et al., “Application of
quantum machine learning using the quantum variational classi-
fier method to high energy physics analysis at the lhc on ibm
quantum computer simulator and hardware with 10 qubits,”
Journal of Physics G: Nuclear and Particle Physics, vol. 48, no. 12,
p. 125003, 2021.

47
[217] S. L. Wu, S. Sun, W. Guan, C. Zhou, J. Chan, C. L. Cheng, T. Pham,
Y. Qian, A. Z. Wang, R. Zhang et al., “Application of quantum
machine learning using the quantum kernel algorithm on high
energy physics analysis at the lhc,” Physical Review Research,
vol. 3, no. 3, p. 033221, 2021.
[218] V. S. Ngairangbam, M. Spannowsky, and M. Takeuchi, “Anomaly
detection in high-energy physics using a quantum autoencoder,”
Physical Review D, vol. 105, no. 9, p. 095004, 2022.
[219] H. Suryotrisongko and Y. Musashi, “Evaluating hybrid quantum-
classical deep learning for cybersecurity botnet dga detection,”
Procedia Computer Science, vol. 197, pp. 223–229, 2022.
[220] C. Gong, W. Guan, A. Gani, and H. Qi, “Network attack detection
scheme based on variational quantum neural network,” The
Journal of Supercomputing, vol. 78, no. 15, pp. 16 876–16 897, 2022.
[221] D. Herr, B. Obert, and M. Rosenkranz, “Anomaly detection with
variational quantum generative adversarial networks,” Quantum
Science and Technology, vol. 6, no. 4, p. 045004, 2021.
[222] C.-H. H. Yang, J. Qi, S. Y.-C. Chen, P.-Y. Chen, S. M. Siniscalchi,
X. Ma, and C.-H. Lee, “Decentralizing feature extraction with
quantum convolutional neural network for automatic speech
recognition,” in ICASSP 2021-2021 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2021,
pp. 6523–6527.
[223] J. J. Garc´ıa-Ripoll, “Quantum-inspired algorithms for multivari-
ate analysis: from interpolation to partial differential equations,”
Quantum, vol. 5, p. 431, 2021.
[224] S. An, M. Lee, S. Park, H. Yang, and J. So, “An ensemble of
simple convolutional neural network models for mnist digit
recognition,” 2020.
[225] M. S. Tanveer, M. U. K. Khan, and C.-M. Kyung, “Fine-tuning
darts for image classification,” in 2020 25th International Confer-
ence on Pattern Recognition (ICPR).
IEEE, 2021, pp. 4789–4796.
[226] S. Sinha and A. B. Dieng, “Consistency regularization for vari-
ational auto-encoders,” Advances in Neural Information Processing
Systems, vol. 34, pp. 12 943–12 954, 2021.
[227] Google, “Google quantum ai,” 2023, accessed: July 31, 2023.
[Online]. Available: https://quantumai.google/
[228] IBM, “Ibm quantum,” 2021, accessed: August 6, 2023. [Online].
Available: https://quantum-computing.ibm.com/
[229] A. W. Services, “Amazon braket,” 2020, accessed: August 6,
2023. [Online]. Available: https://aws.amazon.com/braket/
[230] T. Schlegl, P. Seeb¨ock, S. M. Waldstein, U. Schmidt-Erfurth, and
G. Langs, “Unsupervised anomaly detection with generative
adversarial networks to guide marker discovery,” in International
conference on information processing in medical imaging.
Springer,
2017, pp. 146–157.
[231] R. S. Olson, W. La Cava, P. Orzechowski, R. J. Urbanowicz,
and
J.
H.
Moore,
“Pmlb:
a
large
benchmark
suite
for
machine learning evaluation and comparison,” BioData Mining,
vol.
10,
no.
1,
p.
36,
Dec
2017.
[Online].
Available:
https://doi.org/10.1186/s13040-017-0154-4
[232] G. Evenbly, “Number-state preserving tensor networks as classi-
fiers for supervised learning,” Frontiers in Physics, vol. 10, p. 1146,
2022.
[233] S.
Aaronson
and
L.
Chen,
“Complexity-theoretic
founda-
tions
of
quantum
supremacy
experiments,”
arXiv
preprint
arXiv:1612.05903, 2016.
[234] I. Kerenidis, J. Landman, A. Luongo, and A. Prakash, “q-means:
A quantum algorithm for unsupervised machine learning,” Ad-
vances in neural information processing systems, vol. 32, 2019.
[235] R. Di Sipio, J.-H. Huang, S. Y.-C. Chen, S. Mangini, and M. Wor-
ring, “The dawn of quantum natural language processing,”
in ICASSP 2022-2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8612–8616.
[236] A. Cichocki, N. Lee, I. V. Oseledets, A.-H. Phan, Q. Zhao, and
D. Mandic, “Low-rank tensor networks for dimensionality re-
duction and large-scale optimization problems: Perspectives and
challenges part 1,” arXiv preprint arXiv:1609.00893, 2016.
[237] I. Glasser, R. Sweke, N. Pancotti, J. Eisert, and I. Cirac, “Ex-
pressive power of tensor-network factorizations for probabilis-
tic modeling,” Advances in neural information processing systems,
vol. 32, 2019.
[238] G. Carleo, Y. Nomura, and M. Imada, “Constructing exact rep-
resentations of quantum many-body systems with deep neural
networks,” Nature communications, vol. 9, no. 1, p. 5322, 2018.
[239] Z. Cai and J. Liu, “Approximating quantum many-body wave
functions using artificial neural networks,” Physical Review B,
vol. 97, no. 3, p. 035116, 2018.
[240] K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii, “Quantum
circuit learning,” Physical Review A, vol. 98, no. 3, p. 032309, 2018.
[241] M. Lazzarin, D. E. Galli, and E. Prati, “Multi-class quantum
classifiers with tensor network circuits for quantum phase recog-
nition,” Physics Letters A, vol. 434, p. 128056, 2022.
[242] J.-G. Liu and L. Wang, “Differentiable learning of quantum circuit
born machines,” Physical Review A, vol. 98, no. 6, p. 062324, 2018.
[243] L.-H. Gong, L.-Z. Xiang, S.-H. Liu, and N.-R. Zhou, “Born ma-
chine model based on matrix product state quantum circuit,”
Physica A: Statistical Mechanics and its Applications, vol. 593, p.
126907, 2022.
[244] D. Wecker and K. M. Svore, “LIQUi—¿: A Software Design
Architecture
and
Domain-Specific
Language
for
Quantum
Computing,” 2014. [Online]. Available: arXiv:1402.4467v1
[245] R. N. Pfeifer, G. Evenbly, S. Singh, and G. Vidal, “Ncon: A tensor
network contractor for matlab,” arXiv preprint arXiv:1402.0939,
2014.
[246] S. V. Dolgov, B. N. Khoromskij, I. V. Oseledets, and D. V.
Savostyanov, “Computation of extreme eigenvalues in higher
dimensions using block tensor train format,” Computer Physics
Communications, vol. 185, no. 4, pp. 1207–1216, 2014.
[247] J. Kossaifi, Y. Panagakis, A. Anandkumar, and M. Pantic, “Ten-
sorly: Tensor learning in python,” arXiv preprint arXiv:1610.09555,
2016.
[248] C.
Developers,
“Cirq,”
2023.
[Online].
Available:
https:
//doi.org/10.5281/zenodo.8161252
[249] P. Gel, M. L¨ucke, T. Bake, F. N¨uske, and M. Scherer, “Scikit-tt
tensor train toolbox,” https://github.com/PGelss/scikit tt, 2018.
[250] C. Roberts, A. Milsted, M. Ganahl, A. Zalcman, B. Fontaine,
Y. Zou, J. Hidary, G. Vidal, and S. Leichenauer, “Tensornetwork:
A library for physics and machine learning,” 2019.
[251] J. Miller, “Torchmps,” https://github.com/jemisjoky/torchmps,
2019.
[252] G. G. Guerreschi, J. Hogaboam, F. Baruffa, and N. P. Sawaya,
“Intel quantum simulator: A cloud-ready high-performance sim-
ulator of quantum circuits,” Quantum Science and Technology,
vol. 5, no. 3, p. 034007, 2020.
[253] G. Torlai and M. Fishman, “PastaQ: A package for simulation,
tomography
and
analysis
of
quantum
computers,”
2020.
[Online]. Available: https://github.com/GTorlai/PastaQ.jl/
[254] Y. Suzuki, Y. Kawase, Y. Masumura, Y. Hiraga, M. Nakadai,
J. Chen, K. M. Nakanishi, K. Mitarai, R. Imai, S. Tamiya et al.,
“Qulacs: a fast and versatile quantum circuit simulator for re-
search purpose,” Quantum, vol. 5, p. 559, 2021.
[255] M. Broughton, G. Verdon, T. McCourt, A. J. Martinez, J. H.
Yoo, S. V. Isakov, P. Massey, R. Halavati, M. Y. Niu, A. Zlokapa
et al., “Tensorflow quantum: A software framework for quantum
machine learning,” arXiv preprint arXiv:2003.02989, 2020.
[256] S. Efthymiou, S. Ramos-Calderer, C. Bravo-Prieto, A. P´erez-
Salinas, D. Garc´ıa-Mart´ın, A. Garcia-Saez, J. I. Latorre, and
S. Carrazza, “Qibo: a framework for quantum simulation
with hardware acceleration,” Quantum Science and Technology,
vol.
7,
no.
1,
p.
015018,
dec
2021.
[Online].
Available:
https://doi.org/10.1088/2058-9565/ac39f5
[257] M. Fishman, S. White, and E. Stoudenmire, “The itensor software
library for tensor network calculations,” SciPost Physics Codebases,
p. 004, 2022.
[258] M. Usvyatsov, R. Ballester-Ripoll, and K. Schindler, “tntorch:
Tensor network learning with PyTorch,” Journal of Machine
Learning Research, vol. 23, no. 208, pp. 1–6, 2022. [Online].
Available: http://jmlr.org/papers/v23/21-1197.html
[259] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers,
P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J.
Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk,
M. Brett, A. Haldane, J. F. del R´ıo, M. Wiebe, P. Peterson,
P. G´erard-Marchant, K. Sheppard, T. Reddy, W. Weckesser,
H. Abbasi, C. Gohlke, and T. E. Oliphant, “Array programming
with NumPy,” Nature, vol. 585, no. 7825, pp. 357–362, Sep. 2020.
[Online]. Available: https://doi.org/10.1038/s41586-020-2649-2
[260] C. Psarras, L. Karlsson, J. Li, and P. Bientinesi, “The land-
scape of software for tensor computations,” arXiv preprint
arXiv:2103.13756, 2021.

48
[261] Microsoft,
“Azure
quantum,”
2023,
accessed:
August
6,
2023. [Online]. Available: https://azure.microsoft.com/en-au/
products/quantum
[262] M. Sao, H. Watanabe, Y. Musha, and A. Utsunomiya, “Applica-
tion of digital annealer for faster combinatorial optimization,”
Fujitsu Scientific and Technical Journal, vol. 55, no. 2, pp. 45–51,
2019.
[263] K. Komatsu, A. Onodera, E. Focht, S. Fujimoto, Y. Isobe, S. Mo-
mose, M. Sato, and H. Kobayashi, “Performance and power
analysis of a vector computing system,” Supercomputing Frontiers
and Innovations, vol. 8, no. 2, pp. 75–94, 2021.
[264] F. Hu, B.-N. Wang, N. Wang, and C. Wang, “Quantum machine
learning with d-wave quantum computer,” Quantum Engineering,
vol. 1, no. 2, p. e12, 2019.
[265] A. F. Kockum, Quantum optics with artificial atoms.
Chalmers
Tekniska Hogskola (Sweden), 2014.
[266] C. M. Dawson and M. A. Nielsen, “The solovay-kitaev algo-
rithm,” arXiv preprint quant-ph/0505030, 2005.
[267] R. Iten, R. Colbeck, I. Kukuljan, J. Home, and M. Christandl,
“Quantum circuits for isometries,” Physical Review A, vol. 93,
no. 3, p. 032318, 2016.
[268] J. J. Vartiainen, M. M¨ott¨onen, and M. M. Salomaa, “Efficient
decomposition of quantum gates,” Physical review letters, vol. 92,
no. 17, p. 177902, 2004.
[269] X. Glorot and Y. Bengio, “Understanding the difficulty of training
deep feedforward neural networks,” in Proceedings of the thir-
teenth international conference on artificial intelligence and statistics.
JMLR Workshop and Conference Proceedings, 2010, pp. 249–256.
APPENDIX A
BACKGROUND
In this section, we provide an overview of the fundamental
concepts in both quantum computing and machine learning,
focusing on topics essential for understanding QiML. The
reader is not expected to be already equipped with these
concepts; however, a rudimentary understanding of linear
algebra is presumed for ease of comprehension.
A.1
Quantum Computing
A.1.1
Dirac Notation
Dirac notation, also known as bra-ket notation, is a compact
representation commonly used in quantum mechanics and
quantum computing to represent quantum states, as well
as perform calculations involving these states. It has since
become the standard notation for working with quantum
systems.
In Dirac notation, quantum states are represented by
kets, denoted by |ψ⟩, where ψ is the name of the state. A
quantum state |ψ⟩can be represented as a column vector in
a vector space:
|ψ⟩=


ψ0
ψ1
· · ·
ψn


(64)
where ψ0, ψ1, . . . , ψn ∈C; |ψ⟩∈Cn. The dual states, or
bras, are represented as ⟨ψ| and are the complex conjugate
of the transpose of the corresponding kets, denoted by the †
symbol. For example, |ψ⟩∈C2:
⟨ψ| = |ψ⟩† =


ψ0
ψ1


†
=

ψ∗
0 ψ∗
1

,
(65)
where (*) denotes the complex conjugate. In quantum com-
puting, the computational basis is a set of basis states that
forms an orthonormal basis for the state space of a quantum
system. For a single qubit system, the computational basis
consists of two basis states: |0⟩and |1⟩. These basis states
can be represented as column vectors:
|0⟩=


1
0

,
|1⟩=


0
1


(66)
A general single-qubit quantum state can be represented as
a linear combination of these basis states:
|ψ⟩= α|0⟩+ β|1⟩
(67)
where α and β are complex numbers, and the normalization
condition |α|2 + |β|2 = 1 must be satisfied to maintain the
state as a valid quantum state.
For a multi-qubit system with n qubits, the compu-
tational basis is formed by taking the tensor product of
the single-qubit basis states, resulting in 2n basis states.
The basis states for an n-qubit system are represented as
|i1i2 · · · in⟩, where ik ∈0, 1 for each k = 1, 2, . . . , n. For
example, the computational basis for a two-qubit system
consists of the following four basis states:
|00⟩,
|01⟩,
|10⟩,
|11⟩
(68)
A general n-qubit quantum state can be represented as a
linear combination of these computational basis states:
|ψ⟩=
2n−1
X
i=0
αi|i1i2 · · · in⟩
(69)
ik ∈{0, 1}; k = 1, 2, . . . , n.
(70)
The index i can be thought of as the binary representation
i1i2 · · · in. αi are complex coefficients, and the normalization
condition P
n |αi|2 = 1 must be satisfied.
A.1.2
Inner Product
The inner product, or scalar product, of two quantum states
|ψ⟩and |ϕ⟩is denoted as ⟨ψ|ϕ⟩. The inner product can be
calculated as the product of the corresponding bra and ket:
⟨ψ|ϕ⟩=
X
i
ψ∗
i ϕi,
(71)
where ψ∗
i and ϕi are the i-th components of the states |ψ⟩
and |ϕ⟩, respectively. The result of the inner product is a
complex scalar that carries information about the overlap
between the two states. In particular, the squared magnitude
of the inner product, |⟨ψ|ϕ⟩|2, represents the probability that
the state |ϕ⟩will be found in the state |ψ⟩upon measure-
ment.

49
A.1.3
Outer Product
The outer product of two quantum states |ψ⟩and |ϕ⟩,
denoted as |ψ⟩⟨ϕ|, produces a linear operator that acts on
the Hilbert space where the quantum system that defines
|ψ⟩and |ϕ⟩resides. For |ψ⟩∈C1×n and |ϕ⟩∈C1×n, their
outer product is:
|ψ⟩⟨ϕ| =


ψ0ϕ∗
0
ψ0ϕ∗
1
· · ·
ψ0ϕ∗
n−1
ψ1ϕ∗
0
ψ1ϕ∗
1
· · ·
ψ1ϕ∗
n−1
...
...
...
...
ψn−1ϕ∗
0
ψn−1ϕ∗
1
· · ·
ψn−1ϕ∗
n−1


(72)
This linear operator can be used to calculate the pro-
jection of one state onto another. A projection matrix is
formed by taking the outer products of the basis states of
a subspace and summing them up. Given a basis |ui⟩for
an m-dimensional Hilbert space, the projection matrix P is
defined as:
P =
m
X
i=1
|ui⟩⟨ui|
(73)
Each term |ui⟩⟨ui| is an outer product, a “projector”,
and the sum of these outer products forms the projection
matrix P. P inherits the idempotent (P 2 = P) and Hermi-
tian (P † = P) properties from the outer products. When
applied to a vector |ψ⟩, the projection matrix P results in
the orthogonal projection |ϕ⟩onto the subspace spanned by
the basis states:
|ϕ⟩= P|ψ⟩=
m
X
i=1
|ui⟩⟨ui|ψ⟩
(74)
These projection matrices are essential in performing
measurement.
The outer product also allows for the convenient repre-
sentation of quantum gates and transformations. For exam-
ple, consider the unitary matrix ( 0 1
1 0 ) (The Pauli-X gate σx).
This matrix can be considered as sum of its constituent rank
1 matrices, expressed using outer products:
|0⟩⟨1| + |1⟩⟨0| =


1
0



0
1

+


0
1



1
0

=


0
1
0
0

+


0
0
1
0


=


0
1
1
0


= σx
(75)
A.1.4
Tensor Product
The tensor product linearly combines two quantum states
|ψ⟩and |ϕ⟩to form a new composite state denoted as |ψ⟩⊗
|ϕ⟩or simply as either |ψ⟩|ϕ⟩or |ψϕ⟩. The tensor product is
used extensively in quantum computing to describe multi-
qubit systems. For instance, the tensor product of two qubits
in states |ψ⟩and |ϕ⟩produces a column vector of length 2n,
written as:
|ψ⟩⊗|ϕ⟩= |ψ⟩|ϕ⟩= |ψϕ⟩
=

α |0⟩+ β |1⟩

⊗

γ |0⟩+ δ |1⟩

= α |00⟩+ β |01⟩+ γ |10⟩+ δ |11⟩
(76)
where |αγ|2+|αδ|2+|βγ|2+|βδ|2 = 1. In general, the tensor
product is:
•
not commutative: |ψϕ⟩̸= |ϕψ⟩
•
associative: (|ψ⟩⊗|ϕ⟩) ⊗|χ⟩= |ψ⟩⊗(|ϕ⟩⊗|χ⟩)
The tensor product can also be extended to operators, which
is essential for when dealing with multi-qubit gates. For
example, given the two single-qubit operators A and B,
their tensor product A ⊗B will act on a two-qubit state
|ψϕ⟩as follows:
(A ⊗B)|ψϕ⟩= (A|ψ⟩) ⊗(B|ϕ⟩).
(77)
A.1.5
Qubits and Quantum States
Quantum systems are represented by quantum states, which
are described by vectors in a complex Hilbert space. The
most basic quantum system is the qubit, the quantum coun-
terpart of the classical bit. A qubit is represented by a linear
combination of basis states |0⟩and |1⟩:
|ψ⟩= α |0⟩+ β |1⟩
(78)
where α, β ∈C, and |0⟩and |1⟩are the vectors [ 1
0 ] and [ 0
1 ]
respectively in the two-dimensional Hilbert space H2. |ψ⟩
satisfies the normalization condition: ⟨ψ|ψ⟩= |α|2 + |β|2 =
1; the coefficients α and β are known as probability am-
plitudes, and their squared magnitudes indicate the prob-
abilities of obtaining the corresponding basis states upon
measurement, i.e., the outcome of a single qubit state is
either 0 with probability |α|2 or 1 with probability |β|2.
Measurement on qubits is most typically performed in
the computational basis, each qubit can be measured with
respect to the standard states |0⟩and |1⟩. Other bases can be
used depending on the specific problem at hand.
When a qubit is measured, the measurement process is
described by a set of projection operators. For the computa-
tional basis, the measurement operators are as follows:
M0 = |0⟩⟨0| =


1
0
0
0

, M1 = |1⟩⟨1| =


0
0
0
1

.
(79)
Given a qubit state |ψ⟩= α |0⟩+ β |1⟩, the probability of
measuring the state |0⟩or |1⟩can be computed using the
respective measurement operator:

50
P0 = ⟨ψ| M0 |ψ⟩= |α|2, P1 = ⟨ψ| M1 |ψ⟩= |β|2.
(80)
That is, the probability of obtaining state 0 is given by
P0 = |α|2 while the probability of obtaining state 1 is
P1 = |β|2. The act of measuring a qubit causes it to collapse
into a single state, and classical information is obtained
in the form of a single classical bit, either 0 or 1. This
process is inherently probabilistic, and the outcome of the
measurement cannot be predicted with certainty; the state
of a qubit cannot be directly observed without disturbing
its state.
The quantum state of the system post-measurement is
also now generally different from its initial state. The post-
measurement state is given by:
|ψm⟩= Mm |ψ⟩
p
p(m) .
(81)
Here, |ψm⟩represents the post-measurement state, Mm are
the measurement operators corresponding to the chosen ba-
sis (e.g., the computational basis), |ψ⟩is the initial quantum
state, and p(m) is the probability of obtaining the mea-
surement outcome m. The normalization factor 1/
p
p(m)
ensures that the post-measurement state remains a valid
quantum state with a total probability of 1.
A.1.6
Qubit Representations
Instead of column vectors, quantum states can also be
represented using polar coordinates. Given a qubit state
|ψ⟩= α |0⟩+ β |1⟩, with α, β ∈C, we can express these
complex coefficients in terms of real numbers as follows:
|ψ⟩= cos θ
2 |0⟩+ eiϕ sin θ
2 |1⟩,
(82)
θ, ϕ ∈R.
(83)
θ and ϕ can be interpreted as spherical coordinates, allowing
the quantum state to be plotted as a point on the surface
of a unit sphere in 3-dimensional space — the Bloch sphere.
θ ∈[0, π] and ϕ ∈[0, 2π] are the polar and azimuthal angles,
respectively. The basis states |0⟩and |1⟩correspond to the
north and south poles of the sphere, while states with equal
probability amplitudes (i.e., equal superpositions) lie on the
equator. This geometric representation offers an intuitive
way for visualizing qubits and their transformations.
A.1.7
Quantum Gates and Circuits
Quantum gates are the quantum equivalent of classical logic
gates, which are used in classical digital circuits to manipu-
late bits. In a quantum computer, qubits are manipulated
using quantum gates to perform quantum algorithms. A
quantum gate is any unitary operator that acts on a single
qubit or a set of qubits, the application of which produces a
modification in the state of those qubits. These gates are
represented as a unitary matrices; a single-qubit gate is
represented by a 2 × 2 unitary matrix, a two-qubit gate is
represented by a 4×4 unitary matrix. An n-qubit gate is thus
represented by a 2n matrix. A unitary matrix is any square
matrix U that satisfies U †U = UU † = I. This condition
ensures that quantum gates are reversible and preserve the
Fig. 10. Bloch sphere representation of the qubit state [265].
norm of the quantum state. Table 7 lists a few basic quantum
gates.
The single qubit Pauli gates correspond to X, Y, and Z
correspond to rotations of the qubit state around the x, y,
and z axes of the Bloch sphere, respectively. Specifically, the
Pauli-X gate corresponds to a rotation of the qubit state by
π radians around the x-axis of the Bloch sphere. It is also
often called the NOT gate, since it performs similarly to the
classical NOT gate; it takes in a basis state |0⟩or |1⟩and
returns the opposite state. The Pauli-Y and Z gates behave
similarly; the Pauli-Y gate corresponds to a rotation of the
qubit state by π around the y-axis of the Bloch sphere while
the Pauli-Z gate corresponds to a rotation of the qubit state
by π around the z-axis of the Bloch sphere. The Pauli gates
are specialised cases of the generic rotation gates (Rx, Ry,
Rz) in π degrees. The Hadamard gate produces an equal
superposition of the |0⟩and |1⟩states. The Phase gate is a
parameterized gate that imparts a phase shift to the |1⟩state
of a qubit while leaving the |0⟩state unchanged, thereby
modifying the relative phase of the qubit’s superposition
without affecting its probability amplitudes. Superposition
is discussed further in Section A.1.8.
The two-qubit gates perform operations on two or more
qubits simultaneously, resulting in entanglement or chang-
ing the state of one qubit based on the state of another. The
CNOT gate terms one of its two qubit inputs as the target,
the other being the control. It performs a NOT operation
on the target qubit if the control qubit is in the state |1⟩. If
the control qubit is in the state |0⟩, the target qubit remains
unchanged. The CNOT matrix presented in Table 7 assumes
the first qubit to be the control. The SWAP gate swaps the
states of two qubits. Essentially, if one qubit is in state |0⟩
and the other is in state |1⟩, the gate will swap their states.
The CZ applies a Z operation on the target qubit if the
control qubit is in the state |1⟩. If the control qubit is in the
state |0⟩, the target qubit remains unchanged. Other single
and multiple qubit gates have also been used for various
purposes.
The notion of a universal quantum gate set enables
the implementation of any quantum operation or circuit.
A universal gate set consists of a collection of quantum
gates, including both single and multi-qubit gates, which

51
TABLE 7
Examples of Basic Quantum Gates
Operator
Gate
Matrix
Single-Qubit Gates
Pauli-X
X
or
X, σx =


0
1
1
0


Pauli-Y
Y
Y, σy =


0
−i
i
0


Pauli-Z
Z
Z, σz =


1
0
0
−1


Hadamard
H
H =
1
√
2


1
1
1
−1


Phase
Gate
P(ϕ)
P(ϕ) =


1
0
0
eiϕ


Two-Qubit Gates
Controlled
NOT
(CNOT)
CX =


1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0


SWAP
Gate
SWAP =


1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1


Controlled
Z
(CZ)
Gate
Z
CZ =


1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
−1


can be assembled and ordered to approximate any unitary
transformation on qubits with arbitrary precision [266].
This property is crucial for harnessing the full potential
of quantum computing, as it facilitates the construction of
any quantum algorithm or circuit using a limited set of
fundamental quantum gates. For example, it is possible to
decompose any single-qubit gate into a sequence of three
rotations around fixed axes, such as rotations around the Z,
Y, and Z axes [267]. Another widely recognized example is
the combination of the CNOT gate (a two-qubit gate) and
=
U
RZ
RY
RZ
Fig. 11. Arbitrary single-qubit gate decomposition into Z, Y, Z rotations
arbitrary single-qubit rotations, which together enable the
creation of any quantum operation with a desired degree of
accuracy [268].
A quantum circuit serves as a blueprint for executing
a series of operations or instructions on a quantum com-
puter. Composed of quantum gates arranged sequentially,
the circuit is interpreted from left to right, with each gate
transforming the state of one or more qubits. Wires connect
these gates, illustrating the flow of quantum information
through the circuit. When no gate is present on a wire, the
qubit state remains unchanged, effectively implying that the
identity operator is acting on the qubit at that point.
A.1.8
Superposition
A key consequence of quantum mechanics is that quantum
states are able to exist in a linear superposition, which is a
fundamental departure from classical physics. A quantum
system can exist in multiple states simultaneously, with
the relative weights of these states determined by complex
probability amplitudes. The ability of quantum systems to
exist in superpositions results in a much higher level of
expressivity in comparison with classical systems.
Superposition can be manipulated using quantum gates
to create more complex states. For example, a Hadamard
gate can be used to put a qubit into an equal superposition
of |0⟩and |1⟩, given by the equation H |0⟩=
1
√
2(|0⟩+ |1⟩).
This is the superposition of the |0⟩and |1⟩states, with
equal probability amplitudes. Other gates, such as the Pauli-
X gate or the phase gate, can also be used to manipulate
the probability amplitudes and relative phase of a qubit
in superposition. In addition, the superposition of multiple
qubits can also be achieved by combining the superposition
of individual qubits. For instance, a two-qubit system can
be in a superposition of the four possible basis states:
|ψ⟩= α00 |00⟩+α01 |01⟩+α10 |10⟩+α11 |11⟩. The probability
amplitudes αij determine the probability of measuring the
corresponding basis state while satisfying the normalisation
condition ⟨ψ|ψ⟩= 1.
A.1.9
Entanglement
Quantum entanglement is a unique phenomenon in which
quantum particles become interconnected and are described
in relation to each other, regardless of their spatial separa-
tion. In a bipartite system, this “interconnectedness” is such
that the state of one particle has an instantaneous effect on
the state of the other particle. A system is said to be in an
entangled state if it cannot be factorized into a product of
individual particle states.
One of the most well-known examples of entangled
states are the Bell states, also known as the EPR pairs,
which are maximally entangled states of two qubits. These
states exhibit perfect correlations, meaning that the result
of measuring one qubit instantly determines the outcome of
measuring the other qubit. There are four distinct Bell states,
which form an orthonormal basis:

52
•
|Φ+⟩=
1
√
2(|00⟩+ |11⟩)
•
|Φ−⟩=
1
√
2(|00⟩−|11⟩)
•
|Ψ+⟩=
1
√
2(|01⟩+ |10⟩)
•
|Ψ−⟩=
1
√
2(|01⟩−|10⟩)
These Bell states can be generated using a simple quan-
tum circuit consisting of a Hadamard gate (H) followed by
a CNOT gate. The circuit diagram for generating the first
Bell state |Φ+⟩is shown in Figure 12. Although this is not
the only way to produce the Bell states, it is the simplest
method since it uses the computational basis as input. The
H gate is applied to the first qubit, while the CNOT gate
uses the first qubit as control and the second qubit as target.
The state |Φ+⟩has a 50% probability of being in either the
|00⟩state or the |11⟩state once measured. After this state is
created and upon measuring the first qubit, if the result is 0,
the second qubit will be immediately known to be 0 due to
the entanglement. Similarly, if the first qubit is measured to
be 1, the second qubit will also be 1. Table 8 illustrates the
transformation of the input states to the corresponding Bell
states after the circuit is applied. The Bell state pairs are es-
sential for several quantum information and communication
protocols, such as quantum teleportation and superdense
coding.
TABLE 8
Generation of Bell states
Input
H
CNOT
Bell State
|00⟩
1
√
2(|00⟩+ |10⟩)
1
√
2(|00⟩+ |11⟩)
|Φ+⟩
|10⟩
1
√
2(|00⟩−|10⟩)
1
√
2(|00⟩−|11⟩)
|Φ−⟩
|01⟩
1
√
2(|01⟩+ |11⟩)
1
√
2(|01⟩+ |10⟩)
|Ψ+⟩
|11⟩
1
√
2(|01⟩−|11⟩)
1
√
2(|01⟩−|10⟩)
|Ψ−⟩
|0⟩
H
|Φ+⟩=
1
√
2(|00⟩+ |11⟩)
|0⟩
Fig. 12. Quantum circuit for generating the Bell state |Φ+⟩
A.1.10
Density Matrices
Quantum state vector representations are best suited for
pure states — states with complete knowledge of their
quantum characteristics. In contrast, density matrices allow
for the representation of both pure and mixed states, the
latter referring to statistical combinations of various quan-
tum states that generally arise from partial or incomplete
information about the system.
The density matrix ρ of a quantum system is defined
as an operator that acts on the system’s Hilbert space.
For a system in a pure state described by a normalized
state vector |ψ⟩, the corresponding density matrix is given
by the outer product |ψ⟩⟨ψ|. For a mixed state composed
of possible states |ψi⟩each occurring with a probability
pi, the density matrix is a sum of the individual state’s
density matrices, weighted by their respective probabilities:
ρ = P
i pi |ψi⟩⟨ψi|. The key characterization of ρ is that it is
a positive semi-definite operator with a trace equal to one.
This aligns with the probabilistic interpretation of quantum
mechanics, where the probability of a system being in a
particular state is given by the trace of the product of the
density matrix and the projector onto the state.
The reduced density operator describes the state of a
subsystem by ”tracing out” or ignoring the other parts
of the system. Consider some composite quantum system
consisting of subsystems A and B in a joint state represented
by the density matrix ρAB. Investigating the properties of
subsystem A, independent from subsystem B, is done by
constructing the reduced density operator for A. This is
accomplished by taking the partial trace over subsystem
B, written as ρA = TrB(ρAB), where TrB denotes the
trace over subsystem B’s degrees of freedom. The reduced
density operator gives us all the measurable information
about the subsystem of interest. The subsystem behaves as
if it were in a mixed state given by the reduced density
operator, irrespective of whether the overall system is in a
pure or mixed state.
A.2
Machine Learning
Supervised Learning: In supervised learning, an algorithm
learns from a labeled dataset containing input-output pairs
(x, y). The goal is to learn a function f : x →y that
maps input features to output labels. The learning process
involves minimizing a loss function L(y, f(x; θ)), which
measures the discrepancy between the true output y and
the predicted output f(x; θ), for a given parameter set θ.
This θ is then optimized iteratively, often through gradient
descent-based methods. Two common supervised learning
tasks are regression, where the output is a continuous value,
and classification, where the output is a discrete class label.
An effective model has thus learned a θ that produces ei-
ther accurate continuous predictions for regression tasks or
correct class assignments for classification tasks a sufficient
proportion of the time when presented with generalized,
unseen data.
Unsupervised Learning: Unsupervised learning algo-
rithms learn patterns or structures from an unlabeled
dataset x. The goal is to capture underlying patterns, struc-
tures, or representations in the data by identifying relation-
ships and dependencies within the dataset. In this context,
“learning” can be understood as finding a transformation
function g : x →z, where z represents a lower-dimensional,
structured, or otherwise more informative representation of
the input data x. The learning process involves optimizing
an objective function O(g(x; ϕ)), which measures the quality
of the transformation based on some criterion, such as pre-
serving the intrinsic structure of the data or maximizing the
compactness of clusters. The function parameters ϕ are iter-
atively optimized using unsupervised learning techniques,
such as k-means clustering, hierarchical clustering, or prin-
cipal component analysis (PCA). An effective unsupervised
learning model can thus generalize well to new, unseen
data by accurately representing the underlying patterns and
structure within the discovered data distributions.
Reinforcement Learning: Reinforcement learning in-
volves an agent interacting with an environment to learn

53
optimal actions through a trial-and-error process. Unlike
supervised learning, which relies on labeled data to learn
input-output mappings, and unsupervised learning, which
seeks to find hidden structures in unlabeled data, reinforce-
ment learning focuses on learning through interactions and
feedback in the form of rewards or penalties. This makes
reinforcement learning particularly suitable for problems
where explicit supervision is not available or difficult to
obtain, and where agents need to learn from trial-and-error
experiences. The agent learns an optimal policy function π :
s →a, which maps states s to actions a, so as to maximize
the expected cumulative reward J(π) = E [P∞
t=0 λtRt|π],
where Rt is the reward at time t and λ ∈[0, 1] is the
discount factor (the infinite-horizon discounted return). The
learning process often involves updating the policy or value
function through algorithms such as Q-learning, SARSA,
or policy gradients, using techniques like the action-value
function Qπ(s, a) and Temporal Difference (TD) errors δt.
By leveraging the MDP framework, reinforcement learning
algorithms enable agents to navigate complex and uncertain
environments to maximize long-term rewards.
A.2.1
Gradient Descent
Gradient descent is a widely prevalent optimization algo-
rithm in machine learning and deep learning applications,
serving as the backbone for minimizing a differentiable loss
function L(w) with respect to its parameters w. Parameters
are updated iteratively in the direction of the negative
gradient of the loss function to minimize the loss:
wt+1 = wt −η∇L(wt),
(84)
where η is the learning rate and ∇L(wt) represents
the gradient of the loss function L with respect to the
parameters w at iteration t. Variants of gradient descent
differ in how they compute the gradient:
Batch Gradient Descent (BGD): The gradient is com-
puted using the entire dataset. This approach can be com-
putationally expensive, especially for large datasets:
∇L(wt) = 1
N
X
i = 1N∇Li(wt),
(85)
where N is the number of training examples and Li(wt) is
the loss for the ith example.
Stochastic Gradient Descent (SGD): The gradient is
approximated using a single randomly-selected training
example, which leads to faster iterations but higher variance
in the parameter updates:
∇L(wt) = ∇Li(wt),
(86)
where i is a randomly-selected index in the range [1, N].
Mini-batch Gradient Descent (MBGD): A compromise
between BGD and SGD, the gradient is computed using a
mini-batch of b training examples, which balances the trade-
off between computational efficiency and update variance:
∇L(wt) = 1
b
X
i = 1b∇Li(wt),
(87)
where b is the mini-batch size and the mini-batch is ran-
domly sampled from the training dataset. Gradient descent
can be further improved by incorporating adaptive learning
rates or momentum-based updates, leading to variants like
AdaGrad, RMSprop, Adam, and others.
A.2.2
Key Algorithms and Techniques
Linear Regression: Linear regression is a fundamental tech-
nique in machine learning and statistics, modeling the rela-
tionship between a dependent variable y and one or more
independent variables x. The goal is to find the best-fitting
linear function that can predict the dependent variable from
the independent variables. The linear model can be ex-
pressed as y = w⊺x+b, where w is the weight vector and b is
the bias term. The objective function for linear regression is
the least-squares loss function, which aims to minimize the
sum of squared differences between the observed responses
and the predictions of the linear function:
L(w, b) = 1
N
N
X
i=1
(w⊺xi + b −yi)2
(88)
The optimal weights w and bias term b are found via
gradient descent or by other optimization algorithms. In the
case of a single independent variable, the linear function
forms a straight line, and for multiple independent vari-
ables, it forms a hyperplane. Linear regression can be ex-
tended to incorporate regularization techniques like Ridge
or Lasso regression, which introduce a penalty term to the
objective function, helping to prevent over-fitting as well as
improving generalization.
Logistic Regression: Logistic regression is a widely used
binary classification algorithm that models the probability of
an instance belonging to a specific class. It builds upon the
concept of linear regression by using the logistic function,
also known as the sigmoid function, to model the relation-
ship between the dependent variable and the independent
variables, given by:
σ(z) =
1
1 + e−z
(89)
where z = w⊺x + b. This function transforms the lin-
ear combination of input features into a probability value
between 0 and 1, which can then be thresholded to make
binary classification decisions. The objective function for
logistic regression is the cross-entropy loss, which measures
the discrepancy between the predicted probabilities and the
true class labels:
L(w, b) = −1
N
N
X
i=1
[yi log(σ(w⊺xi + b))
+ (1 −yi) log(1 −σ(w⊺xi + b))].
(90)
Logistic regression can be extended to multi-class clas-
sification problems using techniques such as one-vs-rest or
one-vs-one, or by adopting the softmax function in place of
the sigmoid function.
Support Vector Machines (SVM): SVMs are a class of
powerful and versatile classification algorithms that seek
to find the optimal decision boundary, or hyperplane, that
maximizes the margin between different classes. This is
achieved by focusing on the instances that are closest to the
decision boundary, known as support vectors, which play
a critical role in determining the hyperplane’s position and
orientation. Given a training set (xi, yi), where yi ∈−1, 1,
the primal problem of SVM is formulated as follows:

54
min
w,b
1
2|w|2
s.t.
yi(w⊺xi + b) ≥1, ∀i
(91)
This optimization problem seeks to minimize the norm
of the weight vector, which is equivalent to maximizing the
margin between the classes. The margin can be understood
in terms of the geometric margin, which measures the
distance between the decision boundary and the nearest
data points, and the functional margin, which represents
the confidence in a classification decision. The dual problem
can be derived using Lagrange multipliers, leading to an
optimization problem that can be solved using the Se-
quential Minimal Optimization (SMO) algorithm, gradient
descent, or other quadratic programming methods. The dual
problem is expressed as follows:
max
α
N
X
i=1
αi −1
2
N
X
i=1
N
X
j=1
αiαjyiyjK(xi, xj)
s.t.
0 ≤αi ≤C,
X
i = 1Nαiyi = 0.
(92)
Here, α = (α1, α2, . . . , αN) are the Lagrange multipliers
and C is a regularization parameter that controls the trade-
off between maximizing the margin and minimizing the
classification error.
In Equation 92, the term K(xi, xj) represents the kernel
function, used to transform data into higher-dimensional
spaces, enabling the algorithm to draw more complex
decision boundaries, thus handling non-linearly separable
problems. The kernel function essentially maps the input
vectors into a higher-dimensional feature space and com-
putes the dot product in that space. This is equivalent
to the term (x⊺
i , xj) in the non-kernelized version of the
SVM, but with the ability to capture non-linear relations.
Common kernel functions include the linear, polynomial,
radial basis function (RBF), and sigmoid kernels. The choice
of kernel function and its parameters significantly impacts
the performance of the SVM classifier.
Decision Trees and Random Forests: Decision trees are a
popular machine learning algorithm that learn a hierarchical
structure of if-else rules to predict the target variable. The
tree is constructed by recursively splitting the data based
on the feature that provides the highest information gain or
the lowest Gini impurity. Information gain IG measures the
reduction in entropy before and after the split:
IG(S, A) = H(S) −
X
v∈Values(A)
|Sv|
|S| H(Sv)
(93)
where S represents the set of samples, A is the attribute
being tested, Sv is the subset of samples where attribute A
has value v, and H(S) denotes the entropy of the set S.
Entropy can be calculated using the following formula:
H(S) = −
C
X
i=1
pi log2 pi
(94)
where pi is the proportion of samples in class i and C is
the number of classes.
The Gini impurity is given by:
G(x) = 1 −
C
X
i=1
p2
i
(95)
where pi is the proportion of samples in class i and C is
the number of classes.
Random forests are an extension of decision trees and
represent an ensemble method, which combines multiple
base learners to improve predictive performance. Ensemble
methods like bagging (bootstrap aggregating) and boosting
work by aggregating the predictions of several individual
models. Bagging is employed in random forests, where
multiple decision trees are trained on random subsets of the
dataset with replacement. The final prediction is obtained by
averaging (regression) or voting (classification) the outputs
of these individual trees. Boosting is a sequential ensemble
method that iteratively adjusts the weights of the training
instances based on the errors made by the previous learners,
placing more emphasis on difficult-to-classify instances. Ex-
amples of boosting algorithms include AdaBoost and Gradi-
ent Boosting. Ensemble methods, in general, take advantage
of the strengths of multiple base models, making the final
model more robust and accurate than individual learners.
Clustering: Clustering algorithms aim to group data
points into clusters based on similarity or distance metrics,
which allows for the identification of natural patterns or
structures in the data; a key application of unsupervised
learning. Two popular clustering methods are the k-means
algorithm and KNN algorithm.
K-means is an iterative algorithm that seeks to minimize
the within-cluster sum of squared distances by updating
cluster centroids and reassigning data points. The objective
function for k-means is given by:
J(c, m) =
k
X
i=1
X
x∈Si
|x −mi|2
(96)
where c denotes the cluster assignments, mi denotes
the centroid of cluster i, and Si is the set of points in
cluster i. The algorithm begins by initializing the centroids
randomly and proceeds iteratively through two main steps:
assignment and update. In the assignment step, each data
point is assigned to the nearest centroid, and in the update
step, the centroids are recomputed as the mean of the points
in each cluster.
KNN is a distance-based, non-parametric method used
for both classification and regression tasks. In classification,
given a query point q, the algorithm identifies the k nearest
points to q in the feature space and assigns the majority class
label among those neighbors. For regression, the predicted
value is the average of the target values of the k nearest
neighbors. Distance metrics such as Euclidean distance,
Manhattan distance, or cosine similarity are used to measure
the similarity between instances.
Other popular clustering algorithms include DBSCAN
(Density-Based Spatial Clustering of Applications with
Noise), which groups points based on density, and hierar-
chical clustering, which creates a tree-like structure of nested
clusters.

55
A.2.3
Neural Networks
Feed-forward
Networks:
Feed-forward
networks,
also
known as multilayer perceptrons (MLPs), are the most basic
type of neural network. They comprise multiple layers of
interconnected neurons, where each neuron calculates a
weighted sum of its inputs, then applies a nonlinear activa-
tion function. The output of neuron j in layer l is expressed
as:
a(l)
j
= f


N(l−1)
X
i=1
w(l)
ji a(l−1)
i
+ b(l)
j


(97)
Here, a(l)
j
is the activation of neuron j in layer l, f(·) is
the activation function, w(l)
ji is the weight connecting neuron
i in layer (l −1) to neuron j in layer l, and b(l)
j
is the bias of
neuron j in layer l.
Various different activation functions have been used
within neural networks. The sigmoid and tanh activation
functions were prevalent in early architectures. However,
they have become less common as they suffer from the
vanishing or exploding gradients problem, where gradients
can become increasingly small or exponentially large during
backpropagation, making it difficult for the network to
learn [269]. This issue is particularly pronounced in deep
neural networks with many layers. The Rectified Linear
Unit (ReLU) activation function and its variants, such as
Leaky ReLU or SELU, are now commonly used for their
ability to mitigate this issue. These have sparse activation;
not all neurons are activated simultaneously, leading to
more efficient training and better generalization in deep
networks. Additionally, ReLU is piecewise linear, making
it computationally faster compared to sigmoid and tanh
functions.
Feed-forward neural networks can learn complex input-
output relationships and have been applied in various
machine learning tasks, such as regression, classification,
and pattern recognition. However, they lack the specialized
structures found in more advanced deep learning models
like convolutional and recurrent neural networks, which
excel at handling spatial and temporal data, respectively.
Backpropagation: The backpropagation algorithm has
become the fundamental process by which neural networks
are trained. It efficiently computes the gradients of the
loss function concerning the network’s weights and biases,
enabling gradient-based optimization techniques to update
the parameters and minimize the loss. The challenge in cal-
culating gradients in a neural network stems from the large
number of parameters and the complex, nested nature of
the functions involved. Due to these intricacies, it is difficult
to determine how a small change in a single parameter will
impact the overall loss.
Given a differentiable loss function L(W, b), where W
denotes the weights and b denotes the biases, the goal is
to compute the gradient of the loss function with respect
to each parameter, i.e., ∂L/∂w(l)
ji and ∂L/∂b(l)
j . Backprop-
agation achieves this via the chain rule of calculus, which
computes the gradients in a layer-by-layer manner, starting
from the output layer and moving backward through the
network. By leveraging the chain rule and the feed-forward
−4
−2
2
4
−1
−0.5
0.5
1
x
y
Sigmoid = 1/1 + e−x
Tanh = (ex −e−x)/(ex + e−x)
ReLU = max(0, x)
Fig. 13. Common Activation Functions
structure of neural networks, backpropagation efficiently
calculates the gradients and overcomes the challenges posed
by the complex network architecture.
For each neuron j in layer l, the error term δ(l)
j
is
defined as the product of the gradient of the activation
function, f ′(z(l)
j ), and the weighted sum of errors from the
subsequent layer:
δ(l)
j
= f ′(z(l)
j )
X
k = 1N (l+1)w(l+1)
kj
δ(l+1)
k
(98)
where z(l)
j
= P i = 1N (l−1)w(l)
ji a(l−1)
i
+ b(l)
j
is the weighted
input to neuron j in layer l, and N (l+1) is the number of
neurons in layer (l + 1).
Using the error term δ(l)
j , the gradient of the loss function
with respect to the parameters can be computed as:
∂L
∂w(l)
ji
= a(l−1)
i
δ(l)
j
(99)
∂L
∂b(l)
j
= δ(l)
j
(100)
Once the gradients are computed, the parameters are up-
dated via gradient descent:
w(l)
ji ←w(l)
ji −η ∂L
∂w(l)
ji
(101)
b(l)
j
←b(l)
j
−η ∂L
∂b(l)
j
(102)
where η is the learning rate. This iterative process of forward
propagation, gradient computation using backpropagation,
and parameter updates continues until a stopping criterion,
such as a maximum number of epochs or a threshold on the
improvement in the loss function, is reached.
Convolutional Neural Networks (CNNs): CNNs are a
specialized type of neural network specifically designed for

56
tasks such as image recognition and computer vision appli-
cations. They are composed of a series of layers, including
convolutional layers, pooling layers, and fully connected
layers. Convolutional layers are responsible for feature ex-
traction, utilizing filters or kernels that slide over the input
image, computing the dot product at each position. Each
neuron in a feature map is connected to a neighboring
region of neurons in the previous layer, called the neuron’s
receptive field. The feature value at location (i, j) in the k-th
feature map of the l-th layer, f (l,k)
ij
, is calculated by:
f (l,k)
ij
=
X
m
X
n
w(l,k)
mn · x(l)
i+m,j+n + b(l,k)
ij
(103)
where w(l,k)
mn and b(l,k)
ij
are the weight vector and bias terms
of the k-th filter of the l-th layer, and x(l)
i+m,j+n is the input
patch centered at location (i, j) of the l-th layer.
Pooling layers, such as max pooling or average pooling,
serve to aggregate information from local regions, reducing
the spatial dimensions of the feature maps while preserving
shift-invariance. These layers are typically placed between
two convolutional layers. For each feature map, pooling can
be specified as follows:
z(l,k)
ij
= pool

a(l−1,k)
m,n
|(m, n) ∈Rij

(104)
where Rij is a local neighborhood around location (i, j).
Finally, fully connected layers are used to integrate
the features extracted by the previous layers and produce
the final output, such as class probabilities in the case of
classification tasks. This architecture, with its emphasis on
local connectivity and spatial hierarchy, allows CNNs to
efficiently process and learn from high-dimensional data,
such as images, in a way that traditional feed-forward
networks cannot.
A.2.4
Model Evaluation Metrics
Accuracy: Accuracy is the proportion of correctly classified
instances out of the total instances:
Accuracy = Number of correct predictions
Total number of predictions
(105)
Accuracy is a commonly used metric for classification tasks,
but it may not be ideal when dealing with imbalanced
datasets. In such cases, the model might have high accuracy
by simply predicting the majority class, but it might not
accurately identify the minority class instances, which are
often the more important ones.
Precision, Recall, and F1 Score: Precision is the pro-
portion of true positive predictions out of the total positive
predictions, while recall is the proportion of true positive
predictions out of the total actual positive instances. The F1
score is the harmonic mean of precision and recall:
Precision =
True Positives
True Positives + False Positives
(106)
Recall =
True Positives
True Positives + False Negatives
(107)
F1 Score = 2 × Precision × Recall
Precision + Recall
(108)
Precision and recall are particularly useful when evaluating
models on imbalanced datasets. High precision means that
the model has a low false positive rate, which is crucial
when false positives are costly (e.g., spam detection). High
recall means that the model has a low false negative rate,
which is important when false negatives are more critical
(e.g., cancer detection).
The F1 score provides a single number that balances
the trade-off between precision and recall; it assumes equal
importance for both, which may not be suitable for all
applications.
The choice of evaluation metric depends on the specific
application and the relative importance of false positives,
false negatives, and the class distribution. In the unsu-
pervised learning context, the mentioned metrics may not
adequately reflect model performance due to the absence
of class labels. In such cases, alternative metrics have been
developed to evaluate the underlying structure or relation-
ships captured by the models without relying on ground
truth labels. Some prominent unsupervised learning met-
rics include mutual information, adjusted Rand index, and
variation of information, to name a few. These measures
evaluate the quality of the learned structure or patterns
by assessing their similarity to an unknown ground truth
or by assessing the internal consistency of the discovered
structure itself.

