Algorithms for 
Constructing 
Computably 
Enumerable Sets
Kenneth J. Supowit
Computer Science Foundations and Applied Logic


Computer Science Foundations and Applied
Logic
Editors-in-Chief
Vijay Ganesh, University of Waterloo, Waterloo, Canada
Ilias S. Kotsireas, Department of Physics and Computer Science, Wilfrid Laurier University, Waterloo,
ON, Canada
Editorial Board
Erika Abraham, Computer Science Department, RWTH Aachen University, Aachen,
Nordrhein-Westfalen, Germany
Olaf Beyersdorff, Friedrich Schiller University Jena, Jena, Thüringen, Germany
Jasmin Blanchette, Garching, Germany
Armin Biere, Informatik, ETH Zentrum, RZH, Computer Systems Institute, Zürich, Switzerland
Sam Buss, Department of Mathematics, University of California, San Diego, La Jolla, CA, USA
Matthew England, Engineering and Computing, Coventry University, Coventry, UK
Jacques Fleuriot, The University of Edinburgh, Scotland, Selkirkshire, UK
Pascal Fontaine, University of Lorraine, Villers Les Nancy Cedex, France
Arie Gurﬁnkel, Pittsburgh, PA, USA
Marijn Heule, Algorithms, University of Texas, Austin, TX, USA
Reinhard Kahle, Departamento de Matematica, Universidade Nova de Lisboa, Caparica, Portugal
Phokion Kolaitis, University of California, Santa Cruz, CA, USA
Antonina Kolokolova, Department of Computer Science, Memorial University of Newfoundland, St.
John’s, NL, Canada
Ralph Matthes, Universität München, München, Germany
Assia Mahboubi, Informatique et en Automatique, Institut National de Recherche en, Nantes Cedex 3,
France
Jakob Nordström, Stockholm, Sweden
Prakash Panangaden, School of Computer Science, McGill University, Montreal, QC, Canada
Kristin Yvonne Rozier, Champaign, IL, USA
Thomas Studer, Institute of Computer Science and Applied Mathematics, University of Berne, Berne,
Switzerland
Cesare Tinelli, The University of Iowa, Iowa City, IA, USA
Computer Science Foundations and Applied Logic is a growing series that focuses on the foundations
of computing and their interaction with applied logic, including how science overall is driven
by this. Thus, applications of computer science to mathematical logic, as well as applications of
mathematical logic to computer science, will yield many topics of interest. Among other areas, it
will cover combinations of logical reasoning and machine learning as applied to AI, mathematics,
physics, as well as other areas of science and engineering. The series (previously known as Progress
in Computer Science and Applied Logic) welcomes proposals for research monographs, textbooks
and polished lectures, and professional text/references. The scientiﬁc content will be of strong
interest to both computer scientists and logicians.

Kenneth J. Supowit
Algorithms for Constructing
Computably Enumerable Sets

Kenneth J. Supowit
Department of Computer Science
and Engineering
The Ohio State University
Columbus, OH, USA
ISSN 2731-5754
ISSN 2731-5762 (electronic)
Computer Science Foundations and Applied Logic
ISBN 978-3-031-26903-5
ISBN 978-3-031-26904-2 (eBook)
https://doi.org/10.1007/978-3-031-26904-2
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature
Switzerland AG 2023
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afﬁliations.
This book is published under the imprint Birkhäuser, www.birkhauser-science.com by the registered
company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland


Preface
History
Classical computability theory (formerly known as recursive function theory) was
pioneered by Turing, Post, Kleene and a few others in the late 1930s and throughout
the 1940s. They developed intriguing concepts, some of which, such as reducibility
and completeness, were adapted decades later for the study of concrete complexity.
Also, Kleene’s notation became the basis of the programming language Lisp. These
pioneers asked tough questions and answered some of them. Post’s Problem was
formulated in 1944 and was deemed the holy grail of the subject for a dozen years;
some feared that the discipline had played itself out, that all of the good ideas had
already been discovered, that the grail would lie in eternal darkness (as grails are
wont to do). Then two teenagers, Friedberg in the United States and Muchnik in the
former Soviet Union, independently developed the “ﬁnite injury priority method,” an
elegant and robust algorithmic technique for constructing computably enumerable
(c.e.) sets that satisfy certain “requirements.” Not only did it solve Post’s Problem,
but it led to a ﬂurry of related methods invented to construct c.e. sets subject to more
complicated collections of requirements. The key idea was that a speciﬁc requirement
could be “injured” only ﬁnitely often.
The very idea of an effective “inﬁnite injury” method might seem impossible, but
in the 1960s such methods were indeed invented and employed to prove startling
results. Various models of inﬁnite injury methods were developed; it’s still not
clear whether each one of them can simulate each of the others. In the 1970s an
extremely complicated technique, called the “triple jump method” was developed.
From a certain point of view, the triple jump method bears the same relationship to
inﬁnite injury that inﬁnite injury bears to ﬁnite injury.
During the 1990s, the discipline again seemed played out (and so it may seem
to this day). The latest methods had become so complicated that young logicians
looked elsewhere for research topics. Classical computability theory had become a
victim of its own success.
vii

viii
Preface
This Book
This book is focused on the algorithms used in the proofs, rather than on the results.
They are presented in a uniﬁed notation and framework that will be more accessible
not only to mathematicians but also to computer scientists. Scattered throughout the
book are analogies to certain concepts that arise in computer science. Furthermore,
the algorithms are presented in a pseudo-code notation that is typical of algorithms
books such as [CLRS]. I am unaware of any other book or article in computability
theory that describes algorithms in such pseudo-code.
As part of this focus on algorithms, a typical chapter in this book introduces
a speciﬁc algorithmic technique. Such a chapter contains a single application, a
theorem, to illustrate that technique. For example, Chap. 4 introduces the priority
method, using a simple splitting theorem as the application. As another example,
Chap. 9 introduces the length-of-agreement method, using a version of the Sacks
Splitting Theorem as the application. When choosing the application, I strove to ﬁnd
the simplest, least general theorem to which the technique applies.
The motivation for this book is four-fold:
1. No book focusing on the algorithms for constructing c.e. sets currently exists.
Also, some of the proofs here are worked out in greater detail and with more
intuition than can be found anywhere else. This book lacks many of the standard
topics found in books on computability (such as [Ro] or [So87]). For example, it
doesnotmentionreductionsotherthanTuringreductions,orthearithmeticalhier-
archy, or many of the standard results of computability theory (such as Kleene’s
Recursion Theorem).
2. The algorithmic ideas developed for these abstract problems might ﬁnd appli-
cations in more practical areas. For example, the priority tree—one of the
central ideas of the book—involves multiple processes working on the same task,
although with varying sets of assumptions or “guesses.” At least one of those
processes (and we cannot know a priori which it will be) will guess entirely
correctly; that “true” node will accomplish the task. Is there an application for
this idea amongst the more concrete problems?
3. In typical books on concrete algorithms, such as [CLRS], after discussing a few
data structures, there is a series of chapters that could be permuted in just about
any order without harming the book. It is not a defect in those books; it just
appears to be the nature of the subject. On the other hand, the techniques that
have been developed for constructing c.e. sets tend to build on each other. By
focusing on these techniques, this book clariﬁes their relation to each other.
4. Beyond practicality, there is aesthetics. The violinist Isaac Stern said, “Every
child deserves to know that there is beauty in the world.” Every adult does,
too. One who delves into length-of-agreement arguments and tree-based inﬁnite
injury arguments cannot help but marvel, with a wild surmise, like John Keats
upon ﬁrst looking into Chapman’s Homer.1
1 My apologies for waxing poetic. I’ll try not to do it in the body of the text.

Preface
ix
Included here are a plethora of exercises, most of which will differ greatly from
those in other books on computability. Those books ask the reader to prove results,
essentially using and perhaps combining the methods presented in the text. In this
book,theexercisesaremainlyabouttheconstructionsthemselves—e.g.,whywasthis
step done in this way? Or, consider the following modiﬁed version of our algorithm;
would it work, too? The exercises are an integral part of the book—do them!
Acknowledgements
Carl Jockusch introduced me to recursion theory in a graduate course. A truly great
teacher, he walked us through not only the material, but also (and perhaps more
importantly) his own process of problem solving. Our homework assignments were
not for the faint of heart; a typical assignment had ﬁve problems, with the instruction
“DO ANY ONE” at the top of the page. Decades later, when I returned to the study
of recursion theory (which had become known as computability theory), I peppered
Prof. Jockusch with questions, which he graciously answered.
Denis Hirshfeldt and Peter Cholak also have been generous with their time,
answering many of my questions.
Many students have contributed to this book, by their questions and ideas. Chief
among them was Jacob Urick. He proof-read much of the book and made many,
many suggestions for improving the notation, the organization of speciﬁc proofs,
and the overall plan of the book. He also inﬂuenced the writing style. For example,
he once told me, “Every time you explain something to me, you draw a picture. So,
why not put them in your book?” Then I remembered how, in class, nearly every
time Carl Jockusch explained something, he drew a picture. I took Jacob’s advice,
as you can see from the abundance of ﬁgures.
A Truth Universally Acknowledged
Consider this “result” stated in Chap. 13 of [AK]:
Theorem: Recursion theory is very hard.
As quipped in [AK], this result was “discovered independently by a number of other
mathematicians.”
It is “a truth universally acknowledged.”2
Knowing this truth at the outset might discourage a student. My experience has
been the opposite. When I’m struggling (for weeks, or for months) to understand a
2 As Jane Austen would have said.

x
Preface
proof in computability theory, instead of giving up in frustration, I remind myself
that this ﬁeld is just, plain hard; I tell myself that my rate of progress is acceptable,
perhaps even good. Occasionally, I look back and take stock of the terrain that I have
traversed so far.
Enjoy the journey.
Columbus, USA
Kenneth J. Supowit

Contents
1
Notation and Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Index of Notation and Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Defaults
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
Notes About the Pseudo-code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.4
Miscellaneous Notes About the Text . . . . . . . . . . . . . . . . . . . . . . . .
6
2
Set Theory, Requirements, Witnesses . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.1
Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2
Inﬁnitely Many Inﬁnite Cardinals . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.3
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
3
Computable and c.e. Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.1
Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.2
Computably Enumerable, Computable, c.e.n . . . . . . . . . . . . . . . . .
16
3.3
An Example of a c.e.n. Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.4
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
3.5
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
3.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
4
Priorities (A Splitting Theorem) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
4.1
A Priority Argument . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
4.2
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
4.3
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
4.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
5
Reductions, Comparability (Kleene-Post Theorem) . . . . . . . . . . . . . . .
31
5.1
Oracle Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
5.2
Turing Reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
5.3
The Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
xi

xii
Contents
5.4
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
5.5
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
5.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
6
The Permanence Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
6.1
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
6.2
The Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
6.3
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
6.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
7
Finite Injury (Friedberg-Muchnik Theorem) . . . . . . . . . . . . . . . . . . . .
45
7.1
The Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
7.2
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
7.3
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
7.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
8
Permitting (Friedberg-Muchnik Below C Theorem) . . . . . . . . . . . . . .
51
8.1
The Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
8.2
The Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
8.3
Valid Witnesses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
8.4
Types of Witnesses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
8.5
The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
8.6
Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
8.7
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
8.8
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
8.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
9
Length of Agreement (Sacks Splitting Theorem) . . . . . . . . . . . . . . . . .
61
9.1
The Idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
9.2
The Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
9.3
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
9.4
The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
9.5
Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
9.6
Why Preserve Agreements? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
9.7
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
9.8
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
9.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
10
Introduction to Inﬁnite Injury . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
10.1
A Review of Finite Injury Priority Arguments . . . . . . . . . . . . . . . .
73
10.2
Coping with Inﬁnite Injury . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
10.2.1
Guessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
10.2.2
Other Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75

Contents
xiii
11
A Tree of Guesses (Weak Thickness Lemma) . . . . . . . . . . . . . . . . . . . . .
77
11.1
The “Lemma” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
11.2
The Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
11.3
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
11.4
The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
11.5
Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
11.6
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
11.7
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
11.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
12
An Inﬁnitely Branching Tree (Thickness Lemma) . . . . . . . . . . . . . . . .
99
12.1
The Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
12.2
Deﬁnitions, and a Fact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
12.3
The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
12.4
Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
12.5
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
12.6
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
12.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
13
Joint Custody (Minimal Pair Theorem)
. . . . . . . . . . . . . . . . . . . . . . . . .
113
13.1
The Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
13.2
The Tree, and an Overview of Our Strategy . . . . . . . . . . . . . . . . . .
115
13.3
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
13.4
Interpretation of the Guesses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
13.5
The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
13.6
Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
13.7
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
13.8
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
13.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
14
Witness Lists (Density Theorem) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
14.1
The Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
14.2
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
14.3
The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
14.3.1
Main Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
14.3.2
Subroutines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
14.3.3
Notes on the Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
14.4
Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
14.4.1
More Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
14.4.2
Interpretation of the Guesses . . . . . . . . . . . . . . . . . . . . . . .
136
14.4.3
Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
14.4.4
Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
14.5
What’s New in This Chapter? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157

xiv
Contents
14.6
Designing an Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158
14.7
Afternotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
14.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
15
The Theme of This Book: Delaying Tactics . . . . . . . . . . . . . . . . . . . . . .
163
Appendix A: A Pairing Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
Solutions to Selected Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171

Chapter 1
Notation and Terms
1.1
Index of Notation and Terms
The following is a partial list of the notation and terms (some of which are informal)
used in this book, categorized by the chapter in which they ﬁrst appear. A few of
these terms ﬁrst appear in the exercises of the listed chapter.
Chapter 1
ω =def {0, 1, 2, . . .}
λ
Chapter 2
ℵ0
ℵ1
(Cantor) diagonalization
requirement
witness
Chapter 3
Turing machine
(k) ↓
(k) ↑
(k) = y
lexicographic order
|σ| (the length of string σ)
e (the eth Turing machine)
partial function
total function
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_1
1

2
1
Notation and Terms
f (k) ↓(k is in the domain of the partial function f )
f (k) ↑(k is not in the domain of the partial function f )
equality of two partial functions
e (the partial function computed by the eth Turing machine)
computable partial function
We
computably enumerable (c.e.) set
¯A (the complement of A)
computable set
c.e.n. set
computable enumeration
standard enumeration
⟨x, y⟩
H (the “Halting Problem”)
Chapter 4
how an inﬁnite c.e. set is given as input
As

= {a0, a1, . . . , as}

stage
priority
the binary operation ≺deﬁned on requirements
(R ≺Q means that R has priority over Q)
We[s]

= {we,0, we,1, . . . , we,s }

sparse set
simple set
Chapter 5
oracle Turing machine (OTM)
χA (the characteristic function of A)
B
e (the eth OTM with oracle B)
the computation B
e (k)
B
e (k) (the output of the computation B
e (k) )
B
e (the partial function computed by the eth OTM with oracle B)
ϕB
e (the usage function of B
e )
≤T
≡T
<T
(Turing) incomparable sets
preﬁx
proper preﬁx
the binary operation ≺deﬁned on strings rather than on requirements
(τ ≺σ means that τ is a proper preﬁx of σ)
σ
e (n)
σ⌢0 (= σ appended with 0)

1.1 Index of Notation and Terms
3
σ⌢1 (= σ appended with 1)
A′ (the jump of A)
Chapter 6
A↾n

= A ∩{0, 1, . . . , n −1}

A↾↾n

= A ∩{0, 1, . . . , n}

the sufﬁx [s] (as in B
e (n)[s], or ϕB
e (n)[s], for examples)
the computation B
e (n)[s] being permanent
Chapter 7
Ds (the constructed set D at the start of stage s)
requirement needing attention
requirement receiving attention
spoil a computation
injure a requirement
fresh witness
autoreducible set
Chapter 8
permission
A(k) as a shorthand for χA(k), and accordingly A as a shorthand for χA
valid witness
Type 1, 2, or 3 witness
a witness holding up as Type 1, 2, or 3
Chapter 9
ℓe,i (the length of agreement)
restrainte,i
the liminf of functions from ω to ω
Chapter 10
positive requirements
negative requirements
priority tree
Chapter 11
A[e]
a row of a 0–1 matrix being full
piecewise trivial set
thick subset
tree of guesses, also known as a priority tree
|σ|
binary operations ≺, ⪯, <L, ◁, and ⊴on tree nodes

4
1
Notation and Terms
TPs
visiting a tree node
σ-stage
inﬁnite path
TP (the true path)
σ <L TP (σ is to the left of TP)
TP <L σ (σ is to the right of TP)
σ <L TPs (σ is to the left of TPs)
TPs <L σ (σ is to the right of TPs)
nodes west, southeast, or northeast of σ
σ-believability
B
σ
(a node-speciﬁc version of B
e )
ℓσ (a node-speciﬁc length of agreement)
restraintσ
Restraintσ
˜ℓσ (the maximum of ℓσ)
˜Rσ (the maximum of Restraintσ)
predσ(s) for a σ-stage s
A[e]
s
coﬁnite
Iσ (the injury set for node σ)
Chapter 12
S △T (the symmetric difference)
ℓe,r (the length of disagreement)
unimodal function
Chapter 13
minimal pair of sets
positive witness for Pe, A or Pe, B
negative witness for Pe, A or Pe, B
A
e (k)[s] ↓= B
e (k)[s]
Lσ (the high water mark of ℓσ)
σ-expansionary stage
custody
Chapter 14
coding
nσ (the witness list for node σ)
mσ(s)
realized witness
unrealized witness
star witness
permanent star witness

1.2 Defaults
5
undeﬁning a witness
canceling a node
inserted(σ, i)[s]
last Realized(σ, i)[s]
ϕ(σ, i)[s]
entryIntoC(i)
a fresh number (in this chapter, it must be odd)
a number unrealizes a witness
an inert node
a node performs an unrealization, insertion, or realization
a node takes the default
a witness is ready for insertion
window of opportunity
1.2
Defaults
We employ the following defaults, most of which are ubiquitous in the computability
literature.
1. Lower case English letters (with the exception of f , g, and h, which are reserved
for functions) refer to members of ω, which is deﬁned as {0, 1, 2, . . .}.
2. Starting in Chap. 3, set means a subset of ω, and function means a mapping
from ω to ω (or, occasionally, from ω to {−1, 0, 1, 2, . . .}). Upper case English
letters refer to sets, unless speciﬁed otherwise (for example, whenever they refer
to requirements rather than to sets, we say so).
3. Upper case Greek letters (typically , and  if we need a second one) refer to
oracle Turing machines. Their corresponding lower case letters (ϕ and ψ) refer
to their corresponding usage functions.
4. Typically, indices of machines or of requirements are e, i, or j. Inputs to machines
are usually k, n, or p; in particular, witnesses are usually n. Stages of constructions
are s, t, v, w, x, y, or z; in particular, x is typically a stage (which we cannot
compute) after which certain ﬁnitely occurring events have ceased to occur.
5. The lower case Greek letters ρ, σ, τ, ξ and occasionally α, β, and γ refer to ﬁnite-
length strings. Also, we use λ to denote the empty string. We use ﬁnite-length
strings (if they are binary) to represent initial segments of characteristic functions,
especially for oracles. Also, we use ﬁnite-length strings (whether or not they are
binary) to represent tree nodes; in this context, λ represents the root of a tree.
When considering a tree node σ, we often use ρ for a node to the left of σ, and τ
for an ancestor of σ.

6
1
Notation and Terms
1.3
Notes About the Pseudo-code
To specify algorithms, we often use pseudo-code that resembles that of most text-
books on algorithms, especially [CLRS].
Many of our for loops run forever, e.g.:
for s ←0 to ∞
statement
statement
statement
You won’t see that in [CLRS]!
1.4
Miscellaneous Notes About the Text
Each chapter contains at most one theorem, which allows us to number each theorem
by the chapter in which it appears (e.g., Theorem 11). Lemmas on the other hand, are
numbered with both their chapter numbers and their positions among the lemmas in
that chapter (e.g., Lemma 11.2); facts are numbered analogously.
Starting in Chap. 9, we often refer to the facts and lemmas without includ-
ing the number of the current chapter. For example, in Chap. 14, we usually say
“Fact 6” instead of its full name “Fact14.6,” and “Lemma 1” instead of its full name
“Lemma 14.1.”

Chapter 2
Set Theory, Requirements, Witnesses
This chapter is a very brief introduction to inﬁnite cardinals. Our main interest here is
in “requirements” and in “witnesses,” which are key concepts throughout this book.
First, some deﬁnitions:
1. A set is a collection of elements.
2. The Cartesian product of sets A and B is
A × B =

(a, b) : a ∈A and b ∈B

.
3. A binary relation on sets A and B is a subset of A × B.
4. A function from set A to set B is a binary relation f ⊆A × B such that

∀a ∈A

∃b1 ∈B
 
a, b1

∈f
and

∀b2 ∈B
	
b1 ̸= b2 =⇒(a, b2) /∈f

 
.
The set A is the domain, and B is the range of f . We denote this situation by
f : A →B
and we use the familiar inﬁx notation f (a) = b to denote (a, b) ∈f .
5. A function f : A →B is
(a) one-to-one if

∀a, a′ ∈A
	
a ̸= a′ =⇒f (a) ̸= f (a′)

,
(b) onto if

∀b ∈B

∃a ∈A
	
f (a) = b

,
(c) a one-to-one correspondence if it is both one-to-one and onto.
The alternative, Latin-based terms are injective, surjective, and bijective,
respectively.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_2
7

8
2
Set Theory, Requirements, Witnesses
6. A set S is ﬁnite if there is a one-to-one correspondence
f : S →{1, 2, . . . , k }
for some k.
7. A set S is countably inﬁnite if there is a one-to-one correspondence f : S →ω.
If so, then
S = { f −1(0), f −1(1), . . .}.
8. A set S is countable if it is either ﬁnite or countably inﬁnite.
2.1
Diagonalization
If S is countably inﬁnite then its cardinality is denoted by ℵ0 (pronounced “aleph-
null”).
Here is a handy result for showing that certain sets are countable; it is proved in
[Va]:
Lemma 2.1 Let S be a set. If there exists a one-to-one (but not necessarily onto)
function
f : S →ω
then S is countable.
There would be no point in deﬁning countability if all sets were countable. So
here is one that is not (ℜdenotes the set of real numbers):
Theorem 2 The set S = (0, 1] = {x ∈ℜ: 0 < x ≤1 } is not countable.
Proof Assume for a contradiction that S were countable. Then there is a one-to-one
correspondence f : S →ω. Let ai = f −1(i) for each i ∈ω; thus
S = {a0, a1, . . .}.
Let ai j denote the jth digit in the decimal expansion of ai, for each i, j ∈ω, where
by convention no number has an inﬁnite string of trailing 0’s in its expansion (thus,
we write 0.24999 · · · instead of 0.25000 · · · ). Let b be the real number 0.b0b1 · · · ,
where
bi =

3, if aii = 4
4, otherwise
for each i. Thus 0 < b ≤1 and so b ∈S. However, for each i, b differs from ai in
the ith digit, and so b /∈S, a contradiction.
□

2.2 Inﬁnitely Many Inﬁnite Cardinals
9
Fig. 2.1 Cantor
diagonalization
a0
=
0.a00
a01
a02
a03
a04 · · ·
a1
=
0.a10
a11
a12
a13
a14 · · ·
a2
=
0.a20
a21
a22
a23
a24 · · ·
a3
=
0.a30
a31
a32
a33
a34 · · ·
a4
=
0.a40
a41
a42
a43
a44 · · ·
...
The argument in the proof of Theorem 2 is called “diagonalization,” because of
the key role of the diagonal elements, which are underlined in Fig.2.1. It is also
known as “Cantor diagonalization,” after its discoverer, the pioneer of set theory,
Georg Cantor.
The cardinality of the set (0, 1] is denoted by ℵ1. Here is the list of cardinalities
that we have seen so far:
0, 1, 2, . . . , ℵ0, ℵ1.
Are there others? If so, how many are there? What does it mean for one inﬁnite
cardinal to be larger than another? These questions are (partially) answered in the
next section.
2.2
Inﬁnitely Many Inﬁnite Cardinals
The cardinality of set A is less than or equal to the cardinality of set B, denoted
|A| ≤|B|, if there is a one-to-one function from A to B. Their cardinalities are
equal, denoted |A| = |B|, if there is a one-to-one correspondence from A to B;
otherwise we write |A| ̸= |B|. This generalizes our concept of what it means for two
ﬁnite sets to be equal-sized; if you gave a little kid a bunch of crayons and pencils,
and asked him whether there’s the same number of each, he might just pair them up
and then see whether anything is left over. The cardinality of A is less than that of
B, denote |A| < |B|, if |A| ≤|B| but |A| ̸= |B|.
Lemma 2.2 Let A and B be sets. Then exactly one of the following three conditions
holds:
I. |A| < |B|,
II. |B| < |A|,
III. |A| = |B|.
Furthermore, if |A| ≤|B| and |B| ≤|A| then |A| = |B|.
Although Lemma 2.2 may look trivial, proving it see to require some effort (see [Va]
for a nice proof).

10
2
Set Theory, Requirements, Witnesses
The power set of a set A, denoted POW(A) is the set of all subsets of A. E.g.,
POW

{a, b, c}

=

{}, {a}, {b}, {c}, {a, b}, {a, c}, {b, c}, {a, b, c}

.
Lemma 2.3 Let A be a set. Then |A| < |POW(A)|.
Proof First note that |A| ≤|POW(A)|, because the function f : A →POW(A)
deﬁned by
f (a) = {a}
is one-to-one.
Next we prove |A| ̸= |POW(A)| by showing that there is no one-to-one corre-
spondence from A to POW(A). To do this, let g : A →POW(A); we will show
that g is not onto. Let
S =

a ∈A : a /∈g(a)

.
Thus S ∈POW(A). Assume for a contradiction that S were an image under g, that
is, S = g(a0) for some a0 ∈A.
Case 1: a0 ∈S.
That is, a0 ∈

a ∈A : a /∈g(a)

. Thus a0 /∈g(a0) = S, a contradiction.
Case 2: a0 /∈S.
That is, a0 /∈

a ∈A : a /∈g(a)

. Thus a0 ∈g(a0) = S, a contradiction.
Thus g is not onto, and g was an arbitrarily chosen function from A to POW(A). □
Corollary |ω| < | POW(ω)| < | POW(POW(ω))| < · · · .
Thus, there are inﬁnitely many inﬁnite cardinals.
2.3
What’s New in This Chapter?
Take another look at the proof of Theorem 2. The idea is to construct a number
b ∈{a0, a1, . . .} that satisﬁes, for each i, the requirement
Ri : b ̸= ai .
If each of these inﬁnitely many requirements is satisﬁed, then we have a contradiction.
We construct b so that the diagonal element aii is a witness to the requirement Ri. This
extra terminology—requirements, witnesses—sheds no additional light on Cantor’s
simple proof. However, as this book progresses, we shall see more complicated types
of requirements, and beautiful techniques for obtaining witnesses to them.

2.4 Exercises
11
2.4
Exercises
1. Give examples to show that
(a) the intersection of two countably inﬁnite sets can be either ﬁnite or countably
inﬁnite,
(b) the intersection of two uncountable sets can be ﬁnite, countably inﬁnite, or
uncountable.
2. How many subsets are there of ω? How many of them are ﬁnite?
3. Let S be the union of countably many countable sets. Must S be countable?
4. A real number is rational if it can be expressed as the ratio of two integers. Let
Q denote the set of rational numbers. Show that Q is countable.
5. Suppose that we tried to use diagonalization to show that Q is uncountable, mim-
icking the proof of Theorem 2. Where would the argument fail?
6. Let S be a set of non-overlapping discs, each having positive area (that is, none
of these discs is merely one point), and each contained entirely within the unit
square [0, 1] × [0, 1]. Prove that S is countable.

Chapter 3
Computable and c.e. Sets
Starting here, and for the remainder of this book, the term set refers to a subset of ω
(unless speciﬁed otherwise). In this chapter we deﬁne what it means for a set to be
“computable” and “computably enumerable.” To do so, we ﬁrst (brieﬂy) describe a
formal model of computation, called the “Turing machine.”
3.1
Turing Machines
Informally, a Turing machine  is depicted in Fig.3.1.
The machine  has a two-way inﬁnite tape, and a read/write head whose move-
ments are determined by a “controller.”1 The cells of the tape are numbers by integers.
In Fig.3.1, the read/write head is scanning cell −2, which contains the symbol 1.
The machine moves in discrete time steps. At all times, the controller is in exactly
one “state.” The set of possible states is
Q =

qstart, qhalt, q1, q2, . . . , qr

(3.1)
for some r ∈ω. Thus, Q is ﬁnite. At all times, each tape cell contains either 0 or 1 or
B (which stands for “blank”). At the start of each time step, depending on the current
state and on the symbol currently being scanned by the read/write head, either 0 or
1 or B is written into the cell being scanned (call it cell i), and then the read/write
head moves left (that is, to cell i −1) or right (that is, to cell i + 1).
A Turing machine is speciﬁed as a pair (r, δ), where
r = |Q| −2
1 Think of the controller as a ﬁnite automaton, if you are familiar with that type of machine.
Otherwise, never mind; the description here is self-contained.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_3
13

14
3
Computable and c.e. Sets
Fig. 3.1 A Turing machine (without an oracle)
Fig. 3.2 The machine of Fig.3.1, one step later
as in (3.1), and
δ :

Q −{qhalt}

× {0, 1, B} →Q × {0, 1, B} × {lef t, right}
is the transition function. Informally, δ tells the machine how to move. For example,
if
δ(q8, 1) = (q5, 0, right)
and at the start of step s the machine is in state q8 and looks like Fig.3.1, then at the
start of step s + 1 the machine would be in state q5 and look like Fig.3.2.
Note that δ depends only on the current state and the symbol being scanned; it
does not depend on the number of the cell being scanned.
As input,  takes a member of ω, represented in binary without leading 0s.2
When the machine is turned on (that is, at time 0), the read/write head is scanning
cell 1, and the binary string representing the input k begins at cell 1 and ends at cell

log2(k + 1)

; all other cells contain B at time 0. Thus, if the machine looks like
Fig.3.3 at time 0, then its input is 11.
2 The empty string λ represents 0.

3.1 Turing Machines
15
Fig. 3.3 A Turing machine at time 0 with input 11
The machine  begins at time 0 in state qstart. Suppose that  is started on
input k. If  eventually enters state qhalt then  halts and we say that  accepts
k; we denote this situation by (k) ↓. Otherwise, we say that  rejects k, which
we denote by (k) ↑. If and when  with input k halts, its output is deﬁned as the
number y whose binary representation (which may contain leading 0’s) appears in
cells 1, 2, . . . , m −1, where
m = min{c : c ≥1 and cell c contains symbol B},
and we write
(k) = y.
Such an m must exist because  begins with only ﬁnitely many non-blanks on its
tape, and can change at most one blank to 0 or 1 during each step; hence the number
of non-blanks is ﬁnite when  halts. Therefore, for each k,
(k) ↓=⇒

∃y

(k) = y
	
.
Numerous variations of the Turing machine have been deﬁned and found conve-
nient for various purposes. As examples:
1. The cells could contain symbols from any ﬁnite “alphabet,” not just {0, 1, B}.
2. The number of tapes could be greater than one. It could even be countably inﬁnite.
3. The transition function δ could be “non-deterministic,” which necessitates a
broader deﬁnition of what it means for the machine to accept.
Details of these and other “loaded” versions of the Turing machine are discussed
in textbooks such as [Ma] or [Si], where they are shown to be no more powerful
than our stripped-down model. That is, a stripped-down machine can simulate the
operation of a loaded machine in a step-by-step fashion.

16
3
Computable and c.e. Sets
Other models or notations for computation that (at ﬁrst glance) seem very different
from Turing machines have been deﬁned and studied. Among them are:
1. Kleene’s recursive functions,
2. Church’s λ-calculus,
3. C++ programs,
4. Fortran programs,
5. certain types of cellular automata.
Each such model of computation, it turns out, can simulate Turing machines and vice
versa. These experiences have lent credence to the following widely-held idea:
Church’s Thesis3: Anything that can reasonably be described as discrete, automatic
computation can be simulated by a Turing machine.
Throughout this book we freely and implicitly use Church’s Thesis. That is, whenever
we describe an algorithm in pseudo-code, or prove or assume that an algorithm with
a certain input-output relation exists, we implicitly assume that the algorithm can be
simulated by a Turing machine. Think of Turing machine notation as just another
programminglanguage.So,whydowebothertointroducesuchaprimitivelanguage?
Why not instead base this entire book on the set of Python programs? We could, but
Turing machines help us visualize and reason about oracles, an essential concept
starting in Chap.5.
3.2
Computably Enumerable, Computable, c.e.n
Each Turing machine has a ﬁnite description, which can be encoded as a binary
string, because the transition function δ can be speciﬁed by a ﬁnite number of bits.
These ﬁnite-length binary strings can be put into lexicographic order.4 Therefore
the number of Turing machines is ℵ0. We name them as 0, 1, . . ., according to
lexicographic order.
3 Also known as the “Church-Turing Thesis.”.
4 If α and β are strings, then α precedes β lexicographically if either |α| < |β| or (|α| = |β| and
α precedes β alphabetically), where |γ| denotes the length of string γ. Thus, the entire set of
ﬁnite-length binary strings in lexicograph order is:
λ, 0, 1, 00, 01, 10, 11, 000, 001, . . .
.

3.2 Computably Enumerable, Computable, c.e.n
17
The following algorithm, given e, outputs a description of e:
Algorithm 3.1
count ←−1.
for i ←0 to ∞
if the ith binary string in lexicographic order is a
syntactically correct description of a Turing machine 
count ←count + 1.
if count = e
output this description of .
halt.
More deﬁnitions:
1. A partial function is a function
f : D →ω
for some D ⊆ω. The set D is the domain of f . The partial function f is a (total)
function if D = ω. We write f (k) ↓if k ∈D; otherwise we write f (k) ↑.
2. If f and g are partial functions, then
f = g
means that for each k,

f (k) ↑and g(k) ↑

or

f (k) ↓and g(k) ↓and
f (k) = g(k)

.
3. Turing machine e deﬁnes a partial function, which we denote by e. Thus,
the symbol e refers both to a Turing machine and to the partial function that it
deﬁnes; the context will disambiguate it.
4. A partial function f : ω →ω is computable if

∃e

e = f
	
.
5. Let
We =def

k : e(k) ↓

.
In other words, We is the domain of the partial function e.
6. A set A is computably enumerable (c.e.) if

∃e

A = We
	
.
Thus, W0, W1, . . . constitute the c.e. sets.

18
3
Computable and c.e. Sets
7. The complement of A is
¯A = def ω −A.
8. If A and ¯A are both c.e. then A is computable (as is ¯A).
9. If A is c.e. but not computable then A is c.e.n.
10. A computable enumeration of a set A is a sequence of ﬁnite sets
A0 ⊆A1 ⊆· · ·
such that
(i) 
i Ai = A,
(ii) there is an algorithm that, given i, outputs the set Ai .
11. A standard enumeration of A is a sequence of distinct numbers a0, a1, . . . such
that
A = {a0, a1, . . .},
and that the total function f : ω →A deﬁned by f (i) = ai is computable.
Lemma 3.1 A set has a standard enumeration ⇐⇒it has a computable enumer-
ation.
The proof of Lemma 3.1 is left as Exercise 6.
Lemma 3.2 A set has a standard enumeration ⇐⇒it is c.e.
Proof Fix A.
First, suppose that A has a standard enumeration {a0, a1, . . .}. The following
algorithm is given k as input:
Algorithm 3.2
for i ←0 to ∞
if ai = k
accept.
Thus,
A = {k : Algorithm 3.2 accepts k }.
By Church’s Thesis, there is a Turing machine that implements Algorithm 3.2, and
so A is c.e.5
To show the converse, assume that A is c.e.; that is, there exists e such that A = We.
Consider the following algorithm, which takes no input:
5 This is our ﬁrst and last explicit use of Church’s Thesis, but (as already pointed out) we will often
use it implicitly.

3.3 An Example of a c.e.n. Set
19
Algorithm 3.3
for s ←1 to ∞
for k ←0 to s
if (e halts on input k within s steps) and (k has not yet been output)
output(k).
Because Algorithm 3.3 outputs no number more than once, and because
(∀k)[ k ∈A ⇐⇒Algorithm 3.3 outputs k ],
Algorithm 3.3 outputs a standard enumeration of A.
□
Lemmas3.1and3.2togetherimplythatasetisc.e.ifandonlyifithasacomputable
enumeration; this explains the etymology of “computably enumerable.”
What is the etymology of “computable?” Suppose A is computable, that is, A and
¯A are both c.e. Then there exist e and e′ such that A = We and ¯A = We′. Then the
following algorithm, given k, ascertains whether k ∈A:
Algorithm 3.4
for s ←1 to ∞
if e halts on input k within s steps
output(“k is in A”) and halt.
if e′ halts on input k within s steps
output(“k is not in A”) and halt.
Algorithm3.4isguaranteedtohalt;hencetheproblemofdeterminingmembership
in A is “computable.”
3.3
An Example of a c.e.n. Set
Let < i, j > denote the pairing function deﬁned in Appendix A. Let
H =def

⟨e, k⟩: e(k) ↓

.
The set H is called “the halting problem,” the problem being, given e and k, to
ascertain whether ⟨e, k⟩∈H. We will show that H is c.e.n.
Fact 3.1 H is c.e.
Proof Consider the following algorithm, that is given m as input:

20
3
Computable and c.e. Sets
Algorithm 3.5
e, k ←the unique numbers such that m = ⟨e, k⟩.
Use Algorithm 3.1 to ﬁnd a description of e.
for s ←1 to ∞
if e halts on input k within s steps
halt.
Algorithm 3.5 halts on input m if and only if m ∈H. Hence H is c.e.
□
Fact 3.2 H is not computable.
Proof Assume that H were computable. Then the total function
f (⟨e, k⟩) =

1, if e(k) ↓
0, if e(k) ↑
is computable. Note that f is a function of one variable, call it m, but we think of it
as a function of two variables e and k such that m = ⟨e, k⟩.
The partial function
g(i) =

5,
if f

⟨i, i⟩

= 0
↑, if f

⟨i, i⟩

= 1
is computable; let j be such that  j = g. Then
g( j) ↓=⇒f ( ⟨j, j⟩) = 0 =⇒ j( j) ↑=⇒g( j) ↑
and
g( j) ↑=⇒f ( ⟨j, j⟩) = 1 =⇒ j( j) ↓=⇒g( j) ↓.
Thus, whether or not g( j) ↓, we have a contradiction.
□
Facts 3.1 and 3.2 together imply:
Lemma 3.3 H is c.e.n.
In subsequent chapters, many results begin “Let A be c.e.n. Then . . .” By
Lemma 3.3, such results are not vacuously true.

3.6 Exercises
21
3.4
What’s New in This Chapter?
This chapter introduces Turing machines, Church’s Thesis, and the deﬁnitions of
computable, c.e., and c.e.n. sets.
Chapter 5 will introduce Turing machines augmented with an “oracle;” upon those
enhanced machines all subsequent chapters will be built.
3.5
Afternotes
More thorough introductions to Turing machines and evidence for Church’s Thesis
can be found in books on computability, such as [Ma], which is highly intuitive and
has many good examples and exercises, or [Ro] and [Si], which are more concise.
Kleene’s recursive function notation is described and studied in [Ro].
In the literature, during the 1990s or so, the adjective “computable” replaced the
older term “recursive,” just as “computably enumerable (c.e.)” replaced “recursively
enumerable (r.e.).” “Decidable” is another synonym for “computable,” as is “Turing-
recognizable” for “c.e.”
Some authors write “c.e.a” (abbreviating “c.e. above 0”) to denote what we are
calling “c.e.n.”
3.6
Exercises
1. For each of the following statements, say whether it is true for all A, and prove
your answer:
(a) A is computable ⇐⇒
¯A is computable.
(b) A is c.e. ⇐⇒
¯A is c.e.
(c) At least one of { A, ¯A} is c.e.
(d) A is computable ⇐⇒A has a standard enumeration a0 < a1 < · · ·
(in other words, A is computable if and only if its elements can be computably
enumerated in increasing order).
2. Let A be c.e.n.
(a) Does A have a non-c.e. subset?
(b) Does A have an inﬁnite, computable subset?
3. Let A be non-computable, and let
B0 = {k ∈A : k is even}
and

22
3
Computable and c.e. Sets
B1 = {k ∈A : k is odd}.
(a) Must at least one of { B0, B1 } be computable?
(b) Must at least one of { B0, B1 } be non-computable?
4. Let A be c.e.n. Might ¯A be c.e.?
5. Let A be c.e. Prove that the set
{e : A = We }
is inﬁnite.
6. Prove Lemma 3.1.
7. If we change line 4 of Algorithm 3.4 to
if e′ halts on input k within 2s steps
would Algorithm 3.4 still be guaranteed to halt?
8. We proved Lemma 3.3 mainly to show that there exists a c.e.n. set. Alternatively,
could the existence of a c.e.n. set be shown by a cardinality argument?
9. Let a0, a1, . . . be a standard enumeration of an inﬁnite set A. Let
T =

t : (∀s > t)[as > at]

.
Prove that T is inﬁnite.

Chapter 4
Priorities (A Splitting Theorem)
In the beginning, there was diagonalization. Take another look at Sect.2.3, which
introduces the ideas of requirements and witnesses.
In this chapter, we construct sets B0 and B1 that satisfy certain requirements.
Initially, both B0 and B1 are empty. Our algorithm works in inﬁnitely many “stages.”
During each stage, it adds just one number either to B0 and to B1. The work during
each stage is performed in a ﬁnite amount of time, which ensures that both B0 and
B1 are c.e. The requirements (which are inﬁnite in number) might interfere with each
other; to maintain law and order, we assign priorities to the requirements, and each
one is eventually satisﬁed.
This technique is called a “priority argument.” We will see more priority argu-
ments, in Chaps.7–9, 11–14.
4.1
A Priority Argument
Theorem 4 Let A be c.e.n. Then A can be partitioned into c.e.n. sets B0 and B1.
Proof We describe an algorithm that, given a c.e.n. set A, partitions it into c.e.n. sets
B0 and B1. What does it mean to be “given” an inﬁnite c.e. set? We are actually given
a number j such that W j = A. We could use j to output a standard enumeration
a0, a1, . . . of A as follows:
for s ←1 to ∞
for k ←0 to s
if k has not been output yet
if  j halts on input k within s steps
output(k).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_4
23

24
4
Priorities (A Splitting Theorem)
Here we did not specify how to produce a description of  j, given j; for that, see
Algorithm 3.1.1
For each s, let
As =def {a0, a1, . . . , as }.
We proceed in stages: during each stage s, we put as into B0 or B1 (but not into
both), thereby ensuring that B0 and B1 partition A. Each stage will be computable in
a ﬁnite amount of time, and so both B0 and B1 are c.e. We also need to ensure that
both B0 and B1 are non-computable. How will we do that?
A simple approach would be to assign a0, a2, a4, . . . to B0, and a1, a3, a5, . . . to
B1. This would ensure that either B0 or B1 is non-computable (because otherwise
the set A = B0 ∪B1 would be computable), but there’s no reason why both of them
must be.
Instead of this simple alternation of assignments, we use a more subtle approach.
In particular, we construct B0 and B1 so as to satisfy the requirements
Re,i : We ̸= Bi
(i.e., We and Bi are not complementary) for each e ∈ω and i ∈{0, 1}. Collectively,
the requirements Re,0 ensure that B0 is not c.e., and hence that B0 is not computable.
Likewise, the requirements Re,1 collectively ensure that B1 is not c.e., and hence that
B1 is not computable. During each stage, we try to meet (i.e., satisfy) a requirement
that is not yet met. There are inﬁnitely many unmet requirements; we choose to meet
the one of strongest “priority,” where the requirements are prioritized as follows:
R0,0 ≺R0,1 ≺R1,0 ≺R1,1 ≺R2,0 ≺R2,1 ≺· · · .
Two sets are not complementary if and only if there is an n in both of them or in
neither. Therefore we meet requirement Re,i by producing an n that is in both of We
and Bi, or in neither; such an n is a witness for Re,i. We have some control over Bi,
because we can assign elements of A either to Bi or to B1−i. However, we have no
control over We. Therefore, because We might not be computable, we have no way
to determine whether a given n, not yet enumerated in We, eventually will be.
Our algorithm is as follows, where we,0, we,1, . . . is a standard enumeration of
We, and
We[s] =def {we,0, we,1, . . . , we,s }.
(4.1)
What it means for requirement Re,i to act is deﬁned by the comment just before line
8 of Algorithm 4.1.
1 In the remainder of this book, we won’t specify how we produce a description of e given e, or
how we are “given” a c.e. set.

4.1 A Priority Argument
25
Algorithm 4.1
1
B0 ←∅.
2
B1 ←∅.
3 for s ←0 to ∞
// This is stage s.
// Assign as either to B0 or to B1.
4
assigned ←false.
5
for e ←0 to s
6
for i ←0 to 1
7
if (not assigned) and as ∈We[s] and We[s] ∩Bi = ∅
// Requirement Re,i acts.
8
Put as into Bi.
// This action ensures that We ∩Bi ̸= ∅.
9
assigned ←true.
10
if not assigned
11
Put as into B0.
// Or put as into B1, it doesn’t matter.
Note that for each i ∈{0, 1},
¯A ⊆Bi
(4.2)
(the Venn diagram in Fig.4.1 may be helpful for seeing this).
We need to verify the algorithm; that is, we need to show that each requirement
is met. Fix e. We will show that the requirement Re,0 is met (the proof that Re,1 is
met is analogous, substituting Re,1 for Re,0, and B1 for B0).
Assume for a contradiction that Re,0 were not met; in other words, assume that
We = B0.
(4.3)
Let
A′ =

a :

∃s

a = as ∈We[s]
 
.
Fig. 4.1 B0 and B1
partition A

26
4
Priorities (A Splitting Theorem)
Fig. 4.2 {4, 8, 9} ⊆A′
Informally, A′ consists of all numbers that are enumerated in We before, or at the
same time as, being enumerated in A. In Fig.4.2, such numbers are indicated by line
segments. Note that
A′ ⊆A ∩We
and that

∀t

we,t ∈A =⇒we,t ∈At ∪A′ 
(4.4)
(see Exercise 2).
Case 1: A′ is inﬁnite.
Only ﬁnitely many requirements have stronger priority than that of Re,0, and no
requirement acts more than once. Hence there exists a stage x after which none of
those stronger priority requirements acts. Because A′ is inﬁnite, there exists s > x
such that as ∈A′.
Because s > x, when the condition in line 7 of Algorithm 4.1 is evaluated so as
to decide whether Re,0 will act during stage s, it must be that
assigned = false.
Because as ∈A′, we have
as ∈We[s].
Therefore
We[s] ∩B0 ̸= ∅,
(4.5)
because otherwise Re,0 would act during stage s, contradicting (4.3). However, (4.5)
itself contradicts (4.3).
Case 2: A′ is ﬁnite.
In other words, there are only ﬁnitely many line segments in Fig.4.2.

4.1 A Priority Argument
27
Proposition: ¯A is c.e.
Proof Consider the following algorithm.
Algorithm 4.2
1 for t ←0 to ∞
2
if we,t /∈At ∪A′
3
output we,t.
Fix p.
Case 2.1: p ∈¯A.
Then p ∈B0 = We (by (4.2) and (4.3)) and so p = we,t for some t. Because
p = we,t /∈A and At ∪A′ ⊆A, we have
we,t /∈At ∪A′,
and therefore Algorithm 4.2 outputs we,t = p.
Case 2.2: p /∈¯A.
If p /∈We then Algorithm 4.2 does not output p. So assume p ∈We; then there
exists t such that
p = we,t.
Therefore, because p ∈A, we have
we,t ∈At ∪A′,
by (4.4). Hence the condition in line 2 of Algorithm 4.2 evaluates to false, and so
again Algorithm 4.2 does not output p.
Thus, Algorithm 4.2 outputs p if and only if p ∈¯A; in other words, it enumerates
¯A. The condition in line 2 of Algorithm 4.2 can be evaluated in a ﬁnite amount of
time, because
At ∪A′
is ﬁnite (because we are in Case 2). Hence ¯A is c.e.
QED Proposition
Because A is c.e., the Proposition implies that A is computable, contrary to the
assumption that A is c.e.n. This concludes Case 2.
In summary, in either Case 1 or Case 2, the assumption of (4.3) leads to a contra-
diction. Therefore Re,0 is met.
QED Theorem 4

28
4
Priorities (A Splitting Theorem)
We have just seen a simple priority argument. The essence of its simplicity is
that once a requirement Re,i acts by putting a witness into Bi, that witness remains
credible forever after, and so Re,i stays met. This will not be true in Chap.7 and
beyond, where a requirement R might act but then later get “injured,” that is, its
witness might lose its credibility, and so we would need to ﬁnd a new witness for
R. Chapter 7 introduces the “ﬁnite injury priority method,” where each requirement
is injured only ﬁnitely often. Chapter 10 introduces the beautiful “inﬁnite injury
priority method,” where a speciﬁc requirement may be injured inﬁnitely many times
(and yet somehow it all works out).
Hang on to your hat.
4.2
What’s New in This Chapter?
We just saw our ﬁrst priority argument, which involved the following new ideas:
1. Priorities among requirements.
2. The dynamic choosing of witnesses. That is, witnesses are chosen during the algo-
rithm, rather than being pre-deﬁned as they were in the diagonalization argument
of Theorem 2.
3. The deﬁnition of a certain stage, which we typically call x, after which certain
ﬁnitely occurring events never occur. We used this technique in Case 1 of the proof
of Theorem 4. It is used in all priority arguments (as far as I know). We argue that
such an x exists, even though we need not, and perhaps cannot, compute it.2
4.3
Afternotes
The splitting theorem presented here is a weaker version of the Friedberg Splitting
Theorem ([Fr58]).
2 Hence our priority-based proofs are “non-constructive.” This may be a bit confusing, because
algorithms in this discipline are traditionally called “constructions.” Thus, such a construction is
part of a non-constructive proof.

4.4 Exercises
29
4.4
Exercises
1. Suppose that we replace Algorithm 4.1 by the following:
B0 ←∅.
B1 ←∅.
for s ←0 to ∞
// Assign as either to B0 or to B1.
assigned ←false.
t ←0.
while not assigned
for e ←0 to t
for i ←0 to 1
if (not assigned) and

as ∈We[t]

and

We[t] ∩Bi = ∅

// Requirement Re,i acts.
Put as into Bi.
assigned ←true.
t ←t + 1.
Would the while loop halt for each s? Would each requirement Re,i be met?
2. Prove (4.4).
3. Let A be c.e.n. Then for each k, we can partition A into c.e.n. sets B0, B1, . . . , Bk
by applying Theorem 4, a total of k times. Describe a way to partition A into
inﬁnitely many c.e.n. sets B0, B1, . . .
4. An inﬁnite set S is sparse if for each i,
 S ∩
	
2i, 2i + 1, . . . , 2i+1 −1

  ≤1.
Show that if A is c.e.n., then A can be partitioned into c.e.n. sets B0 and B1 such
that B0 is sparse.
5. For each e ∈ω and i ∈{0, 1}, let
Se,i : We ̸= Bi
(compare Se,i to Re,i), where B0 and B1 are the sets constructed by Algorithm
4.1. Is it possible that Se,i is met for each e ∈ω and i ∈{0, 1}?
6. Suppose that we delete the words
“and We[s] ∩Bi = ∅”
from line 7 of Algorithm 4.1. Explain why the proof would be invalid.

30
4
Priorities (A Splitting Theorem)
7. (from [Po]) A set B is simple if
(i) B is c.e.,
(ii) B is inﬁnite,
(iii) B contains no inﬁnite, c.e. set.
Use a priority argument to prove that a simple set exists.
Hint: Use the requirements
Re : We is inﬁnite =⇒We ∩B ̸= ∅.
Another hint: Your algorithm should construct B so that, for each e,
 B ∩{0, 1, . . . , 2e}
 ≤e
(this will ensure that B is inﬁnite).

Chapter 5
Reductions, Comparability
(Kleene-Post Theorem)
5.1
Oracle Turing Machines
An oracle Turing machine (OTM) is an ordinary Turing machine that has been
enhanced with an “oracle tape,” which has a read-only head, as is illustrated in
Fig.5.1. The oracle tape contains an inﬁnite sequence of 0s and 1s. We interpret that
sequence as the characteristic function of some set B, deﬁned as
χB(k) =

1, if k ∈B
0, otherwise.
Thus, in Fig.5.1, we see that 2 and 4 are in B, whereas 0, 1, and 3 are not. Thus, an
OTM is given an input k on its read/write tape as depicted in Fig.3.3, as well as χB
on its read-only tape. The set B is called the oracle of the computation.
The transition function is now of the form:
δ : (Q −{qhalt}) × {0, 1, B} × {0, 1} →Q × {0, 1, B} × {lef t,right} × {lef t,right}.
For example, if
δ(q6, B, 0) = (q14, 1, lef t, right)
and at the start of step s the machine is in state q6 and looks like Fig.5.1, then at the
start of step s + 1 the machine would be in state q14 and look like Fig.5.2.
Think of the oracle tape as an inﬁnite lookup table. An OTM with oracle B is just
like a plain Turing machine, except that whenever it wants to, it can look up in the
table whether a particular number is in B.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_5
31

32
5
Reductions, Comparability (Kleene-Post Theorem)
Fig. 5.1 An oracle turing machine
Fig. 5.2 The machine of Fig.5.1, one step later
Just like a plain Turing machine, an OTM has a ﬁnite description, because even
though a particular oracle is an inﬁnite binary string, the OTM itself is speciﬁed
merely by its transition function δ. Therefore, like the plain Turing machines, the
oracle Turing machines can be ordered. Let B
e denote the eth OTM with oracle B.
The computation B
e (k) denotes the behavior of the eth OTM with input k and
oracle B. If that computation halts, then we write B
e (k) ↓(otherwise we write
B
e (k) ↑). If it halts with output y, then we write B
e (k) = y. We write B
e (k) ̸= y
if either
B
e (k) ↑or B
e (k) = some number other than y.

5.2 Turing Reductions
33
Thus, B
e deﬁnes a partial function. In analogy to e, the notation B
e refers both to
an OTM and to the partial function that it deﬁnes; the context will disambiguate it.
Often, we are interested in the usage function:
ϕB
e (k) =def
⎧
⎪⎨
⎪⎩
the rightmost position of the oracle head
during the computation B
e (k),
if B
e (k) ↓
−1,
otherwise.
Informally, ϕB
e (k) is the largest number about which the oracle is consulted during
the computation B
e (k) (if it halts). Note that ϕ is a lower-case Greek phi, whereas
 is an upper-case Greek phi.
5.2
Turing Reductions
We write A ≤T B if

∃e

∀k
	
χA(k) = B
e (k)

,
which is often abbreviated as

∃e
	
χA = B
e

,
and we say that A (Turing-)reduces to B. Informally, A ≤T B means that B is at
least as hard (computationally) as A.
Lemma 5.1 contains some simple properties of Turing reductions; we leave its
proof as Exercise 1.
Lemma 5.1 For all A, B, and C,
I. A ≤T A (reﬂexivity)
II. A ≤T ¯A
III. (A ≤T B) and (B ≤T C) =⇒(A ≤T C) (transitivity)
IV. (A ≤T B) and B is computable =⇒A is computable.
If A ≤T B and B ≤T A then we write
A ≡T B
and say that A and B are Turing equivalent. We write
A <T B
if A ≤T B but B ≰T A. Sets A and B are (Turing) incomparable if A ≰T B and
B ≰T A.

34
5
Reductions, Comparability (Kleene-Post Theorem)
Three questions arise, in analogy with those that arose in the theory of inﬁnite
cardinals1:
1. Is there an inﬁnite sequence of sets A0, A1, . . . such that A0 <T A1 <T · · · ?
2. Are there Turing incomparable sets?
3. Is there a set A such that ∅<T A <T H? Recall that H is the halting problem,
deﬁned in Chap.3. We phrased this question in terms of the empty set ∅, but
could have done so in terms of any computable set, because all computable sets
are Turing equivalent to each other.
The answer to all three questions is “yes.” Exercise 10(a) answers question (5.1).
Theorem5 answers questions (2) and (3).
5.3
The Theorem
Theorem 5 (Kleene-Post) There exist incomparable sets A and B below H, that is,
A ≰T B,
B ≰T A,
A <T H,
B <T H.
Proof We construct incomparable sets A and B in stages. In particular, we deﬁne a
sequence of ﬁnite-length binary strings
α0 ≺α1 ≺α2 ≺· · ·
where α j ≺α j+1 means that a j is a proper preﬁx of a j+1. A preﬁx of a string
c0c1 · · · ck is deﬁned as a string c0c1 · · · c j for some j ≤k; the preﬁx is proper if
j < k. Informally, α j ≺α j+1 means that a j+1 is an extension of a j. The use of the
binary operator “≺” on strings is unrelated to its use on requirements; the context
will disambiguate between them.
The set A is deﬁned by its characteristic function
χA = lim
j→∞α j ,
as is illustrated in Fig.5.3.
1 In Chap.2, we used the power set operation to obtain an inﬁnite sequence of ever larger sets. By
Lemma2.2, all pairs of sets are comparable in cardinality. The question of whether there exists a
set strictly larger than the integers but smaller than the reals is the famous “continuum hypothesis;”
it’s a long story (see [Coh]).

5.3 The Theorem
35
Fig. 5.3 Deﬁning a set A by a sequence of ﬁnite-length binary strings
Analogously, we deﬁne
β0 ≺β1 ≺β2 ≺· · · ,
and B by its characteristic function
χB = lim
j→∞β j .
Sets A and B will meet the requirements (for all e):
R2e : χA ̸= B
e
R2e+1 : χB ̸= A
e .
Collectively, the even-numbered requirements guarantee that A ≰T B, and the odd-
numbered requirements guarantee that B ≰T A. Requirement R2e can be written
as
(∃n)[ χA(n) ̸= B
e (n) ];
such an n is a witness for R2e. Algorithm 5.1, which constructs A and B, comes up
with a witness for each R2e, and one for each R2e+1 (i.e., an n such that χB(n) ̸=
A
e (n)).
Here is some more notation used in the pseudo-code for Algorithm 5.1, where σ
is a binary string of ﬁnite length:
1. σ⌢0 denotes σ appended with a 0, and σ⌢1 denotes σ appended with a 1.
2. σ
e (n) ↓means that the eth OTM, with the ﬁrst |σ| bits on its oracle tape con-
stituting σ, and with input n, halts, and during that computation the oracle head
is never to the right of cell |σ|. Figure5.4 illustrates such a machine at time 0;
the word “irrelevant” is based on the assumption that σ
e (n) ↓(Fact5.1 below
clariﬁes this).

36
5
Reductions, Comparability (Kleene-Post Theorem)
Fig. 5.4 The start of a halting computation, with σ as an oracle
If σ
e (n) ↓then the machine must halt with some output y; we denote this situation
by
σ
e (n) = y.
In Algorithm 5.1, our only interest is whether
σ
e (n) = 1.
The key idea behind Algorithm 5.1 is:
Fact 5.1 For all e, n, σ, and τ,

σ ≺τ and σ
e (n) ↓

=⇒

σ
e (n) = τ
e(n)

.
Proof During the computation τ
e(n), the oracle head never moves past the right
boundary of σ.
□

5.3 The Theorem
37
Algorithm 5.1
1 α0 ←λ.
// Recall that λ denotes the empty string.
2 β0 ←λ.
3 for e ←0 to ∞
// Deﬁne α2e+1 and β2e+1 so as to meet R2e.
4
n ←|α2e|.
// Thus, n is the least number whose
// membership in A is yet to be speciﬁed.
5
if

∃σ
	
β2e ≺σ and σ
e (n) = 1

6
α2e+1 ←α2e ⌢0.
// Thus, n /∈A.
7
β2e+1 ←σ.
// Thus, B
e (n) = 1.
else
8
α2e+1 ←α2e ⌢1.
// Thus, n ∈A.
9
β2e+1 ←β2e ⌢0.
// Or β2e ⌢1, it doesn’t matter.
// Deﬁne α2e+2 and β2e+2 so as to meet R2e+1.
10
n ←|β2e+1|.
// Thus, n is the least number whose
// membership in B is yet to be speciﬁed.
11
if

∃σ
	
α2e+1 ≺σ and σ
e (n) = 1

12
β2e+2 ←β2e+1 ⌢0.
// Thus, n /∈B.
13
α2e+2 ←σ.
// Thus, A
e (n) = 1.
else
14
β2e+2 ←β2e+1 ⌢1.
// Thus, n ∈B.
15
α2e+2 ←α2e+1 ⌢0. // Or α2e+1 ⌢1, it doesn’t matter.
We need to verify the algorithm, that is, to prove that all the requirements are
met. Consider requirement R2e. Suppose the condition in line 5 of Algorithm 5.1
evaluates to true, that is,

∃σ
	
β2e ≺σ and σ
e (n) = 1

.
Then
χA(n) = 0 ̸= 1 = σ
e (n) = B
e (n),
by Fact5.1. On the other hand, if
¬

∃σ
	
β2e ≺σ and σ
e (n) = 1

then
χA(n) = 1 ̸= B
e (n),
because either
B
e (n) ↑or B
e (n) = some number other than 1.

38
5
Reductions, Comparability (Kleene-Post Theorem)
In either case n is a witness for R2e, and hence R2e is met. In the latter case, we
extend β2e with either 0 or 1 (see line 9 of Algorithm 5.1). In that case, why do we
bother to extend β2e at all? We do it to ensure that χB is a total function.
The proof that R2e+1 is met is analogous.
Suppose that we had an oracle for H. Then we could ascertain whether

∃σ
	
β2e ≺σ and σ
e (n) = 1

(5.1)
and, if so, produce such a σ; likewise we could ascertain whether

∃σ
	
α2e+1 ≺σ and σ
e (n) = 1

(5.2)
and, if so, produce such a σ (see Exercise 5). Therefore
A ≤T H
and B ≤T H ,
which implies
A <T H
and B <T H
(see Exercise 6).
QED Theorem 5
If A and B are incomparable then neither is computable, because a computable set
reduces to every set. Thus, we have now seen two ways to guarantee that a constructed
set C is non-computable:
1. ensure that We ̸= C for each e, as we did in the proof of Theorem4,
2. construct a set D such that C ≰D, as we did in the proof of Theorem5.
The incomparable sets A and B constructed by Algorithm 5.1 do not appear to
be c.e., because while determining (without the help of an oracle for H) whether
(5.1) or whether (5.2) is true, the algorithm could search forever for the extension
σ. In the vernacular, Algorithm 5.1 might “go out to lunch and not come back.” In
Chap.7, we will see a more subtle algorithm that constructs a pair of incomparable
sets in such a way that each stage is guaranteed to run in a ﬁnite amount of time, and
so those two sets will be c.e.
5.4
What’s New in This Chapter?
1. Oracle Turing machines.
2. Usage functions.
3. Turing reductions.

5.6 Exercises
39
4. Deﬁning a set by an inﬁnite sequence of ﬁnite preﬁxes of its characteristic func-
tion.
5. Protecting a requirement from future injury. In Theorem 5, we were able to guar-
antee such protection; however, in Chap.7 and beyond, we will try to avoid injuries
but will not always succeed.
5.5
Afternotes
In this book, the only reductions that we study are Turing reductions. However, many
other types of reductions have been deﬁned and studied (see [Ro]). Turing reductions
seem the most natural, because A ≤T B corresponds to the idea of algorithm A using
algorithm B as a subroutine.
The practice of using upper-case Greek letters to denote OTM computations and
corresponding lower-case Greek letters to denote their usage functions has become
common in the literature. For example, a paper might refer to OTM’s  and  and
their corresponding usage functions ϕ and ψ. This notation may seem strange at ﬁrst,
but it grows on you.
5.6
Exercises
1. Prove Lemma 5.1.
2. Is ≤T commutative?
3. If A ≤T B and B is c.e., must A be c.e.?
4. Let B be computable. Prove that for each A,
A ≤T B ⇐⇒A is computable
(thus, computable oracles are useless for our purposes).
5. Describe an algorithm that uses an oracle for H to ascertain whether (5.1) is true
and, if so, to produce such a σ. Do likewise for (5.2).
6. Let A, B, and C be sets such that A and B are incomparable, A ≤T C, and
B ≤T C. Prove that A <T C and B <T C.
7. Find the ﬂaw in the following argument: Sets A and B constructed by Algorithm
5.1 are each enumerated in ascending order. Therefore, by Exercise 1(d) of
Chap.3, they are both computable.
8. In Fig.5.3,
α0 = 011
α1 = 011101011
α2 = 0111010110010
α3 = 0111010110010110

40
5
Reductions, Comparability (Kleene-Post Theorem)
However,thissituationisimpossibleiftheαstringsareconstructedbyAlgorithm
5.1. Why?
9. The jump of A is deﬁned as
A′ =

e : A
e (e) ↓

.
Prove the following, for all A and B:
(a) A is computable =⇒A′ ≡T H
(b) A <T A′
(c) A ≤T B =⇒A′ ≤T B′
10. (a) Prove that there is an inﬁnite sequence
A0 <T A1 <T · · · .
(b) Prove that if A0 <T A1 <T . . . then
(∃B)(∀i)[Ai <T B].

Chapter 6
The Permanence Lemma
This chapter introduces some handy notation, and one little—but quite useful—
lemma about computations with c.e. oracles.
6.1
Notation
The following three pieces of notation help to simplify expressions, and are ubiqui-
tous in the literature. At ﬁrst glance they may seem unimportant, but their value will
become apparent as we delve into more complicated arguments.
1. A↾↾k =def A ∩{0, 1, . . . k }.
2. A↾k =def A ∩{0, 1, . . . k −1}.
3. The sufﬁx “[s]” appended to an expression means that everything that can be
modiﬁed by a stage s, is so modiﬁed.
As examples:
(a) B
e (k)[s] denotes the result of the computation Bs
e (k) after s steps (note that
Bs is the oracle here rather than B).
(b) ϕB
e (k)[s] denotes the usage of the computation Bs
e (k) after s steps. Note that

∀e′, C, k, z

ϕC
e′ (k)[z] ≤z

,
because the oracle head starts at cell 0 of the oracle tape and moves at most
one position during each step. In the extreme case, it moves to the right on
each step, resulting in
ϕC
e′ (k)[z] = z.
(c) (C ∩D)[s] = Cs ∩Ds.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_6
41

42
6
The Permanence Lemma
6.2
The Lemma
Suppose that D is a c.e. set, with a computable enumeration
D0 ⊆D1 ⊆· · ·
Fix k and e, and suppose that D
e (k) ↓. Must it be that
D
e (k) = lim
s D
e (k)[s] ?
Must it be that
ϕD
e (k) = lim
s ϕD
e (k)[s] ?
The answer to both questions is “yes;” this follows from Lemma6.1, which we call
the “Permanence Lemma.”
These answers might not be obvious; one might imagine a sequence
t0 < t1 < · · ·
such that
1. D
e (k)[t0] ↓
2. some d1 ≤ϕD
e (k)[t0] enters D after stage t0 (that is, d1 ∈D −Dt0), and causes
D
e (k)[t1] ↑
3. D
e (k)[t2] ↓
4. some d2 ≤ϕD
e (k)[t2] enters D after stage t2, and causes D
e (k)[t3] ↑
and so forth. Thus, lims D
e (k)[s] would not exist. In this scenario, is it possible that
D
e (k) ↓?
The answer to the question is “no,” which follows from the Permanence Lemma.
We say that a computation D
e (k)[s] is permanent if
D
e (k)[s] ↓and Ds ↾↾u = D↾↾u ,
where u = ϕD
e (k)[s]. Informally, this says that no number in D −Ds is small enough
to affect the computation D
e (k)[s]. Note that if the computation D
σ (k)[s] is per-
manent then for each t ≥s,
D
e (k)[t] = D
e (k)[s] and
ϕD
e (k)[t] = ϕD
e (k)[s].

6.4 Exercises
43
For that matter, all other aspects of the computation D
σ (k)[t] are the same as for
D
σ (k)[s]; for example, if the read/write head is positioned at cell 37 exactly 52 times
during the computation D
σ (k)[s] then it will do likewise during the computation
D
σ (k)[t].
Lemma 6.1 (Permanence Lemma) Let D0 ⊆D1 ⊆· · · be a computable enumer-
ation of D. Suppose D
e (k) ↓. Then there exists an s such that the computation
D
e (k)[s] is permanent.
Proof Let x be such that the computation D
e (k) halts in fewer than x steps. Let
s > x be such that
Ds ↾↾ϕD
e (k) = D↾↾ϕD
e (k).
(6.1)
The computation Ds
e (k) is identical to the computation D
e (k); hence it, too, halts
in fewer than x steps, and therefore it halts in fewer than s steps. Hence
ϕD
e (k) = ϕD
e (k)[s].
Therefore, by (6.1),
Ds ↾↾ϕD
e (k)[s] = D↾↾ϕD
e (k)[s],
and so the computation D
e (k)[s] is permanent.
□
6.3
Afternotes
The sufﬁx “[s]” notation was introduced in [La79].
6.4
Exercises
1. Suppose that the computation D
e (k)[s] is permanent. Must it be that
D
e (k) = D
e (k)[s]?
2. In the proof of the Permanence Lemma, must the computation D
e (k)[x] be per-
manent?

44
6
The Permanence Lemma
3. Suppose d0, d1, . . . is a standard enumeration of D, and e and k are such that

∃y

∀z ≥y

D
e (k)[z] = 5

.
Must D
e (k) = 5?

Chapter 7
Finite Injury (Friedberg-Muchnik
Theorem)
In this chapter, the Kleene-Post construction of Chap.5 is enhanced with the idea of
priorities introduced in Chap.4, along with a key new idea: injury.
7.1
The Theorem
Theorem 7 (Friedberg-Muchnik) There exist incomparable, c.e. sets.
Proof In Algorithm 7.1, initially A = B = ∅. During each stage s, a ﬁnite number
of elements might be added to A, and a ﬁnite number to B. Let As and Bs denote the
sets A and B, respectively, at the start of stage s.1 Then
A0 ⊆A1 ⊆· · ·
and
B0 ⊆B1 ⊆· · · .
The set A is deﬁned as 
s As, and B as 
s Bs. This is more ﬂexible than the Kleene-
Post construction (Algorithm 5.1), because there the elements of A were inserted in
increasing order (likewise for B), whereas here that might not happen.
The requirements are identical to those used in the Kleene-Post Theorem, namely:
R2e : χA ̸= B
e
R2e+1 : χB ̸= A
e .
1 We use this notation, namely Ds to denote a constructed set D at the start of stage s, from here
until the end of this book.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_7
45

46
7
Finite Injury (Friedberg-Muchnik Theorem)
Algorithm 5.1 met the requirements R0, R1, . . . in that order. It could get stuck
while working on a particular requirement, and so the constructed sets were not
necessarily c.e. Algorithm 7.1 meets the requirements in a more complicated order.
We prioritize the requirements as follows:
R0 ≺R1 ≺· · · .
The strategy for meeting a single requirement R2e is to attach to it a potential witness
n not yet in A. If we eventually reach a stage s such that
n /∈As and B
e (n)[s] = 0,
then we say that R2e needs attention at stage s.
Analogously, R2e+1 needs attention at stage s if
n /∈Bs and A
e (n)[s] = 0,
for its witness n.
At the start of a given stage s, many requirements might need attention. We attend
to the one of strongest priority. That is, we pick the least j such that R j needs
attention. Suppose that this j is even (otherwise we would swap the roles of A and
B); so j = 2e for some e. We put n j, which is the witness for R j, into A, and we
say that R j receives attention. Then we try to restrain all numbers less than or equal
to ϕB
e (n j)[s] from entering B. If we succeed in restraining all such numbers from
entering B during or after stage s, then we would have
Bs ↾↾ϕB
e (n j)[s] = B ↾↾ϕB
e (n j)[s].
(7.1)
If (7.1) is true then the computation B
e (n j)[s] would be “protected” or “preserved;”
in particular, we would have
B
e (n j)[s] = B
e (n j) = 0
and so R j would be met.
On the other hand, if R j never needs attention at stage s or beyond, that is,

∀t ≥s

B
e (n j)[t] ̸= 0

,
then we would never put n j into A. In this case, we would have
n j /∈A and B
e (n j) ̸= 0
and so, again, R j would be met. Note that B
e (n j)[t] ̸= 0 means either
B
e (n j)[t] ↑or

B
e (n j)[t] = some number other than 0

.

7.1 The Theorem
47
This simple construction might fail, because we might not be able to keep certain
numbers less than or equal to ϕB
e (n j)[s] from entering B. Such a number might enter
B during a stage t ≥s at the behest of a stronger priority requirement, and thereby
“spoil” the computation B
e (n j)[s] (that is, “injure” R j). The term injury, though
widely used, is a bit misleading. The entrance of such a small number into B during
stage t may or may not invalidate the witness n j (that is, it may or may not cause
B
e (n j) ̸= 0). So, rather than saying that witness n j is “injured,” it is more accurate
to say that it has “lost credibility.” However, we need not ascertain whether R j has
actually become injured; rather, we take a cautious approach and assume that if it
might have been injured, it was. It’s like ﬁnding an open box of cereal on the shelf
of a grocery store; its contents may or may not have been “injured,” but we play it
safe by reaching for a different box.
Thus, our witness may lose credibility.2 If it does, then we choose a fresh witness
for R j and keep watching until R j again needs attention (which might never happen).
By “fresh,” we mean that the number is greater than every witness chosen so far, and
greater than the usage (that is, the φ value) of every computation performed so far.
These ideas are clariﬁed in the pseudo-code for Algorithm 7.1. It calls a subroutine
Initialize(k) that assigns a fresh witness to Rk (that is, it assigns a fresh number to the
variable nk). What it means for a requirement R j to act is deﬁned by the comment
just before line 8 of Algorithm 7.1.
Algorithm 7.1
1
A ←∅.
2
B ←∅.
3 n0 ←0.
// This is equivalent here to Initialize(0).
4 for s ←1 to ∞
5
Initialize(s).
6
for j ←0 to s
7
if R j needs attention
// R j acts.
8
Put n j into A (if j is even; otherwise put it into B).
9
Initialize(k) for each k such that j < k ≤s.
A ﬁnite amount of work is performed during each stage of Algorithm 7.1 (whereas
Algorithm 5.1 could get stuck in one stage). Thus, the sets A and B constructed by
this algorithm are c.e.
2 In other words, our witness may be tampered with.

48
7
Finite Injury (Friedberg-Muchnik Theorem)
The theorem is implied by the following lemma:
Lemma 7.1 For each j, R j acts only ﬁnitely often, and is met.
Proof We proceed by strong induction on j.
For the basis, consider j = 0. The witness for R0, namely the variable n0, receives
the value 0 by line 3 of Algorithm 7.1, and never changes, because there is no stronger
priority requirement than R0. If R0 ever needs attention, then it acts and can never
subsequently be injured, and so
0 ∈A and B
e (0) = 0.
Otherwise (R0 never needs attention)
0 /∈A and B
e (0) ̸= 0
(B
e (0) ̸= 0 follows from the converse of the Permanence Lemma). In either case,
R0 is met.
For the inductive step, ﬁx j ≥1. Suppose that j = 2e for some e; otherwise
j = 2e + 1 and the roles of A and B would be swapped. Assume the claim for each
i < j. Then the variable n j has a ﬁnal value; call it n. If R j ever needs attention after
n j receives n, then it acts and can never subsequently be injured, and so
n ∈A and B
e (n) = 0.
Otherwise (R j never needs attention after n j receives n)
n /∈A and B
e (n) ̸= 0
(as in the basis, B
e (0) ̸= 0 here follows from the converse of the Permanence
Lemma). In either case, R j is met.
QED Lemma 7.1
QED Theorem 7
7.2
What’s New in This Chapter?
1. A priority argument with injuries.
2. A witness “needing attention.”
3. A “fresh” witness.

7.4 Exercises
49
7.3
Afternotes
The Kleene-Post algorithm gave us a set A such that ∅<T A <T H. This led to the
question:
Is there a c.e. set A such that ∅<T A <T H?
This question, known as Post’s Problem, was posed in 1944, and was considered the
central question in recursive function theory (which is now called “computability
theory”) until it was answered in the afﬁrmative (it’s a corollary of Theorem7) in
1956, independently by two teenagers—an American named Friedberg [Fr57] and
a Russian named Muchnik [Mu]. Friedberg and Muchnik solved the problem by
inventing essentially the same technique, the “ﬁnite-injury priority method,” which
we saw in this chapter. The method turned out to have many more uses, both in
computability theory and in other branches of logic.
In [Ku], the Friedberg-Muchnik Theorem was proved without using a priority
argument.
7.4
Exercises
1. Why is it necessary that distinct requirements have distinct witnesses?
2. (a) Give an upper bound on the number of times that R0 can act.
(b) Do the same thing for R1.
(c) Do the same thing for R j, for all j ≥1.
(d) Modify Algorithm 7.1 so as to obtain a smaller upper bound than was found
in part (c).
3. (a) Prove that for each s, at the start of stage s,
n0 < n1 < · · · < ns−1
(note that for each j ≥s, at the start of stage s, variable n j has never received
a value).
(b) For each j, the value of the variable n j changes only ﬁnitely often. Let ˜n j
denote the ﬁnal value of n j, and let
˜N = {˜n j : j ∈ω}.
Is ˜N c.e.?
4. Modify Algorithm 7.1 so that not only are the constructed sets A and B incom-
parable and c.e., but also
(a) A and B are sparse (as deﬁned in Exercise 4 of Chap.4).
(b) A ⊆C and B ⊆C, where C is a given inﬁnite, c.e. set.

50
7
Finite Injury (Friedberg-Muchnik Theorem)
(c) A ⊆B.
(d) all three of the above.
5. Prove that there is an inﬁnite family of c.e. sets whose members are pairwise
incomparable.
6. (from [Tr]) A set A is autoreducible if (∃e)(∀k)
k ∈A =⇒A−{k}
e
(k) = 1
and
k /∈A =⇒A−{k}
e
(k) = 0.
In other words, the eth oracle Turing machine determines whether a given k is in
A by asking its oracle questions of the form “Is m ∈A?” for various m’s other
than k.
Describe an algorithm that constructs a c.e. set A that is not autoreducible.
Hint: Use a ﬁnite-injury priority argument. Deﬁne requirements Re that collec-
tively ensure that A is not autoreducible. Then deﬁne what it means for Re to need
attention at stage s. Finally, write pseudo-code for your algorithm.
7. Describe an algorithm that constructs a pair of incomparable, non-autoreducible,
c.e. sets.
8. Let t : ω →ω be a computable total function.
(a) Show that there exists a computable total function
f : ω →{0, 1}
such that every Turing machine that computes f requires more than t(n)
steps to compute f (n) for at least one value of n. This says, for example, that
there is a computable function, with range {0, 1}, that cannot be computed in
worst-case time less than 22n.
Hint: diagonalize.
(b) Strengthen the claim of part (a) by showing that it remains true with “at least
one value” replaced by “inﬁnitely many values.”
(c) Show that it remains true with “at least one value” replaced by “all but ﬁnitely
many values.”
Hint: use a priority argument.

Chapter 8
Permitting (Friedberg-Muchnik
Below C Theorem)
Suppose that we are given a c.e.n. set C, and we wish to construct a c.e. set A below
C (that is, A ≤T C), while A satisﬁes certain other properties as well. Permitting is
a way to do this.
The idea is that a number may enter A only when C permits it. There are various
types of permitting; the simplest is based on the following lemma.
8.1
The Lemma
Lemma 8.1 (Permitting Lemma) Let A0 ⊆A1 ⊆· · · and C0 ⊆C1 ⊆· · · be com-
putable enumerations of A and C, respectively. If

∀s, n

n ∈As+1 −As =⇒Cs ↾↾n ̸= C ↾↾n

(8.1)
then A ≤T C.
Proof Assume

∀s, n

Cs ↾↾n = C ↾↾n =⇒n /∈As+1 −As

,
(8.2)
which is equivalent to (8.1). We will show that A ≤T C. Assume that we have an
oracle for C. Fix n. To determine whether n ∈A, we ﬁrst compute (with the help of
the oracle) the ﬁnite set C ↾↾n. Then we ﬁnd a stage r such that
Cr ↾↾n = C ↾↾n.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_8
51

52
8
Permitting (Friedberg-Muchnik Below C Theorem)
Note that

∀s ≥r

Cs ↾↾n = C ↾↾n

.
Therefore, by (8.2),

∀s ≥r

n /∈As+1 −As

,
and so
n /∈A −Ar .
Hence, to determine whether n ∈A, it sufﬁces to determine whether n ∈Ar.
8.2
The Theorem
Recall that the Friedberg-Muchnik algorithm produced an incomparable pair of c.e.
sets. We enhance that algorithm with permitting, to obtain the following stronger
result:
Theorem 8 (Friedberg-Muchnik below C) Let C be c.e.n. There exists c.e. A and
B such that
A ≰T B
B ≰T A
A <T C
B <T C .
Theorem8 generalizes both Theorem5 (the Kleene-Post Theorem) and Theorem7
(the Friedberg-Muchnick Theorem). In Theorem8 the constructed sets A and B must
be c.e., unlike in Theorem5; also, in Theorem8 the given set C is an arbitrary c.e.n.
set rather than speciﬁcally H as in Theorem5.
Proof Let c0, c1, . . . be a standard enumeration of C, and, as usual, let
Cs =def {c0, c1, . . . , cs}
for each s.
We will construct incomparable, c.e. sets A and B such that A ≤T C and B ≤T C,
which sufﬁce to prove the theorem (see Exercise 6 of Chap.5).
The requirements and their relative priorities are exactly the same as in the
Friedberg-Muchnik proof:
R2e : χA ̸= B
e
R2e+1 : χB ̸= A
e

8.3 Valid Witnesses
53
and
R0 ≺R1 ≺· · · .
Again, at each stage s, we give attention to the strongest priority requirement R j that
needs it. In the Friedberg-Muchnik algorithm, giving attention entailed:
(1) initializing all requirements of weaker priority than that of R j,
(2) putting the witness n j for R j into A (assuming that j is even; otherwise we put
it into B).
In the current construction, we could try to separate these two tasks. That is, we could
do (1) during stage s, but delay (2) until C permits it, that is, until we reach a stage
t ≥s such that ct ≤n j. Thus, by the Permitting Lemma, we would have A ≤T C
and B ≤T C.
Actually, it’s more complicated than that. The trouble with this simple approach
is that n j might never be permitted. That is, perhaps Cs ↾↾n j = C ↾↾n j.
So, we will describe a more complicated approach, which is formalized as
Algorithm 8.1. Before doing so, we deﬁne what is means for a witness to be “valid,”
and the three “types” of witnesses.
8.3
Valid Witnesses
From here until the end of this book, we write A(k) to denote the more cumbersome
χA(k).Analogously,wewritesimply A todenotethefunctionχA.Usingthisnotation,
the requirements are
R2e : A ̸= B
e
R2e+1 : B ̸= A
e .
Thus, A denotes either a set, or its characteristic function; the context will easily
disambiguate it.
A witness n for R2e is valid1 if
A(n) ̸= B
e (n).
(8.3)
1 When introducing the Friedberg-Muchnik algorithm in Chap.7, we used the phrase “potential
witness” to denote a number that may turn out to be a valid witness in this sense. We were tempted
to distinguish between a “potential witness” and a “witness” (i.e., a potential witness that satisﬁes
the full requirement, such as in (8.3)) throughout the remainder of this book. However, the phrase
“potential witness” is too cumbersome, so instead we will distinguish between “witness” and “valid
witness”.

54
8
Permitting (Friedberg-Muchnik Below C Theorem)
Some witness n might satisfy
A(n)[s] ̸= B
e (n)[s]
for some stage s, but that does not imply (8.3).
Likewise, a witness for R2e+1 is valid if
B(n) ̸= A
e (n).
8.4
Types of Witnesses
We deﬁne three types of witnesses n for requirement R j at stage s (that is, at the start
of stage s). They are speciﬁed in Fig.8.1, where we assume that j = 2e (for odd j
we would interchange the roles of A and B).
Note that a Type 1 witness is valid if it holds up, that is, n /∈A and B
e (n) ̸= 0.
Likewise, a Type 3 witness is valid if it holds up, that is, B
e (n) = 0 (n ∈A follows
from n ∈As). On the other hand, a Type 2 witness is useless if it holds up, that is,
n /∈A and B
e (n) = 0. Our hope for a Type 2 witness is that it will be upgraded to
Type 3 (that is, we put it into A, which requires the “permission” of C).
In the Friedberg-Muchnik algorithm, procedure Initialize threw away the current
witness n j for R j, which could be of Type 1, 2, or 3, and replaced it by a fresh
witness n j. Recall that “fresh” means greater than every witness and every usage
number that has been seen so far. If n j was a Type 2 witness, then we said that R j
“needed attention.” During each stage, if there was a requirement needing attention,
then we gave attention to such a requirement of strongest priority. Giving attention
to a requirement meant putting its witness into A (thereby upgrading it to Type 3),
and initializing all weaker priority requirements. Thus, all at times, no requirement
ever had more than one witness.
In Algorithm 8.1, requirement R j maintains three witnesses, which we call n j(1),
n j(2), and n j(3). At the end of each stage, for each r ∈{1, 2, 3}, variable n j(r) is
either undeﬁned (in which case n j(r) = −1), or it is a Type r witness.
If n j(1) never gets upgraded to Type 2 (that is, it holds up as Type 1), then R j is
met. Witness n j(2) is awaiting permission to enter A (in other words, it is awaiting
an upgrade to Type 3).
Fig. 8.1 Three types of
witnesses
Type
status at stage s
1
n ̸∈As
and
ΦB
e (n)[s] ̸= 0
2
n ̸∈As
and
ΦB
e (n)[s] = 0
3
n ∈As
and
ΦB
e (n)[s] = 0

8.5 The Algorithm
55
8.5
The Algorithm
Algorithm 8.1
1
A ←∅.
2
B ←∅.
3 n0(1) ←0.
// In other words, n0(1) ←a fresh number.
4 n0(2) ←−1.
5 n0(3) ←−1.
6 for s ←1 to ∞
7
ns(1) ←a fresh number.
8
ns(2) ←−1.
9
ns(3) ←−1.
10
for j ←0 to s
11
if cs+1 ≤n j(2)
// n j(2) has permission to enter A.
// Requirement R j acts.
12
Put n j(2) into A.
13
n j(3) ←n j(2).
14
n j(2) ←−1.
15
for k ←j + 1 to s
16
nk(1) ←a fresh number.
17
nk(2) ←−1.
18
nk(3) ←−1.
19
if n j(1) is a Type 2 witness and n j(3) = −1
20
n j(2) ←n j(1).
21
n j(1) ←a fresh number.
Notes on the algorithm:
1. The body of the for loop of line 10 is written as though j were even. If j is odd,
then replace A by B.
2. The condition n j(3) = −1 in line 19 prevents the upgrading of a Type 1 witness
to Type 2 when R j currently has a Type 3 witness.
3. Suppose that n j(1)[s] is upgraded to Type 2 (lines 19–21) during stage s, and
suppose also that R j had a Type 2 witness at the start of stage s (in other words,
n j(2)[s] ≥0). Then we could keep both n j(1)[s] and n j(2)[s], and so would
have two Type 2 witnesses awaiting an upgrade to Type 3. However, n j(2)[s] <
n j(1)[s]; hence, if for some t > s,
ct+1 ≤n j(2)[s]
(and so n j(2)[s] would receive permission to enter A during stage t), then
ct+1 ≤n j(1)[s]

56
8
Permitting (Friedberg-Muchnik Below C Theorem)
(and so n j(1)[s] would receive permission to enter A during stage t, too). There-
fore we might as well keep only n j(1)[s] as a Type 2 witness, and throw away
n j(2)[s]. Hence we assign n j(1) to n j(2) (line 20) during stage s.
4. Two apparent difﬁculties with Algorithm 8.1 are:
(a) Suppose that n j(1)[s] gets upgraded to Type 2 during stage s, and for some
t > s,
ct+1 ≤n j(2)[s] and B
e

n j(1)[s]

̸= 0 and B
e

n j(2)[s]

= 0,
where e = ⌊j/2⌋. In other words, n j(1)[s] does not hold up as a Type 3
witness, whereas n j(2)[s] would have, if we had kept it and upgraded it to
Type 3 during stage t. Then wouldn’t we regret our decision to throw away
n j(2)[s] in line 20 during stage s? No, everything will work out ﬁne, because
B
e

n j(1)[s]

s

= 0 ̸= B
e

n j(1)[s]

implies that for some i < j, requirement Ri gets injured during or after stage
s, which cannot happen for sufﬁciently large s, as follows from Lemma8.3.
(b) Suppose that for a particular j, requirement R j never acts, but lines 20 and
21 are executed inﬁnitely often. In other words, the variable n j(2) is assigned
an inﬁnite sequence of increasing values, none of which ever obtains per-
mission to enter A. If so, then R j would not be met. We will see in the
proof of Lemma8.4 that this cannot happen (because otherwise C would be
computable).
8.6
Veriﬁcation
Because A and B are c.e., Theorem8 follows from Lemmas8.2 and 8.4.
Lemma 8.2 A ≤T C and B ≤T C.
Proof To see that A ≤T C, ﬁx s and n, and assume that
n ∈As+1 −As.
In other words, during stage s of Algorithm 8.1, n is put into A by line 12. Hence,
by line 11 of Algorithm 8.1, cs+1 ≤n. Thus,
cs+1 ∈Cs+1 ↾↾n −Cs ↾↾n
⊆C ↾↾n −Cs ↾↾n

8.6 Veriﬁcation
57
and so
Cs ↾↾n ̸= C ↾↾n.
Therefore (8.1) is true, and so, by the Permitting Lemma (Lemma8.1), we have
A ≤T C. Analogous reasoning proves B ≤T C.
Lemma 8.3 Suppose that j and w are such that after stage w, variable n j(1) is
never given a fresh number by line 16 of Algorithm 8.1. Then requirement R j acts at
most once after stage w.
We leave the proof of Lemma8.3 as Exercise 1.
Lemma 8.4 For each j, requirement R j acts only ﬁnitely often, and is met.
Proof We proceed by strong induction on j.
For the basis, consider j = 0. Variable n0(1) is never given a fresh value by line
16 of Algorithm 8.1. Therefore, letting w = 0, Lemma8.3 implies that R j acts at
most once after stage w.
For the inductive step, ﬁx j ≥1. Assume the claim for each i < j. Then there is
a stage w > j such that
(∀i < j )[ Ri never acts after stage w].
Therefore, after stage w, variable n j(1) is never given a fresh number by line 16 of
Algorithm 8.1. So, again, Lemma8.3 implies that R j acts at most once after stage w.
Everything we say from here to the end of the proof applies both to the basis and
to the inductive step.
Because R j acts at most once after stage w, R j acts only ﬁnitely often. Hence
there is an x > w such that
(∀i ≤j )[ Ri never acts after stage x ].
It remains to show that R j is met.
After stage x, every change to n j(2) is caused by line 20 of Algorithm 8.1, and
hence is an increase (see Exercise 2). Therefore n j(2) never decreases after stage x.
We claim that n j(2) is updated only ﬁnitely often; assume the contrary. Then
max
s
n j(2)[s] = ∞.
(8.4)
Then the following algorithm, which knows j and x,2 could determine whether a
given p is in C:
2 If you’re a computer programmer, think of j and x as “hard-coded” constants (also known as
“magic numbers”), whereas p is an input passed to Algorithm 8.2.

58
8
Permitting (Friedberg-Muchnik Below C Theorem)
Algorithm 8.2
1 Run Algorithm 8.1 until reaching the ﬁrst stage s > max{x, j} such that n j(2)[s] > p.
2 if p ∈Cs
3
output(“ p is in C ”)
4 else output(“ p is not in C ”).
Note that, by (8.4), the stage s will indeed be found by line 1 of Algorithm 8.2.
Assume for a contradiction that the output of Algorithm 8.2 were incorrect, that
is, assume
p ∈C −Cs.
Then there exists t ≥s such that p = ct+1. Thus,
ct+1 = p
< n j(2)[s]
(by line 1 of Algorithm 8.2)
≤n j(2)[t]
(because n j(2) never decreases after stage x).
Therefore, because t > j, requirement R j acts during stage t, contradicting t > x.
Thus, the output of Algorithm 8.2 is correct, and so C is computable, contrary
to the assumption in the statement of the theorem. Therefore, n j(2) is updated only
ﬁnitely often.
Let y > x be a stage during or after which n j(2) is never updated. Then n j(1)
never changes during or after stage y (because, whenever Algorithm 8.1 assigns to
n j(1), it also assigns to n j(2)).
Assume that j = 2e; if j is odd then the proof is analogous.
Case 1.
n j(3)[y] ≥0.
Let n = n j(3)[y]. No requirement Ri such that i < j acts after stage w; hence, the
computation B
e (n)[y] is permanent. Therefore n holds up as a Type 3 witness.
Thus,
A(n) = 1 ̸= 0 = B
e (n)[y] = B
e (n),
and so R j is met.
Case 2.
n j(3)[y] = −1.
Let n = n j(1)[y].
Note that

∀y′ ≥y

n j(3)[y′] = −1

,
(8.5)
because R j does not act during or after stage y.
We claim that
0 ̸= B
e (n).
(8.6)
To see this, assume the contrary. Then, by the Permanence Lemma,
B
e (n)[z] = 0

8.8 Afternotes
59
for sufﬁciently large z. Hence, by (8.5), n would eventually become Type 2,
causing n j(2) and n j(1) to be changed by lines 20 and 21 of Algorithm 8.2
during some stage z′ ≥y, contradicting the choice of y. Thus, (8.6) is true.
Furthermore, by (8.5), n never gets upgraded to Type 3; hence
A(n) = 0.
(8.7)
Combining (8.7) and (8.6), we have
A(n) = 0 ̸= B
e (n),
and so R j is met.
QED Lemma 8.4
QED Theorem 8
8.7
What’s New in This Chapter?
1. A simple form of permitting, which is a technique for constructing a c.e. set or
sets below a given c.e.n. set C, while satisfying certain other properties as well.
2. Maintaining three possible witnesses rather than just one witness for a require-
ment. In Chap.14, we generalize this by maintaining an arbitrarily long “witness
list” for each requirement, which facilitates the more complicated form of per-
mitting employed there.
It is possible to implement Algorithm 8.1 with only two (rather than three) possible
witnesses per requirement, but the pseudo-code would be more difﬁcult to under-
stand. Can it be implemented with only one possible witness per requirement? I
don’t know.
3. An algorithm (in particular, Algorithm 8.2) that is introduced as part of a proof by
contradiction, and that knows as a “magic number” a certain stage (which is x, in
Algorithm 8.2) after which certain ﬁnitely occurring events have ceased to occur.
We will see this type of algorithm again, as Algorithms 9.2, 11.2, 12.2, and 13.2.
Algorithm 14.2 is provided with both a magic number and an oracle. Chapter14
combines most of the ideas of this book, and introduces a few new ones, too.
8.8
Afternotes
There are many varieties of permitting. Lemmas8.1 and 8.5 (see Exercise 4) both
fall into the category of “Yates permitting.” There is also “Martin permitting” (which
is explained in Chap.11 of [So87]).
We use another variety of permitting in Chap.14.

60
8
Permitting (Friedberg-Muchnik Below C Theorem)
8.9
Exercises
1. Prove Lemma8.3.
2. Prove that whenever line 20 of Algorithm 8.1 is executed, the variable n j(2)
increases.
3. Let C be c.e.n. Show that there is an inﬁnite sequence
A0 <T A1 <T · · ·
of c.e. sets such that
(∀i)[ Ai <T C ].
4. Prove the following generalization of Lemma8.1:
Lemma 8.5 (Strong Permitting Lemma) Let A0 ⊆A1 ⊆· · · and C0 ⊆C1 ⊆· · · be
computable enumerations of A and C, respectively. Let f : ω →ω be a computable
total function. If

∀s, n

n ∈As+1 −As =⇒Cs ↾↾f (n) ̸= C ↾↾f (n)

then A ≤T C.
Note that Lemma8.1 is the special case of Lemma8.5 in which f is the identity
function.
3. In addition to the three types of witnesses deﬁned in Fig.8.1, we could deﬁne
two more types, as in Fig.8.2.
(a) If a Type 4 witness holds up, would it be valid?
(b) If a Type 5 witness holds up, would it be valid?
Fig. 8.2 Five types of
witnesses
Type
status at stage s
1
n ̸∈As
and
ΦB
e (n)[s] ̸= 0
2
n ̸∈As
and
ΦB
e (n)[s] = 0
3
n ∈As
and
ΦB
e (n)[s] = 0
4
n ̸∈As
and
ΦB
e (n)[s] = 1
5
ΦB
e (n)[s] > 1

Chapter 9
Length of Agreement (Sacks Splitting
Theorem)
The length of agreement method is widely used in the construction of c.e. sets.
Suppose that we are given a c.e.n. set A, and we wish to construct a c.e.n. set B
having certain properties, including A ≰T B. This situation is very different from
that in the proof of the Friedberg-Muchnik Theorem, because there we were building
both A and B. Here, we build B but we have no control over A. We cannot put a
number into A (to serve as a witness), nor can we keep anything out of A.
9.1
The Idea
Suppose that among our requirements are
Re : A ̸= B
e
for each e. Assume that we can preserve whatever halting computations that we want.
That is, if B
e (n)[s] ↓for some n and s, then we can restrain all numbers less than
or equal to ϕB
e (n)[s] from entering B during stage s or beyond. So, for example, if
B
e (n)[s] = 42 then we could lock in that computation and hence
A(n) ̸= 42 = B
e (n),
andson wouldbeavalidwitnessforrequirement Re,asdeﬁnedinSect.8.3.Likewise,
if B
e (n)[s] = 0 for some s and some n ∈As, then we could lock in that computation
and hence
A(n) = 1 ̸= 0 = B
e (n),
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_9
61

62
9
Length of Agreement (Sacks Splitting Theorem)
Fig. 9.1 The length of
agreement is 6
k
0
1
2
3
4
5
6
7
8
As(k)
0
0
1
0
1
1
1
0
0
ΦB
e (k)[s]
0
0
1
0
1
1
↑
0
17
Fig. 9.2 The length of
agreement is 4
k
0
1
2
3
4
5
6
7
8
As(k)
0
0
1
0
0
1
1
0
0
ΦB
e (k)[s]
0
0
1
0
1
1
↑
0
17
and so again n would be a valid witness for Re. However, it might be that neither of
these fortuitous events ever happens. For example, it might be that for some stage s,
n ∈As and B
e (n)[s] ↑
but
B
e (n) = 1.
(9.1)
Thus, n would be useless as a witness for Re. Hence, we should replace n with
another witness for Re, but how could we know that (9.1) is true?
Therefore we employ the following, more powerful technique. Let
ℓe(s) =def max

n :

∀k < n

As(k) = B
e (k)[s]
 
,
which is called the length of agreement between A and B
e at stage s. In Fig.9.1,
ℓe(s) = 6.
In Fig.9.2, ℓe(s) = 4.
Note that B
e (k)[s] ↓for each k < ℓe(s). We try to preserve the computation
B
e (k)[s] for each k < ℓe(s), so as to preserve the initial run of agreements. Further-
more, it is important that we likewise try to preserve the computation B
e (ℓ(s))[s] if
B
e (ℓ(s))[s] ↓(as in Fig.9.2), so as to preserve the ﬁrst disagreement along with the
initial run of agreements.
Recall that in the Friedberg-Muchnik algorithm, when a requirement R j received
attention, we put its witness n j into A, and then tried to preserve the disagreement
A(n j) = 1 ̸= 0 = B
e (n j)[s]

9.2 The Theorem
63
by keeping small numbers out of B. In the length-of-agreement method, we try to
preserve not only the ﬁrst disagreement (if B
e (ℓe)[s] ↓), but also the initial run of
agreements. This is why some authors refer to the length-of-agreement method as
the “Sacks preservation strategy.”
We will prove that the length of agreement cannot grow forever; that is, we will
prove
max
s
ℓs(s) < ∞.
(9.2)
In particular, we will argue that if
max
s
ℓs(s) = ∞,
then, because of the preservation of agreements, A would be computable (contrary to
our assumption that A is c.e.n.). Using (9.2), we will argue that Re has a valid witness.
This is just an overview; the argument is ﬂeshed out in the following application.
9.2
The Theorem
The following result implies both the splitting theorem of Chap. 4 and the Friedberg-
Muchnik Theorem of Chap. 7:
Theorem 9 (Sacks Splitting Theorem) Let A be c.e.n. Then A can be partitioned
into incomparable, c.e. sets B0 and B1.
Proof We describe an algorithm that partitions A into incomparable, c.e. sets B0 and
B1.
Recall Exercise 4(b) of Chap. 7. That problem and our current one are ostensibly
similar. The only difference is that here we cannot ignore any elements of A; each
member of A must be put either into B0 or into B1. This difference is huge; we cannot
simply use the Friedberg-Muchnik approach of taking fresh witnesses, because in
doing so we might skip over some elements of A.
So, instead of building B0 and B1 to meet the Friedberg-Muchnick requirements
B0 ̸= B1
e and B1 ̸= B0
e , so as directly to obtain
B0 ≰T B1 and B1 ≰T B0 ,
(9.3)
we will meet the requirement
Re,i : A ̸= Bi
e
for each e ∈ω and i ∈{0, 1}. Collectively, the Re,i requirements ensure

64
9
Length of Agreement (Sacks Splitting Theorem)
A ≰T B0 and A ≰T B1 ,
which imply (9.3), by the following lemma:
Lemma 9.1 Assume that A = B0 ∪B1. Then
A ≰T B0 =⇒B1 ≰T B0
and
A ≰T B1 =⇒B0 ≰T B1.
Proof To show A ≰T B0 =⇒B1 ≰T B0, we prove its contrapositive:
B1 ≤T B0 =⇒A ≤T B0 .
Assume that B1 ≤T B0 and that we have an oracle for B0. Then, given p, we could
determine whether
(i) p ∈B0 by using the oracle for B0, and
(ii) p ∈B1 by using the Turing reduction from B1 to B0, together with the oracle
for B0.
These two tests sufﬁce to determine whether p ∈A, because A = B0 ∪B1.
The proof that A ≰T B1 =⇒B0 ≰T B1 is completely analogous.
□
On the face of it, the Re,i requirements might seem harder to meet than the
Friedberg-Muchnik requirements, because we have no control over A. However, the
length-of-agreement technique comes to our rescue.
The requirements are prioritized as
R0,0 ≺R0,1 ≺R1,0 ≺R1,1 ≺R2,0 ≺R2,1 ≺· · · .
Let a0, a1, . . . be a standard enumeration of A. As in our other partitioning algo-
rithm (Algorithm 4.1), during each stage s, we put as into B0 or B1 (but not into
both), thereby ensuring that B0 and B1 partition A. Each stage is computable in a
ﬁnite amount of time, and so both B0 and B1 are c.e.
9.3
Deﬁnitions
For each e and s, and each i ∈{0, 1}, let
ℓe,i(s) =def max

n ≤s :

∀k < n

As(k) = Bi
e (k)[s]
 
.
That is, ℓe,i(s) equals either the length of agreement between A and Bi
e at the start
of stage s, or it equals s, whichever is less. The reason for placing an upper bound

9.5 Veriﬁcation
65
(of s) on ℓe,i(s) is to ensure that it is ﬁnite, and that each stage of the algorithm can
be performed in a ﬁnite amount of time.
Let
restrainte,i(s) =def max

ϕBi
e (k)[r] : k ≤ℓe,i(r) and r ≤s
	
.
The algorithm attempts to keep numbers less than or equal to restrainte,i(s) from
entering Bi during or after stage s and thereby possibly diminishing the length of
agreement ℓe,i. Note that restrainte,i(s) is monotonically non-decreasing in s.
Alsonotethatwewrite“k ≤ℓe,i(r)”ratherthanjust“k < ℓe,i(r)”inthedeﬁnition
of restrainte,i(s), so as to try to preserve the ﬁrst disagreement along with the initial
run of agreements, as discussed in Sect.9.1.
9.4
The Algorithm
Algorithm 9.1 differs from Algorithm 4.1 only in lines 7 and 8.
Algorithm 9.1
1
B0 ←∅.
2
B1 ←∅.
3 for s ←0 to ∞
// Assign as either to B0 or to B1.
4
assigned ←false.
5
for e ←0 to s
6
for i ←0 to 1
7
if (not assigned) and (as ≤restrainte,i(s))
// Re,i acts.
8
Put as into B1−i.
// This keeps as out of Bi,
// and so prevents as from diminishing ℓe,i .
9
assigned ←true.
10
if not assigned
11
Put as into B0.
// Or put as into B1, it doesn’t matter.
9.5
Veriﬁcation
For each s, the value restrainte,i(s) can be computed in a ﬁnite amount of time, and
so stage s can be performed in a ﬁnite amount of time. Therefore B0 and B1 are c.e.
Lemma 9.2 For each e ∈ω and each i ∈{0, 1},
I. maxs ℓe,i(s) < ∞,
II. maxs restrainte,i(s) < ∞,
III. Re,i acts only ﬁnitely often.

66
9
Length of Agreement (Sacks Splitting Theorem)
Proof We use strong induction on the priority of Re,i. The inductive step is as
follows.1 Fix e and i, and assume the claim for each e′ and i′ such that
Re′,i′ ≺Re,i .
Then (by Part III) there is a stage x after which no requirement of stronger priority
than Re,i ever acts (informally, after stage x, requirement Re,i is treated like R0,0).
Therefore, for each t > x, if
at ≤restrainte,i(t)
then Re,i acts during stage t, putting at into B1−i. Hence

∀t > x

at ∈Bi =⇒at > restrainte,i(t)

.
(9.4)
Proof of Part I. Assume for a contradiction that Part I were false; that is, assume
max
s
ℓe,i(s) = ∞.
(9.5)
Then the following algorithm, which knows x, could determine whether a given p
is in A:
Algorithm 9.2
1 Run Algorithm 9.1 until reaching the ﬁrst
stage s > x such that ℓe,i(s) > p.
// Such an s exists by (9.5).
2 if Bi
e (p)[s] = 1
3
output(“ p is in A”)
4 else output(“ p is not in A”).
If t ≥s and at ∈Bi then
at > restrainte,i(t)
(by (9.4), because t ≥s > x)
≥restrainte,i(s)
(by the monotonicity of restrainte,i)
≥max

ϕBi
e (k)[s] : k ≤ℓe,i(s)
	
(by the deﬁnition of restrainte,i(s))
≥ϕBi
e (p)[s]
(because p < ℓe,i(s)).
Therefore
Bi[s]↾↾ϕBi
e (p)[s] = Bi ↾↾ϕBi
e (p)[s],
1 The argument for the basis, in which e = i = 0, is a special case of the inductive step. This was
true in our priority arguments in Chaps. 7 and 8, where nevertheless we worked out the details of
the bases. It is true also in the priority arguments in Chaps. 11 and 12, where we won’t bother to
even mention the bases.

9.5 Veriﬁcation
67
and so the computation Bi
e (p)[s] is permanent, as deﬁned in Chap. 6. Hence
Bi
e (p) = Bi
e (p)[s].
(9.6)
If
Bi
e (p)[s] = 1
then p ∈As (because ℓe,i(s) > p), and so p ∈A. On the other hand, if
Bi
e (p)[s] = 0
then (by (9.6))
Bi
e (p) = 0
and so p /∈A, because otherwise
A(p) = 1 ̸= 0 = Bi
e (p),
(9.7)
which would contradict (9.5) (see Exercise 3). To summarize,
p ∈A ⇐⇒Bi
e (p)[s] = 1,
and so the output of Algorithm 9.2 is correct. Thus A is computable, contrary to the
assumption in the statement of the theorem.2 Thus, Part I is proved.
Proof of Part II. Let y > x be such that
ℓe,i(y) = max
s>x {ℓe,i(s)}
(such a y exists by Part I). Let ℓ= ℓe,i(y).
If z ≥y and az ∈Bi then
az > restrainte,i(z)
(by (9.4), because z ≥y > x )
≥restrainte,i(y)
(by the monotonicity of restrainte,i ).
In other words,
Bi[y]↾↾restrainte,i(y) = Bi ↾↾restrainte,i(y).
Therefore

∀k < ℓ

the computation Bi
e (ℓ)[y] is permanent

.
(9.8)
2 This argument is reminiscent of (but more complicated than) the main argument in the proof of
Lemma 8.4.

68
9
Length of Agreement (Sacks Splitting Theorem)
and so

∀z ≥y

ℓe,i(z) = ℓ

.
Hence if z ≥y and Bi
e (ℓ)[z] ↓then
restrainte,i(z) ≥ϕBi
e (ℓ)[z],
and so the computation Bi
e (ℓ)[z] is permanent.3 Thus,

∀z ≥y

Bi
e (ℓ)[z] ↓=⇒ϕBi
e (ℓ) never changes during or after stage z

. (9.9)
Case 1.
Bi
e (ℓ)[y] ↓.
Then ϕBi
e (ℓ) never changes during or after stage y, by (9.9).
Case 2.
Bi
e (ℓ)[y] ↑and Bi
e (ℓ)[z] ↓for some z > y.
Then ϕBi
e (ℓ) changes exactly once during or after stage y, by (9.9).
Case 3.

∀z ≥y

Bi
e (ℓ)[z] ↑

.
Then

∀z ≥y

ϕBi
e (ℓ)[z] = −1

.
In each of the three cases, ϕBi
e (ℓ) changes at most once during or after stage y,
and so, by (9.8),
max

ϕBi
e (k) : k ≤ℓ
	
changes at most once during or after stage y. Hencerestrainte,i changes only ﬁnitely
often, and so
max
s
restrainte,i(s) < ∞.
Proof of Part III. By Part II, there exists r such that
r = max
s
restrainte,i(s).
Let z be such that
Az ↾↾r = A↾↾r.
Then

∀s > z

as > r ≥restrainte,i(s)

.
Therefore, for each s > z, the condition in line 7 of Algorithm 9.1 evaluates to false.
Hence Re,i never acts after stage z, and so it acts only ﬁnitely often.
QED Lemma 9.2
3 This is why we preserve the ﬁrst disagreement along with the initial run of agreements.

9.6 Why Preserve Agreements?
69
Lemma 9.3 For each e ∈ω and each i ∈{0, 1}, Re,i is met.
Proof Fix e and i. Assume for a contradiction that Re,i is not met; that is,
A = Bi
e .
(9.10)
By Part I of Lemma 9.2, there exists
m = max
s { ℓe,i(s) }.
(9.11)
By (9.10) and the Permanence Lemma, there is an x such that

∀k ≤m

the computation Bi
e (k)[x] is permanent

.
Let y > x be such that

∀k ≤m

Ay(k) = A(k)

.
By (9.10), we have
ℓe,i(y) > m ,
contradicting (9.11).
QED Lemma 9.3
The theorem follows from Lemmas 9.1 and 9.3.
QED Theorem 9
9.6
Why Preserve Agreements?
The preservation of agreements is counter-intuitive. Is it necessary?
Suppose that during a stage s > x (where x is as deﬁned in Lemma 9.2), we ﬁnd
some k such that
Bi
e (k)[s] > 1.
(9.12)
or
Bi
e (k)[s] = 0 and k ∈As .
(9.13)
Then we need only preserve the computation Bi
e (k)[s], to make k a valid witness
for Re,i (because if (9.12) is true then it doesn’t matter whether k ∈A, and if (9.13)
is true then k ∈A).

70
9
Length of Agreement (Sacks Splitting Theorem)
Instead of the function restrainte,i(s) deﬁned in Sect.9.3, suppose that we deﬁne
restre,i(s) =def

ϕBi
e (k)[s], if k is the least number satisfying either (9.12) or (9.13) at stage s
−1,
if no such k exists at stage s
and replace the clause as ≤restrainte,i(s) in line 7 of Algorithm 9.1 by as ≤
restre,i(s). Unlike restrainte,i(s), the simpler function restre,i(s) tries to preserve
only a single disagreement; it does not try to preserve agreements. The following is
an invalid argument that this modiﬁed algorithm still works (that is, it constructs sets
B0 and B1 such that each Re,i is met).
Fix e and i. First, we “claim” that
lim inf
s
ℓe,i(s) < ∞;
(9.14)
assume for a contradiction that
lim inf
s
ℓe,i(s) = ∞.
(9.15)
Note that for a function f : ω →ω (such as ℓe,i(s)),
lim inf
s
f (s) = min{k : f (s) = k for inﬁnitely many s },
(9.16)
where the minimum is regarded as ∞if the set is empty. This is a convenient way
to think about the liminf, and is used extensively in Chap. 14.
By (9.15) and (9.16), there exists y such that
(∀s ≥y)[ ℓe,i(s) > p ].
Then the following algorithm, which knows y, could determine whether a given p
is in A:
Algorithm 9.2′
Run Algorithm 9.1 until reaching stage y.
if Bi
e (p)[y] = 1
output(“p is in A”)
else output(“p is not in A”).
Algorithm 9.2′, given p, correctly determines whether p ∈A. Therefore A is
computable, contrary to the assumption in the statement of the theorem.
Now that we have “proven” (9.14), let
n = lim inf
s
ℓe,i(s).

9.8 Afternotes
71
Then
Bi
e (n) ↑,
by the contrapositive of the Permanence Lemma, and so n is a valid witness for Re,i.
QUESTION: What is the ﬂaw in this argument? Perhaps think about it, before
you read the next paragraph.
ANSWER: In the proof of Part I of Lemma 9.2, x was deﬁned independently of
the input p; this justiﬁed our saying that Algorithm 9.2 “knows x.” That is, x was
part of the description of the algorithm. In this invalid argument, y depends on p
(it should be called yp), and so Algorithm 9.2′ cannot know yp. Instead, given p,
Algorithm 9.2′ would need to compute yp, which might be impossible to do.
9.7
What’s New in This Chapter?
1. The length-of-agreement method, also known as the Sacks preservation strategy.
We’ll employ it, in various forms, in most of the subsequent chapters.
2. In Algorithms 7.1 and 8.1, for each requirement we chose a witness, and then
monitored it. Sometimes we upgraded it (in Algorithm 8.1), sometimes we put it
into an appropriate set (A or B), and sometimes we replaced it by a fresh witness.
Algorithm 9.1, on the other hand, does not explicitly choose or even look at any
witnesses; hence, it has no procedure Initialize. Rather, for each requirement we
pay attention only to its restraint function. We rely on the length-of-agreement
method to produce a witness without telling us about it.
9.8
Afternotes
Theorem 9 is actually a slightly weaker version of what is known in the litera-
ture as the “Sacks Splitting Theorem,” ﬁrst proved in [Sa63]. See [DS] for a sur-
vey of splitting theorems, including (but not limited to) various generalizations of
Theorems 4 and 9.
We found in Exercise 2 of Chap. 7 an explicit form (as a function of j) for an upper
bound on the number of times that requirement R j is injured during the Friedberg-
Muchnik algorithm. Here in the Sacks Splitting algorithm, the number of injuries to
Re,i is indeed ﬁnite, but is there a nice form for its upper bound (as a function of e
and i)? Is such a bound even computable?
Can the Sacks Splitting Theorem be proved without a length-of-agreement argu-
ment?

72
9
Length of Agreement (Sacks Splitting Theorem)
9.9
Exercises
1. Can a computable set be partitioned into incomparable c.e. sets B0 and B1?
2. Recall (9.4):

∀t > x

at ∈Bi =⇒at > restrainte,i(t)

.
Is it true that

∀t > x

at ∈Bi ⇐= at > restrainte,i(t)

?
3. Prove that (9.7) contradicts (9.5).
4. Say whether the following sentence is true or false:

∀e ∈ω

∀i ∈{0, 1}

lim
s ℓe,i(s) exists

.
Prove your answer.
5. Let A be c.e.n. Can A can be partitioned into inﬁnitely many c.e. sets B0, B1, . . .
that are pairwise incomparable?
6. The sets B0 and B1 produced by Algorithm 9.1 are both non-computable, because
ifoneofthemwerecomputablethenitwouldTuringreducetotheother.Therefore,
Theorem 9 is a stronger version of Theorem 4. Is there a more direct argument that
they are non-computable, that is, an argument that does not ﬁrst show B0 ≰T B1
and B0 ≰T B1?
7. Consider the following:
Theorem 9′. Let A be c.e., and let C be c.e.n. Then A can be partitioned into c.e.
sets B0 and B1 such that
C ≰B0 and C ≰B1.
(a) Prove that Theorem 9′ implies Theorem 9.
(b) Prove Theorem 9′.
8. Let C and A be c.e.n. Prove that A can be partitioned into incomparable c.e. sets
B0 and B1 such that B0 ≤T C and B1 ≤T C.
9. Show that if A is c.e.n. then A can be partitioned into incomparable, c.e.n. sets B0
and B1 such that B0 is sparse (this is a stronger version of Exercise 4 of Chap. 4).

Chapter 10
Introduction to Inﬁnite Injury
Coping with inﬁnite injury is a subtle and beautiful topic. You should read this short
introductory chapter now, and then again after studying Chap.11. Many of the terms
used in this chapter need deﬁnitions, which will be provided in Chap.11.
10.1
A Review of Finite Injury Priority Arguments
In ﬁnite injury arguments, we construct a set B (or perhaps multiple sets such as
B0 and B1), subject to certain requirements. A typical such argument has positive
requirements Pe that “act” by putting certain elements into B, and negative require-
ments Ne that try to protect certain OTM computations from injury by restraining
sufﬁciently small elements from entering B.1 Thus, the positive and negative require-
ments must coexist in an adversarial relationship. The priorities maintain law and
order, allowing all requirements to ultimately be met.
In particular, if s is a stage at the start of which a negative requirement Ne
is met, then Ne tries to keep numbers b ≤restrainte(s) from entering B, where
restrainte(s) is so large that numbers above it cannot harm the work of Ne. Some-
times Ne fails in this endeavor—that is, it gets “injured”—by the action of a positive
requirement of stronger priority, say, Pe−50. This could result in restrainte(s + 1) >
1 In the proof of the Friedberg-Muchnik Theorem, we did not label our requirements as P for
positive and N for negative. Rather, each requirement, say R2e : A ̸= B
e , was both positive and
negative. That is, R2e sometimes needed to force a witness into A, and sometimes needed to keep
small elements out of B. That’s why we used the neutral label R.
In the proof of the Sacks Splitting Theorem, requirement Re,i certainly was negative. It could
also be viewed as positive in the sense that it might put certain elements into B1−i (but for the sole
purpose of keeping them out of Bi).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_10
73

74
10
Introduction to Inﬁnite Injury
restrainte(s), an irksome situation, because it might prevent a weaker priority pos-
itive requirement (such as Pe+17) from acting. However, in the priority arguments
that we have seen so far, we argued (by induction) that each positive requirement
acts only ﬁnitely often, and so Ne gets injured only ﬁnitely often, and hence
max
s
restrainte(s) < ∞.
10.2
Coping with Inﬁnite Injury
What if positive requirement Pe−50 needs to act inﬁnitely often, causing inﬁnitely
many injuries to Ne? Such positive requirements appear in the proof of the Weak
Thickness Lemma in Chap.11, and in other proofs in subsequent chapters. This could
cause
max
s
restrainte(s) = ∞,
which might prevent Pe+17 from acting as much as it needs to. At ﬁrst glance,
the difﬁculties associated with inﬁnite injury might seem insurmountable. However,
methods for coping with them have been developed.
10.2.1
Guessing
The main technique for coping with inﬁnite injury, which is the only one studied in
this book, involves a “priority tree,” which is a rooted inﬁnite tree in which the nodes
on level e have the responsibility for meeting requirements Pe and Ne.2 Each node on
level e of the tree represents a sequence of guesses, which is why we sometimes refer
to a priority tree as a “tree of guesses.” In the simpler tree arguments (such as those
in Chaps.11 and 12), these are guesses about the given set A. In some of the more
complicated tree arguments (such as those in Chaps.13 and 14), these are guesses
about the constructed set(s) or about some internal variables of the algorithm.
For each node σ of the tree, we deﬁne a function restraintσ(s). Think of it as a
more reﬁned version of restrainte(s), where σ is on level e of the tree, because it
depends not only on e but also on the speciﬁc guesses that are represented by σ.
During each stage s, a path, called TPs, is constructed from the root of the tree
down to some level s node. We think of TPs both as a path, and as the set of s + 1
nodes along that path. Each node σ ∈TPs is said to be “visited” during stage s. When
2 This is true for the ﬁrst generation of priority tree arguments, sometimes called “double-jump”
arguments. For the next generation, the daunting “triple-jump” arguments (which are not treated in
this book), the relationship between level e and the requirements Pe and Ne can be more complicated.

10.2 Coping with Inﬁnite Injury
75
σ is visited, it may act (the deﬁnition of action depends on the particular algorithm;
for the simpler ones, action consists only of putting an element into B). However,
when σ is visited, it must respect the restraints imposed by all nodes of stronger
priority. “Respect” means that σ may not put an element into B that violates one of
those restraints. A node ρ has “stronger priority” than σ if it lies to the left of σ, or
above σ, in the tree (we’ll be more precise about that in Chap.11).
The set
{the leftmost node on level e that is visited inﬁnitely often : e ∈ω}
forms an inﬁnite path, called the “true path.” If σ lies to the left of the true path then
σ is visited only ﬁnitely often, and hence
max
s
restraintσ(s) < ∞.
(10.1)
It also turns out (and this is crucial) that if σ lies on the true path then it, too, satisﬁes
(10.1). Furthermore, if σ lies on the true path then each of the guesses represented
by σ is correct. These two properties of the true path are often the key to proving
that all the requirements are met.
10.2.2
Other Methods
There were other methods (which have largely been replaced by priority trees) devel-
oped for coping with inﬁnite injury. Among them are:
1. The ﬁrst3 method developed for coping with inﬁnite injury involved identifying
a set T of “true stages.” It would be great if
max
t∈T restrainte(t) < ∞,
but that may not be the case. A modiﬁed (known as “hatted”) restraint function
ˆre(s) is deﬁned so that
max
t∈T ˆre(t) < ∞.
The positive requirements are able to get their work done during these true stages.
There is a fair amount of detail in the deﬁnitions of T and of ˆre(s).
3 Arguably, it traces back to 1954 [De]. True stages are not only an algorithmic alternative to
priority trees; in [CGS], for example, the concept of true stages is used in the proof of correctness of
a complicated priority tree algorithm. Various generalizations of true stages are presented in [Mo].

76
10
Introduction to Inﬁnite Injury
2. Then came the “pinball” method, which according to [So87], was “slightly more
cumbersome but more versatile” than the method of true stages.
Which technique (true stages, pinball, or priority tree) is conceptually simplest?
Which is most aesthetically pleasing? Opinions vary on this.

Chapter 11
A Tree of Guesses (Weak Thickness
Lemma)
This chapter contains our ﬁrst inﬁnite injury argument.
11.1
The “Lemma”
For all A and e, let
A[e] =def {⟨e, j⟩∈A : j ∈ω}.
Think of A[e] as row e of A. For example, assume that
A↾↾20 = {3, 5, 7, 10, 12, 14, 17, 18}.
then we can view A as the set of circled elements in the matrix shown in Fig.11.1.
Thus, in Fig.11.1, we see that 17 ∈A[3], for example. Compare Fig.11.1 with the
ﬁgure in Appendix A. Note that if A is c.e., then so is A[e] for each e.
We say that A[e] is full if it equals ω[e]; pictorially, this means that each element
of row e of A is circled. Thus, in Fig.11.1, A[2] might be full, whereas A[0], A[1],
A[3], A[4], and A[5] deﬁnitely are not. Set A is piecewise trivial if

∀e

A[e] is either full or ﬁnite

.
If B ⊆A then B is a thick subset of A if for all e, A[e] −B[e] is ﬁnite (informally,
B[e] almost equals A[e]).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_11
77

78
11
A Tree of Guesses (Weak Thickness Lemma)
e\j
0
1
2
3
4
5
· · ·
0
0
2
5
9
14
20
· · ·
1
1
4
8
13
19
· · ·
2
3
7
12
18
· · ·
3
6
11
17
· · ·
4
10
16
· · ·
5
15
· · ·
...
Fig. 11.1 Viewing a set A as a matrix
Theorem 11 (Weak Thickness Lemma1)
Let A be c.e.n. and piecewise trivial. Then there is a thick, c.e. subset B of A such
that A ≰T B.
Before proving Theorem 11, let’s consider two examples, where C is a c.e.n. set:
1. A = {⟨e, 0⟩: e ∈C }. Then A is piecewise trivial and c.e.n. The set B = ∅is a
thick, c.e. subset of A such that A ≰T B (because B is computable, whereas A is
not).
2. A = {⟨e, j⟩: e ∈C and j ∈ω}. Then A is piecewise trivial and c.e.n. So, by
Theorem 11, there is a thick, c.e. subset B of A such that A ≰T B. However, it’s
not so easy to identify one.
Proof As usual, let
a0, a1, . . .
denote a standard enumeration of A, and for each s, let As =def {a0, a1, . . . , as }.
We construct B to meet the requirements
Pe : A[e] −B[e] is ﬁnite
Ne : A ̸= B
e
for each e.
We never put an element into B unless we have already seen it in A, so we obtain
B ⊆A without the need for additional requirements. If A[e] is full then, in order to
meet Pe, we must put all but ﬁnitely many elements of A[e] into B[e]. On the other
1 See the comment in the Afternotes section about why this result is called a “lemma” in the literature
but a “theorem” in this chapter.

11.2 The Tree
79
hand, if A[e] is ﬁnite then Pe will automatically be met and so in principle we could
ignore it; however, we have no algorithm to determine, for a given e, whether A[e] is
full or ﬁnite, and so in practice we can never ignore Pe.
11.2
The Tree
We would like to prioritize the requirements as
N0 ≺P0 ≺N1 ≺P1 ≺· · ·
and use a length-of-agreement strategy to meet Ne, as we did in the proof of the
Sacks Splitting Theorem. In that proof we tried to protect a computation Bi
e (k)[s]
by preventing all b ≤ϕBi
e (k)[s] from entering Bi during or after stage s and thereby
injuring Re,i. Sometimes this effort would fail, because a small number could enter
Bi by the action of a stronger priority requirement. It all worked out because the
stronger priority requirements acted only ﬁnitely often, and hence (by induction), all
requirements acted only ﬁnitely often. In the current proof, on the other hand, if A[e]
is full then Pe acts inﬁnitely often; thus, for some e′ such that e < e′, the actions of
Pe might inﬁnitely often injure Ne′.
However, if we knew in advance which rows of A were full, then we could proceed
with the length-of-agreement strategy for the N-requirements. How to proceed is a bit
complicated, and we will see the details soon. Of course, we do know not in advance
which rows are full, but we can beneﬁt by guessing. We organize our guesses as
a tree, as depicted in Fig.11.2. It is an inﬁnite binary tree, in which each node’s
left branch is labeled ∞and right branch is labeled f . Thus, we think of nodes as
ﬁnite-length strings over the alphabet {∞, f }. We write:
1. |σ| to denote the length of (i.e., the number of symbols in) string σ. Thus, node
σ lies on level |σ| of the tree. For example, the empty string λ is the root of the
tree, and |λ| = 0.
2. τ ⪯σ if node τ is an ancestor of node σ. Each node is an ancestor of itself, that
is, σ ⪯σ. Viewed as strings rather than as tree nodes, τ ⪯σ means that τ is a
preﬁx (that is, an initial substring) of σ. Recall that in Chap.5 we wrote τ ≺σ to
mean that string τ is a proper preﬁx of string σ; thus, when viewing strings as tree
nodes, τ ≺σ means that τ is a proper ancestor of σ, that is, τ ⪯σ but τ ̸= σ.
3. ρ <L σ if
(∃τ)[ τ ⌢∞⪯ρ and τ ⌢f ⪯σ ],
as is depicted in Fig.11.3; here we say that “ρ is to the left of σ” or “σ is to the
right of ρ.” In each drawing of a tree in this book, a straight line segment indicates
a single edge, whereas a wavy line indicates a path of zero or more edges.

80
11
A Tree of Guesses (Weak Thickness Lemma)
Fig. 11.2 The tree for Algorithm 11.1
Fig. 11.3 ρ <L σ
4. ρ ◁σ if
|ρ| = |σ| and ρ <L σ,
that is, ρ and σ are on the same level of the tree, and ρ is to the left of σ.
5. ρ ⊴σ if
ρ ◁σ or ρ = σ.
Intuitively, each node σ of the tree represents a sequence of |σ| guesses. Consider
the node σ = f ∞∞f , for example, which is indicated in Fig.11.4. This node σ
represents the following |σ| = 4 guesses:

11.2 The Tree
81
Fig. 11.4 The node σ = f ∞∞f
1. A[0] is ﬁnite, because the 0th symbol of σ is f ,
2. A[1] is full, because the 1st symbol of σ is ∞,
3. A[2] is full, because the 2nd symbol of σ is ∞,
4. A[3] is ﬁnite, because the 3rd symbol of σ is f .
During each stage s, Algorithm 11.1 computes TPs, which is the set of nodes on
the path from the root down to some node on level s of the tree. Thus, |TPs| = s + 1.
The following three statements are equivalent:
1. σ ∈TPs.
2. σ is visited during stage s.
3. s is a σ-stage.
Now, you might regard having three phrases to express the same concept as extrav-
agant. We indulge in this luxury because certain sentences ﬂow better using one
of these phrases rather than the others, and because all three are ubiquitous in the
literature.
A set P of nodes in this tree is an inﬁnite path if λ ∈P, and each node in P has
exactly one child in P. Informally, an inﬁnite path starts at the root and descends
forever. Let
TP =def {the leftmost node on level e that is visited inﬁnitely often : e ∈ω}.
(11.1)
Then TP is an inﬁnite path. For each s, TPs may or may not be a subset of TPs+1, or
of TP. The sequence
TP0, TP1, . . .

82
11
A Tree of Guesses (Weak Thickness Lemma)
Fig. 11.5 The true path consists of the twice-circled nodes
may be thought of as a sequence of lightning bolts (each slightly longer than its
predecessor), that zig and zag somewhat independently from each other.2 The name
TP abbreviates “true path,” because all of the (inﬁnitely many) guesses along TP are
actually correct, as we will show as Lemma 11.1. In fact, TP is the only inﬁnite path
whose guesses are all correct, because distinct inﬁnite paths path1 and path2 guess
differently at the node α where they diverge (that is, α⌢∞∈path1 and α⌢f ∈
path2, or vice versa).
In Fig.11.5, each node circled either once or twice is visited inﬁnitely often.
Uncircled nodes are visited only ﬁnitely often. Nodes circled twice constitute the
members of TP; each level has exactly one member of TP. If a node is circled once
then each of its ancestors is circled either once or twice. If a node is circled twice
then so is each of its ancestors.
If
(∃τ ∈TP)[ σ ◁τ ]
then we write σ <L TP and say that “σ is to the left of TP.” Likewise, if
(∃τ ∈TP)[ τ ◁σ ]
then we write TP <L σ and say that “σ is to the right of TP.” Thus, every node is
either to the left of, in, or to the right of TP.
Analogously, if
(∃τ ∈TPs)[ σ ◁τ ]
2 Some students have found this lightning bolt metaphor to be helpful. If you don’t, then ignore it.

11.2 The Tree
83
Fig. 11.6 The partition of the tree nodes induced by σ
then we write σ <L TPs and say that “σ is to the left of TPs.” Likewise, if
(∃τ ∈TPs)[ τ ◁σ ]
then we write TPs <L σ and say that “σ is to the right of TPs.”
Each node σ (whether or not σ ∈TP) partitions all of the tree nodes into three
sets:
1. {ρ : ρ <L σ}. These nodes are west of σ.
2. {τ : (∃α)[ σ ⊴α and α ⪯τ ]}. These nodes are southeast of σ.
3. All the rest. These nodes are northeast of σ.
This terminology is illustrated in Fig.11.6. Note that the southeast region includes
all of its boundary nodes (including σ). The northeast region includes its western
boundary, that is, the proper ancestors of σ.
Intuitively, each node σ on level e of the tree is trying to meet two requirements:
Pe and Ne. To meet Pe, node σ must force certain elements of A[e] into B.
To meet Ne, node σ makes its own length-of-agreement argument. As part of
this length-of-agreement argument, σ tries to prevent each b ≤restraintσ (which is
deﬁned in the next section) from entering B. The nodes southeast of σ must respect
restraintσ; that is, they may not put numbers b ≤restraintσ into B, and hence they
cannot injure Ne. The nodes northeast of σ likewise do not injure Ne, but the proof
in this case is trickier; this is where the guessing comes into play. The nodes west of
σ need not respect restraintσ, and so can indeed injure Ne; however, if σ ∈TP then
such nodes act (and thereby possibly injure Ne) only ﬁnitely often.
For each e, all that we need is for at least one node on level e to succeed in meeting
Pe and Ne. The node on level e of the true path will succeed (it is possible that some
nodes to the right of the true path will succeed also, but we don’t need them to).
Our tree may be thought of as a tree of guesses, but (as we said in Chap.10), it is
called a “priority tree” in the literature.

84
11
A Tree of Guesses (Weak Thickness Lemma)
11.3
Deﬁnitions
Herein lie the details.
Let σ be a node, and let e = |σ|. In other words, σ is on level e of the tree. The
following deﬁnitions assume that the function Restraintρ has been deﬁned for each
ρ such that ρ ̸= σ and σ is southeast of ρ.
1. A computation B
e (k)[s] is σ-believable if

∀τ ⌢∞⪯σ

∀a ∈ω[|τ|]
Restraintτ(s) < a ≤ϕB
e (k)[s] =⇒a ∈Bs

where (as usual) Bs denotes the set B at the start of stage s. Note that a ∈Bs here
is equivalent to a ∈B[|τ|]
s
because a ∈ω[|τ|].
The function Restraintτ(s) is deﬁned below.
We will need to determine whether a given computation B
e (k)[s] is σ-believable
in a ﬁnite amount of time (in order to compute B
σ (k)[s], which is deﬁned next,
in a ﬁnite amount of time). To do this, it might seem that we need to examine
each a ∈ω[|τ|] for certain nodes τ. However, ω[|τ|] is an inﬁnite set; how can we
examine each of its members in a ﬁnite amount of time? The answer is that we
need examine only those a ∈ω[|τ|] such that a ≤ϕB
e (k)[s].
Some intuition about σ-believability is given after the deﬁnition of B
σ (k)[s].
2.
B
σ (k)[s] =def

B
e (k)[s], if the computation B
e (k)[s] is σ-believable
↑,
otherwise.
Informally, the difference between B
σ (k)[s] and the simpler B
e (k)[s] is as fol-
lows. Suppose τ ≺σ, and let i = |τ|.
First, consider the case τ ⌢∞⪯σ; that is, σ guesses that A[i] is full (in other
words, each member of ω[i] is eventually enumerated into A). If this guess is
correct then Pi requires that B[i] be almost full (that is, we eventually must put
all but ﬁnitely many members of ω[i] into B). So, we ignore the computation
B
e (k)[s] in calculating the length of agreement ℓσ(s) (which is deﬁned next)
unless Bs already contains each a ∈ω[i] such that
Restraintτ(s) < a ≤ϕB
e (k)[s].
This way, node τ will not later force a number into B[i] that could spoil the
computation B
e (k)[s].
Now consider the case τ ⌢f ⪯σ; that is, σ guesses that A[i] is ﬁnite. If this guess
is correct then τ can force only ﬁnitely many numbers into B, and in our proof
we will pick a stage x after which it ceases to do so. This is why the deﬁnition of
σ-believability does not depend on nodes τ such that τ ⌢f ⪯σ.
The last few paragraphs provide high-level intuition; the detailed veriﬁcation is
in Sect.11.5.

11.3 Deﬁnitions
85
3.
ℓσ(0) =def 0
and for s ≥1,
ℓσ(s) =def

max

n ≤s :

∀k < n

As(k) = B
σ (k)[s]

, if σ ∈TPs
ℓσ(s −1),
otherwise.
Informally, ℓσ(s) is the minimum of s and the length of agreement between A and
B
σ at the start of the greatest σ-stage less than or equal to s. As in the proof of
Theorem 9, here we guarantee ℓσ(s) ≤s in order to ensure that ℓσ(s) < ∞and
that each stage of the algorithm can be performed in a ﬁnite amount of time.
4.
restraintσ(0) =def 0
and for s ≥1,
restraintσ(s) =def

max
	
ϕB
e (k)[r] : k ≤ℓσ(s) and r ≤s

, if σ ∈TPs
restraintσ(s −1),
otherwise.
5.
Restraintσ(0) =def 0
and for s ≥1,
Restraintσ(s) =def
⎧
⎪⎪⎨
⎪⎪⎩
max

restraintσ(s),
max
	
Restraintρ(s) : σ is southeast of ρ


, if σ ∈TPs
Restraintσ(s −1),
otherwise.
Informally, restraintσ(s) protects the computations, up through the length of
agreement of A and B
σ at each stage r ≤s, whereas Restraintσ(s) does like-
wise for σ and for every stronger priority node. By “protects” we mean “tries to
protect,” because injuries may happen. The reader should carefully compare these
deﬁnitions to the analogous ones in the proof of Theorem 9.
Even though our interest is limited to nodes on the true path, we must calculate
Restraintτ(s) during stage s for each τ ∈TPs, whether or not τ ∈TP, because
we do not know which nodes constitute the true path.
The values of ℓσ, restraintσ, and Restraintσ can change only during σ-stages.
This is quite useful; for example, if σ <L TP then there are only ﬁnitely many
σ-stages and hence
max
s
Restraintσ(s) < ∞.
Both restraintσ(s) and Restraintσ(s) are monotonic in s (see Exercise 4).
6.
˜ℓσ =def max
s
ℓσ(s)
7.
˜Rσ =def max
s
Restraintσ(s)

86
11
A Tree of Guesses (Weak Thickness Lemma)
We will see (in Lemma 11.2) that
σ ∈TP =⇒
 ˜ℓσ < ∞
and
˜Rσ < ∞

.
8. Let s be a σ-stage. Then
predσ(s) =def

−1,
if s is the ﬁrst σ-stage
max{r < s : r is a σ-stage}, otherwise.
9.
A[e]
−1 =def ∅
and for s ≥0,
A[e]
s
=def As
 ω[e].
Thus, if s ≥0 then A[e]
s
is the eth row of As, just as A[e] is the eth row of A.
Our sole reason for deﬁning A[e]
−1 is to simplify the condition in line 6 of
Algorithm 11.1.
11.4
The Algorithm
The algorithm is as follows.
Algorithm 11.1
1
B ←∅.
2 for s ←0 to ∞
// Compute TPs .
3
TPs ←{λ}.
4
τ ←λ.
5
for e ←0 to s −1
6
if
 A[e]
s
 >
 A[e]
predτ (s)

// Numbers have entered A[e] since the last τ-stage.
7
τ ←τ⌢∞
8
else τ ←τ⌢f .
9
TPs ←TPs ∪{τ}.
// Now, |TPs| = e + 2.
// Now, |TPs| = s + 1.
// B ←B ∪{certain elements of A}.
10
for e ←0 to s
11
ξ ←the level e node of TPs.
12
for each a ∈A[e]
s
−B such that a > Restraintξ(s)
// Node ξ forces a into B.
13
Put a into B.

11.5 Veriﬁcation
87
11.5
Veriﬁcation
As usual, B is built in stages, each of which is executable in a ﬁnite amount of time;
hence B is c.e. We never put an element into B unless we know that it is in A, so
B ⊆A. Lemma 11.3 states that the N-requirements are met, and Lemma 11.4 does
likewise for the P-requirements, thereby completing the proof of Theorem 11.
Lemma 11.1 Let σ be the level e node of TP. Then
σ⌢∞∈TP ⇐⇒A[e] is full.
In other words, all guesses along the true path are correct.
Proof If σ⌢∞∈TP then node σ⌢∞is visited inﬁnitely often; therefore A[e] is
inﬁnite (verify!) and hence, since A is piecewise trivial, A[e] is full.
On the other hand, if σ⌢f ∈TP then node σ⌢∞, being to the left of σ⌢f , is
visited only ﬁnitely often, even though σ is visited inﬁnitely often. Therefore A[e] is
ﬁnite and hence not full.
□
Lemma 11.2 contains some properties of nodes on the true path, which are used
in the proofs of Lemmas 11.3 and 11.4.
Lemma 11.2 Let σ be the level e node of TP. Then
I. There is a stage x such that the following four conditions are true:
(i)

∀τ ≺σ

Restraintτ(x) = ˜Rτ

(ii) No ρ <L σ is visited during or after stage x.
(iii)

∀i ≤e

A[i] is ﬁnite
=⇒B[i]
x = B[i] 
(iv)

∀σ-stage s ≥x

∀k ≤ℓσ(s)

B
σ (k)[s] ↓=⇒the computation
B
e (k)[s] is permanent

Note that if k < ℓσ(s) then B
σ (k)[s] ↓, whereas it is possible that
B
σ (ℓσ(s))[s] ↑(as in Fig. 9.1, but not in Fig. 9.2).
II. ˜ℓσ < ∞.
III.
˜Rσ < ∞.
Proof We use strong induction; assume all three parts of the lemma for each τ ≺σ.
Proof of Part I.
There is a stage x that simultaneously satisﬁes conditions (i), (ii),
and (iii), because:
(i)
Node σ has only ﬁnitely many proper ancestors. Suppose τ is one of them.
Then ˜Rτ < ∞(by the inductive hypothesis). Furthermore, Restraintτ(s) is
monotonic in s (see Exercise 4).

88
11
A Tree of Guesses (Weak Thickness Lemma)
(ii)
Let ρ <L σ. Then there exists τ such that τ ⌢∞⪯ρ and τ ⌢f ⪯σ, as in
Fig.11.3. Because σ is in TP, so is its ancestor τ ⌢f . Therefore node τ ⌢∞
is visited only ﬁnitely often; hence, each of its descendants, including ρ, is
visited only ﬁnitely often.
(iii)
If A[i] is ﬁnite then so is B[i], because B ⊆A.
We now show that conditions (i), (ii), and (iii) on the choice of x together imply
condition (iv) on the choice of x. Fix some σ-stage s ≥x and some k ≤ℓσ(s) such
that B
σ (k)[s] ↓; we will show that the computation B
e (k)[s] is permanent. Suppose
that some node ξ forces a number a into B during a stage t ≥s. We will show
a > ϕB
e (k)[s].
(11.2)
Let i = |ξ|. Consider three cases:
Case 1.
ξ is west of σ.
That is, ξ <L σ. This is impossible, by condition (ii) on the choice of x, because
t ≥s ≥x.
Case 2.
ξ is southeast of σ.
The situation is depicted in Fig.11.7 (where e < i, although another possibility
is that e = i). Then
a > Restraintξ(t)
(by line 12 of Algorithm 11.1)
≥Restraintσ(t)
(because ξ is southeast of σ, and ξ ∈TPt)
≥restraintσ(t)
≥restraintσ(s)
(by the monotonicity of restraintσ, because t ≥s)
≥ϕB
e (k)[s]
(by the deﬁnition of restraintσ, because k ≤ℓσ(s) and σ ∈TPs ).
Fig. 11.7 ξ is southeast of σ

11.5 Veriﬁcation
89
Fig. 11.8 ξ is northeast of σ
Case 3.
ξ is northeast of σ.
This is where the guessing comes into play, and so it lies at the heart of this chapter.
Let τ be the level i ancestor of σ, as depicted in Fig.11.8 (in which τ ◁ξ; another
possibility is that τ = ξ). Note that i < e, because the southeast region contains
all of its boundary nodes; thus τ ̸= σ. We have
a ∈B[i]
t+1 −B[i]
t
(because a enters B during stage t)
⊆B[i]
t+1 −B[i]
x
(because t ≥s ≥x)
⊆B[i] −B[i]
x
and so the contrapositive of condition (iii) on the choice of x implies that A[i] is
full. Hence (since σ ∈TP and therefore τ ∈TP) we have τ ⌢∞⪯σ by Lemma
11.1 (as indicated in Fig.11.8). Since B
σ (k)[s] ↓, the computation B
e (k)[s] is
σ-believable; therefore, because τ ⌢∞⪯σ and a ∈ω[i], we have

Restraintτ(s) < a ≤ϕB
e (k)[s]

=⇒a ∈Bs .
(11.3)
Furthermore,
Restraintτ(s) ≤Restraintτ(t)
(by the monotonicity of Restraintτ, because s ≤t)
≤Restraintξ(t)
(because ξ is southeast of τ, and ξ ∈TPt)
< a
(because ξ forces a into B during stage t; see line 12 of Algorithm 11.1).
Hence, since Restraintτ(s) < a and a /∈Bs (because a enters B during stage
t ≥s), we have a > ϕB
e (k)[s] by (11.3).
To summarize: Case 1 is impossible, and in either Case 2 or Case 3 we have (11.2);
hence, the computation B
e (k)[s] is permanent. Therefore stage x satisﬁes condition
(iv). This concludes the proof of Part I.
Proof of Part II. The proof of ˜ℓσ < ∞resembles the proof of Part I of Lemma 9.2.
In particular, assume for a contradiction that

90
11
A Tree of Guesses (Weak Thickness Lemma)
max
s
ℓσ(s) = ∞.
(11.4)
Then the following algorithm, which knows x, could determine whether a given p
is in A:
Algorithm 11.2
1 Run Algorithm 11.1 until reaching the ﬁrst
σ-stage s > x such that ℓσ(s) > p.
// Such an s exists by (11.4).
2 if B
e (p)[s] = 1
3
output(“p is in A”)
4 else output(“p is not in A”).
By condition (iv) on the choice of x, the computation B
e (p)[s] is permanent, and
so
B
e (p) = B
e (p)[s].
(11.5)
If
B
e (p)[s] = 1
then p ∈As (because ℓσ(s) > p), and so p ∈A. On the other hand, if
B
e (p)[s] = 0
then (by (11.5))
B
e (p) = 0
and so p /∈A, because otherwise
A(p) = 1 ̸= 0 = B
e (p)
contradicting (11.4) (see Exercise 6). To summarize,
p ∈A ⇐⇒B
e (p)[s] = 1,
and so the output of Algorithm 11.2 is correct. Therefore A is computable, contrary
to the assumption in the statement of the theorem. Thus, the assumption (11.4) has
led to a contradiction, and so ˜ℓσ < ∞. This concludes the proof of Part II.
Proof of Part III. If σ is southeast of some node ρ, then
ρ <L σ or ρ ≺σ or ρ = σ
(11.6)
(take another look at Fig.11.6, and note that the converse need not be true). From
the deﬁnitions of ˜Rσ and Restraintσ(s), we have

11.5 Veriﬁcation
91
˜Rσ = max
s
	
Restraintσ(s)

≤max
s
	
restraintρ(s) : σ is southeast of ρ

(actually, the above inequality can be shown to be an equation, but the inequality
here sufﬁces for our purposes). Therefore, by (11.6), we have
˜Rσ ≤max
s
	
restraintρ(s) : ρ <L σ

∪
	
restraintτ(s) : τ ≺σ

∪
	
restraintσ(s)


.
(11.7)
By condition (ii) on the choice of x,
max
s>x
	
restraintρ(s) : ρ <L σ

< ∞.
(11.8)
Exercise 5, condition (i) on the choice of x, and the inductive hypothesis together
imply
max
s>x
	
restraintτ(s) : τ ≺σ

≤max
s>x
	
Restraintτ(s) : τ ≺σ

< ∞,
(11.9)
because σ has only ﬁnitely many ancestors. Part II (that is, ˜ℓσ < ∞) and condition
(iv) on the choice of x together imply
max
s>x
	
ϕB
σ (k)[s] : k ≤ℓσ(s)

< ∞,
and so
max
s>x
	
restraintσ(s)

< ∞.
(11.10)
Together, (11.7)–(11.10) imply
˜Rσ < ∞.
QED Lemma 11.2
Lemma 11.3 Ne is met, for all e.
Proof This resembles the proof of Lemma 9.3, but is more complicated because it
deals with ℓσ rather than ℓe.
Fix e. Let σ be the level e node of TP. Assume for a contradiction that Ne is not
met; that is,
A = B
e .
(11.11)
By Part II of Lemma 11.2, there exists
m = max
s { ℓσ(s) }.
(11.12)

92
11
A Tree of Guesses (Weak Thickness Lemma)
Let x be such that
(i) Ax ↾↾m = A↾↾m,
(ii)

∀k ≤m

the computation B
e (k)[x] is permanent

,
(iii)

∀τ ≺σ

Restraintτ(x) = ˜Rτ

.
(iv) m ≤x.
Such an x exists by
(i) the As being an enumeration of A,
(ii) Equation (11.11) and the Permanence Lemma.
(iii) Part III of Lemma 11.2 applied to each proper ancestor of σ, and the mono-
tonicity of Restraintτ.
Let y > x be a σ-stage such that
Ay ↾↾u = A↾↾u
(11.13)
where
u =def max
	
ϕB
e (k)[x] : k ≤m

.
By condition (ii) on the choice of x, and because y > x,
u = max
	
ϕB
e (k)[y] : k ≤m

.
(11.14)
Because each ancestor of σ is in TP, Lemma 11.1 implies

∀τ ⌢∞⪯σ

A[|τ|] is full

.
(11.15)
By (11.13) and (11.15), for each τ ⌢∞⪯σ, each
a ∈ω[|τ|] −By
such that
Restraintτ(y) < a ≤u
is forced into B by node τ during stage y (see lines 12–13 of Algorithm 11.1),
because y is a σ-stage and hence also a τ-stage. Thus,

∀τ ⌢∞⪯σ

∀a ∈ω[|τ|]
Restraintτ(y) < a ≤u
=⇒a ∈By+1

.
(11.16)
Let z > y be a σ-stage. Then by condition (iii) on the choice of x, we have

∀τ ⌢∞⪯σ

[ Restraintτ(z) = Restraintτ(y) ( = ˜Rτ )].
(11.17)

11.5 Veriﬁcation
93
Together, (11.14), (11.16), (11.17), condition (ii) on the choice of x, and By+1 ⊆Bz
imply

∀k ≤m

∀τ⌢∞⪯σ

∀a ∈ω[|τ|]
Restraintτ(z) < a ≤ϕB
e (k)[z] =⇒a ∈Bz

;
in other words,

∀k ≤m

the compututation B
e (k)[z] is σ-believable

.
(11.18)
For each k ≤m,
Az(k) = A(k)
(by condition (i) on the choice of x, because z > x)
= B
e (k)
(by 11.11)
= B
e (k)[z]
(by condition (ii) on the choice of x, because z > x)
= B
σ (k)[z]
(by (11.18), and the deﬁnition of B
σ (k)[z]).
Therefore, by condition (iv) on the choice of x (which implies m + 1 ≤z), and
because σ ∈TPz,
ℓσ(z) ≥m + 1
(take another look at the deﬁnition of ℓσ(z)), contradicting (11.12).
QED Lemma 11.3
Lemma 11.4 Pe is met, for all e.
Proof Fix e. Let σ be the level e node of TP. By Lemma 11.2, ˜Rσ < ∞. Because σ
is visited inﬁnitely often, σ forces every member of
	
a ∈A[e] : a > ˜Rσ

into B, unless some other node on level e forced it into B ﬁrst. Thus, the set
A[e] −B

= A[e] −B[e] 
is ﬁnite (in particular,
 A[e] −B
 ≤˜Rσ + 1).
QED Lemma 11.4
Thus, all the requirements of the theorem are met.
QED Theorem 11

94
11
A Tree of Guesses (Weak Thickness Lemma)
11.6
What’s New in This Chapter?
Trees, of course. Let’s take an informal look at the tree method as it is used in
Theorem 11. Think of each tree node as a little mathematician. Collectively, the
nodes on level e are responsible for meeting both Pe and Ne. They meet Pe by
forcing all but ﬁnitely many of the elements of A[e] into B; various level e nodes
might contribute to this effort. To meet Ne, on the other hand, the level e nodes
work independently. Each one endeavors to meet Ne by its own length-of-agreement
argument, in particular, where the length of agreement is between A and B
σ . Each
one operates with its own set of e assumptions (or “guesses” as we’ve been calling
them).
Nodes are not overworked. Consider a node σ on level e. It labors (on behalf of
Pe or Ne or both) only during σ-stages. During all other stages, it naps. Its effort to
meet Ne can be thwarted (“injured”) by other nodes at various levels of the tree, as
they force numbers into B to meet their own P-requirements. We need to protect
σ’s length-of-agreement argument from such injuries. Nodes southeast of σ cannot
injure it, because they must respect Restraintσ(s). Nodes northeast of σ can indeed
injure it; however, if σ’s guesses are all correct (that is, σ ∈TP), then they injure it
only ﬁnitely often. Nodes west of σ are troublesome, because they could keep forcing
numbers less than Restraintσ into B, while causing Restraintσ to grow without
bound. That might indeed happen for certain nodes σ, but not for σ ∈TP (because if
σ ∈TP then there are only ﬁnitely many stages s such that TPs <L σ). Again, what
saves us is that we don’t need every node on level e to succeed; rather, we need just
one of them to do so. The node at level e on the true path will succeed. Some other
nodes at level e, to the right of the true path, might succeed, too, although it might be
difﬁcult to prove that they do. The key is that one successful node per level is enough.
However, at no stage during the construction can we know which level e node lies on
the true path, so we must keep every node employed. Each node just keeps plugging
away (whenever it is visited); if it does happen to lie on the true path, then its efforts
will not have been in vain. Thus, even though the entire proof is called an “inﬁnite
injury” argument, for nodes in TP it’s just a ﬁnite injury argument.
Here is another perspective, quoting [Cs], on tree-based algorithms for coping
with inﬁnite injury (the bracketed comments are mine):
The idea of “injury” made a lot of sense for describing the original ﬁnite injury priority
arguments. However, with more complicated arguments, perhaps “inﬁnite injury” is the
wrong way to think about it. Really what is happening during these constructions, is that
based on a current approximation [that is, TPs], a view is determined, and a next step is
taken in a construction. The important thing is to have damage control for the harm that can
be done at a stage with a wrong guess [that is, when visiting a node not in TP], and to make
sure that enough good things happen when you have a good guess [that is, when visiting a
node in TP].

11.8 Exercises
95
11.7
Afternotes
1. It might seem incongruous that our Theorem 11 is called a “lemma” in the litera-
ture. That’s because various (and somewhat stronger) versions of this result, and
of our Theorem 12, are used to prove results about the c.e. “degrees” (see [So87]
for details). Because our focus is on algorithms, we regard this result as a theorem
on a par with all of our theorems in Chap.4 and beyond.
2. Many authors (e.g., [DH]) use the phrase σ-believability in various tree arguments
essentially the way we do. Another name for it is σ-correctness (Deﬁnition 3.2
of Chap. XIV of [So87]).
Is there a useful generalization of this concept? Perhaps more than two levels of
believability, deﬁned in some way, might be helpful for certain applications.
3. Nodes that lie to the left of the true path are visited only ﬁnitely often, and hence
are uninteresting. Nodes on the true path are studied extensively, because they
enable us to prove that the requirements are met. To the right of the true path lies
terra incognita. Some of those nodes might be visited inﬁnitely often, and some
not. Some of their corresponding variables (such as their length of agreement
functions, or their restraint functions) might grow without bound, and some not.
There is no published research about them, as far as this author is aware. Perhaps
there are non-trivial results about those nodes—gems waiting to be unearthed—
for this and for other tree algorithms. Perhaps, by looking at those nodes, we could
categorize various tree algorithms in an interesting way. Perhaps, for example, tree
algorithms in which there are only ﬁnitely many inﬁnite paths are more limited
in power than those with inﬁnitely many inﬁnite paths.
11.8
Exercises
1. Give a very simple proof of the Weak Thickness Lemma in the special case in
which each row of the c.e.n. set A is ﬁnite.
2. Let A be piecewise trivial and c.e.n. Must
	
e : A[e] is full

be c.e.?
3. The true path is a set of tree nodes, which we often think of as ﬁnite-length
strings. Lexicographic order on these strings gives us a computable, one-to-one
correspondence between them and ω. In this sense, we can think of TP as a set
of natural numbers.
(a) Give an example of a piecewise trivial, c.e.n. set A for which TP is
computable.
(b) Give an example of a piecewise trivial, c.e.n. set A for which TP is
non-computable.

96
11
A Tree of Guesses (Weak Thickness Lemma)
4. Prove that

∀σ

∀s

restraintσ(s) ≤restraintσ(s + 1)
and
Restraintσ(s) ≤Restraintσ(s + 1)

.
5. Prove that

∀σ

∀s

restraintσ(s) ≤Restraintσ(s)

.
6. Let σ be a node on level e of the tree. Prove that
(∃q)[ B
e (q) ↓
and A(q) ̸= B
e (q)] =⇒max
s
ℓσ(s) < ∞.
7. Suppose that we replace B
σ by B
e in the deﬁnition of ℓσ. In other words, we
ignore the guesses represented by σ. Explain how the proof would fail.
8. During stage s, a node ξ ∈TPs on a level e < s might force elements into B
whether
ξ⌢∞∈TPs
or
ξ⌢f ∈TPs.
Suppose that we modify the algorithm so that only those nodes ξ such that
ξ⌢∞∈TPs may do so. That is, suppose we replace the code
// B ←B ∪{certain elements of A}.
for e ←0 to s −1
ξ ←the level e node of TPs.
for each a ∈A[e]
s −B such that a > Restraintξ(s)
// Node ξ forces a into B.
Put a into B.
by
// B ←B ∪{certain elements of A}.
for e ←0 to s −1
ξ ←the level e node of TPs.
for each a ∈A[e]
s −B such that a > Restraintξ(s)
if ξ⌢∞≺TPs
// Node ξ forces a into B.
Put a into B.
Would the algorithm still work (that is, would the constructed set B still meet all
of the requirements)?

11.8 Exercises
97
9. During each stage we try to put certain numbers into B. What if we allowed
numbers to enter B only during stages that are powers of 2? Would the algorithm
still work?
10. A set S is coﬁnite if ¯S is ﬁnite. Note that ∞and f are the only two nodes on
level 1 of the tree.
(a) Is it possible that the set of ∞-stages is coﬁnite?
(b) Is it possible that the set of f -stages is coﬁnite?
11. Recall the computation B
e (k)[s] is deﬁned as σ-believable if

∀τ ⌢∞⪯σ

∀a ∈ω[|τ|]
Restraintτ(s) < a ≤ϕB
e (k)[s] =⇒a ∈Bs

.
(a) If we change the original deﬁnition to

∀τ⌢∞⪯σ

∀a ∈ω[|τ|]
Restraintτ(s) < a ≤ϕB
e (k)[s] + 1000 =⇒a ∈Bs

.
would the algorithm still work?
(b) If we change the original deﬁnition to

∀τ⌢∞⪯σ

∀a ∈ω[|τ|]
Restraintτ(s) < a ≤restraintσ(s) =⇒a ∈Bs

.
would the algorithm still work?
12. In Lemma 11.2, we showed that
˜Rσ < ∞
if σ ∈TP.
(a) Would it still be true if σ <L TP?
(b) Would it still be true if TP <L σ?
See the remarks about terra incognita in the Afternotes section.
13. In line 2 of Algorithm 11.2, suppose that we change the condition
B
e (p)[s] = 1
to
B
σ (p)[s] = 1.
Would that algorithm still correctly determine whether p ∈A?
14. Deﬁne the injury set for a node σ as
Iσ =
	
a : a enters B during a stage s such that a ≤Restraintσ(s)

.

98
11
A Tree of Guesses (Weak Thickness Lemma)
If σ ∈TP then Iσ is ﬁnite. Can there be a node τ such that Iτ is inﬁnite? If so,
then what can you say about τ?
15. For each σ ∈TP, there is a stage x after which no ρ <L σ is visited; this fact is
essential in this chapter, and (as far as I know) in the veriﬁcation of all algorithms
based on priority trees.
Must there be a stage after which no ρ <L TP is visited?
16. Is there a standard enumeration a0, a1, . . . of a piecewise trivial, c.e.n. set A
such that, if it were given as the input to Algorithm 11.1, the set
{s : TPs ⊂TP}
would be ﬁnite?

Chapter 12
An Inﬁnitely Branching Tree (Thickness
Lemma)
The priority tree used in Chap.11 was binary, that is, each node had exactly two
children. Our next tree is inﬁnitely branching, that is, each node has inﬁnitely many
children.
A set A is piecewise computable if for all e, A[e] is computable. Because every
piecewise trivial set is also piecewise computable, the following result generalizes
the Weak Thickness Lemma.
Theorem 12 (Thickness Lemma) Let A be c.e.n. and piecewise computable. Then
there is a thick, c.e. subset B of A such that A ≰T B.
Proof The requirements are exactly the same as for the Weak Thickness Lemma:
Pe : A[e] −B[e] is ﬁnite
Ne : A ̸= B
e
for each e.
12.1
The Tree
We write S △T to denote the symmetric difference between sets S and T , that is,
S △T =def (S −T ) ∪(T −S).
For each e, because A[e] is computable, there exists an r such that Wr and A[e] are
complements, that is,
A[e] △Wr = ω,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_12
99

100
12
An Inﬁnitely Branching Tree (Thickness Lemma)
Fig. 12.1 A node and its
branches
Fig. 12.2 ρ <L σ
and so
A[e] △W [e]
r
= ω[e].
(12.1)
Each node σ on level e of the tree has branches labeled with the elements of ω,
in ascending order from left to right, as is depicted in Fig.12.1.
As in the proof of the Weak Thickness Lemma, σ represents a sequence of e
numbers, which we think of as a sequence of guesses. Node σ ⌢r represents the
guess that r is the least number such that (12.1) is true.
With this inﬁnitely branching tree, we write ρ <L σ if

∃τ,r,r′
r < r′ and τ ⌢r ⪯ρ and τ ⌢r′ ⪯σ

,
(12.2)
as is illustrated in Fig.12.2. This is a natural generalization of the deﬁnition of
ρ <L σ that we used for the binary tree in the proof of the Weak Thickness Lemma
(see Fig.11.3).
12.2
Deﬁnitions, and a Fact
Let σ be a node on level e of the tree. Then a computation B
e (k)[s] is σ-believable
if

∀τ ⌢r ⪯σ

∀a ∈ω[|τ|] −Wr[s]

Restraintτ(s) < a ≤ϕB
e (k)[s] =⇒a ∈Bs

.
(12.3)

12.2 Deﬁnitions, and a Fact
101
k
0
1
2
3
4
5
6
7
8
A(⟨e, k⟩)[s]
0
0
1
0
1
0
1
0
1
Wr(⟨e, k⟩)[s]
1
1
0
1
0
1
1
0
0
Fig. 12.3 The length of disagreement is 6
Carefully compare this to the deﬁnition of σ-believability in Chap.11.
We deﬁne B
σ (k)[s], ϕB
σ (k)[s], ℓσ(s), restraintσ(s), Restraintσ(s), ˜ℓσ, and ˜Rσ
exactly as in Chap.11, except now these deﬁnitions are based on our modiﬁed deﬁ-
nition of σ-believability.
Informally, Algorithm 12.1 uses B
σ (k)[s] rather than B
e (k)[s] so that it need not
worry about the computation getting spoiled (Algorithm 11.1 did analogously). In
particular, suppose τ ⌢r ⪯σ. Let i = |τ|. If τ ⌢r represents a correct guess, then
A[i] = ω[i] −Wr
and so Pi requires that B[i] contain all but ﬁnitely many members of ω[i] −Wr. So,
we ignore the computation B
e (k)[s] in calculating the length of agreement ℓσ(s)
unless Bs already contains each a ∈ω[i] −Wr[s] such that
Restraintτ(s) < a ≤ϕB
e (k)[s].
This way, node τ will not later force a number into B[i] that could spoil the compu-
tation B
e (k)[s].
Recall that ℓσ denotes the length of agreement function between A and B
σ . We
deﬁne also, for each r, the length of disagreement between A[e] and W [e]
r
as follows:
ℓe,r(s) =def max

n ≤s :

∀k < n

A(⟨e, k⟩)[s] ̸= Wr(⟨e, k⟩)[s]
 
.
In Fig.12.3, ℓe,r(s) = 6, assuming that s ≥6. Compare the length of disagreement
in Fig.12.3 with the length of agreement in Figs.9.1 and 9.2. See Exercise 2.
A function f : ω →ω is unimodal if there is a b such that

∀a < b

f (a) ≤f (a + 1)

and

∀c ≥b

f (c) ≥f (c + 1)

.

102
12
An Inﬁnitely Branching Tree (Thickness Lemma)
Fact 12.1

∀e, r

ℓe,r is unimodal or monotonically non-decreasing

.
The proof of Fact12.1 is left as Exercise 3.
12.3
The Algorithm
The algorithm is as follows:
Algorithm 12.1
1
B ←∅.
2 for s ←0 to ∞
// Compute TPs .
3
TPs ←{λ}.
4
τ ←λ.
5
for e ←0 to s −1
6
if

∃j ≤s
	
ℓe, j

s

> ℓe, j

predτ(s)
 
// ℓe, j has grown since the last τ-stage.
7
r ←the least such j
8
else r ←s.
9
τ ←τ ⌢r.
10
TPs ←TPs ∪{τ}.
// B ←B ∪{certain elements of A}.
11
for e ←0 to s
12
ξ ←the level e node of TPs .
13
for each a ∈A[e]
s
−B such that a > Restraintξ(s)
// Node ξ forces a into B.
14
Put a into B.
Notes on Algorithm 12.1:
1. In line 6, if s is the ﬁrst τ-stage, then predτ(s) = −1. Deﬁne ℓe, j(−1) = −1.
2. During each stage of Algorithm 12.1, two tasks are performed: compute TPs, and
add certain elements to B. The same is true for Algorithm 11.1.
3. Lines 11–14 of Algorithm 12.1 are identical to lines 10–13 of Algorithm 11.1.
12.4
Veriﬁcation
The true path for Algorithm 12.1 is deﬁned word-for-word as in (11.1).
All arguments based on priority trees need TP to be an inﬁnite path. For ﬁnitely
branching trees (such as the binary tree in Chap.11), it is obvious that TP is an inﬁnite
path. It is not so obvious for inﬁnitely branching trees. For Algorithm 12.1, it follows

12.4 Veriﬁcation
103
from Lemma12.1. Lemmas12.1, 12.2, 12.3, and 12.4 are analogous to Lemmas11.1,
11.2, 11.3, and 11.4, respectively.
For each e, let
Je =def

j : A[e] △W [e]
j
= ω[e]
.
Fact 12.2 For each e, Je is non-empty.
Proof Fix e. Because A is piecewise computable, A[e] is computable. Therefore,
letting
D = ω[e] −A[e],
D is the difference of two computable sets, and hence itself is computable. Therefore
D is c.e., and so it equals W j for some j. Furthermore,
A[e] △W j = A[e] △

ω[e] −A[e] 
= ω[e],
and so j ∈Je. Hence, Je is non-empty.
□
Lemma 12.1 Suppose that σ is on level e of TP. Let
re = min(Je)
(this minimum exists, by Fact12.2). Then σ ⌢re ∈TP.
Proof Because σ ∈TP, there is a stage y after which no ρ ◁σ is visited.
Assume for a contradiction that σ ⌢j is visited inﬁnitely often for some j < re.
Then ℓe, j is not unimodal. Therefore, by Fact12.1, it is monotonic, and so
A[e] △W [e]
j
= ω[e],
that is, j ∈Je, contradicting re = min(Je). Hence there is a stage z > y after which
no node τ ◁σ ⌢re is visited.
To showσ ⌢re ∈TP, it remains only to show that σ ⌢re is visited inﬁnitely often.
Because σ ∈TP, there are inﬁnitely many σ-stages. Therefore, becausere ∈Je, there
are inﬁnitely many σ-stages
s > z
such that
ℓe,re

s

> ℓe,re

predσ(s)

;
each of these is a (σ ⌢re)-stage.
QED Lemma 12.1
The root of the tree is in TP. By Lemma12.1, each node in TP has a child in TP.
Therefore, by ordinary induction on the level of the tree, TP is an inﬁnite path. Also
by Lemma12.1, each guess (as speciﬁed in Sect.12.1) in TP is correct.

104
12
An Inﬁnitely Branching Tree (Thickness Lemma)
Lemma 12.2 Let σ be the level e node of TP. Then
I. There is a stage x such that the following three conditions are true:
(i)

∀τ ≺σ

Restraintτ(x) = ˜Rτ

(ii) No ρ <L σ is visited during or after stage x.
(iii)

∀σ-stage s ≥x

∀k ≤ℓσ(s)

B
σ (k)[s] ↓=⇒the computation
B
e (k)[s] is permanent

II. ˜ℓσ < ∞.
III.
˜Rσ < ∞.
Note that conditions (i), (ii), and (iii) here are identical to conditions (i), (ii), and (iv)
of Part I of Lemma11.2, respectively. There is no analog here to condition (iii) of
Part I of Lemma11.2. Parts II and III here are identical to Parts II and III of
Lemma11.2, respectively.
Proof The proofs of Lemmas11.2 and 12.2 are similar. Again, we use strong induc-
tion, assuming all three parts of the lemma for each τ ≺σ.
Proof of Part I There is a stage x that simultaneously satisﬁes conditions (i) and (ii),
because:
(i) Node σ has only ﬁnitely many proper ancestors. Suppose τ is one of them.
Then ˜Rτ < ∞(by the inductive hypothesis). Furthermore, Restraintτ(s) is
monotonic in s.
(ii) Let ρ <L σ. Then there exist τ, r, and r′ such that r < r′ and τ ⌢r ⪯ρ and
τ ⌢r′ ⪯σ, as in Fig.12.2. Because σ is in TP, so is its ancestor τ ⌢r′. Therefore
node τ ⌢r is visited only ﬁnitely often; hence, each of its descendants, including
ρ, is visited only ﬁnitely often.
Wenowshowthatconditions(i)and(ii)onthechoiceof x togetherimplycondition
(iii) on the choice of x. Fix some σ-stage s ≥x and some k ≤ℓσ(s) such that
B
σ (k)[s] ↓; we will show that the computation B
e (k)[s] is permanent. Suppose
that some node ξ forces a number a into B during a stage t ≥s. We will show
a > ϕB
e (k)[s].
(12.4)
Let i = |ξ|. Consider three cases:
Case 1.
ξ is west of σ.
That is, ξ <L σ. This is impossible, by condition (ii) on the choice of x, because
t ≥s ≥x.
Case 2.
ξ is southeast of σ.
The situation is depicted in Fig.11.7 (where e < i, although another possibility
is that e = i). Then

12.4 Veriﬁcation
105
Fig. 12.4 ξ is northeast of σ
a > Restraintξ(t)
(by line 13 of Algorithm 12.1)
≥Restraintσ(t)
(because ξ is southeast of σ, and ξ ∈TPt)
≥restraintσ(t)
≥restraintσ(s)
(by the monotonicity ofrestraintσ, because t ≥s)
≥ϕB
e (k)[s]
(by the deﬁnition ofrestraintσ, because k ≤ℓσ(s) and σ ∈TPs).
Case 3.
ξ is northeast of σ.
Here is where the proofs of Lemmas11.2 and 12.2 differ most.
Let τ be the level i ancestor of σ. Note that i < e, because the southeast region
contains all of its boundary nodes; thus τ ̸= σ. Let r be such that τ ⌢r ⪯σ, as
depicted in Fig.12.4 (in which τ ◁ξ; another possibility is that τ = ξ).
Since B
σ (k)[s] ↓, the computation B
e (k)[s] is σ-believable; therefore, because
τ ⌢r ⪯σ, by (12.3) we have

∀a ∈ω[i] −Wr[s]

Restraintτ(s) < a ≤ϕB
e (k)[s] =⇒a ∈Bs

.
Therefore, because a ∈ω[i],

a /∈Wr[s] and Restraintτ(s) < a ≤ϕB
e (k)[s]

=⇒a ∈Bs .
(12.5)
Furthermore,
Restraintτ (s) ≤Restraintτ (t)
(by the monotonicity of Restraintτ , because s ≤t)
≤Restraintξ(t)
(because ξ is southeast of τ, and ξ ∈TPt)
< a
(because ξ forces a into B during stage t; see line 13 of Algorithm 12.1).
Hence, since Restraintτ(s) < a and a /∈Bs (because a enters B during stage
t ≥s), we have
a ∈Wr[s] or a > ϕB
e (k)[s]
(12.6)

106
12
An Inﬁnitely Branching Tree (Thickness Lemma)
by (12.5).1 Lemma12.1 (applied to τ) and τ ⌢r ⪯σ ∈TP together imply
r ∈Ji
and so
A[i] △W [i]
r
= ω[i].
Hence, because a ∈A[i],
a /∈Wr .
Therefore, by (12.6), we have a > ϕB
e (k)[s].
To summarize: Case 1 is impossible, and in either Case 2 or Case 3 we have (12.4);
hence, the computation B
e (k)[s] is permanent. Therefore stage x satisﬁes condition
(iii). This concludes the proof of Part I of Lemma12.2.
The proofs of Parts II and III of Lemma12.2 are word-for-word the same as those
of Lemma11.2, except that here we replace “condition (iv)” by “condition (iii).”
QED Lemma 12.2
Lemma 12.3 Ne is met, for all e.
Proof This resembles the proof of Lemma11.3.
Fix e. Let σ be the level e node of TP. Assume for a contradiction that Ne is not
met; that is,
A = B
e .
(12.7)
By Part II of Lemma12.2, there exists
m = max
s { ℓσ(s) }.
(12.8)
Let x be such that
(i) Ax ↾↾m = A↾↾m,
(ii)

∀k ≤m

the computation B
e (k)[x] is permanent

,
(iii)

∀τ ≺σ

Restraintτ(x) = ˜Rτ

,
(iv) m ≤x.
Such an x exists by
(i) the As being an enumeration of A,
(ii) Equation(12.7) and the Permanence Lemma,
(iii) Part III of Lemma12.2 applied to each proper ancestor of σ, and the mono-
tonicity of Restraintτ.
1 Boolean algebra is a joy forever, in my opinion.

12.4 Veriﬁcation
107
Let y > x be a σ-stage such that
Ay ↾↾u = A↾↾u
(12.9)
and

∀τ ⌢r ⪯σ

Wr[y]↾↾u = Wr ↾↾u

(12.10)
where
u =def max

ϕB
e (k)[x] : k ≤m

.
By condition (ii) on the choice of x, and because y > x,
u = max

ϕB
e (k)[y] : k ≤m

.
(12.11)
Because each ancestor of σ is in TP, Lemma12.1 implies

∀τ ⌢r ⪯σ

A[|τ|] △W [|τ|]
r
= ω[|τ|] 
.
(12.12)
Let τ and r be such that τ ⌢r ⪯σ, and for notational convenience let i = |τ|. Then
A[i]
y ↾↾u = A[i]↾↾u
(by (12.9))
=

ω[i] −W [i]
r

↾↾u
(by (12.12))
=

ω[i] −Wr

↾↾u
=

ω[i] −Wr

y

↾↾u
(by (12.10));
thus,
A[i]
y ↾↾u =

ω[i] −Wr

y

↾↾u .
(12.13)
By (12.9), each
a ∈A[i] −By
such that
Restraintτ(y) < a ≤u
is forced into B by node τ during stage y (see lines 13–14 of Algorithm 12.1),
because y is a σ-stage and hence also a τ-stage. Therefore

∀a ∈A[i] 
Restraintτ(y) < a ≤u
=⇒a ∈By+1

and so, by (12.9) and (12.13),

∀a ∈ω[i] −Wr[y]

Restraintτ(y) < a ≤u
=⇒a ∈By+1

.
(12.14)

108
12
An Inﬁnitely Branching Tree (Thickness Lemma)
Let z > y be a σ-stage. Then, by condition (iii) on the choice of x, we have
Restraintτ(z) = Restraintτ(y)(= ˜Rτ).
(12.15)
Together, (12.11), (12.14), (12.15), condition (ii) on the choice of x, and By+1 ⊆Bz
imply

∀k ≤m

∀τ ⌢r ⪯σ

∀a ∈ω[|τ|] −Wr[z]

Restraintτ(z) < a ≤ϕB
e (k)[z] =⇒a ∈Bz

;
in other words,

∀k ≤m

the computation B
e (k)[z] is σ-believable

.
(12.16)
For each k ≤m,
Az(k) = A(k)
(by condition (i) on the choice of x)
= B
e (k)
(by 12.7)
= B
e (k)[z]
(by condition (ii) on the choice of x)
= B
σ (k)[z]
(by (12.16), and the deﬁnition of B
σ (k)[z]).
Therefore, by condition (iv) on the choice of x, and the deﬁnition of ℓσ(z), and
because σ ∈TPz,
ℓσ(z) > m ,
contradicting (12.8).
QED Lemma 12.3
Lemma 12.4 Pe is met, for all e.
Proof The proof of Lemma12.4 is word-for-word the same as that of Lemma11.4.
Thus, all the requirements of the theorem are met.
QED Theorem 12
12.5
What’s New in This Chapter?
1. An inﬁnitely branching priority tree. Can Theorem12 be proved (without too
much extra effort) by means of a binary priority tree?
2. We compute the length of disagreement between two functions, namely, A[e] and
W [e]
r . One could think of this as the length of agreement between ω[e] and the set
A[e] △W [e]
r .

12.7 Exercises
109
3. Each of our previous length of agreement functions was used to deﬁne a restraint
function; that is, it was used to decide which computations to preserve by restrain-
ing elements from entering a set B. This is likewise the case for the length of
agreement function ℓσ in this chapter.
The length of disagreement function ℓe,r in this chapter, on the other hand, is not
used to deﬁne a restraint function (A and Wr are given rather than constructed,
so we cannot restrain elements from entering them); rather, ℓe,r is used only to
construct TPs.
12.6
Afternotes
As we have said, in all arguments based on priority trees, TP must be an inﬁnite
path. For ﬁnitely branching trees (such as the binary tree in Chap.11), this property
of TP is obvious. For the inﬁnitely branching tree here in this chapter, it might not
be obvious but we prove it rather easily. For the inﬁnitely branching tree in Chap.14,
we prove it with a more intricate argument, because we do not see an easy one.
12.7
Exercises
1. Let A be c.e.n. and piecewise computable, and let B be a thick, c.e. subset of A
such that A ≰T B. Must B be inﬁnite?
2. The length of agreement (denoted ℓe,i(s) in Chap.9, and ℓσ(s) in Chaps.11, in
this chapter, and 13) is capped at s, in order to ensure that it is ﬁnite and can
be computed in a ﬁnite amount of time, as we have pointed out. On the other
hand, the length of disagreement, ℓe,r(s), deﬁned in this chapter, is not explicitly
capped. Must it always be ﬁnite? In other words, must, for each e, r, and s, the
set

n :

∀k < n

A(⟨e, k⟩)[s] ̸= Wr(⟨e, k⟩)[s]
 
have a maximum?
3. Prove Fact12.1.
4. Recall that τ ⌢r represents the guess that r is the least number such that (12.1)
is true, where e = |τ|. What if it represented the simpler guess that r is some
number such that (12.1) is true? Then TPs could be computed like this:

110
12
An Inﬁnitely Branching Tree (Thickness Lemma)
// Compute TPs .
τ ←λ.
for e ←0 to s −1
if

∃j ≤s

ℓe, j(s) > ℓe, j(s −1)

r ←some such j
else r ←s.
τ ←τ ⌢r.
TPs ←τ.
The only change we made here in Algorithm 12.1 is to replace “the least such j”
by “some such j.”
Would the constructed set still meet all of the requirements? Justify your answer.
5. Generalize the deﬁnition of the true path (speciﬁed in (11.1)) by deﬁning, for each
node σ,
TP(σ) =def {the leftmost descendant of σ on level e that is visited inﬁnitely often : e ∈ω}.
Thus, TP = TP(λ). For Algorithm 12.1, prove that if σ is visited inﬁnitely often,
then TP(σ) is inﬁnite.
6. Suppose that σ ⌢r ∈TP in Algorithm 12.1.
(a) Let j be such that W j ̸= Wr. Can σ ⌢j be visited inﬁnitely often?
(b) Let
J =

j : W j ̸= Wr

.
Can the set

s :

∃j ∈J

s is a (σ ⌢j)-stage

be inﬁnite?
7. Instead of the binary tree that we used in Chap.11, we could use the inﬁnitely
branching tree depicted in Fig.12.5. Here the branch labeled ∞represents the
guess that A[|σ|] is full; for all j, the branch labeled j represents the guess that
 A[|σ|] = j .
This tree holds more information about A than does the binary tree of Chap.11.
(a) Write pseudo-code for an algorithm, based on this tree, that constructs a set
B as required by Theorem11.

12.7 Exercises
111
Fig. 12.5 An alternative tree
for Chap.11
(b) Using your algorithm of part (a), let ξ be a node to the right of TP. Can there
be inﬁnitely many ξ-stages?
(c) Using your algorithm of part (a), is it true that
(∀e)(∃σ on level e)(∃x)(∀s > x)[ s is a σ-stage ]?

Chapter 13
Joint Custody (Minimal Pair Theorem)
In Chaps.11 and 12, the branches emanating from a tree node correspond to guesses
about the given set A. In this chapter and Chap.14, they correspond to guesses about
the construction. In other words, in Chaps.11 and 12, our tree algorithms guess about
what is; in this chapter and Chap.14, they guess about what will be.
In each of our previous length-of-agreement arguments, we showed that the length
of agreement had a maximum. For example, to prove the Sacks Splitting Theorem,
we proved that
max
s
ℓe,i(s) < ∞
to help show that requirement Re,i is met. Similarly, for both of the thickness lemmas,
we proved that
max
s
ℓσ(s) < ∞
for each σ ∈TP, to help show that Ne is met. In the current chapter, there might be
some nodes σ ∈TP such that
max
s
ℓσ(s) = ∞;
nevertheless, we will be able to meet our requirements.
This chapter contains a few other innovations, including the “joint custody”
technique.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_13
113

114
13
Joint Custody (Minimal Pair Theorem)
13.1
The Theorem
A minimal pair is a pair of c.e.n. sets A and B such that

∀c.e. C

(C ≤T A and C ≤T B) =⇒C is computable

.
Informally, c.e.n. sets A and B constitute a minimal pair if no c.e.n. set lies below
each of them.
Theorem 13 (Lachlan-Yates) There exists a minimal pair.
Proof We construct A and B in stages, so as to satisfy the following requirements
for each e:
Pe, A : We ̸= ¯A
Pe, B : We ̸= B
Ne : ¬Q(e) or (A
e is a total, computable function)
where the predicate Q(e) is true if and only if
A
e = B
e = a total function.
As usual, the P (or “positive”) requirements are so named because they try to insert
various elements into A and B. The N (or “negative”) requirements try to keep certain
elements out of A and B.
Lemma 13.1 These requirements cumulatively sufﬁce to prove the theorem.
Proof 1Assume that all of the requirements are met.
The Pe, A requirements cumulatively imply the non-computability of A. The Pe, B
requirements do likewise for B. The Pe, A and Pe, B requirements are just like the
Re,i requirements of Chap.4.
Let C be a c.e. set such that C ≤T A and C ≤T B. We will show that C is
computable.
First, we claim that A ̸= B. To see this, assume for a contradiction that A = B.
Let e′ be such that for all D and k,
D
e′ (k) =def

1, if k ∈D
0, otherwise.
Then for all D,
D
e′ = D.
1 This is an algorithms book. It is primarily about algorithms for constructing c.e. sets to meet
various collections of requirements. Hence, the reader may skip the proof of this lemma without
loss of continuity.

13.2 The Tree, and an Overview of Our Strategy
115
In particular,
A
e′ = A = B = B
e′
(13.1)
which is a total function; thus, Q(e′) = true. Therefore requirement Ne′, together
with (13.1), implies that A is computable, a contradiction. Thus, A ̸= B.
So, either A −B ̸= ∅or B −A ̸= ∅; without loss of generality, assume the latter.
Fix some b ∈B −A. Because C ≤T A there is an i such that
C = A
i ,
and because C ≤T B there is a j such that
C = B
j .
Thus,
A
i = B
j = C.
(13.2)
There exists e such that for all D and k,
D
e (k) =def

D
i (k), if b /∈D
D
j (k), otherwise
as the reader should verify. Because b ∈B −A, we have
A
e = A
i
and
B
e = B
j
and therefore, by (13.2), we have
A
e = B
e = C
which is a total function. Hence Q(e) = true, and so Ne implies that C is com-
putable.
Therefore A and B constitute a minimal pair.
□
13.2
The Tree, and an Overview of Our Strategy
We use a binary priority tree. Emanating from each node are branches labeled ∞
and f , respectively, exactly as in Fig.11.2. The interpretation of the guesses for this
chapter is given after the deﬁnitions section.

116
13
Joint Custody (Minimal Pair Theorem)
Each node σ on level e is trying to meet three requirements: Pe, A, Pe, B, and Ne .
To meet Pe, A , node σ chooses a fresh witness nσ, A for it. Then at least one of the
following events occurs:
(1) While visiting σ, we learn that nσ, A ∈We . Then we put nσ, A into A; thus,
nσ, A ∈A ∩We . In this case, nσ, A is called a positive witness for Pe, A .
(2) While visiting some other node ξ on level e of the tree, we put nξ, A into A as a
positive witness for Pe, A , thereby relieving σ of its responsibility in this regard.
(3) It turns out that nσ, A /∈We, and so we never put nσ, A into A; thus, nσ, A ∈
¯A ∩We . In this case, nσ, A is called a negative witness for Pe, A .
In each case, Pe, A is met. Our plan for meeting Pe, B is analogous, substituting B
for A.
To meet Ne, node σ uses a length-of-agreement argument.
The details come next.
13.3
Deﬁnitions
As in all of our tree-based algorithms, during each stage s we compute a path from
the root of the tree down to a level s node that we call TPs.
Let σ be a node on level e of the tree.
1. Let nσ,A denote the current witness for Pe, A that is attached to node σ. Think of
nσ,A as a variable; its value changes each time that σ is initialized. Likewise, nσ,B
denotes the current witness for Pe, B that is attached to σ.
2. The requirement Pe, A needs attention at stage s if
We[s] ∩As = ∅and nσ,A ∈We[s]
(recall that the notation We[y] is deﬁned in (4.1)). Likewise, Pe, B needs attention
at stage s if
We[s] ∩Bs = ∅and nσ,B ∈We[s].
3.
ℓσ(s) =def

max

n ≤s :

∀k < n

Ae (k)[s] ↓= Be (k)[s]
 
,
if σ ∈TPs
−1,
otherwise
where A
e (k)[s] ↓= B
e (k)[s] means that both are deﬁned, and that they are equal.
Thus, ℓσ(s) is the length of agreement between A
e (k)[s] and B
e (k)[s] (where
both being undeﬁned is regarded as a type of disagreement). As in Chaps.9, 11,
and 12, this length of agreement is capped at s to ensure that it is ﬁnite and that
each stage can be computed in a ﬁnite amount of time.
4. Lσ(s) =def max{ℓσ(r) : r ≤s}.

13.5 The Algorithm
117
Thus, Lσ(s) is the “high water mark” of ℓσ(s) so far. Note that Lσ(s) is monotonic
as a function of s, whereas ℓσ(s) might not be.
5. A stage s ≥1 is σ-expansionary if Lσ(s) > Lσ(s −1). See Exercise 1.
6. To initialize a node σ is to assign a fresh value to nσ,A and a fresh value to nσ,B.
As usual, by “fresh” here we mean larger than any witness or any ϕ value seen
so far in the construction.
Conspicuous by its absence here is some kind of restraint function, which helped
us avoid certain injuries in Chaps.9, 11, and 12. Rather, in this chapter we avoid
certain injuries by picking fresh witnesses, as we did in Chaps.7 and 8.
13.4
Interpretation of the Guesses
The ∞branch emanating from σ represents the guess that there are inﬁnitely many
σ-expansionary stages. The f branch represents the guess that there are only ﬁnitely
many such stages.
13.5
The Algorithm
Algorithm 13.1
1
A ←∅.
2
B ←∅.
3 for s ←0 to ∞
// Compute TPs .
4
TPs ←{λ}.
5
τ ←λ.
6
for e ←0 to s −1
7
if s is a τ-expansionary stage
8
τ ←τ ⌢∞
9
else τ ←τ ⌢f .
10
TPs ←TPs ∪{τ}.
11
Initialize each τ ∈TPs that has never been initialized.
12
Initialize each node to the right of TPs .
// Perhaps put a number into A or B.
13
if P|ξ|, A or P|ξ|, B needs attention for some ξ ∈TPs
14
ξ ←the shortest such node, that is, the one closest to the root.
// Node ξ acts.
15
if P|ξ|, A needs attention
16
A ←A ∪{nξ, A}
17
else B ←B ∪{nξ, B}.

118
13
Joint Custody (Minimal Pair Theorem)
Note that during a given stage, one element may be added to A, or one to B, or to
neither, but not to both. This observation is crucial for the “custody” argument in the
proof of Lemma13.3.
13.6
Veriﬁcation
The true path TP is deﬁned word-for-word as in (11.1).
Fact 13.1 Let σ be the level e node of TP. Then
Q(e) =⇒σ ⌢∞∈TP.
Proof Assume Q(e), that is,
A
e = B
e = a total function.
(13.3)
Assume for a contradiciton that σ ⌢f ∈TP. Then there is a stage z during or after
which σ ⌢∞is never visited. Let m = Lσ(z). Then, because Lσ is monotonic,
m = max
s
Lσ(s).
(13.4)
By (13.3) and the Permanence Lemma,

∀k

∃sk

the computations A
e (k)[sk] and B
e (k)[sk] are both permanent

.
Because σ is on the true path and hence is visited inﬁnitely often, there exists a
σ-stage t such that
t > max{sk : k ≤m}.
Therefore, by (13.3),

∀k ≤m

A
e (k)[t] ↓= A
e (k)[t]

and so
Lσ(t) ≥m + 1,
contradicting (13.4).
□
As in all of our priority arguments, we deﬁne a stage after which various ﬁnitely
occurring events no longer occur. In particular, for each σ ∈TP, let x(σ) denote a
stage during or after which

13.6 Veriﬁcation
119
(i) no ρ <L σ is visited,
(ii) no τ ⪯σ acts,
(iii) σ is not initialized.
Such an x(σ) exists because
(i) σ ∈TP.
(ii) No node acts more than twice: once to put an element into A, and once into
B (acting is deﬁned in a comment just before line 15 of Algorithm 13.1).
Furthermore, σ has only ﬁnitely many ancestors.
(iii) Node σ is initialized either by line 11 of Algorithm 13.1 (which happens only
once), or by line 12 when some ρ <L σ is visited (which happens only ﬁnitely
often, because σ ∈TP).
Lemma 13.2 Each P-requirement is met.
Proof Fix e. Let σ be the level e node of TP, and let x = x(σ). Consider requirement
Pe, A. By condition (iii) on the choice of x, node σ is never initialized during a
stage s ≥x. Therefore, nσ,A(x) is the ﬁnal value of the variable nσ,A; for notational
convenience, let n = nσ,A(x).
Case 1.
n ∈We.
Let y ≥x be a σ-stage such that
n ∈We[y].
Such a y exists because σ ∈TP. Requirement Pe, A does not need attention at stage
y, because otherwise node σ would act during stage y, contradicting condition
(ii) on the choice of x. Therefore
We[y] ∩Ay ̸= ∅
and so Pe, A is met.
Case 2.
n /∈We.
Then n is never put into A during a visit to σ. Nor can that happen during a visit
to some other node, because the witnesses chosen by procedure Initialize are all
distinct. Therefore n is a negative witness for Pe, A.
In either case, Pe, A is met. Analogous reasoning establishes that Pe, B is met. □
Lemma 13.3 Each N-requirement is met.
Proof Fix e; we will show that Ne is met. If Q(e) = false then the proof is imme-
diate; so assume that Q(e) = true, that is,
A
e = B
e = a total function.
(13.5)

120
13
Joint Custody (Minimal Pair Theorem)
Let σ be the level e node of TP. We will show that A
e is computable, thereby
implying that Ne is met.
Let
x = x(σ ⌢∞),
which is deﬁned because σ ⌢∞∈TP (by Fact13.1). The following algorithm, which
knows x, is given p as input. We will show that it outputs A
e (p).
Algorithm 13.2
1 Run Algorithm 13.1 until reaching the ﬁrst (σ ⌢∞)-stage t0 > x such that Lσ(t0) > p.
2 Output(Ae (p)[t0]).
Because σ ⌢∞is on the true path and hence is visited inﬁnitely often, and each of
these visits occurs during a σ-expansionary stage, the function Lσ has no maximum.
Therefore the stage t0 will indeed be found by Algorithm 13.2.
We will show that
A
e (p) = A
e (p)[t0]
by means of what we call the “custody” idea.
In particular, let
t0 < t1 < · · ·
denote all of the (σ ⌢∞)-stages greater than or equal to t0. Then
p < ℓσ(t0) < ℓσ(t1) < ℓσ(t2) < · · ·
and

∀i

A
e (p)[ti] = B
e (p)[ti]

.
(13.6)
For each z, if some a ≤ϕA
e (p)[z] enters A during stage z then we say that the
A-side is injured during stage z. Such an injury is caused by the action of a node ξ
in line 16 of Algorithm 13.1. Likewise, if some a ≤ϕB
e (p)[z] enters B during stage
z then we say that the B-side is injured during stage z. Such an injury is caused by
the action of a node ξ in line 17 of Algorithm 13.1.
Proposition 1 Let i ∈ω, and suppose that the A-side is not injured during stage ti.
Then for each z ∈(ti, ti+1), the A-side is not injured during stage z. Informally, this
implies that A maintains custody of the value A
e (p)[ti] (which, by (13.6), equals
B
e (p)[ti] ) from the start of stage ti to the start of stage ti+1.
Proof Assume for a contradiction that the A-side were injured during a stage in
(ti, ti+1); let z be the ﬁrst such stage. Then
ϕA
e (p)[ti] = ϕA
e (p)[z].
(13.7)

13.6 Veriﬁcation
121
Let ξ be the unique node that acts during stage z. Consider the possible relative
positions of nodes σ ⌢∞and ξ.
Case 1.
ξ <L σ ⌢∞.
This contradicts condition (i) on the choice of x, because z > ti ≥t0 > x =
x(σ ⌢∞).
Case 2.
σ ⌢∞<L ξ.
Then during stage ti, which is a (σ ⌢∞)-stage, the variable nξ,A receives a fresh
value (by line 12 of Algorithm 13.1) that is larger than any number seen in
the construction up to that point (in particular, larger than ϕA
e (p)[ti]). There-
fore, by (13.7), the action of ξ does not cause an A-side injury during stage z, a
contradiction.
Case 3.
ξ ⪯σ ⌢∞.
This contradicts condition (ii) on the choice of x, because z > ti ≥t0 > x =
x(σ ⌢∞).
Case 4.
σ ⌢∞≺ξ.
Node ξ acts during stage z, and so z is a ξ-stage and hence is also a (σ ⌢∞)-stage.
However, there is no (σ ⌢∞)-stage in (ti, ti+1), a contradiction.
□
Thus, each of the four cases leads to a contradiction.
Proposition 2 Let i ∈ω, and suppose that the B-side is not injured during stage ti.
Then for each z ∈(ti, ti+1), the B-side is not injured during stage z.
Proof Identical to the proof of Proposition1, substituting B for A.
□
Proof There is no stage during which both the A-side and the B-side are injured,
because at most one number enters A ∪B during each stage. Therefore, for each i,
either the A-side or the B-side is not injured during stage ti; a side that is not injured
maintains custody of the value A
e (p)[ti] (=B
e (p)[ti]) from the start of stage ti to
the start of stage ti+1, by Propositions1 and 2.2 Therefore, by (13.6) and a simple
induction, we have
A
e (p)[t0] = B
e (p)[t0] = A
e (p)[t1] = B
e (p)[t1] = A
e (p)[t2] = B
e (p)[t2] = · · ·
(13.8)
By (13.5) we have A
e (p) ↓; hence, by the Permanence Lemma, there is a stage y
such that the computation A
e (p)[y] is permanent. Let j be such that t j > y. Then
the computation A
e (p)[t j] is permanent, and so
A
e (p) = A
e (p)[t j].
2 It might be that neither the A-side nor the B-side is injured during stage ti. In that case, both sides
would maintain custody of the value A
e (p)[ti] from the start of stage ti to the start of stage ti+1.

122
13
Joint Custody (Minimal Pair Theorem)
Therefore, by (13.8), we have
A
e (p) = A
e (p)[t0].
Thus, Algorithm 13.2 correctly computes A
e (p). Hence A
e is computable, and so
requirement Ne is met.
QED Lemma 13.3
Proof The theorem follows from the three lemmas.
QED Theorem 13
13.7
What’s New in This Chapter?
I vacillated when naming this chapter. Most chapters in this book are named after
the most salient new algorithmic technique introduced therein. What’s the most
interesting new technique in this chapter?
Perhaps a better title would be “Guessing about the construction,” which we
mentioned in the ﬁrst paragraph of the chapter. Because we are guessing about what
will be rather than what is, this may be viewed is a more abstract or advanced tree
argument than those with which we proved the thickness lemmas. On the other
hand, one might view this as a more primitive tree argument than those, because
it uses no σ-speciﬁc computation function (in other words, it uses no concept of
σ-believability).
A second contender for the title was “Potentially inﬁnite length of agreement,” as
discussed in the second paragraph of the chapter.
A third possibility was “Preserving the result of a computation without necessarily
preserving its usage.” We did this preservation by using “joint custody,” which is a
snappier title.
13.8
Afternotes
1. The Minimal Pair Theorem was originally proved independently by Lachlan
[La66] and Yates [Ya66a]. A proof using trees appears in [DH]; our proof is
essentially based on that one.
2. The key technique in the proof of Lemma13.1 is called “Posner’s Remark.”
3. In [Mi] there is a proof of the minimal pair theorem that uses an inﬁnitely branch-
ing tree. Often in tree arguments, it’s a matter of style as to whether to put more
or less information in the tree.

13.9 Exercises
123
4. Our deﬁnition of minimal pairs is restricted to c.e. sets. However, in the literature,
minimal pairs are usually deﬁned for sets in general.
13.9
Exercises
1. Suppose that stage s is σ-expansionary. Prove that s is a σ-stage.
2. Prove that x(σ) ≥|σ|, for each σ ∈TP.
3. In Algorithm 13.1, to initialize a node σ on level e of the tree is to assign fresh
values to nσ,A and to nσ,B. Suppose that we change the initialization procedure,
so that it ﬁrst tests whether there already is an element in As ∩We[s] and if so, it
doesn’t bother to change nσ,A. Would the algorithm still work (that is, would all
of the requirements still be met)?
4. Suppose that the initialization routine for Algorithm 13.1 were altered so that
nσ,A = nτ,A
whenever |σ| = |τ|. Which argument in the veriﬁcation section would become
invalid?
5. Prove that

∀τ

max
s
nτ,A(s) = ∞⇐⇒TP <L τ

.
6. In Algorithm 13.2, suppose that we replace
Lσ(t0) > p
by
ℓσ(t0) > p.
Would the algorithm still correctly compute A
e (p)?
7. Suppose that σ is the level e node of TP. Fact13.1 says that
Q(e) =⇒σ ⌢∞∈TP.
Must the converse hold, that is, must it be that
σ ⌢∞∈TP =⇒Q(e)?
8. In the proof of Lemma13.3, we deﬁned x = x(σ ⌢∞). Suppose that instead we
deﬁne x = x(σ). Where would the proof fail?

124
13
Joint Custody (Minimal Pair Theorem)
9. In the proof of Lemma13.3, some aspects of the computation A
e (p)[ti+1] might
differ from those of the computation A
e (p)[ti]. For example, perhaps
ϕA
e (p)[ti+1] ̸= ϕA
e (p)[ti].
(13.9)
We have proved only that their output is the same, that is,
A
e (p)[ti+1] = A
e (p)[ti].
Prove that (13.9) is true for only ﬁnitely many values of i.

Chapter 14
Witness Lists (Density Theorem)
The algorithm in this chapter uses and extends many of the techniques that we have
seen so far, and introduces a few more.
Theorem 14 (Sacks Density Theorem) Let A and C be c.e., and assume that
A <T C. Then there exists a c.e. set B such that A <T B <T C.
Proof We prove a slightly stronger result, namely, that there exist incomparable c.e.
sets B0 and B1 such that
A <T B0 <T C
and
A <T B1 <T C.
(14.1)
To do this, we construct c.e. sets B0 and B1 such that
B0 ≰T B1 and
B1 ≰T B0
and
A ≤T B0 ≤T C
and
A ≤T B1 ≤T C. (14.2)
See Exercise 1.
Assume that we are given a standard enumeration a0, a1, . . . of A, and a standard
enumeration c0, c1, . . . of C. As usual, for each s we let
As =def {a0, a1, . . . , as}.
Weobtain B0 ≰T B1 and B1 ≰T B0 bymeetingthefollowingrequirements(famil-
iar to us from the Friedberg-Muchnik construction):
R2e : B1 ̸= B0
e
R2e+1 : B0 ̸= B1
e
for each e.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_14
125

126
14
Witness Lists (Density Theorem)
We obtain A ≤T B0 and A ≤T B1 by “coding,” that is, we “encode” A into B0
and into B1. In particular, during stage s, we simply put 2as into B0 and into B1 (see
Exercise 2). All other numbers in B0 ∪B1 will be odd; thus, all numbers that we
choose as witnesses to the R-requirements will be odd.
We obtain B0 ≤T C and B1 ≤T C by C-permitting: before an odd number can
enter B0 or B1, it needs permission by C. In this chapter, we cannot use the most
straightforward variety of permitting (that is, the one that we saw in Chap.8) because
for each a ∈A, we must put 2a into B0 and into B1, whether or not 2a receives
C-permission. Instead, our C-permitting uses “witness lists,” which are deﬁned in
Sect.14.2.
Thus, our overall strategy is three-pronged: we use the Friedberg-Muchnik tech-
nique to meet the R-requirements, combined with the coding of A into B0 and B1, and
the C-permitting. The coding of A can cause inﬁnite injury to the R-requirements;
we use a priority tree to cope with that.
14.1
The Tree
Each node σ of the tree has inﬁnitely many branches, labeled 0, 1, . . . , R, from
left to right, as is depicted in Fig.14.1. The branches emanating from σ represent
guesses; we will specify what each branch guesses in Sect.14.4.2 (“Interpretation of
the guesses”).
We write ρ <L σ if

∃τ, α, β

α and β are children of τ, and α ◁β, and α ⪯ρ, and β ⪯σ

,
as is illustrated in Fig.14.2. This is analogous to the deﬁnition in (12.2).
14.2
Deﬁnitions
Throughout this section, let σ be a node on level 2e of the tree (if σ were on level
2e + 1 then we would swap B0 with B1 in these deﬁnitions). As usual, whenever
we write a variable name with an attached s, we refer to its value at the start of
stage s.
Fig. 14.1 A node and its
branches

14.2 Deﬁnitions
127
Fig. 14.2 ρ <L σ
1. The witness list for σ is a vector nσ. For each i and s, either nσ(i)[s] is a num-
ber or it is undeﬁned. We write nσ(i)[s] ↓in the former case, and nσ(i)[s] ↑in
the latter. If nσ(i)[s] ↓then we call nσ(i)[s] the ith witness of σ at the start of
stage s.
Let
mσ(s) =def

max I, if I ̸= ∅
−1,
otherwise
where
I = {i : nσ(i)[s] ↓}.
A witness nσ(i) is realized if i < mσ. If i = mσ then nσ(i) is unrealized. The
variables nσ(i) and mσ can change during a stage (not just at the start of a stage).
Likewise, a witness can become realized, unrealized, or undeﬁned during a stage.
At all times σ has exactly mσ realized witnesses, namely,
nσ(0), nσ(1), . . . , nσ(mσ −1),
and exactly one unrealized witness, namely, nσ(mσ). The distinction between the
adjectives “unrealized” and “undeﬁned,” as applied to witnesses, is important. The
situation is illustrated in Fig.14.3 (much information is contained in this ﬁgure;
the reader should keep looking at it while studying the rest of this chapter).

128
14
Witness Lists (Density Theorem)
i
0
1
2
3
4
5
6
7
8
· · ·
nσ(i)[s]
5
17
41
101
365
601
undeﬁned
realized?
yes
yes
yes
yes
yes
no
not applicable
ΦB0
e (nσ(i))[s]
0
0
0
0
0
anything
not applicable
Fig. 14.3 The witness list for σ at the start of stage s. Here mσ (s) = 5
Note in Fig.14.3 that
nσ(0) < nσ(1) < · · · < nσ(mσ);
this is always true, because witnesses are chosen as fresh numbers. Also, for each
i, if nσ(i) is deﬁned then it is odd.
2. A witness b = nσ(i)[s] is a star witness at stage s if it is realized (that is, i <
mσ(s)) and b ∈B1[s].
3. A star witness b = nσ(i)[s] at stage s is permanent if the computation B0
e (b)[s]
is permanent.
If σ has a permanent star witness b then
B1(b) = 1 ̸= 0 = B0
e (b),
and so R2e is met (see Exercise 3).
Furthermore, as will be seen in line 12 of the main code of the algorithm, if σ has
a permanent star witness, then mσ has an upper bound. This will be crucial in the
proof of Fact 5.
4. To cancel σ is to undeﬁne nτ(i) for each τ such that σ ⪯τ and each i ≤mτ, and
then to set mτ to −1. In other words, to cancel a node is to discard all witnesses
of that node and of its descendants.
5. For each i and s,
inserted(σ, i)[s] =def

true,
if (∃z ≤s)

nσ(i)[z] ∈B1(s)

false, otherwise.

14.2 Deﬁnitions
129
Thus, the Boolean variable inserted(σ, i) initially equals false, and stays that
way until we reach a stage t during which nσ(i)[t] is inserted into B1; it then
equals true at the start of stage t + 1 and forever after. If no such t exists then
inserted(σ, i) always equals false.
6. For each i and s,
last Realized(σ, i)[s] =def

max Z, if i < mσ(s)
∞,
otherwise
where
Z =

z < s : nσ(i) became realized during stage z

.
Inotherwords,ifnσ(i)isrealizedatthestartofstages,thenlast Realized(σ, i)[s]
is the last stage before s during which nσ(i) became realized. Thus, the following
three statements are equivalent:
(a) last Realized(σ, i)[s] < ∞,
(b) i < mσ(s),
(c) nσ(i) is realized at the start of stage s.
7. For i < mσ(s),
ϕ(σ, i)[s] =def ϕB0
e

nσ(i)

[r],
wherer = last Realized(σ, i)[s]. Thus, during stage s, only the entrance of some
b ≤ϕ(σ, i)[s]into B0 canspoilthecomputationassociatedwitharealizedwitness
nσ(i)[s].
Note that inserted(σ, i), last Realized(σ, i), and ϕ(σ, i) are not variables of the
algorithm (that is, the algorithm does not explicitly assign values to them); rather,
they are functions deﬁned in terms of such variables.
8. For each i,
entryIntoC(i) =def

s such that i = cs, if i ∈C
−1,
otherwise.
Note that entryIntoC(i) depends neither on the current stage nor on any other
variable of the algorithm. Rather, it depends only on the standard enumeration of
C that we are given.
9. A number is fresh if it is odd and larger than any witness or usage seen during
the execution of the algorithm thus far. This is just like our original deﬁnition of
“fresh” from Chap. 7, except that it now requires oddness.

130
14
Witness Lists (Density Theorem)
14.3
The Algorithm
For each σ, all witnesses of σ are initially undeﬁned; in other words, mσ(0) = −1.
Each stage executes a switch statement, which is a control structure that we have
not yet used in our pseudo-code. Our switch statement (along with its associated
keywords case, break, and default) works as follows: If the condition for Case 0,
1, 2, or 3 applies, then we perform the action associated with the lowest-numbered
one that does. If none of them apply, then we perform the default action. Thus,
we perform exactly one of the ﬁve actions.1 A break statement causes the ﬂow of
control to exit the switch block. Thus, in the main code of Algorithm 14.1, whenever
a break statement is executed, line 14 is executed next.
Thus, we could replace the switch statement in the main code by the following,
equivalent code (assuming that we have ﬁlled in the conditions for Cases 0, 1, 2,
and 3):
if the condition for Case 0 applies
initialization
else if the condition for Case 1 applies
unrealization
else if the condition for Case 2 applies
insertion into B1
else if the condition for Case 3 applies
realization
else σ ←σ ⌢mσ.
It’s a matter of taste.2
We say that initialization has the highest precedence of the ﬁve actions, followed
by unrealization, insertion, realization, and the default.
1 Our switch statement is similar to the switch statement in the programming language Java.
2 My students voted unanimously for the switch statement here.

14.3 The Algorithm
131
14.3.1
Main Code
Algorithm 14.1
1
B0 ←∅.
2
B1 ←∅.
3 for s ←0 to ∞
4
T Ps ←{λ}.
5
σ ←λ.
6
for ℓ←0 to s −1
// Node σ, which is on level ℓof TPs, acts. Also, the child of σ in T Ps is determined.
7
e ←⌊ℓ/2⌋.
8
switch
9
Case 0: mσ = −1.
initialization.
break.
10
Case 1: there is an i < mσ (s) such that some b ≤ϕ(σ, i)[s]
entered B0 during a stage in

last Realized(σ, i)[s], s

.
unrealization.
break.
11
Case 2: there is an i such that inserted(σ, i) = false
and last Realized(σ, i)[s] < entryIntoC(i) ≤s.
// Thus, nσ (i) became C-permitted after it was last realized.
insertion into B1 .
break.
12
Case 3:
B0
e (nσ (mσ ))[s] = 0
and σ had no star witness at the start of stage s.
realization.
break.
13
Default:
σ ←σ ⌢mσ .
14
T Ps ←TPs ∪{σ}.
15
Cancel each node to the right of T Ps .
16
B0 ←B0 ∪{2as}.
// This encodes as into B0.
17
B1 ←B1 ∪{2as}.
// This encodes as into B1.
14.3.2
Subroutines
Here are the four subroutines (or “methods,” as they are known in Java) called by the
main code of Algorithm 14.1. We assume that the subroutines have both read and
write access to all of the variables of the main routine.
initialization
1 nσ(0) ←a fresh number.
2 mσ ←0.
3 σ ←σ ⌢0.

132
14
Witness Lists (Density Theorem)
unrealization
1 Fix the least such i.
//
We say that b unrealizes nσ (i) during stage s, where b was the ﬁrst
//
number ≤ϕ(σ, i)[s] that entered B0 during a stage in

last Realized(σ, i)[s], s

.
2 if nσ (i) ∈B1
3
nσ (i) ←a fresh number.
4 Undeﬁne witness nσ ( j) for each j such that i < j ≤mσ .
5 mσ ←i.
// nσ (i) is now unrealized.
6 σ ←σ ⌢i.
insertion into B1
1 Fix the least such i.
2
B1 ←B1 ∪{nσ(i)}.
3 Cancel node σ ⌢i.
4 σ ←σ ⌢i.
realization
1 i ←mσ.
2 nσ(i + 1) ←a fresh number.
3 mσ ←i + 1.
// nσ(i) is now realized.
4 Cancel node σ ⌢R.
// The R branch leads to an inert node (see Note 5 below).
5 σ ←σ ⌢R.
14.3.3
Notes on the Algorithm
1. During the body of the for loop of line 6 of the main code, we visit node σ, which
is on level ℓof the tree (in other words, |σ| = ℓ). We assume here that ℓis even
(that is, ℓ= 2e); the case in which ℓis odd (that is, ℓ= 2e + 1) is analogous,
swapping B0 with B1.
2. Regarding the condition for Case 0, note that
mσ = −1 ⇐⇒nσ(0) ↑
⇐⇒either this is the ﬁrst visit to σ,
or this is the ﬁrst visit to σ since it was last canceled.
3. How can we determine, in a ﬁnite amount of time, the truth of the condition for
Case 2?

14.3 The Algorithm
133
Note that
last Realized(σ, i)[s] < entryIntoC(i)
implies i < mσ(s), because
i ≥mσ(s) =⇒last Realized(σ, i)[s] = ∞.
Therefore, we need examine only ﬁnitely many values of i.
However, the function entryIntoC(i) is not computable (see Exercise 8). So, for
a ﬁxed i, how can we determine the truth of
last Realized(σ, i)[s] < entryIntoC(i) ≤s
in a ﬁnite amount of time?
Here’s how:
if i /∈Cs
output(false)
// because either i /∈C (and so last Realized(σ, i)[s] > −1 = entryIntoC(i)),
// or i ∈C −Cs (and so entryIntoC(i) > s).
else t ←the number such that i = ct . // Thus, t ≤s and t = entryIntoC(i).
if last Realized(σ, i)[s] < t
output(true)
else output(false).
Because Cs is ﬁnite, this can be executed in a ﬁnite amount of time.
4. Whenever a node σ ⌢R is visited, it gets initialized, because it was just canceled
by line 4 of the realization subroutine. Thus, node σ ⌢R is inert, that is, it can never
perform an insertion. This will not hurt the overall proof, because σ ⌢R /∈TP, by
Fact6. Likewise, each descendant of σ ⌢R is inert.
5. Any increase in mσ is caused by a realization, and hence is an increase of exactly
one.
6. The purpose of line 4 of the unrealization subroutine is to preserve the situation
depicted in Fig.14.3.
7. There are three wrinkles3 in Algorithm 14.1 that probably seem mysterious at this
point:
(a) line 12 of the main code: the clause involving a star witness in the condition
for Case 3 (realization).
(b) lines 2 and 3 of the unrealization subroutine: the possible assignment of a
fresh number to nσ(i).
(c) lines 3–4 of the insertion subroutine: the cancellation of σ ⌢i during the
insertion of nσ(i), followed by a visit to σ ⌢i (rather than, say, a visit to
σ ⌢mσ).
3 See the remarks about wrinkles in the Afternotes section.

134
14
Witness Lists (Density Theorem)
We use (a) in the proof of Fact 5. Furthermore, (a) has no harmful side effects,
because of an important principle employed during the design of Algorithm 14.1:
It doesn’t hurt to delay realization.4
In other words, it’s O.K. for us to slow down, but not stop the growth of mσ (mσ
might stop growing on its own, but we must not force it to do so).
This principle led us to give both unrealization and insertion precedence over
realization in the switch statement.
We use (b) in the proof of Lemma 1.
We use (c) in the proofs of Fact 3, Lemmas 1, and 2.
8. A witness nσ(i) becomes realized when mσ increases to i + 1. One might (mis-
takenly) think that it becomes realized at the moment when B0
e (nσ(i)) becomes
equal to 0. Likewise, nσ(i) becomes unrealized when mσ decreases to i.
9. Algorithm 14.1, and the proofs of the veriﬁcation section, are so laden with details
that it behooves us to step back and and look at the big picture: there’s a trade-off
going on here. The more cancellation done by the algorithm, the easier it is to
prove that odd numbers do not cause unrealization (Fact 3). On the other hand,
cancellation can only make it harder to show that certain witnesses n hang around
forever and result in
B1(n) = 0 ̸= B0
e (n)
(which occurs in a case of the proof of Lemma 1).
14.4
Veriﬁcation
As usual, the purpose of the veriﬁcation section is to show that the constructed sets
meet the requirements.
Sets B0 and B1 are built in stages, each of which can be executed in a ﬁnite amount
of time; hence B0 and B1 are c.e. By coding, we obtain A ≤T B0 and A ≤T B1.
Lemma 1 says that the R-requirements are met, and so B0 ≰T B1 and B1 ≰T B0.
Lemma 2 says that B0 ≤T C and B1 ≤T C, thereby completing the proof of (14.2)
and hence of Theorem14.
Section14.4.3 contains facts that are useful for proving the two lemmas.
Section14.4.4 contains those lemmas.
Figure14.4 depicts a directed graph,5 whose vertices correspond to the results
in this chapter; there is an edge from vertex v to vertex w if we use the result
corresponding to v when proving the result corresponding to w.
4 See Chap.15 for more about delaying tactics.
5 This graph is acyclic, mercifully.

14.4 Veriﬁcation
135
Fig. 14.4 Dependencies among the results in Chap.14
14.4.1
More Deﬁnitions
As in all of our tree arguments, the true path is deﬁned word-for-word as in (11.1).
We say that a node σ is initialized during stage s if the initialization action occurs
while σ is being visited during stage s. We say that σ performs an unrealization (or
insertion or realization) during stage s if that action occurs while σ is being visited
during stage s. Likewise, we say that σ takes the default during stage s if the default
action occurs while σ is being visited during stage s.
A witness nσ(i) is ready for insertion at a σ-stage s if
inserted(σ, i)[s] = false
and
last Realized(σ, i)[s] < entryIntoC(i) ≤s.
Even if nσ(i) is ready for insertion at a σ-stage s, it might not be inserted during
stage s, either because σ is canceled during stage s before being visited, or because
σ performs an unrealization during stage s (because unrealization takes precedence
over insertion), or because nσ( j) is inserted during stage s for some j < i (note the
word “least” in line 1 of the insertion subroutine).

136
14
Witness Lists (Density Theorem)
14.4.2
Interpretation of the Guesses
Now that we have Algorithm 14.1 and some associated terminology, we can explain
the intuition behind the tree (take another look at Fig.14.1).
As usual with tree arguments, each branch emanating from a node represents a
guess. In particular, for each k, the branch labeled k represents the guess that
k = lim inf
s
mσ(s),
which is equivalent to
k = min{i : mσ(s) = i for inﬁnitely many s }
(as is pointed out in (9.16)). Fact 5 says that
lim inf
s
mσ(s) < ∞
(although
max
s
mσ(s) = ∞
is possible, even if σ ∈TP).
So, what guess does the righmost branch (that is, the branch labeled R) represent?
Theansweris“notapplicable,”becauseσ ⌢R ∈TPisimpossible,whichfollowsfrom
Fact 6. The label “R” is for “realization,” because a node σ takes its R branch only
when it realizes a witness (see line 5 of the realization subroutine). The purpose of the
R branch is to allow us to rule out realization when considering the possible actions
taken at a certain node during certains stages; for example, we do this in Cases 3.3
and 4.2 of the proof of Fact 3, and in the proof of Fact 6.
As usual, the true path consists entirely of correct guesses; this follows from Fact
6.
14.4.3
Facts
Fact 14.1 For each σ and i, there is at most one stage during which nσ(i) is inserted.
Proof Suppose that nσ(i) is inserted during a stage s, and let t > s. Then
inserted(σ, i)[t] = true
and so nσ(i) is not ready for insertion at stage t.
QED Fact 1

14.4 Veriﬁcation
137
Fact 14.2 For each σ and i, nσ(i) gets a fresh number by line 3 of the unrealization
subroutine at most once.
Proof Suppose that nσ(i) gets a fresh number by line 3 of the unrealization subrou-
tine during a stage s. Then nσ(i)[s] ∈B1[s] (assuming that |σ| is even; if |σ| were
odd then we would substitute B0 for B1). Let t > s. If nσ(i) gets a fresh number by
line 3 of the unrealization subroutine during stage t, then
nσ(i)[s] ̸= nσ(i)[t] ∈B1[t]
and so nσ(i) is inserted both before and after stage s, contradicting Fact 1.
QED Fact 2
Fact 14.3 Let b be a number that unrealizes a witness. Then b is even.
Recall that the deﬁnition of a number unrealizing a witness is given in the comment
after line 1 of the unrealization subroutine. Even numbers enter B0 and B1 by coding,
whereas odd numbers enter them by the insertion subroutine; therefore, another way
to phrase this result is “If b unrealizes a witness then b/2 ∈A.”
Proof Let σ, i, and u be such that b unrealizes nσ(i) during stage u. Assume that
|σ| is even (otherwise we would swap B0 with B1 in this proof); let e be such that
|σ| = 2e. Thus, σ might insert some numbers into B1, and might try to restrain some
numbers from entering B0.
Let
r = last Realized(σ, i)[u].
Assume for a contradiction that b were odd. Then there exist ξ, j, and t such that
b = nξ( j)[t]
was inserted into B0 during stage t. Thus,
b ≤ϕ(σ, i)[u] = ϕB0
e (nσ(i))[r]
(14.3)
and6
r ≤t < u.
(14.4)
We know t ̸= u because

last Realized(σ, i)[s], s

rather than

last Realized
(σ, i)[s], s

is written in line 10 of the main code.
Consider ﬁve cases, based on the relative positions of nodes ξ and σ; in each case
we will derive a contradiction.
6 I tried to pick the names of these stages mnemonically: r is for r(ealization), t for (inser)t(ion),
and u for u(nrealization). Also, I wanted their alphabetical order to match their numerical order, to
help keep (14.4) in mind.

138
14
Witness Lists (Density Theorem)
Fig. 14.5 Relative sizes of j and k, in Case 3
Case 1.
ξ <L σ.
Then σ was canceled during stage t by line 15 of the main code (because t is a
ξ-stage), and so r > t, contradicting (14.4).
Case 2.
σ <L ξ.
Then ξ was canceled during stage r by line 15 of the main code (because r is a
σ-stage). Therefore nξ( j) was assigned the value b during a stage in (r, t) (this
could not have happened during stage r, because r is not a ξ-stage). Because b
was chosen to be fresh,
b > ϕB0
e (nσ(i))[r],
contradicting (14.3).
Case 3.
ξ ≺σ.
It cannot be that ξ ⌢R ⪯σ, because otherwise σ would be canceled during every
σ-stage (by line 4 of the realization subroutine), and so nσ(i) would never be
realized. Hence, ξ ⌢k ⪯σ for some k.
Note that t is a (ξ ⌢j)-stage, by line 4 of the insertion subroutine.
Case 3.1
j < k (as in Fig.14.5a).
Then σ was canceled during stage t by line 15 of the main code. Therefore
r > t, contradicting (14.4).
Case 3.2
j = k (as in Fig.14.5b).
Then σ was canceled during stage t by line 3 of the insertion subroutine (as
a reminder, whenever a node is canceled, so are all of its descendant nodes).
Therefore r > t, contradicting (14.4).
Case 3.3
j > k (as in Fig.14.5c).
Consider the action performed by ξ during stage r (which is a ξ-stage because
it is a σ-stage). It was not initialization, because otherwise σ would not have
performed a realization during stage r. It was neither unrealization nor the
default action, because otherwise mξ(r + 1) = k < j and so nξ( j)[r + 1] ↑,
and hence during a stage in (r, t), nξ( j) would have received the fresh number

14.4 Veriﬁcation
139
Fig. 14.6 Relative sizes of i and k, in Case 4
b > ϕB0
e (nσ(i))[r],
contradicting (14.3). Nor was it insertion, because otherwise ξ ⌢k (and hence
σ) would have been canceled (by line 3 of the insertion subroutine) during
stage r, but then σ would not have performed a realization during stage r. Nor
was it realization, because otherwise r would have been a (ξ ⌢R)-stage rather
than a (ξ ⌢k)-stage. Thus, we have a contradiction.
So, in each of the three subcases of Case 3, we have a contradiction.
Case 4
σ ≺ξ.
It cannot be that σ ⌢R ⪯ξ, because otherwise ξ would be canceled during every
ξ-stage (by line 4 of the realization subroutine), and so nξ( j) would never be
inserted (or even realized). Hence, σ ⌢k ⪯ξ for some k.
Case 4.1
i < k (as in Fig.14.6a).
By (14.3), nξ( j) was assigned the fresh value b during some stage r0 < r
(r0 = r is impossible because r is a σ ⌢R-stage and hence is not a ξ-stage).
Therefore r0 is a ξ-stage and hence
mσ(r0) ≥k .
Furthermore,
mσ(r) = i < k ,
because nσ(i) became realized during stage r. Let r1 be the largest number in
[r0, r) such that
mσ(r1) ≥k and mσ(r1 + 1) < k .
(14.5)
See Fig.14.7, in which k1 < i (although k1 ≥i is possible, too). The existence
of such an r1 follows from what could be regarded as a discrete version of
the mean-value theorem. The variable mσ decreased to some k1 < k during
stage r1. Therefore during stage r1, either node σ was initialized (in which

140
14
Witness Lists (Density Theorem)
Fig. 14.7 Stages r0 and r1 in Case 4.1
case k1 = 0), or witness nσ(k1) became unrealized. Hence σ ⌢k1 ∈TPr1 and
so node σ ⌢k (and therefore ξ) was canceled during stage r1 by line 15 of the
main code. After stage r1, node ξ was not visited until after stage r, because
r1 is deﬁned as the largest number in [r0, r) satisfying (14.5); this contradicts
(14.3).
Case 4.2
i ≥k (as in Fig.14.6b or c).
Because t is a ξ-stage, it is also a (σ ⌢k)-stage. Consider the action performed
by σ during stage t. It was neither initialization nor the unrealization of nσ(k)
nor the default action, because otherwise
mσ(t + 1) = k ≤i
and so nσ(i) would have been either unrealized or undeﬁned at the end of
stage t + 1, hence we would have r > t, contradicting (14.4). Nor was it the
insertion of nσ(k), because that would have canceled ξ by line 3 of the insertion
subroutine, and so ξ could not have performed an insertion during stage t. Nor
was it realization, because otherwise t would have been a (σ ⌢R)-stage rather
than a (σ ⌢k)-stage. Thus, none of the ﬁve actions could have been performed
by σ during stage t.
So, in both of the subcases of Case 4, we have a contradiction.
Case 5.
σ = ξ.
This is impossible, because ξ is on an odd level of the tree (because it inserted a
number into B0), whereas σ is on an even level.
Thus, in each of the ﬁve cases we have a contradiction. Therefore, b is even.
QED Fact 3

14.4 Veriﬁcation
141
Fact 14.4 Suppose that σ is visited inﬁnitely often, that b = nσ(k)[s] is ready for
insertion at a σ-stage s, and that
(∀z ≥s)[ k < mσ(z) ].
(14.6)
Then during some stage t ≥s, b is inserted (into B1 if |σ| is even; otherwise into
B0) and is a permanent star witness at stage t + 1.
Proof It must be that
inserted(σ, i)[s] = false
because nσ(k) is ready for insertion at stage s. Assume for a contradiction that nσ(k)
is never inserted during or after stage s. Then, during each subsequent σ-stage,
witness nσ(k) remains ready for insertion, because (by (14.6))

∀t > s

last Realized(σ, k)[t] = last Realized(σ, k)[s]

.
So, what prevents nσ(k) from being inserted during a σ-stage greater than or equal
to s? Only three actions of σ take precedence over the insertion of nσ(k):
(a) the initialization of σ,
(b) the unrealization of a witness of σ,
(c) the insertion of nσ(i) for some i < k.
Action (a) does not occur during or after stage s, by (14.6). Action (c) occurs at
most k times, by Fact 1. Hence, there is a stage x ≥s during or after which action
(b) occurs during each σ-stage. However, σ cannot perform an inﬁnite sequence of
unrealizations while performing no realizations (over which insertion takes prece-
dence), because mσ cannot decrease inﬁnitely often while never increasing. Hence,
because there are inﬁnitely many σ-stages, there exists a σ-stage y ≥x during which
neither (a) nor (b) nor (c) occurs. Witness nσ(k) is inserted during stage y. This is a
contradiction.
Thus, b = nσ(k)[s] is inserted during a stage t ≥s. No number b0 ≤ϕB0
e (k)[t] is
inserted into B0 during or after stage t, by (14.6). In other words,
B0[t]↾↾ϕB0
e (k)[t] = B0 ↾↾ϕB0
e (k)[t]
and so b is a permanent star witness, starting at stage t + 1.
QED Fact 4
It would be convenient if, for each σ ∈TP, the function mσ had a ﬁnite maximum.
Unfortunately, that might not be true. However, Fact 5 says that mσ does have a ﬁnite
liminf, and that will sufﬁce for our purposes. Actually, Fact 5 is a tad stronger than
we need, because it holds for each σ, regardless of whether σ is on the true path.7
7 We don’t beneﬁt from this extra strength, because Fact 5 is used only in the proof of Fact 6, which
concerns only nodes in TP.

142
14
Witness Lists (Density Theorem)
Fact 14.5 For each σ,
lim inf
s
mσ(s) < ∞.
Proof Fix σ. Assume for a contradiction that
lim inf
s
mσ(s) = ∞.
(14.7)
Then σ is visited inﬁnitely often, because mσ can increase only when σ is visited.
We will show C ≤T A by describing an algorithm that, using an oracle for A, decides
membership in C.
Let x be such that

∀s ≥x

mσ(s) ≥1

(such an x exists by (14.7)). Node σ is never initialized during or after stage x.
The following algorithm (which knows σ and x) is given p as input, and deter-
mines whether p ∈C; it uses an oracle for A.
Algorithm 14.2
1
y ←some number such that
2
y > x ,
3
p < mσ(y),
and
4
¬
	
∃s ≥y


2as ≤max

ϕ(σ, i)[y] : i ≤p
 
.
5 if p ∈Cy
6
output(“p is in C”)
7 else output(“p is not in C”).
In line 4, we multiply as by 2 because members of A are doubled when inserted
by lines 16 and 17 of the main code of Algorithm 14.1. Informally, line 4 says that
during or after stage y, no even number gets inserted and subsequently unrealizes
nσ(i)[y] for some i ≤p.
The claim C ≤T A follows from the following propositions.
Proposition 1 Using an oracle for A we can determine, in a ﬁnite amount of time,
whether a given y satisﬁes lines 2 through 4 of Algorithm 14.2.
Proof It is trivial to determine the truth of lines 2 and 3. To determine the truth of
line 4, we do the following:

14.4 Veriﬁcation
143
a ←0.
answer ←true.
while answer = true and 2a ≤max

ϕ(σ, i)[y] : i ≤p

if a ∈A −Ay−1
// Here is where we use the oracle for A.
answer ←false.
a ←a + 1.
output(answer).
The while loop iterates no more than
1 + max

ϕ(σ, i)[y] : i ≤p

/2
times.
□
Proposition 2 There exists y that satisﬁes lines 2 through 4 of Algorithm 14.2.
Proof By (14.7), there exists y > x such that

∀s ≥y

p < mσ(s)

.
(14.8)
Thus, y satisﬁes lines 2 and 3 of Algorithm 14.2.
We claim that y satisﬁes line 4 of Algorithm 14.2; assume the contrary. Then
	
∃s ≥y


2as ≤max

ϕ(σ, i)[y] : i ≤p
 
,
which is equivalent to

∃i ≤p

∃s ≥y

2as ≤ϕ(σ, i)[y]

;
let i′ be the least such i, and let z ≥y be such that
2az ≤ϕ(σ, i′)[y].
(14.9)
Then
ϕ(σ, i′)[z] = ϕ(σ, i′)[y],
(14.10)
because otherwise
last Realized(σ, i′)[z] ̸= last Realized(σ, i′)[y],
which would imply that nσ(i′) became unrealized during a stage in [y, z), which
would contradict (14.8) because i′ ≤p.
During stage z, the number 2az is inserted by lines 16 and 17 of Algorithm 14.1.
By (14.9) and (14.10),
2az ≤ϕ(σ, i′)[z].

144
14
Witness Lists (Density Theorem)
Therefore, during the ﬁrst σ-stage z′ > z, the condition for unrealization (in Case 1
of the switch statement) is satisﬁed. Hence, during stage z′, witness nσ(i′) becomes
unrealized, because
(i) σ is never initialized after stage x,
(ii) unrealization takes precedence over every other action except for initialization,
(iii) i′ was deﬁned as the least such i.
Therefore
mσ(z′ + 1) = i′ ≤p,
contradicting (14.8).
Thus, y satisﬁes line 4 of Algorithm 14.2.
□
Let y be the number computed by Algorithm 14.2 (it exists by Proposition 2).
Proposition 3 The output of Algorithm 14.2 is correct, that is,
p ∈C ⇐⇒p ∈Cy .
Proof Assume the contrary; then p ∈C −Cy. In other words,
y < entryIntoC(p).
(14.11)
Let z ≥y. During stage z:
(i) σ is not initialized (by line 2 of Algorithm 14.2, because σ is never initialized
after stage x),
(ii) for each i ≤p, no even number unrealizes nσ(i) (by line 4 of Algorithm 14.2),
(iii) for each i ≤p, no odd number unrealizes nσ(i) (by Fact 3).
Therefore, by line 3 of Algorithm 14.2,

∀z ≥y

p < mσ(z)

.
(14.12)
Witness nσ(p) does not receive C-permission to be inserted before stage
entryIntoC(p). Let s be the ﬁrst σ-stage such that s ≥entryIntoC(p) (such an s
exists because σ is visited inﬁnitely often, as is pointed out near the start of the proof
of Fact 5). Then
inserted(σ, p)[s] = false.
By (14.11),
y < entryIntoC(p) ≤s.

14.4 Veriﬁcation
145
Hence
last Realized(σ, p)[s] < y
(by (14.12), because s ≥y )
< entryIntoC(p)
≤s .
Thus, nσ(p) is ready for insertion at stage s. Therefore, by (14.12) and Fact 4, nσ(p)
eventually becomes a permanent star witness. Hence σ performs only ﬁnitely many
realizations, by the condition for realization in Case 3 of the switch statement, and
so
lim inf
s
mσ(s) < ∞,
contradicting (14.7).
Therefore the output of Algorithm 14.2 is correct.
□
Propositions1, 2, and 3 together imply that Algorithm 14.2 correctly determines
whether p ∈C, in a ﬁnite amount of time, using an oracle for A. Therefore C ≤T A,
contradicting the assumption A <T C of Theorem14. Thus, the assumption (14.7)
has led to a contradiction.
QED Fact 5
Fact 14.6 Let σ ∈TP, and let
k = lim inf
s
mσ(s)
(14.13)
(the existence of this liminf follows from Fact 5). Then σ ⌢k ∈TP.
Proof If σ is initialized inﬁnitely often8 then k = 0 and σ ⌢k ∈TP, and so this proof
would be ﬁnished. So assume that σ is initialized only ﬁnitely often. Let x be such
that
(i) σ is not initialized during or after stage x,
(ii)

∀z ≥x

mσ(z) ≥k

,
(iii)

∀i ≤k

nσ(i) is not inserted during or after stage x

.
Such an x exists by
(i) our assumption that σ is initialized only ﬁnitely often,
(ii) Equation (14.13),
(iii) Fact 1, applied to each i ≤k.
Let i < k. Assume for a contradiction that some y > x is a (σ ⌢i)-stage. Consider
the action performed by σ during stage y. It was not initialization, by condition (i)
8 This is impossible, by Fact 7. However, we cannot use Fact 7 here, because our proof of Fact 7
uses Fact 6.

146
14
Witness Lists (Density Theorem)
on the choice of x. Nor was it unrealization, because otherwise mσ(y + 1) = i < k,
contradicting condition (ii) on the choice of x. Nor was it insertion, by condition
(iii) on the choice of x. Nor was it realization, because otherwise y would be a
(σ ⌢R)-stage. Nor was it the default action, because otherwise mσ(y + 1) = i < k,
contradicting condition (ii) on the choice of x. Therefore, no (σ ⌢i)-stage is greater
than x, and so there are only ﬁnitely many (σ ⌢i)-stages. Hence, because σ ∈TP,
(∀ρ ◁σ ⌢k)[ρ is visited only ﬁnitely often].
To prove that σ ⌢k ∈TP, it remains only to show that there are inﬁnitely many
(σ ⌢k)-stages. Assume for a contradiction that there is a stage t larger than each
(σ ⌢k)-stage. Let
z > max{x, t}
be a σ-stage such that
mσ(z) = k.
Such a z exists because, by (14.13), there are inﬁnitely many values of s such that
mσ(s) = k, and the variable mσ can change only when σ is visited or canceled.
Consider the action performed by σ during stage z. It was not initialization, by
condition (i) on the choice of x. Nor was it unrealization, because otherwise mσ(z +
1) < k, contradicting condition (ii) on the choice of x. Nor was it insertion, by
condition (iii) on the choice of x. Nor was it the default action, because otherwise z
would be a (σ ⌢k)-stage, contradicting z > t and the choice of t.
Nor was it realization; to see this, assume the contrary. Then mσ(z + 1) = k + 1.
By (14.13), there is a least stage z′ > z during which σ performs an unrealization
on the witness nσ(k). Hence z′ is a (σ ⌢k)-stage, contradicting z′ > t and the choice
of t.
Thus, the assumption that there are only ﬁnitely many (σ ⌢k)-stages has led to a
contradiction. Therefore
σ ⌢k ∈TP.
QED Fact 6
Fact 6 has a number of nice corollaries, such as:
1. For each σ, σ ⌢R /∈TP.
2. The set TP is an inﬁnite path. This can be proved by ordinary induction on the
level of the tree, as follows. The root of the tree is in TP. By Fact 6, if σ ∈TP
then σ has a child in TP.
3. Along the true path all of the guesses (as described in Sect.14.4.2) are correct.
Fact 14.7 Let σ ∈TP. Then σ is canceled only ﬁnitely often.
Proof Cancellation occurs in three places: line 3 of the insertion subroutine, line 4
of the realization subroutine, and line 15 of the main code.

14.4 Veriﬁcation
147
Line 3 of the insertion subroutine cancels σ when witness nτ(i) is inserted for
some τ and i such that τ ⌢i ⪯σ. This occurs only ﬁnitely often, by Fact 1, because
σ has only ﬁnitely many ancestors.
Line 4 of the realization subroutine never cancels σ, because otherwise τ ⌢R ⪯σ
for some τ, and so τ ⌢R ∈TP, contradicting Fact 6.
We now show by induction on |σ| that line 15 of the main code cancels σ only
ﬁnitely often. In other words, we show that the set
Lσ = {s : TPs <L σ }
is ﬁnite.
As the basis for the induction, note that
L = ∅.
For the inductive step, suppose that τ is the parent of σ, and assume the claim
for τ; that is, assume that Lτ is ﬁnite. Let s ∈Lσ; then TPs <L τ is impossible.
Therefore, either TPs <L τ or τ ∈TPs.
Case 1.
TPs <L τ (as in Fig.14.8a).
There are only ﬁnitely many such s, because Lτ is ﬁnite.
Case 2.
τ ∈TPs (as in Fig.14.8b).
By Fact 6 applied to τ, we have σ = τ ⌢k, where k = lim infz mτ(z).
Assume for a contradiction that there were inﬁnitely many such s. Then at least
one of the nodes in the ﬁnite set

τ ⌢0, τ ⌢1, . . . , τ ⌢(k −1)

is visited inﬁnitely often, contradicting τ ⌢k ∈TP.
QED Fact 7
Fig. 14.8 Cases in the proof
of Fact 7

148
14
Witness Lists (Density Theorem)
14.4.4
Lemmas
Lemma 14.1 Each R-requirement is met.
Proof Fix e. We will show that R2e is met, the proof for R2e+1 being analogous. By
Fact 6, TP is an inﬁnite path, so there is a node σ on level 2e of TP. Also by Fact 6,
σ ⌢k ∈TP where
k = lim inf
s
mσ(s).
(14.14)
When is nσ(k) assigned a fresh value? It occurs when
(a) σ is initialized (line 1 of the initialization subroutine), if k = 0.
(b) nσ(k) becomes unrealized when nσ(k) ∈B1 (line 3 of the unrealization subrou-
tine).
(c) nσ(k −1) becomes realized (line 2 of the realization subroutine).
By Fact 7, (a) occurs only ﬁnitely often. By Fact 2, (b) occurs at most once. If (c)
occurs during a stage s then mσ(s) = k −1; therefore by (14.14), (c) occurs only
ﬁnitely often. To summarize, the value of nσ(k) changes only ﬁnitely often.
Therefore lims nσ(k)[s] exists9; call it n. Let x be the stage during which variable
nσ(k) is assigned the value n. Thus

∀s > x

n = nσ(k)[s]

.
(14.15)
Therefore

∀s > x

k ≤mσ(s)

,
(14.16)
because mσ(s) < k would imply nσ(k)[s] ↑.
We claim that n /∈B1; assume the contrary. Then there was a stage t during which
n = nσ(k)[t] was inserted into B1. Note that t > x. Furthermore,
k < mσ(t),
because otherwise
last Realized(σ, k)[t] = ∞
and so nσ(k) would not have been ready for insertion at stage t. Let v be the least
number such that v > t and
k = mσ(v + 1)
(such a v ≥t exists by (14.4), and v = t is impossible because the insertion subrou-
tine does not change mσ). See Fig.14.9. By (14.16), σ performed an unrealization on
nσ(k) during stage v. Hence, because n ∈B1[v], during stage v the value of nσ(k)
9 Thus, the variable nσ (k) has a ﬁnal value, whereas mσ might not (although it does have a ﬁnite
liminf).

14.4 Veriﬁcation
149
Fig. 14.9 The stages t and v in the proof of Lemma 1
was changed from n by line 3 of the unrealization subroutine, contradicting (14.15).10
Thus, n /∈B1; in other words,
B1(n) = 0.
(14.17)
Case 1.
lims mσ(s) exists.
Then k = lims mσ(s), by (14.14). Let y > x be such that

∀s ≥y

k = mσ(s)

.
Case 1.1
σ has a star witness at the start of stage y.
That star witness is nσ(i)[y] for some i < k. It is permanent, because mσ never
changes during or after stage y. Hence R2e is met.
Case 1.2
σ has no star witness at the start of stage y.
If B0
e (n)[y] = 0 then σ would perform a realization during the ﬁrst σ-stage
greater than or equal to y (look again at the condition for realization in Case 3
of the switch statement), which would increment mσ, a contradiction.
Therefore
B0
e (n)[z] ̸= 0
for each of the inﬁnitely many σ-stages greater than or equal to y. Hence
B0
e (n) ̸= 0,
(14.18)
by the contrapositive of the Permanence Lemma.
By (14.17) and (14.18),
B1(n) = 0 ̸= B0
e (n)
and so R2e is met.
10 The sole purpose of lines 2 and 3 of the unrealization subroutine is to ensure that n /∈B1 here.

150
14
Witness Lists (Density Theorem)
Case 2.
lims mσ(s) does not exist.
Then, by (14.14), the variable mσ rises above k and then falls back to k inﬁnitely
often. Therefore nσ(k) becomes realized and unrealized inﬁnitely often. In par-
ticular, because
n = lim
s nσ(k)[s],
there are inﬁnitely many stages z during which some
b0 ≤ϕB0
e (n)[z]
enters B0. Hence
B0
e (n) ↑,
by the contrapositive of the Permanence Lemma. Therefore
B1(n) ̸= B0
e (n)
and so R2e is met.
Thus, in either Case 1 or Case 2, R2e is met.
QED Lemma 1
Lemma 14.2 B0 ≤T C and B1 ≤T C.
Proof We will show B1 ≤T C, the proof of B0 ≤C being analogous.
Algorithm 14.3 is given an input b1, and uses an oracle for C to determine whether
b1 ∈B1. The oracle for C also functions as an oracle for A, because A <T C.
It calls three subroutines: The subroutine ReportYes outputs “b1 ∈B1” and then
halts the main code. The subroutine Report No outputs “b1 /∈B1” and likewise halts
the main code.
The third subroutine, cannotVisit Nodeξ(s2), called in line 23, is more compli-
cated. It returns a boolean value. In particular, it returns true if and only if there
exist σ, i, and i′ such that
i < i′ and σ ⌢i ⪯ξ
and σ ⌢i′ ∈TPs2
(see Fig. 14.10)
and
¬

∃s ≥s2

2as ≤ϕ(σ, i)[s2]

.
(14.19)
In (14.19), we multiply as by 2 because members of A are doubled when inserted by
lines 16 and 17 of the main code of Algorithm 14.1.11 Informally, (14.19) says that
during or after stage s2, no even number gets inserted and subsequently unrealizes
nσ(i)[s2] (no odd number does, either, by Fact 3).
11 Equation (14.19) is similar to line 4 of Algorithm 14.2, which was used in the proof of Fact 5.

14.4 Veriﬁcation
151
Algorithm 14.3
1 if b1 is even
2
if b1/2 ∈A
// Use the oracle for A to determine this.
3
ReportYes
// Line 17 of the main code of Algorithm 14.1 puts b1 into B1.
4
else Report No.
5 Run Algorithm 14.1 until it chooses a witness b2 ≥b1.
6 if b2 > b1
7
Report No.
// See Note 1 below.
8 ξ, j, s1 ←values such that nξ ( j) is assigned the value b1 during stage s1 of Algorithm 14.1.
9 if |ξ| is odd
10
Report No.
// Node ξ might insert b1 into B0 ,
// but it cannot insert anything into B1.
11 if inserted(ξ, j)[s1] = true
// Witness nξ ( j) will never be ready for insertion
// during or after stage s1. Therefore b1 /∈B1.
12
Report No.
13 if j /∈C
// Use the oracle for C to determine this.
14
Report No.
// b1 will never be C-permitted.
15 Compute entryIntoC( j) by looking through c0, c1, . . . until ﬁnding
the value of s such that j = cs, and then entryIntoC( j) = s.
// See Note 2, regarding the window of opportunity for inserting b1.
16 if entryIntoC( j) < s1 or nξ ( j)[entryIntoC( j)] is either unrealized or unequal to b1
17
Report No.
// See Note 3.
// The window of opportunity for inserting b1 is now open.
18 for s2 ←entryInto( j) to ∞
19
if b1 ∈B1[s2]
20
ReportYes.
// b1 ∈B1[s2] ⊆B1.
21
if mξ (s2) ≤j
// nξ ( j) is now either unrealized or undeﬁned.
22
Report No. // The window of opportunity for inserting b1 has closed.
23
if cannotVisit Nodeξ(s2)
24
Report No.
// See Note 4 and Exercise 16.
Notes on Algorithm 14.3:
1. Because witnesses are always fresh when chosen, they are chosen in increasing
order; hence if b2 > b1 (line 7) then b1 is never chosen as a witness, and so we
know that b1 /∈B1. Otherwise (b2 = b1), we continue.
2. Informally, strait is the gate through which b1 must pass before entering B1.12 The
variablenξ( j)mustremainequaltob1 until j entersC duringastagewhen j < mξ
(in other words, when nξ( j) is realized). This opens a “window of opportunity”
wherein b1 is ready for insertion into B1 during the next ξ-stage (see Fig.14.11).
Even during that stage and during subsequent ξ-stages, b1 might not be inserted,
because that action might have to defer to an action of higher precedence. So b1
waits. However, as it waits, if nξ( j) becomes undeﬁned or even unrealized (in
12 Nevertheless, inﬁnitely many odd numbers do ﬁnd their way into B1 (see Exercise 5).

152
14
Witness Lists (Density Theorem)
other words, mξ becomes less than or equal to j), then the window of opportunity
for inserting b1 closes, never to reopen, because forever after that
last Realized(ξ, j) ≥entryIntoC( j)
(14.20)
and so nξ( j) is never again ready for insertion.
3. Consider the condition in line 16.
If entryIntoC( j) < s1 then b1 is never C-permitted, because nξ( j) was assigned
the value b1 during stage s1, and so forever after that (14.20) is true. Therefore,
the window of opportunity for inserting b1 never opens.
If nξ( j)[entryIntoC( j)] is unrealized then forever after that (14.20) is true.
Therefore, the window of opportunity for inserting b1 never opens.
If nξ( j)[entryIntoC( j)] ̸= b1 then then b1 will never again be a witness for
any node during or after stage entryIntoC( j), because witnesses are fresh when
chosen.
4. Why is the boolean function cannotVisit Nodeξ(s2) so-named? In the proof
of Proposition5, we will argue that if line 24 is executed then (because
cannotVisit Nodeξ(s2) = true), node ξ cannot be visited during or after stage
s2, which implies that b1 /∈B1. This is just an overview of that argument.
Proposition4 implies that Algorithm 14.3 can be implemented to run in a ﬁnite
amount of time. Proposition5 implies the correctness of the output of Algorithm
14.3, thereby completing the proof of Lemma 2, and hence of Theorem14.
Proposition 4 The for loop in line 18 of Algorithm 14.3 does not run forever.
Proof Assume the contrary; that is, assume that line 18 of Algorithm 14.3 is reached,
but lines 20, 22, and 24 are never executed.
Consider three cases, which depend on the position of ξ relative to the true path:
Case 1.
TP <L ξ.
Then ξ is canceled inﬁnitely often, by line 15 of the main code of Algorithm
14.1. Let s ≥entryIntoC( j) be a stage during which such a cancellation occurs.
During stage s, witness nξ( j) is set to undeﬁned, and so
j < mξ(s + 1).
Therefore, on iteration s2 = s + 1 of the for loop of Algorithm 14.3, line 22 is
executed, a contradiction.
Case 2.
ξ ∈TP.
Then there are inﬁnitely many ξ-stages.
Line 17 did not execute (because otherwise line 18 would not have been reached),
so
nξ( j)[entryIntoC( j)] = b1.
(14.21)

14.4 Veriﬁcation
153
Fig. 14.10 Part of the
condition for
cannotVisit Nodeξ(s2)
Line 22 did not execute, so

∀s ≥entryIntoC( j)

mξ( j)[s] > j

.
(14.22)
Together, (14.21) and (14.22) imply

∀s ≥entryIntoC( j)

nξ( j)[s] = b1

.
Therefore, because line 20 is never executed,

∀s ≥entryIntoC( j)

inserted(ξ, j)[s] = false

.
Hence, for each s ≥entryIntoC( j),
last Realized(ξ, j)[s] < entryIntoC( j) ≤s.
Therefore, b1 = nξ( j) is ready for insertion at each ξ-stage that is greater than or
equal to entryIntoC( j) (informally, the window of opportunity for inserting b1
never closes).
Hence, (14.22) and Fact 4 together imply that b1 = nξ( j) is eventually inserted
into B1 during some stage t, and so on iteration s2 = t + 1 of the for loop of
Algorithm 14.3, line 20 is executed, a contradiction.
Case 3.
ξ <L TP.
Then there exists a node σ, with children β and γ , such that β ◁γ and β ⪯ξ
and γ ∈TP. Informally, the path from the root to ξ diverges from TP at node
σ. Note that β ̸= σ ⌢R, because σ ⌢R is the rightmost child of σ. Also note that
γ ̸= σ ⌢R, by Fact 6, because γ ∈TP. Thus, there exist i and i′ such that
i < i′ and β = σ ⌢i and γ = σ ⌢i′
(see Fig.14.12, and compare it to Fig.14.10).

154
14
Witness Lists (Density Theorem)
Fig. 14.11 The window of opportunity for inserting b1
Fig. 14.12 ξ <L TP
Let t > entryIntoC( j) be a γ -stage during or after which no node to the left of
γ is visited (such a t exists because γ ∈TP). Then ϕ(σ, i) never changes during
or after stage t. Let
u = ⌈ϕ(σ, i)[t]/2⌉,
and let s2 > t be such that
As2−1 ↾↾u = A↾↾u.
(14.23)
Note that ϕ(σ, i)[s2] = ϕ(σ, i)[t] and so
u = ⌈ϕ(σ, i)[s2]/2⌉.
Therefore, by (14.23),
¬

∃s ≥s2

as ≤⌈ϕ(σ, i)[s2]/2⌉

,

14.4 Veriﬁcation
155
which implies (14.19). Therefore, on iteration s2 of the for loop of Algorithm
14.3, line 24 is executed, a contradiction.
Thus, in each of the three cases, we have a contradiction.
QED Proposition 4
Proposition 5 The output of Algorithm 14.3 is correct.
Proof In the comments within Algorithm 14.3, and in the Notes on that algorithm,
we have already justiﬁed each call to ReportYes or to Report No up through line
22. It remains only to justify the call to Report No in line 24.
Assume that line 24 executes during a stage s2. Then
cannotVisit Nodeξ(s2) = true.
Therefore we have the situation depicted in Fig.14.10, and also (14.19) is true.
We need to prove that b1 /∈B1.
Assume for a contradiction that
b1 ∈B1.
Then there is a stage s4 during which b1 is inserted into B1. Note that s4 is a ξ-stage
and hence a (σ ⌢i)-stage. Because line 20 did not execute, we have
b1 /∈B1[s2]
and so
s2 ≤s4.
Furthermore, s2 ̸= s4 (because s2 is not a ξ-stage; again look at Fig.14.10). Therefore
s2 < s4.
Node ξ was not canceled during a stage in [s1, s4], because b1 = nξ( j) is inserted
into B1 during stage s4.
Which action did σ perform during stage s4? It was not insertion, because that
would have canceled ξ. Nor was it realization, because otherwiseσ ⌢R ∈TPs4 (which
is impossible, because s4 is a ξ-stage). Nor was σ initialized during stage s4, because
if it were then b1 would not equal nξ( j) when ξ is visited during stage s4, which
would imply b1 /∈B1.
Therefore σ either took the default action or performed an unrealization on nσ(i)
during stage s4; either way,
mσ(s4 + 1) = i.
(14.24)
See Fig.14.13 (in which σ took the default action during stage s4).

156
14
Witness Lists (Density Theorem)
Fig. 14.13 The stages s2, s3, and s4 in the proof of Case 3 of Proposition5
Node σ did not perform a realization during stage s2, because σ ⌢R /∈TPs2. Fur-
thermore, realization is the only action that would increase mσ. Therefore
mσ(s2) ≥i′ > i.
Hence there is a least stage s3 ≥s2 such that
mσ(s3) > i and mσ(s3 + 1) ≤i.
We have s3 ≤s4, by (14.24). Thus
s1 < s2 ≤s3 ≤s4 .
Which action did σ perform during stage s3?
It must have been either initialization or unrealization, because mσ(s3) decreased
during stage s3. It was not initialization, because otherwise σ (and hence ξ) would
have been canceled during a stage in [s2, s3].
Therefore σ performed an unrealization during stage s3. In particular, it must
have been the unrealization of nσ(i), because if it were the unrealization of nσ(i0)
for some i0 < i then ξ would have been canceled during stage s3 by line 15 of the
main code of Algorithm 14.1.
Let b be the number that unrealized nσ(i) during stage s3; then
b ≤ϕ(σ, i)[s3].
(14.25)
Because nσ(i) stayed realized from the start of stage s2 to the start of stage s3, we
have
last Realized(σ, i)[s3] < s2.
Hence
last Realized(σ, i)[s3] = last Realized(σ, i)[s2].

14.5 What’s New in This Chapter?
157
Therefore
ϕ(σ, i)[s3] = ϕ(σ, i)[s2],
and so, by (14.25),
b ≤ϕ(σ, i)[s2].
By Fact 3, b is even. Hence, there exists s ≥s2 such that
b = 2as ≤ϕ(σ, i)[s2]
(see Exercise 17), contradicting (14.19). Thus, our assumption that b1 ∈B1 has led
to a contradiction.
QED Proposition 5
By Proposition5, B1 ≤T C.
QED Lemma 2
The theorem follows from Lemmas 1 and 2.
QED Theorem 14
14.5
What’s New in This Chapter?
1. Coding. It’s a simple way to guarantee that A ≤T B where A is a given c.e. set
and B is a c.e. set under construction. In this chapter, we encoded the members
of A as the even numbers of B0 and B1, thereby leaving us free to use the odd
numbers of B0 and B1 to meet other requirements.
2. A way to guarantee that B ≰T A, where A is a given c.e. set and B is a c.e. set
under construction. In particular, we used the Friedberg-Muchnik method to build
incomparable c.e. sets B0 and B1, while using coding to guarantee that A ≤T B0
and A ≤T B1. Therefore, we had B0 ≰T A and B1 ≰T A.
Some of the strategies employed in this book are summarized in Fig.14.14. In the
ﬁrst line of the table, we are building two c.e. sets B0 and B1. In remaining four
lines of the table, we are given a c.e. set A and want to build a c.e. set B with the
listed property (and perhaps with certain other properties).
What makes the proof of the Density Theorem so delicate is that it simultane-
ouslyimplementsthreeofthesestrategies—coding,permitting,andtheFriedberg-
Muchnik method—while preventing them from interfering with each other. Fur-
thermore, the witness list idea in this chapter seems akin to the length-of-
agreement method; so, in a sense, our proof of the Density Theorem uses the
entire table.

158
14
Witness Lists (Density Theorem)
desired property
strategy
B0 ̸≤T B1 and B1 ̸≤T B0
Friedberg-Muchnik method
B ≤T A
various types of permitting
A ̸≤T B
length-of-agreement method
A ≤T B
coding
B ̸≤T A
combining the Friedberg-Muchnik method with coding
Fig. 14.14
Some strategies employed in this book
3. In the proof of Theorem7 (Frieberg-Muchnik), for each requirement we main-
tained a single witness. In the proof of Theorem8 (Friedberg-Muchnik below C),
for each requirement we maintained up to three witnesses, to facilitate a simple
form of permitting. In the proof of Theorem14 (Density), for each node σ, we
maintain an arbitrarily long list of witnesses (nσ), to facilitate a more complicated
form of permitting.
4. The “window of opportunity” for inserting an element clariﬁed the workings of
Algorithm 14.3. Might this metaphor help us to understand certain more compli-
cated algorithms?
5. In all tree constructions, it’s essential that TP be an inﬁnite path. That result is
trivial for ﬁnitely branching trees, and rather simple for the inﬁnitely branching
tree used in Chap.12. For the tree in this chapter, the result is still true (it follows
from Fact 6), but proving it takes some effort.
Is the difﬁculty of proving TP to be an inﬁnite path a useful way to classify the
complexity of various tree algorithms? Can it be formalized?
14.6
Designing an Algorithm
Books about algorithms are misleading. They present a problem, then an algorithm to
solve it, then a proof of correctness (and then an analysis of the running time, unless
the book is about computability theory). You get the impression that the research
occurred in that order. However, for complicated algorithms such as Algorithm 14.1,
it almost never happens that way. Rather, the algorithm and its proof of correctness
are developed together, to some extent. That is, you think about the algorithm in broad
strokes, and how the proof might be structured, which leads to reﬁnements of the
algorithm, which leads to changes in the proof, necessitating more “wrinkles” in the
algorithm. Even more changes in the algorithm occur as you double-check the proof
(and perhaps implement and run the algorithm, for the more concrete problems). For
example, while trying to verify an early draft of Algorithm 14.1, I saw that
σ ∈TP =⇒lim inf
s
mσ(s) < ∞

14.7 Afternotes
159
(which now follows from Fact 5) was essential to the argument. When trying to prove
this, I thought that it would be helpful if no odd number could cause an unrealization;
at that time, this was not true, but I made a few changes to the algorithm to make it
so (it is now Fact 3).
At various points during the design of Algorithm 14.1, I was faced with a choice,
either of which seemed workable. For example, should unrealization take precedence
over insertion, or vice versa? If neither alternative was clearly preferable, then I would
choose one, and put the other in my “back pocket.” Whenever I ran into a difﬁculty
in writing out the veriﬁcation section, I searched that back pocket (which contained
as many as half a dozen items), to see whether any of those ideas would get me past
the sticking point. Unfortunately, sometimes a back pocket algorithmic idea would
resolve the current veriﬁcation difﬁculty but create others.
Algorithm 14.1 is a very different animal from Algorithm 5.1, in terms of the
design process.
14.7
Afternotes
Even though this is a book about algorithms, we cannot help but marvel at the
partial ordering induced on c.e. sets by Turing reductions. It is dense, according
to Theorem14. On the other hand, the partial ordering on all (not necessarily c.e.)
sets induced by Turing reductions is not dense. For example, there exists a minimal
set, that is, a non-computable, non-c.e. set A such that there is no B such that ∅<T
B <T A (see Chapter V of [Le80]). Thus, the c.e. sets are dense with regard to Turing
reductions, whereas sets in general are not. This is surprising, because there are ℵ1
sets in general but only ℵ0 c.e. sets; yet in this sense the smaller family is the dense
one.
The Density Theorem was originally proved in [Sa64], using intricate combina-
torial arguments; it’s not an easy read. Another proof, using index sets (which we
have not discussed in this book), appeared in [Ya66b]. Yet another proof, using true
stages and hatted restraint functions, appeared in [So87]. A version using a priority
tree was sketched in [DH].
Our proof is mostly based on the sketch in [DH], but has some new ideas and is
far more detailed. In the tree used by [DH], each node σ has branches
(0, u), (0, f ), (1, u), (1, f ), . . .
(see Fig.14.15). The interpretation of the guesses corresponding to the branches is
more complicated for their tree than for ours. An algorithm based on their tree would
appear to necessitate a more slippery veriﬁcation argument (if written out in the level
of detail seen in this chapter). In priority tree arguments, small changes in the tree
can lead to big changes in conceptual simplicity.

160
14
Witness Lists (Density Theorem)
Fig. 14.15 Downey-
Hirshfeldt’s tree for the proof
of the Density Theorem
14.8
Exercises
1. Prove that (14.2) implies (14.1).
2. Prove that A ≤T B0 and A ≤T B1.
3. Fix e. As we stated in Sect.14.2,
(∃σ on level 2e of the tree)[σ has a permanent star witness ] =⇒R2e is met.
Must the converse be true? In other words, is it possible that R2e is met but
(∀σ on level 2e of the tree)[σ has no permanent star witness ]?
4. If mσ(s) converges, can σ perform inﬁnitely many insertions?
5. (a) Prove that B0 and B1 each contains inﬁnitely many odd numbers.
(b) Must they each contain inﬁnitely many even numbers?
6. Supposethatnσ(i)isundeﬁnedatthestartofa(σ ⌢i)-stage.Whatarethepossible
values of i?
7. Fix i. Can there be more than one node σ such that nσ(i) gets a fresh number
by line 3 of the unrealization subroutine?
8. Prove that the function entryIntoC(i) is not computable (recall that the deﬁni-
tion of a function being computable is in Chap.3).
9. Suppose that we modify Algorithm 14.1 so that when a number i enters C,
witness nσ(i) is immediately put into B1 (if |σ| is even) or into B0 (if |σ| is odd),
for each σ such that i < mσ(i). In other words, we immediately act once witness
nσ(i) is C-permitted, without waiting for the next σ-stage. Would the algorithm
still work (that is, would B0 and B1 still satisfy all of the requirements)?
10. Find the ﬂaw in the following argument:
“Claim:” Suppose that we remove Case 2 from the switch statement in
Algorithm 14.1 (thus, B0 ∪B1 would contain no odd numbers). Then the R-
requirements would still be met.
“Proof:” Fix e. Let σ be the level 2e node of TP, and let
k = lim inf
s
mσ(s).
Then k is a valid witness for R2e. Analogously, R2e+1 is met.
□

14.8 Exercises
161
This argument must have a ﬂaw, because without odd numbers, B0 and B1 would
be equal, contradicting the incomparability of B0 and B1.
11. Let σ, e, and k be such that |σ| = 2e and σ ⌢k ∈TP. For each i, let f (i) denote
the ﬁnal value of nσ(i), if such a value exists; otherwise, we write f (i) ↑.
(a) Might there exist i < k such that B0
e ( f (i)) ̸= 0?
(b) Might B0
e ( f (k)) = 0?
(c) Might there exist i > k such that B0
e ( f (i)) = 0?
12. Let σ ∈TP. Might σ perform inﬁnitely many insertions?
13. (a) Does it matter whether insertion takes precedence over realization?
(b) Does it matter whether unrealization takes precedence over insertion?
(c) Does the precedence of initialization, relative to that of unrealization, inser-
tion, and realization, matter?
14. Let σ ∈TP. Prove that
lim
s mσ (s) exists
⇐⇒σ takes the default action during all but ﬁnitely many visits to σ.
15. In the proof of Lemma 1, must n be a valid witness for R2e?
16. Describe a way to compute cannotVisit Nodeξ(s2) (which is used in Algorithm
14.3), given s2, in a ﬁnite amount of time, using an oracle for A.
17. Near the end of the proof of Proposition5, is written:
By Fact 3, b is even. Hence, there exists s ≥s2 such that
b = 2as ≤ϕ(σ, i)[s2].
Explain why s ≥s2.
18. Is it possible that

∃m

∀σ

∀s

mσ(s) ≤m

?

Chapter 15
The Theme of This Book: Delaying
Tactics
There is no sense of urgency in the construction of c.e. sets.1 We do not care how
much time a stage takes, as long as it is ﬁnite. Nor do we care how many stages we
must wait for an event to happen, as long as that, too, is a ﬁnite number.
When all ﬁnite amounts of time are regarded as the same, delaying tactics do not
hurt, but they might help. Indeed, many of the key algorithmic ideas in this book are
delaying tactics; as examples:
1. When permitting is used, a witness must wait for permission to enter the set under
construction.
2. A node σ in a priority tree is idle except during σ-stages.
3. During each stage s such that TPs <L σ, a node σ in a priority tree gets re-
initialized or canceled in some way, thereby (typically) wiping out any progress
that it has already made toward meeting its associated requirements. Fortunately,
if σ ∈TP then there are inﬁnitely many σ-stages, but only ﬁnitely many such
cancellations.
4. In Chaps.11 and 12, we use the node-speciﬁc computation B
σ (k)[s], rather than
B
e (k)[s], to deﬁne the length of agreement. In other words, we pay attention to
the computation B
e (k)[s] only if it is σ-believable. This has the side effect of
slowing down (but not stopping) the growth of the length of agreement, if σ ∈TP.
5. In Chap.14, the star witness concept has the (harmless) side effect of slowing
down but not stopping the growth of a witness list.
Likewise in Chap.14, both unrealization and insertion take precedence over real-
ization. This, too, might slow down (but not stop) the growth of a witness list.
Both of these delaying tactics are discussed in Note 7 of Sect.3.3 of Chap.14.
1 Hence there is no mention of fast data structures in this book.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2_15
163

Appendix A
A Pairing Function
Let
⟨i, j⟩=def
1
2(i + j)(i + j + 1) + j.
We are interested in ⟨i, j⟩because it is a computable, one-to-one correspondence
from ω × ω to ω (it is not unique in that regard).
A minor technical detail arises here: we have not yet deﬁned what it means for
a function of two (or more) variables to be computable. One way to do this is to
modify our deﬁnition of the input to a Turing machine. In particular, suppose that
we want to give the machine a ﬁnite sequence of inputs k1, k2, . . . , kr. Then, when
the machine starts, the binary encoding of k1 appears on the tape starting at cell 1,
followed by one blank symbol (B), followed by the binary encoding of k2, followed
by one B, and so forth, up through the binary encoding of the last input (kr); the rest
of the tape holds nothing but B’s.
Note that ⟨i, j⟩is monotonically increasing in both i and j. We can view ⟨i, j⟩
as a matrix, as in Fig.A.1.
Fig. A.1 The pairing
function ⟨i, j⟩
i\j
0
1
2
3
4
5
· · ·
0
0
2
5
9
14
20
· · ·
1
1
4
8
13
19
· · ·
2
3
7
12
18
· · ·
3
6
11
17
· · ·
4
10
16
· · ·
5
15
· · ·
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer
Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2
165

Bibliography
1 Books
[AK]
Ash CJ, Knight J (2000) Computable structures and the hyperarithmetical
hierarchy, Studies in Logic and the Foundations of Mathematics, vol 144
Elsevier, Amsterdam
[CLRS]
Cormen TH, Leiserson CE, Rivest RL, Stein C (2009) Algorithms, 3rd ed.
MIT Press
[Co]
Cooper SB (2004) Computability theory. Chapman and Hall/CRC Math-
ematics, London, New York
[Coh]
Cohen PJ (2008) Set theory and the continuum hypothesis. Dover,
Minneola, NY
[DH]
Downey RG, Hirshfeldt DR (2010) Algorithmic randomness. Springer,
New York
[Ku]
Kunen K (1983) Set theory: an introduction to independence proofs.
North Holland
[Le80]
Lerman M (1980) Degrees of unsolvability. Perspectives in Mathematical
Logic. Springer
[Le10]
Lerman M (2010) A framework for priority arguments. Cambridge Uni-
versity Press
[Ma]
Martin J (2002) Introduction to languages and the theory of computation,
3rd ed. McGraw-Hill
[Od]
Odifreddi
P
(1989)
Classical
recursion
theory.
North-Holland,
Amsterdam
[Ro]
Rogers H Jr (1967) Theory of recursive functions and effective com-
putability. McGraw-Hill, New York
[Si]
Sipser M (2012) Introduction to the theory of computation, 3rd ed. Cen-
gage Learning
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer
Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2
167

168
Bibliography
[So87]
Soare RI (1987) Recursively enumerable sets and degrees. Perspectives
in Mathematical Logic. Springer, Heidelberg
[So16]
Soare RI (2016) Turing computability. Springer
[Va]
Vaught RL (2001) Set Theory, 2nd ed. Birkhauser, Boston
2 Articles
[CGS]
Cholak P, Groszek M, Slaman T (2001) An almost deep degree. J Symbol
Logic 66(2)
[Cs]
Csima BF (2021) Understanding frameworks for priority arguments in
computability theory. Assoc Symbol Logic Invited Add Joint Math Meet
[De]
Dekker JCE (1954) A theorem on hypersimple sets. Proc Am Math Soc
5:791–796
[DS]
Downey R, Stob M (1993) Splitting theorems in recursion theory. Ann
Pure Appl Logic 65(1):1–106
[Fr57]
Friedberg RM (1957) Two recursively enumerable sets of incomparable
degrees of unsolvability. Proc Nat Acad Sci 43:236–238
[Fr58]
FriedbergRM(1958)Threetheoremsonrecursiveenumeration.JSymbol
Logic 23:308–316
[Ku]
Kucera A (1986) An alternative, priority-free solution to Post’s problem.
In: Lecture Notes in Computer Science, vol 233. Springer, pp 493–500
[La66]
Lachlan AH (1966) Lower bounds for pairs of recursively enumerable
degrees. In: Proceedings of the London Mathematical Society, vol 16, pp
537–569
[La79]
Lachlan AH (1979) Bounding Minimal Pairs. J Symbol Logic 44, No 4
[Mi]
Miller A (2007) Lecture Notes in Computability Theory (available as a
pdf online). University of Wisconsin
[Mo]
Montalbán A (2014) Priority arguments via true stages. J Symbol Logic
79, No 4:1315–1355
[Mu]
Muchnik AA (1956) On the unsolvability of the problem of reducibility
in the theory of algorithms. Doklady Akademii Nauk SSSR, NS, vol 108,
pp 194–197 (Russian)
[Po]
Post EL (1944) Recursively enumerable sets of positive integers and their
decision problems. Bull Am Math Soc 50:284–316
[Sa63]
Sacks GE (1963) On the degrees less than 0′. Ann Math 77, No 2:211–231
[Sa64]
Sacks GE (1964) The recursively enumerable degrees are dense. Ann
Math 80, No 2:300–312
[Tr]
Trahtenbrot BA (1970) On autoreducibility. Doklady Akademii Nauk
SSSR 192:1224–1227. An English translation is in Soviet Math 11, No
3:814–817

Bibliography
169
[Ya66a]
Yates CEM (1966) A minimal pair of recursively enumerable degrees.
J Symbol Logic 31:159–168
[Ya66b]
Yates CEM (1966) On the degrees of index sets. Trans Am Math Soc
121:309–328

Solutions to Selected Exercises
Chapter 3
1. (b) No, H (or any other c.e.n. set) is a counter-example.
(c) No. There are only ℵ0 c.e. sets, and only ℵ0 complements of c.e. sets (both
because there are only ℵ0 Turing machines). However, there are ℵ1 subsets
of ω.
2. (a) Yes, because A is inﬁnite (because it is non-computable) and therefore has
ℵ1 subsets, but there are only ℵ0 c.e. sets.
(b) Yes. Let a0, a1, . . . be a standard enumeration of A. Let B be constructed
by the following algorithm:
B ←∅.
for i ←1 to ∞
if ai > max{a0, a1, . . . , ai−1 }
put ai into B.
Then B is inﬁnite and computable.
3. (a) No. Let P be non-computable, and let
A = {2p : p ∈P } ∪{2p + 1 : p ∈P }.
Then
B0 = {2p : p ∈P }
and
B1 = {2p + 1 : p ∈P }.
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer
Nature Switzerland AG 2023
K. J. Supowit, Algorithms for Constructing Computably Enumerable Sets,
Computer Science Foundations and Applied Logic,
https://doi.org/10.1007/978-3-031-26904-2
171

172
Solutions to Selected Exercises
Set B0 is not computable, because otherwise we could determine whether a
given p is in P as follows:
if 2p ∈B0
output(“yes”)
else output(“no”).
Likewise B1 is not computable, because otherwise we could determine
whether a given p is in P as follows:
if 2p + 1 ∈B1
output(“yes”)
else output(“no”).
(b) Yes, because otherwise A would be the union of two computable sets, and
hence computable.
Chapter 4
1. The while loop would indeed halt for each s. To prove this, ﬁx s. There is a Turing
machine that accepts as but nothing else; let j be its index. Thus, W j = {as}.
Let z > j be such that  j(as)[z] ↓. During stage s, if the while loop eventually
sets t to z, then assigned is set to TRUE.
I don’t see how every requirement would be met. Fix some e such that We is
inﬁnite. Consider the requirement Re,1. Perhaps for each s such that as ∈We,
some requirement of weaker priority than Re,1 puts as into B0. In particular, let z
be the least number such that as ∈We[z]. There might be some e′ > e and some
z′ < z such that
as ∈We′[z′] and We′[z′] ∩B0 = ∅
when t = z′ during stage s; if so, then as is put into B0.
3. Our requirements are now
Re,i : We ̸= Bi
for each e ∈ω and each i ∈ω (whereas before it was just i ∈{0, 1}).
The priorities among the requirements are deﬁned as follows: Re,i ≺Re′,i′ if
and only if
⟨e, i⟩< ⟨e′, i′⟩.
The algorithm is now:

Solutions to Selected Exercises
173
for s ←0 to ∞
Bs ←∅.
assigned ←FALSE.
for k ←0 to s
e, i ←the unique numbers such that k =< e, i >.
if (not assigned) and as ∈We[s] and We[s] ∩Bi = ∅
// Requirement Re,i acts.
Put as into Bi.
assigned ←TRUE.
if not assigned
Put as into B0.
// Or put as into B1, it doesn’t matter.
The rest of the proof is very much like the proof of Theorem4.
Incidentally, here’s an incorrect answer that some students have given for this
problem: “Use Theorem4 repeatedly, to obtain, by induction, a partition of A
into inﬁnitely many c.e.n. sets.” It’s incorrect because what it actually proves is
the weaker statement:
(∀k)[A can be partitioned into k c.e.n. sets.]
5. No, because B0 and B1 are c.e.
Chapter 5
5. Let σ0, σ1, . . . denote the ﬁnite-length binary strings in lexicographic order.
Consider the following algorithm, which takes n as input:
Algorithm Y
for s ←0 to ∞
for i ←0 to s
if β2e ≺σi
if the computation σie (n) halts within s steps, and σie (n)=1
output(σi).
halt.
The number e and the string β2e are “hard-coded” into Algorithm Y.
Now, consider another algorithm, which is given n as input, and has H as an
oracle:
Algorithm Z
e′ ←the index of a Turing machine that executes Algorithm Y.
if ⟨e′, n⟩∈H
// The oracle for H is used here.
σ ←the output of Algorithm Y on input n.
output( σ )
else output(“no”).

174
Solutions to Selected Exercises
Algorithm Z solves the problem for (5.1). The argument for (5.2) is analogous,
substituting α2e+1 for β2e.
7. It is not clear that a Turing machine without an oracle can determine whether
(5.1) or (5.2) are true; so, even though the elements of A and of B are enumerated
in ascending order, they are not necessarily computably enumerated.
Note: one might think that this constitutes a proof that A and B are non-c.e., but
such an argument would be invalid, because it would not rule out the possibility
that A and B could be computably enumerated by an algorithm very different
from Algorithm 5.1.
8. If the α strings were constructed by Algorithm 5.1, then we would have
|α0| = 0,
|α1| = 1,
and
|α3| = |α2| + 1.
Indeed, for each e, we would have
|α2e+1| = |α2e| + 1
and
|β2e+2| = |β2e+1| + 1.
10. (a) Use Exercise 9(b), and induction.
(b) Let B = {⟨i, k⟩: k ∈Ai }. Then Ai ≤T B for each i. Informally, B encodes
all of the information about the Ai.
To show (∀i)[ B ≰T Ai ], assume for a contradiction that there were an i
such that B ≤T Ai. Then
Ai+1 ≤T B ≤T Ai ,
contradicting Ai <T Ai+1.
Chapter 6
1. Yes. Let u = ϕD
e (k)[s]. By the deﬁnition of permanence, D
e (k)[s] ↓and
Ds ↾↾u = D↾↾u.
Up through step s, during the computation D
e (k), the oracle head never moves
to the right of cell u. Hence, computations D
e (k) and D
e (k)[s] are identical in
every way.
2. No, because D −Dx might contain a number less than or equal to ϕD
e (k)[x].

Solutions to Selected Exercises
175
3. No. Suppose that

∀z ≥y

dz+1 ≤ϕD
e (k)[z]

.
It might be that

∀z ≥y

ϕD
e (k)[z] < ϕD
e (k)[z + 1]

even though

∀z ≥y

ϕD
e (k)[z] = 5

.
Thus, for each z, the computation D
e (k)[z] is not permanent. Therefore D
e (k) ↑,
by the contrapositive of the Permanence Lemma.
Chapter 7
1. Consider two even numbers i < j. If Ri and R j share a witness, which is put into
A because R j acts, then that witness might injure Ri. Thus, we would no longer
be able to argue that a requirement can be injured only by the action of a stronger
priority requirement.
The analogous problem could arise if two odd-numbered requirements shared a
witness.
I don’t see a difﬁculty with an even-numbered requirement sharing a witness with
an odd-numbered requirement.
2. (a) One.
(b) Two.
(c) An upper bound of 2 j can be derived by assuming that Ri acts the maximum
number of times, for each i < j.
(d) If j is even then the action of R j puts an element into A, which cannot injure
an even-numbered requirement (because those try to keep elements out of B).
Likewise, if j is odd then the action of R j cannot injure an odd-numbered
requirement. Hence, line 9 of Algorithm 7.1 can be replaced by:
if j is even
Initialize(k) for each odd k such that j < k ≤s
else Initialize(k) for each even k such that j < k ≤s
The Fibonacci numbers are usually deﬁned as F0 = 0, F1 = 1, and Fj =
Fj−1 + Fj−2 for all j ≥2.
There are two well-known identities that regard sums of all even- or odd-
indexed Fibonacci numbers (both of which can be proved by a standard
inductive argument):
j

i=1
F2i = F2 j+1 −1
and
j−1

i=0
F2i+1 = F2 j.
Deﬁne a( j) to be the maximum number of times R j can act, then we have
a(0) = 1, a(1) = 2, and in general

176
Solutions to Selected Exercises
a( j) =
⎧
⎪⎨
⎪⎩
1 + ( j−1)/2
i=0
a(2i),
if j is odd
1 +  j/2−1
i=0
a(2i + 1), if j is even.
We will prove by induction that
a( j) = Fj+2.
Our base cases have been established, so assume that a(i) = Fi+2 for all
i < k. Then we have two calculations depending on whether k is odd or even.
If it is odd then
a(k) = 1 +
(k−1)/2

i=0
a(2i)
= 1 +
(k−1)/2

i=0
F2i+2
= 1 +
(k+1)/2

i=1
F2i
= 1 + (F2·(k+1)/2+1 −1)
= Fk+2.
If it is even then
a(k) = 1 +
k/2−1

i=0
a(2i + 1)
= 1 +
k/2−1

i=0
F2i+3
= 1 +
k/2

i=1
F2i+1
= 1 + (F2·(k/2+1) −1)
= Fk+2.
Thus a(k) = Fk+2, so by induction a( j) = Fj+2 for all j.
It is known that
Fj =

1+
√
5
2
 j
−

1−
√
5
2
 j
√
5

Solutions to Selected Exercises
177
and so
Fj ∈
1 +
√
5
2
 j
.
Because
1 +
√
5
2
≈1.618 · · ·
this is asymptotically better than 2 j.
This solution was provided by Oscar Coppola.
Can the algorithm be further modiﬁed so as to reduce this upper bound even
more? I don’t know.
6. For each e, deﬁne the requirement:
Re : (∃n)

χA(n) ̸= A−{n}
e
(n)

.
Let ne denote the witness for Re (ne is a variable, as in the Friedberg-Muchnik
proof). Say that Re needs attention at stage s if
ne /∈As and A−{ne}
e
(ne) = 0.
We deﬁne A as the union of ﬁnite sets
A0 ⊆A1 ⊆· · · .
Here’s the algorithm, where procedure Initialize works exactly as it did in our
proof of the Friedberg-Muchnik Theorem:
A ←∅.
n0 ←0.
// This is equivalent here to Initialize(0).
for s ←1 to ∞
Initialize(s).
for j ←0 to s
if R j needs attention
// R j acts.
Put n j into A.
Initialize(k) for each k such that j < k ≤s.
The set A is c.e. because each stage can be performed in a ﬁnite amount of time.
To show that each Re is met, we could use an inductive proof that is analogous to
that of Lemma 7.1.

178
Solutions to Selected Exercises
Chapter 8
1. Suppose that R j acts during a stage z > w. This action creates a Type 3 witness
for R j; in other words, n j(3)[z + 1] ≥0. It also removes the Type 2 witness; in
other words, n j(2)[z + 1] = −1. The only way for n j(2) to become non-negative
again is by line 20 of Algorithm 8.1, which cannot happen as long as n j(3) ≥0
(because of the second condition in line 19). Furthermore, the only way for n j(3)
to reset to −1 after stage z is for Ri to act for some i < j, which is impossible
because z > w.
Therefore, R j never acts after stage z, and so R j acts at most once after stage w.
5. (a) Yes.
(b) Yes.
Chapter 9
3. Assume (9.7). Then Bi
e (p) ↓and A(p) ̸= Bi
e (p). By the Permanence Lemma,
there is a stage s0 such that the computation Bi
e (p)[s0] is permanent. Let s1 > s0
be such that
As1(p) = A(p).
Then

∀s ≥s1

As(p) ̸= Bi
e (p)[s]

and so
max
s≥s1 ℓe,i(s) ≤p.
Therefore
max
s
ℓe,i(s) < ∞,
contradicting (9.5).
7. (a) Let A = C be c.e.n. Assuming Theorem9′, A can be partitioned into c.e.
sets B0 and B1 such that
A ≰B0 and
A ≰B1.
By Lemma 9.1, B0 and B1 are incomparable.
(b) Hint: Modify the proof of Theorem9 by deﬁning ℓe,i as the length of agree-
ment between C and Bi
e .
8. Hint: Enhance Algorithm 9.1 with C-permitting (as was used in Algorithm 8.1).
Chapter 11
1. Let B = ∅.
3. Let C be c.e.n.
(a) Let A = {⟨e, 0⟩: e ∈C }, and suppose that A is given as input to Algorithm
11.1. Each row of A is ﬁnite, and so, by Lemma 11.1,

Solutions to Selected Exercises
179
(∀σ ∈TP)[σ ⌢f ∈TP].
Therefore, TP is computable.
(b) Let A = {⟨e, j ⟩: e ∈C and j ∈ω}, and suppose that A is given as input
to Algorithm 11.1. Then

∀e

e ∈C ⇐⇒A[e] is full ⇐⇒σ ⌢∞∈TP

,
where σ is the level e node of TP. Therefore, given e, we could determine
whether e ∈C by using an oracle for TP. In other words,
C ≤T TP
and hence TP is non-computable.
11. (b) No. This change would make the deﬁnitions circular:restraintσ depends on
ℓσ, which depends on B
σ , which depends on σ-believability, which would
depend on restraintσ.
13. Yes, because s > x, p < ℓσ(s), and condition (iv) on the choice of x in Part I of
Lemma 11.2 together imply
B
σ (p)[s] = B
e (p)[s].
Chapter 12
2. Yes. Fix e, r, and s. Both As and Wr[s] are ﬁnite sets; in particular,
|As| = |Wr(s)| = s + 1.
Hence, for sufﬁciently large k,
A(⟨e, k⟩)[s] = Wr(⟨e, k⟩)[s] = 0.
3. Fix e and r. Suppose that
ℓe,r(s −1) > ℓe,r(s)
for some s ≥1. Then

∃k < ℓe,r(s −1)

A(⟨e, k⟩)[s] = 1 = Wr(⟨e, k⟩)[s]

.
Thus, whenever ℓe,r decreases, it never regains its former size. Therefore, if
ℓe,r decreases at some point then it is unimodal; otherwise, it is monotonically
increasing.
It is possible that ℓe,r is both unimodal and monotonically increasing.

180
Solutions to Selected Exercises
7. (a) The following algorithm differs from Algorithm 11.1 only in that line 8 has
been changed from “else τ ←τ ⌢f .”
1
B ←∅.
2 for s ←0 to ∞
// Compute T Ps .
3
T Ps ←{λ}.
4
τ ←λ.
5
for e ←0 to s −1
6
if
 A[e]
s
 >
 A[e]
predτ (s)

7
τ ←τ ⌢∞
8
else τ ←τ ⌢ A[e]
s
.
9
T Ps ←T Ps ∪{τ}.
// B ←B ∪{certain elements of A}.
10
for e ←0 to s −1
11
ξ ←the level e node of T Ps.
12
for each a ∈A[e]
s −B such that a > Restraintξ(s)
13
Put a into B.
(b) No. Because TP <L ξ, there exists σ ∈TP such that σ ◁ξ. The picture
looks either like Fig.S.1a or b below, for some τ and r, where e = |τ|.
If A[e] is full (as in Fig.S.1a) then there is an x such that
 A[e]
x
 > r.
Node τ ⌢r is never visited after stage x. Hence there are only ﬁnitely many
ξ-stages.
So assume that A[e] is ﬁnite; let k =
 A[e] (as in Fig.S.1b). Then, because
k < r, there are no ξ-stages.
(c) Not necessarily. Suppose that A[0] is full, and that A −A[0] is inﬁnite. Then
there are inﬁnitely many s such that ∞∈TPs, and inﬁnitely many s such
that ∞/∈TPs. Thus, the claim would fail for e = 1.
Chapter 13
3. Yes. If As ∩We[s] ̸= ∅then requirement P|σ|,A never needs attention during or
after stage s.
4. The argument in Case 2 of the proof of Proposition 1 within the proof of Lemma
13.3 would fail.
5. Fix τ. Let
S = {s : TPs <L τ}.
The value of nτ,A can change (after its ﬁrst assignment) only during a stage
in S. If either τ <L TP or τ ∈TP then S would be ﬁnite, and so nτ,A would have
a ﬁnal value.
To prove the converse, suppose TP <L τ. Let σ be the level |τ| node of TP. Node
τ is initialized (with a larger value than it had before) during each σ-stage, of
which there are inﬁnitely many.

Solutions to Selected Exercises
181
Fig. S.1 Illustration for a solution to Exercise 6(b) of Chap. 12
6. Yes, because t0 is a σ-expansionary stage, which implies
Lσ(t0) = ℓσ(t0).
9. As pointed out near the end of the proof of Lemma 13.3, there is a stage y such
that the computation A
e (p)[y] is permanent. For each i such that ti > y,
ϕ A
e (p)[ti+1] = ϕ A
e (p)[ti] = ϕ A
e (p)[y].
Chapter 14
1. Assume (14.2). We will show A <T B0 <T C (the proof that A <T B1 <T C
is analogous). Because A ≤T B0 ≤T C, we need prove only that B0 ≰T A and
C ≰T B0. If B0 ≤T A then
B0 ≤T A ≤T B1 ,
contradicting B0 ≰T B1. Likewise, if C ≤T B0 then
B1 ≤T C ≤T B0 ,
contradicting B1 ≰T B0.
3. The converse might fail. For example, it might be that the eth OTM never halts,
regardless of its input and what is written on its oracle tape. In that case,

∀k

B0
e (k) ↑

and so R2e would be met, but no node on level 2e would ever perform a realiza-
tion; therefore no node on level 2e would ever have a star witness.

182
Solutions to Selected Exercises
4. No. Suppose that k and s0 are such that
(∀s ≥s0)[ mσ(s) = k ].
If nσ(i) is inserted after stage s0, then i < k. Thus, by Fact 1, after stage s0, node
σ inserts at most k times.
5. (a) There must be inﬁnitely many odd numbers in B0, because otherwise
B0 ≤T A and hence (since A ≤T B1), we would have B0 ≤T B1. By analo-
gous reasoning, B1 contains inﬁnitely many odd numbers.
(b) No, because A might be ﬁnite, in which case B0 and B1 each contains exactly
|A| even numbers.
6. The number i must be 0. If nσ(i)[s] ↑and (σ ⌢i) ∈TPs then σ performs an
initialization during stage s.
8. For each i,
i ∈C ⇐⇒entryIntoC(i) ≥0.
Hence, if entryIntoC(i) were computable, then, given i, we could ascertain
whether i ∈C by computing entryIntoC(i), and so C would be computable,
contradicting the assumption that A <T C.
10. Here’s the ﬂaw: without insertion, there would be no star witnesses, and so the
proof of Fact 5, which says
lim inf
s
mσ(s) < ∞,
would fail.
13. (c) No, because whenever the condition for initialization holds, the conditions
for unrealization, insertion, and realization do not hold.
15. No. In Case 1.1 of the proof of Lemma1.1, node σ has a permanent star witness,
which could prevent nσ(k) from being realized. Therefore, it is possible that
B1(n) = 0 = B0
e (n).
16. This is similar to the proof of Proposition 1 (within the proof of Fact 5).
To determine the truth of cannotVisit Nodeξ(s2), for each σ, i, and i′ such that
i < i′ and σ ⌢i ⪯ξ
and σ ⌢i′ ∈TPs2 ,
we need to determine whether
¬

∃s ≥s2

2as ≤ϕ(σ, i)[s2]

.

Solutions to Selected Exercises
183
We do this as follows:
a ←0.
answer ←TRUE.
while answer = TRUE and 2a ≤ϕ(σ, i)[s2]
if a ∈A −As2−1
// Here we use the oracle for A.
answer ←FALSE.
a ←a + 1.
output(answer).
The while loop iterates no more than
1 + ϕ(σ, i)[s2]/2
times.
18. No; assume for a contradiction that there is an m such that

∀σ

∀s

mσ(s) ≤m

.
Let z be such that
Cz ↾↾m = C ↾↾m.
Suppose that n is chosen as a fresh witness after stage z. Then n never receives
C-permission, and so n /∈B0 ∪B1. Therefore, B0 and B1 are both ﬁnite. Hence
B0 ≡T B1 ,
contradicting B0 ≰T B1 (and B1 ≰T B0).

