arXiv:2106.07890v2  [math.CT]  18 Nov 2021
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM
SYNTAX TO SEMANTICS
TAI-DANAE BRADLEY1, JOHN TERILLA2, AND YIANNIS VLASSOPOULOS3
ABSTRACT. State of the art language models return a natural language text con-
tinuation from any piece of input text. This ability to generate coherent text
extensions implies signiﬁcant sophistication, including a knowledge of grammar
and semantics. In this paper, we propose a mathematical framework for pass-
ing from probability distributions on extensions of given texts, such as the ones
learned by today’s large language models, to an enriched category containing
semantic information. Roughly speaking, we model probability distributions on
texts as a category enriched over the unit interval. Objects of this category are
expressions in language, and hom objects are conditional probabilities that one
expression is an extension of another. This category is syntactical—it describes
what goes with what. Then, via the Yoneda embedding, we pass to the enriched
category of unit interval-valued copresheaves on this syntactical category. This
category of enriched copresheaves is semantic—it is where we ﬁnd meaning,
logical operations such as entailment, and the building blocks for more elaborate
semantic concepts.
CONTENTS
1.
Introduction
2
1.1.
Compositionality
3
1.2.
Why category theory?
5
1.3.
Constructions in the unenriched setting
5
2.
Enriched category theory
8
2.1.
Categories enriched over [0, 1]
8
2.2.
The syntax category L
11
3.
Enriched copresheaves
12
3.1.
The [0, 1]-enriched Yoneda lemma
13
3.2.
The semantic category bL
14
4.
Enriched products and coproducts in bL
15
4.1.
Weighted limits and colimits
15
4.2.
Weighted products in bL
17
4.3.
Weighted coproducts in bL
19
1X, THE MOONSHOT FACTORY AND SANDBOX@ALPHABET, MOUNTAIN VIEW, CA
2THE CITY UNIVERSITY OF NEW YORK AND TUNNEL, NEW YORK, NY
3TUNNEL, NEW YORK, NY
E-mail addresses: tai.danae@math3ma.com, jterilla@gc.cuny.edu,
yiannis@tunnel.tech.
1

2
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
4.4.
Enriched implication
20
5.
A metric space interpretation
21
5.1.
Tropical module structure
24
6.
Conclusion
25
6.1.
Applications and future directions
26
Acknowledgements
27
References
27
1. INTRODUCTION
The world’s best large language models (LLMs) have recently attained new lev-
els of sophistication by effectively learning a probability distribution on possible
continuations of a given text. Interactively, one can input preﬁx text and then sam-
ple repeatedly from a next word distribution to generate original, high quality texts
[VSP+17, RNSS18, RWC+18, B+20]. Intuitively, the ability to continue a story
implies a great deal of sophistication. A grammatically correct continuation re-
quires a mastery of syntax, careful pronoun matching, part of speech awareness, a
sense of tense, and much more. A language model that effectively learns a prob-
ability distribution on possible continuations must apparently also have learned
some semantic knowledge. For the continuation of a story to be reasonable and
internally consistent requires knowledge of the world: dogs are animals that bark,
golf is played outdoors during the day, Tuesday is the day after Monday, etc. What
is striking is that these LLMs can be trained using unlabeled samples of text to pre-
dict a next word. No grammatical or semantic input is provided, nevertheless com-
plex syntactic structures, semantic information, and world knowledge are learned
and demonstrated. The present work is a response to the real-world evidence that it
is possible to pass from probability distributions on text continuations to semantic
information. We propose a mathematical framework for this passage.
We deﬁne a syntax category, which is a category enriched over the unit-interval
[0, 1], that models probability distributions on text continuations [BV20]. Our se-
mantic category is then deﬁned to be the enriched category of [0, 1]-valued co-
presheaves on the syntax category. The Yoneda embedding, which maps the syn-
tax category as a subcategory of the semantic category, assigns to a given text its
representable copresheaf. We regard the [0, 1]-valued copresheaf represented by a
text as the meaning of the text, as in dynamic semantics [NBvEV16], whose slo-
gan is “meaning is context change potential.” Furthermore, there are categorical
operations in the semantic category that allow one to combine meanings that corre-
spond to certain logical operations. In particular, there is a kind of context-sensitive
implication that models whether a certain text is true, given that another text is true.
The paper is organized as follows. Section 1.1 motivates the category theoretical
approach to language by contrasting it to an algebro-geometric perspective and de-
scribing a few advantages with explicit examples in Section 1.2. As noted there, a
primary advantage is the ability to marry both the compositional and distributional
structures of language in a principled way, a claim fully developed in Section 2.

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
3
There, a few basic deﬁnitions from enriched category theory are recalled before
deﬁning the language syntax category L as a category enriched over [0, 1]. We then
review more enriched category theory and prove the results we need in the special
case that the enriching category is the unit interval. This sets the stage to pass to
our semantic category of [0, 1]-valued copresheaves on L. The next section details
operations on enriched copresheaves that are akin to conjunction, disjunction, and
implication between meanings of expressions in language. Section 5 makes use of
an isomorphism between [0, 1] and the set of nonnegative extended reals [0, ∞] to
recast our ideas in the language of generalized metric spaces and tropical geometry.
Finally, a conclusion and summary is provided in Section 6.
The intended audience for this work includes mathematicians as well as mathe-
matical data scientists who may be intrigued by recent advances in deep learning
and natural language, as well as researchers interested in interpretability. Some
of the content may be helpful to philosophers concerned with reasoning, artiﬁcial
intelligence, and logic in both abstract theory and applications. We do assume the
reader is familiar with basic concepts in category theory: categories, functors, nat-
ural transformations, limits, and colimits. Excellent introductions to the subject
are readily available, including [Rie17, Lei14, FS19, LS09]. We do not assume fa-
miliarity with enriched category theory and introduce deﬁnitions and prove results
as needed. For a gentle introduction to the topic, see [FS19, Chapter 2] as well as
[Kel82] for a more formal treatment. For a historical background of statistical lan-
guage models and common techniques, see the survey in [JX19], and for nontech-
nical discussions of the current best-in-class LLMs see [Met20, Hea20, Hao21].
1.1. Compositionality. Language is a compositional structure: expressions can
be combined to make longer expressions. This is not a new idea. Theoretical lin-
guists have studied grammatical structures for a long time — think, for example, of
the richly developed ﬁeld of formal grammars [Wik21]. Focusing on the composi-
tionality, one may consider language as some kind of algebraic structure, perhaps
simply as the free monoid on some set of atomic symbols quotiented by the ideal
of all expressions that are not grammatically correct; or, perhaps, as something
more operadic, resembling the tree-like structures of parsed texts, labelled with
classes that can be assembled by some set of tree-grafting rules. In either case, the
algebraist might identify the meaning of a word, such as “red” with the principal
ideal of all expressions that contain “red.” The concept of red, then, is the ideal
containing red ruby, bright red ruby, red hot chili peppers, red rover, blood red,
red blooded, the workers’ and peasants’ red army, red meat, red ﬁretruck,... If
one knows every expression that contains a word, then, as the thinking goes, one
understands the meaning of that word. One is then led to a geometric picture by
an algebro-geometric analogy: construct a space whose points are ideals and view
the algebraic structure as a sort of coordinate algebra on the space. From this per-
spective, language is a coordinate algebra on a space of meanings. This geometric
picture points in appealing directions. If one can say the same kinds of things in
different languages, then those languages should be regarded as having comparable
ideal structures (or possibly equivalent categories of modules), translating between

4
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
different languages involves a change of coordinates on the space of meanings, and
so on. Going further, one has a way to model semantics via sheaves on this space
of meanings, as Lawvere in his classic 1969 paper [Law69] about syntax-semantic
duality.
We think this algebro-geometric picture is inspirational, but incomplete. Here,
we furnish it with an important reﬁnement. The compositional structure of lan-
guage is only part of the structure that exists in language. What is missing is the
distribution of meaningful texts. What has been, and what will be, said and writ-
ten is crucially important. When one encounters the word “ﬁretruck” it’s relevant
that “red ﬁretruck” has been observed more often than “green ﬁretruck.” The fact
that “red ﬁretruck” is observed more often than “red idea” contributes to the mean-
ing of red. Encountering an unlikely expression like “red idea” involves a shift in
expectation that demands attention and contributes to the meaning of the broader
context containing it. Again, the idea that distributional structure is important in
language is not new. The so called distributional hypothesis [Har54] in linguis-
tic states that linguistic items with similar distributions have similar meanings. In
machine learning, word frequency counts have been employed to automatically ex-
tract knowledge from a text corpus in Latent Semantic Analysis and vector models
of meaning [TP10].
So, the ideals mentioned above are just a ﬁrst approximation for the space of
meanings. In this paper, we formalize a marriage of the compositional structure
of language with the distributional structure as a category enriched over the unit
interval, which we denote by L and call the language syntax category. Then, once
we have deﬁned this language syntax category, we pass to the category of enriched
copresheaves on L, the objects of which are unit interval valued functions on L sat-
isfying a certain monotonicity condition. The category bL of enriched copresheaves,
which is itself a category enriched over the unit interval, is the semantic category.
Meanings reside in this semantic category, which also admits ways for meanings
to be manipulated and combined to form higher semantic concepts.
Before we discuss why we are using category theory, let us comment brieﬂy on
two other efforts to study natural language using categorical methods. The DisCo-
Cat program of [CSC10] seeks to combine compositional and distributional struc-
tures in a single categorical framework, though a choice of grammar is needed as
input. In our work, syntactical structures are inherent within the enriched category
theory and no grammatical input is required. This is motivated by the observation
that LLMs can continue texts in a grammatically correct way without the addi-
tional input of a grammatical structure, suggesting that all important grammatical
information is already contained in the enriched category. In short, the DisCo-
Cat program aims to attribute meaning to parts of texts and grammatical rules for
combining them to build meaning of larger texts. This is like the reverse of our
work, which asserts that the meaning of small texts is derived from the distribution
of larger texts that contain them. In [AS14], Abramsky and Sadrzadeh consider
a sheaf theoretic framework for studying language. That work makes some inter-
esting use of the gluing condition for sheaves. Here, we work with copresheaves,
without any kind of gluing conditions, but the most important difference is that

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
5
FIGURE 1. A category with at most one morphism between any
two objects. Here the objects are expressions in a language and
morphisms indicate when one expression is a continuation of an-
other. Idenity morphisms are not pictured, but are understood to
be present.
Abramsky and Sadrzadeh are not using the distributional structure that we are fo-
cused on here.
1.2. Why category theory? The algebraic perspective of viewing ideals as a proxy
for meaning is consistent with a category theoretical perspective, and the latter pro-
vides a better setting in which to merge the compositional and distributional struc-
tures of language. But even before adding distributional structures, moving from
an algebraic to a categorical perspective provides certain conceptual advantages
that we highlight ﬁrst.
Consider the category whose objects are elements of the free monoid on some
ﬁnite set of atomic symbols, where there is a morphism x →y whenever x is a
substring of y, that is, when the expression y is a continuation of x. If the ﬁnite set
is taken to be a set of English words, for example, then there are morphisms red
→red ﬁretruck and red →bright red ruby and so on. Each string is a substring of
itself, providing the identity morphisms, and composite morphisms are provided
by transitivity: if x is a substring of y and y is a substring of z, then x is a substring
of z. This category is simple to visualize: it is thin, which is to say there is at most
one morphism between any two objects, and so one might have in mind a picture
like that in Figure 1. A consequence of the Yoneda lemma implies that a ﬁxed
object in this category is determined up to unique isomorphism by the totality of its
relationships to all other objects in the category. One thus thinks of identifying an
expression x with the functor hx := hom(x, −) whose value on an expression y is
the one-point set ∗if x is contained in y and is the empty set otherwise. The functor
hx is an example of a copresheaf, the name given to a functor from a given category
to the category of sets, and is in fact a representable copresheaf, represented by the
object x. So in this way the preimage of ∗is precisely the principal ideal generated
by x. Representable copresheaves, therefore, are comparable to principal ideals and
both can be thought of as a ﬁrst approximation for the meaning of an expression.
1.3. Constructions in the unenriched setting. An immediate advantage to the
shift in perspective is that the functor category of copresheaves bC := SetC on

6
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
any small category C is better behaved than C itself. It has all limits and colimits
and is Cartesian closed. In fact, the category of copresheaves is a topos, which
is known to be the appropriate setting for intuitionistic logic [MM12]. The full
theory of toposes is not incorporated in this work, but intuition abounds when
one takes inspiration from the ﬁeld. Importantly, one has concrete operations for
building new copreasheaves from representable ones, suggestive of the “meanings
are composable instructions” perspective of internalist semantics [Pie18].
1.3.1. Coproducts. In the algebraic setting, the union of ideals is not an ideal,
and so one is left to wonder what algebraic structure might represent the disjunc-
tion of two concepts, say “red” or “blue.” In the categorical setting, the answer is
straightforward. The coproduct of copresheaves is again a copreasheaf. Given any
objects x and y in a small category C, the coproduct of representable copresheaves
hx ⊔hy : C →Set is computed “pointwise.” So suppose C is the category of ex-
pressions in language deﬁned above, and let x = red and y = blue. The coproduct
hred ⊔hblue therefore maps an expression c to the set hred(c) ⊔hblue(c). This set is
isomorphic to ∗if c contains either red or blue, and it is isomorphic to a two-point
set if c contains both, and otherwise it is the empty set. The output of the functor
hred ⊔hblue is therefore nonempty on the union of all expressions that contain red
or blue or both, and this matches well with the role of union as logical “or” among
sets.
1.3.2. Products. Now think about limits and in particular products. Like the co-
product, the product of representable copresheaves hx × hy : C →Set is again a
copresheaf computed pointwise. So if C is the category of language and x = red
and y = blue, then the value of the product on any expression c is given by
hred(c) × hblue(c), which is isomorphic to ∗if c contains both red and blue and
is the empty set otherwise. So the output of the functor hred × hblue is nonempty
on the intersection of expressions that contain red with those that contain blue, and
this coincides with the role of intersection as logical “and” among sets.
1.3.3. Cartesian closure. Advantages of the categorical perspective can further
be illustrated with a third example, which comes from the Cartesian closure of the
category of copresheaves bC on a small category C. To say that bC is Cartesian closed
means that it has ﬁnite products and moreover that the product of copresheaves has
a right adjoint (See section I.6. of [MM92]). More precisely, there is a bifunctor
[ , ]: bC × bC →bC called the internal hom that ﬁts into an isomorphism bC(F ×
G, H) ∼= bC(F, [G, H]) for all copresheaves F, G and H. Here and throughout we
use the notation A(x, y) to denote the set of morphisms from objects x to y in a
category A. The relevance of this new copresheaf [G, H] can be seen in its kinship
to implication in logic. Indeed, in intuitionistic propositional calculus one works
within a certain kind of thin Cartesian closed category called a Heyting algebra.
The usual notation in this setting is to write x ≤y if there is a morphism from
x →y. The categorical product is denoted by ∧and the internal hom is denoted
by ⇒. So, in this notation, one has: x ∧y ≤z if and only if x ≤(y ⇒z) for
all elements x, y, z. Keep this analogy in mind as we dissect an example of the

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
7
internal hom in bC when C is the category of expressions in language. Let’s do
some unwinding to see that the copresheaf
[hred, hblue]: C →Set
captures something like an implication red ⇒blue. First use the Yoneda lemma
and then the deﬁning property of the internal hom to get
(1)
[hred, hblue](c) = bC(hc, [hred, hblue]) = bC(hc × hred, hblue).
So, the internal hom assigns to an expression c the set of natural transformations
from the functor hc × hred to the functor hblue. The data of a single natural trans-
formation from hc × hred to hblue consists of a collection of set functions
(2)

(hc × hred)(d) →hblue(d)
	
d∈ob(C)
that ﬁt into a particular commutative square. Here, the domain (hc × hred)(d) and
the codomain (hblue)(d) are either empty or a singleton, and so the data of a natural
transformation either does not exist or is uniquely speciﬁed and automatically ﬁts
into the required commutative square. Therefore, [hred, hblue](c) is either the empty
set ∅or the one-point set ∗.
To determine whether [hred, hblue](c) is empty or not, let’s look closer at the
functions in (2). The product is computed pointwise, and thus the domain (hc ×
hred)(d) = hc(d)×hred(d) is isomorphic to the one-point set ∗whenever d contains
both c and red and is the empty set otherwise. The codomain hblue(d) is likewise
either ∗if d contains blue and is the empty set otherwise. Note that if there exists a
text d that contains c and red but does not contain blue then (hc ×hred)(d) = ∗and
hblue(d) = ∅, hence there does not exist a function(hc × hred)(d) →hblue(d) and
the set of natural transformations hc × hred to hblue is empty. On the other hand,
if every text d that contains c and red also contains blue then there is a unique
function (hc × hred)(d) →hblue(d) which is speciﬁed as follows:





∗→∗
when d contains c, red, and blue
∅→∗
when d does not contain both c and red, and d does contain blue
∅→∅
when d does not contain both c and red, and d does not contain blue.
For example, if c is the expression French ﬂag, then there exists a unique natural
transformation from hc × hred to hblue if every text that contains both French ﬂag
and red as subtexts, also contains blue and so [hred, hblue](French ﬂag) = ∗. If
c is the expression ruby then there exists no natural transformation hc × hred to
hblue if there is a text that contains red and ruby and does not contain blue and so
[hred, hblue](ruby) = ∅. One might say “red implies blue in the context of French
ﬂag” but “red does not implies blue in the context of ruby.”
In summary, the copresheaf [hx, hy]: C →Set is given by
[hx, hy](c) =
(
∗
if every text that contains both c and x also contains y
∅
otherwise.
Let’s review the picture of language presented in this section. One has a sim-
pliﬁed language category C whose objects are expressions in the language having

8
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
a single morphism x →y if x is a subexpression of y. The functor Cop →SetC
that maps an expression x to the representable functor hx = hom(x, −) is called
the Yoneda embedding and captures something of the meaning of x. Here Cop is
the opposite category of C. As a category, it has the same objects as C and the
morphisms are deﬁned to be Cop(x, y) := C(y, x).) This is comparable to mak-
ing a passage from syntax to semantics, after which meanings can be combined
by computing products, coproducts, and internal homs. Even more is possible, for
combined meanings can be combined again forming higher concepts, and there
are other categorical limits and colimits beyond products and coproducts, such as
pushouts, pullbacks, equalizers, and so on.... Yet this picture is still incomplete.
The distributional structure of language has not yet been accounted for. For this,
we use enriched category theory.
2. ENRICHED CATEGORY THEORY
Enriched category theory provides a ready-made way to decorate morphisms in
the simpliﬁed language category C from Section 1.2 with conditional probabilities
as in Figure 2. Enriched category theory begins with the observation that the set
of morphisms between objects may have more structure than just a set. Examples
are plentiful. The set of linear maps between vector spaces is itself a vector space,
for instance. In enriched category theory, the morphisms between objects is itself
an object in a category called the base category or the category over which the
category is enriched. When the enriching category is the category Set of sets, en-
riched category theory reduces to ordinary category theory. In order for enriched
category theory to have the desired structures and axioms (such as having com-
posable morphisms), the base category must have some of the structures that the
category of sets has. One can go rather far assuming the base category is a symmet-
ric monoidal category. In order to have convenient versions of enriched presheaves
and copresheaves, the base category should be a symmetric monoidal category that
is also closed, which allows the base category to be enriched over itself. If, in ad-
dition, the base category is complete and cocomplete, meaning that it contains all
limits and colimits, then the categories of copresheaves and presheaves are com-
plete and cocomplete. From a categorical point of view, our setting is relatively
simple. The set C(x, y) of morphisms from x to y is either the empty set or the
one point set. Also, the category we wish to enrich over is the unit interval [0, 1],
which becomes a complete, closed, symmetric monoidal category in a simple way
that ﬁts our purposes well. So, we will not burden the reader with the full machin-
ery of general enriched category theory [Kel82] but rather take advantage of the
simpliﬁcations afforded by our setting and specialize some of the deﬁnitions.
2.1. Categories enriched over [0, 1]. A preorder is a set together with a reﬂex-
ive, transitive relation. Any preorder becomes a category whose objects are the
elements of the set, with a single morphism from a to b if and only if a is related to
b. Reﬂexivity provides identity morphisms and transitivity provides composition
of morphisms, which is deﬁned in the only way it can be. Limits and colimits of

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
9
.04
.10
.07
.97
.17
.11
.07
.02
.65
.33
FIGURE 2. The compositional and distributional structures of lan-
guage are married by decorating arrows with the conditional prob-
ability that one expression contains another.
ﬁnite diagrams always exists and are easy to compute: the limit is the minimum of
the elements in the diagram and the colimit of a diagram is the maximum.
A monoid is a set together with an associative binary operation and a unit for that
operation. A commutative monoid is a monoid whose operation is commutative.
A preorder with a compatible commutative monoidal structure naturally becomes
a kind of category over which one can enrich. Formally,
Deﬁnition 1. A commutative monoidal preorder (V, ≤, ⊗, 1) is a preorder (V, ≤)
and a commutative monoid (V, ⊗, 1) satisfying x ⊗y ≤x′ ⊗y′ whenever x ≤x′
and y ≤y′.
Deﬁnition 2. Let (V, ≤, ⊗, 1) be a commutative monoidal preorder. The data of
a (small) V-enriched category, or simply a V-category, consists of a set of objects
C, and for every pair of objects x and y, there is an element C(x, y) ∈V called a
V-hom object. This data satisﬁes
1 ≤C(x, x)
(3)
C(y, z) ⊗C(x, y) ≤C(x, z)
(4)
for all objects x, y, z ∈C.
The unit interval [0, 1] := {x ∈R : 0 ≤x ≤1} is a commutative monoidal
preorder with multiplication being the monoidal product, having 1 as the unit, and
with the usual ≤relation being the preorder. The data of a [0, 1]-category consists
of a set of objects C and a [0, 1]-valued function (x, y) 7→C(x, y) deﬁned for every
x, y ∈C satisfying C(x, x) = 1 for every x ∈C and C(y, z)C(x, y) ≤C(x, z) for
all x, y, z ∈C.
Deﬁnition 3. A commutative monoidal preorder V is said to be closed provided
for every pair of elements x and y in V there is an element [x, y] ∈V , called the
internal hom, satisfying
(5)
x ⊗y ≤z if and only if x ≤[y, z]
for all x, y, z in V.
The relevance is that a closed commutative monoidal preorder V becomes a
category enriched over itself by replacing the hom set V(x, y), which is either the

10
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
empty set or the one-point set, by the internal hom [x, y], which is an object of V.
To see that the assignment (x, y) 7→[x, y] does in fact make V a category enriched
over itself, one needs to check that 1 ≤[x, x] and [y, z] ⊗[x, y] ≤[x, z]. The ﬁrst
inequality follows from the fact that 1 ⊗x ≤x and Equation (5). One can check
that [y, z] ⊗[x, y] ≤[x, z] in two steps. First, [y, z] ≤[y, z] and Equation (5) gives
[y, z] ⊗y ≤z and similarly, [x, y] ⊗x ≤y. Putting these two inequalities together
yields [y, z] ⊗[x, y] ⊗x ≤z which implies [y, z] ⊗[x, y] ≤[x, z]. Much can
be said about commutative monoidal preorders and enrichment, for instance, see
[FS19, Chapter 2], though for now our primary focus will be on the unit interval.
Lemma 1. The unit interval [0, 1] is a closed commutative monoidal preorder. The
monoidal product a ⊗b := ab is the usual product of numbers and the internal
hom is truncated division: for all a, b ∈[0, 1] deﬁne
(6)
[a, b] :=
(
b/a
if b < a,
1
otherwise.
Proof. To verify closure, we need to check the formula for truncated division works
as an internal hom; that is, ab ≤c if and only if a ≤[b, c]. There are two cases. If
c < b then ab ≤c implies that a ≤c
b = [b, c]. If b ≤c then [b, c] = 1 and a ≤[b, c]
automatically.
□
While everything constructed using the internal hom in [0, 1] ultimately involves
statements about multiplication and less than or equal to, trying to argue using
only multiplication and order can get messy, as statements often break down into
multiple cases owing to the minimum in truncated division. Using the fact that for
all a, b, c ∈[0, 1] we have
(7)
ab ≤c if and only if a ≤[b, c]
often makes arguments simpler. In what follows, we refer to the equivalence in (7)
as the closure property of [0, 1]. We occasionally will write min{b/a, 1} when a
might be zero. The reader should interpret b/0 ≥1 for any 0 ≤b ≤1.
Also, keep in mind that we use juxtaposition ab for ordinary multiplication of
numbers. For our purposes, this is shorthand for the monoidal product a ⊗b. We
also have the categorical product in [0, 1], which is given by the minimum. So, for
two numbers a, b ∈[0, 1] the expression a × b means min{a, b}. The coproduct is
given by the maximum and is denoted by a ⊔b. So, when we are working in the
unit interval [0, 1], remember Equation (6) for the internal hom and
a × b := min{a, b}
a ⊔b := max{a, b}.
As a ﬁnal remark, any thin category C can become a category enriched over
[0, 1] by setting C(x, y) = 1 if there is a morphism between objects x →y and
C(x, y) = 0 otherwise. In fact, 2 := {0, 1}, the set that contains zero and one
only, is itself a commutative monoidal preorder that is closed, a sub-commutative
monoidal preorder of the unit interval, and can serve as a base category over which
to enrich.

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
11
2.2. The syntax category L. Following [BV20], we now deﬁne a category L en-
riched over [0, 1].
Deﬁnition 4. We deﬁne the syntax category L to be the category enriched over
[0, 1] whose objects are expression in the language and where [0, 1]-objects are
deﬁned by
L(x, y) := π(y|x),
where π(y|x) denotes the probability that expression y extends expression x. If x
is not a subtext of y then necessarily π(y|x) = 0.
So, for example, one might have
L (red,red ﬁretruck) = 0.02
L (red,red idea) = 10−5
L (red,blue sky) = 0.
That L is indeed a category enriched over [0, 1] follows from the fact that π(x|x) =
1 and
(8)
π(z|y)π(y|x) = π(z|x)
for all texts x, y, z and so satisﬁes the required inequalities (3) and (4) with equal-
ities. The reader might think of these probabilities π(y|x) as being most well de-
ﬁned when y is a short extension of x. While one may be skeptical about assigning
a probability distribution on the set of all possible texts, it’s reasonable to say there
is a nonzero probability that cat food will follow I am going to the store to buy a
can of and, practically speaking, that probability can be estimated. Indeed, existing
LLMs [RNSS18, RWC+18, B+20] successfully learn these conditional probabili-
ties π(y|x) using standard machine learning tools trained on large corpora of texts,
which may be viewed as providing a wealth of samples drawn from these condi-
tional probability distributions. Figure 2 gives the right toy picture: the objects are
expressions in language, and the labels on the arrows describe the probability of
extension.
As in the unenriched setting of Section 1.2, the category L is inherently syn-
tactical—it encodes “what goes with what” together with the statistics of those
expressions. But what about semantics? Following the same line of reasoning that
led to the consideration of copresheaves in the unenriched case, we now wish to
pass from L to the enriched version of copresheaves on L where we propose that
meaning lies, and where concepts can again be combined through the enriched
version of limits, colimits, and internal homs. This discussion ﬁrst requires a few
mathematical preliminaries, beginning with what an enriched functor is, what an
enriched copresheaf is, and what the enriched version of natural transformations
between functors are.
Before going on to the next section, we brieﬂy comment on the work in [BV20].
In that paper, a [0, 1]-enriched functor is deﬁned between the syntax category L and
another [0, 1]-enriched category, embedding L as subcategory of a [0, 1]-enriched
category D consisting of “density” operators. In this paper we construct a category

12
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
bL of [0, 1]-copresheaves on L, and the enriched version of the Yoneda embedding
deﬁnes an embedding L →bL. The reader may think of the [0, 1]-enriched Yoneda
embedding as factoring through the functor L →D deﬁned in [BV20], as in the
picture below:
L
bL
D
While technical details are needed to construct the third arrow D
bL , one
may think of the category D is a kind of intermediary. The logical and semantic
possibilities within D are more limited than in the semantic bL. However, the cate-
gory D, as is argued in [BV20], can be approximated by a computer model, while
providing more room for semantic exploration than in the syntactic category L.
3. ENRICHED COPRESHEAVES
Recall from Section 1.2 that a copresheaf on a category C is a functor C →Set.
To understand the enriched version, then, one must ﬁrst have a notion of functors
between enriched categories.
Deﬁnition 5. Suppose C and D are categories enriched over a commutative monoidal
preorder (V, ⊗, ≤, 1). An enriched functor C →D is a function f : C →D satis-
fying
(9)
C(x, y) ≤D(fx, fy)
for all objects x and y in C.
In the case that V is closed, it is enriched over itself and we can take D = V to
make sense of enriched copresheaves.
Deﬁnition 6. Suppose C is a category enriched over a closed commutative monoidal
preorder V. An enriched copresheaf is a function f : C →V satisfying C(x, y) ≤
V(fx, fy) for all objects x and y in C.
Now, we discuss how to make a V-enriched category DC whose objects are V-
functors from C to D. We need a hom object DC(f, g) ∈V between any two such
functors f, g: C →D. Formally, it is deﬁned by an end, which is a particular kind
of limit in V
(10)
DC(f, g) :=
Z
c∈C
D(fc, gc)
which always exists if V is complete. The precise deﬁnition of an end can be found
in Section 7.3 in [Rie14], but now we will specialize to the case of copresheaves
and the right hand side of Equation (10) reduces to something simple, summarized
in Lemma 2 below. The takeaway is that just as mappings between (ordinary) cat-
egories deﬁne a functor category, so mappings between enriched categories deﬁne

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
13
an enriched functor category, at least when the base category is closed and com-
plete.
From now on, we specialize to the case that the enriching category is [0, 1]. We
begin with a lemma that says how to assign an element of [0, 1] to a pair of functors
enriched over [0, 1].
Lemma 2. If C is a category enriched over [0, 1], then the category bC := [0, 1]C
of copresheaves is also enriched over [0, 1]. The [0, 1]-object between any pair of
copresheaves f, g: C →[0, 1] is given by the following inﬁmum over all objects in
C,
(11)
bC(f, g) = inf
c∈C{[fc, gc]}
Proof. The proof is a computation of the end on the right hand side of Equation
(10) when the target category is the unit interval.
□
Recalling that the internal hom in the unit interval is given by truncated division,
one has the [0, 1]-hom object associated to a pair of [0, 1]-functors f, g: C →[0, 1]
is given by bC(f, g) = infc∈C{1, gc/fc}.
Lemma 3. Let C be a [0, 1]-category. For every object x, the function
hx := C(x, −)
is a [0, 1]-functor.
Proof. Setting V = [0, 1] and also D = [0, 1], Equation (9) says what is required
for a function C →[0, 1] to be an enriched functor. So, let c, d ∈C. We need to
check that
C(c, d) ≤[hx(c), hx(d)]
That is, we need to check that C(c, d) ≤[C(x, c), C(x, d)], which by the closure
condition in (7) is equivalent to C(c, d)C(x, c) ≤C(x, d), which is satisﬁed since
C is enriched over [0, 1].
□
Deﬁnition 7. For any object x in a [0, 1]-category C, we call the functor hx :=
C(x, −) the copresheaf represented by x. We say a copresheaf f : C →[0, 1] is
representable if f = hx for some x ∈C.
3.1. The [0, 1]-enriched Yoneda lemma. Lemma 2 says that enriched copresheaves
form an enriched category. Lemma 3 says that each object deﬁnes a representable
copresheaf. In this subsection, we see that the assignment x 7→hx deﬁnes an en-
riched functor that embeds (the opposite of) a [0, 1]-category within its category of
copresheaves. It is a corollary of an enriched Yoneda lemma.
Theorem 1 (The Enriched Yoneda Lemma). For any object x in a [0, 1] category
C and any [0, 1]-copresheaf f : C →[0, 1] we have bC(hx, f) = f(x).
Proof. Fix an object x ∈C and a copresheaf f. Since bC(hx, f) = infc∈C{[hx(c), fc]}
we have for any particular c ∈C that bC(hx, f) ≤[hx(c), fc]. For c = x, we have
bC(hx, f) ≤[hx(x), fx] = [1, fx] = fx.

14
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
On the other hand, since f is a [0, 1]-functor from C to [0, 1], we have C(x, c) ≤
[fx, fc] for any c ∈C. By the closure of [0, 1] the inequality C(x, c) ≤[fx, fc]
is equivalent to C(x, c)fx ≤fc which in turn is equivalent to fx ≤[C(x, c), fc].
Having fx ≤[C(x, c), fc] for every c ∈C implies that fx ≤infc∈C{[hx(c), fc]} =
bC(hx, f) and the theorem is proved.
□
Corollary 1. C(y, x) = bC(hx, hy) for all objects x, y in a [0, 1]-category C.
Proof. Setting f = hy in Theorem 1 yields bC(hx, hy) = hy(x) = C(y, x).
□
Therefore, we have the expected interpretation of Corollary 1 as an enriched
version of the (co)Yoneda embedding.
Corollary 2. For any [0, 1]-category C, the assignment x 7→hx deﬁnes an enriched
functor Cop →bC, embedding Cop as an enriched subcategory of bC.
The op in Cop stands for “opposite” and is there because the assignment x 7→hx
reverses morphisms, as in the statement of Corollary 1 above.
3.2. The semantic category bL. Often, when a mathematical object Y has nice
structure, the set of functions {X →Y } from a ﬁxed X has nice struture also,
even if X does not. Real valued functions on any set form a vector space, etc....
The unit interval has rich structure from the perspective of category theory. It
is commutative monoidal closed and complete and cocomplete. For any [0, 1]-
enriched category C, the category bC = [0, 1]C of copresheaves on C, as a category
of functors into the unit interval, inherits rich structure, as well. In particular, it
has enriched versions of products, coproducts, and an internal hom. Corollary 2,
from this perspective, says that any [0, 1]-category embeds into the [0, 1]-category
bC, which is often much nicer.
We now look at the copresheaves on the enriched syntax category L which will
provide a place for the combination of concepts in language in a way that’s parallel
to the ideas explored in Section 1.2.
Deﬁnition 8. Let L be the syntax category (Deﬁnition 4). The semantic category
bL := [0, 1]L is the [0, 1]-category of [0, 1]-enriched copresheaves on the [0, 1]-
category L.
For each object x in L, the representable copresheaf hx := L(x, −) is given by
the conditional probability of extending x,
(12)
c 7→hx(c) :=
(
π(c|x)
if x ≤c
0
otherwise.
where we use “x ≤c” as shorthand for “x is contained as a subtext of c.”
The representable enriched copresheaf hx is our proposal for the meaning of
the expression x. Its support consists of all expressions containing x, which co-
incides with the principal ideal associated to x, and hx further accounts for the
statistics associated with those expressions, which is precisely the distributional
structure missing from Section 1.2. In other words, the meaning of a text is the

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
15
varying potential of all contexts in which it is used, making our deﬁnition of hx as
the meaning of the text x consistent with at least several philosophical traditions,
including a use theory of meaning [Hor04] and dynamic semantics [NBvEV16].
The embedding in Corollary 2 assigns to a text x its meaning hx.
From a mathematical point of view, the assignment x 7→hx faithfully embeds
the syntax category L into the category of copresheaves on L. The category bL is
complete and cocomplete and so contains all (the enriched versions of) limits and
colimits. The meanings of texts that live in bL can be manipulated and combined
to form higher concepts in bL, none of which is possible within the conﬁnes of L.
These operations on copresheaves are the subject of the next section.
The reversal of arrows in the [0, 1]-enriched Yoneda Lemma has a nice interpre-
tation here also. Suppose the text y extends the text x and L(x, y) = a ̸= 0. We
have the following picture:
x
y
hy
hx
a
a
On the left, the text y is an extension of the text x in the syntax category L. Passing
to the semantic category bL on the right, hx is the meaning of the text x, which rep-
resents the varying potential contexts in which x might appear, and hy represents
the varying potential contexts in which the text y appears. The contexts that x can
appear extend the contexts that in which y can appear. Continuing an expression
restricts the potential contexts in which the expression can be used.
4. ENRICHED PRODUCTS AND COPRODUCTS IN bL
Think back to Section 1.3.1, for instance, where we described the coproduct
of ordinary copresheaves associated to the words red and blue, which represented
their disjunction red or blue. Section 1.3.2 likewise computed the product of co-
presheaves representing the conjunction red and blue, and Section 1.3.3 computed
the copresheaf representing the implication red ⇒blue. In this section, we discuss
the analogous constructions for enriched copresheaves. We have asserted several
times that the [0, 1]-category of [0, 1]-copresheaves on a [0, 1]-category C contains
all of the enriched versions of (co)limits. Without having yet said what the en-
riched version of (co)limits are, one can understand that the reason [0, 1]-enriched
copresheaf categories are (co)complete is that the appropriate (co)limits are com-
puted “pointwise” in [0, 1]. The real-analysis completeness of the interval implies
that the inﬁmum and supremum of any subset of the interval exists, hence the [0, 1]-
enriched categorical (co)limit of any diagram in [0, 1] exists. This is analagous to
the way (co)limits in the functor category SetC when C is an ordinary category are
built up from (co)limits of sets, which always exist in Set.
To make sense of (co)limits in this new enriched setting, the familiar deﬁnition
must be slightly modiﬁed. This leads to the notion of weighted limits and colimits,
which are the appropriate notions of limits and colimits in enriched category theory.
4.1. Weighted limits and colimits. The appropriate notion of limits and colimits
in enriched category theory are called weighted limits and colimits. We focus on

16
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
building intuition around limits as the discussion for colimits is analogous. Now,
to understand weighted limits, it will ﬁrst help to recall the deﬁning isomorphism
for ordinary limits, also called “conical limits.” To that end, let J be an indexing
category and let C be any category. Recall that the limit of a diagram F : J →C,
if it exists, is an object lim F in C together with the following isomorphism of
functors C →Set,
(13)
C(−, lim F) ∼= SetJ(∗, C(−, F))
which is natural in the ﬁrst variable. Here, ∗is used to denote the constant functor
at the one-point set:
(14)
J
Set
i
∗
j
∗
∗
id∗
So for each object Z in C there is a bijection C(Z, lim F) ∼= SetJ(∗, C(Z, F))
and so “morphisms into the limit are the same as a cone over the diagram, whose
legs commute with morphisms in the diagram,” and one has in mind the following
picture.
Z
lim F
Fi
Fj
Taking a closer look at the right-hand side of the isomorphism in (13), notice that
for each object Z in C, a natural transformation from ∗to C(Z, F) consists of a
function ∗= ∗(i) →C(Z, Fi) for each object i in J. This simply picks out a
morphism Z →Fi in C, and these morphisms comprise the legs of the limit cone
over lim F. Naturality ensures that these legs are compatible with the morphisms
in the diagram.
With this background in mind, we now introduce weighted limits. The essential
difference between the two constructions is that the constant functor in (14) is re-
placed with a so-called V-functor of weights W : J →V, where V is the base cat-
egory over which enrichment takes place and where J is an indexing V-category.
Here is the formal deﬁnition.
Deﬁnition 9. Let V be a closed commutative monoidal category and let J and E
be V-categories. Given a V-functor F : J →E and a V-functor W : J →V,
the weighted limit of F, if it exists, is an object limW F of E together with the
following isomorphism of V-functors E →V,
(15)
E(−, lim
W F) ∼= VJ (W, E(−, F))
that is natural in the ﬁrst variable.

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
17
So in other words, for each object Z in E there is an isomorphism E(Z, limW F) ∼=
VJ (W, E(Z, F)) of objects in V. The idea behind a weighted colimit is analogous.
Given a V-functor F : J →E and a V-functor of weights W : J op →V, the
weighted colimit of F is an object colimW F of E together with an isomorphism
(16)
E(colimW F, −) ∼= VJ op(W, E(F, −))
Details may be found in [Rie14, Chapter 7].
4.2. Weighted products in bL. Let’s unwind the isomorphism in (15) in the simple
case when V = [0, 1], when E = bL := [0, 1]L, and when the indexing category J
is a discrete category with two objects, call them 1 and 2, enriched over [0, 1] by
setting J (i, j) = δij for i, j ∈{1, 2}. To begin, ﬁx a functor of weights W : J →
[0, 1]. This is nothing more than a choice of two numbers w1 := W(1) and w2 :=
W(2). Further, for a ﬁxed pair of copresheaves f, g : L →[0, 1], deﬁne F : J →
bL to be the [0, 1]-functor with f := F(1) and g := F(2).
Deﬁnition 10. Denote the weighted limit of F with respect to the weight W by
(w1, f) × (w2, g) := lim
W F.
Theorem 2. The weighted limit (w1, f) × (w2, g) : L →[0, 1] is given by c 7→
min
 fc
w1
, gc
w2
, 1

.
Proof. To check that c 7→min
n
fc
w1, gc
w2 , 1
o
satisﬁes the universal property of the
weighted limit, let Z : L →[0, 1] be any copresheaf and look at Equation (15)
evaluated at Z. We need to check that
(17)
bL(Z, limW F) ∼= [0, 1]J (W, bL(Z, F)).
On the left hand side, with the claimed copresheaf substituted for the limit, we have
inf
c∈L

Zc, min
 fc
w1
, gc
w2
, 1

= inf
c∈L
 fc
w1Zc,
gc
w2Zc, 1

.
Let’s begin looking at the right hand side of Equation (17) by simplifying the ex-
pression bL(Z, F), which is a functor J →[0, 1]. Evaluating at i in J yields the
number
bL(Z, Fi) = inf
c∈L{[Zc, Fic]} = inf
c∈L
Fic
Zc , 1

.
Now, the right hand side of Equation (17), as a hom-object in [0, 1]J , is the mini-
mum over the objects of J which are 1 and 2. Using f = F(1) and g = F(2), we
have:
[0, 1]J (W, bL(Z, F)) = min

w1, inf
c∈L
 fc
Zc, 1

,

w2, inf
c∈L
n gc
Zc, 1
o
= inf
c∈L
 fc
w1Zc,
gc
w2Zc, 1

.

18
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
It remains to check that the assignment c 7→min
n
fc
w1, gc
w2 , 1
o
is indeed a [0, 1]-
copresheaf; that is, that L(c, d) ≤[limW F(c), limW F(d)], or equivalently L(c, d) limW F(c) ≤
limW F(d), for all c, d ∈L. This desired inequality follows from the simple ob-
servation that,
L(c, d) lim
W F(c) = min
L(c, d)f(c)
w1
, L(c, d)g(c)
w2
, L(c, d)

≤min
f(d)
w1
, g(d)
w2
, 1

≤min
f(d)
w1
, g(d)
w2
, 1

= lim
W F(d),
where the ﬁrst inequality follows from the assumption that f and g are [0, 1]-
copresheaves.
□
Keeping in mind that products in intuitionist logic serve as a kind of conjunction,
let’s look closer at the weighted product (w1, f) × (w2, g) in the case f and g are
representable. Fix a pair of expressions x and y in L and a pair of nonzero weights
w1, w2 ∈[0, 1]. The weighted product (w1, hx) × (w2, hy): L →[0, 1] assigns to
an expression c:
((w1, hx) × (w2, hy))(c) = min
hx(c)
w1
, hy(c)
w2
, 1

.
(18)
Remembering Equation (12), the value of this minimum depends on whether the
expressions x and y are jointly contained within c, and whether π(c|x) ≤w1 and
π(c|y) ≤w2.
((w1, hx) × (w2, hy))(c) =



min
n
π(c|x)
w1 , π(c|y)
w2 , 1
o
if x ≤c and y ≤c
0
otherwise.
(19)
The support of this copresheaf thus coincides with the support of logical “and”
in Boolean logic. As the weights decrease, the values of the quotients in Equation
(19) increase and thus contribute less to the value of w1hx(c)×w2hy(c), which is a
minimum. The copresheaf (w1, hx)×(w2, hy) captures something like a weighted
conjunction.
It’s worth looking closer when the weights are w1 = w2 = 1. In this case,
we denote the weighted product simply by f × g. This weighted product when
the weights both equal 1 works a lot like an ordinary product in the sense that
morphisms into the product correspond precisely to products of morphisms. Here,
“morphisms into the product” means a particular [0, 1]-object, and “products of
morphisms” means the product of [0, 1]-objects in [0, 1], which recall is a mini-
mum. That is,
Lemma 4. For any copresheaves f, g, h: L →[0, 1] we have
bL(h, f × g) = bL(h, f) × bL(h, g).

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
19
Proof. We compute:
bL(h, f × g) = inf
c∈L{[hc, (f × g)c]}
= inf
c∈L{[hc, min{fc, gc}]}
= inf
c∈L
fc
hc, gc
hc, 1

= min

inf
c∈L
fc
hc, 1

, inf
c∈L
ngc
hc, 1
o
, 1

= bL(h, f) × bL(h, g)
□
Lemma 4 will be helpful when we discuss enriched implications. But before we
do, let’s discuss weighted coproducts.
4.3. Weighted coproducts in bL. Now, let’s look at a simple weighted coproduct
in bL := [0, 1]L. Again, let the indexing category J be the same discrete category
with two objects. Fix a functor of weights W : J op →[0, 1] setting w1 := W(1)
and w2 := W(2) and a diagram F : J →bL with f := F(1) and g := F(2).
Deﬁnition 11. Denote the weighted colimit of F with respect to the weight W by
(w1, f) ⊔(w2, g) := colimW F.
Theorem 3. The weighted colimit (w1, f) ⊔(w2, g): L →[0, 1] is given by c 7→
max {w1fc, w2gc} .
Proof. Note that J = J op since there are no nonidentiy morphisms in J . Let
Z : L →[0, 1] be any copresheaf. We need to show
(20)
bL(colimW F, Z) = [0, 1]J (W, bL(F, Z)).
Substituting the claimed colimit into the left hand side and evaluating at an object
i in J yields
bL(colimW Fi, Z) = inf
c∈L{[max {w1fc, w2gc} , Zc]}
= inf
c∈L

Zc
max {w1fc, w2gc}, 1

= inf
c∈L
 Zc
w1fc, Zc
w2gc

.
Evaluating bL(F−, Z) at i = 1, 2 yields
bL(Fi, Z) = inf
c∈L{[Fic, Zc]} = inf
c∈L
 Zc
Fic, 1

.

20
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
Using f = F(1) and g = F(2) the right hand side of Equation (20) is
[0, 1]J (W, bL(F, Z)) = min

w1, inf
c∈L
Zc
fc , 1

,

w2, inf
c∈L
Zc
gc , 1

= inf
c∈L
 Zc
w1fc, Zc
w2gc, 1

.
It remains to check that the assignment c 7→max {w1fc, w2gc} is indeed a [0, 1]-
copresheaf; that is, that L(c, d) ≤[colimW F(c), colimW F(d)], or equivalently
L(c, d) colimW F(c) ≤colimW F(d), for all c, d ∈L. This desired inequality
follows from the simple observation that
L(c, d) colimW F(c) = max {L(c, d)w1f(c), L(c, d)w2g(c)}
≤max
f(d)
w1
, g(d)
w2

= colimW F(d),
where the middle inequality follows from the assumption that f and g are [0, 1]-
copresheaves. This proves the copresheaf deﬁned by c 7→max {w1fc, w2gc}
satisﬁes the universal property to be the claimed weighted coproduct.
□
The support of the copresheaf (w1, f) ⊔(w2, g) coincides with the support of
logical “or,” and this matches the intuition that colimits capture something of dis-
junction. Here, as the weights decrease, the corresponding cofactors contribute less
to the weighted coproduct.
Now, let’s now turn to a discussion of implication in the enriched setting.
4.4. Enriched implication. Recall from Equation (1) in Section 1.3.3 that the
internal hom between a pair of ordinary copresheaves G, H : C →Set is the co-
presheaf [G, H] deﬁned on objects by c 7→bC(hc × G, H), which is the set of
natural transformations from hc × G to H.
Replacing the base category of sets with the unit interval gives the enriched
version.
Deﬁnition 12. For any f, g: L →[0, 1] let [f, g]: L →[0, 1] be deﬁned by
[f, g](c) := bL(hc × f, g).
where hc × f is the product deﬁned above Lemma 4.
Lemma 5. For any f, g: L →[0, 1] the function [f, g]: L →[0, 1] is, in fact, a
[0, 1]-copresheaf.
Proof. We need to show that L(c, d) ≤[[f, g](c), [f, g](d)], which is equivalent to
showing that L(c, d)[f, g](c) ≤[f, g](d). Start with the fact that g is a copresheaf
to get L(c, d) ≤[g(c), g(d)] which is equivalent to L(c, d)g(c) ≤g(d) which by
the enriched Yoneda Lemma is equivalent to L(c, d) bL(hc, g) ≤bL(hd, g). This is,
in turn, equivalent to L(c, d)

bL(hc, g) × bL(f, g)

≤bL(hd, g) × bL(f, g). Using
Lemma 4, we can rewrite this inequality as L(c, d) bL(hc × f, g) ≤bL(hd × f, g),

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
21
which is equivalent to L(c, d) ≤[ bL(hc×f, g), bL(hd ×f, g)] = [[f, g](c), [f, g](d)]
as needed.
□
We have an enriched functor (f × −): bL →bL and Deﬁnition 12 presents [f, −]
which is enriched right adjoint to −×f. The enriched Yoneda Lemma (Theorem 1)
says bL(hc, [f, g]) = [f, g](c) and by deﬁnition, we have [f, g](c) = bL(hc × f, g).
The equality bL(hc, [f, g]) = bL(hc ×f, g) for all representables hc implies equality
for all copresheaves h:
bL(h × f, g) = bL(h, [f, g]).
That is, [f, g] serves as an internal-hom for bL making it enriched Cartesian closed.
Now, let’s take a closer look at this internal hom [f, g] when f and g are repre-
sentable.
Theorem 4. For any x, y ∈L, we have
(21)
[hx, hy](c) = inf
d∈L

π(d|y)
min{π(d|c), π(d|x)}, 1

.
Proof. We compute
[hx, hy](c) = bL(hc × hx, hy)
= inf
d∈L {[(hc × hx)(d), hy(d)]}
= inf
d∈L

π(d|y)
π(d|c) × π(d|x), 1

.
= inf
d∈L

π(d|y)
min{π(d|c), π(d|x)}, 1

.
For the last equality, remember that the categorical product in the unit interval is
the minimum.
□
Note that if there is a text c that contains x but does not contain y, then [hx, hy](c) =
0 for the inﬁmum in Equation (21) is realized when d = c: the numerator π(c|y) =
0 and the denominator π(c|c) × π(c|x) = π(c|x) ̸= 0. So, if [hx, hy](c) ̸= 0,
then within the context c, either c does not contain x or c contains both x and y,
capturing a quantitative sort of implication.
Deﬁnition 13. For any expressions x, y ∈L, deﬁne the implication x ⇒y to be
the copresheaf [hx, hy]: L →[0, 1].
5. A METRIC SPACE INTERPRETATION
It is sometimes desirable to work in a category M that is a slight variant of the
syntax category L. One can get from L to M by applying the negative logarithm
to each hom object. First notice the set of nonnegative extended reals [0, ∞] to-
gether with addition of real numbers is a commutative monoid with unit 0, where
a + ∞:= ∞and ∞+ a := ∞for all a. If one further speciﬁes a morphism
from a to b whenever b ≤a, then [0, ∞] is a commutative monoidal preorder.
As a category [0, ∞], like the unit interval, is also monoidal closed as well as

22
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
red
red
ruby
red
idea
FIGURE 3. When viewing language as a generalized metric space,
a text such as red is identiﬁed with its corresponding [0, ∞]-
copresheaf dM(red, −). Expressions that are unlikely continua-
tions of red are therefore far away, whereas expressions that are
more likely to be continuations of red are closer to it.
complete and cocomplete: the internal hom is given by truncated subtraction,
[a, b] := max{b −a, 0}, and the limit of any diagram is supremum of the num-
bers in the diagram, while the colimit is given by the inﬁmum. In particular, the
categorical product and coproduct are respectively given by a × b = max{a, b}
and a⊔b = min{a, b} for all a and b in [0, ∞]. The function −ln: [0, 1] →[0, ∞]
deﬁnes an isomorphism of commutative monoidal preorders, the inverse of which
[0, ∞] →[0, 1] is the map a 7→exp(−a), and both maps are continuous and
co-continuous isomorphisms of categories.
By applying −ln to morphisms of L we thus obtain a new category M, en-
riched over the commutative monoidal preorder [0, ∞], having the same objects as
L and where the hom object between any pair of expressions x and y is given by
M(x, y) := −ln L(x, y). It is then straightforward to check that 0 ≥M(x, x)
for all expressions x and moreover that M(x, y) + M(y, z) ≥M(x, z) for all
expressions x, y, z thus showing that M is indeed a [0, ∞]-category. As was the
case in L, both inequalities in M are in fact equalities.
Categories enriched over [0, ∞] are typically called generalized metric spaces
[Law73, Law86] since composition of morphisms is precisely the triangle inequal-
ity, though notice that symmetry is not required as with usual metrics. Even so, we
embrace the generalized metric space point of view and will denote hom objects
in M by dM(x, y) := M(x, y), thinking of dM as deﬁning a kind of distance
between texts. Texts that are likely extensions of other texts have small distances
from the texts they extend, and texts that are not extensions of one another are in-
ﬁnitely far apart. Figure 3 illustrates a picture one might have in mind. Paths in this
generalized metric space go in only one direction and represent stories: you begin
somewhere, there are expectations of where the story is going, the story continues,
expectations of where you are going are revised, the story continues, and so on. So
the two categories L and M have the same information, but the ﬁrst emphasizes
the probabilistic point of view while the second gives a geometric picture.
Naturally, the idea that semantic information lies in a copresheaf category ap-
plies in M as well. There is a generalized metric space c
M := [0, ∞]M whose

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
23
objects are [0, ∞]-copresheaves on M, which are functions f : M →[0, ∞] satis-
fying dM(x, y) ≥[fx, fy] = max{fy −fx, 0} for all expressions x and y in the
language. We can think of [0, ∞] as a generalized metric space itself and denote
[a, b] = max{b−a, 0} by d[0,∞](a, b). So a copresheaf is like a metric contraction;
that is, a function f : M →[0, ∞] satsifying d[0,∞](fx, fy) ≤dM(x, y).
Translating Equation (11) tells us that the hom object between any pair of [0, ∞]-
copresheaves f and g is given by
c
M(f, g) = sup
x∈M
d[0,∞](fx, gx) = sup
x∈M
max{gx −fx, 0}
deﬁning the generalized metric on the space of functions that one would expect.
Translating the results from Sections 3 and 4, one ﬁnds the generalized metric
space c
M is also both complete and cocomplete with respect to weighted (co)limits,
and moreover the Yoneda embedding Mop →c
M that maps x 7→dM(x, −) de-
ﬁnes an isometric embedding of (the opposite category of) M as the representable
copresheaves within c
M. Expressions x and y that may be unrelated in M, can
be combined in c
M in ways that are completely analagous to the weighted prod-
ucts and coproducts in bL. See also [Wil13] for a similar discussion of generalized
metric spaces and what is called the categorical Isbell completion.
Theorem 5. For any copresheaves f, g ∈c
M and any weights w1, w2, the weighted
product and coproducts of f and g are given by
((w1, f) × (w2, g))(c) = max{fc −w1, gc −w2, 0}
((w1, f) ⊔(w2, g))(c) = min{fc + w1, gc + w2}
Proof. We translate the results from Sections 4.2 and 4.3. Let f, g: M →[0, ∞]
and w1, w2 ∈[0, ∞]. Deﬁne f ′, g′ : L →[0, 1] and weights w′
1, w′
2 ∈[0, 1] by
setting f ′c = exp(−fc), g′c = exp(−gc), and w′
i = exp(−wi) for i = 1, 2. Then
in bL we have
((w′
1, f ′) × (w′
2, g′))(c) = min
f ′c
w′
1
, g′c
w′
2
, 1

Translating back to c
M by applying −ln yields
((w1, f) × (w2, g))(c) = −ln
 ((w′
1, f ′) × (w′
2, g′))(c)

= −ln

min
f ′c
w′
1
, g′c
w′
2
, 1

= max

−ln
f ′c
w′
1

, −ln
g′c
w′
2

, 0

= max

−ln(f ′c) + ln(w′
1), −ln(g′c) + ln(w′
2), 0
	
= max {fc −w1, gc −w2, 0} .

24
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
Similarly,
((w1, f) ⊔(w2, g))(c) = −ln
 ((w′
1, f ′) ⊔(w′
2, g′))(c)

= −ln
 max

w′
1f ′c, w′2g′c
	
= min

−ln
 w′
1f ′c

, −ln
 w′
2g′c
	
= min

−ln(f ′c) −ln(w′
1), −ln(g′c) −ln(w′
2)
	
= min {fc + w1, gc + w2} .
□
Geometrically, one may think of nonrepresentable copresheaves, such as the
weighted products and coproducts modeling conjoined and disjoined meanings, as
new “points” added in c
M, which are speciﬁed precisely by giving their distance
to all other points in the category M. The general idea that copresheaves f in c
M
are speciﬁed precisely by indicating the distance between f and the representable
copresheaves dM(x, −) suggests that the categorical completion c
M resembles a
kind of metric completion.
5.1. Tropical module structure. We note now that the completion c
M has a semi-
tropical module structure, that is, the structure of a module over the semi-tropical
semi-ring that we deﬁne in the next paragraph. The terminology semi-tropical,
and the connection to categorical cocompletions of generalized metric spaces can
be found in [Wil13]. In our context, the semi-tropical structure is analogous to the
fact that the set of scalar valued functions on a given set has the structure of a vector
space, which it inherits from the ﬁeld of scalars. Here elements of c
M are functions
valued in [0, ∞], which is a semi-tropical semi-ring and therefore c
M inherits the
structure of a semi-tropical module.
Deﬁnition 14. The data ((−∞, ∞], ⊕, ⊙) with operations ⊕and ⊙deﬁned by
s1 ⊕s2 = min{s1, s2} and s1 ⊙s2 = s1 + s2
deﬁnes a semi-ring called the tropical semi-ring or the (min, +) algebra. The sub
semi-ring ([0, ∞], ⊕, ⊙) is called the semi-tropical semi-ring. A module over a
semi-ring is a commutative monoid with an action of the semi-ring.
Theorem 6. The coproduct (with trivial weights w1 = w2 = 0) in c
M makes it into
a commutative monoid. The map [0, ∞] × c
M →c
M deﬁned by (s, f) 7→s ⊙f
where (s ⊙f)(x) = f(x) + s makes M into a module over the semi-tropical
semi-ring.
Proof. We check: (s1 ⊕s2)f(x) = min{s1, s2}f(x) = min{fx + s1, fx + s2}
and (s1 ⊙s2)f(x) = fx + s1 + s2 which is the same as (s1) ⊙(s2 ⊙f)(x) =
(s2 ⊙f)(x) + s1 = fx + s2 + s1.
□

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
25
In fact, the formulas for the weighted coproduct in c
M can be re-expressed as a
tropical linear combination:
((w1, f) ⊔(w2, g))(c) = min {fc + w1, gc + w2} = w1 ⊙fc ⊕w2 ⊙gc.
We have a hunch that tropical geometry provides more than merely a differ-
ent language to express the results described in bL. Over the last twenty years,
tropical geometry has been an area of active research. One thing it provides is
a way to transform non-linear algebraic geometry to piecewise linear geometry
[Mac12, BIMS15, DS04b, DS04a, Yos11, SS09, SS03, Vir01]. The passage is
achieved by considering usual polynomials in several variables and replacing the
additions and products by their tropical counterparts. The zero set of the original
polynomial is mapped to the singular set of the tropical one which is a piecewise
linear space. Semi-tropical module structures on presheaves on generalized metric
spaces, along with a variation where min is replaced with max, were studied in
[Wil13].
A potentially relevant discovery is a tropical module structure [DS04b, DS04a,
SS09] generated by distance functions, like the ones we have in this paper, but
coming from a symmetric metric and satisfying a tropical Pl¨ucker relation.
A
one-dimensional tropical polytope arises, which realizes the metric deﬁned by the
distance functions as a tree metric. The achievement here is that extra limit points
are added in precise locations, indicating branching points, so that the distances be-
tween leaves are realized as distances along tree paths connecting them. This result
is used in phylogenetics for the production of trees encoding common ancestors of
species using distances deﬁned by their DNA [SS09, DS04c]. We conjecture that
a similar result for the metric semantic category c
M will give rise to phylogenetric
structures that work like metric knowledge graphs.
More speculatively, tropical structures might provide insights about how large
language models actually learn semantic information. Recently, it has been dis-
covered [ZNL18, MCT21, CM19, SM19] that feedforward neural networks with
ReLU activation functions compute tropical rational maps, opening a new way to
study the mathematical structure underlying them. LLMs use neural architectures
to learn probabability distributions on text continuations. That is, LLMs learn the
syntax category L. It would be very interesting to determine if there is any link
between the tropical module structures in the metric semantic language category
c
M and some tropical rational maps computed within the neural architectures of
LLMs.
6. CONCLUSION
Knowing how to continue texts implies a good deal of semantic information. So
one might think that it would be very difﬁcult to learn the syntactic language cate-
gory L which encodes probability distributions on text continuations. Two intelli-
gent researchers debating ﬁve years ago could reasonably disagree about whether
or not a computer model that coherently continues texts would require, as part of
its training input, some semantic knowledge. However, existing LLMs prove that is

26
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
not the case. Standard machine learning techniques demonstrate that the enriched
category L can be learned in unsupervised way directly from samples of exist-
ing texts. These models have then necessarily learned some semantic information.
In this paper, we provide a mathematical framework for where, mathematically
speaking, that semantic information lives. Namely, it lives in the category of [0, 1]-
copresheaves on L. We then described a number of categorical constructions that
deﬁne meaningful operations on that semantic information.
6.1. Applications and future directions. There are several near-term applica-
tions of the work described in this paper. One is to study the architectures of trained
LLMs, like GPT3, which have successfully learned probability distributions on text
continuations. In the language of this paper, these distributions are precisely the
representable enriched copresheaves. One can look for other enriched categorical
structures in these architectures and since tropical structures appear in both feed-
forward ReLU networks and the category of copresheaves, they might be a place
to start. Then the work in this paper could lead to well-deﬁned mathematical op-
erations on the parameters of trained language models that allow users to access,
combine, and manipulate the semantic knowledge that is mathematically implicit
in models that have learned to continue texts. For a simple example, the concept
of a gender neutral pronoun could be created by taking the coproduct of the co-
presheaves representing “he” and “she.” Also, weighted limits and colimits allow
one to build concepts from multiple texts with precise shapes and weights, and by
going back from copresheaves to texts, one would have control of the concepts
contained in generated texts. Realizing the internal hom operation, which is like a
context-sensitive implication operator, would yield a powerful entailment tool, per-
mitting certain texts to be input as given “true” when performing other NLP tasks.
Perhaps it is possible to use internal hom or the polyhedral structures arising from
the tropical modules (like higher dimensional phylogenetic trees) to automatically
create knowledge graphs, or otherwise organize semantic information, from the
parameters of trained LLMs. All of this depends on being able to realize certain
categorical operations within existing architectures. Another direction would be
to design novel architectures which have the built-in capability to implement these
categorical operations. Using density operators to model probability distributions
on text continuations is one idea [BV20]. There are natural operations on density
operators, like taking convex combinations, that correspond to operations on rep-
resentable copresheaves. One can average the density operators representing “he”
and “she” to obtain a density that doesn’t represent any particular word in the lan-
guage, but rather captures the concept “he” ⊔“she.” Density operators also have
spectral structures that can be accessed and manipulated realizing other operations
with categorical interpretations. Furthermore, tensor networks give highly efﬁcient
algorithms for storing and manipulating densities on existing classical hardware,
making a tensor network language model an attractive possibility [MRT21, and
references within].
In short, we have presented a mathematical framework that puts the kind of
syntactical information that large language models learn into an enriched-category

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
27
theoretic setting. Understanding that semantic information resides in a category
of [0, 1]-copresheaves and understanding how categorical operations act on that
information leads to a number of concrete and appealing applications which should
be explored further.
ACKNOWLEDGEMENTS
The authors thank Olivia Caramello, Shawn Henry, Maxim Kontsevich, Laurent
Lafforgue, Jacob Miller, David Jaz Myers, David Spivak, and Simon Willerton
for helpful mathematical discussions. The authors thank Juan Gastaldi and Luc
Pellissier for discussions about their philosophical work [Gas21, GP21] and the
anonymous referees who made suggestions that greatly improved this article.
REFERENCES
[AS14]
Samson Abramsky and Mehrnoosh Sadrzadeh. Semantic Uniﬁcation, pages 1–13.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2014.
[B+20]
Tom Brown et al. Language models are few-shot learners. In H. Larochelle, M. Ran-
zato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc., 2020.
[BIMS15]
Erwan Brugall´e, Ilia Itenberg, Grigory Mikhalkin, and Kristin Shaw. Brief introduction
to tropical geometry, 2015.
[BV20]
Tai-Danae Bradley and Yiannis Vlassopoulos. Language modeling with reduced densi-
ties. arXiv:2007.03834, 2020. To appear in Compositionality.
[CM19]
Vasileios Charisopoulos and Petros Maragos. A tropical approach to neural networks
with piecewise linear activations, 2019.
[CSC10]
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Mathematical foundations for
a compositional distributional model of meaning. arXiv:1003.4394, 2010.
[DS04a]
M. Develin and B. Sturmfels. Tropical covexity erratum. Documenta Mathematica, 9
1-27, 2004.
[DS04b]
Mike Develin and Bernd Sturmfels. Tropical convexity. Documenta Mathematica, 9:1–
27, 2004. Erratum pp. 205–206.
[DS04c]
Mike Develin and Bernd Sturmfels. Tropical convexity. Documenta Mathematica, 9:1–
27, 2004.
[FS19]
Brendan Fong and David I. Spivak. An Invitation to Applied Category Theory: Seven
Sketches in Compositionality. Cambridge University Press, 2019.
[Gas21]
Juan Luis Gastaldi. Why can computers understand natural language? Philosophy &
Technology, 34(1):149–214, 2021.
[GP21]
Juan Luis Gastaldi and Luc Pellissier. The calculus of language: explicit representation
of emergent linguistic structure through type-theoretical paradigms. Interdisciplinary
Science Reviews, 46(4):569–590, 2021.
[Hao21]
Karen
Hao.
“The
race
to
understand
the
exhilarating,
dangerous
world
of
language
AI”.
MIT
Technology
Review,
May
20,
2021.
https://www.technologyreview.com/2021/05/20/1025135/ai-large-language-models-bigs
Accessed June 1, 2021.
[Har54]
Zellig S. Harris. Distributional structure. WORD, 10(2-3):146–162, 1954.
[Hea20]
Will Douglas Heaven. “OpenAI’s new language generator GPT-3 is shock-
ingly good—and completely mindless”. MIT Technology Review, July 20, 2020.
https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-langu
Accessed June 1, 2021.
[Hor04]
Paul Horwich. A use theory of meaning. Philosophy and Phenomenological Research,
68(2):351–372, 2004. http://www.jstor.org/stable/40040682.

28
AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
[JX19]
Kun Jing and Jungang Xu. A survey on neural network language models. arXiv
1906.03591, 2019.
[Kel82]
G.M. Kelly. Basic Concepts of Enriched Category Theory. Lecture note series / London
mathematical society. Cambridge University Press, 1982.
[Law69]
F. William Lawvere. Adjointness in foundations. Dialectica, 23(3/4):281–296, 1969.
[Law73]
F. William Lawvere. Metric spaces,
generalized logic and closed categories.
Rendiconti del seminario matematico e ﬁsico di MIlano,
43:135–166,
1973.
https://doi.org/10.1007/BF02924844.
[Law86]
F. William Lawvere. Taking categories seriously. Revista Colombiana de Matematicas,
20(3-4):147–178, 1986.
[Lei14]
Tom Leinster. Basic Category Theory. Cambridge Studies in Advanced Mathematics.
Cambridge University Press, 2014.
[LS09]
F.W. Lawvere and S.H. Schanuel. Conceptual Mathematics: A First Introduction to
Categories. Cambridge University Press, 2009.
[Mac12]
Diane Maclagan. Introduction to tropical algebraic geometry, 2012. arXiv:1207.1925.
[MCH+20] Christopher D. Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer
Levy. Emergent linguistic structure in artiﬁcial neural networks trained by self-
supervision. Proceedings of the National Academy of Sciences, 117(48):30046–30054,
2020.
[MCT21]
Petros Maragos, Vasileios Charisopoulos, and Emmanouil Theodosis. Tropical geome-
try and machine learning. Proceedings of the IEEE, 109(5):728–755, 2021.
[Met20]
Cade
Metz.
“Meet
GPT-3.
it
has
learned
to
code
(and
blog
and
argue).”.
New
York
Times,
Nov.
24,
2020.
https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html.
Accessed June 1, 2021.
[MM92]
Saunders MacLane and Ieke Moerdijk. Sheaves in geometry and logic: a ﬁrst introduc-
tion to topos theory. Universitext. Springer, Berlin, 1992.
[MM12]
Saunders MacLane and Ieke Moerdijk. Sheaves in Geometry and Logic: A First Intro-
duction to Topos Theory. Universitext. Springer New York, 2012.
[MRT21]
Jacob Miller, Guillaume Rabusseau, and John Terilla. Tensor networks for probabilis-
tic sequence modeling. In Arindam Banerjee and Kenji Fukumizu, editors, The 24th
International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2021, April
13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research,
pages 3079–3087. PMLR, 2021.
[NBvEV16] Rick Nouwen, Adrian Brasoveanu, Jan van Eijck, and Albert Visser. Dynamic Seman-
tics. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Metaphysics
Research Lab, Stanford University, Winter 2016 edition, 2016.
[Pie18]
Paul M. Pietroski. Conjoining Meanings: Semantics Without Truth Values. Oxford Uni-
versity Press, Berlin, 2018.
[Rie14]
Emily Riehl. Categorical Homotopy Theory. New Mathematical Monographs. Cam-
bridge University Press, 2014.
[Rie17]
Emily Riehl. Category Theory in Context. Aurora: Dover Modern Math Originals.
Dover Publications, 2017.
[RNSS18]
Alec
Radford,
Karthik
Narasimhan,
Tim
Salimans,
and
Ilya
Sutskever.
Language
models
are
unsupervised
multitask
learners.
2018.
https://www.cs.ubc.ca/˜amuham01/LING530/papers/radford2018improving.pdf.
[RWC+18]
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. Improving language understanding by generative pre-training. 2018. url-
https://www.cs.ubc.ca/ amuham01/LING530/papers/radford2018improving.pdf.
[SM19]
Georgios Smyrnis and Petros Maragos. Tropical polynomial division and neural net-
works, 2019. arXiv:1911.12922.
[SS03]
David Speyer and Bernd Sturmfels. The tropical grassmannian. Advances in Geometry,
4:389–411, 2003.

AN ENRICHED CATEGORY THEORY OF LANGUAGE: FROM SYNTAX TO SEMANTICS
29
[SS09]
David Speyer and Bernd Sturmfels. Tropical mathematics. Mathematics Magazine,
82(3):163–173, 2009.
[TP10]
P. D. Turney and P. Pantel. From frequency to meaning: Vector space models of se-
mantics. Journal of Artiﬁcial Intelligence Research, arXiv:1003.1141, 37:141–188, Feb
2010.
[Vir01]
Oleg Viro. Dequantization of real algebraic geometry on logarithmic paper. In Carles
Casacuberta, Rosa Maria Mir´o-Roig, Joan Verdera, and Sebasti`a Xamb´o-Descamps,
editors, European Congress of Mathematics, pages 135–146, Basel, 2001. Birkh¨auser
Basel.
[VSP+17]
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, undeﬁnedukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Pro-
ceedings of the 31st International Conference on Neural Information Processing Sys-
tems, NIPS’17, page 6000–6010, Red Hook, NY, USA, 2017. Curran Associates Inc.
[Wik21]
Wikipedia contributors. Formal grammar — Wikipedia, the free encyclopedia, 2021.
[Online; accessed 28-October-2021].
[Wil13]
Simon Willerton. Tight spans, Isbell completions and semi-tropical modules. Theory
and Applications of Categories, 28:696–732, 2013.
[Yos11]
Shuhei Yoshitomi. Generators of modules in tropical geometry, 2011. arXiv:1001.0448.
[ZNL18]
Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural
networks. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th In-
ternational Conference on Machine Learning, volume 80 of Proceedings of Machine
Learning Research, pages 5824–5832. PMLR, 10–15 Jul 2018.

