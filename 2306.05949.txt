Evaluating the Social Impact of Generative AI Systems
in Systems and Society
Irene Solaiman∗
Hugging Face
Zeerak Talat∗
Independent Researcher
William Agnew
University of Washington
Lama Ahmad
OpenAI
Dylan Baker
DAIR
Su Lin Blodgett
Microsoft Research
Hal Daumé III
University of Maryland
Jesse Dodge
Allen Institute for AI
Ellie Evans
Cohere
Sara Hooker
Cohere For AI
Yacine Jernite
Hugging Face
Alexandra Sasha Luccioni
Hugging Face
Alberto Lusoli
Simon Fraser University
Margaret Mitchell
Hugging Face
Jessica Newman
UC Berkeley
Marie-Therese Png
Oxford University
Andrew Strait
Ada Lovelace Institute
Aposotol Vassilev
NIST
Abstract
Generative AI systems across modalities, ranging from text, image, audio, and
video, have broad social impacts, but there exists no official standard for means of
evaluating those impacts and which impacts should be evaluated. We move toward
a standard approach in evaluating a generative AI system for any modality, in two
overarching categories: what is able to be evaluated in a base system that has no
predetermined application and what is able to be evaluated in society. We describe
specific social impact categories and how to approach and conduct evaluations in
the base technical system, then in people and society. Our framework for a base
system defines seven categories of social impact: bias, stereotypes, and representa-
tional harms; cultural values and sensitive content; disparate performance; privacy
and data protection; financial costs; environmental costs; and data and content
moderation labor costs. Suggested methods for evaluation apply to all modalities
and analyses of the limitations of existing evaluations serve as a starting point for
necessary investment in future evaluations. We offer five overarching categories for
what is able to be evaluated in society, each with their own subcategories: trustwor-
thiness and autonomy; inequality, marginalization, and violence; concentration of
authority; labor and creativity; and ecosystem and environment. Each subcategory
includes recommendations for mitigating harm. We are concurrently crafting an
evaluation repository for the AI research community to contribute existing evalua-
tions along the given categories. This version will be updated following a CRAFT
session at ACM FAccT 2023.
∗Both authors contributed equally. Following author order is alphabetical by last name.
Contact information: irene@huggingface.co and zeerak_talat@sfu.ca
Preprint. Under review.
arXiv:2306.05949v1  [cs.CY]  9 Jun 2023

1
Introduction
Understanding an AI system from conception to training to deployment requires insight into aspects
such as training data, the model itself, material infrastructure, and the context in which the system
is deployed. It also requires understanding people, society, and how societal processes, institutions,
and power are changed and shifted by the AI system. Generative AI systems are machine learning
models trained to generate content, often across modalities. For generative AI systems, such as
language models, social impact evaluations are increasingly normalized but there is no standard set
that is applied across many models [238]. Generative AI has been widely adopted for different and
varied downstream tasks by adapting and fine-tuning pretrained models. In this report, we propose a
framework for social impact evaluations of generative AI systems across modalities. We address this
work to three groups of readers: researchers and developers, third-party auditors and red-teamers,
and policymakers who evaluate and address the social impact of systems through technical and
regulatory means. Our goal is to lower the barrier to engage with these topics. We define social
impact as the effect of a system on people and communities along any timeline with a focus on
marginalization, and active, harm that can be evaluated. This paper is concerned with risks that have
already been documented or directly follow from current methods. Impacts on future generations,
such as existential risk, are out of scope. Social impact evaluation covers many overlapping topics.
We propose a technical framework of the aspects of a system that can be evaluated along its lifecycle
from training to deployment.
We focus on generative models across four modalities: text, image, video, and audio. We also
consider potential future modalities, and how to address these. The given categories and methods for
evaluation are based on popularly deployed evaluations in use today and do not exhaustively cover all
methods. A forthcoming updated version of this paper will more rigorously analyze attention to and
gaps in evaluations across modalities and categories.
Social impact evaluations offered in our categorical framework are key to but differ from harmful
impact mitigation and value alignment; evaluations aim to improve understanding of social impact,
not take action. Evaluations can be both quantitative and qualitative, and should seek to capture
nuances in complex social topics [123]. While evaluations that quantify harm and risk of harm make
regulation and mitigation more effective, they can miss nuances. Risk and potential for downstream
harm is dependent on the context with which systems are deployed. Harmful impacts reflected in
generative AI systems are rarely limited to the system itself. Long-term societal inequity, power
imbalances, and systemic injustices [265] feed training data [242], influence system development and
deployment [237], and shape social impact [121]. While technical evaluations can probe and isolate
aspects of social impact in a specific system, more robust evaluation and mitigation encompasses
human and infrastructural social harms.
The pace of capability development outstrips evaluation development; evaluation suites are quickly
saturated in one area or rendered obsolete. The level of attention and resourcing to capability
development often outweighs that given to evaluations. Safety evaluations can also overfit to certain
lenses and geographies, such as evaluating a multilingual system only in the English language. Often
developers and deployers will rely on evaluations built within the same company (e.g. OPT-175B[271]
from Meta’s safety evaluations). There is currently no consensus or governing body to determine
what constitutes the social impact of any AI system. A broader evaluation suite, forthcoming in an
updated version of this paper, can both make these complex evaluations more accessible and unify
metrics across which to compare social aspects across generative AI systems.
2
Background
The social impact aspects of an AI system are often largely dependent on context, from the sector in
which they are developed to the use-cases and contexts in which they are deployed. Base generative
AI systems have no specific predefined application, sector, or use case, making them notoriously
difficult to evaluate [166]. They include but are not limited to generative models such as text-based
language models (BLOOM [30], GPT-3 [40], OPT[271]), text-to-image models (ImaGen [205],
DALL·E[170], Stable Diffusion [200]), and increasingly multimodal models [53] (GPT-4 [171]).
Generative AI systems can be referred to as General Purpose AI Systems (GPAIS): a system capable
of a wide range of tasks that is applicable across sectors and use cases. These systems are popularly
examined for generalization properties and societal impact [35] but evaluations are generally not
2

standardized and do not provide adequate coverage across risks or demographics [81]. Although
there are more common evaluations for performance and accuracy (e.g. GLUE [256]), many of
these evaluations are overrepresented and a select few cannot capture full general capabilities [191].
Social impact is a complex concept and is not fully measurable or quantifiable. Evaluations without
application or deployment context are complex and leave gaps [113], but are necessary.
In tandem with the rise of AI systems’ integration with society, many legal jurisdictions have
begun to propose AI regulation, which include or mention assessing the impact of an AI system.
Regulatory bodies that have announced plans and guidelines skew heavily toward Western and East
Asian governmental bodies:the European Union [74], United States of America [250], Canada [148],
United Kingdom [68], South Korea [196], Japan [240], and China [69]. While many of these proposed
requirements only apply to systems that fall into “high risk” categories as defined by the proposed
regulation, generative AI systems are largely being scoped.
2.1
Methodology
We convened thirty experts across industry, academia, civil society, and government to contribute to
a two-part workshop series. The first workshop created a framework for defining and categorizing
social impacts that can be evaluated. The second workshop examined categories’ ability to be
evaluated, including past approaches to evaluations and metrics, limitations, and future directions
for improvements. For the first workshop, we asked experts to discuss possible impacts of systems
for each of the five modalities of generative systems. For the second workshop, we created meta
categories of impacts and collected existing methods for evaluation within these categories. The
findings from the discussions inform our framework and evaluation method sections. Both workshops
were conducted under modified Chatham House Rules, where contributors could opt in to authorship.
Another workshop in the form of a CRAFT session at ACM FAccT 2023 will inform an updated
version of this paper.
3
Related Work
Toolkits and repositories for evaluating qualitative aspects of AI systems are broad and constantly
evolving. Many are aimed at public agency procurement and deployment. In 2018, AI Now released
their framework for algorithmic impact assessments aimed at public agencies [63]. Many public
interest organizations and government initiatives have since published frameworks and assessment
tools, such as the OECD’s Classification Framework for AI risks [168] and Canada’s Algorithmic
Impact Assessment Tool [247]. The U.S. National Institute of Standards and Technology (NIST)
Artificial Intelligence Risk Management Framework (AI RMF) [159] is also intended to be applicable
to all AI systems, although specific applications to generative AI systems are in progress.
Evaluation suites across system characteristics for specific generative system modalities, such as
language, include Holistic Evaluation of Language Models (HELM) [139], BigBench [232], Language
Model Evaluation Harness [85]. These evaluation suites incorporate capabilities evaluations as well
as evaluations across the categories in this paper, and are similarly living resources. We are not aware
of research on evaluation or an evaluation suite dedicated to social impacts or across modalities.
Technical evaluation suites are often specific to a type of system and harm; for example, biases
in natural language processing systems [33]. Partnership on AI’s ABOUT ML (Annotation and
Benchmarking on Understanding and Transparency of Machine learning Lifecycles) project crafted
a resource library for developers, deployers, and procurers to better document the system life-
cycle [176]. Auditing frameworks (e.g., [190]) are powerful tools that necessarily depend on the sector
of deployment. Increasing literature taxonomizes dangers [26], social impacts [110], sociotechnical
harms [219], and social risks of all [80] or certain generative AI systems like language models [258],
but evaluating these risks and impacts is a complementary yet distinct ongoing research area.
3

4
Categories of Social Impact
We divide impacts into two categories for evaluation: what can be evaluated in a technical system and
its components, and what can be evaluated among people and society. The former section includes
evaluations for base systems and evaluations popularly run or published in top AI conferences.
Base systems refer to AI systems, including models and components, that have no predetermined
application. The latter section examines systems in context and includes recommendations for
infrastructurally mitigating harmful impacts. Both sections can have overlap as the same category
can be evaluated differently in a system (see 4.1.4 Privacy and Data Protection) and impact on people
and society (see 4.2.1.3 Personal Privacy and Sense of Self).
4.1
Impacts: The Technical Base System
Below we list the aspects relatively able to be evaluated in a generative system from training to
deployment testing. These categories, and the suggested evaluations afford application and use-case
independent tests of the base model. Evaluation of base systems can be qualitative or quantitative,
but only provide a narrow insight into the described aspect of the type of generative AI system. The
depth of literature and research on evaluations differ by modality, but the themes for evaluations can
be applied to most systems.
The following categories are high-level, non-exhaustive, and present a synthesis of the findings across
different modalities. They refer solely to what can be evaluated in a base technical system:
• Bias, Stereotypes, and Representational Harms
• Cultural Values and Sensitive Content
• Disparate Performance
• Privacy and Data Protection
• Financial Costs
• Environmental Costs
• Data and Content Moderation Labor
4.1.1
Bias, Stereotypes, and Representational Harms
Generative AI systems can embed and amplify harmful biases that are most detrimental to marginal-
ized peoples. Categories of bias, from system to human to statistical, interact with each other and
are intertwined [211]. For bias evaluations that do not narrowly capture biases as they occur in
Generative AI systems, it is necessary to consider work outside of the field of question. For instance,
for natural language processing, bias evaluations must seriously engage with the relationship between
the modality (i.e. language) and social hierarchies [33]. When thinking about representational
harms [125], it is also important to consider the extent to which any representation could confer harm
(see 4.2.2.2 Long-term Amplifying Marginalization by Exclusion (and Inclusion)).
Although bias evaluations in data have been subject to a large body of research, bias is not only
a “data problem.” Biases are not only introduced in the data pipeline but throughout the entire
machine learning pipeline [237]. The overall level of harm is also impacted by modeling choice
[108]. These can include choices about many stages of the optimization process [237, 129]; privacy
constraints [24], widely used compression techniques [109, 15, 169] and the choice hardware [273]
have all been found to amplify harm on underrepresented protected attributes [28]. The geographic
location, demographic makeup, and team structures of researcher and developer organizations can
also introduce biases.
What to Evaluate
While the degree of harm depends on many factors from type of output to the
cultural context of training and deployment, focus on bias evaluations has centered on protected
classes as defined by United States [77] and United Nations [249] guidelines. These guidelines
are non-exhaustive and harms exist outside of their proposed categories but can be evaluated by
adding categories. For instance, for generative AI systems developed on data from the South Asian
subcontinent, it may also be useful to include considerations of caste bias [217]. Additional harmful
biases include misrepresentations of humans generally, such as associated humans or a group of
humans with other animals [223].
4

Popular evaluations for biases use association tests [46] or examine stereotypes [157, 156, 138],
correlations and co-occurrences [272], and sentiment analysis [66]. In language, these evaluations
can occur at the word or sentence level. For images, additional tools such as captioning systems can
be used. For certain modalities, such as language, biases can be represented differently [142]. Across
modalities, biases can be evaluated using intrinsic and extrinsic methods [91], where the former seeks
to evaluate biases within model weights and the latter evaluates the expression of biases in the outputs
for downstream tasks (e.g. captioning). Evaluations can also be specific to a certain function of a
modality, such as question-answering in language [175].
Limitations
There are often legal obstacles around collecting certain protected attributes, which
leads to selection bias in the availability of protected features annotations. Moverover, as geographic
and cultural contexts shift, so do the meaning of different categories. Annotators often have different
perceptions of concepts like race or are influenced by their own lived experience when categorizing
protected categories.
Due to its contextual and evolving nature [83], bias evaluation cannot be fully standardized and
static [117]. Protected class categorization itself cannot be exhaustive and can be inherently harmful.
By framing work within such considerations, it is possible to delineate which qualities that are
evaluated for. Precisely identifying which framing is used for bias evaluation and mitigation can help
delineate the particular areas where robust evaluation has been done, where developers expect biases
to arise, and which groups for whom they believe biases are unlikely to arise, or bias evaluations have
not been as rigorous, e.g., due to a lack of bias evaluation resources. Certain protected classes, such
as race and gender, are often more represented in publications and publication venues around biases
of (generative) systems. Many evaluations focus on distinct or binary groups, due to the complexity
of operationalising intersectionality [257, 133]; in many cases, assumptions used to simplify for the
sake of mathematical notation and interpretation result in obscuring the very phenomena they seek to
describe [64].
Obtaining data for bias evaluations is not straightforward, as there are often legal obstacles around
collecting data about protected attributes, which leads to selection bias in the availability of protected
features annotations [21, 252]. Moverover, as geographic and cultural contexts shift, so do the
meaning of different categories [206, 112] and must be interpreted according to their local meaning.
Annotators often have different perceptions of concepts like race or are influenced by their own lived
experience [234] when categorizing protected categories [187].
When conducting association tests, although based in human associations, one should remain aware
that general societal attitudes do not always represent subgroups of people and cultures. Evaluations
for stereotype detection can raise false positives and can flag relatively neutral associations based in
fact (e.g. population x has a high proportion of lactose intolerant people) [238]. Whenever additional
tooling is used to aid in identifying biases, e.g., the use of an image captioning system in addition to
the base system, tool added introduces its own biases, similarly introduced in each step of developing
the tool, which are embedded into the ecosystem of the biases of the system under study.
4.1.2
Cultural Values and Sensitive Content
Cultural values are specific to groups and sensitive content is normative. Sensitive topics also vary by
culture and can include hate speech, which itself is contingent on cultural norms of acceptability [242].
Abusive and offensive language are a large umbrella for unsafe content, which can also include
abuse and hate speech[151, 236]. What is considered a sensitive topic, such as egregious violence or
adult sexual content, can vary widely by viewpoint. Due to norms differing by culture, region, and
language, there is no standard for what constitutes sensitive content.
Increasing politicization of model training and outputs, as seen in projects such as with projects like
RightWingGPT [202], raises urgency in evaluating the complexity of political values. Distinct cultural
values present a challenge for deploying models into a global sphere, as what may be appropriate in
one culture may be unsafe in others [238]. Generative AI systems cannot be neutral or objective, nor
can they encompass truly universal values. There is no “view from nowhere”; in evaluating anything,
a particular frame of reference [207] is imposed [237].
4.1.2.1
Hate, Toxicity, and Targeted Violence
Beyond hate speech and toxic language, genera-
tions may also produce harmful biases [87], stereotypes [165] (overlapping with 4.1.1Bias, Stereo-
5

types, and Representational Harms), violent or non-consensual imagery or audio, and physically
threatening language, i.e., threats to the lives and safety of individuals or groups of people. Although
base systems cannot act on the content that is generated by them, they can still inflict harms upon
viewers who are targeted, help normalize harmful content, and aid in the production of harmful
content for distribution (e.g., misinformation and non-consensual imagery).
In an early example, Microsoft’s Tay bot showed these exact vulnerabilities and generated violent
language such as Holocaust denial and threats to women and people of color within 24 hours of its
release [255]. Recent harms have proved fatal [268]. For these reasons, it is of the utmost importance
that generative AI systems are evaluated for their potential to generate harmful content and how such
content may be propagated without appropriate measures for identifying and addressing them.
What to Evaluate
Cultural values can highlight specific prominent topics according to a given
application and modality. For example, An image generative model prompted on politics can
segment generations with disparate geographic and political party, building, infrastructural, and figure
representation, alongside ideological cues. Cultural sensitive topics can range from physical aspects of
human appearance and health to less visible or descriptive aspects of human behavior and emotional
expression. A non-exhaustive categorical framework and human reviewed evaluations [228] can
capture some aspects of culture.
Hate, Toxicity, and Targeted Violence and safe to hurtful outputs can be evaluated in context of safe
discussions, toxicity metrics [87, 182], hurtfulness [165], and level of offense [71] for language.
Nonconsensual generations of existing people should be evaluated with the person themselves.
Research toward approaches to characterizing harmful content is ongoing by modality [193].
Training data, including fine-tuning and other data can be examined to explain many of the behaviors
of large data-driven generative systems, and particularly their potentially harmful behaviors; what
associations in the training corpus led to toxic behaviors, whether generated information corresponds
to trustworthy training sources, examining whether the data collection abides by ethical frameworks
for the rights of data subjects, etc. Different levels of access and description of the training data can
help answer these questions with due consideration for privacy needs [183].
Limitations
Evaluating cultural values requires examining an infinite list of topics that contribute
to a cultural viewpoint. Human-led evaluations [173] for hateful and sensitive content can have a
high psychological cost, as seen in content moderation labor (see 4.1.7 Data and Content Moderation
Labor). The types and intensity of sensitive content that may be produced across modalities may vary.
For example, the creation of hate speech and hateful imagery may overlap in their target, yet provide
different levels of psychological distress in generated content. For evaluations which rely on a third
party API, such as the many benchmarks which leverage Google Perspective API [182] for toxicity
detection, it is important to make sure comparisons between models are standardized using the same
version of the API to avoid reproducibility issues [185].
4.1.3
Disparate Performance
In the context of evaluating the impact of generative AI systems, disparate performance refers to AI
systems that perform differently for different subpopulations, leading to unequal outcomes for those
groups. A model that is trained on a dataset that is disproportionately skewed towards one particular
demographic group may perform poorly for other demographic groups [43].
Data availability differs due to geographic biases in data collection [216], disparate digitization of
content globally due to varying levels of internet access for digitizing content, and infrastructure
created to support some languages or accents over others, among other reasons. Much of the training
data for state of art generative models comes from the internet. However, the composition of this
data reflects historical usage patterns; 5% of the world speaks English at home, yet 63.7% of internet
communication is in English [197]. This has implications for downstream model performance where
models underperform on parts of the distribution underrepresented in the training set. For example,
automatic speech recognition models (ASR), which convert spoken language (audio) to text have been
shown to exhibit racial disparities [130], forcing people to adapt to engage with such systems [100]
and has implications (see 4.2.3.2 Imposing Norms and Values) for popular audio generation accent
representation.
6

Interventions to mitigate harms caused by generative AI systems may also introduce and exhibit
disparate performance issues [238]. For instance, automated hate speech detection driven by annotated
data with an insensitivity to dialect differences can amplify harm to minority or marginalized groups
by silencing their voices (see 4.2.2.1 Community Erasure) or incorrectly labeling their speech as
offensive [67]. This therefore requires that the interventions used are documented for which particular
populations and norms that they seek to cover, and which they do not.
What to Evaluate
Dataset composition and decisions can give insight to subsequent performance.
The language, speech, and imagery included in datasets as well as decisions made about that data,
including filtering and reward modeling, will impact how the model performs for different groups or
categories of concepts associated with groups. Generative image models for example, may output
varying quality generations when producing different concepts, with quality referring to photorealism,
aesthetic quality, and conceptual richness [170].
Evaluating model generations across subpopulation languages, accents, and similar topics using the
same evaluation criteria as the highest performing language or accent can illustrate areas where there
is disparate performance and can help document areas for further model development and mitigation
work.
Limitations
Similar limitations that lead to disparate system performance contribute to disparate
attention to evaluations for different groups. Performance evaluations for similar tasks in non-English
languages will vary by the amount of resourcing for a given language. More spoken and digitized
languages may have more evaluations than lower-resource languages.
4.1.4
Privacy and Data Protection
Examining the ways in which generative AI systems providers leverage user data is critical to
evaluating its impact. Protecting personal information and personal and group privacy depends
largely on training data, training methods, and security measures. The data on which the system
was trained or adapted should be consensually and lawfully collected and secured and secured
under the rules of the jurisdictions in which the data subjects and the entity collecting the data are
based. Moreover, there are strong intellectual property and privacy concerns, with generative models
generating copyrighted content [254] and highly sensitive documents [49] or personally identifiable
information (PII), such as phone numbers, addresses and private medical records.
Providers should respect the consent and choices of individuals for collecting, processing, and sharing
data with external parties, as sensitive data could be inevitably leveraged for downstream harm such
as security breaches, privacy violations, and other adversarial attacks. Oftentimes, this might require
retroactively retraining a generative AI system, in accordance with policy such as the California
Consumer Privacy Act (CCPA) [4].
What to Evaluate
Although some evaluations operate as a proxy for a system’s ability to generate
copyrighted or licensed content found within pretraining data [139], there is great potential for more
comprehensive evaluations.
Memorization of training examples remains a critical security and privacy concern [49, 50]. Address-
ing this issue may yield improvements in performance for various downstream applications [172].
Additionally, generative AI systems providers may maintain the right to authorize access of user
data to external third-parties, such as human annotation vendors. For sharing data to third-parties,
data providers should ensure that only lawful data is shared, consent for sharing is obtained from
data subjects, and that shared data does not contain any private, personally identifiable, or otherwise
sensitive data.
Limitations
Generative AI systems are harder to evaluate without clear documentation, systems
for obtaining consent (e.g., opt-out mechanisms), and appropriate technical and process controls to
secure user data that can threaten the privacy and security of individuals. Thus, robustly evaluating
privacy risks will often require full process and governance audits that go beyond evaluating artifacts
in isolation. Rules for leveraging end-user data for training purposes are unclear, where user prompts,
geolocation data, and similar data can be used to improve a system. The immense size of training
datasets [118] makes scrutiny increasingly difficult.
7

4.1.5
Financial Costs
The estimated financial costs of training, testing, and deploying generative AI systems can restrict the
groups of people able to afford developing and interacting with these systems. Concretely: sourcing
training data, computing infrastructure for training and testing, and labor hours contribute to the
overall financial costs. These metrics are not standard to release for any system, but can be estimated
for a specific category, such as the cost to train and host a model.
What to Evaluate
Researchers and developers can estimate infrastructure, hardware costs, and
hours of labor from researchers, developers, and crowdworkers. Popular existing estimates focus on
compute using low-cost or standard pricing per instance-hour [137]. Research lowering training costs
also show tracking compute cost by day as the model trains and scales [253]. Frameworks break
down cost per system component: data cost, compute cost, and technical architecture of the system
itself [163]. Other variables used to calculate cost include size of dataset, model size, and training
volume [218].
Limitations
Only accounting for compute cost overlooks the many variables that contribute to a
system’s training. Costs in pre- and post-deployment, depending on how a system is released [227]
are also difficult to track as cost variables may not be directly tied to a system alone. Human labor and
hidden costs similarly may be indirect. Costs also change over time and with a changing economy
for all components. Finally, it is necessary to keep track of the changes of costs and economy of
components over time.
4.1.6
Environmental Costs and Carbon Emissions
The computing power used in training, testing, and deploying generative AI systems, especially
large scale systems, uses substantial energy resources and thereby contributes to the global climate
crisis by emitting greenhouse gasses [233]. While the environmental costs of compute has become
an area of active research, with workshops dedicated to the question, the environmental costs of
manufacturing hardware remains under-explored. One potential reason for this discrepancy may be
that estimating compute and energy costs, while complex, is a comparably transparent task compared
to tracing the emissions of the of emissions throughout the manufacturing process. However, recent
estimates suggest that the manufacturing process have substantial environmental costs [96]. Overall,
information about emissions is scarce and there is no consensus for what constitutes the total carbon
footprint of AI systems.
What to Evaluate
The existing efforts in evaluating the energy consumed and carbon emitted
by AI systems have pursued two main directions: the creation of tools to evaluate these impacts
and empirical studies of one or several models. For instance, [132] proposes both a web-based and
programmatic approach for quantifying the carbon emissions of models, meanwhile [104] proposes
an experiment-impact-tracker, for energy and carbon usage reporting research. Other popular work
includes conversion based on power consumed in the U.S. [233] and examining environmental impact
across compute-related impacts, immediate impacts of applying AI, and system-level impacts [120].
Existing metrics for reporting range from energy, compute, and runtime, to carbon emissions. CPU,
GPU, and TPU related information such as hardware information, package power draw, GPU
performance state, and CPU frequency, as well as memory usage are additional metrics. In addition
to metrics, consideration of the region/location of the energy grid where the experiment is being run
on is important given significant differences in carbon emissions between energy grids, and informs
the move to run experiments in “clean regions”. Tools such as CodeCarbon can be used to estimate
power consumption [61].
Limitations
There is still a lot of uncertainty around certain variables, such as the relative contribu-
tion of added parameters to their energy consumption and carbon footprint, as well as the proportion
of energy used for pre-training versus fine-tuning models for different tasks and architectures [267].
Conducting further research on these variables can benefit the field both from the perspective of
sustainability and overall efficiency.
8

4.1.7
Data and Content Moderation Labor
Human labor is a substantial component of machine learning model development, including generative
AI systems. This labor is typically completed via a process called crowd computation, where
distributed data laborers, also called crowdworkers, complete large volumes of individual tasks that
contribute to model development. This can occur in all stages of model development: before a model
is trained, crowdworkers can be employed to gather training data, curate and clean this data, or provide
data labels. While a model is being developed, crowdworkers evaluate and provide feedback to model
generations before the final deployed model is released, and after model deployment, crowdworkers
are often employed in evaluating, moderating, or correcting a model’s output. Crowdwork is often
contracted out by model developers to third-party companies.
Two key ethical concerns in the use of crowdwork for generative AI systems are: crowdworkers
are frequently subject to working conditions that are taxing and debilitative to both physical and
mental health, and there is a widespread deficit in documenting the role crowdworkers play in
AI development. This contributes to a lack of transparency and explainability in resulting model
outputs. Manual review is necessary to limit the harmful outputs of AI systems, including generative
AI systems. A common harmful practice is to intentionally employ crowdworkers with few labor
protections, often taking advantage of highly vulnerable workers, such as refugees [119, p. 18],
incarcerated people [54], or individuals experiencing immense economic hardship [98, 181]. This
precarity allows a myriad of harmful practices, such as companies underpaying or even refusing to
pay workers for completed work (see Gray and Suri [93, p. 90] and Berg et al. [29, p. 74]), with no
avenues for worker recourse. Finally, critical aspects of crowdwork are often left poorly documented,
or entirely undocumented [88].
What to Evaluate
Researchers and developers close to the system development should check
that crowdworking is conducted under basic ethical standards, such as the 18 Criteria for Fairer
Microwork proposed by Berg et al. [29, p. 105] in Digital Labour Platforms and the Future of Work
or the Oxford Internet Institute’s Fairwork Principles [75]. Concurrently, researchers and developers
should document the role of crowdwork in all dataset development undertaken during generative AI
systems development, e.g. using frameworks like CrowdWorkSheets [70] and sections 3.3 and 3.4 in
Datasheets for Datasets [86]. Basic details such as crowdworkers’ demographics, the instructions
given to them, or how they were assessed and compensated, are foundational for interpreting the
output of AI systems shaped by this labor [147]. All aspects of data labor should be transparently
reported (as done by Glaese et al. [89], for example), both as a tool for understanding model output
and as a means to audit unethical labor practices.
External evaluators can use evaluation metrics designed specifically around crowdwork, such as
those proposed by Fair Work [75], to evaluate quality of working conditions. Relevant labor law
interventions by jurisdiction may also apply. Since many critical crowdworking jobs and evaluation
of this work involves long-term exposure to traumatic content [199], such as child sexual abuse
material or graphic depictions of violence [181], it may also be necessary to consider professional
support for mental health and practices to limit the degree of exposure in any one work day.
Limitations
The lack of regulation and rules around crowdworker protection for AI contributes to
minimal to no documentation or transparency. The lack of information makes crowdwork difficult to
evaluate. Incentives to conduct crowdwork at a low cost with little transparency contribute to less
literature on evaluating crowdwork. Outsourcing labor also creates barriers to evaluation by further
complicating reporting structures, communication, and working conditions.
9

4.2
Impacts: People and Society
Evaluating the effect AI has on people and societies, and evaluating people and groups themselves
encounters similar challenges as those arising in sampling [20], surveying [126], determining prefer-
ences [270], and working with human subjects [131, 12], in addition to challenges that stem from the
planetary scale at which AI development seeks to be applied for, and therefore comes to engage with
national and global social systems, e.g., economies and cultures. Taxonomies of risks and harms of
generative AI systems [80], including their impacts on human rights [111, 186], strongly overlap with
what should be evaluated. However, most societal impact taxonomies lack evaluations or examples of
evaluating society. We must understand the reason for our evaluation; often we are seeking proof, in
the form of evaluations, that is necessary for further action against harmful impacts.
Concretely when evaluating impact, timing will change how we view a system. What is being trained
on and generated may not reflect the current world in which it is deployed [235]. Further, when
we seek to evaluate society, we cannot escape the ways in which our perception of society, and
society itself, has already been influenced by existing AI and social media tools. In crafting and
conducting evaluations, we can often encroach on others’ privacy and autonomy due to the need for
highly personal information to evaluate how harms are enacted and distributed across populations.
For this reason, it is necessary that any engagements with impact assessments also critically examine
how consent is obtained, and what the limits of consent are, when it comes to being subject to bias
evaluation and assessment. Similarly, impact assessments must also take into consideration the
existing and possible future impacts of being included as a data subject. Participatory justice-led
initiatives provide particularly promising avenues for such considerations and engagements. Long-
term effects of systems embedded in society, such as economic or labor impact, largely require
ideation of generative AI systems’ possible use cases and have fewer available general evaluations.
The following categories are high-level, non-exhaustive, and present a synthesis of the findings across
different modalities. They refer solely to what can be evaluated in a base technical system:
• Trustworthiness and Autonomy
– Trust in Media and Information
– Overreliance on Outputs
– Personal Privacy and Sense of Self
• Inequality, Marginalization, and Violence
– Community Erasure
– Long-term Amplifying Marginalization by Exclusion (and Inclusion)
– Abusive or Violent Content
• Concentration of Authority
– Militarization, Surveillance, and Weaponization
– Imposing Norms and Values
• Labor and Creativity
– Intellectual Property and Ownership
– Economy and Labor Market
• Ecosystem and Environment
– Widening Resource Gaps
– Environmental Impacts
These context-specific categories heavily depend on how generative AI systems are deployed, in-
cluding sector and application. In the broader ecosystem, methods of deployment [229] affect social
impact.
10

4.2.1
Trustworthiness and Autonomy
Human trust in systems, institutions, and people represented by system outputs evolves as generative
AI systems are increasingly embedded in daily life. WIth the increased ease of access to creating
machine generated content, which produce misinformation [260] as a product, distinguishing between
human and machine generated content, verified and misinformation, will become increasingly difficult
and poses a series of threats to trust in media and what we can experience with our own hearing and
vision.
4.2.1.1
Trust in Media and Information
High capability generative AI systems create believable outputs across modalities and level of risk
depends on use case. From impersonation spurring spamming to disinformation campaigns, the
spread of misinformation online can be perpetuated by reinforcement and volume; people are more
likely to believe false information when they see it more than once, for example if it has been shared
by multiple people in their network [179]. This can have devastating real world impacts, from
attempting dangerous COVID-19 treatments [160], to inciting violence [146], and the loss of trust in
mainstream news [95]. The increasing sophistication of generative AI in recent years has expanded
the possibilities of misinformation and disinformation campaigns, and made it harder for people to
know when they should trust what they see or hear [41].
What to Evaluate
Surveying trust can apply to trust in AI systems [184, 107] to output factual
information, trust in researchers, developers, and organizations developing and deploying AI [143],
mitigation and detection measures [222], and trust in overall media and how it is distributed [251].
Trust can be evaluated in the category of information, such as information about democratic and
policy institutions [177]. Evaluations and countermeasures of false and misleading information
remain challenging. There is no universal agreement about what constitutes misinformation and much
of the research on intervention remains siloed [94]. Furthermore, current research efforts towards
watermarking text remain brittle and the area of developing watermarks for machine generated outputs
is an active research area [128].
Mitigation and Interventions
Interventions on technical systems include encouraging people
to shift their attention to the accuracy of posts they might share [180], using crowd-sourced fact
checking [90], and using digital forensics to detect AI-generated content [76]. However, technical
tools such as detection are less accurate as AI systems become more powerful [204].
Emerging legal and regulatory approaches around the world include the EU AI Act, which requires
labeling AI-generated content, and certain U.S. state laws that criminalize non-consensual deepfake
pornography and deepfake content that interferes with elections [38], where lessons can be extrap-
olated to generated AI outputs. Policymakers and developers can also ban use cases where false
outputs have highest risks.
4.2.1.2
Overreliance on Outputs
Overreliance on automation in general is a long-studied problem [174], and carries over in novel and
important ways to AI-generated content [178]. People are prone to overestimate and put a higher
degree of trust in AI generated content, especially when outputs appear authoritative or when people
are in time-sensitive situations [45].
This can be dangerous because many organizations are pursuing the use of large language models
to help analyze information despite persistent flaws and limitations, which can lead to the spread of
biased and inaccurate information [103]. The study of human-generative AI relationships is nascent,
but growing, and highlights that the anthropomorphism [13] of these technologies may contribute to
unfounded trust and reliance [192, 225]. Improving the trustworthiness of AI systems is an important
ongoing effort across sectors [159, 161].
Persistent security vulnerabilities in large language models and other generative AI systems are
another reason why overreliance can be dangerous. For example, data poisoning, backdoor attacks,
and prompt injection attacks can all trick large language models into providing inaccurate information
in specific instances [220].
11

What to Evaluate
For language, in the case of AI chatbots specifically, the conversational interface
can additionally elicit trust and other strong emotions from people, even when they understand the
limitations of the technology [201]. Overreliance on such tools can not only make people prone
to believe inaccurate information, but can also be abused to subtly change or manipulate people’s
behaviors, for example to make them more likely to purchase particular products or even encourage
self-harm [99].
For language models trained on code and code generative systems, inaccurate outputs [60] can
nullify potential benefits. Code generative systems can be evaluated for their limitations [56] and
hazards [127], from alignment questions like producing bugs and harmful biases, to economic and
environmental impacts (see Section 4.1 Impacts: The Technical Base System).
Mitigation and Interventions
There are few protections against these risks. Vulnerability disclo-
sure, bug bounties, and AI incident databases can help report the vulnerabilities and limitations of
generative AI systems. Several components of the EU AI Act may also be helpful, for example re-
quiring labeling of AI-generated content, and prohibiting certain kinds of manipulation. For example,
Section 5.2.2 of the 2021 proposal prohibits "practices that have a significant potential to manipulate
persons through subliminal techniques beyond their consciousness or exploit vulnerabilities of spe-
cific vulnerable groups such as children or persons with disabilities in order to materially distort their
behavior in a manner that is likely to cause them or another person psychological or physical harm.”
The proposal also notes, “Other manipulative or exploitative practices affecting adults that might be
facilitated by AI systems could be covered by the existing data protection, consumer protection and
digital service legislation that guarantee that natural persons are properly informed and have free
choice not to be subject to profiling or other practices that might affect their behavior.” [8]
4.2.1.3
Personal Privacy and Sense of Self
Privacy is linked with autonomy; to have privacy is to have control over information related to oneself.
Privacy can protect both powerful and vulnerable peoples and is interpreted and protected differently
by culture and social classes throughout history [152]. Personal and private information has many
legal definitions and protections globally [2] and when violated, can be distinct from harm [47] and
refer to content that is shared, seen, or experienced outside of the sphere a person has consented to.
What to Evaluate
As seen in the Technical Base System section on 4.1.4 Privacy and Data
Protection, privacy can be evaluated in a system as well as its impacts on society. Impacts [230] and
harms [59] from the loss and violation of privacy are difficult to enumerate and evaluate, such as
loss of opportunity or reputational damage. Harms can lead to shifts in power differentials and less
respect or influence in an affected environment, in addition to personal changes in expectations of
privacy [144] and autonomy. The type of private information violated, such as medical information,
can trigger different impacts and responses.
Mitigation and Interventions
Mitigation first should determine who is responsible for an individ-
ual’s privacy, while recognizing that all individuals may not have the same level of technical or data
literacy. Robustly protecting privacy and autonomy requires both individual and collective action; an
individual must be data-conscious in addition to technical and policy privacy protection provisions
[18]. Outside of an individualistic framework, certain rights such as refusal [58] and inclusion also
requires consideration of individual self-determination: establishing how an individual wants to
interact with technology.
Technical methods to preserve privacy in a generative AI system, as seen in privacy-preserving ap-
proaches to language modeling [39], cannot guarantee full protection. Upholding privacy regulations
requires engagement from multiple affected parties [189] and can protect individuals but fail at
loopholes, as seen with tracking continuing when an individual opts-out [42] from data collection
[140]. Improving common practices and better global regulation for collecting training data can help.
Opt-in approaches can protect individuals but are often not practiced due to economic incentives that
stem from collecting data [244]. Privacy options for users should ease accessibility [263], such as
standardized form factors when users visit a website requesting privacy permissions.
12

4.2.2
Inequality, Marginalization, and Violence
Generative AI systems are capable of exacerbating inequality, as seen in sections on 4.1.1 Bias,
Stereotypes, and Representational Harms and 4.1.2 Cultural Values and Sensitive Content, and
Disparate Performance. When deployed or updated, systems’ impacts on people and groups can
directly and indirectly be used to harm and exploit vulnerable and marginalized groups.
4.2.2.1
Community Erasure
Biases in a system’s development process and safety provisions for generative AI systems, such
as content moderation, can lead to community erasure [97]. Avoiding the generation of the harms
outlined is seen as a generally desirable outcome. However, the removal of harmful content can come
with its own costs of lower general performances for sub-populations that use models for generation
[269]. Mitigation thus currently serves as a double-edged sword, where removal of toxic content also
has negative implications, in particular for marginalized communities. Both the benefits and the costs
of content moderation are unequally distributed. The automatic systems that remove undesirable
content can perform next to randomly or be harmful for marginalized populations [208], while the
selection criteria for what constitutes safe content are aligned with technical safety and mitigation
decisions. These impacts compound to make marginalized populations pay a greater cost for an
intervention that they benefit from less.
The production of harmful content is currently mitigated using combinations of four methods: data
sourcing [30]; human moderation of content included in training data [65]; automated moderation of
content included in training data [101]; and keyword deny-lists [149]. Given that the exclusion of
harmful content within datasets stand to create distinct harms to marginalized communities, efforts
towards mitigation of generating harmful content becomes a question of the politics of classification
[36, 135, 72, 242] and its potential harms.
What to Evaluate
Evaluating Disparate Performance once systems have undergone safety provi-
sions can give signal to possible erasure. Accounting for the demographics and composition of human
crowdworkers can also provide information [209] about subsequent impacts. Longer-term impacts of
erasure depend on the system’s deployment context, leading to opportunity loss or reinforced biases
and norms.
Mitigation and Interventions
Better democratic processes for developing and deploying systems
and safety provisions such as content moderation should work with marginalized populations. This
should include more investment in representative crowdworkers and appropriate compensation and
mental health support. Lessons from social media content moderation can apply, such as working
with groups who have been erased and documenting patterns of erasure to improve future approaches
[213].
4.2.2.2
Long-term Amplifying Marginalization by Exclusion (and Inclusion)
Biases, dominant cultural values, and disparate performance seen in lack of representation in training
and development of generative AI systems can exacerbate marginalization when those systems
are deployed. For example, increasing resourcing and performance for already highly resourced
languages reinforces those languages’ dominance.
Inclusion without consent can also harm marginalized groups. While some research strives to improve
performance for underrepresented Indigenous languages [116], the same Indigenous groups resist AI
approaches to use of their language [158]. Profit from Indigenous languages and groups who have
been systematically exploited continues directly and indirectly.
Disparate Performance in Critical Infrastructure
Generative AI use in critical infrastructure
that directly impacts human wellbeing can also be classified as high-risk use cases. This includes use
in judicial systems, healthcare such as mental health and medical advice, and democratic processes,
such as election or political information. An example is generative AI systems used to replace
care work, such as crisis intervention and research [82] and action [153] to use chatbots for eating
disorder prevention. Technical tooling used in human systems and processes that have long-recorded
discrimination patterns [261] can instead exacerbate harm [134].
13

Generative AI used in medical education and potentially in clinical decision-making will continue to
underserve and expose institutionally marginalised individuals and communities to life-impacting
risks. From inaccurate skin cancer diagnosis [262], to the scoring of Black patients in the U.S.
medical system as less sick than the reality of their complex health and resource allocation needs
[167], the use of generative AI in medical settings must be sensitive to existing challenges to equality
within medical practice [114].
What to Evaluate
Systems should again undergo Disparate Performance evaluations once updated
for a high-risk task in critical infrastructure and account for the additional deployment context. Long-
term impacts in addition to marginalization can include erasure. Evaluating marginalization will
depend on context, and should account for marginalization when work by marginalized populations is
less visible or uncredited [264]. Evaluating marginalization impacts on individuals, such as through
health [23], is ongoing research.
Mitigation and Intervention
Improving evaluation work for underrepresented populations and such
as for low-resource languages, and crediting local researchers [34], can help give more information
to disparate performance. Engagement with populations should be done in ways that embody local
approaches [37]. Policies should be crafted to better respect rights to refusal [224]. Regulations
for AI that address these discriminatory patterns should coordinate with other nations to ensure
protections are global and regulations are not “patchworked”.
When attempting to improve performance for underrepresented indigenous languages, it is important
to adhere to established principles such as the Indigenous Data Sovereignty principles, e.g.: The
CARE Principles for Indigenous Data Governance [51] or FAIR principles [52].
Participatory methodologies in AI development have [31] included engaging locally led and com-
pensated focus groups with impacted community members, in collaboration with engineers, to think
through potential harmful outcomes. “Red-teaming” - testing AI models for potential vulnerabili-
ties, biases, and weaknesses through real-world simulations is also an entry point for engaging the
‘epistemic privilege’ [246] of those most affected by the social impacts of generative AI systems.
Addressing barriers to evaluations are rendered difficult, and at times impossible, given that the model
is enclosed in software or only available through an API. Therefore, given the overlaps in the public
sphere, advocacy of open-sourced / licensed access are increasingly popular and compelling [231].
Smuha [226] proposes accountability and monitoring mechanisms at a public oversight level, for
example mandatory impact assessments of AI systems which incorporates opportunities for societal
feedback. Smuha also emphasises the importance of independent information collection and distri-
bution about AI’s societal impact. Further, it will be necessary to introduce procedural rights - for
example "right to access to information, access to justice, and participation in public decision-making
on AI, regardless of the demonstration of individual harm".
4.2.2.3
Abusive or Violence Content
Generative AI systems can generate outputs that are used for abuse, constitute non-consensual content,
or are threats of violence and harassment [9]. Non-consensual sexual representations of people,
include representations of minors as generative child sexual abuse material (CSAM) [155]. Abuse
and violence can disparately affect groups, such as women and girls [10].
What to Evaluate
Sensitive topics and trauma’s impacts on people are by nature challenging
to evaluate and must be done with care. Consequences of abuse of children and minors can be
long-term or lifelong [17]. Impacts and trauma can resurface throughout a person’s life in many
aspects. Evaluations for generative AI impacts can overlap with similar harms such as image-based
sexual abuse [122]. As seen in 4.1.2 Cultural Values and Sensitive Content, consent from existing
people should be evaluated with the person themselves.
Mitigation and Intervention
Research to detect, mitigate, and report abusive and violent content
such as CSAM is ongoing [241] and tools specific to modalities such as images can help identify
content that is not yet labeled as CSAM [243]. Relevant regulation should be updated to address
generated content that may not accurately portray an existing person or their body or self, but lead to
real harms.
14

4.2.3
Concentration of Authority
Use of generative AI systems to contribute to authoritative power and reinforce dominant values
systems can be intentional and direct or more indirect. Concentrating authoritative power can also
exacerbate inequality and lead to exploitation.
4.2.3.1
Militarization, Surveillance, and Weaponization
Concentrating power can occur at increasing levels, from small groups to national bodies. Code
generative systems can improve development for technical surveillance systems and language models
can be used to surveil text communication within work, social, and other environments [1].
Generative AI mechanisms for accumulating power and control at a national level, such as surveillance,
has not yet happened, but government and military interest in deploying and weaponizing generative
AI systems is growing [106]. Use includes generating synthetic data for training AI systems [102]
and military planning [78]. Military use is not inherently weaponization and risk depends on the use
case and government interest. Favorable arguments use AI to protect national security and require
differentiating national security interests from undue harm [44].
Generative AI systems are also enabling new kinds of cyberattacks, and amplifying the possibilities
of existing cyberattacks. For example, synthetic audio has been used to copy the sound of someone’s
voice for more compelling fraud and extortion [124]. Large language models are also facilitating
disinformation campaigns, influence operations, and phishing attacks [92].
What to Evaluate
If deployed covertly, under NDA, or without transparency, generative AI systems
used for surveillance or weaponization cannot be tracked or evaluated. Evaluations can broadly
analyze the quantity of where such systems have been deployed, such as the number of devices sold,
or number of system deployments, as a brute force measure.
Mitigation and Intervention
For procurement of technical systems, developers can restrict surveil-
lance and weaponization as use cases. Government development of generative AI systems for
surveillance and weaponization requires additional protocols. Governments and militaries can make
commitments toward ethical and responsible uses of AI [6] and joint commitments from multiple
countries [11] can create accountability among military powers. Regulatory approaches can draw
boundaries for harmful uses by militaries, but will grapple with tensions for what constitutes national
security [266].
4.2.3.2
Imposing Norms and Values
Global deployment of a model can consolidate power within a single, originating culture, to determine
and propagate acceptability [245] across cultures [150]. Highest performing characteristics of gener-
ative systems such as language, dominant cultural values, and embedded norms can overrepresent
regions outside of where a system is deployed. For example, a language model that is highest
performing in the English language can be deployed in a region with a different dominant language
and incentivize engaging in English. Establishing or reinforcing goodness with certain languages,
accents, imagery, social norms, and other representations of peoples and cultures can contribute to
this norms and values imposition.
Certain modality characteristics such as language carry within it its own logics and frames. Though
English as a lingua franca is globally beneficial, the consequences of its dominance as a result of a
historic process of militarised colonization should be examined. Insidious effects which generative
AI systems could further embed include the erosion of global multilingualism, undermine the right to
language and culture, and further marginalize the necessity for widespread multilingual education.
The effects of generative AI systems on child development, including the technologically mediated
socialisation of norms and values is also an area to be inquired. These are in addition to the emotional
and behavioural effects of chatbots on children. This, according to UNICEF [248], included the
enforcement of "bias, given that they often select a predetermined reply based on the most matching
keywords or similar wording pattern".
15

What to Evaluate
In addition to evaluations and limitations in 4.1.2 Cultural Values and Sensitive
Content, complex, qualitative, and evolving cultural concepts such as beauty and success are viewed
differently in context of an application and cultural region. Impacts of norm and value impositions
are still being determined, but can manifest in a given use case [130].
Mitigation and Interventions
Mitigations should be cognizant of preserving irreducible differences
among cultures [73] and practicing value sensitive design [84], including by focusing on system
components such as data extraction and use [62]. Methods for cultural value alignment [228] can
improve and require improving methods and infrastructure for working with underrepresented groups.
Novel alignment techniques [259, 25] by modality can determine preferable principles and values for
generative AI systems. Prominent AI regulations such as the EU AI Act should account for copycat
legislation in other countries.
4.2.4
Labor and Creativity
Economic incentives to augment and not automate human labor, thought, and creativity should
examine the ongoing effects generative AI systems have on skills, jobs, and the labor market.
4.2.4.1
Intellectual Property and Ownership
Rights to the training data and replicated or plagiarized work in addition to and rights to generated
outputs are ongoing legal and policy discussions, often by specific modality. Impacts to people and
society will necessarily coexist with impacts and development of intellectual property law.
What to Evaluate
Determining whether original content has been used in training data depends
on developer transparency or research on training data extraction [50]. Given the large sizes of
training datasets, possible methods of evaluating original content inclusion could be through search
and matching tools. In addition to unclear legal implications, the ambiguity of impacts on content
ownership [239] makes evaluation difficult.
Mitigation and Intervention
Similar to 4.2.1.3 Personal Privacy and Sense of Self, opt-in and
opt-out mechanisms can protect intellectual property but depend on adherence. Regulation and
stricter rules from a developer organization about training material will differ by modality. Ongoing
lawsuits will set legal precedent [55]. Tools [215] are being developed to protect certain modalities
from being used as training data.
4.2.4.2
Economy and Labor Market
Key considerations about the impact of automation and AI on employment center on whether these
technologies will generate new jobs or, in contrast, will lead to a large-scale worker displacement
in the next future. Narratives about machines taking over the production of goods and services
resurfaced periodically: from the early nineteenth-century Luddite movement against the introduction
of the spinning jenny in textile manufacturing, to British farmers’ Swing Riots against mechanical
threshers, to protests against the dial telephone, introduced in the U.S. during the Great Depression
and responsible, according to its detractors, of mass unemployment among telephone operators [221].
Labor in system development such as crowdwork can encompass short-lived relations between inde-
pendent contractors and their clients offers several advantages over traditional forms of employment.
For example, companies can avoid overhead personnel costs (e.g., HR), while contract workers
can decide how much, from where, and when to work. However, as contractors, crowdworkers are
excluded from employment protective norms. As a result, they can be paid significantly less than
minimum wage, have no access to healthcare benefits, are not subject to working time restrictions,
and may not have access to holidays or sick leaves [188]. Further, crowdworkers are exposed to
increasingly subtle forms of surveillance, which is becoming essential for implementing algorithmic
forms of management, understood as "a diverse set of technological tools and techniques to remotely
manage workforces [and] enable automated or semi-automated decision-making" [162]. The goal
of full automation remains perpetually beyond reach since the line between what machines can
and cannot solve is constantly redrawn by AI advancements. This phenomenon, the "paradox of
automation’s last mile", is a self-propelling cycle in which every solution to automation problems
creates new problems to be automated, and hence new demands for ghost workers [93].
16

What to Evaluate
Long-term impact on the global economy is unclear and depends on industry
decisions to use generative AI to augment or automate jobs. Factors to be evaluated include un-
employment rates, salaries for a given skill or task, economic class divisions, and overall cost of
services. [57] argues that the substitution of labor for capital, as in the case of the introduction of
labor-substituting technologies, might lead to cost cuts in the short term. The externalities2 of AI and
automation, however, can be detrimental in the long term and could lead to unemployment, smaller
tax bases, and economic inequality between skilled workers and a growing underclass. [136] offers a
complementary perspective when arguing how AI in the workplace can stimulate competition, drive
prices down, and have a net-positive effect on employment. For specific tasks, evaluating quality
of generated output compared to human output can give signal to the likelihood of a generative AI
system replacing human labor [212].
A task-polarization model [22] shows how AI can potentially widen the gap between high and low-
wage occupations at the expense of the middle tier. [14] shows how technological advancements have
historically increased earning inequality between education, sex, race, and age groups. Therefore,
looking at the overall growth or decline of the labor market might mislead about the real impact of AI;
AI might be displacing labor and yet, at the same time, creating new jobs, thus making it challenging
to fully evaluate its implications unless we investigate into the kind of jobs that are being created and
destroyed.
See 4.1.7 Data and Content Moderation Labor for evaluating human labor in the research, develop-
ment, and deployment process.
Mitigation and Intervention
In additional to labor protection laws, more inclusive design pro-
cesses, as argued by [214] can open technological decisions to democratic participation as a way to
steer innovation in socially desirable directions.
For human labor in AI development, a central challenge is discerning genuinely self-employed
crowd-contractors from salaried workers and platforms’ responsibilities within the multiparty rela-
tionship between crowdworkers and crowdsourcers. Traditionally, crowd platforms (such as Amazon
Mechanical Turk, Clickwork, and Appen) have positioned themselves as mere conduits through
which client companies (crowdsourcers) can publicize their tasks and hire crowdworkers. Because of
the nature of crowdsourced work–usually small tasks requiring only several minutes to complete–it
is not uncommon for crowdworkers to work for hundreds of different companies in a week [79].
Crowdworkers have commonly been framed as contractors for crowdsourcing companies, while
platforms maintain a neutral position and profit from service fees applied to each transaction.
To protect crowdworkers, regulators are proposals new rules For instance, California’s Bill AB-5
[5] advances new rules for determining whether a worker is an independent contractor or should
be treated, instead, as an employee. Europe might follow suit soon with the introduction of the
proposed Directive 2021/0414(COD) [7], which sets precise criteria for determining platform workers’
employment status and access to labor protection rights in addition to transparency requirements
about employees’ working conditions. The Directive dovetails with the proposed AI Act (COM(2021)
206 final) [74] that aims to ensure that AI algorithms employed in the EU respect fundamental human
rights. The proposed AI Act also requires high-risk AI systems to fulfill stringent transparency and
data accessibility requirements and imposes platforms to disclose to workers the criteria and the data
used in automated decision-making systems.
4.2.5
Ecosystem and Environment
Impacts at a high-level, from the AI ecosystem to the Earth itself, are necessarily broad but can be
broken down into components for evaluation.
4.2.5.1
Widening Resource Gaps
As described in section Financial Costs, the high financial and resource costs necessarily excludes
groups who do not have the resources to train, evaluate, or host models. The infrastructure needed
to contribute to generative AI research and development leads to widening gaps which are notable
among sectors, such as between industry and academia [145], or among global powers and countries
[19].
2Externalities broadly refer to the unanticipated effects of economic activities on the social environment.
17

Access and Benefit Distribution
Ability to contribute to and benefit from a system depends on
ability to engage with a system, which in turn depends on the openness of the system, the system
application, and system interfaces. Level of openness and access grapples with tensions of misuse
and risk. Increasing trends toward system closedness [227] is shifting access distribution.
Geographic and Regional Activity Concentration
In the field of AI as a whole, top AI research
institutions from 1990-2014 have concentrated in the U.S. [164]. More recent data highlights the
U.S., EU, and China as primary hubs [198]. Even within the U.S. AI activity concentrates in urban,
coastal areas [154].
What to Evaluate
Evaluation should first determine AI-specific resources then tracking trends by
sector and region. To determine and evaluate level of access, first components of access should be
established. This includes technical details, upstream decisions, auditing access, and opt-out or opt-in
reliability. Specific resources such as computing power [16] are popularly tracked by annual reports
on the field of AI [145, 27].
Mitigation and Intervention
Policymakers can minimize resource gaps by making high-cost
resources, such as computing power, accessible via applications and grants to researchers and low-
resource organizations. Intercultural dialogues [48] that meaningfully address power imbalances and
lowering the barrier for underrepresented peoples to contribute can improve harms from resource
gaps. This can include accessible interfaces to interact with and conduct research on generative AI
systems and low- to no-code tooling.
4.2.5.2
Environmental Impacts
In addition to the 4.1.6 Environmental Costs and Carbon Emissions from a system itself, evaluating
impact on the Earth can follow popular frameworks and analyses.
What to Evaluate
Environmental, social, and governance (ESG) frameworks and the Scope 1, 2,
and 3 system can give structure to how developers track carbon emissions [195]. Scope 3 emissions,
the indirect emissions often outside a developer’s control, should account for a generative AI system’s
lifecycle including in deployment [141]. Long-term effects of AI environmental impacts on the world
and people can range from from inequity to quality of life [194]. Research to evaluate overall impacts
of climate change is ongoing [3].
Given the intensive use of energy and compute required to develop generative AI systems, due dili-
gence is required regarding sustainability claims. Company practices of offsetting carbon footprints
include purchasing renewable energy certificates (RECs), e.g. tokens representing a utility’s green
energy generation. However REC purchases may offset carbon emissions to achieve “net zero” on
paper, while in reality still using fossil fuel based energy to run systems. This is due to the purchased
renewable energy being generated at another time and location than the energy used by the company.
Tracking the validity of high energy users claims that their theoretical carbon usage matches their
actual use of carbon intensive energy can be carried out using time stamped certificates. Further
transparency around industry figures on energy consumption will be required to adequately intervene
[115].
Mitigation and Interventions
Systemic change is a prerequisite to energy and carbon efficiency
in AI systems, from energy efficient default settings for platforms and tools, and an awareness of
balancing gains with cost, for example, weighing energy costs, both social and monetary, with the
performance gains of a new model before deploying it. Best practices for developers and researchers
include choosing efficient testing environments, promoting reproducibility, and standardized reporting.
An energy efficiency leaderboard can incentivise responsible research [105].
Reducing carbon emissions should start with standards and transparency for carbon emissions
reporting and accounting for efficiency. Having a more standardized approach, such as ISO standards,
to reporting the carbon emissions of AI can help better understand their evolution, and to compare
the emissions of different approaches and models. While certain conferences such as NeurIPS are
starting to include compute information in submissions in submission checklists, there is still a
lot of variability in carbon reporting, and figures can vary widely depending on what factors are
included. The current pursuit of accuracy above all else is often at odds with other aspects of model
18

performance, including efficiency. Including these metrics when comparing two or more models
(e.g. in benchmarks and leaderboards) can help users make trade-offs that consider both aspects and
choose the model that best corresponds to their use case and criteria.
5
Broader Impacts of Base System Evaluations
Understanding an AI system from conception to training to deployment requires insight into training
data, the model itself, and the use case/application into which the system is deployed. It also requires
understanding people, society, and how societal processes, institutions, and power are changed and
shifted by an AI system.
5.1
Context for the System
Context is critical to robust evaluation; the way in which we properly define and evaluate harm in
any given application requires an understanding of the target industry, task, end-user, and model
architecture. Communication across model developers, model deployers, and end-users is key to
developing a comprehensive evaluation and risk mitigation strategy. Actors across the ecosystem
should collaborate to craft robust evaluations and invest in the safeguards needed to prevent harm.
5.2
Context of the Evaluation
Systems can be deployed in contexts where there is not sufficient attention towards evaluating and
moderating performance. This means disparate performance is not caught, as seen with social media
platform moderation outside of the most commonly-written languages and wealthiest countries [203].
Moreover, as cultural values change between cultural contexts, both within and outside of any given
language, the particular cultural values that are being evaluated should be made explicit. A byproduct
of such specificity is that it becomes clear where evaluations should be extended while providing a
framework for such extensions.
5.3
Choosing Evaluations
The evaluations selected to determine a model’s performance will impact the values that it propagates
out during deployment. There is no universal evaluation by which to evaluate a model’s performance,
and any evaluation metrics should be used with deployment context in mind [210, 191]. Furthermore,
notable work at top AI ethics publication venues has not adequately centered on the least powerful
in society [32], thereby further exacerbating disparate outcomes by only providing avenues for
mitigation for some.
6
Conclusion
Just as generative AI systems undergo performance evaluations, they must be evaluated for social
impacts. The seven categories in our framework for technical base systems move toward a standard
for all modalities of a base system. Our analyses of popular evaluation methods per category can
help to improve research in producing novel evaluations. Evaluating people and society interacts
with risk and harms taxonomies for generative AI systems. Existing people and societal evaluations
are limited and must consider challenges and ethics of determining human responses. Since social
impact evaluations can only give limited information about each impact type, we recommend that all
categories are given equal importance, and that all relevant stakeholders are meaningfully consulted
throughout the development, evaluation, and deployment processes.
Acknowledgments and Disclosure of Funding
We thank the unnamed workshop contributors. We also thank Hugging Face for supporting this
work. Thank you to Rishi Bommasani, Nima Boscarino, Deep Ganguli, and Andrew Smart for their
thoughtful feedback. Any remaining errors are the authors’ alone.
19

Contributions, listed in alphabetical order by last name:
Workshop Leads: Irene Solaiman, Zeerak Talat
FAccT 2023 CRAFT Leads: William Agnew, Marie-Therese Png, Irene Solaiman, Zeerak Talat
Major contributors: William Agnew, Lama Ahmad, Dylan Baker, Ellie Evans, Sara Hooker, Yacine
Jernite, Alberto Lusoli, Sasha Luccioni, Jessica Newman, Marie-Therese Png, Irene Solaiman, Zeerak
Talat
Workshop Contributors: Willie Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daumé III,
Jesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite, Sasha Luccioni, Margaret Mitchell, Jessica
Newman, Marie-Therese Png, Irene Solaiman, Andrew Strait, Zeerak Talat, Apostol Vassilev
References
[1] Analyse and automate every message | Re:infer, . URL https://www.reinfer.io/.
[2] Global Comprehensive Privacy Law Mapping Chart, .
URL https://iapp.org/
resources/article/global-comprehensive-privacy-law-mapping-chart/.
[3] Methodologies
and
Tools
to
Evaluate
Climate
Change
Im-
pacts
and
Adaptation,
.
URL
https://unfccc.int/
methodologies-and-tools-to-evaluate-climate-change-impacts-and-adaptation-2.
[4] California Consumer Privacy Act of 2018, 2018. URL https://leginfo.legislature.
ca.gov/faces/codes_displayText.xhtml?division=3.&part=4.&lawCode=CIV&
title=1.81.5.
[5] AB-5 Worker status: employees and independent contractors., 2019. URL https://leginfo.
legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB5.
[6] DOD
Adopts
Ethical
Principles
for
Artificial
Intelligence
>
U.S.
Department
of
Defense
>
Release,
Feb.
2020.
URL
https:
//www.defense.gov/News/Releases/Release/Article/2091996/
dod-adopts-ethical-principles-for-artificial-intelligence/.
[7] COM (2021) 762: Proposal for a DIRECTIVE OF THE EUROPEAN PARLIAMENT AND
OF THE COUNCIL on improving working conditions in platform work, 2021. URL https:
//eur-lex.europa.eu/procedure/EN/2021_414. Doc ID: 2021_414 Doc Title: Proposal
for a DIRECTIVE OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL on
improving working conditions in platform work Usr_lan: en.
[8] Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE
COUNCIL LAYING DOWN HARMONISED RULES ON ARTIFICIAL INTELLIGENCE
(ARTIFICIAL INTELLIGENCE ACT) AND AMENDING CERTAIN UNION LEGISLA-
TIVE ACTS, 2021.
URL https://eur-lex.europa.eu/legal-content/EN/TXT/
?uri=celex%3A52021PC0206.
[9] Americans’
Views
on
Generative
Artificial
Intelligence,
Hate
and
Ha-
rassment
|
ADL,
2023.
URL
https://www.adl.org/resources/blog/
americans-views-generative-artificial-intelligence-hate-and-harassment.
[10] Cyber
violence
is
a
growing
threat,
especially
for
women
and
girls,
June
2023.
URL
https://eige.europa.eu/newsroom/news/
cyber-violence-growing-threat-especially-women-and-girls?language_
content_entity=en.
[11] REAIM
2023
|
Ministry
of
Foreign
Affairs
|
Government.nl,
2023.
URL
https://www.government.nl/ministries/ministry-of-foreign-affairs/
activiteiten/reaim.
20

[12] L. Abbott and C. Grady. A Systematic Review of the Empirical Literature Evaluating IRBs:
What We Know and What We Still Need to Learn. Journal of empirical research on human
research ethics : JERHRE, 6(1):3–19, Mar. 2011. ISSN 1556-2646. doi: 10.1525/jer.2011.6.1.
3. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3235475/.
[13] G. Abercrombie, A. C. Curry, T. Dinkar, and Z. Talat. Mirages: On Anthropomorphism in Dia-
logue Systems, May 2023. URL http://arxiv.org/abs/2305.09800. arXiv:2305.09800
[cs].
[14] D. Acemoglu and P. Restrepo. Tasks, Automation, and the Rise in U.S. Wage Inequal-
ity. Econometrica, 90(5):1973–2016, 2022. ISSN 1468-0262. doi: 10.3982/ECTA19815.
URL https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA19815.
_eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA19815.
[15] O. Ahia, J. Kreutzer, and S. Hooker.
The Low-Resource Double Bind: An Empiri-
cal Study of Pruning for Low-Resource Machine Translation.
In Findings of the As-
sociation for Computational Linguistics: EMNLP 2021, pages 3316–3333. Association
for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.282. URL
https://aclanthology.org/2021.findings-emnlp.282.
[16] N. Ahmed and M. Wahed. The De-democratization of AI: Deep Learning and the Compute
Divide in Artificial Intelligence Research, Oct. 2020. URL http://arxiv.org/abs/2010.
15581. arXiv:2010.15581 [cs].
[17] A. Al Odhayani, W. J. Watson, and L. Watson. Behavioural consequences of child abuse.
Canadian Family Physician, 59(8):831–836, Aug. 2013. ISSN 0008-350X. URL https:
//www.ncbi.nlm.nih.gov/pmc/articles/PMC3743691/.
[18] A. Allen.
Protecting One’s Own Privacy in a Big Data Economy.
Harvard Law Re-
view, 130(2), Dec. 2016.
URL https://harvardlawreview.org/forum/vol-130/
protecting-ones-own-privacy-in-a-big-data-economy/.
[19] C.
Alonso,
S.
Kothari,
and
S.
Rehman.
How
Artificial
Intelli-
gence
Could
Widen
the
Gap
Between
Rich
and
Poor
Nations,
Dec.
2020.
URL
https://www.imf.org/en/Blogs/Articles/2020/12/02/
blog-how-artificial-intelligence-could-widen-the-gap-between-rich-and-poor-nations.
[20] C. Andrade. The Limitations of Online Surveys. Indian Journal of Psychological Medicine,
42(6):575–576, Nov. 2020. doi: https://doi.org/10.1177/0253717620957496. URL https:
//journals.sagepub.com/doi/epub/10.1177/0253717620957496.
[21] M. Andrus, E. Spitzer, J. Brown, and A. Xiang. What We Can’t Measure, We Can’t Understand:
Challenges to Demographic Data Procurement in the Pursuit of Fairness. In Proceedings
of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 249–
260. ACM, 2021-03-03. ISBN 978-1-4503-8309-7. doi: 10.1145/3442188.3445888. URL
https://dl.acm.org/doi/10.1145/3442188.3445888.
[22] D. Autor. The Labor Market Impacts of Technological Change: From Unbridled Enthusiasm
to Qualified Optimism to Vast Uncertainty, May 2022. URL https://www.nber.org/
papers/w30074.
[23] F. O. Baah, A. M. Teitelman, and B. Riegel. Marginalization: Conceptualizing patient
vulnerabilities in the framework of social determinants of health – An integrative review.
Nursing inquiry, 26(1):e12268, Jan. 2019. ISSN 1320-7881. doi: 10.1111/nin.12268. URL
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6342665/.
[24] E. Bagdasaryan, O. Poursaeed, and V. Shmatikov. Differential privacy has disparate impact on
model accuracy. In Proceedings of the 33rd International Conference on Neural Information
Processing Systems. Curran Associates Inc., 2019.
[25] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-
seini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li,
E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite,
21

L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Lar-
son, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton,
T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei,
N. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional AI: Harmlessness from AI
Feedback, Dec. 2022. URL http://arxiv.org/abs/2212.08073. arXiv:2212.08073 [cs].
[26] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the Dangers of Stochastic
Parrots: Can Language Models Be Too Big?
. In Proceedings of the 2021 ACM Conference
on Fairness, Accountability, and Transparency, pages 610–623. ACM, 2021-03-03. ISBN
978-1-4503-8309-7. doi: 10.1145/3442188.3445922. URL https://dl.acm.org/doi/10.
1145/3442188.3445922.
[27] N. Beniach and I. Hogarth. State of AI Report 2022. URL https://www.stateof.ai/.
[28] C. L. Bennett, C. Gleason, M. K. Scheuerman, J. P. Bigham, A. Guo, and A. To. “It’s
Complicated”: Negotiating Accessibility and (Mis)Representation in Image Descriptions of
Race, Gender, and Disability. In Proceedings of the 2021 CHI Conference on Human Factors
in Computing Systems, pages 1–19. ACM, 2021-05-06. ISBN 978-1-4503-8096-6. doi: 10.
1145/3411764.3445498. URL https://dl.acm.org/doi/10.1145/3411764.3445498.
[29] J. Berg, M. Furrer, E. Harmon, U. Rani, and M. S. Silberman. Digital Labour Platforms
and the Future of Work: Towards Decent Work in the Online World. International Labour
Organization, 2018. ISBN 978-92-2-031024-3.
[30] BigScience Workshop:, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow,
R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, J. Tow, A. M. Rush, S. Biderman, A. Webson,
P. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase,
R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O.
Suarez, V. Sanh, H. Laurençon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan,
A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue,
C. Klamm, C. Leong, D. van Strien, D. I. Adelani, D. Radev, E. G. Ponferrada, E. Lev-
kovizh, E. Kim, E. B. Natan, F. D. Toni, G. Dupont, G. Kruszewski, G. Pistilli, H. Elsahar,
H. Benyamina, H. Tran, I. Yu, I. Abdulmumin, I. Johnson, I. Gonzalez-Dios, J. de la Rosa,
J. Chim, J. Dodge, J. Zhu, J. Chang, J. Frohberg, J. Tobing, J. Bhattacharjee, K. Almubarak,
K. Chen, K. Lo, L. V. Werra, L. Weber, L. Phan, L. B. allal, L. Tanguy, M. Dey, M. R. Muñoz,
M. Masoud, M. Grandury, M. Šaško, M. Huang, M. Coavoux, M. Singh, M. T.-J. Jiang,
M. C. Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner, N. Khamis, O. Nguyen,
O. Espejel, O. de Gibert, P. Villegas, P. Henderson, P. Colombo, P. Amuok, Q. Lhoest, R. Har-
liman, R. Bommasani, R. L. López, R. Ribeiro, S. Osei, S. Pyysalo, S. Nagel, S. Bose, S. H.
Muhammad, S. Sharma, S. Longpre, S. Nikpoor, S. Silberberg, S. Pai, S. Zink, T. T. Tor-
rent, T. Schick, T. Thrush, V. Danchev, V. Nikoulina, V. Laippala, V. Lepercq, V. Prabhu,
Z. Alyafeai, Z. Talat, A. Raja, B. Heinzerling, C. Si, D. E. Ta¸sar, E. Salesky, S. J. Mielke, W. Y.
Lee, A. Sharma, A. Santilli, A. Chaffin, A. Stiegler, D. Datta, E. Szczechla, G. Chhablani,
H. Wang, H. Pandey, H. Strobelt, J. A. Fries, J. Rozen, L. Gao, L. Sutawika, M. S. Bari,
M. S. Al-shaibani, M. Manica, N. Nayak, R. Teehan, S. Albanie, S. Shen, S. Ben-David, S. H.
Bach, T. Kim, T. Bers, T. Fevry, T. Neeraj, U. Thakker, V. Raunak, X. Tang, Z.-X. Yong,
Z. Sun, S. Brody, Y. Uri, H. Tojarieh, A. Roberts, H. W. Chung, J. Tae, J. Phang, O. Press,
C. Li, D. Narayanan, H. Bourfoune, J. Casper, J. Rasley, M. Ryabinin, M. Mishra, M. Zhang,
M. Shoeybi, M. Peyrounette, N. Patry, N. Tazi, O. Sanseviero, P. von Platen, P. Cornette, P. F.
Lavallée, R. Lacroix, S. Rajbhandari, S. Gandhi, S. Smith, S. Requena, S. Patil, T. Dettmers,
A. Baruwa, A. Singh, A. Cheveleva, A.-L. Ligozat, A. Subramonian, A. Névéol, C. Lovering,
D. Garrette, D. Tunuguntla, E. Reiter, E. Taktasheva, E. Voloshina, E. Bogdanov, G. I. Winata,
H. Schoelkopf, J.-C. Kalo, J. Novikova, J. Z. Forde, J. Clive, J. Kasai, K. Kawamura, L. Hazan,
M. Carpuat, M. Clinciu, N. Kim, N. Cheng, O. Serikov, O. Antverg, O. van der Wal, R. Zhang,
R. Zhang, S. Gehrmann, S. Mirkin, S. Pais, T. Shavrina, T. Scialom, T. Yun, T. Limisiewicz,
V. Rieser, V. Protasov, V. Mikhailov, Y. Pruksachatkun, Y. Belinkov, Z. Bamberger, Z. Kasner,
A. Rueda, A. Pestana, A. Feizpour, A. Khan, A. Faranak, A. Santos, A. Hevia, A. Unldreaj,
A. Aghagol, A. Abdollahi, A. Tammour, A. HajiHosseini, B. Behroozi, B. Ajibade, B. Saxena,
C. M. Ferrandis, D. Contractor, D. Lansky, D. David, D. Kiela, D. A. Nguyen, E. Tan, E. Bay-
lor, E. Ozoani, F. Mirza, F. Ononiwu, H. Rezanejad, H. Jones, I. Bhattacharya, I. Solaiman,
22

I. Sedenko, I. Nejadgholi, J. Passmore, J. Seltzer, J. B. Sanz, L. Dutra, M. Samagaio, M. El-
badri, M. Mieskes, M. Gerchick, M. Akinlolu, M. McKenna, M. Qiu, M. Ghauri, M. Burynok,
N. Abrar, N. Rajani, N. Elkott, N. Fahmy, O. Samuel, R. An, R. Kromann, R. Hao, S. Alizadeh,
S. Shubber, S. Wang, S. Roy, S. Viguier, T. Le, T. Oyebade, T. Le, Y. Yang, Z. Nguyen, A. R.
Kashyap, A. Palasciano, A. Callahan, A. Shukla, A. Miranda-Escalada, A. Singh, B. Beil-
harz, B. Wang, C. Brito, C. Zhou, C. Jain, C. Xu, C. Fourrier, D. L. Periñán, D. Molano,
D. Yu, E. Manjavacas, F. Barth, F. Fuhrimann, G. Altay, G. Bayrak, G. Burns, H. U. Vrabec,
I. Bello, I. Dash, J. Kang, J. Giorgi, J. Golde, J. D. Posada, K. R. Sivaraman, L. Bulchandani,
L. Liu, L. Shinzato, M. H. de Bykhovetz, M. Takeuchi, M. Pàmies, M. A. Castillo, M. Nezhu-
rina, M. Sänger, M. Samwald, M. Cullan, M. Weinberg, M. D. Wolf, M. Mihaljcic, M. Liu,
M. Freidank, M. Kang, N. Seelam, N. Dahlberg, N. M. Broad, N. Muellner, P. Fung, P. Haller,
R. Chandrasekhar, R. Eisenberg, R. Martin, R. Canalli, R. Su, R. Su, S. Cahyawijaya, S. Garda,
S. S. Deshmukh, S. Mishra, S. Kiblawi, S. Ott, S. Sang-aroonsiri, S. Kumar, S. Schweter,
S. Bharati, T. Laud, T. Gigant, T. Kainuma, W. Kusa, Y. Labrak, Y. S. Bajaj, Y. Venkatraman,
Y. Xu, Y. Xu, Y. Xu, Z. Tan, Z. Xie, Z. Ye, M. Bras, Y. Belkada, and T. Wolf. Bloom: A
176b-parameter open-access multilingual language model, 2023.
[31] A. Birhane, W. Isaac, V. Prabhakaran, M. Diaz, M. C. Elish, I. Gabriel, and S. Mohamed.
Power to the people? opportunities and challenges for participatory AI. In Equity and Access in
Algorithms, Mechanisms, and Optimization. ACM, oct 2022. doi: 10.1145/3551624.3555290.
URL https://doi.org/10.1145%2F3551624.3555290.
[32] A. Birhane, E. Ruane, T. Laurent, M. S. Brown, J. Flowers, A. Ventresque, and C. L. Dancy.
The Forgotten Margins of AI Ethics. In 2022 ACM Conference on Fairness, Accountability,
and Transparency, pages 948–958. ACM, 2022-06-21. ISBN 978-1-4503-9352-2. doi: 10.
1145/3531146.3533157. URL https://dl.acm.org/doi/10.1145/3531146.3533157.
[33] S. L. Blodgett, S. Barocas, H. Daumé III, and H. Wallach. Language (Technology) is Power: A
Critical Survey of “Bias” in NLP. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pages 5454–5476. Association for Computational Linguistics,
2020. doi: 10.18653/v1/2020.acl-main.485. URL https://www.aclweb.org/anthology/
2020.acl-main.485.
[34] M. J. Bockarie.
We need to end
“parachute” research which sidelines
the
work of African scientists, Jan. 2019.
URL https://qz.com/africa/1536355/
african-scientists-are-sidelined-by-parachute-research-teams.
[35] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,
J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji,
A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus,
S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel,
N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong,
K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani,
O. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee,
T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani,
E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C.
Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech,
E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz,
J. Ryan, C. Ré, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin,
R. Taori, A. W. Thomas, F. Tramèr, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M.
Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng,
K. Zhou, and P. Liang. On the Opportunities and Risks of Foundation Models, 2022-07-12.
URL http://arxiv.org/abs/2108.07258.
[36] G. Bowker and S. L. Star.
Sorting Things Out Classification and Its Consequences.
The MIT Press, 2000. ISBN 978-0-262-52295-3. URL https://mitpress.mit.edu/
9780262522953/sorting-things-out/.
[37] M. Brereton, P. Roe, R. Schroeter, and A. Lee Hong. Beyond ethnography: engagement
and reciprocity as foundations for design research out here. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, CHI ’14, pages 1183–1186, New York,
23

NY, USA, Apr. 2014. Association for Computing Machinery. ISBN 978-1-4503-2473-1. doi:
10.1145/2556288.2557374. URL https://doi.org/10.1145/2556288.2557374.
[38] S. Briscoe. U.S. Laws Address Deepfakes, Dec. 2021. URL http://www.asisonline.
org/security-management-magazine/latest-news/today-in-security/2021/
january/U-S-Laws-Address-Deepfakes/.
[39] H. Brown, K. Lee, F. Mireshghallah, R. Shokri, and F. Tramèr. What Does it Mean for a
Language Model to Preserve Privacy?, Feb. 2022. URL http://arxiv.org/abs/2202.
05520. arXiv:2202.05520 [cs, stat].
[40] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,
R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,
S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei.
Language Models are Few-Shot Learners, 2020-07-22. URL http://arxiv.org/abs/2005.
14165.
[41] B. Buchanan,
A. Lohn,
M. Musser,
and K. Sedova.
Truth,
Lies,
and Au-
tomation,
May
2021.
URL
https://cset.georgetown.edu/publication/
truth-lies-and-automation/.
[42] D. Bui, B. Tang, and K. G. Shin. Do Opt-Outs Really Opt Me Out? In Proceedings of the
2022 ACM SIGSAC Conference on Computer and Communications Security, pages 425–439,
Los Angeles CA USA, Nov. 2022. ACM. ISBN 978-1-4503-9450-5. doi: 10.1145/3548606.
3560574. URL https://dl.acm.org/doi/10.1145/3548606.3560574.
[43] J. Buolamwini and T. Gebru. Gender Shades: Intersectional Accuracy Disparities in Com-
mercial Gender Classification. In S. A. Friedler and C. Wilson, editors, Proceedings of the
1st Conference on Fairness, Accountability and Transparency, volume 81 of Proceedings of
Machine Learning Research, pages 77–91, New York, NY, USA, Feb. 2018. PMLR. URL
http://proceedings.mlr.press/v81/buolamwini18a.html.
[44] W. W. Burke-White. Human Rights and National Security: The Strategic Correlation. Harvard
Human Rights Journal, 17:249–280, 2004.
[45] Z. Buçinca, M. B. Malaya, and K. Z. Gajos.
To Trust or to Think: Cognitive Forcing
Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the
ACM on Human-Computer Interaction, 5(CSCW1):1–21, Apr. 2021. ISSN 2573-0142. doi:
10.1145/3449287. URL http://arxiv.org/abs/2102.09692. arXiv:2102.09692 [cs].
[46] A. Caliskan, J. J. Bryson, and A. Narayanan. Semantics derived automatically from language
corpora contain human-like biases. Science, 356(6334):183–186, 2017-04-14. ISSN 0036-
8075, 1095-9203. doi: 10.1126/science.aal4230. URL https://www.science.org/doi/
10.1126/science.aal4230.
[47] M. R. Calo. The Boundaries of Privacy Harm. INDIANA LAW JOURNAL, 86(3), 2011.
[48] R. Capurro and J. Díaz Nafría. Intercultural information ethics. In Glossarium BITri: Glossary
of Concepts, Metaphors, Theories and Problems Concerning Information, pages 329–336.
Nov. 2010. ISBN 978-84-9773-554-4. Journal Abbreviation: Glossarium BITri: Glossary of
Concepts, Metaphors, Theories and Problems Concerning Information.
[49] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying Memoriza-
tion Across Neural Language Models. URL http://arxiv.org/abs/2202.07646.
[50] N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramèr, B. Balle, D. Ippolito, and
E. Wallace. Extracting Training Data from Diffusion Models. 2023. doi: 10.48550/ARXIV.
2301.13188. URL https://arxiv.org/abs/2301.13188.
[51] S. R. Carroll, I. Garba, O. L. Figueroa-Rodríguez, J. Holbrook, R. Lovett, S. Materechera,
M. Parsons, K. Raseroka, D. Rodriguez-Lonebear, R. Rowe, R. Sara, J. D. Walker, J. Anderson,
and M. Hudson. The CARE Principles for Indigenous Data Governance. 19(1):43, Nov. 2020.
ISSN 1683-1470. doi: 10.5334/dsj-2020-043. URL https://datascience.codata.org/
articles/10.5334/dsj-2020-043. Number: 1 Publisher: Ubiquity Press.
24

[52] S. R. Carroll, E. Herczog, M. Hudson, K. Russell, and S. Stall. Operationalizing the CARE and
FAIR Principles for Indigenous data futures. Scientific Data, 8(1):108, Apr. 2021. ISSN 2052-
4463. doi: 10.1038/s41597-021-00892-0. URL https://www.nature.com/articles/
s41597-021-00892-0. Number: 1 Publisher: Nature Publishing Group.
[53] A. Chan, R. Salganik, A. Markelius, C. Pang, N. Rajkumar, D. Krasheninnikov, L. Langosco,
Z. He, Y. Duan, M. Carroll, M. Lin, A. Mayhew, K. Collins, M. Molamohammadi, J. Burden,
W. Zhao, S. Rismani, K. Voudouris, U. Bhatt, A. Weller, D. Krueger, and T. Maharaj. Harms
from Increasingly Agentic Algorithmic Systems, May 2023. URL http://arxiv.org/abs/
2302.10329. arXiv:2302.10329 [cs].
[54] A. Chen.
Inmates in Finland are training AI as part of prison labor,
2019-
03-28T16:05:06.
URL
https://www.theverge.com/2019/3/28/18285572/
prison-labor-finland-artificial-intelligence-data-tagging-vainu.
[55] M.
Chen.
Artists
and
Illustrators
Are
Suing
Three
A.I.
Art
Gen-
erators
for
Scraping
and
’Collaging’
Their
Work
Without
Con-
sent,
Jan.
2023.
URL
https://news.artnet.com/art-world/
class-action-lawsuit-ai-generators-deviantart-midjourney-stable-diffusion-2246770.
Section: Law.
[56] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,
N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry,
P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter,
P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H.
Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders,
C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,
M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
I. Sutskever, and W. Zaremba. Evaluating Large Language Models Trained on Code, July
2021. URL http://arxiv.org/abs/2107.03374. arXiv:2107.03374 [cs].
[57] D. Y. Choi and J. H. Kang. Net Job Creation in an Increasingly Autonomous Economy:
The Challenge of a Generation. Journal of Management Inquiry, 28(3):300–305, July 2019.
ISSN 1056-4926. doi: 10.1177/1056492619827372. URL https://doi.org/10.1177/
1056492619827372. Publisher: SAGE Publications Inc.
[58] M. Cifor, P. Garcia, T. Cowan, J. Rault, T. Sutherland, A. Chan, J. Rode, A. Hoffmann,
N. Salehi, and L. Nakamura. Feminist Data Manifest-No. URL https://www.manifestno.
com.
[59] D. K. Citron and D. J. Solove. Privacy Harms, Feb. 2021. URL https://papers.ssrn.
com/abstract=3782222.
[60] T. Claburn. AI assistants help developers produce code that’s insecure. URL https://www.
theregister.com/2022/12/21/ai_assistants_bad_code/.
[61] CodeCarbon. About CodeCarbon, 2023-05-12T16:13:58Z. URL https://github.com/
mlco2/codecarbon.
[62] N. Couldry and U. A. Mejias. The decolonial turn in data and technology research: what
is at stake and where is it heading?
Information, Communication & Society, 26(4):
786–802, Mar. 2023. ISSN 1369-118X. doi: 10.1080/1369118X.2021.1986102. URL
https://doi.org/10.1080/1369118X.2021.1986102.
Publisher: Routledge _eprint:
https://doi.org/10.1080/1369118X.2021.1986102.
[63] K. Crawford, R. Dobbe, G. Fried, E. Kaziunas, A. Kak, V. Mathur, R. Richardson, J. Schultz,
O. Schwartz, S. M. West, and M. Whittaker. AI Now 2018 Report, 2018. URL https:
//ainowinstitute.org/publication/ai-now-2018-report-2.
[64] K. Crenshaw. Mapping the Margins: Intersectionality, Identity Politics, and Violence against
Women of Color. 43(6):1241, 1991-07. ISSN 00389765. doi: 10.2307/1229039. URL
https://www.jstor.org/stable/1229039?origin=crossref.
25

[65] B. Dang, M. J. Riedl, and M. Lease. But Who Protects the Moderators? The Case of
Crowdsourced Image Moderation, Jan. 2020. URL http://arxiv.org/abs/1804.10999.
arXiv:1804.10999 [cs].
[66] N. C. Dang, M. N. Moreno-García, and F. D. la Prieta. Sentiment analysis based on deep learn-
ing: A comparative study. Electronics, 9(3):483, mar 2020. doi: 10.3390/electronics9030483.
URL https://doi.org/10.3390%2Felectronics9030483.
[67] T. Davidson, D. Bhattacharya, and I. Weber. Racial Bias in Hate Speech and Abusive Language
Detection Datasets. In Proceedings of the Third Workshop on Abusive Language Online, pages
25–35. Association for Computational Linguistics. doi: 10.18653/v1/W19-3504. URL
https://www.aclweb.org/anthology/W19-3504.
[68] Department for Digital, Culture, Media and Sport.
Establishing a pro-innovation ap-
proach to regulating AI, 2022. URL https://www.gov.uk/government/publications/
establishing-a-pro-innovation-approach-to-regulating-ai.
[69] Department of International Cooperation Ministry of Science and Technology. Next Generation
Artificial Intelligence Development Plan, 2017. URL http://fi.china-embassy.gov.cn/
eng/kxjs/201710/P020210628714286134479.pdf.
[70] M. Díaz, I. Kivlichan, R. Rosen, D. Baker, R. Amironesei, V. Prabhakaran, and E. Denton.
CrowdWorkSheets: Accounting for Individual and Collective Identities Underlying Crowd-
sourced Dataset Annotation. In 2022 ACM Conference on Fairness, Accountability, and
Transparency, pages 2342–2351. ACM, 2022-06-21. ISBN 978-1-4503-9352-2. doi: 10.1145/
3531146.3534647. URL https://dl.acm.org/doi/10.1145/3531146.3534647.
[71] E. Dinan, G. Abercrombie, A. S. Bergman, S. Spruit, D. Hovy, Y.-L. Boureau, and V. Rieser.
Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling, 2021-07-23.
URL http://arxiv.org/abs/2107.03451.
[72] M. Douglas.
Purity and Danger: An Analysis of Concepts of Pollution and Taboo.
Routledge, 2002, 1 edition. ISBN 978-0-415-28995-5. URL https://www.routledge.
com/Purity-and-Danger-An-Analysis-of-Concepts-of-Pollution-and-Taboo/
Douglas/p/book/9780415289955.
[73] C. Ess. Ethical pluralism and global information ethics. Ethics and Information Technology, 8
(4):215–226, Nov. 2006. ISSN 1572-8439. doi: 10.1007/s10676-006-9113-3. URL https:
//doi.org/10.1007/s10676-006-9113-3.
[74] European
Commission.
Proposal
for
a
Regulation
laying
down
harmonised
rules
on
artificial
intelligence
|
Shaping
Europe’s
digital
future,
2021-
04-21.
URL
https://digital-strategy.ec.europa.eu/en/library/
proposal-regulation-laying-down-harmonised-rules-artificial-intelligence.
[75] Fair Work. About Fairwork, 2023. URL https://fair.work/en/fw/homepage/.
[76] H. Farid. Creating, Using, Misusing, and Detecting Deep Fakes. Journal of Online Trust
and Safety, 1(4), Sept. 2022. ISSN 2770-3142. doi: 10.54501/jots.v1i4.56. URL https:
//www.tsjournal.org/index.php/jots/article/view/56. Number: 4.
[77] Federal Trade Commission.
Protections Against Discrimination and Other Prohibited
Practices, 2013-07-16T10:34:31-04:00. URL https://www.ftc.gov/policy-notices/
no-fear-act/protections-against-discrimination.
[78] P. Feldman, A. Dant, and D. Rosenbluth. Ethics, Rules of Engagement, and AI: Neural
Narrative Mapping Using Large Transformer Language Models, Feb. 2022. URL http:
//arxiv.org/abs/2202.02647. arXiv:2202.02647 [cs].
[79] A. Felstiner. Working the Crowd: Employment and Labor Law in the Crowdsourcing Industry.
Berkeley Journal of Employment and Labor Law, 32(1):143–203, 2011. ISSN 1067-7666.
URL https://www.jstor.org/stable/24052509. Publisher: University of California,
Berkeley, School of Law.
26

[80] G. Fergusson, C. Fitzgerald, C. Frascella, M. Iorio, T. McBrien, C. Schroeder, B. Win-
ters, and E. Zhou.
Generating Harms:
Generative AI’s Impact & Paths Forward –
EPIC – Electronic Privacy Information Center.
URL https://epic.org/documents/
generating-harms-generative-ais-impact-paths-forward/.
[81] A. Field, S. L. Blodgett, Z. Waseem, and Y. Tsvetkov. A Survey of Race, Racism, and
Anti-Racism in NLP. In Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 1905–1925. Association for Computational
Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.149. URL https://aclanthology.
org/2021.acl-long.149.
[82] E. E. Fitzsimmons-Craft, W. W. Chan, A. C. Smith, M.-L. Firebaugh, L. A. Fowler, N. Topooco,
B. DePietro, D. E. Wilfley, C. B. Taylor, and N. C. Jacobson.
Effectiveness of a chat-
bot for eating disorders prevention: A randomized clinical trial.
International Journal
of Eating Disorders, 55(3):343–353, 2022.
ISSN 1098-108X.
doi: 10.1002/eat.23662.
URL https://onlinelibrary.wiley.com/doi/abs/10.1002/eat.23662.
_eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/eat.23662.
[83] S. A. Friedler, C. Scheidegger, and S. Venkatasubramanian. The (Im)possibility of fairness:
Different value systems require different mechanisms for fair decision making. 64(4):136–143,
2021-04. ISSN 0001-0782, 1557-7317. doi: 10.1145/3433949. URL https://dl.acm.org/
doi/10.1145/3433949.
[84] B. Friedman, P. H. Kahn, A. Borning, and A. Huldtgren.
Value Sensitive Design and
Information Systems.
In N. Doorn, D. Schuurbiers, I. van de Poel, and M. E. Gor-
man, editors, Early engagement and new technologies: Opening up the laboratory, Phi-
losophy of Engineering and Technology, pages 55–95. Springer Netherlands, Dordrecht,
2013.
ISBN 978-94-007-7844-3.
doi: 10.1007/978-94-007-7844-3_4.
URL https:
//doi.org/10.1007/978-94-007-7844-3_4.
[85] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell,
N. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A
framework for few-shot language model evaluation, Sept. 2021. URL https://doi.org/
10.5281/zenodo.5371628.
[86] T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii, and K. Crawford.
Datasheets for datasets. 64(12):86–92, 2021-12. ISSN 0001-0782, 1557-7317. doi: 10.1145/
3458723. URL https://dl.acm.org/doi/10.1145/3458723.
[87] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Eval-
uating Neural Toxic Degeneration in Language Models. In Findings of the Association
for Computational Linguistics: EMNLP 2020, pages 3356–3369. Association for Com-
putational Linguistics, 2020.
doi: 10.18653/v1/2020.findings-emnlp.301.
URL https:
//www.aclweb.org/anthology/2020.findings-emnlp.301.
[88] R. S. Geiger, K. Yu, Y. Yang, M. Dai, J. Qiu, R. Tang, and J. Huang. Garbage in, garbage
out?: Do machine learning application papers in social computing report where human-labeled
training data comes from? In Proceedings of the 2020 Conference on Fairness, Accountability,
and Transparency, pages 325–336. ACM, 2020-01-27. ISBN 978-1-4503-6936-7. doi: 10.
1145/3351095.3372862. URL https://dl.acm.org/doi/10.1145/3351095.3372862.
[89] A. Glaese, N. McAleese, M. Tr˛ebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Wei-
dinger, M. Chadwick, P. Thacker, L. Campbell-Gillingham, J. Uesato, P.-S. Huang, R. Co-
manescu, F. Yang, A. See, S. Dathathri, R. Greig, C. Chen, D. Fritz, J. S. Elias, R. Green,
S. Mokrá, N. Fernando, B. Wu, R. Foley, S. Young, I. Gabriel, W. Isaac, J. Mellor, D. Hassabis,
K. Kavukcuoglu, L. A. Hendricks, and G. Irving. Improving alignment of dialogue agents via
targeted human judgements, 2022-09-28. URL http://arxiv.org/abs/2209.14375.
[90] W. Godel, Z. Sanderson, K. Aslett, J. Nagler, R. Bonneau, N. Persily, and J. A. Tucker.
Moderating with the Mob: Evaluating the Efficacy of Real-Time Crowdsourced Fact-Checking.
Journal of Online Trust and Safety, 1(1), Oct. 2021. ISSN 2770-3142. doi: 10.54501/jots.v1i1.
15. URL https://tsjournal.org/index.php/jots/article/view/15. Number: 1.
27

[91] S. Goldfarb-Tarrant, E. Ungless, E. Balkir, and S. L. Blodgett. This prompt is measuring
<mask>: Evaluating bias evaluation in language models, 2023.
[92] J. A. Goldstein, G. Sastry, M. Musser, R. DiResta, M. Gentzel, and K. Sedova. Generative
Language Models and Automated Influence Operations: Emerging Threats and Potential
Mitigations, Jan. 2023. URL http://arxiv.org/abs/2301.04246. arXiv:2301.04246
[cs].
[93] M. L. Gray and S. Suri. Ghost Work: How to Stop Silicon Valley from Building a New Global
Underclass. Houghton Mifflin Harcourt, 2019. ISBN 978-0-358-12057-5 978-1-328-56624-9.
[94] Y. Green, A. Gully, Y. Roth, A. Roy, J. A. Tucker, and A. Wanless.
Evidence-
Based Misinformation Interventions:
Challenges and Opportunities for Measure-
ment and Collaboration.
URL https://carnegieendowment.org/2023/01/09/
evidence-based-misinformation-interventions-challenges-and-opportunities-for-measurement-an
[95] A. M. Guess, P. Barberá, S. Munzert, and J. Yang. The consequences of online partisan media.
Proceedings of the National Academy of Sciences, 118(14):e2013464118, Apr. 2021. doi: 10.
1073/pnas.2013464118. URL https://www.pnas.org/doi/10.1073/pnas.2013464118.
Publisher: Proceedings of the National Academy of Sciences.
[96] U. Gupta, Y. G. Kim, S. Lee, J. Tse, H.-H. S. Lee, G.-Y. Wei, D. Brooks, and C.-J. Wu. Chasing
Carbon: The Elusive Environmental Footprint of Computing. 42(4):37–47, 2022-07-01. ISSN
0272-1732, 1937-4143. doi: 10.1109/MM.2022.3163226. URL https://ieeexplore.
ieee.org/document/9744492/.
[97] O. L. Haimson, D. Delmonaco, P. Nie, and A. Wegner. Disproportionate Removals and
Differing Content Moderation Experiences for Conservative, Transgender, and Black Social
Media Users: Marginalization and Moderation Gray Areas. Proceedings of the ACM on
Human-Computer Interaction, 5(CSCW2):466:1–466:35, Oct. 2021. doi: 10.1145/3479610.
URL https://dl.acm.org/doi/10.1145/3479610.
[98] K. Hao and H. Andrea Paola.
How the AI industry profits from catastro-
phe,
2022.
URL https://www.technologyreview.com/2022/04/20/1050392/
ai-industry-appen-scale-data-labels/.
[99] H. Harreis, T. Koullias, R. Roberts, and K. Te.
Generative AI in fashion | McKinsey,
Mar. 2023. URL https://www.mckinsey.com/industries/retail/our-insights/
generative-ai-unlocking-the-future-of-fashion.
[100] C. N. Harrington, R. Garg, A. Woodward, and D. Williams. “It’s Kind of Like Code-Switching”:
Black Older Adults’ Experiences with a Voice Assistant for Health Information Seeking. In
CHI Conference on Human Factors in Computing Systems, pages 1–15, New Orleans LA
USA, Apr. 2022. ACM. ISBN 978-1-4503-9157-3. doi: 10.1145/3491102.3501995. URL
https://dl.acm.org/doi/10.1145/3491102.3501995.
[101] T. Hartvigsen, S. Gabriel, H. Palangi, M. Sap, D. Ray, and E. Kamar. ToxiGen: A Large-Scale
Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection, July 2022.
URL http://arxiv.org/abs/2203.09509. arXiv:2203.09509 [cs].
[102] A. Hawkins. GENERATION GENERATION - USAASC, Feb. 2023. URL https://asc.
army.mil/web/news-generation-generation/.
[103] W. D. Heaven.
Why Meta’s latest large language model survived only three
days online.
URL https://www.technologyreview.com/2022/11/18/1063487/
meta-large-language-model-ai-only-survived-three-days-gpt-3-science/.
[104] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau.
Towards the
Systematic Reporting of the Energy and Carbon Footprints of Machine Learning. URL
http://arxiv.org/abs/2002.05651.
[105] P. Henderson, J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau.
Towards the
Systematic Reporting of the Energy and Carbon Footprints of Machine Learning, Nov. 2022.
URL http://arxiv.org/abs/2002.05651. arXiv:2002.05651 [cs].
28

[106] M.
Hirsh.
How
AI
Will
Revolutionize
Warfare,
Apr.
2023.
URL
https://foreignpolicy.com/2023/04/11/
ai-arms-race-artificial-intelligence-chatgpt-military-technology/.
[107] L. Hofeditz, M. Mirbabaie, S. Stieglitz, and J. Holstein. Do you Trust an AI-Journalist? A
Credibility Analysis of News Content with AI-Authorship. June 2021.
[108] S. Hooker. Moving beyond “algorithmic bias is a data problem”. 2(4):100241, 2021-04. ISSN
26663899. doi: 10.1016/j.patter.2021.100241. URL https://linkinghub.elsevier.
com/retrieve/pii/S2666389921000611.
[109] S. Hooker, N. Moorosi, G. Clark, S. Bengio, and E. Denton. Characterising Bias in Compressed
Models, 2020-12-18. URL http://arxiv.org/abs/2010.03058.
[110] D. Hovy and S. L. Spruit. The Social Impact of Natural Language Processing. In Proceedings
of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 591–598. Association for Computational Linguistics, 2016. doi: 10.18653/v1/
P16-2096. URL http://aclweb.org/anthology/P16-2096.
[111] A. T. I. Human. AI and Human Rights: Building a Tech Future Aligned With the Public Interest
— All Tech Is Human. URL https://alltechishuman.org/ai-human-rights-report.
[112] B. Hutchinson and M. Mitchell. 50 Years of Test (Un)fairness: Lessons for Machine Learning.
In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 49–
58. ACM, 2019-01-29. ISBN 978-1-4503-6125-5. doi: 10.1145/3287560.3287600. URL
https://dl.acm.org/doi/10.1145/3287560.3287600.
[113] B. Hutchinson, N. Rostamzadeh, C. Greer, K. Heller, and V. Prabhakaran. Evaluation Gaps
in Machine Learning Practice. In 2022 ACM Conference on Fairness, Accountability, and
Transparency, pages 1859–1876. ACM, 2022-06-21. ISBN 978-1-4503-9352-2. doi: 10.1145/
3531146.3533233. URL https://dl.acm.org/doi/10.1145/3531146.3533233.
[114] Institute of Medicine (US) Committee on Understanding and Eliminating Racial and Ethnic
Disparities in Health Care. Unequal Treatment: Confronting Racial and Ethnic Disparities
in Health Care. National Academies Press (US), Washington (DC), 2003. URL http:
//www.ncbi.nlm.nih.gov/books/NBK220358/.
[115] R. International.
How time-stamping works in eac markets.
Jan 2021.
URL https:
//recs.org/download/?file=How-time-stamping-works-in-EAC-markets.pdf&
file_type=documents.
[116] J. James, V. Yogarajan, I. Shields, C. Watson, P. Keegan, K. Mahelona, and P.-L. Jones.
Language Models for Code-switch Detection of te reo M¯aori and English in a Low-resource
Setting. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 650–
660, Seattle, United States, 2022. Association for Computational Linguistics. doi: 10.18653/v1/
2022.findings-naacl.49. URL https://aclanthology.org/2022.findings-naacl.49.
[117] Y. Jernite. Let’s talk about biases in machine learning! Ethics and Society Newsletter #2, 2022.
URL https://huggingface.co/blog/ethics-soc-2.
[118] Y. Jernite, H. Nguyen, S. Biderman, A. Rogers, M. Masoud, V. Danchev, S. Tan, A. S. Luccioni,
N. Subramani, I. Johnson, G. Dupont, J. Dodge, K. Lo, Z. Talat, D. Radev, A. Gokaslan,
S. Nikpoor, P. Henderson, R. Bommasani, and M. Mitchell. Data governance in the age of large-
scale data-driven language technology. In 2022 ACM Conference on Fairness, Accountability,
and Transparency. ACM, jun 2022. doi: 10.1145/3531146.3534637. URL https://doi.
org/10.1145%2F3531146.3534637.
[119] P. Jones. Work without the Worker: Labour in the Age of Platform Capitalism. Verso, 2021.
ISBN 978-1-83976-043-3.
[120] L. H. Kaack, P. L. Donti, E. Strubell, G. Kamiya, F. Creutzig, and D. Rolnick. Aligning
artificial intelligence with climate change mitigation. 12(6):518–527, 2022-06. ISSN 1758-
678X, 1758-6798. doi: 10.1038/s41558-022-01377-7. URL https://www.nature.com/
articles/s41558-022-01377-7.
29

[121] P. Kalluri.
Don’t ask if artificial intelligence is good or fair,
ask how it
shifts power.
Nature, 583(7815):169–169, 2020-07-09.
ISSN 0028-0836, 1476-
4687. doi: 10.1038/d41586-020-02003-2. URL http://www.nature.com/articles/
d41586-020-02003-2.
[122] M. Kamal and W. Newman. Revenge Pornography: Mental Health Implications and Related
Legislation. The journal of the American Academy of Psychiatry and the Law, 44:359–367,
Sept. 2016.
[123] M. E. Kaminski. Regulating the Risks of AI. Boston University Law Review, 103, Forthcoming.
ISSN 1556-5068. doi: 10.2139/ssrn.4195066. URL https://www.ssrn.com/abstract=
4195066.
[124] F. Karimi. ’Mom, these bad men have me’: She believes scammers cloned her daughter’s
voice in a fake kidnapping, Apr. 2023. URL https://www.cnn.com/2023/04/29/us/
ai-scam-calls-kidnapping-cec/index.html.
[125] J. Katzman, A. Wang, M. Scheuerman, S. L. Blodgett, K. Laird, H. Wallach, and S. Barocas.
Taxonomizing and Measuring Representational Harms: A Look at Image Tagging, 2023-05-02.
URL http://arxiv.org/abs/2305.01776.
[126] K. KELLEY, B. CLARK, V. BROWN, and J. SITZIA.
Good practice in the conduct
and reporting of survey research. International Journal for Quality in Health Care, 15
(3):261–266, May 2003. ISSN 1353-4505. doi: 10.1093/intqhc/mzg031. URL https:
//doi.org/10.1093/intqhc/mzg031. _eprint: https://academic.oup.com/intqhc/article-
pdf/15/3/261/5251095/mzg031.pdf.
[127] H. Khlaaf, P. Mishkin, J. Achiam, G. Krueger, and M. Brundage. A Hazard Analysis Frame-
work for Code Synthesis Large Language Models, July 2022. URL http://arxiv.org/
abs/2207.14157. arXiv:2207.14157 [cs].
[128] J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Goldstein. A Watermark
for Large Language Models, June 2023.
URL http://arxiv.org/abs/2301.10226.
arXiv:2301.10226 [cs].
[129] W.-Y. Ko, D. D’souza, K. Nguyen, R. Balestriero, and S. Hooker. FAIR-Ensemble: When
Fairness Naturally Emerges From Deep Ensembling, 2023-03-01. URL http://arxiv.org/
abs/2303.00586.
[130] A. Koenecke, A. Nam, E. Lake, J. Nudell, M. Quartey, Z. Mengesha, C. Toups, J. R. Rickford,
D. Jurafsky, and S. Goel. Racial disparities in automated speech recognition. 117(14):7684–
7689, 2020-04-07. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1915768117. URL
https://pnas.org/doi/full/10.1073/pnas.1915768117.
[131] S. M. Labott, T. P. Johnson, M. Fendrich, and N. C. Feeny. Emotional Risks to Respondents in
Survey Research: Some Empirical Evidence. Journal of empirical research on human research
ethics : JERHRE, 8(4):53–66, Oct. 2013. ISSN 1556-2646. doi: 10.1525/jer.2013.8.4.53.
URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3996452/.
[132] A. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres. Quantifying the Carbon Emissions of
Machine Learning. 2019. doi: 10.48550/ARXIV.1910.09700. URL https://arxiv.org/
abs/1910.09700.
[133] J. Lalor, Y. Yang, K. Smith, N. Forsgren, and A. Abbasi. Benchmarking Intersectional Biases
in NLP. In Proceedings of the 2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pages 3598–3609.
Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.263.
URL https://aclanthology.org/2022.naacl-main.263.
[134] J. Larson, S. Mattu, J. Angwin, and L. Kirchner.
How We Analyzed the COM-
PAS Recidivism Algorithm, 2016.
URL https://www.propublica.org/article/
how-we-analyzed-the-compas-recidivism-algorithm.
30

[135] J. Lepawsky. No insides on the outsides, Sept. 2019. URL https://discardstudies.com/
2019/09/23/no-insides-on-the-outsides/.
[136] D. I. Levine. Automation as Part of the Solution. Journal of Management Inquiry, 28(3):
316–318, July 2019. ISSN 1056-4926. doi: 10.1177/1056492619827375. URL https:
//doi.org/10.1177/1056492619827375. Publisher: SAGE Publications Inc.
[137] C. Li. OpenAI’s GPT-3 Language Model: A Technical Overview, 2020-06-03. URL https:
//lambdalabs.com/blog/demystifying-gpt-3.
[138] T. Li, D. Khashabi, T. Khot, A. Sabharwal, and V. Srikumar.
UNQOVERing stereo-
typing biases via underspecified questions.
In Findings of the Association for Compu-
tational Linguistics: EMNLP 2020, pages 3475–3489, Online, Nov. 2020. Association
for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.311. URL https:
//aclanthology.org/2020.findings-emnlp.311.
[139] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan,
Y. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. Ré,
D. Acosta-Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao,
J. Wang, K. Santhanam, L. Orr, L. Zheng, M. Yuksekgonul, M. Suzgun, N. Kim, N. Guha,
N. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Ganguli,
T. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang, and
Y. Koreeda. Holistic Evaluation of Language Models. 2022. doi: 10.48550/ARXIV.2211.09110.
URL https://arxiv.org/abs/2211.09110.
[140] Z. Liu, U. Iqbal, and N. Saxena. Opted Out, Yet Tracked: Are Regulations Enough to Protect
Your Privacy?, Feb. 2023. URL http://arxiv.org/abs/2202.00885. arXiv:2202.00885
[cs].
[141] A. S. Luccioni, S. Viguier, and A.-L. Ligozat. Estimating the Carbon Footprint of BLOOM, a
176B Parameter Language Model, Nov. 2022. URL http://arxiv.org/abs/2211.02001.
arXiv:2211.02001 [cs].
[142] V. Malik, S. Dev, A. Nishi, N. Peng, and K.-W. Chang. Socially Aware Bias Measurements for
Hindi Language Representations. In Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 1041–1052. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.
naacl-main.76. URL https://aclanthology.org/2022.naacl-main.76.
[143] H. Margetts, F. Enock, M. Cross, A. Peppin, R. Modhvadia, A. Colom, A. Strait, O. Reeve,
P. Sturgis, K. Kostadintcheva, and O. Bosch-Jover. How do people feel about AI? Tech-
nical report, June 2023.
URL https://www.adalovelaceinstitute.org/report/
public-attitudes-ai/.
[144] K. Martin. The penalty for privacy violations: How privacy violations impact trust online.
Journal of Business Research, 82:103–116, Jan. 2018. ISSN 0148-2963. doi: 10.1016/j.
jbusres.2017.08.034. URL https://www.sciencedirect.com/science/article/pii/
S0148296317302965.
[145] N. Maslej, L. Fattorini, E. Brynjolfsson, J. Etchemendy, K. Ligett, T. Lyons, J. Manyika,
H. Ngo, J. C. Niebles, V. Parli, Y. Shoham, R. Wald, J. Clark, and R. Perrault. The AI Index
2023 Annual Report. Technical report, Stanford University, Stanford, CA, Apr. 2023. URL
https://aiindex.stanford.edu/.
[146] C. Merrill, J. Timberg, J. B. Kao, and C. Silverman.
Facebook Hosted Surge
of Misinformation and Insurrection Threats in Months Leading Up to Jan. 6 At-
tack, Records Show, Jan. 2022.
URL https://www.propublica.org/article/
facebook-hosted-surge-of-misinformation-and-insurrection-threats-in-months-leading-up-to-j
[147] M. Miceli, M. Schuessler, and T. Yang. Between Subjectivity and Imposition: Power Dynamics
in Data Annotation for Computer Vision. 4:1–25, 2020-10-14. ISSN 2573-0142. doi:
10.1145/3415186. URL https://dl.acm.org/doi/10.1145/3415186.
31

[148] Minister of Innovation Science and Industry. An Act to enact the Consumer Privacy Protection
Act, the Personal Information and Data Protection Tribunal Act and the Artificial Intelligence
and Data Act and to make consequential and related amendments to other Acts, 2022. URL
https://www.parl.ca/DocumentViewer/en/44-1/bill/C-27/first-reading.
[149] P. Mishkin, L. Ahmad, M. Brundage, G. Krueger, and G. Sastry. Dall·e 2 preview - risks
and limitations. 2022. URL [https://github.com/openai/dalle-2-preview/blob/
main/system-card.md](https://github.com/openai/dalle-2-preview/blob/
main/system-card.md).
[150] S. Mohamed, M.-T. Png, and W. Isaac. Decolonial AI: Decolonial Theory as Sociotechnical
Foresight in Artificial Intelligence. Philosophy & Technology, 33(4):659–684, Dec. 2020.
ISSN 2210-5433, 2210-5441. doi: 10.1007/s13347-020-00405-8. URL http://arxiv.org/
abs/2007.04068. arXiv:2007.04068 [cs, stat].
[151] I. Mollas, Z. Chrysopoulou, S. Karlos, and G. Tsoumakas.
ETHOS: A multi-label
hate speech detection dataset.
8(6):4663–4678, 2022-12.
ISSN 2199-4536, 2198-
6053. doi: 10.1007/s40747-021-00608-2. URL https://link.springer.com/10.1007/
s40747-021-00608-2.
[152] B. Moore Jr.
Privacy:
Studies in Social and Cultural History.
Routledge, 3 edi-
tion,
1984.
ISBN 978-1-138-04526-2.
URL
https://www.routledge.com/
Privacy-Studies-in-Social-and-Cultural-History/Moore-Jr/p/book/
9781138045262.
[153] C.
Morris.
National
Eating
Disorder
Association
will
utilize
an
AI
chat-
bot
|
Fortune
Well,
2023.
URL
https://fortune.com/well/2023/05/26/
national-eating-disorder-association-ai-chatbot-tessa/.
[154] M. Muro and S. Liu. The geography of AI, Aug. 2021. URL https://www.brookings.
edu/research/the-geography-of-ai/.
[155] Murphy. Predators Exploit AI Tools to Generate Images of Child Abuse. Bloomberg.com,
May
2023.
URL
https://www.bloomberg.com/news/articles/2023-05-23/
predators-exploit-ai-tools-to-depict-abuse-prompting-warnings.
[156] M. Nadeem, A. Bethke, and S. Reddy. Stereoset: Measuring stereotypical bias in pretrained
language models, 2020.
[157] N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman. CrowS-pairs: A challenge dataset for
measuring social biases in masked language models. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1967, Online, Nov.
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.154.
URL https://aclanthology.org/2020.emnlp-main.154.
[158] C. Nast. M¯aori are trying to save their language from Big Tech. Wired UK. ISSN 1357-0978.
URL https://www.wired.co.uk/article/maori-language-tech. Section: tags.
[159] National Institute of Standards and Technology. AI Risk Management Framework: AI RMF
(1.0), 2023. URL https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf.
[160] T. Nelson, N. Kagan, C. Critchlow, A. Hillard, and A. Hsu. The Danger of Misinformation in
the COVID-19 Crisis. Missouri Medicine, 117(6):510–512, 2020. ISSN 0026-6620. URL
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7721433/.
[161] J.
Newman.
A
Taxonomy
of
Trustworthiness
for
Artificial
Intelli-
gence.
Jan.
2023.
URL
https://cltc.berkeley.edu/publication/
a-taxonomy-of-trustworthiness-for-artificial-intelligence/.
[162] A.
Nguyen
and
A.
Mateescu.
Explainer:
Algorithmic
Management
in
the
Workplace,
Feb.
2019.
URL
https://datasociety.net/library/
explainer-algorithmic-management-in-the-workplace/.
Publisher:
Data &
Society Research Institute.
32

[163] D. Nikolaiev.
Behind the Millions:
Estimating the Scale of Large Language
Models,
2023-04-28T17:53:12.
URL
https://towardsdatascience.com/
behind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b.
[164] J. Niu, W. Tang, F. Xu, X. Zhou, and Y. Song. Global Research on Artificial Intelligence
from 1990–2014: Spatially-Explicit Bibliometric Analysis. ISPRS International Journal of
Geo-Information, 5(5):66, May 2016. ISSN 2220-9964. doi: 10.3390/ijgi5050066. URL
https://www.mdpi.com/2220-9964/5/5/66. Number: 5 Publisher: Multidisciplinary
Digital Publishing Institute.
[165] D. Nozza, F. Bianchi, and D. Hovy. HONEST: Measuring Hurtful Sentence Completion in
Language Models. In Proceedings of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pages 2398–2406.
Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.191.
URL https://aclanthology.org/2021.naacl-main.191.
[166] D. Nozza, F. Bianchi, and D. Hovy. Pipelines for social bias testing of large language
models. In Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives
in Creating Large Language Models, pages 68–74, virtual+Dublin, May 2022. Association
for Computational Linguistics.
doi: 10.18653/v1/2022.bigscience-1.6.
URL https://
aclanthology.org/2022.bigscience-1.6.
[167] Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan. Dissecting racial bias in an algorithm
used to manage the health of populations. Science (New York, N.Y.), 366(6464):447–453, Oct.
2019. ISSN 1095-9203. doi: 10.1126/science.aax2342.
[168] OECD Policy Observatory. OECD Framework for the Classification of AI Systems: A tool for
effective AI policies, 2023-04-27. URL https://oecd.ai/en/classification.
[169] K. Ogueji, O. Ahia, G. Onilude, S. Gehrmann, S. Hooker, and J. Kreutzer. Intriguing properties
of compression on multilingual models. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, pages 9092–9110. Association for Computational
Linguistics, 2022-12. URL https://aclanthology.org/2022.emnlp-main.619.
[170] OpenAI. DALL-E 2 Preview - Risks and Limitations, 2022. URL https://github.com/
openai/dalle-2-preview.
[171] OpenAI. GPT-4 Technical Report, Mar. 2023. URL http://arxiv.org/abs/2303.08774.
arXiv:2303.08774 [cs].
[172] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agar-
wal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell,
P. Welinder, P. Christiano, J. Leike, and R. Lowe.
Training language models to fol-
low instructions with human feedback. 2022. doi: 10.48550/ARXIV.2203.02155. URL
https://arxiv.org/abs/2203.02155.
[173] A. Ovalle, P. Goyal, J. Dhamala, Z. Jaggers, K.-W. Chang, A. Galstyan, R. Zemel, and
R. Gupta. "I’m fully who I am": Towards Centering Transgender and Non-Binary Voices to
Measure Biases in Open Language Generation, June 2023. URL http://arxiv.org/abs/
2305.09941. arXiv:2305.09941 [cs].
[174] R. Parasuraman and V. Riley. Humans and Automation: Use, Misuse, Disuse, Abuse. Human
Factors, 39(2):230–253, June 1997. ISSN 0018-7208. doi: 10.1518/001872097778543886.
URL https://doi.org/10.1518/001872097778543886. Publisher: SAGE Publications
Inc.
[175] A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thompson, P. M. Htut, and
S. Bowman. BBQ: A hand-built bias benchmark for question answering. In Findings of
the Association for Computational Linguistics: ACL 2022, pages 2086–2105. Association
for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-acl.165. URL https:
//aclanthology.org/2022.findings-acl.165.
33

[176] Partnership on AI. ABOUT ML Resources Library. URL https://partnershiponai.
org/about-ml-resources-library/.
[177] J. Paschen. Investigating the emotional appeal of fake news using artificial intelligence
and human contributions. Journal of Product & Brand Management, 29, May 2019. doi:
10.1108/JPBM-12-2018-2179.
[178] S. Passi and M. Vorvoreanu. Overreliance on AI: Literature Review. Technical Report
MSR-TR-2022-12, Microsoft, June 2022. URL https://www.microsoft.com/en-us/
research/publication/overreliance-on-ai-literature-review/.
[179] G. Pennycook, T. D. Cannon, and D. G. Rand. Prior exposure increases perceived accuracy
of fake news. Journal of Experimental Psychology. General, 147(12):1865–1880, Dec. 2018.
ISSN 1939-2222. doi: 10.1037/xge0000465.
[180] G. Pennycook, Z. Epstein, M. Mosleh, A. A. Arechar, D. Eckles, and D. G. Rand. Shifting
attention to accuracy can reduce misinformation online. Nature, 592(7855):590–595, Apr.
2021. ISSN 1476-4687. doi: 10.1038/s41586-021-03344-2. URL https://www.nature.
com/articles/s41586-021-03344-2. Number: 7855 Publisher: Nature Publishing Group.
[181] B. Perrigo.
Inside Facebook’s African Sweatshop, 2022.
URL https://time.com/
6147458/facebook-africa-content-moderation-employee-treatment/.
[182] Perspective API. Perspective API. URL https://www.perspectiveapi.com/#/home.
[183] A. Piktus, C. Akiki, P. Villegas, H. Laurençon, G. Dupont, A. S. Luccioni, Y. Jernite, and
A. Rogers. The ROOTS Search Tool: Data Transparency for LLMs, 2023-02-27. URL
http://arxiv.org/abs/2302.14035.
[184] V. Polonski.
AI trust and AI fears:
A media debate that could divide so-
ciety,
Jan.
2018.
URL
https://www.oii.ox.ac.uk/news-events/news/
ai-trust-and-ai-fears-a-media-debate-that-could-divide-society.
[185] L. Pozzobon, B. Ermis, P. Lewis, and S. Hooker. On the Challenges of Using Black-Box
APIs for Toxicity Evaluation in Research, 2023-04-24. URL http://arxiv.org/abs/2304.
12397.
[186] V. Prabhakaran, Z. Waseem, S. Akiwowo, and B. Vidgen. Online Abuse and Human Rights:
WOAH Satellite Session at RightsCon 2020. In Proceedings of the Fourth Workshop on Online
Abuse and Harms, pages 1–6, Online, Nov. 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.alw-1.1. URL https://aclanthology.org/2020.alw-1.1.
[187] V. Prabhakaran, A. Mostafazadeh Davani, and M. Diaz. On Releasing Annotator-Level
Labels and Information in Datasets. In Proceedings of The Joint 15th Linguistic Annotation
Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop, pages 133–
138. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.law-1.14. URL
https://aclanthology.org/2021.law-1.14.
[188] J. Prassl and M. Risak. The Legal Protection of Crowdworkers: Four Avenues for Workers’
Rights in the Virtual Realm. In P. Meil and V. Kirov, editors, Policy Implications of Virtual
Work, Dynamics of Virtual Work, pages 273–295. Springer International Publishing, Cham,
2017. ISBN 978-3-319-52057-5. doi: 10.1007/978-3-319-52057-5_11. URL https://doi.
org/10.1007/978-3-319-52057-5_11.
[189] S. Quach, P. Thaichon, K. D. Martin, S. Weaven, and R. W. Palmatier. Digital technologies:
tensions in privacy and data. Journal of the Academy of Marketing Science, 50(6):1299–
1323, Nov. 2022.
ISSN 1552-7824.
doi: 10.1007/s11747-022-00845-y.
URL https:
//doi.org/10.1007/s11747-022-00845-y.
[190] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud,
D. Theron, and P. Barnes. Closing the AI accountability gap: Defining an end-to-end frame-
work for internal algorithmic auditing. In Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency, pages 33–44. ACM, 2020-01-27. ISBN 978-1-4503-6936-
7. doi: 10.1145/3351095.3372873. URL https://dl.acm.org/doi/10.1145/3351095.
3372873.
34

[191] I. D. Raji, E. M. Bender, A. Paullada, E. Denton, and A. Hanna.
AI and the Ev-
erything in the Whole Wide World Benchmark.
In Proceedings of the Neural Infor-
mation Processing Systems Track on Datasets and Benchmarks. Curran, 2021.
URL
https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/
2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf.
[192] A. Rapp, L. Curti, and A. Boldi. The human side of human-chatbot interaction: A systematic
literature review of ten years of research on text-based chatbots.
International Journal
of Human-Computer Studies, 151:102630, July 2021. ISSN 1071-5819. doi: 10.1016/j.
ijhcs.2021.102630. URL https://www.sciencedirect.com/science/article/pii/
S1071581921000483.
[193] M. Rauh, J. Mellor, J. Uesato, P.-S. Huang, J. Welbl, L. Weidinger, S. Dathathri, A. Glaese,
G. Irving, I. Gabriel, W. Isaac, and L. A. Hendricks. Characteristics of harmful text: Towards
rigorous benchmarking of language models, 2022.
[194] T. Ray. Common but Different Futures: AI Inequity and Climate Change. URL https:
//www.orfonline.org/research/common-but-different-futures/.
[195] Read. What is the difference between Scope 1, 2 and 3 emissions, and what are companies
doing to cut all three?, Sept. 2022. URL https://www.weforum.org/agenda/2022/09/
scope-emissions-climate-greenhouse-business/.
[196] Republic of Korea. Input by the Government of the Republic of Korea on the Themes of an
Expert Consultation on the Practical Application of the United Nations Guiding Principles on
Business and Human Rights to the Activities of Technology Companies, 2022.
[197] F. Richter. English Is the Internet’s Universal Language, 2022-02-21. URL https://www.
statista.com/chart/26884/languages-on-the-internet.
[198] R. Righi, S. Samoili, M. López Cobo, M. Vázquez-Prada Baillet, M. Cardona, and G. De Prato.
The AI techno-economic complex System: Worldwide landscape, thematic subdomains and
technological collaborations. Telecommunications Policy, 44(6):101943, July 2020. ISSN
0308-5961. doi: 10.1016/j.telpol.2020.101943. URL https://www.sciencedirect.com/
science/article/pii/S0308596120300355.
[199] S. T. Roberts. Behind the Screen: Content Moderation in the Shadows of Social Media.
2021. ISBN 978-0-300-26147-9. URL https://yalebooks.yale.edu/9780300261479/
behind-the-screen.
[200] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image
Synthesis with Latent Diffusion Models, Apr. 2022. URL http://arxiv.org/abs/2112.
10752. arXiv:2112.10752 [cs].
[201] K. Roose. A Conversation With Bing’s Chatbot Left Me Deeply Unsettled. The New York
Times, Feb. 2023.
ISSN 0362-4331.
URL https://www.nytimes.com/2023/02/16/
technology/bing-chatbot-microsoft-chatgpt.html.
[202] D. Rozado. RightWingGPT – An AI Manifesting the Opposite Political Biases of ChatGPT,
2023-02-16. URL https://davidrozado.substack.com/p/rightwinggpt.
[203] J. Sablosky. “Dangerous organizations: Facebook’s content moderation decisions and eth-
nic visibility in Myanmar”.
43(6):1017–1042, 2021-09.
ISSN 0163-4437, 1460-3675.
doi: 10.1177/0163443720987751. URL http://journals.sagepub.com/doi/10.1177/
0163443720987751.
[204] V. S. Sadasivan, A. Kumar, S. Balasubramanian, W. Wang, and S. Feizi. Can AI-Generated
Text be Reliably Detected?, Mar. 2023.
URL http://arxiv.org/abs/2303.11156.
arXiv:2303.11156 [cs].
[205] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan,
S. S. Mahdavi, R. G. Lopes, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi. Photorealistic
Text-to-Image Diffusion Models with Deep Language Understanding, 2022-05-23. URL
http://arxiv.org/abs/2205.11487.
35

[206] N. Sambasivan, E. Arnesen, B. Hutchinson, T. Doshi, and V. Prabhakaran. Re-imagining
Algorithmic Fairness in India and Beyond. In Proceedings of the 2021 ACM Conference
on Fairness, Accountability, and Transparency, pages 315–328. ACM, 2021-03-03. ISBN
978-1-4503-8309-7. doi: 10.1145/3442188.3445896. URL https://dl.acm.org/doi/10.
1145/3442188.3445896.
[207] S. Santurkar, E. Durmus, F. Ladhak, C. Lee, P. Liang, and T. Hashimoto. Whose opinions do
language models reflect?, 2023.
[208] M. Sap, D. Card, S. Gabriel, Y. Choi, and N. A. Smith. The Risk of Racial Bias in Hate Speech
Detection. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pages 1668–1678. Association for Computational Linguistics, 2019. doi: 10.
18653/v1/P19-1163. URL https://www.aclweb.org/anthology/P19-1163.
[209] M. Sap, S. Swayamdipta, L. Vianna, X. Zhou, Y. Choi, and N. Smith. Annotators with Attitudes:
How Annotator Beliefs And Identities Bias Toxic Language Detection. In Proceedings of
the 2022 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 5884–5906. Association for Computational
Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.431. URL https://aclanthology.
org/2022.naacl-main.431.
[210] M. K. Scheuerman, A. Hanna, and E. Denton. Do Datasets Have Politics? Disciplinary
Values in Computer Vision Dataset Development. 5:1–37, 2021-10-13. ISSN 2573-0142. doi:
10.1145/3476058. URL https://dl.acm.org/doi/10.1145/3476058.
[211] R. Schwartz, A. Vassilev, K. Greene, L. Perine, A. Burt, and P. Hall. Towards a Standard
for Identifying and Managing Bias in Artificial Intelligence, 2022-03-15.
URL https:
//nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf.
[212] A. See, A. Pappu, R. Saxena, A. Yerukola, and C. D. Manning. Do Massively Pretrained
Language Models Make Better Storytellers?
In Proceedings of the 23rd Conference on
Computational Natural Language Learning (CoNLL), pages 843–861, Hong Kong, China,
Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/K19-1079. URL
https://aclanthology.org/K19-1079.
[213] F. Shahid and A. Vashistha. Decolonizing Content Moderation: Does Uniform Global Commu-
nity Standard Resemble Utopian Equality or Western Power Hegemony? In Proceedings of the
2023 CHI Conference on Human Factors in Computing Systems, CHI ’23, pages 1–18, New
York, NY, USA, Apr. 2023. Association for Computing Machinery. ISBN 978-1-4503-9421-5.
doi: 10.1145/3544548.3581538. URL https://doi.org/10.1145/3544548.3581538.
[214] H. Shaiken.
Work transformed; automation and labor in the computer age.
Holt,
Rinehart and Winston, 1985. ISBN 978-0-03-042681-0. URL https://www.abebooks.
com/first-edition/Work-transformed-automation-labor-computer-age/
7635506864/bd.
[215] S. Shan, J. Cryan, E. Wenger, H. Zheng, R. Hanocka, and B. Y. Zhao. Glaze: Protecting artists
from style mimicry by text-to-image models, 2023.
[216] S. Shankar, Y. Halpern, E. Breck, J. Atwood, J. Wilson, and D. Sculley. No Classification
without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing
World. arXiv, 2017-11-22. URL http://arxiv.org/abs/1711.08536.
[217] M.
Shanmugavelan.
The
Case
for
Critical
Caste
and
Technol-
ogy
Studies,
Sept.
2022.
URL
https://points.datasociety.net/
the-case-for-critical-caste-and-technology-studies-b987dcf20c8d.
[218] O. Sharir, B. Peleg, and Y. Shoham. The Cost of Training NLP Models: A Concise Overview.
2020. doi: 10.48550/ARXIV.2004.08900. URL https://arxiv.org/abs/2004.08900.
[219] R. Shelby, S. Rismani, K. Henne, g.-i. family=Moon, given=Ajung, N. Rostamzadeh,
P. Nicholas, N. Yilla, J. Gallegos, A. Smart, E. Garcia, and G. Virk. Identifying Sociotechnical
Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction, 2023-02-08. URL
http://arxiv.org/abs/2210.05791.
36

[220] J. Shi, Y. Liu, P. Zhou, and L. Sun. BadGPT: Exploring Security Vulnerabilities of ChatGPT via
Backdoor Attacks to InstructGPT, Feb. 2023. URL http://arxiv.org/abs/2304.12298.
arXiv:2304.12298 [cs].
[221] Shiller. Narrative Economics. Oct. 2019. ISBN 978-0-691-18229-2. URL https://press.
princeton.edu/books/hardcover/9780691182292/narrative-economics.
[222] J. Shin and S. Chan-Olmsted. User perceptions and trust of explainable machine learning fake
news detectors. International Journal of Communication, 17(0), 2022. ISSN 1932-8036. URL
https://ijoc.org/index.php/ijoc/article/view/19534.
[223] T.
Simonite.
When
It
Comes
to
Gorillas,
Google
Photos
Remains
Blind.
2018.
ISSN
1059-1028.
URL
https://www.wired.com/story/
when-it-comes-to-gorillas-google-photos-remains-blind/.
[224] A. Simpson. On Ethnographic Refusal: Indigeneity, ‘Voice’ and Colonial Citizenship. Junc-
tures, (9), 2007.
[225] M. Skjuve, A. Følstad, K. I. Fostervold, and P. B. Brandtzaeg. My Chatbot Companion - a
Study of Human-Chatbot Relationships. International Journal of Human-Computer Studies,
149:102601, May 2021. ISSN 1071-5819. doi: 10.1016/j.ijhcs.2021.102601. URL https:
//www.sciencedirect.com/science/article/pii/S1071581921000197.
[226] N. A. Smuha.
Beyond the individual: governing ai’s societal harm.
Internet Policy
Review, 10(3), Sep 2021.
URL https://policyreview.info/articles/analysis/
beyond-individual-governing-ais-societal-harm.
[227] I. Solaiman. The Gradient of Generative AI Release: Methods and Considerations. 2023. doi:
10.48550/ARXIV.2302.04844. URL https://arxiv.org/abs/2302.04844.
[228] I. Solaiman and C. Dennison. Process for Adapting Language Models to Society (PALMS)
with Values-Targeted Datasets, 2021-11-23. URL http://arxiv.org/abs/2106.10328.
[229] I. Solaiman, M. Brundage, J. Clark, A. Askell, A. Herbert-Voss, J. Wu, A. Radford, G. Krueger,
J. W. Kim, S. Kreps, M. McCain, A. Newhouse, J. Blazakis, K. McGuffie, and J. Wang.
Release Strategies and the Social Impacts of Language Models, Nov. 2019. URL http:
//arxiv.org/abs/1908.09203. arXiv:1908.09203 [cs].
[230] D. J. Solove. A Taxonomy of Privacy. University of Pennsylvania Law Review, 154(3):
477–564, 2006. ISSN 0041-9907. doi: 10.2307/40041279. URL https://www.jstor.org/
stable/40041279. Publisher: The University of Pennsylvania Law Review.
[231] A. Spirling. Why open-source generative ai models are an ethical way forward for science.
616(7957):413–413, Apr 2023. doi: https://doi.org/10.1038/d41586-023-01295-4.
[232] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro,
A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray,
A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain,
A. Askell, A. Dsouza, A. Slone, A. Rahane, A. S. Iyer, A. Andreassen, A. Madotto, A. Santilli,
A. Stuhlmüller, A. Dai, A. La, A. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta,
A. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes,
A. Kirubarajan, A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karaka¸s,
B. R. Roberts, B. S. Loe, B. Zoph, B. Bojanowski, B. Özyurt, B. Hedayatnia, B. Neyshabur,
B. Inden, B. Stein, B. Ekmekci, B. Y. Lin, B. Howald, C. Diao, C. Dour, C. Stinson, C. Ar-
gueta, C. F. Ramírez, C. Singh, C. Rathkopf, C. Meng, C. Baral, C. Wu, C. Callison-Burch,
C. Waites, C. Voigt, C. D. Manning, C. Potts, C. Ramirez, C. E. Rivera, C. Siro, C. Raffel,
C. Ashcraft, C. Garbacea, D. Sileo, D. Garrette, D. Hendrycks, D. Kilman, D. Roth, D. Free-
man, D. Khashabi, D. Levy, D. M. González, D. Perszyk, D. Hernandez, D. Chen, D. Ippolito,
D. Gilboa, D. Dohan, D. Drakard, D. Jurgens, D. Datta, D. Ganguli, D. Emelin, D. Kleyko,
D. Yuret, D. Chen, D. Tam, D. Hupkes, D. Misra, D. Buzan, D. C. Mollo, D. Yang, D.-H.
Lee, E. Shutova, E. D. Cubuk, E. Segal, E. Hagerman, E. Barnes, E. Donoway, E. Pavlick,
E. Rodola, E. Lam, E. Chu, E. Tang, E. Erdem, E. Chang, E. A. Chi, E. Dyer, E. Jerzak,
E. Kim, E. E. Manyasi, E. Zheltonozhskii, F. Xia, F. Siar, F. Martínez-Plumed, F. Happé,
37

F. Chollet, F. Rong, G. Mishra, G. I. Winata, G. de Melo, G. Kruszewski, G. Parascandolo,
G. Mariani, G. Wang, G. Jaimovitch-López, G. Betz, G. Gur-Ari, H. Galijasevic, H. Kim,
H. Rashkin, H. Hajishirzi, H. Mehta, H. Bogar, H. Shevlin, H. Schütze, H. Yakura, H. Zhang,
H. M. Wong, I. Ng, I. Noble, J. Jumelet, J. Geissinger, J. Kernion, J. Hilton, J. Lee, J. F.
Fisac, J. B. Simon, J. Koppel, J. Zheng, J. Zou, J. Koco´n, J. Thompson, J. Kaplan, J. Radom,
J. Sohl-Dickstein, J. Phang, J. Wei, J. Yosinski, J. Novikova, J. Bosscher, J. Marsh, J. Kim,
J. Taal, J. Engel, J. Alabi, J. Xu, J. Song, J. Tang, J. Waweru, J. Burden, J. Miller, J. U.
Balis, J. Berant, J. Frohberg, J. Rozen, J. Hernandez-Orallo, J. Boudeman, J. Jones, J. B.
Tenenbaum, J. S. Rule, J. Chua, K. Kanclerz, K. Livescu, K. Krauth, K. Gopalakrishnan,
K. Ignatyeva, K. Markert, K. D. Dhole, K. Gimpel, K. Omondi, K. Mathewson, K. Chiafullo,
K. Shkaruta, K. Shridhar, K. McDonell, K. Richardson, L. Reynolds, L. Gao, L. Zhang,
L. Dugan, L. Qin, L. Contreras-Ochando, L.-P. Morency, L. Moschella, L. Lam, L. Noble,
L. Schmidt, L. He, L. O. Colón, L. Metz, L. K. ¸Senel, M. Bosma, M. Sap, M. ter Hoeve,
M. Farooqi, M. Faruqui, M. Mazeika, M. Baturan, M. Marelli, M. Maru, M. J. R. Quintana,
M. Tolkiehn, M. Giulianelli, M. Lewis, M. Potthast, M. L. Leavitt, M. Hagen, M. Schubert,
M. O. Baitemirova, M. Arnaud, M. McElrath, M. A. Yee, M. Cohen, M. Gu, M. Ivanitskiy,
M. Starritt, M. Strube, M. Sw˛edrowski, M. Bevilacqua, M. Yasunaga, M. Kale, M. Cain,
M. Xu, M. Suzgun, M. Tiwari, M. Bansal, M. Aminnaseri, M. Geva, M. Gheini, M. V. T,
N. Peng, N. Chi, N. Lee, N. G.-A. Krakover, N. Cameron, N. Roberts, N. Doiron, N. Nan-
gia, N. Deckers, N. Muennighoff, N. S. Keskar, N. S. Iyer, N. Constant, N. Fiedel, N. Wen,
O. Zhang, O. Agha, O. Elbaghdadi, O. Levy, O. Evans, P. A. M. Casares, P. Doshi, P. Fung,
P. P. Liang, P. Vicol, P. Alipoormolabashi, P. Liao, P. Liang, P. Chang, P. Eckersley, P. M. Htut,
P. Hwang, P. Miłkowski, P. Patil, P. Pezeshkpour, P. Oli, Q. Mei, Q. Lyu, Q. Chen, R. Banjade,
R. E. Rudolph, R. Gabriel, R. Habacker, R. R. Delgado, R. Millière, R. Garg, R. Barnes, R. A.
Saurous, R. Arakawa, R. Raymaekers, R. Frank, R. Sikand, R. Novak, R. Sitelew, R. LeBras,
R. Liu, R. Jacobs, R. Zhang, R. Salakhutdinov, R. Chi, R. Lee, R. Stovall, R. Teehan, R. Yang,
S. Singh, S. M. Mohammad, S. Anand, S. Dillavou, S. Shleifer, S. Wiseman, S. Gruetter, S. R.
Bowman, S. S. Schoenholz, S. Han, S. Kwatra, S. A. Rous, S. Ghazarian, S. Ghosh, S. Casey,
S. Bischoff, S. Gehrmann, S. Schuster, S. Sadeghi, S. Hamdan, S. Zhou, S. Srivastava, S. Shi,
S. Singh, S. Asaadi, S. S. Gu, S. Pachchigar, S. Toshniwal, S. Upadhyay, Shyamolima, Debnath,
S. Shakeri, S. Thormeyer, S. Melzi, S. Reddy, S. P. Makini, S.-H. Lee, S. Torene, S. Hatwar,
S. Dehaene, S. Divic, S. Ermon, S. Biderman, S. Lin, S. Prasad, S. T. Piantadosi, S. M.
Shieber, S. Misherghi, S. Kiritchenko, S. Mishra, T. Linzen, T. Schuster, T. Li, T. Yu, T. Ali,
T. Hashimoto, T.-L. Wu, T. Desbordes, T. Rothschild, T. Phan, T. Wang, T. Nkinyili, T. Schick,
T. Kornev, T. Telleen-Lawton, T. Tunduny, T. Gerstenberg, T. Chang, T. Neeraj, T. Khot,
T. Shultz, U. Shaham, V. Misra, V. Demberg, V. Nyamai, V. Raunak, V. Ramasesh, V. U.
Prabhu, V. Padmakumar, V. Srikumar, W. Fedus, W. Saunders, W. Zhang, W. Vossen, X. Ren,
X. Tong, X. Zhao, X. Wu, X. Shen, Y. Yaghoobzadeh, Y. Lakretz, Y. Song, Y. Bahri, Y. Choi,
Y. Yang, Y. Hao, Y. Chen, Y. Belinkov, Y. Hou, Y. Hou, Y. Bai, Z. Seid, Z. Zhao, Z. Wang, Z. J.
Wang, Z. Wang, and Z. Wu. Beyond the Imitation Game: Quantifying and extrapolating the
capabilities of language models, June 2022. URL http://arxiv.org/abs/2206.04615.
arXiv:2206.04615 [cs, stat].
[233] E. Strubell, A. Ganesh, and A. McCallum. Energy and Policy Considerations for Deep Learning
in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pages 3645–3650. Association for Computational Linguistics, 2019. doi: 10.
18653/v1/P19-1355. URL https://www.aclweb.org/anthology/P19-1355.
[234] Z. Talat. Are You a Racist or Am I Seeing Things? Annotator Influence on Hate Speech
Detection on Twitter. In Proceedings of the First Workshop on NLP and Computational Social
Science, pages 138–142. Association for Computational Linguistics, 2016. doi: 10.18653/v1/
W16-5618. URL http://aclweb.org/anthology/W16-5618.
[235] Z. Talat and A. Lauscher. Back to the Future: On Potential Histories in NLP, Oct. 2022. URL
http://arxiv.org/abs/2210.06245. arXiv:2210.06245 [cs].
[236] Z. Talat, T. Davidson, D. Warmsley, and I. Weber. Understanding Abuse: A Typology of
Abusive Language Detection Subtasks. In Proceedings of the First Workshop on Abusive
Language Online, pages 78–84. Association for Computational Linguistics, 2017. doi: 10.
18653/v1/W17-3012. URL http://aclweb.org/anthology/W17-3012.
38

[237] Z. Talat, S. Lulz, J. Bingel, and I. Augenstein. Disembodied Machine Learning: On the Illusion
of Objectivity in NLP. Jan. 2021. URL http://arxiv.org/abs/2101.11974.
[238] Z. Talat, A. Névéol, S. Biderman, M. Clinciu, M. Dey, S. Longpre, S. Luccioni, M. Masoud,
M. Mitchell, D. Radev, S. Sharma, A. Subramonian, J. Tae, S. Tan, D. Tunuguntla, and O. Van
Der Wal. You reap what you sow: On the challenges of bias evaluation under multilingual
settings. In Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives
in Creating Large Language Models, pages 26–41. Association for Computational Linguistics,
2022-05. URL https://aclanthology.org/2022.bigscience-1.3.
[239] team.
Generative
AI
and
the
protection
of
intellectual
property
rights,
May
2023.
URL
https://www.dreyfus.fr/en/2023/05/22/
generative-ai-balancing-innovation-and-intellectual-property-rights-protection/.
[240] The Ministry of Economy, Trade and Industry.
Governance guidelines for implementa-
tion of ai principles ver. 1.1, 2022. URL https://www.meti.go.jp/press/2021/01/
20220125001/20220124003.html.
[241] Thorn. Generative AI: Now is the Time for Safety by Design, May 2023. URL https:
//www.thorn.org/blog/now-is-the-time-for-safety-by-design/.
[242] N. Thylstrup and Z. Talat. Detecting ‘Dirt’ and ‘Toxicity’: Rethinking Content Moderation as
Pollution Behaviour. SSRN Electronic Journal, 2020. ISSN 1556-5068. doi: 10.2139/ssrn.
3709719. URL https://www.ssrn.com/abstract=3709719.
[243] N.
Todoric
and
A.
Chaudhuri.
Using
AI
to
help
organizations
detect
and
report
child
sexual
abuse
material
online,
Sept.
2018.
URL
https://blog.google/around-the-globe/google-europe/
using-ai-help-organizations-detect-and-report-child-sexual-abuse-material-online/.
[244] J. A. Tomain. Online Privacy and the First Amendment: An Opt-In Approach to Data
Processing, Feb. 2014. URL https://papers.ssrn.com/abstract=2573206.
[245] J.
Tomlinson.
Cultural
Imperialism.
In
The
Wiley-Blackwell
Encyclo-
pedia
of
Globalization.
John
Wiley
&
Sons,
Ltd,
2012.
ISBN
978-0-
470-67059-0.
doi:
10.1002/9780470670590.wbeog129.
URL
https://
onlinelibrary.wiley.com/doi/abs/10.1002/9780470670590.wbeog129.
_eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470670590.wbeog129.
[246] B. Toole. On standpoint epistemology and epistemic peerhood: A defense of epistemic
privilege. Journal of the American Philosophical Association, forthcoming.
[247] Treasury
Board
of
Canada
Secretariat.
Algorithmic
Impact
Assessment
Tool,
2021-03-22.
URL
https://www.canada.ca/en/government/system/
digital-government/digital-government-innovations/responsible-use-ai/
algorithmic-impact-assessment.html.
[248] UNICEF.
Policy
guidance
on
AI
for
children
2.0.
2021.
URL
https://www.unicef.org/globalinsight/media/2356/file/
UNICEF-Global-Insight-policy-guidance-AI-children-2.0-2021.pdf.
[249] United Nations Human Rights Office of the High Commissioner. Enhancing equality and
countering discrimination. URL http://romena.ohchr.org/en/node/188.
[250] United States Congress. Algorithmic Accountability Act of 2022, 2022-02-04. URL http:
//www.congress.gov/.
[251] C. Vaccari and A. Chadwick. Deepfakes and Disinformation: Exploring the Impact of Synthetic
Political Video on Deception, Uncertainty, and Trust in News. Social Media + Society, 6(1):
205630512090340, Jan. 2020. ISSN 2056-3051, 2056-3051. doi: 10.1177/2056305120903408.
URL http://journals.sagepub.com/doi/10.1177/2056305120903408.
39

[252] M. Veale and R. Binns. Fairer machine learning in the real world: Mitigating discrimination
without collecting sensitive data. 4(2):205395171774353, 2017-12. ISSN 2053-9517, 2053-
9517. doi: 10.1177/2053951717743530. URL http://journals.sagepub.com/doi/10.
1177/2053951717743530.
[253] A. Venigalla and L. Li. Mosaic LLMs (Part 2): GPT-3 quality for < $500k, 2022. URL
https://www.mosaicml.com/blog/gpt-3-quality-for-500k.
[254] J. Vincent.
AI art tools Stable Diffusion and Midjourney targeted with copyright
lawsuit - The Verge.
URL https://www.theverge.com/2023/1/16/23557098/
generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart.
[255] J. Vincent.
Twitter taught Microsoft’s AI chatbot to be a racist asshole in less than
a day - The Verge, 2016.
URL https://www.theverge.com/2016/3/24/11297050/
tay-microsoft-chatbot-racist.
[256] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. GLUE: A Multi-Task
Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the
2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,
pages 353–355. Association for Computational Linguistics, 2018. doi: 10.18653/v1/W18-5446.
URL http://aclweb.org/anthology/W18-5446.
[257] A. Wang, V. V. Ramaswamy, and O. Russakovsky. Towards Intersectionality in Machine
Learning: Including More Identities, Handling Underrepresentation, and Performing Eval-
uation. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages
336–349. ACM, 2022-06-21. ISBN 978-1-4503-9352-2. doi: 10.1145/3531146.3533101.
URL https://dl.acm.org/doi/10.1145/3531146.3533101.
[258] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese,
B. Balle, A. Kasirzadeh, Z. Kenton, S. Brown, W. Hawkins, T. Stepleton, C. Biles, A. Birhane,
J. Haas, L. Rimell, L. A. Hendricks, W. Isaac, S. Legassick, G. Irving, and I. Gabriel. Ethical
and social risks of harm from Language Models, 2021-12-08. URL http://arxiv.org/
abs/2112.04359.
[259] L. Weidinger, K. McKee, R., R. Everett, S. Huang, T. Zhu, M. Chadwick, C. Summerfield, and
I. Gabriel. Using the Veil of Ignorance to align AI systems with principles of justice, 2023.
URL https://www.pnas.org/doi/10.1073/pnas.2213709120.
[260] B. Weiser. Here’s What Happens When Your Lawyer Uses ChatGPT. The New York Times,
May 2023. ISSN 0362-4331. URL https://www.nytimes.com/2023/05/27/nyregion/
avianca-airline-lawsuit-chatgpt.html.
[261] R. Weitzer. Racial discrimination in the criminal justice system: Findings and problems
in the literature. Journal of Criminal Justice, 24(4):309–322, Jan. 1996. ISSN 0047-2352.
doi: 10.1016/0047-2352(96)00015-3. URL https://www.sciencedirect.com/science/
article/pii/0047235296000153.
[262] D. Wen, S. M. Khan, A. Ji Xu, H. Ibrahim, L. Smith, J. Caballero, L. Zepeda, C. de Blas Perez,
A. K. Denniston, X. Liu, and R. N. Matin. Characteristics of publicly available skin cancer
image datasets: a systematic review. The Lancet. Digital Health, 4(1):e64–e74, Jan. 2022.
ISSN 2589-7500. doi: 10.1016/S2589-7500(21)00252-1.
[263] F. Westin and S. Chiasson. Opt out of privacy or "go home": understanding reluctant privacy
behaviours through the FoMO-centric design paradigm. In Proceedings of the New Security
Paradigms Workshop, NSPW ’19, pages 57–67, New York, NY, USA, Jan. 2020. Association
for Computing Machinery. ISBN 978-1-4503-7647-1. doi: 10.1145/3368860.3368865. URL
https://doi.org/10.1145/3368860.3368865.
[264] C. C. Williams and A. Efendic. Evaluating the relationship between marginalization and
participation in undeclared work: lessons from Bosnia and Herzegovina. Southeast European
and Black Sea Studies, 21(3):481–499, July 2021. ISSN 1468-3857. doi: 10.1080/14683857.
2021.1928419. URL https://doi.org/10.1080/14683857.2021.1928419. Publisher:
Routledge _eprint: https://doi.org/10.1080/14683857.2021.1928419.
40

[265] L. Winner. Do Artifacts Have Politics? Daedalus, 109(1), 1980. URL http://www.jstor.
org/stable/20024652.
[266] A. Wolfers. "National Security" as an Ambiguous Symbol. Political Science Quarterly, 67(4):
481–502, 1952. ISSN 0032-3195. doi: 10.2307/2145138. URL https://www.jstor.org/
stable/2145138. Publisher: [Academy of Political Science, Wiley].
[267] C.-J. Wu, R. Raghavendra, U. Gupta, B. Acun, N. Ardalani, K. Maeng, G. Chang, F. A.
Behram, J. Huang, C. Bai, M. Gschwind, A. Gupta, M. Ott, A. Melnikov, S. Candido,
D. Brooks, G. Chauhan, B. Lee, H.-H. S. Lee, B. Akyildiz, M. Balandat, J. Spisak, R. Jain,
M. Rabbat, and K. Hazelwood. Sustainable AI: Environmental Implications, Challenges and
Opportunities. URL http://arxiv.org/abs/2111.00364.
[268] C. Xiang.
’He Would Still Be Here’: Man Dies by Suicide After Talking with AI
Chatbot, Widow Says, Mar. 2023. URL https://www.vice.com/en/article/pkadgm/
man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says.
[269] A. Xu, E. Pathak, E. Wallace, S. Gururangan, M. Sap, and D. Klein. Detoxifying Language
Models Risks Marginalizing Minority Voices, Apr. 2021. URL http://arxiv.org/abs/
2104.06390. arXiv:2104.06390 [cs].
[270] J. Zaller and S. Feldman. A Simple Theory of the Survey Response: Answering Questions ver-
sus Revealing Preferences. American Journal of Political Science, 36(3):579–616, 1992. ISSN
0092-5853. doi: 10.2307/2111583. URL https://www.jstor.org/stable/2111583.
Publisher: [Midwest Political Science Association, Wiley].
[271] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V.
Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang,
and L. Zettlemoyer. OPT: Open Pre-trained Transformer Language Models, 2022-06-21. URL
http://arxiv.org/abs/2205.01068.
[272] J. Zhao, T. Wang, M. Yatskar, R. Cotterell, V. Ordonez, and K. Chang. Gender bias in
contextualized word embeddings. CoRR, abs/1904.03310, 2019. URL http://arxiv.org/
abs/1904.03310.
[273] D. Zhuang, X. Zhang, S. L. Song, and S. Hooker. Randomness In Neural Network Training:
Characterizing The Impact of Tooling, 2021-06-22. URL http://arxiv.org/abs/2106.
11872.
41

