Image Super-Resolution using Efﬁcient Striped Window Transformer
Jinpeng Shi1,3†‡, Hui Li1,3†, Tianle Liu1,2, Yulong Liu1,3, Mingjian Zhang1,3,
Jinchen Zhu1,3, Dong Liang1, Ling Zheng1, Shizhuang Weng1‡
1Anhui University
2University of Science and Technology of China
3Fried Rice Lab
jinpeeeng.s@gmail.com, weng 1989@126.com
Abstract
Transformers have achieved remarkable results in
single-image super-resolution (SR). However, the challenge
of balancing model performance and complexity has hin-
dered their application in lightweight SR (LSR). To tackle
this challenge, we propose an efﬁcient striped window
transformer (ESWT). We revisit the normalization layer in
the transformer and design a concise and efﬁcient trans-
former structure to build the ESWT. Furthermore, we in-
troduce a striped window mechanism to model long-term
dependencies more efﬁciently.
To fully exploit the po-
tential of the ESWT, we propose a novel ﬂexible win-
dow training strategy that can improve the performance of
the ESWT without additional cost. Extensive experiments
show that ESWT outperforms state-of-the-art LSR trans-
formers, and achieves a better trade-off between model per-
formance and complexity. The ESWT requires fewer pa-
rameters, incurs faster inference, smaller FLOPs, and less
memory consumption, making it a promising solution for
LSR. The Code is available at https://github.com/
Fried-Rice-Lab/FriedRiceLab.
1. Introduction
High-resolution (HR) images are essential in various do-
mains, such as satellite imaging [7], medical imaging [36],
and surveillance monitoring [44]. However, the acquisition
of HR images is often constrained by hardware limitations,
which leads to low-resolution (LR) images. Single-image
super-resolution (SR) is aimed at the reconstruction of HR
images from their LR counterpart, which is a challenging
problem due to the non-unique mapping between HR and
LR images. In recent years, SR transformers, including IPT
[4], EDT [25], and HAT [5], have shown great promise in
addressing this problem. Nevertheless, the extensive model
† Co-ﬁrst authors. ‡ Co-corresponding authors.
Figure 1: Qualitative trade-off comparison between model
performance and complexity of ×4 image SR on the Ur-
ban100 [16] dataset. The proposed ESWT is marked in or-
ange. The comparison results demonstrate the superiority of
our method. “†” means the use of ﬂexible window training
strategy.
size and high computational cost of these methods limit
their real-world applications.
To achieve a better balance between model performance
and complexity, a number of lightweight SR (LSR) trans-
formers have been proposed.
SwinIR provides a light-
weight version that utilizes window self-attention (WSA)
and shifted window mechanism to decrease the computa-
tional complexity of transformers.
ESRT [32] computes
WSA within the overlapped windows to achieve a bal-
ance between model parameters and long-term dependent
modeling capabilities. ELAN [43] simpliﬁes the compu-
tation operation of WSA and introduces an attention shar-
ing mechanism to maintain a performance comparable to
that of SwinIR while exhibiting a lower model complex-
ity. LBNet [13] utilizes a recursive mechanism to enhance
feature representation capability by increasing the depth of
arXiv:2301.09869v2  [cs.CV]  14 Mar 2023

the transformer without adding other parameters. Although
LSR transformers have made remarkable progress, existing
designs remain inefﬁcient. Shifted WSA has limited capa-
bility to capture long-term dependencies [18, 9], whereas
overlapped WSA entails high computational costs [32, 5].
Recursive mechanisms can reduce model complexity but
at the expense of slower inference.
Furthermore, serval
works have shown that the normalization layer may pe-
nalize the generalization capability [23, 41] and introduce
noise [21, 37] to models with limited capacity. LSR trans-
formers are often very shallow, and thus the inﬂuence of
normalization layers needs to be investigated.
To further reduce the model complexity and improve the
quality of SR images, we propose an efﬁcient striped win-
dow transformer (ESWT). We ﬁrst revisit the normalization
layer in the shallow transformer and introduce a concise
and efﬁcient transformer structure. Moreover, we propose
a striped window mechanism that allows ESWT to model
long-term dependencies efﬁciently. Considering that high-
quality SR images originate from the joint optimization of
model and training strategy, we also propose a novel ﬂexible
window training strategy. This strategy enables ESWT to
model long-term dependencies progressively and thus better
explore contextual information. The quantitative trade-off
comparison of model performance and complexity (Figure
1) demonstrates the superiority of the proposed ESWT.
The contributions of this paper are summarized as fol-
lows:
• We revisit the normalization layer in the shallow trans-
former and design a concise and efﬁcient transformer
structure to build the proposed ESWT.
• We propose a striped window mechanism, that can ex-
plore contextual information efﬁciently and has a low
computational complexity.
• We propose a novel ﬂexible window training strategy
enables further exploration of the potential of the pro-
posed ESWT without additional cost.
2. Related Works
Normalization Layer in Transformers
Normalization
layers are common components in transformers. They are
typically applied before self-attention (SA) and multilayer
perceptron (MLP) to mitigate the effects of internal covari-
ate bias and reduce the dependence of gradients on model
size to stabilize training and enhance model generalization.
Several works [38, 10, 8] have demonstrated the importance
of normalization layers for training deep transformers with
hundreds of layers. However, the transformers used for LSR
[32, 13, 43] are very shallow, typically with only a few tens
of layers. The role of normalization layers in such shallow
transformers has not been fully explored yet.
Efﬁcient SA
SA is widely used in SR transformers [26, 5,
4, 43, 25]. However, the computational complexity of SA
is quadratic to the image size, which introduces high model
complexity for these transformers and affects their further
practical applications. For the reduction of model complex-
ity, SwinIR [26] performs SA in local windows to achieve
linear computational complexity. ELAN [43] applies SA by
group, which further reduces the model complexity while
maintaining a performance comparable to that of SwinIR.
However, calculation of SA within the local window causes
difﬁculty in modeling long-term dependencies. To solve
this problem, some methods adopt shifted window mech-
anism [32, 13] or overlapped window mechanism [32, 5],
which either indirectly solves this problem or introduces ad-
ditional computational cost, respectively.
Training Strategies for LSR Methods
Numerous train-
ing strategies have been proposed to further improve the
performance of models on image SR. RCAN-it [28] ﬁne-
tunes pre-training weights to achieve a model performance
that is comparable to that of scratch training, while keep-
ing a lower training cost. EDT [25] demonstrates a cor-
relation between different image restoration tasks, such as
image SR, deraining, and denoising. Based on this corre-
lation, EDT proposes a related task training strategy to fur-
ther improve the feature representation of the model. RLFN
[24] proposes a three-stage training strategy that accelerates
model convergence and improves the quality of SR images.
All of these methods demonstrate that a better training strat-
egy can improve the model performance.
3. Methodology
In this section, we ﬁrst revisit the normalization layer
in shallow transformers and propose a concise and efﬁ-
cient transformer structure.
Subsequently, we propose a
striped window mechanism that enables transformers to
model long-range dependencies efﬁciently. Finally, we in-
vestigate the feature representations of the ESWT using dif-
ferent striped windows and propose a ﬂexible window train-
ing strategy.
3.1. Towards Concise Structure
The transformer layer (TL) is the basic module of trans-
formers[26, 30, 9, 5, 25, 43]. As shown in the lower right
of Figure 2, an original TL consists of a SA and a MLP.
The normalization layer and residual connection are applied
before and after SA and MLP, respectively. To clarify the
impact of the normalization layer on shallow transformers,
following previous works [32, 13, 43], we build a shallow
LSR transformer with model size around 600K based on
the SwinIR structure. To provide a more detailed analy-
sis, we evaluate the performance of the LSR transformer

Figure 2: Left: Overall architecture of the proposed ESWT. Upper right: Illustration of SA in a local window W of size
c × h × w using the BN-embedded SA. Lower right: Illustration of the original TL, in which various normalization layers
can be applied.
with different normalization layers across various learning
rates given that normalization can signiﬁcantly inﬂuence the
training strategy of the model (see Section 4.1 for details).
Although shallow transformers can be trained success-
fully without normalization layers, our experiments show
that normalization layers are crucial for achieving a better
balance between model performance and training cost. In
addition, considering the limited model capacity, batch nor-
malization (BN) [20] enables shallow transformers to gain
better generalization capabilities by allowing transformers
to capture generic features in the mini-batch than layer nor-
malization (LN) [2]. Combining the above ﬁndings and in-
spired by ELAN [43], we remove the normalization layer
from TL and embed BN into SA to build an efﬁcient and
concise transformer structure.
BN-embedded SA
WSA is widely used in computer vi-
sion transformers. WSA partitions an input feature F ∈
RC×H×W into local windows W ∈Rc×h×w, and uses
SA to model dependencies within each local window sep-
arately. The upper right of Figure 2 shows the process of
modeling dependencies in a local window W using BN-
embedded SA (BSA). The BSA initially computes the query
matrix Q and the value matrix V using two 1 × 1 convolu-
tional layers. BN is applied after each convolutional layer
to provide the transformer a more stable training and more
powerful generalization capability [43]. This process can
be expressed as follows:
Q = HBNQ(KQW),
V = HBNV (KV W),
(1)
where KQ and KV are the kernel of 1 × 1 convolutional
layers used for the computation of Q and V , respectively;
HBNQ(·) and HBNV (·) represent the function of different
BNs. In addition, BN can be embedded into its preceding
convolutional layer to accelerate the inference of the model
further.
Subsequently, the BSA uses matrix Q to compute atten-
tion matrix A. Key matrix K is not used here to stream-
line the transformer further. Finally, the BSA uses a 1 × 1
convolutional layer to map the matrix V weighted by ma-
trix A to the desired feature space to obtain the output
Fout ∈Rc×h×w. This process can be expressed as follows:
Fout = HConv(HSoftmax(QQT )
scale
V ),
(2)
where HSoftmax(·) represents the function of Softmax,
HConv(·) denotes the function of the convolutional layer at
the tail of the BSA, and scale is a constant used to control
the magnitude of matrix A.
Overall Structure
We build our ESWT based on the ef-
ﬁcient and concise structure discussed above. As shown in
the left of Figure 2, the proposed ESWT consists of three
main modules: shallow feature extraction module (SFEM),
deep feature extraction module (DFEM), and SR recon-
struction module (SRRM). For a given LR image ILR, the
ESWT ﬁrst converts it from color space to feature space us-
ing SFEM to extract the shallow feature Fs. This process
can be formulated as follows:
Fs = HSF EM(ILR),
(3)
where HSF EM(·) represents the function of SFEM.
Subsequently, the ESWT extracts deep feature Fd from
Fs using DFEM. This module contains n efﬁcient trans-
former blocks (ETBs) and uses them to extract deeper fea-
tures block by block. This process can be formulated as
follows:
Fd = HDF EM(Fs)
= HET Bn(HET Bn−1(· · · HET B1(Fs) · · · )),
(4)
where HDF EM(·) represents the function of DFEM, and
HET Bn(·) refers to the function of n-th ETB in DFEM.
The ETB contains m efﬁcient TLs (ETLs). In addition,
a convolution layer is applied to the tail of each ETB to in-
troduce inductive bias in the transformer [26]. The function

(a) Modeling long-term dependencies with striped window mechanism
(b) Modeling long-term dependencies whithout striped window mech-
anism
Figure 3: Modeling long-term dependencies with/without
striped window mechanism. Take the blue and orange posi-
tions in the input feature as an example, using the striped
window mechanism can model the dependency between
the two positions more efﬁciently. This ﬁnding proves the
advantage of the striped window mechanism in modeling
long-term dependencies. For concise illustration, the size
of the input feature is set to 4×4, and local windows are set
to 1 × 4 and 2 × 2 .
HET Bn(·) can be further expressed as follows:
Fn = HET Bn(Fin)
= HC(HET Ln,m(HET Ln,m−1(· · · HET Ln,1(Fn−1) · · · ))),
(5)
where HET Ln,m(·) represents the function of m-th ETL in
n-th ETB; Fn−1 and Fn are the input and output of n-th
ETB, respectively.
Finally, the ESWT converts Fs and Fd from feature
space to color space using SRRM to reconstruct the SR
image ISR. By transmitting Fs, which contains rich low-
frequency information, directly to SRRM, the transformer
can focus more on reconstructing the lost high-frequency
information. The process of SR image reconstruction can
be expressed as follows:
ISR = HSRRM(Fs + Fd),
(6)
where HSRRM(·) represents the function of SRRM.
3.2. Efﬁcient Long-term Dependency Modeling
Despite WSA makes transformers more cost-effective
[30, 26], it also weakens their capability to model long-
term dependencies across local windows. To overcome this
issue, some transformers use the shifted or overlapped win-
dow mechanism. However, this makes them require stack-
ing more TLs to reach global reception ﬁeld or need addi-
tional computational costs, both of which are detrimental to
lightweight models. Inspired by previous work [18, 9], we
propose a striped window mechanism to model long-term
dependencies efﬁciently by capturing contextual informa-
tion from different dimensions in a targeted manner.
Striped Window Mechanism
The use of striped window
mechanism to model long-term dependencies between the
blue and orange positions is shown in Figure 3a. The input
feature Fin is split equally into two independent features
along the channel dimension. Then, WSAs with (h, w) ver-
tical or (w, h) horizontal striped windows are applied to the
two features. This method allows the establishment of in-
window dependencies over a larger range of speciﬁc dimen-
sions for further exploration of contextual information. Fi-
nally, the two features are concatenated along the channel
dimension. Given that the vertical and horizontal striped
windows are boundary-crossed, a 1 × 1 convolutional layer
is used to blend the in-window dependencies therein. As a
result, the transformer models the long-term dependencies
between blue and orange locations more effectively.
Complexity Analysis of Modeling Dependencies
The
computational complexity of modeling dependencies in a
local window using the BSA discussed in Section 3.1 is as
follows:
Ω= 3c2hw + 2c(hw)2,
(7)
where h, w, and c are the height, width, and channel number
of the local window W, respectively. According to [30, 9],
we omit BN and Softmax here to simplify the analysis.
Based on the above discussion and Equation 7, the com-
putational complexity of model long-term dependencies be-
tween the blue and orange positions using striped window
mechanism is as follows:
Ω= HW
hw × [2 × (2(C
2 )2hw + 2(C
2 )(hw)2) + C2hw]
= (2C + 2N)CHW,
(8)
where N = h × w denotes the total number of pixels in
a local window. The length h and width w of the striped
window can be adjusted freely to trade-off between model
performance and complexity.
Compared with the striped window mechanism, the
long-term dependence between the blue and orange posi-
tions cannot be modeled directly using the shifted window
mechanism. As shown in Figure 3b, this mechanism re-
quires a WSA and a shifted WSA to model long-term de-
pendencies. Based on the analyses in [30] and [26] and
Equation 7, the computational complexity of model such
dependencies using shifted window mechanism can be ex-
pressed as follows:
Ω= 2 × HW
kk
× [3C2kk + 2C(kk)2]
= (6C + 4N)CHW,
(9)

where N = k × k denotes the total number of pixels in a
local window. The above equation shows that the complex-
ity of the striped window mechanism is lower than that of
the shifted window mechanism, which proves its efﬁciency.
We exclude the discussion of the overlapped window mech-
anism due to its excessive computational complexity. Please
refer to [5, 32] for more information.
3.3. Flexible Window Training Strategy
A better training strategy is crucial for achieving better
image SR results [28, 29, 25, 5, 24]. According to EDT
[25], allowing models to leverage similar feature represen-
tations across the same or related tasks is essential for the
success of these strategies. For instance, image SR can ben-
eﬁt from similar feature representation of image denoising
[25], and ×4 image SR can beneﬁt from the similar feature
representation of ×2 image SR [28, 29]. Thus, we question
whether ESWTs utilizing different striped windows (e.g.,
(a, b) and (c, d)) exhibit similar feature representations, and
if so, whether they can beneﬁt from each other to enhance
their SR performance.
To investigate the similarity of feature representation
across models, we introduced central kernel alignment
(CKA) [6]. Speciﬁcally, CKA takes two activations X ∈
Rm×p1 and Y ∈Rm×p2 of two layers, which contain m
samples and p1/p2 neurons, as input. The CKA of the two
layers can be calculated as follows.
CKA(K, L) =
HSIC(K, L)
p
HSIC(K, K)HSIC(L, L)
,
(10)
where K = XX⊤and L = YY⊤denote the Gram
matrices for the two layers, and HSIC(·) is the Hilbert-
Schmidt independence criterion [14]. Given the centering
matrix H = In −1
n11⊤, K
′ = HKH and L
′ = HLH
are centered Gram matrices, then we have HSIC(K, L) =
vec(K
′) · vec(L
′)/(m −1)2. For more efﬁcient evaluation,
we use the minibatch estimator [35] of CKA with a mini-
batch of 288.
We train three ESWTs from scratch using the same train-
ing strategy with (12, 12), (24, 6), and (36, 4) striped win-
dows. The CKA similarity between all convolutional layer
pairs are reported in Figure 4. We observe that ESWT using
different striped windows exhibit highly similar feature rep-
resentations. Based on this ﬁnding, we propose a novel ﬂex-
ible window training strategy that consists of several stages.
The latter stage loads the pre-trained model from the pre-
vious one and calculates WSA using stretched striped win-
dows. Experiments demonstrate that this training strategy
can effectively improve the performance of ESWT without
additional training cost (see Section 4.3 for details).
Figure 4: CKA similarities between all pairs of convolu-
tional layers in ESWT using different striped windows. X
and Y axes indexing layers from input to output. As shown
on the diagonal of the heat maps, different ESWTs have
similar feature representations.
4. Experiments
In this section, we ﬁrst perform a series of ablation stud-
ies to validate the design choices of ESWT described above.
Then, we apply the proposed ESWT to LSR to demon-
strate its effectiveness. Finally, the ﬂexible window training
strategy is employed to further improve the performance of
ESWT.
4.1. Ablation studies
Following previous works [13, 32, 19, 12, 29], we con-
duct ablation studies using the following settings if not
speciﬁed: the DF2K [1] dataset selected as the training
and validation set, and ﬁve benchmark datasets, namely,
Set5 [3], Set14 [42], BSD100 [33], Urban100 [16], and
Manga109 [34], as the test set. The number n of ETBs
in ESWT is set to 3, and the number m of ETLs in each
ETB is set to 6. The striped window mechanism using an
(24, 6) striped window. We train models by minimizing the
L1 loss using the Adam optimizer [22] with β1 = 0.9 and
β2 = 0.999 for a total of 100K iterations. The initial learn-
ing rate is set to 5×10−4 and it is decreased to 5×10−6 us-
ing the cosine annealing scheduler [31]. The training batch
size is 64 and the training patch size is 72×72. Random ro-
tation and random horizontal ﬂip are used as data enhance-
ment.
For evaluation of the quality of the SR images, the PSNR
and SSIM calculated on the Y channel of the YCbCr space
are used. For a more comprehensive evaluation, four met-
rics are used to measure model complexity, “#Params” the
total number of learnable parameter, “#Memory” the max-
imum GPU memory consumption, “#Latency” the average
inference time per image on a dataset, “#FLOPs” the ab-
breviation for ﬂoating point operations. To ensure a fair
comparison, we report all “#FLOPs” using a 3 × 256 × 256
image for ×4 image SR.

Initial
Normalization
Urban100
Manga109
Learning Rate
Layer
PSNR
SSIM
PSNR
SSIM
1e-4
Identity
23.29
0.6985
27.92
0.8434
Identity†
25.12
0.7443
28.63
0.8775
LN
25.11
0.7526
28.59
0.8800
BN
25.22
0.7910
28.73
0.8831
Single BN
25.27
0.7917
28.77
0.8843
Embedded BN
25.49
0.7923
29.11
0.8901
2e-4
Identity‡
-
-
-
-
LN
26.23
0.7905
30.60
0.9099
BN
26.31
0.7933
30.69
0.9113
Single BN
26.36
0.7951
30.75
0.9120
Embedded BN
26.45
0.7974
30.87
0.9126
5e-4
Identity‡
-
-
-
-
LN
26.41
0.7955
30.83
0.9125
BN
26.45
0.7970
30.87
0.9133
Single BN
26.49
0.7991
30.89
0.9133
Embedded BN
26.54
0.8013
30.89
0.9137
Table 1: Ablation study about the role of normalization lay-
ers in shallow transformers. “Identity” means no normal-
ization layer is used. “Single BN” means the BN layer is
used before SA. “Embedded BN” means the BN layer is
embedded into SA. “†” means the use of pre-training and
ﬁne-tuning strategies in [28] and training for ×5 total itera-
tions. “‡” means the training is unstable.
Normalization
Params
FLOPs
Memory
Latency
Layer
[K]
[G]
[M]
[ms]
Identity
585
36.84
116.71
760.98
LN
589
37.91
142.84
820.54
BN
589
38.19
122.36
820.59
Single BN
587
38.05
119.06
820.55
Embedded BN
589
38.19
121.82
760.08
Table 2: Model complexity of ×4 image SR on the DIV2K
[1] validation dataset using different normalization layers.
Mechanism
Window Size
BSD100
Urban100
Manga109
PSNR
SSIM
PSNR
SSIM
PSNR
SSIM
w/o Striped Window
(12, 12)
27.69
0.7404
26.48
0.7977
30.88
0.9129
w/ Striped Window
(16, 9)
27.69
0.7409
26.49
0.7987
30.91
0.9130
(18, 8)
27.69
0.7407
26.51
0.7990
30.90
0.9134
(24, 6)
27.70
0.7410
26.56
0.8006
30.94
0.9136
Table 3: Ablation study about the effectiveness of striped
window mechanism.
The study was performed on the
BSD100 [33], Urban100 [16], and Manga109 [34] datasets
at ×4 image SR. The BEST results are highlighted.
From LN to Embedded BN
We ﬁrst examine the impact
of the normalization layer in shallow transformers. The re-
sults of the ablation study are shown in Table 1. Our key
ﬁndings can be summarized as follows: Although remov-
ing the normalization layer in ESWT at low learning rates
can decrease model complexity (Table 2) without compro-
mising performance, this strategy incurs signiﬁcant train-
ing costs and restricts the transformer from fully exploiting
the performance gains achievable with higher learning rates
[40, 39, 17]. In addition, although EDSR [27] has claimed
that BN is unsuitable for SR models with high model ca-
pacity due to its tendency to remove differences between
low-level visual features (such as edges and textures) across
samples in the mini-batch, our experiments indicate that in-
corporating BN can improve the generalization capability of
shallow transformers with limited capacity more effectively
than LN. This ﬁnding is particularly noteworthy given that
our model accounts for only 1.4% of the parameter num-
ber of EDSR. We also conﬁrm that excessive normalization
layers may penalize the performance of models with lim-
ited capacity [21, 37, 23, 41]. Finally, inspired by [43], we
embed the BN layer into SA and observe that it can further
improve the model performance and has a low model com-
plexity. To sum up, we suggest the removal of the normal-
ization layer in TL and incorporation of BSA to improve the
generalization capability and performance of shallow trans-
formers, while maintaining a balance between model com-
plexity, performance, and training cost.
Given that this research is a basic ablation study, we re-
tain a number of ﬁxed factors that can be investigated fur-
ther, including the normalization layer location and model
parameters. We will consider them for future exploration.
Effectiveness of Striped Window Mechanism
In Sec-
tion 3.2, we propose a striped window mechanism to model
long-term dependencies effectively. Table 3 reports the per-
formance of ESWT on ×4 image SR using different mech-
anisms and striped windows. For fair comparison, all lo-
cal windows contain 144 pixels. The results of the ablation
study indicate that the striped window mechanism can im-
prove the performance of the ESWT. Moreover, the stretch-
ing of striped window can further enhance the model perfor-
mance. To better understand the proposed striped window
mechanism, we introduce local attribution map (LAM) for
overall model analysis. Suppose that F : Rh×w 7→Rsh×sw
is an image SR model with ×s upscaling factor, LAM em-
ploys the path integral gradient for its attribution analysis
[15]. The LAM results for the ESWT using different striped
windows are shown in Figure 6 and the LAM results for
the model using different mechanisms are shown in Figure
7. It can be observed that by incorporating (24, 6) striped
windows, the ESWT can make more efﬁcient utilization of
contextual information. To further investigate the effect of
the proposed mechanism on the integration of contextual
information in the transformer, we analyze the mean atten-
tion distance (MAD) at different SA layers, which is anal-
ogous to receptive ﬁeld in CNNs. Figure 8 shows that the
proposed mechanism improves the MAD of the shallow SA
layers of the ESWT and thus enhances its perception of con-
textual information. These analysis results conﬁrm that the
striped window mechanism contributes to better exploration
of contextual information and thus improves model perfor-
mance. As the ofﬁcial pre-training weights had not been
released, we retrained the ESRT used to calculate LAM us-

Method
Scale
Params
Set5
Set14
BSD100
Urban100
Manga109
PSNR
SSIM
PSNR
SSIM
PSNR
SSIM
PSNR
SSIM
PSNR
SSIM
ESRT[32]
×3
770
34.42
0.9268
30.43
0.8433
29.15
0.8063
28.46
0.8574
33.95
0.9455
LBNet[13]
736
34.47
0.9277
30.38
0.8417
29.13
0.8061
28.42
0.8559
33.82
0.9460
SwinIR[26]
886
34.62
0.9289
30.54
0.8463
29.20
0.8082
28.66
0.8624
33.98
0.9478
ELAN-light[43]
590
34.61
0.9288
30.55
0.8463
29.21
0.8081
28.69
0.8624
34.00
0.9478
ESWT(ours)
578
34.63
0.9290
30.55
0.8464
29.23
0.8088
28.70
0.8628
34.05
0.9479
ESWT†(ours)
578
34.65
0.9301
30.56
0.8467
29.24
0.8097
28.71
0.8633
34.07
0.9480
ESRT[32]
×4
751
32.19
0.8947
28.69
0.7833
27.69
0.7379
26.39
0.7962
30.75
0.9100
LBNet[13]
742
32.29
0.8960
28.68
0.7832
27.62
0.7382
26.27
0.7906
30.76
0.9111
SwinIR[26]
897
32.44
0.8976
28.77
0.7858
27.69
0.7406
26.47
0.7980
30.92
0.9151
ELAN-light[43]
601
32.43
0.8975
28.78
0.7858
27.69
0.7406
26.54
0.7982
30.92
0.9150
ESWT(ours)
589
32.46
0.8979
28.80
0.7866
27.70
0.7410
26.56
0.8006
30.94
0.9136
ESWT†(ours)
589
32.48
0.8982
28.82
0.7869
27.71
0.7415
26.57
0.8011
30.95
0.9143
Table 4: Quantitative comparison of ×3 and ×4 image SR. The BEST and second BEST results are highlighted and
underlined. “†” means the use of ﬂexible window training strategy.
Figure 5: Qualitative comparison of ×4 image SR on Urban100 [16] dataset.
Method
BSD100
Params
FLOPs
Memory
Latency
PSNR
SSIM
[K]
[G]
[M]
[ms]
ESRT
27.69
0.7379
752
75.83
584.80
20.16
LBNet
27.62
0.7382
742
173.26
144.04
25.41
SwinIR
27.69
0.7406
897
218.78
71.90
34.25
ELAN-light
27.69
0.7406
601
39.09
45.14
15.22
ESWT(ours)
27.70
0.7410
589
38.20
37.60
12.08
Table 5: Quantitative trade-off comparison between model
performance and complexity of ×4 image SR on BSD100
[33] dataset. The BEST results are highlighted.
Stage
Window Size
Learning Rate
iterations
One
(12, 12)
5 × 10−4 →5 × 10−6
50K
Two
(18, 8)
1 × 10−4 →1 × 10−6
25K
Three
(24, 6)
1 × 10−4 →1 × 10−6
25K
Table 6: A simple three-stage ﬂexible window training strat-
egy. The latter stage loads the pre-training weights from the
previous stage to beneﬁt from the learned feature represen-
tations.
ing the training settings reported in its paper [32].
4.2. Application
We now apply ESWT to LSR following the training set-
tings of ablation study and compare it with state-of-the-art
(SOTA) methods in terms of the following aspects: qualita-
tive and quantitative results of image SR, and the trade-off
between model complexity and performance.
Quantitative and Qualitative Comparisons
The quan-
titative comparison results are shown in Table 4. The pro-
posed ESWT performs better in almost all cases, surpassing
ELAN [43] 0.0024 SSIM on the Urban100 [16] dataset of
×4 image SR. The qualitative comparison results are illus-
trated in Figure 5. Compared with other methods, the SR
result reconstructed by the proposed ESWT contains rich
structural details, has sharper edges, and looks more natu-
ral.
Model Performance and Complexity Trade-off
In the
evaluation of LSR methods, the trade-off between model
performance and complexity is also a key factor to consider.
The qualitative trade-off comparison between ESWT and
SOTA methods is shown in Table 5. The proposed ESWT
reduces the complexity of the model and further enhances
the quality of SR images, achieving a better trade-off com-
pared with SOTA methods.
4.3. Training with Flexible Window
High-quality SR images are obtained through joint opti-
mization of the model, data, and training strategy. In Sec-
tion 3.3, we analyze the effectiveness of existing training
strategies and propose a new ﬂexible window training strat-
egy. We now apply the proposed training strategy on ESWT
to verify its advancement. Referring to RLFN [24], we de-
sign a simple three-stage training strategy (Table 6). In the
ﬁrst stage, a high learning rate is used to improve the gener-
alization capability of the model. In subsequent stages, the
striped window is stretched to expand the receptive ﬁeld of

Figure 6: LAM results of ESWT using different striped
windows in ×4 image SR. When reconstructing the patches
marked with red boxes, a dark color indicates a larger de-
gree of contribution.
Figure 7: LAM results of different SR models in ×4 im-
age SR. ESRT uses the overlapped window mechanism and
SwinIR uses the shifted window mechanism. ESWT using
striped window mechanism and/or ﬂexible window training
strategy can explore a larger range of contextual informa-
tion. “†” means the use of ﬂexible window training strategy.
Figure 8: MAAs (light blue areas) of SwinIR and ESWT
using different striped windows in ×4 image SR. MAAs
are determined by the mean and standard deviation of 100
MADs (blue lines) obtained on the DIV2K [1] validation
set. MADs are obtained by averaging the distances between
query pixels and all other pixels, weighted by the attention
weights [11, 35]. Due to hardware limitation, we do not
analyze the MAA of ESRT. “†” means the use of ﬂexible
window training strategy.
Figure 9: Areas of interest across different SR models. Red
and blue regions both contribute to the SR images, but the
blue regions are more challenging for SR models to utilize.
Samples are selected from [15].
the model, and a lower learning rate is used for the model
to ﬁne-tune its feature representation for better SR perfor-
mance. We maintain the same training settings in the abla-
tion study except those speciﬁed in the Table 6.
Qualitative experimental results are presented in Table
4. The proposed ﬂexible window training strategy improves
the performance of ESWT considerably without any addi-
tional training costs. The following reasons may contribute
to its effectiveness: Given that ESWTs with different striped
windows have highly similar feature representations (Fig-
ure 4), by loading the pre-training weight of the previous
stage, the latter stage can beneﬁt from shared feature repre-
sentations. Moreover, we analyzed the mean attention area
(MAA) of the SA layer in different models, which indi-
cates the most important areas attended by SA [25, 35, 11]
(Figure 8). The proposed strategy broadens the MAA of
SA layers, indicating that it enables ESWT to incorporate
more contextual information to build high-quality SR im-
ages. Furthermore, Gu et al. [15] observed a difﬁcult-to-
learn contextual information boundary for SR models (Fig-
ure 9). The proposed ﬂexible windowing strategy enables
ESWT to vary its receptive ﬁelds during different stages
of training, which allows it to initially capture contextual
information at local scales and subsequently leverage it to
better model dependencies at larger scales. Compared with
increasing the depth of the model [26] and using overlapped
WSA [32] to enlarge the receptive ﬁeld, we consider that the
proposed training strategy is an effective method for cross-
ing this boundary.
5. Conclusions
In this paper, we propose an efﬁcient striped win-
dow transformer (ESWT) for lightweight super-resolution
(LSR). Speciﬁcally, by removing the normalization layer
and embedding batch normalization (BN) in self-attention
(SA), we propose an efﬁcient backbone for shallow trans-
formers, which gives the proposed ESWT a concise struc-
ture. In addition, we propose a striped window mechanism
that exhibits superior long-term dependency modeling ca-
pability and lower complexity than the shifted and over-
lapped window mechanisms. Furthermore, we propose a
ﬂexible window training strategy. By dynamically adjusting
the receptive ﬁeld at different stages of training and shar-
ing the learned feature representation, the proposed strategy
assists the ESWT to better utilize contextual information
without any additional cost. Extensive experiments reveal
that the proposed method outperforms SOTA methods with
fewer parameters, faster inference, smaller FLOPs, and less
memory consumption, achieving a better trade-off between
model performance and complexity.

References
[1] Eirikur Agustsson and Radu Timofte.
Ntire 2017 chal-
lenge on single image super-resolution: Dataset and study.
In IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 5, 6, 8
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.
Layer normalization.
arXiv preprint arXiv: 1607.06450,
2016. 3
[3] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and
Marie line Alberi Morel.
Low-complexity single-image
super-resolution based on nonnegative neighbor embedding.
In British Machine Vision Conference (BMVC), 2012. 5
[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-
ing Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,
and Wen Gao.
Pre-trained image processing transformer.
In IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2021. 1, 2
[5] Xiangyu Chen, Xintao Wang, Jiantao Zhou, and Chao
Dong.
Activating more pixels in image super-resolution
transformer. arXiv preprint arXiv: 2205.04437, 2022. 1,
2, 5
[6] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh.
Algorithms for learning kernels based on centered alignment.
arXiv preprint arXiv: 1203.0550, 2012. 5
[7] Hasan Demirel and Gholamreza Anbarjafari.
Discrete
wavelet transform-based satellite image resolution enhance-
ment. IEEE Transactions on Geoscience and Remote Sens-
ing, 49:1997–2004, 2011. 1
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-
formers for language understanding. arXiv preprint arXiv:
1810.04805, 2018. 2
[9] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming
Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo.
Cswin transformer: A general vision transformer backbone
with cross-shaped windows. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2022. 2,
4
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv: 2010.11929, 2020. 2
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv: 2010.11929, 2020. 8
[12] Guangwei Gao, Wenjie Li, Juncheng Li, Fei Wu, Huimin Lu,
and Yi Yu. Feature distillation interaction weighting network
for lightweight image super-resolution. In AAAI Conference
on Artiﬁcial Intelligence (AAAI), 2022. 5
[13] Guangwei Gao, Zhengxue Wang, Juncheng Li, Wenjie Li,
Yi Yu, and Tieyong Zeng. Lightweight bimodal network for
single-image super-resolution via symmetric cnn and recur-
sive transformer. In International Joint Conference on Arti-
ﬁcial Intelligence (IJCAI), 2022. 1, 2, 5, 7
[14] Arthur Gretton, Kenji Fukumizu, Choon Teo, Le Song, Bern-
hard Sch¨olkopf, and Alex Smola. A kernel statistical test of
independence. In Advances in Neural Information Process-
ing Systems (NeurIPS), 2007. 5
[15] Jinjin Gu and Chao Dong. Interpreting super-resolution net-
works with local attribution maps.
arXiv preprint arXiv:
2011.11036, 2020. 6, 8
[16] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Sin-
gle image super-resolution from transformed self-exemplars.
In IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2015. 1, 5, 6, 7
[17] Xiao Shi Huang, Felipe Perez, Jimmy Ba, and Maksims
Volkovs. Improving transformer optimization through bet-
ter initialization. In International Conference on Machine
Learning (ICML), 2020. 6
[18] Zilong Huang, Xinggang Wang, Lichao Huang, Chang
Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross
attention for semantic segmentation. In IEEE International
Conference on Computer Vision (ICCV), 2019. 2, 4
[19] Zheng Hui, Xinbo Gao, Yunchu Yang, and Xiumei Wang.
Lightweight image super-resolution with information multi-
distillation network. In ACM International Conference on
Multimedia (ACM MM), 2019. 5
[20] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. arXiv preprint arXiv: 1502.03167, 2015. 3
[21] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila.
Analyzing and improv-
ing the image quality of stylegan.
arXiv preprint arXiv:
1912.04958, 2019. 2, 6
[22] Diederik P. Kingma and Jimmy Ba.
Adam: A method
for stochastic optimization. In International Conference on
Learning Representations (ICLR), 2015. 5
[23] Ron Kohavi and David Wolpert. Bias plus variance decom-
position for zero-one loss functions. In International Con-
ference on Machine Learning (ICML), 1996. 2, 6
[24] Fangyuan Kong, Mingxi Li, Songwei Liu, Ding Liu, Jing-
wen He, Yang Bai, Fangmin Chen, and Lean Fu.
Resid-
ual local feature network for efﬁcient super-resolution. In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2022. 2, 5, 7
[25] Wenbo Li, Xin Lu, Shengju Qian, Jiangbo Lu, Xiangyu
Zhang, and Jiaya Jia.
On efﬁcient transformer-based im-
age pre-training for low-level vision. arXiv preprint arXiv:
2112.10175, 2021. 1, 2, 5, 8
[26] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc
Van Gool, and Radu Timofte. Swinir: Image restoration us-
ing swin transformer. In IEEE International Conference on
Computer Vision (ICCV), 2021. 2, 3, 4, 7, 8
[27] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and
Kyoung M. Lee. Enhanced deep residual networks for single
image super-resolution. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2017. 6

[28] Zudi Lin, Prateek Garg, Atmadeep Banerjee, Salma Abdel
Magid, Deqing Sun, Yulun Zhang, Luc Van Gool, Donglai
Wei, and Hanspeter Pﬁster.
Revisiting rcan:
Improved
training for image super-resolution. arXiv preprint arXiv:
2201.11279, 2022. 2, 5, 6
[29] Jie Liu, Jie Tang, and Gangshan Wu. Residual feature dis-
tillation network for lightweight image super-resolution. In
European Conference on Computer Vision (ECCV), 2020. 5
[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
IEEE International Conference on Computer Vision (ICCV),
2021. 2, 4
[31] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient
descent with warm restarts. In International Conference on
Learning Representations (ICLR), 2017. 5
[32] Zhisheng Lu, Juncheng Li, Hong Liu, Chaoyan Huang, Lin-
lin Zhang, and Tieyong Zeng. Transformer for single image
super-resolution. In IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2022. 1, 2, 5, 7, 8
[33] David Martin, Charless Fowlkes, Doron Tal, and Jitendra
Malik.
A database of human segmented natural images
and its application to evaluating segmentation algorithms and
measuring ecological statistics. In IEEE International Con-
ference on Computer Vision (ICCV), 2001. 5, 6, 7
[34] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto,
Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa.
Sketch-based manga retrieval using manga109 dataset. Mul-
timedia Tools and Applications, 76:21811–21838, 2017. 5,
6
[35] Thao Nguyen, Maithra Raghu, and Simon Kornblith.
Do
wide and deep networks learn the same things? uncover-
ing how neural network representations vary with width and
depth. arXiv preprint arXiv: 2010.15327, 2020. 5, 8
[36] Wenzhe
Shi,
Jose
Caballero,
Christian
Ledig,
Xia-
hai Zhuang, Wenjia Bai, Kanwal Bhatia, Antonio M.
Simoes Monteiro de Marvao, Tim Dawes, Declan O’Regan,
and Daniel Rueckert. Cardiac image super-resolution with
global correspondence using multi-atlas patchmatch.
In
Medical Image Computing and Computer-Assisted Interven-
tion (MICCAI), 2013. 1
[37] Chunwei Tian, Yixuan Yuan, Shichao Zhang, Chia-Wen Lin,
Wangmeng Zuo, and David Zhang. Image super-resolution
with an enhanced group convolutional neural network. arXiv
preprint arXiv: 2205.14548, 2022. 2, 6
[38] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,
Gabriel Synnaeve, and Herv´e J´egou. Going deeper with im-
age transformers. arXiv preprint arXiv: 2103.17239, 2021.
2
[39] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang,
Dongdong Zhang, and Furu Wei. Deepnet: Scaling trans-
formers to 1,000 layers. arXiv preprint arXiv: 2203.00555,
2022. 6
[40] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin
Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei
Wang, and Tie-Yan Liu.
On layer normalization in the
transformer architecture. arXiv preprint arXiv: 2002.04745,
2020. 6
[41] Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and
Yi Ma. Rethinking bias-variance trade-off for generalization
of neural networks. In International Conference on Machine
Learning (ICML), 2020. 2, 6
[42] Roman Zeyde, Michael Elad, and Matan Protter. On sin-
gle image scale-up using sparse-represen-tations. In Inter-
national Conference on Curves and Surfaces (ICCS), 2012.
5
[43] Xindong Zhang, Hui Zeng, Shi Guo, and Lei Zhang.
Efﬁcient long-range attention network for image super-
resolution. arXiv preprint arXiv: 2203.06697, 2022. 1, 2,
3, 6, 7
[44] Wilman W. W. Zou and Pong C. Yuen. Very low resolution
face recognition problem. IEEE Transactions on Image Pro-
cessing, 21:327–340, 2012. 1

