arXiv:2107.01562v1  [math.PR]  4 Jul 2021
Random Neural Networks in the Inﬁnite Width Limit as
Gaussian Processes
Boris Hanin
Department of Operations Research and Financial Engineering
Princeton University
July 6, 2021
Abstract
This article gives a new proof that fully connected neural networks with random
weights and biases converge to Gaussian processes in the regime where the input di-
mension, output dimension, and depth are kept ﬁxed, while the hidden layer widths tend
to inﬁnity. Unlike prior work, convergence is shown assuming only moment conditions for
the distribution of weights and for quite general non-linearities.
1
Introduction
In the last decade or so neural networks, originally introduced in the 1940’s and 50’s [29, 45],
have become indispensable tools for machine learning tasks ranging from computer vision [32]
to natural language processing [9] and reinforcement learning [47]. Their empirical success
has raised many new mathematical questions in approximation theory [13, 55, 56], probability
(see §1.2.2 for some references), optimization/learning theory [7, 8, 31, 58] and so on. The
present article concerns a fundamental probabilistic question about arguably the simplest
networks, the so-called fully connected neural networks, deﬁned as follows:
Deﬁnition 1.1 (Fully Connected Network). Fix a positive integer L as well as L+2 positive
integers n0, . . . , nL+1 and a function σ : R →R. A fully connected depth L neural network
with input dimension n0, output dimension nL+1, hidden layer widths n1, . . . , nL, and non-
linearity σ is any function xα ∈Rn0 7→z(L+1)
α
∈RnL+1 of the following form
z(ℓ)
α =
(
W (1)xα + b(1),
ℓ= 1
W (ℓ)σ(z(ℓ−1)
α
) + b(ℓ),
ℓ= 2, . . . , L + 1
,
where W (ℓ) ∈Rnℓ×nℓ−1 are matrices, b(ℓ) ∈Rnℓare vectors, and σ applied to a vector is
shorthand for σ applied to each component.
The parameters L, n0, . . . , nL+1 are called the network architecture, and z(ℓ)
α ∈Rnℓis called
the vector of pre-activations at layer ℓcorresponding to input xα. A fully connected network
with a ﬁxed architecture and given non-linearity σ is therefore a ﬁnite (but typically high)
dimensional family of functions, parameterized by the network weights (entries of the weight
matrices W (ℓ)) and biases (components of bias vectors b(ℓ)).
1

This article considers the mapping xα 7→z(L+1)
α
when the network’s weights and biases are
chosen independently at random and the hidden layer widths n1, . . . , nL are sent to inﬁnity
while the input dimension n0, output dimension nL+1, and network depth L are ﬁxed. In
this inﬁnite width limit, akin to the large matrix limit in random matrix theory (see §1.2),
neural networks with random weights and biases converge to Gaussian processes (see §1.4 for
a review of prior work). Unlike prior work Theorem 1.2, our main result, is that this holds
for general non-linearities σ and distributions of network weights (cf §1.3).
Moreover, in addition to establishing convergence of wide neural networks to a Gaussian
process under weak hypotheses, the present article gives a mathematical take aimed at prob-
abilists of some of the ideas developed in the recent monograph [44]. This book, written in
the language and style of theoretical physics by Roberts and Yaida, is based on research done
jointly with the author. It represents a far-reaching development of the breakthrough work of
Yaida [50], which was the ﬁrst to systematically explain how to compute ﬁnite width correc-
tions to inﬁnite width Gaussian process limit of random neural networks for arbitrary depth,
width, and non-linearity. Previously, such ﬁnite width (and large depth) corrections were only
possible for some special observables in linear and ReLU networks [21, 23, 24, 25, 39, 57].
The present article deals only with the asymptotic analysis of random neural networks as the
width tends to inﬁnity, leaving to future work a probabilistic elaboration of the some aspects
of the approach to ﬁnite width corrections from [44].
1.1
Roadmap
The rest of this article is organized as follows. First, in §1.2 we brieﬂy motivate the study of
neural networks with random weights. Then, in §1.3 we formulate our main result, Theorem
1.2. Before giving its proof in §2, we ﬁrst indicate in §1.4 the general idea of the proof and
its relation to prior work.
1.2
Why Random Neural Networks?
1.2.1
Practical Motivations
It may seem at ﬁrst glance that studying neural networks with random weights and biases is
of no practical interest. After all, a neural network is only useful after it has been “trained,”
i.e. one has found a setting of its parameters so that the resulting network function (at least
approximately) interpolates a given training dataset of input-output pairs (x, f(x)) for an
otherwise unknown function f : Rn0 →RnL+1.
However, the vast majority of neural network training algorithms used in practice are
variants of gradient descent starting from a random initialization of the weight matrices W (ℓ)
and bias vectors b(ℓ). Studying networks with random weights and biases therefore provides
an understanding of the initial conditions for neural network optimization.
Beyond illuminating the properties of networks at the start of training, the analysis of
random neural networks can reveal a great deal about networks after training as well. Indeed,
on a heuristic level, just as the behavior of the level spacings of the eigenvalues of large ran-
dom matrices is a surprisingly good match for emission spectra of heavy atoms [49], it is not
unreasonable to believe that certain coarse properties of the incredibly complex networks used
in practice will be similar to those of networks with random weights and biases. More rig-
orously, neural networks used in practice often have many more tunable parameters (weights
and biases) than the number of datapoints from the training dataset. Thus, at least in certain
2

regimes, neural network training provably proceeds by an approximate linearization around
initialization, since no one parameter needs to move much to ﬁt the data. This so-called NTK
analysis [14, 16, 30, 31, 35] shows, with several important caveats related to network size and
initialization scheme, that in some cases the statistical properties of neural networks at the
start of training are the key determinants of their behavior throughout training.
1.2.2
Motivation from Random Matrix Theory
In addition to being of practical importance, random neural networks are also fascinating
mathematical objects, giving rise to new problems in approximation theory [12, 13, 22, 55, 56],
random geometry [26, 27], and random matrix theory (RMT). Perhaps the most direct, though
by no means only, connection to RMT questions is to set the network biases b(ℓ) to zero and
consider the very special case when σ(t) = t is the identity (in the machine learning literature
these are called deep linear networks). The network function
z(L+1)
α
= W (L+1) · · · W (1)xα
(1.1)
is then a linear statistic for a product of L + 1 independent random matrices. Such matrix
models have been extensively studied, primarily in two regimes. The ﬁrst is the multiplicative
ergodic theorem regime [10, 17, 18, 46], in which all the layer widths n0, . . . , nL+1 are typically
set to a ﬁxed value n and the network depth L tends to inﬁnity. The second regime, where
L is ﬁxed and the layer widths nℓ(i.e. matrix dimensions) tend to inﬁnity, is the purview of
free-probability [38, 48].
In the presence of a non-linearity σ, random neural network provide non-linear general-
izations of the usual RMT questions. For instance, the questions taken up in this article are
analogs of the joint normality of linear statistics of random matrix products in the free proba-
bility regime. Further, random neural networks give additional motivation for studying matrix
products appearing in (1.1) when the matrix dimensions nℓand the number of terms L are
simultaneously large. This double scaling limit reveals new phenomena [3, 4, 5, 6, 20, 24, 25]
but is so far poorly understood relative to the ergodic or free regimes.
Finally, beyond studying linear networks, random matrix theory questions naturally ap-
pear in neural network theory via non-linear analogs of the Marchenko-Pastur distribution for
empirical covariance matrices of z(L+1)
α
when α ∈A ranges over a random dataset of inputs
[1, 28, 41, 43] as well as through the spectrum of the input-output Jacobian [24, 42] and the
NTK [2, 16].
1.3
Main Result
Our main result shows that under rather general conditions, when the weights W (ℓ) and
biases b(ℓ) of a fully connected network are chosen at random, the resulting ﬁeld xα 7→z(L+1)
α
converges to a centered Gaussian ﬁeld with iid components when the input dimension n0
and output dimension nL+1 are held ﬁxed but the hidden layer widths n1, . . . , nL tend to
inﬁnity. To give the precise statement in Theorem 1.2 below, ﬁx a fully connected neural
network with depth L ≥1, input dimension n0, output dimension nL+1, hidden layer widths
n1, . . . , nL ≥1, and non-linearity σ : R →R. We assume that σ is absolutely continuous and
that its almost-everywhere deﬁned derivative (and hence σ itself) is polynomially bounded:
∃k > 0 s.t. ∀x ∈R


σ′(x)
1 + |x|k


L∞(R)
< ∞.
(1.2)
3

All non-linearities used in practice satisfy these rather mild criteria. Further, let us write W (ℓ)
ij
for the entries of the weight matrices W (ℓ) and b(ℓ)
i
for the components of the bias vectors
b(ℓ). For ℓ≥2 the Deﬁnition 1.1 of fully connected networks means that the formula for the
components of the pre-activations z(ℓ)
α
at layer ℓin terms of those for z(ℓ−1)
α
reads
z(ℓ)
i;α := b(ℓ)
i
+
nℓ−1
X
j=1
W (ℓ)
ij σ(z(ℓ−1)
j;α
),
i = 1, . . . , nℓ,
(1.3)
where we’ve denoted by z(ℓ)
i;α the ith component of the nℓ-dimensional vector of pre-activations
z(ℓ)
α
in layer ℓcorresponding to a network input xα ∈Rnℓ. We make the following assumption
on the network weights:
W (ℓ)
ij :=
 CW
nℓ−1
1/2
c
W (ℓ)
ij ,
c
W (ℓ)
ij ∼µ
iid,
(1.4)
where µ is a ﬁxed probability distribution on R such that
µ has mean 0, variance 1, and ﬁnite higher moments.
(1.5)
We further assume the network biases are iid Gaussian1 and independent of the weights:
b(ℓ)
i
∼N(0, Cb)
iid.
(1.6)
In (1.4) and (1.6), CW > 0 and Cb ≥0 are ﬁxed constants. These constants do not play an
important role for the analysis in this article but will be crucial for followup work. With the
network weights and biases chosen at random the vectors z(ℓ)
α
are also random. Our main
result is that, in the inﬁnite width limit, they have independent Gaussian components.
Theorem 1.2. Fix n0, nL+1 and a compact set T ⊆Rn0.
As the hidden layer widths
n1, . . . , nL tend to inﬁnity, the sequence of stochastic processes
xα ∈Rn0
7→
z(L+1)
α
∈RnL+1
converges weakly in C0(T, RnL+1) to a centered Gaussian process taking values in RnL+1 with
iid coordinates. The coordinate-wise covariance function
K(L+1)
αβ
:=
lim
n1,...,nL→∞Cov

z(L+1)
i;α
, z(L+1)
i;β

for this limiting process satisﬁes the layerwise recursion
K(ℓ+1)
αβ
= Cb + CW E [σ(zα)σ(zβ)] ,
 zα
zβ

∼N
 
0,
 
K(ℓ)
αα
K(ℓ)
αβ
K(ℓ)
αβ
K(ℓ)
ββ
!!
(1.7)
for ℓ≥2, with initial condition
K(2)
αβ = Cb + CWE
h
σ

z(1)
1;α

σ

z(1)
1;β
i
,
(1.8)
where the distribution of (z(1)
1;α, z(1)
1;β) is determined via (1.3) by the distribution of weights and
biases in the ﬁrst layer and hence is not universal.
We prove Theorem 1.2 in §2. First, we explain the main idea and review prior work.
1As explained in §1.4 the universality results in this article are simply not true if the biases are drawn iid
from a ﬁxed non-Gaussian distribution.
4

1.4
Theorem 1.2: Discussion, Main Idea, and Relation to Prior Work
At a high level, the proof of Theorem 1.2 (speciﬁcally the convergence of ﬁnite-dimensional
distributions) proceeds as follows:
1. Conditional on the mapping xα 7→z(L)
α , the components of the neural network output
xα 7→z(L+1)
α
are independent sums of nL independent random ﬁelds (see (1.3)), and
hence, when nL is large, are each approximately Gaussian by the CLT.
2. The conditional covariance in the CLT from step 1 is random at ﬁnite widths (it depends
on z(L)
α ). However, it has the special form of an average over j = 1, . . . , nL of the same
function applied to each component z(L)
j;α of the vector z(L)
α
of pre-activations at the last
hidden layer. We call such objects collective observables (see §2.1.2 and (1.10)).
3. While z(ℓ)
j;α are not independent at ﬁnite width when ℓ≥2, they are weakly suﬃciently
correlated that a LLN still applies to any collective observable in the inﬁnite width limit
(see §2.1.2).
4. The LLN from step 3 allows us to replace the random conditional covariance matrix
from steps 1 and 2 by its expectation, asymptotically as n1, . . . , nL tend to inﬁnity.
We turn to giving a few more details on steps 1-4 and reviewing along the way the relation
of the present article to prior work. The study of the inﬁnite width limit for random neural
networks dates back at least to Neal [37], who considered networks with one hidden layer:
z(2)
i;α = b(2)
i
+
n1
X
j=1
W (2)
ij σ

z(1)
j;α

,
z(1)
j;α = b(1)
j
+
n0
X
k=1
W (1)
jk xk;α,
where i = 1, . . . , n2. In the shallow L = 1 setting of Neal if in addition n2 = 1, then neglecting
the bias b(2)
1
for the moment, the scalar ﬁeld z(2)
1;α is a sum of iid random ﬁelds with ﬁnite
moments, and hence the asymptotic normality of its ﬁnite-dimensional distributions follows
immediately from the multidimensional CLT. Modulo tightness, this explains why z(2)
1;α ought
to converge to a Gaussian ﬁeld. Even this simple case, however, holds several useful lessons:
• If the distribution of the bias b(2)
1
is ﬁxed independent of n1 is and non-Gaussian, then
the distribution of z(2)
1;α will not be Gaussian, even in the limit when n1 →∞.
• If the ﬁrst layer biases b(1)
j
are drawn iid from a ﬁxed distribution µb and σ is non-linear,
then higher moments of µb will contribute to the variance of each neuron post-activation
σ(z(1)
j;α), causing the covariance of the Gaussian ﬁeld at inﬁnite width to be non-universal.
• Unlike in deeper layers, as long as n0 is ﬁxed, the distribution of each neuron pre-
activation z(1)
j;α in the ﬁrst layer will not be Gaussian, unless the weights and biases in
layer 1 are themselves Gaussian. This explains why, in the initial condition (1.8) the
distribution is non-Gaussian in the ﬁrst layer.
In light of the ﬁrst two points, what should one assume about the bias distribution? There
are, it seems, two options. The ﬁrst is to assume that the variance of the biases tends to
5

zero as n1 →∞, putting them on par with the weights. The second, which we adopt in this
article, is to declare all biases to be Gaussian.
The ﬁrst trick in proving Theorem 1.2 for general depth and width appears already when
L = 1 but the output dimension n2 is at least two.2 In this case, even for a single network
input xα, at ﬁnite values of the network width n1 diﬀerent components of the random n2-
dimensional vector z(2)
α
are not independent, due to their shared dependence on the vector
z(1)
α . The key observation, which to the author’s knowledge was ﬁrst presented in [34], is
to note that the components of z(2)
α
are independent conditional on the ﬁrst layer (i.e. on
z(1)
α ) and are approximately Gaussian when n1 is large by the CLT. The conditional variance,
which captures the main dependence on z(1)
α , has the following form:
Σ(2)
αα := Var
h
z(2)
i;α
 z(1)
α
i
= Cb + CW
n1
n1
X
j=1
σ

z(1)
j;α
2
.
(1.9)
This is an example of what we’ll call a collective observable, an average over all neurons in
a layer of the same function applied to the pre-activations at each neuron (see §2.1.2 for the
precise deﬁnition). In the shallow L = 1 setting, Σ(2)
αα is a sum of n1 iid random variables with
ﬁnite moments. Hence, by the LLN, it converges almost surely to its mean as n1 →∞. This
causes the components of z(2)
α
to become independent in the inﬁnite width limit, since the
source of their shared randomness, Σ(L+1)
αα
, can be replaced asymptotically by its expectation.
The proof for general L follows a similar pattern. Exactly as before, for any 0 ≤ℓ≤L,
the components of the pre-activations at layer ℓ+ 1 are still conditionally independent, given
the pre-activations at layer ℓ. When the width nℓis large the conditional distribution of each
component over any ﬁnite collection of inputs is therefore approximately Gaussian by the
CLT. Moreover, the conditional covariance across network inputs has the form:
Σ(ℓ+1)
αβ
:= Cov

z(ℓ+1)
i;α
, z(ℓ+1)
i;β
 z(ℓ)
α , z(ℓ)
β

= Cb + CW
nℓ
nℓ
X
j=1
σ

z(ℓ)
j;α

σ

z(ℓ)
j;β

.
(1.10)
The summands on the right hand side are no longer independent at ﬁnite width if ℓ≥2.
However, Σ(ℓ+1)
αβ
are still collective observables, and the crucial point is to check that their
dependence is suﬃciently weak that we may still apply the LLN. Verifying this is the heart
of the proof of Theorem 1.2 and is carried out in §2.1.2.
Let us mention that, in addition to the approach outlined above, other methods for show-
ing that wide neural networks are asymptotically Gaussian processes are possible. In the
prior article [36], for instance, the idea is to use that the entries of z(ℓ)
α
are exchangeable and
argue using an exchangeable CLT. This leads to some technical complications which, at least
in the way the argument is carried out in [36], result in unnatural restrictions on the class
of non-linearities and weight distributions considered there. Let us also mention that in the
article [34], the non-independence at ﬁnite width of the components of z(ℓ)
α
for large ℓwas
circumvented by considering only the sequential limit in which nℓ→∞in order of increasing
ℓ. The eﬀect is that for every ℓthe conditional covariance Σ(ℓ)
αβ has already converged to its
2Neal [37] states erroneously on page 38 of his thesis that z(2)
i;α and z(2)
j;α will be independent because the
weights going into them are independent. This is not true at ﬁnite width but becomes true in the inﬁnite
width limit.
6

mean before nℓ+1 is taken large. However, this way of taking the inﬁnite width limit seems
to the author somewhat unnatural and is any case not conducive to studying ﬁnite width
corrections as in [44, 50], which we plan to take up in future work.
We conclude this section by pointing the reader to several other related strands of work.
The ﬁrst are articles such as [11], which quantify the magnitude of the diﬀerence
1
nℓ
nℓ
X
i=1
z(ℓ)
i;αz(ℓ)
i;β −
lim
n1,...,nℓ→∞E
"
1
nℓ
nℓ
X
i=1
z(ℓ)
i;αz(ℓ)
i;β
#
between the empirical overlaps n−1
ℓ
D
z(ℓ)
α , z(ℓ)
β
E
of pre-activations and the corresponding inﬁ-
nite width limit uniformly over network inputs xα, xβ in a compact subset of Rn0. In a similar
vein are articles such as [15], which give quantitative estimates at ﬁnite width for the distance
from xα 7→z(ℓ)
α
to a nearby Gaussian process.
The second is the series of articles starting with the work of Yang [51, 52, 53, 54], which
develops the study not only of initialization but also certain aspects of inference with inﬁnitely
wide networks using what Yang terms tensor programs. As part of that series, the article
[52] establishes that in the inﬁnite width limit many diﬀerent architectures become Gaussian
processes. However, the arguments in those articles are signiﬁcantly more technical than the
ones presented here since they are focused on building the foundation for the tensor program
framework. At any rate, to the best of the author’s knowledge, no prior article addresses
universality of the Gaussian process limit with respect to the weight distribution in deep
networks (for shallow networks with L = 1 this was considered by Neal in [37]). Finally,
that random neural networks converge to Gaussian processes in the inﬁnite width limit under
various restrictions but for architectures other than fully connected is taken up in [19, 40, 52].
2
Proof of Theorem 1.2
Let us recall the notation. Namely, we ﬁx a network depth L ≥1, an input dimension n0 ≥1,
an output dimension nL+1 ≥1, hidden layer widths n1, . . . , nL ≥1 and a non-linearity σ
satisfying (2.5). We further assume that the networks weights and biases are independent
and random as in (1.4) and (1.6). To prove Theorem 1.2 we must show that the random
ﬁelds xα 7→z(L+1)
α
converge weakly in distribution to a Gaussian process in the limit where
n1, . . . , nL tend to inﬁnity. We start with the convergence of ﬁnite-dimensional distributions.
Let us therefore ﬁx a collection
xA = {xα,
α ∈A}
of |A| distinct network inputs in Rn0 and introduce for each ℓ= 0, . . . , L + 1, every i =
1, . . . , nℓ, and all α ∈A the vectorized notation
z(ℓ)
i;A :=

z(ℓ)
i;α, α ∈A

∈R|A|,
z(ℓ)
A :=

z(ℓ)
i;A, i = 1, . . . , nℓ

∈Rnℓ×|A|.
The following result states that the distribution of the random variable z(L+1)
A
with values in
RnL+1×|A| converges to that of the claimed Gaussian ﬁeld.
Proposition 2.1 (Convergence of Finite-Dimensional Distributions). Fix L ≥1 and n0, nL+1.
The distribution of z(L+1)
A
converges weakly as n1, . . . , nL →∞to that of a centered Gaussian
7

in RnL+1×|A| with iid rows for which the covariance
K(L+1)
αβ
=
lim
n1,...,nL→∞Cov

z(L+1)
i;α
, z(L+1)
i;β

,
α, β ∈A
between the entries in each row satisﬁes the recursion (1.7) with initial condition (1.8).
Once we have proved Proposition 2.1 in §2.1, it remains to show tightness. For this, we ﬁx
a compact subset T ⊆Rn0. The tightness of xα 7→z(L+1)
α
in C0(T, RnL+1) follows immediately
from the Arzel`a-Ascoli Theorem and the following result, which we prove in §2.2.
Proposition 2.2 (High Probability Equicontinuity and Equiboundedness of z(L+1)
α
). For
every L ≥1, ǫ > 0 there exists C = C(ǫ, σ, T, L, Cb, CW ) > 0 so that
sup
xα,xβ∈T

z(L+1)
α
−z(L+1)
β


2
||xα −xβ||2
≤C
and
sup
xα∈T

z(L+1)
α

 ≤C
(2.1)
with probability at least 1 −ǫ.
2.1
Finite-Dimensional Distributions: Proof of Proposition 2.1
We will prove Proposition 2.1 in two steps. First, we prove a special case in which we keep
the weights in layer 1 completely general as in (1.4) but take weights in layers ℓ≥2 to be
independent Gaussians:
W (ℓ)
ij
∼N
 0, CW n−1
ℓ−1

,
iid.
We continue to assume (as in the statement of Theorem 1.2) that all biases are Gaussian:
b(ℓ)
i
∼N(0, Cb),
iid.
The argument in this case is the technical heart of this paper and is presented in §2.1.1 - §2.1.2.
Ultimately, it relies on the analysis of collective observables, which we isolate in §2.1.2. A
simple Lindeberg swapping argument and induction on layer detailed in §2.1.3 allows us to
extend Proposition 2.1 to general weights in layers ℓ≥2 from the Gaussian case.
2.1.1
Proof of Proposition 2.1 with Gaussian Weights in Layers ℓ≥2
Fix
Ξ = (ξi, i = 1, . . . , nL+1) ∈RnL+1×|A|,
ξi = (ξi;α, i = 1, . . . , nL+1, α ∈A) ∈R|A|
and consider the characteristic function
χA(Ξ) = E
h
exp
h
−i
D
z(L+1)
A
, Ξ
Eii
= E
"
exp
"
−i
X
α∈A
nL+1
X
i=1
z(L+1)
i;α
ξi;α
##
of the random variable z(L+1)
A
∈RnL+1×|A|. By Levy’s continuity theorem, it is suﬃcient to
show that
lim
n1,...,nL→∞χA(Ξ) = exp
"
−1
2
nL+1
X
i=1
D
K(L+1)
A
ξi, ξi
E#
,
(2.2)
8

where
K(L+1)
A
=

K(L+1)
αβ

α,β∈A
is the matrix deﬁned by the recursion (1.7) with initial condition (1.8). Writing
Fℓ:= ﬁltration deﬁned by
n
W (ℓ′), b(ℓ′), ℓ′ = 1, . . . , ℓ
o
,
(2.3)
we may use the tower property to write
χA(Ξ) = E
h
E
h
exp
h
−i
D
z(L+1)
A
, Ξ
Ei  FL
ii
.
(2.4)
Note that conditional on FL, the random vectors z(L+1)
i;A
∈R|A| in layer L + 1 for each
i = 1, . . . , nL+1 are iid Gaussians, since we’ve assumed for now that weights in layers ℓ≥2
are Gaussian. Speciﬁcally,
z(L+1)
i;A
d=

Σ(L+1)
A
1/2
Gi,
Gi ∼N
 0, I|A|

iid
1 ≤i ≤nL+1,
where for any α, β ∈A the conditional covariance is

Σ(L+1)
A

αβ = Cov

z(L+1)
i;α
, z(L+1)
i;β
 FL

= Cb + CW
nL
nL
X
j=1
σ

z(L)
j;α

σ

z(L)
j;β

.
(2.5)
Using (2.4) and the explicit form of the characteristic function of a Gaussian reveals
χA(Ξ) = E
"
exp
"
−1
2
nL+1
X
i=1
D
Σ(L+1)
A
ξi, ξi
E##
.
(2.6)
The crucial observation is that each entry of the conditional covariance matrix Σ(L+1)
A
is an
average over j = 1, . . . , nL of the same ﬁxed function applied to the vector z(L)
j;A. While z(L)
j;A
are not independent at ﬁnite values of n1, . . . , nL−1 for L > 1, they are suﬃciently weakly
correlated that a weak law of large numbers still holds:
Lemma 2.3. Fix n0, nL+1. There exists a |A| × |A| PSD matrix
K(L+1)
A
=

K(L+1)
αβ

α,β∈A
such that for all α, β ∈A
lim
n1,...,nL→∞E

Σ(L+1)
A

αβ

= K(L+1)
αβ
and
lim
n1,...,nL→∞Var

Σ(L+1)
A

αβ

= 0.
Proof. Lemma 2.3 is a special case of Lemma 2.4 (see §2.1.2).
Lemma 2.3 implies that Σ(L+1)
A
converges in distribution to K(L+1)
A
.
In view of (2.6)
and the deﬁnition of weak convergence this immediately implies (2.2). It therefore remains
9

to check that K(L+1)
A
satisﬁes the desired recursion.
For this, note that at any values of
n1, . . . , nL we ﬁnd
Cov

z(L+1)
i;α
, z(L+1)
i;β

= E

Σ(L+1)
A

αβ

= Cb + CW E
h
σ

z(L)
1;α

σ

z(L)
1;β
i
.
When L = 1, we therefore see that
Cov

z(2)
i;α, z(2)
i;β

= Cb + CWE
h
σ(z(1)
i;α)σ(z(1)
i;β )
i
,
where the law of (z(1)
i;α, z(1)
i;β ) is determined by the distribution µW of weights in layer 1 and
does not depend on n1. This conﬁrms the initial condition (1.8). Otherwise, if L > 1, the
convergence of ﬁnite-dimensional distributions that we’ve already established yields
K(L+1)
αβ
=
lim
n1,...,nL→∞Cov

z(L+1)
i;α
, z(L+1)
i;β

=
lim
n1,...,nL−1→∞

Cb + CWE
h
σ

z(ℓ)
1;α

σ

z(ℓ)
1;β
i
.
Since σ is continuous we may invoke the continuous mapping theorem to conclude that
K(L+1)
αβ
= Cb + CW E(zα,zβ)∼G(0,K(L)) [σ(zα)σ(zβ)] ,
which conﬁrms the recursion (1.7).
This completes the proof that the ﬁnite-dimensional
distributions of z(L+1)
α
converge to those of the desired Gaussian process, modulo two issues.
First, we must prove Lemma 2.3. This is done in §2.1.2 by proving a more general result,
Lemma 2.4. Second, we must remove the assumption that the weights in layers ℓ≥2 are
Gaussian. This is done in §2.1.3.
□
2.1.2
Collective Observables with Gaussian Weights: Generalizing Lemma 2.3
This section contains the key technical argument in our proof of Proposition 2.1. To state the
main result, deﬁne a collective observable at layer ℓto be any random variable of the form
O(ℓ)
nℓ,f;A := 1
nℓ
nℓ
X
i=1
f(z(ℓ)
i;A),
where f : R|A| →R is measurable and polynomially bounded:
∃C > 0, k ≥1 s.t. ∀z ∈R|A|
|f(z)| ≤C

1 + ||z||k
2

.
We continue to assume, as §2.1.1, that the weights (and biases) in layers ℓ≥2 are Gaussian
and will remove this assumption in §2.1.3. Several key properties of collective observables are
summarized in the following Lemma:
Lemma 2.4 (Key Properties of Collective Observables). Let O(ℓ)
nℓ,f;A be a collective observable
at layer ℓ. There exists a deterministic scalar O
(ℓ)
f;A such that
lim
n1,...,nℓ−1→∞E
h
O(ℓ)
nℓ,f;A
i
= O
(ℓ)
f;A.
(2.7)
10

Moreover,
lim
n1,...,nℓ→∞Var
h
O(ℓ)
nℓ,f;A
i
= 0.
(2.8)
Hence, we have the following convergence in probability
lim
n1,...,nℓ→∞O(ℓ)
nℓ,f;A
p
−→
O
(ℓ)
f;A.
Proof. This proof is by induction on ℓ, starting with ℓ= 1. In this case, z(1)
i;A are independent
for diﬀerent i. Moreover, for each i, α we have
z(1)
i;α = b(1)
i
+
n0
X
j=1
W (1)
ij xj;α = b(1)
i
+
CW
n0
1/2 n0
X
j=1
c
W (1)
ij xj;α.
Hence, z(1)
i;α have ﬁnite moments since b(1)
i
are iid Gaussian and c
W (1)
ij
are mean 0 with ﬁnite
higher moments. In particular, since f is polynomially bounded, we ﬁnd for every n1 that
E
h
O(1)
n1,f;A
i
= E
h
f(z(1)
1;A)
i
,
which is ﬁnite and independent of n1, conﬁrming (2.7). Further, O(1)
n1,f;A is the average of n1
iid random variables with all moments ﬁnite. Hence, (2.8) follows by the weak law of large
numbers, completing the proof of the base case.
Let us now assume that we have shown (2.7) and (2.8) for all ℓ= 1, . . . , L. We begin by
checking that (2.7) holds at layer L + 1. We have
E
h
O(L+1)
nL+1,f;A
i
= E
h
f(z(L+1)
1;A
)
i
.
(2.9)
Since the weights and biases in layer L + 1 are Gaussian and independent of FL, we ﬁnd
z(L+1)
1;A
d=

Σ(L+1)
A
1/2
G,
(2.10)
where Σ(L+1)
A
is the conditional covariance deﬁned in (2.5) and G is an |A|-dimensional stan-
dard Gaussian. The key point is that Σ(L+1)
A
is a collective observable at layer L. Hence,
by the inductive hypothesis, there exists a PSD matrix Σ
(L+1)
A
such that Σ(L+1)
A
converges in
probability to Σ
(L+1)
A
as n1, . . . , nL →∞. To establish (2.7) it therefore suﬃces in view of
(2.9) to check that
lim
n1,...,nL→∞E

f

Σ(L+1)
A
1/2
G

= E

f

Σ
(L+1)
A
1/2
G

,
(2.11)
where the right hand side is ﬁnite since f is polynomially bounded and all polynomial moments
of G are ﬁnite. To establish (2.11), let us invoke the Skorohod representation theorem to ﬁnd
a common probability space on which there are versions of Σ(L+1)
A
– which by an abuse of
notation we will still denote by Σ(L+1)
A
– that converge to Σ
(L+1)
A
almost surely. Next, note
that since f is polynomially bounded we may repeatedly apply ab ≤1
2(a2 + b2) to ﬁnd
f((Σ(L+1)
A
)1/2G)
 ≤p

(Σ(L+1)
A
)1/2
+ q(G),
(2.12)
11

where p is a polynomial in the entries of (Σ(L+1)
A
)1/2, q a polynomial in the entries of G, and
the polynomials p, q don’t depend on n1, . . . , nL. The continuous mapping theorem shows
that
lim
n1,...,nL→∞E
h
p

(Σ(L+1)
A
)1/2i
= E
h
p

(Σ
(L+1)
A
)1/2i
.
Thus, since all moments of Gaussian are ﬁnite, (2.11) follows from the generalized dominated
convergence theorem. It remains to argue that (2.8) holds at layer L+1. To do this, we write
Var
h
O(L+1)
nL+1,f;A
i
=
1
nL+1
Var
h
f(z(L+1)
1;A
)
i
+

1 −
1
nL+1

Cov

f(z(L+1)
1;A
), f(z(L+1)
2;A
)

. (2.13)
Observe that
Var
h
f(z(L+1)
1;A
)
i
≤E
h
f(z(L+1)
1;A
)2i
= E
"
1
nL+1
nL+1
X
i=1

f(z(L+1)
i;A
)
2
#
< ∞,
since we already showed that (2.7) holds at layer L + 1. Next, recall that, conditional on FL,
neurons in layer L + 1 are independent. The law of total variance and Cauchy-Schwartz yield
Cov

f(z(L+1)
1;A
), f(z(L+1)
2;A
)
 =
Cov

E
h
f(z(L+1)
1;A
) | FL
i
, E
h
f(z(L+1)
2;A
) | FL
i
≤Var
h
E
h
f(z(L+1)
1;A
) | FL
ii
.
(2.14)
Using (2.10) and the polynomial estimates (2.12) on f, we conclude that the conditional
expectation on the previous line is some polynomially bounded function of the components
of (Σ(L+1)
A
)1/2. Hence, we may apply dominated convergence as above to ﬁnd
lim
n1,...,nL→∞Var
h
E
h
f(z(L+1)
1;A
) | FL
ii
= Var
h
E
h
f(Σ
1/2
A )G
ii
= 0,
G ∼N(0, I|A|)
since E
h
f(Σ
1/2
A )G
i
is a constant. This proves (2.8) for observables at layer L+1 and completes
the proof of Lemma 2.4.
2.1.3
Proof of Proposition 2.1 for General Weights
In this section, we complete the proof of Proposition 2.1 by removing the assumption from
§2.1.1 that weights in layers ℓ≥2 are Gaussian. To do this, let us introduce some notation.
Let us write
xα 7→z(ℓ)
α
for the pre-activations at layers ℓ= 0, . . . , L + 1 of a random fully connected network in
which, as in the general case of Theorem 1.2, all weights and biases are independent, biases
are Gaussian as in (1.6) and weights in all layers are drawn from a general centered distribution
with the correct variance and ﬁnite higher moments as in (1.4) and (1.5). Next, let us write
xα 7→ez(ℓ)
α ,
for the vector of pre-activations obtained by replacing, in each layer ℓ= 2, . . . , L + 1, all
weights W (ℓ)
ij
by iid centered Gaussians with variance CW /nℓ−1. We already saw that the
distribution of ez(L+1)
A
converges weakly to that of the desired Gaussian in the inﬁnite width
12

limit. Our goal is thus to show that, as n1, . . . , nL tend to inﬁnity, z(L+1)
A
and ez(L+1)
A
converge
weakly to the same distribution. We will proceed by induction on L. When L = 0 the claim
is trivial since, by construction, the weight and bias distributions in layer 1 are identical in
both z(1)
α
and ez(1)
α
(recall that when we proved Proposition 2.1 in §2.1.1 we had Gaussian
weights only starting from layer 2 and general weights in layer 1.)
Suppose therefore that we have shown the claim for ℓ= 0, . . . , L. By the Portmanteau
theorem and the density of smooth compactly supported functions in the space of continuous
compactly supported functions equipped with the supremum norm, it suﬃces to show that
for any smooth function g : RnL+1×|A| →R with compact support we have
lim
n1,...,nL→∞

E
h
g(z(L+1)
A
)
i
−E
h
g(ez(L+1)
A
)
i
= 0.
(2.15)
To check (2.15), let us deﬁne an intermediate object:
z(L+1),•
α
= b(L+1) + W (L+1),•σ

z(L)
α

,
where the entries W (L+1),•
ij
of W (L+1),• are iid Gaussian with mean 0 and variance CW /nL.
That is, we take the vector σ(z(L)
α ) of post-activations from layer L obtained by using general
weights in layers 1, . . . , L and use Gaussian weights only in layer L + 1. Our ﬁrst step in
checking (2.15) is to show that it this relation holds when z(L+1)
A
is replaced by z(L+1),•
A
.
Lemma 2.5. Let xA = {xα, ∈A} be a ﬁnite subset of Rn0 consists of |A| distinct elements.
Fix in addition nL+1 ≥1 and a smooth compactly supported function g : RnL+1×|A| →R.
There exists C > 0 and a collective observable O(L)
nL,f;A at layer L so that
E
h
g

z(L+1)
A
i
−E
h
g

z(L+1),•
A
i ≤Cn3
L+1n−1/2
L
E
h
O(L)
nL,f;A
i
.
Proof. This is a standard Lindeberg swapping argument. Namely, for each α ∈A and k =
0, . . . , nL deﬁne
z(L+1),k
α
:= b(L+1) + W (L+1),kσ

z(L)
α

,
where the ﬁrst k entries of each row of W (L+1),k are iid Gaussian with mean 0 and variance
CW /nL, while the remaining entries are (CW /nL)−1/2 times iid draws c
W (L+1)
ij
from the general
distribution µ of network weights, as in (1.4) and (1.5). With this notation, we have
z(L+1)
α
= z(L),0
α
,
ez(L+1),•
α
= z(L),nL
α
.
Thus,
E
h
g

z(L+1)
A
i
−E
h
g

ez(L+1)
A
i
=
nL
X
k=1
E
h
g

z(L+1),k−1
A
i
−E
h
g

z(L+1),k
A
i
.
For any Z ∈Rnℓ+1×|A| and
δZ = (δzi;α i = 1, . . . , nℓ+1, α ∈A) ∈Rnℓ+1×|A|
13

consider the third order Taylor expansion of g around Z.
g(Z + δZ) = g(Z) +
X
α∈A
i=1,...,nL+1
gi;αδzi;α +
X
α1,α2∈A
i1,i2=1,...,nL+1
gi1,i2;α1,α2δzi1;α1δzi2;α2
+ O




X
α1,α2,α3∈A
i1,i2,i3=1,...,nL+1
|δzi1;α1δzi2;α2δzi3;α3|



.
Let us write
z(L+1),k−1
i;α
= z(L+1),k
i;α
+ n−1/2
L
Yi,k;α,
δZi,k;α = C1/2
W

f
W (L+1)
ik
−c
W (L+1)
ik

σ(z(L)
k;α),
where f
W (L+1)
ik
∼N(0, 1). Then, Taylor expanding g to third order around Zk = z(L+1),k
i;α
and,
using that the ﬁrst two moments of (CW n−1
L )1/2c
W (L)
ij
match those of N(0, CW n−1
L ), we ﬁnd
that
E
h
g

z(L+1),k−1
A
i
−E
h
g

ez(L+1),k
A
i
= O

n−3/2
L
n3
L+1E
h
p
σ(z(L)
k;α)
 , α ∈A)
i
,
where p is a degree 3 polynomial in the |A|-dimensional vector of absolute values |σ(z(ℓ)
k;α)|, α ∈
A. Summing this over k completes the proof.
To make use of Lemma 2.5 let us consider any collective observable O
(L)
nL,f;A at layer L.
Recall that by (2.9) and (2.13) both the mean and variance of O
(L)
nL,f;A depend only on the
distributions of ﬁnitely many components of the vector z(L)
A . By the inductive hypothesis we
therefore ﬁnd
lim
n1,...,nL→∞E
h
O(L)
nL,f;A
i
=
lim
n1,...,nL→∞E
h
eO(L)
nL,f;A
i
,
(2.16)
where the right hand side means that we consider the same collective observable but for ez(L)
A
instead of z(L)
A , which exists by Lemma 2.4. Similarly, again using Lemma 2.4, we have
lim
n1,...,nL→∞Var
h
O(L)
nL,f;A
i
= 0.
(2.17)
Therefore, we conclude that
O(L)
nL,f;A −eO(L)
nL,f;A
d
−→
0,
as n1, . . . , nL →∞.
(2.18)
Note that by (2.16) the mean of any collective observable E
h
O(L)
nL,f;A
i
is bounded independent
of n1, . . . , nL since we saw in Lemma 2.4 that the limit exists and is bounded when using
Gaussian weights. Since nL+1 is ﬁxed and ﬁnite, the error term n−1/2
L
n3
L+1E
h
O
(L)
nL,f;A
i
in
Lemma 2.5 is therefore tends to zero as n1, . . . , nL →∞, and (2.15) is reduced to showing
that
lim
n1,...,nL→∞

E
h
g(z(L+1),•
A
)
i
−E
h
g(ez(L+1)
A
)
i
= 0.
(2.19)
14

This follows from (2.17) and the inductive hypothesis. Indeed, by construction, conditional on
the ﬁltration FL deﬁned by weights and biases in layers up to L (see (2.3)), the |A|-dimensional
vectors z(L+1),•
i;A
are iid Gaussians:
z(L+1),•
i;A
d=

Σ(L+1)
A
1/2
Gi,
Gi ∼N(0, I|A|)
iid,
where Σ(L+1)
A
is the conditional covariance matrix from (2.5). The key point, as in the proof
with all Gaussian weights, is that each entry of the matrix Σ(L+1)
A
is a collective observable
at layer L. Moreover, since the weights and biases in the ﬁnal layer are Gaussian for z(L+1),•
A
the conditional distribution of g(z(L+1),•
A
) given FL is completely determined by Σ(L+1)
A
. In
particular, since g is bounded and continuous, we ﬁnd that
E
h
g(z(L+1),•
A
)
i
−E
h
g(ez(L+1)
A
)
i
= E
h
h

Σ(L+1)
A
i
−E
h
h

eΣ(L+1)
A
i
,
where h : RnL+1×|A| →R is a bounded continuous function and eΣ(L+1)
A
is the conditional co-
variance matrix at layer L+1 for ez(L+1)
A
. Combining this with the convergence in distribution
from (2.18) shows that (2.19) holds and completes the proof of Proposition 2.1 for general
weight distributions.
□
2.2
Tightness: Proof of Proposition 2.2
In this section, we provide a proof of Proposition 2.2. In the course of showing tightness, we
will need several elementary Lemmas, which we record in the §2.2.1. We then use them in
§2.2.2 to complete the proof of Proposition 2.2.
2.2.1
Preparatory Lemmas
For the ﬁrst Lemma, let us agree to write C(A) for the cone over a subset A in a euclidean
space and B1(Rn) for the unit ball in Rn.
Lemma 2.6. Fix integers n0, n1 ≥1 and a real number λ ≥1. Suppose that T is a compact
of Rn0 and f : Rn0 →Rn1 is λ-Lipschitz with respect to the ℓ2-norm on both Rn0 and Rn1.
Deﬁne the Minkowski sums
bT = f(T) + C (f(T) −f(T)) ∩B1(Rn1).
There exists a constant C > 0 a compact subset T ′ of R3n0+1, and a Cλ-Lipschitz map
g : R3n0+1 →Rn1 (all depending only T, λ,), so that bT = g(T ′).
Proof. By deﬁnition,
bT = g(T × T × T × [0, 1]),
g(x, y, z, r) = f(x) + r(f(y) −f(z)).
(2.20)
In particular, for some constant C depending on T0, λ, we have
g(x, y, z, r) −g(x′, y′, z′, r′)

2 ≤
f(x) −f(x′)

2 +
f(y) −f(y′)

2 +
f(z) −f(z′)

2
+
r −r′ ||f(y) −f(z)||2
≤λ
 x −x′
2 +
y −y′
2 +
z −z′
2 + Diam(T0) |r −r0|

≤Cλ
(x −x′, y −y′, z −z′, r −r′)

2 .
15

Hence, bT is the image under a Lipschitz map with a Lipschitz constant depending only on
T0, λ of a compact set in R3n0+1.
The second Lemma we need is an elementary inequality.
Lemma 2.7. Let a, b, c ≥0 be real numbers and k ≥1 be an integer. We have
(a + b + c)k ≤22k−1 
1 + a2k
+ 1
4
h
(2 + b)4k + (1 + c)4ki
.
Proof. For any a, b ≥0 we have
(a + b)k =
k
X
j=0
k
j

ajbk−j ≤
k
X
j=0
k
j

(1 + a)k bk−j = (1 + a)k(1 + b)k
≤1
2

(1 + a)2k + (1 + b)2k
(2.21)
Further, breaking into cases depending on whether 0 ≤a ≤1 or 1 ≤a we ﬁnd that
(a + b)k ≤22k−1 
1 + a2k
+ 1
2(1 + b)2k.
(2.22)
Combining (2.21) with (2.22) we see as desired that any a, b, c ≥0
(a + b + c)k ≤22k−1 
1 + a2k
+ 1
4
h
(2 + b)4k + (1 + c)4ki
.
The next Lemma is also an elementary estimate.
Lemma 2.8. Fix an integer k ≥1, and suppose X1, . . . , Xk are non-negative random vari-
ables. There exists a positive integer q = q(k) such that
E
" k
Y
i=1
Xi
#
≤
 k
Y
i=1
E [Xq
i ]
!1/q
.
Proof. The proof is by induction on k. For the base cases when k = 1, we may take q = 1
and when k = 2 we may take q = 2 by Cauchy-Schwartz. Now suppose we have proved the
claim for all k = 1, 2, . . . , K for some K ≥3. Note that 1 ≤v⌈(K + 1)/2⌉≤K. So we may
use Cauchy-Schwartz and the inductive hypothesis to obtain
E
"K+1
Y
i=1
Xi
#
= E


⌈K+1
2
⌉
Y
i=1
Xi
K
Y
i=⌈K+1
2
⌉+1
Xi


≤E


⌈K+1
2
⌉
Y
i=1
X2
i


1/2
E


K
Y
i=⌈K+1
2
⌉+1
X2
i


1/2
≤
 K+1
Y
i=1
E
h
X2q
i
i!1/2q
,
where q = max

q
 ⌈1
2(K + 1)⌉

, q
 K −⌈1
2(K + 1)⌉−1
	
.
16

The next Lemma is an elementary result about the moments of marginals of iid random
vectors.
Lemma 2.9. Fix an even integer p ≥2 a positive integer, and suppose µ is a probability
measure on R with mean 0 and ﬁnite higher moments. Assume also that w = (w1, . . . , wn)
is a vector with iid components, each with distribution µ. Then, there exists a constant C
depending only on p and ﬁrst p moments of µ such that for all n ≥1
sup
||u||=1
E [|w · u|p] ≤C.
Proof. We will use the following result of  Lata la [33, Thm. 2, Cor. 2, Rmk. 2]. Suppose Xi
are independent random variables and p is a positive even integer. Then
E
"
n
X
i=1
Xi

p#
≃inf
(
t > 0

n
X
i=1
log E
1 + Xi
t

p
≤p
)
,
(2.23)
where ≃means bounded above and below up to universal multiplicative constants. Let us
ﬁx a unit vector u = (u1, . . . , un) ∈Sn−1 and apply this to Xi = uiwi. Since wi have mean 0
and p is even we ﬁnd
n
X
i=1
log E
1 + Xi
t

p
≤
n
X
i=1
log

1 +
p
X
k=2
p
k
|ui|k E
h
|w1|ki
tk

.
Note that for each k = 2, . . . , p we have
E
h
|w1|ki
≤E [(1 + |w1|)p]
and
|ui|k ≤u2
i .
Hence, using that log(1 + x) ≤x we ﬁnd
n
X
i=1
log E
1 + Xi
t

p
≤
n
X
i=1
log
 
1 + u2
i E [(1 + |w1|)p]
p
X
k=2
p
k
 1
tk
!
=
n
X
i=1
log

1 + u2
i E [(1 + |w1|)p]

1 + 1
t
p
−1 −p
t

≤E [(1 + |w1|)p]

1 + 1
t
p
−1 −p
t

.
Note that for 2 < t, there is a universal constant C > 0 so that


1 + 1
t
p
−1 −p
t
 ≤Cp2
t2 .
Thus, there exists a constant C′ > 0 so that
t > C′p
pE [(1 + |w1|)p])
⇒
n
X
i=1
log E
1 + Xi
t

p
≤p.
Combining this with (2.23) completes the proof.
17

The ﬁnal Lemma we need is an integrability statement for the supremum of certain non-
Gaussian ﬁelds over low-dimensional sets.
Lemma 2.10. Fix a positive integer n0, an even integer k ≥1, a compact set T0 ⊆Rn0,
a constant λ > 0, and a probability measure µ on R with mean 0, variance 1, and ﬁnite
higher moments. For every ǫ ∈(0, 1) there exists a constant C = C(T0, ǫ, λ, n0, k, µ) with the
following property. Fix any integer n1 ≥1 and a λ-Lipschitz map f : Rn0 →Rn1. Deﬁne
T1 := f(T0), and let w = (w1, . . . , wn1) be a vector with iid components wi, each drawn from
µ. Then, for any ﬁxed y0 ∈T1
E
"
sup
y∈T1
(w · (y −y0))k
#
≤C.
(2.24)
Proof. The proof is a standard chaining argument. For each y ∈T1 write Πk(y) for the closest
point to y in a 2−k net in T1 and assume without loss of generality that the diameter of T1
is bounded above by 1 and that Π0(y) = y0 for all y ∈T1. We have using the usual chaining
trick that
E


 
sup
y∈T1
w · (y −y0)
!k
≤
∞
X
q1,...,qk=0
E
" k
Y
i=1
sup
yi∈T1
|w · (Πqi(yi) −Πqi−1(yi))|
#
.
(2.25)
By Lemma 2.8, there exists q depending only on k so that for any q1, . . . , qk we have
E
" k
Y
i=1
sup
yi∈T1
|w · (Πqi(yi) −Πqi−1(yi))|
#
≤
k
Y
i=1
 
E
"
sup
y∈T1
|w · (Πqi(y) −Πqi−1(y))|q
#!1/q
.
(2.26)
We seek to bound each expectation on the right hand side in (2.26). To do this, write
E
"
sup
y∈T1
|w · (Πqi(y) −Πqi−1(y))|q
#
=
Z ∞
0
P
 
sup
y∈T1
|w · (Πqi(y) −Πqi−1(y))|q > t
!
dt.
Note that the supremum is only over a ﬁnite set of cardinality at most
|Πqi|
Πqi−1
 ≤2cn0qi
for some c > 0 depending only T0, λ. This is because, by assumption T1 is the image of T0
under a λ-Lipschtiz map and Lipschitz maps preserve covering numbers. Thus, by a union
bound,
P
 
sup
y∈T1
|w · (Πqi(y) −Πqi−1(y))|q > t
!
≤2cn0qi sup
y∈T1
P (|w · (Πqi(y) −Πqi−1(y))|q > t)
But for any y ∈T1 and any s > 0, p ≥1 we have
P (|w · (Πq(y) −Πq−1(y))|q > s) ≤sup
||u||=1
E [|w · u|p]
||Πk(y) −Πk−1(y)||p
sp/q

= 2−pqis−p/q sup
||u||=1
E [|w · u|p] .
18

Putting this all together we ﬁnd for any p ≥2 max {q + 2, cn0} that
E
"
sup
y∈T1
|w · (Πqi(y) −Πqi−1(y))|q
#
≤2−cn0qi sup
||u||=1
E [|w · u|p] .
Thus, substituting this into (2.25) yields
E


 
sup
y∈T1
w · (y −y0)
!k
≤sup
||u||=1
E [|w · u|p]
∞
X
q1,...,qk=0
2−cn0
Pk
i=1 qi ≤2k sup
||u||=1
E [|w · u|p] .
Appealing to Lemma 2.9 completes the proof of Lemma 2.10.
2.2.2
Proof of Proposition 2.2 Using Lemmas from §2.2.1
Let us ﬁrst establish the equi-Lipschitz condition, which we recall states that for each ǫ ∈(0, 1)
and each compact set T ⊆Rn0 there exist C > 0 so that with probability at least 1 −ǫ we
have
sup
xα,xβ∈T

z(L+1)
α
−z(L+1)
β


2
||xα −xβ||2
≤C.
(2.27)
For (2.27) to hold, we need a result about the Lipschitz constant of each layer. To ease the
notation deﬁne a normalized single layer with random weights W and random biases b via
the map ψ : Rn1 →Rn2:
ψ(x; W, b) =
1
√n2
σ (Wx + b) ,
(2.28)
where b ∼N(0, CbIn2) and W = (wij) ∈Rn2×n1 with wij drawn iid from a distribution
with mean 0, variance 1, and ﬁnite higher moments. We choose the variance of wij to be 1
instead of CW /n1 since we will later think of x as the normalized vector (CW /nℓ)1/2σ(z(ℓ)
α )
of post-activations in a given layer.
Lemma 2.11. Fix an integer n0 ≥1, a compact set T0 ⊆Rn0, and a constant λ > 0. For
every ǫ ∈(0, 1) there exists a constant C = C(ǫ, n0, T0, σ, λ) with the following property. Fix
any integers n1, n2 ≥1, and deﬁne ψ : Rn1 →Rn2 as in (2.28). Suppose that T1 ⊆Rn1 is the
image of T0 under a λ-Lipschitz map from Rn0 to Rn1. Then,
P
 
sup
xα,xβ∈T1
||ψ(xα) −ψ(xβ)||2
||xα −xβ||2
≤C
!
≥1 −ǫ.
Proof. Fix xα ̸= xβ ∈T1 and deﬁne
ξαβ =
xα −xβ
||xα −xβ||2
.
Write Wi for the i-th row of W and bi for the i-th component of b. Since ab ≤1
2(a2 + b2) and
19

σ is absolutely continuous, we have
||ψ(xα) −ψ(xβ)||2
2
= 1
n2
n2
X
i=1
(σ (Wi · xα + bi) −σ (Wi · xα + bi))2
= 1
n2
n2
X
i=1
 
Wi · ξαβ
Z ||xα−xβ||2
0
σ′ (Wi · (xβ + tξαβ) + bi) dt
!2
≤||xα −xβ||2
2
1
n2
n2
X
i=1
sup
y∈bT
 σ′ (Wi · y + bi)
2 sup
ξ∈eT
(Wi · ξ)2
≤||xα −xβ||2
2
1
n2
n2
X
i=1
"
sup
y∈bT
 σ′ (Wi · y + bi)
4 + sup
ξ∈eT
(Wi · ξ)4
#
,
(2.29)
where we’ve set
eT = C(T1 −T1) ∩Sn1−1
and
bT := T1 + C(T1 −T1) ∩B1(Rn1),
and have denoted by C(A) the cone over a set A and by B1(Rn1) the unit ball in Rn1. The
estimate (2.29) yields
P
 
sup
xα,xβ∈T
||ψ(xα) −ψ(xβ)||2
2
||xα −xβ||2
2
> C
!
≤P
 
1
n2
n2
X
i=1
"
sup
y∈bT
 σ′ (Wi · y + bi)
4 + sup
ξ∈eT
(Wi · ξ)4
#
> C
!
Since σ′ is polynomially bounded by assumption (1.2), we ﬁnd by Markov’s inequality that
there exists an even integer k ≥2 so that for any C > 1
P
 
sup
xα,xβ∈T
||ψ(xα) −ψ(xβ)||2
||xα −xβ||2
> C
!
≤
1 + E
h
supy∈bT |W1 · y + b1|k + supξ∈eT |W1 · ξ|4i
C −1
.
(2.30)
Our goal is now to show that the numerator in (2.30) is bounded above by a constant that
depends only on T0, n0, λ. For this, let us ﬁx any y0 ∈bT and apply Lemma 2.7 as follows:
|W1 · y + b1|k = |W1 · (y −y0) + W1 · y0 + b1|k
≤22k−1 
1 + |W1 · (y −y0)|2k
+ 1
4
h
(2 + |W1 · y0|4k) + (1 + |b1|)4ki
.
Substituting this and the analogous estimate for |W · ξ|4 into (2.30), we see that since all mo-
ments of the entries of the weights and biases exist, there exists a constant C′ > 0 depending
on λ, T0, k so that
P
 
sup
xα,xβ∈T
||ψ(xα) −ψ(xβ)||2
||xα −xβ||2
> C
!
≤
C′ + E
h
supy∈bT |W1 · (y −y0)|2k + supξ∈eT |W1 · (ξ −ξ0)|4i
C −1
,
(2.31)
20

where ξ0 ∈eT is any ﬁxed point. Note that by Lemma 2.6, the sets bT, eT are both contained in
the image of a compact subset T ′ ⊆R3n0+1 under a Lipschitz map, with Lipschitz constant
depending only on λ, T. Thus, an application of Lemma 2.6 shows that there exists a constant
C′′ depending only λ, T, k so that
E
"
sup
y∈bT
|W1 · (y −y0)|2k
#
+ E
"
sup
ξ∈eT
|W1 · (ξ −ξ0)|4
#
≤C′′.
Substituting this into (2.31) and taking C suﬃciently large completes the proof of Lemma
2.6.
Lemma 2.11 directly yields the equi-Lipschitz estimate (2.27). Indeed, let us ﬁx ǫ ∈(0, 1)
and a compact set T ⊆Rn0. Let us deﬁne
h(ℓ)
α :=









q
CW
n0 xα,
ℓ= 0
q
CW
nℓσ(z(ℓ)
α ),
ℓ= 1, . . . , L
1
√nL+1z(L+1)
α
,
ℓ= L + 1
.
For ℓ= 1, . . . , L + 1 we have
h(ℓ)
α =



q
CW
nℓσ

c
W (ℓ)h(ℓ−1)
α
+ b(ℓ)
,
ℓ= 1, . . . , L
1
√nL+1

c
W (L+1)h(L)
α
+ b(L+1)
,
ℓ= L + 1
where the rescaled weight matrices c
W (ℓ), deﬁned in (1.4), has iid mean 0 variance 1 entries
with ﬁnite higher moments. To each of the transformations h(ℓ)
α 7→h(ℓ+1)
α
we may now apply
Lemma 2.11. Speciﬁcally, applying Lemma 2.11 in the ﬁrst layer shows that there exists
C(1) > 0 so that the rescaled ﬁrst layer map
sup
xα,xβ∈T

h(1)
α −h(1)
β


||xα −xβ||
≤C(1)
with probability at least 1 −ǫ/(L + 1). Thus, the image
T (1) := h(1)(T)
of T under the normalized ﬁrst layer map is the image under a C(1)−Lipschitz map of the
compact set T ⊆Rn0.
This allows us to apply Lemma 2.11 again, but this time to the
second layer, to conclude that, again, there exist C(2) > 0 so that with probability at least
1 −2ǫ/(L + 1) the normalized second layer map satisﬁes
sup
xα,xβ∈T

h(2)
α −h(2)
β


||xα −xβ||
≤
sup
xα,xβ∈T

h(2)
α −h(2)
β



h(1)
α −h(1)
β


sup
xα,xβ∈T

h(1)
α −h(1)
β


||xα −xβ||
≤C(1)C(2).
Proceeding in this way, with probability at least 1 −ǫ we that
sup
xα,xβ∈T

z(L+1)
α
−z(L+1)
β


||xα −xβ||
= n1/2
L+1
sup
xα,xβ∈K

h(L+1)
α
−h(L+1)
β


||xα −xβ||
≤n1/2
L+1C(1) · · · C(L+1).
21

Since nL+1 is ﬁxed and ﬁnite, this conﬁrms (2.27). It remains to check the uniform bound-
edness condition in (2.1). For this note that for any ﬁxed xβ ∈K by Lemma 2.4, we have
sup
n1,...,nL≥1
E

1
nL+1

z(L+1)
β


2
< ∞.
Thus, by Markov’s inequality,

z(L+1)
β

 is bounded above with high probability. Combined
with the equi-Lipschitz condition (2.27), which we just saw holds with high probability on K,
we conclude that for each ǫ > 0 there exists C > 0 so that
P

sup
xα∈K

z(L+1)
α

 ≤C

≥1 −ǫ,
as desired.
□
References
[1] Ben Adlam, Jake Levinson, and Jeﬀrey Pennington. A random matrix perspective on
mixtures of nonlinearities for deep learning. arXiv preprint arXiv:1912.00827, 2019.
[2] Ben Adlam and Jeﬀrey Pennington.
The neural tangent kernel in high dimensions:
Triple descent and a multi-scale theory of generalization. In International Conference on
Machine Learning, pages 74–84. PMLR, 2020.
[3] Andrew Ahn.
Fluctuations of beta-jacobi product processes.
arXiv preprint
arXiv:1910.00743, 2019.
[4] Gernot Akemann and Zdzislaw Burda. Universal microscopic correlation functions for
products of independent ginibre matrices. Journal of Physics A: Mathematical and The-
oretical, 45(46):465201, 2012.
[5] Gernot Akemann, Zdzislaw Burda, and Mario Kieburg. Universal distribution of lya-
punov exponents for products of ginibre matrices. Journal of Physics A: Mathematical
and Theoretical, 47(39):395202, 2014.
[6] Gernot Akemann, Zdzislaw Burda, and Mario Kieburg.
From integrable to chaotic
systems: Universal local statistics of lyapunov exponents. EPL (Europhysics Letters),
126(4):40001, 2019.
[7] Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overﬁtting
in linear regression. Proceedings of the National Academy of Sciences, 117(48):30063–
30070, 2020.
[8] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
Reconciling modern
machine-learning practice and the classical bias–variance trade-oﬀ. Proceedings of the
National Academy of Sciences, 116(32):15849–15854, 2019.
[9] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini
Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya
22

Ramesh, Daniel M. Ziegler, Jeﬀrey Wu, Clemens Winter, Christopher Hesse, Mark
Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
models are few-shot learners. 2020.
[10] Andrea Crisanti, Giovanni Paladin, and Angelo Vulpiani. Products of random matrices:
in Statistical Physics, volume 104. Springer Science & Business Media, 2012.
[11] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural
networks: The power of initialization and a dual view on expressivity. Proceedings of
Neural Information Processing Systems, 2016.
[12] Ingrid Daubechies, Ronald DeVore, Simon Foucart, Boris Hanin, and Guergana Petrova.
Nonlinear approximation and (deep) relu networks. Constructive Approximation, pages
1–46, 2021.
[13] Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural network approximation.
Acta Numerica 2021 (to appear). arXiv:2012.14501, 2020.
[14] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably
optimizes over-parameterized neural networks. Proceedings of International Conference
on Representation Learning, 2019.
[15] Ronen Eldan, Dan Mikulincer, and Tselil Schramm. Non-asymptotic approximations of
neural networks by gaussian processes. arXiv preprint arXiv:2102.08668, 2021.
[16] Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel
for linear-width neural networks. Proceedings of Neural Information Processing Systems,
2020.
[17] Harry Furstenberg. Noncommuting random products. Transactions of the American
Mathematical Society, 108(3):377–428, 1963.
[18] Harry Furstenberg and Harry Kesten. Products of random matrices. The Annals of
Mathematical Statistics, 31(2):457–469, 1960.
[19] Adri`a Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolu-
tional networks as shallow gaussian processes. arXiv preprint arXiv:1808.05587, 2018.
[20] Vadim Gorin and Yi Sun. Gaussian ﬂuctuations for products of random matrices. To
appear in American Journal of Mathematics. arXiv:1812.06532, 2018.
[21] Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradi-
ents? Proceedings of Neural Information Processing Systems 2018, 2018.
[22] Boris Hanin. Universal function approximation by deep neural nets with bounded width
and relu activations. Mathematics, 7(10):992, 2019.
[23] Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent
kernel. Proceedings of Interntional Conference on Representation Learning, 2019.
[24] Boris Hanin and Mihai Nica. Products of many large random matrices and gradients in
deep neural networks. Communications in Mathematical Physics, pages 1–36, 2019.
23

[25] Boris Hanin and Grigoris Paouris. Non-asymptotic results for singular values of gaussian
matrix products. Geometric and Functional Analysis, 31(2):268–324, 2021.
[26] Boris Hanin and David Rolnick.
Complexity of linear regions in deep networks.
In
International Conference on Machine Learning, pages 2596–2604. PMLR, 2019.
[27] Boris Hanin and David Rolnick. Deep relu networks have surprisingly few activation
patterns. Proceedings Neural Information Processing Systems, 2019.
[28] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in
high-dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560,
2019.
[29] Donald Olding Hebb. The organization of behavior; a neuropsycholocigal theory.
A
Wiley Book in Clinical Psychology, 62:78, 1949.
[30] Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural
tangent hierarchy. In International Conference on Machine Learning, pages 4542–4551.
PMLR, 2020.
[31] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence
and generalization in neural networks.
Proceedings of Neural Information Processing
Systems, 2018.
[32] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with
deep convolutional neural networks. Advances in neural information processing systems,
25:1097–1105, 2012.
[33] Rafa l Lata la. Estimation of moments of sums of independent real random variables. The
Annals of Probability, 25(3):1502–1513, 1997.
[34] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeﬀrey Pennington,
and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. Proceedings of
Interntional Conference on Representation Learning, 2019.
[35] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. On the linearity of large non-linear mod-
els: when and why the tangent kernel is constant. Proceedings of Neural Information
Processing Systems, 2020.
[36] Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin
Ghahramani. Gaussian process behaviour in wide deep neural networks. Proceedings of
International Conference on Representation Learning, 2018.
[37] Radford M Neal. Priors for inﬁnite networks. In Bayesian Learning for Neural Networks,
pages 29–53. Springer, 1996.
[38] Alexandru Nica and Roland Speicher. Lectures on the combinatorics of free probability,
volume 13. Cambridge University Press, 2006.
[39] Lorenzo Noci, Gregor Bachmann, Kevin Roth, Sebastian Nowozin, and Thomas Hof-
mann. Precise characterization of the prior predictive distribution of deep relu networks.
arXiv preprint arXiv:2106.06615, 2021.
24

[40] Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron,
Daniel A Abolaﬁa, Jeﬀrey Pennington, and Jascha Sohl-Dickstein.
Bayesian deep
convolutional networks with many channels are gaussian processes.
arXiv preprint
arXiv:1810.05148, 2018.
[41] S P´ech´e. A note on the pennington-worah distribution. Electronic Communications in
Probability, 24:1–7, 2019.
[42] Jeﬀrey Pennington, Samuel Schoenholz, and Surya Ganguli. The emergence of spectral
universality in deep networks. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 1924–1932. PMLR, 2018.
[43] Jeﬀrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning.
Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124005, 2019.
[44] Daniel Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory.
arXiv:2106.10165 (under contract at Cambridge University Press), 2021.
[45] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and
organization in the brain. Psychological review, 65(6):386, 1958.
[46] David Ruelle.
Ergodic theory of diﬀerentiable dynamical systems.
Publications
Math´ematiques de l’Institut des Hautes ´Etudes Scientiﬁques, 50(1):27–58, 1979.
[47] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang,
Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mas-
tering the game of go without human knowledge. nature, 550(7676):354–359, 2017.
[48] Dan Voiculescu. Addition of certain non-commuting random variables. Journal of func-
tional analysis, 66(3):323–346, 1986.
[49] Eugene P Wigner. On the distribution of the roots of certain symmetric matrices. Annals
of Mathematics, pages 325–327, 1958.
[50] Sho Yaida. Non-gaussian processes and neural networks at ﬁnite widths. In Mathematical
and Scientiﬁc Machine Learning, pages 165–192. PMLR, 2020.
[51] Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process
behavior, gradient independence, and neural tangent kernel derivation. arXiv preprint
arXiv:1902.04760, 2019.
[52] Greg Yang. Tensor programs i: Wide feedforward or recurrent neural networks of any ar-
chitecture are gaussian processes. Proceedings of Neural Information Processing Systems,
2019.
[53] Greg Yang.
Tensor programs ii: Neural tangent kernel for any architecture.
arXiv
preprint arXiv:2006.14548, 2020.
[54] Greg Yang. Tensor programs iii: Neural matrix laws. arXiv preprint arXiv:2009.10685,
2020.
[55] Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural
Networks, 94:103–114, 2017.
25

[56] Dmitry Yarotsky.
Optimal approximation of continuous functions by very deep relu
networks. In Conference on Learning Theory, pages 639–649. PMLR, 2018.
[57] Jacob A Zavatone-Veth and Cengiz Pehlevan. Exact priors of ﬁnite neural networks.
arXiv preprint arXiv:2104.11734, 2021.
[58] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Un-
derstanding deep learning (still) requires rethinking generalization. Communications of
the ACM, 64(3):107–115, 2021.
26

