BAYESIAN MODELING OF UNCERTAINTY 
IN LOW-LEVEL VISION 

THE KLUWER INTERNATIONAL SERIES IN 
ENGINEERING AND COMPUTER SCIENCE 
ROBOTICS: VISION, MANIPULATION AND SENSORS 
Consulting Editor 
Takeo Kanade 
Carnegie Mellon University 
Other books in the series: 
Robotic Grasping and Fine Manipulation, M. Cutkosky 
ISBN 0-89838-200-9 
Shadows and Silhouettes in Computer Vision, S. Shafer 
ISBN 0-89838-167-3 
Perceptual Organization and Visual Recognition, D. Lowe 
ISBN 0-89838-172-X 
Robot Dynamics Algorithms, R. Featherstone 
ISBN 0-89838-230-0 
Three Dimensional Machine Vision, T. Kanade (editor) 
ISBN 0-89838-188-6 
Kinematic Modeling, Identification and Control of Robot Manipulators, 
H.W. Stone 
ISBN 0-89838-237-8 
Object Recognition Using Vision and Touch, P.K. Allen 
ISBN 0-89838-245-9 
Integration, Coordination and Control of Multi-Sensor Robot Systems, 
H.F. Durrant-Whyte 
ISBN 0-89838-247-5 
Motion Understanding: Robot and Human Vision, 
W.N. Martin and 1.K. Aggrawal (Editors) 
ISBN 0-89838-258-0 

BAYESIAN MODELING OF 
UNCERTAINTY IN 
LOW-LEVEL VISION 
by 
Richard Szeliski 
Carnegie Mellon University 
with a foreword by 
Takeo Kanade 
1rII... " 
KLUWER ACADEMIC PUBLISHERS 
Boston/Dordrecht/London 

Distributors for North America: 
Kluwer Academic Publishers 
tOl Philip Drive 
Assinippi Park 
Norwell, Massachusetts 02061 USA 
Distributors for all other countries: 
Kluwer Academic Publishers Group 
Distribution Centre 
Post Office Box 322 
3300 AH Dordrecht, THE NETHERLANDS 
Library of Congress Cataloging-in-Publication Data 
Szeliski, Richard. 1958-
Bayesian modeling of uncertainty in low-level vision I Richard 
Szeliski ; foreword by Takeo Kanade. 
p. cm. -
(The Kluwer international series in engineering and 
computer science; #79) 
ISBN-13: 978-1-4612-8904-3 
DOl: 10.1007/978-1-4613-1637-4 
e-ISBN-13: 978-1-4613-1637-4 
1. Computer vision-Mathematical models. 
I. Title. 
II. Series: 
Kluwer international series in engineering and computer science: 
SECS 79. 
TA1632.S94 1989 
89-15632 
Copyright © 1989 by Kluwer Academic Publishers 
Softcover reprint of the hardcover 1st edition 1989 
CIP 
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system 
or transmitted in any form or by any means, mechanical, photocopying, recording, or other-
wise, without the prior written permission of the publisher, Kluwer Academic Publishers, tOl 
Philip Drive, Assinippi Park, Norwell, Massachusetts 02061. 
. 

To my parents, Zdzislaw Ludwig Szeliski and Jadwiga Halina Szeliska, who 
have always encouraged my academic aspirations, and whose integrity and 
love have been a lifelong inspiration. 

Contents 
Foreword by Takeo Kanade 
xiii 
Preface ..... 
xv 
1 Introduction . 
1 
1.1 
Modeling uncertainty in low-level vision 
2 
1.2 
Previous work . . . 
5 
1.3 
Overview of results 
9 
1.4 
Organization . . . . 
12 
2 Representations for low-level vision . 
15 
2.1 
Visible surface representations . 
15 
2.2 
Visible surface algorithms . . . . 
18 
2.2.1 
Regularization . . . . . . 
18 
2.2.2 
Finite element discretization . 
21 
2.2.3 
Relaxation . . . . . . . 
22 
2.3 
Multiresolution representations 
23 
2.3.1 
Multigrid algorithms . . 
25 
2.3.2 
Relative representations 
27 
2.3.3 
Hierarchical basis functions 
33 
2.4 
Discontinuities . . . . . . . 
45 
2.5 
Alternative representations . . . . . 
46 
3 Bayesian models and Markov Random Fields 
49 
3.1 
Bayesian models . . . . . . 
49 
3.2 
Markov Random Fields .. 
50 
3.3 
Using probabilistic models. 
56 
4 Prior models .......... 
59 
4.1 
Regularization and fractal priors. 
60 
4.2 
Generating constrained fractals 
65 
4.3 
Relative depth representations (reprise) 
75 

viii 
Bayesian Modeling of Uncertainty in Low-Level Vision 
4.4 Mechanical vs. probabilistic models . 
5 Sensor models. . . . . . . . . . . . 
5.1 
Sparse data: spring models .. 
5.2 Sparse data: force field models 
5.3 
Dense data: optical flow ... 
5.4 Dense data: image intensities 
6 Posterior estimates . . . . . 
6.1 
MAP estimation . . . . 
6.2 
Uncertainty estimation . 
6.3 
Regularization parameter estimation . 
6.4 
Motion estimation without correspondence 
7 Incremental algorithms for depth-from-motion 
7.1 
Kalman filtering . . . . . . . . . . . . 
7.2 
Incremental iconic depth-from-motion . 
7.2.1 
Mathematical analysis . . . . 
7.2.2 Evaluation.......... 
7.3 
Joint modeling of depth and intensity 
7.3.1 
Regularized stereo ..... . 
7.3.2 
Recursive motion estimation 
7.3.3 
Adding discontinuities. 
8 Conclusions..... 
8.1 
Summary.... 
8.2 
Future research . 
Bibliography . . . . . . 
A Finite element implementation. 
B Fourier analysis. . . . . . . . . . . . . . . . . . 
B.1 Filtering behavior of regularization . . . . . 
B.2 Fourier analysis of the posterior distribution 
B.3 Analysis of gradient descent. . . . . . 
B.4 Finite element solution. . . . . . . . . 
B.5 Fourier analysis of multigrid relaxation 
C Analysis of optical flow computation . . . 
78 
83 
84 
87 
93 
95 
99 
99 
101 
106 
112 
121 
122 
126 
130 
133 
139 
140 
144 
146 
149 
149 
151 
155 
167 
173 
173 
175 
176 
177 
179 
183 

Contents 
D Analysis of parameter estimation . . 
D.1 Computing marginal distributions 
D.2 Bayesian estimation equations . 
D.3 Likelihood of observations. 
Table of symbols 
Index ..... . 
ix 
187 
187 
188 
190 
193 
195 

List of Figures 
1.1 
Visual processing hierarchy . . . . . . . . . . . . . . . . 
3 
1.2 Interpolated surface, typical surface, and uncertainty map 
11 
2.1 
A more complex visual processing hierarchy 
17 
2.2 
Sample data and interpolated surface . . . 
20 
2.3 
Single level relaxation algorithm solutions . 
24 
2.4 
Multiresolution pyramid . . . . . . . . . . . 
26 
2.5 
Coarse-to-fine multiresolution relaxation solution . 
28 
2.6 
Random-dot stereogram showing Comsweet illusion 
29 
2.7 
Disparity profiles for random-dot stereogram 
29 
2.8 
Relative multiresolution decomposition . . . 
32 
2.9 
Hierachical basis pyramid . . . . . . . . . . 
34 
2.10 Hierarchical basis representation of solution 
36 
2.11 Conjugate gradient relaxation example ... 
38 
2.12 Algorithms for nodal and hierarchical conjugate gradient descent 
39 
2.13 Hierarchical conjugate gradient (L = 4) relaxation example. 
41 
2.14 Algorithm convergence as a function of L ..... 
42 
2.15 Algorithm convergence as a function of interpolator . . . . 
43 
2.16 Dual lattice for representing discontinuities . . . . . . . . . 
45 
3.1 
Simple example of Bayesian modeling: terrain classification. 
51 
3.2 
Conditional probabilities for terrain model . . . . . . . . . . 
52 
3.3 
Restoration of noisy images through Markov Random Fields . 
54 
4.1 
Typical sample from the thin plate prior model . . . 
61 
4.2 
Fractal (random) solution . . . . . . . . . . . . . . 
63 
4.3 
Multiresolution fractal sample from thin plate model 
68 
4.4 
Power spectrum of mixed membrane / thin plate . . 
70 
4.5 
Interpolated surface and fractal surface for mixed membrane / 
thin plate . . . . . . . . . . . . . . . . . . . . . . . . . . . .. 
70 
4.6 
Power spectrum of fractal approximation . . . . . . . . . . " 
71 
4.7 
Interpolated surface and fractal surface for fractal approximation 
71 
4.8 
Power spectrum of composite (relative representation) surface. 
72 

xii 
Bayesian Modeling of Uncertainty in Low-Level Vision 
4.9 
Depth constraints for fractal generation example . . . . . . .. 
74 
4.10 Constrained fractal with spatially varying fractal dimension and 
variance . . . . . . . . . . . . . . . . . . . . . 
74 
4.11 Relative representation for (3 = 3 interpolator . . . . . 
77 
4.12 Random sample using weak membrane as prior. . . . 
80 
4.13 Random sample from a three-dimensional elastic net . 
81 
5.1 
Cubic spline fit with different data constraints ... 
85 
5.2 
Constraint energy for contaminated Gaussian . . . . 
86 
5.3 
Depth constraint with three-dimensional uncertainty 
90 
5.4 Simple camera model showing blur, sampling and noise 
95 
6.1 
Sample covariance and variance fields ..... 
102 
6.2 
Stochastic estimates of covariance and variance. 
104 
6.3 
Cubic spline with confidence interval . . . 
105 
6.4 Typical solutions for various (>., T) settings 
108 
6.5 
Family of splines of varying smoothness 
110 
6.6 
Maximum likelihood estimate of O"p • • • • 
110 
6.7 
"Blocks world" synthetic range data. . . . 
117 
6.8 
Interpolated surfaces from sparse block data 
118 
6.9 
Motion estimate for blocks worid data 
119 
7.1 
Kalman filter block diagram . . . . . . 
7.2 Iconic depth estimation block diagram 
7.3 
Illustration of disparity prediction stage 
7.4 
Computation of disparity using least squares fit . 
7.5 
Tiger image and edges . . . . 
7.6 
RMS error in depth estimate. 
7.7 
CIL image ........ . 
7.8 
CIL depth maps ..... . 
7.9 
CIL-2 and SRI depth maps 
7.10 An Epipolar Plane Image . 
A.1 Continuity strengths and computational molecules 
B.1 Filter and noise model . . . . . . . . . . . . . . . 
B.2 Effective frequency response of multigrid relaxation 
124 
127 
129 
132 
133 
134 
136 
137 
138 
141 
168 
176 
180 

Foreword 
Vision has to deal with uncertainty. The sensors are noisy, the prior knowledge is 
uncertain or inaccurate, and the problems of recovering scene information from 
images are often ill-posed or underconstrained. This research monograph, which 
is based on Richard Szeliski's Ph.D. dissertation at Carnegie Mellon University, 
presents a Bayesian model for representing and processing uncertainty in low-
level vision. 
Recently, probabilistic models have been proposed and used in vision. Sze-
liski's method has a few distinguishing features that make this monograph im-
portant and attractive. First, he presents a systematic Bayesian probabilistic 
estimation framework in which we can define and compute the prior model, 
the sensor model, and the posterior model. Second, his method represents and 
computes explicitly not only the best estimates but also the level of uncertainty 
of those estimates using second order statistics, i.e., the variance and covariance. 
Third, the algorithms developed are computationally tractable for dense fields, 
such as depth maps constructed from stereo or range finder data, rather than 
just sparse data sets. Finally, Szeliski demonstrates successful applications of 
the method to several real world problems, including the generation of fractal 
surfaces, motion estimation without correspondence using sparse range data, and 
incremental depth from motion. 
The work reported here represents an important progress toward a systematic 
treatment of uncertainty in vision. Yet, there is much more to be done, both 
theoretically and experimentally. I believe this research monograph will be a 
rich source of references and ideas for further research in this important area. 
Takeo Kanade 
May 2,1989 

Preface 
Over the last decade, many low-level vision algorithms have been devised for 
extracting depth from intensity images. The output of such algorithms usually 
contains no indication of the uncertainty associated with the scene reconstruc-
tion. The need for such error modeling is becoming increasingly recognized, 
however. This modeling is necssary because of the noise which is inherent 
in real sensors and the need to optimally integrate information from different 
sensors or viewpoints. 
In this book, which is a revision of my 1988 Ph.D. thesis at Carnegie Mellon 
University, I present a Bayesian model which captures the uncertainty associated 
with low-level vision. My model is applicable to two-dimensional dense fields 
such as depth maps, and it consists of three components: a prior model, a 
sensor model, and a posterior model. The prior model captures any a priori 
information about the structure of the dense field. I construct this model by 
using the smoothness constraints from regularization to define a Markov Random 
Field. The sensor model describes the behavior and noise characteristics of the 
measurement system. A number of sensor models are developed for both sparse 
depth measurements and dense flow and intensity measurements. The posterior 
model combines the information from the prior and sensor models using Bayes' 
Rule. This model can then be used as the input to later stages of processing. I 
show how to compute optimal estimates from the posterior model and also how 
to compute the uncertainty (variance) in these estimates. 
This book describes the application of Bayesian modeling to a number of 
low-level vision problems. The main application is the on-line extraction of 
depth from motion, for which I use a two-dimensional generalization of the 
Kalman filter. The resulting incremental algorithm provides a dense on-line es-
timate of depth which is initially crude, but whose uncertainty and error are 
reduced over time. In other applications of Bayesian modeling, I show how 
the Bayesian interpretation of regularization can be used to choose the opti-
mal smoothing parameter for interpolation; I develop a Bayesian model which 
estimates observer motion from sparse depth measurements without correspon-
dence; and I use the fractal nature of the prior model to construct multiresolution 
relative surface representations. The uncertainty modeling techniques that I de-

xvi 
Bayesian Modeling of Uncertainty in Low-Level Vision 
velop, and the utility of these techniques in various applications, demonstrate 
the usefulness of Bayesian modeling for low-level vision. 
The work in this book owes its genesis and success to the support of many 
colleagues and friends. In particular, I would like to thank my thesis advisors, 
Geoffrey Hinton and Takeo Kanade, for their ideas, encouragement and guid-
ance. During my five year stay at Carnegie Mellon, they helped me discover 
the joys of research, the necessity for clear and positive presentation, and the 
excitement of interaction with peers and colleagues. I thank the other members 
of my thesis committee, Jon Webb and Alex Pentland, for their comments on 
the thesis and their additional ideas and insights. 
I thank the members of the IUS vision research group at Carnegie Mellon, 
especially Steve Shafer, Martial Hebert, Larry Matthies, and In So Kweon, for 
many interesting discussions on computer vision problems. I am also grateful 
to the members of the Boltzmann research group, especially David Plaut, for 
providing an interesting alternative perspective on many perception problems. 
I thank Steve Zucker for first firing my interest in vision, and for many 
subsequent discussions. I also thank the computer vision researchers at other in-
stitutions, who both during visits to CMU and during discussions at conferences 
have provided many interesting ideas, insights and suggestions. 
I am grateful for the outstanding graduate research environment which exists 
in the Computer Science Department at Carnegie Mellon. I thank all of the 
faculty, administrators and students whose efforts contribute to this environment. 
I also thank of all the many friends in the department who made my stay in 
Pittsburgh so enjoyable. 
Since my graduation from Carnegie Mellon, I have been fortunate to work 
with two outstanding research groups, first at Schlumberger Palo Alto Research 
and then at SRI International. At Schlumberger, I had interesting discussions 
with Jay Tenenbaum, and was fortunate to collaborate with Demetri Terzopoulos 
on a number of ideas, and to learn from him the intricacies of physically-based 
modeling and numerical relaxation. At SRI International, I am grateful for the 
support and guidance of Martin Fischler, and for stimulating interactions with 
Yvan Leclerc and the other members of the perception group. 
Most of all, I thank my wife, Lyn Lovell McCoy, whose love and support 
over the last few years have brought joy to my daily endeavors and made the 
completion of this work possible. 
This research was sponsored in part by the Defense Advanced Research 
Projects Agency (DOD), ARPA Order No. 5976 under Contract F33615-87-C-
1499 and monitored by: Avionics Laboratory, Air Force Wright Aeronautical 
Laboratories, Aeronautical Systems Division (AFSC), Wright-Patterson AFB, 
OH 45433-6543. Support was also given by an Allied Corporation scholarship 

Preface 
xvii 
and by the National Science Foundation under Grant Number IRI-8520359, and 
by Schlumberger and SRI International. 
Richard Szeliski 
May 11, 1989 

BAYESIAN MODELING OF UNCERTAINTY 
IN LOW-LEVEL VISION 

Chapter 1 
Introduction 
This book examines the application of Bayesian modeling to low-level vision. 
Bayesian modeling is a probabilistic estimation framework that consists of three 
separate models. The prior model describes the world or its properties which 
we are trying to estimate. The sensor model describes how anyone instance of 
this world is related to the observations (such as images) which we make. The 
posterior model, which is obtained by combining the prior and sensor models 
using Bayes' Rule, describes our current estimate of the world given the data 
which we have observed. 
The main thesis of this book is that Bayesian modeling of low-level vision 
is both feasible and useful. In the course of the book, we will develop a number 
of Bayesian models for low-level vision problems such as surface interpolation 
and depth-from-motion. To show that our approach is feasible, we will build 
computationally tractable Bayesian models using Markov Random Fields. To 
show that these models are useful, we will develop representations and algo-
rithms that yield significant improvements in terms of capabilities and accuracy 
over existing regularization and energy-based low-level vision algorithms. 
The computationally tractable versions of Bayesian models that we use in-
volve estimating first and second order statistics. The first order statistics of 
a probability distribution are simply its mean values. Many low-level vision 
algorithms already perform this estimation, either by explicitly using Bayesian 
models, or by using optimization to find the best estimate (the best and mean 
estimates often coincide in these vision problems). The second order statistics, 
which encode the uncertainty or the variance in these estimates, are used much 
less frequently. The application of uncertainty estimation in computer vision 
and robotics has thus far been limited to systems that have a small number of 
parameters, such as the position and orientation of a mobile robot, or the location 
of discrete features. In this book, we will extend uncertainty modeling to dense 
correlated fields, such as the depth or optical flow maps that are commonly used 
in low-level vision. 

2 
Bayesian Modeling of Uncertainty in Low-Level Vision 
In the introduction, we begin by examining what characterizes low-level 
vision problems and why the modeling of uncertainty in this context is important. 
We follow this discussion with a brief survey of previous research in low-level 
vision and other related areas which forms the background for our work. The 
main results contained in this monograph are then summarized, followed by the 
organization of the remainder of this book. 
1.1 
Modeling uncertainty in low-level vision 
Low-level visual processing is often characterized as the extraction of intrinsic 
images (Barrow and Tenenbaum 1978) such as depth, orientation or reflectance 
from the input intensity images (Figure 1.1). A characteristic of these images 
is that they often represent dense fields, i.e., the information is available at all 
points in the two-dimensional visual field. This dense, retinotopic information 
is then segmented and grouped into coherent surfaces, parts and objects by later 
stages of processing. 
Intrinsic images form a useful intermediate representation and facilitate the 
task of higher level processing. Intrinsic characteristics such as depth or re-
flectance are more useful than raw intensities for scene understanding or object 
recognition since they are closer to the true physical characteristics of the scene. 
Intrinsic images provide a more stable description than intensities, one that does 
not change, say, with illumination. These intermediate representations also pro-
vide a framework for integrating information from multiple low-level vision 
modules such as stereo, shading, occluding contours, motion, and texture, and 
for integrating information over time. 
Much of the processing that occurs in these early stages of vision deals with 
the solution of inverse problems (Hom 1977). The physics of image formation 
confounds many different phenomena such as lighting, surface reflectivity, sur-
face geometry, and projective geometry. Low-level visual processing attempts 
to recover some or all of these features from the sampled image array by mak-
ing assumptions about the world being viewed. For example, when solving the 
surface interpolation problem-the determination of a dense depth map from a 
sparse set of depth values-the assumption is made that surfaces vary smoothly 
in depth (except at object or part boundaries). In Chapter 4 we will argue that 
many of these assumptions can be viewed as intrinsic models-probabilistic 
assumptions about the form of likely intrinsic images. 
The inverse problems arising in low-level vision are generally ill-posed (Pog-
gio and Torre 1984), since the data insufficiently constrains the desired solution. 
One approach to overcoming this problem, called regularization (Tikhonov and 
Arsenin 1977), imposes additional weak smoothness constraints in the form of 
stabilizers. Another approach, Bayesian modeling (Gem an and Geman 1984, 

Introduction 
HIGH 
INTERMEDIATE 
Edge detection 
LOW 
Objects, 
parts 
Coordinate 
transfonnation 
Surfaces 
"2 Ih -D Sketch" 
Image(s) 
Figure 1.1: Visual processing hierarchy 
3 
object-centered 
viewer-centered 
eye-centered 
(retinotopic ) 

4 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Szeliski 1986), assumes a prior statistical distribution for the data being esti-
mated, and models the image formation and sensing phenomena as stochastic or 
noisy processes. This latter approach is the one adopted in this book. In Section 
3.2 we will see that regularization is a special case of the more general Bayesian 
approach to the formulation of inverse problems. 
Currently, both regularization and Bayesian modeling techniques are used 
only to determine a single (optimal) estimate of a particular intrinsic image. 
Bayesian modeling, however, can also be used to calculate the uncertainty in 
the computed estimate. This modeling of uncertainty, which is the main subject 
of this book, is important for several reasons. First, the sensors that are used 
in vision applications are inherently noisy and thus the resulting estimates are 
themselves uncertain. Just as with sensor measurement, we must quantify this 
uncertainty if we are to develop robust higher level algorithms. Second, the 
prior models used in low-level vision can be uncertain (because of unknown 
parameters) or inaccurate (due to oversimplification). Third, the data obtained 
from the sensors and subsequent calculations may be insufficient to uniquely 
constrain the solution, or it may require integration with other measurements. 
The Bayesian approach allows us to handle both of these cases, namely under-
determined and overdetermined systems, in a single unified framework. Lastly, 
uncertainty modeling is essential to the development of dynamic estimation al-
gorithms whose accuracy improves over time. These dynamic algorithms can 
then be applied to problems such as the on-line extraction of depth from motion. 
The Bayesian approach to low-level vision has other advantages as well. 
We can use this approach to estimate statistically optimal values for the global 
parameters that control the behavior of our algorithms (Section 6.3). We can 
generate sample elements from our prior distributions to determine if these are 
consistent with our intuitions about the visual world or the class of objects 
being modeled (Section 4.1). We can also integrate probabilistic descriptions 
of our sensors (which can often be obtained by calibration or analysis) into our 
estimation algorithms (Chapter 5). 
The specific Bayesian models that we will develop in this book are based 
on Markov Random Fields, which describe complex probability distributions 
over dense fields in terms of local interactions (Section 3.2). Markov Random 
Fields have several features that make them attractive for low-level vision. The 
MRF description is very compact, requiring the specification of only a few 
local interaction terms, while the resulting behaviors can have infinite range. 
MRFs can also easily encode (and estimate) the location of discontinuities in 
the visual surface. Another attractive characteristic of Markov Random Fields 
is the existence of massively parallel algorithms for computing the most likely 
or mean estimates. As we will see in Section 6.2, these same algorithms can be 
used for computing uncertainty estimates. Using these algorithms, we can thus 
directly exploit the increasing parallelism that is becoming available in image 

Introduction 
5 
processing architectures. 
Unfortunately, iterative parallel algorithms sometimes converge very slowly 
towards the desired estimate. Multiresolution techniques can be used to speed up 
the convergence. These techniques, which operate on a pyramidal representation 
of the field, have previously been applied to deterministic relaxation algorithms 
such as those used in conjunction with regularization. In this book, we will 
extend these techniques to stochastic algorithms, such as those that are used in 
uncertainty estimation. We will also develop algorithms that use a relative rep-
resentation. In such a representation, each level in a multiresolution hierarchy 
encodes the local difference in depth at an appropriate scale, and the sum of all 
the levels represents the absolute depth map. The advantage of this decompo-
sition is that we can develop multiresolution relaxation algorithms that operate 
on all of the levels in parallel, and that we produce a multiscale description of 
the surface. 
To summarize, intrinsic images are a useful intermediate representation for 
disambiguating different physical characteristics that are confounded during im-
age formation and for providing a more stable description for higher levels of 
processing. By modeling the uncertainty associated with these intrinsic maps, 
we can cope with noisy sensors, develop a uniform integration framework, and 
process dynamic data. Using a Bayesian model also allows us to estimate global 
parameters, to study our prior models, and to incorporate probabilistic sensor 
models. The particular Bayesian models that we will use in this book are Markov 
Random Fields combined with multiresolution representations and algorithms. 
Before discussing these ideas in more detail, let us briefly review some of the 
relevant previous work. 
1.2 Previous work 
The research described in this book has been inspired by several different threads 
of research running through the fields of computer vision, artificial intelligence 
and estimation theory. In this section, we briefly mention some of this relevant 
research, tracing the development of ideas in the field. 
First, we review a 
number of general representations, computational theories and algorithms used 
in vision and related fields. Second, we examine some specific low-level vision 
problems and describe how the theories can be applied to their solution. The 
ideas presented in this section are discussed in more detail later in the book, 
particularly in Chapters 2 and 3. 
The formulation of low-level vision as a transformation from input images 
to intermediate representations was first advocated by Barrow and Tenenbaum 
(1978) and Marr (1978). 
Marr (1982) particularly stressed the importance 
of representations in developing theories about vision. Marr's idea of a "2 Ih-

6 
Bayesian Modeling of Uncertainty in Low-Level Vision 
D sketch" was further fonnalized when Terzopoulos (1984) proposed visible 
surface representations as a unifonn framework for interpolating and integrating 
sparse depth and orientation infonnation. More recently, Blake and Zissennan 
(1987) have suggested that discontinuities in the visible surface are the most 
stable and important features in the intennediate level description, a view that 
seems to be echoed by Poggio et al. (1988). These and other representational 
issues are discussed in more detail in Chapter 2. 
The computational theories used in conjunction with these surface repre-
sentations were fonnulated first in tenns of variational principles by Grimson 
(1983) and Terzopoulos (1983), then laterfonnalized using regularization theory 
(Poggio et al. 1985a, Terzopoulos 1986a). Several methods have been proposed 
for discontinuity detection, including continuation (Terzopoulos 1984), Markov 
Random Fields (Marroquin 1984), weak continuity constraints (Blake and Zis-
sennan 1986b), and minimum length encoding (Leclerc 1989). Similar energy-
based models have also been extended to full three-dimensional modeling by 
Terzopoulos et al. (1987). 
The common element in these computational theories is the minimization 
of a global energy function composed of many local energy components. This 
minimization has usually been implemented using iterative algorithms. The ear-
liest cooperative algorithms were applied to the stereo matching problem (Julesz 
1971, Dev 1974, Marr and Poggio 1976). A different class of iterative algorithms 
called relaxation labeling (Waltz 1975, Rosenfeld et al. 1976, Hinton 1977) was 
used to find solutions to symbolic constraint satisfaction problems. The idea of 
constraint propagation for numerical problems was first suggested by Ikeuchi 
and Hom (1981), and has been used in many subsequent low-level vision al-
gorithms (Hom and Schunck 1981, Hildreth 1982, Grimson 1983). Multigrid 
methods (Terzopoulos 1983), which are based on multiple resolution representa-
tions (Rosenfeld 1980), have been used to speed up the convergence of numerical 
relaxation. 
A common problem with relaxation algorithms is that they can find only 
locally optimal solutions, instead of finding the global minimum of the energy 
function. To overcome this, one possible strategy is to use coarse-to-fine match-
ing algorithms (Marr and Poggio 1979, Witkin et al. 1987), which can be based 
on a multiresolution description of the signal (Witkin 1983). Another solution 
is to use a stochastic minimization technique called simulated annealing (Kirk-
patrick et al. 1983, Hinton and Sejnowski 1983). This latter approach can also 
be related to Bayesian modeling through the use of Markov Random Fields 
(Geman and Geman 1984, Marroquin 1985). Detenninistic approximations to 
simulated annealing can also be used (Koch et al. 1986), and these algorithms 
can be implemented using neural networks (Hopfield 1982, Ackley et al. 1985, 
Rumelhart et al. 1986). Continuation methods have also been investigated (Ter-
zopoulos 1988, Blake and Zissennan 1987, Leclerc 1989). An alternative to 

Introduction 
7 
these cooperative algorithms is to use global search (Ohta and Kanade 1985). 
The application of Bayesian modeling to low-level vision has received rela-
tively little attention. Markov Random Field models have been used to charac-
terize piecewise constant images (Geman and Geman 1984) or surfaces (Mar-
roquin 1985). Error models have been developed for stereo matching (Matthies 
and Shafer 1987) and for more abstract sensors (Durrant-Whyte 1987). The use 
of different loss functions to derive alternative optimal posterior estimators has 
been studied by Marroquin (1985). In the domain of real-time processing of dy-
namic data, the Kalman filter has been applied to the tracking of sparse features 
such as points or lines (Faugeras et al. 1986, Rives et al. 1986), and has recently 
been extended to dense fields (Matthies et al. 1987). In this book, we will extend 
the Bayesian approach to low-level vision by analyzing the uncertainty inherent 
in dense estimates, and by developing a number of new algorithms based on the 
Bayesian framework. 
The representations and algorithms just described have been applied to a 
variety of low-level vision problems. Three that are central to this book are 
stereo, motion, and surface interpolation. Additional low-level vision problems 
include shape from contour (Barrow and Tenenbaum 1981, Kanade 1981), shape 
from shading (Ikeuchi and Horn 1981, Woodham 1981), and shape from texture 
(Witkin 1981). While these latter applications are not examined in this book, the 
same uncertainty modeling techniques that are developed here can be applied to 
many of these problems. 
Stereo matching is one of the earliest vision problems to have been studied 
using a computational approach (Barnard and Fischler 1982). Early algorithms 
focused on the correspondence problem (Marr 1982), using random~dot stere-
ograms as their test domain (Julesz 1971, Dev 1974, Marr and Poggio 1976). 
Subsequent algorithms have used zero-crossings (Marr and Poggio 1979, Grim-
son 1981, Kass 1984), edges (Henderson et al. 1979, Baker 1982, Arnold 1983, 
Ohta and Kanade 1985) or image intensities (Lucas 1984, Witkin et al. 1987) 
as the basic matching primitives. A variety of computational characterizations 
of the task have been used, including local support (Prazdny 1985, Szeliski and 
Hinton 1985, Drumheller and Poggio 1986), edge similarity (Arnold 1983, Kass 
1984), segment similarity (Ohta and Kanade 1985) and regularization (Poggio 
et al. 1985a, Witkin et al. 1987). Among the solution methods used are coop-
erative algorithms (Marr and Poggio 1976), coarse-to-fine matching (Grim son 
1981, Witkin et al. 1987), dynamic programming (Ohta and Kanade 1985), 
iterative improvement (Lucas 1984), and simulated annealing (Szeliski 1986, 
Barnard 1986). Despite the great diversity among stereo algorithms, some com-
mon characteristics do exist. Stereo correspondence is usually cast as an energy 
minimization problem, where the energy encodes both the feature compatibilities 
(similarity) and the smoothness or coherence in the estimated depth map. Since 
the energy function is usually non-convex, search techniques such as relaxation, 

8 
Bayesian Modeling of Uncertainty in Low-Level Vision 
coarse-to-fine matching, global search or stochastic optimization must be used. 
For stereo matching, the imaging geometry is usually known, thus reducing this 
search to a one-dimensional problem along epipolar lines in the two images 
(Ohta and Kanade 1985). While the range of target matches is thus restricted, 
the search itself usually cannot be performed independently on each line because 
of the two-dimensional smoothness constraint on the disparity field. 
Extracting depth from motion resembles stereo matching in many ways, 
since similar features, matching criteria and algorithms can be used. Often, 
however, the epipolar lines are not known, and two dimensional flow vectors 
must be estimated. Optical flow can be computed by calculating the ratio of 
spatial and temporal gradients (Horn and Schunck 1981) or using correlation 
(Anandan 1984). Since the motion between frames is usually small, the opti-
cal flow estimates can be quite noisy, and must be smoothed using area-based 
(Horn and Schunck 1981, Anandan and Weiss 1985) or contour-based (Hildreth 
1982) methods. Smoothing is also necessary to overcome the aperture problem 
(Hildreth 1982), although correlation-based flow estimators can avoid this am-
biguity near corners and in textured areas (Anandan 1984). While optical flow 
has usually been measured from successive pairs of images, more recent motion 
algorithms have attempted to use the whole image sequence, by either fitting 
lines to the spatio-temporal data (Bolles and Baker 1985), using spatio-temporal 
filtering (Adelson and Bergen 1985, Heeger 1986), or using Kalman filtering 
(Matthies et al. 1987). In the context of this book, motion processing has three 
interesting characteristics. First, since the motions involved are often small, the 
correspondence problem is reduced. Second, the reliability of the flow infor-
mation varies spatially, and hence this information must be smoothed. Third, 
the time varying nature of the data provides a good application for dynamic 
Bayesian models such as the Kalman filter. 
Surface interpolation is often seen as a post-processing stage that integrates 
the sparse output of independent low-level vision modules (Marr 1982), al-
though it has recently been used in conjunction with other algorithms such as 
stereo (Hoff and Ahuja 1986, Chen and Boult 1988). Surface interpolation was 
first studied in the context of stereo vision (Grim son 1981). An interpolation 
algorithm based on variational principles was developed by Grimson (1983), 
then extended to use multiresolution computation by Terzopoulos (1983), and 
finally reformulated using regularization (Poggio et al. 1985a). Recent research 
has focused on detecting discontinuities in the visible surface, using Markov 
Random Fields (Marroquin 1984), continuation methods (Terzopoulos 1986b), 
and weak continuity constraints (Blake and Zisserman 1986a). Determining the 
optimal amount of smoothing has also been investigated by Boult (1986) (see 
also Section 6.3). In the early parts of this book, surface interpolation will 
be studied as an isolated system. In Chapter 7, we will show how it can be 
integrated with other low-level vision modules. 

Introduction 
9 
As we have seen in this section, visible surface representations, energy-based 
models, cooperative computation, multiresolution algorithms, and Bayesian mod-
eling are all themes that will play an important role in our presentation. In this 
book, we will develop a Bayesian model of visible surfaces based on regular-
ization and Markov Random Fields. This model will allow us to compute the 
uncertainty in the surface estimate and to integrate new data over time. The 
algorithms that we develop will use massively parallel cooperative computation 
and multiresolution techniques. We will apply our new model to surface inter-
polation, and later to motion and stereo. With this background in mind, let us 
tum to the specific new results in this book. 
1.3 
Overview of results 
This book contains both theoretical and experimental results. Its main con-
tribution to computer vision theory is the development of a new probabilistic 
framework for modeling dense fields and their associated uncertainties. This 
framework is used to study the prior models defined by regularization, the sen-
sor models used by low-level vision algorithms, and the posterior estimates 
obtained as the output of such vision algorithms. These models extend the the-
ory of low-level visual processing and are used to develop a number of new 
low-level vision algorithms. The experimental results that we present use both 
synthetic and real scenes and are used to quantify the performance of these new 
algorithms. Additional details about these contributions are presented below. 
The most important result in this book is the development of a Bayesian 
model for the dense fields that are commonly used in low-level vision. This ap-
proach is based on the observation that the estimates obtained with regularization 
and other energy-based minimization algorithms are equivalent to the posterior 
estimates obtained with Markov Random Field modeling. While this viewpoint 
is becoming more commonly accepted (Poggio et al. 1988), probabilistic mod-
eling of low-level vision has so far only been used as a better tool for obtaining 
a single deterministic estimate (Marroquin 1985) 1. The Bayesian framework 
has not previously been used to study the properties of the prior models, to 
better characterize sensor characteristics, or to treat the posterior estimate as a 
probabilistic quantity. 
In developing our Bayesian framework, we start by interpreting regular-
ization as Bayesian estimation, and by interpreting the stabilizer that enforces 
smoothness in the solution as defining probabilistic prior model. While this 
prior model favors smooth surfaces, it does not say how smooth typical sur-
I In the related area of connectionist modeling, however, Hinton and Sejnowski (1983) 
have used estimated co-occurrence statistics to develop a learning algorithm for the Boltzmann 
Machine. 

10 
Bayesian Modeling of Uncertainty in Low-Level Vision 
faces will be. Using Fourier analysis, we re-write the energy of the stabilizer in 
tenns of the power spectrum of the surface. Since the energy of the surface is 
related to its probability through the Gibbs distribution (Section 3.2), we show 
that the prior model is correlated Gaussian noise with a power spectrum defined 
by the choice of stabilizer. For the membrane and the thin plate, the two most 
commonly used regularization models, this spectrum is fractal (Szeliski 1987). 
This result leads us to a new understanding of the role of regularization: the 
choice of a particular stabilizer (degree of smoothness) is equivalent to assuming 
a particular power spectrum for the prior model. 
Our development of the regularization-based probabilistic prior model allows 
us to construct two new algorithms. The first is a computer graphics algorithm 
that uses multigrid stochastic relaxation to generate fractal surfaces (Szeliski 
and Terzopoulos 1989a). These surfaces can be arbitrarily constrained-with 
depth and orientation constraints and depth and orientation discontinuities-and 
thus exhibit a degree of flexibility not present in previous algorithms. Our 
second result is the construction of a relative multiresolution representation. We 
show how the power spectrum of the composite representation can be computed 
by summing the power spectra of the individual levels. This gives us a new 
technique for shaping the frequency response characteristics of each level, and 
for ensuring the desired global smoothing behavior. 
We also apply probabilistic modeling to the sensors used in low-level vision. 
We start by reviewing the equivalence between a point sensor with Gaussian 
noise and a simple spring constraint, and show how to extend this model to 
other one-dimensional uncertainty distributions. We then develop a new sensor 
model that incorporates the full three-dimensional uncertainty in a sparse depth 
measurement. The constraint corresponding to this model acts like a force field, 
and is related to the elastic net model of Durbin and Willshaw (1987). We 
also show how Bayesian modeling can be used to analyze the uncertainty in 
a correlation-based optical flow estimator and to develop an error model for a 
simple imaging system. 
After developing the prior and sensor models, we examine the characteristics 
of the posterior model. We show how the uncertainty in the posterior estimate 
can be calculated from the energy function of the system, and we devise two new 
algorithms to perfonn this computation. The first algorithm uses detenninistic 
relaxation to calculate the uncertainty at each point separately. The second 
algorithm generates typical random samples from the posterior distribution, and 
calculates statistics based on these samples (Figure 1.2). 
The probabilistic description of the posterior estimate allows us to construct 
two new parameter estimation algorithms. The first algorithm estimates the 
optimal amount of smoothing to be used in regularization. This is achieved 
by maximizing the likelihood of the data points that were observed given a 
particular (parameterized) prior model. A similar Bayesian model is used to 

Introduction 
~ 
(a) 
Figure 1.2: Interpolated surface, typical surface, and uncertainty map 
11 
This figure shows pictorially two of the most important ideas in this book. The 
nine data points in (a) are smoothed using regularization to obtain surface (b). 
While (b) shows the estimated mean value of the surface from which the points 
were taken, (c) shows a typical surface from which they might have come (this 
surface is locally fractal). By generating a number of such surfaces, we can 
estimate the local uncertainty in the interpolated solution at each point on the 
surface (d). This uncertainty, which tells us about the local variance in the 
estimate, has been magnified for easier interpretation. Note how the uncertainty 
is higher near the corners where the plate has more wobble and is also high near 
the tear in the surface. 

12 
Bayesian Modeling of Uncertainty in Low-Level Vision 
determine observer or object motion given two or more sets of sparse depth 
measurements. Our new algorithm determines this motion-without using any 
correspondence between the sensed points-by maximizing the likelihood that 
two point sets came from the same smooth surface (Szeliski 1988a). 
The final application of our new theory is the extraction of depth from motion 
sequences. We extend the Bayesian model developed previously to temporal 
sequences using a two-dimensional generalization of the Kalman filter. Careful 
attention is paid to computational issues and to alternative representations, since 
the storage of the full covariance matrix for a dense field is not feasible. We 
review an incremental depth-from-motion algorithm that was developed using 
this framework (Matthies et al. 1987), and point out some of its shortcomings. 
A new algorithm is then developed by jointly modeling the current intensity 
and depth maps as piecewise smooth surfaces. This joint modeling gives the 
algorithm characteristics of both feature-based and area-based methods, and also 
enables a more realistic imaging (sensor) model to be used. Depth-from-motion 
thus serves as an example of the usefulness of the Bayesian estimation theory 
we have developed, and demonstrates the feasibility of its implementation. 
1.4 Organization 
This book contains a mixture of theory and applications. The theory is built up 
incrementally, starting with the representations used for low-level vision and a 
review of the Bayesian modeling framework. This framework is then instanti-
ated by developing prior models, sensor models, and posterior models. Simple 
examples of applications are introduced with each of these three models. The 
Bayesian framework is then extended to a dynamic environment, where it is 
used to develop a new depth-from-motion algorithm. A more detailed overview 
of the structure of the book is as follows: 
Chapter 2. Visible surface representations are introduced as a framework 
for sensor integration and dynamic vision. A discrete implementation' of this 
representation-based on finite element analysis-is presented, and the cooper-
ative solution of regularized problems is explained. Multiresolution representa-
tions are then reviewed, followed by an introduction to relative representations 
and a discussion of hierarchical basis functions. Finally, the role and implemen-
tation of discontinuities is discussed, along with alternative visual representa-
tions. 
Chapter 3. The division of Bayesian models into prior models, sensor 
models, and posterior models is explained in the context of low-level vision. 
Markov Random Fields are introduced, and some basic mathematical results are 
reviewed. The utility of probabilistic and MRF models for low-level vision is 
discussed. 

Introduction 
13 
Chapter 4. The special role of prior models in low-level vision is presented. 
The stabilizers used in regularization are related to prior models through the 
Gibbs distribution. Fourier analysis of the resulting prior is used to show that 
these models are fractal, and to suggest a spectral domain interpretation for 
the choice of stabilizer. The application of these results to the generation of 
constrained fractals is described. The application of the spectral analysis to the 
construction of relative representations is presented. The chapter closes with a 
discussion of mechanical vs. probabilistic models. 
Chapter 5. This chapter examines both sparse measurements, such as those 
obtained from stereo matching or range finders, and dense measurements, as 
available from optical flow or raw intensity images. First, the relationship of 
Gaussian noise to spring models is reviewed. A new sensor model based on three 
dimensional Gaussian noise is then introduced, and its properties are examined. 
An error model for correlation-based optical flow measurements is developed. 
Finally, a simple probabilistic model of an image sensor (CCD camera) is pre-
sented. 
Chapter 6. By combining a prior model and an appropriate sensor model we 
obtain a posterior model of the visible surface. Maximum A Posteriori (MAP) 
estimation is introduced along with alternative optimal estimates. Two new 
methods for calculating the uncertainty in these estimates are then described. 
The probabilistic framework is used to derive a new method for estimating the 
regularization (smoothing) parameter. This same framework is also llsed to 
develop a new algorithm for motion estimation that does not require correspon-
dence between sensed points. 
Chapter 7. The framework developed in Chapter 3 is extended to work with 
temporally varying data using the Kalman filter. An incremental depth-from-
motion algorithm based on this framework (Matthies et al. 1987) is reviewed. 
Motivated by certain shortcomings in this algorithm, a new algorithm is proposed 
that explicitly models the estimated intensity and disparity fields and localizes 
discontinuities to sub-pixel accuracy. 
Chapter 8. The final chapter summarizes the main results in this work and 
discusses open questions and areas of future research. 
Appendices. These contain derivations of mathematical results used in the 
body of the book: (A) a description of the finite element discretization and multi-
grid relaxation algorithm; (B) the Fourier analysis of both the prior model and 
the stochastic posterior estimation algorithms; (C) an error model for correlation-
based optical flow estimation; (D) the equations for the posterior estimate and 
the marginal distribution of the data. 

Chapter 2 
Representations for low-level vision 
Representations play a central role in the study of any visual processing sys-
tem (Marr 1982). The representations and algorithms that describe a visual 
process are a particular instantiation of a general computational theory, and are 
constrained by the hardware that is available for their implementation. Repre-
sentations make certain types of information explicit, while requiring that other 
information be computed when needed. For example, a depth map and an ori-
entation map may represent the same visible surface, but complex computations 
may be required to convert from one representation to the other. The choice 
of representation becomes crucial when the information being represented is 
uncertain (McDermott 1980). 
In this chapter, we will examine representations suitable for modeling visible 
surfaces. In the context of the hierarchy of visual processing (Figure 1.1), these 
representations are actually at the interface between the low and intermediate 
stages of vision. We will first review retinotopic visible surface representations, 
and discuss the difference between continuous and discrete fields. 
We will 
then examine the use of regularization, finite element analysis and relaxation 
for specifying and solving low-level vision problems. This is followed by an 
introduction to multigrid algorithms and relative multiresolution representations. 
The modeling and localization of discontinuities in the surface is then examined. 
Lastly, we mention a number of alternative representations used in low-level 
computer vision. 
2.1 
Visible surface representations 
The visible surface representations that we develop in this book are related to 
Marr's 2 1h. -dimensional (2 11z -D) sketch (Marr 1978) and Barrow and Tenen-
baum's intrinsic images (Barrow and Tenenbaum 1978). The 2 1h-D sketch 
is a retinotopic map that encodes local surface orientation and distance to the 

16 
Bayesian Modeling of Uncertainty in Low-Level Vision 
viewer, as well as discontinuities in the orientation and distance maps. Intrinsic 
images represent scene characteristics such as distance, orientation, reflectance 
and illumination in multiple retinotopic maps. The role of these intermediate 
representations in visual processing is indicated in Figure 2.1. 
Visible surface representations are dense retinotopic maps that can be mod-
eled as functions over a two-dimensional domain aligned with the image. The-
oretically, these fields are continuous, with the domain being a subset of R2. In 
practice, discrete fields are used, with the domain being a subset of Z2. One 
possible method for deriving these dis~rete approximations from the continuous 
representation is finite element analysis, which we examine in the next section. 
Visible surface representations can be used to integrate the output of dif-
ferent vision modules or different sensors (Figure 2.1). They can also be used 
to integrate information from different viewpoints and to fill in or smooth out 
information obtained from low-level processes. One possible technique for per-
forming this integration and interpolation is regularization, which we examine in 
the next section. Another technique is Markov Random Field modeling, which 
we examine in Section 3.2. Both of these approaches are examples of intrinsic 
models (Chapter 4)--locally parameterized models of shape that describe sur-
faces at an intermediate level before their segmentation or grouping into parts 
or objects. 
Before proceeding with a description of these techniques, we should briefly 
discuss the question: "Are visible surface representations necessary?" The early 
work on intermediate representations (Barrow and Tenenbaum 1978, Marr 1978) 
was motivated by a disappointment with feature-based approaches to vision and 
a desire to incorporate computational models of image formation. Some of the 
recent research in computer vision, however, has suggested that image features 
can be grouped and matched directly to a model (Lowe 1985) or to a more 
general parts description (Pentland 1986). 
Psychophysical studies and recent computational modeling suggest that both 
models of visual processing (hierarchical and direct) are present in human vision 
and can be used in computer vision applications. The ability to obtain depth 
perception from random-dot stereograms (Julesz 1971) strongly suggests an inde-
pendent stereo vision module that produces an intermediate depth map. Studies 
in neurophysiology show the existence of multiple visual maps in the cortex 
(Van Essen and Maunsell 1983). These multiple maps may be the structure 
used by intermediate level processes involving visual attention and pre-attentive 
grouping. In this book, we will concentrate on the formation and representation 
of intermediate level maps and ignore for now the problems associated with 
higher levels of visual processing. 

Representations for low-level vision 
Occupancy 
Sonar, 
Laser 
Objects, 
parts 
Image(s) 
Pose or 
motion 
Color image 
Reflectance 
Figure 2.1: A more complex visual processing hierarchy 
17 

18 
Bayesian Modeling of Uncertainty in Low-Level Vision 
2.2 Visible surface algorithms 
Intensity images and visible surface representations define the input and output 
representations for low-level vision processes. To complete the description of a 
low-level vision module, we must define the algorithm that maps between these 
two representations. A number of general techniques have been proposed for 
this task, including constraint propagation (Ikeuchi and Hom 1981), variational 
principles (Grim son 1983), and regularization (Poggio et al. 1985a). In this sec-
tion, we will use regularization since it subsumes most of the previous methods 
and provides a general framework for many low-level vision problems. We will 
then describe how the regularized equations are implemented on a discrete field 
using finite element analysis and how they are solved using iterative relaxation 
algorithms. 
2.2.1 
Regularization 
The inverse problems arising in low-level vision are generally ill-posed (Poggio 
and Torre 1984), i.e., the data insufficiently constrains the desired solution. 
Regularization is a mathematical technique used to solve ill-posed problems 
that imposes weak smoothness constraints on possible solutions (Tikhonov and 
Arsenin 1977). Given a set of data d from which we wish to recover a regularized 
solution u, we define an energy function Ed(u, d) that measures the compatibility 
between the solution and the sampled data. We then add a stabilizing function 
Ep(u) that embodies the desired smoothness constraint and find the solution u* 
that minimizes the total energy 
(2.1) 
The regularization parameter>. controls the amount of smoothing performed. In 
general, the data term d and the solution u can be vectors, discrete fields (two-
dimensional arrays of data such as images or depth maps), or analytic functions 
(in which case the energies are functionals). 
For the surface interpolation problem, the data is usually a sparse set of 
points {dd, and the desired solution is a two-dimensional function u(x,y). The 
data compatibility term can be written as a weighted sum of squares 
(2.2) 
where the confidence Cj is inversely related to the variance of the measurement 
dj, i.e., Cj = aj2. Two examples of possible smoothness functionals (taken from 
Terzopoulos (1984» are the membrane model 
(2.3) 

Representations for low-level vision 
19 
which is a small deflection approximation of the surface area, and the thin plate 
model 
(2.4) 
which is a small deflection approximation of the surface curvature (note that here 
the subscripts indicate partial derivatives). These two models can be combined 
into a single functional 
where p(x,Y) is a rigidity function, and T(X,Y) is a tension function. The rigidity 
and tension functions can be used to allow depth (p(x,Y) = 0) and orientation 
(T(X,Y) = 0) discontinuities. The minimum energy solutions of systems that use 
the above smoothness constraint are "generalized piecewise continuous splines 
under tension" (Terzopoulos 1986b). 
As an example, consider the nine data points shown in Figure 2.2a. The 
regularized solution using a thin plate model is shown in Figure 2.2b. We 
can also introduce a depth discontinuity along the left edge and an orientation 
discontinuity along the right to obtain the solution shown in Figure 2.2c. 
The stabilizer Ep(u) described by (2.5) is an example of the more general 
controlled-continuity constraint 
(2.6) 
where x is the (multidimensional) domain of the function u. A generalized 
version of the data compatibility term is 
(2.7) 
where d(x) and c(x) are now continuous functions. These general formulations 
will be used in Appendix B in order to analyze the filtering behavior of the 
regularized solution. 
Regularization has been applied to a wide variety of low-level vision prob-
lems (Poggio and Torre 1984). In addition to surface interpolation, it has been 
used for shape from shading (Horn and Brooks 1986), stereo matching (Barnard 
1986, Witkin et al. 1987), and optical flow (Anandan and Weiss 1985). Problems 
such as surface interpolation and optical flow smoothing have a quadratic en-
ergy function, and hence have only one local energy minimum. Other problems, 
such as stereo matching, may have many local minima and require different al-
gorithms for finding the optimum solution (Szeliski 1986, Barnard 1986, Witkin 
et al. 1987). 

20 
Bayesian Modeling of Uncertainty in Low-Level Vision 
(a) 
Figure 2.2: Sample data and interpolated surface 
(a) data points (b) interpolated thin plate solution (c) thin plate solution with a 
tear along the left edge and a crease along the right edge 

Representations for low-level vision 
21 
2.2.2 Finite element discretization 
To find the minimum energy solution on a digital or analog computer, It IS 
necessary to discretize the domain of the solution u(x) using a finite number of 
nodal variables. The usual and most flexible approach is to use finite element 
analysis (Terzopoulos 1984). In this work, we will restrict our attention to 
rectangular domains on which a rectangular fine grained mesh has been applied. 
The topology of this mesh is fixed and does not depend on the location of 
the data points. It can thus be used for integrating data from various sensors 
or from various viewpoints. The fine grained nature of the mesh leads to a 
natural implementation on a massively parallel array of processors. This kind 
of massively parallel network is similar to the visual processing architecture of 
the retina and primary visual cortex. 
As an example, let us examine the finite element approximation for the sur-
face interpolation problem. Using a triangular conforming element for the mem-
brane and a non-conforming rectangular element for the thin plate (Terzopoulos 
1984), we can derive the energy equations 
1"" 
2 
2 
Ep(U) = "2 L)(Ui+lJ -
UiJ) + (UiJ+l -
UiJ> ] 
(iJ) 
for the membrane (the subscripts indicate spatial position) and 
_ 1 -2"" 
2 
Ep(U) -"2h L) (Ui+lJ -
2UiJ + Ui-lJ) 
(iJ) 
+ 2(Ui+lJ+l -
UiJ+l -
Ui+lJ + UiJ)2 
+ (UiJ+l - 2UiJ + UiJ_l)2] 
(2.8) 
(2.9) 
for the thin plate, where h = ILlxI = lL1yl is the size of the mesh (isotropic in x 
and y). These equations hold at the interior of the surface. Near border points 
or discontinuities some of the energy terms are dropped or replaced by lower 
continuity terms (see Appendix A). The equation for the data compatibility 
energy is simply 
1"" 
2 
Ed(u, d) = "2 L Ci/UiJ - diJ) 
(iJ) 
(2.10) 
with CiJ = a at points where there is no input data. 
If we concatenate all the nodal variables {UiJ} into one vector u, we can 
write the prior energy model as one quadratic form 
1 
Ep(u) = "luT Apu. 
(2.11) 
This quadratic form is valid for any controlled-continuity stabilizer, though the 
coefficients will differ. The stiffnessl matrix Ap is typically very sparse, but it 
lThis term comes from the finite element analysis of structures. 

22 
Bayesian Modeling of Uncertainty in Low-Level Vision 
is not tightly banded because of the two-dimensional structure of the field. The 
rows of Ap are fields of the same dimensionality and extent as the discretized 
field x and can be described in terms of computational molecules (Terzopoulos 
1988). For the membrane and thin plate, typical molecules are 
1 
2 -8 
2 
and 
h-2 
1 -8 20 -8 1 
2 -8 
2 
1 
The Ap matrix is analogous to the weight matrix of a connectionist network. 
For the data compatibility model we can write 
(2.12) 
where A.J is usually diagonal (for uncorrelated sensor noise) and may contain 
zeros along the diagonal. The resulting overall energy function £(u) is quadratic 
in u 
with 
1 T 
T 
£(u) = -u Au - u b + c 
2 
A = Ap + Ad and b = A.Jd. 
The energy function has a minimum at 
and can thus be re-written as 
£(u) = ~(u - u*l A(u - u*) + k. 
2.2.3 Relaxation 
(2.13) 
(2.14) 
(2.15) 
(2.16) 
Once the parameters of the energy function have been determined, we can cal-
culate the minimum energy solution u* using relaxation. This approach has 
two advantages over direct methods such as Gaussian elimination or triangular 
decomposition. First, direct methods do not preserve the sparseness of the A 
matrix, and thus require more than just a small amount of storage per node. 
Second, relaxation methods can be implemented on massively parallel locally 
connected computer architectures (or even on analog networks (Koch et al. 

Representations for low-level vision 
23 
1986». A number of relaxation methods such as Jacobi, Gauss-Seidel, succes-
sive overrelaxation (SOR), and conjugate gradient have been used for visible 
surface interpolation2• 
For our simulations on a sequential machine, we use Gauss-Seidel relaxation 
where nodes are updated one at a time. This method is simple to implement, 
converges faster than the parallel Jacobi method, and can easily be converted to 
a stochastic version known as the Gibbs Sampler (see Section 4.2). At each step, 
a selected node is set to the value that locally minimizes the energy function. 
The energy function for node Uj (with all other nodes fixed) is 
(2.17) 
where the subscripts i andj are actually two-element vectors that index the image 
position. The node value that minimizes this energy is therefore 
(2.18) 
Note that it is possible to use a parallel version of Gauss-Seidel relaxation so 
long as nodes that are dependent (have a non-zero aij entry) are not updated si-
multaneously. This parallel version can be implemented on a mesh of processors 
for greater computational speed. 
The result of applying this iterative algorithm on the nine data points of 
Figure 2.2a are shown in Figure 2.3 for 10, 100 and 1000 iterations. As can 
be seen, this relaxation algorithm converges very slowly towards the optimal 
solution. This slow convergence may not be a problem in a dynamic system 
(Chapter 7) where iteration can proceed in parallel with data acquisition a:ld the 
system can converge to an adequate solution over the course of time3• However, 
for one-shot interpolation problems, the convergence speed may be more critical, 
and the multigrid techniques described in the next section may be required. 
2.3 
Multiresolution representations 
Multiresolution pyramids are representations that can be used for efficiently solv-
ing many image processing tasks (Rosenfeld 1984). Pyramids have also been 
used for generating multiscale descriptions of signals such as intensity images 
2Terzopoulos (1984) presents a brief survey of relaxation methods. Gauss-Seidel is used 
by Terzopoulos (1984), while conjugate gradient descent is used by Choi (1987), and SOR by 
Blake and Zisserman (1987). 
30nce the system has latched-on to a good solution, it can then track changes in the scene 
using only a few iterations to correct its estimate. Similar arguments have recently been advanced 
in support of dynamic deformable models by Terwpoulos (1987) and Kass et al. (1988). 

24 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Figure 2.3: Single level relaxation algorithm solutions 
(a) after 10 iteration (b) 100 iterations (c) 1000 iterations 

Representations for low-level vision 
25 
(Burt and Adelson 1983, Crowley and Stem 1982, Mallat 1987). The use of 
multiresolution representations and multigrid algorithms for visible surface in-
terpolation was first extensively studied by Terzopoulos (1984). In this section, 
we will review these multigrid algorithms and then introduce relative represen-
tations as an alternative to regular multi scale descriptions. The full development 
of these relative representations will have to wait until Chapter 4 where we 
present a probabilistic interpretation of surface representations. We will also 
present hierarchical basis functions as an alternative to multigrid relaxation. 
2.3.1 
Multigrid algorithms 
Multigrid relaxation algorithms (Hackbusch and Trottenberg 1982, Hackbusch 
1985) are based on the observation that local iterative methods are good at 
reducing the high frequency components of the interpolation error, but are poor 
at reducing the low frequency components. By solving the same problem on a 
coarser grid, this low frequency error can be reduced more quickly. Multigrid 
algorithms thus operate on a hierarchy of resolution levels as shown in Figure 
2.4. 
To develop a multigrid algorithm, several components must be specified: 
• the number of levels and the size of the grid at each level 
• the method used to derive the energy equations at the coarser levels from 
the fine level equations 
• a restriction operation that maps a solution at a fine level to a coarser grid 
• a prolongation operation that maps from the coarse to the fine level 
• a coordination scheme that specifies the number of iterations at each level 
and the sequence of prolongations and restrictions. 
A sophisticated version of multigrid relaxation was developed by Terzopou-
los (1984). In his implementation, the coarser meshes are coincident with the 
original mesh, Le., the coarser level nodes are a subset of the finer level nodes. 
Simple injection (subsampling) is used for restriction, and third-degree Lagrange 
interpolation is used for prolongation. A Full Multigrid (FM) method is used to 
coordinate the inter-level interactions. This ensures that the coarse levels have 
the same accuracy as the fine level solutions. 
In this work, we will use a simpler version, since optimal relaxation algo-
rithms and convergence rates are not our primary concern. Averaging is used for 
restriction and bilinear interpolation for prolongation. The coordination scheme 
is a simple coarse-to-fine algorithm, where the prolongated coarse solution is 
used as a starting point for the next finer level. Figure 2.5 shows the regularized 

26 
coarse 
medium 
fine 
I 
I 
I 
I 
I 
I 
I 
I 
I 
b 
Bayesian Modeling of Uncertainty in Low-Level Vision 
!r-. 
II \ \ 
I I \ \ 
I I 
\ 
\ 
I~ fi /\ 
j/ /I( \ 
I 
\ 
I 
I 
\ 
\ 
I 
I 
I ~~~~--~~ 
I 
I 
I 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
I 
~~~~--~~~~~~~--~ 
b 
Figure 2.4: Multiresolution pyramid 

Representations for low-level vision 
27 
solution at the coarse, medium and fine resolution levels for the example that 
was introduced previously. 
To derive the energy functions at the coarser level, the discontinuities and 
data points are first mapped between levels using simple block averaging. The 
new energy equations are then derived in the same fashion as for the finest level. 
Appendix A gives the details of the multigrid algorithm which we have adopted, 
and Appendix B contains a Fourier analysis of its convergence properties. 
2.3.2 Relative representations 
The multigrid techniques presented in the previous section keep a full copy 
of the current depth estimate at each level4• Relaxation is applied to each 
level separately, and projection operators are used to map between levels. This 
computational framework thus fails to exploit the full parallelism inherent in a 
pyramidal representation. Some attempts have been made to implement fully 
parallel relaxation algorithms using this multigrid representation (Terzopoulos 
1985), but they have met with limited success. 
An alternative to the absolute multiresolution representation just described 
is a relative representation. In this representation, each level encodes details 
pertinent to its own scale, and the sum of all the levels provides the current 
depth estimate. The relative representation is thus analogous to a band-pass 
image pyramid (Burt and Adelson 1983, Crowley and Stem 1982, Mallat 1987), 
while the absolute representation is similar to a low-pass pyramid. In contrast 
to these band-pass representations, the relative representation which we develop 
here is not obtained by repeated filtering of a fine-resolution map. Instead, 
each level in the pyramid has its own associated prior energy (weak smoothness 
constraint). The data compatibility is measured in terms of the distance between 
the summed depth map and the data points. The interpolated solution is then 
obtained by finding the minimum energy configuration of the whole system. We 
will present a more detailed description of this approach later in this section. 
The relative multiresolution representation offers several possible advantages 
over the usual absolute representation. Fully parallel relaxation can be used 
with this representation, and yields a multiscale decomposition of the visible 
surface. Discontinuities can be assigned to just one level, thus permitting a 
better description of the scene. For example, in a blocks world scene (an indoors 
robotics environment with several objects lying on a table), the fine level may 
describe the shape of an individual object, while the coarse level may describe 
the table elevation. The use of relative representations can also increase the 
descriptive power of a method when uncertainty is being modeled (McDermott 
1980). For example, knowing that a particular block sitting on a table is 1O±0.9 
4In the multigrid literature, this is called the Full Approximation (FA) scheme. 

28 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Figure 2.5: Coarse-to-fine multiresolution relaxation solution 
(a) coarse level (b) medium level (c) fine level 

Representations for low-level vision 
...... : .::~"",,::~t.-:,: ::-~:. -,:1.-:,.':, 
: ... ~ "'.~ .1:'. I. :. .. "':" -I.:: 
.:_:. ,.1.".-,'. ",I:,., ";' :-.!:_. I .. , .~!.: 
··;;'·'~:·i :~!:"-:'····:'·':"··"·::~'·-':·i' 
".t:,:.,.:: :: ... ·:~· .... ·!I:·.;-·'.: 
• 
• , 
I •••• I,' "1 :-t ," ••• '1 ,I, fI'.·. 
:":'.": :.-.:. :.',,-,,: .. ::..~., :'.:.;:' :.. 
.' ".:~. ': .. ' ~ ......... ' ', .. 
:~ . 
.. • ~ •• ' ••• I •••• I. ~ .It',-••• , 
..... 
".#.~ ........ ' .......... . 
.·:-.·:':·~r ..... ;~ ... \, .... ~ ..... I 
.,. ,. ,I •• 
-:- II • III ,1,' •• 1. ~ ~ •• "' ••• ~ .. 
I 
•• -:., .. : ...... " 
: ....... ' : .............. ' 
•• ~.,' 'I, II", ,I, 'I:.~"'" I, ....... :,' 
','::':::-=- .\~."·"~·':'.-';.I ~ ..... a.~ 
. .: II ,::.:..,.1':.". ... :.: ......... ! .. 
.... , .,:' .... ), \ ....... II::,.", . 
...• , ~ .. :.:- '::.': '.:' ,. , ... " 
~ 
' 
•
••• IS 
, 
... , 
.... ~. .. 
~. 'I. • 
.~~",.~ .-: .",' ''':.- ..... ~! ...... .,.rl}. 
....... .. :." f.·.· .... · .,'" ... . 
_ 
..... : .::~', ",.':1 ;,,':': ::-~.:. '. : ... ~ •. :, 
:::.\.~ :.:\ ... ~.: •.. ; ... :. :.'".-:i·j .-;.:: 
,:::,~,::,':",:.:.I •. • ':: ... :-.·.~.-:I :.~:. 
...... : .. ~ ..... :. ... ~! ... ., ··.,·~ .... ~ ... ·I. 
" .c •• ) .... ' rI' •• " 
..... I.i·. .. .. 
' .. :'~ '.:.:.".:t, ,::'.! ..... :.:'-:.:. 
:".1 IU .. ' ". :..' ."" ' .... \'1 •••• , •• ,., 
•• ".:~ • ..... -1..' , .. ~ ... • ' ' ..... :. • 
"'. ~ •... '.: ... ' .. 
:~ •. : .. ~ ~~'=-:: . .••.. 
'.- ··;··~·r·114·~I' .. " .. . , ...• ' 
., ••• ,:.-. ':. '. • ••• : 'I' \I' ••••• ", 
~ .'::, ..... :: ••• ~.":) ..... •• ::~: ••• '!': ..... : ... 
.• :l',. ".':": ...•. ·I:.~ .. ~·. "'1':,' 
~·I'::·.·~:- .':-.·,"'."':'.\.-':,1 ~ .. "" ~.~ 
" .. "', .. : .. :.., .. 1':·, __ · .. ; ... · ...... ··.1.· 
r'l .\ .1:' .... ". ) ... ..... '1:-: .... , ' . 
..... ' .-:.:- • : •• 'J • ' .. ~,. I" .. ~ 
' ... II·. , ....... "' ... -c •• .r. 
,~ • • f. - •. ",. ''':.-. ~.:: .. .... : ...... .,~. 
:'.:-.... ·.::·1' ! ........ IfiI.\.·; .... ' •• 
Figure 2.6: Random-dot stereogram showing Com sweet illusion 
\'----
true disparity profile 
perceived disparity profile 
Figure 2.7: Disparity profiles for random-dot stereogram 
29 
cm tall is more precise than knowing that the block top is 80 ± 4.1 cm above 
the floor and that the table top is 70 ± 4 cm above the floo~. 
A relative representation also permits us to incorporate relative depth mea-
surements. An example of such a measurement is the disparity gradients avail-
able from stereopsis when the vergence angle is not known. A similar effect 
is apparent in the emergence of depth perception from random-dot stereograms. 
Figure 2.6, which was suggested by Geoffrey Hinton, shows a stereo pair whose 
disparity profile is the same as that used in the Corn sweet illusion (see also 
Anstis and Howard (1978)). Using a stereo viewer or by cross-eyed viewing of 
the stereogram, the reader may elicit a perception of a surface in depth. The first 
and strongest perception is that of a step edge, which is due to the large dispar-
ity gradient in the center of the image (Figure 2.7). This illusion also suggests 
that multiresolution channels may be responsible for disparity processing. These 
channels would feed naturally into a relative multiresolution representation. The 
exact details of this mechanism remain to be worked out. 
To explore the possibility of using a relative representation, we will write 
down the equations for this approach and try a simple example. Using UI to 
represent the depth map at level I, we can write the overall depth map as a sum 
SFor this example, we add variances instead of standard deviations to obtain the uncertainties. 

30 
Bayesian Modeling of Uncertainty in Low-Level Vision 
of the individual level maps 
L 
U = LI/UI 
1=1 
where II is the interpolation function associated with each level (note that the UI 
get smaller as we go up the pyramid). Each level has its own associated prior 
(smoothness) energy 
(2.19) 
The data compatibility energy is defined using the summed representation u, 
1 
T 
Ed(u, d) = 2(u - d) Ad(u - d). 
(2.20) 
Using ii = [uf ... uIf to denote the concatenation of the individual state vectors, 
i = [11 ... Id to denote the concatenated interpolation matrices, and 
[ 
A~ 0 
.. . 
o A2 .. . 
-
p 
Ap = 
: 
: 
o 0 1] 
to denote the composite prior energy matrix, we can write the overall energy 
function as 
L 
E(ii) = Ed(ii, d) + L E~(U/) 
1=1 
1 (1--
)TA (1--
) 
L TA--
= 2 
U - d 
d U - d + 2u 
pU 
L TA- -
-Tb-
= -u 
U -
U 
+c 
2 
(2.21) 
where 
(2.22) 
The minimum energy solution can be calculated from this quadratic form as 
(2.23) 
Designing a set of spline energies for each level that decompose U into a 
reasonable multiresolution description and also have a desired global smoothing 
behavior is nontrivial. If we try something simple, such as setting A~ at each 
level to the same value as would be used in the regular multigrid algorithm (e.g., 
those values specified in Appendix A), we find that the matrix A is singular which 
precludes a unique relative representation solution. This occurs because E~(u/) 

Representations for low-level vision 
31 
is invariant with respect to additive constants, so that adding a constant value 
to one level and subtracting it from another affects neither the solution energy 
nor the summed solution (assuming that the interpolators reproduce constant 
functions). 
To overcome this problem, we can add a small quadratic energy term to each 
level, i.e., increment A~ by €I, where I is the unit matrix. Since we also want 
the larger variations in depth to be encoded by the coarser level, we reduce the 
magnitude of the energy as we go up the pyramid by setting 
A!, = s-/(A~ + f), with s> 1, 
where ~ is derived using the usual finite element analysis (Appendix A). Figure 
2.8 shows the three level relative decomposition and the summed solution using 
this approach (with s = 4). 
The summed depth map shown in Figure 2.8d obviously differs from the 
absolute solution shown in Figure 2.2b. One reason for this is that bilinear 
interpolation used in Figure 2.8 is not sufficiently smooth to be used with a 
thin plate. More generally, the effective smoothing behavior of the relative 
representation is different than that of the absolute representation. 
To compute the exact smoothing behavior, we examine the minimum energy 
solution written in terms of the prior energy matrices ~, the data energy matrix 
Ad, and the data points d. From (2.23), we see that the overall summed depth 
solution is 
(2.24) 
For this to be equivalent to u* = (Ap + Ad)-l Add, the minimum of the original 
energy equation (2.13), we must have 
i(iAdi + Ap)-l F = (Ap + Ad)-l. 
Using the matrix inversion lemma (Appendix D) we can re-write this as 
i(A;"1 - A;liT(iA;"IF + A;Jl)-liA;I)iT = A;l - A;1(A;1 + A;Jl)-1 A;l 
which is satisfied if 
L 
A;l = iA;ljT = L I/(A!,)-II;' 
(2.25) 
1=1 
We thus have a method for computing the global smoothing behavior of the 
relative representation in terms of the smoothing energy at each level and the 
interpolation functions6• If we can satisfy this condition, we can obtain a mul-
tiresolution description of our depth map while performing parallel relaxation, 
without changing the global smoothing properties of our interpolator. 
6Equation (2.25) can also be derived by assuming that each level is a correlated Gaussian 
with covariance (A~)-l and calculating the covariance of the summed surface (see Section 4.3). 

32 
Bayesian Modeling of Uncertainty in Low-Level Vision 
(c) 
Figure 2.8: Relative multiresolution decomposition 
(a) coarse level (b) medium level (c) fine level (d) summed solution 

Representations for low-level vision 
33 
In practice, choosing a set of A~ that satisfy (2.25) for a given Ap (say a 
thin plate) and perform an adequate multi scale decomposition is difficult because 
while the A~ and Ap matrices are very sparse, their inverses are not (or they 
may not even exist). As we will see in Section 4.3, however, the smoothness 
constraints used in regularization are equivalent to assuming a particular spectral 
distribution for the prior model. This will allow us to re-cast the problem of 
satisfying (2.25) in terms of shaping the spectrum of each individual level such 
that the summed spectrum has the desired shape. We will therefore defer the 
full development of the relative representation until Section 4.3, where we can 
examine its construction using some new tools. 
2.3.3 Hierarchical basis functions 
The relative representation which we have developed is similar in structure to 
the hierarchical basis functions developed recently by Yserentant (1986). In 
this approach, the usual nodal basis set u is replaced by a hierarchical basis set 
v. Certain elements of the hierarchical basis set have larger support than the 
nodal basis elements, and this allows the relaxation algorithm to converge more 
quickly when using the hierarchical set. 
To convert from the hierarchical to the nodal basis set, we use a simple linear 
(matrix) transform 
u = Sv where S = S,S2 ... SL_' 
(2.26) 
and L is the number of levels in the hierarchical basis set. Each of the sparse 
matrices S, interpolates the nodes at level I + 1 to level I and adds in the nodes 
corresponding to the new level. The columns of S give the values of the hier-
archical basis functions at the nodal variable locations. 
In his paper, Y serentant uses recursive subdivision of triangles to obtain the 
nodal basis set. The corresponding hierarchical basis then consists of the top-
level (coarse) triangularization, along with the subtriangles that are generated 
each time a larger triangle is subdivided. Linear interpolation is used on a 
triangle each time it is subdivided. We can generalize this notion to arbitrary 
interpolants defined over a rectangular grid (Szeliski 1989). Each node in the 
hierarchical basis is assigned to the level in the multiresolution pyramid where 
it first appears (Figure 2.9). This is similar to the relative representation, except 
that each level is only partially populated, and the total number of nodes is 
the same in both the nodal and hierarchical basis sets. To fully define the 
hierarchical basis set, we select an interpolation function that defines how each 
level is interpolated to the next finer level before the new node values are added 
in. 
The resulting algorithms for mapping between the hierarchical and nodal 
basis sets are simple and efficient. We use 

34 
Bayesian Modeling of Uncertainty in Low-Level Vision 
coarse 
medium 
fine 
/ 
/ 
/ 
/ 
/ 
/ 
/ 
/ 
/ 
t 
/ 
/ 
~ 
//\ \ 
/ / \ \ 
/ / 
\ 
\ 
~.rY' 
Fr?J( \\ 
/ 
/ 
\ 
\ 
/ 
r-~~~~~--~ 
/ 
/ 
/ 
\ 
\ 
\ 
/ 
r-~~~--~~~ 
__ --~~~-y 
t 
Figure 2.9: Hierachical basis pyramid 
\ 
\ 
\ 
\ 
The circles indicate the nodes in the hierarchical basis. 
\ 
\ 
\ 

Representations for low-level vision 
procedure S 
for I = L - 1 down to 1 
for i EM/ 
for j E N; 
u(i) = u(i) + w(i;j)u(j) 
end S 
35 
to convert from the hierarchical to the nodal basis set. In this procedure, which 
goes from the coarsest level (I = L) to the finest (I = 1), each node is assigned 
to one of the level collections M/. Each node also has a number of neighbor-
ing nodes N; on the next coarser level that contribute to its value during the 
interpolation process. The w(i;j) are the weighting functions that depend on the 
particular choice of interpolation function (these are the off-diagonal tenns in 
the S/ matrices). 
We will also use the adjoint of this operation 
procedure ST 
for I = 1 to L - 1 
for i EM/ 
for j E N; 
u(;) = u(;) + w(i;j)u(i) 
end ST 
in the conjugate gradient descent algorithm which we develop in the next section. 
An example of the hierarchical basis representation for the surface previously 
presented in Figure 2.2b is shown in Figure 2.10. In this representation, nodes 
that are coincident with higher level nodes have a 0 value. 
These mapping algorithms are easy to code (once the interpolation functions 
have been precomputed) and require very few computations to perfonn. On a 
serial machine, these procedures use O(n) operations (multiplications and addi-
tions), where n is the number of nodes. On a parallel machine, O(L) parallel 
steps are required, where L ::::; t log n is the number of levels in the pyramid. 
This is the same number of steps as is needed to perfonn the global summations 
(inner products) used in the conjugate gradient algorithm. Note that although 
we have defined the hierarchical basis over a pyramid, it can actually be rep-
resented in the same space as the usual nodal basis, and the transfonnations 
between bases can be accomplished in place. 
The hierarchical basis set allows us to minimize exactly the same energy 
as the one we obtained from the discretization on the finest grid. Substituting 
u = Sv into (2.13), we obtain the new energy equation 
1 
E(v) = _VT(ST AS)v - VT(STb) + c 
2 
1 T A 
TA 
= -v Av - v b + c 
(2.27) 
2 

36 
Bayesian Modeling of Uncertainty in Low-Level Vision 
~ 
xx> 
x 
'<..7 
Figure 2.10: Hierarchical basis representation of solution 

Representations for low-level vision 
37 
where the A identifies the hierarchical basis vectors and matrices. The advantage 
of minimizing this new equation is that the condition number of the matrix A 
is much smaller than that of the original matrix A (Yserentant 1986), implying 
much faster convergence for iterative algorithms such as conjugate gradient. 
Unfortunately, the A matrix is not as sparse as the original matrix A, so 
that a direct minimization of (2.27) is impractical. Instead, we use the recursive 
mappings Sand ST in conjunction with the original matrix A and vector b 
to compute the required residuals and inner products. The resulting conjugate 
gradient descent algorithm is Identical to the usual single-level algorithm except 
that we use a smoothed version of the residual f = SST r to choose the new 
direction (Szeliski 1989). We describe the development of this algorithm below. 
Conjugate gradient descent is a numerical optimization technique closely 
related to steepest descent algorithms (Press et al. 1986). At each step k, a 
direction Pk is selected in the state space, and an optimal sized step is taken in 
this direction. In steepest descent, the direction is always equal to the current 
gradient of the function being minimized. In conjugate gradient descent, we 
modify this direction so that successive directions are conjugate with respect to 
A, i.e., Pk+IApk = O. Figure 2.11 shows the result of applying conjugate gradient 
descent to the surface interpolation problem introduced in Figure 2.2. 
A description of the usual (nodal basis) conjugate gradient descent is shown 
in the left column of Figure 2.12. Having selected a direction Ph we choose the 
optimal step size Qk so as to minimize .::1E(Uk + QkPk). This involves computing 
the product of the sparse matrix A and the Ph and the inner product of the 
resulting vector Wk and Pk. On a fine-grained parallel architecture, the matrix 
operation is computable in constant time (dependent on the size of the neigh-
borhoods or molecules in A) and the inner product summation is computable in 
log n steps using a summing pyramid. After updating the new state, we compute 
the new residual rk+1 and find the value of f3k+1 which will make the new and 
old directions conjugate. 
For a quadratic energy equation such as (2.13) or (2.27), the conjugate gra-
dient algorithm is guaranteed to converge to the correct solution in n steps, in 
the absence of roundoff error. As we mentioned in the previous section, how-
ever, we can obtain much faster convergence to an approximate solution if we 
minimize (2.27) instead of (2.13). The resulting algorithm is shown in the right 
column of Figure 2.12. In this algorithm, we update the state of the hierar-
chical basis vector Vk by computing the residual vector fk and direction vector 
Pk. To implement the matrix multiplications APk and AVh we use the mapping 
operations Sand ST before and after the matrix product with the original sparse 
matrix A. In the process, we convert the quantities Pk and Vk into the nodal 
representations Pk and Uk. The overall algorithm thus uses two calls to Sand 
two calls to ST to compute the required quantities for the conjugate gradient 
descent. 

38 
Bayesian Modeling of Uncertainty in Low-Level Vision 
(a) 
Figure 2.11: Conjugate gradient relaxation example 
after (a) 1 iteration (b) 10 iterations (c) 100 iterations 

Representations for low-level vision 
39 
I Nodal basis conjugate gradient I I Hierarchical basis conjugate gradient I 
O. 
initialize ro using 6. 
O. 
initialize 1'0 using 6a. and 6b. 
and set Po = ro 
and set Po = 1'0 
la. 
Pic = SPIe 
1. 
Wle = Aple 
lb. 
Wlc = Aple = ASpic 
Ie. 
Wle = ST WA: = (ST AS)Pk 
2. 
oP 
A: = Pk . W/c = prApk 
2. 
ar = Pic . Wk = Pk . Wk 
3. 
0{ = Pk' rk = prrA: 
3. 
0{ = PA: . 1'A: = Pie . rk 
4·t 
aA: = o{/af 
4. 
aA: = o{/ar 
Sa. 
VA:+I = VA: + a.tPk 
S. 
Uk+1 = Uk + aicPA: 
Sb. 
Uk+1 = SVA:+I = Uk + aicPk 
6. 
rhl = b - AUhl 
6a. 
rk+1 = b - AUA:+I 
6b. 
1'k+1 
ST rk+1 = STb - (ST AS)Vk+1 
6e. 
'A:+I = S'hl 
7. 
f3'tl = rhl . WA: = rr+IApA: 7. 
f3'tl = 'hI' WA: = 'hI' Wk 
8. 
f3A:+1 = f3'tl/ar 
8. 
f3hl = f3'tl / af 
9. 
PhI = rhl - f3A:+IPk 
9a. 
P.t+1 = 1'hl - f3A:+IPk 
9b. PhI = SP.t = 'k+1 - f3hlPA: 
lO. loop to 1. 
lO. loop to lb. 
t 
t1E(u + ap) = ~pTAp - apTr 
Figure 2.12: Algorithms for nodal and hierarchical conjugate gradient descent 

40 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Inspection of the algorithm shown in Figure 2.12 shows that it can be sim-
plified to reduce the number of mappings required. We note that in steps 2 and 
3 the quantities elf and o:f (the numerator and denominator of O:k) can be com-
puted just as easily using the regular nodal basis representation. Upon reflection, 
this result is not surprising, since once the direction Pk (or equivalently Pk) has 
been selected, the size of the step must be the same independent of which repre-
sentation is used Similarly, in step 7 we can replace the inner product fhl . Whl 
with the inner product rk+1 • Wk+l, where rk+1 = SSTrk+1 is the smoothed residual 
vector. If we now use step 5b instead of 5a and 9b instead of 9a and la, we 
obtain an algorithm that is nearly identical to the original conjugate gradient 
algorithm. The only difference is that we now smooth the residual vector rhl 
using a sweep up (ST) and then back down (S) the pyramid to obtain the vector 
rk+l. This smoothed vector dictates the new direction. 
To evaluate the performance of our new algorithm, we ran a number of 
experiments on synthetic data sets. The set of points used in each run is the one 
shown in Figure 2.2a. For each of the three models tested (membrane, thin plate, 
and controlled-continuity thin plate), the optimal solution u* for the 33 x 33 grid 
was computed by using 2000 iterations of conjugate gradient descent. For each 
experiment (defined by a suitable choice of interpolator and number of levels), 
the root mean squared (RMS) error 
was plotted as a function of the number of iterations k. 
Figure 2.13 shows the state of the hierarchical conjugate gradient algorithm 
(L = 4) after 1, to, and 100 iterations. A dramatic speedup over both Gauss-
Seidel relaxation (Figure 2.3) and conjugate gradient (Figure 2.11) is evident. 
Figure 2.14 shows the effects of varying the number of smoothing levels in 
the pyramid (L) on the convergence rate. The topmost curve (L = 1) is the 
convergence rate of the usual nodal basis conjugate gradient descent algorithm. 
The surprising result is that the fastest convergence is obtained when L = 4 
and L = 5, instead of L = 6 (the full pyramid). Initially, the larger number of 
smoothing levels used leads to a faster convergence. As time goes on, however, 
the large number of levels tends to over-smooth the residual vector. The optimum 
number of levels to be used seems to be related to the density of the underlying 
data points, but this remains to be verified empirically. Another possibility, 
which remains to be investigated, is to adjust the number of levels adaptively 
during the relaxation. 
Figure 2.15 shows the effects of using different interpolators on the con-
vergence rate (for this plot, the best value of L, usually 4 or 5, was used). 
The bilinear interpolator and bilinear interpolator with discontinuities seem to 
work the best. The convergence rates for the thin plate without discontinuities 

Representations for low-level vision 
Figure 2.13: Hierarchical conjugate gradient (L = 4) relaxation example 
after (a) 1 iteration (b) 10 iterations (c) 100 iterations 
41 

42 
Q)162 
g 
(1) 
§ 
0.1000 
0.0316 
0.0100 
0.0032 
0.0010 
Bayesian Modeling of Uncertainty in Low-Level Vision 
,'. , ". 
, '. 
" , -. 
\", 
'" 
--------------
\ '.' 
..... -------
" 
\ 
' ..... ,..... 
.. ..... . 
.. ',', 
..... ..... 
.. .... 
\... 
- ............. 
\., 
\ 
..... 
"--'-,-
...... 
. . . . 
" 
' .. till' •
.... "",,_. __ 
, 
............. 
, 
, 
'.. 
.....-..... , 
., 
'. 
.. ..... , 
.. 
... .... 
" .... 
L=l 
... , 
... 
.... \ 
" " 
.~ 
~, 
, , , 
. 
" 
" 
" '" 
........ ", '. ' ......... 
.. , \ 
...... 
,,'. 
" 
~....... 
, 
'\ ' ..... , 
, 
, .-.'. , 
-.. 
L=2 
L=3 
.. 
.....'--., 
..... - .... 
, 
',,-
", 
~: ..... 
... 
L=4 
L=5 
... 
L=6 
... .. 
, . 
0.0003 +--+-----4--+--+--+-~f___+-_+_-+_____f 
o 
20 
40 
60 
80 
100 120 140 160 180 200 
iterations 
Figure 2.14: Algorithm convergence as a function of L 
Controlled-continuity thin plate. bilinear interpolator. 

Representations for low-level vision 
Q.3162 
g 
II) 
'" 
8.1000 
0.0316 
0.0100 
0.0032 
0.0010 
0.0003 
~ ... , .... ..... 
':"'~".. ...... .... . 
..... 
"':' ~ ..... :. 
.... "'". 
... . 
linear 
.. ... 
.......... ,. 
~ 
".~" " 
.. "." 
...... :\,. 
'\ 
~ 
... \' , 
bilinear 
"'~"'" 
bilinear with discontinuities <.... :: 
, 
..... 
biquadratic 
" 
bicubic 
... , 
... , 
... ... ... 
. . 
, , 
... 
... , 
0.0001 +-_+-_I--~_-+_-+-_-+-_+-_I--~_-I 
o 
20 
40 
60 
80 
100 
120 
140 
160 
180 200 
iterations 
Figure 2.15: Algorithm convergence as a function of interpolator 
Controlled-continuity thin plate, L = 4 or 5. 
43 

44 
Bayesian Modeling of Uncertainty in Low-Level Vision 
and continuous membrane are even faster. Compared to coarse-to-fine Gauss-
Seidel relaxation, the hierarchical conjugate gradient algorithm is much faster. A 
comparison with full-multigrid (Hackbusch 1985) is currently being performed 
(Szeliski and Terzopoulos 1989b). 
From the experiments described here, we see that the hierarchical basis con-
jugate gradient algorithm is dramatically faster than single-resolution conjugate 
gradient (which is itself much faster than Gauss-Seidel). Similar speedups are 
available using multigrid techniques (Hackbusch 1985). The biggest advantage 
of this new approach over multi grid techniques is the ease of implementation 
and its suitability for massively parallel architectures. 
When designing a multigrid algorithm, we must first devise a hierarchy of 
problems, i.e., for each level, we must re-derive the finite element equations (this 
often involves averaging data from the finer level). We also have to specify both 
the injection (subsampling) and prolongation (interpolation) operations, as well 
as choose an inter-level coordination scheme. With hierarchical basis functions, 
only a single interpolation function needs to be specified. There is no need to 
explicitly build a pyramid for representation or computation (this is also true 
for some multigrid techniques). Most of the computation proceeds in parallel at 
the fine level, with only occasional excursions up or down the virtual pyramid 
for summing or smoothing. Since we can choose the interpolation function 
(hierarchical basis) independent of the problem being solved, we have much 
greater flexibility. For example, wavelets (Mallat 1987) could be used as an 
alternative to the polynomial bases which we have studied in this paper. The 
hierarchical basis function idea can easily be extended to domains other than 
two-dimensional surfaces. It could just as easily be applied to 3-D elastic models 
that use cylindrical coordinates (Terzopoulos et al. 1987), or to 3-D elastic net 
models built from a recursively tessellated sphere (Section 4.4). 
Comparing the hierarchical basis approach to the relative representation 
which we introduced previously, we see that main difference between these 
two representations is the amount of state utilized. The relative representation 
uses more state than the original fine-level description, whereas the hierarchical 
basis uses the same amount. To constrain the extra degrees of freedom in the 
relative representation, we must use a separate smoothness functional for each 
level. This makes it more difficult to match the original smoothing character-
istics of the fine-level equations, but also gives us more control over the exact 
frequency characteristics. With the hierarchical basis representation, the exact 
same energy equation as was used with the fine level is minimized. 

Representations for low-level vision 
45 
010101 
---
010101 
---
010101 
Figure 2.16: Dual lattice for representing discontinuities 
Depth values are represented by circles and discontinuities by line segments. 
2.4 
Discontinuities 
Representing and localizing discontinuities is an important component of the 
study of surface interpolation and other low-level vision processes. The detection 
of intensity discontinuities (edge detection) has a long history, dating back to 
the earliest days of the computer vision field (Roberts 1965, Hueckel 1971); it 
remains an active area of research (Marr and Hildreth 1980, Canny 1986, Leclerc 
and Zucker 1987). The estimation of depth and orientation discontinuities in 
parallel with surface interpolation was first studied by Terzopoulos (1984). 
In the finite element representation developed by Terzopoulos, depth and 
orientation discontinuities are located on the same grid as the depth values 
themselves. More recent implementations (Gem an and Geman 1984, Marro-
quin 1984, Harris 1987, Blake and Zisserman 1987) use a dual lattice for depth 
discontinuities (Figure 2.16). We will adopt this latter representation in this 
work, with the addition of orientation discontinuities coincident with the orig-
inal depth nodes (see Appendix A). The quantization of discontinuity curves 
represented on the dual lattice is far from ideal. As observed by Blake and 
Zisserman (1987), diagonal curves have a longer apparent length in this repre-
sentation, and are thus penalized more in the detection phase. More sophisticated 
representations of curves are currently being developed (Zucker 1986), but have 
yet to be integrated with visible surface representations. 
Several different methods have been developed for detecting discontinuities 
in parallel with the surface interpolation process. Continuation methods, which 
gradually introduce discontinuities at locations of high curvature, have been in-
vestigated by Terzopoulos (1984). Markov Random Fields have been used in 
conjunction with stochastic optimization by Geman and Geman (1984) and Mar-
roquin (1984). A deterministic approximation to these stochastic algorithms that 
uses analog "neural nets" was studied by Koch et al. (1986). Weak continuity 
constraints, which are similar to Markov Random Field descriptions, have been 

46 
Bayesian Modeling of Uncertainty in Low-Level Vision 
used with the graduated non-convexity (GNC) algorithm by Blake and Zisser-
man (1986b). The use of intensity edges for constraining the location of depth 
discontinuities has been studied by Gamble and Poggio (1987). 
The accurate localization of depth discontinuities is an important element of 
visible surface estimation. Without discontinuities, regularization-based methods 
tend to over-smooth the data, and the accuracy of the reconstruction is reduced. 
Discontinuity detection can also be combined with surface segmentation (Leclerc 
1989), which is an important first step in higher level analysis. It has even been 
recently suggested that discontinuities in the visible surface are more important 
than the depth values themselves (Blake and Zisserman 1987, Poggio et al. 
1988). 
When intensity edges are used for motion tracking or stereo matching, the 
precision of the edge position affects the accuracy of the estimated depth of the 
feature (Matthies and Shafer 1987). Edge detectors that localize the edge position 
to sub-pixel precision (Nalwa 1986) are extremely useful in this context. Sub-
pixel localization of discontinuities in a depth map can also be useful, especially 
in a multiresolution hierarchy. In such a representation, the sub-pixel position of 
a discontinuity at a coarse level can be used to modify the prolongation operation 
(coarse-to-fine interpolation), thus providing a better initial fine level solution. 
The coarse level discontinuity position can be determined unambiguously from 
the fine level so long as the discontinuities are sufficiently far apart. 
The research presented in this book largely ignores the problem of discon-
tinuity detection and localization because of their inherent complexity. The 
treatment of discontinuities is limited in the earlier parts of the book to the in-
troduction of discontinuities by hand to illustrate the flexibility of prior models 
and the effects of discontinuities on uncertainty maps. In Chapter 7, we show 
how the sub-pixel localization of intensity edges and depth discontinuities can 
be used in an incremental depth-from-motion algorithm to significantly improve 
the accuracy of the depth estimates. The automatic detection and localization 
of discontinuities is an important extension that should be added to the methods 
described in this book. 
2.5 
Alternative representations 
In this chapter, we have examined the use of visible surface representations 
for describing intrinsic images. Other computer vision researchers have used 
a variety of alternative representations, both in conjunction with low-level vi-
sion algorithms such as stereo matching and in higher level processing such as 
navigation or object recognition. In this section, we will briefly mention some 
of these alternatives, and compare their characteristics with those of the visible 
surface representation. 

Representations for low-level vision 
47 
One of the most common variations on the single valued visible surface rep-
resentation is the multiple valued disparity field. In this representation, a number 
of possible disparities d are represented at each retinotopic location (x, y). The 
three dimensional cube of data (x, y, d) can contain either binary values (repre-
senting depth hypotheses) or analog values (representing the confidence in these 
hypotheses). This representation has been used in many stereo correspondence 
algorithms (Marr and Poggio 1976, Drumheller and Poggio 1986), and is capable 
of representing transparent surfaces (Prazdny 1985). While it is easy to represent 
competing hypotheses during stereo matching, it is not obvious how to encode 
a physically meaningful measure of smoothness. A more general discussion of 
these considerations appears in (Szeliski 1986). 
A variety of representations have been used for integrating range data ob-
tained from a mobile robot. Spatial occupancy maps use a grid aligned with the 
floor to represent the "certainty" of each cell being occupied (Elfes and Matthies 
1987, Moravec 1988). The grid can also be used to represent the elevation of 
terrain in an outdoor environment (Hebert and Kanade 1988, Hebert et al. 1988). 
Three-dimensional occupancy maps have been used successfully to obtain high 
resolution maps from sonar data (Stewart 1987). These grid-based representa-
tions are more suitable than visible surface representations for fusing information 
from a wide variety of viewpoints, especially since most surface interpolation 
algorithms are viewpoint dependent (Blake and Zisserman 1986a). However, it 
is difficult to tell from these representation where the object surface might lie, 
and to localize its position to less than the grid cell size. 
The visible surface representation used in this book has been generalized to 
three-dimensional objects through the use of energy-based models (Terzopoulos 
et al. 1987). These symmetry-seeking models (or "air bags") use a finite element 
representation parameterized by cylindrical coordinates that encodes the three-
dimensional position of each nodal variable. A similar model can be developed 
using an elastic net (Durbin and Willshaw 1987). The error modeling techniques 
developed in this book can be applied directly to these energy-based models, 
although Fourier analysis techniques are no longer applicable. 
An alternative to energy-based surface representations is the use of spatial 
likelihood maps (Christ 1987). This method uses a probability density func-
tion defined over a spherical coordinate system to integrate location and surface 
normal information obtained from a tactile sensor. Like the disparity field and 
occupancy map representations, a single surface is not represented, and smooth-
ness constraints are difficult to implement. 
Another family of representations that is popular for object and part rep-
resentation is that of lumped parameter models. These models, which include 
generalized cylinders (Brooks et al. 1979, Shafer and Kanade 1983) and su-
perquadrics (Pentland 1986), define three-dimensional surfaces (and volumes) 
using a small number of global geometric parameters. These models are obvi-

48 
Bayesian Modeling of Uncertainty in Low-Level Vision 
ously at a higher level than the visible surfaces which we have been studying, 
but they can often be used for similar applications. The parameter values for 
these models can often be directly estimated from intensity or range data. The 
small dimensionality of these parameter sets makes their estimation more ro-
bust and can lead to easier matching at later stages. However, these models are 
less flexible in terms of representing a wide variety of shapes compared to the 
distributed models which use energy-based finite element representations. 
While these alternative representations may offer advantages over visible 
surface representations in some situations, in this work we concentrate on the 
latter representation. Visible surface representations can be used to model a wide 
variety of intrinsic images and can be used to integrate the output of independent 
low-level vision modules. This intermediate representation can serve as a basis 
for higher level processes such as segmentation and grouping. In this chapter, 
we have seen how regularization, finite element analysis and relaxation can be 
used to formulate and solve low-level vision problems. We have shown how 
multiresolution representations can increase the efficiency of these algorithms 
and can also increase the descriptive power of the representation. Lastly, we 
have discussed how discontinuities play an important role in the development 
of a suitably flexible and accurate representation. In the next chapter, we will 
examine how a Bayesian formulation can be used in conjunction with these 
representations to develop more sophisticated low-level vision algorithms. 

Chapter 3 
Bayesian models and Markov 
Random Fields 
In the early days of computer vision, Bayesian modeling was a popular technique 
for formulating estimation and pattern classification problems (Duda and Hart 
1973). This probabilistic approach fell into disuse, however, as computer vision 
shifted its attention to the understanding of the physics of image formation and 
the solution of inverse problems. Bayesian modeling has had a recent resurgence, 
due in part to the increased sophistication available from Markov Random Field 
models, and due to a realization of the importance of sensor and error modeling. 
In this chapter, we will briefly review the general Bayesian modeling framework. 
This will be followed by an introduction to Markov Random Fields and their 
implementation. We will then discuss the utility of probabilistic models in later 
stages of vision and preview the use of Bayesian modeling in the remainder of 
the book. 
3.1 
Bayesian models 
A Bayesian model is a statistical description of an estimation problem which 
consists of two separate components. The first component, the prior model, p(u), 
is a probabilistic description of the world or its properties before any sense data 
is collected. The second component, the sensor model, p(dlu), is a description 
of the noisy or stochastic processes that relate the original (unknown) state u 
to the sampled input image or sensor values d. These two probabilistic models 
can be combined to obtain a posterior model, p(uld), which is a probabilistic 
description of the current estimate of u given the data d. To compute this 
posterior model we use Bayes' Rule 
( Id) = p(dlu)p(u) 
p u 
p(d) 
(3.1) 

50 
Bayesian Modeling of Uncertainty in Low-Level Vision 
where 
p(d) = L:p(dlu). 
u 
In its usual application (Geman and Geman 1984), Bayesian modeling is used 
to find the Maximum A Posteriori (MAP) estimate, i.e., the value of u which 
maximizes the conditional probability p(uld). In the more general case (Section 
6.1), the optimal estimator u* can be the solution that minimizes the expected 
value of a loss function L(u, u*) with respect to this conditional probability. 
As we will show in Section 3.3, additional useful information (such as the 
uncertainty in our estimates) can be extracted from the posterior distribution. 
A simple example of Bayesian modeling is the classification of terrain ac-
cording to multispectral satellite data (this example is adapted from Duda and 
Hart (1973)). Consider the problem of classifying terrain into vegetation (*) and 
non-vegetation (0) based on the readings from a single infrared band. The prior 
probability of the two classes at any pixel is 
p(Uj = *) = 0.7 and p(Uj = 0) = 0.3 
(in this example, all pixel values are uncorrelated). The two sensor model curves 
p(djluj = *) and p(d;Juj = 0) are shown in Figure 3.1a. By applying Bayes' Rule, 
we can compute the posterior distributions p(Uj = *Idj ) and p(Uj = oldj ) as shown 
in Figure 3.1b. As we can see from this figure, there exists a single threshold 
that can be used for classifying pixels from the sensor data. This restricted use 
of Bayesian modeling is called Bayes decision theory (Duda and Hart 1973). 
To use the Bayesian framework in conjunction with visible surface represen-
tation, we must somehow encode the smoothness inherent in these fields. We 
can do this by using the prior model to describe the correlation between adjacent 
pixels. A simple method for modeling such correlation is presented next. 
3.2 Markov Random Fields 
A Markov Random Field is a probability distribution defined over a discrete 
field where the probability of a particular variable Uj depends only on a small 
number of its neighbors, 
(3.2) 
We can use MRF's to model the correlated structure of dense fields or the 
smoothness inherent in visible surfaces. For our terrain classification example, 
we can specify conditional probabilities for particular configurations of terrain 
type (Figure 3.2). In this figure, we see that the existence of a particular terrain 
type (* or 0) is less likely if it is surrounded by different terrain than if it has 
similar neighbors. 

Bayesian models and Markov Random Fields 
51 
d 
(a) 
d 
(b) 
Figure 3.1: Simple example of Bayesian modeling: terrain classification 
(a) sensor models (b) posterior estimates 

52 
Bayesian Modeling of Uncertainty in Low-Level Vision 
I Configuration I !p(Ui = *Iu)! 
new p(ui=*lu) 
p(lIi=olu) 
0 
0*0 
.05 
.05 
4 
.06 
0 
0 
0*0 
.20 
.25 
3 
.25 
* 0 
0** 
.50 
1 
2 
1 
* 0 
*** 
.80 
4 
1 
4 
* 
* 
*** 
.95 
19 
0 
16 
* 
Figure 3.2: Conditional probabilities for terrain model 
The conditional probabilities p(udu) can be used to generate a prior model 
p(u). However, calculating p(u) such that all of the marginal distributions are 
correct is in general a difficult problem. Fortunately, there exists a simple, 
though indirect, way of specifying a probability distribution whose conditional 
probabilities are Markovian. As shown by Geman and Geman (1984), we can 
use a Gibbs (or Boltzmann) distribution of the form 
1 
p(u) = Zp exp( -Ep(u)/Tp), 
(3.3) 
where Tp is the temperature of the model and Zp is the partition function 
(3.4) 
u 
The energy function Ep(u) can be written as a sum of local clique energies 
Ep(u) = L EC<u), 
cEC 
where each clique energy Ec(u) depends only on a few neighboring points. Thus, 
to build up our conditional probabilities, we use a linear summation of simple 
energy terms. These local energies (or cost functions) can be thought of as a set 
of weak constraints (Hinton 1977) that penalize unlikely configurations of our 
prior model. 
For our terrain model example, we can use a very simple prior energy 
Ep(u) = 2)1 - 8(Uij, Ui+lj» + (1 -
8(Uij, Uij+l» 
(3.5) 
(iJ) 

Bayesian models and Markov Random Fields 
53 
which simply counts the number of adjacent pixels which differ. U sing this 
model, we can calculate the local energy Ep(ujlu} of the configurations in Figure 
3.2, as shown in the fourth column. By computing the energy associated with 
the two values for the center pixel, we can derive the probability ratio of the 
two configurations 
p(Uj = *Iu} = 
p(Uj = olu} 
(3.6) 
Choosing Tp = (In 2)-1 , we obtain the values shown in the fifth column of Figure 
3.2. These values are close to the ones that were originally specified. To obtain 
exactly the same values, higher order (three element) cliques would have to be 
used (Gem an and Geman 1984). 
To compute the probability of any configuration u using (3.3) is straightfor-
ward, but it may be prohibitively expensive due the exponential complexity of 
the partition function. For most applications, however, this computation is not 
necessary. If we wish to generate a random sample from the distribution (3.3), 
we can use an algorithm called the Gibbs Sampler (Gem an and Geman 1984). 
This iterative algorithm successively updates each state variable Uj by randomly 
picking a value from the local Gibbs distribution 
(3.7) 
where 
Zj = L exp( -Ep(u;jU}/Tp}. 
Wi 
This random updating rule is guaranteed to converge (in the ensemble sense) to a 
representative sample from the Gibbs distribution. To speed up this convergence, 
simulated annealing (Metropolis et al. 1953, Kirkpatrick et al. 1983, Hinton and 
Sejnowski 1983) can be used. The stochastic multi grid techniques discussed in 
Section 4.2 (and also Barnard (1989), Konrad and Dubois (1988}) can also be 
used to speed up convergence. 
Applying the Gibbs Sampler to our simple terrain classification example, 
we find that a typical random sample looks like Figure 3.3a. Notice that the 
terrain appears in clumps. Even though the energy function used in the Gibbs 
distribution contains only local terms, the Markov Random Field itself has long-
range interactions (Le., even points that are far apart are still correlated). We can 
use this correlated prior model to improve our terrain classification algorithm. 
For our measurement model, we will use the truncated Gaussian distributions 
shown in Figure 3.1b 
p(ddUj = v) = ~ exp( (dj - J-lv? ) 
Zv 
20"; 
(3.8) 

54 
Bayesian Modeling of Uncertainty in Low-Level Vision 
(a) 
(b) 
(c) 
(d) 
Figure 3.3: Restoration of noisy images through Markov Random Fields 
(a) typical MRF (b) noisy sample (c) MAP estimate (d) MPM estimate 

Bayesian models and Markov Random Fields 
55 
with 
(J.10, 0"0) = (0.2,0.2) and (J.1h O"d = (0.8,0.3). 
Applying this noisy measurement process to the random sample that we have 
just generated, we obtain the image shown in Figure 3.3b. For calculating the 
posterior distribution, it will be convenient to use the log probability of the 
measurement distribution 
E~(Ui' di) = 
-logp(ddui) = L b(Ui, v) [(di ~ ~v)2 + 10gZv] . 
(3.9) 
VE{O,I} 
0" v 
We can thus write the joint distribution of the measurement model as 
1 
p(dlu) = Zd exp( -Ed(U, d» 
(3.10) 
with 
Ed(u, d) = L E~(Ui' d;). 
i 
We are now in a position to derive the posterior distribution p(uld) using 
Bayes' Rule. From (3.1), (3.3) and (3.10) we have 
( Id) - p(dlu)p(u) _ 1 
(E(» 
p u 
-
p(d) 
- Z exp -
u 
(3.11) 
where 
(3.12) 
We thus see that the posterior distribution is itself a Markov Random Field. To 
compute the MAP estimate, we need only to minimize E(u). 
The energy function described by (3.12) has many local minima, so we 
must use simulated annealing to perform the optimization. The Gibbs Sampler 
algorithm (using E(u) as the energy function) can be used directly to find the 
MAP estimate, so long as the system is frozen at the end of the annealing 
(Geman and Geman 1984). The result of applying the MAP algorithm to our 
sampled image is shown in Figure 3.3c. Alternatively, we could also calculate 
the Maximizer of Posterior Marginals (Marroquin 1985), which minimizes the 
expected number of misclassified pixels (Figure 3.3d). 
Comparing (3.12) to the regularization equation (2.1) developed in the pre-
vious chapter, we see that regularization is an example of the more general 
Bayesian approach to optimal estimation. This observation has been made pre-
viously in both the numerical analysis literature (Kimeldorf and Wahba 1970) 
and in the computer vision field (Terzopoulos 1986b, Bertero et af. 1987). Some 
newly discovered implications of this relationship will be discussed in Section 
4.1. The Bayesian interpretation of regularization will also be used in Chapter 
6 to develop uncertainty estimation and parameter estimation techniques. 

56 
Bayesian Modeling of Uncertainty in Low-Level Vision 
The example that we have used to explain Markov Random Fields is a simple 
version of the MRF approach to image restoration developed by Geman and Ge-
man (1984) and Marroquin (1985). Markov Random Fields have also been used 
for solving the stereo correspondence problem (Marroquin 1985, Szeliski 1986, 
Barnard 1986) and for determining discontinuities in visible surfaces (Marro-
quin 1984). In this latter application, the line processes which we encountered 
in the previous chapter can be used to represent the discontinuities (Geman and 
Geman 1984). The use of line processes to encode and localize discontinuities 
is currently one of the chief attractions of the MRF approach to low-level vision 
(Poggio et al. 1988). 
Despite their attractive computational properties and their flexibility, Markov 
Random Fields have some limitations. Markov Random Fields represent distri-
butions with a particularly simple structure, and may be unsuited for modeling 
more complicated distributions or even distributions with limited correlations 
(see Section 4.1). MRFs are good at modeling fields or surfaces such as ter-
rain maps that have a certain smoothness or coherence but that can have many 
bumps or wiggles. They are less appropriate for modeling surfaces with more 
global properties such as piecewise planar surfaces!. The direct estimation and 
modeling of global geometric parameters may be more appropriate in such cases 
(Durrant-Whyte 1987). 
3.3 Using probabilistic models 
The Bayesian models and Markov Random Fields that we have introduced in 
this chapter have previously been used to obtain optimal estimates such as those 
shown in Figure 3.3. In this book, we will argue that additional useful infor-
mation can be extracted from the posterior distribution, and that a probabilistic 
development of prior and sensor models can yield new insights into the solution 
of low-level vision problems. 
A simple way to make better use of a posterior distribution is to calcu-
late higher order statistics (such as variance) to measure the uncertainty in our 
estimates. The variance of each point can be calculated independently as 
(3.13) 
The full covariance matrix of the field u can also be calculated as 
Cov(u) = Eu = J 
(u - u*)(u - u*l p(uld) du, 
(3.14) 
I A Markov Random Field can sometimes be designed such that it has the desired surface 
(e.g., a plane) in its null space (Leclerc 1989). However, the estimation of the surface using the 
MRF will be much slower than direct least squares fitting. 

Bayesian models and Markov Random Fields 
57 
but this infonnation may be too voluminous to store for reasonably sized fields. 
Higher order statistics could also be estimated, but these are not examined in 
this monograph for reasons of simplicity. In many cases, the distributions that 
we deal with will be multivariate Gaussians, so that the first and second order 
statistics completely capture the infonnation about the distribution. 
An alternative to calculating higher order statistics is to pass the whole 
probability distribution to the next higher level of processing. This infonnation 
can then be used to answer specific questions, such as detennining if a point 
belongs to the surface, or implementing certainty-weighted model matching. In 
general, a full posterior probability distribution may be too complex to describe. 
In the case of MRFs, however, the distribution can be specified with only a few 
parameters per pixel. 
Maintaining a probabilistic description of our current estimate is particularly 
useful in the context of dynamic vision. In such a system, new infonnation is 
continually being acquired due to either observer or scene motion, and estimates 
are continually being updated. A useful fonnalism for modeling such a system 
is the Kalman filter, which we will examine in Chapter 7. The Kalman filter 
approach extends the Bayesian framework introduced in this chapter by adding a 
system model to the prior and measurement models. This system model describes 
the evolution of the state being estimated and contains a stochastic component 
to account for unknown disturbances and model errors. 
The work described in this book differs from traditional Bayesian modeling 
in that some of the parameters in the prior or sensor models may be unknown 
and may need to be estimated from the data. In the case of prior models, this 
may involve estimating the amount of smoothing to be applied (Section 6.3) 
or the shape of the smoothing (which is related to the correlation of the prior 
model). For sensor models, it may be necessary to detennine the confidence in 
individual measurements from the data itself (Section 5.3). 
The general Bayesian modeling framework presented in this chapter can be 
instantiated in many ways, depending on the particular visual task, visual domain, 
and sensing strategies being studied. In the next three chapters, we will examine, 
in tum, prior models, sensor models, and posterior models. The prior models 
that we will study are based on Markov Random Fields and regularization. 
The relationship of these prior models to fractal surfaces will be examined. 
We will also develop relative multiresolution representations by examining the 
spectral characteristics of the prior model. A variety of sensor models will 
then be developed, including sparse depth sensors with one-dimensional and 
three-dimensional Gaussian noise and an optical flow estimator. Probabilistic 
posterior models will then be developed. From these models, we will devise 
new techniques for estimating posterior uncertainty, estimating regularization 
parameters, and estimating observer motion. Finally, the Bayesian framework 
will be extended to handle temporal sequences by developing Kalman filter-based 

58 
Bayesian Modeling of Uncertainty in Low-Level Vision 
algorithms for on-line estimation in a dynamic environment. 

Chapter 4 
Prior models 
As we have seen in the previous chapter, prior models play an essential role in 
the formulation of Bayesian estimators. A prior model can be as simple as the 
prior probabilities of different terrain types used in our remote sensing example 
of Section 3.1, or as complicated as the initial state (position, orientation and 
velocity) estimate of a satellite in a Kalman filter on-line estimation system. 
When applied to low-level vision, prior models encode the smoothness or co-
herence of the two-dimensional fields that are being estimated from the image. 
In this chapter, we will examine the spectral characteristics of our prior models, 
develop algorithms for efficiently generating random samples, develop a relative 
representation using a frequency domain approach, and compare our probabilis-
tic models to deterministic (mechanical) models. Let us start by previewing how 
these four ideas fit together. 
The use of Markov Random Fields for modeling smooth fields was first 
suggested by Geman and Geman (1984). In their implementation, they used 
discrete values for the intensity and an energy function similar to (3.5), which 
favored piecewise continuous surfaces. They were also the first to use line 
processes in conjunction with a MRF representation. Subsequent research has 
used fields whose energy resembles that obtained from discretizing the membrane 
model (Marroquin 1984). In Section 4.1 we will extend this idea by examining 
the effect of using the stabilizers used in regularization to define our probabilistic 
prior models. In particular, we will show how the choice of stabilizer determines 
the power spectrum of the prior model. We will also compare these MRF models 
to models based on assuming a particular correlation structure for the surface. 
The ability to generate sample elements from our model space is one of the 
attractions of the probabilistic approach. This capability allows us to determine 
if these random samples are consistent with our intuitions about the domain that 
we are modeling. To generate these typical samples, we will use the Gibbs 
Sampler algorithm described in Section 3.2. As we will show in Section 4.2, 
the implementation of this algorithm for models such as the membrane and thin 

60 
Bayesian Modeling of Uncertainty in Low-Level Vision 
plate is particularly simple and only requires adding a controlled amount of 
Gaussian noise to the usual Gauss-Seidel relaxation algorithm. We will examine 
how multigrid (coarse-to-fine) stochastic relaxation can help speed up the ap-
proach of the Gibbs Sampler towards equilibrium. We will also show how this 
algorithm can be used for generating constrained fractals for computer graphics 
applications. 
In Section 4.2, we re-examine the question of designing a multiresolution 
relative representation. Using the results of Section 4.1, we show how to design 
a multiresolution representation by matching the sum of the power spectra at 
each level to the spectrum of the original single-resolution implementation. This 
composite representation thus has a smoothing and interpolating behavior similar 
to that of the original model (since they implement similar priors), and also 
decomposes the signal into a multiresolution description. 
The prior models that we will be studying in this chapter are commonly used 
to describe intrinsic images and can thus be thought of as "intrinsic models" 
(this term was coined by Gudrun Klinker). In the hierarchy of visual processing 
(Figure 2.1), intrinsic models span the middle ground between the object models 
used in high-level vision and the physical models that describe image formation. 
Object models are normally used to determine the identity and pose (position and 
orientation) of a three-dimensional object. These models are typically described 
by a small number of lumped parameters, such as the pose, the relative positions 
of parts for articulated objects, and perhaps some shape parameters for models 
such as superquadrics (Pentland 1986). In certain cases, the parameters of these 
models can be determined directly from the image data (Lowe 1985, Pentland 
1986). Intrinsic models, on the other hand, have a large number of distributed 
parameters, such as the depth value at each node for a surface model. If we 
are to recover these parameters from the limited data available in the image, we 
must restrict their values and thus restrict the space of possible models. 
Two approaches that can be used to achieve this restriction are energy-based 
models (e.g., regularization) and Bayesian prior models. As we discussed in 
the previous chapter, these two approaches are somewhat equivalent. In their 
recent book, Blake and Zisserman (1987) argue that the mechanical (energy-
based) approach is preferable to the probabilistic (Bayesian) one. In Section 
4.4, we will give counterarguments in support of the Bayesian approach, and 
show how our method can be extended to include weak continuity constraints 
and three-dimensional models. 
4.1 
Regularization and fractal priors 
When selecting a Markov Random Field prior model, we must choose some 
energy function to define the Gibbs distribution. As we saw in Section 3.2, 

Prior models 
61 
Figure 4.1: Typical sample from the thin plate prior model 
choosing the regularization smoothness constraint as the energy function results 
in a MAP estimate which is identical to that obtained from regularization. While 
this observation has been used as a statistical justification for regularization, the 
characteristics of the prior model have not previously been investigated. In this 
section, we will study the prior model in isolation and determine what class of 
surfaces it describes. 
One way of studying the prior model is to generate some typical random 
samples using the Gibbs Sampler algorithm described in Section 3.2 (the im-
plementation details are given in the next section). Using the thin plate whose 
energy is given in (2.4) as our model, we can generate a typical sample from 
the prior distribution as shown in Figure 4.1. This surface has an interesting 
rough or bumpy structure that is quite different from the smooth shape that one 
might expect. A convenient way to characterize this roughness is to compute 
the spectral characteristics of the surface using Fourier analysis. 
The Fourier transform (Bracewell 1978) of a multidimensional signal v(x) is 
defined by 
F { v } == J v(x) exp(27ri f . x) dx = V(f), 
and the transform of its partial derivative is given by 
F {~;x)} = (27rifj) V(f). 
J 
Using Rayleigh's energy theorem 
J 
Iv(x)12dx = J 
jV(f)12df, 
(4.1) 
(4.2) 
(4.3) 

62 
Bayesian Modeling of Uncertainty in Low-Level Vision 
we can re-write the smoothness functional Ep(u) in tenns of the Fourier transfonn 
U(O = F { u} to obtain the new energy function E~(U). 
For our smoothness functional, we will use the general fonn given in (2.6) 
with the simplifying assumption that the weighting functions wm(x) are constant. 
While this assumption does not strictly apply to the general case of piecewise 
continuous interpolation, it provides an approximation to the local behavior of 
the regularized system away from boundaries and discontinuities. Applying (4.2) 
and (4.3) to (2.6) we obtain 
or 
(4.4) 
where 
P 
IHp(OI2 = L WmI27rfl2m· 
(4.5) 
m=O 
For the membrane interpolator, IHp(OI2 ex 127rf12 and for the thin plate model, 
IHp(OI2 ex I 27rf1 4. 
The results of this analysis can be combined with a similar analysis of the 
data constraint to derive the filtering behavior of regularization-based smoothers 
(see Appendix B). In this section, however, we are interested in studying the 
spectral characteristics of the prior model. To derive these, we note that since 
the Fourier transfonn is a linear operation, if u(x) is a random variable with 
a Gibbs distribution with energy Ep(u), then U(O is a random variable with a 
Gibbs distributionl with energy E~(U). We thus have 
from which we see that the probability distribution at any frequency f is 
Thus, U(O is a random Gaussian variable with variance IHP(OI-2, and the signal 
u(x) is correlated Gaussian noise with a spectral distribution 
(4.6) 
From this analysis, we can conclude that using a regularization-based smooth-
ness constraint is equivalent to using a correlated Gaussian field as the Bayesian 
IThis is because the Jacobian lau/aul is a constant for a linear operator. 

Prior models 
63 
Figure 4.2: Fractal (random) solution 
prior. The spectral characteristics of this Gaussian field are determined by the 
choice of stabilizer. For the membrane and the thin plate models, we have 
(4.7) 
and 
(4.8) 
These equations are interesting because they correspond in form to the spectra 
of Brownian fractals. 
Fractals are a class of mathematical objects that exhibit self-similarity over a 
range of scales (Mandelbrot 1982). Fractals have been used to generate intricate 
geometric designs, to study the statistical properties of coastlines and structured 
noise, and to generate realistic images of terrain. A stochastic fractal is a random 
process or a random field that exhibits self-affine statistics over a range of scales. 
A common way to characterize such a fractal is to say that it follows a power 
law in its spectral density 
(4.9) 
This spectral density characterizes a fractal Brownian function VH(X) with 2H = 
f3 - E, whose fractal dimension is D = E + 1 - H (where E is the dimension of 
the Euclidean space) (Voss 1985). A function that satisfies (4.9) may also be 
fractional Gaussian noise (Rensink 1986). 
Comparing (4.7) or (4.8) to (4.9), we can conclude that the smoothness as-
sumptions embedded in certain regularization methods are equivalent to assum-
ing that the underlying processes is fractal (Szeliski 1987). When regularization 

64 
Bayesian Modeling of Uncertainty in Low-LeveL Vision 
techniques are used, it is usual to find the minimum energy solution (Figure 
2.2c), which also corresponds to the mean value solution for those cases where 
the energy functions are quadratic. Thus, the fractal nature of the process is 
not evident. A far more representative solution can be generated if a random 
(fractal) sample is taken from this distribution (Figure 4.2). The amount of noise 
(and hence roughness) that is desirable or appropriate can be derived from the 
data (see Section 6.3). 
The fractal nature of the membrane and thin plate models means that these 
interpolators have no natural scale, and can thus be applied to any size of data2• 
It also suggests that we could use priors with in-between (truly fractional) de-
grees of smoothness. In theory, this is straightforward, since we can specify the 
prior model to be a Brownian fractal field with an arbitrary ;3. In practice, im-
plementing the resulting interpolator is difficult. Boult (1986) has implemented 
such fractional interpolators using reproducing kernel splines. In Section 4.2 we 
will present an alternative approach based on multiresolution relaxation. 
The prior models which we use need not be isotropic or homogeneous. In 
general, we can choose a prior model with any arbitrary correlation function3 
(the correlation is the inverse Fourier transform of the spectral density). We 
could thus model mountain ridges or terrain with varying degrees of smoothness 
within a single framework. We will explore how to implement such arbitrary 
prior models in the next section. 
Since the prior models which we have examined in this section are examples 
of correlated Gaussian noise, it is interesting to compare these surfaces to those 
obtained by filtering white noise through a finite impulse response (FIR) filter. 
A regularization-based prior model described by (4.5) and (4.6) is a pseudo-
Markovian Gaussian field of order p (Adler 1981). This field has an infinite 
correlation, which is what makes it particularly suitable for interpolation. The 
discrete version of this field is a Markov Random Field, and its energy function 
can be described by a small number of additive terms4• The generative model for 
this field is the Gibbs Sampler, which can be quite slow since the system must 
be near equilibrium to generate a representative sample. The MAP estimate can 
be found using a similar relaxation algorithm. 
In contrast, FIR filtered white noise has a finite correlation and is non-
Markovian (no two sample points are conditionally independent)5. The genera-
tive model for this field is extremely simple Gust filter white noise). However, 
local relaxation algorithms for doing MAP estimation do not in general exist. 
We can thus see the two main advantages of using Markov Random Fields for 
2This scale independence is in marked contrast to the viewpoint dependence of these models 
(Blake and Zissennan 1986a). 
3This suggestion was made by Alex Pentland. 
4In fact, its infonnation (inverse covariance) matrix is sparse and banded (see 6.2). 
sThe exception is white noise, which is both an MRF and (trivially) FIR filtered white noise. 

Prior models 
65 
modeling surfaces and other intrinsic images: they can model infinite range cor-
relations with only local interactions, and they lead to local relaxation algorithms 
for MAP estimation. 
4.2 Generating constrained fractals 
To generate the random samples from either the prior or posterior models, we 
have to use the Gibbs Sampler algorithm described in Section 3.2. As we have 
seen, the ability to generate such random samples from the prior model gives us 
a useful tool for studying the behavior of the prior. In Section 6.2, we will see 
how this random sampler can also be used to determine the uncertainty in the 
posterior estimate. In this section, we will describe our implementation of the 
Gibbs Sampler for visible surfaces, and show how this new algorithm can be 
used to generate constrained fractal surfaces for computer graphics applications. 
As we saw in Section 3.2, the Gibbs Sampler is an iterative stochastic algo-
rithm where each state variable Ui is updated asynchronously (sequentially) by 
picking a value from the local Gibbs distribution (3.7). For the visible surface 
representations that we studied in Section 2.2, the local energy function (2.17) 
is quadratic, with a minimum value ut given by (2.18) and a second derivative 
equal to aii. The local Gibbs distribution is therefore 
( I ) 
( aii(ui - Ui)2) 
P Ui U ex exp ---2-1',-"-
p 
(4.10) 
which is a Gaussian with mean ut and variance Tpj aii. We thus see that the 
Gibbs Sampler is equivalent to the usual Gauss-Seidel relaxation algorithm with 
the addition of some locally controlled Gaussian noise at each step (Szeliski 
1987). The temperature parameter Tp controls the amount of roughness in the 
random sample. In Section 6.3, we will present a method for determining the 
appropriate value of Tp from the sampled data. 
As is the case with deterministic relaxation, the above algorithm converges 
very slowly towards its equilibrium distribution (the point at which the system 
exhibits negligible statistical dependence on its starting configuration (Ackley et 
al. 1985». To speed up this convergence, we can use a coarse-to-fine technique 
similar to the one used with deterministic relaxation. We simply generate a 
random sample using the Gibbs Sampler at a coarser level, and then use the 
interpolated sample as a starting configuration for the finer level. This starting 
configuration will already be closer to equilibrium than a non-random config-
uration such as the zero state. More importantly, it will contain more of the 
low-frequency components of the random field than can be obtained by iterat-
ing for a long time on the fine level. Appendix B contains a Fourier analysis 
of the convergence rates of the multiresolution Gibbs Sampler. Multiresolution 

66 
Bayesian Modeling of Uncertainty in Low-Level Vision 
stochastic resolution has also been studied by Barnard (1989) and Konrad and 
Dubois (1988). 
For a correct implementation of the multiresolution algorithm, we must en-
sure that the coarse level generates a random sample which has the same statis-
tics as the subsampled version of a random fine level sample. In general, this is 
a difficult problem which may require the application of group renormalization 
techniques (Wilson 1979). Fortunately, when the energy equations are quadratic, 
it suffices to ensure that the energy equations at the coarse level are a good ap-
proximation to the energy of the deterministic continuous system (which can 
be ensured by using finite element analysis). To prove that this is sufficient, 
we need to show that the distribution obtained from substituting the minimum 
energy configuration that matches the coarse level sample into the fine level 
energy equations is close to the true probability distribution. 
Consider a fine level system UI with an associated energy equation 
(4.11) 
This equation implies that UI is a multivariate Gaussian with mean ih and a 
covariance All. We can obtain a coarse level system U2 using a subsampling 
matrix H, i.e., U2 = HUI. The coarse level system is thus a multivariate Gaussian 
with mean ih = HUI and a covariance HT AIIH. Let 01 denote the minimum 
energy solution to (4.11) which satisfies U2 = Hftl. We wish to show that the 
coarse level distribution is the same as would be obtained by substituting 01 into 
(4.11). To do this, we consider the joint distribution p(u\, U2), which is also a 
multivariate Gaussian. Using the result shown in Appendix D, we can show that 
marginalizing a multivariate Gaussian with respect to some of its state variables 
is equivalent to substituting the minimum energy solution for those variables (in 
terms of the remaining variables) into the joint density function. Since in our 
case 01 = HU2, the joint density is equivalent to the prior distribution on UI, and 
we thus have the desired result 
(4.12) 
The significance of this result is as follows. In general, to derive a coarse 
level probability density function from a fine level function, we must marginalize 
over all of the possible configurations of the fine model that are consistent with 
the coarse model. For the case of our visible surface models, whose typical 
elements are rough fractals, we expect the average energy of a fine surface 
coincident with the coarse level solution to be much higher than the minimum 
energy possible. Fortunately, this difference in energy is a constant independent 
of the coarse level configuration. Since we are using a Gibbs distribution, this 
means that the probability density obtained using the minimum energy (smooth) 

Prior models 
67 
solution is the same as would be obtained by taking into account the rough 
fractal nature of the prior model. 
By considering the continuous surface model as the fine level, this same 
argument can be used to justify why it is acceptable to develop finite element 
energy equations assuming that the system is always in its minimum energy state, 
and to then use these energy equations in the probability distribution. This unique 
situation only exists when the distributions are Gaussian fields (or equivalently, 
the energy functionals are quadratic). In the general case, such as for spin 
glasses or other systems with an underlying discrete structure and non-quadratic 
energy functions, we have to use more sophisticated techniques (Wilson 1979, 
Barnard 1989). The same is true for our visible surface representations when 
we include line processes as part of the state description (rather than keeping 
them fixed). In our current implementation, we ignore the problems associated 
with the re-normalization of line process variables and simply use energy-based 
approximations. 
An example of our multiresolution Gibbs Sampler being used to generate 
a fractal surface is shown in Figure 4.3 (this example uses a thin plate energy 
model). The solution at each level is used as a starting point for the Gibbs 
Sampler at the next finer level. Since iteration at the finer level can significantly 
move the solution away from its starting point, the coarse levels are no longer 
subsampled versions of the fine level. If we wish the fine level solutions to have 
a closer resemblance to the coarse level solution (say for generating a zoom 
sequence in computer graphics (Fournier et al. 1982)), we can either use fewer 
iterations on the fine level, or add an extra data compatibility constraint between 
the coarse and fine level surfaces (Szeliski and Terzopoulos 1989a). 
We can use the multiresolution Gibbs Sampler algorithm which we have just 
developed to generate constrained fractals with arbitrary discontinuities. Using 
the same data points as we used for the thin plate interpolation example (Figure 
2.2a) and also the same energy equations, we can apply the Gibbs Sampler 
to the posterior distribution defined by (3.11). A typical sample generated by 
this approach is shown in Figure 4.2. While this sample is not truly fractal 
since it depends on the data points, it is a typical sample from the fractal prior 
distribution conditioned on the data points that were observed. We can thus 
shape the fractal by imposing arbitrary depth constraints, orientation constraints 
(Terzopoulos 1984), depth discontinuities or creases. 
A number of different fractal generation algorithms have been developed 
over the last few years for computer graphics applications. 
Fournier et al. 
(1982) use a technique called random midpoint displacement which creates a 
tessellated surface by recursively subdividing triangles. As new interior points 
are created, their height value is randomly perturbed away from their original 
interpolated value, with the magnitude of the perturbation being related to the 
level of the subdivision. By varying the relationship between this perturbation 

68 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Figure 4.3: Multiresolution fractal sample from thin plate model 

Prior models 
69 
magnitude and the subdivision level, fractals of arbitrary degree can be created. 
Voss (1985) uses successive random additions, which is similar to random mid-
point displacement, except that all of the points are randomly perturbed at each 
subdivision step (not just the ones which are newly created). Recently, Lewis 
(1987) has introduced a refinement ofrandom midpoint displacement which he 
calls generalized stochastic subdivision. Instead of displacing each midpoint 
independently, correlated Gaussian noise is added. This reduces the creases that 
are sometimes visible with the former method. 
All three of these methods can implement fractals of arbitrary dimension, 
but suffer from statistics that are not spatially stationary. Their biggest deficit is 
that they are difficult to constrain. The usual approach to this problem is to first 
layout a coarse solution, and to then add fractal texture to this solution using 
recursive subdivision. In contrast to these previous methods, our multiresolution 
Gibbs Sampler easily accommodates arbitrary constraints as additional terms in 
its energy equation. The solution is also guaranteed to have spatially stationary 
statistics, so long as the fine level solution is iterated a sufficient number of 
times (in practice, this number can be quite low). The biggest limitation of 
the multiresolution Gibbs Sampler as we have thus far described it is that it is 
limited to prior models with spectra of the form 
Su(f) ex IW2m, 
i.e., surfaces that have integral fractal dimensions. We thus have to extend our 
algorithm to overcome this limitation. 
The simplest way to approximate a fractal with an in-between fractal di-
mension is to use a blend of the thin plate and membrane models (Terzopoulos 
(1986b) calls these "splines under tension"). If we choose WI = 127r/o12w2 in 
(2.6), we obtain the power spectrum shown in Figure 4.4. This Bode plot (log-
log power plot) shows that the prior model behaves as Su ex Ifl-3 in the vicinity 
of/o (2.0 in this case), as a membrane at lower frequencies, and as a thin plate at 
higher frequencies. The interpolated solution using this mixed model is shown 
in Figure 4.5a, and a typical sample from the posterior distribution is shown in 
Figure 4.5b. 
We can extend the range of in-between fractal behavior by modifying the 
coarse-to-fine Gibbs Sampler. Instead of implementing the same energy equation 
at each level, we modify the algorithm so that a different blend of membrane 
and thin plate is used at each level. Since we wish to have Su ~ Ifl-,6, we 
require that 
(4.13) 
where 1",,(f)1-2 is the power spectrum of the prior model at level I. From this 
equation, we obtain the relationship 
(4.14) 

70 
Bayesian Modeling of Uncertainty in Low-Level Vision 
........ 40 
~ 
~20 
[ 
til 0 
~" 
...... , 
Frequency (Hz 
1~ 
05 
20 
40 
80 
H. o 
-20 
"" " 
-40 
", 
-60 
~" 
-80 
~ 
, , , , 
-100 
\ K"'", 
-120 
\ ~ 
-140 
\V 
\ 
Figure 4.4: Power spectrum of mixed membrane I thin plate 
The coarse, medium and fine level spectra are shown (the fine level curve is 
uppermost). The dashed line shows the Ifl-3 asymptote. 
Figure 4.5: Interpolated surface and fractal surface for mixed membrane I thin 
plate 

Prior models 
71 
~" 
..... ~ 
Frequency (Hz 
05 
1 
20 
40 
80 
1(. o 
" "" 
-20 
-40 
~ 
-60 
~ 
-80 
\, 
~, 
-100 
~ ~" 
-120 
\ r\ 
-140 
\V 
\ 
Figure 4.6: Power spectrum of fractal approximation 
The coarse, medium and fine level spectra are shown (the fine level curve is 
uppermost). The dashed line shows the Ifl-3 asymptote. 
Figure 4.7: Interpolated surface and fractal surface for fractal approximation 

72 
Bayesian Modeling of Uncertainty in Low-Level Vision 
....... 40 
~ 
~20 
~ 
en 0 
Frequency (Hz 
40 
80 
1 .0 
-20 
-40 
-60 
-80 
-100 
-120 
-140 
Figure 4.8: Power spectrum of composite (relative representation) surface 
The coarse, medium and fine level spectra are shown, along with the summed 
spectrum (uppermost solid curve). The dashed line shows the Ifl-3 asymptote. 
Since the relaxation at a given level mostly affects the high frequency compo-
nents, we can use the coarse levels to shape the low frequencies and the fine 
levels to shape the high frequencies. The effective power spectrum of the result-
ing interpolator (or random sample) now depends on the number of iterations 
performed at each level. 
Figure 4.6 shows a spectrum generated with a three level hierarchy, using 
a large number of iterations at the coarse level followed by five iterations at 
the medium level and five at the fine level. Compared to Figure 4.4, the range 
over which the spectrum closely approximates Ifl-3 is extended from one octave 
to three octaves. The interpolated solution and typical posterior sample using 
this fractal approximation are shown in Figures 4.7a and 4.7b. It is difficult to 
observe any significant difference between these results and the simple blend of 
membrane and thin plate (Figure 4.5). These differences might become more 
apparent if we were to blow up the surface to reveal more detail. 
An alternative approach to constructing a prior model with an in-between 
fractal spectrum is to create a composite fractal by summing up the interpolated 
depth maps from each level in a multiresolution hierarchy. Using this approach, 

Prior models 
73 
each level has its own associated prior energy function, and the power spectrum 
of the composite surface is the sum of the individual level spectra (since we are 
adding independent Gaussian fields). To ensure that the summed spectrum has 
the desired fractal dimension, we impose the same condition on the Wm values 
at each level as we used with the coarse-to-fine Gibbs Sampler (4.14). 
The resulting composite power spectrum for a three level pyramid is shown 
in Figure 4.8. The shape of this spectrum is not as linear as that shown in Figure 
4.6, but this situation could be improved by using more coarse levels. The main 
advantage of using the composite fractal is that the spectrum is not dependent on 
the number of iterations performed at each level (more precisely, each level must 
be iterated until it is at equilibrium). Additional advantages include the ability to 
perform relaxation at all levels simultaneously and a decomposition of the signal 
into a multiresolution relative representation6• This method, however, is sensitive 
to the choice of interpolator used at each level and may also have very slow 
convergence towards equilibrium because of the tight coupling between levels 
when doing interpolation (MAP estimation). We will defer these problems until 
the next section, where we re-examine the multiresolution relative representation 
in light of our new results about fractal priors. 
Before closing our discussion of fractal generation, it should be pointed out 
that we can easily choose non-homogeneous or non-isotropic fractal models 7• 
In fact, we can choose arbitrary correlations for our prior model, although these 
may lead to energy equations whose discrete implementation does not have a 
sparse (MRF) structure. Figure 4.10 shows an example of the sophisticated 
fractals that can be generated using our new multiresolution Gibbs Sampler 
method. This fractal scene is constrained by the data points shown in Figure 
4.9, and has a crease coinciding with the ravine on the upper right side, and 
a depth discontinuity on the lower right. Four different continuity models are 
used: a blend of membrane and thin plate in the upper quadrant, a membrane 
in the right quadrant, a thin plate in the left quadrant, and a very stiff thin plate 
in the lower quadrant. 
The methods which we have described in this section can be improved in 
several ways. A better representation for discontinuities should lead to more 
natural looking breaks, and a more sophisticated implementation of the finite 
element discretization could increase the power of the method (e.g., by allow-
ing the placement of data points on non-lattice locations). The use of weak 
continuity constraints could also be investigated, allowing our fractal generation 
algorithm to introduce tears into the surface automatically. Finally more efficient 
algorithms for attaining equilibrium should be investigated. 
6In fact, we can use this approach to invert the fractal process, i.e., to decompose a fractal 
surface into its multiresolution description. 
7The issue of non-isotropic interpolators has also been studied by Boult (1986). 

74 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Figure 4.9: Depth constraints for fractal generation example 
Figure 4.10: Constrained fractal with spatially varying fractal dimension and 
variance 

Prior models 
75 
In summary, the new fractal generation algorithm which we have developed 
in this section has several advantages over existing techniques. It allows depth 
and orientation constraints as well as depth and orientation discontinuities to be 
imposed on the fractal generation process at arbitrary locations. The resulting 
fractal has homogeneous statistics (conditioned on the constraints), and these 
statistics can be varied locally. The coarse-to-fine algorithm which we developed 
efficiently produces a random sample from this constrained fractal, which can 
be of arbitrary degree or spectral shape. 
4.3 Relative depth representations (reprise) 
In Section 2.3, we saw how a multiresolution relative representation could de-
compose the visible surface into a number of local depth maps, each of which 
would describe surface detail at its own level of resolution. Discontinuities could 
be assigned to just one level in the resolution hierarchy, and each level could 
also have a separate degree of uncertainty, thus permitting the representation of 
fine detail even when the absolute depth is unknown. Such a multiresolution 
representation could also use fully parallel relaxation. 
Our investigation into relative representations was hampered by the difficulty 
of selecting appropriate prior models and interpolation functions for the levels 
in the hierarchy. We derived an equation (2.25) which placed conditions on 
the prior information matrices A~ and the interpolation matrices II which would 
ensure equivalence to a single-resolution model. Unfortunately, this equation 
is not very intuitive and involves inverse information matrices, which are not 
sparse. 
A simpler development can be obtained using the results presented in this 
chapter. Instead of defining the prior model by a smoothness energy functional, 
we consider the prior model to be a correlated Gaussian field with a known 
power spectrum. We can then calculate the power spectrum of the composite 
(summed) surface in the relative representation by adding up the spectra of the 
levels multiplied by the filtering behavior of the interpolators. This design in 
frequency space is significantly simpler than trying to manipulate large matrices. 
To formalize this description, let IH~(f)1-2 be the spectrum of the prior model 
at level I. From Appendix B, we see that 
p 
IH~(f) 12 = L wmh/2m( 4 - 2 cos 27rfxhl - 2 cos 27r/yhl )2m 
171--0 
where hI = I Llxl I = ILlyI\ is the grid spacing at level I. Alternatively it can be 
obtained by taking the Fourier transform of a row of the spline model matrix 
A~, 
(4.15) 

76 
Bayesian Modeling of Uncertainty in Low-Level Visio. 
This power spectrum closely approximates (4.5) for frequencies that are small 
compared to 1/ hI (Le., the approximation error of the finite element implemen-
tation is proportional to the grid size). Let FI(f) be the frequency response of 
the interpolation filter. For example, for the bilinear interpolator, we have 
F(f) = (Sin 7rfxhl) 2 (Sin 7rhhl) 2 
7rfxhl 
7rfyhl 
The composite spectrum can now be derived as 
L 
IHif)I-2 = L IF(f)12IH~(f)1-2. 
(4.16) 
1=1 
Comparing this equation to (2.25) (Section 2.3), we see that the new equation 
deals with spectral characteristics, which are easy to visualize and for which 
established design techniques exist. For example, if we wish to use our relative 
representation to model surfaces with a given fractal spectrum, we can use the 
relationship given in (4.14), which results in the spectrum shown in Figure 4.8. 
Figure 4.11 shows the five-level relative representation with a fractal degree f3 = 
3 applied to our sample data set. For this example, a blend of a membrane and a 
thin plate is used at each level along with third-degree Langrange interpolation. 
Some care must be taken with the spectral approach to prior modeling. Since 
we wish the composite surface, which is a Gaussian field, to have spatially homo-
geneous (shift-invariant) statistics, we have to ensure that the fields interpolated 
from each level also have this characteristic. This condition is equivalent to 
ensuring that the Fourier transform of interpolated field is uncorrelated noise, 
or equivalently, that the power in the sidebands (the region lt1 > l/hJ) is small 
compared to the power at the other (finer) levels. In practice, this means that we 
have to use an interpolator of a sufficiently high degree to avoid visible creases 
or other artifacts that arise from interpolating the coarse level. 
The initial results with the relative representations are encouraging, but much 
work remains to be done. The biggest current problem is with the speed of 
convergence. Because of the tight coupling between the levels, usual Gauss-
Seidel relaxation does not work well. Conjugate gradient descent (Press et al. 
1986) does better, since it can quickly get out of the narrow ravines in the 
energy landscape that are caused by the tight coupling. Another possibility 
worth exploring is to initially relax the data coupling term, since this again 
makes the system less stiff (Hinton 1977). Using higher order interpolators 
should also produce better (smoother) results, although at the expense of longer 
computation times. Finally, the introduction of discontinuities at the appropriate 
level, and the interaction between discontinuities at different levels, should prove 
to be a promising area of research. 

Prior models 
77 
(e) 
Figure 4.11: Relative representation for (3 = 3 interpolator 

78 
Bayesian Modeling of Uncertainty in Low-Level Vision 
4.4 Mechanical vs. probabilistic models 
In their book Visual Reconstruction, Blake and Zisserman (1987) layout an 
elegant approach to the problem of piecewise continuous surface reconstruction. 
In explaining their method, Blake and Zisserman argue in favor of adopting a 
deterministic mechanical (energy-based) approach in preference to a stochastic 
probabilistic approach. In this section, we will argue the converse, that the 
Bayesian approach has many advantages over the mechanical approach. We 
will also study the use of weak continuity constraints in defining prior models, 
and show how our ideas can be extended to three-dimensional models. 
One of Blake and Zisserman's main arguments in favor of the mechani-
cal viewpoint is that the models should be continuous. Using such continuous 
models facilitates variational analysis, allows the implementation of viewpoint-
invariant interpolators, and matches the continuous nature of surfaces and in-
tensity fields in the real world. Fortunately, continuous models are not incom-
patible with probabilistic modeling. As we have seen in the previous section, 
regularization-based models are equivalent to correlated Gaussian fields. Even 
models such as viewpoint-invariant interpolators that do not have quadratic en-
ergy functions can be turned into probability distributions through the use of the 
Gibbs distribution. 
A continuous field, of course, cannot be simulated on a computer without 
first discretizing the energy or probability equations. The best way to perform 
this discretization for mechanical models is to use finite element analysis. The 
same discrete equations which are derived from the mechanical model can then 
be used to define a Markov Random Field. We can thus view the MRF as a 
discretized version of the continuous pseudo-Markovian field. The parameters 
for this field need not be computed by assigning conditional probabilities. They 
can be derived the same way as they are for mechanical models, i.e., using 
parameters with natural interpretations or physical correlates. 
Because of the equivalence between energy functions and probability density 
functions which can be established using the Gibbs distribution, we can design 
probabilistic models which have the exact same performance as that obtained 
with mechanical models. The question is then "what possible, advantages do 
probabilistic models offer?" One advantage is that we can develop probabilistic 
sensor models which closely match the characteristics of real sensors. As we will 
see in Chapter 5, we can model the three-dimensional noise inherent in range 
measurements or analyze the uncertainty in optical flow estimation. Another 
advantage is that we can choose different loss functions to use with our poste-
rior estimator (Section 6.1), whereas mechanical models always find the MAP 
estimate. With a probabilistic model we can also determine the uncertainty in 
our estimate (Section 6.2), which corresponds to determining the stiffness of the 
mechanical model. Additional applications of the probabilistic approach will be 

Prior models 
79 
presented in Chapters 6 and 7. 
Perhaps the most interesting result of Bayesian modeling, however, is what 
it tells us about the continuity of the models that we use in low-level vision. 
A thin plate spline is piecewise continuous in its third derivative (therefore 
continuous everywhere in its second), with the discontinuities coinciding with 
the data points. A typical random sample from the thin plate prior model, on 
the other hand, is discontinuous everywhere in its second derivative (in fact, 
its second derivative is white noise). The minimum energy solution is thus 
smoother that an actual typical sample from which the data may have come; this 
smoothness is attributable to the averaging behavior of the mean estimate. We 
thus have an interesting dichotomy: the minimum energy solution is smoother 
but shows higher order discontinuities coincident with the data points, whereas 
a typical sample from the posterior distribution is rougher but shows no clues 
as to where the data points might be located. 
So far, we have been discussing prior models that have a uniform continuity 
across the visual field. These models give rise to fractal surfaces which may be 
appropriate for modeling terrain or uninterpreted image data, but which seem 
like poor models for most visual surfaces. The weak continuity models used 
by Blake and Zisserman (1987) seem much more appropriate for the structured 
environments in which we live. To investigate if these weak continuity models 
are indeed useful as a prior model, we can use their energy equations as part of 
a Gibbs distribution and generate typical samples using the Gibbs Sampler. 
Figure 4.l2a shows a random sample taken from the weak membrane prior 
model. Compared to the regular membrane model shown in Figure 4.12c, the 
weak membrane has less roughness in the continuous areas since we can run 
this model at a lower temperature. The depth discontinuities, whose locations 
are shown in Figure 4.12b, allow for a greater variation in depth than would be 
possible with the continuous membrane. By turning down the temperature even 
lower while decreasing the line-break penalty, we can obtain a system which 
models piecewise constant surfaces and whose behavior resembles the model 
used by Leclerc (1989) in his scene segmentation work. 
Using weak thin plates instead of weak membranes should lead to better 
prior models and better posterior estimates. With an appropriate selection of 
parameters, we can use these to model piecewise planar surfaces. Even higher 
order models are possible. For example, we could use a third order Tikhonov 
stabilizer «2.6) with non-zero W3). Blake and Zisserman (1987) have also sug-
gested using models which are piecewise concave or piecewise convex, which 
corresponds to a weak continuity constraint on the sign of the curvature. Un-
fortunately, as the complexity of the model increases, so does the time required 
to generate a typical sample or the time to find a MAP estimate. Nevertheless, 
these possibilities deserve further study. 
An alternative to using weak continuity constraints or other energy-based 

80 
Bayesian Modeling of Uncertainty in Low-Level Vision 
(c) 
Figure 4.12: Random sample using weak membrane as prior 
(a) random sample (b) discontinuities shown as missing lines (c) using regular 
membrane 

Prior models 
81 
Figure 4.13: Random sample from a three-dimensional elastic net 
models is to use process models to define the priors. As discussed by Man-
delbrot (1982), process models describe repeated actions such as breaking or 
bending operating at a number of resolutions whose compound action results in 
a fractal surface. For the fractal surfaces examined in the previous section, the 
process model is rather simple. We take an elastic sheet and bombard it at every 
point with random vertical disturbances (white Gaussian noise) (Szeliski and 
Terzopoulos 1989a). More sophisticated process models which introduce rips 
and creases into the surface or bend the surface by applying torques at sparse 
locations should result in prior models that better describe visible surfaces and 
other intrinsic images. 
Probabilistic modeling also need not be restricted to two-dimensional fields. 
We can apply the same techniques for converting energy-based models into 
Bayesian priors to three-dimensional models such as those being investigated 
by Terzopoulos et al. (1987). Figure 4.13 shows a three dimensional elastic net 
(based on the two-dimensional nets developed by Durbin and Willshaw (1987)) 
which was obtained by tessellating a sphere. The energy equation for this net is 
E(Pi) = 1:: 1:: IPi - pl- p 1:: IpiI-
i jeNj 
i 
Applying the Gibbs Sampler to this system, we obtain the typical sample shown 
in Figure 4.13. This figure resembles the examples of fractal textured spheres 
shown in (Mandelbrot 1982) and (Pentland 1986) which were generated by 
adding fractal texture onto the surface of a sphere. 
From this example, we see that the difference between intrinsic models that 

82 
Bayesian Modeling of Uncertainty in Low-Level Vision 
describe visible (retinotopic) surfaces and three-dimensional energy-based mod-
els that describe objects may not be that large. Bayesian modeling may thus serve 
as a common mathematical framework for describing the multiple transforma-
tions which occur in going from images to three-dimensional models. Moreover, 
the uncertainty computed at an earlier stage of processing can be used to derive 
the uncertainty in later estimates (e.g., the uncertainty in the object model shape 
or position parameters can be computed from the probabilistic description of the 
surface). The investigation of appropriate intrinsic models for the intermediate 
level descriptions should thus prove to be an interesting research topic. 
In the end, the difference between mechanical and probabilistic models may 
not be that large, since many probabilistics systems (based on MRFs) have me-
chanical analogues and vice versa. The mechanical approach may be preferable 
for developing energy equations and specifying model parameters. For MAP 
estimation, specially tailored deterministic algorithms-such as those developed 
by Witkin et al. (1987) and Blake and Zisserman (1987)-should perform better 
than general stochastic optimization. On the other hand, the probabilistic ap-
proach enables the development of more sophisticated estimates, including the 
use of different loss functions and the estimation of model parameters. We will 
examine these and other advantages of the probabilistic approach in the next 
three chapters of this book. 

Chapter 5 
Sensor models 
Modeling the error inherent in sensors and using these error models to improve 
performance are becoming increasingly important in computer vision (Matthies 
and Shafer 1987). In the context of the Bayesian estimation framework, sensor 
models form the second major component of our Bayesian model. In this chapter, 
we will examine a number of different sensor models which arise from both 
sparse (symbolic) and dense (iconic) measurements. 
One of the most common measurements used in building visible surface 
representations is the sparse depth measurement, which can be obtained either 
by passive methods such as stereo triangulation or by active methods such as 
laser range finding. In Section 5.1, we will examine how the uncertainty as-
sociated with such a depth measurement can be modeled by the stiffness of a 
spring attached to the surface. In Section 5.2, we will show how the full three-
dimensional uncertainty of the measurement can be incorporated into a sensor 
model. We then turn our attention to dense measurements. In Section 5.3, we 
analyze the uncertainty inherent in a correlation-based flow measurement algo-
rithm. Lastly, in Section 5.4, we present a simple imaging model of a CCD 
camera. 
The four sensor models presented in this chapter are just a few of the many 
sensor models which could be developed. Using techniques similar to the ones 
presented here, we could develop error models for shape from shading, spatio-
temporal velocity filters, and laser range finders, to mention a few. By choosing 
just a few examples, we aim to demonstrate in this chapter the advantages 
of sensor modeling, and to support the usefulness of the Bayesian modeling 
framework. 

84 
Bayesian Modeling of Uncertainty in Low-Level Vision 
5.1 
Sparse data: spring models 
A noisy depth measurement, such as the three-dimensional location of a feature 
obtained by stereo triangulation, can be characterized by a three-dimensional 
probability distribution. Although the shape of this distribution may be quite 
complex, it can often be approximated by a 3-D Gaussian (Matthies and Shafer 
1987). An advantage of using a Gaussian is that the position vector pj = (Xj,Yj, z;) 
and the 3 x 3 covariance matrix Cj completely specify the distribution. 
To determine the interaction between a data point and the visible surface 
which we are building from our depth measurements, we must first convert 
this three-dimensional distribution in space into a one-dimensional distribution 
in depth. Assuming that x and Y are the underlying natural coordinates of our 
visible surface representation, we set dj = Zj and use the probability distribution 
(d,' ) = 
1 
(_ (u(Xj, Yj) - dj)2) 
P 
I U 
PC 
exp 
2 2 
. 
V 27ra Zj 
a Zj 
(5.1) 
If the errors for the various depth measurements are uncorrelated, which is 
usually the case, this leads to the usual data compatibility equation (2.2). When 
our retinotopic representation is not aligned with the sensor reference frame, the 
position and covariance measurements must first be transformed using simple 
matrix algebra (Matthies and Shafer 1987). 
The effect of these depth constraints on the visible surface is similar to 
tying the surface to the depth values through springs (Terzopoulos 1984). The 
strength of the spring constants is inversely proportional to the variance of the 
depth measurements. To convert the analog constraints (5.1) to a discrete form, 
we usually associate the constraint with the nearest node. This introduces a 
quantization error; Terzopoulos (1984) compensates for this error by scaling the 
spring constant by an h-2 factor (where h is the grid spacing). 
An alternative way to derive the discrete form of the data constraint is to 
write out the measurement equation 
dj = Hu + rj, where rj f'V N(O, (7). 
(5.2) 
The measurement matrix H encodes how the surface point u(Xj,Yj) which gives 
rise to the measurement dj is obtained from the nodal variables u; the H ma-
trix thus depends on the choice of interpolator. If we choose block (constant) 
interpolation, then the discrete data constraint equation is as before (i.e., we 
associate the depth constraint with the nearest nodal variable). If we choose 
bilinear interpolation, we have 
u(x,y) = hooujj + hOlujj+i + hlOuj+ij + hlluj+ij+i 
where Ujj,"" Uj+ij+i are the four nodal variables nearest to (x,y) and 1100, ... , hll 
are interpolation constants which depend on x and y. The data constraint equation 

Sensor models 
10 
9 
8 
7 
6 
5 
4 
3 
2 
Method 0, stddev 0,01, error = 0.3484 
Method 0, stddev 1.00, error = 0.2607 
Method I, stddev 0.01, error = 0.3663 
Method I, stddev 1.00, error = 0.0816 
Method 2, stddev 0.01, error = 0.0001 
Method 2, stddev 1.00, error = 0.0000 
85 
O~---+----+----+----r---~--~----~---+----+---~ 
o 
2 
3 
4 
5 
6 
7 
8 
9 
10 
Figure 5.1: Cubic spline fit with different data constraints 
thus becomes 
1 -2 
2 
Ed(d, u) = 20" 
(d -
hooujJ -
h()!UjJ+l -
h1OUj+1J -
h l1uj+l.j+d· 
(5.3) 
This unfortunately introduces off-diagonal terms into our data compatibility ma-
trix Ad, and complicates the implementation of the finite element relaxation. We 
could drop the off-diagonal terms from Ad, but this might introduce flat spots 
into our interpolated surface. 
To explore the effects of these various choices on the quality of the surface 
fit, we can try a simple synthetic example. Figure 5.1 shows a number of cubic 
splines (the one-dimensional versions of a thin plate) which we fit to a set of 
four data points (shown as circles) which were sampled from a straight line. 
For each of the three data constraint methods described below, we fit the cubic 
spline to the data (using two different values of O"j), and calculate the root mean 
squared (RMS) error between the fit and the original line. Method 0 rounds the 
data points to the nearest grid point, and performs better when weaker springs 
(with a larger O"j) are used. Method 1 spreads the data constraints across the 
two neighboring nodes, without adding off-diagonal terms into the information 
matrix A. For small O"j, this approach creates marked flat spots in the solution, 

86 
-8 
-6 
-4 
Bayesian Modeling of Uncertainty in Low-Level Vision 
-2 
6 
5 
4 
o 
2 
4 
6 
8 
(d - u) 
Figure 5.2: Constraint energy for contaminated Gaussian 
but for larger ai, it performs better that Method 1. Method 2 uses the data 
compatibility equation developed in (5.3), and thus obtains the correct straight 
line fit for all values of ai. 
The probability distribution used to characterize the uncertainty in our depth 
measurement need not be Gaussian. The advantage of using a Gaussian is that 
the resulting constraint energy is quadratic. It is therefore easy to manipulate, 
and is characterized completely by its mean and variance values (first and second 
order statistics). A Gaussian distribution is appropriate when the error in the 
measurement is the result of the aggregation of many small random disturbances. 
Many sensors, however, have a normal operating range characterized by a small 
a 2, but also occasionally produce gross errors. A more appropriate model for 
such a sensor is the contaminated Gaussian used by Durrant-Whyte (1987), 
which has the form 
(dl) 
1-f 
(U(Xi,Yi)-di)2) 
f 
(U(Xi,Yi)-di)2) 
P i U = --- exp -
2 
+ --- exp -
2 
Vfffal 
2al 
Vfffa2 
2a2 
(5.4) 
with d ~ ar and 0.05 < f < 0.1. This model behaves as a sensor with small 
variance ar most of the time, but occasionally generates a measurement with a 
large variance a2. By taking the negative logarithm of the probability density 
function, we can obtain the constraint energy shown in Figure 5.2. This energy is 
similar in shape to the weak springs which arise in the weak continuity models of 
Blake and Zisserman (1987) and the <P function of Geman and McClure (1987). 
Gaussians and contaminated Gaussians are just two of the many possible 

Sensor models 
87 
distributions that can be used to characterize sensors. The advantage of using 
a Bayesian approach to visual processing is that any sensor model that we 
develop can be incorporated directly into the estimation algorithm. In practice, 
finding good sensor models involves a tradeoff between the fidelity of the model, 
the compactness of its representation, and the tractability of its equations. We 
will see more examples of this tradeoff in the next section, where we examine 
how to better model the three-dimensional uncertainty inherent in sparse depth 
measurements. 
5.2 Sparse data: force field models 
One of the biggest deficiencies of the spring model that we have just presented 
is that it can only model the uncertainty in position along one dimension. In 
practice, most depth sensors have a full three-dimensional uncertainty associated 
with each position estimate (Matthies and Shafer 1987). Moreover, since the 
spring model ties the data point to exactly one attachment point on the surface, 
it is difficult to use with three dimensional models and parametric surfaces. In 
this section, we will introduce a new depth constraint model which incorpo-
rates three-dimensional uncertainty in point locations and thus overcomes these 
problems. 
The key observation in developing such a model is that a sensed point can 
come from anywhere on the surface (within the limits of the sensor uncertainty). 
For example, if the sensed point Pi = (Xi, Yi, Zi) has a Gaussian distribution 
characterized by a covariance Ci, then the likelihood that this measurement 
resulted from a noisy sensing of the surface point q = (x,y, u(x,y» is 
I 
(I 
T 
i 
) 
p(pdq) = ~ICil exp -2:[Pi - q] Ci [Pi - q] . 
Thus, to compute the likelihood of sensing Pi given a particular surface u(x,y), 
we must integrate over all possible sites from which the point may have come, 
p(Pilu) = J 
~ICil exp ( -~[Pi - q(x,y)fCii[Pi - q(X,y)]) dxdy. 
(5.5) 
The constraint energy corresponding to this probability density function is a 
complicated non-linear function of the data and state variables. To study its 
properties, we will start with a few simple cases. 
The simplest possible case is when we assume that the data came from one 
of the nodal points, i.e., from one of the points Q,j = (Xj,Yh Uj) where (Xj,Yj) are 
the Cartesian coordinates of the nodal variable Uj. In this case, we can replace 
the integral in (5.5) with a discrete summation 
p(pdu) = L giUj) 
j 
(5.6) 

88 
Bayesian Modeling of Uncertainty in Low-Level Vision 
where 
and 
1 
T) 
}j(Uj) = 2[Pi - q;l Ci [Pi - q;l. 
The corresponding constraint energy equation is thus 
Err(Pi, u) = -log L #Uj) 
j 
(5.7) 
which, unlike the models studied in the previous section, cannot be partitioned 
into a sum of local energies involving the data point Pi and the nodal variables 
Uj. 
The energy equation (5.7) is interesting since it acts like a force field, at-
tracting nearby surface points towards the data point, rather than tying the data 
to a particular fixed location. When the surface is intrinsically parameterized, 
i.e., when the three-dimensional surface point locations are functions of some 
intrinsic parameters (Terzopoulos et al. 1987), the energy equation behaves like 
a "slippery spring,") allowing the surface to slide by the data point. This force 
field, derived from statistical considerations, is also qualitatively similar to the 
repulsive "volcano" used by Kass et al. (1988). 
An energy equation similar to (5.7) has been used by Durbin and Willshaw 
(1987) in their algorithm for solving the Traveling Salesman Problem (their 
energy equation was derived heuristically, without any statistical interpretation). 
The form of the energy allows a small elastic net similar to a rubber band to be 
progressively stretched until it passes through all of the cities on the salesman's 
tour. To find the minimum energy solution, Durbin and Willshaw start with a 
large value of Ci (identical and spherically symmetric at each point), and slowly 
reduce its value while relaxing the net. Durbin et al. (1989) provide an analysis 
of the net behavior and its relationship to Bayesian modeling. 
To gain some further insight into the behavior of our new energy constraint, 
we can calculate the instantaneous force that it exerts on the surface points. 
The force exerted on variable Uj can be calculated by differentiating (5.7) with 
respect to Uj 
8Err 
1 
8g· 
g-(u-) 
( 
8t; ) 
Fj = - 8uj = Lkgk(Uk) 8~ = Lk1gk(Uk) 
- 8~j . 
(5.8) 
The first part of this force equation, #Uj)/(Lkgk(Uk», determines how the force 
from the data point is distributed among the surface points Uk. Since gk(Uk) 
corresponds to a Gaussian distribution, the magnitude of the force falls off like 
IThis term was suggested by Alex Pentland. 

Sensor models 
89 
a Gaussian (skewed, of course, by the shape of the covariance matrix Ci). Note 
that there is a complex interaction between all of the points near Pi. If some 
other point Uk moves closer to Pi, the effective force at Uj will be reduced (similar 
observations have been made by Durbin and Willshaw (1987)). 
The second part of the force equation (5.8) is 
where 
If the measurement noise is not skewed (Le., if Ci is diagonal), this force is 
simply proportional to a;;2(zi - Uj), which is the same as the simple spring model 
used in the previous section. If the measurement noise is skewed (non-zero axz 
or ayz), the null point of the spring (ignoring the first part of the energy term) 
can be calculated as 
The interaction between the surface points and a single data point with 3-D 
uncertainty is illustrated in Figure 5.3. We can thus think of this model as 
implementing a distributed spring constraint. 
The non-linear energy equation given by (5.7) can be used as part of a 
surface interpolation or other low-level vision algorithm, and models the three-
dimensional uncertainty associated with the sparse depth measurement, unlike 
the simple spring model presented in the previous section. Assuming that the 
sensor noise is independent for each depth measurement, we can sum (5.7) over 
the points {p;} to obtain the composite energy equation Ed(u, d) (here d stands 
for all of the data {Pi}). To solve the resulting set of regularized equations, 
we can use non-linear gradient descent techniques such as the ones used by 
Terzopoulos (1987) (see also Press et al. (1986)). 
In many applications, however, we cannot afford to keep all of the data 
points Pi and their associated covariance matrices Ci• This is especially true in 
dynamic vision applications, where a single scene or object may be observed 
from many viewpoints. We thus have two choices for maintaining a constant 
amount of data while acquiring more measurements. The first approach, sym-
bolic matching, finds a correspondence between the points {pH seen at time t 
and the points {pj+l} seen at the next interval. The second approach, iconic 
modeling, approximates the complicated energy equation arising from the inter-
action of points described by (5.7) with a simple equation of the form used in 
standard regularization (2.13). 
Symbolic matching is appropriate when the same surface points can be mea-
sured from different viewing directions or tracked over time. This is usually 
the case when the depth measurements result from matching discrete features 

90 
Bayesian Modeling of Uncertainty in Low-Level Vision 
z 
x 
Figure 5.3: Depth constraint with three-dimensional uncertainty 
The effect of data point (Xi, di) on nodal variable Uj is shown as a spring con-
necting Uj to the null position dj • The strength of the spring depends on the 
location of the other neighboring points Uk and the shape of the noise covariance 
(indicated by a gray ellipse). 
using stereo or motion (Matthies and Shafer 1987). Once the correspondence is 
known, we can compute the new updated estimates for the point location and 
covariance using the equations given in (Matthies and Shafer 1987) or (Matthies 
et al. 1987). After a series of n measurement, the variance in the point location 
is reduced by a factor of n in all directions2• 
The problem with symbolic matching is that it is often difficult to establish 
point correspondences. Moreover, certain depth sensors such as laser range 
finders do not even measure the same points when their viewing position changes 
(we will have more to say about this in Section 6.4). In this case, the iconic 
approach is more suitable. The iconic approach also has the advantage that the 
representations and associated algorithms are uniform and parallel, and thus map 
naturally onto massively parallel architectures. 
To incorporate the data constraint (5.7) into an iconic framework (where all 
2If the shape of the covariance matrix changes with viewing direction, an even greater 
reduction can be obtained. 

Sensor models 
91 
quantities are represented as two-dimensional fields), we must approximate this 
equation with one of the form 
(5.9) 
We should choose the Cj so that they encode the strength or stiffness of the 
constraint between Uj and dj. Alternatively, we can view Cj as encoding the 
inverse variance (or information) of the constraint. 
To compute the values for Cj, we can choose either to match the energy 
equations (5.7) and (5.9), or to match the probability densities from which these 
were derived. In this section, we will examine the former approach. A simple 
way to choose a value for Cj is to compute the second partial derivative of Err 
with respect to Uj 
(5.10) 
We can ignore the second term in this expression if ali/ aUj ~ 0, which is true 
in the vicinity of the minimum energy configuration Uj ~ dj• In this case, the 
formula for Cj reduces to 
g-(U-) 
C -
J 1 
a 
j -" () zz· 
L.J/r. g/c U/c 
(5.11) 
We can think of this formula as distributing the depth information available 
from the measurement at point Pi among the nodal variables Uj according to 
the ratio giUj)/ Lkg/r.(U/r.), (Le., with a Gaussian fall-off). Note that the sum of 
all the local information quantities Cj is am the amount of depth information 
available in the original measurement (here we are using the term information 
interchangeably with inverse variance). 
An alternative but closely related way to derive a tractable approximation to 
(5.7) is to use a Taylor series expansion around the current solution point u*, 
Ed(Pi, u) = Eff(U*) + L :E~rl 
(Uj - uj) + L L -21 ;2:rr I (Uj - Uj)(Uk - uk). 
j 
UUJ u' 
j 
/r. 
uuJuU/c u' 
(5.12) 
Note that we only keep the first three terms in the Taylor series expansion. This 
allows us to write the data constraint energy as a quadratic form 
1 T 
T 
Ed(Pi, u) = '2u AiU - bi U + ki 
and to thus incorporate this constraint into our previously developed surface 
interpolation algorithms. We can use higher order terms to measure the skew in 
the local distribution, and to check the validity of our approximation. Expanding 

92 
Bayesian Modeling of Uncertainty in Low-Level Vision 
the solution around u· instead of d (the minimum energy solution of Eff) means 
that we either have to use the full form of Eff(Pi, u) to find the solution, or that 
we have to apply (5.12) iteratively until we converge to such a solution. The 
advantage of expanding around u· is that we can better approximate the behavior 
of our system near its current operating point. 
This approach can in general be applied to any arbitrary constraint energy 
Eff• If the energy equation is too complex to be differentiated analytically, 
the partial derivatives used in (5.12) can be evaluated numerically using small 
perturbations in the solution u·. We can also choose to ignore the off-diagonal 
(mixed partial derivatives) in (5.12), or to only evaluate the terms over limited 
spatial neighborhoods (since these terms denote the cross-correlation between 
adjacent nodal variables). The success of using (5.12) to model our sensor 
depends on the smoothness of the local energy landscape and how well it can be 
locally approximated by a quadratic form. We thus expect the method to work 
well with multivariate Gaussians, but perhaps not as well with uniform bounded 
distributions. The viability of our approach remains to be tested in practice. 
When first developing (5.7), we assumed that the sensed data point came 
from one of the nodal points. This assumption was made mostly for simplicity 
of exposition. In practice, we can evaluate the integral equation (5.5) by choosing 
an interpolation function which maps from the nodal variables u to the surface 
u(x,y). For a three-dimensional Gaussian distribution and a polynomial-based 
interpolator, the energy equation (and also its partial derivatives) can be analyt-
ically evaluated. The resulting equations involve Gaussians and error functions 
(integrals of Gaussians) and are thus quite cumbersome to manipulate. However, 
they yield a better model for the interaction between the data and the surface, 
especially when the point variance is small compared to the finite element grid 
size. In the limit, as the x and y uncertainties approach 0, the behavior of 
the three-dimensional uncertainty model approaches that of the one-dimensional 
model studied in the previous section. 
When the iconic framework is used to integrate information over time, the 
variance in the depth estimate is reduced in proportion to the number of measure-
ments acquired Gust as with symbolic matching). The uncertainty in the (x,y) 
position, which is characterized by the distribution function gj(Uj)/ L-kgk(Uk), 
does not diminish. Thus, symbolic matching is preferable whenever possible, 
since the depth constraint becomes more localized. In those situations where 
such matching is impossible, however, the iconic approach developed in this 
section provides a tractable model of the three-dimensional uncertainty in sparse 
depth measurements. 
The new sensor model which we have developed in this section has several 
interesting characteristics not found in previous models. Our model uses the full 
three-dimensional covariance of the sensor noise, and can easily be extended 
to model any three-dimensional probability distribution. It should therefore be 

Sensor models 
93 
useful in situations where the shape of the uncertainty is highly skewed, such 
as when range data is projected from camera coordinates to terrain coordinates 
(Hebert and Kanade 1988, Hebert et al. 1988). The constraints resulting from 
this sensor model do not tie the data points to any fixed points on the surface and 
are thus ideally suited for full three-dimensional models or parametric surfaces. 
The behavior of this data constraint can be similar to a force field or to a slippery 
spring, depending on the shape and size of the position uncertainty. 
In addition to introducing the new sensor model, we have shown how it can 
be incorporated into our visible surface representation framework using a locally 
quadratic approximation to the energy function. This allows many measurements 
to be integrated over time while keeping the representation uniform and parallel. 
We will use this technique of developing locally quadratic approximations to 
the energy, which is equivalent to approximating the probability density by a 
multivariate Gaussian, at later points in the book. 
5.3 
Dense data: optical flow 
Probabilistic sensor modeling need not be restricted to sparse measurements 
obtained directly from sensors. We can also apply error analysis to low-level 
vision algorithms and characterize these algorithms as virtual sensors with their 
own associated error models. In this section, we will analyze an intensity-based 
optical flow estimator and show how the uncertainty in the flow measurement 
at each point can be determined from local measurements already present in the 
algorithm. The flow field computed in this manner can be used as the input to 
a depth-from-motion algorithm (Section 7.2). The optical flow algorithm is an 
example of a virtual sensor that provides dense measurements; these measure-
ments are similar in form to the visible surface representation into which they 
are integrated. 
The problem of extracting optical flow from a sequence of intensity images 
has been extensively studied in computer vision. Early approaches used the 
ratio of the spatial and temporal image derivatives (Horn and Schunck 1981), 
while more recent approaches have used correlation between images (Anandan 
1984) or spatio-temporal filtering (Heeger 1987). In this section, we analyze a 
simple version of correlation-based matching. This technique, called the Sum of 
Squared Differences (SSD) method by Anandan (1984), integrates the squared 
intensity difference between two shifted images over a small area to obtain an 
error measure 
e,(d; x):;:: J 
w(,X)[f,(x + d + ,X) - It-I (x + ,X)]2 d,X, 
(5.13) 
where It and It-I are the two intensity images, x is the image position, d is 
the displacement (flow) vector, and w('x) is a windowing function. The SSD 

94 
Bayesian Modeling of Uncertainty in Low-Level Vision 
measure is computed at each pixel x for a number of possible flow values d. 
In Anandan's algorithm, a coarse-to-fine technique is used to limit the range of 
possible flow values. In Matthies et al. (1987), a single-resolution algorithm is 
used since the range of possible motions is small due to high temporal sampling 
rates. 
The resulting error surface e,(d; x) is used to detennine the best displacement 
estimate d and the confidence in this estimate. Anandan and Weiss (1985) 
observed that the shape of the error surface differs depending on whether both, 
one or none of the displacement components are known (corresponding to an 
intensity corner, an edge, or a homogeneous area). They proposed an algorithm 
for computing the confidence measures based on the principal curvatures and 
the directions of the principal axes in the vicinity of the error surface minimum. 
Matthies et al. (1987) have shown how for a one-dimensional displacement, 
the variance in the displacement estimate can be computed from the second 
derivative in a parabola fit to the error curve (Lucas (1984) also has a similar 
analysis of gradient-based stereo matching). 
In this section, we extend the result of Matthies et al. (1987) to two dimen-
sions, thus providing a statistical justification for the heuristics developed by 
Anandan and Weiss. We start by fitting a quadratic of the fonn 
e;(d; x) = (d - dl A(d - d) + c 
(5.14) 
to the error surface computed by (5.13). We then set the disparity estimate at 
x to d, and set the variance of this measurement to 2a;A -I, where a; is the 
variance of the white noise process. The inverse covariance matrix A can be 
singular, since it need not be inverted when being used in regularization-based 
smoothing of the flow field (Anandan and Weiss 1985). 
The statistical justification for the above algorithm is given in Appendix 
C. The derivation involves modeling the two images frames It and It-I as 
displaced versions of the same image corrupted with additive white Gaussian 
noise. This simple model does not account for occlusions, disparity gradients or 
other optical effects. It is thus only valid over small windows3, and breaks down 
in certain areas such as at occlusion boundaries. Nevertheless, it has proven to 
be remarkably effective in extract depth from small-motion sequences (Matthies 
et al. 1987). The analysis presented in Appendix C can also be used to derive the 
correlation between adjacent flow estimates and between flow estimates obtained 
from successive frames. These correlations are presently ignored in the algorithm 
developed by Matthies et al. (1987), but they could easily be integrated into an 
on-line estimation framework (Gelb 1974). 
From the results presented in this section, we see how the statistical analysis 
of an optical flow algorithm can provide an error model for its output. This 
3Note how !he variance of estimate is reduced as the window size is increased, but !he 
validity of !he model becomes more suspect. 

Sensor models 
scene 
blur 
~ 
sampling 
~~ 
tj 
white noise 
output 
Figure 5.4: Simple camera model showing blur, sampling and noise 
95 
output can thus be treated as a virtual sensor which can then be incorporated 
into a Bayesian estimation framework. This approach permits us to take into 
account the spatially varying uncertainties which are often inherent in low-level 
visual processes. We expect that similar analysis can be applied to other low-
level vision algorithms with similar benefits. 
5.4 Dense data: image intensities 
The sensor model for optical flow presented in the previous section, when com-
bined with a dynamic Bayesian estimation framework, can provide surprisingly 
good depth estimates from small-motion sequences (Matthies et al. 1987). This 
approach, however, has some problems due mainly to the simplifying assump-
tions used in the flow estimator (these problems are discussed in Section 7.2). 
An alternative to using a separate flow estimation stage is to jointly estimate 
the intensity and flow fields (see Section 7.3). In order to perform this joint 
estimation, we need to develop a sensor model for the CCO camera used as our 
input device. 
The model which we develop in this section, shown in Figure 5.4, has 
three components: blur, sampling, and noise. This model is thus quite simple, 
and does not include many of the known optical (geometric) distortions in the 
imaging process (Gremban et al. 1988). The blur induced by the optical system 
can be modeled by a point spread function g(x, y). The shape of this function 
depends on the lens aperture, the focusing distance, and the distance to the 
object. The pattern of light falling on the CCO array can thus be computed as 
f(x,y) = J(x,y) * g(x,y), 
where J(x, y) is the intensity array determined through the ideal perspective pro-
jection operation, and * denotes convolution. 

96 
Bayesian Modeling of Uncertainty in Low-Level Vision 
The output of one CCD cell can be computed by integrating over the area 
of the cell 
j h/2 jW/2 
Ojj = 
f(iLlx + >",jiJ.y + "') d>" d"" 
-h/2 -w/2 
where wand h are the width and height of the cell, and Llx and iJ.y are the array 
spacing. This integration is equivalent to first convolving f(x, y) with a box 
filter b(x,y), and then sampling the resulting signal. We can model the blur and 
sampling system together by convolving I(x,y) with a new point-spread function 
g'(x,y) = b(x,y) * g(x,y) 
and then sampling the resulting signal. The advantage of this latter approach is 
that we can compute the Fourier transform of g'(x,y), and thereby determine the 
aliasing inherent in the system. 
For a system which performs a regular point-wise (Dirac delta) sampling 
of a filtered image, the amount of aliasing can be determined by examining 
the magnitude of the effective filter response in the areas outside the Nyquist 
frequency (fx,/y) = (1/ Llx, 1/ iJ.y) compared to the magnitude inside (Rosenfeld 
and Kak 1976). Increasing the blur in the optics or using a larger cell size 
w x h will both reduce the aliasing, but at the expense of resolution. In practice, 
aliasing can affect computer vision algorithms such as depth-from-motion in 
several ways. Fine textures may appear to drift in the wrong direction if aliasing 
is present, and intensity edges appear to jump instead of slowly moving across 
the image. As we will see in Section 7.3, aliasing also makes precise (sub-pixel) 
positioning of edges difficult. 
To overcome aliasing, we can de-focus the camera, but this may compound 
the problem of the variation of blur with depth. The best solution would be to 
design CCD sensors with larger sampling areas, or with a uniform blur close to 
the sensor surface. Another possible solution is to introduce micro-jitter in the 
imaging system. By rotating the camera around its optical axis over the range of 
one pixel, we can significantly reduce aliasing while only marginally reducing 
the resolution. 
The final component of our simple camera model is the sensor noise. This 
noise can come both from the electrical noise present in the CCD array and 
from the quantization and distortion effects present in the digitization hardware. 
Using a white noise process results in the simplest possible model and estimation 
equations. More complicated noise models, such as those used by Geman and 
Geman (1984), could also be used within the Bayesian estimation framework. 
To determine the validity of this simple model, we performed some ex-
periments with a Sony XC-37 CCD camera with 16mm lens in the Calibrated 
Imaging Lab at Carnegie Mellon (Szeliski 1988b). We were able to compute 
the horizontal and vertical line spread functions by taking images of horizontal 

Sensor models 
97 
and vertical stripes which were slightly tilted with respect to the raster. The 
profiles obtained by averaging many aligned scan lines were then differentiated 
to obtain the line spread functions. Determining the aliasing effects was more 
difficult. The location of the midpoint of the step profile at each successive line 
did show some oscillation, but it was not sufficiently periodic to develop an 
aliasing model. This difficulty is probably due to the interaction between the 
CCD grid spacing and the digitizer sampling rate, which are different. 
An additional effect that showed up in evaluating the camera model was 
a random horizontal shift of the pixels on successive scan lines. This jitter 
had previously been observed by Matthies et al. (1987) while developing an 
incremental depth-from-motion algorithm. To quantify the magnitude of this 
effect, a reference image was first obtained by averaging many images of the 
same static scene. A new image of the same scene was then taken, and each scan 
line was aligned to sub-pixel precision with the reference image using a least 
squares fit. The average magnitude of the observed jitter was about 0.10 pixels. 
While this jitter is not extreme, it is sufficiently large to affect the precision 
of the (Matthies et al. 1987) depth-from-motion algorithm, where the average 
inter-frame displacement is less than a pixel. 
To reduce the amount of jitter, we can average several images of the same 
scene. A better solution is to use a CCD camera with a direct digital readout 
of each cell (traditional cameras convert the CCD array values to an NTSC 
analog signal). In addition to eliminating the jitter problem, using such a camera 
facilitates the calibration of the imaging sensor (Shafer 1988). The transfer 
function, blur function and aliasing of each cell can be measured separately. An 
alternative to this calibration procedure would be to enhance the CCD sensor with 
on-board circuitry that could "learn" to compensate for optical and geometric 
distortions by first being trained on a set of calibration data4• 
The development of a sensor model for image acquisition hardware thus 
serves two important functions. First, problems that are detected with the camera 
can often be compensated for by using pre-processing. Second, problems or 
uncertainties that cannot be eliminated can be modeled, and these models can 
be used by later stages of processing. In general, probabilistic modeling of 
sensors and low-level vision algorithms can greatly increase the accuracy of later 
processing stages and facilitate the optimal integration of information. Several 
examples of the benefits of this approach will be explored in the next chapter. 
4This procedure could be analogous to the learning that occurs in the early visual path-
ways during childhood development. On-chip visual computation is rapidly becoming feasible 
(Sivilotti et al. 1987, Mead and Mahowald 1988). 

Chapter 6 
Posterior estimates 
In the previous two chapters, we have developed a prior model for visible sur-
faces and a variety of sensor models for the inputs to low-level vision algorithms. 
In this chapter, we will see how the prior and sensor models can be combined 
using Bayes' Rule to obtain a posterior model. We will study how to compute 
optimal estimates of the visible surface from the posterior distribution. We will 
also show to calculate from this distribution the uncertainty inherent in a visible 
surface estimate, and discuss why such uncertainty modeling is important. Two 
novel algorithms which are based on the probabilistic posterior model will then 
be presented. The first algorithm estimates the regularization parameter A from 
the sensed data using a maximum likelihood approach. The second algorithm 
estimates observer motion by matching sparse range data without using corre-
spondence. These two algorithms illustrate the advantages of using a Bayesian 
approach to low-level vision. 
6.1 
MAP estimation 
The probabilistic prior models and sensor models which we have studied in 
the previous two chapters are instances of Markov Random Fields. From the 
results which we obtained in Section 3.2, we know in this case that the posterior 
distribution is itself a MRF. This field can be described by a Gibbs distribution 
with an associated energy 
(6.1) 
where Ep(u) is the energy function associated with the prior model, and Ed(u, d) 
is the energy function that describes the sensor model. Computing the Maximum 
A Posteriori estimate is thus equivalent to minimizing the energy E(u). 
Several techniques can be used for performing this minimization, depending 
on the application. For surface interpolation or optical flow smoothing, the 

100 
Bayesian Modeling of Uncertainty in Low-Level Vision 
energy functions Ep(u), Ed(u, d), and hence E(u) are quadratic. Performing the 
minimization is thus equivalent to solving a large set of sparse linear equations. 
As discussed in Section 2.2, we can use one of several relaxation techniques 
to find the minimum energy solution. The advantage of using such iterative 
techniques over direct solution methods is that they can be implemented on 
massively parallel architectures with local connectivity. Terzopoulos (1984), 
Choi (1987), Blake and Zisserman (1987), and Szeliski (1989) present a number 
of different relaxation-based surface interpolation algorithms. 
For stereo matching, the energy function being minimized has many local 
minima, so some search technique must be used. The most general (domain 
independent) approach is to use simulated annealing (Marroquin 1985, Szeliski 
1986, Barnard 1989), which is a natural extension of the Gibbs Sampler algo-
rithm discussed in Section 3.2. Alternative approaches include dynamic pro-
gramming (Ohta and Kanade 1985) and scale space continuation (Witkin et al. 
1987). Determining the location of discontinuities in the surface representation 
while performing surface interpolation also involves a non-convex energy min-
imization. A variety of continuation methods have been developed to perform 
this localization (Terzopoulos 1984, Koch et al. 1986, Blake and Zisserman 
1987), and simulated annealing has also been used (Marroquin 1984). 
The MAP estimate, however, is not the only estimate that can be computed 
from the posterior distribution p(uld). As has been observed by Marroquin 
(1985), any loss function L(u, u*) can be used to define the optimal estimate. 
Given such a loss function, the optimal estimate u* is the one that minimizes 
the expectation of loss 
(L) = J 
L(u, u*)p(uld) duo 
For MAP estimation, the loss function is a negative delta (we "win" one for 
guessing the correct estimate, but all other estimates are equally bad). A more 
sensible loss function for the terrain classification problem that we examined in 
Chapter 3 would be one which counts the number of misclassified pixels. This 
leads to the Maximizer of Posterior Marginals (MPM) estimator (Marroquin 
1985)1. For many applications, we can also choose to compute the Minimum 
Mean Squared Error (MMSE) estimate2• 
The advantage of using loss functions to define the optimal estimate is that we 
can tailor the loss function to our particular application. In addition to allowing 
the development of task specific algorithms, this approach allows some top-
down influence to be exerted on the low-level process. For example, in a robot 
navigation application where we want to avoid hitting obstacles, we can use a 
loss function that penalizes overestimates in distance more than underestimates. 
ITbe MPM algorithm was used to generate the result shown in Figure 3.3d. 
2For quadratic energy functions, the MAP and MMSE estimates are identical. 

Posterior estimates 
101 
Using different loss functions can increase the power of probabilistic methods 
over simple energy minimization approaches. Having a single optimal estimate, 
however, still does not tell us how certain, accurate, or typical such an estimate 
might be. Ideally, we would like to pass the whole probability distribution on 
to the next level of processing. The Boltzmann machine (Hinton and Sejnowski 
1983) is an example of a system that provides such a distribution. In practice, 
however, we often have to restrict ourselves to a more parsimonious description. 
This forms the subject of our next section. 
6.2 Uncertainty estimation 
To characterize the uncertainty inherent in the output of a low-level vision algo-
rithm, we can compute the second order statistics (covariance) of the estimate. 
These uncertainty estimates can be used to integrate new data, to guide search 
(set disparity limits in stereo matching), or to indicate where more sensing is 
required. For many distributions, second order statistics do not capture all of the 
useful information present in the distribution, but they are a good start. In this 
section, we will examine how uncertainty can be derived from the energy func-
tion characterizing the posterior distribution, and present two new algorithms for 
computing this uncertainty. 
When we combine the regularization-based prior models developed in Sec-
tion 4.1 with the simple sensor models developed in Section 5.1, we obtain 
posterior models that are Markov Random Fields with quadratic energy func-
tions. This energy can be re-written in the form given in (2.16), i.e., 
1 
E(u) = '2(u - U*)T A(u - u*) + k, 
(6.2) 
where u* is the minimum energy solution. The Gibbs distribution corresponding 
to this quadratic form is a multivariate Gaussian with mean u· and covariance 
A -1. Thus, to obtain the covariance matrix, we need only invert the A matrix. 
One way of doing this is to use the multigrid algorithm presented in Section 
2.2 to calculate the covariance matrix one row at a time. To obtain a single 
covariance field, we set b = ej in (2.14), Le., we set all but one of the data 
points to 0 without modifying A, and solve as before. 
Figures 6.1a and 6.1b show two covariance fields, one for the point in the 
lower right comer, and one for the point in the center. These fields are identical 
in shape (but not in magnitude) to the Green's functions (blending functions or 
shift-variant filters) that can be used to solve the interpolation problem. Their 
shape does not depend on the data values dj, but only on the smoothing function 
(prior model) and the data confidence measures Cj. Intuitively, these covariance 
fields show how the overall surface would wiggle if one particular Doint Wll~ 

102 
Bayesian Modeling of Uncertainty in Low-Level Vision 
(a) 
(b) 
Figure 6.1: Sample covariance and variance fields 
(a) and (b) sample covariance fields (c) variance field 

Posterior estimates 
103 
moved up and down. For the special case of isotropic (shift-invariant) smoothing, 
the Green's function is equivalent to the smoothing filter hs(x) that is derived in 
Appendix B. 
Storing all of the covariance fields is impractical because of their large size 
(for a 512 x 512 image, the covariance matrix has over 1010 entries). We can, 
however, keep only the variance at each point, i.e., the diagonal elements of 
the covariance matrix. These variance values are an estimate of the confidence 
associated with each point in the regularized solution (e.g., the residual uncer-
tainty in optical flow after smoothing). Alternatively, they can be viewed as the 
amount of fluctuation of a point in the Markov Random Field (in the ensemble of 
typical fractal solutions). Figure 6.1c shows the variance estimate corresponding 
to the regularized solution of Figure 2.2c (this variance has been magnified for 
easier interpretation). The variance of the field increases near the edges and 
discontinuities; this is as expected, if we interpret the variance as the wobble 
or inverse global stiffness of the thin plate. We can thus develop a dense error 
model for the visible surface representation. Error modeling has not previously 
been applied to systems with such a large number of parameters. 
Calculating the variance field using the above deterministic algorithm re-
quires re-solving the "system for each point in the field and is thus very time 
consuming. An alternative to this is to run the multigrid Gibbs Sampler at a 
non-zero temperature, and to estimate the desired statistics using a Monte Carlo 
approach. For example, we can estimate the variance at each point (the diag-
onal of the covariance matrix) by simply keeping a running total of the depth 
values and their squares. Unfortunately, the straightforward application of the 
Gibbs Sampler results in estimates that are biased or take extremely long to 
converge. This is because the Gibbs Sampler is a multidimensional version of 
the Markov random walk, so that successive samples are highly correlated, and 
time averages are ergodic only over very long time scales (even if the system 
is already at equilibrium). To help decorrelate the signal, we can use successive 
coarse-to-fine iterations and only gather a few statistics at the fine level each 
time. Examples of the variance field estimates obtained with such a stochastic 
algorithm are shown in Figure 6.2. 
The stochastic estimation technique can also be used with systems that have 
non-quadratic (and non-convex) energy functions. In this case, the mean and 
covariance are not sufficient to completely characterize the distribution, but they 
can still be estimated. For stereo matching, once the best match has been found 
(say by using simulated annealing), it may still be useful to estimate the variance 
in the depth values. Alternatively, stochastic estimation may be used to provide 
a whole distribution of possible solutions, perhaps to be disambiguated by a 
higher level process. 
Once we have calculated the variance field, we can use it to grow a con-
fidence region around the mean or minimum energy solution. This confidence 

104 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Figure 6.2: Stochastic estimates of covariance and variance 
(a) multigrid, 1000 iterations (b) multigrid, 2500 iterations (c) single level, 
1000 iterations 

Posterior estimates 
10 
9 
8 
7 
6 
5 
4 
3 
2 
1 
... ... 
:\. 
\ 
\ 
... , , , 
\ 
\ , , 
I 
I 
, 
I 
... ... 
I 
... -
I 
'l 
... 
' .... ",,-, 
,," ... 
---.",. 
\ 
\ 
\ 
O~~~~--~-r--+--+--+-~--;-~ 
o 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
Figure 6.3: Cubic spline with confidence interval 
105 
region can be used in applications such as path planning or navigation. For 
example, we can use a 95% confidence interval if we wish to be "95% certain" 
of not hitting the surface. We can also look at the size of the confidence region 
(which is related to the local variance) to decide where additional active sensing 
may be required. Figure 6.3 shows a one-dimensional example of such a confi-
dence region built around a cubic spline solution. Note that, just as for the thin 
plate, the uncertainty grows as we extrapolate away from known data. 
The modeling of uncertainty in the visible surface representation is impor-
tant if we are using this representation to aggregate data. In Section 6.4 we will 
examine the aggregation of sparse data taken from different viewpoints, while in 
Section 7.2 we will study the aggregation of dense depth measurements obtained 
from optical flow. Uncertainty modeling is also important if we will be matching 
the visible surface to models from an object database or to other intermediate 
representations. In Section 6.4 we will show how including such uncertainty 
modeling is essential to developing a surface-to-surface matching algorithm that 
can handle occlusions, limited areas of overlap, and sparse data. When matching 
to a model database, more accurate results may sometimes be obtained by di-
rectly matching the sensed points to the candidate model. However, it may often 
be easier to match the model to a dense intermediate representation, especially 
if a scale space approach (Witkin et al. 1987) is used to smooth the surface, and 
hence smooth the energy landscape of the matching algorithm. 
The uncertainty representation scheme developed in this section differs in 

106 
Bayesian Modeling of Uncertainty in Low-Level Vision 
several important ways from previously developed representations. The spatial 
likelihood map developed by Christ (1987) uses spherical coordinates to repre-
sent the likelihood of a surface patch at a particular location. This method does 
not have an explicit smoothness constraint; instead, it uses a local planar model 
to extrapolate the surface away from known data points. 
Occupancy maps (Elfes and Matthies 1987, Moravec 1988) use a two- or 
three-dimensional array of scalar values to indicate the occupancy of different 
portions of space. This method is well suited to path planning, and can represent 
three-dimensional objects and obstacles. However, the resolution of the method 
is limited to the grid size. Acquiring more samples of a surface cannot be used 
to improve the estimate of the distance to the surface, since the scalar value in 
each cell is used to indicate the probability of occupancy rather than the surface 
location. 
Finally, the approach used by Wahba (1983) to obtain confidence intervals on 
splines is similar to ours. However, Wahba's method obtains a single variance 
estimate for the whole curve, rather than having a spatially varying variance. 
This method thus fails to capture some importance characteristics of the un-
certainty, such as the increase in variance as we extrapolate away from known 
data. 
In summary, we have shown how the second order statistics of the posterior 
estimate can be obtained by inverting the stiffness matrix A. Two new algorithms 
have been presented to perform this inversion, the first being deterministic and 
sequential, the second being stochastic and parallel. We have also given some 
examples of how these uncertainty estimates can be used in later stages of visual 
processing. Additional examples will be given in this chapter and the next. 
6.3 
Regularization parameter estimation 
One of the recurring problems associated with regularization and other energy-
based estimation techniques is the need to select good values for the global 
parameters that control the algorithm. Some progress has been made in this 
direction (Craven and Wahba 1979), but mostly these parameters are still adjusted 
by hand. The advantage of using a Bayesian approach to low-level vision is that 
the unknown parameters can often be derived from knowledge of the problem 
domain or from the data itself. In Chapter 5, we saw how certain parameters 
can be computed from the noise characteristics of our sensors. In this section, 
we will examine how we can use the sensed data itself to estimate the remaining 
parameters. 
Let us start by examining how certain global parameters affect the shape of 
the most likely and typical solutions of the surface interpolation problem. One 

Posterior estimates 
107 
possible parameterization for a regularization-based posterior estimate is 
p(uld) <X exp -[Ed(u, d) + AEp(u)]jT 
(6.3) 
where A controls the amount of smoothing, while T controls the roughness 
(amount of noise) in a typical sample from the MRF. Note that 1fT relates to 
the confidence ascribed to the data points, while A/T encodes the variance in 
the prior model. Thus, if we have low confidence in the data, and wish the 
typical solution to be smooth, we use a high A and a low T (Figure 6.4a); in the 
limit, this solution will approach a plane for the thin plate model. If we have 
a high confidence in the data, but still wish a typical solution to be smooth, 
we can use a low A and a low T (Figure 6.4c); in the limit, this solution will 
approach an interpolating spline. Using higher T values does not affect the 
minimum variance solution, but typical surfaces have either more "crinkles" or 
more "wobble" (Figures 6.4b and 6.4d). 
If the measurement noise can be determined from knowledge about the sen-
sors, we can use the alternative parameterization 
(6.4) 
where a; simply encodes the overall variance in the prior model. While ap 
can be set on the basis of some prior knowledge about the application domain, 
it can also be determined from the data using maximum likelihood estimation. 
Intuitively, if a p is very low, then typical surfaces are extremely flat or planar, 
and it is unlikely that the given (non-flat) data sample would actually occur. 
Similarly, if typical surfaces are very rough, then the probability of a given data 
sample occurring becomes small. There exists some optimal value of a p that 
maximizes the probability p(d) of actually having observed the given data. 
To compute this probability, we will use the notation described in Appendix 
D which is derived from estimation theory. We start by modeling the prior as a 
multivariate Gaussian 
u",N(O,Po) 
with 
Pol=a;2Ap 
(ap2 is equivalent to the regularization parameter A). For our sensor model, we 
use the measurement matrix H to map from the surface u to the data points d, 
and assume that the measurement noise is Gaussian 
d = Hu + r, with r", N(O, R). 
The marginal distribution for d can be computed as 

108 
Bayesian Modeling of Uncertainty in Low-Level Vision 
(a) 
Figure 6.4: Typical solutions for various ('\, T) settings 
(a) high ,\ low T (b) high ,\ high T (c) low ,\ low T (d) low ,\ high T 

Posterior estimates 
109 
The probability of observing the data p(d) is an implicit function of O"p. To 
obtain the maximum likelihood estimate of O"p, we simply maximize the value of 
p(d) with respect to this parameter. Unfortunately, (6.5) is difficult to evaluate 
since it requires computing the whole covariance field of the prior model Po. 
Instead, we can use the simpler form derived in Appendix 0, which allows us 
to write the negative logarithm of p(d) as an energy function 
E(d) = -logp(d) = EI(d) + E2(d) 
(6.6) 
where EI(d) is the logarithm of the partition function and E2(d) is the energy 
(quadratic form) associated with the Gaussian. From (0.15), we have 
1 
1 
1 
EI(d) = 2 log 10";2Ap + HTR-IHI- 2 log 127rR-II- 2 log 10";2Apl, 
(6.7) 
and from (0.17) and (0.19), we have 
1 TR I 
H" 
E2(d) = 2d 
- (d -
UI) 
(6.8) 
1 
-2 
(d 
H " )TR-I(d 
H") 
O"p "Tp-I" 
= 2 -
UI 
-
UI + TUI 0 UI· 
(6.9) 
The log partition function EI (d) is minimized as O"p -T 0 where its value 
approaches log 127rRI. Note that for a membrane or a thin plate, Ap is singular, 
so we must add a small diagonal element €I to this matrix when evaluating E\ (d). 
The energy function E2(d), on the other hand, decreases as O"p -T 00, since in 
this case u* -T d as we approach the interpolated solution. Note that this energy 
can be written in two different ways (additional forms are shown in Appendix 
D). The first form, which is easier to compute, simply measures the weighted 
dot product of the data points and the residuals to the interpolated surface. The 
second form shows how this energy consists of a data compatibility term and a 
smoothness constraint, both of which are evaluated with respect to the minimum 
energy solution. 
For a one-dimensional spline, the calculation of the matrix determinants is 
easy because the matrices are pentadiagonal (banded). This computation can be 
performed simultaneously with the solution of the pentadiagonal system. Con-
sider the set of data points with known confidence intervals (standard deviations) 
shown in Figure 6.5. The cubic smoothing splines for various O"p settings are 
shown in the same figure. For this example, we can easily evaluate the terms 
in (6.6) to obtain the log probability as a function of O"p as shown in Figure 6.6. 
From these curves, we see that the maximum likelihood estimate of a p is about 
5. 
For two-dimensional interpolators, the calculation of the determinants is not 
as simple. We could use a direct algorithm to compute the determinant, but we 

110 
0... 
bl) 
.9 
10 
9 
8 
7 
6 
5 
4 
3 
2 
1 
Bayesian Modeling of Uncertainty in Low-Level Vision 
'-I 
' 
\ 
I 
\ 
I 
\ 
I 
•• , _'. \ 
I •• -
... \ 
~. 
---t 
s_p = 0.1 
s_p = 1.0 
s_p = 10.0 
O~ 
__ ~-+ __ ~ 
__ ~-+ __ ~ 
__ ~-+ __ ~~ 
30 
25 
20 
15 
10 
5 
0 
o 
1 
2 
3 
4 
5 
6 
7 
8· 9 
10 
Figure 6.5: Family of splines of varying smoothness 
E(d) 
E_l(d) 
E_2(d) 
_5.0 6 
.062 
0.25 
1.0 
4.0 
16.0 
64.0 
256. 
s_p 
Figure 6.6: Maximum likelihood estimate of O"p 

Posterior estimates 
111 
would then have to keep intennediate results which are on the order of several 
image scan lines. An alternative is to analyze the asymptotic behavior of the 
matrix detenninant as O'p ----+ 0 and O'p ----+ 00, and to use these to approximate 
(he true detenninant. 
When the data being smoothed is dense and unifonn, we can express the 
probability distributions in the frequency domain. The sensed data d is then the 
sum of two Gaussian noise fields: the prior model, whose power spectrum is 
0';IHp(t)1-2, and the sensor noise, whose power spectrum is O'J. The probability 
of the signal d(x), whose Fourier transfonn is D(t) is thus 
1 (1 J 
ID(t)12 
) 
p(D(t)) = Z exp -2 
0'~IHp(t)1-2 + O'J 
df 
with 
10gZ = -~ J 
10g(0';IHp(t) 1-2 + O'J)df. 
We can use a maximum likelihood technique to estimate O'p and O'd, or we 
can just match the observed power spectrum ID(t)12 with the expected spectrum 
0';IHp(t)1-2+O'J. The ability to compute the sensor noise and prior model variance 
simultaneously depends on the ability to separate out two noise processes with 
different spectral shapes (the prior model is pink or smooth, while the sensor 
noise is white or rough). This same method can also be used to estimate the 
fractal dimension of the surface in a manner similar to that proposed by Pentland 
(1984). 
The maximum likelihood approach described in this section is just one pos-
sible method for estimating the desired degree of smoothing. The generalized 
cross-validation method developed by Craven and Wahba (1979) involves min-
imizing the distance between each data point di and the spline approximation 
fit to the remaining data points. For each sample value of A, n spline fits must 
be calculated, where n is the number of data points; by contrast, our approach 
only requires a single fit. Spectral analysis has been used by Anderssen and 
Bloomfield (1974) to compute the optimal degree of smoothing. Their method 
estimates the prior model (signal) spectrum and noise spectrum from the spec-
trum of the data, and can thus only be used with regularly spaced measurements. 
Process whitening (Maybeck 1982, Marroquin 1984) chooses a smoothing model 
that makes the residuals between the data points and the surface as uncorrelated 
as possible. This approach, however, requires a large number of data points be-
fore its statistical assumptions can be justified. Geman and McClure (1987) use 
the expected value of the energy of the posterior estimate to calculate global pa-
rameter values. The exact theoretical connection between our new approach and 
these various methods and their relative perfonnance remains to be investigated. 
Two other intriguing possibilities associated with parameter estimation are 
learning and local smoothness estimation. Learning algorithms have recently 

112 
Bayesian Modeling of Uncertainty in Low-Level Vision 
had great success in teaching "neural nets" to associate input/output patterns 
(Ackley et al. 1985, Rumelhart et al. 1986) or even to perform simple visual tasks 
(Lehky and Sejnowski 1988). Since the amount of smoothing is characterized 
by a small number of global parameters, these should in theory be learnable; 
this would involve showing the surface interpolation system a sufficient number 
of examples of sparse input data and dense output solutions. The idea of locally 
estimating the degree of smoothness is suggested by the work of Pentland (1984), 
where the fractal dimension of the image is used to segment a scene. The 
resulting interpolator should behave similarly to the locally weighted bicubic 
splines studied by Foley (1987). 
The Bayesian modeling approach to surface interpolation thus allows us to 
estimate the model parameters directly from the sampled data. The method 
which we have developed in this section estimates A by finding the value that 
maximizes the likelihood of observing the given data d. The negative logarithm 
of this likelihood, which we treat as an energy function, measures both the 
overall uncertainty in the model, which is encoded in the log determinants, and 
the fit between the data and the surface. The minimum of this energy function 
corresponds to the maximum likelihood estimate of A. In principle, this same 
approach can be extended to estimate other model parameters, such as fractal 
dimension. 
6.4 Motion estimation without correspondence 
The probabilistic framework developed previously shows how a sparse set of 
measurements can be converted into a dense iconic map, and how the uncertainty 
in this map can be modeled and estimated. This same framework can be used 
to solve an extended version of the motion estimation problem: given two sets 
of points that come from the same surface but from different viewing directions, 
what is the most likely coordinate transformation between the two sets? This 
question is important in robot navigation and manipulation applications where 
the motion of the observer or object is to be determined. 
Traditionally, motion estimation and pose determination problems have as-
sumed that a correspondence is given or computable between the two sets of 
points to be matched (Ullman 1979, Webb and Aggarwal 1981). The prob-
lem is then to find a transformation T(p, e) such that the distance between the 
transformed points and the original points is minimized (Tsai and Huang 1984, 
Faugeras and Hebert 1987). The new method presented in this section shows 
how to estimate this transformation even when no such correspondence exists. 
The two point sets can have a different number of points and limited areas of 
overlap. The approach is thus well suited for use with laser range finders or 
other active range sensors that do not sample the same points from different 

Posterior estimates 
113 
viewing position. It is also particularly well suited for terrain maps, since it can 
handle data points that are irregularly spaced (from perspective de-projection), 
and can incorporate prior knowledge from cartographic data. A more detailed 
description of the new algorithm is given in (Szeliski 1988a). 
In describing our algorithm, we will use the same notation as in the previous 
section, except that we will subscript the data points dk to indicate which set (or 
sensor position) they came from. Similarly, we will subscript the interpolated 
estimate Uk to indicate the best estimate after k data sets have been processed, 
and Pk to indicate its associated covariance3• The smoothness of the surface u 
is characterized by its prior covariance matrix Po, 
u '" N(O, Po), 
and the data points are derived as some linear function of the surface corrupted 
by additive Gaussian noise 
The measurement matrix Hk is used to convert the dense map u into a sparse 
measurement dk• In general, the measurement vector dk and the measurement 
matrix Hk depend (perhaps non-linearly) on some coordinate transformation pa-
rameter vector 8 k• For now, we will assume that dl and HI are known, and that 
only the second set of measurements d2(8) and measurement matrix H2(8) are 
parameterized. 
A simple approach for determining the motion parameters would be to in-
terpolate the first set of data and to then measure the distance between the new 
set of points and the interpolated solution. We start by assuming that the first 
set of data points PI is registered with respept to the world coordinate frame. 
We then form an interpolated surface u(x,y) by splitting each point into its lo-
cation (Xi,Yi), which is incorporated into the HI matrix, and its depth value di, 
which becomes part of d l . The variance assigned to each point is obtained by 
projecting the three-dimensional covariance matrix Ci onto the z-axis4• 
The second set of points P2 is obtained by taking the sensor-based set of depth 
measurements P2 and transforming them through a geometric transformation T 
which is parameterized by 8 
P2 = T(p~, 8). 
These transformed points are then used to derive H2, R2 and d2, which are now 
all functions of 8. The exact form of 8 is not important here since we are not 
3The subscript k corresponds to the discretized time variable used in Kalman filtering (Section 
7.1). 
4 Another alternative is to use the force-field constraints developed in Section 5.2. 

114 
Bayesian Modeling of Uncertainty in Low-Level Vision 
trying to obtain a closed-fonn solution. In this paper, we will use the translation 
vector (tx, ty, tz) and three rotation angles (9;x, Oy, Oz) as our parameters (see Tsai 
and Huang (1984) and Faugeras and Hebert (1987) for alternative fonnulations). 
Assuming that the second set of data came from the interpolated surface u" 
we can calculate the probability of the measurement vector d2 being observed 
as 
p(d2IB) = 127rR21-'/2 exp ( -~(d2 - H2u,lR2"'(d2 - H2U,») . 
(6.10) 
We can find the maximum likelihood solution for B by maximizing the above 
equation, or equivalently, minimizing the negative log likelihood 
Es(d2) = -logp(d2IB) = ~ log 127rR21 + ~(d2 - H2u,lR2"'(d2 - H2u,) (6.11) 
(the subscript's' stands for "simple"). In general, (6.11) does not have a closed-
fonn solution for the minimum energy transfonnation (the vertical translation 
tz is the exception). This energy equation must therefore be minimized using 
non-linear optimization techniques such as gradient descent (Press et al. 1986). 
Unfortunately, using the above energy equation as the basis of our motion 
estimator has several problems. The matching of new data points to the extrap-
olated parts of the surface is inaccurate, since little is known about the surface 
in these areas. This is symptomatic of the more general problem with this tech-
nique, which is that it does not incorporate any knowledge about the uncertainty 
in the original interpolated surface. For example, range data will often have 
"shadowed" areas where the extrapolated data can be extremely uncertain. To 
overcome these problems, we must go back to the original Bayesian fonnulation 
and derive an optimal motion estimator. 
To compute the optimal estimate of the motion, we find the value of B 
that makes it most likely that the two sets of data points p, and P2 came from 
the same smooth surface. To compute the likelihood of observing the depth 
values d2 (through the measurement matrix H2), we note that the distribution 
u, '" N(u" PI) completely describes what is known about the smooth surface 
after the first set of data points has been incorporated. The second set of data 
points d2 must thus be drawn from the distribution 
d2 '" N(H2u" H2P,H~ + R2) 
(this same result can be obtained by writing down the joint probability function 
p(u, d" d2) and calculating the conditional probability P(d2Id,». The negative 
log likelihood function in this case is 
Eo(d2) = ~ log 127r(H2P,HI + R2)1 + ~(d2 - H2u, l(H2P, HI + R2)-'(d2 - H2u,) 
(6.12) 

Posterior estimates 
115 
(the subscript '0' stands for "optimal"). Unfortunately, (6.12) is difficult to eval-
uate since it involves the covariance matrix PI, which is not sparse. However, 
using some algebraic manipulation (Appendix D), we can simplify the formula 
for the log likelihood to obtain 
(6.13) 
where 
(6.14) 
and 
(6.15) 
The first component of the energy, EI , measures the reduction in likelihood due 
to the sensor noise as traded off against the increase in posterior information. In 
practice, this component of the energy varies fairly slowly with the transforma-
tion parameter e and can usually be ignored. The second part of the energy, E2, 
measures the distance between the new data points d2 and the surfaces UI and 
th. Note how (6.15) is similar to (6.11) except that one side of the quadratic 
form now involves the new surface estimate. Points that lie closer to the new 
surface estimate U2 than to the old estimate UI are thus penalized less by the op-
timal energy measure. In this way, areas where the surface values are originally 
uncertain (because the data is uncertain, the area is shadowed, or the surface is 
being extrapolated) contribute less to the matching criterion. 
A related energy measure which could be used instead of E2 as a matching 
criterion is 
E3(d2) = ~urpolU2 + ~ t(di - HiU2)TR;-I(di - HiU2) 
1=1 
(6.16) 
which is derived from the joint probability density function of the data and the 
surface (Appendix D). This new energy clearly shows the symmetry of the 
formula with respect to data set ordering. This alternate form also has a simple 
interpretation as the weighted distance between the data points and the optimal 
surface estimate (spring energy) plus the smoothness value of the surface (strain 
energy). The determination of an optimal motion estimate is thus equivalent to 
minimizing the energy of the composite surface and spring system. 
The implementation of the optimal motion estimator is somewhat more com-
plicated than simply matching the new data points d2 to the old surface estimate 
th. This is because the optimal surface estimate U2 must be re-computed each 
time a new transformation e is generated. Fortunately, if successive transfor-
mations are close, the new surface estimate can be obtained from the previous 
estimate using only a few relaxation iterations. More importantly, since U2 ac-
tually corresponds to the minimum energy solution of (6.16), we can jointly 

116 
Bayesian Modeling of Uncertainty in Low-Level Vision 
optimize ih and fJ using a continuation method similar to that used by Witkin 
et al. (1987). 
The method which we have just described computes an optimal motion es-
timate by finding the transformation that minimizes an energy (negative log 
likelihood) function. This motion estimate is itself uncertain, i.e. it has a vari-
ance that can be determined from the shape of the error surface. We can justify 
estimating the distribution of the transformation fJ from the likelihood of d2 
using Bayes' Rule, 
(fJld ) = p(d2IfJ)p(fJ) 
(d IfJ) 
p 
2 
p(d2) 
ex. P 
2 
if the prior distribution of the transformation p(fJ) is uniform. We can thus 
estimate a complete distribution for p(fJld2) by simply normalizing p(d2IfJ), 
which is the negative exponential of our energy function Eo(d2)' This distribution 
can encode multiple hypotheses about fJ by having multiple humps. 
In practice, estimating a complete distribution for fJ may be impractical. 
Instead, we can take the optimal motion estimate B and augment it with a co-
variance matrix Ceo One way of calculating Ce is to first sample the complete 
distribution p(fJld2) and to then compute its first and second moments. A sim-
pler, though potentially less accurate method is to simply fit a multi-dimensional 
parabola (second order polynomial) to the energy surface (see Szeliski (1988a) 
for details). The variance estimate thus obtained can be used to integrate the 
motion estimate provided by our new algorithm with other motion estimates, 
such as those provided by dead reckoning or inertial navigation systems. 
The scheme we have just developed for estimating the motion between two 
frames can easily be extended to estimate a whole sequence of motions. The 
formulas given in (6.14) and (6.15) can be converted into a recursive form by 
simply substituting k for the subscript 2 and k - 1 for the subscript 1. After each 
motion Bk is computed, the new surface estimate Uk and its inverse covariance 
p;1 are updated (we could also use the recursive formulas given in (0.9) and 
(0.10». The Bayesian scheme which we have developed can also be used to 
incorporate domain-specific prior knowledge, such as cartographic information. 
The easiest way to do this is to introduce a phantom range observation before 
the first real set of data. 
The method presented in this section has been tested on a number of noisy 
synthetic range images. Details of these experiments are reported in (Szeliski 
1988a). To illustrate the behavior of our algorithm, we will present one simple 
example in this section. Figure 6.7a shows a very simple "blocks world" scene 
which has been sampled from two different directions (Figures 6.7b and 6.7c). 
The interpolated solution from the first set (Figure 6.8a) is quite inaccurate in 
the shadowed area behind the block, and yet the motion estimate obtained from 

Posterior estimates 
117 
Figure 6.7: "Blocks world" synthetic range data 
(a) synthetic scene (b) sparse range data from first viewing position (c) sparse 
range data from second viewing position 

118 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Figure 6.8: Interpolated surfaces from sparse block data 
(a) from first set of data (b) from both sets of data 

Posterior estimates 
119 
0.8000 
0.8005 
0.9100 
-0.1000 
-0.1000 
-0.1100 
(9*= 
-0.1000 
B -
-0.1000 
Bs = 
-0.9980 
-0.0952 
0-
-0.0864 
-0.1002 
-0.1000 
-0.0975 
-0.0950 
-1.6345 
-1.6320 
-1.6345 
Figure 6.9: Motion estimate for blocks world data 
The true motion is shown on the left, and the optimal estimate in the middle, 
and the simple method estimate on the right. 
this data is still very good (Figure 6.9). The final surface reconstructed from the 
combined data is quite reasonable (Figure 6.8b). 
We can compare this motion estimate to that obtained using the simple 
matching criterion given by (6.11). As shown in Figure 6.9, this estimate is 
quite far from the true motion. This is mostly due to the mismatch between the 
ground plane data seen from the second viewpoint (Figure 6.7c) and the solution 
interpolated from the first viewpoint in the shadowed area (Figure 6.8a). 
The novel motion estimation algorithm presented in this section thus has 
several interesting features not present in previous algorithms. Our approach 
can handle sparse and irregularly spaced data and does not require any corre-
spondence between sensed data points. The dense depth map that is built up 
incrementally becomes increasingly more accurate as more data is acquired. The 
depth map can also be extrapolated to shadowed or invisible areas without af-
fecting the performance of the motion estimation algorithm, since our framework 
implicitly models the uncertainty in the interpolated surface. The uncertainty in 
the motion estimate can be calculated from the shape of the energy surface in 
the vicinity of the optimal estimate, and this information can then be used to 
integrate the motion estimate with other position sensors. The success of this 
algorithm in performing motion estimation further demonstrates the advantages 
of using a Bayesian approach to low-level vision. 

Chapter 7 
Incremental algorithms for 
depth-from-motion 
The Bayesian models which we have developed in this book allow us to obtain 
optimal estimates of static visible surfaces, to integrate infonnation from multiple 
viewpoints, and to analyze the uncertainty in our estimates. Many computer 
vision applications, however, deal with dynamic environments. This may involve 
tracking moving objects or updating the model of the environment as the observer 
moves around. Recent results by Aloimonos et al. (1987) suggest that taking 
an active role in vision (either through eye or observer movements) greatly 
simplifies the complexity of certain low-level vision problems. In this chapter, 
we will examine one such problem, namely the recovery of depth from motion 
sequences. 
The study of depth-from-motion has long been an active area of research in 
computer vision. Early work concentrated on extracting the optical flow field 
from a pair of images, using either gradient-based (Horn and Schunck 1981) or 
correlation-based (Anandan 1984) techniques. More recent motion algorithms 
have attempted to use batch processing of the whole image sequence, either 
by fitting lines to the spatio-temporal data (Bolles and Baker 1985) or using 
spatio-temporal filtering (Adelson and Bergen 1985, Heeger 1986). 
In this chapter we will develop an incremental algorithm which produces a 
dense on-line estimate of depth from the motion image sequence. The advantage 
of using the incremental approach is that coarse depth measurements are avail-
able immediately, and the quality of these estimates improves over time as more 
images are acquired. Incremental processing also has lower storage requirements 
than batch processing. The algorithm we develop uses an extended version of 
our Bayesian model which is based on the Kalman filter. In contrast to the al-
gorithms which we have previously studied in this book, the depth-from-motion 
algorithm integrates the depth measurement with the surface interpolation stages, 
and thus provides a complete solution to a low-level vision problem. 

122 
Bayesian Modeling of Uncertainty in Low-Level Vision 
In the first section of this chapter, we start by reviewing the Kalman filter 
approach to on-line estimation with dynamic systems. We then show how this 
framework can be applied to modeling visible surfaces, and discuss approxima-
tions needed to develop fast and simple algorithms. In the second section, we 
present an iconic depth-from-motion algorithm based on this framework which 
was developed by Matthies et al. (1987)(1989). We provide experimental re-
sults which show that the incremental approach to extracting depth from motion 
sequences yields good performance. We also discuss some problems with this 
algorithm, which are due mostly to the spatial and temporal correlations present 
in the area-based optical flow estimator. In the third section, we present a new in-
cremental depth-from-motion algorithm which aims to overcome these problems. 
By jointly modeling the intensity and disparity fields, this algorithm can model 
the piecewise continuous nature of intensity images and obtain a performance 
which has the best features of both area- and edge-based depth-from-motion 
algorithms. 
7.1 
Kalman filtering 
The Kalman filter is a Bayesian estimation technique used to track stochastic 
dynamic systems being observed with noisy sensors. The filter is based on 
three separate probabilistic models. The first model, the prior model, describes 
the knowledge about the system state 00 and its covariance Po before the first 
measurement is taken, 
U '" N(oo, Po). 
(7.1) 
As we have seen in Chapter 4, this model can capture the smoothness constraint 
associated with visible surfaces by setting Po 1 = Ap. The second model, the 
measurement (or sensor) model, relates the measurement vector dk to the current 
state through a measurement matrix Hk with the addition of Gaussian noise with 
a covariance Rk, 
(7.2) 
When applied to surface estimation, the measurement matrix Hk is used to con-
vert the dense map Uk into the sparse measurement dk• These two models form 
the basis for the Bayesian estimation framework which we have developed in 
this book. The Kalman filter introduces a third model, the system model, which 
describes the evolution over time of the current state vector Uk. The transi-
tion between states is characterized by the known transition matrix F k with the 
addition of Gaussian noise with a covariance Qk, 
(7.3) 

Incremental algorithms for depth-from-motion 
123 
In the case of depth-from-motion, the transition matrix Fk describes the mapping 
of surface estimates from one coordinate frame to the next as the observer 
changes position. 
The above three models describe the evolution of the state Uk and its rela-
tionship to the measurements dk• To obtain an optimal estimate Uk of the current 
state, the Kalman filter operates in two phases. The extrapolation phase predicts 
the new state given the previous best estimate 
and updates the covariance matrix associated with the predicted estimate 
Pi" = Fk-1P;_IFLl + Qk-l' 
(7.4) 
(7.5) 
The correction phase updates the state estimate using the new measurements. 
First, we compute the Kalman filter gain matrix 
(7.6) 
We then update the state estimate by adding the residual between the observed 
and predicted measurements scaled by the Kalman filter gain 
(7.7) 
and reduce the covariance of the new estimate using 
(7.8) 
A block diagram for this implementation of the Kalman filter is shown in Figure 
7.1. 
Kalman filtering is usually applied to systems with a fairly small number of 
state variables. In the domain of motion sequence analysis, it has previously 
been used to track edges (Rives et al. 1986, Matthies and Shafer 1987, Baker 
and Bolles 1989, Baker 1989), but has not been used in conjunction with dense 
(iconic) fields such as depth maps. When modeling dense maps, the information 
matrices (inverse covariance matrices) are sparse and banded (because of the 
nature of the prior information matrix Ap), while the covariance matrices are 
not. A different formulation of the Kalman filter, which preserves the sparse 
nature of the matrices, is thus preferable. 
Bierman (1977) discusses a number of efficient techniques for doing the 
Kalman filter update which rely on using matrix decomposition or factorization 
methods. These various decompositions (including the Square Root Information 
Filter) do not, however, result in matrices that are as sparse as the original infor-
mation matrix. Thus for applications where the prior and posterior distributions 
are Markov Random Fields (of reasonable size), factorization methods are not 

124 
Bayesian Modeling of Uncertainty in Low-Level Vision 
dk 5:>-
Kk 
-to 
u! 
'--
Hk 
u; 
Fk_ 1 -
Delay I+--
Figure 7.1: Kalman filter block diagram 
useful since they require too much storage space (O(nyln) or O(n2) where n is 
the number of pixels). 
For these reasons, we will use the the information matrix Ak == Pk 1 and the 
cumulative weighted data vector bk == AkUk as the quantities to be updated. The 
advantage of using this formulation is that the updating equations are particularly 
simple, 
and 
A+ - A- +HTR-1HT 
k-
k 
k 
k 
k 
(7.9) 
(7.10) 
and they require no matrix inversions (similar equations are used for sensor 
integration in Appendix D). The current estimate uk can be computed at any 
time by solving 
u; = (A;)-lb; 
using multigrid relaxation. In practice, we can use the previous state estimate 
uL 1 or the predicted state estimate uk as the starting point for the relaxation 
and only iterate for a few steps. This may not yield the optimal solution for 
the given data, but given enough time, the estimate will converge to such an 
optimal solution. Thus, a tradeoff can be made between the desired accuracy of 
the data and the amount of computation performed. 
The prediction equations for our depth estimation system are somewhat more 
difficult to implement. This is because the mapping from one depth map Uk-l to 
the next is not a predetermined linear operation. Instead, the whole depth map 
is warped according to the local disparity to obtain the new depth map (Quam 
1984). The exact form of this warping is explained in the next section. For 
now, let us assume that we can compute the transition matrix Fk- 1 by finding 
the linear mapping that defines how each point in the new field Uk is obtained as 

Incremental algorithms for depth-from-motion 
125 
a weighted combination of points in the previous field U,t_1 (the alternative is to 
use the extended Kalman filter (Gelb 1974) to compute F,t-I from the Jacobian 
of the state transition function). 
Once we have computed the transition matrix F,t_], we can use (7.4) and (7.5) 
to calculate the predicted state u; and covariance P;. To keep the updating step 
simple, however, we would rather compute the predicted cumulative weighted 
data vector b; and the information matrix A;. First, we will replace (7.5) with 
the simpler equation 
(7.11) 
Thus, rather than assuming that the true system model is corrupted by additive 
Gaussian noise q,t (which seems strange for a static visible surface), we simply 
assume that our uncertainty in the old estimate (characterized by P,t) uniformly 
increases by a small factor f. In the Kalman filter literature, this is known 
as overweighting the most recent data (Maybeck 1982). Next, we re-write the 
information matrix prediction equation as 
A - = (1 + f)-I(F _ (A+ )-IFT )-1 
,t 
k I 
k-I 
,t-I 
(1 
)-IF-T A+ F-I 
= + f 
,t-I k-I k-I' 
Finally, using (7.11) and (7.4) we write 
b; = A;u; 
= (1 + f)-I(Fk_ 1 (AZ_I)-I FLI)-I F,t_1 (AZ_I)-lbZ_1 
= (1 + f)-IF;!I bLI' 
The above equations involve the inverse state transition matrix F;!I' This 
inverse may in general be difficult to compute, although the sparse nature of F ,t-I 
(since we are dealing with local map deformations) should make this easier. In 
particular, by assuming that the transformation is locally a translation, we can 
make the approximation 
F- I 
FT 
,t-I ~ ,t-I' 
since inverting the motion of a depth map is equivalent to interchanging the 
source and destination pixel locations. 
One further simplifying assumption that we can make is that the prior model 
component of the information matrix A,t does not change as the observer changes 
viewpoint. This means that we keep the smoothness constraint expressed by Ap 
invariant, and only predict the certainties associated with the data points, which 
we can represent using 
Ak == A,t - Ap. 
Note that if the prior model were actually implementing a viewpoint-invariant 
smoother or interpolator (Blake and Zisserman 1986a), we could map this model 

126 
Bayesian Modeling of Uncertainty in Low-Level Vision 
through any rigid transfonnation without changing its behavior. The extension 
of these ideas to full three-dimensional models and general motion should be a 
promising area for further research. 
The simplified Kalman filter for tracking visual surfaces can thus be sum-
marized by the following prediction and updating equations: 
Ai" = (1 + f)-IFk_1AZ_1FLI 
bi" = (1 + f)-IFk-1bZ_1 
AZ = Ai" + HIRi"IHI 
b; = bi" + HIRi"ldk. 
The current state estimate can be obtained by solving 
uZ = (Ap + AZ)-lbZ. 
These simplified equations allow us to keep the amount of storage to a minimum, 
since bk is a single field equal in size to the depth map Uk> and Ak can be restricted 
to being diagonal (and is thus also a single field). The computations involved 
are simple, since the prediction step involves a warping of the data and inverse 
variance fields, and the updating involves a weighted addition. The need for 
matrix inversions is avoided, and the solution of the state estimate equation can 
be perfonned using multigrid relaxation. 
The Kalman filtering framework which we have developed in this section 
is thus specially tailored to match the structure of the visible surface estimation 
problem. By using information matrices rather than covariance matrices, we can 
keep the representations sparse and the computations simple. However, since the 
state transition equations that describe the evolution of the retinotopic depth map 
are actually nonlinear, an optimal implementation would require the use of an 
extended Kalman filter (Gelb 1974). As we will see in the next section, however, 
we can still design a practical incremental depth-from-motion algorithm without 
resorting to this more complicated model. 
7.2 Incremental iconic depth-from-motion 
The Kalman filter-based framework described in the previous section has been 
used by Matthies, Szeliski and Kanade (1987, 1989) to develop an incremen-
tal image-based (iconic) depth-from-motion algorithm. In this section, we de-
scribe this algorithm, and show how it fits in with our general dynamic estima-
tion framework. We then discuss the convergence properties of our estimator, 
and compare it to a feature-based (symbolic) algorithm which was also im-
plemented. We present some quantitative performance results using a simple 
domain (a flat scene) and some qualitative results obtained with more realistic 

Incremental algorithms for depth-from-motion 
127 
image pairs 
raw 
cumulative 
smoothed 
-
disparitv_ 
disparitv_ 
disDaritv 
Correlation 
variance 
Integration 
-
variance Regularization v. Hance 
.- __ J ___ ., 
predicted 
I 
I 
disparity 
-
I 
I 
I 
Motion 
I 
variance 
Prediction 
I 
I 
... ---1----' 
I 
Figure 7.2: Iconic depth estimation block diagram 
three-dimensional scenes. Lastly, we discuss some of the limitations of this 
approach, and preview how they may be resolved with a more sophisticated 
approach. 
The algorithm developed by Matthies et al. (1987) consists of four main 
stages (Figure 7.2). The first stage uses correlation to compute an estimate of 
the displacement vector and its associated covariance. It converts this estimate 
into a disparity (inverse depth) measurement using the known camera motion. 
The second stage integrates this information with the disparity map predicted at 
the previous time step. The third stage uses regularization-based smoothing to 
reduce measurement noise and to fill in areas of unknown disparity. The last 
stage uses the known camera motion to predict the disparity field that will be 
seen in the next frame, and re-samples the field to keep it iconic (pixel-based). 
The information propagated between these four stages consists of two fields 
(iconic maps). The first field is the disparity estimate computed at each pixel 
in the image. The second field is the variance associated with each disparity 
estimate. Modeling the variance at every pixel is essential because it can vary 
widely over the image, with low variance near edges and in textured areas, and 
high variance in areas of uniform intensity. These two fields roughly correspond 
to the cumulative data vector bk and the point certainty matrix Ak used in the 
previous section. We will discuss how these representation differ, but first we 
will describe in more detail each of the four processing stages. 
The first stage of the Kalman filter computes a disparity map from the differ-
ence in intensity between the current image and the previous image. In theory, 
this computation proceeds in two parts. First, a two-dimensional displacement 

128 
Bayesian Modeling of Uncertainty in Low-Level Vision 
(or optical flow) vector is computed at each point using the correlation-based 
algorithm described in Section 5.3. Second, this vector is converted into a dis-
parity measurement using the known camera motion. In the actual algorithm 
implemented by Matthies et al. (1987), a one-dimensional flow (displacement) 
estimator is used since the flow is known to be parallel to the image raster (only 
horizontal and vertical motions perpendicular to the optical axis are used). At 
each point in the image, a parabola 
e(d) = a J2 + b d + c 
is fit to the local error surface near its minimum error position, and the lo-
cal disparity estimate d = -b/2a and the disparity variance d = 2a;/a are 
computed. 
The next stage of the iconic depth estimator integrates the new disparity 
measurements with the predicted disparity map (this step is omitted for the first 
pair of images). The algorithm assumes that each value in the measured or 
predicted disparity map is not correlated with its neighbors, so that the map 
updating can be done at each pixel independently. To update a pixel value, we 
first compute the variance of the updated disparity estimate 
P; = (p;)-I + (a1)-I)-1 
and then update the disparity value estimate with 
+ 
+ (p-)-I - + ( 2 )-ld
A 
) 
Uk = Pk 
" 
Uk 
a d1 
k· 
This updating rule is simply a linear combination of the predicted and measured 
values inversely weighted by their respective variances. 
The raw depth or disparity values obtained from optical flow measurements 
can be very noisy, especially in areas of uniform intensity. We use regularization-
based smoothing of the disparity field in the third stage to reduce the noise and to 
fill in underconstrained areas. The smoothing algorithm we use is the generalized 
piecewise continuous spline under tension presented in Section 2.2. We use the 
inverse variance of the disparity estimate as the confidence associated with each 
measurement. The spline is computed using a three level coarse-to-fine multigrid 
relaxation method. This algorithm is amenable to implementation on a parallel 
computer, as are the other three processing stages. The variance field is not 
affected by the smoothing stage. 
After the initial smoothing has been performed, depth discontinuities are de-
tected by thresholding the angle between the view vector and the local surface 
normal (Matthies et al. 1987, Appendix B), and doing non-maximum suppres-
sion. Once discontinuities have been detected, they are incorporated into the 
piecewise continuous smoothing algorithm, and a few more relaxation steps are 

Incremental algorithms for depth-from-motion 
129 
X,.-____ -+ __ ~~-----+---. 
Figure 7.3: Illustration of disparity prediction stage 
performed. Our approach to discontinuity detection, which interleaves smooth-
ing and edge detection, is similar to the continuation method developed by 
Terzopoulos (1986b). The alternative of trying to estimate the boundaries in 
conjunction with the smoothing (Marroquin 1984, Leclerc 1989) has not been 
tried, but could easily be implemented within our framework. The detected dis-
continuities could also be propagated to the next frame, but this has not been 
implemented. 
The final stage of the incremental depth-from-motion algorithm predicts the 
new disparity and variance fields from the previous fields and the observer 
motion estimate. At time k, the current disparity map and motion estimate are 
used to predict the optical flow between images k and k + 1, which in turn 
indicates where the pixels in frame k will move to in the next frame. In general 
this prediction process will yield estimates of disparity in between pixels in the 
new image (Figure 7.3), so we need to resample to obtain predicted disparity 
at pixel locations. For a given pixel x' in the new image, we find the square 
of extrapolated pixels that overlap x' and compute the disparity at x' by bi-
linear interpolation of the extrapolated disparities. Note that it may be possible 
to detect occlusions by determining where the extrapolated squares turn away 
from the camera. Detecting disocclusions, where newly visible areas become 
exposed, is not possible if the disparity field is assumed to be continuous, but is 
possible if disparity discontinuities have been detected. 
The same warping and resampling algorithm used for the disparity is applied 
to the variance field. To partially model the increase in uncertainty which occurs 
in the prediction phase (due to uncertainty in the motion parameters, errors in 
calibration, and inaccurate models of the camera optics), we inflate the current 

130 
Bayesian Modeling of Uncertainty in Low-Level Vision 
variance estimates by a small multiplicative factor 
(7.12) 
This corresponds to overweighting of the most recent data, as we saw in the 
previous section. A more exact approach is to attempt to model the individual 
sources of error and to propagate their effects through the prediction equations 
(Matthies et al. 1987, Appendix C). 
The depth-from-motion algorithm which we have just described differs slightly 
from the dynamic estimation framework developed in the previous section. The 
algorithm uses a variance field, which is the inverse of Ak in the case where 
this matrix is diagonal. Whether we choose to propagate variance or inverse 
variance does not affect the integration and smoothing stages, but it does make 
a slight difference in the prediction stage because we must interpolate between 
the variances (or certainties). The algorithm also uses a disparity field instead of 
the weighted cumulative data vector bk• This field is actually more convenient, 
since it is easier to interpret, and can be used directly to predict the optical flow 
in the next frame. By feeding the result of the smoothed disparity field into the 
prediction stage instead of the raw field, however, the depth-from-motion algo-
rithm actually oversmooths the data, since the measurements get re-smoothed 
every time they go around the prediction-integration-smoothing loop. This can 
be compensated for by decreasing the amount of smoothing used at each step 
and forgetting old data using (7.12). Despite the simplifications built into the 
depth-from-motion algorithm, the results obtained using this method are still 
very good, as we will see shortly. 
7.2.1 
Mathematical analysis 
The depth estimates provided by the incremental depth-from-motion algorithm 
improve in accuracy as more images are acquired. Because the algorithm is 
based on a Bayesian formulation, and because we have a good model for the 
expected error in the flow estimates (Section 5.3), we can analytically derive the 
expected decrease in depth variance as a function of the number of frames. We 
can compare the convergence rate of our iconic algorithm to that of a symbolic 
(feature-based) algorithm and also to the results of stereo matching the first and 
last frames. Our results will be presented here using informal arguments. The 
full mathematical analysis can be found in (Matthies et al. 1987). 
For the iconic method, we will ignore process noise in the system model and 
assume that the variance of successive flow measurements is constant. In this 
case, the disparity and variance updating equations we use in the depth-from-
motion algorithm are equivalent to computing the average flow (Gelb 1974). 
From Appendix C, we know that the variance in an individual flow estimate d is 

Incremental algorithms for depth-from-motion 
131 
20";/ a, where 0"" is the standard deviation of the image noise and a is proportional 
to the average squared intensity gradient within the correlation window. If these 
flow measurements were independent, the resulting variance of the disparity 
estimate after t image pairs would be 
20"2 " 
ta 
(7.13) 
However, the flow measurements are not actually independent. As we show in 
Appendix C, the error in the flow estimate iJ is proportional to the difference 
between the two error terms bo and bl , which are proportional in turn to the 
average noise in the previous and current image. Generalizing this result from 
an image pair to an image sequence, we see that the correlation between two 
successive measurements is 
A 
A 
/ 
A 
_ 
A 
_) 
1 
a; 
Cov(dk,dk+l ) = ,(dk - d)(dk+1 - d) = a2 (bk_1 - bk)(bk - bk+I ») = --;;. 
With this correlation structure, averaging the flow measurements actually yields 
the following variance for the estimated flow: 
2) 2a; 
0"1(1 = -2-· 
t a 
(7.14) 
This result is interesting and rather surprising. Comparing equations (7.13) 
and (7.14), the correlation structure that exists in the measurements means that 
the algorithm converges faster than we first expected. An intuitive way of 
understanding this is shown in Figure 7.4. The disparity estimate obtained 
from each pair of images is equivalent to the slope of the dotted line segment 
connecting two position measurements. Of course, for the iconic method, we 
are not explicitly matching features; however, the error in each disparity (slope) 
estimate is still determined by the difference between two independent errors, 
one associated with each image. The average of all these slope measurements 
is thus the effective slope connecting the first and last positions. As we will see 
shortly, this is equivalent to the result obtained with stereo matching. 
For the feature-based approach implemented by Matthies et al. (1987), both 
the sub-pixel position of each edgel and its disparity are jointly estimated by 
the Kalman filter. The resulting disparity estimate is equivalent to performing a 
least-squares fit to the edge positions as a function of time, as shown in Figure 
7.4. The variance of the disparity estimate can be shown to be 
2 
12a; 
0" (t) - ---"--
F 
- t(t + 1)(t + 2) 
(7.15) 
where 0"; is the variance in the edge position estimate. The variance of the 
displacement or flow estimate d thus decreases as the cube of the number of 
images. 

132 
§ 
'l:l 
.... 
ell 
8-
~ 
.... 
Q.. 
o 
1 
2 
Bayesian Modeling of Uncertainty in Low-Level Vision 
3 
4 
5 
6 
7 
8 
9 
10 
time 
Figure 7.4: Computation of disparity using least squares fit 
To compare these methods to stereo matching on the first and last frames 
of the image sequence, we must scale the stereo disparity and its uncertainty 
to match the flow between frames. This implies dividing the stereo disparity 
by t and the uncertainty by fl. For the iconic method, we assume that the 
uncertainty in a stereo measurement will be the same as that for an individual 
flow measurement. Thus, the scaled uncertainty is 
2 
20"; 
O"IS(t) = -2-' 
ta 
(7.16) 
This is the same as is achieved with our incremental algorithm which processes 
all of the intermediate frames. For the feature-based approach, the uncertainty 
in stereo disparity is twice the uncertainty 0"; in the feature position; the scaled 
uncertainty is therefore 
O"~(t) = 2~;. 
(7.17) 
t 
The results obtained with the incremental feature-based algorithm are thus more 
accurate than those obtained with stereo matching. 
Extracting depth from a small-motion image sequence thus has several ad-
vantages over stereo matching between the first and last frames. The ease of 
matching is increased, reducing the number of correspondence errors. Occlusion 
is less of a problem, since it can be predicted from early measurements. Finally, 
better accuracy is available using the feature-based method. It would be nice if 
the iconic approach had the same improved accuracy. This is possible in theory, 
by extending our Kalman filter formulation to model the correlated nature of the 

Incremental algorithms for depthlrom-motion 
133 
Figure 7.5: Tiger image and edges 
measurements. We will examine a possible solution to this problem in Section 
7.3. 
7.2.2 
Evaluation 
The depth-from-motion algorithm described in this section has been tested on 
a number of image sequences acquired in the Calibrated Imaging Laboratory 
at Carnegie Mellon University. To measure the accuracy of our algorithm and 
to detennine its rate of actual convergence, we needed a scene whose ground 
truth depth map was known. This was achieved by digitizing an image sequence 
of a flat-mounted poster'. Figure 7.5 shows the poster and the edges extracted 
from it. The ground truth value for the depth was detennined by fitting a plane 
to the measured values, and the accuracy of the estimates was detennined by 
computing the RMS deviation of the measurements from the plane fit. Since the 
scene used was flat, we could have reduced this RMS error to zero using a large 
degree of smoothing. For this reason, we did not use any regularization for our 
quantitative experiments. The results we present therefore show only the effect 
of the Kalman filtering algorithm. 
To examine the convergence of the Kalman filter, the RMS depth error 
was computed after processing each image in the sequence for both the iconic 
algorithm described here and the feature-based algorithm described in (Matthies 
et al. 1987). We computed two sets of statistics, one for "sparse depth" and one 
for "dense depth." The sparse statistic is the RMS error for only those pixels 
where both algorithms gave depth estimates (i.e., where edges were found), 
whereas the dense statistic is the RMS error of the iconic algorithm over the full 
image. Figure 7.6 plots the relative RMS errors as a function of the number of 
1 Details of the actual physical setup (camera, Jens, distance to scene and motion) are given 
in (Matthies et at. 1987). 

134 
ii 14.0 
'-' 
~ 
~ 12.0 
4) 
.~ 
~ 
10.0 
8.0 
6.0 
4.0 
2.0 
Bayesian Modeling of Uncertainty in Low-Level Vision 
, 
, 
\ 
\ 
\ 
. 
\ 
61 ... 
\ 
.... 
\ 
.... " .... 
, 
'ra ','" 
"':':'" 
B-•••••••• E1 
+ ....... + 
~ 
Theoretical sparse feature-based 
Theoretical sparse iconic. 
Actual sparse feature-based (1522 pixels) 
Actual sparse iconic (1522 pixels) 
Actual dense iconic (15360 pixels) 
,'. 
..... 
... "':" ...... -
...... -
- '!II- _ 
··.s.... 
-!II- -
.... 
.... s. ........ & •••••••• & •••••••• & •••••••• E1 
....... + ....... + ....... + ....... + ...... . 
0.0 +--+---+---I---+--+--+---I----f-~I-----I 
o 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
Frame 
Figure 7.6: RMS error in depth estimate 
images processed. Comparing the sparse error curves, the convergence rate of the 
iconic algorithm is slower than the feature-based algorithm, as expected. In this 
particular experiment, both methods converged to an error level of approximately 
0.5% percent after processing eleven images. Since the poster was 20 inches 
from the camera, this equates to a depth error of 0.1 inches. Note that the overall 
baseline between the first and the eleventh image was only 0.44 inches. 
To compare the theoretical convergence rates derived earlier to the exper-
imental rates, the theoretical curves were scaled to coincide with the experi-
mental error after processing the first two frames. These scaled curves are also 
shown in Figure 7.6. For the iconic method, the theoretical rate plotted is the 
quadratic convergence predicted by the correlated flow measurement model. The 
agreement between theory and practice is quite good for the first three frames. 

Incremental algorithms for depth-from-motion 
135 
Thereafter, the experimental RMS error decreases more slowly; this is probably 
due to the effects of unmodeled sources of noise. For the feature-based method, 
the experimental error initially decreases faster than predicted because the im-
plementation required new edge matches to be consistent with the prior depth 
estimate. When this requirement was dropped, the results agreed very closely 
with the expected convergence rate. Finally, Figure 7.6 also compares the RMS 
error for the sparse and dense depth estimates from the iconic method. The 
dense flow field is considerably noisier than the flow estimates that coincide 
with edges, though there is just over two percent error by the end of eleven 
frames. 
The iconic and edge-based algorithms were also tested on complicated, real-
istic scenes obtained from the Calibrated Imaging Laboratory. Two sequences of 
ten images were taken with camera motion of 0.05 inches between frames; one 
sequence moved the camera vertically, the other horizontally. The overall range 
of motion was therefore 0.5 inches; this compares with distances to objects in 
the scene of 20 to 40 inches. We will present some of the results obtained with 
the iconic algorithm here; a full discussion of the experimental results, including 
a comparison of the iconic and feature-based methods is given in (Matthies et 
al. 1987). 
Figure 7.7 shows one of the images (a picture of a miniature town). Figure 
7.8a shows a reduced version of the image and Figure 7.8b shows the intensity 
coded depth map produced by the iconic algorithm (lighter areas in the depth 
maps are nearer). This result was produced by combining disparity estimates 
from both the horizontal and the vertical camera motion sequences. The same 
depth map is shown in Figure 7.8c as a 3-D perspective reconstruction. Ani-
mated versions of the 3-D reconstructions proved to be very useful for detecting 
problems with our algorithms and for subjectively evaluating their performance. 
Figure 7.8d shows the occluding boundaries (depth discontinuities) found by the 
algorithm after the initial smoothing. The method found most of the prominent 
building outlines and the outline of the bridge in the upper left. From these 
figures, we can see that the main structures of the scene are recovered quite 
well. 
Figures 7.9a and 7.9b show the results of our algorithms on a different model 
set up in the Calibrated Imaging Laboratory. The same camera and camera 
motion were used as before. Figure 7.9a shows the first frame, and Figure 
7.9b shows the depth maps obtained with the iconic algorithm using both the 
vertical and horizontal motion sequences. Again, the algorithm did a good job 
in recovering the structure of the scene. 
Finally, we present the results of using the first 10 frames of the image 
sequence used by Bolles et al. (1987). Figures 7.9c and 7.9d show the first 
frame from the sequence and the depth map obtained with the iconic algorithm. 
The results from using the feature-based method presented in (Matthies et al. 

136 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Figure 7.7: elL image 
1987) are similar to those obtained with the Epipolar-Plane Image technique. 
The iconic algorithm produces a denser estimate of depth than is available from 
either edge-based technique. These results show that the sparse (edge-based) 
batch processing algorithm for small motion sequences introduced by Bolles et 
al. (1987) can be extended to use dense depth maps and incremental processing. 
The results obtained with the iconic incremental depth-from-motion algo-
rithm are extremely encouraging. For the quantitative experiments performed 
with a flat scene, an accuracy of 0.5% was obtained after 11 images taken with 
a relatively narrow baseline2• The qualitative results obtained with different re-
alistic toy model scenes are also suprisingly good. This algorithm is a powerful 
demonstration of the advantages of using a Bayesian modeling framework for 
solving low-level vision problems. 
Nevertheless, there are some known problems and limitations with this al-
2This error figure "floor" may be due to the exponential weight decay that we use in updating 
the variances. 

Incremental algorithms for depth-from-motion 
(a) 
(b) 
(c) 
(d) 
Figure 7.8: elL depth maps 
137 
fII:_ 
r 
r 
L 
c 
(a) first frame (b) combined motion depth map (c) perspective view (d) occluding 
boundaries 

138 
Bayesian Modeling of Uncertainty in Low-Level Vision 
(a) 
(b) 
(c) 
(d) 
Figure 7.9: CIL-2 and SRI depth maps 
(a) first frame from second CIL sequence (b) converged motion depth map (c) 
first frame from SRI EPI sequence (d) horizontal motion depth map 

Incremental algorithms for depth-from-motion 
139 
gorithm. Because the temporal correlation between successive flow estimates is 
ignored, the optimal cubic convergence rate is not actually achieved. Because 
spatial correlations between measurements are ignored, the algorithm produces 
flat spots in the vicinity of strong image features such as intensity ed~es. The ap-
proximations used in developing the sensor model for the correlation-based flow 
estimator also tend to break down near such prominent features3• For these rea-
sons, we present in the next section a new depth-from-motion algorithm which 
aims to overcome these limitations. 
7.3 J oint modeling of depth and intensity 
In this book, we have presented a general Bayesian framework for formulating 
low-level vision problems, and we have demonstrated the advantages of this 
approach over conventional methods. We have shown how prior models can 
capture the smoothness inherent in visible surfaces, how sensor models can 
describe the uncertainty in our measurements, and how dynamic system models 
can integrate information over time. In this section, we develop a new depth-
from-motion algorithm which combines all of these ideas. Instead of using 
a pre-processing stage to extract flow from image pairs, we directly use the 
sampled images in our Bayesian formulation. More importantly, in addition to 
estimating the disparity field incrementally (as we did in the previous section), 
we also incrementally estimate the true intensity field. 
The joint estimation of intensity and disparity has not previously been stud-
ied, yet it has the potential for significantly improving the accuracy of depth-
from-motion algorithms. The usual regularized stereo formulation (Poggio et 
al. 1985a) assumes that the second image is a corrupted version of the first im-
age, and uses an energy function which depends only on the intensity difference 
between the two images 
E(d) = J 
{['\72G * (L(x,y) - R(x + d(x,y),y»f + A('\7d)2} dxdy. 
In this section, we will argue that explicitly modeling and estimating the inten-
sity field is preferable, because we can use better models of smoothness (e.g., 
piecewise continuous models), and because we can improve our estimates over 
time. The aim of our new formulation is to better model the visual world by 
using explicit prior models rather than implicitly building these assumptions into 
our algorithms. We also hope to improve the convergence rate of the incremen-
tal depth-from-motion algorithm by having two states (intensity and disparity) 
at each pixel instead of just one. 
3Since the Taylor series expansion used in Appendix C is a particularly poor approximation 
to a step edge. 

140 
Bayesian Modeling of Uncertainty in Low-Level Vision 
We start our presentation by re-examining the two-frame image matching 
problem, ignoring for the moment discontinuities in the intensity or disparity 
fields. Based on our probabilistic models, we derive a new equation for regular-
ized stereo, and we show how the uncertainty in our estimates can be computed 
from variati9ns in this energy. Next, we show how these estimates can be com-
bined with a new image to obtain updated estimates of the intensity and disparity 
fields and their associated uncertainties. Lastly, we show how discontinuities can 
be integrated into our framework, and discuss possible extensions of this work. 
7.3.1 Regularized stereo 
The general depth-from-motion problem consists of processing a sequence of 
images taken from a moving camera in order to extract a depth map of the scene. 
For the work described here, we assume that the camera is moving horizontally 
(perpendicular to the optic axis) at a known constant velocity (extensions to 
other motions are discussed by Bolles et al. (1987)). Under these conditions, 
the intensity and disparity fields as a function of time can be described by 
f(x + t· d(x, y), y) = j(x, y) 
dl(x + t· d(x,y),y) = d(x,y) 
(7.18) 
(7.19) 
wherej(x,y) and d(x,y) are the intensity and disparity fields at time t = O. The 
evolution of the intensity field over time is shown in Figure 7.10. This Epipolar 
Plane Image (Bolles et al. 1987) shows the intensity of one row of an image as 
a function of time. The slope of the lines seen in this image corresponds to the 
disparity at those points. Of course the above two equations do not account for 
occlusions, but we will ignore this for now. 
To model the smoothness in the intensity and disparity fields, we use the 
energy-based smoothness constraints introduced in Section 2.2. The disparity 
field is modeled by a thin plate, since it has the same smoothness as the visible 
surface that we are estimating4; the intensity field is modeled by a membrane5• 
We will designate the two functionals that measure this smoothness by Et<J) and 
Ed(d). 
The sensor model we use for our CCD camera is the one developed in 
Section 5.4, 
(7.20) 
4For the camera motion we are assuming, the disparity is inversely related to depth. See 
(Matthies et al. 1987) for a discussion of the relative merits of disparity vs. depth representations. 
5 An informal justification for this choice is that if intensity is related to depth through the 
gradient (as it is for Lambertian reflection), then its degree of continuity should be one less than 
that of the depth itself. 

Incremental algorithms for depth-from-motion 
141 
t 
x 
Figure 7.10: An Epipolar Plane Image 
where b(x, y) is the two-dimensional blur function that captures both the blur 
induced by the optics and the integration over the finite area of the CCD sensor 
cell. 
Putting these three models together, we obtain a new equation for regularized 
stereo 
Elf, d) = A/Efj) + AdEAd) 
1 ~ [1 
1]2 
+ 2c:r2 ~ (b * f )(Xj, yj) - gj 
I 
+ 2~2 L [(b * f)(xj, yJ - g?]2 
I 
where 
/(X,y) = f(x,y) 
and 
f(x - d(x,y),y) = f(x,y) 
(7.21) 
(7.22) 
(7.23) 
(here we are estimating the intensity and disparity fields at time t = 1). Com-
paring this new energy equation to the formulation developed by Poggio et al. 
(1985a), we see that our formula does not contain any explicit smoothing of the 
intensity fields. This smoothing is achieved implicitly by the functional Efj). 
The two image intensity samples gf and g? are not differenced directly, but 
are instead subtracted from the estimated intensity function fix, y). By warping 
the intensity function before differencing with the samples, our equation cor-
rectly models the non-uniform sampling induced by disparity gradients. Our 
formulation also differs from that of Witkin et al. (1987), who use normalized 

142 
Bayesian Modeling of Uncertainty in Low-Level Vision 
cross-correlation as their similarity functional. Their approach is less sensitive 
to bias or scaling in the images; if this were a concern, we could add global 
parameters to the sensor model to account for these distortions. Our formulation 
is related to that of Hung et al. (1988), who estimate the parameters of a surface 
patch rather than a full disparity map. 
To find the minimum energy solution of (7.22), we can use a variety of 
techniques. We can take the correlation-based flow measurements described in 
Section 5.3 as a starting solution, and then do gradient descent on the energy 
function. Alternatively, we can use scale space continuation similar to that 
of Witkin et al. (1987) by initially using larger values of AI (oversmoothing the 
intensity estimates). Finding the minimum energy solution of (7.22) corresponds 
to finding the MAP estimate. We can also run the system at a finite temperature 
to find the mean (MMSE) estimate. It is likely that these two estimates are fairly 
close, but this remains to be verified. 
Before we can actually implement the energy minimization on a computer, 
we must first discretize the energy equations. As usual, we represent both.f(x,y) 
and d(x,y) as discrete fields f = {ji} and d = {di} on a rectangular grid (we 
will make these fields coincident with the sampled image g = {gi}). We use 
the same discrete representations for the prior energies Et<f) and Ed( d) as we 
developed in Section 2.2. For computing the data compatibility equations, we 
use bilinear interpolation6 to convert from the discrete fields to the continuous 
functions used in (7.22). We therefore write 
f(x,y) = LJjl(x - Xj,Y - Yj) 
j 
where l(x,y) is the basis function for the interpolator. From this, we compute 
(b * f)(Xi, Yi) = L/j (b * l)(xi - xj, Yi - Yj) = L hij/j 
j 
j 
where the hij coefficients encode how the information in measurement gi is 
spread among the nodal variables /j. We now write the first data compatibility 
term in (7.22) as 
To compute the estimated intensity field in the previous frame (l(x,y), we 
use (7.23) and find that(l(x,y) is still piecewise bilinear, but with a spacing that 
is no longer regular. The basis functions fj(x,y) now depend on the structure of 
6Using a higher order interpolator for d(x, y) may improve the accuracy of the discrete 
approximation, but also complicates the subsequent calculations. 

Incremental algorithms for depth-from-motion 
143 
the local disparity field around (Xj,Yj), and so we obtain 
(b * f)(Xj, Yj) = Ljj (b * ~)(Xj - xh Yj - Yj) = L h~( d)jj 
j 
j 
and 
1 
[0 
1] 2 
1 
T 
Eo(f) = 20-2 ~ L hij(d)jj - gj = 20-2 (Hof - go) (Hof - go) 
I 
J 
(note that Ho is a function of the disparity field d). 
Having derived the discrete energy equations, we now compute the partial 
derivatives and derive a quadratic approximation to the energy in the vicinity of 
the optimal solution. As we explained in Section 5.2, we then use this approxi-
mate energy to define a multivariate Gaussian distribution which represents our 
current estimate. If we are only interested in computing the uncertainty in the 
current depth estimate d, we can use the approximation 
E'(d) = Ed(d) + L cj(dj - dj )2 
j 
where Cj can be approximated by [f(XhYj)]2/20-2• However, we will model the 
correlation between the intensity and disparity estimates in order to obtain a 
faster convergence of the incremental algorithm. 
Taking partial derivatives of 
E(f, d) = Et<f) + Ed(d) + E1 (f) + Eo(f, d) 
(7.24) 
we obtain 
These partial derivatives are somewhat less daunting then they look. We can 
derive fairly simple formulas for the partials of Ho(d) by making simplifying 
assumption about the blur function b(x, y). An alternative to performing this 
analysis is to use numerical techniques to estimate the partial derivatives. By 
measuring the increase in energy as we perturb the solution away from its optimal 

144 
Bayesian Modeling of Uncertainty ,in Low-Level Vision 
value, we can estimate these partial derivatives (this is analogous to measuring 
the local stiffness of the system). 
The partial derivatives can be used to model our current estimate as a mul-
tivariate Gaussian with an energy function 
E'(f, d) = -21 [ (f - fl (d - dl ] [8$1' 
8~:d] [ (f - 9 ] 
&cfl'8f 
8~~d 
(d - d) 
. 
Of course, we want to keep this representation as sparse as possible in order to 
minimize the amount of storage and subsequent computation that are required. 
Other than the prior matrices Af and Ad, we may be able to keep only the 
diagonal components, thus reducing these infonnation matrices to iconic fields. 
For improved accuracy, it may be desirable to keep a few off-diagonal tenns (e.g., 
between pixels that are adjacent horizontally). The exact tradeoffs involved will 
have to be detennined empirically when the algorithm is actually implemented. 
The new model for stereo matching presented here has several advantages 
over previous fonnulations. Because we are using a Bayesian framework, we can 
obtain better models for the sensors, model the non-unifonn sampling effects 
due to disparity gradients, and explicitly specify the expected smoothness of 
our intensity and disparity fields. U sing Bayesian models also allows use to 
compute the certainty in our disparity estimates. The quality of these certainty 
estimates is better than that obtained from the flow estimator used in the previous 
section, since we directly analyze the local variation of energy (log likelihood) 
with disparity. Most importantly, by modeling the cross-correlation between the 
intensity and disparity field estimates, we can integrate this stereo measurement 
with new images to obtain an on-line estimate of depth. 
7.3.2 Recursive motion estimation 
One of the shortcomings of the iconic depth-from-motion algorithm described 
in Section 7.2 is that in only attains a quadratic rate of convergence. By com-
parison, the symbolic depth-from-motion algorithm also studied by Matthies et 
al. (1987) is able to attain a cubic rate of convergence by tracking both the 
edge disparities and their sub-pixel positions. The joint modeling of intensity 
and disparity which we examine in this section has the potential to equal the 
accuracy of the symbolic method. As an infonnal justification, we note that if 
the local intensity gradient is constant, then the error in a disparity estimate dj 
is proportional to the error in the intensity estimate fi, which in turn depends 
on the error in the sampled intensities gi near node j. By refining our estimate 
of jj incrementally, and modeling the correlation between this estimate and the 
disparity estimate dj , we are perfonning the same kind of two-state correlated 
estimation which leads to the cubic convergence rate of the symbolic method. 

Incremental algorithms for depth-from-motion 
145 
Implementing the new iconic depth estimation algorithm, however, is more 
difficult than implementing the symbolic matching algorithm, since we must 
account for the spatial correlations induced by our prior smoothness models and 
by other sources such as the blur in the sensor model. As we explained in 
Section 7.1, we can remove the prior model components from the information 
matrix before we perform the prediction, in order to keep the matrix structure 
more sparse and to make the smoothness constraint constant over time. One 
problem with the straightforward prediction equations developed in Section 7.1 
is that in order to obtain a new disparity estimate, we need to measure the 
displacement between the current image and the old intensity estimate. For this 
reason, we will first present an update equation which avoids predicting intensity 
and disparity fields. 
We model the old intensity and disparity estimates by f and d, and the 
information matrix by 
where the second form shows the partitioning of the information matrix into 
a prior model component and a current data certainty component. The energy 
equation for the new estimates f and d afterincorporating the new measurements 
g, is 
1 
T 
E(f, d) = 2172 (H,f - g,) (H,f - g,) 
+! [(f 
_ f)A T (d 
_ d)T] [A/+ All 
Ald. 
] [ (f,_1 - 9 ] 
2 
I-I 
I-I 
Actt 
~ + ~ (d,_1 - d) 
where f,_1 and d,_1 are derived from f and d by applying the system model 
equations corresponding to (7.23). Since we wish to have the smoothness con-
straints involving AI and ~ expressed in terms of the current estimates f and d, 
we re-write the above equation as 
where 

146 
Bayesian Modeling of Uncertainty in Low-Level Vision 
are the estimates for intensity and disparity when the smoothing has been dis-
counted. By replacing f/_1 and d/_1 by f and d in the first two terms of (7.25), 
we obtain the desired energy equation. From this new energy equation, we can 
compute the optimal intensity and disparity estimates, and repeat the variational 
analysis that we presented for regularized stereo to obtain an error model. 
An alternative to this approach is to actually predict the flow and disparity 
fields from the old estimate, and to then compute the corrections that must be 
added to incorporate the new measurement. This approach is closer in spirit to 
the extended Kalman filter which we discussed in Section 7.1. In performing the 
prediction, we would separate out the prior model components of the information 
matrix, and preserve the sparseness of the remaining matrices. Which of the two 
approaches presented here is preferable remains to be seen. 
The new incremental iconic depth-from-motion algorithm which we have 
developed here has the potential to achieve near-optimal estimates of intensity 
and depth from the image sequence 7• It will be interesting to compare the results 
obtained with our algorithm to those obtained by "regularizing the whole depth-
from-motion problem,',g i.e., obtaining a batch estimate for the dense intensity 
and disparity fields from the spatio-temporal block of data. It may be possible 
for scenes with no occlusion-such as the flat-mounted poster used in Section 
7.2-to show that the results are equivalent. 
7.3.3 Adding discontinuities 
In the development of our new iconic depth-from-motion algorithm, we have so 
far omitted the discussion of intensity and disparity discontinuities. This omis-
sion was made to simplify the presentation. In fact, modeling the discontinuities 
in the two fields is one of the prime motivations for studying the joint modeling 
of intensity and disparity. The ability to use weak continuity constraints as prior 
models is a major advantage of the Bayesian estimation framework. These weak 
constraints better model the piecewise continuous nature of intensity images and 
visible surfaces. 
To incorporate discontinuities into our prior models, we add line variables 
positioned on a dual grid (Section 2.4). We then modify the discretized energy 
equations to include these new variables, and add energy terms to the prior 
models Elf> and Ed(d) which penalize discontinuities (Appendix A). Previ-
ous algorithms which use discontinuity processes (Terzopoulos 1984, Marroquin 
1984, Blake and Zisserman 1987) always represent these as binary variables; 
for our application, we will also represent the sub-pixel positions of the dis-
7 A linear or linearized filter such as the Kalman filter cannot give optimal performance on a 
non-linear problem such as depth-from-motion. 
8Suggested by Andrew Witkin. 

Incremental algorithms for depth-from-motion 
147 
continuities. Estimating the sub-pixel edge position will allow us to use the 
symbolic matching algorithm developed by Matthies et al. (1987) to obtain a 
cubic convergence rate for the depth estimates at these locations. 
The sub-pixel position of the edge can be represented by attaching a single 
real value to each binary line variable. A more iconic representation for this 
position is the interpolation coding scheme developed by Ballard (1987), which 
makes the warping operations used during the prediction phase more uniform. 
The estimation of the sub-pixel position can be done in conjunction with the 
line variable estimation or as a post-processing stage. We can use the Graduated 
Non-Convexity (GNC) algorithm of Blake and Zisserman (1987) for the intensity 
fields, since it is particularly well suited to finding discontinuities in dense input 
data. We can compute the edge position and its uncertainty from the shape of 
the blur function, the camera noise, and the local intensity values around the 
edge. 
The estimation of disparity discontinuities is more difficult, since they cannot 
be directly inferred from the intensity images, and may even be difficult to infer 
from the disparity map. This is especially true if the disparity map itself is 
sparse, which is often the case with stereo or motion-based depth estimates. 
One approach that may help is to link the likelihood of disparity edges to the 
existence of intensity edges (Gamble and Poggio 1987); this fits in well with our 
Bayesian modeling approach. Another possibility is to try to detect occlusions 
or disocclusions in the motion sequence. For each depth discontinuity that is 
found we can assign the depth estimate associated with that edge to the upper 
(nearer) surface bounded by the edge. Alternatively, we can use a separate depth 
value variable for each discontinuity that is detected. 
Much work needs to be done to convert the ideas presented in this section into 
a working algorithm. The potential for improved accuracy of such an algorithm 
over existing depth-from-motion estimators, however, is promising. By includ-
ing discontinuity processes into our intensity and disparity models, we obtain a 
more veridical description of the scene, and also improve the convergence rate 
of our depth estimates. The Bayesian formulation of depth-from-motion allows 
us to use better sensor models, to model the non-uniform sampling induced by 
disparity gradients, and compute the certainty associated with our estimates. We 
can also extend this framework to model other intrinsic images. For example, 
we could estimate the reflectance functions of the surfaces rather than the inten-
sities, and thus account for the variation of intensity with viewer position. Using 
full three-dimensional models instead of retinotopic maps would also increase 
the descriptive power of our system, and allow us to "remember" the surfaces 
that become temporarily occluded during a motion sequence (Figure 7.10). 
The dynamic Bayesian estimation framework which we have developed in 
this chapter is a good demonstration of the advantages of Bayesian modeling for 
low-level vision. Using the Kalman filter, we can accumulate measurements over 

148 
Bayesian Modeling of Uncertainty in Low-Level Vision 
time and improve the accuracy of our estimates. As we have shown, however, 
we must be careful about choosing our representations in order to produce a 
feasible algorithm. The iconic depth-from-motion algorithm which we reviewed 
in Section 7.2 is a practical demonstration of the utility of these ideas and of 
their applicability in real-world domains. The new algorithm which we have 
proposed in this section further exploits the power of Bayesian modeling, and 
suggests that significant improvements in descriptive power and accuracy are 
possible with this approach. 

Chapter 8 
Conclusions 
In this book, we have developed a Bayesian model for the dense fields that 
arise in low-level vision, and shown how this model can be be applied to a 
number of low-level vision problems. We have used this model to analyze the 
assumptions inherent in existing vision algorithms, to improve the performance 
of these algorithms, and to devise novel algorithms for problems which have 
not previously been studied. In this chapter, we will summarize these important 
results and discuss how they can be extended in the future to other computer 
vision problems. 
8.1 
Summary 
The main focus of this work has been the development of a Bayesian frame-
work for modeling dense fields and their associated uncertainties. Such fields 
are used in low-level vision to represent visible surfaces and intrinsic images. 
These retinotopic maps form a useful intermediate representation for integrating 
information from different low-level vision modules and sensors. Modeling the 
uncertainty in these maps is an essential component of the integration process, 
and provides a richer description for later stages of processing. 
The Bayesian framework we have developed is based on three separate prob-
abilistic models. The prior model describes the a priori knowledge that we have 
about the structure of the visual world. The sensor model describes how indi-
vidual measurements (such as image intensities) are obtained from a particular 
scene. The posterior model is derived from the first two models using Bayes' 
Rule and describes our current estimate of the scene given the measurements. 
By examining each of these models in turn, we have developed new algorithms 
for low-level vision problems as well as providing new insights into existing 
algorithms. 
In studying the prior model, we have analyzed the statistical assumptions 

150 
Bayesian Modeling of Uncertainty in Low-Level Vision 
in regularization-based smoothing, developed a new graphics algorithm, and 
developed a new multiresolution representation. The prior model captures the 
smoothness or coherence assumptions associated with a visible surface. We 
construct this model by using the smoothness constraint (stabilizer) from reg-
ularization to define the energy function of a Markov Random Field. Using 
Fourier analysis, we show how the choice of the stabilizer used in regulariza-
tion determines the power spectrum (and hence the correlation function) of the 
prior model. For the membrane and the thin plate-two of the most commonly 
used regularization models-we show that the resulting prior model describes a 
random fractal surface. This result leads us to a new understanding of the role 
of regularization: the choice of a particular stabilizer (degree of smoothness) is 
equivalent to assuming a particular power spectrum for the prior model. 
Using the regularization-based probabilistic prior model, we have devised 
two new algorithms for modeling surfaces. The first algorithm uses multigrid 
stochastic relaxation to generate fractal surfaces for computer graphics applica-
tions. These surfaces can be arbitrarily constrained with depth and orientation 
constraints and discontinuities; our method thus exhibits a degree of flexibility 
not present in previous algorithms. Our second algorithm is a relative multires-
olution representation for visible surfaces. This representation encodes local 
deviations in depth at each level of the pyramid, with the sum of all the levels 
defining the absolute depth map. We have shown how the power spectrum of 
the composite representation can be computed by summing the power spectra 
of the individual levels. This gives us a new technique for shaping the fre-
quency response characteristics of each level and for ensuring the desired global 
smoothing behavior of the system. 
In studying sensor models, we have developed a new constraint for sparse 
depth measurements and analyzed the uncertainty in optical flow and intensity 
images. Starting with the equivalence between a point sensor with Gaussian 
noise and a simple spring constraint, we show how to extend this model to 
other one-dimensional uncertainty distributions. We then develop a new sensor 
model which incorporates the full three-dimensional uncertainty associated with 
a sparse depth measurement. The constraint corresponding to this model acts 
like a force field or a slippery spring, and can thus be used in conjunction with 
parametrically defined surfaces. We show how to approximate this non-linear 
constraint by a quadratic energy function which fits into our visible surface 
representation. 
We have also applied sensor modeling to systems which produce dense mea-
surements. First, we analyze the uncertainty associated with correlation-based 
optical flow measurements. The uncertainty in the estimates is derived from 
the shape of the local error surface, thereby accounting for the spatially varying 
reliability of optical flow estimates. These flow measurements, along with their 
uncertainties, are used as part of an iconic depth-from-motion algorithm. Sec-

Conclusions 
151 
ond, we develop a simple stochastic model for a CCO camera which describes 
the blur, sampling and noise inherent in the imaging system. 
In studying the posterior model, we have shown how to calculate the un-
certainty in the posterior estimate from the energy function of the system, and 
we have developed two new algorithms to perform this computation. The first 
algorithm uses deterministic relaxation to calculate the uncertainty at each point 
separately. The second algorithm generates typical random samples from the 
posterior distribution and calculates statistics based on these samples. The un-
certainty map obtained from these algorithms can be used to set confidence limits 
on our measurements or to suggest where further active sensing is required. 
Using the probabilistic description of the posterior estimate, we have devel-
oped two new parameter estimation algorithms. The first algorithm estimates 
the optimal amount of smoothing to be used with regularization. This estimate 
is obtained by maximizing the likelihood of the data points that were observed 
given a particular (parameterized) prior model. The second algorithm determines 
observer or object motion given two or more sets of sparse depth measurements. 
The algorithm determines this motion-without using any correspondence be-
tween the sensed points-by maximizing the likelihood that point sets came 
from the same smooth surface. 
Finally, we extend our Bayesian model to temporal sequences using a two-
dimensional generalization of the Kalman filter. By paying careful attention to 
computational issues and to alternative representations, we obtain simple formu-
lations for the updating equations. Using this framework, we have developed 
two new depth-from-motion algorithms. 
The first algorithm estimates optical flow from successive pairs of images 
and incrementally refines the resulting disparity estimates and their associated 
confidences. This algorithm produces a dense on-line estimate of depth which 
improves over time. Experiments with real images have demonstrated the im-
proved accuracy which can be obtained with this approach, and shown that the 
reconstructed depth maps of the scene are quite realistic. The second depth-from-
motion algorithm jointly models the intensity and disparity fields along with their 
discontinuities. By using explicit prior and sensor models, this algorithm avoids 
some of the problems with existing techniques, such as the correlation between 
measurements in flow-based methods and the difficulty in dealing with dispar-
ity gradients. These two algorithms demonstrate the advantages of applying 
Bayesian modeling to low-level vision problems. 
8.2 Future research 
The Bayesian framework we have developed has thus far only been applied to 
visible surfaces (2Ih -D sketches) and to surface interpolation and depth-from-

152 
Bayesian Modeling of Uncertainty in Low-Level Vision 
motion. In future work, we plan to extend our Bayesian approach to other 
visual representations and to other computer vision problems. These include the 
extension to full three-dimensional models and to mUltiple intrinsic images as 
well as the development of more general depth-from-motion and shape-from-x 
algorithms. 
The extension to viewpoint-invariant surface models and energy-based tbree-
dimensional models is the most straightforward. To perform this extension, we 
use the smoothness energy associated with the surfaces to define the prior model 
through the Gibbs distribution. While the resulting distributions are no longer 
correlated Gaussians (because the energy functions are not quadratic), they are 
still Markov Random Fields (because of the local structure of the energy). Ap-
plying the Bayesian approach to these representations should produce similar 
benefits to those which we have demonstrated in this book, including the ability 
to use better sensor models and the ability to characterize the uncertainty in the 
estimates. We also plan to examine the extension to locally tensioned splines 
and to non-spline models such as the constant curvature sign models suggested 
by Blake and Zisserman (1987). 
Three-dimensional elastic net models are an ideal application for the new 
depth constraint developed in Section 5.2. We plan to study how these constraints 
can be used to fit three-dimensional models either to sparse data, such as that 
available from direct range sensing or tactile sensing, or to contour information. 
The advantage of the elastic net, which is a parametric surface, coupled with 
the force field depth constraint, is that we need not establish a correspondence 
between data and surface points. 
We also plan to apply the Bayesian modeling approach to multiple intrin-
sic images, thus providing a unified framework for describing many different 
low-level vision algorithms. For example, we can extend our new depth-from-
motion algorithm by estimating the reflectance functions (albedos) of the visual 
surfaces, and thus incorporate shading cues into the reconstruction process. We 
would also like to study the more general idea of intrinsic models-probabilistic 
descriptions of intrinsic images-and how to link these models to higher level 
three-dimensional models. In particular, we should examine how to use the 
uncertainty in the intermediate level estimates to determine the uncertainty in 
the three-dimensional model parameters. For this approach to be viable, how-
ever, we will first have to solve the problems of grouping, segmentation, and 
discontinuity detection. 
The relative multiresolution depth representation which we have introduced 
needs further refinement. In future work, we will develop efficient parallel al-
gorithms for the solution of the interpolation problems using this representation, 
and study how to assign the discontinuities to the appropriate level. We will also 
examine how to integrate information from local depth cues such as disparity 
gradients and how to use multiresolution inputs derived from frequency channel 

Conclusions 
153 
(band-pass) descriptions of the image. 
The extension of of our depth-from-motion algorithm to general motion is 
another area of future research. Combining this idea with full three-dimensional 
models, we could construct an active vision system which builds a three-di-
mensional description of its environment by roaming around. The Bayesian 
modeling of surfaces which we have developed in this book would be an essen-
tial component of such a system, allowing information from many viewpoints 
and sensor modalities to be integrated in a natural and statistically optimal fash-
ion. This statistical framework could also be used to jointly estimate the scene 
description and the observer motion, which is a much more difficult problem. 
The representations and algorithms which we have been studying are all ex-
amples of massively parallel computation. By implementing these algorithms 
on parallel computers, we will obtain significant speed-ups in operation and 
also shed light on desirable characteristics for new computer architectures. Ulti-
mately, some of the low-level operations will be implemented directly in silicon 
using VLSI technology, perhaps even using analog computations. 
In conclusion, we have developed in this book a Bayesian approach for 
estimating visual surfaces and other two-dimensional fields. The modeling of 
surfaces using this approach has several advantages. We have used Bayesian 
models to compute the uncertainties associated with visible surface estimates and 
to develop a number of novel robust low-level vision algorithms. By demonstrat-
ing that these algorithms can be used to solve real-world vision problems more 
efficiently than currently existing techniques, we have established that Bayesian 
modeling is a powerful and practical framework for low-level vision. 

Bibliography 
Ackley, D. H., Hinton, G. E., and Sejnowski, T. 1. (1985). A learning algorithm 
for Boltzmann Machines. Cognitive Science, 9" 147-169. 
Adelson, E. H. and Bergen, 1. R. (1985). Spatiotemporal energy models for the 
perception of motion. Journal of the Optical Society of America, A 2(2), 
284-299. 
Adler, R. J. (1981). The Geometry of Random Fields. 1. Wiley, Chichester, 
England. 
Aloimonos, J., Weiss, I., and Bandyopadhyay, A. (1987). Active vision. In 
First International Conference on Computer Vision (ICCV' 87), pages 35-
54, IEEE Computer Society Press, London, England. 
Anandan, P. (1984). Computing dense displacement fields with confidence 
measures in scenes containing occlusion. In Image Understanding Work-
shop, pages 236-246, Science Applications International Corporation, New 
Orleans, Louisiana. 
Anandan, P. and Weiss, R. (1985). Introducing a smoothness constraint in a 
matching approach for the computation of displacement fields. In Image 
Understanding Workshop, pages 186-196, Science Applications Interna-
tional Corporation, Miami Beach, Florida. 
Anderssen, R. S. and Bloomfield, P. (1974). A time series approach to numerical 
differentiation. Technometrics, 16(1), 69-75. 
Anstis, S. M. and Howard, I. P. (1978). A Craik-O'Brien-Cornsweet illusion 
for visual depth. Vision Research, 18, 213--217. 
Arnold, R. D. (1983). Automated Stereo Perception. Technical Report AIM-
351, Artificial Intelligence Laboratory, Stanford University. 
Baker, H. H. (1982). Depth from Edge and Intensity Based Stereo. Technical 
Report AIM-347, Artificial Intelligence Laboratory, Stanford University. 
Baker, H. H. (1989). Building surfaces of evolution: the weaving wall. Inter-
national Journal of Computer Vision, 3(1). 
Baker, H. H. and Bolles, R. C. (1989). Generalizing epipolar-plane image 
analysis on the spatiotemporal surface. International Journal of Computer 
Vision, 3(1). 
Ballard, D. H. (1987). Interpolation coding: a representation for numbers in 

156 
Bayesian Modeling of Uncertainty in Low-Level Vision 
neural models. Biological Cybernetics, 57, 389-402. 
Barnard, S. T. (1986). A stochastic approach to stereo vision. In Fifth National 
Conference on Artificial Intelligence (AMI-86), pages 676--680, Morgan 
Kaufmann Publishers, Philadelphia, Pennsylvania. 
Barnard, S. T. (1989). Stochastic stereo matching over scale. International 
Journal of Computer Vision, 3(1). 
Barnard, S. T. and Fischler, M. A. (1982). Computational stereo. Computing 
Surveys, 14(4), 553--572. 
Barrow, H. G. and Tenenbaum, J. M. (1978). Recovering intrinsic scene 
characteristics from images. In Hanson, A. R. and Riseman, E. M., editors, 
Computer Vision Systems, pages 3--26, Academic Press, New York, New 
York. 
Barrow, H. G. and Tenenbaum, J. M. (1981). Interpreting line drawings as 
three-dimensional surfaces. Artificial Intelligence, 17, 75-116. 
Bertero, M., Poggio, T., and Torre, V. (1987). III-posed problems in early 
vision. A. I. Memo 924, Massachusetts Institute of Technology. 
Bierman, G. J. (1977). Factorization Methods for Discrete Sequential Estima-
tion. Academic Press, New York, New York. 
Blake, A. and Zisserman, A. (1986a). Invariant surface reconstruction using 
weak continuity constraints. In IEEE Computer Society Conference on 
Computer Vision and Pattern Recognition (CVPR'86), pages 62-68, IEEE 
Computer Society Press, Miami Beach, Florida. 
Blake, A. and Zisserm an , A. (1986b). Some properties of weak continuity 
constraints and the GNC algorithm. In IEEE Computer Society Conference 
on Computer Vision and Pattern Recognition (CVPR' 86), pages 656--661, 
IEEE Computer Society Press, Miami Beach, Florida. 
Blake, A. and Zisserman, A. (1987). Visual Reconstruction. MIT Press, 
Cambridge, Massachusetts. 
Bolles, R. C. and Baker, H. H. (1985). Epipolar-plane image analysis: a tech-
nique for analysing motion sequences. In Third International Symposium 
of Robotics Research, pages 192-199, Gouvieux, France. 
Bolles, R. c., Baker, H. H., and Marimont, D. H. (1987). Epipolar-plane image 
analysis: an approach to determining structure from motion. International 
Journal of Computer Vision, 1, 7-55. 
Boult, T. E. (1986). Information Based Complexity in Non-Linear Equations 
and Computer Vision. Ph.D. thesis, Columbia University. 
Bracewell, R. N. 
(1978). 
The Fourier Transform and its Applications. 
McGraw-Hill, New York, New York, 2nd edition. 
Brooks, R. A., Greiner, R., and Binford, T. O. (1979). The ACRONYM model-
based vision system. In Sixth International Joint Conference on Artificial 
Intelligence (IJCAI-79), pages 105-113, Tokyo, Japan. 
Burt, P. J. and Adelson, E. H. (1983). The Laplacian pyramid as a compact 

Bibliography 
157 
image code. IEEE Transactions on Communications, COM-3I(4), 532-
540. 
Canny, J. (1986). A computational approach to edge detection. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, PAMI-8(6), 679-698. 
Chen, L. and Boult, T. E. (1988). An integrated approach to stereo matching, 
surface reconstruction and depth segmentation using consistent smoothness 
assumptions. In Image Understanding Workshop, pages 166-176, Morgan 
Kaufmann Publishers, Cambridge, Massachusetts. 
Choi, D. 1. (1987). Solving the depth interpolation problem on a fine grained, 
mesh- and tree-connected SIMD machine. In Image Understanding Work-
shop, pages 639-643, Morgan Kaufmann Publishers, Los Angeles, Cali-
fornia. 
Christ, J. P. (1987). Shape Estimation and Object Recognition Using Spatial 
Probability Distributions. Ph.D. thesis, Carnegie Mellon University. 
Craven, P. and Wahba, G. (1979). Smoothing noisy data with spline functions: 
estimating the correct degree of smoothing by the method of generalized 
cross-validation. Numerische Mathematik, 31, 377-403. 
Crowley, J. L. and Stern, R. M. (1982). Fast Computation of the Difference of 
Low-Pass Transform. Technical Report CMU-RI-TR-82-18, The Robotics 
Institute, Carnegie Mellon University. 
Dev, P. (1974). Segmentation Processes in Visual Perception: A Coopera-
tive Neural Model. COINS Technical Report 74C-5, University of Mas-
sachusetts at Amherst. 
Drumheller, M. and Poggio, T. (1986). On parallel stereo. In IEEE Interna-
tional Conference on Robotics and Automation, pages 1439-1448, IEEE 
Computer Society Press, San Francisco, California. 
Duda, R. O. and Hart, P. E. (1973). Pattern Classification and Scene Analysis. 
J. Wiley, New York, New York. 
Durbin, R. and Willshaw, D. (1987). An analogue approach to the traveling 
salesman problem using an elastic net method. Nature, 326, 689-691. 
Durbin, R., Szeliski, R., and Yuille, A. (1989). An analysis of the elastic net 
approach to the travelling salesman problem. In Snowbird Neural Networks 
Meeting, Snowbird, Utah. 
Durrant-Whyte, H. F. (1987). Consistent integration and propagation of dis-
parate sensor observations. International Journal of Robotics Research, 
6(3),3-24. 
Elfes, A. and Matthies, L. (1987). Sensor integration for robot navigation: 
combining sonar and stereo range data in a grid-based representation. In 
IEEE Conference on Decision and Control, IEEE Computer Society Press. 
Faugeras, O. D. and Hebert, M. (1987). The representation, recognition and 
positioning of 3-D shapes from range data. In Kanade, T., editor, Three-
Dimensional Machine Vision, pages 301-353, Kluwer Academic Publish-

158 
Bayesian Modeling of Uncertainty in Low-Level Vision 
ers, Boston, Massachusetts. 
Faugeras, O. D., Ayache, N., and Faverjon, B. (1986). Building visual maps by 
combining noisy stereo measurements. In IEEE International Conference 
on Robotics and Automation, pages 1433--1438, IEEE Computer Society 
Press, San Francisco, California. 
Foley, T. A. (1987). Weighted bicubic spline interpolation to rapidly varying 
data. ACM Transactions on Graphics, 6(1), 1-18. 
Fournier, A., Fussel, D., and Carpenter, L. (1982). Computer rendering of 
stochastic models. Communications of the ACM, 25(6), 371-384. 
Gamble, E. and Poggio, T. (1987). Visual integration and detection of dis-
continuities: the key role of intensity edges. A. I. Memo 970, Artificial 
Intelligence Laboratory, Massachusetts Institute of Technology. 
Gelb, A., editor. (1974). Applied Optimal Estimation. MIT Press, Cambridge, 
Massachusetts. 
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distribution, 
and the Bayesian restoration of images. IEEE Transactions on Pattern 
Analysis and Machine Intelligence, PAMI-6(6), 721-741. 
Geman, S. and McClure, D. E. (1987). Statistical methods for tomographic im-
age reconstruction. In 46th Session of the International Statistical Institute, 
Bulletin of the lSI, vol. 52. 
Gremban, K. D., Thorpe, C. E., and Kanade, T. (1988). Geometric cam-
era calibration using systems of linear equations. In IEEE International 
Conference on Robotics and Automation, pages 562-567, IEEE Computer 
Society Press, Philadelphia, Pennsylvania. 
Grimson, W. E. L. (1981). From Images to Surfaces: a Computational Study 
of the Human Early Visual System. MIT Press, Cambridge, Massachusetts. 
Grimson, W. E. L. (1983). An implementation of a computational theory 
of visual surface interpolation. Computer Vision, Graphics, and Image 
Processing, 22, 39-69. 
Hackbusch, W. (1985). Multigrid Methods and Applications. Springer-Verlag. 
Berlin. 
Hackbusch, W. and Trottenberg, U., editors. 
(1982). Multigrid Methods, 
Springer-Verlag, Berlin, Heidelberg, New York. 
Harris, J. G. (1987). A new approach to surface reconstruction: the cou-
pled depth/slope model. In First International Conference on Computer 
Vision (ICCV' 87), pages 277-283, IEEE Computer Society Press, London, 
England. 
Hebert, M. and Kanade, T. (1988). 3-D vision for outdoor navigation by an 
autonomous vehicle. In Image Understanding Workshop, pages 365-382, 
Morgan Kaufmann Publishers, Cambridge, Massachusetts. 
Hebert,M., Kanade, T., and Kweon, I. (1988). 3-D Vision Techniques for 
Autonomous Vehicles. Technical Report CMU-RI-TR-88-12, The Robotics 

Bibliography 
159 
Institute, Carnegie Mellon University. 
Heeger, D. 1. (1986). Depth and flow from motion energy. In Fifth National 
Conference on Artificial Intelligence (AAAl-86), pages 657-661, Morgan 
Kaufmann Publishers, Philadelphia, Pennsylvania. 
Heeger, D. J. (1987). Optical flow from spatiotemporal filters. In First Inter-
national Conference on Computer Vision (ICCV'87), pages 181-190, IEEE 
Computer Society Press, London, England. 
Henderson, R. L., Miller, W. J., and Grosch, C. B. (1979). Automated stereo 
reconstruction of man-made targets. SPIE, 186(Digital Processing of Aerial 
Images), 24~248. 
Hildreth, E. C. (1982). The integration of motion information along contours. 
In IEEE Workshop on Computer Vision, pages 8>-91, IEEE Computer 
Society Press, Rindge, New Hampshire. 
Hinton, G. E. (1977). Relaxation and its Role in Vision. Ph.D. thesis, University 
of Edinburgh. 
Hinton, G. E. and Sejnowski, T. J. (1983). Optimal perceptual inference. 
In IEEE Computer Society Conference on Computer Vision and Pattern 
Recognition (CVPR' 83), pages 448-453, IEEE Computer Society Press, 
Washington, D. C. 
Hoff, W. and Ahuja, N. (1986). Surfaces from stereo. In Eighth International 
Conference on Pattern Recognition (ICPR' 86), pages 516-518, IEEE Com-
puter Society Press, Paris, France. 
HopfieJd, 1. 1. (1982). Neural networks and physical systems with emergent 
collective computational abilities. Proceedings of the National Academy of 
Sciences U.sA., 79, 2554-2558. 
Horn, B. K. P. (1977). Understanding image intensities. Artificial Intelligence, 
8,201-231. 
Horn, B. K. P. and Brooks, M. J. (1986). The variational approach to shape from 
shading. Computer Vision, Graphics, and Image Processing, 33, 174-208. 
Horn, B. K. P. and Schunck, B. G. (1981). Determining optical flow. Artificial 
Intelligence, 17, 185-203. 
Hueckel, M. H. (1971). An operator which locates edges in digitized pictures. 
Journal of the Associationfor Computing Machinery, 18(1), 11>-125. 
Hung, Y., Cooper, D. B., and Cernushi-Frias, B. (1988). Bayesian estimation 
of 3-D surfaces from a sequence of images. In IEEE International Confer-
ence on Robotics and Automation, pages 906-911, IEEE Computer Society 
Press, Philadelphia, Pennsylvania. 
Ikeuchi, K. and Horn, B. K. P. (1981). Numerical shape from shading and 
occluding boundaries. Artificial Intelligence, 17, 141-184. 
Julesz, B. (1971). Foundations of Cyclopean Perception. Chicago University 
Press, Chicago, Illinois. 
Kanade, T. (1981). Recovery of the three-dimensional shape of an object from 

160 
Bayesian Modeling of Uncertainty in Low-Level Vision 
a single view. Artificial Intelligence, 17, 409-460. 
Kass, M., Witkin, A., and Terzopoulos, D. (1988). Snakes: active contour 
models. International Journal of Computer Vision, 1(4),321-331. 
Kass, M. H. (1984). Computing Stereo Correspondence. Master's thesis, 
Massachusetts Institute of Technology. 
Kimeldorf, G. and Wahba, G. (1970). A correspondence between Bayesian 
estimation on stochastic processes and smoothing by splines. The Annals 
of Mathematical Statistics, 41(2), 495-502. 
Kirkpatrick, S., Gelatt, C. D. J., and Vecchi, M. P. (1983). Optimization by 
simulated annealing. Science, 220, 671-680. 
Koch, C., Marroquin, J., and Yuille, A. (1986). Analog "neuronal" networks 
in early vision. Proceedings of the National Academy of Sciences U.SA., 
83,4263-4267. 
Konrad, J. and Dubois, E. (1988). Multigrid bayesian estimation of image mo-
tion fields using stochastic relaxation. In Second International Conference 
on Computer Vision (ICCV' 88), pages 354-362, IEEE Computer Society 
Press, Tampa, Florida. 
Leclerc, Y. G. (1989). Constructing simple stable descriptions for image parti-
tioning. International Journal of Computer Vision, 3(1),75-104. 
Leclerc, Y. G. and Zucker, S. W. (1987). The local structure of image dis-
continuities in one dimension. IEEE Transactions on Pattern Analysis and 
Machine Intelligence, PAMI-9(3), 341-355. 
Lehky, S. R. and Sejnowski, T. J. (1988). Network model of shape-from-
shading: neural function arises from both receptive and projective fields. 
Nature, 333, 452-454. 
Lewis, J. P. (1987). Generalized stochastic subdivision. ACM Transactions on 
Graphics, 6(3), 167-190. 
Lowe, D. G. (1985). Perceptual Organization and Visual Recognition. Kluwer 
Academic Publishers, Boston, Massachusetts. 
Lucas, B. D. (1984). Generalized Image Matching by the Method of Differ-
ences. Ph.D. thesis, Carnegie Mellon University. 
Mallat, S. G. (1987). Scale change versus scale space representation. In First 
International Conference on Computer Vision (ICCV' 87), pages 592-596, 
IEEE Computer Society Press, London, England. 
Mandelbrot, B. B. (1982). The Fractal Geometry of Nature. W. H. Freeman, 
San Francisco, California. 
Marr, D. (1978). Representing visual infonnation. In Hanson, A. R. and 
Riseman, E. M., editors, Computer Vision Systems, pages 61-80, Academic 
Press, New York, New York. 
Marr, D. (1982). Vision: A Computational Investigation into the Human 
Representation and Processing of Visual Information. W. H. Freeman, San 
Francisco, California. 

Bibliography 
161 
Marr, D. and Hildreth, E. (1980). Theory of edge detection. Proceedings of 
the Royal Society of London, B 207, 187-217. 
Marr, D. and Poggio, T. (1976). Cooperative computation of stereo disparity. 
Science, 194, 283--287. 
Marr, D. C. and Poggio, T. (1979). A computational theory of human stereo 
vision. Proceedings of the Royal Society of London, B 204, 301-328. 
Marroquin, J. L. (1984). Surface Reconstruction Preserving Discontinuities. 
A. I. Memo 792, Artificial Intelligence Laboratory, Massachusetts Institute 
of Technology. 
Marroquin, J. L. (1985). Probabilistic Solution of Inverse Problems. Ph.D. 
thesis, Massachusetts Institute of Technology. 
Matthies, L. H. and Shafer, S. A. (1987). Error modeling in stereo navigation. 
IEEE Journal of Robotics and Automation, RA-3(3), 239-248. 
Matthies, L. H., Szeliski, R., and Kanade, T. (1987). Kalman Filter-based 
Algorithms for Estimating Depth from Image Sequences. Technical Re-
port CMU-CS-87-185, Computer Science Department, Carnegie Mellon 
University. 
Matthies, L. H., Szeliski, R., and Kanade, T. (1989). Kalman filter-based algo-
rithms for estimating depth from image sequences. International Journal 
of Computer Vision, . 
Maybeck, P. S. (1979). Stochastic Models, Estimation, and Control. Volume 1, 
Academic Press, New York, New York. 
Maybeck, P. S. (1982). Stochastic Models, Estimation, and Control. Volume 2, 
Academic Press, New York, New York. 
McDermott, D. (1980). Spatial Inferences with Ground, Metric Formulas on 
Simple Objects. Research Report 173, Department of Computer Science, 
Yale University. 
Mead, C. A. and Mahowald, M. A. (1988). A silicon model of early visual 
processing. Neural Networks, 1, 91-97. 
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, 
E. (1953). Equations of state calculations by fast computing machines. 
Journal of Chemical Physics, 21,1087-1091. 
Moravec, H. P. (1988). Sensor fusion in certainty grids for mobile robots. AI 
Magazine, 9(2), 61-74. 
Nalwa, V. (1986). On detecting edges. IEEE Transactions on Pattern Analysis 
and Machine Intelligence, PAMI-8(6), 699-714. 
Ohta, Y. and Kanade, T. (1985). Stereo by intra- and inter-scanline search 
using dynamic programming. IEEE Transactions on Pattern Analysis and 
Machine Intelligence, PAMI-7(2), 139-154. 
Pentland, A. P. (1984). Fractal-based description of natural scenes. IEEE 
Transactions on Pattern Analysis and Machine Intelligence, PAMI-6(6), 
661-674. 

162 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Pentland, A. P. (1986). Perceptual organization and the representation of natural 
fonn. Artificial Intelligence, 28(3),293-331. 
Poggio, T. et al .. (1988). The MIT vision machine. In Image Understanding 
Workshop, pages 177-198, Morgan Kaufmann Publishers, Boston, Mas-
sachusetts. 
Poggio, T. and Torre, V. (1984). III-posed problems and regularization anal-
ysis in early vision. In Image Understanding Workshop, pages 257-263, 
Science Applications International Corporation, New Orleans, Louisiana. 
Poggio, T., Torre, V., and Koch, C. (1985a). Computational vision and regu-
larization theory. Nature, 317(6035), 314-319. 
Poggio, T., Voorhees, H., and Yuille, A. (1985b). A Regularized Solution 
to Edge Detection. A. I. Memo 833, Artificial Intelligence Laboratory, 
Massachusetts Institute of Technology. 
Prazdny, K. (1985). Detection of binocular disparities. Biological Cybernetics, 
52,93-99. 
Press, W. et al .. (1986). Numerical Recipes: The Art of Scientific Computing. 
Cambridge University Press, Cambridge, England. 
Quam, L. H. 
(1984). Hierarchical warp stereo. In Image Understanding 
Workshop, pages 149-155, Science Applications International Corporation, 
New Orleans, Louisiana. Also available as Technical Note No. 402, SRI 
International, December, 1986. 
Rensink, R. A. (1986). On the Visual Discrimination of Self-Similar Random 
Textures. Master's thesis, The University of British Columbia. 
Rives, P., BreuiI, E., and Espiau, B. (1986). Recursive estimation of 3D fea-
tures using optical flow and camera motion. In Conference on Intelligent 
Autonomous Systems, pages 522-532, Elsevier Science Publishers. (also 
appeared in 1987 IEEE International Conference on Robotics and Automa-
tion). 
Roberts, L. G. (1965). Machine perception of three-dimensional solids. In 
Tippett et al., editors, Optical and Electro-Optical Information Processing, 
chapter 9, pages 159-197, MIT Press, Cambridge, Massachusetts. 
Rosenfeld, A. (1980). Quadtrees and pyramids for pattern recognition and im-
age processing. In Fifth International Conference on Pattern Recognition 
(ICPR' 80), pages 802-809, IEEE Computer Society Press, Miami Beach, 
Florida. 
Rosenfeld, A., editor. (1984). Multiresolution Image Processing and Analysis, 
Springer-Verlag, New York, New York. 
Rosenfeld, A. and Kak, A. C. (1976). Digital Picture Processing. Academic 
Press, New York, New York. 
Rosenfeld, A., Hummel, R. A., and Zucker, S. W. (1976). Scene labeling by re-
laxation operations. IEEE Transactions on Systems, Man, and Cybernetics, 
SMC-6,420--433. 

Bibliography 
163 
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal 
representations by error propagation. In Rumelhart, D. E., McClelland, 
J. L., and the PDP research group, editors, Parallel distributed processing: 
Explorations in the microstructure of cognition, Bradford Books, Cam-
bridge, Massachusetts. 
Shafer, S. A. 
(1988). Automation and Calibration for Robot Vision Sys-
tems. Technical Report CMU-CS-88-147, Computer Science Department, 
Carnegie Mellon University. 
Shafer, S. A. and Kanade, T. (1983). The Theory of Straight Homogeneous 
Generalized Cylinders and A Taxonomy of Generalized Cylinders. Tech-
nical Report CMU-CS-83-105, Computer Science Department, Carnegie 
Mellon University. 
Sivilotti, M. A., Mahowald, M. A., and Mead, C. A. 
(1987). Real-time 
visual computations using analog CMOS processing arrays. In Losleben, 
P., editor, Advanced Research in VLSI: Proceedings of the 1987 Stanford 
Conference, pages 295-312, MIT Press, Cambridge, Massachusetts. 
Stewart, W. K. (1987). A non-deterministic approach to 3-D modeling un-
derwater. In Fifth International Symposium on Unmanned Untethered Sub-
mersible Technology, pages 283-309, University of New Hampshire Marine 
Systems Engineering Laboratory. 
Szeliski, R. (1986). Cooperative Algorithms for Solving Random-Dot Stere-
ograms. Technical Report CMU-CS-86-133, Computer Science Depart-
ment, Carnegie Mellon University. 
Szeliski, R. 
(1987). Regularization uses fractal priors. In Sixth National 
Conference on Artificial Intelligence (AAAI-87), pages 749-754, Morgan 
Kaufmann Publishers, Seattle, Washington. 
Szeliski, R. (1988a). Estimating motion from sparse range data without cor-
respondence. 
In Second International Conference on Computer Vision 
(ICCV' 88), pages 207-216, IEEE Computer Society Press, Tampa, Florida. 
Szeliski, R. (1988b). Some experiments in calibrating the Sony CCD camera. 
Internal report, IUS group, Carnegie Mellon University. 
Szeliski, R. (1989). Fast surface interpolation using hierarchical basis func-
tions. In IEEE Computer Society Conference on Computer Vision and Pat-
tern Recognition (CVPR' 89), IEEE Computer Society Press, San Diego, 
California. 
Szeliski, R. and Hinton, G. (1985). Solving random-dot stereograms using 
the heat equation. In IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition (CVPR' 85), pages 284-288, IEEE Computer 
Society Press, San Francisco, California. 
Szeliski, R. and Terzopoulos, D. (1989a). From splines to fractals. Computer 
Graphics (SIGGRAPH'87), 23(4). 
Szeliski, R. and Terzopoulos, D. (1989b). Parallel multigrid algorithms and 

164 
Bayesian Modeling of Uncertainty in Low-Level Vision 
applications to computer vision. In Fourth Copper Mountain Conference 
on Multigrid Methods, (Preprints), Copper Mountain, Colorado. 
Terzopoulos, D. (1983). Multilevel computational processes for visual surface 
reconstruction. Computer Vision, Graphics, and Image Processing, 24, 
52-96. 
Terzopoulos, D. (1984). Multiresolution Computation of Visible-Surface Rep-
resentations. Ph.D. thesis, Massachusetts Institute of Technology. 
Terzopoulos, D. (1985). Concurrent multilevel relaxation. In Baumann, L. S., 
editor, Image Understanding Workshop, pages 156--162, Science Applica-
tions International Corporation, Miami Beach, Florida. 
Terzopoulos, D. (1986a). Image analysis using multigrid relaxation methods. 
IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-
8(2), 129-139. 
Terzopoulos, D. (1986b). Regularization of inverse visual problems involv-
ing discontinuities. IEEE Transactions on Pattern Analysis and Machine 
Intelligence, PAMI-8(4), 413-424. 
Terzopoulos, D. (1987). Matching deformable models to images: direct and 
iterative solutions. In Topical Meeting on Machine Vision, pages 164-167, 
Optical Society of America, Washington, D. C. 
Terzopoulos, D. (1988). The computation of visible-surface representations. 
IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-
10(4),417-438. 
Terzopoulos, D., Witkin, A., and Kass, M. (1987). Symmetry-seeking models 
and 3D object reconstruction. International Journal of Computer Vision, 
1(3),211-221. 
Tikhonov, A. N. and Arsenin, V. Y. (1977). Solutions of Ill-Posed Problems. 
V. H. Winston, Washington, D. C. 
Tsai, R. Y. and Huang, T. S. (1984). Uniqueness and estimation of three-
dimensional motion parameters of rigid objects with curved surfaces. IEEE 
Transactions on Pattern Analysis and Machine Intelligence, PAMI-6( 1), 
13-27. 
Ullman, S. (1979). The Interpretation of Visual Motion. MIT Press, Cambridge, 
Massachusetts. 
Van Essen, D. C. and Maunsell, J. H. R. (1983). Hierarchical organization 
and functional streams in the visual cortex. Trends in Neuroscience, 6, 
370-375. 
Voss, R. F. (1985). Random fractal forgeries. In Earnshaw, R. A., editor, 
Fundamental Algorithms for Computer Graphics, Springer-Verlag, Berlin. 
Wahba, G. (1983). Bayesian "confidence intervals" for the cross-validated 
smoothing spline. Journal of the Royal Statistical Society, B 45(1), 133-
150. 
Waltz, D. L. (1975). Understanding line drawings of scenes with shadows. 

Bibliography 
165 
In Winston, P., editor, The Psychology of Computer Vision, McGraw-Hill, 
New York, New York. 
Webb, J. A. and Aggarwal, J. K. (1981). Visually interpreting the motion of 
objects in space. Computer, 14(8),40-46. 
Wilson, K. G. (1979). Problems in physics with many scales of length. Scien-
tific American, 241(2), 158-179. 
Witkin, A., Terzopoulos, D., and Kass, M. (1987). Signal matching through 
scale space. International Journal of Computer Vision, 1, 133-144. 
Witkin, A. P. (1981). Recovering surface shape and orientation from texture. 
Artificial Intelligence, 17, 17-45. 
Witkin, A. P. (1983). Scale-space filtering. In Eighth International Joint 
Conference on Artificial Intelligence (IJCAl-83), pages 1019-1022, Morgan 
Kaufmann Publishers. 
Woodham, R. J. (1981). Analysing images of curved surfaces. Artificial 
Intelligence, 17, 117-140. 
Yserentant, H. (1986). On the multi-level splitting of finite element spaces. 
Numerische Mathematik, 49, 379-412. 
Zucker, S. W. (1986). Early orientation selection: inferring trace, tangent, and 
curvature fields. In Eighth International Conference on Pattern Recognition 
(ICPR' 86), pages 294-302, IEEE Computer Society Press, Paris, France. 

Appendix A 
Finite element implementation 
In this Appendix, we present the discrete implementation of the energy equations 
used in Section 2.2. Our implementation is based on previous work described 
by Terzopoulos (1984), Marroquin (1984), Blake and Zisserman (1987), and 
Harris (1987). The energy equations which we develop are formulated in terms 
of a number of discrete fields which form the basic data structures used in the 
surface interpolation algorithm: 
U' , 
',j 
d' , 
',j 
Cj,j 
lj,j,mj,j 
nj,j 
surface depth (free variables) 
depth constraints 
depth constraint weights «(]"-2) 
depth discontinuities [0,1] 
orientation discontinuities [0,1] 
The line variables lj,j and mj,j are located on a dual grid, as shown in Figure 
2.16. The crease variables nj,j are coincident with the depth value nodes. 
To describe the discrete version of the smoothness constraint, we define a 
number of implicit fields which are not actually computed by the algorithm but 
serve only as a notational shorthand. First, we define the finite differences 
U~, = Uj+I)' -
Uj)' 
't) 
, 
, 
ul,j = Uj,j+ 1 -
Uj,j 
u~ = U~l ,- u~, = Uj+2)' -
2u,'+I)' +u,,), 
',} 
l+ ,i 
'tl 
' 
" 
u~ = u~, 1 -
u~, = uy 1 ,- uy, = Uj+1 )'+,' -
Uj+1 )' -
Uj )'+1 + Uj)' 
't) 
l,}+ 
I,) 
&+ Ii 
I,} 
, 
, t  
, 
ul,} = Ul,j+l -
ul,j = Uj,j+2 -
2Uj,j+l + Uj,j' 
Next, we define the continuity strengths 

168 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Figure A.1: Continuity strengths and computational molecules 
f3i3 = (1 -li,j)(l - li,j+l)(1 - mi,j)(1 - mi+l,j)(l - ni,jni+l,j+l)(1 - ni+l,jni,j+l) 
f3r3 = (1 - mi,j)(l - mi,j+l)(1 - ni,j+l). 
These continuity strengths determine which of the molecules defined by the 
u[j' ... ' uB finite differences are active (we use phantom line variables around 
the edges of the grid to take care of boundary conditions). 
The interactions between the depth field, the line and crease variables, and 
the continuity strengths are shown in Figure A.I. Intuitively, the line variables 
disable any molecules that are cut when the line variable is turned on. The crease 
variables disable the utJ or uB molecules whose centers are coincident with the 
crease, and disable the ut} molecules if two opposite corners are creased. Note 
that instead of splicing in a small membrane element at a crease location (as 
is done by Terzopoulos (1984», we simply disable the hinge (second order) 
element centered on the crease. Creases in the surface thus do not have a 
tendency to flatten the surface locally. 
The prior energy (weak smoothness constraint) is constructed from these 
elements 
where h = .:1.x = Lly is the grid spacing. The weighting functions wm, which 
come from the general controlled-continuity constraint (2.6), define the order of 
the interpolator. In terms of the notation used by Terzopoulos (1986b) which is 
shown in (2.5), we have 
{wo, wI. W2} = {O, p(x,y)[l - T(X,y)], p(X,y)T(X, y)} 

Finite element implementation 
169 
(in our implementation, we use the line and crease variables to represent the 
discontinuities rather than spatially varying wm). In tenns of the notation used 
by Blake and Zissennan (1987), we have 
{wo, WI, W2} = {D, oX2,jl4}. 
Blake and Zisserman (1987) claim that the oX and jl parameters are more natural, 
since they correspond to the characteristic lengths or scales of the smoothing 
filters implied by regularization (Appendix B). Our own preference for using 
the Wm parameterization comes from its usefulness when designing interpolators 
with fractional degrees of continuity (Section 4.2). 
The equation for the data compatibility constraint is the same as we presented 
in Section 2.2, namely 
1 '" 
2 
Ed(U, d) = 2: ~ 
Ci,j(Ui,j -
di,j) 
(i,j) 
(A.2) 
with Ci,j = a;} being the inverse variance of the measurement di,j' If we wish 
to add penalty terms for the lines and creases (because we are performing joint 
optimization of the surface and its discontinuities), we can use the same equation 
as used by Blake and Zisserman (1987) 
Ec(l, m, n) = L Odi,j + Ommi,j + Ollni,j' 
(i,}) 
(A.3) 
More complicated energies which try to enforce continuity in the lines could 
also be used (Geman and Geman 1984). 
The stiffness matrix A and the weighted data vector b can be obtained from 
the discrete energy equation 
by setting 
a(i,}),(k, I) = a ~ E(u)1 
Ui,j Uk, I 
u:O 
(A.4) 
and 
bi,j = - aa E(u)1 
. 
Ui,j 
u=o 
(A.S) 
In the case where E(u) is not quadratic, we can evaluate the above expressions 
at the current estimate u, and solve ALlu = b. 
To derive the energy equations at a coarse level from the fine level equations, 
we have two choices. We can use the interpolation equations that map from the 
coarse to the fine level to derive the new equations; alternatively, we can compute 
the coarse level data constraints di,j and Ci,j and the line and crease variables li,j, 

170 
Bayesian Modeling of Uncertainty in Low-Level Vision 
mi,j and ni,j from the fine level, and then use the usual finite-element analysis 
to obtain the discrete energy equations. 
Using the first approach, we choose an interpolation matrix F~ to map from 
the coarse to the fine level 
The fine level quadratic equation can thus be re-written as 
E(uf ) = ufT Afuf _ UfT bf + k 
= (F~uCl Af(F~uC) -
ucTF~Tbf + k 
= ucT ACuc _ ucTbc + k 
where 
The problem with this approach is that the neighborhoods implementing the 
smoothness constraint get progressively larger and lose their simple structure. 
The smoothing behavior, which can be detennined by taking the Fourier trans-
fonn of an entry from A (see Appendix B) also becomes worse (compared to 
that obtained using the original finite-element equations). 
For this reason, we have chosen to use the second approach. The depth 
value constraints for the coarse level are obtained from the fine level constraints 
by weighted averaging 
1 
1 
C~,j = L L cfzi+k,2j+1 
(A.6) 
k=O 1=0 
and 
1 
1 
1 
dt,j = C~. L L CLk,2j+/~i+k,2j+I' 
I,J k=O 1=0 
(A.7) 
This corresponds to assuming a block interpolation function. Using more so-
phisticated interpolation functions will give better coarse level constraints, but 
will also introduce off-diagonal tenns into the data constraints (Section 5.1). 
The line and crease variables are similarly obtained from the fine level equa-
tions using the logical OR function, 
1 
1 
f;,j = V V lzi+k,2j+1 
k=O/=O 
1 
1 
m'j,j = V V mLk,2j+1 
k=O 1=0 
1 
1 
nL = V V 
nLk,2j+I' 
k=O 1=0 

Finite element implementation 
171 
Again, more sophisticated combination rules could be used, but they have 
not been implemented for reasons of simplicity. Once all of the new fields 
di,j, ..• , ni,j have been derived for the coarse level, we can compute the discrete 
smoothness and data compatibility equations as before. 

Appendix B 
Fourier analysis 
By taking a Fourier transform of the function u(x) and expressing the energy 
equations in the frequency domain, we can analyze the spectral properties of our 
Bayesian models and the convergence properties of our estimation algorithms. 
In Section 4.1, we used Fourier analysis to compute the spectral characteristics 
of the prior model. In this Appendix, we will analyze the filtering behavior 
of regularization and the spectral characteristics of the posterior model. We 
will also examine the convergence properties of gradient descent and the fre-
quency response characteristics of our discrete implementation. We will then 
use these results to determine the convergence properties of multigrid relaxation, 
and to demonstrate its advantages both for deterministic MAP estimation and 
for stochastic sampling. 
B.I Filtering behavior of regularization 
As we showed in Section 4.1, the energy of the smoothness constraint (prior 
model) used in regularization can be expressed in the frequency domain as 
(B.l) 
where 
P 
IHp(f)12 = L WmI27rfl2m· 
=<J 
Calculating the energy associated with the smoothness constraint is thus equiv-
alent to passing the signal u(x) through a filter whose impulse response is hp(x), 
and then taking the usual (signal squared) energy measure 
(B.2) 

174 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Similarly, if we assume that the measured data d(x) is dense and has uniform 
weighting (7-2, we can derive from (2.7) the data compatibility term expressed 
in the Fourier domain 
(B.3) 
where 
IHd(f)12 = (7-2. 
Thus, the overall energy function, written in the frequency domain, is 
where 
IH(f)12 = IHp(f)12 + IHd(f)12. 
The minimum energy solution for U(f) can be calculated as 
U*(f) = IHd(f)12 D(f) 
IH(t) 12 
. 
(B.5) 
In the spatial domain, this is equivalent to filtering the data d(x) with a shift-
invariant filter hs(x), 
u*(x) = hs(x) * d(x) 
(B.6) 
where 
(B.7) 
This equivalence between regularization and convolution (filtering) has previ-
ously been noted by Poggio and Torre (1984) and Terzopoulos (1986b). 
For the membrane surface interpolator we have 
with a resulting smoothing filter 
1 
Hs(t) = 1 + (72 1 27l"f12 . 
(B.8) 
Similarly, for the thin plate surface interpolator we have 
1 
Hs(t) = 1 + (72 1 2'rrfl 4 . 
(B.9) 
The shape of the frequency response for these two models is qualitatively similar 
to that of a Gaussian filter. The exact equations for the one- and two-dimensional 
impulse responses corresponding to these filters are given in (Poggio et al. 
1985b). 

Fourier analysis 
175 
B.2 Fourier analysis of the posterior distribution 
In the previous section, we saw how for the case of dense unifonn data, the re-
sults of using regularization are equivalent to convolution with a shift-invariant 
filter hs(x). If we are using Bayesian modeling instead of regularization, how-
ever, the posterior estimate is characterized by a probability distribution rather 
than just the optimal estimate u*. To characterize the posterior distribution 
p(uld), we can look at the residual signal 
v == u - u*, 
i.e., the difference between the signal and the optimal estimate. Substituting 
U*(f) + V(f) for U(f) in (B.4), we obtain 
E(U*+V) = ~ J IH(f)121U*(f)+V(f)12-2IHd(f)121U*(f)+V(f)IID(f)I+IHd(f)12ID(f)12df. 
Using (B.5), this simplifies to 
(B. 10) 
Following the same reasoning as in Section 4.1, we see that v(x) is pink (corre-
lated) Gaussian noise with a power spectrum 
(B.ll) 
The estimated signal u(x) is thus the combination of the filtered data u*(x) 
and the pink noise v(x) 
u(x) = u*(x) + v(x) = h.(x) * d(x) + h-I(x) * w(x) 
(B.12) 
where w(x) is white Gaussian noise (Figure B.l). The Fourier transfonn of this 
estimate is 
U(f) = H.(f)D(f) + IH(f)I-IW(f), 
(B. 13) 
where W(f) is the Fourier transfonn of white Gaussian noise, and is hence itself 
a white Gaussian signal. Taking expectations, we have 
(U(f») = Hs(f)D(f) = U*(f) 
(B.14) 
and 
(B.15) 
Thus, if we define the power spectrum to be the average power (signal squared) 
in a narrow frequency band we have the above result. If we define the power 
spectrum to be the variance in the Fourier transform, we have 
(B.16) 

176 
Bayesian Modeling of Uncertainty in Low-Level Vision 
w(x) 
h-1(x) 
VeX) 
white noise 
fractal filter 
pink noise 
d(x) 
hix) 
u*(x) 
+ 
u(x) 
data 
smoothing filter 
optimal estimate 
solution 
Figure B.l: Filter and noise model 
B.3 
Analysis of gradient descent 
In this section we will perform a Fourier analysis of the gradient descent tech-
nique which we use to estimate the MAP solution, and of the stochastic gradient 
descent technique (Gibbs Sampler) which we use to generate random samples. 
For the moment, we will confine our analysis to the continuous-space case. The 
next section will discuss the finite element (discrete-space) implementation. 
Gradient descent techniques (which include most of the relaxation algorithms 
which we examined in Section 2.2) iteratively move the system state towards its 
optimal solution by taking small steps in the direction of the energy gradient 1 • 
To analyze the convergence of the algorithm towards the final solution, we 
can either compute the gradient in the spatial or the frequency domain (this is 
possible because the u(x) and its transform U(f) are linear transforms of each 
other). In this section, we will take the latter approach since it results in a 
simpler exposition. We will also confine our analysis to the discrete time case. 
The continuous time case (which corresponds to analog systems such as those 
studied by Koch et al. (1986» can be analyzed using differential equations 
instead of difference equations. 
The gradient of the energy equation (B.4) with respect to U(f) is given by 
1 For stochastic gradient descent, we modify this rule by adding a small random disturbance 
at each step. 

Fourier analysis 
177 
For the discrete time system, the updating equation is 
where a controls the step size of the gradient descent. The solution of this 
recurrence relation is 
U(f, t) = U(f,O)[1 - aIH(fWJ' + I~(~~ D(O (1 - [1 - aIH(OI2]1) . 
(B.17) 
Note that the geometric series converges only if aIH(OI2 < 2, which is not in 
general true for the controlled-continuity constraint. However, as we will see 
in the next section, the discrete space system has strictly bounded responses for 
IH(OI2. 
To analyze the convergence of stochastic relaxation (the Gibbs Sampler), we 
will ignore the component of the solution that is converging towards the optimal 
solution, and only look at the residual v(x). In this case, we use the stochastic 
updating equation 
V(f,t+ 1) = V(f,t) - a[V'E+,W(f,t)] = [1- aIH(OI2]V(f,t) - a,W(f,t), 
where W(f, t) is the Fourier transform of the white noise field w(x) that is added 
to the solution at time t, and , is the magnitude of this noise. The recurrence 
relation for the power spectrum of v(x, t) can be computed from the above 
equation as 
P(f, t + 1) = [1 - aIH(f)12]2p(f, t) + ( 2)'2. 
The solution of this recurrence relation is 
( 2)'2 
( 
) 
P(f, t) = P(f, 0)[1 - aIH(OI2]2t + 1 _ [1 _ a1H(012]2 1 - [1 - aIH(OI2]2t . 
(B.18) 
Thus, if we set)' = J2/a, the steady state power spectrum is 
(B. 19) 
which approaches the true spectrum (B.1l) as a -
O. The discrepancy between 
the power spectrum of stochastic Jacobi relaxation and that of the Markov Ran-
dom Field is due to the synchronous updating of the stochastic system (the Gibbs 
Sampler uses asynchronous updating). 
B.4 Finite element solution 
The finite-element energy approximations for the membrane and the thin plate are 
derived by Terzopoulos (1984) by considering particular local spline elements, 

178 
Bayesian Modeling of Uncertainty in Low-Level Vision 
showing that they are acceptable using a patch test, and then calculating the 
energy in tenns of the nodal variables. This approach is also useful for the 
computation of energy tenns near discontinuities, as we saw in Appendix A. 
An alternative approach is to consider the system as a discrete approximation to 
the filtering system analyzed in Section B.1. 
Following this latter approach, we desire a prior filter whose frequency re-
sponse is 
P 
IHp(t)12 = L wmI2dl2m· 
m=O 
This can be approximated by a digital filter whose frequency response is 
(B.20) 
where h = ILlxI = ILlyl is the grid size, and n is the dimensionality of the 
x domain2• The impulse response of the individual filters Im(x) are the finite 
difference approximations to the Laplacian, i.e., lo(x) = 8(x), 
{ 
2n 
if x = 0 
II (x) = h-2 
-1 if Ixl = 1 , 
o 
otherwise 
and 
Im(x) = II (x) * ... * II(x). 
, 
J 
... 
m times 
The integral equation that describes the energy functional 
can now be replaced by the summation 
(B.21) 
We therefore have to modify the above definitions of Im(x) by scaling Up3 by h". 
2The small w approximation for cos w is 1 - w2/2! + w4 /4! - ... 
3The scaling by h is required to make the multigrid version algorithm have consistent energy 
approximations, since the magnitude of the energy determines the variance of the stochastic u(x) 
signal. 

Fourier analysis 
179 
For two dimensions, the II and h operators correspond to the membrane and 
thin plate neighborhoods4• The shape of these neighborhoods is 
1 
2 -8 
2 
and 
h-2 
1 -8 20 -8 1 
2 -8 
2 
1 
Note that these neighborhoods are not the only ones possible for the finite-
element approximation, just the simplest to implement on a digital computer. 
For example, we could use the filter 
[ -1 -1 -1] 
-1 
8-1 
-1 -1 -1 
for the membrane interpolator, since it has the correct small f approximation. 
B.5 Fourier analysis of multigrid relaxation 
The Fourier domain tools developed in the previous sections can be used to an-
alyze the convergence rates of multigrid relaxation. In this section, we combine 
the Jacobi gradient descent technique examined in Section B.3 with the discrete 
filters obtained in Section B.4 to perform this analysis. As a starting point we 
set U(f,O) = 0, and use a data compatibility filter IH d(f) 12 = (J-2. We will define 
the effective filter response F(f, t) as the ratio between the Fourier transform of 
the solution and the Fourier transform of the data 
(B.22) 
For a (two dimensional) thin plate interpolator on a discrete grid of size h, 
we have 
and 
IH(f)12 = h-2(4 - 2cos 27rfxh - 2cos 27r/yh)2 + h2(J-2. 
To ensure that the Jacobi iteration converges (without ringing) we will make 
the conservative assumption that aIH(f)12 :::; 1. Since IH(f)12 attains a maximum 
of 64h-2 + h2(J-2 at (7r, 7r), we set a-I = 64h-2 + h2(J-2. 
4These neighborhoods correspond to the rows of the Ap matrix. 

180 
Bayesian Modeling of Uncertainty in Low-Level Vision 
Frequency (Hz) 
Frequency (Hz) 
0°.5 
1.0 
2.0 
4.0 
8.0 
16.0 
0°.5 
1.0 
2.0 
4.0 
8.0 
16.0 
,-.. 
,-.. 
:g -10 
......, 
:g -10 
......, 
~ -20 
~ -20 
c 
c 
~ -30 
8. -30 
'" 
0 
0 
~ -40 
~ -40 
-50 
-50 
-60 
-60 
-70 
-70 
-80 
-80 
-90 
-90 
(a) 
(b) 
Frequency (Hz) 
Frequency (Hz) 
00.5 
1.0 
2.0 
4.0 
8.0 
16.0 
00.5 
1.0 
2.0 
4.0 
8.0 16.0 
,-.. 
,-.. 
:g -10 
......, 
:g -10 
......, 
0 
~ -20 
;g -20 
8. -30 
~ -30 
'" 
0 
tl. 
~ -40 
r.n -40 
-50 
-50 
-60 
-60 
-70 
-70 
-80 
-80 
-90 
-90 
(c) 
(d) 
Figure B.2: Effective frequency response of multigrid relaxation 
(a) fine level only (b) coarse level only (c) multigrid (d) spectral density 

Fourier analysis 
lSI 
The frequency response F(ifx, 0), t) with a = 0.005 and h = 1/32 (33 X 33 
points) is shown in Figure B.2a after 10, 25, 50 and 100 iterations. Note how 
slowly the low-frequency response approaches the equilibrium value. On a 
coarser grid (h = l/S), the low-frequency convergence-shown in Figure B.2b 
after 1, 5, 25 and 50 iterations-is much faster, but the high frequencies are not 
annihilated. Multigrid relaxation is shown in Figure B.2c, with 25 iterations at 
h = l/S, 25 iterations at h = 1/16, and a final 25 iterations at h = 1/32. 
A similar analysis can be applied to the spectral density of the random field. 
Recall from (B.1S) that 
P(f,t) = IH(t)1-2[l- ~IH(t)12rl (1- [1- aIH(t)12]21). 
The plots of the spectral density as a function of time are shown in Figure B.2d. 
Note that even though the Jacobi iteration does not converge to the true spectral 
density IH(t)1-2, the error is not very large. 
While the Fourier analysis on which these results are based is valid only for 
the spatially isotropic dense data case, these results can still be used to explain 
the improved convergence obtained with multigrid relaxation. First, we note that 
the main advantage of multigrid relaxation is that it approximates more quickly 
the low frequency filtering (smoothing) behavior of the system, while ignoring 
the exact high frequency response. Second, the use of the power spectrum for the 
stochastic signal gives us a measure of how quickly the system is approaching 
thermal eqUilibrium. This is because a multivariate Gaussian distribution (which 
results from a quadratic energy function) can be completely characterized by its 
current mean and variance. For the spatially isotropic case just examined, the 
effective filter response and power spectrum capture these statistics in a succinct 
fashion. The convergence rate results which we have presented in this Appendix 
can also be applied to number of the algorithms studied in this book, including 
the multiresolution fractal generation algorithms (Section 4.2) and the Monte 
Carlo variance estimator (Section 6.2). 

Appendix C 
Analysis of optical flow computation 
In this Appendix, we analyze the perfonnance of the simple correlation-based 
flow estimator introduced in Section 5.3. This analysis is an extension of the 
one-dimensional analysis presented in (Matthies et al. 1987). The flow estimator 
which we examine selects at each pixel the disparity that minimizes the SSD 
error measure 
e,(d; x) = J 
w(..\)[fi (x + d + ..\) - 10 (x + ..\)]2 d..\, 
(C.l) 
where/o(x) and/l(x) are the two successive image frames, and w(x) is a symmet-
ric, non-negative weighting function. To perfonn our analysis, we will assume 
that the two image frames are generated from an underlying true intensity image, 
fix), to which uncorrelated (white) Gaussian noise with variance a; has been 
added: 
lo(x) = fix) + no(x), 
II (x + d) = I(x) + nl (x). 
Using this model, we can rewrite the error measure asl 
e(d; x) = J 
w(..\)[f(x + d - d +..\) - fix +..\) + nl(x +..\) - no(x + ..\)]2 d..\. 
If d ~ d, we can use a Taylor series expansion to obtain 
e(d; x) = J w(..\)[V'j{x +..\) . (d - d) + nl (x + ..\) - no(x + ..\)]2 d..\ 
= (d - d)T A(d - d) + 2(bl(x) - bo(x)l(d - d) + c 
(C.2) 
IThis equation is actually incorrect, since it should contain nl(x + d - it + A) instead of 
nl (x + A). The effect of including the correct term is to add small random terms involving 
integrals of W(A), V'W(A), V'f(x + A), V'V'f(x + A) and nj (x) to the quadratic coefficient A(x), 
bl (x) and c(x) that are derived below. This intentional omission has been made to simplify the 
presentation. 

184 
where 
Bayesian Modeling of Uncertainty in Low-Level Vision 
A = J 
W(A)Vf(X + A)VTf(x + A) dA, 
bk = J 
W(A)Vf(X + A)nk(x + A) dA, 
c = J 
w(A)[nl (x + A) - no(x + A)]2 dA. 
The four coefficients A, bo, bl and c define the shape of the error surface 
e(d; x). The first coefficient, A, is related to the average roughness or slope 
of the intensity surface, and determines the confidence given to the disparity 
estimate (see below). If Vf(x) is relatively constant, this matrix has rank 1, and 
we have the classical aperture problem. The second and third coefficients, bo 
and bt. are independent zero mean Gaussian random variables that determine 
the difference between the displacement estimate d and the true displacement 
d, (i.e., the error in flow estimator). The fourth coefficient, c, is a chi-squared 
distributed random variable with mean (20"; f W(A) dA) that defines the height of 
the error surface at d = d. We can thus use c to obtain a local estimate of the 
image noise. High values of c can also be used to flag areas-such as occluded 
areas-where the flow computation model is inaccurate. 
To estimate the disparity at a point x given the error surface e(d; x), we find 
the displacement vector a such that 
e(d; x) = min e(d; x). 
d 
By differentiating the quadratic2 equation (C.2), we see that this minimum will 
be at 
A 
-
I 
d = d + A - (bo - bd. 
(C.3) 
To calculate the variance in this estimate, we must first calculate the variance in 
Var(bk) = (bkbI) = 0"; J 
W2(A)Vf(X + A)VTf(X + A) dA. 
If we set w(x) = 1 on some finite interval and zero elsewhere, this ensures that 
w2(A) = W(A), and the variance reduces to O";A. If an arbitrary window function 
w(x) is used, we can scale the variance estimate by (f w2(A) dA) / (f W(A) dA), 
which is valid if Vf is relatively constant with respect to the window size. 
Combining the above results, we conclude that 
2The true equation (when higher order Taylor series tenus are included) is a polynomial 
series in (d - d) with random coefficients of decreasing variance. This explains the rough nature 
of the e(d; x) observed in practice. 

Analysis of optical flow computation 
185 
since bo and b 1 are uncorrelated. The covariance matrix for the displacement 
estimate can thus be detennined from the quadratic form matrix fitted to the error 
surface. Empirical studies performed on real images with a one-dimensional 
version of this flow estimator have shown a good fit between the estimated and 
actual variances (Matthies et al. 1987). The approach used in this Appendix 
can also be used to calculate the spatial or temporal correlations between flow 
estimates by performing a two-dimensional generalization of the derivation given 
by Matthies et al. (1987). 

Appendix D 
Analysis of parameter estimation 
In this Appendix, we will develop our Bayesian estimation theory in terms of 
multivariate Gaussian distributions and derive some of the results that were 
needed in Sections 4.2, 6.3, and 6.4. We will use the notation x '" N(m, P) to 
denote that x is a multivariate normal variable with mean m and covariance P 
(Gelb 1974). The probability density function for x can be written as 
where IPI is the determinant of the matrix P (we use 127rPI-I /2 instead of 
(27r)-"/2IPI-1/ 2 for notational succinctness). 
D.I Computing marginal distributions 
The first result which we wish to establish concerns computing marginal distri-
butions from a joint distribution. Consider the distribution 
P ([ x ]) ex: exp _! [ x - Xo ] T [ AT B] [ x - Xo ] T 
Y 
2 
y - Yo 
B 
C 
y - Yo 
This probability distribution attains a maximum w.r.t. x (corresponding to the 
minimum of the quadratic form) for 
x = Xo - A-1B(y - yo). 
We can diagonalize the probability distribution by substituting for Xo in the 
original equation 
P ([x]) ex: exp-~ [x - x - A-1B(y - yo)]T [AT B] [x - x - A-1B(y - Yo)] 
y 
2 
y - Yo 
B C 
y - Yo 

188 
Bayesian Modeling of Uncertainty in Low-Level Vision 
=ex _![X_X]T[I_A-IB]T[A B] [I-A-1B] [x-x] 
p 2 Y - Yo 
0 
I 
BT COl 
Y - Yo 
1 [X_X]T[A 
0 
] [x-x] 
= exp - 2 Y-Yo 
0 C-BTA-IB 
Y-Yo 
Because of this diagonal structure, we can marginalize w.r.t. x to obtain 
1 
TTl 
p(y) ex exp -2(Y - Yo) (C - B A - B)(y - Yo). 
However, evaluating A -I may be expensive, especially if A is large and sparse. 
The alternative to performing this evaluation is to compute x in terms of the 
current value of y (this usually involves solving a sparse set of linear equations), 
and to substitute this value into the probability density function 
Thus, we see that marginalizing a multivariate Gaussian distribution with respect 
to some of the variables is equivalent to substituting the minimum energy solu-
tion for those variables into the original equation (this result is used in Section 
4.2). 
D.2 Bayesian estimation equations 
In the remainder of this Appendix, we will develop the equations for the various 
conditional and marginal distributions which we use in our Bayesian estimation 
framework. The notation which we use is derived from the Kalman filtering 
literature (Gelb 1974). In this Appendix, we assume that all of our observations 
come from a static surface characterized by the state vector u; in Section 7.1, 
we extend our estimator to include a dynamic system model. 
We start by modeling the prior distribution as a correlated Gaussian 
u '" N(uo, Po)· 
(D.2) 
For our sensor model, we use a linear system with additive Gaussian noise 
d = Du + r, with r", N(O, R). 
(D.3) 

Analysis of parameter estimation 
189 
The measurement matrix H encodes the sparse sampling which converts from 
the dense depth map u to the sparse set of depth values d. This rectangular 
matrix usually contains l's where the nodal variables coincide with the data 
points, and O's elsewhere. If the data points do not lie on the grid (e.g., if we 
have sub-pixel positioning accuracy), then the H matrix can be derived from 
the local interpolation function and will contain non-integer values (see Section 
5.1). The R matrix encodes the covariance of the measurement noise process, 
and is usually diagonal with rji = aT for uncorrelated sensor noise. 
From the sensor model (D.3), we can derive the distribution of the data d 
conditioned on the initial state u as 
diu", N(Hu, R). 
(D.4) 
Similarly, we can derive the marginal distribution of the data by integrating over 
all possible initial states to obtain 
d '" N(Hoo, HPoHT + R) 
(D.5) 
(this result can also be derived using the formula for sums of Gaussian random 
variables). 
The posterior estimate Ul after the first set of measurements can be derived 
from (D.2) and (DA) using Bayes' Rule. This estimate is a multivariate Gaussian 
(D.6) 
with a mean 
(D.7) 
and a covariance 
PI = (Pol + HTR-IH)-l 
(D.8) 
(see (Gelb 1974) or (Maybeck 1979) for derivations). 
The mean estimate 010 which is also the MAP estimate, corresponds to the 
minimum energy solution of the spline approximation (2.15) if we make the 
following correspondences 
00 = 0, pol = Ap, HTR-1H = Ad, and Hd = ii, 
where ii is the zero-padded observation vector. The advantage of using an 
explicit measurement equation (D.3) is that unlike ~, R is not singular and can 
easily model correlated sensor noise. The measurement matrix H also allows us 
to model a wider variety of sensors, since measurements need not be coincident 
with grid locations. Finally, the prior state estimate Do can be used to encode 
any prior knowledge which we have about the surface (e.g., from a digital terrain 
map in navigation applications). 

190 
Bayesian Modeling of Uncertainty in Low-Level Vision 
The new state and state covariance estimates UI and PI capture all of the 
information that is available from the prior model and the first observation. 
These values could then be substituted into (0.2) and used to obtain posterior 
estimates ih and P2 from a second set of measurements. In general, the Bayesian 
model can be extended to include multiple measurements of the same surface 
using 
dk = Hku + rk, with rk '" N(O, Rk)· 
After n measurements have been processed. the posterior estimate has a mean 
value 
" 
u" = (p;I)-1 LHIRk"ldk 
h=1 
and a covariance 
PIt = (POl + ~HIRk"IHk)-1 
These equations can be re-written into a recursive form using 
k 
Ak = POl + LHJRj-IHj = Ak_1 +HIRk"IHk 
(0.9) 
j=1 
and 
k 
bk = L HJRTldj = bk_1 + HIRk"ldk 
(0.10) 
j=! 
to accumulate the inverse covariance and cumulative weighted data vector. The 
Ak's and bk's are the same as are used in spline interpolation of multiple data 
sets (Ak is sparse and banded). The optimal estimate at time k can be obtained 
by solving 
(0.11) 
D.3 Likelihood of observations 
The parameter estimation techniques developed in Sections 6.3 and 6.4 are based 
on maximizing the likelihood of having observed the given data. This likelihood 
is described by (0.5), and can thus be written as 
This probability distribution, however, is difficult to evaluate since it involves 
computing the covariance matrix Po, which is not sparse (or may not even exist). 

Analysis of parameter estimation 
191 
To get around this problem, we will re-write this equation in terms of quan-
tities such as uo, Ul. POl, R-I and PII which we know how to compute. In 
order to do this, we will need to introduce two Lemmas. The first is a matrix 
inversion Lemma (Maybeck 1979, p. 280) 
(0.13) 
where H need not be a square matrix. The second Lemma is related to matrix 
determinants 
(0.14) 
(Maybeck 1979, p. 280). 
We can re-write the energy equation corresponding to the negative logarithm 
of (0.12) as 
E(d) = ~ log 127r{HPoHT + R)I + ~(d - HUol(HPoHT + R)-I(d - Huo)} 
= EI (d) + E2(d) 
Applying the (0.14) and (0.8) to the first term, we obtain 
1 
_I 
1 
I 
_I 
1 
I -II 
EI(d) = "2 log IPI 1- "2 log 27rR 1- "2 log Po . 
(0.15) 
We can re-write the second term of this energy equation by applying (0.13), 
(0.7) and (0.8) 
E2(d) = !(d - Hiiol[R- 1 - R-IH(POI + HTR-IH)-IHTR-I](d - Hiio) 
2 
= !(d - HiiolR-I[(d - Huo) - HPI(HTR-Id - HTR-IHuo)] 
2 
1 
= "2(d - HUolR-I[(d - Huo) - HPI«Pllul - POIUO) - (PI I - POI)UO)] 
= !(d - HUO)TR-I(d - Huo) - !(d - HUolR-IH(ul - uo) 
(0.16) 
2 
2 
= !(d - HUolR-I(d - HUI) 
(0.17) 
2 
= ~(d - HUllR-I(d - HUI) + ~(d - HUllR-IH(ul - uo) 
(0.18) 
= ~(d - HUI)TR-I(d - HUI) + ~(UI - uolpOI(ul - uo). 
(0.19) 
These alternate forms of the energy equation have different interpretations. 
The form given in (0.16) shows that if we measure the energy using the residual 
between the data points and the prior surface estimate, we must reduce the 

192 
Bayesian Modeling of Uncertainty in Low-Level Vision 
energy by a tenn proportional to how much the surface moves as it is updated. 
Similarly, in (D.18) we see that using the residual between the data and the 
posterior estimate underestimates the energy by the same amount. The fonn 
given in (D.17) uses a cross-product between the residual to the prior estimate 
and the residual to the posterior estimate. Finally, (D.19) is a symmetric fonn 
which adds the data compatibility energy computed with respect to the posterior 
estimate to the strain (prior model) energy associated with the difference in 
solutions. 

Table of symbols 
SYMBOL 
A 
/-l 
172 
b 
d(x,y) 
dij 
d 
f 
h 
iij, mij 
nij 
p 
q 
r 
U(X,y) 
Uij 
u 
U* 
U 
v 
x 
A 
Ap 
Ad 
C 
Ed(u, d) 
Ep(u) 
£(u) 
F 
MEANING 
regularization parameter 
mean value 
noise or estimate variance 
cumulative weighted data vector 
data signal 
data point 
data vector (or disparity) 
frequency (or intensity) 
finite element size 
line variables 
crease variables 
three dimensional point position 
system noise 
measurement noise 
solution or state 
discrete form of u(x, y) 
state vector 
optimal solution 
current estimate 
hierarchical representation 
spatial domain 
cumulative state information matrix 
prior information matrix 
data information matrix 
three dimensional point covariance matrix 
data compatibility energy 
prior energy 
combined energy 
state transition matrix 

194 
Bayesian Modeling of Uncertainty in Low-Level Vision 
measurement matrix 
Kalman filter gain matrix 
normal (Gaussian) distribution 
state covariance matrix 
system noise covariance matrix 
measurement noise covariance matrix 
hierarchical basis set matrix 
temperature (in Gibbs distribution) 
partition function (in Gibbs distribution) 

Index 
aliasing, 96 
Bayes' Rule, 49, 55 
Bayesian modeling, 7, 49-50,57,82 
Boltzmann distribution, see Gibbs dis-
tribution 
camera motion, 127-128, 135 
computer graphics applications, 67 
computer vision 
algorithms, 6-7 
problems, 7-8 
representations, 5 
theories, 6 
conjugate gradient descent, 23, 76 
constrained fractal generation, see frac-
tal generation 
constraints, 52, 69, 84, 89 
continuation, 8, 45, 100 
controlled-continuity constraint, 19, 
168 
cooperative algorithms, 6-7 
correspondence problem, see match-
ing, stereo 
covariance 
field, 101 
matrix, 56, 84, 101 
data compatibility 
constraint, 84-87, 169 
energy, 18-19, 21-22, 30 
dense data, see optical flow 
dense field, 2, 9, 16, 50, 103, 123 
depth discontinuities, see discontinu-
ities 
depth map, 15, 30, 119, 123-124, 
135 
depth-from-motion, 8, 121-122 
edge-based, 131 
feature-based, 131 
iconic, 126-130 
deterministic relaxation, 22-23, 25, 
115 
discontinuities, 19, 146-147 
discontinuity detection, 6, 8,45, 128 
disparity, 47, 132 
estimation, 127-129, 139-144, 183-
184 
gradient, 29, 141 
dynamic vision, 57, 121, 126 
edge detection, see discontinuity de-
tection 
edges, see intensity edges 
elastic net, 47, 81, 88 
energy function, 6, 18, 52 
compatibility, 18, 169 
smoothness, 18, 168 
energy minimization, 7, 19, 22-23, 
55 
energy-based model, 6, 18,47, 78 
equilibrium, 65, 103, 181 
error model, 7, 83, 86, 94, 103, 146 
error surface, 94, 116,128, 184-185 
extended Kalman filter, see Kalman 
filter 
field 

196 
Bayesian Modeling of Uncertainty in Low-Level Vision 
continuous, 16,64 
discrete, 16, 18, 50, 64, 78 
finite element analysis, 21-22, 167-
171 
flow, see optical flow 
force field model, 87-93 
Fourier analysis, 61-63, 75-76, 111, 
173-181 
Fourier transform, 61 
fractal, 63-64, 76, 81-82 
fractal dimension, 63, 69, 73, 111 
fractal generation, 65-75 
Gauss-Seidel relaxation, 23, 65 
Gaussian distribution, 53, 65, 84 
contaminated, 86 
correlated, 62, 64 
multivariate, 57, 66, 87,101,107, 
143, 187-190 
Gaussian field, 75, 78, 111 
Gibbs distribution, 52-53, 60, 62, 
65, 78, 99, 101 
Gibbs Sampler, 53, 55, 61, 65, 79, 
81, 103 
multiresolution, 65-73, 177 
gradient descent algorithms, 23, 114, 
142, 176--177, see also conju-
gate gradient descent 
hierarchical basis conjugate gradient, 
35-44 
hierarchical basis set, 33-37, 44 
iconic processing, 89-92, 126--130, 
144-146 
ill-posed problem, 2, 18 
image sequence, see motion sequence 
incremental depth-from-motion, 121, 
126--130, 139, 144-146 
information matrix, 85, 123, 126, 145 
integration, multi-sensor, 4, 16 
intensity edges, 7,45-46, 123, 147 
intensity image, 93, 95-97, 139 
interpolation, 25, 3{}-31, 44, 76, 84, 
170, see also surface interpo-
lation 
intrinsic image, 2, 16 
intrinsic model, 2, 60, 82 
inverse problem, 2, 18 
iterative algorithms, 6, 53, see also 
relaxation 
Jacobi relaxation, 23, 177, 179 
jitter, 96--97 
Kalman filter, 122-126, 133 
extended, 125-126, 146 
line process, 45, 56, 59, 146, see 
also discontinuity detection 
log likelihood, 109, 114-116 
loss function, 7, 50, 79, loa-101 
low-level vision, 2, 5-9, 15, 18-19, 
48, 101 
Markov Random Field, 4, 6, 8, 50-
56,64, 103 
massively parallel algorithms, 21-22, 
100, 153 
matching 
stereo, 6--7, 19,47, 100, 132, 144 
surface, 112-119 
symbolic, 89 
Maximum A Posteriori (MAP) esti-
mation, 50, 55,65, 79, 81-82, 
99-100 
maximum likelihood estimation, 107, 
109, 111, 114 
measurement matrix, 84, 107, 113, 
122, 189 
measurement model, see sensor model 
measurement noise, 18, 84, 86--87, 
89, 94, 107, 113, 131, 169, 
183, 189 

Index 
mechanical model, 78-82 
membrane, 18, 21-22, 59, 62, 64, 
69, 73, 79, 140, 179 
Minimum Mean Squared Error (MMSE) 
estimation, 100, 142 
model, see energy-based model, in-
trinsic model, mechanical model, 
posterior model, prior model, 
probabilistic model, sensor model, 
system model 
model parameter estimation, 106-112 
motion, see depth-from-motion 
motion estimation, without correspon-
dence, 112-119 
motion sequence, 93, 121, 123, 133, 
135 
multigrid relaxation, 25-27, 44, 179 
stochastic, 53, 65-67, 103, 181 
multiresolution algorithms, 8,25-27, 
35-44,65 
multiresolution representation, 6, 23-
29, 33-35, 44, 75-78 
multiscale decomposition, 23, 27, 33 
multivariate Gaussian distribution, see 
Gaussian distribution 
neural nets, 6, 45, 112 
nodal variables, 21, 33, 84, 87, 91 
noise 
pink (correlated), 62, 64, 111,175 
white (uncorrelated), 64, 94, 111 
object model, 47, 60, 82 
optical flow, 8, 19,93-95, 121, 127, 
129, 183-185 
optimal estimate, 4, 50, 55-56, 100, 
114, 123, 146 
parallel algorithms, see massively par-
allel algorithms 
parameter estimation, see model pa-
rameter estimation 
197 
partition function, 52-53, 109 
piecewise continuous surface recon-
struction, 19,59,78-79,128 
posterior distribution, 55-56, 79, 99, 
101,175, see also posterior model 
posterior estimate, 7, 50, 55, 100, 
189-190, 192 
posterior model, 49, 99 
power spectrum, 69-73, 75-76, 111, 
175-181 
prediction equations, 125-126, see 
also system model 
prior distribution, 49-50, 67, 188, 
see also prior model 
prior energy, 21, 30, 52, 168 
prior information matrix, 75, 125 
prior model, 9, 49, 59-61, 64-65, 
73, 79 
probabilistic model, 49, 56-57, 78-
82, 122 
process model, 81 
random-dot stereogram, 7, 16, 29 
range data, 47, 93, 112, 114 
regularization, 2, 6, 18-19, 55, 61, 
63, 106, 128, 173 
relative representation, 27-29, 44, 75-
76 
relaxation algorithms, 22-23, see also 
conjugate gradient descent, de-
terministic relaxation, Gauss-
Seidel relaxation, Jacobi relax-
ation, stochastic relaxation 
relaxation labeling, 6 
representations, 5, 15-16,23,27,33, 
46-48,105 
residual,37-40, 111, 123, 175, 192 
retinotopic representation, 15,47,84, 
147 
robot navigation, 100, 105, 112 
scale space continuation, 100, 105, 

198 
Bayesian Modeling of Uncertainty in Low-Level Vision 
142 
sensor model, 49, 53, 78, 83, 140, 
188 
sensor noise, 89, 96, 111 
simulated annealing, 6, 53, 100 
slippery spring, 88 
smoothness 
constraint, 2, 18,27,62,122,145, 
167-169 
degree, 64 
energy, 18-19,21, 30 
sparse data, 8, 18,84, 87, 119 
sparse depth measurement, see spring 
model, force field model 
spline, 64, 85, 106-107, 109, 112, 
152 
under tension, 19 
spring model, 84-89 
stabilizer, 19,63, 79 
stereo, see matching 
stochastic optimization, 6, 45 
stochastic relaxation, 65-67, 103, 177, 
181 
surface interpolation, 8, 18, 21, 45, 
78 
algorithms, 23, 167 
system model, 57, 122, 125, 145 
temperature, in a Gibbs distribution, 
52,65 
thin plate, 19,21-22,62,64, 69, 73, 
179 
uncertainty, 1, 11, 56, 82 
estimation, 101-105 
modeling, 4, 87, 105 
variance, 1, 18,56,86,94, 103-105, 
113, 127 
viewpoint-invariant surface interpo-
lation, 78, 125 
visible surface representation, 6, 15-
16, 47, 75, 103 
weak constraint, 52 
continuity, 8, 45, 79, 86 
smoothness, 18, 27, 168 

