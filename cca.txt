Cristian S. CALUDE, Gheorghe P˘AUN
COMPUTING WITH CELLS AND ATOMS
An Introduction to Quantum, DNA and Membrane Computing

AUTHORS ADDRESSES:
Cristian S. Calude
Department of Computer Science
School of Mathematical and Information Sciences
The University of Auckland
Private Bag 92019, Auckland, New Zealand
E-mail: cristian@cs.auckland.ac.nz
Gheorghe P˘aun
Institute of Mathematics of the Romanian Academy
PO Box 1-764, 70700 Bucure¸sti, Romania
E-mail: gpaun@imar.ro

Cristian S. Calude, Gheorghe P˘aun
COMPUTING
WITH CELLS AND ATOMS
An Introduction to Quantum, DNA
and Membrane Computing
Taylor and Francis
London, New York, 2000

Copyright: Taylor and Francis 2000

Contents
Preface
vii
1
Prerequisites
1
1.1
Preliminary Notions and Notations . . . . . . . . . . . . . . .
1
1.2
Operations on Strings and Languages
. . . . . . . . . . . . .
2
1.3
A General Computing Framework
. . . . . . . . . . . . . . .
3
1.4
Chomsky Grammars . . . . . . . . . . . . . . . . . . . . . . .
8
1.5
Lindenmayer Systems
. . . . . . . . . . . . . . . . . . . . . .
10
1.6
Automata and Transducers
. . . . . . . . . . . . . . . . . . .
11
1.7
Characterizations of Computably Enumerable Languages
. .
14
1.8
Universal Turing Machines and Type-0 Grammars
. . . . . .
16
1.9
Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.10 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . .
21
2
DNA Computing
23
2.1
The Structure of DNA . . . . . . . . . . . . . . . . . . . . . .
23
2.2
Complementarity Induces Computational Completeness
. . .
26
2.3
Operations on DNA Molecules
. . . . . . . . . . . . . . . . .
30
2.4
Adleman’s Experiment . . . . . . . . . . . . . . . . . . . . . .
36
2.5
Other DNA Solutions to NP Complete Problems . . . . . . .
43
2.6
A Two-Dimensional Generalization . . . . . . . . . . . . . . .
50
2.7
Computing by Carving . . . . . . . . . . . . . . . . . . . . . .
55
2.8
Sticker Systems . . . . . . . . . . . . . . . . . . . . . . . . . .
64
2.9
Extended H Systems . . . . . . . . . . . . . . . . . . . . . . .
77
2.10 Controlled H Systems
. . . . . . . . . . . . . . . . . . . . . .
86
2.11 Distributed H Systems . . . . . . . . . . . . . . . . . . . . . .
95
2.12 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . .
106
3
Membrane Computing
109
3.1
P Systems with Labelled Membranes . . . . . . . . . . . . . .
110
3.2
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
3.3
The Power of P Systems . . . . . . . . . . . . . . . . . . . . .
121
3.4
Decidability Results
. . . . . . . . . . . . . . . . . . . . . . .
127
v

vi
Contents
3.5
Rewriting P Systems . . . . . . . . . . . . . . . . . . . . . . .
131
3.6
P Systems with Polarized Membranes
. . . . . . . . . . . . .
135
3.7
Normal Forms . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
3.8
P Systems on Asymmetric Graphs
. . . . . . . . . . . . . . .
154
3.9
P Systems with Active Membranes . . . . . . . . . . . . . . .
156
3.10 Splicing P Systems . . . . . . . . . . . . . . . . . . . . . . . .
168
3.11 Variants, Problems, Conjectures
. . . . . . . . . . . . . . . .
176
3.12 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . .
178
4
Quantum Computing
179
4.1
The Church–Turing Thesis
. . . . . . . . . . . . . . . . . . .
179
4.2
Computation is Physical . . . . . . . . . . . . . . . . . . . . .
181
4.3
Reversible Computation . . . . . . . . . . . . . . . . . . . . .
183
4.4
The Copy Computer . . . . . . . . . . . . . . . . . . . . . . .
186
4.5
Maxwell’s Demon . . . . . . . . . . . . . . . . . . . . . . . . .
187
4.6
Quantum World
. . . . . . . . . . . . . . . . . . . . . . . . .
189
4.7
Bits and Qubits . . . . . . . . . . . . . . . . . . . . . . . . . .
190
4.8
Quantum Calculus . . . . . . . . . . . . . . . . . . . . . . . .
191
4.9
Qubit Evolution
. . . . . . . . . . . . . . . . . . . . . . . . .
195
4.10 No Cloning Theorem . . . . . . . . . . . . . . . . . . . . . . .
203
4.11 Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
4.12 Zeno Machines
. . . . . . . . . . . . . . . . . . . . . . . . . .
206
4.13 Inexhaustible Uncertainty . . . . . . . . . . . . . . . . . . . .
209
4.14 Randomness . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
4.15 The EPR Conundrum and Bell’s Theorem . . . . . . . . . . .
216
4.16 Quantum Logic . . . . . . . . . . . . . . . . . . . . . . . . . .
223
4.17 Have Quantum Propositions Classical Meaning?
. . . . . . .
226
4.18 Quantum Computers . . . . . . . . . . . . . . . . . . . . . . .
234
4.19 Quantum Algorithms . . . . . . . . . . . . . . . . . . . . . . .
241
4.20 Quantum Complexity
. . . . . . . . . . . . . . . . . . . . . .
263
4.21 Quantum Cryptography . . . . . . . . . . . . . . . . . . . . .
272
4.22 Information and Teleportation
. . . . . . . . . . . . . . . . .
274
4.23 Computing the Uncomputable? . . . . . . . . . . . . . . . . .
279
4.24 Bibliographical Notes . . . . . . . . . . . . . . . . . . . . . . .
280
5
Final Remarks
283
Bibliography
285
Index
307

Preface
Quand je vous aimer?
Ma foi, je ne sais pas.
Peut-ˆetre jamais,
Peut-ˆetre demain!
Carmen
The computers as we know them today, based on silicon chips, are getting
better and better, and are doing more and more for us; in practice, we can no
longer live without our intelligent prostheses. Nonetheless, they still give rise
to frustrations, and not just among academics. Though ever faster in speed
and ever larger size, silicon-based computers are not yet able to cope with
many tasks of practical interest. There are still too many problems that are
eﬀectively intractable because to “solve” them using “classical” machines re-
quires a huge amount of computing time. It seems that progress in electronic
hardware (and the corresponding software engineering) is not enough; for
instance, the miniaturization is approaching the quantum boundary, where
physical processes obey laws based on probabilities and non-determinism,
something almost completely absent in the operation of “classical” comput-
ers. So, new breakthrough is needed.
The idea of such a breakthrough is not really new. It involves going “to
the bottom”, where – Feynman assures us – “there is plenty of room”. So one
possible way to proceed is to get closer to the innermost structure of matter,
beyond micro, into nano, even into femto. This means computing directly
with molecules and atoms, maybe even with their parts.
From the physical point of view, this means quantum physics, an incred-
ible successful, but controversial subject with a great history and a most
promising future.
One hope is for a quantum computer, a computer of a
completely diﬀerent type that can make use of the strange but wonderful
phenomena that occur at the atomic and intra-atomic levels. However, there
are problems: just think about the wave-particle duality of quanta, about
the possibility of a particle existing in several places at the same time and
of several particles being in the same place at the same time, about tele-
portation, and so forth. There are a number of challenging diﬃculties from
the computability point of view: for instance, that observation modiﬁes the
vii

viii
Preface
observed system, which means that reading some information destroys that
information, or that quantum interference of particles cannot be controlled
using current technology.
Fortunately, for billions of years nature itself has been “computing” with
molecules and cells. When speaking about computability in nature, we think
it is better to use quotation marks; we do not accept that nature computes
except in a metaphorical sense. Bees know no geometry, rain drops solve no
diﬀerential equation when falling. Only man sees hexagons in beehives and a
potential ﬁeld on the surface of a water droplet. However, less metaphorical
and so more visible is the way nature has used (and is still using . . . ) DNA
in evolution. Thus another hope for the future is to use DNA and other
proteins, manipulated by tools already known to biochemistry and genetic
engineering (for instance, enzymes, “one of the most intelligent gifts nature
has made to us”, as it has been said), and to develop “wet computers”, with
core chips consisting of bio-molecules.
Both these ideas, Quantum Computing and DNA Computing, have been
explored intensively in recent years. A third idea, explored only recently,
is also biochemically inspired: Membrane Computing. This seeks to make
use of the way nature “computes” at the cellular level, where an intricate
processing of materials, energy, and information takes place in a membrane
structure which determines the architecture of the “cell-computer” and ac-
tively participate in the “computation”.
The aim of this book is to introduce the reader to these fascinating areas,
currently located towards the science-ﬁction edge of science: computing with
cells and atoms.
Although mathematically oriented (more precisely, mathematics of the
kind involved in theoretical computer science), the book does not go into
full formal detail (complete proofs, for instance). Our intention is to present
basic notions and results from the three domains mentioned above, Quantum
Computing, DNA Computing, and Membrane Computing, providing just a
glimpse of each. Of course our selection is biased and partisan, favouring
subjects which we have dealt with in our personal research, but a number of
fundamental matters are included: Adleman’s experiment and its continua-
tions, in the DNA area, and Shor’s algorithm, in the quantum area.
Because our interest (and our competence) tends toward formal models
of computability, we shall skip over much “practical” information to do with
physics and biochemistry. The general strategy we adopt is to look to reality
(whatever this means;1 passively, but safely, for us reality is what we can ﬁnd
in books and call such) in search of data supports (hence data structures) and
operations on these data structures. With these ingredients we can deﬁne a
process (in the form of a sequence of transitions among conﬁgurations de-
scribing states of a system) which, provided that an input and an output can
1“We are both onlookers and actors in the great drama of existence”, according to
N. Bohr.

Preface
ix
be associated with it, is considered a computation. The complete machin-
ery is individualized as a computing system. Although this is crucial from a
practical point of view, we do not restrict ourselves to “realistic” systems,
implementable today or in the immediate future. We look for plausible com-
puters and, in the main, for models of them. Progress in technology, physical
or biochemical, is so rapid (and unpredictable) that it is not eﬃcient to limit
consideration to knowledge and tools that are available today (or tomorrow).
In a sense, we are replaying Turing’s game: just consider machines with inﬁ-
nite tapes and prove universality theorems about them; maybe, one day this
will at least encourage some practitioners to build real devices that resemble
these machines.
In the areas of DNA and Membrane Computing, we are inspired by what
happens in vivo and we try to approach as possible what can be done in
vitro, but we work in info, in symbolic terms. In Quantum Computing we
also look at possibilities of transcending Turing computability barrier.
The constant test bed for the new computability devices we consider will
be the usual hierarchies in computability theory, namely Turing and Chomsky
classes, and complexity classes. More speciﬁcally, we are interested in attain-
ing the power of Turing machines which (according to the Church–Turing
thesis) have the most comprehensive level of algorithmic computability, if
possible, in a constructive manner. Constructively simulating a Turing ma-
chine by means of a device from a new class of computing machines provides
us with universality for free: starting from a universal Turing machine we
get a universal device in our new class. This is important if we are looking
for programmable computers of the new type. Programmability means uni-
versality. In the sense of Conrad [63], it also means non-learnability and/or
non-eﬃciency (Conrad advocates convincingly a general trade-oﬀbetween
programmability, learnability and eﬃciency), but where we do not consider
such “details”. The particular domains of Quantum and DNA Computing
(not to speak about Membrane Computing) are not yet able to deal with
such questions.
In order to make the book as self-contained as possible, we start by spec-
ifying a series of prerequisites drawn variously from automata and language
theory, complexity, quantum physics and the biochemistry of DNA. Of course,
some broad familiarity with these subjects would be useful, but we “pretend”
that the book can be read by any (mathematically) educated layman.
Acknowledgements
Parts of the work brought together in this book have been done while
the authors have visited a number of institutions. The ﬁrst author has been
supported by a research grant of the University of Auckland Research Com-
mittee, Sandia National Laboratories, Albuquerque (April 1999), the Tech-
nical University of Vienna (May-July 1999), the Japanese Advanced Insti-
tute of Science and Technology and the Japanese Ministry of Education,
Science, Sports and Culture (September-December 1999). The second au-

x
Preface
thor has beneﬁted from research in Tokyo (April-May 1999 and supported
by the “Research for Future” Program no. JSPS-RFTF 96I00101, from the
Japanese Society for the Promotion of Science), Magdeburg (July-August
1999, supported by the Alexander von Humboldt Foundation), and Turku
(September-October 1999, supported by the Academy of Finland, Project
137358).
The authors are much indebted to Ioannis Antoniou, Elena Calude, John
Casti, Greg Chaitin, Michael Dinneen, Peter Hertling, Hajime Ishihara, Sorin
Istrail, Seth Lloyd, David Mermin, Georgio Odifreddi, Bernard ¨Omer, Grze-
gorz Rozenberg, Boris Pavlov, Ilya Prigogine, Arto Salomaa, Shao Chin Sung,
Karl Svozil, Marius Zimand for comments, discussions and criticism. Spe-
cial thanks are due to Dilys Alam, Luke Hacker, Tony Moore, Grant Soanes,
to the Taylor and Francis team in London and to Steven Gardiner Ltd of
Cambridge for their most pleasant and eﬃcient co-operation.
Cristian S. Calude
Gheorghe P˘aun

Chapter 1
Prerequisites
The aim of this chapter is to introduce the notions and notation of theoretical
computer science we shall use in the subsequent chapters, thus making the
book self-contained from this point of view. Of course, some previous famil-
iarity with these notions would be useful; a reader familiar with the ﬁeld can
consult this chapter only when need arises.
1.1
Preliminary Notions and Notations
The family of subsets of a set X is denoted by P(X); if X is an inﬁnite set,
then we denote by Pf(X) the family of ﬁnite subsets of X. The cardinality of
X is denoted by card(X). The set of natural numbers, {0, 1, 2, . . .} is denoted
by N. The empty set is denoted by ∅.
An alphabet is a ﬁnite nonempty set of abstract symbols. For an alphabet
V we denote by V ∗the set of all strings of symbols in V . The empty string
is denoted by λ. The set of nonempty strings over V , that is V ∗−{λ}, is
denoted by V +. Each subset of V ∗is called a language over V . A language
which does not contain the empty string (hence it is a subset of V +) is said
to be λ-free.
The length of a string x ∈V ∗(the number of symbol occurrences in x)
is denoted by |x|. The number of occurrences of a given symbol a ∈V in
x ∈V ∗is denoted by |x|a. If x ∈V ∗, U ⊆V, then by |x|U we denote the
length of the string obtained by erasing from x all symbols not in U, that is,
|x|U =

a∈U
|x|a.
If V = {a1, a2, . . . , an} (the order of symbols is important) and w ∈V ∗,
then ΨV (w) = (|w|a1, . . . , |w|an) is the Parikh vector associated with w (and
V ). For a language L ⊆V ∗, ΨV (L) = {ΨV (w) | w ∈L} is called the Parikh
set of L.
1

2
Prerequisites
A multiset (over a set X) is a mapping M : X −→N ∪{∞}.
For
a ∈X, M(a) is called the multiplicity of a in the multiset M. The sup-
port of M is the set supp(M) = {a ∈X | M(a) > 0}.
A multiset
M of ﬁnite support, supp(M) = {a1, . . . , an} can be written in the form
{(a1, M(a1)), . . . , (an, M(an))}. We can also represent this multiset by the
string w(M) = aM(a1)
1
. . . aM(an)
n
, as well as by any permutation of w(M).
Conversely, having a string w ∈V ∗, we can associate with it the multiset
M(w) : V −→N ∪{∞} deﬁned by M(w)(a) = |w|a, a ∈V .
For two multisets M1, M2 (over the same set X) we say that M1 is included
in M2, and we write M1 ⊆M2 if M1(a) ≤M2(a) for all a ∈X. The union of
M1, M2 is the multiset M1 ∪M2 deﬁned by (M1 ∪M2)(a) = M1(a) + M2(a).
We deﬁne here the diﬀerence M2 −M1 of two multisets only if M1 ⊆M2 and
this is the multiset deﬁned by (M2 −M1)(a) = M2(a) −M1(a).
1.2
Operations on Strings and Languages
The union and the intersection of two languages L1, L2 are denoted by L1 ∪
L2, L1 ∩L2, respectively. The concatenation of L1, L2 is L1L2 = {xy | x ∈
L1, y ∈L2}.
We deﬁne further:
L0 = {λ},
Li+1 = LLi, i ≥0,
L∗=
∞

i=0
Li (the ∗-Kleene closure),
L+ =
∞

i=1
Li (the + -Kleene closure).
A mapping h : V −→U ∗, extended to s : V ∗−→U ∗by h(λ) = {λ} and
h(x1x2) = h(x1)h(x2), for x1, x2 ∈V ∗, is called a morphism. If λ /∈h(a), for
each a ∈V , then h is a λ-free morphism.
A morphism h : V ∗−→U ∗is called a coding if h(a) ∈U for each a ∈V
and a weak coding if h(a) ∈U ∪{λ} for each a ∈V . If h : (V1 ∪V2)∗−→V ∗
1
is the morphism deﬁned by h(a) = a for a ∈V1, and h(a) = λ otherwise,
then we say that h is a projection (associated with V1) and we denote it by
prV1.
For x, y ∈V ∗we deﬁne their shuﬄe by
x ⊔⊥y = {x1y1 . . . xnyn | x = x1 . . . xn, y = y1 . . . yn,
xi, yi ∈V ∗, 1 ≤i ≤n, n ≥1}.
The mirror image of a string x = a1a2 . . . an, for ai ∈V, 1 ≤i ≤n, is the
string mi(x) = an . . . a2a1.

A General Computing Framework
3
In general, if we have an n-ary operation on strings, g : V ∗×. . .×V ∗−→
P(U ∗), we extend it to languages over V by
g(L1, . . . , Ln) =

xi ∈Li
1 ≤i ≤n
g(x1, . . . , xn).
For instance, mi(L) = {mi(x) | x ∈L}.
A family FL of languages is closed under an n-ary operation g if, for all
languages L1, . . . , Ln in FL, the language g(L1, . . . , Ln) is also in FL.
1.3
A General Computing Framework
The aim of this section is to “deﬁne”, in a general, somewhat formal, but
not in a very precise way, the notions of computation and computing de-
vice/model. In fact, we are not looking for a comprehensive deﬁnition, but
for a comprehensive framework for an activity which can be called a com-
putation. We do not address here semiotic or philosophical questions (can
a computation be a natural process, or is it a speciﬁc human artifact? is a
process a computation per se, or only for some observer? and so on), but
we step into an operational direction: we want to systematize the notions
we can encounter when devising a computing machinery, in a comprehensive
manner which is also general enough. Otherwise stated, we do not adopt a
minimalistic approach, like Turing’s one, but a globalistic one, able to cover
all/most of the computing devices we will discuss in the subsequent chapters.
A computation takes place in an environment where the following three
elements are available:
• a set S, potentially inﬁnite, of states (or conﬁgurations);
• a transition relation on this set, →⊆S × S; any sequence of transitions
between elements of S, s1 →s2 →. . . →sn, for s1, . . . , sn ∈S, n ≥1
(we write s1 →∗sn; we also accept that s1 = sn and we say that →∗is
the reﬂexive and transitive closure of →), is called a computation; we
denote by K the set of all computations with respect to S and →;
• a stop predicate π : K −→{0, 1}; by Kπ we denote the set of computa-
tions which stop correctly with respect to π, that is, Kπ = {s1 →∗s2 ∈
K | π(s1 →∗s2) = 1}.
In order to assign a result to a computation, we need two further items:
• a set R of possible results;
• an output mapping ρ : Kπ −→R.

4
Prerequisites
Let us denote by Γ the system of the ﬁve elements mentioned above,
Γ = (S, →, π, R, ρ).
Such a system deﬁnes two mappings, one from S to the power set of S,
and one from S to the power set of R:
Γ(s1) = {s2 | s1 →∗s2 ∈Kπ}, for each s1 ∈S,
resΓ(s1) = {ρ(s1 →∗s2) | s2 ∈S, s1 →∗s2 ∈Kπ}.
In some sense, we have here two levels of observing the system, one which
deals only with states (conﬁgurations), without assigning a “meaning” to
these states or to sequences of transitions between them, and one which also
assigns a “result” to a correctly completed computation. It is important to
note that the result is associated with the whole computation, not only with
the last conﬁguration, the halting one.
The previous setup is rather vague, but already at this stage we can
introduce a series of important computability concepts: determinism, equiv-
alence, universality, reversibility, time complexity. We do not approach at
this moment such questions, but we make one further step “toward reality”,
specifying less general ingredients of a computing framework.
We call a computing framework a construct where the following items can
be identiﬁed (some of them can also be present by default):
– An alphabet A, whose elements are identiﬁed with symbols, without
parts, without a structure (they are atoms, in the etymological sense of
the word).
– Using the elements in A, one constructs certain types of data struc-
tures, whose set is denoted by D(A); it is possible that D(A) contains
data structures of various forms, for instance, both strings and double
stranded sequences; it is also possible that, in order to construct the
set D(A), a structure of A is needed, in the form of a relation over A
(an order relation, a complementarity relation, a semi-commutativity
relation, etc.), of a partition of A (we can distinguish between input
symbols, output symbols, state symbols, and so on), etc., but we do
not consider in this moment such distinctions.
– For computing, we need certain resources; we consider here the case
when these resources consist of elements of D(A), with multiplicities,
that is, we consider a multiset W : D(A) −→N ∪{∞}; a computation
is assumed to consume elements of W, which means that only compu-
tations which do not need more resources than those provided by W
are possible. (Of course, in a more general approach, we can consider
a speciﬁed set M of resources – raw materials, energy, etc. – and a
multiset W deﬁned on this set, W : M −→N ∪{∞}.)

A General Computing Framework
5
– A set C of conﬁgurations, which describe the state of the computing
machinery at a given moment of time; a comprehensive enough rep-
resentation of such a state is that of a tuple of elements from D(A)
(remember that D(A) can contain elements of various types, from sym-
bols from A to complex constructs based on such symbols).
– Essential for deﬁning the notion of a computation is to have a well-
deﬁned transition relation among conﬁgurations, =⇒f⊆C × C (the
subscript f indicates the fact that we consider =⇒f a free transition
relation, unrestricted by a control, as we will mention immediately); as
usual, the reﬂexive and transitive closure of =⇒f is denoted by =⇒∗
f.
In many – if not all – cases of a practical interest, the transitions are
deﬁned starting from a given set of operations on data structures: hav-
ing such operations, one extends them to conﬁgurations in order to get
transitions.
– For various reasons, a restriction of =⇒f or of =⇒∗
f can be use-
ful/necessary, that is, subsets =⇒ctr⊆=⇒f or =⇒∗
ctr⊆=⇒∗
f should be
considered; note that the control can be deﬁned directly for =⇒∗
f, not
ﬁrst for =⇒f and then extended to =⇒∗
f.
In this moment, we can deﬁne two sets of computations, as sequences
of transitions, namely, arbitrary computations (without limiting the re-
sources they use),
K = {C1 =⇒∗
ctr C2 | C1, C2 ∈C},
and feasible computations (which do not exceed the resources in W).
Let us denote by w(C1 =⇒∗
crt C2) the multiset of resources consumed
by the computation C1 =⇒∗
ctr C2. Then, we deﬁne
KW = {C1 =⇒∗
ctr C2 | C1, C2 ∈C, w(C1 =⇒∗
ctr C2) ⊆W}.
(Of course, the inclusion above holds in the multisets sense.)
– A stop condition π : KW −→{0, 1}, in the form of a predicate;
π(C1 =⇒∗
ctr C2) = 1 means that the (feasible) computation C1 =⇒∗
ctr
C2 is completed in an acceptable manner (whatever this means in a spe-
ciﬁc situation). By KW,π we denote the set of all feasible computations
which halt in a correct way.
– An output space R, which can be a set containing various types of ele-
ments, such as data structures from D(A), natural numbers or relations
over natural numbers, Boolean values, and so on.
– An output mapping, ρ : KW,π −→R, associating to each feasible and
correctly halted computation a result.

6
Prerequisites
The previous presentation is still rather general and informal, but suﬃ-
cient for our purposes.
Let us denote by Γ such a computing framework (also called a computing
system).
Of course, for pragmatic and also for mathematical reasons, the elements
of Γ are supposed to be “as simple as possible”: the alphabet A is either
ﬁnite or, at most, denumerable (and in a speciﬁc computation only a ﬁnite
set of symbols are used), the relations and the mappings involved (the tran-
sition relation =⇒, free or controlled, the stop predicate π, and the output
mapping ρ) are supposed to be at least eﬀectively computable, if not “easily”
computable. Such conditions will be implicitly fulﬁlled by all speciﬁc models
we shall consider in the subsequent chapters.
As above, with a conﬁguration C1 we can associate both the set of all
conﬁgurations which can be reached at the end of feasible and correctly com-
pleted computations which start in C1,
Γ(C1) = {C2 | C1 =⇒∗
ctr C2 ∈KW,π},
and the set of possible results of these computations,
resΓ(C1) = {ρ(C1 =⇒∗
ctr C2) | C1 =⇒∗
ctr C2 ∈KW,π}.
Several properties of computing frameworks (of a speciﬁed class) can be
deﬁned in this context. We consider here only two of them.
A computing framework Γ is said to be (strongly) deterministic if, for
each conﬁguration C1 ∈C, there is a unique conﬁguration C2 ∈C such that
C1 =⇒ctr C2. For some conﬁguration C1 ∈C, a computing framework Γ is
said to be C1-deterministic, if, for each C2 ∈C such that C1 =⇒∗
ctr C2 ∈KW
(in particular, for C2 = C1), there is a unique conﬁguration C3 ∈C such that
C1 =⇒∗
ctr C2 =⇒crt C3 ∈KW (all computations starting from C1 proceed
deterministically, with one possible choice at each step). Note that we take
into account only feasible computations.
A computing framework Γu is said to be universal if, for each computing
framework Γ, there is code(Γ) ∈D(A) such that for each conﬁguration C1 of
Γ we can get a conﬁguration C′
1 of Γu which “contains” (in a speciﬁed sense)
both C1 and code(Γ) and we have
resΓu(C′
1) = resΓ(C1).
That is to say, if we introduce the code of Γ in an initial conﬁguration of
Γu, together with an initial conﬁguration of Γ, then the universal computing
framework Γu behaves exactly as the particular one, Γ, when starting from
that initial conﬁguration.
The previous setup is so general that it covers many particular cases,
so it is a good reference framework, but it is too general in order to obtain

A General Computing Framework
7
relevant mathematical results. For speciﬁc cases, we have to particularize
also the items considered above.
The overall strategy is the following: we examine the reality1, looking for
the basic ingredients of our computing framework, the data structures and
the operations on them; also basic, but somewhat of a second level, are the
possible controls on the operations. (Here, possible means, in general, realis-
tic, plausible, sometimes only desirable and not proved yet to be impossible,
and not necessarily feasible, implementable in the present day laboratories.)
After having these basic ingredients, data structures, operations, controls,
we pass to the next stage, of integrating these items in a computing machinery
(of a theoretical type): we deﬁne the transitions, the stop conditions, hence
the computations and, after that, the results of computations. In short, we
have to deﬁne the computing models we look for.
Then, a third phase is to be followed, of a mathematical type, aiming to
answer questions of the following forms: How powerful the deﬁned models
are (in comparison with classic models investigated in theoretical computer
science)?
Are there universality results?
(A positive answer ensures the
theoretical possibility of programmability.) How useful, from a computational
point of view the obtained models are? This means mainly time complexity
of solving problems or of computing functions in the new framework, in the
hope that diﬃcult problems/functions can be solved/computed signiﬁcantly
faster than with a classic computing device.
Of course, the fourth phase should be concerned with the implementation
of a computing device, the actual building of a “computer” based on the
above speciﬁed model. This closes the circle, returning to the reality, and
this is deﬁnitely the validating step from a practical point of view. However,
here we will not go that far, because the implementation of the models we
consider is out of the scope of the present book. Currently, both Quantum
and Molecular Computing are mainly developed at the level of the ﬁrst three
phases and the reasons are obvious: the theory acts in an ideal world where
all wishes which are logically consistent (and not contradicted by the state-
of-the-art of biochemistry and physics) can be assumed as being fulﬁlled,
everything which is plausible can also be assumed as being possible, which
is not exactly what the technology provides to us.
There also is a great
attraction towards theory and theoretical results, as a manifestation of the
need for knowledge. Knowledge is in some sense a goal per se: How nature
“computes”? is a fascinating question (even if, always when speaking about
nature, we keep the quotation marks around the term “computation”).
1What is the reality is another story.
In fact, we look for the reality as it appears
in books – biochemical books in the case of Molecular Computing, books of physics in
the case of Quantum Computing.
Otherwise stated, we always have in mind a repre-
sentation/conceptualization of “reality” and we do not care too much how adequate this
representation is, we simply rely on the current state-of-the-art of the mentioned scientiﬁc
ﬁelds.

8
Prequisites
1.4
Chomsky Grammars
We consider now one of the most important classes of computing devices
in theoretical computer science, the rewriting systems, with their particular
form of Chomsky grammars. We do not systematically particularize the items
of a computing framework as introduced in the previous section, but we only
mention that we work with strings as data structures, while the operation
we use is rewriting (replacing a short substring of a string by another short
string).
A rewriting system is a pair γ = (V, P), where V is an alphabet and P
is a ﬁnite subset of V ∗× V ∗; the elements (u, v) of P are written in the
form u →v and are called rewriting rules/productions (or simply rules or
productions). For x, y ∈V ∗we write x =⇒γ y if x = x1ux2, y = x1vx2, for
some u →v ∈P and x1, x2 ∈V ∗. If the rewriting system γ is understood,
then we write =⇒instead of =⇒γ. The reﬂexive and transitive closure of
=⇒is denoted by =⇒∗.
If an axiom is added to a rewriting system and all rules u →v have
u ̸= λ, then we obtain the notion of a pure grammar. For a pure grammar
G = (V, w, P), where w ∈V ∗is the axiom, we deﬁne the language generated
by G by
L(G) = {x ∈V ∗| w =⇒∗x}.
A Chomsky grammar is a quadruple G = (N, T, S, P), where N, T are
disjoint alphabets, S ∈N, and P is a ﬁnite subset of (N ∪T)∗N(N ∪T) ×
(N ∪T)∗.
The alphabet N is called the nonterminal alphabet, T is the terminal
alphabet, S is the axiom, and P is the set of production rules of G. The rules
(we also say productions) (u, v) of P are written in the form u →v. Note
that |u|N ≥1.
For x, y ∈(N ∪T)∗we write
x =⇒G y
iﬀ
x = x1ux2, y = x1vx2,
for some x1, x2 ∈(N ∪T)∗and u →v ∈P.
One says that x directly derives y (with respect to G).
Each string w ∈
(N ∪T)∗such that S =⇒∗
G w is called a sentential form.
The language generated by G, denoted by L(G), is deﬁned by
L(G) = {x ∈T ∗| S =⇒∗x}.
(Note that the stop condition is the condition to have no nonterminal symbol
present in the obtained string and that the result of a computation precisely
consists of this last string, composed of only terminal symbols.) Two gram-
mars G1, G2 are called equivalent if L(G1) −{λ} = L(G2) −{λ} (the two
languages coincide modulo the empty string).

Chomsky Grammars
9
In general, in this book we consider two generative mechanisms equivalent
if they generate the same language when we ignore the empty string (the
empty string has no “real life” counterpart, so in many cases we ignore it).
According to the form of their rules, the Chomsky grammars are classiﬁed
as follows. A grammar G = (N, T, S, P) is called:
– monotonous/context-sensitive, if for all u →v ∈P we have |u| ≤|v|;
– context-free, if each production u →v ∈P has u ∈N;
– linear, if each rule u →v ∈P has u ∈N and v ∈T ∗∪T ∗NT ∗;
– regular, if each rule u →v ∈P has u ∈N and v ∈T ∪TN ∪{λ}.
The arbitrary, monotonous, context-free, and regular grammars are also said
to be of type 0, type 1, type 2, and type 3, respectively.
We denote by CE, CS, CF, LIN, and REG the families of languages gen-
erated by arbitrary, context-sensitive, context-free, linear, and regular gram-
mars, respectively (CE stands for computably enumerable; we keep here this
“classic” terminology, although we agree with [273] that computably enumer-
able is a more adequate sintagm). By FIN we denote the family of ﬁnite
languages.
The following strict inclusions hold:
FIN ⊂REG ⊂LIN ⊂CF ⊂CS ⊂CE.
This is the Chomsky hierarchy, the constant reference for the investigations
in the following chapters.
As the context-free grammars are not powerful enough for covering most
of the important syntactic constructions in natural and artiﬁcial languages,
while the context-sensitive grammars are too powerful (for instance, the fam-
ily CS has many negative decidability properties and the derivations in a
non-context-free grammar cannot be described by a derivation tree), it is of
interest to increase the power of context-free grammars by controlling the
use of their rules (by imposing a control on the computations in a context-
free grammar, as we have considered in the general computing framework
discussed in Section 1.3.)
This leads to considering regulated context-free
grammars. We do not enter here into details (the reader can consult [75] as
well as the corresponding chapter from [258]), although several of the controls
used in formal language theory will also suggest useful ways of controlling
the computations in various models deﬁned in the DNA Computing area.
Another chapter of formal language theory which suggests fruitful ideas
to DNA Computing is grammar systems theory, which deals with systems
consisting of several grammars which work together in a well speciﬁed man-
ner and generate one single language. There are two main classes of grammar
systems, the sequential ones (introduced in [68] under the name of cooperat-
ing distributed grammar systems), where the components work in turns, on a

10
Prerequisites
common sentential form, and the parallel systems (introduced in [229] under
the name of parallel communicating grammar systems), whose components
work synchronously, on their own sentential forms, and communicate by re-
quest or by command. In both cases, a signiﬁcant increase of the power of
context-free grammars is obtained. Details can be found in [69] and in the
corresponding chapter from [258].
For certain classes of grammars in the Chomsky hierarchy it is possible
to work with grammars of a speciﬁed form without losing the generative
power. We mention here only two normal forms of this type, for grammars
characterizing CE, because they will be useful later.
Theorem 1.1 (Kuroda normal form) For every type-0 grammar G, an equiv-
alent grammar G′ = (N, T, S, P) can be eﬀectively constructed, with the rules
in P of the forms A →BC, A →a, A →λ, AB →CD, for A, B, C, D ∈N
and a ∈T.
A similar result holds true for monotonous grammars, where rules of the
form A →λ are no longer allowed.
Theorem 1.2 (Geﬀert normal forms) (i) Each computably enumerable lan-
guage can be generated by a grammar G = (N, T, S, P) with N = {S, A, B, C}
and the rules in P of the forms S →uSv, S →x, with u, v, x ∈(T ∪
{A, B, C})∗, and only one non-context-free rule, ABC →λ.
(ii) Each computably enumerable language can be generated by a grammar
G = (N, T, S, P) with N = {S, A, B, C, D} and the rules in P of the forms
S →uSv, S →x, with u, v, x ∈(T ∪{A, B, C, D})∗, and only two non-
context-free rules, AB →λ, CD →λ.
1.5
Lindenmayer Systems
A variant of rewriting systems, related to Chomsky grammars, but with
the derivation steps performed in a maximally parallel manner (see precise
deﬁnitions below), are the Lindenmayer systems, introduced with biological
motivations. We present here only a few elementary notions.
Basically, a 0L (0-interactions Lindenmayer) system is a context-free pure
grammar with parallel derivations: G = (V, w, P), where V is an alphabet,
w ∈V ∗(axiom), and P is a ﬁnite set of rules of the form a →v with
a ∈V, v ∈V ∗, such that for each a ∈V there is at least one rule a →v
in P (we say that P is complete). For w1, w2 ∈V ∗we write w1 =⇒w2 if
w1 = a1 . . . an, w2 = v1 . . . vn, for ai →vi ∈P, 1 ≤i ≤n. The generated
language is L(G) = {x ∈V ∗| w =⇒∗x}.
If for each rule a →v ∈P we have v ̸= λ, then we say that G is
propagating (non-erasing); if for each a ∈V there is only one rule a →v in
P, then G is said to be deterministic. If we distinguish a subset T of V and

Automata and Transducers
11
we deﬁne the generated language by L(G) = {x ∈T ∗| w =⇒∗x}, then we
say that G is extended. The family of languages generated by 0L systems
is denoted by 0L; we add the letters P, D, E in front of 0L if propagating,
deterministic, or extended 0L systems are used, respectively.
A tabled 0L system, abbreviated T0L, is a system G = (V, w, P1, . . . , Pn),
such that each triple (V, w, Pi), 1 ≤i ≤n, is a 0L system; each Pi is called a
table, 1 ≤i ≤n. The generated language is deﬁned by
L(G) = {x ∈V ∗| w =⇒Pj1 w1 =⇒Pj2 . . . =⇒Pjm wm = x,
m ≥0, 1 ≤ji ≤n, 1 ≤i ≤m}.
(Each derivation step is performed by the rules of the same table.)
A T0L system is deterministic when each of its tables is deterministic.
The propagating and the extended features are deﬁned in the usual way.
The family of languages generated by T0L systems is denoted by T0L; the
ET0L, EDT0L, etc. families are obtained in the same way as E0L, ED0L,
etc.
The D0L family is incomparable with FIN, REG, LIN, CF, whereas E0L
strictly includes the CF family; ET0L is the largest family of Lindenmayer
languages without interactions (context-free) and it is strictly included in
CS.
1.6
Automata and Transducers
Automata are language deﬁning devices which work in the direction opposite
to grammars: they start from the strings over a given alphabet and ana-
lyze them (we also say recognize), telling us whether or not any input string
belongs to a speciﬁed language.
The ﬁve basic families of languages in the Chomsky hierarchy, REG, LIN,
CF, CS, CE, are also characterized by recognizing automata. These automata
are: the ﬁnite automaton, the one-turn pushdown automaton, the pushdown
automaton, the linear-bounded automaton, and the Turing machine, respec-
tively. We present here only the ﬁnite automaton and the Turing machine,
which mark, in some sense, the two poles of computability.
A (non-deterministic) ﬁnite automaton is a construct
M = (K, V, s0, F, δ),
where K and V are disjoint alphabets, s0 ∈K, F ⊆K, and δ : K × V −→
P(K); K is the set of states, V is the alphabet of the automaton, s0 is the
initial state, F is the set of ﬁnal states, and δ is the transition mapping.
If card(δ(s, a)) ≤1 for all s ∈K, a ∈V , then we say that the automaton
is deterministic.
A relation ⊢is deﬁned as follows on the set K × V ∗: for
s, s′ ∈K, a ∈V, x ∈V ∗, we write (s, ax) ⊢(s′, x) if s′ ∈δ(s, a); by deﬁnition,

12
Prerequisites
(s, λ) ⊢(s, λ). If ⊢∗is the reﬂexive and transitive closure of the relation ⊢,
then the language recognized by the automaton M is deﬁned by
L(M) = {x ∈V ∗| (s0, x) ⊢∗(s, λ), s ∈F}.
Deterministic and non-deterministic ﬁnite automata characterize the
same family of languages, namely REG. The power of ﬁnite automata is not
increased if we also allow λ-transitions, that is, if δ is deﬁned on K×(V ∪{λ})
(the automaton can also change its state when reading no symbol from its
tape), or when the input string is scanned in a two-way manner, going along
it to the right or to the left, without changing its symbols.
s
✻
Figure 1.1: A ﬁnite automaton.
An important related notion is that of a sequential transducer; we shall
use the abbreviation gsm, from “generalized sequential machine”. Such a
device is a system g = (K, V1, V2, s0, F, δ), where K, s0, F are the same as in
a ﬁnite automaton, V1, V2 are alphabets (the input and the output alphabet,
respectively), and δ : K × V1 −→Pf(V ∗
2 × K). If δ(s, a) ⊆V +
2 × K for all
s ∈K, a ∈V1, then g is said to be λ-free. If card(δ(s, a)) ≤1 for each s ∈K,
a ∈V1, then g is said to be deterministic. For s, s′ ∈K, a ∈V1, y ∈V ∗
1 ,
x, z ∈V ∗
2 , we write (x, s, ay) ⊢(xz, s′, y) if (z, s′) ∈δ(s, a).
Then, for
w ∈V ∗
1 , we deﬁne
g(w) = {z ∈V ∗
2 | (λ, s0, w) ⊢∗(z, s, λ), s ∈F}.
The mapping g is extended in the natural way to languages over V1.
If V1 ⊆V2, then g can be iterated. We denote: g0(L) = L, gi+1(L) =
g(gi(L)), i ≥0, and g∗= 
i≥0 gi(L).
We can imagine a ﬁnite automaton as in Figure 1.1, where we distinguish
the input tape, in whose cells we write the symbols of the input string, the
read head, which scans the tape from the left to the right, and the memory,
able to hold a state from a ﬁnite set of states. In the same way, a gsm is a
device as in Figure 1.2, where we also have an output tape, where the write
head can write the string obtained by translating the input string.
Sometimes it is useful to present the transition mapping of ﬁnite automata
and of gsm’s as a set of rewriting rules: we write sa →as′ instead of s′ ∈

Automata and Transducers
13
s
✻
❄
Figure 1.2: A sequential transducer.
δ(s, a) in the case of ﬁnite automata and sa →zs′ instead of (z, s′) ∈δ(s, a) in
the case of gsm’s. Then the relations ⊢, ⊢∗are exactly the same as =⇒, =⇒∗
in the rewriting system obtained in this way and, for a gsm g and a language
L ∈V ∗
1 , we get
g(L) = {z ∈V ∗
2 | s0w =⇒∗zs, w ∈L, s ∈F}.
For ﬁnite automata we have a special case: L(M) = {x ∈V ∗| s0x =⇒∗xs,
s ∈F}.
A Turing machine is a construct
M = (K, V, T, B, s0, F, δ),
where K, V are disjoint alphabets (the set of states and the tape alphabet),
T ⊆V (the input alphabet), B ∈V −T (the blank symbol), s0 ∈K (the
initial state), F ⊆K (the set of ﬁnal states), and δ is a partial mapping from
K × V to P(K × V × {L, R}) (the move mapping; if (s′, b, d) ∈δ(s, a), for
s, s′ ∈K, a, b ∈V, and d ∈{L, R}, then the machine reads the symbol a in
state s and passes to state s′, replaces a with b, and moves the read-write head
to the left when d = L and to the right when d = R). If card(δ(s, a)) ≤1 for
all s ∈K, a ∈V , then M is said to be deterministic.
An instantaneous description of a Turing machine as above is a string
xsy, where x ∈V ∗, y ∈V ∗(V −{B}) ∪{λ}, and s ∈K. In this way we
identify the contents of the tape, the state, and the position of the read-write
head: it scans the ﬁrst symbol of y. Observe that the blank symbol may
appear in x, y, but not in the last position of y; both x and y may be empty.
We denote by IDM the set of all instantaneous descriptions of M.
On the set IDM one deﬁnes the direct transition relation ⊢M as follows:
xsay ⊢M xbs′y iﬀ(s′, b, R) ∈δ(s, a),
xs ⊢M xbs′ iﬀ(s′, b, R) ∈δ(s, B),

14
Prerequisites
xcsay ⊢M xs′cby iﬀ(s′, b, L) ∈δ(s, a),
xcs ⊢M xs′cb iﬀ(s′, b, L) ∈δ(s, B),
where x, y ∈V ∗, a, b, c ∈V, s, s′ ∈K.
The language recognized by a Turing machine M is deﬁned by
L(M) = {w ∈T ∗| s0w ⊢∗
M xsy for some s ∈F, x, y ∈V ∗}.
(This is the set of all strings such that the machine reaches a ﬁnal state when
starting to work in the initial state, scanning the ﬁrst symbol of the input
string.)
It is also customary to deﬁne the language accepted by a Turing machine
as consisting of the input strings w ∈T ∗such that the machine, starting
from the conﬁguration s0w, reaches a conﬁguration where no further move
is possible (we say that the machine halts). The two modes of deﬁning the
language L(M) are equivalent.
Graphically, a Turing machine can be represented as a ﬁnite automaton
(Figure 1.1). The diﬀerence between a ﬁnite automaton and a Turing machine
is visible only in their functioning: the Turing machine can move its head
in both directions and can rewrite the scanned symbol, possibly erasing it
(replacing it with the blank symbol).
Both the deterministic and the non-deterministic Turing machines char-
acterize the family of computably enumerable languages.
A Turing machine can be also viewed as a mapping-deﬁning device, not
only as a mechanism deﬁning a language.
Speciﬁcally, consider a Turing
machine M = (K, V, T, B, s0, F, δ). If β ∈IDM such that β = x1sax2 and
δ(s, a) = ∅, then we write β ↓(we say that β is a halting conﬁguration). We
deﬁne the mapping FM : IDM −→P(IDM) by FM(α) = {β ∈IDM | α ⊢∗
M
β and β ↓}. If M is deterministic, then FM is a mapping from IDM to IDM.
Given a mapping f : U ∗
1 −→U ∗
2 , where U1, U2 are arbitrary alphabets,
we say that f is computed by a deterministic Turing machine M if there are
two (computable) mappings C and D (of coding and decoding),
C : U ∗
1 −→IDM, D : IDM −→U ∗
2 ,
such that
D(FM(C(x))) = f(x).
In Section 1.8, when discussing and presenting universal Turing machines,
we shall use this interpretation of Turing machines (as well as the termination
of a computation by halting conﬁgurations, not by using ﬁnal states).
1.7
Characterizations of Computably
Enumerable Languages
We have mentioned that our constant framework here is Chomsky hierar-
chy and the two poles of computability we refer to are the regular languages

Characterizations of CE Languages
15
(corresponding to the power of ﬁnite automata) and the computably enu-
merable languages (characterized by Turing machines).
According to the
Church–Turing Thesis, the power of Turing machines is the highest level of
algorithmic computability. For this reason (and because we do not have good
universality results at the level of ﬁnite automata – see also Section 1.8), when
looking for new computability models it is desirable to obtain models equal
in power to Turing machines. In order to prove such a result, the characteri-
zations of the computably enumerable languages (for instance, by grammars
in the normal forms discussed in Section 1.4) will be very useful. When such
a direct simulation is not possible/visible, representation results available for
computably enumerable languages can be of a great help.
Some of these
results are quite non-intuitive, which makes their consequences rather inter-
esting. We present here without proofs some theorems of this type, where
we start from “small” subfamilies of CE and, by using powerful operations
(such as intersection, quotients, etc.), we cover the whole family CE.
Theorem 1.3 Each language L ∈CE, L ⊆T ∗, can be written in the form
L = g∗(a0) ∩T ∗, where g = (K, V, V, s0, F, P) is a gsm and a0 ∈V .
A quite powerful (and useful for some of the next chapters) representation
of computably enumerable languages starts from equality sets of morphisms.
For two morphisms h1, h2 : V ∗−→U ∗, the set
EQ(h1, h2) = {w ∈V ∗| h1(w) = h2(w)}
is called the equality set of h1, h2.
Theorem 1.4 Every computably enumerable language L ⊆T ∗can be written
in the form L = prT (EQ(h1, h2) ∩R), where h1, h2 are two morphisms, R is
a regular language, and prT is the projection associated with the alphabet T.
A variant of this result, useful in Section 2.8, is the following one.
Theorem 1.5 For each computably enumerable language L ⊆T ∗, there exist
two λ-free morphisms h1, h2, a regular language R, and a projection prT such
that L = prT (h1(EQ(h1, h2)) ∩R).
Note the diﬀerence between the representations in Theorems 1.4 and 1.5:
in the ﬁrst case the language L is obtained as a projection of the intersection
of the equality set with a regular language, whereas in the second case the
language L is the projection of the intersection of a regular language with the
image of the equality set under one of the morphisms deﬁning the equality
set.

16
Prerequisites
1.8
Universal Turing Machines and Type-0
Grammars
A computer is a programmable machine, able to execute any program it re-
ceives. From a theoretical point of view, this corresponds to the notion of a
universal Turing machine, in general, to the notion of universality as intro-
duced in Section 1.3.
Consider an alphabet T and a Turing machine M = (K, V, T, B, s0, F, δ).
As we have seen above, M starts working with a string w written on its tape
and reaches or not a ﬁnal state (and then halts), depending on whether or
not w ∈L(M). A Turing machine can be also codiﬁed as a string of symbols
over a suitable alphabet. Denote such a string by code(M). Imagine a Turing
machine Mu which starts working from a string which contains both w ∈T ∗
and code(M) for a given Turing machine M, and stops in a ﬁnal state if and
only if w ∈L(M).
In principle, the construction of Mu is simple. Mu only has to simulate
the way of working of Turing machines, and this is clearly possible: look for a
transition, as deﬁned by the mapping δ, depending on the current state and
the current position of the read-write head (this information is contained in
the instantaneous descriptions of the particular machine); whenever several
choices are possible, make copies of the current instantaneous description
and branch the machine evolution; if two copies of the same instantaneous
description appear, then delete one of them; if at least one of the evolution
variants leads to an accepting conﬁguration, stop and accept the input string,
otherwise continue.
Such a machine Mu is called universal. It can simulate any given Turing
machine, providing that a code of a particular one is written on the tape of
the universal one, together with a string to be dealt with by the particular
machine.
The parallelism with a computer, as we know the computers in their
general form, is clear: the code of a Turing machine is its program, the
strings to be recognized are the input data, the universal Turing machine is
the computer itself (its operating system).
Let us stress here an important distinction, that between computational
completeness and universality. Given a class C of computability models, we
say that C is computationally complete if the devices in C can characterize
the power of Turing machines. This means that given a Turing machine M
we can ﬁnd an element C in C such that C is equivalent with M. Thus,
completeness refers to the capacity of covering the level of computability (in
grammatical terms, this means to generate all computably enumerable lan-
guages). Universality is an internal property of C and it means the existence
of a ﬁxed element of C which is able to simulate any given element of C, in
the way described above for Turing machines.
Of course, we can deﬁne the completeness in a relative way, not referring to

Universal Turing Machines
17
the whole class of Turing machines but to a subclass of them. For instance,
we can look for context-free completeness (the possibility of generating all
context-free languages). Accordingly, we can look for universal elements in
classes of computing devices which are computationally complete for smaller
families of languages than the family of computably enumerable languages.
However, important for any theory which attempts to provide general models
of computing are the completeness and universality with respect to Turing
machines, and this will be the level we shall consider in this book.
The idea of a universal Turing machine was introduced by A. Turing
himself, who has also produced such a machine [289]. Many universal Tur-
ing machines are now available in the literature, mainly for the case when
Turing machines are considered as devices which compute mappings. In such
a framework, we say that a Turing machine is universal if it computes a
universal partial computable function (modulo the coding–decoding “inter-
face” mentioned in Section 1.6). Similarly, a Turing machine M1 simulates a
Turing machine M2 if there are two coding–decoding mappings
C : IDM2 −→IDM1, D : IDM1 −→IDM2,
such that for each α ∈IDM2 we have
D(FM1(C(α))) = FM2(α).
The (descriptional, static) complexity of a Turing machine can be evalu-
ated from various points of view: the number of states, the number of tape
symbols (the blank symbol included), or the number of moves (quintuples
(s, a, b, d, s′) such that (s′, b, d) ∈δ(s, a)).
We denote by UTM(m, n) the class of universal deterministic Turing
machines with m states and n symbols (because we must have halting con-
ﬁgurations, there can exist at most m · n −1 moves).
Small universal Turing machines were produced already in [267] (with
two states) and [194] (with seven states and four symbols). The up-to-date
results in this area are summarized in [252] (we also include the improvement
from [253]):
Theorem 1.6 (i) The classes UTM(2, 3), UTM(3, 2) are empty.
(ii) The following classes are non-empty:
UTM(22, 2), UTM(10, 3),
UTM(7, 4), UTM(5, 5), UTM(4, 6), UTM(3, 10), UTM(2, 18).
For the remaining 49 classes UTM(m, n) the question is open.
In most of the constructions on which the proofs in the subsequent chap-
ters are based, we shall start from a Chomsky type-0 grammar.
Given a Turing machine M we can eﬀectively construct a type-0 grammar
G such that L(M) = L(G). (Similarly, we can produce a type-0 grammar
G such that G computes, in a natural way and using appropriate coding–
decoding mappings, the same mapping FM as M. So, a grammar can be

18
Prerequisites
considered a function computing device, not only a language generating mech-
anism.)
The idea is very simple. Take a Turing machine M = (K, V, T, B, s0, F, δ)
and construct a non-restricted Chomsky grammar G working as follows:
starting from its axiom, G non-deterministically generates a string w over
V , then it makes a copy of w (of course, the two copies of w are separated
by a suitable marker; further markers, scanners and other auxiliary symbols
are allowed, because they can be erased when they are no longer necessary).
On one of the copies of w, G can simulate the work of M, choosing non-
deterministically a computation as deﬁned by δ; if a ﬁnal state is reached,
then the witness copy of w is preserved and everything else is erased.
Applying a construction of this type to a universal Turing machine Mu,
we obtain a universal type-0 Chomsky grammar Gu, a grammar which is
universal in the following sense: the language generated by Gu consists of
strings of the form, say, w#code(M), such that w ∈L(M). (We can call
the language {w#code(M) | w ∈L(M)} itself universal, and thus any gram-
mar generating this language is universal.) A “more grammatical” notion of
universality can be the following.
A triple G = (N, T, P), where the components N, T, P are as in a usual
Chomsky grammar, is called a grammar scheme. For a string w ∈(N ∪T)∗
we deﬁne the language L(G, w) = {x ∈T ∗| w =⇒∗x}, the derivation being
performed according to the productions in P.
A universal type-0 grammar is a grammar scheme Gu = (Nu, Tu, Pu),
where Nu, Tu are disjoint alphabets, and Pu is a ﬁnite set of rewriting
rules over Nu ∪Tu, with the property that for any type-0 grammar G =
(N, Tu, S, P) there is a string w(G) such that L(Gu, w(G)) = L(G).
Therefore, the universal grammar simulates any given grammar, provided
that a code w(G) of the given grammar is taken as a starting string of the
universal one.
There are universal type-0 grammars in the sense speciﬁed above. The
reader can ﬁnd a complete construction in [54] (it is also presented in [52]
and [224]). That universal grammar codiﬁes in terms of type-0 grammars the
derivation process used by a grammar: choose a rule, remove an occurrence
of its left hand member and introduce instead of it an occurrence of its right
hand member, check whether or not a terminal string is obtained.
A natural question here, also important for molecular computing, is
whether or not universality results hold also for other classes of automata
and grammars than Turing machines and type-0 grammars, in particular for
ﬁnite automata.
If the question is understood in the strict sense, then the answer is neg-
ative for ﬁnite automata: no ﬁnite automaton can be universal for the class
of all ﬁnite automata. The main reason is the fact that one cannot encode
the way of using a ﬁnite automaton in terms of a ﬁnite automaton (we have
to remember symbols in the input string without marking them, and this

Complexity
19
cannot be done with a ﬁnite set of states).
However, universal ﬁnite automata in a certain restricted sense can be
found; in some extent, this is a matter of deﬁnition of universality.
For
instance, in [56] one proves that given a ﬁnite set of ﬁnite automata, in a
certain completion of this set one can ﬁnd an automaton which is universal
for the considered set. Similarly, a universal ﬁnite automaton is constructed
in [224] for the ﬁnite set of automata with a bounded number of states,
providing that the starting string (containing both the string to be recognized
and the code of a particular ﬁnite automaton) is of a complex form (of a non-
context-free type). We do not enter here into details, because, although the
topic is of a central interest for our book, the solutions in [56] and [224] are
not “good enough” from a practical point of view (they are too particular).
1.9
Complexity
Let M be a Turing machine. For each input of length n, if M makes at most
t(n) moves before it stops, then we say that M runs in time t or M has time
complexity t. If M uses at most s(n) tape cells in the above computation,
then we say that M uses s space, or has space complexity s. Accordingly, we
can deﬁne the following classes of languages:
• the class of languages accepted by deterministic Turing machines in
time O(t),2 DTIME[t],
• the class of languages accepted by non-deterministic Turing machines
in time O(t), NTIME[t];
• the class of languages accepted by deterministic Turing machines in
space O(t), DSPACE[t];
• the class of languages accepted by non-deterministic Turing machines
in space O(t), NSPACE[t];
• the class of languages accepted by deterministic Turing machines in
polynomial time, P = ∪c DTIME(nc);
• the class of languages accepted by non-deterministic Turing machines
in polynomial time, NP = ∪c NTIME(nc);
• the class of languages accepted by deterministic Turing machines in
polynomial space, PSPACE = ∪c DSPACE(nc);
• the class of languages accepted by non-deterministic Turing machines
in polynomial space, NPSPACE = ∪c NTIME(nc).
2For two real-valued functions f, g deﬁned on non-negative integers, f(n) = O(g(n)) if
there exist two constants c, N > 0 such that |f(n)| ≤c · |g(n)|, for all n ≥N.

20
Prerequisites
The following relations hold true:
P ⊆NP ⊆PSPACE = NPSPACE.
An important open problem is to check which of the above inclusions is
proper.
Let’s come back to the notion of non-deterministic Turing machine M =
(K, V, T, B, s0, F, δ). Without losing generality we will assume that for every
s ∈K, a ∈V , card(δ(s, a)) ≤2 and for every input x the machine M executes
the same number of steps. In this way, the set of all possible computations of
M on an input x can be described by a binary tree: the nodes of the tree are
conﬁgurations of the machine, the root is the initial conﬁguration, and for
every node c, its children are those conﬁgurations which can be reached from
c in one move according to the transition function of the machine. Leaves
of the tree are ﬁnal conﬁgurations, some of which may accept, others reject.
An accepting computation is a path starting at the root and ﬁnishing in an
accepting leaf. According to the deﬁnition of acceptance, an input is accepted
if and only if there is at least one accepting leaf in its tree of computation.
At each internal node, the selection of the continuation of the path is done in
a non-deterministic manner. According to our convention, the computation
tree on every input which is accepted is ﬁnite.
Here is a simple example of a non-deterministic algorithm solving the
satisﬁability problem SAT: given a Boolean formula with n variables, decide
whether F is satisﬁable (i.e., there is an assignment mapping variables to
{0, 1} which satisﬁes F). We will assume a ﬁxed encoding of Boolean formu-
las, e(F).
input e(F)
check that e(F) encodes a correct Boolean formula
for each variable x occurring in F do
choose in a non-deterministic manner
F = F|x=0 or
F = F|x=1
simplify the resulting formula without variables
if the result is 1, then accept and halt
end
In contrast with a deterministic algorithm for solving SAT, that might
need to explore all possible assignments, the non-deterministic algorithm has
to guess “correctly” just one assignment (guessing the correct solution) and
then to verify its correctness. So, SAT is in NP, but it is not known whether
SAT is or is not in P. In fact SAT has a central position: if SAT would be in
P, then P = NP.

Bibliographical Notes
21
1.10
Bibliographical Notes
There are many monographs in automata, formal language theory and com-
plexity theory. Several titles can be found in the bibliographical list which
concludes the book: [4], [11], [69], [75], [113], [123], [198], [256], [258], [259],
[260], [273]. We recommend them to the reader interested in further details.
Results similar to Theorem [207] were given in [207], [255], [299].
Characterizations of computably enumerable languages starting from
equality sets of morphisms were given in [261], [72]; complete proofs can be
found in [262]. The variant of Theorem 1.4 given in Theorem 1.5 is proved
in [149].
Universal Turing machines can be found in [194], [267], and, mainly, in
[252].

22
Prerequisites

Chapter 2
DNA Computing
The aim of this chapter is to have a glimpse in the fast emerging area of DNA
Computing. We start by brieﬂy presenting the structure of DNA molecule
and the operations which can be carried (mainly in vitro) on it, then we
present a series of experiments already done in this area (starting with the
famous Adleman’s experiment) or only proposed to be done, we derive from
this practical approach a series of theoretical developments, and we end with
an overview of the most developed chapter of DNA Computing, the theory
of splicing systems.
2.1
The Structure of DNA
The main data structure we work with is the double stranded sequence, with
the paired elements related by a complementarity relation. After introducing
this data structure, we discuss its intrinsic (computational) power, by relat-
ing it to computably enumerable languages via a characterization of CE by
means of the so-called twin-shuﬄe languages. We anticipate the very impor-
tant conclusion of this connection: the DNA (structure) has a sort of in-built
computational completeness, it is a “blue print” of computability at the level
of Turing machines.
Our approach to the structure of DNA is directed to our goals, so we
consider the DNA molecule in a rather simpliﬁed manner, ignoring a lot of
biochemical information not necessary for the subsequent sections. We follow
the style of Chapter 1 of [224], where more details can be found.
DNA is an abbreviation for Deoxyribonucleic Acid. A DNA molecule is
a polymer consisting of two sequences of monomers, which are also called
deoxyribonucleotides, in short, nucleotides. Each nucleotide consists of three
components: a sugar, a phosphate group, and a nitrogenous base.
The sugar has ﬁve carbon atoms, numbered from 1′ to 5′. The phosphate
group is attached to the 5′ carbon, and the base is attached to the 1′ carbon.
23

24
DNA Computing
To the 3′ carbon there is attached a hydroxyl (OH) group. We have men-
tioned these biochemical technicalities because they are important in giving
a directionality to the two strands of the DNA molecule.
Diﬀerent nucleotides diﬀer only by their bases, which are of two types:
purines and pyrimidines. There are two purines: adenine and guanine, ab-
breviated A and G, and two pyrimidines: cytosine and thymine, abbreviated
C and T. Since the nucleotides diﬀer only by their bases, they are simply
referred to as A, G, C, and T.
We are not interested here in the structure of a nucleotide, but in the way
the nucleotides are linked together. This induces the very structure of DNA
molecules.
Any two nucleotides can link together by a covalent bond; thus, several
nucleotides can join and form a sequence in the same way as several sym-
bols form a string.
As for symbols in a string, there is no restriction on
the nucleotides in the sequence. We say that we get a single stranded se-
quence, in order to distinguish the obtained polymer from that obtained by
binding together two such sequences by means of hydrogen bonds between
corresponding nucleotides.
There are three fundamental facts in building double stranded sequences:
• The hydrogen bonds can be established only between A and T, as well
as between C and G. One says that the nucleotides in the pairs (A, T)
and (C, G) are complementary.
• The single stranded sequences of nucleotides have a directionality, given
by the carbons used by the covalent bonds.
The ﬁrst and the last
nucleotide establish only one such bond, hence a place for a further
bond is available in these positions. Because these places refer to the
3′ and the 5′ carbons, one end of the sequence is marked with 3′ and
the other one by 5′.
• The hydrogen bonds, based on complementarity, can bring together
single stranded sequences of nucleotides only if they are of opposite
directionality.
The above mentioned complementarity is called the Watson–Crick com-
plementarity, after James D. Watson and Francis H. C. Crick who discovered
the double helix structure of DNA in 1953, and won the Nobel Prize for that.
5′–
A
C
C
T
G
T
A
T
G
C
–3′
3′–
T
G
G
A
C
A
T
A
C
G
–5′
Figure 2.1: Example of a DNA molecule.
Following a standard convention in biochemistry, when we will represent
a double stranded molecule, we will draw the two strands one over the other,

The Structure of DNA
25
with the upper strand oriented from left to right in the 5′–3′ direction and
the lower strand oriented in the opposite direction. Figure 2.1 presents a
DNA molecule composed of ten pairs of nucleotides. The reader is urged to
notice the two important restrictions which work here: the complementarity
of the paired nucleotides and the opposite directionality of the two strands.
These two features are crucial for all our investigations and they induce the
computability power of DNA.
Of course, representing a double stranded DNA molecule as two linear
strands bound together by Watson–Crick complementarity is a major sim-
pliﬁcation of reality. First, in a DNA molecule the two strands are wound
around each other to form the famous double helix – see Figure 2.2. More-
over, the DNA molecule is folded in an intricate way in order to ﬁnd room
in the limited space of the cell. For the purposes of this book, the spatial
structure of DNA is not important, so we ignore it and we assume that a
DNA molecule is a double string-like structure as that in Figure 2.1.
Figure 2.2: The double helix.
There also are many cases where the DNA molecule is not a linear, but a
circular one (this happens for many bacteria).
We have called above “molecules” both single stranded and double
stranded sequences of nucleotides. We will also work with “incomplete dou-
ble stranded” molecules, of the type suggested in Figure 2.3. One sees that
four nucleotides in the left end and ﬁve in the right end are not paired with
nucleotides from the opposite strand. We say that the molecule in this ﬁgure
has sticky ends.
We close this section by pointing out again that we work here in an
idealized world, governed by clear and reliable rules, error-free, completely
predictable. This is far from reality, especially from what happens in vivo,

26
DNA Computing
A
C
C
T
G
G
T
T
A
A
C
C
A
A
T
T
A
T
A
C
G
Figure 2.3: A DNA molecule with sticky ends.
where there are many exceptions, non-determinism, probabilistic behaviours,
mismatches.
For instance, there also are other nucleotides, less frequent
and less important than A, C, G, T, while these four basic nucleotides are
sometimes paired in a wrong way. We disregard such “real life errors”, on
the one hand, because we develop here computability models rather than
computer designs, on the other one, because there are biochemical techniques
for isolating the errors, diminishing their ratio in the whole amount of DNA
molecules we work with. Moreover, the progresses in the DNA technology
are so fast that it is wiser to work in a plausible framework rather than at
the present-day level of the technology.
2.2
Complementarity Induces Computational
Completeness
We now step back to a formal language theory framework in order to point out
a rather surprising connection between the structure of the DNA molecule
and computability (at the level of Turing machines).
This connection is
established via the following consequence of Theorem 1.4.
Consider an alphabet V and its barred variant, V = {¯a | a ∈V }. The
language
TSV =

x∈V ∗
(x ⊔⊥¯x)
is called the twin-shuﬄe language over V . (For a string x ∈V ∗, ¯x denotes
the string obtained by replacing each symbol in x with its barred variant.)
For the morphism h : (V ∪V )∗−→V ∗deﬁned by
h(a) = λ, for a ∈V,
h(¯a) = a, for a ∈V,
we clearly have the equality TSV = EQ(h, prV ). In view of Theorem 1.4,
this makes the following result plausible.
Theorem 2.1 Each computably enumerable language L ⊆T ∗can be written
in the form L = prT (TSV ∩R), where V is an alphabet and R is a regular
language.
In this representation, the language TSV depends on the language L. This
can be avoided in the following way. Take a coding, f : V −→{0, 1}∗, for

Complementarity Induces Completeness
27
instance, f(ai) = 01i0, where ai is the ith symbol of V in a speciﬁed ordering.
The language f(R) is regular. A gsm can simulate the intersection with a
regular language, the projection prT , as well as the decoding of elements in
f(TSV ). Thus we obtain
Corollary 2.1 For each computably enumerable language L there is a gsm
gL such that L = gL(TS{0,1}).
Therefore, each computably enumerable language can be obtained by a
sequential transducer starting from the unique language TS{0,1}. One can
also prove that this transducer can be a deterministic one.
We stress the fact that the language TS{0,1} is unique, the same for all
computably enumerable languages, while the (deterministic) gsm in this rep-
resentation depends on the particular language. We may say that all com-
putably enumerable languages are “hidden” in TS{0,1} and in order to recover
them we only need a gsm, the simplest type of a transducer, nothing else than
a ﬁnite automaton with outputs; one reading of strings in TS{0,1}, from left
to right, with the control ensured by a ﬁnite set of states, suﬃces. Otherwise
stated, the twin-shuﬄe language plus a ﬁnite state sequential transducer are
equal in power to a Turing machine. It is clear that the “main” computability
capacity lies in the twin-shuﬄe language.
The technically surprising fact here is the simplicity and the uniqueness
of the twin-shuﬄe language. Still more surprising – and signiﬁcant from the
DNA computing viewpoint – is the connection between TS{0,1} and DNA.
Let us start from the simple observation that we have four “letters” both
in the case of DNA and in the case of the twin-shuﬄe language over {0, 1}.
Let us consider a correspondence between these “letters”, taking care of the
complementarity: a barred letter is the complement of its non-barred variant.
For instance, consider the pairing
A = 0, G = 1, T = ¯0, C = ¯1.
Thus, the letters in the pairs (0, ¯0) and (1, ¯1) are complementary and this
complementarity corresponds to the Watson–Crick complementarity of the
nucleotides associated with the symbols 0, 1, ¯0, ¯1.
Consider now a double stranded molecule, for instance, that in Figure
2.1,
ACCTGTATGC
TGGACATACG
and let us imagine the following scenario: two clever “insects” are placed
at the left end of the two strands of this molecule and asked to “read” the
strands, from left to right, step by step, with the same speed, telling us what
nucleotide they meet according to the codiﬁcation speciﬁed above (0 for A,
1 for G, etc.). It is clear that what we obtain is a string of symbols 0, 1, ¯0, ¯1
which is the letter by letter shuﬄe of the strings associated with the two
strands of the molecule.

28
DNA Computing
For simplicity, let us ﬁrst rewrite the letters according to the association
indicated:
0¯1¯1¯01¯00¯01¯1
¯0110¯10¯00¯11
Then, the string obtained as suggested above is
0¯0¯11¯11¯001¯1¯000¯0¯001¯1¯11
This is a string in TS{0,1}. Other readings of the molecule also lead to a string
in this language; this is the case, for instance, of reading ﬁrst completely the
upper strand and then the lower one.
However, not all strings in TS{0,1} can be obtained in this way: the string
00¯01¯0¯1 is clearly in TS{0,1}, but it cannot be produced by the step by step
up-down reading (it starts with two copies of 0), or by completely reading one
strand and then the other (the two halves of the string are not complementary
to each other).
Thus, with the codiﬁcation of the nucleotides considered above, for each
DNA molecule we can ﬁnd a string in TS{0,1} which corresponds to a reading
of the molecule, but the converse is not true. This is mainly explained by the
fact that the reading of the molecule is too rigid. Unfortunately, if the two
“insects” are left to move with uncorrelated speeds along the two strands,
then we can get strings which are not in TS{0,1}. The reader is advised to
try with the previous example.
There are other codiﬁcations of the nucleotides, also taking into account
their places with respect to the two strands, which lead to characterizations
of the language TS{0,1}. For instance, consider the encoding suggested below:
upper strand
lower strand
A, T
0
¯0
C, G
1
¯1
In other words, both nucleotides A and T are identiﬁed with 0, without
a bar when appearing in the upper strand and barred when appearing in
the lower strand; the nucleotides C, G are identiﬁed with 1 in the upper
strand and with ¯1 in the lower strand. Now, given a DNA (double-stranded)
molecule, by reading the two strands from left to right, with non-deterministic
non-correlated speeds in the two strands, we get a string in TS{0,1}: because
of the complementarity, if we ignore the bars, then the two strands are de-
scribed by the same sequence of symbols 0 and 1.
The reader might try with the molecule considered above, imposing no
restriction on the speeds of the two “insects”.
Moreover, and this is very important, in this way we can obtain all strings
in TS{0,1}: just consider all molecules (complete double stranded sequences)
and all possibilities to read them as speciﬁed above.
The same result is
obtained if we use molecules containing in the upper strand only nucleotides
in any of the pairs

Complementarity Induces Completeness
29
(A, C), (A, G), (T, C), (T, G).
Consequently, we may metaphorically write
DNA ≡TS{0,1},
so, also metaphorically, we can replace the twin-shuﬄe language in Corollary
2.1 with DNA.
Thus, we may say that all possible computations are encoded in the DNA
and we can recover them by means of a deterministic ﬁnite state sequential
transducer – and two “clever insects” as used above.
The reader is requested to accept the “newspaper style” of this discussion,
without losing the signiﬁcance of the statement (and the precise mathematical
meaning of it). Of course, we dare to say nothing about how realistic this
discussion is if we think of implementing a “DNA computer” based on these
observations.
In particular, there is here a point where the previous discussion is vul-
nerable: the two “insects” are both placed in the left end of the two strands
of a molecule, although we know (see again Figure 2.1) that the two strands
are oriented in opposite directions. Let us take into account this opposite
directionality and place the “insects”, say, in the 5′ ends of the two strands.
Pleasantly enough, the conclusion will be the same, because of the robust-
ness of the characterization of computably enumerable languages by means
of twin-shuﬄe languages.
More speciﬁcally, the result in Corollary 2.1 is true also for a “mirror”
variant of the twin-shuﬄe language.
For an alphabet V , consider the language
RTSV =

x∈V ∗
(x ⊔⊥mi(¯x)).
This is the reverse twin-shuﬄe language associated to V .
Theorem 2.2 For each computably enumerable language L there is a deter-
ministic gsm gL such that L = gL(RTS{0,1}).
Clearly, reading the upper strand of a molecule from left to right and the
lower one from right to left, with non-deterministic uncorrelated speeds, with
the codiﬁcation of nucleotides as used above, we get strings in RTS{0,1}; all
such strings can be obtained by using all molecules and all possible readings.
Again, we can identify the language RTS{0,1} with DNA and claim that it
characterizes CE modulo a deterministic gsm and a certain codiﬁcation of
nucleotides.
We will make use of the twin-shuﬄe characterizations of computably enu-
merable languages and of the connection with the DNA structure also in
subsequent sections.

30
DNA Computing
We close this section by emphasizing that we have worked here with com-
plete DNA molecules, without sticky ends, and that we have mainly used
the structure of the DNA molecule. This suggests to consider a general data
structure with similar properties: double stranded sequences, composed of
symbols from an arbitrary alphabet, subject to a complementarity relation
(which should be at least symmetric).
2.3
Operations on DNA Molecules
We return to the biochemistry of DNA, looking for operations possible with
molecules. Of course, we will consider these operations in a simpliﬁed manner,
in terms of the data structure we have speciﬁed in Section 2.1 and found to
be so useful/powerful in Section 2.2. More biochemical details can be again
found in [224] and in the bibliographical sources we indicate at the end of
this chapter.
Separating DNA strands. Because the hydrogen bonds between com-
plementary nucleotides are much weaker than the covalent bonds between
nucleotides adjacent in the two strands, we can separate the two strands
without breaking the single strands. This is easily done by heating the DNA
solution (at about 85◦–95◦C) until the two strands come apart. This oper-
ation is called denaturation.
Binding together DNA strands. The operation opposite to denatu-
ration is also possible: if a DNA solution of a high temperature (as above, of
85◦–95◦C) is cooled down, then the separated strands fuse again by hydro-
gen bonds, and we get double stranded molecules. This operation is called
renaturation, or annealing.
Of course, when the single strands in the solution do not match com-
pletely, then by annealing we will obtain DNA molecules with sticky ends;
Figure 2.4 presents such a situation.
5′ −ACCTAGCGC −3′
3′ −TCGCGTTA −5′
⇓
ACCTAGCGC
TCGCGTTA
Figure 2.4: Annealing of partially matching strands.
Many operations on DNA molecules can be mediated by enzymes, which
are proteins catalysing various reactions where DNA is involved.

Operations on DNA Molecules
31
Filling in incomplete strands. A molecule as that in Figure 2.4 can
be “completed” to a molecule without sticky ends by using the enzymes
called polymerases. These enzymes are able to add nucleotides, in the 5′–3′
direction, until pairing each nucleotide with its Watson–Crick complement.
There are two important details here: the overhanging strand acts as
a template (it precisely speciﬁes the nucleotides to be added, by means of
the complementarity restriction) and there already should exist a sequence
which is bonded to the template; the addition of nucleotides starts from this
sequence and proceeds step by step in the 5′–3′ direction.
This sequence
is called a primer. Consequently, the polymerase extends repeatedly the 3′
end of the “shorter strand” (starting from the primer), complementing the
sequence on the template strand. (Of course, the required nucleotides should
be available in the solution where the reaction takes place.) This is called a
polymerase chain reaction, in short, PCR.
All polymerases require a primer in order to start adding nucleotides, but
there are some polymerases that will extend a DNA molecule without using
a prescribed template.
Synthesizing DNA molecules. If we need a speciﬁc double stranded
molecule for which we already have one strand (a template), then we can
obtain it by priming the given strand and then using a polymerase to extend
the primer according to the template (in the 5′–3′ direction).
One can also synthesize single stranded molecules following a prescribed
sequence of nucleotides.
This synthesis adds nucleotide by nucleotide to
the already synthesized chain in the 3′–5′ direction. The procedure is well-
controlled and there already are many “synthesizing robots”. We may con-
clude that “writing” on DNA molecules is already an easy operation (from a
technological point of view).
Short chemically synthesized single stranded molecules are called oligonu-
cleotides or simply oligos. They are very useful in genetic engineering, as well
as in DNA computing (for instance, they are used as primers).
Shortening DNA molecules. There are certain enzymes, called DNA
nucleases, that degrade DNA, removing nucleotides from the two strands.
There are two classes of nucleases, exonucleases and endonucleases.
Exonucleases shorten DNA by removing nucleotides from the ends of the
DNA molecule.
Some of them remove nucleotides from the 5′ end while
others will do this from the 3′ end. Some exonucleases may be speciﬁc for
single stranded molecules while others will be speciﬁc for double stranded
ones (and some can degrade both).
Figure 2.5 shows the action of a 3′-nuclease (it degrades strands in the
3′–5′ direction). In this way a molecule is obtained with overhanging 5′ ends.
Because the removing of nucleotides from the two ends takes (almost) the
same time, the operation is synchronized, the two sticky ends have (almost)
the same length.

32
DNA Computing
5′ −ACCGTCAAGTG −3′
3′ −TGGCAGTTCAC −5′
⇓
ACCGTCAAGT
GGCAGTTCAC
⇓
ACCGTCAAG
GCAGTTCAC
⇓
ACCGTCAA
CAGTTCAC
Figure 2.5: An exonuclease in action.
Other exonucleases are cutting pairs of nucleotides from the ends of a
molecule, thus synchronously shortening the whole molecule from the two
ends (and leaving blunt ends).
Cutting DNA molecules.
Endonucleases are able to cut DNA
molecules (by destroying the covalent bonds between adjacent nucleotides).
Some of them cut only single strands, others can cut double strands; some
endonucleases cut DNA molecules at any place, others are site restricted, they
“recognize” a certain pattern in the molecule and cut at a speciﬁed place (in
general, inside this pattern).
We are not particularly interested in cutting molecules at random, while
the enzymes which can cut at speciﬁc sites are crucial for DNA computing,
so we will insist on them, recalling some examples from [224].
Endonucleases which cut at speciﬁc sites are called restriction endonucle-
ases or restriction enzymes.
The cut itself can be blunt (straight through both strands) or staggered,
leaving sticky ends.
3′−N N N C T T A A −5′
5′−N N N G −3′
3′−G N N N −5′
5′−A A T T C N N N −3′
❄❄
Eco RI
3′−N N N C T T A A G N N N −5′
5′−N N N G A A T T C N N N −3′
Figure 2.6: Eco RI in action.

Operations on DNA Molecules
33
Figure 2.6 presents the action of the restriction enzyme EcoRI; the sub-
molecules ﬁlled in with N are arbitrary (of course, the pairs in the two strands
are complementary), what counts is the recognition site, 5′–GAATTC, where
EcoRI will bind. The directionality is very important: EcoRI will not bind
to 3′-GAATTC. The cut is staggered, leaving two overhanging 5′ ends.
Note that the recognition site is a palindrome in the sense that reading
one of the strands in the 5′–3′ direction one gets the same result (GAATTC)
as reading the other strand in the 5′–3′ direction. This is often the case for
restriction enzymes.
Of course, if a molecule of DNA contains several recognition sites, then
the restriction enzyme will in principle cut all of them.
Figure 2.7 presents the case of a restriction endonuclease (SmaI) which
produces blunt ends. The recognition site is 5′–CCCGGG.
3′−N N N G G G −5′
5′−N N N C C C −3′
5′−G G G N N N −3′
3′−C C C N N N −5′
❄❄
Sma I
3′−N N N G G G C C C N N N −5′
5′−N N N C C C G G G N N N −3′
Figure 2.7: Sma I in action.
There are many enzymes which have the same restriction sites and pro-
duce identical sticky ends. There also are enzymes which produce 3′ sticky
ends, as well as enzymes which cut outside the restriction site.
3′−N N N G G G C C C N N N −5′
5′−N N N C C C G G G N N N −3′
❄❄
3′−N N N G G G C C
5′−N N N C
C C G G G N N N −3′
C N N N −5′
Figure 2.8: A ligation operation.
We emphasize the fact that we can cut DNA molecules at fairly precise

34
DNA Computing
places and producing sticky ends known in advance, and that the enzymes
are catalysts, they are not consumed during the operation. Thus, if we wait
long enough, then at least in principle all sites where a restriction enzyme
can cut will be used.
Linking DNA molecules. Fragments of molecules having complemen-
tary sticky ends can be linked together, by an operation called ligation (medi-
ated by a class of enzymes called ligases). Figure 2.8 presents such a situation.
Thus, by restriction enzymes we can cut DNA molecules and produce
sticky ends, while by ligation we can paste together the obtained fragments,
possibly obtaining new molecules, by the recombination of fragments. To-
gether with denaturation and annealing, this way of manipulating DNA
molecules is one of the basic operations which can be used in DNA com-
puting.
Also possible, but not so useful, is the ligation of blunt ends; in that
case the linking of fragments is done without dependence on the nucleotides
placed in the two ends.
Inserting or deleting short subsequences. Such operations can be
performed by using the annealing of partially matching strands. Schemati-
cally, this can be done as follows. Assume that we want to insert a sequence
γ in between the sequences α, β of a longer molecule. For any molecule π,
denote by πu the upper strand and by πd the lower strand of π. We ﬁrst
denaturate the DNA molecules, thus producing single stranded sequences
containing either subsequences αuβu or subsequences αdβd. The strands of
the latter form are removed from the solution, then we add sequences αdγdβd
and decrease the temperature. The complementary sequences αu, βu will an-
neal to αd, βd, while the sequence γd will be folded. By a polymerase chain
reaction we complete the molecules to double stranded sequences with a sin-
gle stranded loop γd (a primer zd is also needed, see step (b) in Figure 2.9).
We melt again the DNA and we use once more the PCR technique for ob-
taining double stranded molecules; they have the subsequence γ inserted in
the context α, β, as desired.
In a similar way one can delete (or even substitute) a subsequence from
a DNA molecule.
Multiplying DNA molecules.
One of the main attractive features
of DNA from the point of view of computability is the parallelism: in a
small space one can have a huge number of molecules. Very important in
this context is the fact that we can also have a huge number of identical
molecules. This can be achieved by a technique called ampliﬁcation, which
is performed by means of a Polymerase Chain Reaction.
The ampliﬁcation by the PCR technique is very simple and very eﬃcient:
in n steps it can produce 2n copies of the same molecule. The operation is
done in cycles consisting of three phases: denaturation, priming, extension.
Let us suppose that we have a molecule α which we want to amplify. We

Operations on DNA Molecules
35
ﬁrst heat the solution in such a way that all copies of α are denatured, the
upper strand of α – denoted as above by αu – is separated from the lower
strand, αd. We add primers to the solution, short sequences of nucleotides
which are complementary to the left end of αu and to the right end of αd.
By cooling down the temperature, these primers will anneal to the single
strands αu and αd (at their 5′ ends). By adding a polymerase (and a large
enough amount of free nucleotides), both αu and αd will act as templates,
they will be completed with complementary nucleotides, starting from the
primers, until obtaining double stranded molecules. In this way, the number
of copies of the molecule α is doubled. The process is then repeated.
(a)
(b)
(c)
(d)
5′
5′
3′
xu
αu
βu
yu
zu
3′
5′
βd
γd
αd
3′
xu
αu
βu
yu
zu
3′
αd
βd
zd
5′
3′
xu
αu
βu
yu
zu
zd
yd
βd
αd
xd
γd
γd
xd
αd
γd
βd
yd
zd
5′
Figure 2.9: Insertion by mismatching annealing.
There are many technical details here which are not important for us. We
emphasize only the eﬃciency of the operation, and the fact that by means
of the primers we can select the molecules which are ampliﬁed. This is very
useful for controlling the errors. Assume that we know that in a solution
we have approximately the same number of molecules of type α and of type
β and that molecules of type β represent errors. We can increase the ratio
of molecules of type α by amplifying only them (using primers which attach
to single strands of α and not to single strands of β). In this way, we can
increase as much as we want the number of molecules of type α, without
modifying the number of molecules of type β.
Filtering a solution. The separation of molecules of type α from those

36
DNA Computing
of type β, as discussed above, can also be done in other ways. If we know
that molecules of type β contain a pattern where a given restriction enzyme
can act, then we can cut them and after that we can ﬁlter the solution
by making use of the sticky ends obtained in this way. The operation can
also be done for single stranded molecules which contain a known pattern.
Principally, the process is very simple: we attach to a solid support strands
which are complementary to the known patterns and we pour the solution
over this support; the strands complementary to those bound to the support
will remain there, the others will be collected separately.
This technique can also be used for separating from a given solution the
set of single stranded molecules which contain a speciﬁed subsequence.
Separation of molecules by length. It is also possible to separate
the molecules present in a solution according to their length, by a technique
called gel electrophoresis. In fact, the molecules are separated according to
their weight, which is almost proportional to their length. In several wells
performed at one end of a rectangular sheet of gel, one pours a small amount
of DNA, then one applies an electrical ﬁeld. Because the DNA molecules are
negatively charged, they will move toward the positive pole, with a speed
which depends on their weight (and the gel porosity). In this way, longer
molecules will remain behind the shorter ones and we can both check the
existence of molecules of a given length (by comparing the trace of the un-
known molecules with that of a probe which is run in parallel), as well as
separate the molecules according to their length.
Gel electrophoresis (that is, separation by length) is one of the ways of
“reading” the result of a DNA computation. The procedure is rather precise,
even molecules which diﬀer by one nucleotide only can be distinguished from
each other.
Reading DNA molecules. Of course, separation by length cannot tell
us the precise content of molecules, so there are problems where this technique
is not suﬃcient. Fortunately, there also are techniques able to read a DNA
molecule nucleotide by nucleotide.
We do not present here the details of
such a procedure (Chapter 1 of [224] can be consulted in this aim), but we
only mention that such procedures exist (and that, for the time being, they
are slower than the procedures for synthesizing molecules with a speciﬁed
contents; at this moment, it is easier to “write” on DNA than to “read” it).
2.4
Adleman’s Experiment
Most of the experiments in DNA computing reported so far are based on the
annealing operation. This is the case also with Adleman’s seminal experi-
ment. We start now to present several experiments of this type, then we will
discuss some theoretical or practical generalizations of them, leading to new

Adleman’s Experiment
37
computability models or to new computability strategies suggested by these
experiments.
Speculations about using DNA molecules as a support for computations
were already made some decades ago, e.g., by Ch. Bennett [23] and M. Con-
rad [63], but they were not followed by practical attempts to implement those
revolutionary ideas. The ﬁrst successful experiment of using DNA molecules
and techniques for computing was reported by L. M. Adleman in 1994 [1]:
a small instance of the Hamiltonian path problem in a directed graph was
solved by purely biochemical means. The number of the laboratory steps was
linear in terms of the size of the graph (the number of vertices), although
the problem itself is known to be NP-complete (hence intractable for usual
computers). Following the terminology of [124], this was a convincing demo,
which has proved that genetic engineering materials and techniques consti-
tute a possible new framework for computability. Adleman’s experiment is
important not only because it was the ﬁrst of this type, but also by the way it
was conducted (by the way it has made use of the structure of the DNA and
of the operations possible with DNA molecules). Thus, we will recall this his-
tory making experiment (skipping the biochemical details and emphasizing
the steps of the procedure).
The graph considered by Adleman was that in Figure 2.10. We have seven
vertices and fourteen arcs. The question is whether or not there is a path
from vertex 0 to vertex 6 which passes exactly once through each of the other
vertices. That is, we have a decidability problem, with a yes/no answer.
✒✑
✏
✒✑
✏
✒✑
✏
✒✑
✏
✒✑
✏
✒✑
✏
✒✑
✏




✒
✲
✲
✛
❅
❅
❅
❅
❅❅
❘
❅
❅
❅
❅
❅
❅
I
✡
✡
✡
✡✢
✘
✘
✘
✘
✘
✾





y
❏
❏
❏❏
❈
❈
❈
❈
❈
❈
❈
❈
❈
❈
❈❈❖
✄
✄
✄
✄
✄
✄
✄
✄
✄
✄
✄✄✗
❍❍❍❍❍❍❍❍❍❍❍❍
❥
❙
❙
❙
❙
❙
❙
❙
❙
❙
❙
❙o
0
1
2
3
4
5
6
Figure 2.10: The graph in Adleman’s experiment.
By a simple examination of the graph, we see that there is a path as de-
sired, namely that following the numbering of the vertices: 0, 1, 2, 3, 4, 5, 6.
However, we have mentioned that the problem is a hard one, among the hard-
est which can be solved in polynomial time by non-deterministic algorithms:

38
DNA Computing
it is an NP-complete problem.
Otherwise stated, all known deterministic
algorithms for solving this problem are essentially equally complex as the
exhaustive search. However, we can trade time for space, and this is exactly
what Adleman has done, making use of the massive parallelism of DNA (in
some sense, massive parallelism can simulate non-determinism, and in this
way non-deterministic algorithms can be implemented).
The algorithm used by Adleman was the following:
Input: A directed graph G with n vertices, among which there are
two designated vertices vin and vout.
Step 1: Generate paths in G randomly in large quantities.
Step 2: Remove all paths that do not begin with vin
or do not end in vout.
Step 3: Remove all paths that do not involve exactly n vertices.
Step 4: For each of the n vertices v, remove all paths that
do not involve v.
Output: “Yes” if any path remains, “No” otherwise.
It is easy to see that, providing that each of the operations in the algorithm
takes exactly one time unit, the solution is obtained in a number of time units
which is linear in n, the number of vertices in the graph: steps 1, 2, 3 need
a constant number of time units (say, 4: generate all paths, select the paths
starting with vin, select the paths ending with vout, select the paths of length
n), while step 4 takes n time units (check for each vertex its presence in the
currently non-rejected paths).
The main diﬃculty lies in step 1, where we have to generate a large
number of paths in the graph, as large as possible, in order to reach with a
high enough probability the Hamiltonian paths, if any. Of course, when the
graph also contains cycles, the set of paths is inﬁnite. In a graph without
cycles, the set of paths is ﬁnite, but it can be of an exponential cardinality
with respect to the number of vertices. This is the point where the massive
parallelism and the non-determinism of chemical reactions were cleverly used
by Adleman in such a way that this step was performed in a time practically
independent of the size of the graph.
The biochemical implementation of the above described algorithm was
the following.
Each vertex of the graph was encoded by a single stranded sequence of
nucleotides, namely of length 20. These codes were constructed at random;
the length 20 is enough in order to ensure that the codes are “suﬃciently
diﬀerent”. A huge number of these oligonucleotides (ampliﬁed by PCR) were
placed in a test tube. Then, in the same test tube are added (a huge number
of) codes of the graph edges, of the following form: if there is an edge from

Adleman’s Experiment
39
vertex i to vertex j and the codes of these vertices are si = uivi, sj = ujvj,
where ui, vi, uj, vj are sequences of length 10, then the edge i →j is encoded
by the Watson-Crick complement of the sequence viuj.
For instance, for the codes of vertices 2, 3, 4 speciﬁed below
s2 = 5′ −TATCGGATCGGTATATCCGA −3′,
s3 = 5′ −GCTATTCGAGCTTAAAGCTA −3′,
s4 = 5′ −GGCTAGGTACCAGCATGCTT −3′,
the edges 2 →3, 3 →2 and 3 →4 were encoded by
e2→3 = 3′ −CATATAGGCTCGATAAGCTC −5′,
e3→2 = 3′ −GAATTTCGATATAGCCTAGC −5′,
e3→4 = 3′ −GAATTTCGATCCGATCCATG −5′.
By annealing, the codes of the vertices act as splints in rapport with codes
of edges and longer molecules are obtained, encoding paths in the graph. The
reader can easily see how the molecules speciﬁed above will lead to sequences
encoding the paths 2 →3 →4, 3 →2 →3 →4, etc.
Adleman has let the process to proceed four hours, in order to be sure
that all ligation operations take place. What we obtain is a solution contain-
ing a lot of non-Hamiltonian paths (short paths, cycles, paths passing twice
through the same vertex). The rest of the procedure consists of checking
whether or not at least a molecule exists which encodes a Hamiltonian path
which starts in 0 and ends in 6.
There is here a very important point, which will be referred to later. The
diﬃcult step of the computation was carried out “automatically” by the DNA
molecules, making use of the parallelism and the Watson–Crick complemen-
tarity. In this way, we get a large set of candidate solutions, (hopefully) both
molecules which encode paths which are looked for, but also many molecules
which should be ﬁltered out, a sort of “garbage” to be rejected.
This second part of the procedure, of ﬁltering the result of step 1 in order
to see whether a solution to our problem exists, was carried out by Adleman in
about seven days of laboratory work, by performing the following operations:
By a PCR ampliﬁcation with primers representing the input and the output
vertices (0 and 6), only paths starting in 0 and ending in 6 were preserved.
After that, by gel electrophoresis there have been extracted the molecules of
the proper length: 140, because we have 7 vertices encoded by 20 nucleotides
each. Thus, at the end of step 3 we have a set of molecules which encode
paths in the graph which start in 0, end in 6, and pass through 7 vertices.
(It is worth noting that this does not ensure that such a path is Hamiltonian
in our graph: 0, 3, 2, 3, 4, 5, 6 is a path which visits seven vertices, but it
passes twice through 3 and never through 1.) Roughly speaking, step 4 is
performed by repeating for each vertex i the following operations: melt the

40
DNA Computing
result of step 3, add the complement of the code si of vertex i and leave to
anneal; remove all molecules which do not anneal.
If any molecule survives step 4, then it encodes a Hamiltonian path in
our graph, namely one which starts in vertex 0 and ends in vertex 6.
The practical details of this procedure are not very important for our
goals. They depend on the present day laboratory possibilities and can be
performed also by other techniques; furthermore, the algorithm itself can be
changed, improved or completely replaced by another one. What is important
here is the proof that such a computation is possible. Purely biochemical
means were used in order to solve a hard problem, actually an intractable
one, in a linear time as the number of lab operations. These operations, in
an abstract formulation, form another main output of this experiment and
of the thought about it, leading to a sort of a programming language based
on test tubes and DNA molecules manipulation.
Such a “test tube programming language” was proposed in [168], devel-
oped in [2] and then discussed in many places. In short, the framework is the
following.
By deﬁnition, a (test) tube is a multiset of words (ﬁnite strings) over the
alphabet {A, C, G, T}. The following basic operations are initially deﬁned
for tubes, that is, multisets of DNA single strands [2]. However, appropriate
modiﬁcations of them will be applied for DNA double strands as well.
Merge. Given tubes N1 and N2, form their union N1 ∪N2 (understood
as a multiset).
Amplify. Given a tube N, produce two copies of it, N1 and N2.
Detect. Given a tube N, return true if N contains at least one DNA
strand, otherwise return false.
Separate (or Extract). Given a tube N and a word w over the alphabet
{A, C, G, T}, produce two tubes +(N, w) and −(N, w), where +(N, w)
consists of all strands in N which contain w as a (consecutive) substring
and, similarly, −(N, w) consists of all strands in N which do not contain
w as a substring.
The four operations of merge, amplify, detect, and separate allow us
to program simple questions concerning the occurrence and non-occurrence
of subwords. For instance, the following program extracts from a given test
tube all strands containing at least one of the purines A and G, preserving
at the same time the multiplicity of such strands:
(1)
input(N)
(2)
amplify(N) to produce N1 and N2
(3)
NA ←+(N1, A)
(4)
NG ←+(N2, G)

Adleman’s Experiment
41
(5)
N ′
G ←−(NG, A)
(6)
merge(NA, N ′
G)
Besides the four operations listed above, Adleman’s experiment makes
use of the following modiﬁcations of the operation separate:
Length-separate.
Given a tube N and an integer n, produce the
tube (N, ≤n) consisting of all strands in N with the length less than
or equal to n.
Position-separate. Given a tube N and a word w, produce the tube
B(N, w) (resp. E(N, w)) consisting of all strands in N which begin
(resp. end) with the word w.
Using them, we can describe the ﬁltering procedure in Adleman’s exper-
iment (steps 2, 3, 4) as follows. We start with the input tube N, consisting
of the result of step 1, in the form of single stranded molecules (obtained by
melting the result of step 1). The program is the following:
(1)
input(N)
(2)
N ←B(N, s0)
(3)
N ←E(N, s6)
(4)
N ←(N, ≤140)
(5)
for i = 1 to 5 do begin N ←+(N, si) end
(6)
detect(N).
The operations merge, amplify, detect, separate will be used also in
the subsequent sections. We do not persist here in further discussing these
operations, but we point out one of the main weakness of Adleman’s proce-
dure, from a practical point of view: the number of necessary single strands,
codes of vertices or of edges, is of the order of n!, where n is the number
of vertices in the graph. This imposes drastic limitations on the size of the
problems which can be solved in this manner. (J. Hartmanis has computed
[125] that in order to handle in this manner graphs with 200 nodes, a size of
a practical importance and easily handled by electronic computers, we need
3 · 1025 Kg of DNA, which is more than the weight of the Earth!) In short,
Adleman’s procedure is elegant, copes in a nice way with the errors, but it
cannot be scaled-up.
The question of increasing the size of the graphs for which we can solve
the Hamiltonian path problem was approached by several authors. A general
strategy is to diminish the set of candidate solutions one has to generate by
a basic step where the parallelism of DNA is essentially used. This leads to
the idea of evolutionary computing, based on a repeated improvement of the
current set of candidate solutions, similar to the style of genetic algorithms,
where only “individuals” with a high ﬁtness survive each iteration. Such an
approach to our problem can be found in [15].

42
DNA Computing
Another approach to the Hamiltonian path problem was proposed by T.
Head [128]. It starts from the observation that single stranded DNA cannot
be used for large graphs, because of the “bad behaviour” of long sequences
of nucleotides (they are fragile, can selfanneal, etc.). Head’s procedure still
has two phases, one when candidate solutions are generated and one when
non-solutions are eliminated, but the ﬁrst phase is carried out in a way that
ensures the fact that the candidate solutions does not grow too much. More
speciﬁcally, one takes care that only molecules encoding paths of a length
less than or equal to the length of the Hamiltonian paths are produced. To
this aim, a number of steps which is proportional to the number of vertices
is performed. The ﬁltering phase is similar to that in Adleman’s procedure;
thus, in total, the number of biochemical steps is again linear with respect
to the size of the graph.
The key tool used in Head’s algorithm is the restriction enzyme Dra III,
which recognizes a pattern of the form
CACNNNGTG
GTGNNNCAC
and cuts the molecule such that the following sticky ends are produced:
CACNNN
GTG
GTG
NNNCAC
Here, N denotes any nucleotide (of course, the upper nucleotides should be
complementary to the lower ones), therefore we can have 43 = 64 possibilities.
Let us choose a single stranded sequence of length three, si, for each vertex
i of a graph, as well as another sequence, ui, which is double stranded, also
associated to vertex i. For the initial edges, 0 →i, we consider molecules of
the form
si
ui
where the parts of the molecule placed to the left and to the right of ui are
used for purposes we do not mention here. For each edge i →j we consider
molecules of the form
ui
s′
i
αj
where s′
i is the Watson-Crick complement of si and αj is the sequence
CAC sj GTG
GTG s′
j CAC

Solutions to Other NP Complete Problems
43
that is, a pattern recognized by Dra III, with the three central pairs of nu-
cleotides associated with vertex j.
If we place these molecules in the same test tube, they will anneal, and
paths of length two of the form 0 →j are produced, encoded by molecules
with blunt ends. If Dra III is added, then the end of these molecules is cut
and a sticky end associated with vertex j is produced.
We continue the procedure: add molecules associated with edges j →k, of
the same form as those associated with edges i →j, then add the restriction
enzyme, and so on and so forth. At each step, the length of the generated
paths increases by one. After n steps, where n is the number of vertices in the
graph, we have completed the ﬁrst phase, of generating candidate solutions.
By a ﬁltering phase, similar to that in Adleman’s experiment or a diﬀerent
one, making use of the speciﬁc molecules we use, we can check whether or
not at least a path is Hamiltonian.
The eﬃciency of this procedure with respect to the quantity of DNA used
is obvious (but we know no information about a possible implementation of
it). A fundamental limitation of Head’s proposal is the number of patterns
of length three we can use for encoding the vertices: 64 for Dra III. However,
as T. Head says ([128], page 81), “the report of a laboratory solution based
on a graph with 50 vertices and 100 edges would be very reassuring” and we
agree with that.
2.5
Other DNA Solutions to NP Complete
Problems
Adleman’s biochemical solution to the Hamiltonian path problem was imme-
diately followed by several generalizations and extensions to other NP com-
plete problems. It is perhaps interesting to note that several authors have
proposed experiments aiming to solve hard and interesting problems from
a theoretical or a practical point of view, but not all of these experiments
were eﬀectively done in the laboratory. G. Rozenberg has coined a nice term
for describing this activity, menmology, from mental molecular biology. It
is important to note the practical ﬂavour of menmology in comparison with
DNA computing in info, which is focused on models of computability, not on
their possible implementation.
We continue here in “menmological terms”, by brieﬂy describing several
experiments, some of them actually done, which have followed Adleman’s
seminal paper and were of a similar type (they have basically used the opera-
tions merge, amplify, separate, detect described in the previous section).
Together with M. Amos, the ﬁrst author of a PhD Thesis in DNA computing,
[8], we say that these experiments are based on ﬁltering models. M. Amos
considers two other classes: splicing models and constructive models. We will
devote several sections to splicing models, while constructive models will be

44
DNA Computing
considered in subsequent sections of this chapter, without explicitly calling
them “constructive”.
One of the most signiﬁcant immediate extensions of Adleman’s method
was proposed by R. Lipton [168], who indicates a way to solve the satisﬁability
problem for propositional formulas (in short, the SAT problem) in terms
of DNA manipulations; the paper [168] is the place where the idea of the
“DNA programming language” based on the operations mentioned above
was considered for the ﬁrst time.
We do not give too many prerequisites from logics, but we only recall that
the SAT problem refers to the existence of a truth assignment for the variables
of a well-formed formula in the conjunctive normal form which satisﬁes the
formula.
For instance,
α = (x1∨∼x2 ∨x3) ∧(x2 ∨x3) ∧(x1 ∨x3)∧∼x3
is such a formula. It involves three variables, x1, x2, x3, as well as the con-
nectives ∼, ∨, ∧(negation, disjunction, conjunction). We have four clauses,
x1∨∼x2∨x3, x2∨x3, ∼x1∨x3, and ∼x3. If we can ﬁnd a truth assignment
such that each clause is true, then the whole formula would be satisﬁable.
(Incidentally, this is the case for α above: the last clause is true only when
x3 is false; from the second clause we then need x2 true and from the third
clause we need that x1 is true; these truth values also make the ﬁrst clause
true. Consequently, formula α is satisﬁable, the truth assignment x1 = x2 =
true, x3 = false leads to value true for the formula.)
The SAT problem is probably the most used NP complete problem, in
very many places in order to prove that a problem is NP complete one reduces
it (in polynomial time) to SAT.
Lipton’s solution is based on a natural reduction to a graph problem and
consists of two phases, the generation of all paths in a graph and looking for
a truth assignment which satisﬁes a given formula. Here are some details.
Let us assume that we have a propositional formula with k variables. We
construct the graph in Figure 2.11.
s
s
s
s
s
s
s
s
s
s
s
s
s
s
s


✒❅
❅❅
❘

✒❅
❅❅
❘

✒
❅
❅❅
❘

✒❅
❅❅
❘
❅
❅❅
❘

✒❅
❅❅
❘

✒❅
❅❅
❘


✒❅
❅❅
❘

✒
v0
a0
1
a0
2
a0
3
a0
k−1
a0
k
a1
1
a1
2
a1
3
a1
k−1
a1
k
v1
v2
vk−1
vk
. . .
Figure 2.11: A graph associated with a truth assignment.
The vertices v1, . . . , vk are associated with the variables, the vertices a0
i , a1
i

Solutions to Other NP Complete Problems
45
are associated with the truth values false, true, respectively. A path vi−1aj
ivi
encodes the fact that the variable xi takes the truth value j, where 0 = false
and 1 = true. Thus, a path from v0 to vk will indicate a truth assignment to
the k variables x1, . . . , xk.
The ﬁrst phase of Lipton’s method consists precisely of generating all
paths from v0 to vk, hence all truth assignments for the k variables. This can
be done in the same way as in Adleman’s experiment: consider single stranded
oligonucleotides associated with vertices, as well as complementary sequences
associated with preﬁxes and suﬃxes of these sequences; by annealing, these
sequences will play a “domino game” which will lead to molecules encoding
paths in our graph. Select all paths which start in v0 and end in vk. This
condition is suﬃcient in order to ensure that all vertices vi, 1 ≤i ≤k −1,
are visited exactly once, while from each pair (a0
i , a1
i ), 1 ≤i ≤k, exactly one
vertex is visited.
Several details are worth emphasizing. The graph is associated with the
variables, not with the formula, that is, we can prepare a tube containing all
the paths from v0 to vk once for all formulas with k variables. This graph
has no cycles, therefore the set of possible paths is ﬁnite, of a well deﬁnite
upper length. There is no Hamiltonian path in this graph, but also Head’s
method for the Hamiltonian path problem can be used in the ﬁrst phase of
Lipton’s method.
In short, making use of the DNA parallelism, we can generate all possi-
ble truth assignments for a formula with a given number of variables (in a
number of biochemical steps which is linear with respect to the number of
variables). Let us denote by N0 the test tube (in the sense of the program-
ming language introduced in the previous section, that is, as a multiset of
molecules) produced by the ﬁrst phase.
The second phase of the algorithm takes N0 and selects from it all paths
which satisfy the ﬁrst clause, produces a new tube, N1, and continues in this
way; after having a tube Ni, 0 ≤i ≤k −1 (which contains paths which
correspond to truth assignments which satisfy the ﬁrst i clauses), one selects
those paths from Ni which also satisfy clause i+1 and one produces the tube
Ni+1. If tube Nk still contains any molecule, then the formula is satisﬁable.
All these operations can be performed biochemically and can be written
as instructions merge, separate, detect. We exemplify with the formula
considered in [168], by recalling some paragraphs from Chapter 2 of [224].
Consider the propositional formula
β = (x1 ∨x2) ∧(∼x1∨∼x2).
We have two variables, hence the corresponding graph is as shown in Figure
2.12.
Each of the four paths through this graph corresponds to one of the
four truth assignments for the variables x1 and x2. The initial test tube,
N0, constructed as indicated above, contains strands for each of the paths

46
DNA Computing
s
s
s
s
s
s
s


✒❅
❅❅
❘

✒❅
❅❅
❘
❅
❅❅
❘

✒❅
❅❅
❘

✒
vin
v1
vout
a0
1
a0
2
a1
2
a1
1
Figure 2.12: The graph associated with formula β.
and, consequently, for each of the truth assignments. Given the length of
the oligonucleotides encoding the vertices ai
j, these oligonucleotides will be
easily distinguishable from each other, even in the case of a much larger
number of variables. For instance, the oligonucleotide encoding a1
1 does not
appear in the paths elsewhere than in the intended position. If we apply the
operation separate, forming the test tube +(N0, a1
1), then we get those truth
assignments where x1 assumes the value 1 (true). This simple observation is
the basis of the whole procedure.
We denote the truth assignments by two-bit sequences in the natural way.
Thus, 01 stands for the assignment x1 = 0, x2 = 1. A similar notation is also
used if there are more than two variables. This notation of bit sequences
is extended to the DNA strands resulting from our basic graphs. Thus, the
strand vina0
1v1a1
2vout is denoted simply by 01.
The following program solves the satisﬁability problem for the proposi-
tional formula β:
(1)
input(N0)
(2)
N1 = +(N0, a1
1)
(3)
N ′
1 = −(N0, a1
1)
(4)
N2 = +(N ′
1, a1
2)
(5)
merge(N1, N2) = N3
(6)
N4 = +(N3, a0
1)
(7)
N ′
4 = −(N3, a0
1)
(8)
N5 = +(N ′
4, a0
2)
(9)
merge(N4, N5) = N6
(10)
detect(N6)
The reader can check that the contents of the tubes at the diﬀerent steps
of the program are as follows:
Step
1
2
3
4
5
6
7
8
9
Tube
00,01,10,11
10,11
00,01
01
10,11,01
01
10,11
10
01,10

Solutions to Other NP Complete Problems
47
Thus, at step (10) the program returns true, that is, formula β is satisﬁ-
able.
The program is based on exhaustive search. The initial tube at step (1)
contains all possible truth assignments. The tube at step (5) contains the
assignments satisfying the ﬁrst clause of the propositional formula β. (Either
x1 or x2 must assume the value 1. At step (2) we have those assignments
for which x1 is 1. Of the remaining ones we still take, at step (4), those
for which x2 is 1.) The assignments in this tube, N3, are ﬁltered further to
yield at step (9) those assignments that also satisfy the second clause of the
propositional formula β.
A further extension of the ﬁltering methods of Adleman’s type was pro-
posed in [9] and developed in a series of papers of the authors of [9] and
their collaborators.
The method uses the following four basic operations
(N, N1, . . . , Nk are test tubes, that is, multisets of molecules, and S is a set
of molecules):
• remove(N, S): remove from the tube N any molecule which contains
as a subsequence at least one occurrence of each element of S;
• union({N1, . . . , Nk}, N): create the tube N by merging the tubes
N1, . . . , Nk;
• copy(N, {N1, . . . , Nk}): produce k copies of N, denoted by
N1, . . . , Nk;
• select(N): if N is not empty, then select an element of N at random;
otherwise return empty.
(We have preserved the terminology of [8], although some of these operations
have counterparts also in the programming language used above.)
These basic operations are assumed to take constant time when executed
in a parallel manner.
In this framework, algorithms for many NP complete problems can be
described. Actually, it is claimed in [8] – but only proved by examples – that
all NP complete problems can be approached in this way; among the consid-
ered problems, we mention: generating all permutations of a given ﬁnite set,
the three-vertex-colourability problem (given a graph, decide whether or not
the vertices can be coloured with three colours in such a way that no two
adjacent vertices have the same colour), the Hamiltonian path problem, the
subgraph isomorphism problem (given two graphs, ﬁnd whether or not one
of them is a subgraph of the other, modulo renaming the vertices), ﬁnd the
maximum clique in a graph (a clique is a complete graph, that is, containing
all possible edges).
It is interesting to mention the complexity of the algorithms able to solve
these problems, as the number of operations remove, union, copy, select:
the permutations of n elements are generated in O(n), the same time is needed
in the case of three-vertex-colouring, where n is the number of vertices, the

48
DNA Computing
Hamiltonian path problem is solved in constant time providing that one starts
from a solution of the permutation generation problem, the last two problems
are again solved in parallel time O(n), where n is the number of vertices.
Because of the (historical) importance of the Hamiltonian path problem,
we recall from [8] the algorithm for solving it:
Problem: Generate the set Pn of all permutations of the set
{1, 2, . . . , n}.
Input: A tube N containing all strings of the form p1i1p2i2 . . . pnin,
where pj is a code for “position j” and ij ∈{1, 2, . . . , n}, for each
1 ≤j ≤n (note that each integer ij can appear several times in an
element of N). Tube N can be produced in an easy way, by the “splint
procedure” based on annealing as in Adleman’s experiment.
Algorithm:
for j = 1 to n −1 do
begin
copy(N, {N1, . . . , Nn})
for i = 1, 2, . . . , n
in parallel do remove(Ni, {pjl | l ̸= i} ∪{pki | k > j})
union({N1, . . . , Nn}, N)
end
Pn ←N
Complexity: O(n) parallel time.
After the jth iteration of the for loop, in the surviving sequences the
integer ij does not appear at any of the positions k > j. Thus, at the end
of the computation each of the surviving strings will contain exactly one
occurrence of each integer in the set {1, 2, . . . , n}, hence the string represents
a permutation of this set. The output set, Pn, is used as input in the following
algorithm.
Problem: Determine whether or not a given graph G = (V, E) (with n
vertices in the set V ) contains a Hamiltonian path.
Input: The tube Pn produced by the previous algorithm. An integer i at
position pk in a permutation present in Pn is interpreted as follows: the
string represents a candidate solution to the problem in which vertex i
is visited at step k.
Algorithm:
for 2 ≤i ≤n −1 and j, k such that (j, k) /∈E
in parallel do remove(Pn, {jpik})
select(Pn)
Complexity: Constant parallel time given Pn.

Solutions to Other NP Complete Problems
49
Clearly, in the surviving molecules there is an edge in G for each con-
secutive pair of vertices encoded in the molecules.
Since we start from a
permutation of the vertex set, the surviving paths are Hamiltonian.
Note that we obtain all Hamiltonian paths, not only the answer to the
question whether or not such a path exists.
It is worth noting the variety of the answers given by the algorithms
mentioned above, illustrating the variety of problems the DNA computing
can approach. In Adleman’s experiment we have a decidability problem, the
answer is yes or no and it can be “read” by a detect operation (select, in
[8]). Also in the graph isomorphism problem we look for a Boolean answer.
This is not the case with the problem of generating all permutations of a
set, where we have to construct a set of molecules with a speciﬁed property.
However, these permutations are known in advance, we need them, in general,
as an input for other algorithms (this is perfectly illustrated above with the
algorithm given in [8] for solving the Hamiltonian path problem). Diﬀerent
from these is the case of the maximal clique problem, where we ask for a
number. A number can be “read” from DNA molecules in various modes,
the simplest one being probably gel electrophoresis, by carefully handling
the length of the used molecules. Of course, by eﬀectively sequencing DNA
molecules we can get as an output of an algorithm any type of information.
For instance, in the Hamiltonian path problem we might ask not only whether
a path exists or not, but, in the aﬃrmative case, to explicitly exhibit a path
or all existing paths.
In [8], the maximal clique problem is reduced to the graph isomorphism
problem, in the following manner: consider a graph G with n vertices; for
each total graph Ki, with i vertices, i ≤n, check whether or not Ki is a
subgraph of G; the maximum i for which the answer is aﬃrmative gives the
answer to our problem.
A much more appealing procedure is considered in [203] (and eﬀectively
implemented; this was one of the earliest successful experiments dealing with
NP complete problems reported after Adleman’s breakthrough). The algo-
rithm in [203] is a direct one and it consists of the following four steps:
1. Consider a graph G = (V, E) with n vertices. The vertices are denoted
by the digits 0, 1, . . . , n −1, while subsets of vertices (hence also the
possible cliques) are represented by binary strings of length n: a digit
set to 1 indicates that the corresponding vertex is present in the subset,
a digit set to 0 indicates that the corresponding vertex is not present;
we start counting from the right (the rightmost position is assigned
to vertex 0, the next one is assigned to vertex 1, and so on until the
leftmost position which is associated with vertex n −1). For instance,
in a graph with six vertices, the possible clique {1, 3, 4} is represented
by the binary string 011010.
In this way, the set of all possible sets of vertices is represented by the
set of all binary strings (we may consider them numbers) of length n.

50
DNA Computing
This set is called the complete data pool.
2. We construct the complementary graph with respect to G, that is, the
graph with the same vertices, but containing an edge (i, j) if and only
if (i, j) is not an edge in the graph G. Thus, any two vertices which
are connected in the complementary graph are disconnected in G and
therefore cannot be members of the same clique.
3. We eliminate from the complete data pools all strings containing con-
nections in the complementary graph. The surviving strings correspond
to all cliques in the original graph.
4. We sort the remaining data pool in order to ﬁnd the strings containing
the largest number of occurrences of 1. Each of these occurrences of
1 represents a vertex in the corresponding clique. The clique with the
largest number of 1’s indicates the size of the maximal clique.
The biochemical details of the way of implementing the two main phases
of the procedure, constructing the complete data pool and ﬁltering it until
obtaining the maximal clique, can be found in [203]. The basic operations
were again annealing, PCR with selective primers, digestion with restriction
enzymes and length selection by gel electrophoresis. We do not enter into
details, but we invite the reader to remember an important feature of all these
experiments, irrespective whether or not they were actually done or they are
only “mental experiments” in the menmology sense: one ﬁrst generates a
large set of candidate solutions, then one ﬁlters this set until only actual
solutions, if any, remain.
The ﬁrst phase is crucially based on the huge
parallelism made available by the DNA biochemistry.
We will return to
this observation in a subsequent section, when we will discuss the computing
by carving. Many other experiments in DNA computing were carried out
in various places. Some of them were successful, some of them are still in
progress; some of them will be discussed in the subsequent sections, some
of them not.
For convenience, we mention some references, by no means
trying to be exhaustive (this is simply impossible: on the one hand, not
all experiments were reported in largely available places, on the other hand,
many experiments are continuously reported and it is diﬃcult to keep pace
with all of them): [18], [120], [148], [151], [170], [200], [201], [217], [300].
2.6
A Two-Dimensional Generalization
The annealing operation does not suﬃce in order to capture the full power of
Turing machines – we shall formally prove this in Section 2.8. Consequently,
the computing models starting from the annealing operation and using oper-
ations of the form separate, merge, amplify, etc., are not enough in what
concerns their computability competence. This was observed also in what

A Two-Dimensional Generalization
51
concerns the performance of such models: in [293] it is shown that these op-
erations are not suﬃciently powerful in order to invert functions deﬁned by
circuits in linear time.
This remark raises the question of looking for an additional fea-
ture/operation in such a way to increase the power of the obtained model.
A theoretical solution is discussed in Section 2.8: considering some control
on the annealing operation.
Another solution, which was also practically
checked, was developed in a series of papers by E. Winfree and his collabo-
rators (see [294], [295], [298]): annealing plus ligation, in a two-dimensional
framework.
Taking into account that one uses no explicit control on applying these
operations, the fact that we obtain computational completeness is somewhat
surprising. The “explanation” lies in the power of two-dimensionality. The
link between DNA and computability is made in this case by cellular au-
tomata, in the variant known under the name of partitioning cellular au-
tomata in 2D, [287], which are known to characterize the computably enu-
merable languages. Following the mentioned papers by E. Winfree, we call
them blocked cellular automata.
In short, such an automaton consists of a bi-inﬁnite tape where symbols
from a given alphabet, V , can be written; two symbols from V are distin-
guished; one is the blank symbol and the other one is the stop symbol. One
gives a set of instructions (we also call them rules, as in a rewriting system)
of the form ab →cd, where a, b, c, d are symbols from V . These rules are
used in parallel. At any step, all symbols written on the tape are rewritten
by using such rules, with the following important restriction: from a step to
the next one, the place of using the rules is shifted with one position. The
computation starts from an initial conﬁguration of the tape (a given string
written on it) and stops at the ﬁrst step when the stop symbol is introduced.
(An equivalent variant is to stop in a halting conﬁguration.) The contents of
the tape after a completed computation is the result of the computation.
It is known that blocked cellular automata are computationally complete,
they characterize the computably enumerable languages. (From the form of
rules, they have both context-sensitivity and erasing possibilities; the shifted
application of rules ensures the global control on the tape contents, that is,
the possibility of sending information at an arbitrary distance. Such features
are generally known to equal the power of type-0 Chomsky grammars, hence
that of Turing machines.)
The “codiﬁcation” of a blocked cellular automaton in terms of DNA is
rather tricky and based on a DNA structure already known since several
years, see [109], the so-called DAE (from “Double-crossover, Antiparallel he-
lical strands with an Even number of half-turns between crossovers”). It is
obtained as suggested in Figure 2.13.
We start from ﬁve single stranded sequences as indicated in the left side
of the ﬁgure. By annealing, they will form the “domino” in the right hand

52
DNA Computing


❅
❅

✒
✏
✑
✏
✑

✒
5′
5′
5′
5′
5′
3′
3′
3′
3′
3′
3′
5′
3′
5′
u
v
w
¯a
¯w
¯x
c
¯y
¯v
d
¯z
¯u
¯b
x
y
z
x
y
z
c
d
¯a
w
v
u
¯b
¯x
¯w
¯y
¯v
¯z
¯u
Figure 2.13: Constructing a DAE.
of the ﬁgure. It is known that this construct is rigid and planar and has
four sticky ends. The subsequences x, y, z, u, v, w, a, b, c, d should be carefully
chosen; the barred sequences ¯x, ¯y, etc. are the complementary strands with
respect to x, y, etc.
It is important to note that we have four sticky ends, ¯a,¯b, c, and d. They
are denoted as the four letters in an instruction ab →cd of a blocked cellular
automaton and this is the key idea of E. Winfree. We shall present the way of
simulating a blocked cellular automaton by using DAE constructs as sketched
above in a more schematic manner, namely, by representing a DAE “domino”
as a square – Figure 2.14.
We only preserve the information concerning
the four sticky ends (hence the instruction of the cellular automaton we
want to simulate). This directly leads to another, related, characterization
of computably enumerable languages, by means of W. Hao dominoes.
❅
❅
❅


❅
❅
❅



¯a
¯b
c
d
Figure 2.14: A DAE represented as a square.
The initial contents of the tape is represented as suggested in Figure
2.15 (# is the blank symbol). The string aabab written on the tape (part
(i) of the ﬁgure), is represented by means of DAE blocks as in part (ii),
which corresponds to the geometric representation in part (iii).
Note the
presence of the sticky ends which encode the symbols #, a, a, b, a, b, # and
which correspond to the edges marked with the same symbols in the geometric

A Two-Dimensional Generalization
53
representation.
✞
✝

✆
✞
✝

✆
✞
✝

✆
✞
✝

✆
✞
✝

✆

✆
✞
✝

✆
✞
✝

✆
#
a a
b
a
b
#
(ii)
#
a
a
b
a
b
#
(i)
❅
❅
❅

❅
❅
❅

❅
❅
❅

❅
❅
❅

❅
❅
❅
#
a
a
b
a
b
#
#
#
(iii)
Figure 2.15: The initial contents of the tape.
The rules of the cellular automaton are given by means of DAE structures
(hence squares, in our simpliﬁcation). Such structures are left free in the
same solution with the initial tape. Because of the complementarity, these
structures will anneal to the corresponding places, which exactly means the
simulation of the corresponding instructions. By ligation, these new blocks
will start to construct a sort of carpet, where each row represents one stage
of the computation (a conﬁguration of the automaton tape).
Figure 2.16
shows one step parallel annealing-ligation, starting from the tape described
in Figure 2.15 (that is, from the string #aabab#) and using the following
transitions
aa →ba, ba →ab, b# →aa,
as well as the “completion rule”
## →##.
We obtain the string #baabaa#, exactly as in the cellular automaton.
Note the important fact that the next step will use pairs of symbols which
are shifted with one position with respect to the previous places where we
have used the transitions (this is automatically ensured by the shape of the
DAE structures and the shape of the initial description of the tape).
The special stop symbol is encoded by a distinguished sticky end of a
DAE structure, which can be immediately recognized during the annealing-
ligation process. (Several possibilities are imagined in [294], [295], but we
do not enter into details.
Just imagine that when this special sticky end
appears, it leads to a well prepared reaction which turns the whole solution

54
DNA Computing
to a given colour, say blue. The reader can also ﬁnd in the mentioned papers
various biochemical solutions to several other problems which appear in this
framework: how to promote the annealing and the ligation, how to get rid of
mismatches, how to stop the reaction in the moment when the stop symbol
is introduced, and so on and so forth.)
When the computation stops, the last row of DAE structures in our “car-
pet” represents the output. (Again, a biochemical problem is how to separate
this last row and to read it; details can be found in [294], [295].)
❅
❅❅
❅
❅❅
❅
❅❅
❅
❅❅
❅
❅❅
#
a
a
b
a
b
#
#
#
❅
❅❅
❅
❅❅
❅
❅❅
❅
❅❅

❅
❅
❅



❅
❅
❅



❅
❅
❅



❅
❅
❅



¯#
¯#
¯a
¯a
¯b
¯a
¯b
¯#
#
#
b
a
a
b
a
a
❅
❅
❅


¯#
#
Figure 2.16: Example of a one step computation.
Following [294], we summarize the procedure as follows.
1. Express your problem in terms of blocked cellular automata.
2. Create the initial molecule, which represents the initial contents of the
automaton tape.
3. Create the DAE structures associated with the instructions of the cel-
lular automaton.
4. Mix the molecules created in steps 2 and 3 together in a test tube and
keep under precise conditions such that the DNA “carpet” crystallize.
5. When the solution “turns blue”, extract the strand with the halting
symbol.
6. Read the result by sequencing the strand obtained at step 5.
Since the time when this way of computing by means of annealing was
proposed, several experiments were reported, practically proving that small
computations can be performed in this manner. Still, we know no compu-
tation of a practically signiﬁcant size done in this way (E. Winfree is quite
optimistic in this sense; hopefully, before this book will be printed, a con-
vincing application of DNA computing will be obtained).
Several characteristics of the previous procedure are worth mentioning:

Computing by Carving
55
– The computation is fully parallel, ﬁrst because the cellular automata
work in parallel, then because the annealing process develops in parallel.
This is very promising from the time complexity point of view.
– However, the amount of DNA involved in a computation is large and not
used in a very eﬃcient way: at each moment, the whole history of the
computation (all the previous descriptions of the tape) are preserved. In
E. Winfree terms: Can some conditions be found such that the bottom
of the lattice is dissolving while the top of the lattice is growing? We
are aware of no answer to this question.
– Theoretically, the computation itself requires no energy at all, but melt-
ing the errors, producing the initial molecules, and, especially, reading
the output are energy consuming. This illustrates Ch. Bennett’s prin-
ciple [23], that computation is free, but input and output are costly.
A quite promising – both theoretically and practically – generalization
is to work with three dimensional structures. Some details can be found in
[295], but we do not approach here this generalization.
2.7
Computing by Carving
The overall strategy of most, if not all, experiments in DNA computing re-
ported or only imagined so far, of ﬁrst generating a large set of candidate
solutions and then removing non-solutions (see again the examples in the
previous sections), suggests a mode of computation which is not very usual
in computer science or in computability theory (it appears, however, in a few
places in other areas, for instance in operations research): produce a “com-
plete data pool” (a set of candidate solutions) and ﬁlter it iteratively in such
a way that only the solutions of the problem remain. This seems to be a
very adequate strategy for DNA computing, because the ﬁrst phase can be
carried out biochemically, in a single step, using the huge parallelism made
possible by DNA. In this section, we try to exploit the power of this mode of
computing by carving in a general framework.
In formal language theory terms, this idea can be formulated in the follow-
ing quite new way of identifying a language L: start from a superlanguage of
L, say1 M, large and easy to be obtained, then remove from M strings or sets
of strings, iteratively, until obtaining L. Contrast this strategy with the usual
“positivistic” grammatical approach, where one produces the strings in L, at
the end of successful derivations, discarding the “wrong derivations” (deriva-
tions not ending with a terminal string), and constantly ignoring/avoiding
the complement of L.
In particular, M above can be V ∗, the total language over the alphabet V
of the language L. Therefore, as a particular case, we can try to produce V ∗−
1From “marble”, to ﬁt the idea of “computing by carving”. . . .

56
DNA Computing
L, the complement of L, and to extract it from V ∗. As one knows, the family
of computably enumerable languages is not closed under complementation.
Therefore, by carving we can “compute” non-computably enumerable langua-
ges !
This is just another formulation of the fact that the family RE is not
closed under complementation. But this reformulation is very natural for
DNA computing, because of the following two facts:
1. We can generate any computably enumerable language by using one of
the many generative devices based on DNA-type of operations which
were already investigated. The 2D annealing procedure presented in
the previous section is one of the possibilities. In Sections 2.10 and
2.11 we will discuss several others. In particular, we can generate (in
an easy way) the language V ∗(the “complete data pool” for our case),
or any needed regular language.
2. A procedure to implement the diﬀerence of two languages in DNA terms
can be easily imagined (in menmological terms, assuming that the oper-
ations are going in the ideal fashion, without errors, mismatching, etc.),
and such operations were already used in the experiments mentioned
in the previous sections.
Let us imagine a general procedure of this type, for “computing” the
diﬀerence L1 −L2 of two given languages L1, L2 consisting of DNA
double stranded molecules. More speciﬁcally, assume that both L1, L2
consist of molecules codifying some information in such a way that in
the upper strands of molecules one uses only the nucleotides A and
C and in the lower strands one uses the complementary nucleotides T
and G. (This gives us the possibility to speak about the upper and the
lower strands of a molecule also after denaturing the molecules and
separating the strands.) Moreover, let us add an occurrence of T in be-
tween each two nucleotides in the upper strand, and hence A in between
each two nucleotides in the lower strand. Let us also concatenate the
molecules in the two languages with some blocks α and β which never
appear as substrings of the strings in L1, L2. (The alternating occur-
rences of T in the upper strands and of A in the lower strands make
this possible.) The language L1 bounded in this way is placed in a
test tube. We separate the strands (by denaturing the DNA) and we
remove all lower strand sequences (for instance, we can pour the solu-
tion over a solid support having the upper strand of α bound to it; the
lower strands will anneal and will remain bound to the solid support,
the upper strands can be washed out). We make a copy of the tube.
We do the same with the language L2, in another test tube, preserving
only the lower strands. We merge the two tubes and leave the strands
to anneal, then we remove all molecules which are not complete du-
plexes (having sticky ends or loops, hence nucleotides not paired with a

Computing by Carving
57
complement); this can again be done by using solid support techniques
as above. What remains is the intersection of the two languages. We
melt again and remove the upper strands, then we merge the obtained
solution with the copy of the ﬁrst tube. After annealing, we remove all
complete molecules and keep all upper strands which were not paired
with their complementary lower strands. This is the diﬀerence L1 −L2
(if needed, by polymerisation/ampliﬁcation we can return to double
stranded molecules).
Of course, from a biochemical point of view, this is a completely idealized
procedure (not to speak about the fact that we have tacitly dealt with inﬁnite
languages codiﬁed as DNA molecules and handled in test tubes, which, for
a while are nothing else than ﬁnite objects . . . ). However, this is the same
kind of idealization as when passing from a computer to a Turing machine
as a computing model, with an inﬁnite tape, working an unbounded number
of steps, etc. Important is that the diﬀerence and the complementation are
“DNA-like operations”.
In sum: we can produce V ∗, we can produce any given computably enu-
merable language L and we can compute the diﬀerence V ∗−L. In this way,
we can step outside the family of computably enumerable languages!
The above described procedure can hardly be considered an algorithm, in
any broad meaning of the term, hence we cannot infer that in this way we
violate the Church–Turing Thesis. (We involve operations between inﬁnite
sets, considered as “one step operations”, some of these operations are per-
formed in an analog rather than a symbolic way, etc. Analog super-Turing
computations were described also in other places, see, e.g. [271], based on
other techniques.) However, in some sense (because of the lack of precise
deﬁnitions of the involved notions), this is a matter of terminology: what an
algorithm is, what the Church–Turing Thesis says, when crossing the bar-
rier of computable enumerability means violating the Church–Turing Thesis?
We do not persist in this speculative direction, but we want to look for more
precise deﬁnitions of the “computing by carving” in formal language theory
terms.
In practice, we can always follow a ﬁnite number of steps (dealing with
ﬁnite objects). This could be seen as a drastic decrease of the interest for
computing by carving, but we advocate for the opposite conclusion. First, in
practice we deal with instances of problems, which are always ﬁnite; still, in
order to solve them in a uniform way, we need to study the general problem,
the whole, possibly inﬁnite, family of instances of it. Second, in practice we
are often satisﬁed with an approximation of the solution. An initial sequence
of steps from an inﬁnite sequence of approximating steps which lead in a sharp
manner to a solution can provide a satisfactory approximation2. (Actually,
we do not have a good notion of an approximation of a language in our
2There are Michelangelo unﬁnished statues which are equally impressive and valuable
as those which are believed to be ﬁnished. . . .

58
DNA Computing
framework; we will discuss this question lately.)
Let us now start a “classic” approach to computing by carving in formal
language theory terms. The main goal is to ﬁnd a way to compute “diﬃcult”
languages (beyond Turing, as it was possible in the general framework of the
previous discussion), by “as easy as possible” tools.
Identifying a language L by generating ﬁrst its complement, H = V ∗−L,
and then computing the diﬀerence V ∗−H is quite a rough procedure. Let us
consider a more subtle one, where L is obtained at the end of several diﬀerence
operations. The “most subtle” case is that when we have an inﬁnite sequence
of such operations: we construct V ∗, as well as certain languages L1, L2, . . .,
and we iteratively compute V ∗−L1, (V ∗−L1) −L2,. . . , such that L is
obtained at the limit. In total, this means
L = V ∗−(∪i≥1Li).
We have returned again to the complement of only one language, the union

i≥1 Li. Moreover, the languages Li, i ≥1, can be rather simple while their
union can be arbitrarily complex. For example, take Li as consisting of the
ith string in the lexicographic ordering of a given language, which can be of
any complexity in the Chomsky hierarchy. Each Li is a singleton, the union
of these languages can be non-computably enumerable.
The problem is still ill formulated. The sequence of languages Li, i ≥1,
must be deﬁned in a regular, ﬁnite, way. Here is a proposal:
A sequence L1, L2, . . . of languages over an alphabet V is called regular3
if L1 is a regular language and there is a gsm g such that Li+1 = g(Li), for
each i ≥1.
Thus, L1 is given, all other languages are iteratively constructed by ap-
plying the gsm g to L1. In this way, we can identify the sequence by its
generating pair (L1, g).
Then, a language L ⊆V ∗is said to be C-REG computable if there is a
regular sequence of languages, L1, L2, . . ., and a regular language M ⊆V ∗
such that L = M −(
i≥1 Li).
Note that when deﬁning C-REG computable languages we also allow the
use of a speciﬁed initial (regular) language M, which is not necessarily V ∗as
in the previous discussion.
As in Section 1.6, we denote by g∗(L1) the iteration of the gsm g over L1.
Then, we can write L = M −g∗(L1). We are again back to the complement
with respect to M of a single language, g∗(L1), but this language is the union
of a regular sequence of languages (the languages in the sequence are deﬁned
by ﬁnite tools: a regular grammar – or a ﬁnite automaton – for the language
L1, and the gsm g).
3Please distinguish between “a sequence of regular language” and “a regular sequence
of languages”, as deﬁned here. Every regular sequence of languages is a sequence of regular
languages, but the converse is not true.

Computing by Carving
59
The regular sequences of languages deserve a more detailed study. We
present here only a few properties of them.
First, it is easy to see that the union of a regular sequence of languages
can be of a high complexity, in spite of the fact that each ﬁnite union is a
regular language. For instance, it is easy to see that the sequence
Li = {a2i}, i ≥1,
is regular: start from L1 = {a2} and take the gsm which doubles each occur-
rence of a. However, the language {a2i | i ≥1} is non-context-free.
However, not all computably enumerable languages can be written as
the union of a regular sequence. We can ﬁnd counterexamples by using the
following necessary condition, whose proof is omitted:
Lemma 2.1 If (L1, g) identiﬁes an inﬁnite language L = g∗(L1), then there
is a constant k such that for each x ∈L we can ﬁnd y ∈L such that |x| <
|y| ≤k|x|.
For instance, the language
L = {a22n
| n ≥1}
does not have the property in Lemma 2.1, hence it cannot be obtained as
the union of a regular sequence of languages.
Note that this language is
context-sensitive.
On the other hand, the family of languages which can be obtained in this
way is not at all a small one – remember Theorem 1.3 from Section 1.7:
Every computably enumerable language L ⊆T ∗can be written in the form
L = g∗({a0}) ∩T ∗, where g is a gsm – depending on L – and a0 is a ﬁxed
symbol not in T.
This result has an important consequence: because we have
V ∗−(g∗({a0}) ∩T ∗) = (V ∗−g∗({a0})) ∪(V ∗−T ∗),
and V ∗−T ∗is a regular language, it follows that V ∗−g∗({a0}) is not
necessarily a computably enumerable language: if V ∗−(g∗({a}) ∩T ∗) is not
computably enumerable (and this can be the case, because g∗({a0}) ∩T ∗
can be any computably enumerable language), then V ∗−g∗({a0}) is not
computably enumerable either.
This statement is worth formulating as a
theorem:
Theorem 2.3 There are C-REG computable languages which are not com-
putably enumerable languages.
A question appears here in a natural way: How powerful is the comput-
ing by carving (when using regular sequences of languages)? The answer is
provided by the following result.

60
DNA Computing
Theorem 2.4 A language is C-REG computable if and only if it can be
written as the complement of a computably enumerable language.
Proof.
Consider a language L ⊆T ∗such that the language T ∗−L is com-
putably enumerable. Consider a type-0 Chomsky grammar G = (N, T, S, P)
for the language T ∗−L.
We construct the regular sequence of languages starting with
L1 = {S},
and using the gsm
g = (K, V, V, s0, {s1}, R),
with the following components:
K = {s0, s1} ∪{[x] | x ∈(N ∪T)+, xy →v ∈P,
for some y ∈(N ∪T)+},
V = N ∪T,
R = {s0α →αs0 | α ∈N ∪T}
∪{s0α1 →[α1], [α1]α2 →[α1α2], . . . ,
[α1 . . . αi−2]αi−1 →[α1 . . . αi−2αi−1],
[α1 . . . αi−1]αi →vs1 | αj ∈N ∪T, 1 ≤j ≤i,
i ≥2, and α1 . . . αi →v ∈P}
∪{s0α →vs1 | α →v ∈P, α ∈N}
∪{s1α →αs1 | α ∈N ∪T}.
It is easy to see that g∗(L1) ∩T ∗= L(G) (at each iteration of g one
simulates the application of a rule in P).
Therefore, for the regular language M = T ∗we obtain M −g∗(L1) =
T ∗−L(G) = T ∗−(T ∗−L) = L. In conclusion, L is a C-REG computable
language.
Conversely, assume that L ⊆T ∗is a C-REG computable language, hence
L can be written in the form L = M −g∗(L1), for some regular sequence of
languages deﬁned by (L1, g) and a regular language M ⊆T ∗. Let us consider
the complement of L, that is T ∗−L. We have
T ∗−L = T ∗−(M −g∗(L1)) = (T ∗−M) ∪g∗(L1).
The language T ∗−M is regular (the family of regular languages is closed
under diﬀerence) and g∗(L1) is computably enumerable. Consequently, T ∗−L
is a computably enumerable language.
✷
Corollary 2.2 (i) Every context-sensitive language is C-REG computable.
(ii) The family of C-REG computable languages is incomparable with the
family of computably enumerable languages.

Computing by Carving
61
Proof. Assertion (i) follows from the fact that the family of context-sensitive
languages is closed under the complementation.
Assertion (ii) is a consequence of the non-closure of the family of com-
putably enumerable languages under the complementation.
✷
We do not know whether or not Theorem 2.4 (and assertion (i) in Corol-
lary 2.2) remains true for other, more restrictive, deﬁnitions of a regular
sequence of languages. What about restricted forms of gsm mappings? In
particular, what about deterministic gsm’s?
A related question is dealt with (and solved) below: does the number
of states of the gsm used in the deﬁnition of regular sequences of languages
induce an inﬁnite hierarchy of C-REG computable languages? (The answer
will be negative.)
Let us denote by CREGn, n ≥1, the family of languages of the form
M −g∗(L1), for M, L1 regular languages and g a gsm with at most n states;
by CREG we denote the union of all these families, that is, the family of all
C-REG computable languages.
Theorem 2.5 CREG1 ⊂CREG2 ⊆CREG3 = CREG.
Proof.
The inclusions ⊆are obvious.
According to Theorem 2.4, each
language L ⊆T ∗, L ∈CREG, has the complement in RE.
Take such
a language L and consider a grammar G = (N, T, S, P) for the language
T ∗−L in the Geﬀert normal form, that is with N = {S, A, B, C} and with P
containing context-free rules S →x, x ∈(N ∪T)∗, and the non-context-free
rule ABC →λ.
Consider L1 = {S} and the gsm
g = (K, V, V, s0, {s0}, R),
with the following components:
K = {s0, sA, sB}
V = N ∪T,
R = {s0α →αs0 | α ∈N ∪T}
∪{s0S →xs0 | S →x ∈P}
∪{s0A →sA, sAB →sB, sBC →s0}.
It is easy to see that g∗(L1) ∩T ∗= L(G) (at each iteration of g one
can simulate the application of a rule in P; if several rules are simulated at
the same iteration, then this does not change the generated language). We
proceed now as in the proof of Theorem 2.4: we take M = T ∗and we obtain
M −g∗(L1) = L. In conclusion, L ∈CREG3.
The inclusion CREG1 ⊂CREG2 is clearly proper, because by iterating
a gsm with only one state we get a language in 0L.
✷

62
DNA Computing
The previous theorem shows that in order to classify the C-REG com-
putable languages we need other parameters describing the complexity of
the regular sequences of languages, the number of states is not suﬃciently
sensitive. Such more sensitive parameters remain to be found.
Consider again a pair (L1, g), identifying a regular sequence of languages,
used for computing a language L ⊆V ∗as a diﬀerence L = M −g∗(L1), for
some M ⊆V ∗. If we stop the carving at any stage n, that is, we compute
the language Fn = M −n
i≥0 gi(L1), then we obtain a decreasing sequence
of languages
M ⊇F0 ⊇F1 ⊇. . . ⊇Fn ⊇Fn+1 ⊇. . . ⊇L.
Therefore, Fn can be considered an approximation of L, of a better quality
for bigger values of n. However, because M and L1 are regular, and the family
REG is closed under gsm mappings and diﬀerence, each language Fn, n ≥0,
is a regular one. Therefore, all these approximations are regular, irrespective
of the type of the approximated language L.
This seems to be a serious
drawback to this way of approximating a language, but we have to mention
that this happens frequently in grammar inference, when one tries to learn
an approximate grammar for an approximation of a language.
From a mathematical point of view, this observation raises the question
whether or not there are languages which can be approximated by regular
languages in a satisfactory manner. We have to start by an adequate deﬁni-
tion of the notion of an approximation.
Here is a natural proposal: for L ⊆V ∗we say that L′ ⊆V ∗is a good
regular approximation of L if L ⊆L′ and there is no regular language L′′ ⊆V ∗
such that L ⊆L′′ ⊆L′ and L′ −L′′ is inﬁnite.
Note that if we remove a ﬁnite number of strings from a regular language,
then what we obtain is again regular. Thus, if we only remove ﬁnite sets from
a given regular approximation L′ of a non-regular language L (this clearly
implies that L′ −L is inﬁnite), then we can do this operation an inﬁnite
number of steps. No satisfactory approximation in this sense can exist, that
is why we request that L′ −L′′ is inﬁnite.
A good regular approximation of a language can be useful for computing
by carving: instead of computing L, we try to compute a good regular ap-
proximation L′ of L. Because L′ is regular, it is possible to ﬁnd two regular
languages M, L0 and a gsm g such that L′ = Fn, where Fn is deﬁned as
above.
Several problems appear here: Given a language L and a regular approx-
imation L′ of it, construct (algorithmically) M, L1, g such that L′ = Fn, for
some n. (Of course, we look for a nontrivial answer: because L′ is regular, we
can take M = L′ and L1 = ∅. Moreover, we need triples M, L1, g with some
convenient properties, which have to be speciﬁed in each practical case sepa-
rately.) Does a good regular approximation exist for each language? If a good

Computing by Carving
63
approximation exists for a given language, is it unique? Find algorithmically
good regular approximations.
We attack here only one of these problems and prove that there are (linear)
languages for which there is no good regular approximation in the sense of
the previous deﬁnition. This suggests that the deﬁnition is not satisfactory
(hence the other problems should be approached only after ﬁnding such a
good deﬁnition – if any).
Theorem 2.6 The language {anbn | n ≥1} has no good regular approxima-
tion.
Proof.
Denote our language by L (we have L ∈LIN −REG) and assume
that a regular language L′ includes this language, L′ ⊆{a, b}∗.
Consider the pumping lemma for regular languages, in the following form:
for each regular language M there is a constant p such that each string z ∈M
with |z| ≥p can be written in the form z = uvw with v ̸= λ and uviw ∈M
for all i ≥0; moreover, there are such decompositions of z with |uv| ≤p
and, other decompositions, with |vw| ≤p. (We can iterate either the ﬁrst
nonterminal symbol which appears for the second time when counting from
the beginning of a derivation in a regular grammar for M, or the nonterminal
symbol which appears for the second time when counting from the end of a
derivation.)
All strings anbn are in L′. We apply the pumping lemma to such strings
with n ≥p, iterating ﬁrst a substring of an. As a consequence, we ﬁnd that
all strings of the form an+ikbn, for i ≥0, for a given k ≥1, are in L′. Now,
consider a string an+ikbn as above and apply the pumping lemma in such a
way to iterate a substring of bn. It follows that all strings of the form
ap+ikbp+jl,
for all i, j ≥0 and given k, l ≥1, are in L′.
Consider now the following regular language:
Lpkl = {ap+2likbp+(2j+1)l | i, j ≥1}.
This language is clearly included in L′ and inﬁnite. The language
L′′ = L′ −Lpkl
is regular (it is the diﬀerence of two regular languages) and it includes the
language L: no string anbn is in Lpkl, hence no string of this form has been
removed from L′. Indeed, we cannot have p + 2lik = p + (2j + 1)l: after
reducing p and simplifying with l we get 2ki = 2j + 1, which is contradic-
tory. Because L′ −L′′ = Lpkl, it follows that L′ cannot be a good regular
approximation. Because L′ was an arbitrary regular language containing L,
the theorem is proved.
✷

64
DNA Computing
We do not know whether there are non-regular languages for which good
regular approximations exist; anyway, this is only a mathematical question.
As we have said above, a more signiﬁcant question is to ﬁnd an adequate
deﬁnition of the notion of a good regular approximation. This is a task for
further investigations.
2.8
Sticker Systems
The aim of this section is twofold. First, we want to formalize and gener-
alize the basic operation used in many of the experiments described in the
previous sections, starting with Adleman’s experiment: annealing. While in
Adleman’s experiment (as well as in other experiments) this operation is ex-
plicitly mentioned, in the algorithms described in terms of the “programming
language” based on operations of the form of merge, detect, select, etc.,
the actual DNA operations are no longer mentioned. However, the basic op-
erations are crucial for implementing these “second order operations” merge,
detect, select, etc. For instance, in several of the experiments mentioned
in the previous sections, the initial data pool, whatever this means in each
case, was built up by using the annealing operation.
A natural question
appears here: how powerful this operation is, in terms of the Chomsky hi-
erarchy? The ﬁltering procedure is in all cases principally ﬁnite; otherwise
stated, a ﬁnite number of squeezing operations are performed starting from
an initial set of candidate solutions. In most cases, such an operation means
a separate operation, that is, removing strings which contain certain sub-
strings. Operations of this type preserve the place in the Chomsky hierarchy:
the language of the strings which contain certain substrings is regular, hence
also its complement – let us denote it by M – is regular; removing from a
language L the strings which contain certain substrings means intersecting
L with the language M; the intersection with a regular language preserves
the place in the Chomsky hierarchy. This implies that the language of the
solutions to a given problem which are given by procedures as above cannot
be more complex – in Chomsky’s sense – than the initial complete data pool.
This implies the need of theoretically investigating the power of annealing.
Another important reason to investigate this operation is related to its
close connection with DNA and the new data structure suggested by DNA
molecules, the double stranded sequences with the corresponding elements
complementary in a given sense. We have seen in Section 2.2 that the double
stranded sequences based on complementarity have a sort of intrinsic compu-
tational completeness (via characterizations of computably enumerable lan-
guages as images of the twin-shuﬄe language over {0, 1} by deterministic
sequential transducers).
It is easy to see that annealing itself is not very powerful: the correct
construction of double stranded sequences starting from a ﬁnite set of single
stranded sequences can be controlled with a ﬁnite memory (a “window” of

Sticker Systems
65
the length equal to the length of the longest single strand is suﬃcient to
ensure the correctness of the process); this means that in this way we get
only regular languages. Thus, on the one hand, we have a weak operation,
the annealing; on the other hand, we deal with data structures having an in-
built computational power. How can this power be used in order to increase
the power of annealing?
A more precise formulation of this question is:
what kind of a computing framework can we devise, using annealing as its
basic operation and the double strand as the basic data structure, in such
a way that a large enough computational power is obtained, maybe even
computational completeness and universality?
We will give below an answer to this question in the form of a generative
machinery; also an automata counterpart will be brieﬂy mentioned at the
end of this section.
We do not give a complete formalization of the operation we use, but we
prefer to introduce it mainly through pictures. Full technical details can be
found in Chapter 4 of [224].
Consider an alphabet V and a relation ρ ⊆V × V (of complementarity)
over V .
Intuitively, the complementarity is a symmetric relation, so we assume
that ρ is symmetric. (The Watson–Crick complementarity is also injective,
but we do not impose here this property.)
Using the relation ρ we can deﬁne the set of all double stranded sequences
over V as follows. Denote
V
V

ρ
= {
a
b

| a, b ∈V, (a, b) ∈ρ},
WKρ(V ) =
V
V
∗
ρ
.
The set WKρ(V ) is called the Watson–Crick domain associated with
the alphabet V
and the complementarity relation ρ.
The elements

a1
b1
 
a2
b2

. . .

an
bn

∈WKρ(V ) are also written in the form

w1
w2

, for w1 =
a1a2 . . . an, w2 = b1b2 . . . bn. For a sequence (sometimes we also say molecule)

w1
w2

∈WKρ(V ), we say that the two component strings, w1, w2, are strands;
w1 is the upper strand and w2 is the lower strand.
We emphasize the two properties characterizing the elements

w1
w2

of
WKρ(V ), because they are essential for the models considered below:
– the two strands w1, w2 are of the same length,
– the corresponding symbols in the two strands are complementary in the
sense of the relation ρ.

66
DNA Computing
We shall also use below “incomplete molecules,” that is, elements of the
set
Wρ(V ) = Lρ(V ) ∪Rρ(V ) ∪LRρ(V ),
where
Lρ(V ) = (
	 λ
V ∗

∪
	V ∗
λ

)
V
V
∗
ρ
,
Rρ(V ) =
V
V
∗
ρ
(
	 λ
V ∗

∪
	V ∗
λ

),
LRρ(V ) = (
	 λ
V ∗

∪
	V ∗
λ

)
V
V
+
ρ
(
	 λ
V ∗

∪
	V ∗
λ

).
(The notation

L1
L2

, for two languages L1, L2, denotes the set {

x
y

| x ∈
L1, y ∈L2}. When one of these languages is a singleton, say L1 = {x}, then
we write

x
L2

instead of

{x}
L2

. Note the essential diﬀerence between

x
y

and

x
y

: in the latter case we have no reference to the complementarity
relation, there is no relation between the symbols appearing in x and those
appearing in y;

x
y

is just a pair of strings, written one over the other, while

x
y

is a molecule, with x, y having the two properties mentioned above.)
The possible shapes of elements in Wρ(V ) are illustrated in Figure 2.17.
In all cases, we have a well-formed double stranded sequence x and overhangs
y, z in one or two sides of x. These overhangs (sticky ends) can be placed
in the upper strand or in the lower one. Note that in the case of Lρ(V ) and
Rρ(V ), the block x may be empty, but in the elements of LRρ(V ) the block
x ∈contains at least one element
 a
b

with (a, b) ∈ρ. In turn, the overhangs
can also be empty; what remains is then an element of WKρ(V ), therefore
WKρ(V ) is included in each set Lρ(V ), Rρ(V ), LRρ(V ).
Any element of Wρ(V ) which contains at least a position
 a
b

, a ̸= λ,
b ̸= λ, is called a well-started double stranded sequence; of course, when
several “columns”
 a
b

, with (a, b) ∈ρ, appear, they appear consecutively. In
general, the elements of Wρ(V ) are also called dominoes.
Among the elements of Wρ(V ) we can deﬁne a partial operation, mod-
elling the annealing operation: a well-started molecule (hence a sequence
having at least a position ﬁlled in both of the two strands) x can be pro-
longed to the right or to the left with a domino y, providing that the sticky
ends match, that is, they are complementary in the corresponding positions.
The result should always be a well-started molecule, hence a sequence which
does not have empty places surrounded by symbols from V .
These eight possible cases when a molecule x is prolonged to the right
with a molecule y are indicated in Figure 2.18. The operation is denoted by
µ(x, y).

Sticker Systems
67
y
x
y
x
y
x
y
x
y
x
z
y
x
z
z
x
y
z
x
y
Lρ(V ):
Rρ(V ):
LRρ(V ):
Figure 2.17: Possible shapes of “dominoes”.
In the symmetric way we can deﬁne µ(y, x), the prolongation of a well-
started molecule x, by a sequence y, to the left. Note that we do not need to
distinguish the “left prolongation” from the “right prolongation” by denoting
them in diﬀerent ways: in any case at least one of the terms of the operation
must be a well-started molecule and the result – it is a well-started molecule,
too – entirely depends on the order of the two sequences and on their sticky
ends.
Note that in cases 3 and 6 we do not use annealing (hence the complemen-
tarity relation). The maximal length of an overhang in a sequence z ∈Wρ(V )
is also called the delay of z and it is denoted by d(z); it represents the delay
in completing the two strands with symbols in V . (Hence, in cases 1, 2 in
Figure 2.18, the “right delay” of x and the “left delay” of y should coincide
when y is also a well-started double stranded sequence.)
In the same way as rewriting is the underlying operation for Chomsky
grammars, the sticking operation is the underlying one for sticker systems
introduced in [149], and then investigated in [222], [105], [224]. The reader
is advised to have in mind the discussion in Section 1.3 about a computabil-
ity framework: we follow here the strategy discussed in Section 1.3, with
particularizations to the sticking operation.
Free sticking (annealing) is presumably weak as it cannot produce more
than regular languages, so we deﬁne here the sticker systems in a controlled
variant: when building molecules, we start from well-started sequences and
prolong them in both directions, to the left and to the right at the same time,
synchronously. As we shall see, this restriction is able to lead to characteri-

68
DNA Computing
x
y
x
y
x
x
x
y
y
y
y
x
y
x
y
x
(1)
(2)
(3)
(6)
(7)
(4)
(5)
(8)
Figure 2.18: The sticking operation.
zations of computably enumerable languages.
A sticker system is a construct
γ = (V, ρ, A, D),
where V is an alphabet, ρ ⊆V × V is a symmetric relation, A is a ﬁnite
subset of LRρ(V ), and D is a ﬁnite subset of Wρ(V ) × Wρ(V ).
The elements of A are called axioms. Starting from these axioms and
using the pairs (u, v) of dominoes in D, we can obtain a set of double stranded
sequences in WKρ(V ), hence complete molecules, by using the operation µ
of sticking.
Formally, for a given sticker system γ = (V, ρ, A, D) and two sequences
x, y ∈LRρ(V ), we write
x =⇒y iﬀy = µ(u, µ(x, v)), for some (u, v) ∈D.
A sequence x1 =⇒x2 =⇒. . . =⇒xk, with x1 ∈A, is called a computation
in γ. A computation x1 =⇒∗xk is complete when xk ∈WKρ(V ) (no sticky
end is present in the last sequence).
The set of all molecules over V produced at the end of complete compu-
tations in γ is denoted by LM(γ) (LM stands for “language of molecules”):
LM(γ) = {w ∈WKρ(V ) | x =⇒∗w, x ∈A}.
In what follows we consider the sticker systems as generating languages
of strings. To this aim, we associate with LM(γ) the language
L(γ) = {x ∈V ∗|
 x
x′

∈LM(γ) for some x′ ∈V ∗}.
We say that L(γ) is the language generated by γ.
Several variants of sticker systems are of interest.
A system γ =
(V, ρ, A, D) is said to be:

Sticker Systems
69
– one-sided, if for each pair (u, v) ∈D we have either u = λ or v = λ,
– regular, if for each pair (u, v) ∈D we have u = λ,
– simple, if all pairs (u, v) ∈D have either u, v ∈

V ∗
λ

, or u, v ∈

λ
V ∗

.
In one-sided systems, the prolongation to the left is independent of the
prolongation to the right; in regular systems we only prolong the sequences
to the right (hence the axioms must be of the form x1x2, with x1 ∈WKρ(V )
and x2 ∈

V ∗
λ

∪

λ
V ∗

). In a computation in a simple sticker system we add
symbols only to one of the two strands.
We denote by ASL the family of languages of the form L(γ), for γ a
sticker system of an arbitrary form (SL stands for “sticker language” and
A indicates the use of sticker systems of “arbitrary forms”).
When only
sticker systems which are one-sided, regular, simple, simple and one-sided, or
simple and regular are used, we replace A in front of SL by O, R, S, SO, SR,
respectively. We stress the fact that these families contain string languages,
not languages of molecules, hence we can discuss their relationships with
families in the Chomsky hierarchy without further precautions.
We give here complete proofs for the two basic results about sticker sys-
tems: systems without a control on the operation (under the form of a syn-
chronized prolongation to the left and to the right) generate only regular
languages, while sticker systems of the general form (in fact, simple systems
suﬃce) characterize the computably enumerable languages modulo a weak
coding.
Theorem 2.7 OSL = RSL = REG.
Proof.
(OSL ⊆REG) Consider a one-sided sticker system γ = (V, ρ, A, D).
Let us denote by d the length of the longest sticky end or of the longest single
stranded sequence appearing in A or in the pairs of D.
We construct the context-free grammar
G = (N, T, S, P),
where
N = {⟨
u
λ

⟩l, ⟨
u
λ

⟩r, ⟨
	λ
u

⟩l, ⟨
	λ
u

⟩r | u ∈V ∗, 0 ≤|u| ≤d}
∪{S},
T =
V
V

ρ
,
and P contains the following rules:
1. S →⟨

u1
u2

⟩l

x1
x2

⟨

v1
v2

⟩r, for

u1
u2
 
x1
x2
 
v1
v2

∈A, with

u1
u2

,

v1
v2

∈

λ
V ∗

∪

V ∗
λ

and

x1
x2

∈WKρ(V ).

70
DNA Computing
2. ⟨

u1
u2

⟩l →⟨

u′
1
u′
2

⟩l

w1
w2

, where

u1
u2

,

u′
1
u′
2

∈

λ
V ∗

∪

V ∗
λ

,

w1
w2

∈WKρ(V ), and there is a pair in D of the form
(

u′
1
u′
2
 
x1
x2
 
y1
y2

,

λ
λ

) with

y1
y2

∈

V ∗
λ

∪

λ
V ∗

,
and

x1
x2

∈WKρ(V ), such that

x1y1u1
x2y2u2

=

w1
w2

.
(We prolong the sequence to the left, using the pairs with an empty right
hand member, in accordance with the sticky end; we remember which
is the sticky end by means of the nonterminal ⟨

u1
u2

⟩l; the subscript l
stands for “left”. Note that

w1
w2

above can be equal to

λ
λ

.)
3. ⟨

u1
u2

⟩r →

w1
w2

⟨

u′
1
u′
2

⟩r, where

u1
u2

,

u′
1
u′
2

∈

λ
V ∗

∪

V ∗
λ

,

w1
w2

∈WKρ(V ), and there is a pair in D of the form
(

λ
λ

,

x1
x2
 
y1
y2
 
u′
1
u′
2

), with

x1
x2

∈

V ∗
λ

∪

λ
V ∗

,
and

y1
y2

∈WKρ(V ), such that

u1x1y1
u2x2y2

=

w1
w2

.
(The same idea as above, but prolonging the sequence to the right.)
4. ⟨

λ
λ

⟩l →λ,
⟨

λ
λ

⟩r →λ.
(When no sticky end is present, we can ﬁnish the derivation.)
It is easy to see that L(G) = LM(γ): because we only use one-sided
pairs in order to build sequences, the operation of prolonging sequences to
the left is independent of the operation of prolonging sequences to the right
and conversely; consequently, we can always use that pair (

z1
z2

,

λ
λ

) or
(

λ
λ

,

z1
z2

) from D which sticks to the existing overhanging ends, which
means that the overhanging ends are not longer than those already existing
in A or in D. Thus, the nonterminals in N can control the process in the
same way as the sticky ends do this.
In the grammar G there is no derivation of the form X =⇒∗uXv with
both u and v being non-empty strings. Consequently ([113], Exercise 9, page
55), the language L(G) is regular. Because L(G) = LM(γ) and L(γ) is a
coding of LM(γ), we also have L(γ) ∈REG.
(REG ⊆RSL) Consider a ﬁnite automaton M = (K, V, s0, F, δ) with
K = {s0, s1, . . . , sk}, n ≥0. We construct the regular sticker system
γ = (V, ρ, A, D),
with
ρ = {(a, a) | a ∈V },

Sticker Systems
71
a
b
c
Figure 2.19: Dominoes used in the proof of inclusion REG ⊆RSL.
A = {
x
x

| x ∈L(M), |x| ≤k + 2}
∪{
x
x
 u
λ

| |xu| = k + 2, |x| ≥1, |u| = i, for
1 ≤i ≤k + 1 such that s0xu =⇒∗xusi−1},
D = {(
	λ
λ

,
	λ
v

 x
x
 u
λ

) | 1 ≤|v| ≤k + 1, |xu| = k + 2, |x| ≥1,
|u| = i, for 1 ≤i ≤k + 1, such that sjxu =⇒∗xusi−1,
and j = |v| −1}
∪{(
	λ
λ

,
	λ
v

 x
x

) | 1 ≤|v| ≤k + 1, 1 ≤|x| ≤k, and
sjx =⇒∗xsf, sf ∈F, where j = |v| −1}.
The idea is to start with a domino of the form shown in Figure 2.19a, to
iteratively use dominoes of the form shown in Figure 2.19b, and to end the
computation with a domino of the form shown in Figure 2.19c.
The overhangs codify the states of M by their lengths. The axioms in A
which are not already in WKρ(V ) and the dominoes of the form in Figure
2.19b appearing in the right hand member of pairs in D have overhangs of
lengths i, 1 ≤i ≤k + 1, which identify the state si−1 by the length i. This
state is reached by M when receiving the string in the upper strand of the
well-started molecule which is obtained using the domino. All dominoes of
the forms in Figure 2.19b and Figure 2.19c have a non-empty left overhang,
hence a molecule in WKρ(V ) cannot be prolonged.
Thus, after using a
domino of type c, the computation must stop.
Since the system γ has a
delay of at most k + 1, we have L(γ) = L(M), which completes the proof of
the inclusion REG ⊆RSL.
The inclusion RSL ⊆OSL follows from the deﬁnitions.
✷
Completing the previous result, it is interesting to note that REG −
SOSL ̸= ∅, but each regular language can be written as the coding of a
language in the family SRSL.
In short, one sided sticker systems cannot go beyond the power of ﬁ-
nite automata. The annealing operation in the “natural” form, without any
further constraints than the complementarity, is rather weak from a com-
putational point of view. In contrast, the pairing of dominoes increases the
power at the level of Turing machines (modulo a weak coding).

72
DNA Computing
Theorem 2.8 Every language L ∈RE can be written in the form L = h(L′),
where h is a weak coding and L′ ∈SSL.
Proof.
Consider a language L ⊆T ∗, L ∈RE. According to Theorem 1.5,
there exist two λ-free morphisms h1, h2 : V ∗
1 −→V ∗
2 , a regular language
R ⊆V ∗
2 and a projection prT : V ∗
2 −→T ∗for T ⊆V2, such that L =
prT (h1(EQ(h1, h2)) ∩R).
Consider a deterministic ﬁnite automaton M = (K, V2, s0, F, δ) recogniz-
ing the language R.
We construct the simple sticker system
γ = (V, ρ, A, D),
with
V
=
V2 ∪V 2 ∪K ∪{$, E, E′, C, Z},
ρ
=
{(X, X) | X ∈V },
A
=
{
s0
λ
 $
$
 	Z
λ

},
and D contains the following pairs of dominoes:
1. For every a ∈V1 such that h1(a) = b1 . . . bk, k ≥1, and h2(a) =
c1 . . . cm, m ≥1, with b1, . . . , bk, c1, . . . , cm ∈V2, and for sij ∈K,
0 ≤j ≤m, such that δ(sij, ci) = sij+1, 0 ≤j ≤m, we introduce in D
the pair
(
sim¯cmsim−1 . . . si2¯c2si1si1¯c1si0
λ

,
	b1CZb2CZ . . . CZbkCZ
λ

).
(To the left of

$
$

we produce the reversed image of some h2(a), for
a ∈V1, and at the same time we guess a valid path through M over
h2(a): si0c1c2 . . . cm =⇒∗sim. To the right we produce the image of
a through h1, with the symbols of h1(a) separated by the auxiliary
symbols CZ.)
2. (

E′sf
λ

,

E
λ

), for sf ∈F.
(The recognition of the string in the upper strand of the left part of the
sequence by means of M is ﬁnished correctly.)
3. (

λ
ss

,

λ
Z

), for all s ∈Q.
(These rules check the correct continuation of the recognition path
through M: if s1x =⇒∗xs2 is followed by s3y =⇒∗ys4, then we
must have s2 = s3, otherwise the complementarity is not observed
when using the block

λ
ss

.)

Sticker Systems
73
4. (

λ
¯b

,

λ
bC

), for b ∈V2.
(The string of symbols ¯b generated to the left of

$
$

in the upper strand
is compared with the string of symbols b generated to the right of

$
$

in the upper strand. Note that the symbols Z are “consumed” together
with the pairs of states, by rules of type 3; now we also “consume” the
symbols C introduced in the upper strand, to the right of

$
$

.)
5. (

λ
E′

,

λ
E

).
(Only in this way we can get a complete molecule.)
From the explanations above, one can see that the complete molecules
produced by γ are of the form
E′sfsf ¯ctstst . . . ¯c2s1s1¯c1s0s0
E′sfsf ¯ctstst . . . ¯c2s1s1¯c1s0s0
 $
$
 Zb1CZb2CZ . . . CZbtCZE
Zb1CZb2CZ . . . CZbtCZE

,
for
c1c2 . . . ct = h1(w) = h2(w) = b1b2 . . . bt,
for some w ∈V ∗
1 , and s0c1 . . . ct+1 =⇒∗c1 . . . ct+1sf in M for sf ∈F, hence
h1(w) ∈R.
No complete computation can be continued, because the upper strands
of dominoes (in groups 1 and 2) have one state only in the left end of the
left domino, whereas the lower strands of dominoes (in groups 3, 4, 5) have
either two states or a symbol ¯b, b ∈T, or the symbol E′ in that position.
Consider now the weak coding (in fact, a projection) h deﬁned by
h(a)
=
a, for a ∈T,
h(¯a)
=
λ, for a ∈T,
h(s)
=
λ, for s ∈Q,
h(E)
=
h(E′) = h($) = h(C) = h(Z) = λ.
Clearly, we get L = h(L(γ)), which completes the proof.
✷
The use of the weak coding in the previous characterization cannot be
avoided: in a sticker system of any type we always increase the current (in-
complete) molecule, that is, the obtained language is context-sensitive (the
work of a a sticker system can be simulated by a monotonous grammar). In
order to obtain more than context-sensitive languages, we need erasing.
The situation exhibited by the previous two theorems illustrates a very
important point: a natural operation (the annealing here), used in a free man-
ner (as in one sided sticker systems), can lead only to regular languages; when
a control on this operation is imposed (in our case, the pairing of dominoes
used at each step), the power of the considered mechanism is spectacularly

74
DNA Computing
increased. Badly enough, the control is not “natural”, it should be imple-
mented in a way which is not known from nature. In the case above, there
is no realistic way of imposing that a molecule is prolonged simultaneously
to the right and to the left with dominoes in a pair from a given set of pairs.
The biochemistry ensures computation at the level of ﬁnite automata, but in
order to get more computational power we have to wait for a new technology,
able to implement our restrictions on the operations we use. For a while, we
have to look for problems which can be solved at the level of ﬁnite automata,
by ad hoc methods (remember that at this level we do not have good uni-
versality results), a programmable DNA computer based on the annealing
operation (in the form of a sticker system) does not seem realizable in this
moment.
A similar conclusion can be drawn also for other computability frame-
works inspired by DNA biochemistry, in particular, for those based on the
splicing operation (see Sections 2.9 – 2.11).
We close this presentation of sticker systems by pointing out that there is
one more way of characterizing the computably enumerable languages (mod-
ulo a weak coding) by means of sticker systems, namely, by considering co-
herent computations. In fact, simple regular sticker systems are used. The
idea is the following.
Let us write a simple regular sticker system in the
form γ = (V, ρ, A, Dl, Du), where V, ρ, A are as above, Dl is the set of lower
dominoes (they are single strands which are placed in the lower strand of the
molecules we build), and Du are upper dominoes. Consider that both the
dominoes in Dl and those in Du are labelled with elements in the same set
M. Assume that in a computation in γ one uses the sequence xi1, xi2, . . . , xik
of dominoes from Du and the sequence yi1, yi2, . . . , yir of dominoes from Dl.
If k = r and the labels of xi1, xi2, . . . , xik are the same as the labels of
yi1, yi2, . . . , yik, then we say that the computation is coherent.
A theorem from [149] (see also Theorem 4.12 in [224]) says that each
computably enumerable language is the weak coding of a language generated
by a simple regular sticker system by means of coherent computations.
Again, the control we consider increases considerably the power in com-
parison to the power of non-controlled systems (remember that the systems
are at the same time simple and regular, hence of the most restricted type of
sticker systems), but the control is rather similar to that involved in bidirec-
tional systems: dominoes used at a possibly large distance (actually, arbitrar-
ily large, otherwise again a ﬁnite state machine can supervise the process)
should have identical labels, which is not at all very realistic at this moment.
Let us now remember the way of “reading” a DNA molecule as proposed
in Section 2.2, in such a way to obtain the twin-shuﬄe language over {0, 1},
having also in mind the characterization of computably enumerable languages
as images of TS{0,1} by (deterministic) sequential transducers. This imme-
diately suggests considering a kind of automata, with the tape of the form

Sticker Systems
75
of a double stranded sequence, with two reading heads (playing the role of
the two “insects” mentioned in Section 2.2) which scan the two strands from
left to right, controlled by a ﬁnite memory. We obtain a variant of a ﬁnite
automaton as illustrated in Figure 2.20, which are the automata counterpart
of simple regular sticker systems.
We call such an automaton a Watson–Crick ﬁnite automaton. Formally,
such a machinery is deﬁned as a construct
M = (K, V, ρ, s0, F, δ),
where K and V are disjoint alphabets, ρ ⊆V × V is a symmetric relation,
s0 ∈K, F ⊆K, and δ : K ×

V ∗
V ∗

−→P(K) is a mapping such that
δ(s,

x
y

) ̸= ∅only for ﬁnitely many triples (s, x, y) ∈K × V ∗× V ∗.
The elements of K are called states, V is the (input) alphabet, ρ is a
complementarity relation on V , s0 is the initial state, F is the set of ﬁnal
states, and δ is the transition mapping. The interpretation of s′ ∈δ(s,

x1
x2

)
is: in state s, the automaton passes over x1 in the upper level strand and
over x2 in the lower level strand of a double stranded sequence, and enters
the state s′.
s
❄
✲
✻
✲
Figure 2.20: A Watson–Crick ﬁnite automaton.
A transition in a Watson–Crick ﬁnite automaton can be deﬁned as follows:
For

x1
x2

,

u1
u2

,

w1
w2

∈

V ∗
V ∗

such that

x1u1w1
x2u2w2

∈WKρ(V ) and s, s′ ∈K,
we write
	x1
x2

s
	u1
u2

 	w1
w2

=⇒
	x1
x2

 	u1
u2

s′
	w1
w2

iﬀs′ ∈δ(s,
	u1
u2

).
We denote by =⇒∗the reﬂexive and transitive closure of the relation =⇒.
As in the case of sticker systems, we consider here the language of strings
appearing in the upper strands of Watson–Crick tapes recognized by our
automata, that is the language
L(M) = {w1 ∈V ∗| s0
w1
w2

=⇒∗
w1
w2

sf, for sf ∈F,

76
DNA Computing
and w2 ∈V ∗,
w1
w2

∈WKρ(V )}.
Informally speaking, we start with the two heads positioned on the left-
most symbols in each strand, in the initial state, and we move the heads
to the right according to the transitions speciﬁed by δ. The string in the
upper strand is recognized if the two heads reach the rightmost symbols in
the corresponding strands and the automaton enters a ﬁnal state. We stress
the fact that we deal with complete molecules; the two strings written in the
two strands of the tape have equal lengths.
From the construction above and the discussion we have started with, the
following result is rather expected:
Theorem 2.9 Each computably enumerable language is the projection of a
language recognized by a Watson–Crick ﬁnite automaton.
Characterizations of RE can be obtained also by using restricted classes
of Watson–Crick ﬁnite automata, for instance, with all moves of the form
s

x1
x2

→

x1
x2

s′ where |x1x2| = 1 (at each move we scan a symbol, either
in the upper strand or in the lower one), with only one state, with all states
being ﬁnal, etc. In these restricted cases, more powerful squeezing devices
are necessary (morphisms and/or intersections with regular languages), a
projection is no longer suﬃcient.
Taking into account the opposite directionality of the two strands of a
DNA molecule, we can consider a variant of a Watson–Crick ﬁnite automaton
which starts from a conﬁguration as that in Figure 2.21: at the beginning
of a computation, the upper reading head is placed in the ﬁrst cell of the
upper strand while the lower head is placed in the rightmost cell of the lower
strand. These automata are called reverse Watson–Crick ﬁnite automata.
Using such automata, we get characterizations of RE of the same form as in
the case of the basic form of automata.
s0
❄
✲
✻
✛
Figure 2.21: A reverse Watson–Crick ﬁnite automaton.
Finally, we observe that the projection involved in the statement of The-
orem 2.9 (more generally, any morphism) can be simulated by means of an

Extended H Systems
77
output mapping, which leads to considering Watson–Crick sequential trans-
ducers, of the form illustrated in Figure 2.22. They are nothing else than a
mixture of a Watson–Crick automaton and a gsm: the transition mapping δ
also speciﬁes an output (as in gsm’s).
s
❄
❄
✲
✻
✲
✲
Figure 2.22: A Watson–Crick transducer.
It is easy to see that for each computably enumerable language L ⊆V ∗
there is an alphabet V ′, a complementarity relation ρ over V ′, and a Watson–
Crick transducer gL such that L = gL(WKρ(V ′)).
Further details about Watson–Crick automata can be found in [107], [106],
as well as in the chapter of [224] devoted to this topic.
2.9
Extended H Systems
We have mentioned in Section 2.3 the recombination operation, where two
DNA molecules are cut by restriction enzymes in such a way that matching
sticking ends are produced and the obtained fragments are pasted together
(under the eﬀect of a ligase) such that possibly new molecules are formed.
This operation can be used as a starting point of a computability framework.
After [127], where a formal model of the recombination was considered, we
call this operation splicing and the computing models based on it are called
H systems.
In this section we introduce the uncontrolled H systems and we give the
two basic results about their generative power, the regularity lemma and the
universality lemma. Controlled and distributed H systems will be discussed in
the subsequent sections (all of them characterize the computably enumerable
languages, so they are possible models for DNA “computers” based on the
splicing operation).
We ﬁrst trace the abstraction process from the cut and paste operation
carried out by restriction enzymes and ligases to the formal operation of
splicing (we follow the style of [129] and [224], Section 7.1).

78
DNA Computing
We start from an example. Consider the following two DNA molecules
5′ −CCCCCTCGACCCCC −3′
3′ −GGGGGAGCTGGGGG −5′
5′ −AAAAAGCGCAAAAA −3′
3′ −TTTTTCGCGTTTTT −5′
as well as the restriction enzymes TaqI and SciNI, which recognize, respec-
tively, the following patterns
C G C G
C
G
C
G
A
G
C
T
A G C T
We have also indicated the way of cutting the DNA molecules. Thus, when
acting on the two molecules mentioned above, these enzymes will produce the
following four fragments:
5′ −CCCCCT
CGACCCCC −3′
3′ −GGGGGAGC
TGGGGG −5′
5′ −AAAAAG
CGCAAAAA −3′
3′ −TTTTTCGC
GTTTTT −5′
We have obtained molecules with identical sticky ends, therefore the four
fragments can be bound together, either restoring the initial molecules, or
producing new molecules by recombination.
The recombination gives the
following new molecules
5′ −CCCCCTCGCAAAAA −3′
3′ −GGGGGAGCGTTTTT −5′
5′ −AAAAAGCGACCCCC −3′
3′ −TTTTTCGCTGGGGG −5′
Because of the precise Watson–Crick complementarity, we can consider
the operation above as acting on single stranded sequences (hence on strings).
For instance, the two molecules we have started with are precisely identiﬁed
by the strings
CCCCCTCGACCCCC,
AAAAAGCGCAAAAA,
respectively, with the convention that they represent a strand of a DNA
molecule read in the 5′ to 3′ direction.
In what concerns the pattern recognized by a restriction enzyme, it can be
described by a triple (u, x, v), of strings over the alphabet {A, C, G, T}, with

Extended H Systems
79
the following meaning: (u, v) is the context where the cutting takes place
and x is the overhanging sequence. In the case of the two enzymes above we
have the triples:
(T, CG, A),
(G, CG, C).
Thus, the operation can be formalized as follows: having two strings
w1, w2 and two triples (u1, x1, v1), (u1, x2, v2), such that
w1 = w′
1u1x1v1w′′
1,
w2 = w′
2u2x2v2w′′
2,
we allow the recombination operation only when x1 = x2, and the strings
obtained in this way are
z1 = w′
1u1xv2w′′
2,
z2 = w′
2u2xv1w′′
1,
where x = x1 = x2.
Tacitly, we have made here one more generalizing step, by working with
strings over an arbitrary alphabet.
In order to get the most general operation with strings, modelling the
previously described one, we have to advance three more steps.
First, we put together pairs of triples (u, x, v) and we start directly from
pairs ((u1, x, v1), (u2, x, v2)) (which will be considered “splicing rules”, gov-
erning the operation).
Secondly, when having a pair ((u1, x, v1), (u2, x, v2)) and two strings
w1, w2 as above, w1 = w′
1u1xv1w′′
1 and w2 = w′
2u2xv2w′′
2, we can consider
only the string z1 = w′
1u1xv2w′′
2 as a result of the recombination, because the
string z2 = w′
2u2xv1w′′
1 is the result of the one-output-recombination with
respect to the symmetric pair, ((u2, x, v2), (u1, x, v1)).
Thirdly, instead of pairs of triples ((u1, x, v1), (u2, x, v2)) as above, we can
consider pairs of pairs: the passing from w1 = w′
1u1xv1w′′
1, w2 = w′
2u2xv2w′′
2
to z1 = w′
1u1xv2w′′
2 with respect to ((u1, x, v1), (u2, x, v2)) is equivalent to
the passing from w1 = w′
1u′
1v1w′′
1, w2 = w′
2u′
2v2w′′
2 to z1 = w′
1u′
1v2w′′
2 with
respect to ((u′
1, v1), (u′
2, v2)), where u′
1 = u1x and u′
2 = u2x. Similarly, we
can consider the quadruple ((u1, xv1), (u2, xv2)).
Altogether, we are led to the following operation with strings over an
alphabet V : a quadruple (u1, u2; u3, u4), of strings over V , is called a splicing
rule; with respect to such a rule r, for x, y, z ∈V ∗we write
(x, y) ⊢r z
iﬀ
x = x1u1u2x2,
y = y1u3u4y2,
z = x1u1u4y2,
for some x1, x2, y1, y2 ∈V ∗.

80
DNA Computing
We say that we splice x, y at the sites u1u2, u3u4, respectively, and the result
is z. This is the basic operation we shall deal with in this chapter.
When understood from the context, we omit the speciﬁcation of r and
write ⊢instead of ⊢r.
Of course, if we want to bring our models back to laboratory, we have to
renounce to all the aforementioned abstraction steps, going back to consid-
ering speciﬁed enzymes, with speciﬁed recognition patterns, but we will not
enter here such details.
Because we place our discussion at the most general level, we will formal-
ize the splicing rules as strings, in the following natural way. Consider an
alphabet V and two special symbols, #, $, not in V . A splicing rule (over V )
is a string of the form
r = u1#u2$u3#u4,
where u1, u2, u3, u4 ∈V ∗.
The maximal length of strings u1, u2, u3, u4 is called the radius of the
splicing rule r = u1#u2$u3#u4.
The passing from x, y to z, via ⊢r, can be represented as shown in Figure
2.23.
✲
❄
✲
x
x1
u1
u2
x2
y
y1
u3
u4
y2
z
x1
u1
u4
y2
Figure 2.23: The splicing operation.
For a better readability, in many cases we shall indicate by a vertical bar
the place where the terms of the splicing are cut, in the style:
(x1u1|u2x2, y1u3|u4y2) ⊢r x1u1u4y2,
for r = u1#u2$u3#u4.
We step now from this operation to building a computability device.
An H scheme is a pair
σ = (V, R),
where V is an alphabet and R ⊆V ∗#V ∗$V ∗#V ∗is a set of splicing rules.
Note that R can be inﬁnite and that we can consider its place in the
Chomsky hierarchy, or in another classiﬁcation of languages. In general, if

Extended H Systems
81
R ∈FL, for a given family of languages, FL, then we say that the H scheme
σ is of FL type.
For a given H scheme σ = (V, R) and a language L ⊆V ∗, we deﬁne
σ(L) = {z ∈V ∗| (x, y) ⊢r z, for some x, y ∈L, r ∈R}.
When some restriction enzymes and a ligase are present in a test tube,
they do not stop acting after one cut and paste operation, but they act
iteratively. Accordingly, for an H scheme σ = (V, R) and a language L ⊆V ∗
we deﬁne
σ0(L) = L,
σi+1(L) = σi(L) ∪σ(σi(L)), i ≥0,
and
σ∗(L) =

i≥0
σi(L).
Consequently, σ∗(L) is the closure of L under the splicing with respect
to σ, i.e. the smallest language L′ which contains L, and is closed under the
splicing with respect to σ, that is to say, σ(L′) ⊆L′.
For two families of languages, FL1, FL2, we deﬁne
H(FL1, FL2) = {σ∗(L) | L ∈FL1 and σ = (V, R) with R ∈FL2}.
The basic results in this area, of crucial importance for DNA computing
based on splicing, are the following two. Because of their importance, we
give (almost) complete proofs.
Lemma 2.2 (The Regularity Preserving Lemma) H(REG, FIN) ⊆REG.
Proof.
Let L ⊆V ∗be a regular language recognized by a ﬁnite au-
tomaton M = (K, V, s0, F, δ).
Consider also an H scheme σ = (V, R)
with a ﬁnite set R ⊆V ∗#V ∗$V ∗#V ∗.
Assume that R = {r1, . . . , rn}
with ri = ui,1#ui,2$ui,3#ui,4, 1 ≤i ≤n, n ≥1. Moreover, assume that
ui,1ui,4 = ai,1ai,2 . . . ai,ti, for ai,j ∈V, 1 ≤j ≤ti, ti ≥0, 1 ≤i ≤n. For each
i, 1 ≤i ≤n, consider the new states qi,1, qi,2, . . . , qi,ti, qi,ti+1. Denote their
set by K′ and consider the ﬁnite automaton
M0 = (K ∪K′, V, s0, F, δ0),
where
δ0(s, a) = δ(s, a), for s ∈K, a ∈V,
δ0(qi,j, ai,j) = {qi,j+1}, 1 ≤j ≤ti, 1 ≤i ≤n.

82
DNA Computing
We construct a sequence of ﬁnite automata (with λ transitions) Mk =
(K ∪K′, V, s0, F, δk), k ≥1, starting from M0, by passing from Mk to
Mk+1, k ≥0, in the following way.
Consider each splicing rule ri = ui,1#ui,2$ui,3#ui,4, 1 ≤i ≤n.
If s is a state in K ∪K′ such that
1. qi,1 /∈δk(s, λ),
2. there is s1 ∈K ∪K′ and x1, x2 ∈V ∗such that
s ∈δk(s0, x1),
s1 ∈δk(s, ui,1ui,2),
δk(s1, x2) ∩F ̸= ∅,
(therefore, x1ui,1ui,2x2 ∈L(Mk)), then we put
δk+1(s, λ) = {qi,1}.
We say that this is an initial transition of level k + 1.
Moreover, if s′ is a state in K ∪K′ such that
1. s′ /∈δk(qi,ti+1, λ),
2. there is s1 ∈K ∪K′ and y1, y2 ∈V ∗such that
s1 ∈δk(s0, y1),
s′ ∈δk(s1, ui,3ui,4),
δk(s′, y2) ∩F ̸= ∅,
(therefore, y1ui,3ui,4y2 ∈L(Mk)), then we put
δk+1(qi,ti+1, λ) = {s′}.
We say that this is a ﬁnal transition of level k + 1.
Then, δk+1 is the extension of δk with the initial and ﬁnal transitions of
level k+1, with respect to all splicing rules in R and all states s, s′ in K ∪K′.
As the set of states is ﬁxed, the above procedure stops after at most
2·n·card(K∪K′) steps, that is, there is an integer m such that Mm+1 = Mm.
We shall prove that L(Mm) = σ∗(L).
Since σ∗(L) is the smallest language containing L and closed under the
splicing with respect to σ, it is enough to prove that
(i) L ⊆L(Mm),
(ii) L(Mm) is closed under the splicing with respect to σ,
(iii) L(Mm) ⊆σ∗(L).

Extended H Systems
83
Point (i) is obvious from the construction of the automaton Mm.
In order to prove point (ii), let us consider a splicing rule ri
=
ui,1#ui,2$ui,3#ui,4 in R and two strings x, y ∈L(Mm) such that x =
x1ui,1ui,2x2, y = y1ui,3ui,4y2. There are two states s1, s2 ∈K ∪K′ such
that
s1 ∈δm(s0, x1), δm(s1, ui,1ui,2x2) ∩F ̸= ∅,
s2 ∈δm(s0, y1ui,3ui,4), δm(s2, y2) ∩F ̸= ∅.
From the construction of Mm we have
qi,1 ∈δm(s1, λ) and s2 ∈δm(qi,ti+1, λ).
This implies that x1ui,1ui,4y2 ∈L(Mm). The situation is illustrated in Figure
2.24. Consequently, σ(L(Mm)) ⊆L(Mm).
By an induction argument which we omit here, one can prove that each
string recognized by Mm can be produced by iterated splicing with respect
to σ starting from strings in L, and this concludes the proof.
✷
F
✖✕
✗✔
s2
✛
y2
✚✙
✛✘
qi,ti+1
❄
λ
✖✕
✗✔
qi,1
✲
ui,1ui,4
✖✕
✗✔
s1
✖✕
✗✔
s0
✲
✲
✻
❄
✲
λ
x1
ui,1ui,2x2
y1ui,3ui,4
Figure 2.24: Simulating a splicing in Mm.
Thus, by using a ﬁnite set of splicing rules we get regular languages only.
Making the smallest step forward (in the Chomsky hierarchy), that is, using
a regular set of splicing rules, leads to a maximal jump in generative power:
we obtain a characterization of computably enumerable languages, modulo a
ﬁltering operation in the form of an intersection with a regular language:
Lemma 2.3 (The Basic Universality Lemma) Every language L ∈CE, L ⊆
T ∗, can be written in the form L = L′ ∩T ∗for some L′ ∈H(FIN, REG).
Proof.
Consider a type-0 grammar G = (N, T, S, P), denote U = N ∪T ∪
{B}, where B is a new symbol, and construct the H scheme
σ = (V, R),

84
DNA Computing
where
V = N ∪T ∪{X, X′, B, Y, Z} ∪{Yα | α ∈U},
and R contains the following groups of rules:
Simulate :
1.
Xw#uY $Z#vY,
for u →v ∈P, w ∈U ∗,
Rotate :
2.
Xw#αY $Z#Yα,
for α ∈U, w ∈U ∗,
3.
X′α#Z$X#wYα,
for α ∈U, w ∈U ∗,
4.
X′w#Yα$Z#Y,
for α ∈U, w ∈U ∗,
5.
X#Z$X′#wY,
for w ∈U ∗,
Terminate :
6.
#ZY $XB#wY,
for w ∈U ∗,
7.
#Y $XZ#.
Consider also the language
L0 = {XBSY, ZY, XZ}
∪{ZvY | u →v ∈P}
∪{ZYα, X′αZ | α ∈U}.
We obtain L = σ∗(L0) ∩T ∗.
Let us examine the work of σ, namely the possibilities to obtain a string
in T ∗.
No string in L0 is in T ∗. All rules in R involve a string containing the
symbol Z, but this symbol will not appear in the string produced by splicing.
Therefore, at each step we have to use a string in L0 and, excepting the case
of using the string XBSY in L0, a string produced at a previous step.
The symbol B is a marker for the beginning of the sentential forms of G
simulated by σ.
By rules in group 1 we can simulate the rules in P. Rules in groups 2–5
move symbols from the right hand end of the current string to the left hand
end, thus making possible the simulation of rules in P at the right hand end
of the string produced by σ. However, because B is always present and marks
the place where the string of G begins, we know at each moment which is
that string. Namely, if the current string in σ is of the form β1w1Bw2β2, for
some β1, β2 markers of types X, X′, Y, Yα with α ∈U, and w1, w2 ∈(N ∪T)∗,
then w2w1 is a sentential form of G.
We start from XBSY , hence from the axiom of G, marked to the left
hand with B and bracketed by X, Y .
Let us see how the rules of types 2–5 work. Take a string XwαY , for
some α ∈U, w ∈U ∗. By a rule of type 2 we get
(Xw|αY, Z|Yα) ⊢XwYα.
The symbol Yα memorizes the fact that α has been erased from the right
hand end of wα. No rule in R can be applied to XwYα, excepting the rules
of type 3:
(X′α|Z, X|wYα) ⊢X′αwYα.

Extended H Systems
85
Note that the same symbol α removed at the previous step is now added in
the front of w. Again we have only one way to continue, namely by using a
rule of type 4. We get
(X′αw|Yα, Z|Y ) ⊢X′αwY.
If we use now a rule of type 7, removing Y , then X′ (and B) can never be
removed, the string cannot be turned to a terminal one. We have to use a
rule of type 5:
(X|Z, X′|αwY ) ⊢XαwY.
We have started from XwαY and have obtained XαwY , a string with the
same end markers. We can iterate these steps as long as we want, so any
circular permutation of the string between X and Y can be produced. More-
over, what we obtain are exactly the circular permutations and nothing more
(for instance, at every step we still have one and only one occurrence of B).
To every string XwY we can also apply a rule of type 1, providing w ends
with the left hand member of a rule in P. Any rule of P can be simulated in
this way, at any place we want in the corresponding sentential form of G, by
preparing the string as above, using rules in groups 2–5.
Consequently, for every sentential form w of G there is a string XBwY ,
produced by σ, and, conversely, if Xw1Bw2Y is produced by σ, then w2w1
is a sentential form of G.
The only way to remove the symbols not in U from the strings produced
by σ is by using rules in groups 6, 7. More precisely, the symbols XB can be
removed only if Y is present (hence the work is blocked if we use ﬁrst rule 7,
removing Y : the string cannot participate to any further splicing, and it is
not terminal) and the symbol B is in the left hand position. After removing
X and B we can remove Y , too, and what we obtain is a string in U ∗. From
the previous discussion, it is clear that if such a string is from T ∗, then it
is in L(G), hence σ∗(L0) ∩T ∗⊆L(G). Conversely, each string in L(G) can
be produced in this way, hence L(G) ⊆σ∗(L0) ∩T ∗. We have the equality
L(G) = σ∗(L0) ∩T ∗, which completes the proof.
✷
Many variants of the rotate-and-simulate procedure from the previous
proof are used in the proofs of the results in the following sections.
The intersection with T ∗in the Basic Universality Lemma suggests the
deﬁnition of extended H systems, which are quadruples
γ = (V, T, A, R),
where V is an alphabet, T ⊆V , A ⊆V ∗, and R ⊆V ∗#V ∗$V ∗#V ∗, where
#, $ are special symbols not in V .
We call V the alphabet of γ, T is the terminal alphabet, A is the set of
axioms, and R the set of splicing rules.
The language generated by γ is deﬁned by
L(γ) = σ∗(A) ∩T ∗,

86
DNA Computing
where σ = (V, R) is the underlying H scheme of γ.
For two families of languages, FL1, FL2, we denote by EH(FL1, FL2) the
family of languages L(γ) generated by extended H systems γ = (V, T, A, R),
with A ∈FL1, R ∈FL2.
The following counterpart of the Regularity Preserving Lemma can be
easily proved.
Lemma 2.4 REG ⊆EH(FIN, FIN).
The known results about the generative power of extended H sys-
tems are summarized in Table 2.1, where at the intersection of the row
marked with FL1 with the column marked with FL2 there appear either
the family EH(FL1, FL2), or two families FL3, FL4 such that FL3 ⊂
EH(FL1, FL2) ⊆FL4. These families FL3, FL4 are the best possible esti-
mations among the six families considered here.
Table 2.1: The generative power of extended H systems.
FIN
REG
LIN
CF
CS
CE
FIN
REG
CE
CE
CE
CE
CE
REG
REG
CE
CE
CE
CE
CE
LIN
LIN, CF
CE
CE
CE
CE
CE
CF
CF
CE
CE
CE
CE
CE
CS
CE
CE
CE
CE
CE
CE
CE
CE
CE
CE
CE
CE
CE
Thus, the only family which is not equal to a family in the Chomsky
hierarchy is EH(LIN, FIN).
2.10
Controlled H Systems
Two of the relations summarized in Table 2.1 are central for the DNA com-
putability based on splicing:
1.
EH(FIN, FIN) = REG,
2.
EH(FIN, REG) = CE.
The ﬁrst one says that ﬁnite “computers” based on splicing are weak, they
only equal the power of ﬁnite automata; remember that at this level we do not
have satisfactory notions and results of universality. On the other hand, using
a non-ﬁnite regular set of splicing rules is not practically possible. The price
we pay for getting computational completeness (and universality: because
the proof of the Basic Universality Lemma is constructive, if we start from

Controlled H Systems
87
a universal type-0 grammar G, then we get an equivalent universal extended
H system γ) is too large. We have to look for ways of improving the power
of H systems with ﬁnite components. A suggestion comes from the regulated
rewriting area in formal language theory, where the power of context-free
grammars is increased by imposing restrictions on the use of rules. This is
exactly what we will do in this section and the result is rather promising:
“weak” controls on using the splicing rules are enough in order to obtain
characterizations of computably enumerable languages by means of ﬁnite H
systems.
Actually, suggestions on how to obtain controls of this type comes from
the very proof of Lemma 2.3. Indeed, examining this proof, one can see that
the set of splicing rules is inﬁnite because of the appearance of substrings
w in rules of types 1, 2, 3, 4, 5, 6. However, these substrings contain no
information, they are arbitrary strings over the alphabet N ∪T ∪{B}. The
role of these substrings w is to allow information to be obtained about the
symbol appearing behind them, namely X, X′ in the left hand end of a term
of the splicing and Y, Yα, α ∈N ∪T ∪{B}, in the right hand end of the other
term of the splicing. Otherwise stated, we have in fact a ﬁnite set of splicing
rules, applied only to strings containing (at their ends) certain symbols, from
well speciﬁed sets. This suggests considering the following type of H systems
with controlled splicing.
An extended H system with permitting contexts is a quadruple
γ = (V, T, A, R),
where V is an alphabet, T ⊆V , A is a ﬁnite language over V , and R is a
ﬁnite set of triples of the form p = (r; C1, C2), with r = u1#u2$u3#u4 being
a splicing rule over V and C1, C2 being subsets of V .
For x, y, z ∈V ∗and p ∈R, p = (r; C1, C2), we deﬁne (x, y) ⊢p z if and
only if (x, y) ⊢r z, every element of C1 appears in x and every element of C2
appears in y; when C1 = ∅or C2 = ∅, then no condition on x, respectively
y, is imposed.
The language generated by γ is deﬁned in the natural way:
L(γ) = σ∗(A) ∩T ∗,
where σ = (V, R) is the underlying H scheme with permitting context rules.
We denote by EH([n], p[m]), n, m ≥1, the family of languages L(γ) gen-
erated by extended H systems with permitting contexts, γ = (V, T, A, R),
with card(A) ≤n and rad(R) ≤m, where rad(R) is the maximal radius
of splicing rules r in triples (r; C1, C2) from R. When no restriction on the
number of axioms or on the maximal radius is considered (but, of course,
these numbers are still ﬁnite), we replace [n] or [m], respectively, by FIN.
The following lemma can be obtained by rephrasing the proof of Lemma
2.3 in terms of extended H systems with permitting contexts; the details are
left to the reader.

88
DNA Computing
Lemma 2.5 CE ⊆EH(FIN, pFIN).
Actually, a stronger result can be proved, with either the number of ax-
ioms or the radius of the splicing rules bounded (by small values):
Theorem 2.10 CE = EH([1], pFIN) = EH(FIN, p[2]).
A proof can be found in [224]. It is not known whether or not the two
parameters can be simultaneously bounded. (Note that the usual argument
leading to a bound on the parameters describing an H system – starting from
a universal type-0 grammar we get a ﬁxed equivalent H system, also universal
– does not work in the cases when axioms are counted, because the axioms
are modiﬁed according to the particular system simulated by the universal
one.)
Consider now again the proof of Lemma 2.3. The symbols whose presence
is checked are elements of the set of control symbols
Q = {X, X′, Y } ∪{Yα | α ∈N ∪T ∪{B}}.
The presence of a symbol is equivalent with the absence of all other sym-
bols, because the control symbols are always present at the ends of the strings
– except when ﬁnishing the generation of a terminal string. Thus, we can
consider a dual variant of extended H systems with permitting contexts, that
is, systems with forbidding contexts.
An extended H system with forbidding contexts is a quadruple γ =
(V, T, A, R), where V is an alphabet, T ⊆V (the terminal alphabet), A
is a ﬁnite language over V (axioms), and R is a ﬁnite set of triples (we
call them rules with forbidding contexts) of the form p = (r; D1, D2), where
r = u1#u2$u3#u4 is a splicing rule over V and D1, D2 are subsets of V .
For x, y, z, w ∈V ∗and p ∈R, p = (r; D1, D2), we deﬁne (x, y) ⊢p z if
and only if (x, y) ⊢r z, no element of D1 appears in x and no element of D2
appears in y; when D1 = ∅or D2 = ∅, then no condition on x, respectively
y, is imposed.
From a biochemical point of view, the permitting contexts can be inter-
preted as catalysts or promoters, favouring the splicing by the associated
splicing rule, while the forbidding contexts can be interpreted as inhibitors,
suppressing the associated splicing rule.
The pair σ = (V, R) is called an (underlying) H scheme with forbidding
contexts rules. The language generated by γ is deﬁned in the usual way:
L(γ) = σ∗(A) ∩T ∗.
We denote by EH([n], f[m]), n, m ≥1, the family of languages L(γ) gen-
erated by extended H systems with forbidding contexts, γ = (V, T, A, R),
with card(A) ≤n and rad(R) ≤m, where rad(R) is the maximal radius of
splicing rules r in triples (r; D1, D2) in R.

Controlled H Systems
89
As expected from the previous discussion, we have the following result,
stronger than that obtained in the case of permitting contexts: both param-
eters are bounded.
Theorem 2.11 EH([1], f[2]) = CE.
The control through forbidding symbols, can also be implemented by
considering a priority relation on the set of splicing rules.
Speciﬁcally, an ordered extended H system is a construct γ = (V, T, A,
R, >), where V is an alphabet, T ⊆V (the terminal alphabet), A is a ﬁnite
language over V (axioms), R is a ﬁnite set of splicing rules over V , and > is
a partial order relation on R.
For x, y, z ∈V ∗and r ∈R we allow the relation (x, y) ⊢r z only if we
do not have (x, y′) ⊢r′ z′ for some y′, z′ ∈V ∗and r′ ∈R such that r′ > r.
(A splicing is performed by a rule which is maximal among all splicing rules
which can be applied to the ﬁrst string and any other second string.)
We denote by EH([n], ord[m]), n, m ≥1, the family of languages gener-
ated by ordered extended H systems with at most n axioms and of radius at
most m. We get the following counterpart of the previous theorem:
Theorem 2.12 CE = EH([1], ord[2]).
The order relation in systems as above can be interpreted as modelling
the diﬀerence between the reactivity of the enzymes involved in the splicing
rules: when two diﬀerent enzymes can cut the same string, the more reactive
one will actually work.
Another way of regulating the splicing can be achieved by target lan-
guages, which correspond to another biochemical aspect, encountered in vivo:
nature selects the oﬀsprings of the evolutionary process in a rather dramatic
manner, not allowing the perpetuation of “unsuitable” forms of life.
An extended H system with local targets is a construct γ = (V, T, A, R),
where V is an alphabet, T ⊆V (the terminal alphabet), A is a ﬁnite lan-
guage over V (axioms), and R is a ﬁnite set of pairs p = (r, Qp), where
r = u1#u2$u3#u4 is a splicing rule over V and Qp is a regular language over
V . For x, y, z ∈V ∗and p = (r, Qp) in R we write (x, y) ⊢p z if and only if
(x, y) ⊢r z and z ∈Qp (the result of the splicing with respect to r belongs to
Qp).
If, for such an extended H system with local targets γ = (V, T, A, R), we
have Qp1 = Qp2 for all p1 = (r1, Qp1), p2 = (r2, Qp2) in R, then we say that
γ is a system with a global target. If Q is the common target language of
rules in R, then we write the system in the form γ = (V, T, A, R′, Q), with
R′ consisting of the splicing rules in R.
As usual, we denote by EH([n], lt[m]), n, m ≥1, the family of languages
generated by extended H systems with local targets having at most n axioms

90
DNA Computing
and splicing rules of radius at most m; in the case of global targets we replace
lt by gt.
As expected, we have:
Theorem 2.13 CE = EH([1], gt[2]) = EH([1], lt[2]).
We can relate the target language control also to the style of genetic algo-
rithms area, formulating the conditions about splicing operations by means
of ﬁtness mappings: consider a mapping assessing the quality (ﬁtness, reac-
tivity) of the strings and let us control the process by asking that strings
with a low degree of ﬁtness are not used in further splicings.
Proofs of all the results mentioned above and further results of this type
(including other ways of controlling the splicing operation in order to increase
the power of H systems) can be found in [224].
We now pass to considering another idea, which is not of a type inves-
tigated in the “classic” formal language theory, that of using multisets for
controlling the splicing operation.
Again a characterization of computably enumerable languages can be ob-
tained. The initial proof of this result (see also Lemma 8.13 in [224]) is not
of a rotate-and-simulate type, but a proof of this type is possible also in this
case. We give here such a proof – following [206] –, so we present this variant
of H systems in a more detailed manner.
In the deﬁnition of splicing operations used in this and in the previous
section, after splicing two strings x, y we may use again x or y as a term of a
splicing, these strings are not consumed by splicing; moreover, we may splice
strings from one “generation” with strings from another “generation”. Also
the new strings obtained by splicing are supposed to appear in an arbitrary
number of copies each.
This assumption, that whenever a string is available then arbitrarily many
copies of it are available, is realistic in the sense that, usually, a large num-
ber of copies of each string are used whenever a string is used. Moreover,
producing a large number of copies of a DNA sequence is easily feasible by
ampliﬁcation through PCR techniques.
However, the existence of several
copies of each string raises the diﬃcult problem of controlling the splicing so
as to prevent “wrong” operations. For instance, after cutting several copies of
a string x into fragments x1, x2 and modifying (part of the copies of) x1, x2 to
some x′
1, x′
2, the test tube will contain strings of all four forms, x1, x2, x′
1, x′
2;
it might be possible to recombine x1 with x′
2 or x′
1 with x2 in such a way as
to obtain illegal strings which “look like” legal strings x1x2 or x′
1x′
2.
A possibility to avoid this diﬃculty is to use at least some of the strings
in a speciﬁed number of copies and to keep track of these numbers during
the work of the system. This leads to the idea of an H system working with
multisets.

Controlled H Systems
91
An extended µH system is a quadruple
γ = (V, T, A, R),
where V is an alphabet, T ⊆V (the terminal alphabet), A is a multiset over
V + with supp(A) ﬁnite (axioms), and R is a ﬁnite set of splicing rules over
V .
For such a µH system and two multisets M1, M2 over V ∗we deﬁne
M1 =⇒γ M2
iﬀ
there are x, y, z, w ∈V ∗such that
(i)
M1(x) ≥1, (M1 −{(x, 1)})(y) ≥1,
(ii)
x = x1u1u2x2, y = y1u3u4y2,
z = x1u1u4y2, w = y1u3u2x2,
for x1, x2, y1, y2 ∈V ∗, u1#u2$u3#u4 ∈R,
(iii)
M2 = (((M1 −{(x, 1)}) −{(y, 1)})
∪{(z, 1)}) ∪{(w, 1)}.
At point (iii) we have operations with multisets. The writing above is meant
to also cover the case when x = y (then we must have M1(x) ≥2 and we
must subtract 2 from M1(x)), or z = w (then we must add 2 to M2(z)).
When γ is understood, we write =⇒instead of =⇒γ.
In plain words, when passing from a multiset M1 to a multiset M2, ac-
cording to γ, the multiplicity of two elements of M1, x and y, is diminished
by one, and the multiplicity of the words which can be obtained by recombi-
nation, z and w, is augmented by one. The multiplicity of all other elements
in supp(M1) is not changed. The obtained multiset is M2.
The language generated by an extended µH system γ consists of all words
containing only terminal symbols and whose multiplicity is at least once
greater than or equal to one during the work of γ. Formally, we deﬁne this
language by
L(γ) = {w ∈T ∗| w ∈supp(M) for some M such that A =⇒∗
γ M}.
An extended H system γ = (V, T, A, R), as deﬁned in Section 2.9, can
be interpreted as an extended µH system with A(x) = ∞for all x ∈A and
with M(x) = ∞for all multisets M whose support is composed of strings x
derived from A. Such multisets (with M(x) = ∞, if and only if M(x) > 0)
are called ω-multisets, hence the corresponding H systems can be called ωH
systems.
The family of languages generated by extended µH systems γ = (V, T,
A, R) with card(supp(A)) ≤n and rad(R) ≤m, n, m ≥1, is denoted by
EH(µ[n], [m]); when n or m are not bounded, then we replace [n], [m] by
FIN.
Similarly, we may write the families EH(FL1, FL2) as EH(ωFL1, FL2)
in order to stress the fact that we work with ω-multisets.

92
DNA Computing
It is important to point out here the fact that writing M(x) = ∞for a
string in supp(M) does not necessarily mean that we actually have at our
disposal inﬁnitely many copies of x. It only means that we do not count the
number of copies of x: at any moment when we need a copy of x we have
it. In the DNA framework, this means that when we need further copies of
a given sequence, we can produce them (for instance, by ampliﬁcation).
Using multisets, hence counting the number of occurrences (of some) of
the strings used, provides once again the tools for controlling the work of H
systems in such a way as to characterize the family CE.
Lemma 2.6 CE ⊆EH(µFIN, [3]).
Proof. Consider a type-0 Chomsky grammar G = (N, T, S, P), with the rules
in P of the form u →v with 1 ≤|u| ≤2, 0 ≤|v| ≤2, u ̸= v (for instance,
we can take G in the Kuroda normal form). Also assume that the rules in P
are labelled in a one-to-one manner. By U we denote the set N ∪T ∪{B},
where B is a new symbol, and we construct the extended µH system
γ = (V, T, A, R),
where
V = N ∪T ∪{B, X, Y, Z} ∪{Yα, Zα | α ∈U} ∪{Zr | r ∈P},
the multiset A contains the word
w0 = XSBBY,
with the multiplicity A(w0) = 1, and the following words with an inﬁnite
multiplicity:
wr
=
ZrvY, for r : u →v ∈P,
wα
=
ZαXYα, for α ∈U,
wt
=
Z.
The set R contains the following splicing rules:
Simulate :
1.
β1β2#uY $Zp#vY,
for p : u →v ∈P and β1, β2 ∈U,
Rotate :
2.
β1β2#αY $ZαX#Yα,
for α, β1, β2 ∈U,
3.
ZαXα#Y $X#β,
for α, β ∈U,
4.
β#Yα$X#Y,
for α, β ∈U,
5.
Zα#Xα$#XYα,
for α, β ∈U,
Terminate :
6.
X#β$#Z,
for β ∈U,
7.
#BBY $XZ#.

Controlled H Systems
93
The system works according to the rotate-and-simulate procedure, with
the peculiarity of the multiset control.
The sentential forms of the grammar G are reproduced in the system γ
in a circularly permuted form (with the beginning marked by two copies of
the new symbol B) and with the ends marked with X and Y . Consider that
we have such a string XwY (initially, w = SBB), in exactly one copy.
The simulation of rules in P is done by splicing rules of type 1:
(Xw′|uY, Zp|vY ) ⊢(Xw′vY, ZpuY ).
Note that such rules cannot be applied to an axiom ZpvY , because to the left
of Y we need at least three symbols from U: β1, β2 and at least one symbol
in u. The by-product string ZpuY cannot enter new splicings (because for
each rule u →v ∈P we have u ̸= v, the string ZpuY cannot be an axiom).
The rotation is performed in the following way. Take a string XwαY , for
some α ∈U, w ∈U ∗, which is present in exactly one copy. Actually, |w| ≥3,
because always we have at least two copies of B and at least one further
symbol (remember that we ignore the empty string, hence we do not need to
generate it by γ).
By the corresponding rule of type 2 we get
(Xw|αY, ZαX|Yα) ⊢(XwYα, ZαXαY ).
The obtained strings are present in exactly one copy each, the string XwαY
is no longer present.
The strings obtained in this way can enter the unique splicing
(ZαXα|Y, X|wYα) ⊢(ZαXαwYα, XY ),
by using a rule of type 3. Again, the input strings are consumed, the output
strings are present in exactly one copy each.
We continue by splicing the resulting strings by means a rule of type 4:
(ZαXαw|Yα, X|Y ) ⊢(ZαXαwY, XYα).
Now, the string ZαXαwY (present in one copy only) can be spliced by
using a rule of type 5 either with an axiom ZαXYα (and nothing new is
produced), or with the current string XYα (present in one copy), and we get:
(Zα|XαwY, |XYα) ⊢(ZαXYα, XαwY ).
The axiom ZαXYα is reproduced, the string XwαY was permuted with one
symbol.
The process can be iterated. Thus, each derivation step in the grammar
G can be simulated in the system γ.
In order to terminate, we use the rules of types 6 and 7. First, we remove
the symbol X,
(X|wY, |Z) ⊢(XZ, wY ),

94
DNA Computing
and we produce in this way the string XZ, in a unique copy; then, using the
string XZ, we can remove Y , in the presence of the two copies of B:
(w|BBY, XZ|) ⊢(w, XZBBY ).
If the string w is terminal, then it is accepted in the language L(γ), if not,
then it is “lost”, because no splicing can be applied to it.
It is easy to see that no “illegal” splicing can be done, because of the
control ensured by the multiplicity of strings and by the witness symbols
β, β1, β2 (which prevent splicing axioms instead of strings XwY ). In conclu-
sion, L(G) = L(γ).
Note that the system γ has the radius equal to three.
✷
By putting together the axioms of ﬁnite multiplicity and, separately, the
axioms of inﬁnite multiplicity, we can get the following result, whose proof is
left to the reader (it can also be found in [224]):
Lemma 2.7 EH(µFIN, [m]) ⊆EH(µ[2], [m]), for all m ≥1.
It is also easy to see that EH(µ[1], FIN) ⊆REG (take an extended µH
system γ = (V, T, A, R) with supp(A) = {w}; if A(w) < ∞, then L(γ) is
obviously a ﬁnite language, and if A(w) = ∞, then L(γ) ∈EH([1], FIN) ⊆
EH(FIN, FIN) = REG) and that REG ⊆EH(ω[1], [2]).
Combining these relations, we obtain:
Theorem 2.14 REG = EH(µ[1], [2]) = EH(µ[1], FIN) ⊂EH(µ[2],
FIN) = EH(µ[2], [m]) = CE, for all m ≥3.
We have mentioned that the restrictions on the splicing operation in ex-
tended H systems we have considered in the ﬁrst part of this section are
mainly inspired from the regulated rewriting area. These restrictions are of
a non-biochemical nature, hence they raise serious diﬃculties for present day
laboratory techniques if we want to implement them. Unfortunately, also
the multiset approach has a serious drawback: having two strings, each of
them with multiplicity one, and splicing them is an event with a very low
probability.
The results above show that the extended H systems controlled in various
ways are computationally complete and, because all proofs are constructive,
they also provide universal H systems of the considered types: starting the
constructions from universal type-0 grammars, we get equivalent universal
H systems. This is a proof that, from a theoretical point of view, the pro-
grammable DNA computer based on splicing is possible. Unfortunately, as
we have mentioned above, it is a long way from the proofs to laboratory.
Signiﬁcant progress in biochemistry and genetic engineering is necessary be-
fore even hoping that the universal programmable DNA computer (based on
splicing) can take its place near the silicon computer.

Distributed H Systems
95
2.11
Distributed H Systems
One of the important drawbacks of the models considered in the previous sec-
tion is the fact that in the constructions involved in the proofs we need many
splicing rules. Each rule corresponds to two restriction enzymes. However,
each enzyme needs speciﬁc reaction conditions, which means that several en-
zymes cannot work together, simultaneously. A suggestion how to cope with
this diﬃculty comes from the grammar systems area: using distributed archi-
tectures, separating parts of an H system which are as small as possible and
able to work independently, cooperating in a speciﬁed way, and synthesizing
the result of the computation from the partial results produced by the parts.
Several types of distributed H systems were considered so far (see a survey
in Chapter 10 of [224]). We present here only four variants, in general with
sketched proofs. We give details only when they cannot be found in [224].
We will take into account two types of problems. One is of a mathemati-
cal interest: which is the minimum number of components of a distributed H
system of a given type suﬃcient for characterizing the family of computably
enumerable languages? From a practical point of view (having in mind the
motivation we have started with), more interesting is the “orthogonal” prob-
lem: without imposing a bound on the number of components, which is the
minimal size of the components (as the number of splicing rules) of a dis-
tributed H system which is able to characterize the computably enumerable
languages? Of course, a trade-oﬀis expected among these two parameters,
the number of components and their size.
The ﬁrst model we consider here is rather similar to a parallel commu-
nicating grammar system: the components are usual context-free grammars,
working separately, synchronously, on their own sentential forms, and splic-
ing their sentential forms according to a given set of splicing rules. Thus,
we have a hybrid model, involving both rewriting operations and splicing
operations.
A splicing grammar system (of degree n, n ≥1) is a construct
Γ = (N, T, (S1, P1), (S2, P2), . . . , (Sn, Pn), R),
where
(i)
N, T are disjoint alphabets, Si ∈N, and Pi, 1 ≤i ≤n, are ﬁnite
sets of rewriting rules over N ∪T,
(ii)
R is a ﬁnite subset of (N ∪T)∗#(N ∪T)∗$(N ∪T)∗#(N ∪T)∗,
with #, $ two distinct symbols not in N ∪T.
The sets Pi are called the components of Γ.
For two n-tuples (we call them conﬁgurations) x = (x1, x2, . . . , xn), and
y = (y1, y2, . . . , yn), xi, yi ∈(N ∪T)∗, 1 ≤i ≤n, we write x =⇒y if and only

96
DNA Computing
if one of the following two conditions holds:
(i)
for each 1 ≤i ≤n, either xi =⇒Pi yi, or xi ∈T ∗;
(ii)
there exist 1 ≤i, j ≤n such that xi = x′
iu1u2x′′
i , xj = x′
ju3u4x′′
j ,
and yi = x′
iu1u4x′′
j , yj = x′
ju3u2x′′
i , for u1#u2$u3#u4 ∈R;
for all k ̸= i, j, we have yk = xk.
In the above deﬁnition, point (i) deﬁnes a rewriting step, whereas point
(ii) deﬁnes a splicing step, corresponding to a communication step in parallel
communicating grammar systems.
Note that no priority of any of these
operations over the other one is assumed and that as the result of a splicing
operation we consider both strings obtained by recombination. In case (ii)
we denote the passing from (xi, xj) to (yi, yj) by (xi, xj) ⊢(yi, yj).
The language generated by the system Γ is the language generated by its
ﬁrst component, that is,
L(Γ) = {x1 ∈T ∗| (S1, . . . , Sn) =⇒∗(x1, . . . , xn), xj ∈(N ∪T)∗, 2 ≤j ≤n}.
We denote by SGSn(X) the families of languages L(Γ), generated by
splicing grammar systems of degree at most n, n ≥1, with components of
type X. When no restriction is imposed on the number of components, then
we replace the subscript n with ∗.
Note that in view of the deﬁnition of the relation =⇒between conﬁg-
urations of a splicing grammar system, we implicitly use multiplicities of
strings, because each component of the system has in every moment – both
after a rewriting and a splicing step – exactly one current string; no string
of arbitrary multiplicity is used. Thus, as expected, such systems can char-
acterize CE; somewhat unexpected is the fact that two λ-free context-free
components suﬃce.
Theorem 2.15 CE = SGSn(CF) = SGS∗(CF), for all n ≥2.
Proof.
The inclusions SGSn(CF) ⊆SGSn+1(CF) ⊆SGS∗(CF) ⊆CE,
n ≥1, are obvious. We have only to prove the inclusion CE ⊆SGS2(CF).
Here is the core construction involved in this proof.
Consider a language L ⊆T ∗, L ∈CE, and take a grammar G =
(N, T, S, P) in the Geﬀert normal form as speciﬁed in Theorem 1.2: N =
{S, A, B, C} and P contains context-free rules of the form S →x, x ∈
({S, A, B, C} ∪T)+, as well as the rule ABC →λ. Denote by P ′ the set
of context-free rules in P. We construct the splicing grammar system
Γ = (N ∪{X, Y, Z, S2}, T, (S, P1), (S2, P2), R),
with
P1 = P ′ ∪{A →A},
P2 = {S2 →Y XXABCZ, X →XX, X →X},
R = {#ABC$Y X#XABC, #XABC$Y XABC#}.

Distributed H Systems
97
We obtain L(G) = L(Γ) (the details are left to the reader).
✷
One sees that the “main” component of the system is P1, which contains a
number of rules which depends on the grammar G, and that we need only two
splicing rules. We do not know whether or not only one splicing rule suﬃces,
but the result is optimal as the number of components (in systems with
only one component we have no communication, hence they are equivalent
to context-free grammars).
Much more interesting is the following class of distributed H systems,
which are “purely biochemical” (they correspond to parallel communicating
grammar systems which communicate by command, in a way similar to the
WAVE paradigm in distributed computing, where messages are broadcast to
all components of a system and accepted by these components according to
certain ﬁlters associated with them).
A communicating distributed H system (of degree n, n ≥1) is a construct
Γ = (V, T, (A1, R1, V1), . . . , (An, Rn, Vn)),
noindent where V is an alphabet, T ⊆V , Ai are ﬁnite languages over V , Ri
are ﬁnite sets of splicing rules over V , and Vi ⊆V , 1 ≤i ≤n.
Each triple (Ai, Ri, Vi), 1 ≤i ≤n, is called a component of Γ; Ai, Ri, Vi
are the set of axioms, the set of splicing rules, and the selector (or ﬁlter) of
the component i, respectively; T is the terminal alphabet of the system.
We denote
B = V ∗−
n

i=1
V ∗
i .
The pair σi = (V, Ri) is the underlying H scheme associated with the
component i of the system.
An n-tuple (L1, . . . , Ln), Li ⊆V ∗, 1 ≤i ≤n, is called a conﬁguration of
the system; Li is also called the contents of the ith component, understanding
the components as test tubes where the splicing operations are carried out.
For two conﬁgurations (L1, . . . , Ln), (L′
1, . . . , L′
n), we deﬁne
(L1, . . . , Ln) =⇒(L′
1, . . . , L′
n) iﬀ
L′
i =
n

j=1
(σ∗
j (Lj) ∩V ∗
i ) ∪(σ∗
i (Li) ∩B),
for each i, 1 ≤i ≤n.
In words, the contents of each component are spliced according to the
associated set of rules (we pass from Lj to σ∗
j (Lj), 1 ≤j ≤n), and the
result is redistributed among the n components according to the selectors
V1, . . . , Vn; the part which cannot be redistributed (which does not belong
to some V ∗
i , 1 ≤i ≤n) remains in the component. As we have imposed no

98
DNA Computing
restriction over the alphabets Vi, for example, we did not suppose that they
are pairwise disjoint, when a string in σ∗
j (Lj) belongs to several languages
V ∗
i , then copies of this string will be distributed to all components i with this
property.
The language generated by Γ is deﬁned by
L(Γ) = {w ∈T ∗| w ∈L1 for some L1, . . . , Ln ⊆V ∗such
that (A1, . . . , An) =⇒∗(L1, . . . , Ln)}.
That is, the ﬁrst component of the system is designated as its master and
the language of Γ is the set of all terminal strings generated (or collected by
communications) by the master.
We denote by CDHn the family of languages generated by communicating
distributed H systems of degree at most n, n ≥1. When n is not speciﬁed,
we replace the subscript n with ∗.
Without a proof, we give here the basic results about these systems:
Theorem 2.16 CE = CDHn = CDH∗, for all n ≥3.
Communicating distributed H systems of degree 1 do not use communi-
cation, hence they are extended ﬁnite H systems, that is, CDH1 = REG ⊆
CDH2. In fact, it is known that the inclusion REG ⊂CDH2 is proper.
It is an open problem whether or not the inclusion CDH2 ⊆CDH3 is also
proper, hence whether or not the result in Theorem 2.16 can be strengthened,
to n = 2. We expect a negative answer. (We even conjecture that CDH2 ⊂
CF.)
In what concerns the problem of using components of a small size, we
have:
Theorem 2.17 For each type-0 grammar G = (N, T, S, P) we can construct
a communicating distributed H system Γ such that L(G) = L(Γ), the degree
of Γ is 2(card(N ∪T) + 1) + card(P) + 9, and each component of Γ contains
only one splicing rule.
Therefore, at the expense of working with a large number of tubes, the
size of each tube is minimal: only one splicing rule.
The work of enzymes can be controlled by “environment” conditions, in
particular, by the temperature. By cyclically varying the temperature, we
cyclically use the enzymes in a test tube.
This leads to the notion of a
time-varying H system.
A time-varying H system (of degree n, n ≥1) is a construct
Γ = (V, T, A, R1, R2, . . . , Rn),
where V is an alphabet, T ⊆V (terminal alphabet), A is a ﬁnite subset of
V ∗(axioms), and Ri are ﬁnite sets of splicing rules over V, 1 ≤i ≤n.

Distributed H Systems
99
At each moment k = n · j + i, j ≥0, 1 ≤i ≤n, the component Ri is used
for splicing the currently available strings. Formally, we deﬁne
L0 = A,
Lk = σi(Lk−1), for i ≡k(mod n), k ≥1,
where σi = (V, Ri), 1 ≤i ≤n.
The language generated by Γ is deﬁned by
L(Γ) = (

k≥0
Lk) ∩T ∗.
We denote by TV Hn, n ≥1, the family of languages generated by time-
varying distributed H systems of degree at most n and by TV H∗the family
of all languages generated by time-varying H systems.
Theorem 2.18 CE = TV Hn = TV H∗, n ≥4.
Proof.
Clearly, we have to prove only the inclusion CE ⊆TV H4.
Consider a type-0 grammar G = (N, T, S, P).
Assume that N ∪T =
{α1, . . . , αn−1}. Let αn = B be a new symbol. We label the rules in P by
rj : uj →vj, for n + 1 ≤j ≤m. We also consider that ui = vi = αi, for
1 ≤i ≤n.
We construct the time-varying H system
Γ = (V, T, A, R1, R2, R3, R4),
with
V = N ∪T ∪{X, Y, Z, Z′, Q, B} ∪{Xi, Yi | 0 ≤i ≤m},
A = {XBSY, ZY, ZQ, XZ′, QZ′} ∪{XjZ′, ZYj | 0 ≤j ≤m}
∪{XjvjZ′ | 1 ≤j ≤m},
R1 = {#Y $#Y, Z#$Z#, #Z′$#Z′, X0#$X0#}
∪{#Yj$#Yj | 1 ≤j ≤m},
R2 = {X#$X#, Z#$Z#, #Z′$#Z′, #Y0$#Y0}
∪{Xj#$Xj# | 1 ≤j ≤m},
R3 = {Z#$Z#, #Z′$#Z′, #Y0$Z#Y, #Y0$ZQ#}
∪{#ujY $Z#Yj, #Yj$Z#Yj−1 | 1 ≤j ≤m},
R4 = {Z#$Z#, #Z′$#Z′, X#Z′$X0#, #Z′Q$X0B#}
∪{Xjvj#Z′$X#, Xj#$Xj−1#Z′ | 1 ≤j ≤m}.
We will prove that L(Γ) = L(G).
The idea of this proof is again “rotate-and-simulate” as introduced in the
proof of the Basic Universality Lemma.

100
DNA Computing
Here both the simulation of the rules in G and the circular permutation
of strings are performed in Γ in the same way: a suﬃx u of the current string
is removed and the corresponding string, v, is added in the left end of the
string. For u →v a rule in P we simulate a derivation step in G. For u = v a
symbol in N ∪T ∪{B} we have one symbol “rotation” of the current string.
The simulation and “rotation” are performed by the components R3 and
R4; the components R1, R2 have the role of checking the correctness of these
operations (see below).
L(G) ⊆L(Γ). Consider a string of the form XwY (initially, w = BS)
when R1 is active. Using the rule #Y $#Y , the string is passed unchanged
to R2. From R2 the string is passed to R3 by using the rule X#$X#. Now,
if w = w′ui for some n + 1 ≤i ≤m (that is, ui →vi ∈P), then in R3 we
can perform:
(Xw′|uiY, Z|Yi) ⊢Xw′Yi.
(The axioms are passed from a component to the next one by using the rules
Z#$Z#, #Z′$#Z′, which are present in all components.) The string Xw′Yi
is passed to R4 where we perform:
(Xivi|Z′, X|w′Yi) ⊢Xiviw′Yi.
This string arrives in R1, it is passed unchanged to R2 (using the rule
#Yi$#Yi ∈R1), then to R3 (by Xj#$Xj# ∈R2).
Here, the subscript
of Y is decreased by one; the resulting string, Xiviw′Yi−1, is passed to R4,
where the subscript of X is decreased by one. The process can be iterated, so
eventually we get the string X0viw′Y0. This string passes through R1, R2, in
R3 we can substitute Y0 by Y , and in R4 we substitute X0 by X. In this way
we have passed from Xw′uiY to Xviw′Y , which corresponds to simulating
the rule ui →vi from P.
If 1 ≤i ≤n, that is, ui = vi = αi ∈N ∪T ∪{B}, then in this way we
circularly permute the string with one symbol.
By iterating this procedure, we can simulate any rule of P in any desired
position. Thus, for every derivation S =⇒∗x1x2 in G, we can produce the
string Xx2Bx1Y in Γ. By a circular permutation, we can also produce the
string X0Bx1x2Y0. When we get a string of the form X0BwY0, we can remove
Y0 by the rule #Y0$ZQ# ∈R3 and X0B by the rule #Z′Q$X0B# ∈R4.
If the obtained string, w, is a terminal one, then w ∈L(Γ). Consequently
L(G) ⊆L(Γ).
L(G) ⊇L(Γ). Let us examine the form of the strings which can be passed
from a component of Γ to the next one.
First of all, we notice that all the axioms (except XSBY ) can pass through
all components by using the rules Z#$Z#, #Z′$#Z′.
Let us assume that we have performed the operation
(Xw′|uiY, Z|Yi) ⊢Xw′Yi

Distributed H Systems
101
in R3 and in R4 we performed
(Xjvj|Z′, X|w′Yi) ⊢Xjvjw′Yi,
for some 1 ≤i, j ≤m. A string of the form Xjvjw′Yi with i ≥1 can pass
through R1 by using the rule #Yi$#Yi. Then, the string can pass through
R2 using the rule Xj#$Xj#, providing that j ≥1.
In R3 we decrement by one the subscript of Y and this is the only possible
operation in this component. Similarly, in R4 we can only decrement the
subscript of X. We repeat this process until at least one subscript reaches
zero.
Suppose that after R4 we have a string X0wYk, for k ≥1. Because R1
contains the rules X0#$X0#, #Yk$#Yk, the string can be passed unchanged
to R2. There is no rule in R2 which can be applied to this string, so the string
is “lost”.
If after R4 we have a string XkwY0, for k ≥1, then it cannot pass through
R1.
So, the only strings having at least a subscript equal to zero which “sur-
vive” are those of the form X0wY0. This implies that the string Xjvjw′Yi
from which we have started to decrement the subscripts has i = j. This
means that we have correctly simulated the use of a rule ui →vi in P, or we
have circularly permuted the string by one symbol.
Consequently, the only terminal strings which can be generated by Γ
correspond to strings in L(G); because X0 can be removed only when B is
adjacent to it, the string is in the same circular permutation as the associated
string in L(G), that is, the two strings are identical.
✷
It is highly possible that the result above can be improved: a number
of components smaller than four could be suﬃcient, but we do not know a
convincing proof of such a result. In what concerns the question about the
size of components, the best known result is the following one:
Theorem 2.19 Each computably enumerable language can be generated by
a time-varying distributed H system whose components contain at most three
splicing rules.
Proof.
We give only the core construction of the proof. Consider a type-
0 grammar G = (N, T, S, P) with N ∪T = {α1, . . . , αn−1}, n ≥3, and
P = {ui →vi | 1 ≤i ≤m}. Let αn = B be a new symbol. We construct the
time-varying distributed H system
Γ = (V, T, A, R1, . . . , R2n+m+2),
with
V = N ∪T ∪{X, Y, Y ′, Z, Z′, B},

102
DNA Computing
A = {XBSY, ZY, ZY ′, ZZ′}
∪{ZvY | u →v ∈P}
∪{XαiZ | 1 ≤i ≤n},
and the following sets of splicing rules
Ri = {#uiY $Z#viY, #Y $Z#Y, Z#$Z#}, 1 ≤i ≤m,
Rm+2j−1 = {#αjY $Z#Y, #Y $Z#Y ′, Z#$Z#}, 1 ≤j ≤n,
Rm+2j = {Xαj#Z$X#, #Y ′$Z#Y, Z#$Z#}, 1 ≤j ≤n,
Rm+2n+1 = {#ZZ′$XB#, #Y $Z#Y ′, Z#$Z#},
Rm+2n+2 = {#Y $ZZ′#, #Y ′$Z#Y, Z#$Z#}.
The equality L(G) = L(Γ) can be proved in a way similar to that in the proof
of the previous theorem.
✷
We have mentioned that in grammar systems theory there also exists a
large class of systems whose components work in a sequential manner (one
component is active in each time unit, the other ones are just waiting). Also
a counterpart of such a distributed system was considered for the case of H
systems.
A sequential distributed H system (of degree n, n ≥1) is a construct
Γ = (V, T, w, (A1, R1), . . . , (An, Rn)),
where V is an alphabet, T ⊆V , w ∈V ∗, Ai is a ﬁnite subset of V ∗, and Ri
is a ﬁnite subset of V ∗#V ∗$V ∗#V ∗, 1 ≤i ≤n.
V is the alphabet of Γ, T is the terminal alphabet, w is the axiom, (Ai, Ri)
is called a component of the system; Ai is the set of axioms of the component
i, Ri is the set of splicing rules of the component i, 1 ≤i ≤n.
The work of Γ consists of iterated applications of the splicing schemes
σi = (V, Ri) to pairs (x, z), (z, x) for x ∈Ai, where z is the string obtained
at the previous step, taking w as the starting string; moreover, the use of σi
is maximal in the sense that we stop using it only if no further splicing is
possible. Formally, for x, y ∈V ∗and i ∈{1, 2, . . . , n}, we deﬁne
x →i y
iﬀ
(x, z) ⊢r y or (z, x) ⊢r y, for some z ∈Ai, r ∈Ri,
x →∗
i y
iﬀ
x = y, or x = x0 →i x1 →i . . . →i xk = y, k ≥1,
xj ∈V ∗, 1 ≤j ≤k,
x =⇒i y
iﬀ
x →∗
i y and there is no u ∈V ∗such that y →i u.
The language generated by Γ is deﬁned by
L(Γ) = {x ∈T ∗| w =⇒i1 w1 =⇒i2 . . . =⇒im wm,
x = wm, m ≥1, wj ∈V ∗, 1 ≤ij ≤n, 1 ≤j ≤m}.

Distributed H Systems
103
Note that, although we work systematically on strings which are obtained
from the axiom w, we do not have here multisets; each string is assumed to
appear in an arbitrary number of copies. This is especially important for the
sets Ai, 1 ≤i ≤n, which are continuously available in their initial form.
We denote by SDHn the family of languages generated by sequential
distributed H systems of degree at most n, n ≥1, and by SDH∗the union
of all these families.
Theorem 2.20 CE = SDHn = SDH∗, n ≥3.
Proof.
We prove only the inclusion CE ⊆SDH3.
Consider a type-0 Chomsky grammar G = (N, T, S, P). Consider a new
symbol B and assume that
N ∪T ∪{B} = {α1, . . . , αn},
with B = α1; because both N and T are non-empty, we have n ≥3.
We construct the sequential distributed H system
Γ = (V, T, w, (A1, R1), (A2, R2), (A3, R3)),
where
V = N ∪T ∪{X, X′, X, Y, Y ′, Y ′′, Y , Z, B, C},
w = XBSY,
A1 = {ZvY | u →v ∈P}
∪{ZCiY ′ | 1 ≤i ≤n}
∪{ZY ′′, XZ, XZ},
R1 = {#uY $Z#vY | u →v ∈P}
∪{#αiY $Z#CiY ′ | 1 ≤i ≤n}
∪{#CY ′$Z#Y ′′, X′#$X#Z, X#$X#Z, #Y $XZ#},
A2 = {X′CZ, ZY ′, ZY },
R2 = {X′C#Z$X#, #Y ′′$Z#Y ′, XB#$#ZY, #Y $Z#Y },
A3 = {XαiZ, XαiCZ | 1 ≤i ≤n}
∪{ZY, XBZ, ZY , ZCY, XCZ, ZY ′′},
R3 = {Xαi#Z$X′Ci# | 1 ≤i ≤n}
∪{#Y ′$Z#Y, XB#Z$XC#, #Y ′$Z#Y ,
#CY $Z#CY, XC#$XC#Z, #Y ′′$Z#Y ′′}
∪{XαiC#$XαiC#Z | 1 ≤i ≤n} ∪{XBC#$XBC#Z}.
Let us examine the work of the system Γ.
Each component contains certain splicing rules which are meant to be
trap rules: if they can be applied once, they can be applied for ever, hence

104
DNA Computing
the corresponding component cannot stop correctly its work.
Such rules
are X#$X#Z in the ﬁrst component, #Y $Z#Y in the second one, and
#CY $Z#CY, XC#$XC#Z, #Y ′′$Z#Y ′′, XαiC#$XαiC#Z, 1 ≤i ≤m
and XBC#$XBC#Z in the third component.
Thus, if a string of the form XxY, x ∈(N ∪T ∪{B})∗(initially, we have
x = BS) is processed by the second component, then the system is blocked.
The third component cannot modify such a string, hence it is passed away
unchanged.
In the ﬁrst component, a string XxY is processed as follows. At the end of
x we can simulate rules of G ((Xx1|uY, Z|vY ) ⊢Xx1vY , for x = x1u and u →
v ∈P. Also, a symbol αi can be cut from the end of x ((Xx1|αiY, Z|CiY ′) ⊢
Xx1CiY ′, when x = x1αi); the removed symbol is replaced by i occurrences
of the symbol C and the replacement is also indicated by replacing Y with
Y ′. We have to continue with (Xx1Ci−1|CY ′, Z|Y ′′) ⊢Xx1Ci−1Y ′′. Note
that one occurrence of C has been removed and Y ′ has been replaced with
Y ′′.
No further splicing can be done in the ﬁrst component involving the
obtained string Xx1Ci−1Y ′′, so it has to be passed to another compo-
nent.
The only possibility which does not block the system is to start
working in the second component.
Two splicings can be done here,
at the ends of the string:
(X′C|Z, X|x1Ci−1Y ′′) ⊢X′Cx1Ci−1Y ′′ and
(X′Cx1Ci−1|Y ′′, Z|Y ′) ⊢X′Cx1Ci−1Y ′. In this way, one occurrence of C
has been introduced in the left end of the string and the markers at the ends
of the string have become X′, Y ′. No further splicing can be done here. We
have two cases:
(1) If the string is passed to the ﬁrst component, then, after using the
rule X′#$X#Z, we obtain the string XCx1Ci−1Y ′ and again we have to
cut one occurrence of C from its right hand and to replace Y ′ with Y ′′. As
above, the string must be passed to the second component and the process
can be iterated. In this way, any number of occurrences of the symbol C can
be removed from the right hand end of the string and reintroduced in the left
hand end.
(2) If the string is passed to the third component, then again two splicings
will be done here, at the ends of the string. Assume that we start working in
(A3, R3) on a string of the form X′Cjx1CkY ′ for some j, k ≥0. The symbol
Y ′ is replaced by Y .
If k ≥1, then the rule #CY $Z#CY can be used
forever. Therefore, we must start from a string X′Cjx1Y ′. If we perform
a splicing (Xαs|Z, X′Cs|Cj−sx1Y ) ⊢XαsCj−sx1Y with j > s, then again
the system is blocked: the rule XαsC#$XαsC#Z can be used indeﬁnitely.
A correct continuation is possible only when we have s = j; this leads to
the string Xαsx1Y . In this way, all occurrences of C were replaced by the
corresponding symbol in N ∪T ∪{B}. This means that exactly the symbol
αi which has been removed from the right hand end of the string has been
reintroduced in the left hand end (with the notation above, i = j = s). Thus,

Distributed H Systems
105
any circular permutation of the string can be obtained.
The string produced by the third component is of the form XzY , hence it
can only be processed by the ﬁrst component, where it is possible to simulate
other rules in P in the end of z.
By iterating this rotate-and-simulate procedure, we can simulate in Γ any
derivation in G.
Note that B is circulated as any other symbol. Initially, B is introduced
in the left hand of S, the axiom of G. Because at every moment exactly one
occurrence of B is present, it indicates the actual beginning of the sentential
form of G simulated by Γ in a permuted form: if the string produced by Γ is
Xz1Bz2Y , possibly with X, Y replaced with some primed versions of them,
then z2z1 is a sentential form of G.
After receiving a string X′Cjx1Y ′, j ≥1, the third component can also
use the rules of the form XB#Z$XC#, #Y ′$Z#Y . If both these rules are
used, then we get the string XBCj−1x1Y . If j −1 ≥1, then the system
is blocked by the rule XBC#$XBC#Z.
Therefore, the string must be
XBx1Y . The ﬁrst component is blocked if receiving this string; the second
one can work one step, removing the preﬁx XB. The obtained string has
to be taken by another component.
The third one cannot modify it, the
ﬁrst one will remove the symbol Y from its end. If the obtained string is
terminal, then it is an element of the generated language. Because the unique
occurrence of the symbol B has been removed when placed in the leftmost
position, it follows that the string is in the same circular permutation as the
corresponding sentential form of G.
Finally, assume that the third component produces a string with only one
barred symbol. If this string is of the form XBxY , then no component can
process it without blocking the system: the ﬁrst component contains the rule
X#$X#Z, the second one contains the rule #Y $Z#Y . If the string is of
the form XBxY , then it can reach both the ﬁrst and the second components.
In the ﬁrst component, the string XBxY will lose the symbol Y , then it
has to be moved to the second component. Here, a symbol C is added in the
left end, producing X′CBx. This string can go back to the ﬁrst component;
iterating these steps, we can produce X′CiBx for some i ≥1. Eventually,
the string will be processed by the third component, and a string XαiBx is
produced. If αi ̸= B, then B can never be removed: it cannot be moved near
X, because no rotation phase is possible (the symbol Y is no longer present).
If αi = B (hence i = 1), then XB can be removed, but we get Bx, which is
not a terminal string.
If the string XBxY arrives in the second component, then again we can
introduce an occurrence of C in its left end, the obtained string X′CBxY
is then processed by the ﬁrst component, which removes Y , and we have
returned to a situation as above: no terminal string can be produced, because
the symbol B cannot be removed.
In conclusion, every derivation in G can be simulated in Γ and, conversely,

106
DNA Computing
if a terminal string is produced by the system Γ, then it is an element of L(G).
That is, L(G) = L(Γ).
✷
We do not know which of the inclusions SDH1 ⊆SDH2 ⊆SDH3 are
proper. In what concerns the number of splicing rules in each component,
the following result, whose proof is omitted, holds:
Theorem 2.21 For each type-0 grammar G = (N, T, S, P) we can construct
a sequential distributed H system Γ of degree 2 · card(N ∪T) + card(P) + 10,
with each component containing at most three splicing rules, and such that
L(G) = L(Γ).
2.12
Bibliographical Notes
From the many good books in molecular biology and genetic engineering
which are available we only mention [6], [81], [274].
Good introductions
to the biochemistry of DNA are [142] and [266].
A presentation of DNA
structure and operations with implicit and explicit orientation to computer
science and DNA computing can be found in the ﬁrst chapter of [224].
Theorem 2.2 is from [92]. The connection between DNA structure and
the twin-shuﬄe language was ﬁrst observed in [257] and then discussed in
various places: [224], [225], [264], etc.
In Sections 2.4–2.8 we have given a series of references, which we do not
recall here.
The idea of computing by carving was proposed in [214] and then pre-
sented in [181]. Iterated gsm’s were investigated in [207], [255], [299].
The splicing operation was introduced in [127], while the extended H
systems were ﬁrst considered in the explicit form in [223]. The Regularity
Preserving Lemma is due to [240]; the complete proof also appears in [224].
A more general result is given in [242]: each abstract family of languages
(AFL) is closed under iterated splicing with respect to a ﬁnite set of splicing
rules. This proof is also given in [129]. The Basic Universality Lemma is
from [209].
In Section 2.10 we have given some references about the mentioned results;
further details can be found in [224].
A chapter about distributed H systems can be found in [224], where also a
further class of systems is presented (while the sequential distributed H sys-
tems are not discussed in [224]), the so-called two-level distributed H systems,
introduced in [213] (systems of this type with three components characterize
CE, but no result is known about the size of the components).
Splicing grammar systems were introduced in [74]; Theorem 2.15 is from
[210]. Further results can be found in [111].
Communicating distributed H systems were introduced in [70] and inves-
tigated in [211], [306], [243]; the proof of Theorem 2.16 can be found in [216].
Theorem 2.17 is from [215].

Bibliographical Notes
107
The time-varying distributed H systems are introduced in [211]; Theorem
2.18 is from [205].
Theorem 2.19 is from [216].
In [183] it is announced
that time-varying H systems with two components generate each computably
enumerable languages.
The sequential distributed H systems were introduced in [184]; Theorems
2.20 and 2.21 are from this paper.
Another computability model based on the operation of annealing is pro-
posed in [303] and further elaborated in [304]; it is based on a careful analysis
of the proof of Geﬀert normal form (Theorem 1.2 (ii)), [110].

108
DNA Computing

Chapter 3
Membrane Computing
If any entity should be thought of
as a governor of cellular activity,
then this should certainly be the membrane.
All major activities of cells
are topologically connected to membrane.
J. Hoﬀmeyer, 1998
In this chapter we introduce a new computability model which is biochem-
ically inspired, but not necessarily using DNA molecules and DNA operations.
It is a general distributed model, highly parallel, based on the notion of a
membrane structure. Such a structure consists of several cell-like membranes,
recurrently placed inside a unique “skin” membrane. A plane representation
is a Venn diagram without intersected sets and with a unique superset. In the
regions delimited by the membranes there are placed objects. These objects
are assumed to evolve: each object can be transformed into other objects,
can pass through a membrane, or can dissolve the membrane in which it is
placed. A priority relation between evolution rules can be considered. The
evolution is done in parallel for all objects able to evolve. In this way, we ob-
tain a computing device (we call it a P system): start with a certain number
of objects in certain membranes and let the system evolve; if it will halt (no
object can further evolve), then the computation is ﬁnished, with the result
given as the number of objects in a speciﬁed membrane. If the development
of the system goes for ever, then the computation fails to have an output.
Also membrane division can be considered.
In this way, an enhanced
parallelism is obtained, which can be very useful from the point of view of
computational complexity (we will see that SAT can be solved in linear time
in such a framework).
We will consider here several variants of P systems, giving proofs only
for some basic results (in general, dealing with the power of these systems).
109

110
Membrane Computing
We enter into some technical details, because we want to let the reader to be
acquainted with this very recently developed topic.
3.1
P Systems with Labelled Membranes
We start from the observation that life “computes” not only at the genetic
level, but also at the cellular level. More generally, any non-trivial biolog-
ical system is a hierarchical construct, composed of several “organs” which
are well deﬁned and delimited from the neighbouring organs, which evolve
internally and also cooperate with the other organs in order to keep alive the
system as a whole; an intricate ﬂow of materials, energy, and information
underlies the functioning of such a system.
At a more speciﬁc level with respect to the models we are going to de-
ﬁne, important to us is the fact that the parts of a biological system are
well delimited by various types of membranes, in the broad sense of the term,
starting from the membranes which delimit the various intra-cell components,
going to the cell membrane and then to the skin of organisms, and ending
with more or less virtual “membranes” which delimit, for instance, parts of
an ecosystem. In very practical terms, in biology and chemistry one knows
membranes which keep together certain chemicals and allow other chemicals
to pass, in a selective manner, sometimes only in one direction (for instance,
through protein channels placed in membranes). Membranes delimiting sub-
systems of a symbol manipulating system are also considered in the logical
framework to the so-called metabolic systems, as deﬁned in [180], or in the
so-called chemical abstract machine, introduced in [32].
Formalizing the previous intuitive ideas, we now introduce the basic struc-
tural ingredient of the computing devices we will deﬁne later: membrane
structures.
Let us consider ﬁrst the language MS over the alphabet {[, ]}, whose
strings are recurrently deﬁned as follows:
1. [ ] ∈MS;
2. if µ1, . . . , µn ∈MS, n ≥1, then [µ1 . . . µn] ∈MS;
3. nothing else is in MS.
Consider now the following relation over the elements of MS: x ∼y if and
only if we can write the two strings in the form x = µ1µ2µ3µ4, y = µ1µ3µ2µ4,
for µ1µ4 ∈MS and µ2, µ3 ∈MS (two pairs of parentheses which are neigh-
bours at the same level are interchanged, together with their contents). We
also denote by ∼the reﬂexive and transitive closure of the relation ∼. This
is clearly an equivalence relation. We denote by MS the set of equivalence
classes of MS with respect to this relation. The elements of MS are called
membrane structures.

P Systems with Labelled Membranes
111
Each matching pair of parentheses [, ] appearing in a membrane structure
is called a membrane. The number of membranes in a membrane structure
µ is called the degree of µ and denoted by deg(µ). The external membrane
of a membrane structure µ is called the skin membrane of µ. A membrane
which appears in µ ∈MS in the form [ ] (no other membrane appears inside
the two parentheses) is called an elementary membrane.
The depth of a membrane structure µ, denoted by dep(µ), is deﬁned re-
currently as follows:
1. if µ = [ ], then dep(µ) = 1;
2. if µ = [µ1 . . . µn], for some µ1, . . . , µn ∈MS,
then dep(µ) = max{dep(µi) | 1 ≤i ≤n} + 1.
A membrane structure can be represented in a natural way as a Venn
diagram. This makes clear the fact that the order of membrane structures
of the same level in a larger membrane structure is irrelevant; what matters
is the topological structure, the relationships between membranes. In the
sequel we will make an extensive use of such a representation.
The Venn representation of a membrane structure µ also makes clear the
notion of a region in µ: any closed space delimited by membranes is called
a region of µ. It is clear that a membrane structure of degree n contains n
internal regions, one associated with each membrane. We also use to speak
about the outer region, the whole space outside the skin membrane.
Figure 3.1 illustrates some of the notions mentioned above.
✬
✫
✩
✪
✬
✫
✩
✪
✛
✚
✘
✙
✛
✚
✘
✙

✒
✏
✑

✒
✏
✑
✤
✣
✜
✢

✒
✏
✑

✒
✏
✑
✡
✡
✡✡✢
❅
❅❅
❘




✠
Membrane
❆❆
Skin
Elementary membrane
Membrane
Region✟✟
✯
❍❍❍❍❍❍❍
❍
❥
❅
❅
❅
❅
❘
Figure 3.1: A membrane structure.
We now make one more step towards the deﬁnition of a computing device,
by adding objects to a membrane structure.

112
Membrane Computing
Let U be a ﬁnite set whose elements are called objects.
Consider a membrane structure µ of degree n, n ≥1, with the membranes
labelled in a one-to-one manner, for instance, with the numbers from 1 to n.
In this way, also the regions of µ are identiﬁed by the numbers from 1 to n.
If a multiset Mi : U −→N is associated with each region i of µ, 1 ≤i ≤n,
then we say that we have a super-cell (note that we do not allow inﬁnite
multiplicities of objects in U).
Any multiset Mi mentioned above can be empty. In particular, all of
them can be empty, that is, any membrane structure is a super-cell. On the
other hand, each individual object can appear in several regions, in several
copies in each of them.
Several notions deﬁned for membrane structures are extended in the nat-
ural way to super-cells: degree, depth, region, etc.
The multiset corresponding to a region of a super-cell (in particular, it
can be an elementary membrane) is called the contents of this region. The
total multiplicities of the elements in an elementary membrane m (the sum
of their multiplicities) is called the size of m and is denoted by size(m).
If a membrane m′ is placed in a membrane m such that m and m′ con-
tribute to delimiting the same region (namely, the region associated with m),
then all objects placed in the region associated with m are said to be adja-
cent to membrane m′ (so, they are immediately “outside” membrane m′ and
“inside” membrane m).
A super-cell can be described by a Venn diagram where both the mem-
branes and the objects are represented (in the case of the objects, taking care
of multiplicities).
We are now ready to introduce the subject of our investigation, a com-
puting mechanism essentially designed as a distributed parallel machinery,
having as the underlying structure a super-cell. The basic additional feature
is the possibility of objects to evolve, according to certain rules. Another
feature refers to the deﬁnition of the input and the output (the result) of a
computation.
A P system is a super-cell provided with evolution rules for its objects
and with a designated output region.
More formally, a P system of degree m, m ≥1, is a construct
Π = (V, T, C, µ, w1, . . . , wm, (R1, ρ1), . . . , (Rm, ρm), i0),
where:
(i) V is an alphabet; its elements are called objects;
(ii) T ⊆V (the output alphabet);
(iii) C ⊆V, C ∩T = ∅(catalysts);

P Systems with Labelled Membranes
113
(iv) µ is a membrane structure consisting of m membranes, with the mem-
branes and the regions labelled in a one-to-one manner with elements
of a given set Λ; in this section we use the labels 1, 2, . . . , m;
(v) wi, 1 ≤i ≤m, are strings representing multisets over V associated with
the regions 1, 2, . . . , m of µ;
(vi) Ri, 1 ≤i ≤m, are ﬁnite sets of evolution rules over V associated
with the regions 1, 2, . . . , m of µ; ρi is a partial order relation over Ri,
1 ≤i ≤m, specifying a priority relation among rules of Ri.
An evolution rule is a pair (u, v), which we will usually write in the
form u →v, where u is a string over V and v = v′ or v = v′δ, where v′
is a string over
{ahere, aout, ainj | a ∈V, 1 ≤j ≤m},
and δ is a special symbol not in V . The length of u is called the radius
of the rule u →v. (The strings u, v are understood as representations
of multisets over V , in the natural sense.)
(vii) i0 is either a number between 1 and m and then it speciﬁes the output
membrane of Π, or it is equal to ∞, and then it indicates that the
output is read in the outer region.
When presenting the evolution rules, the indication “here” is in general
omitted. Remember that the multiset associated with a string w is denoted
by M(w).
If Π contains rules of radius greater than one, then we say that Π is
a system with cooperation.
Otherwise, it is a non-cooperative system.
A
particular class of cooperative systems is that of catalytic systems: the only
rules of a radius greater than one are of the form ca →cv, where c ∈C, a ∈
V −C, and v contains no catalyst; moreover, no other evolution rules contain
catalysts (there is no rule of the form c →v or a →v1cv2, for c ∈C).
A system is said to be propagating if there is no rule which diminishes the
number of objects in the system (note that this can be done by “erasing”
rules, but also by sending objects out of the skin membrane).
Of course, any of the multisets M(w1), . . . , M(wn) can be empty (that is,
any wi can be equal to λ) and the same is valid for the sets R1, . . . , Rn and
their associated priority relations ρi.
The components µ and w1, . . . , wm of a P system deﬁne a super-cell.
Graphically, we will draw a P system by representing its underlying super-
cell, and also adding the rules to each region, together with the corresponding
priority relation. In this way, we can have a complete picture of a P system,
much easier to understand than a symbolic description.
The (m + 1)-tuple (µ, w1, . . . , wm) constitutes the initial conﬁguration of
Π. In general, any sequence (µ′, w′
i1, . . . , w′
ik), with µ′ a membrane struc-
ture obtained by removing from µ all membranes diﬀerent from i1, . . . , ik (of

114
Membrane Computing
course, the skin membrane is not removed), with w′
j strings over V , 1 ≤j ≤k,
and {i1, . . . , ik} ⊆{1, 2, . . . , m}, is called a conﬁguration of Π.
An important detail that should be noted is that the membranes preserve
the initial labeling in all subsequent conﬁgurations; in this way, the corre-
spondence between membranes, multisets of objects, and sets of evolution
rules is well speciﬁed by the subscripts of these elements.
A more compact and easy to read writing of a conﬁguration, avoiding the
use of subscripts for multisets and sets above is that where the objects of
the multisets are written (using multisets or in the form of a string) directly
in the region to which they belong; similarly, the rules are written in the
region where they can act. This is in good correspondence with the graphical
representation of a P system and we will use it especially for conﬁgurations
where many components are empty.
For two conﬁgurations
C1 = (µ′, w′
i1, . . . , w′
ik), C2 = (µ′′, w′′
j1, . . . , w′′
jl),
of Π we write C1 =⇒C2, and we say that we have a transition from C1 to
C2, if we can pass from C1 to C2 by using the evolution rules appearing in
Ri1, . . . , Rik in the following manner (rather than a completely cumbersome
formal deﬁnition we prefer an informal one, explained by examples).
Consider a rule u →v in a set Rit. We look to the region of µ′ associated
with the membrane it. If the objects mentioned by u, with the multiplici-
ties speciﬁed by u, appear in w′
it (that is, the multiset M(u) is included in
M(w′
it)), then these objects can evolve according to the rule u →v. The
rule can be used only if no rule of a higher priority exists in Rit and can be
applied at the same time with u →v. More precisely, we start to examine
the rules in the decreasing order of their priority and assign objects to them.
A rule can be used only when there are copies of the objects whose evolution
it describes and which were not “consumed” by rules of a higher priority and,
moreover, there is no rule of a higher priority, irrespective which objects it
involves, which is applicable at the same step. Therefore, all objects to which
a rule can be applied must be the subject of a rule application. All objects
in u are “consumed” by using the rule u →v, that is, the multiset M(u) is
subtracted from M(w′
it).
The result of using the rule is determined by v. If an object appears in v in
the form ahere, then it will remain in the same region it. If an object appears
in v in the form aout, then a will exit the membrane it and will become an
element of the region immediately outside it (thus, it will be adjacent to the
membrane it from which it was expelled). In this way, it is possible that an
object leaves the system: if it goes outside the skin of the system, then it
never comes back. If an object appears in the form ainq, then a will be added
to the multiset M(w′
q), providing that a is adjacent to the membrane q. If
ainq appears in v and membrane q is not one of the membranes delimiting
“from below” the region it, then the application of the rule is not allowed.

P Systems with Labelled Membranes
115
If the symbol δ appears in v, then membrane it is removed (we say dis-
solved) and at the same time the set of rules Rit (and its associated priority
relation) is removed. The multiset M(w′
it) is added (in the sense of multi-
sets union) to the multiset associated with the region which was immediately
external to membrane it. We do not allow the dissolving of the skin, be-
cause this means that the super-cell is lost and we no longer have a correct
conﬁguration of the system.
All these operations are performed in parallel, for all possible applicable
rules u →v, for all occurrences of multisets u in the region associated with
the rules, for all regions at the same time.
No contradiction appears be-
cause of multiple membrane dissolving, or because simultaneous appearance
of symbols of the form aout and δ. If at the same step we have aini outside
a membrane i and δ inside this membrane, then, because of the simultaneity
of performing these operations, again no contradiction appears: we assume
that a is introduced in membrane i at the same time when it is dissolved,
thus a will remain in the region placed outside membrane i; that is, from the
point of view of a, the eﬀect of aini in the region outside membrane i and δ
in membrane i is ahere.
Remark 3.1 The mode of evolving of objects in a super-cell provided with
evolution rules as described above can be interpreted in the following – ideal-
ized – biochemical way. We have an organism, delimited by a skin (the skin
membrane). Inside, there are organs and free molecules, organized hierar-
chically. The molecules and the organs ﬂoat randomly in the “cytoplasmic
liquid” of each membrane. Under speciﬁc conditions, the molecules evolve,
alone or with the help of certain catalysts; these, of course, are not modiﬁed
by the reactions. This is done in parallel, synchronously for all molecules (a
universal clock is assumed to exist). The new molecules can remain in the
same region where they have appeared, or can pass through the membranes
delimiting this space, selectively. Some reactions not only modify molecules,
but also break membranes. (We may imagine that certain chemicals are pro-
duced which break/dissolve the membrane.) When a membrane is broken,
the molecules previously placed inside it will remain free in the larger space
newly created, but the evolution rules of the former membrane are lost. The
assumption is that the reaction conditions from the previous membrane are
modiﬁed by the disappearance of the membrane and in the newly created
space only the rules speciﬁc to this space can act. Of course, when the ex-
ternal membrane is broken, then the organism ceases to exist, its organs fall
apart.
The priority among evolution rules can be interpreted as corresponding to
diﬀerent reactives of the involved chemicals. Moreover, we assume that the
reactions also consume some resources diﬀerent from objects (for example,
energy); that is why when a rule of a higher priority is used, no rule of a
lower priority can be used.
✷
A sequence of transitions between conﬁgurations of a given P system

116
Membrane Computing
Π is called a computation with respect to Π. A computation is successful
if and only if it halts, that is, there is no rule applicable to the objects
present in the last conﬁguration, and, if the system is provided with an output
region i0 ̸= ∞, then the membrane i0 is present as an elementary one in the
last conﬁguration of the computation.
(Note that the output membrane
was not necessarily an elementary one in the initial conﬁguration.)
The
result of a successful computation can be the total number of objects present
in the output membrane of the halting conﬁguration, or ΨT (w), where w
describes the multiset of objects from T present in the output membrane in
a halting conﬁguration (ΨT is the Parikh mapping associated with T), or a
language, as it will be deﬁned immediately. The set of vectors ΨT (w) for
w describing the multiset present in the output membrane of a system Π
in a halting conﬁguration is denoted by Ps(Π) (from “Parikh set”) and we
say that it is generated by Π in the internal mode. When we are interested
only in the number of objects present in the output membrane in the halting
conﬁgurations of Π, we denote by N(Π) the set of numbers “generated” in
this way.
When no output membrane is speciﬁed (i0 = ∞), that is, we observe
the system from outside, then we collect the objects ejected from the skin
membrane, in the order they are ejected. Using these objects, we form a
string. When several objects are ejected at the same time, any permutation
of them is considered. In this way, a string or a set of strings is associated
with each computation, that is, a language is associated with the system.
We denote by L(Π) the language computed by Π in the way described
above. We say that it is generated by Π in the external mode.
Remark 3.2 A natural representation of the P systems architecture can be
obtained in terms of trees, a fact also suggested by the way of writing a
membrane structure by means of parentheses. Each membrane is associated
with a node. If membrane i is placed inside membrane j and no intermediate
membrane exists, then an arc from membrane j to membrane i is considered.
In this way, the skin membrane is associated with the root of the tree, while
the elementary membranes are associated with the leaves of the tree. To each
node we also associate a multiset of objects, a set of evolution rules, and a
priority relation. Thus, a node is fully described by a quadruple (i, Mi, Ri, ρi).
Using a rule means changing the multiset Mi and, possibly, the multisets
placed in the parent and the children nodes of node i: ahere means adding a
to Mi (after subtracting the multiset in the left hand side of the rule from
the current multiset), aout means to send a to the parent node, ainj means
to send a to the child j, providing that it exists (otherwise the rule cannot
be applied). Dissolving a membrane i means now removing node i; its rules
are removed, its objects are passed to the parent node; all the children of i
become children of the parent node of i. The root cannot be removed.
In this new framework, a computation means the circulation of objects in
the tree, with the change of the tree itself due to dissolving actions. In the

Examples
117
internal style, the result of a computation is collected in a leaf node. In the
external style, the result is observed in the root (and it is a partially ordered
sequence of objects which leave the P system through its skin).
In this book we do not make use of this tree representation;
al-
though mathematically appealing, it loses the biochemical intuition behind
P systems.
✷
3.2
Examples
We will consider a series of examples, in order to clarify the way of working
of P systems.
Example 3.1 Consider the P system of degree 4
Π1 = (V, T, C, µ, w1, w2, w3, w4, (R1, ρ1), (R2, ρ2), (R3, ρ3), (R4, ρ4), 4),
V = {a, b, b′, c, f},
T = {c},
C = ∅,
µ = [1[2[3 ]3[4 ]4]2]1,
w1 = λ, R1 = ∅, ρ1 = ∅,
w2 = λ, R2 = {b′ →b, b →bcin4, r1 : ff →af, r2 : f →aδ},
ρ2 = {r1 > r2},
w3 = af, R3 = {a →ab′, a →b′δ, f →ff}, ρ3 = ∅,
w4 = ∅, R4 = ∅, ρ4 = ∅.
The initial conﬁguration of the system is presented in Figure 3.2.
No object is present in membrane 2, hence no rule can be applied here.
The only possibility is to start in membrane 3, using the objects a, f, present
in one copy each. Using the rules a →ab′, f →ff, in parallel for all oc-
currences of a and f currently available, after n steps, n ≥0, we get n
occurrences of b′ and 2n occurrences of f. In any moment, instead of a →ab′
we can use a →b′δ (note that we always have only one copy of a). In that
moment we have n + 1 occurrences of b′ and 2n+1 occurrences of f and we
dissolve membrane 3. The obtained conﬁguration is
[1 [2 b′n+1f 2n+1, b′ →b, b →b(c, in4),
r1 : ff →af, r2 : f →aδ, r1 > r2, [4 ]4 ]2 ]1.
The rules of the former active membrane 3 are lost, the rules of membrane
2 are now active.
Due to the priority relation, we have to use the rule
ff →af as much as possible. In one step, we pass from b′n+1 to bn+1, while
the number of f occurrences is divided by two. In the next step, from bn+1,
n + 1 occurrences of c are introduced in membrane 4 (each occurrence of the

118
Membrane Computing
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
#
✧
✥
✦
3
2
1
4
af
a →ab′
a →b′δ
f →ff
b′ →b
b →bcin4
(ff →af) > (f →aδ)
Figure 3.2: A P system generating n2, n ≥1.
symbol b introduces one occurrence of c). At the same time, the number of f
occurrences is divided again by two. We can continue. At each step, further
n + 1 occurrences of c are introduced in the output membrane. This can be
done n + 1 steps: n times when the rule ff →af is used (thus diminishing
the number of f occurrences to one), and one when using the rule f →aδ (it
may now be used). In this moment, membrane 2 is dissolved, which entails
the fact that its rules are removed. No further step is possible. The obtained
conﬁguration is
[1 a2n+1bn+1, [4 c(n+1)2 ]4 ]1.
Consequently, N(Π1) = {m2 | m ≥1}.
Note that the system Π1 is propagating (no object is removed by a rule
or “lost” in the outer region) and it has only one cooperative rule.
The previous P system is a generative one: it starts from a unique initial
conﬁguration and, because of the non-deterministic evolution, it collects in
its output membrane diﬀerent values of n2, n ≥1. A variant of interest could
be a system just computing n2 for a given n. We leave to the reader the task
of constructing such a system.
Example 3.2 In order to illustrate the possibly intricate work of a P system,
we now consider a system which has a decidability task: we introduce in the
input conﬁguration two numbers, n and k, and ask whether or not n is a
multiple of k. In the aﬃrmative case, we will ﬁnish with one object in the
output membrane; in the negative case we will have two objects in the output
membrane.

Examples
119
The system is the following (of degree 3):
Π2 = ({a, c, c′, d, †}, {a}, ∅, µ, λ, anckd, a, (R1, ∅), (R2, ∅), (∅, ∅), 3),
µ = [1[2 ]2[3 ]3]1,
R1 = {a →a, dcc′ →ain3},
R2 = {ac →c′, ac′ →c, d →d, d →dδ}.
The structure of Π2 is better seen in Figure 3.3.
In membrane 2 we
subtract k from n, repeatedly (by the rules ac →c′, ac′ →c: at each
step, k copies of a disappear, while c is reproduced, primed or not primed,
alternating the priming from a step to another one). At any time, the symbol
d will introduce δ, and this membrane is dissolved.
✬
✫
✩
✪
✬
✫
✩
✪
#
✧
✥
✦
1
2
3
a
anckd
ac →c′
ac′ →c
d →d
d →dδ
a →a
dcc′ →ain3
Figure 3.3: A P system deciding whether k divides n.
If an occurrence of a arrives in membrane 1, then the computation never
ends. Therefore, we have to dissolve membrane 2 only after exhausting the n
occurrences of a. If n is a multiple of k – and only in this case – then we never
have both occurrences of c and of c′ present simultaneously in membrane 2 (or
in membrane 1, after dissolving membrane 2). Therefore, the rule dcc′ →ain3
is used in membrane 1 if and only if n is not a multiple of k.
Note that this rule can be used at most once, because we have only
one occurrence of d, and that the computation stops after using the rule
dcc′ →ain3. Similarly, the computation stops after using the rule d →dδ in
membrane 2 and we do not have both copies of c and of c′ (no further step
can be performed).
In conclusion, if the computation is correctly ﬁnished, then the output
membrane contains two objects if and only if n is not a multiple of k (in the
opposite case, we have here only one object).

120
Membrane Computing
We now pass to considering P systems with an external output.
Example 3.3 Consider the system with catalysts
Π3 = ({a, c, d, e}, {a, d, e}, {c}, [1[2 ]2]1, ca, λ, (R1, ∅), (R2, ∅), ∞),
R1 = {a →aaoutdin2, a →ain2, cd →cdouteout},
R2 = {a →δ}.
For each copy of a which is sent outside the system, an occurrence of d is
sent to the inner membrane, where it waits for the symbol a which determines
the membrane dissolution. When the copies of d are released from the inner
membrane, the catalyst c sends out one by one pairs de, ed. Thus, in our
string we get consecutive substrings of the form de, ed, that is, the generated
language is
L(Π3) = {anx1x2 . . . xn | n ≥1, xi ∈{de, ed}, 1 ≤i ≤n}.
The use of the catalyst is essential. If we would use non-cooperative rules
of the form d →d′dout, d′ →eout, where d′ is a new symbol, then they must
be used at the same time for all occurrences of d, hence we will obtain a
string of the form andnen.
Example 3.4 The system
Π4 = ({a, b}, {a, b}, ∅, [1 ]1, a, (R1, ∅), ∞),
R1 = {a →aaoutb, a →a, b →b, a →aoutb, b →bout},
generates the Dyck language over {a, b}, that is, the language generated by
the context-free grammar G = ({S}, {a, b}, S, {S →λ, S →SS, S →aSb}).
(Remember that when comparing the power of two mechanisms, the empty
string is ignored.) We leave to the reader the task of checking this assertion.
Example 3.5 The system
Π5 = ({a, b}, {a, b}, ∅, [1[2 ]2]1, λ, ab, (R1, ∅), (R2, ∅), ∞),
R1 = {a →aout, b →bout},
R2 = {a →aa, b →bbb, a →aaδ},
generates the language
L(Π5) = {x ∈{a, b}∗| |x|a = 2n, |x|b = 3n, n ≥0}.
In the inner membrane one produces 2n copies of a and 3n copies of b, for
any n ≥0. When membrane 2 is dissolved, all these symbols are left free in
membrane 1 and sent out at the same time. Thus, any sequence of them is
a valid output string. (Note that this language is not an ET0L language.)

The Power of P Systems
121
Example 3.6 Let p1, p2, . . . , pm be diﬀerent prime numbers and consider
the system (of degree m + 1):
Π6 = ({a}, {a}, ∅, [1[2[3 . . . [m+1 ]m+1 . . . ]3]2]1, λ, . . . , λ, a,
(R1, ∅), . . . , (Rm+1, ∅), ∞),
R1 = {a →aout},
Ri+1 = {a →api, a →apiδ}, 1 ≤i ≤m.
The reader can easily verify that we obtain
L(Π6) = {api1
1 pi2
2 ...pim
m | ij ≥1, 1 ≤j ≤m}.
3.3
The Power of P Systems
First, let us ﬁx some notations. The set of Parikh images of languages in a
family FL is denoted by PsFL, while the family of permutation closures of
languages in a family FL is denoted by pFL.
The family of languages L(Π) generated (in the external mode) by P
systems with priority, catalysts, and the membrane dissolving action, and
of degree at most m, m ≥1, is denoted by LPm(Pri, Cat, δ); when one
of the features α ∈{Pri, Cat, δ} is not present, we replace it with nα.
The union of all families LPm(α, β, γ), m ≥1, is denoted by LP∗(α, β, γ),
α ∈{Pri, nPri}, β ∈{Cat, nCat}, γ ∈{δ, nδ}. When collecting the result of
a computation in an internal output membrane, we denote by PsPm(α, β, γ)
the family of vectors Ps(Π) as deﬁned in Section 3.1, with the same meaning
for the involved parameters. When the output membrane is already elemen-
tary in the initial conﬁguration, we write PsP ′ instead of PsP.
The following relations directly follow from the deﬁnitions (note that when
we have only one membrane, the dissolving action is useless):
Lemma 3.1 (i) XPm(α, β, γ) ⊆XPm+1(α, β, γ), for all m ≥1 and for all
possible α, β, γ, X ∈{Ps, L}. Moreover, XP1(α, β, δ) = XP1(α, β, nδ).
(ii) XPm(α′, β′, γ′) ⊆XPm(α, β, γ), for all m ≥1 and all possible α, β, γ
such that α′, β′, γ′ are either equal to α, β, γ or to nα, nβ, nγ, respectively,
when n is not already present, X ∈{Ps, L}.
(iii) PsP ′
m(α, β, γ) ⊆PsPm(α, β, γ), for all possible m and α, β, γ.
It is interesting to note that there are values of m, α, β, γ for which
the inclusion in (iii) is proper: it was pointed out in [76] that PsE0L ⊆
PsP2(nPri, nCat, δ), but we do not have PsE0L ⊆PsP ′
2(nPri, nCat, δ).
Thus, at least for this case we do not have a normal form result, stating that
it is enough to have the output membrane elementary in the initial conﬁgu-
ration. We do not know whether or not this is true also in other cases. In
view of the following results, this problem is worth investigating.

122
Membrane Computing
We are mainly interested in characterizations of computably enumerable
languages, so most of the results mentioned below which are not of this type
are given without proofs.
Lemma 3.2 pLPm(α, β, γ) ⊆LPm(α, β, γ), for all α, β, γ and m ≥1.
Consider ﬁrst the relationships between internal and external outputs.
Theorem 3.1 (The Bridge Theorem) (i) PsP ′
m(α, β, γ) ⊆PsLPm(α, β, γ),
for all m ≥1 and all possible α, β, γ.
(ii) PsPm(α, β, γ) ⊆PsLPm(Pri, β, γ), for all m ≥1 and all possible
α, β, γ.
(iii) For all m ≥1 and all possible α, β, γ, we have PsLPm(α, β, γ) ⊆
PsP ′
m′(α, β, γ), for m′ = m + 1. If γ = nδ, then m′ = m.
The reason why we need a further membrane in the general case men-
tioned in (iii) is the fact that the membrane dissolving is a dynamic event,
from one computation to another one diﬀerent membranes can be dissolved.
In this way, we cannot ﬁx in advance a membrane as an output membrane.
We will also see later that the question whether or not a given membrane
is ever dissolved during the computations of a P system is not algorithmically
solvable in the case of general systems (but it can be solved algorithmically
in the case of systems of type (nPri, nCat, δ)).
When “powerful enough” ingredients are used, characterizations of com-
putably enumerable relations are obtained by using P systems of a simple
structure. In view of the bridge between the internal and the external mode
of collecting the output given by Theorem 3.1, we state this basic result only
for the internal mode. Because this is an important result, we also give its
proof.
This proof will use the characterization of computably enumerable lan-
guages by means of matrix grammars with appearance checking.
Such a grammar is a construct G = (N, T, S, M, F), where N, T are
disjoint alphabets, S ∈N, M is a ﬁnite set of sequences of the form
(A1 →x1, . . . , An →xn), n ≥1, of context-free rules over N ∪T (with
Ai ∈N, xi ∈(N ∪T)∗, in all cases), and F is a set of occurrences of rules in
M (we say that N is the nonterminal alphabet, T is the terminal alphabet,
S is the axiom, while the elements of P are called matrices).
For w, z ∈(N ∪T)∗we write w =⇒z if there is a matrix (A1 →x1,
. . . , An →xn) in M and the strings wi ∈(N ∪T)∗, 1 ≤i ≤n + 1, such
that w = w1, z = wn+1, and, for all 1 ≤i ≤n, either wi = w′
iAiw′′
i , wi+1 =
w′
ixiw′′
i , for some w′
i, w′′
i ∈(N ∪T)∗, or wi = wi+1, Ai does not appear in
wi, and the rule Ai →xi appears in F. (The rules of a matrix are applied
in order, possibly skipping the rules in F if they cannot be applied; we say
that these rules are applied in the appearance checking mode.) If F = ∅, then
the grammar is said to be without appearance checking (and F is no longer
mentioned).

The Power of P Systems
123
We denote by =⇒∗the reﬂexive and transitive closure of the relation =⇒.
The language generated by G is deﬁned by L(G) = {w ∈T ∗| S =⇒∗w}.
The family of languages of this form is denoted by MATac. When we use only
grammars without appearance checking, then the obtained family is denoted
by MAT.
It is known that CF ⊂MAT ⊂MATac = CE. Further details about
matrix grammars can be found in [75] and in [258].
A matrix grammar G = (N, T, S, M, F) is said to be in the binary normal
form if N = N1 ∪N2 ∪{S, †}, with these three sets mutually disjoint, and
that the matrices in M are of one of the following forms:
1. (S →XA), with X ∈N1, A ∈N2,
2. (X →Y, A →x), with X, Y ∈N1, A ∈N2, x ∈(N2 ∪T)∗,
3. (X →Y, A →†), with X, Y ∈N1, A ∈N2,
4. (X →λ, A →x), with X ∈N1, A ∈N2, and x ∈T ∗.
Moreover, there is only one matrix of type 1 and F consists exactly of all
rules A →† appearing in matrices of type 3. The symbols in N1 are mainly
used to control the use of rules of the form A →x with A ∈N2, while † is
a trap-symbol; once introduced, it is never removed. A matrix of type 4 is
used only once, at the last step of a derivation.
According to Lemma 1.3.7 in [75], for each matrix grammar there is an
equivalent matrix grammar in the binary normal form.
The following result describes the power of P systems with labelled mem-
branes.
Theorem 3.2 PsLPm(Pri, Cat, nδ) = PsLPm(Pri, Cat, δ) = PsLP∗(Pri,
Cat, nδ) = PsLP∗(Pri, Cat, δ) = PsCE, for all m ≥2.
The only relation which needs a proof is the following one:
Lemma 3.3 (The Computational Completeness Lemma for P Systems)
PsCE ⊆PsLP2(Pri, Cat, nδ).
Proof.
Clearly, each set Q ⊆Nk, for some k ≥1, can be identiﬁed with
the language L(Q) = {an1
1 . . . ank
k
| (n1, . . . , nk) ∈Q} and Q is computably
enumerable if and only if L(Q) is computably enumerable. Take a matrix
grammar with appearance checking, G = (N, T, S, M, F), in the binary nor-
mal form, generating the language L(Q), for a given computably enumerable
set Q of vectors of natural numbers.
Assume that all matrices of forms 2, 3, 4 are labelled in a one-to-one
manner, by m1, m2, . . . , ms, for some s ≥1.
We construct the P system with catalysts
Π = (V, T, {c}, [1[2 ]2]1, w1, λ, (R1, ρ1), (∅, ∅), 2),

124
Membrane Computing
where
w1 = XAcZ, for (S →XA) the initial matrix in M,
V = N1 ∪N2 ∪T ∪{c, D, †, Z} ∪{Xi, X′
i, X′′
i | X ∈N1, 1 ≤i ≤s},
and the set R1 contains the following rules (h is the morphism deﬁned by
h(α) = α, α ∈N2, and h(a) = ain2, for a ∈T):
1. X →Xi, for all X ∈N1 and 1 ≤i ≤s.
2. Xi →Y ′, for mi : (X →Y, A →x) a matrix of type 2 in M.
3. cA →c h(x)D, for mi : (X →Y, A →x) a matrix of type 2 in M.
4. cD →c.
5. cZ →c†.
6. † →†.
7. Y ′ →Y , for all Y ∈N1.
8. cXi →cY , for mi : (X →Y, A →†) a matrix of type 3 in M.
9. A →†, for all A ∈N2.
10. Xi →X′
i, for mi : (X →λ, A →x) a matrix of type 4 in M.
11. cA →c h(x)D, for mi : (X →λ, A →x) a matrix of type 4 in M.
12. X′
i →X′′
i , for mi : (X →λ, A →x) a matrix of type 4 in M.
13. Z →λ.
14. X′′
i →λ, for mi : (X →λ, A →x) a matrix of type 4 in M.
The priorities are the following (at the same time, we give explanations about
the work of Π):
– each rule of type 1 has priority over all rules of other types;
(In the presence of a symbol from N1 no rule can be used, excepting a
rule of type 1, which speciﬁes a matrix to be simulated by the subscript
of the symbol X.)
– each rule Xi →Y ′ of type 2 has priority over all rules of type 3 associ-
ated with matrices mj with j ̸= i, as well as over all rules of types 5,
9, 11, 13;
(If a symbol Xi is present, identifying a matrix mi : (X →Y, A →x) of
type 2 from M, then the only rules which can be applied are Xi →Y ′,
because only Xi is present, and cA →c h(x)D, because all other rules
are either of a lower priority than Xi →Y ′, or do not have symbols

The Power of P Systems
125
to which they can be applied; note that always we have exactly one
occurrence of the catalyst, hence the rule cA →c h(x)D can be used
at most once; by using this rule, one occurrence of the symbol D is
introduced.)
– the rule of type 4 has priority over the rule of type 5;
(This is a very important point of the construction, making a full use
of the catalyst: if there is no occurrence of D in the multiset, then the
rule cZ →c† must be applied, introducing the trap-object † which will
evolve forever by the rule † →†. Thus, at the same time with Xi →Y ′
we have to use the corresponding rule cA →c h(x)D, which means
that the use of the matrix mi is correctly simulated. Note that the rule
cZ →c† cannot be used at the previous steps, because of the priority
of Xi →Y ′ over it.)
– each rule Y ′ →Y of type 7, for Y ∈N1, has priority over all rules of
types 3, 9, 11, 13;
(At the same time with the rule cD →c, providing that D is present,
we can use the rule Y ′ →Y ; no rule associated with a rule appearing
in the second position in a matrix can be applied, the simulation of the
matrix mi is completed.)
– each rule cXi →cY of type 8 has priority over all rules cA →c h(x)D
associated with matrices of types 2 and 4, over all rules B →† with
B ∈N2 such that B ̸= A, as well as over all rules of types 5, 11, 13;
(When the symbol Xi points to a matrix mi of type 3, then the catalyst
is “kept busy” by the rule cXi →cY , in order not to use the rule
cZ →c†; no rule for evolving a symbol from N2 can be used, because
of the priority; if, however, the symbol A from mi : (X →Y, A →†)
appears in the current multiset, then the corresponding rule of type
9 should be used and the trap-object is introduced. In this way, we
simulate the use of this rule in the appearance checking mode.)
– each rule Xi →X′
i of type 10 has priority over all rules of type 3, of
type 11 associated with matrices mj with j ̸= i, as well as over all rules
of types 5, 9, 13;
(When simulating the use of a matrix of type 4, at the last step of a
derivation in G, we proceed as for matrices of type 2, with the diﬀerence
that at the end we have also to remove the primed successors of X.)
– each rule X′
i →X′′
i of type 12 has priority over all rules of types 3, 11,
13;
(After introducing Xi we replace it with X′
i and, at the same time, we
use the corresponding rule cA →c h(x)D. At the next step, we check
whether or not D is introduced, that is, whether or not the simulation
is correct. The symbol Z is still present, but it is not used, because of

126
Membrane Computing
the priorities mentioned above. At the same time, we check whether or
not any nonterminal symbol from N2 is still present: the rules A →†
are available and no other rule using symbols from N2 can be used; if
any rule A →† can be applied, then it has to be applied.)
– each rule of type 14 has priority over cZ →c†;
(If a symbol X′′
i is present, then this means that the computation is
ﬁnished; we remove X′′
i and we also remove the “semi-trap” object Z;
the rule cZ →c† cannot be used.)
From the previous explanations, it is easy to see that each derivation in G
can be simulated by a computation in Π and, conversely, each computation in
Π corresponds to a derivation in G. It is worth mentioning that this is possible
because we are interested in the Parikh set of a language, hence the order
of symbols appearing in a sentential form of G is not important, only their
presence matters (exactly as in a multiset). Moreover, at each moment when
an occurrence of a terminal symbol is introduced, it is introduced directly
into the output membrane. Nothing else can reach the output membrane. If
the derivation is not correctly simulated or it is not terminal, then at least
a rule can be further applied, in particular, the rule † →† if this symbol
was produced. Thus, we can conclude that, because L(G) = L(Q), we have
N(Π) = Q.
✷
We also have the following results, where P families are compared with
families in the Chomsky or in the Lindenmayer area:
Theorem 3.3 (i) REG ⊂LP∗(nPri, nCat, nδ).
(ii) PsP1(nPri, nCat, nδ) = PsP∗(nPri, nCat, nδ) = PsCF.
(iii) All languages in LP(nPri, nCat, nδ) are semilinear.
Theorem 3.4 (i) PsE0L ⊆PsP2(nPri, nCat, δ).
(ii) PsET0L ⊆PsP1(Pri, nCat, nδ).
(iii) PsP∗(nPri, nCat, δ) ⊆PsET0L.
The last two inclusions, proved in [76], imply similar inclusions for PsLP
instead of PsP.
A natural question in this framework is whether or not the number of
membranes (the degree of P systems) induces an inﬁnite hierarchy. We have
seen above that in several cases, this hierarchy collapses. In general, when
no dissolving action is used, this hierarchy collapses (both in the internal and
the external mode).
Theorem 3.5 PsP∗(α, β, nδ) = PsPm(α, β, nδ), for all α ∈{Pri, nPri},
β ∈{Cat, nCat}, and m ≥2. For families LP instead of PsP, the result
above holds for m = 1.

Decidability Results
127
With the exception of relation (i) in Theorem 3.3, up to now all results
refer to Parikh images of languages. Here are two results dealing directly
with languages:
Let us say that a language L is strictly bounded if L ⊆a∗
1a∗
2 . . . a∗
k, for
some symbols a1, a2, . . . , ak diﬀerent from each other.
Theorem 3.6 If L is a strictly bounded semilinear language, then L ∈
LP1(Pri, nCat, nδ).
If we also use catalysts, then more complex languages than the strictly
bounded ones can be produced.
Theorem 3.7 If h is a morphism and L is a strictly bounded semilinear
language, then h(L) ∈LP1(Pri, Cat, nδ).
3.4
Decidability Results
If, for a given P system, we could know in advance that a given elementary
membrane will never be dissolved, then in Theorem 3.1(iii) we can use it as an
output membrane, without being necessary to consider one further membrane
(see the proof in [226]). Unfortunately, this is an undecidable question for
systems of a general type.
Theorem 3.8 It is undecidable whether or not a speciﬁed membrane of a P
system of type (Pri, Cat, δ) will be ever dissolved.
Proof.
According to Theorem 3.2 and using the fact that for lan-
guages L over the one-letter alphabet we have L = p(L), for each com-
putably enumerable set of numbers M
⊆
N, the language l(M)
=
{an
|
n
∈
M} belongs to the family LP∗(Pri, Cat, δ).
Let Π
=
(V, {a}, C, µ, w1, . . . , wm, (R1, ρ1), . . . , (Rm, ρm)) be a P system such that
L(Π) = l(M), for an arbitrary set M ⊆N. We construct a new system,
Πt, where t is any integer t ≥0, in the following way.
The total alphabet of Πt is
V ′ = V ∪{b, b′, c, c′, d, d′, e, e′, f},
where c, c′ are new catalysts, that is, the set of catalysts of Πt is C ∪{c, c′},
and its output alphabet is T = {f}. The symbols b, b′, c, c′, d, d′, e, e′, f are
not in V .
Assume that the membranes in Π are labelled with 1′, . . . , m′ (the shell
membrane is 1′). Then, the system Πt is as indicated in Figure 3.4, that is,
it has:
µ′ = [1[2 µ ]2[3 ]3[4 ]4]1,
w′
1 = λ,

128
Membrane Computing
w′
2 = b2tf,
w′
3 = cc′d′,
w′
4 = f,
R′
1 = {a →a, e →ein3, b →bin3, f →fout, d →din4},
ρ′
1 = ∅,
R′
2 = {r1 : f →f, r2 : f →f ′, r3 : a →e2, f ′ →δ},
ρ′
2 = {r1 > r3, r2 > r3},
R′
3 = {r4 : ce →ce′, r5 : c′b →c′b′, r6 : cb →cδ,
r7 : c′e →c′δ, r8 : d′ →dout},
ρ′
3 = {r4 > r6, r5 > r7, r6 > r8, r7 > r8},
R′
4 = {d →δ},
ρ′
4 = ∅.
All the components of Π remain unmodiﬁed. The shadowed membrane 1′
in Figure 3.4 indicates that the contents of this membrane is not of interest
for what follows, important is that as a result of the complete computations
in this membrane we get strings an, for n ∈M.
Claim: Membrane 4 in system Πt is dissolved if and only if t ∈M.
In order to prove this claim, and hence the theorem, let us examine the
work of Πt. The rule f →f in membrane 2 can be used an arbitrary number
of times. In the meantime, from membrane 1′ we can get the output of the
system Π, in the form of a string ar. In any moment, we can use the rule
f →f ′. In parallel, further occurrences of a can exit from membrane 1′.
Assume that after the use of the rule f →f ′ we have in total n occurrences
of a. At the next step we have to use the rule f ′ →δ, in parallel with the
rule a →e2 (this last rule cannot be used before, because of the priority
relations). In this way, membrane 2 is dissolved, its rules are removed (in
particular, the rule a →e2 is no longer available). In membrane 1 we will
have 2n occurrences of e, as well as the 2t occurrences of b introduced in the
initial conﬁguration of Πt.
It is important to note that if any new occurrence of a was produced in the
same step when membrane 2 was dissolved or any further occurrence of a will
ever be expelled from membrane 1′ at the next steps, then the computation
in Πt will never correctly end, because of the rule a →a present in membrane
1. Therefore, the work of Π must be ﬁnished when an is made available, that
is, n must be an element of M.
The rules e →ein3, b →bin3 just pass to membrane 3 the 2n copies of e
and the 2t copies of b. Note that if n ̸= t, then 2n diﬀers from 2t by at least
2. This is important below.
In membrane 3 we have two catalysts, c and c′, in one copy each. In every
time unit, each catalyst decreases by one the number of copies of e and of b

Decidability Results
129
(by priming them). This is done by the rules ce →ce′, c′b →c′b′ which have
priority over all other rules in membrane 3.
If n = t, then the objects e, b are ﬁnished at the same time. This makes
possible the use of the rule d′ →dout (no rule with a higher priority is
applicable). From membrane 1, the symbol d enters membrane 4 and dissolves
it. Just to have some output, the symbol f released in this way is sent out
of the system.
✬
✫
✩
✪

✒
✏
✑
✬
✫
✩
✪
✬
✫
✩
✪
#
✧
✥
✦











1
2
1′
3
4
f
d →δ
d →din4
f →fout
cc′d′
r6 : cb →cδ
r7 : c′e →c′δ
>
>
r4 : ce →ce′
r5 : c′b →c′b′
>
r8 : d′ →dout
a →a
e →ein3
b →bin3
b2tf
r1 : f →f
r2 : f →f ′
f ′ →δ
> r3 : a →e2
❄
an
Figure 3.4: The system in the proof of the theorem.
Assume now that n ̸= t.
The two possibilities, n > t and t > n are
handled in the same way, so we discuss only the case when n > t.
This
implies that 2n −2t ≥2. After using the rules r4, r5 for 2t times, we have
no further copy of b in membrane 3, but we still have at least two copies of
e. One of them will be used by the rule ce →ce′; because neither the rule
ce →ce′ can be used once more (the catalyst c is present in one copy only),
nor the rule c′b →c′b′ (no b is present), the rule c′e →c′δ can be used (and
should be used).
This means that membrane 3 is dissolved. Its content is released in mem-
brane 1, its rules are lost. Thus, the symbol d′ will remain unmodiﬁed, that
is the rule d →din4 from membrane 1 cannot be applied. The computa-
tion stops here without dissolving membrane 4 (and without sending out any
symbol, because f remains “locked” in membrane 4).

130
Membrane Computing
In conclusion, membrane 4 is dissolved if and only t = n, that is, if and
only if t ∈M. Because M is an arbitrary computably enumerable set, this
is undecidable (if M is not computable, then its membership problem is not
decidable).
✷
Actually, we can also say something about the number of membranes in
the system used in the previous proof: from Theorem 3.2 we know that Π is
of degree 2, hence Πt is of degree 6. We do not know which is the optimal
result from this point of view.
From the previous proof, we can infer many other undecidability results.
Note that the appearance of the symbol d, the use of the rule d →δ, and the
fact whether or not f is sent out of membrane 1 depend on the fact whether
or not t ∈M, which is undecidable. Thus all these events are undecidable.
At
the
bottom
of
the
hierarchy
over
the
parameters
α, β, γ
in
PsLP(α, β, γ) we can, however, decide whether or not a membrane is ever
dissolved.
Theorem 3.9 It is decidable whether or not a speciﬁed membrane in P sys-
tems of type (nPri, nCat, δ) is ever dissolved during a successful computation.
Proof. Consider a system Π of type (nPri, nCat, δ) and a speciﬁed membrane
i in it. Assume that the output alphabet of Π is T. We modify the system
as follows. Take a new symbol, d, and add it to the output alphabet; denote
by T ′ this new output alphabet.
If there is a rule of the form a →vδ
in membrane i, then we replace it with a →vdδ. Now, we add the rule
d →dout to all membranes, including the skin one. Let us denote by Π′ the
system obtained in this way.
It is clear that L(Π′) contains all the strings in L(Π) which were obtained
at the end of computations where membrane i was not dissolved, as well
as strings of the form w1dw2 . . . dwk for k ≥1 and w1w2 . . . wk ∈L(Π)
which were obtained by computations where membrane i was dissolved. (The
computation in Π′ cannot stop before sending out the symbol d, because each
region contains the rule d →dout. It is possible to use several rules of the
form a →vdδ at the step when membrane i is dissolved.)
From Theorem 3.4(iii) we know that PsLP∗(nPri, nCat, δ) ⊆PsET0L.
This inclusion follows from the constructive proof of Theorem 3.1 and the
relation PsP∗(nPri, nCat, δ) ⊆PsET0L proved in [76]. This last relation
is also proved in a constructive way. Therefore, starting from our system
Π′, we can eﬀectively construct an ET0L system G such that ΨT ′(L(Π′)) =
ΨT ′(L(G)). Consider the language L = L(G) ∩T ′∗{d}T ′∗. This is also an
ET0L language (the family ET0L is closed under intersection with regular
languages). Clearly, L is non-empty if and only if the symbol d appears in at
least one string of L(G), that is, if and only if membrane i was ever dissolved
during the computation of a string in L(Π). The emptiness is decidable for
ET0L languages, hence our problem is decidable.
✷

Rewriting P Systems
131
There are two types of P systems, placed in between those considered in
Theorems 3.8 and 3.9, for which the decidability of the membrane dissolving
remains to be investigated: (Pri, nCat, δ) and (nPri, Cat, δ).
3.5
Rewriting P Systems
The P systems of the form considered up to now can be interpreted as using
no data structure for codifying the information: the numbers are encoded as
the cardinality of multisets, hence they are represented in the base one. This
can be adequate to a biochemical implementation, but it looks ineﬃcient from
a classic point of view. Moreover, in this way we can deal only with problems
on numbers, not (directly, without a number codiﬁcation) with symbolic
computations.
That is why we look now for representing information by
using a data structure of a standard type, strings.
Thus, in this section, instead of objects of an atomic type (i.e., without
“parts”), we consider objects which can be described by ﬁnite strings over a
given ﬁnite alphabet. The evolution of an object will then correspond to a
transformation of the string. In this section we consider transformations in
the form of rewriting steps, as usual in formal language theory.
The rules are also provided with indications on the target membrane of the
produced string (we do no longer consider the membrane dissolving action,
because, similarly to the case of Theorem 3.2, it will not be necessary in order
to obtain computational completeness; of course, if for other purposes it will
be useful/necessary to use this action, then it can be introduced in the same
way as in the previous sections). Always we use only context-free rules, that
is, the rules are of the form
(X →v; tar),
where X →v is a context-free rule and tar ∈{here, out, inj} (“tar” comes
from “target”, j is the label of a membrane), with the obvious meaning: the
string produced by using this rule will go to the membrane indicated by tar.
Note the important fact that a string is now a unique object, hence it
passes through membranes as a unique entity, its symbols do not follow dif-
ferent itineraries, as it was possible for the objects in a multiset; of course, in
the same region we can have several strings at the same time, but is irrelevant
whether or not we consider multiplicities of strings: each string follows its
own “fate”. That is why we do not speak here about multiplicities. In this
framework, also the catalysts are meaningless.
Consequently, we obtain a language generating mechanism of the form
Π = (V, T, µ, L1, . . . , Ln, (R1, ρ1), . . . , (Rn, ρn), i0),
where V is an alphabet, T ⊆V , µ is a membrane structure, L1, . . . , Ln are
ﬁnite languages over V , R1, . . . , Rn are ﬁnite sets of context-free evolution

132
Membrane Computing
rules, ρ1, . . . , ρn are partial order relations over R1, . . . , Rn, and i0 is the
output membrane.
We call such a system a rewriting P system.
The language generated by a system Π is denoted by L(Π) and it is deﬁned
as explained in Section 3.1, with the diﬀerences speciﬁc to an evolution based
on rewriting: we start from an initial conﬁguration of the system and proceed
iteratively, by transition steps performed by using the rules in parallel, to all
strings which can be rewritten, obeying the priority relations, and collecting
the terminal strings generated in a designated membrane, the output one.
Note that each string is processed by one rule only, the parallelism refers
here to processing simultaneously all available strings by all applicable rules.
If several rules can be applied to a string, maybe in several places each,
then we take only one rule and only one possibility to apply it and consider
the obtained string as the next state of the object described by the string.
It is important to have in mind the fact that the evolution of strings is
not independent of each other, but interrelated in two ways: (1) if we have
priorities, a rule r1 applicable to a string x can forbid the use of another
rule, r2, for rewriting another string, y, which is present at that time in the
same membrane; after applying the rule r1, if r1 is not applicable to y or to
the string x′ obtained from x by using r1, then it is possible that the rule
r2 can now be applied to y; (2) even without priorities, if a string x can be
rewritten for ever, in the same membrane or on an itinerary through several
membranes, and this cannot be avoided, then all strings are lost, because the
computation never stops, irrespective of the strings collected in the output
membrane and which cannot evolve further.
We denote by RPm(Pri) the family of languages generated by rewriting P
systems of degree at most m, m ≥1, using priorities; when priorities are not
used, we replace Pri with nPri; the union of all families RPm(α) is denoted
by RP∗(α), α ∈{Pri, nPri}.
We do not recall here all known results about these families, but only the
main one, the characterization of computably enumerable languages; some
further details can be found in [218].
In order to illustrate the way of working in a rewriting P system, we
consider an example (which also proves that the family RP2(nPri) contains
non-context-free languages):
Π = ({A, B, a, b, c}, {a, b, c}, [1[2 ]2]1, ∅, {AB}, (R1, ∅), (R2, ∅), 2),
R1 = {(B →cB; in2)},
R2 = {(A →aAb; out), (A →ab; here), (B →c; here)}.
It is easy to see that L(Π) = {anbncn | n ≥1} (if a string aiAbiciB is
rewritten in membrane 2 to aiAbici+1 and then to ai+1Abi+1ci+1 and sent
out, then it will never come back again in membrane 2, the computation
stops, but the output membrane will remain empty). This is not a context-
free language.
✷

Rewriting P Systems
133
Theorem 3.10 RPm(Pri) = RP∗(Pri) = CE, for all m ≥3.
Of course, the only relation which needs a proof is that in the next lemma:
Lemma 3.4 (The Computational Completeness Lemma for Rewriting P
Systems) CE ⊆RP3(Pri).
Proof.
Let G = (N, T, S, M, F) be a matrix grammar with appearance
checking in the binary normal form. We replace each matrix (X →λ, A →x)
of type 4, with x ∈T ∗, by the matrix (X →X′, A →x), which is considered
of type 4′; we also add the matrices (X′ →λ); X′ is a new symbol associated
with X. Clearly, the generated language is not changed. We assume the
matrices of the types 2, 3, 4′ labelled in a one-to-one manner with m1, . . . , mk.
We construct the following rewriting P system:
Π = (V, T, µ, L1, L2, L3, (R1, ρ1), (R2, ρ2), (R3, ρ3), 2),
V = N1 ∪N2 ∪{E, Z, †} ∪T ∪{Xi, X′
i | X ∈N1, 1 ≤i ≤k},
µ = [1[2 ]2[3 ]3]1,
L1 = {XAE}, for (S →XA) the initial matrix of G,
L2 = L3 = ∅,
R1 = {rα : (α →α; here) | α ∈V −T, α ̸= E}
∪{r0 : (E →λ; in2)}
∪{(X →Yi; in2) | mi : (X →Y, A →x) is a matrix of type 2}
∪{(X →Yi; in3) | mi : (X →Y, A →†) is a matrix of type 3}
∪{(X →X′
i; in2), (X′
i →λ; here) | mi : (X →X′, A →x)
is a matrix of type 4′}
∪{(Yi →Y ; here), (Y ′
i →Y ; here) | Y ∈N1, 1 ≤i ≤k},
ρ1 = {rα > r0 | α ∈V −T, α ̸= E},
R2 = {ri : (Yi →Yi; here), r′
i : (A →x; out) | mi : (X →Y, A →x)
is a matrix of type 2}
∪{ri : (X′
i →X′
i; here), r′
i : (A →x; out) | mi : (X →X′, A →x)
is a matrix of type 4′}
ρ2 = {ri > r′
j | i ̸= j, for all possible i, j},
R3 = {pi : (Yi →Y ′
i ; here), p′
i : (Y ′
i →Yi; here),
p′′
i : (A →†; out) | mi : (X →Y, A →†)
is a matrix of type 3}
∪{p0 : (E →E; out)},
ρ3 = {p′′
i > pi, pi > p0 | for all possible i}.
The system works as follows. Observe ﬁrst that the rules (α →α; in2)
from membrane 1 change nothing, can be used for ever, and prevent the use

134
Membrane Computing
of the rule (E →λ; here), which sends the string to membrane 2, the output
one.
Assume that in membrane 1 we have a string of the form XwE (initially,
we have here the string XAE, for (S →XA) ∈M). In membrane 1 one
chooses the matrix to be simulated, mi, and one simulates its ﬁrst rule,
X →Y , by introducing Yi in the case of matrices of types 2, 3 and X′
i in
the case of matrices of type 4′; the string is sent to membrane 2 if we deal
with a matrix of types 2 or 4′ (without a rule which has to be applied in
the appearance checking mode), and to membrane 3 if we have to simulate a
matrix of type 3.
In membrane 2 we can use the rule (Yi →Yi; here) forever. The only way
to quit this membrane is by using the rule A →x appearing in the second
position of a matrix of type 2. Due to the priority relation, this matrix should
be exactly mi as speciﬁed by the subscript of Yi. Therefore, we can continue
the computation only when the matrix is correctly simulated.
The process is similar in membrane 3: The rules (Yi →Y ′
i ; here), (Y ′
i →
Yi; here) can be used for ever. We can quit the membrane either by using
a rule (A →†; out) or by using the rule (E →E; out). In the ﬁrst case the
computation will never end. Because of the priority relation, such a rule must
be used if the corresponding symbol A appears in the string. If this is not
the case, then the rule (Yi →Y ′
i ; here) can be used. If we now use the rule
(Y ′
i →Yi; here), then we get nothing. If we use the rule (E →E; out), and
this is possible because Yi is no longer present, then we send out a string of
the form Y ′
i wE.
In membrane 1 we replace Yi or Y ′
i by Y , and thus the process of simu-
lating the use of matrices of types 2, 3 can be iterated.
A slightly diﬀerent procedure is followed for the matrices of type 4′; they
are of the form mi : (X →X′, A →x). In membrane 1 we use (X →X′
i; in2)
and the string arrives in membrane 2.
Again the only way to leave this
membrane is by using the associated rule (A →x; out). In membrane 1 we
have to apply (X′
i →λ; here). If no symbol diﬀerent from E and terminals is
present, then we can apply the rule (E →λ; in2). Thus, a terminal string is
sent to membrane 2, where no rewriting can be done, the computation stops.
If any nonterminal symbol is still present, then the computation will never
halt, because of the rules (α →α; here) from membrane 1.
Therefore, we collect in the output membrane exactly the terminal strings
generated by the grammar G, that is L(G) = L(Π).
✷
Without a proof, we mention the following result, related to the previous
one:
Lemma 3.5 MAT ⊆RP∗(nPri).
Several problems are open in this area: Is the hierarchy RPm(nPri), m ≥
1, an inﬁnite one? Is the result CE ⊆RP3(Pri) optimal? Is the inclusion

P Systems with Polarized Membranes
135
MAT ⊆RP∗(nPri) proper? (The diﬃculty in proving that RP∗(nPri) ⊆
MAT lies in the dependence between the evolution of the words initially
placed in a rewriting P system: even if a string has reached the output
membrane and it cannot further evolve, in order to accept it we have to
make sure that no other string present in the system can further evolve. This
can easily be controlled in a matrix grammar with appearance checking, but
we see no way to do it without using the appearance checking.)
3.6
P Systems with Polarized Membranes
The way of controlling the communication of objects by means of labels, in
the form ainj, looks rather unrealistic. In real cells, the molecules can pass
through membranes mainly because of concentration diﬀerence in neighbour-
ing regions, or by means of electrical charges (ions can be transported in
spaces of opposite polarization). This last variant is a much more restricted
possibility as compared with the speciﬁcation of the target membrane by its
label: we only have two labels, + and −, associated in a non-injective way
with the membranes. However, from the two Computational Completeness
Lemmas (Lemmas 3.3 and 3.4) we know that systems with three membranes
suﬃce. Three membranes means a skin membrane and two inner membranes
which can be labelled with + and −. Consequently, using only polarized
membranes we can still obtain computational completeness. To this aim, we
have to use priority relations. However, the priority also looks a little bit
“non-biochemical”, so it would be good to also avoid the use of this feature.
Pleasantly enough, a variant of P systems with communication controlled by
an “electrical charge” associated with each object and without a priority re-
lation among evolution rules, still characterizes the computably enumerable
sets of numbers, providing that a further feature is considered: controlling
the thickness (permeability) of membranes by means of evolution rules.
The charge of objects can be “positive”, “negative” (identiﬁed with +, −,
respectively), or “neutral”. An object marked with + will enter any of the
membranes marked with −which are adjacent to the region where this object
is produced; symmetrically, an object marked with −will enter a membrane
marked with +, non-deterministically chosen from the set of reachable mem-
branes. The neutral objects are not introduced in an inner membrane (they
can only be sent out of the current membrane – the indications here and out
are used as in the previous sections).
The control of membrane permeability can be achieved as follows: besides
the action of dissolving a membrane (indicated by introducing the symbol δ),
we also use the action of making a membrane thicker (this is indicated by
the symbol τ). Initially, all membranes have the thickness 1. If a rule in
a membrane of thickness 1 introduces the symbol τ, then the membrane
becomes of thickness 2. A membrane of thickness 2 does not become thicker
by using further rules which introduce the symbol τ, but no object can enter

136
Membrane Computing
✒✑
✏
✒✑
✏

✒
✏
✑
Dissolution
1
2
✛
P
P
P
P
✏
✏
✏
✮
PPP✏✏✏
✶
✎
✎

✌
❄
❄
✛
δ
δτ
δτ
δ
τ
τ
Figure 3.5: The eﬀect of actions δ, τ.
or exit it. If a rule which introduces the symbol δ is used in a membrane of
thickness 1, then the membrane is dissolved; if the membrane had thickness
2, then it returns to thickness 1. If at the same step one uses rules which
introduce both δ and τ in the same membrane, then the membrane does
not change its thickness. These actions of the symbols δ, τ are illustrated in
Figure 3.5.
We consider here only P systems with polarized membranes of a variable
thickness with external output (so, the output region is no longer speciﬁed,
it is always ∞). Such a system of degree m, m ≥1, is a construct
Π = (V, T, C, µ, w1, . . . , wm, R1, . . . , Rm),
where all components are deﬁned as in the previous sections, with the fol-
lowing diﬀerences:
– µ is a membrane structure consisting of m membranes (labelled, for
reference only, with 1, 2, . . . , m), where each membrane is marked with
one of the symbols +, −, 0 (these markers are written as superscripts
of the right square bracket representing the membrane, e.g. [1[2 ]+
2 ]0
1);
all membranes in µ have thickness 1;
– the evolution rules in sets Ri, 1 ≤i ≤m, are of the forms a →v or
ca →cv, where a is an object from V −C and v = v′ or v = v′δ or
v = v′τ, where v′ is a string over
{ahere, aout, a+, a−| a ∈V −C},
and δ, τ are special symbols not in V . (As usual, the indication here is
in general omitted.)
When applying a rule, if an object appears in v marked with here, then it
remains in the same region; if we have aout and the membrane has thickness 1,
then a copy of the object a will be introduced in the region placed immediately
outside; if we have a+ (or a−), then a copy of a is introduced in one of the
membranes marked with −(respectively +), of thickness one, and adjacent

P Systems with Polarized Membranes
137
to the region of the rule (if no such membrane exists, then the rule cannot
be applied); if the special symbol δ appears in v and the membrane where
we work has thickness 1, then this membrane is dissolved; in this way, all
the objects in this region become elements of the region placed immediately
outside, while the rules of the dissolved membrane are removed.
The communication of objects has priority on the actions δ, τ, in the sense
that if at the same step one both sends a symbol through a membrane and
one changes the thickness of that membrane, then one ﬁrst transmits the
symbol and after that one changes the thickness.
We denote by LP ±
∗(Cat, δ, τ) the family of languages generated by P
systems as above, using catalysts and both actions indicated by δ, τ; when
one of the features α ∈{Cat, δ, τ} is not used, we write nα instead of α. If
only systems with at most m membranes are used, then we add the subscript
m to LP.
The following example illustrates this deﬁnition. Consider the P system
of degree 4
Π = (V, T, C, µ, w1, w2, w3, w4, R1, R2, R3, R4),
V = {a, a′, b, b′, c, d, †},
T = {a},
C = {c},
µ = [1[2[3 [4 ]0
4]0
3]0
2]0
1,
w1 = λ, R1 = {a →aout},
w2 = λ, R2 = {a →a′, a′ →aout, b →†, d →τ, † →†},
w3 = λ, R3 = {a →aouta, cb →cb′, cd →cdδ},
w4 = ancd, R4 = {a →abδ}.
The initial conﬁguration of the system is presented in Figure 3.6. With
each label of a membrane, we present its polarity; here, all membranes are
neutral. (Actually, the polarity is not used, all objects are neutral.)
The system works as follows. Initially, we have objects only in membrane
4, where n copies of a and one copy of each of c and d are present. The rule
a →abδ should be applied to each of these n copies of a; as a result we get
n copies of b and the membrane is dissolved. The used rule is “lost” and the
objects (n copies of a, n copies of b, one copy of c, and one of d) are left free
in the next region, that associated with membrane 3. If the rule cd →cdδ
is used in a moment when copies of b are still present, then such a symbol
will reach region 2, where the rule b →† will be used. The symbol † can
evolve indeﬁnitely by using the rule † →†, that is, the computation never
halts (hence we have no output). In order to avoid the use of this trap-rule,
we have to make sure that we dissolve membrane 3 after transforming all
symbols b in b′. This means that we have to use n times (the number of
occurrences of b) the rule cb →cb′; in parallel, we have to use n times the

138
Membrane Computing
rule a →aouta for all copies of a which are present. Therefore, n copies of a
are sent out of membrane 3 at each step.
In membrane 2, each a becomes a′ and, at the next step, is sent out,
to membrane 1. From membrane 1, each object a leaves the system, hence
participates to the generated string.
✬
✫
✩
✪
a →aout
✬
✫
✩
✪
d →τ
b →†,
† →†
a →a′,
a′ →aout
✬
✫
✩
✪
cd →cdδ
cb →cb′
a →aouta
#
✧
✥
✦
a →abδ
ancd
4(0)
3(0)
2(0)
1(0)
Figure 3.6: An example of a P system using action τ.
When d arrives in membrane 2, it must evolve by the rule d →τ. In this
way, membrane 2 becomes thicker and from now on no symbol can leave it.
This means that, after using the rule d →τ, the rule a′ →aout is no longer
applicable. Thus, although from membrane 3 we send n2 + n copies of a to
membrane 2 (n2 during transforming b into b′ and n at the step when we use
the rule cd →cdδ), from membrane 2 to membrane 1 we send only n2 copies
of a (in the step when using d →τ we also use a →a′ and a′ →aout, but
from now on this latter rule will not be applied).
Consequently, the output consists of n2 copies of the symbol a. We can
say that the previous P system computes the mapping f(n) = n2.
It is easy to modify the previous system in such a way to generate the
(non-context-free) language {an2 | n ≥1}: consider one further membrane
inside membrane 4, with the rules f →af, f →afcdδ, where f is a new
object; when this membrane is dissolved, n copies of a, for some n ≥1, are
left free in membrane 4 (together with the catalyst c and the auxiliary symbol
d; of course, no object there initially is in membrane 4).
We do not start here a systematic study of the generative power of the

P Systems with Polarized Membranes
139
diﬀerent variants of P systems with polarized membranes (with or without
catalysts, with or without using the actions δ and τ, etc.), but we only provide
a basic result, stating that they are computationally complete.
Theorem 3.11 PsCE = PsLP ±
∗(Cat, δ, τ).
As usual, we only prove the inclusion ⊆:
Lemma 3.6 (The Computational Completeness Lemma for P Systems with
Polarized Membranes) PsCE ⊆PsLP ±
∗(Cat, δ, τ).
Proof.
Consider a language L ∈CE, L ⊆T ∗, and take a matrix grammar
with appearance checking, G = (N, T, S, M, F), in the binary normal form,
generating this language.
We modify the grammar G by replacing each rule X →λ in matrices of
type 4 by X →b, where b is a new symbol. We denote by G′ = (N, T ∪
{b}, S, M ′, F) the obtained grammar. Clearly, L(G′) = {b}L(G).
We label the matrices in M ′ in a one-to-one manner; assume that matrices
m1, . . . , mk are of type 3 (with rules A →† appearing in F) and matrices
mk+1, . . . , mk+n are of types 2 and 4, for some k ≥0 and n ≥1.
We construct the system (of degree 2k + 3n −1)
Π = (V, T, C, µ, w1, w1′, w2, w2′, . . . , wk, wk′, w(k+1), w(k+1)′, w(k+1)′′,
w(k+2), w(k+2)′, w(k+2)′′, . . . , w(k+n−1), w(k+n−1)′,
w(k+n−1)′′, w(k+n)′, w(k+n)′′,
R1, R1′, R2, R2′, . . . , Rk, Rk′, R(k+1), R(k+1)′, R(k+1)′′,
R(k+2), R(k+2)′, R(k+2)′′, . . . , R(k+n−1), R(k+n−1)′,
R(k+n−1)′′, R(k+n)′, R(k+n)′′),
with the following components:
V = {X, X′, X′′, X′′′, Xiv, ¯X | X ∈N1}
∪{A, A′, A′′, A′′′, Aiv, ¯A, A(1), A(2) | A ∈N2}
∪{D, D′, D′′, ¯D, b, c, †} ∪T,
C = {c},
µ = [1[1′ ]−
1′[2[2′ ]−
2′ . . . [k[k′ ]−
k′[(k+1)[(k+1)′[(k+1)′′ ]−
(k+1)′′]−
(k+1)′
. . . [(k+n−1)[(k+n−1)′[(k+n−1)′′ ]−
(k+n−1)′′]−
(k+n−1)′
[(k+n)′[(k+n)′′ ]−
(k+n)′′]+
(k+n)′]+
(k+n−1) . . . ]+
(k+1)]+
k . . . ]+
2 ]0
1,
w1 = XA, for (S →XA) the initial matrix of G,
wi′ = λ, 1 ≤i ≤k,
wi = λ, 2 ≤i ≤k + n −1,

140
Membrane Computing
w(k+i)′ = c, 1 ≤i ≤n,
w(k+i)′′ = λ, 1 ≤i ≤n,
R1 = {X →X+, X →X−, ¯X →X | X ∈N1}
∪{A →A+, A →A−, ¯A →A, A′′ →†, A(2) →† | A ∈N2}
∪{¯a →aout | a ∈T} ∪{† →†},
Ri = {X →X+, X →X−, ¯X →¯Xout | X ∈N1}
∪{A →A+, A →A−, ¯A →¯Aout, A′′ →†, A(2) →† | A ∈N2}
∪{¯a →¯aout | a ∈T} ∪{† →†},
for i = 2, . . . , k + n −1,
Rj′ = {X →Y ′τ, Y ′ →Y ′′, Y ′′ →¯Yout, A →†, † →†}
∪{Z →† | Z ∈N1, Z ̸= X}
∪{B →B(1), B(1) →B(2)δ, B(2) →¯Bout | B ∈N2, B ̸= A},
for i = 1, 2, . . . , k, with mi = (X →Y, A →†),
Rj′ = {X →Y ′D+τ, Y ′ →Y ′′, Y ′′ →Y ′′′, Y ′′′ →Y iv,
Y iv →¯Y (out), Y iv →†, † →†, cA →cA′, A′ →A′′δ,
A′′ →A′′′, A′′′ →Aiv, Aiv →hout(x)}
∪{B →B+, ¯B →¯Bout, B(2) →† | B ∈N2}
∪{Z →† | Z ∈N1, Z ̸= X},
where hout is the morphism deﬁned by hout(α) = ¯αout,
for each α ∈N2 ∪T,
Ri′′ = {D →D′τ}
∪{B →B(1), B(1) →B(2)δ, B(2) →¯Bout | B ∈N2},
for i = k + 1, . . . , k + n, with mi = (X →Y, A →x)
(note that Y ∈N1 ∪{b}).
As in the previous example, once the symbol † is introduced, the compu-
tation is lost, because it will never end.
The shape of the initial conﬁguration – for the case k = 2, n = 4 – is
indicated in Figure 3.7.
The system Π works as follows.
In the initial conﬁguration we can apply a rule only in membrane 1.
Consider an arbitrary step, when in membrane 1 we have one occurrence
of a symbol X ∈N1 and several occurrences of symbols from N2 (initially,
we have here only one object from N2). In other regions we have at most
symbols ¯b, D′, and c (initially, only occurrences of the catalyst c).
All symbols in membrane 1 get an “electrical charge”, in a non-
deterministic manner, by means of the rules of the form X →X+, X →X−
and A →A+, A →A−.
This happens also in each of the membranes
2, 3, . . . , k + n −1.
The symbols marked in this way have to be commu-

P Systems with Polarized Membranes
141
nicated to a membrane of an opposite polarity. Always we have only two
membranes where the symbols can be moved, of opposite polarities (note
that membrane (k + n −1)′ is marked with −and membrane (k + n)′ with
+).
✬
✫
✩
✪
✬
✫
✩
✪
✗
✖
✔
✕
✛
✚
✘
✙
✬
✫
✩
✪
✬
✫
✩
✪
✤
✣
✜
✢
✬
✫
✩
✪
✬
✫
✩
✪
✤
✣
✜
✢
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✤
✣
✜
✢
✤
✣
✜
✢
R1′
R2′
R3′
R4′
R5′
R6′
R3′′
R4′′
R5′′
R6′
R1
R2
R3
R4
R5
1(0)
1′(−)
2′(−)
2(+)
3(+)
3′(−)
4(+)
4′(−)
5(+)
5′(−)
6′(+)
3′′(−)
4′′(−)
5′′(−)
6′′(−)
Figure 3.7: The shape of the system in the proof of lemma.
Assume that some symbols are moved in this way until reaching a mem-
brane i′, for some 1 ≤i ≤k, associated with a matrix mi = (X →Y, A →†)
from the grammar G′. We distinguish several cases:
1. If an occurrence of A is present, then the rule A →† from Ri′ must be
used, hence the computation never halts.

142
Membrane Computing
2. If a symbol Z ∈N1 diﬀerent from X is present, then the rule Z →†
must be used and the computation never halts.
3. If X is not present, but some symbols B ∈N2 −{A} are present, then
we have to use the rules B →B(1), B(1) →B(2)δ. The membrane is
dissolved and the symbol B(2) is left free in region i. The rule B(2) →†
is present here and it has to be used, hence the computation never
halts.
4. Assume now that both X and A are present in region i′. At the ﬁrst
step, X is replaced with Y ′, one also introduces the symbol τ, and all
symbols B ∈N2 −{A} are replaced by B(1). The membrane gets thick-
ness 2. At the next step, we use the rules Y ′ →Y ′′ and B(1) →B(2)δ,
for all B ∈N2 −{A} which are present in the membrane.
(When
simulating a derivation with respect to the grammar G′, in the sys-
tem Π there is at least one symbol B as above, because the matrices
of this form are not used at the last step of a derivation. We shall
see immediately that the nonterminals must travel through membranes
together, otherwise the computation will never end.) In this way, the
membrane returns to thickness one. At the next step, all symbols Y
and B ∈N2 −{A} are sent out, in a barred form (and this is possible,
because the membrane has the thickness 1). In this way, the matrix
mi has been correctly simulated: X is replaced with Y , providing that
A is not present. We will see below that A is not only absent from
membrane i′, but is was absent also from membrane 1 at the beginning
of the simulation of the matrix mi.
Let us now consider the case when some symbols enter a membrane j′ for
some j = k + 1, . . . , k + n, associated with a matrix mj = (X →Y, A →x),
with Y ∈N1 ∪{b} and x ∈(N2 ∪T)∗. We again distinguish several cases:
1. If a symbol Z ∈N1 diﬀerent from X is present in membrane j′, then
the rule Z →† must be used and the computation never halts.
2. If the right symbol X is present, but A is not present, then in the ﬁrst
step we use the rule X →Y ′D+τ (hence membrane j′ becomes thicker
and the object D is sent to membrane j′′) and B →B+ for all symbols
B ∈N2 which are present. At the next step, we use the rule Y ′ →Y ′′
in membrane j′ and the rules D →D′τ, B →B(1) in membrane j′′
(which becomes thicker). We continue by using the rule Y ′′ →Y ′′′
in membrane j′ and B(1) →B(2)δ in membrane j′′ (which returns to
thickness 1).
At the next step we use Y ′′′ →Y iv in membrane j′
and B(2) →¯Bout in membrane j′′. We continue by using Y iv →† in
membrane j′ (the rule Y iv →¯Yout cannot be used, because membrane
j′ is of thickness 2.) The computation will never stop.
3. If both X and A are present, then the rules we use at the next steps
are the following:

P Systems with Polarized Membranes
143
step 1: X →Y ′D+τ, cA →cA′, B →B+,
for B ∈N2, in membrane j′ (which becomes thicker);
step 2: Y ′ →Y ′′, A′ →A′′δ,
in membrane j′ (which returns to thickness 1)
D →D′τ, B →B(1),
for B ∈N2, in membrane j′′ (which becomes of thickness 2),
step 3: Y ′′ →Y ′′′, A′′ →A′′′, in membrane j′,
B(1) →B(2)δ, for B ∈N2, in membrane j′′ (which returns to
thickness 1),
step 4: Y ′ →Y iv, A′′′ →Aiv, in membrane j′,
B(2) →¯Bout, for B ∈N2, in membrane j′′,
step 5: Y iv →¯Yout, Aiv →hout(x), ¯B →¯Bout,
for B ∈N2, in membrane j′.
In this way, the matrix mj is correctly simulated.
In conclusion, we either correctly simulate the matrix mj, or we intro-
duce the trap-symbol †. It is important to note that the symbol † is
introduced also when a symbol B ∈N2 enters this membrane without
also having here the symbol X; this symbol B can be any one from N2
(equal or not to A). When simulating a matrix (X →λ, A →x) at the
last step of a derivation in G, in G′ we use the matrix (X →b, A →x),
hence Y = b.
If the rule cA →cA′ is not used at the ﬁrst step, then the computation
never stops: at step 5 we cannot use the rule Y iv →¯Yout, because
membrane j′ remains of thickness 2; this implies that the rule Y iv →†
must be used and the derivation will never stop.
4. If no symbol from N1 is present, but some symbols from N2 are present
in membrane j′, then we have two subcases.
If also A is present and we use the rule cA →cA′ in membrane j′, then,
at the next step we have to use the rule A′ →A′′δ, the membrane is
dissolved, and A′′ is left free in membrane j. Here, the rule A′′ →†
must be used.
If we do not use the rule cA →cA′, then all symbols are moved to
membrane j′′ by means of rules B →B+. In membrane j′′ we use the
rules B →B(1), B(1) →B(2)δ and this membrane is dissolved. The
symbols B(2) are left free in membrane j′, where the rules B(2) →†
must be used.
Consequently, we either correctly simulate the matrix mj, or the trap
symbol † is introduced and the computation never ends.
Note that the barred symbols diﬀerent from ¯b are always sent out of
membranes 2, 3, . . . , k +n−1, until reaching membrane 1. In this membrane,

144
Membrane Computing
the nonterminals lose the bars (while ¯a, for a ∈T, sends out of the system a
copy of a). In this way, the process can be iterated.
We emphasize again that if some symbols from N2 arrive in a mem-
brane i′, for any i, and the corresponding symbol X is not present, then the
computation cannot stop. This means that always the symbols must travel
together, namely together with the symbol from N1 present in the current
conﬁguration. This ensures that the simulation of matrices (X →Y, A →†)
is correctly done: we are sure that A is not present, because all symbols from
N2 are present in the same membrane.
Thus, the equality ΨT (L(Π)) = ΨT (L(G)) follows (the symbol b does not
exit the system).
✷
Note that the system above has a number of membranes which depend
on the number of matrices in the starting matrix grammar. It is an open
problem to ﬁnd a bound to the degree of P systems of this type which are
able to characterize the Parikh sets of computably enumerable languages.
3.7
Normal Forms
The tree describing the membrane structure of the system involved in the
proof of Lemma 3.6 is of the form indicated in Figure 3.8 (actually, we have
considered the precise case of the membrane structure from Figure 3.7). The
number of nodes depends on the number of matrices in the starting grammar;
branches of type i, i′ are associated with matrices used in the non-appearance
checking manner and branches of type i, i′, i′′ are associated with matrices
containing rules used in the appearance checking manner.
❅
❅❅
❅
❅❅
❅
❅❅
❅
❅❅
❅
❅❅



t
t
t
t
t
t
t
t
t
t
t
t
❅
❅❅t
t
❅
❅❅t
1
2
3
4
5
6′
6′′
1′
2′
3′
4′
3′′
4′′
5′
5′′
Figure 3.8: The shape of the tree in the previous proof.

Normal Forms
145
It is a natural question (also of a possible practical importance) whether
or not we can obtain the same result, but using a tree of a simpler form,
for instance, a linear one, without any branch.
Note that the membrane
structure described by such a tree is of the form in Figure 3.9.
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✛
✚
✘
✙
Figure 3.9: A membrane structure described by a linear tree.
We do not have an answer to this question for P systems as considered in
the previous section, but we will obtain such a normal form providing that
a more powerful type of catalysts is used, namely allowing them to have a
“short memory”: each catalyst c is allowed to have two states, c and ¯c; the
rules involving catalysts always switch among these states. Thus, the allowed
rules are of the forms: ca →¯cv, and ¯ca →cv. We say that we use ﬂip-ﬂop
catalysts. We indicate the use of such bi-stable catalysts by writing 2Cat
instead of Cat. If the number of catalysts is bounded by a constant r, then
the corresponding families of languages are denoted by LP ±
m(2Catr, δ, τ),
possibly with the subscript m replaced by ∗.
We also look for an “orthogonal normal form”, where the tree is of a
minimal depth, a star. In the same framework as before – with bi-stable
catalysts and actions δ, τ – we will obtain a normal form theorem similar to
that announced above.
There is here an important point. In systems of the type described in Fig-
ure 3.9 there is no need to give any target indication, always we have exactly
one lower membrane (excepting the central membrane) and one upper mem-
brane (excepting the skin membrane). Thus, when we want to communicate,
we can only indicate in and out, without any other information. (Speciﬁcally,
we write ain, aout if we want to send a into any lower level membrane or out
of the current membrane, respectively.) What about using such loose indica-
tions in systems with a non-linear graph and allowing to the symbols to go
to any membrane accessible in the indicated direction? (Of course, always
an object can pass through one membrane only.) Surprisingly enough, in our
setup this does not decrease the power; furthermore, a representation of pCE

146
Membrane Computing
is obtained by systems using a membrane structure of depth two, hence with
a star graph. Note that in the case of a star, the ambiguity of an indication
of the form ain introduced by a rule in the skin membrane is maximal, all
second level membranes can be the target of the symbol a.
When dealing with only in/out indications, no sign +, −will be associ-
ated with objects and membranes; the corresponding families are denoted by
LP i/o
∗
(2Cat, α, β), α ∈{δ, nδ}, β ∈{τ, nτ}.
Let us ﬁrst prove the fact that the bi-stable catalysts are very powerful
(if their number is not restricted).
Theorem 3.12 pCE ⊆LP i/o
1
(2Cat, nδ, nτ).
Proof.
We use again the equality MATac = CE. Let us consider a ma-
trix grammar with appearance checking, G = (N, T, S, M, F), in the binary
normal form, that is, with N = N1 ∪N2 ∪{S, †} and matrices of the four
types speciﬁed in Section 3.3. Assume that the two-rules matrices of M are
labelled in a one-to-one manner by m1, . . . , mn; that is, we assume that we
have n matrices of this type.
We construct the P system (of degree one)
Π = (V, T, C, µ, w1, R1),
with
V
=
N1 ∪N2 ∪T ∪C ∪{b, d, e, e′, e′′, f, †}
∪
{Ei | 1 ≤i ≤n} ∪{X′ | X ∈N1},
C
=
{ci, ¯ci | 0 ≤i ≤n},
µ
=
[1 ]1,
w1
=
XAbec0c1c2 . . . cn, for (S →XA) the initial matrix of G,
and R1 containing the following rules:
1. For each matrix mi of the form (Xi →zi, Ai →xi), with Xi ∈N1, zi ∈
N1 ∪{λ}, Ai ∈N2, xi ∈(N2 ∪T)∗, we consider the rules:
Xi →X′
i,
ciX′
i →¯ci,
¯ciAi →ciei,
ei →zixi,
¯cib →ci † .
2. For each matrix mi of the form (Xi →Yi, Ai →#), with Xi, Yi ∈
N1, Ai ∈N2, we consider the rules:
Xi →X′
i,
ciX′
i →¯cif,
¯ciAi →ci†,
¯cid →ciYi.

Normal Forms
147
3. Moreover, we consider the following rules:
f →d,
a →aout, for all a ∈T,
c0e →¯c0e′,
¯c0B →c0B,
c0e′ →¯c0e′′,
¯c0e′′ →c0e,
e′′ →†, for all B ∈N2.
The system works as follows.
At each moment, at most one symbol from N1 is present, therefore at
most one catalyst ci, 1 ≤i ≤n, can be used. At the ﬁrst step, the symbol
from N1 is primed.
If we apply a rule c1X′
i →¯ci, associated with a matrix mi : (Xi →
zi, Ai →xi), hence not used in the appearance checking mode, then at the
next step we have to use the rule ¯ciAi →ciEi, otherwise the trap-object †
is introduced and the computation will never stop. At the fourth step, the
symbols of Yixi are introduced. Thus, the matrix mi is correctly simulated
(both its rules are simulated).
If we apply a rule ciX′
i →¯cif, associated with a matrix mi : (Xi →
Yi, Ai →#), hence used in the appearance checking mode, then at the next
step we either apply the rule ¯ciAi →ci†, if Ai is present, or the catalyst ¯ci
is not modiﬁed, if Ai is not present. In the ﬁrst case, the computation never
halts. The symbol f is immediately transformed in d; thus, at the third step,
the rule ¯cid →ciYi can be used. In the former case the computation will
never stop, in the latter case the matrix is correctly simulated.
The simulation of any matrix takes always four steps.
The process can be iterated. Note that in both cases the symbol Yi is
available only after completing the simulation of a matrix and that no symbol
from N2 can be processed if we do not ﬁrst process a symbol from N1.
The terminal symbols can be sent out of the system at any moment,
so any permutation of a string can be obtained.
As long as any symbol
B ∈N2 is present, the computation can continue. This is ensured by the
rules which involve the catalyst c0: in cycles of four steps, at step two, a
symbol B ∈N2 is used by the rule ¯c0B →c0B. Note that the simulation of
matrices use symbols from N2 at the third step, hence the simulation is not
confused by the use of rules in group 3. Moreover, at step four we have to
use the rule ¯c0e′′ →c0e (instead of ¯c0B →c0B for some B ∈N2), otherwise
the trap-object is introduced. Therefore, on the one hand, we have to send
out all terminal symbols, on the other hand, a computation stops only if it
corresponds to a terminal derivation in G.
Consequently, p(L(G)) = L(Π).
✷
Corollary 3.1 PsCE = PsLP i/o
∗
(2Cat, nδ, nτ).

148
Membrane Computing
In the previous construction, the number of catalysts can be arbitrarily
large, it depends on the number of matrices in the starting grammar. If we
bound the number of catalysts we use, then we obtain a similar result, but
without having a bound on the number of membranes. Thus, a trade-oﬀ
between the number of membranes and the number of catalysts seems to
hold.
Theorem 3.13 pCE ⊆LP i/o
∗
(2Cat2, δ, τ); moreover, P systems with a lin-
ear underlying tree suﬃce.
Proof.
As in the previous proof, consider a matrix grammar G =
(N, T, S, M, F) in the binary normal form, with n matrices m1, . . . , mn.
We construct the P system (of degree n + 1)
Π = (V, T, C, µ, w0, w1, . . . , wn, R0, R1, . . . , Rn)
(the skin membrane is labelled with 0) with the following components:
V = N1 ∪N2 ∪T ∪C ∪{d1, d2, d′
1, d′
2, d′′
1, d′′
2, e, †}
∪{α′, α′′, α′′′, ¯α | α ∈N1 ∪N2}
∪{αiv | α ∈N1 ∪N2 ∪T}
∪{(α, i) | α ∈N1 ∪N2, 1 ≤i ≤n}
∪{fi | 1 ≤i ≤n},
C = {c1, c2, ¯c1, ¯c2},
µ = [0[1[2 . . . [n−1[n ]n]n−1 . . . ]2]1]0,
w0 = XA, for (S →XA) being the initial matrix of G,
wi = c1c2fi, 1 ≤i ≤n,
and with the sets R0, R1, . . . , Rn constructed as follows.
1. The set R0 contains the following rules:
1. α →(α, i)in, for α ∈N1 ∪N2, 1 ≤i ≤n
(send down nonterminals, at random addresses indicated by the
second component of (α, i)),
2. a →a,
a →aout, for a ∈T
(send out terminals, at any time after having them in the skin
membrane),
3. fi →†, for all 1 ≤i ≤n,
† →†
(trap-rules; once introduced, the symbol † can evolve for ever).
2. Each set Ri, 1 ≤i ≤n, contains the following rules (slight diﬀerences
for the cases i = 1 and i = n will be mentioned below):

Normal Forms
149
1. (α, j) →(α, j)in, for α ∈N1 ∪N2, i < j ≤n
(send down nonterminals, to the addresses speciﬁed in the skin
membrane),
2. (α, i) →¯α
(when a symbol sent to membrane i reaches this membrane, its
barred version is introduced),
3. ¯α →αδ, for α ∈N2,
¯β →βτ, for β ∈N1
(these rules check whether or not all nonterminals are present in
the same place; this is a very important point of the construction
– see below complete explanations),
4. fj →†, for all i < j ≤n,
† →†
(if any symbol fj with j > i reaches membrane i, then the com-
putation never stops),
5. αiv →αiv
out, for α ∈N1 ∪N2 ∪T
(the symbols marked with iv are sent to the upper membrane).
3. In the set R1, instead of rules of type 5 above we introduce the rules
4′. αiv →αout, for α ∈N1 ∪N2 ∪T
(the symbols reach the skin membrane without any marking).
In the set Rn no rule of type 1 above is introduced.
4. If the matrix mi is of the form (X →z, A →x), for some X ∈N1, A ∈
N2, x ∈(N2 ∪T)∗, and z ∈N1 ∪{λ} (we cover at the same time both
the case of nonterminal and of terminal matrices which are not used
in the appearance checking mode), then in Ri we also introduce the
following rules:
1. α →α′, for α ∈N1 ∪N2,
c1X →¯c1h(z)d′
1e,
c2A →¯c2h(x)d′
2e,
where h is the morphism deﬁned by
h(α) =

α′,
if α ∈N1 ∪N2,
αiv
out,
if α ∈T
(we simulate the two rules of matrix mi),
2. α′ →α′′, for α ∈N1 ∪N2,
¯c1d′
1 →c1d′′
1,
¯c2d′
1 →c2d′′
2,
c1e →¯c1†,
c2e →¯c2†,

150
Membrane Computing
c2d′
1 →¯c2†,
c1d′
2 →¯c1†
(we check whether or not both rules were simulated; if only one of
them was simulated, then the trap-symbol † is introduced and the
computation never ends; see more complete explanations below),
3. α′′ →α′′′, for α ∈N1 ∪N2,
c1d′′
1 →¯c1,
c2d′′
2 →¯c2
(the auxiliary symbols d1, d2, in their primed versions, are re-
moved),
4. α′′′ →αiv
out, for α ∈N1 ∪N2,
¯c1e →c1,
¯c2e →c2
(also the two copies of the auxiliary symbol e are removed; the
catalysts return to their non-barred state).
5. If the matrix mi is of the form (X →Y, A →†), for X, Y ∈N1, A ∈N2
(hence with the second rule used in the appearance checking manner),
then we introduce in Ri the following rules:
1. X →Y iv
out,
Z →†, for all Z ∈N1 −{X},
2. A →†,
3. α →αiv
out, for all α ∈N2 −{A}
(all nonterminals go out, except A; if A is present, then the com-
putation will never ﬁnish).
We claim that p(L(G)) = L(Π), which would conclude the proof.
Let us examine the work of the system Π. As already sketched above, the
skin membrane sends randomly the current nonterminal symbols to the inner
membranes, by attaching to them target integers: each α is replaced with
(α, i), for some 1 ≤i ≤n. Membranes 1, 2, . . . , n simulate the corresponding
matrices m1, m2, . . . , mn.
The symbols (α, i) are sent down until reaching the membrane i. This
means exactly i steps (counting also the step when the rules α →(α, i) were
used). When (α, i) reaches membrane i, we introduce the barred variant of
α. It is important to note that all symbols sent to membrane i arrive in this
membrane at the same time.
Our aim is to simulate in Π derivations of G. Initially, we have in the
skin membrane the symbols XA. If the simulation is correct, then always we
will have in our system only one occurrence of a symbol from N1. Assume
that we start from such a situation, that is, from a multiset in membrane 0
which contains exactly one occurrence of a symbol from N1.
Suppose that a nonterminal A ∈N2 has reached a membrane i, but the
currently available symbol X ∈N1 is not present in the same membrane.

Normal Forms
151
Then, the rule ¯A →Aδ is used in membrane i, without also using a rule of
the type ¯β →βτ, for some β ∈N1. In this way, the membrane is dissolved,
the symbol fi is left free in the upper membrane (or a membrane placed at
a higher level, in the case when several membranes are dissolved at the same
time). In the upper membrane (or any superior one) we can use the rule
fi →† and the computation is never ﬁnished.
Consequently, all the nonterminals from N2 present in the system must
be together with the unique symbol from N1. This means that all the nonter-
minals are together. Although the skin membrane uses the rules α →(α, i)
randomly, the computation will continue in a correct way only when all sym-
bols gets the same “address” i. This is a crucial observation for the good
functioning of our system, for instance, in the case of simulating the matrices
with appearance checking rules.
Let us now look how the matrices of G are simulated.
Consider ﬁrst the case of a matrix mi of the form (X →Y, A →†), hence
with the second rule used in the appearance checking manner. Assume that
all nonterminals have reached membrane i. We can change X with Y iv
out and
continue correctly only if no occurrence of A is present, otherwise the trap-
symbol † is introduced. If a symbol Z ∈N1 diﬀerent from X is present (this
means that membrane 0 has incorrectly guessed the membrane where the
nonterminals are sent), then again the trap-symbol † is introduced. Thus, we
either correctly simulate the matrix, or the computation will never end. Note
here the important role of the maximal parallelism: if a rule can be applied
to an available symbol, then it must be applied.
Assume now that we are in a membrane i associated with a matrix mi =
(X →Y, A →x). If we use only rules of the forms α →α′, α′ →α′′, α′′ →
α′′′, α′′′ →αiv
out, then we can return all the symbols unmodiﬁed to the skin
membrane. In particular, we can simulate only one rule of the matrix, using
only one rule from the pair c1X →¯c1Y ′d′
1e, c2A →¯c2h(x)d′
2e.
Assume
that the ﬁrst rule is used, the second not; the other case is symmetric. This
means that in membrane i we have the symbols ¯c1, c2, d′
1, e. The symbol e
can now be paired either with ¯c1 or with c2, that is one of the rules ¯c1e →c1,
c2e →¯c2† must be used.
The second rule introduces the trap-symbol †,
hence the computation will never stop. If we use the ﬁrst rule, then also the
rule c2d′
1 →¯c2† must be used, hence again the trap-symbol † is introduced.
Therefore, both rules c1X →¯c1Y ′d′
1e, c2A →¯c2h(x)d′
2e must be used. The
catalysts (barred or not) will remove the auxiliary symbols d1, d2, e and will
return to their non-barred forms. This is done in four steps and exactly at
the same time all the nonterminals present in the membrane will be sent to
the upper membrane, in the form αiv. Such symbols will be pushed up until
they again reach the skin membrane.
The simulation of a terminal matrix (X →λ, A →x) is done exactly in
the same way.
Note that any terminal symbol is immediately sent up. In the skin mem-

152
Membrane Computing
brane, each terminal can wait as long as we want (by using rules a →a)
or it can be sent out of the system (by rules a →aout). In this way, all
permutations of a string in L(G) can be obtained.
After simulating a terminal matrix, we remove the symbol from N1, there-
fore if any symbol from N2 is still present in the system, then the computation
will never halt (such nonterminals will dissolve membranes, hence symbols fi
will be released in upper membranes). Thus, a computation in Π halts only
if the corresponding derivation in G is a terminal one.
It is now clear that each derivation in G can be simulated in Π in such a
way that any permutation of the terminal string generated by this derivation
can be produced at the end of the computation and, conversely, all compu-
tations in Π which end in a correct way correspond to terminal derivations
in G.
✷
Note that during a correctly ended computation the actions δ, τ are never
used; these operations are only useful for preventing “wrong” steps, which
do not correspond to correct derivation steps in the grammar G.
The depth of the system in the previous proof depends on the number
of matrices in the grammar we start with. Let us now look for a graph of a
minimal depth. A counterpart of the previous theorem can be obtained (even
using no target indication when sending symbols from the skin membrane
to the lower level membranes; in this moment, this is somewhat expected,
taking into account the way of keeping together all the nonterminal symbols,
by using the actions δ, τ).
Theorem 3.14 Each language p(L), L ∈CE, can be generated by a P sys-
tem of type (2Cat2, δ, τ), communicating by using in/out indications only,
and with a membrane structure of depth two.
Proof.
The proof is similar to that of Theorem 3.13, but because of the
importance of this result, for the sake of completeness, we give the core
construction with almost full details.
Start again from a matrix grammar with appearance checking, G =
(N, T, S, M, F), in the binary normal form, with n matrices, m1, . . . , mn.
We construct the P system (of degree n + 1)
Π = (V, T, C, µ, w0, w1, . . . , wn, R0, R1, . . . , Rn)
(the skin membrane is labelled with 0) with the following components:
V = N1 ∪N2 ∪T ∪C ∪{d1, d2, d′
1, d′
2, d′′
1, d′′
2, e, f, †}
∪{α′, α′′, α′′′, ¯α | α ∈N1 ∪N2}
C = {c1, c2, ¯c1, ¯c2},
µ = [0[1 ]1 . . . [n ]n]0,
w0 = XA, for (S →XA) being the initial matrix of G,
wi = c1c2f, 1 ≤i ≤n,

Normal Forms
153
and with the sets R0, R1, . . . , Rn constructed as follows.
1. R0 contains the following rules:
1. α →¯αin, for α ∈N1 ∪N2
(send down nonterminals, to any membrane 1, 2, . . . , n),
2. a →a,
a →aout, for a ∈T
(send out terminals, at any time after having them in the skin
membrane),
3. f →†,
† →†
(trap-rules).
2. Each set Ri, 1 ≤i ≤n, contains the following rules:
1. ¯α →αδ, for α ∈N2,
¯β →βτ, for β ∈N1
(check whether or not all nonterminals are present in the same
place),
2. † →†.
3. If the matrix mi is of the form (X →z, A →x), for some X ∈N1, A ∈
N2, x ∈(N2 ∪T)∗, and z ∈N1 ∪{λ}, then in Ri we introduce the rules
of types 4.1 – 4.3 from the previous proof, as well as the following rules:
α′′′ →αout, for α ∈N1 ∪N2,
¯c1e →c1,
¯c2e →c2
(also the auxiliary symbol e is removed; the catalysts return to
their non-barred state).
4. If the matrix mi is of the form (X →Y, A →†), for X, Y ∈N1, A ∈N2
(hence with the second rule used in the appearance checking manner),
then we introduce to Ri the following rules:
1. X →Yout,
Z →†, for all Z ∈N1 −{X},
2. A →†,
3. α →αout, for all α ∈N2 −{A}
(all nonterminals go out, except A; if A is present, then the com-
putation will never ﬁnish).
In the same way as in the proof of Theorem 3.13, we have the equality
p(L(G)) = L(Π).
It is clear that the membrane structure of Π is of the
desired type.
✷

154
Membrane Computing
A similar result is obtained in [237] for a class of P systems which is not
know to characterize the computably enumerable sets of vectors; we give this
theorem without a proof (note that in [237] one deals with target indications
of the form inj and priorities, but not with catalysts and action τ):
Theorem 3.15 For each language L ∈LPm(Pri, nCat, δ), m ≥1, we can
construct a P system Π of degree m and of depth 2 such that L(Π) = L.
3.8
P Systems on Asymmetric Graphs
The observation that a membrane structure corresponds to a tree and that we
can compute directly on a tree suggests the following general idea: consider
computing devices similar to P systems (we call them P′ systems), using an
arbitrary graph as an underlying structure. Some of the operations speciﬁc to
P systems (using evolution rules in parallel, maybe subject to some priority
relations, moving symbols from a node to another one, reading the result in
a node or outside the graph) can be easily extended to such a general case,
but others should be carefully deﬁned. This is the case with the dissolving
operation, δ, and with its dual, τ.
✬
✫
✩
✪
✚✚✚✚✚✚✚✚✚✚✚✚✚✚✚✚




















✒
✲
❅
❅
❅
❅
❘
✡
✡
✡
✡
✡✢
✛
❏
❏
❏
❏
❏
Figure 3.10: A possible support for a computation.
The idea of using graphs diﬀerent from trees is not completely unrealistic
from a (bio)chemical point of view. For instance, we can imagine a “mem-
brane structure” as that in Figure 3.10, where six regions are delimited by six
“walls” built in a large “pot”. A computation based on multiset processing
can be done also in such a framework (described by a graph in the form of a
ring).

P Systems on Asymmetric Graphs
155
In general, any type of a planar map (hence planar graph) can be con-
sidered as an underlying structure for a P′ system, but again we look for
structures which are as simple as possible. Here is an immediate question:
what about of the support of the type in Figure 3.10, with a ring as a graph?
There appears here an interesting point. In trees, each edge is supposed
to be used for passing objects in both directions; in most cases, a symbol
which reaches a node must also leave the node. In the case of a ring graph (in
general, for arbitrary graphs) we may ask whether or not we can pass symbols
through walls only in one direction. This corresponds to using asymmetric
graphs: for each vertices i, j, at most one of (i, j), (j, i) is an edge.
In the case of a cycle-ring, the communication must go in one direction
only, that is, each wall is supposed to be one-way. Thus, the only possibility
to send a symbol a from a region to another one can be simply indicated by
ago; as usual, aout means sending a outside the system (this is necessary in
order to collect the result of a computation) and ahere means to keep a in
the same region (the subscript here will be systematically omitted).
We can deﬁne the actions δ, τ also for asymmetric graphs, in the following
way. In general, dissolving a membrane means to move all its objects to the
upper membrane and remove its rules; the membrane itself disappears. In
the case of planar maps it is not clear how a room can disappear (how the
neighbouring rooms must connect each other after removing the room). Thus,
we keep here only the ﬁrst action mentioned above: when δ is introduced by
a rule in a given room and we do not introduce at the same time in that room
also the symbol τ, then all the objects in that room will be communicated,
randomly, to any of the neighbouring rooms to which a communication can
take place (the action is equivalent to the fact that all symbols a in that
room are transformed into ago and they move through the walls which allow
communication). If in some room we introduce both δ and τ at the same
step, then no symbol leaves the room because of δ, but, if there are rules
which introduce symbols aout of ago, then these symbols leave the room. If
τ is introduced in a room without introducing at the same time δ also, then
nothing happens. Thus, actions δ, τ do not act on the walls of the rooms,
but on the objects in that room.
Of course, the case when arbitrarily many ﬂip-ﬂop catalysts are used is
trivial: in view of Theorem 3.12, one room is enough. When we bound the
number of catalysts, we can obtain one more representation of pCE similar
to those in Theorems 3.13 and 3.14:
Theorem 3.16 Each language in pCE can be generated by a P′ system using
two bi-stable catalysts, actions δ, τ, and an underlying graph in the form of a
cycle-ring.
Proof. (Sketch) We proceed as in the proof of Theorem 3.13. Starting from
a matrix grammar with appearance checking in the binary normal form, with
n matrices, m1, . . . , mn, we construct a map like that in Figure 3.10, with

156
Membrane Computing
a room associated with each matrix (labelled with the numbers 1, 2, . . . , n)
and a further room, labelled by 0, placed between rooms n and 1.
Each room i = 1, 2, . . . , n has associated a symbol fi, placed initially
there, together with the two bi-stable catalysts c1, c2. In room 0 we initially
place the objects XA for (S →XA) the S-matrix of G.
From room 0 we send all nonterminal symbols α of G to room 1, in the
form (α, i), where 1 ≤i ≤n (to this aim, we use rules α →(α, i)go). In each
room i we consider rules which send the symbols (α, j) with j > i to the next
room ((α, j) →(α, j)go). For (α, i), we introduce the rules (α, i) →¯α. As
in the proof of Theorem 3.13 we now check whether or not all nonterminals
are in the same room (that is, whether or not in room 0 we have associated
the same address k to all symbols (α, k)). In the opposite case, the symbol δ
is introduced and the symbol fi is sent to the next room. In each room, we
provide rules fj →† for all j smaller than the label of the room.
The simulation of matrices of G are performed in the same way as in the
proof of Theorem 3.13 (using the two bi-stable catalysts and the auxiliary
symbols d1, d2, d′
1, d′
2, e).
For all symbols αiv produced in each room when simulating a matrix, we
introduce the symbols αiv
go; moreover, each room – excepting room number n
– contains rules αiv →αiv
go. In room n, these rules are replaced by αiv →αgo.
In this way, all symbols return (at the same time) to room 0. The process
can be iterated.
As usual, the terminal symbols are collected in room 0, where they can
wait for any number of steps (by using rules a →a) or can be sent out of
the system. After using a terminal matrix, hence after removing the unique
occurrence of a symbol from N1, if any nonterminal is still present, then the
computation never stops: no further matrix can be simulated, the nonter-
minals have to travel through the rooms of the map until reaching a room
where they entail the use of action δ and the trap-symbol † will be produced.
Thus, our system can generate all permutations of all strings in the lan-
guage generated by the grammar we start with. The formal details are left
to the reader.
✷
It is of interest to note that without using the actions δ, τ and using only
two bi-stable catalysts we still can generate a large family of languages; we
omit the proof, which is similar to that of the previous theorem:
Theorem 3.17 Each language in the family pMAT can be generated by a P′
system using two bi-stable catalysts, no action δ, τ, and having a cycle-ring
as the underlying graph.
3.9
P Systems with Active Membranes
In all variants of P systems considered in the previous sections, the number of
membranes can only decrease during a computation, by dissolving membranes

P Systems with Active Membranes
157
as a result of applying evolution rules to the objects present in the system.
A natural possibility is to let the number of membranes also to increase
during a computation, for instance, by division, as it is well-known in biology.
Actually, the membranes from biochemistry are not at all passive, like those in
the models discussed so far. For example, the passing of a chemical compound
through a membrane is often done by a direct interaction with the membrane
itself (with the so-called protein channels or protein gates present in the
membrane); during this interaction, the chemical compound which passes
through membrane can be modiﬁed, while the membrane itself can in this
way be modiﬁed (at least locally).
We will here make use of these observations and we will consider P sys-
tems where the central role in the computation is played by the membranes:
evolution rules are associated both with objects and membranes, while the
communication through membranes is performed with the direct participa-
tion of the membranes; moreover, the membranes cannot only be dissolved,
but they also can multiply by division. An elementary membrane can be di-
vided by means of an interaction with an object from that membrane. As in
Section 3.6, each membrane is supposed to have an “electrical polarization”,
one of the three possible: positive, negative, or neutral. If in a membrane we
have two immediately lower membranes of opposite polarizations, one pos-
itive and one negative, then that membrane can also divide in such a way
that the two membranes of opposite charge are separated; all membranes of
neutral charge and all objects are duplicated and a copy of each of them is
introduced in each of the two new membranes. The skin is never divided.
In this way, the number of membranes can grow, even exponentially. As
expected, by making use of this increased parallelism we can compute faster.
We will see that this is the case, indeed: SAT can be solved in this framework
in linear time (the time units are steps of a computation in a P system as
sketched above, where we perform in parallel, in all membranes of the system,
applications of evolution rules or division of membranes).
Moreover, the
model is shown to be computationally universal: any computably enumerable
set of (vectors of) natural numbers can be generated by our systems.
A P system with active membranes is a construct
Π = (V, T, H, µ, w1, . . . , wm, R),
where:
(i) m ≥1 (the initial degree of the system);
(ii) V is an alphabet (the total alphabet of the system);
(iii) T ⊆V (the terminal alphabet);
(iv) H is a ﬁnite set of labels for membranes;

158
Membrane Computing
(v) µ is a membrane structure, consisting of m membranes, labelled (not
necessarily in a one-to-one manner) with elements of H; all membranes
in µ are supposed to be neutral;
(vi) w1, . . . , wm are strings over V , describing the multisets of objects placed
in the m regions of µ;
(vii) R is a ﬁnite set of developmental rules, of the following forms:
(a) [ha →v]α
h,
for h ∈H, α ∈{+, −, 0}, a ∈V, v ∈V ∗
(object evolution rules, associated with membranes and depending
on the label and the charge of the membranes, but not directly
implying the membranes, in the sense that the membranes are
neither taking part to the application of these rules nor are they
modiﬁed by them);
(b) a[h ]α1
h →[hb]α2
h ,
for h ∈H, α1, α2 ∈{+, −, 0}, a, b ∈V
(communication rules; an object is introduced in the membrane,
maybe modiﬁed during this process; also the polarization of the
membrane can be modiﬁed, but not its label);
(c) [ha ]α1
h →[h ]α2
h b,
for h ∈H, α1, α2 ∈{+, −, 0}, a, b ∈V
(communication rules; an object is sent out of the membrane,
maybe modiﬁed during this process; also the polarization of the
membrane can be modiﬁed, but not its label);
(d) [ha ]α
h →b,
for h ∈H, α ∈{+, −, 0}, a, b ∈V
(dissolving rules; in reaction with an object, a membrane can be
dissolved, while the object speciﬁed in the rule can be modiﬁed);
(e) [ha ]α1
h →[hb ]α2
h [hc ]α3
h ,
for h ∈H, α1, α2, α3 ∈{+, −, 0}, a, b, c ∈V
(division rules for elementary membranes; in reaction with an ob-
ject, the membrane is divided into two membranes with the same
label, maybe of diﬀerent polarizations; the object speciﬁed in the
rule is replaced in the two new membranes by possibly new objects;
all other objects are reproduced in both the two new membranes);
(f) [h0[h1 ]α1
h1 . . . [hk ]α1
hk [hk+1 ]α2
hk+1 . . . [hn ]α2
hn]α0
h0
→[h0[h1 ]α3
h1 . . . [hk ]α3
hk]α5
h0 [h0[hk+1 ]α4
hk+1 . . . [hn ]α4
hn]α6
h0 ,
for k ≥1, n > k, hi ∈H, 0 ≤i ≤n, and α0, . . . , α6 ∈{+, −, 0}
with {α1, α2} = {+, −}; if this membrane with the label h0 con-
tains other membranes than those with the labels h1, . . . , hn spec-
iﬁed above, then they should have neutral charge in order to may
apply this rule

P Systems with Active Membranes
159
(division of non-elementary membranes; this is possible only if a
membrane contains two immediately lower membranes of opposite
polarization, + and −; the membranes of opposite polarizations
are separated in the two new membranes, but their polarization
can change; always, all membranes of opposite polarizations are
separated by applying this rule; all objects and all other mem-
branes from membrane h0 are reproduced in both the two new
membranes with label h0).
Note that in all rules of types (a)–(e) only one object is speciﬁed (that is,
the objects do not directly interact) and that, with the exception of rules of
type (a), always single objects are transformed into single objects (the two
objects produced by a division rule of type (e) are placed in two diﬀerent
regions).
These rules are applied according to the following principles:
1. All the rules are applied in parallel: in a step, the rules of type (a)
are applied to all objects to which they can be applied, all other rules
are applied to all membranes to which they can be applied; an object
can be used by only one rule, non-deterministically chosen (there is no
priority relation among rules), but any object which can evolve by a
rule of any form, should evolve.
2. If a membrane is dissolved, then all the objects in its region are left free
in the region immediately above it. Because all rules are associated with
membranes, the rules of a dissolved membrane are no longer available
at the next steps. The skin membrane is never dissolved.
3. All objects and membranes not speciﬁed in a rule and which do not
evolve are passed unchanged to the next step. For instance, if a mem-
brane with the label h is divided by a rule of type (e) which involves
an object a, then all other objects in membrane h which do not evolve
are introduced in each of the two resulting membranes h. Similarly,
when dividing a membrane h by means of a rule of type (f), the neutral
membranes are reproduced in each of the two new membranes with the
label h, unchanged if no rule is applied to them (in particular, the con-
tents of these neutral membranes are reproduced unchanged in these
copies, providing that no rule is applied to their objects).
4. If at the same time a membrane h is divided by a rule of type (e) and
there are objects in this membrane which evolve by means of rules of
type (a), then in the new copies of the membrane we introduce the
result of the evolution; that is, we may suppose that ﬁrst the evolution
rules of type (a) are used, changing the objects, and then the division is
produced, so that in the two new membranes with label h we introduce
copies of the changed objects. Of course, this process takes only one
step. The same assertions apply to the division by means of a rule of

160
Membrane Computing
type (f): always we assume that the rules are applied “from bottom-
up”, in one step, but ﬁrst the rules of the innermost region and then
level by level until the region of the skin membrane.
5. The rules associated with a membrane h are used for all copies of this
membrane, irrespective whether or not the membrane is an initial one
or it is obtained by division. At one step, a membrane h can be the
subject of only one rule of types (b)–(f).
6. The skin membrane can never divide. As any other membrane, the skin
membrane can be “electrically charged”.
We can pass from a conﬁguration to another one by using the rules from
R according to the principles given above. We say that we have a (direct)
transition among conﬁgurations. We do not deﬁne formally a transition. We
will immediately consider an example which can enlighten the idea.
As usual, a computation is complete if it cannot be continued: there is no
rule which can be applied to objects and membranes in the last conﬁguration.
Note that during a computation the number of membranes (hence the
degree of the system) can increase and decrease but the labels of these mem-
branes are always among the labels of membranes present in the initial con-
ﬁguration (by division we only produce membranes with the same label as
the label of the divided membrane).
During a computation, objects can leave the skin membrane (by means
of rules of type (c)). By arranging these symbols in a string, a language is
associated with Π, denoted by L(Π).
In order to prove the usefulness of using active membranes (in particular,
membrane division) and in order to illuminate the informal deﬁnition of a
transition in a P system as given above, we will consider an example which
is also very signiﬁcant by itself: solving the SAT problem by a P system with
active membranes.
Theorem 3.18 The SAT problem can be solved by a P system with active
membranes in a time which is linear in the number of variables and the
number of clauses.
Proof.
Let us consider a propositional formula
γ = C1 ∧C2 ∧. . . ∧Cm,
with
Ci = yi,1 ∨. . . ∨yi,pi,
for some m ≥1, pi ≥1, and yi,j ∈{xk, ∼xk | 1 ≤k ≤n}, for each 1 ≤i ≤m,
1 ≤j ≤pi.
We construct the P system
Π = (V, T, H, µ, w0, w1, . . . , wm, wm+1, R)

P Systems with Active Membranes
161
with the components
V = {ai, ti, fi | 1 ≤i ≤n}
∪{ci | 0 ≤i ≤2n + m −1}
∪{t},
T = {t},
H = {0, 1, . . . , m + 1},
µ = [m+1[m[m−1 . . . [1[0 ]0
0]0
1 . . . ]0
m−1]0
m]0
m+1,
w0 = c0a1a2 . . . an,
wi = λ, for all i = 1, 2, . . . , m + 1,
while the set R contains the following rules:
1. [0ci →ci+1]α
0 , for all 0 ≤i ≤2n + m −2 and α ∈{+, −, 0}
(we count to 2n + m −1, which is the time needed for produc-
ing all 2n truth assignments for the n variables, as well as 2n
membrane sub-structures which will examine the truth value of
formula γ for each of these truth assignments; this counting is done in
the central membrane, irrespective which is its polarity);
2. [0ai]0
0 →[0ti]+
0 [0fi]−
0 , for all 1 ≤i ≤n
(in membrane 0, when it is “electrically neutral”, we non-deter-
ministically choose one variable xi and both values true and false are
associated with it, in the form of objects ti, fi, which are separated
in two membranes with the label 0 which diﬀer only by these objects
ti, fi and by their charge);
3. [0c2n+m−1]0
0 →t
(after 2n + m −1 steps, each copy of membrane 0 is dissolved
and its contents are released in the upper membranes, those labelled
with 1);
4. [jti]0
j →ti, if xi appears in clause Cj, 1 ≤i ≤n, 1 ≤j ≤m, and
[jfi]0
j →fi, if ∼xi appears in clause Cj, 1 ≤i ≤n, 1 ≤j ≤m
(a membrane with label j, 1 ≤j ≤m, is dissolved if and only
if clause Cj is satisﬁed by the current truth assignment; if this is the
case, then the truth values associated with the variables are released
in the upper membrane, that associated with the next clause, Cj+1,
otherwise these truth values remain blocked in membrane j and never
used at the next steps by the membranes placed above; note that, as

162
Membrane Computing
we will see immediately, after 2n + m −1 steps we have 2n membrane
sub-structures of the form [m[m−1 . . . [1 ]0
1 . . . ]0
m−1]0
m working in
parallel in the skin membrane);
5. [m+1t]0
m+1 →[m+1 ]+
m+1t
(together with the truth assignments, we also have the object t,
which can be passed from a level to the upper one only by dissolving
membranes; this object reaches the skin membrane if and only if all
membranes in a sub-structure of the form [m[m−1 . . . [1 ]0
1 . . . ]0
m−1]0
m
are dissolved, which means that the associated truth assignment has
satisﬁed all the clauses, that is, the formula is satisﬁable; therefore, t
leaves the system if and only if the formula is satisﬁable; when this
rule is applied, the skin membrane gets a “positive charge”, so the rule
can be applied only once);
6. [i+1[i ]+
i [i ]−
i ]0
i+1 →[i+1[i ]0
i ]+
i+1[i+1[i ]0
i ]−
i+1, for all 0 ≤i ≤m−2, and
[m[m−1 ]+
m−1[m−1 ]−
m−1]0
m →[m[m−1 ]0
m−1]0
m[m[m−1 ]0
m−1]0
m
(division rules for membranes labelled with 0, 1, . . . , m; the opposite
polarization introduced when dividing a membrane 0 is propagated
from lower levels to upper levels of the membrane structure and the
membranes are continuously divided until dividing also membrane m
– which will get a neutral charge).
From the previous explanations one can easily see that
L(Π) =

{t},
if formula γ is satisﬁable,
∅,
otherwise.
Therefore, we get the answer to the question whether or not γ is satisﬁable by
examining the emptiness of the language L(Π) (by checking whether or not
any object leaves the system Π during the computation). This is achieved
in 2n + 2m + 1 steps: in 2n + m −1 steps we create the 2n membrane sub-
structures of the form [m[m−1 . . . [1 ]0
1 . . . ]0
m−1]0
m (as well as the 2n diﬀerent
truth assignments), then we dissolve all membranes 0 (one further step) and
we check the satisﬁability of each clause for each truth assignment, in par-
allel in the 2n sub-structures (this takes further m steps); one more step is
necessary in order to send out of the system one copy of the object t, if any
copy of t has reached the skin membrane. If no copy of t leaves the system
at this step, then the formula is not satisﬁable.
✷
Note that we have used rules of all types (a)–(f), excepting the type (b),
and that also in the case of rules of type (a) we have object-to-object rules
(and not object-to-multiset).

P Systems with Active Membranes
163
We illustrate the previous construction and the work of the system Π
obtained in this way for the propositional formula
β = (x1 ∨x2) ∧(∼x1∨∼x2),
also considered in Section 2.5.
Thus, n = 2, m = 2. The initial conﬁguration of the system is
[3[2[1[0c0a1a2]0
0]0
1]0
2]0
3.
The computation proceeds as follows (we specify the current conﬁguration at
each step):
Step 0: [3[2[1[0c0a1a2]0
0]0
1]0
2]0
3;
Step 1: [3[2[1[0c1t1a2]+
0 [0c1f1a2]−
0 ]0
1]0
2]0
3
(in
parallel,
the
rule
[0c0
→
c1]0
0
and
the
division
rule
[0a1]0
0 →[0t1]+
0 [0f1]−
0
were used; membrane 1 must immediately
divide, because of the two copies of membrane 0 with opposite
polarizations);
Step 2: [3[2[1[0c2t1a2]0
0]+
1 [1[0c2f1a2]0
0]−
1 ]0
2]0
3
(the counter c1 is replaced with c2 and membrane 1 is divided;
because the two membranes with label 0 are not of neutral polarity, no
new truth value is introduced at this step; at the next step, membrane
2 must divide);
Step 3: [3[2[1[0c3t1t2]+
0 [0c3t1f2]−
0 ]0
1]0
2[2[1[0c3f1t2]+
0 [0c3f1f2]−
0 ]0
1]0
2]0
3
(simultaneously, the counter c2 is replaced by c3, each membrane
0 is divided again, producing membranes of opposite polarity, and
membrane 2 is also divided, because of the existence of the two
membranes 1 with opposite polarity; the generation of the truth
assignments is completed, but we still have to divide membranes,
because of the existence of membranes of opposite polarity);
Step 4:
[3[2[1[0c4t1t2]0
0]+
1 [1[0c4t1f2]0
0]−
1 ]0
2[2[1[0c4f1t2]0
0]+
1 [1[0c4f1f2]0
0]−
1 ]0
2]0
3
(we divide the two membranes 1,
in parallel,
and we increase
the counter; no other rule can be applied);
Step 5: [3[2[1[0c5t1t2]0
0]0
1]0
2[2[1[0c5t1f2]0
0]0
1]0
2[2[1[0c5f1t2]0
0]0
1]0
2
[2[1[0c5f1f2]0
0]0
1]0
2]0
3

164
Membrane Computing
(we increase again the counter and we divide the two membranes 2).
The membrane structure (and the contents of membranes with label 0)
is represented in Figure 3.11. One sees that for each truth assignment
we have a sub-structure of the form [2[1[0 ]0]1]2. All membranes have
neutral polarity.
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
✬
✫
✩
✪
3
2
1
0
2
1
0
2
1
0
2
1
0
c5t1t2
c5t1f2
c5f1t2
c5f1f2
Figure 3.11: The membrane structure in the example, after Step 5.
Step 6: [3[2[1tt1t2]0
1]0
2[2[1tt1f2]0
1]0
2[2[1tf1t2]0
1]0
2[2[1tf1f2]0
1]0
2]0
3
(the counter has reached its maximal value; it can be transformed in t
while dissolving all membranes 0);
Step 7: [3[2tt1t2]0
2[2tt1f2]0
2[2tf1t2]0
2[2[1tf1f2]0
1]0
2]0
3
(clause 1 is satisﬁed by the ﬁrst three truth assignments;
the
corresponding membranes with label 1 are dissolved; the last truth
assignment does not satisfy the ﬁrst clause, its associated membrane 1
remains unchanged and the truth values in it will be of no use from
now on);
Step 8: [3[2tt1t2]0
2tt1f2tf1t2[2[1tf1f2]0
1]0
2]0
3
(clause 2 is satisﬁed by the second and the third truth assign-
ments, so the corresponding membranes 2 are dissolved; two copies of
t are left free in the skin membrane, corresponding to the two truth
assignments which satisfy the formula);
Step 9: One of the two copies of t will be sent out of the system, so we
know that the formula was satisﬁed. This is the last step of the com-
putation, because no further rule can be applied (the skin membrane
is now “positively charged”, so the second copy of t cannot leave it).

P Systems with Active Membranes
165
It is of a clear interest to ﬁnd also other NP-complete problems which can
be solved (directly, not via SAT) by means of P systems with active mem-
branes. We leave this research topic to the reader and we pass to investigate
the computability power of P systems with active membranes.
We denote by LPA the family of languages L(Π), generated by P systems
with active membranes as (informally) deﬁned above.
Theorem 3.19 PsCE = PsLPA.
Proof.
The inclusion LPA ⊆CE follows from Church–Turing thesis or can
be proved directly, in a straightforward (but involving a long construction)
way. This implies the inclusion PsLPA ⊆PsCE. So, we only have to prove
the inclusion PsCE ⊆PsLPA, which is done in the following lemma.
✷
Lemma 3.7 (The Computational Completeness Lemma for P Systems with
Active Membranes) PsCE ⊆PsLPA.
Proof.
We make use of the equality PsCE = PsMATac.
Let G =
(N, T, S, M, F) be a matrix grammar with appearance checking in the bi-
nary normal form, with N = N1 ∪N2 ∪{S, †}. Each matrix of the form
(X →λ, A →x), X ∈N1, A ∈N2, x ∈T ∗, is replaced by (X →Z, A →x),
where Z is a new symbol. We denote by G′ the obtained grammar. As-
sume that we have n1 matrices of the form (X →Y, A →x), with
X ∈N1, Y
∈N1 ∪{Z}, x ∈(N2 ∪T)∗, and n2 matrices of the form
(X →Y, A →†), X, Y ∈N1, A ∈N2.
(That is, we consider separately
the matrices having rules used in the appearance checking mode and the
matrices not having such rules.)
We construct the P system (with p = n1 + n2 + 2 initial membranes)
Π = (V, T, H, µ, w1, . . . , wp, R),
with
V = N1 ∪N2 ∪T ∪{Z, †}
∪{ ¯X | X ∈N1}
∪{⟨x⟩| x ∈(N2 ∪T)∗, (X →Y, A →x) is a matrix in G′},
H = {1, 2, . . . , p},
µ = [p[p−1[1 ]0
1[2 ]0
2 . . . [n1 ]0
n1[n1+1 ]0
n1+1 . . . [n1+n2 ]0
n1+n2]p−1]0
p,
wi = λ, for all i ∈H −{p −1},
wp−1 = XA, for (S →XA) the initial matrix of G,
and the following set R of rules:
1. For each matrix mi = (X →Y, A →x), 1 ≤i ≤n1, we introduce the
rules:

166
Membrane Computing
X[i ]0
i →[iY ]+
i ,
A[i ]+
i →[iA]−
i ,
[iA]−
i →[i ]0
i ⟨x⟩,
[iY ]0
i →[i ]0
i Y,
as well as the rules
[p−1⟨x⟩→x]0
p−1.
2. For each matrix mi = (X →Y, A →†), n1 + 1 ≤i ≤n1 + n2, we
introduce the rules:
X[i ]0
i →[iX]0
i ,
[iX]0
i →[i ¯X]+
i [iY ]−
i ,
[p−1[i ]+
i [i ]−
i ]0
p−1 →[p−1[i ]+
i ]+
p−1[p−1[i ]−
i ]0
p−1,
A[i ]+
i →[i † ]0
i ,
[iY ]−
i →[i ]0
i Y,
[i† →†]0
i .
3. We also consider the following rules, for all a ∈T,
[p−1a]0
p−1 →[p−1 ]0
p−1a,
[pa]0
p →[p ]0
pa,
as well as the following rules for all α ∈N1 ∪N2
[p−1α →α]0
p−1.
The system works as follows.
Membranes labelled with 1, 2, . . . , n1 are associated with matrices not
used in the appearance checking mode; each matrix is simulated with the help
of the associated membrane. In any moment, in membrane p−1 there is only
one symbol from the set N1. If this symbol enters a membrane with the label
i, 1 ≤i ≤n1, then the membrane gets the “electrical charge” + (initially,
all membranes are neutral). We can continue only by introducing in this
membrane i also the corresponding symbol A (at that time, the membrane
gets a negative “electrical charge”). The continuation is deterministic: we
send out of membrane i the symbol ⟨x⟩(and the membrane is again neutral),
then we send out also the symbol Y ; at the second step, the symbol ⟨x⟩is
replaced in membrane p−1 by the string x. In this way, in membrane p−1 we
have simulated the use of the matrix mi = (X →Y, A →x). Note how the
“electrical charge” of the membrane controls the process and that the symbol
Y is available in membrane p −1 only after completing the simulation of the
matrix.
Membranes with labels n1 + 1, . . . , n1 + n2 are associated with matrices
used in the appearance checking mode.
Let i, n1 + 1 ≤i ≤n1 + n2, be

P Systems with Active Membranes
167
such a membrane, associated with mi = (X →Y, A →†). As above, the
symbol X can enter membrane i (unchanged); having this symbol inside,
this membrane is divided in two membranes, of opposite polarity. In the
ﬁrst membrane, that with positive “charge”, we check whether or not any
occurrence of A is present in the membrane p −1. This is done as follows.
Because of this opposite polarity of membranes with label i, membrane p −1
is also divided, in a copy of positive “charge” and a neutral copy. In this way,
all objects from the former membrane with the label p−1 are duplicated and
introduced in each of these membranes. If the symbol A is present, then at
the same step when membrane p −1 is divided, we introduce the trap-object
† in the positively “charged” copy of membrane p−1. This object will evolve
forever, so the computation will never ﬁnish. Also in parallel with the division
of membrane p −1, we release the symbol Y in the copy of membrane p −1
which is neutral. Thus, the simulation of the matrix mi is successful if and
only if the computation will ever stop, that is, if and only if the symbol A
was not present.
The process can continue. Each terminal symbol present in the membrane
with the label p −1 and of neutral polarity is sent to the skin membrane
and from here it is sent out of the system.
As long as any nonterminal
symbol from N1 ∪N2 is present in the membrane with the label p −1 and
of neutral polarity, the computation is not halting. (Note that the copies of
membrane p−1 produced for simulating matrices in the appearance checking
mode and used only for checking the non-appearance of symbols A ∈N2
have a positive charge, so the nonterminal symbols present in them do not
evolve.) Consequently, we simulate in Π exactly the terminal derivations in
G′. Because the symbol Z cannot evolve, we have the equality ΨT (L(G)) =
ΨT (L(Π)).
✷
In the construction above we have used rules of all types (a)–(f), with the
exception of dissolving rules of type (d).
Because the division of membranes is used only for simulating matrices
which contain rules used in the appearance checking manner, from the pre-
vious construction we obtain the fact that the Parikh sets of languages in
MAT can be obtained as the Parikh sets of languages generated by P sys-
tems which do not use membrane division. We denote by LPA(ndiv) this
family of languages, hence we can write:
Corollary 3.2 PsMAT ⊆PsLPA(ndiv).
Actually, this is a proper inclusion, because of the following result:
Theorem 3.20 LPA(ndiv) −MAT ̸= ∅.
Proof.
We consider the P system
Π = ({a, b}, {a}, {1}, [1 ]0
1, ab, R),
R = {[1a →aa]0
1, [1b →b]0
1, [1b]0
1 →[1b]+
1 , [1a]+
1 →[1 ]+
1 a}.

168
Membrane Computing
As long as membrane 1 is neutral, the number of occurrences of object a is
doubled. At the same time, the object b is “doing nothing”. At any moment,
object b can determine the change of the polarity of membrane 1 (it becomes
positive). From now on, no other rule can be used than those which send
out of the system all available copies of object a. Consequently, we have
L(Π) = {a2n | n ≥1}. This is not a language in the family MAT (it is a
non-regular one-letter language).
✷
3.10
Splicing P Systems
We now relate the idea of computing with membranes to the splicing opera-
tion. Speciﬁcally, we consider P systems with objects in the form of strings
(as also considered in Section 3.5) and with the evolution rules based on the
splicing operation (as investigated in Chapter 2).
A splicing P system (of degree m, m ≥1) is a construct
Π = (V, T, µ, L1, . . . , Lm, R1, . . . , Rm),
where:
(i) V is an alphabet;
(ii) T ⊆V (the output alphabet);
(iv) µ is a membrane structure consisting of m membranes (labeled with
1, 2, . . . , m);
(v) Li, 1 ≤i ≤m, are languages over V associated with the regions
1, 2, . . . , m of µ;
(vi) Ri, 1 ≤i ≤m, are ﬁnite sets of evolution rules associated with the
regions 1, 2, . . . , m of µ, given in the following form: (r; tar1, tar2),
where r = u1#u2$u3#u4 is a usual splicing rule over V and tar1, tar2 ∈
{here, out} ∪{inj | 1 ≤j ≤m}.
Note that, as usual in H systems, when a string is present in a region of
our system, it is assumed to appear in arbitrarily many copies (any number
of copies of a DNA molecule can be obtained by ampliﬁcation). Thus, we do
not use here multisets, as in basic P systems.
Any m-tuple (M1, . . . , Mm) of languages over V is called a conﬁgura-
tion of Π.
For two conﬁgurations (M1, . . . , Mm), (M ′
1, . . . , M ′
m) of Π we
write (M1, . . . , Mm) =⇒(M ′
1, . . . , M ′
m) if we can pass from (M1, . . . , Mm)
to (M ′
1, . . . , M ′
m) by applying the splicing rules from each region of µ, in
parallel, to all possible strings from the corresponding regions, and following
the target indications associated with the rules. More speciﬁcally (but not
completely formal), if x, y ∈Mi and (r = u1#u2$u3#u4; tar1, tar2) ∈Ri

Splicing P Systems
169
such that we can have (x, y) ⊢r (w, z), then w and z will go to the regions in-
dicated by tar1, tar2, respectively. If tarj = here, then the string remains in
Mi, if tarj = out, then the string is moved to the region immediately outside
membrane i (maybe, in this way the string leaves the system), if tarj = ink,
then the string is moved to the region k, providing that this is immediately
below; if not, then the rule cannot be applied. Note that the strings x, y are
still available in region Mi, because we have supposed that they appear in
arbitrarily many copies (an arbitrarily large number of them were spliced,
arbitrarily many remain), but if a string w, z is sent out of region i, then no
copy of it remains here.
A sequence of transitions between conﬁgurations of a given P system Π,
starting from the initial conﬁguration (L1, . . . , Lm), is called a computation
with respect to Π. The result of a computation consists of all strings over T
which are sent out of the system at any time during the computation. We
denote by L(Π) the language of all strings of this type. We say that L(Π) is
generated by Π.
Note that in this section we do not consider halting computations. We
leave the process to continue forever and we just observe it from outside and
collect the terminal strings leaving it.
We denote by SPL(tar, m, p) the family of languages L(Π) generated by
splicing P systems as above, of degree at most m, m ≥1, and of depth at
most p, p ≥1.
If all target indications tar1, tar2 in the evolution rules of a P system are
of the form here, out, in, then we say that Π is of the i/o type; the strings
produced by splicing and having associated the indication in are moved into
any lower region immediately below the region where the rule is used. The
family of languages generated by P systems with this weaker target indication
and of degree at most m and depth at most p is denoted by SPL(i/o, m, p).
We start by four results showing the computational universality of splicing
P systems of rather simple forms.
Theorem 3.21 SPL(i/o, 3, 3) = CE.
Proof.
Let G = (N, T, S, P) be a type-0 Chomsky grammar and let B be a
new symbol. Assume that N ∪T ∪{B} = {α1, . . . , αn} and that P contains
m rules, ui →vi, 1 ≤i ≤m. Consider also the rules um+j →vm+j, 1 ≤j ≤
n, for um+j = vm+j = αj.
We construct the splicing P system (of degree 3)
Π = (V, T, µ, L1, L2, L3, R1, R2, R3),
V = N ∪T ∪{B, X, X′, Y, Z, Z′}
∪{Xi | 1 ≤i ≤n + m}
∪{Yi | 0 ≤i ≤n + m},
µ = [1[2[3 ]3]2]1,

170
Membrane Computing
L1 = {XSBY, Z′} ∪{ZYi | 0 ≤i ≤n + m},
L2 = {X′Z} ∪{XiviZ, XiZ | 1 ≤i ≤n + m},
L3 = {ZY },
R1 = {(#uiY $Z#Yi; in, here), (#Yi$Z#Yi−1; in, here) | 1 ≤i ≤n + m}
∪{(#BY $Z′#; here, here), (X#$#Z′; here, out)},
R2 = {(X#$Xivi#Z; here, out) | 1 ≤i ≤n + m}
∪{(Xi#$Xi−1#Z; here, out) | 2 ≤i ≤n + m}
∪{(X1#$X′#Z; here, in), (X′#$X#Z; here, out)},
R3 = {(#Y0$Z#Y ; out, here)}.
The idea of the proof is again the “rotate-and-simulate” technique, as
used in the proof of Lemma 2.3.
The sentential forms generated by G are simulated in Π in a circular
permutation: Xw1Bw2Y , maybe with variants of X, Y , will be present in
a region of Π if and only if w2w1 is a sentential form of G. Note that we
can remove the nonterminal symbol Y only together with B from strings of
the form XwBY . In this way, we ensure that the string is in the correct
permutation.
The simulation of rules in P and the rotation are done in the same way.
Assume that some string XwuiY is present in region 1, 1 ≤i ≤n + m;
initially we have here the string XSBY . We can perform (Xw|uiY, Z|Yi) ⊢
(XwYi, ZuiY ); the string XwYi is sent to region 2, ZuiY remains in re-
gion 1 (and all possible splicings which involve it produce two strings start-
ing with Z).
In region 2 we can only perform a splicing of the form
(X|wYi, Xjvj|Z) ⊢(XZ, XjvjwYi), for some 1 ≤j ≤n + m. The string
XjvjwYi is sent back to region 1, XZ remain here and will be used later.
Now, in region 1 the only splicing which can involve the string XjvjwYi is
(Xjvjw|Yi, Z|Yi−1) ⊢(XjvjwYi−1, ZYi). The string XjvjwYi−1 is sent to re-
gion 2, ZYi−1 remains in region 1 (it is an axiom). In region 2 we now decrease
by one the subscript of Xj, by (Xj|vjwYi−1, Xj−1|Z) ⊢(XjZ, Xj−1vjwYi−1).
The process of decreasing the subscripts continues.
If at some moment we reach X1, hence in region 2 we have a string
X1vjwYk,
then we perform (X1|vjwYk, X′|Z)
⊢
(X1Z, X′vjwYk) and
X′vjwYk is sent to membrane 3. If k ̸= 0, then nothing can be done, the
string is “lost”. Otherwise, Y0 is replaced with Y and the string X′vjwY is
sent to region 2; X′ is replaced here by X and the string XvjwY is sent to
the skin membrane.
If at some moment in region 2 we get a string XkvjwY0, for k ≥2, then
this string cannot be processed in the skin membrane, hence it is “lost”.
Thus, we can correctly continue only when i = j, hence we have passed
from XwuiY to XviwY ; in this way we have either correctly simulated a rule
from P or we have circularly permuted the string with one symbol.
The process of simulating a rule or of rotating the string with one symbol

Splicing P Systems
171
can be iterated. Therefore, all derivations in G can be simulated in Π and,
conversely, all correct computations in Π correspond to correct derivations in
G. Because we collect only terminal strings which leave the system, we have
the equality L(G) = L(Π).
✷
In order to minimize the depth of the used system we have to pay some
price. In the next theorem, this price is the use of target information; the
proof is similar to the previous one, but slightly simpler, because we can
make use of the powerful indications of the form inj:
Theorem 3.22 SPL(tar, 3, 2) = CE.
However, at the price of using two more membranes, we can get rid of the
target addresses, still using a system of depth 2:
Theorem 3.23 SPL(i/o, 5, 2) = CE.
Proof. Starting from a type-0 grammar G = (N, T, S, P) as above and with
B a new symbol, we construct the system (of degree 5)
Π = (V, T, µ, L1, L2, L3, L4, L5, R1, R2, R3, R4, R5),
V = N ∪T ∪{B, X, X′, Y, Y ′, Y ′′, Z, Z′}
∪{Xi | 1 ≤i ≤n + m}
∪{Yi | 0 ≤i ≤n + m},
µ = [1[2 ]2[3 ]3[4 ]4[5 ]5]1,
L1 = {XSBY, ZY ′′, Z′} ∪{ZYi | 0 ≤i ≤n + m},
L2 = {XiviZ | 1 ≤i ≤n + m},
L3 = {XiZ | 1 ≤i ≤n + m},
L4 = {XZ, ZY },
L5 = {X′Z, ZY ′},
R1 = {(#uiY $Z#Yi; in, here), (#Yi$Z#Yi−1; in, here) | 1 ≤i ≤n + m}
∪{(#Y ′$Z#Y ′′; in, here), (#BY $Z′#; here, here),
(X#$#Z′; here, out)},
R2 = {(X#$Xivi#Z; here, out) | 1 ≤i ≤n + m},
R3 = {(Xi#$Xi−1#Z; here, out) | 2 ≤i ≤n + m},
R4 = {(X′#$X#Z; here, out), (#Y ′′$Z#Y ; here, here)},
R5 = {(X1#$X′#Z; here, here), (#Y0$Z#Y ′; out, here)}.
This system works in a way similar to the system in the proof of Theorem
3.21, but the fact that we can continue the simulation of rules in P or the
circular permutation of strings only in the correct manner is much more
subtle.

172
Membrane Computing
In region 1 we start to simulate the application of rules in P and the
circular permutation of a string, by cutting from the right hand end of the
string a substring ui, 1 ≤i ≤n + m. In membrane 2 we add to the left
end of the string a preﬁx vj. These operations are “memorized” also in the
subscripts of the end markers X and Y . The subscript of X is decreased
in membrane 3 while the subscript of Y is decreased in membrane 1. Only
when the two subscripts reach the value 0 at the same time the process has
to correctly continue. This happens, indeed.
In order to see this, let us consider the two possible cases when in region
1 we get a string with one of the subscripts of X and Y of a minimal value
(this string should be sent to an inner membrane). When the subscript of X
is minimal, our string is of the form X1wYk, with k ≥1. When the subscript
of Y is minimal, the string is of the form XkwY0, for some k ≥1. Only the
case X1wY0 should continue correctly. We consider in detail each case:
1. The string X1wYk, k ≥1, can enter no splicing in membranes 2, 3,
4, while the only possible splicing in membrane 5 leads to the string
X′wYk which stops here. No continuation is possible.
2. A string of the form X1wY0 can enter no splicing in membranes 2, 3,
4. In membrane 5 we have two possibilities:
2.1. If we produce the string X1wY ′ which is sent to the skin mem-
brane, this string is transformed here into X1wY ′′ and it must
again enter an inner membrane. No splicing is possible on it in
membranes 2, 3. In membrane 4 we can produce X1wY , but the
string does not leave the membrane. In membrane 5 we can pro-
duce X′wY ′′ and again the string is “lost”, because it cannot leave
the membrane.
2.2. If in membrane 5 we apply both productions, then we produce
the string X′wY ′ which is sent to the skin membrane. Here we
produce X′wY ′′, which should again enter an inner membrane. In
membranes 2, 3, 5 we can apply no splicing rule, but in membrane
4 we have two possibilities. If we produce the string XwY ′′ which
is sent to the skin membrane, then the only possible splicing here is
that by the rule (X#$#Z′; here, out). The obtained string leaves
the system, but it is not terminal. If in membrane 4 we apply
both rules, then we get XwY , which is the correct continuation.
The process of simulating rules in P and of circularly permuting
the string can be continued.
3. A string of the form XkwY0, k ≥2, enters no splicing in membranes 2
and 4. In membrane 3 we can produce Xk−1wY0, which is sent to the
skin membrane, but no further splicing is possible here. In membrane 5
we can produce the string XkwY ′ which is sent to the skin membrane.
From membrane 1 we return to the inner membranes the string XkwY ′′.

Splicing P Systems
173
This string can enter no splicing in membranes 2 and 5. In membrane 3
we can produce Xk−1wY ′′, which is sent to the skin membrane, where
no further splicing can involve this string.
In membrane 4 we can
produce XkwY , but this string does not leave the membrane.
In none of the cases diﬀerent from X1wY0 we can produce a string
which can be further processed.
We conclude that all computations in Π which lead to terminal strings
which leave the system correspond to terminal derivations in G. Conversely,
it is easy to see that each terminal derivation with respect to G can be
simulated in Π. Consequently, L(G) = L(Π).
✷
We do not know whether or not these results can be improved. Here are
some information about the power of systems of a lower degree (we present
them without a proof; the reader is referred to [231] for details.
Theorem 3.24 (ii) SPL(i/o, 1, 1) = REG, (ii) SPL(i/o, 2, 2) −MAT ̸= ∅.
P systems based on splicing can also work on asymmetric graphs, as
already considered for P systems with symbol objects in Section 3.8. We
denote by SP ′L(go, m) the family of languages generated by such systems of
degree at most m, m ≥1, and by SP ′L(go, ∗) the union of all these families.
As expected, also splicing P′ systems are computationally universal. How-
ever, the proof of this assertion does not give a (small) bound on the degree
of the necessary systems.
Theorem 3.25 SP ′L(go, ∗) = CE.
Proof. Let G = (N, T, S, P) be a type-0 Chomsky grammar, B a new symbol,
ui →vi, 1 ≤i ≤m, the rules in P and um+j →vm+j, 1 ≤j ≤n, new rules,
with um+j = vm+j = αj, for N ∪T ∪{B} = {α1, . . . , αn}. We construct the
P′ system (of degree 2(n + m + 1))
Π = (V, T, g, L0, L0′, L1, L1′, . . . , L(n+m+1), L(n+m+1)′,
R0, R0′, R1, R1′, . . . , R(n+m+1), R(n+m+1)′),
V = N ∪T ∪{B, X, Y, Z, Z′}
∪{Xi, Yi | 0 ≤i ≤n + m},
g = ({0, 0′, 1, 1′, . . . , (n + m + 1), (n + m + 1)′},
{(i, i′) | 0 ≤i ≤n + m + 1}
∪{(i′, i + 1), (i′, n + m + 1) | 0 ≤i ≤n + m}
∪{((n + m + 1)′, 0)},
L0 = {XSBY, Z′} ∪{ZYi | 1 ≤i ≤n + m},
L0′ = {XiviZ | 1 ≤i ≤n + m},
Li = {ZYj | 0 ≤j ≤n + m}, 1 ≤i ≤n + m,

174
Membrane Computing
t
t
t
✟
✟
✟
✟
✟
✟
✟
✟
✟
✙
t
t
✲
❍❍❍
❥t
❅❅
❘
✻
✻
✻
❄
❄
t








}
✂
✂✂✌t
✏
✏
✏
✏
✮
t
t✛
✻
✛
t
P
P
P
P
✐
✑✑✑✑✑✑✑✑
✸
t
❇
❇
❇
❇❇M
✻
tPPPPPPPPP
q
❙
❙
❙
❙
❙
❙
❙❙✇
✁
✁✁✕
✑✑
✑
✸
t
t
0
0′
1
1′
. . .
(i −1)
(i −1)′
i
i′
(i+1)
(i+1)′
. . .
(n+m −1)′
(n+m)
(n+m)′
(n+m+1)′
(n+m+1)
Figure 3.12: The graph of the system in the proof of the theorem.
Li′ = {XjZ | 0 ≤j ≤n + m}, 1 ≤i ≤n + m,
L(n+m+1) = {XZ},
L(n+m+1)′ = {ZY },
R0 = {(#uiY $Z#Yi; go, here) | 1 ≤i ≤n + m}
∪{(#BY $Z′#; here, here), (X#$#Z′; here, out)},
R0′ = {(X#$Xivi#Z; here, go) | 1 ≤i ≤n + m},
Ri = {(#Yj$Z#Yj−1; go, here) | 1 ≤j ≤n + m}, 1 ≤i ≤n + m,
Ri′ = {(Xj#$Xj−1#Z; here, go) | 1 ≤j ≤n + m}, 1 ≤i ≤n + m,
R(n+m+1) = {(X0#$X#Z; here, go)},
R(n+m+1)′ = {(#Y0$Z#Y ; go, here)}.
The graph in this system and a planar map associated with it are given
in Figures 3.12 and 3.13, respectively.
The system works as follows. Any string of the form XwuiY from re-
gion 0 (initially we have XSBY ) enters here a splicing (Xw|uiY, Z|Yi) ⊢
(XwYi, ZuiY ), for some 1 ≤i ≤n + m; the string XwYi passes to re-
gion 0′, where we perform (X|wYi, Xjvj|Z) ⊢(XZ, XjvjwYi), for some
1 ≤j ≤n + m.
The string XjvjwYi passes now from region to region,
clockwise, and in regions k the subscript of Y is decreased by one, while in
regions k′ the subscript of X is decreased by one. If in a region k we receive
a string of the form XrzY0, then this string cannot be processed. Similarly, if
a string of the form X0zYs arrives in a region k′, then it cannot be processed.

Splicing P Systems
175
In any moment, from a region k′ we can pass to region (n + m + 1). Only
strings of the form X0zYs can be spliced here: (X0|zYs, X|Z) ⊢(X0Z, XzYs).
The string XzYs is passed to region (n + m + 1)′, where we can process this
string only if s = 0. This means that the subscripts of X and Y have reached
the value 0 at the same time, that is we have i = j. The simulation of rules
in P and the circular permutation of symbols is done in a correct manner.
Thus, any derivation in G can be simulated in Π, with the sentential form
circularly permuted, and, conversely, each computation in Π corresponds to
a correct derivation in G.
A string can leave the system only from region 0; if no nonterminal is
present (hence also B and Y were removed), this means that the symbols
B, Y were adjacent when removing them, that is, the obtained string is in
the same permutation as in G. We conclude that L(G) = L(Π).
✷
We close this section by pointing out the naturalness of splicing P sys-
tems, which are intrinsically biochemical-like: both the membrane computing
and the splicing operation are biochemically inspired. Of course, this means
nothing in what concerns the possibility of implementing such computing
devices.
✬
✫
✩
✪
✬
✫
✩
✪
✫
✪
❉
❉
❉
❉
❉❉
✂
✂
✂
✂
✂✂
✪
✪
✪
✪
✪✪
✟✟✟✟✟✟
❍
❍
❍
❍
❍
❍
❅
❅
❅
❅
❅
❅

❆
❆
❆
❆
❆
✚
✚
✚
✚
✚
✚
✔
✔
✔
✔
✔
✻
✻
✲
❅❅
❘
❄
❄
❄



✠
✛
✛
✛
❅
❅
I
✻
✻
✻

✒
✟
✟
✟
✙
❆
❆
❆❑
✁
✁✁✕
✘✘✘✘
✿
✻
❍❍❍
❥
❈
❈
❈❈
0
0′
1
1′
. . .
(i −1)
(i −1)′
i
i′
(i+1)
(i+1)′
. . .
(n+m −1)′
(n+m)
(n+m)′
(n+m+1)′
(n+m+1)
Figure 3.13: The planar map of the system in the proof of the theorem.

176
Membrane Computing
3.11
Variants, Problems, Conjectures
The possibility of inventing variants of the P systems seems to be limitless.
One can go both towards considering simpler variants or more complex vari-
ants, looking either for adequacy to the biochemical origin of the idea or
for mathematical appealing (whatever this means: elegance of deﬁnitions,
strength of results, similarity with other areas of theoretical computer sci-
ence). We do not intend to start here such an approach, but we just want
to point out some questions which seem more natural and important for the
immediate development of the theory.
First, the theory of computing with membranes misses basic tools for pro-
ducing examples and counterexamples (necessary conditions). For instance,
we believe that {anbn | n ≥1} /∈LP(nPri, Cat, nδ) and {anx1x2 . . . xn | n ≥
1, xi ∈{de, ed}, 1 ≤i ≤n} /∈LP(Pri, nCat, δ). (The idea supporting these
conjectures is that we have to ﬁrst send outside the system the preﬁx an,
hence we have to store inside the system the information about n; we can see
no way of doing that which does not produce strings outside our languages.)
A basic question concerns the hierarchies on the number of membranes.
We have seen above that in many cases these hierarchies collapse.
We conjecture that at least the hierarchy LPm(nPri, nCat, δ), m ≥1,
is inﬁnite. A candidate sequence of languages for proving this assertion is
that considered in Example 3.6. Similarly, we also believe that most of the
hierarchies LP ±
m(α, β, γ), for α ∈{Cat, nCat}, β ∈{δ, nδ}, and γ ∈{τ, nτ}
are inﬁnite.
The system in Example 3.6 has the depth equal to m + 1, but one can
generate the same language by a system of depth two:
Π′
5 = (V, {a}, ∅, µ, a1, λ, . . . , λ, (R1, ∅), . . . , (Rm+1, ∅), ∞),
V = {a1, . . . , am, a},
µ = [1[2 ]2[3 ]3 . . . [m ]m]1,
R1 = {ai →(ai+1)ini+2 | 1 ≤i ≤m −1}
∪{am →aout},
Ri+1 = {ai →api
i , ai →api
i δ}, 1 ≤i ≤m.
This suggests another open problem of interest: are there classes (α, β, γ)
of P systems, α ∈{Pri, nPri}, β ∈{Cat, nCat}, γ ∈{δ, nδ}, for which the
depth of the membrane structure induces inﬁnite hierarchies of languages in
families LP(α, β, γ)? The same question holds for P systems with polarized
membranes or using only i/o indications.
We have seen in Section 3.7 that the answer is negative for the case of
families LP i/o
∗
(2Cat2, δ, τ) and LPm(Pri, nCat, δ), m ≥1
In [237] one conjectures that for families LPm(nPri, nCat, δ), m ≥1, the
hierarchy with respect to the depth is inﬁnite.

Variants, Problems, Conjectures
177
In the systems above there is an asymmetry between the way of dealing
with the objects and the rules in the case of membrane dissolving. Variants
can be considered: with the rules preserved after membrane dissolving, or
also with multisets of rules (thus, a limited parallelism is obtained). More
generally, we can treat in the same way the rules and the objects, by means of
certain meta-rules; thus, the rules can swim in the system, passing through
membranes, and acting on objects when reaching each other in the same
region. A natural variant is to leave the objects free in their regions and
to consider certain “ﬁlters” associated with the membranes (in the sense
used in networks of language processors, [71]); an object can pass through a
membrane only when this is allowed by the associated ﬁlter.
Also natural is to look for a normal form where the same rules are used in
all regions (then, by dissolving membranes we only handle objects, the rules
are the same everywhere). Does such a restriction decrease the power of P
systems of various types?
For all these variants, “classic” problems can be formulated: generative
power, hierarchies (on the degree and on the depth), descriptional complex-
ity, inﬂuence of determinism, of erasing (non-propagation). There also are
several speciﬁc questions: normal forms, the power of parallelism and of syn-
chronization (in the basic model, a universal clock is assumed, which is not
very realistic from a biochemical point of view). Concerning normal forms,
again we can consider classic problems (for instance, restricted forms of the
rules), but also speciﬁc problems: Which is the inﬂuence of the topological
properties of the membrane structure on the properties of the associated P
systems? Can all P systems of degree m and of a given type (α, β, γ) be
simulated by a P system with a ﬁxed arrangement of membranes and of the
same type?
Of a particular interest is the case of P systems whose membranes can
be divided. For instance, a possible parameter for this case is the maximal
number of membranes during a computation. Is this number computable?
Does it give an inﬁnite hierarchy of languages? Then, of a deﬁnite importance
is the computational complexity direction of investigation. Which other NP-
complete problems than SAT can be solved in a direct way by a P system,
in polynomial time?
One more problem, of a clear natural computing signiﬁcance (also related
to quantum computing): what about reversible P systems? (We ﬁrst need a
good deﬁnition of reversibility – diﬃculties appear in the case of using the
membrane dissolving action – and then we have to see how restrictive this
condition is.)
All these variants and questions are valid both for the case when we work
with symbol-objects and with string-objects. Of a special interest is the case
when we deal with string transformations of a biochemical inspiration, thus
having a more homogeneous natural computing model.
Of course, an important research topic is to implement a P system, either

178
Membrane Computing
in a biochemical media or in an electronic media. Attempts of the latter
type were done in [179] and [278], but the problem still needs further eﬀorts.
What about designing a speciﬁc (silicon) hardware adequate for P systems?
3.12
Bibliographical Notes
The P systems were introduced in [218] and then investigated in a series of
papers. An early survey of the domain can be found in [219]. Theorem 3.1
is from [226], Theorems 3.2 and 3.10 are from [218], Theorems 3.3–3.7 are
from [76] and [226]. Theorems 3.8 and 3.9 are from [226]. Theorem 3.6 is
from [220], where P systems with polarized membranes (and action τ) are
introduced. Sections 3.7 and 3.8 are based on [227]; Theorem 3.15 is from
[237]. Systems with active membranes were introduced in [221]; Section 3.9
is based on this paper.
Splicing P systems were already considered in [218], in a form diﬀerent
from that discussed in Section 3.10.
In [218], the rules are given in the
form (r, y; tar), where r is a splicing rule, y is a string, and tar is a target
indication. To apply such a rule to a string x means to perform a splicing
(x, y) ⊢r (w, z) or (y, x) ⊢r (w, z); only the string w is considered as an
output of this operation and it is sent to the region indicated by tar.) The
variant of splicing P systems considered in Section 3.10 was introduced in
[231], where Theorems 3.21–3.25 can be found.
Bi-stable catalysts were ﬁrst considered in [233], where the role of syn-
chronization is investigated. Unsynchronized P systems without priority, but
using catalysts and action δ, generate only Parikh sets of matrix languages,
which is less than PsCE; if both a priority and bi-stable catalysts are used,
then unsynchronized P systems characterize again PsCE.
Further variants of P systems are introduced in [102] (systems working
on graphs and on arrays).
In [77] one considers P systems where the objects are moved from a re-
gion to another one because of diﬀerences in their concentrations: after using
evolution rules, one counts the number of occurrences of (certain) objects
and one redistributes them among regions in such a way to have the same
number of occurrences (plus/minus one) of these objects in all regions. Again
a characterization of PsCE is obtained when arbitrarily many bi-stable cat-
alysts are used, while PsMAT can be covered by concentration controlled P
systems with only one catalyst.
A sort of one-membrane P systems is investigated in [230]: a multiset of
symbol-objects is processed by a given set of gsm’s. (Characterizations of
the permutation closures of computably enumerable languages are obtained,
both by systems with only two components, without a bound on the number
of states in each component, or by systems with an unbounded number of
components but with at most two states in each component.) One-membrane
devices processing multisets of objects are considered in [279], [280], [281].

Chapter 4
Quantum Computing
The aim of this chapter is to oﬀer a glimpse into Quantum Computing. We
start by explaining that computation, be it mental, mechanic, molecular or
silicon, is ultimately a physical process.
4.1
The Church–Turing Thesis
The Church–Turing Thesis, a prevailing paradigm in classical computation
theory, states that no realizable computing device can be “globally” more
powerful, that is, aside from relative speedups, than a universal Turing ma-
chine (see, for example, Odifreddi [198]). The modern form of the Church–
Turing Thesis states that
any “reasonable” model of computation can be eﬀectively simu-
lated by a (probabilistic) Turing machine.
As one can immediately note, the above statement is a thesis, and not a
theorem, as it relates an informal notion – a realizable computing device –
to the mathematical notion of (probabilistic) Turing machine.
Here are some reasons supporting the Church–Turing Thesis:
• Philosophical argument: Due to Turing’s analysis it seems very diﬃ-
cult to imagine some other method which falls outside the scope of his
description.
• Mathematical evidence: Every mathematical notion of computability
which has been proposed was proven equivalent to Turing computabil-
ity.
• Sociological evidence: No example of classical computing device which
cannot be simulated by a Turing machine has been given, i.e. the thesis
179

180
Quantum Computing
has not been disproved despite having been proposed for more than 60
years.
The Church–Turing’s Thesis includes a syntactic as well as a physical
claim. In particular, it speciﬁes which types of computations are physically
realizable. According to Deutsch ([84, p. 101]; see also [86]):
The reason why we ﬁnd it possible to construct, say, electronic cal-
culators, and indeed why we can perform mental arithmetic, can-
not be found in mathematics or logic. The reason is that the laws
of physics “happen” to permit the existence of physical models
for the operations of arithmetic such as addition, subtraction and
multiplication. If they did not, these familiar operations would
be non-computable functions. We might still know of them and
invoke them in mathematical proofs (which would be presumably
called “non-constructive”) but we could not perform them.
It’s not surprising that the Church–Turing Thesis was challenged by lo-
gicians (Kalmar [153], Davis [78], Kreisel [154]; see also Odifreddi [198]),
computer scientists (Rosen [254], Hogarth [138], Siegelmann [271, 272]) and
physicists (Landuaer [158, 159, 160, 161, 162], Svozil [284, 285]). For example,
Davis’ classical book [78, p. 11] includes the following question:
“. . . how can we ever exclude the possibility of our being presented,
some day (perhaps by some extraterrestrial visitors), with a (per-
haps extremely complex) device or “oracle” that “computes” an
uncomputable function?”
Thinking is an essential, if not the most essential, component of human
life – it is a mark of “intelligence”.1 The Church–Turing Thesis has been
used to approach formally the notion of “intelligent being”. In simple terms,
the Church–Turing Thesis was stated as follows:
What is human computable is computable by a universal Turing
machine.
This sentence equates information-processing capabilities of a human be-
ing with the “intellectual capacities” of a universal Turing machine. This
discussion leads directly to the traditional problem of mind and matter which
exceeds the aim of this chapter. Instead, we will concentrate on its physical
aspects.
1Descartes placed the essence of being in thinking.

Computation is Physical
181
4.2
Computation is Physical
An operation is “logically reversible” if it can be undone, if it can be run
backwards, that is, if its inputs can always be deduced from the outputs.
Most logical gates are irreversible; a typical example is the NAND gate
(a, b) &→¬(a ∧b)
(4.1)
which has two input bits and only one output bit.
We cannot recover a
unique input from the output bit because the result 1 can be obtained from
three distinct inputs: (0, 0), (0, 1), (1, 0).
Assume we operate the gate NAND with two Boolean variables, a, b, and
suppose that the four initial states, (0, 0), (0, 1), (1, 0), (1, 1), have the same
probability distribution,
1
4.
Then, the initial entropy, which is calculated
with Shannon’s formula2
H = −

i
pi · log pi,
is then
Hinitial = −4 · (1
4 log 1
4) = 2 bits.
The result will be a system with only two possible states, 0 and 1, the outcome
0 appearing with probability 1
4 and the outcome 1 appearing with probability
3
4. Consequently, the ﬁnal entropy is
Hfinal = −(3
4 log 3
4 + 1
4 log 1
4) = 2 −3
4 log 3 bits,
which means a loss of
Hinitial −Hfinal = 3
4 log 3 bits.
Assume now that we operate the gate
(a, b) &→(a ∨b, a ∧b),
and, again, suppose that the four initial states of the Boolean variables a, b
have the same probability distribution, 1
4. This gate has ﬁnally only three
ﬁnal states, namely (0, 0), (1, 0), (1, 1), two of them with probability 1
4 and
one with probability 1
2. Consequently, the ﬁnal entropy is
Hfinal = −(2 · 1
4 log 1
4 + 1
2 log 1
2) = 1.5 bits.
In this case, the gate decreases the entropy by 0.5 bits.
2Here log is the logarithm in base 2.

182
Quantum Computing
The ﬁrst gate is “more irreversible” than the second one, since it decreases
the entropy more.
In thermodynamics the entropy is deﬁned by
S = −k ·

i
pi · ln (pi).3
This notion is coupled to energy through the temperature T of the system:
when the entropy of a system is decreased by some amount, then the system
dissipates energy equal to the amount of entropy reduction times the tem-
perature. Von Neumann noticed that the two entropies are related by some
constant factor, so they are in fact the same notion. When the probability
distribution of the system is changed so that the entropy H is decreased by 1
bit, then the entropy S is decreased by k · ln2 joule/◦kelvin, and the system
dissipates kT · ln2 joules of energy in the form of heat. So, a challenging
question arises:
what is the minimum energy required to carry out a computation?4
Does the above analysis apply to computation? In 1961 Landauer [158]
(see also [161]) has produced evidence for the aﬃrmative answer. To operate
a computer we have to make sure that distinct logical states are represented
by distinct physical states. Each bit has two values, 0, 1, so it has one de-
gree of freedom; it corresponds to one or more degrees of freedom of physical
states. In general, a set of n bits has n degrees of freedom; they correspond
to 2n physical states. If we erase n bits5, say we reset all to 0, then we have
compressed 2n logical states into a single state, a loss of entropy. The irre-
versible loss information increases temperature of the system, which means
heat dissipation. Consequently, operations which are not one-to-one, which
map distinct logical states into a common one, cost energy.
This cost is
expressed by Landauer’s principle:
erasure of information is a dissipative process.
Here is a simple example. One can store one bit of information by placing
a single molecule in a box, either on the left side or the right side of a partition
that divides the box. In this context, erasure means that we choose to move
the molecule to the left (or right) side irrespective of whether it started
out on the left or right. However, one can suddenly remove the partition,
and then slowly compress the one-molecule “gas” with a piston until the
molecule reaches the left side. This procedure reduces the entropy of the
gas by ∆S = k ·ln2, and a ﬂow of heat from the box to the environment
is produced. Assume now that the process is isothermal at temperature T.
Then work
3Here k ≈1.38 × 10−23 joule/◦kelvin is Boltzmann’s constant.
4See Feynman [98] for a detailed discussion.
5Which could be in any of the 2n possible logical states.

Reversible Computation
183
W = kT · ln2
(4.2)
is performed on the box, and this work has to be provided. If one decides
to erase information, then “a power bill will be generated” and should be
paid.
For example, according to formula (4.2), the execution of the gate
(a, b) &→(a ∨b, a ∧b) dissipates at least 1
2kT · ln2 joules of energy. This is
a theoretical limit expressing how long the gate can be operated with ﬁnite
resources of energy.
The energy dissipation has been reduced by approximately a factor of ten
every ﬁve years, so a rough extrapolation suggests that a reduction of the
energy dissipation per logic operation below kT (thermal noise, that is of
the order of 10−18 picojoule at room temperature) may become relevant in
about 10-15 years. This issue may cause a variety of problems for classical
computers, e.g. cooling may be diﬃcult (according to current day knowl-
edge/technology).
4.3
Reversible Computation
Irreversible operations as the NAND gate (4.1), the binary addition (a, b) &→
(a ⊕b, a ∧b) (sum and carry) and the real addition (x, y) &→x + y, dissipate
energy. Is logical reversibility dissipation free?
First, let’s note that the above irreversible operations can be easily sim-
ulated by reversible ones. A reversible version of the NAND gate6 is, for
example, Toﬀoli’s gate
(a, b, c) &→(a, b, c ⊕(a ∧b)).7
(4.3)
Indeed, (4.3) is a reversible 3-bit gate that ﬂips the third bit if the ﬁrst two
both take the value 1 and does nothing otherwise. Hence, the third output
bit becomes the NAND of a and b in case c = 1.
The price paid to get
reversibility consisted of adding a new variable c.
Similar tricks can be used to produce reversible versions of the binary
addition, (a, b) &→(a, a ⊕b, a ∧b) and real addition (x, y) &→(x + y, x −y). In
the ﬁrst case we replicated the ﬁrst variable a; in the second case we added
a new component storing some additional value.
A computer may be fully reversible and yet dissipate energy! The impor-
tant point is that the laws of physics allow for technologies to make reversible
computers operate with negligible dissipation. To build a reversible computer
one needs only two types of logical gates, say AND and NOT (because any
other gate can be constructed from these two types – they are universal).
6A single NAND gate is as good as having both AND and NOT: ¬a =NAND(a, 1),
AND(a, b) = ¬(NAND (a, b)) = NAND(NAND(a, b), 1).
7Recall that a ⊕b is 1 only if a and b have diﬀerent values, i.e.
a = 0, b = 1 or
a = 1, b = 0.

184
Quantum Computing
Clearly, the NOT gate is reversible as its composition with itself gives the
initial input. However, the AND gate is irreversible. Reason: it has two
inputs and only one output, so it has to lose information (it is impossible
to tell exactly what inputs must have been if all one is told is the output 0:
any of the three combinations (0, 0), (0, 1), (1, 0), could have been the “real
input”).
To make a reversible variant of the gate AND we need to make sure that
we have the same number of output lines as input ones, so, in principle, we
can just add some “garbage” output lines to solve the problem. However,
this may not be enough, as we want to guarantee also universality!
One
possibility is Toﬀoli’s [101] reversible 3-bit gate which uses in addition to a, b
a control bit c. Input bits a and b do not change their states; the control bit,
however, will change its state, but only when a = b = 1. Toﬀoli’s truth table
is the following:
input
output
a
b
c
a
b
c
0
0
0
0
0
0
0
1
0
0
1
0
1
0
0
1
0
0
1
1
0
1
1
1
input
output
a
b
c
a
b
c
0
0
1
0
0
1
0
1
1
0
1
1
1
0
1
1
0
1
1
1
1
1
1
0
Table 4.1: Toﬀoli’s gate.
Fredkin’s [101] reversible 3-bit gate also uses in addition to a, b a control
bit c in the following way: (a) if c = 0, then the values of a, b are transmitted
unaltered, i.e. the output is the pair (a, b), (b) if c = 1, then the values of
a, b are switched to the opposite output, i.e. the output is the pair (b, a). Its
truth table is the following:
input
output
a
b
c
a
b
c
0
0
0
0
0
0
0
1
0
0
1
0
1
0
0
1
0
0
1
1
0
1
1
0
input
output
a
b
c
a
b
c
0
0
1
0
0
1
0
1
1
1
0
1
1
0
1
0
1
1
1
1
1
1
1
1
Table 4.2: Fredkin’s gate.
Fredkin’s gate is universal in the sense that it can be used to construct

Reversible Computation
185
reversible variants of all Boolean gates, and satisﬁes one additional require-
ment: the number of 1s and 0s never changes.8 To prove universality it is
enough to show that the gates NOT and AND can be represented using Fred-
kin’s gate FREDKIN with particular inputs. It is not diﬃcult to check that
the following formulae work:
FREDKIN (a, b, c) = ((((¬c) ∧a) ∨(c ∧b)), ((a ∧c) ∨(b ∧(¬c)), c),
FREDKIN (1, 0, c) = (¬c, c, c),
(4.4)
and
FREDKIN (0, b, c) = (b ∧c, b ∧(¬c), c),
(4.5)
So, both NOT and AND can be simulated by FREDKIN.
Fredkin’s gate has often been used for photon based gates where a 1
represents a photon and a 0 simply denotes the absence of a photon; nonlinear
optics is used to control the output of an interferometer (see Milburn [192]).
The number of 1s cannot change as the number of photons cannot change –
absorption is not allowed for reversible gates.
In both formulae (4.4) and (4.5) there are more outputs than are re-
quired for the computed functions (one for NOT and two for AND): these
outputs, called garbage bits, are a necessary consequence of reversible logic.
Consequently, one may wonder whether we have only postponed the energy
cost; garbage bits can be irreversibly erased, but that would require to pay
Landauer’s price . . .
Bennett [23] found in 1973 that any computation can be performed using
only reversible steps, and so “in principle” requires no dissipation and no
power expenditure: we do not need to erase the garbage bits! By pointing
out that a reversible computer can run forward to the end of a computation,
print out a copy of the answer (a logically reversible operation) and then
reverse all of its steps to return to its initial conﬁguration, Bennett invented
a procedure to remove the garbage without any energy cost. Here is a simple
illustration of this technique (cf. Milburn [193]):
INPUT00000 &→COMPUTER00000 &→INPUTOUTPUT &→
COPYOUTPUT &→RETUPMOC &→INPUT00000 &→OUTPUT
The inevitability of handling garbage bits implies the necessity to allow
more input bits than are needed in an irreversible computation (INPUT &→
INPUT00000); of course, some garbage bits may be useful only for internal
computation, but still, a certain number of additional bits will be required.
Consequently, in principle, we need not pay any “power bill” to compute
reversibly. In practice, the (irreversible) computers in use today dissipate
8Note that Toﬀoli’s gate does not satisfy this condition; for example, (1, 1, 0) is trans-
formed into (1, 1, 1).

186
Quantum Computing
energy of orders of magnitude more than (4.2) per gate, so today Landauer’s
limit seems not to be an important engineering principle. But as computing
hardware continues to shrink in size, it may become important to beat Lan-
dauer’s limit, for example, to prevent the components from melting. Then
reversible computation may be one, if not the only one, option (for more
information we refer to the results of MIT group on reversible computation
reported in [47]).
4.4
The Copy Computer
In this section we are going to follow Bennett [24] to discuss a very simple,
almost dumb, computation: the copy computation.
Interestingly enough,
nature uses extensively this “computation”.
In the beginning it is not at all obvious that we can copy information
from one place to another “for free”, that is, without expending at least
some energy. Indeed, if the speed of the copying process is reasonable low,
then, copying can be done for free. Here is how one can do it, at least in
principle.
Assume that we have a Turing machine and we consider a message and
its copy as two (identical) messages on tape. If we know what the original
message was, then we need no energy to waste in order to clear the tape
and none need be expended for the copy tape: we just turn it over when
necessary. However, if we don’t know what the original message was, then
clearing the tape will cost energy, but not for the copy. We can use the ﬁrst
tape content to clear the copy by turning bits over again. Reason: there is
no more information in the original message plus copy than in the original
message. Consequently, clearing the band should not require more energy in
the ﬁrst case than in the second.
By now the reader knows what proteins are and how crucial they are for
the structure and functioning of living organisms and . . . Molecular Comput-
ing. Protein synthesis is done in two stages, but in what follows we will be
interested solely in the ﬁrst one9, in which the formation of another linear
strand of sugar phosphates with bases attached is produced with the help of
the messenger RNA. “Computationally”, the code on the DNA is copied onto
the RNA strand base by base, according to a matching rule. The completed
RNA leaves the nucleus and travels elsewhere. The “computer”10 which ex-
ecutes the copying operation is an enzyme called RNA polymerase.
What actually happens has been directly or implicitly already described.
The DNA and enzyme are ﬂoating around in a soup which contains (among
other phosphates) nucleotides with two extra phosphates attached. The en-
9Recall that the “rules” reside in the DNA. DNA comprises a double chain, each strand
of which is made up of alternating phosphate and pentose sugar groups by means of the
four bases A, T, C, G. A speciﬁc sequence of bases provides the code for protein synthesis.
10Christened “Brownian computer” by Bennett.

Maxwell’s Demon
187
zyme attaches itself to the part of the DNA strand which will be duplicated:
it moves along it and builds the RNA copy, base by base.11 The nucleotides
are provided in the triphosphate form; in the process of addition, two of the
phosphates will be released into the soup, but in an “entangled” way as a
pyrophosphate.12
Enzymes are merely catalysts: they help reactions, but they don’t inﬂu-
ence the direction in which they proceed. Chemical reactions are reversible,
so it is equally possible for the polymerase reaction to go the other way, that
is to undo the RNA chain. The relative concentration of pyrophosphates and
triphosphates may trigger the reaction one way or another. If there are a lot
of triphosphates, then there are more forward-moving than backward-moving
states available, so RNA polymerase will tend to group towards the former
state, and conversely. If the concentration is about right, then the copier will
oscillate forever and will fail to make the copy. In a real cell the concentration
of pyrophosphates is low (by hydrolysis), so the copy “computer” will work.
The dissipated energy is about 20-200 kT per logical step, which is not
very eﬃcient, but far less than the energy dissipation per logical step in any
conventional computer.13 This copy “computation” can be made arbitrarily
close to being reversible by “running” it suﬃciently slow, but, for the sake of
life, the reaction has to be done at a certain speed!
Other models of reversible computers include the ballistic (billiard ball)
computer invented by Fredkin and Toﬀoli [101] and a modiﬁed variant of the
billiard ball computer by Bennett and Landauer [29].
4.5
Maxwell’s Demon
The above analysis led Bennett [24, 25] in 1982 to the reconciliation of
Maxwell’s demon with the second law of thermodynamics. Maxwell’s De-
mon, a hypothetical intelligent entity capable of performing measurements
on a thermodynamic system, was imagined by James Clerk Maxwell to “con-
tradict” the second law of thermodynamics; see Maxwell [185], Leﬀand Rex
[163]. Suppose that you have a box ﬁlled with a gas at some temperature.
The average speed of the molecules depends on the temperature. Some of the
molecules will be going faster than average and some will be going slower than
average. Suppose that the box is divided by a partition into two parts, with
both sides of the box ﬁlled with the gas at the same temperature. Maxwell
imagined a molecule sized trapdoor in the partition and a demon poised at
the door to observe molecules.
The demon observes the molecules in the
box as they approach the trapdoor, allowing fast ones to pass from left to
11An essential ingredient is the use of “complementarity” relationships; see more in
Chapter 2.
12The chosen nucleotide should be chosen correctly, that is complementary to the base
on the DNA strand which is to be copied.
13A transistor dissipates about 108 kT.

188
Quantum Computing
right, and slow ones from right to left. After performing these operations the
demon ends up with a box in which all the faster than average molecules are
in the left side and all the slower than average ones are in the right side. So
the box is hot on the left and cold on the right, and the expenditure of work
is negligible, in apparent violation of the second law of thermodynamics.14
Figure 4.1: Maxwell’s Demon.
Bennett built his solution on Szilard’s [286] subtle analysis of the Maxwell
demon.15 The resolution is that the demon must collect and store information
about the molecules in the box (position and velocity of each molecule). A
demon with a ﬁnite memory cannot continue to cool the gas indeﬁnitely;
eventually, information must be erased, and, as we now know, at exactly that
point, we have to pay the “power bill”. This argument has been extended to
quantum physics and may be even experimentally testable, cf. Lloyd [172];
see also the discussion in Zurek [308].
The moral we draw from the above discussion is, once again, that, to use
Landauer’s [161] words,
information is inevitably physical.
So, let’s explore what physics, especially quantum physics, has to tell us
about information.
14One can use this device to run a heat engine by allowing the heat to ﬂow from the hot
side to the cold side. Another possible action of the demon is to observe the molecules and
only open the door if a molecule is approaching the trapdoor from the right. This would
result in all the molecules ending up on the left side. Again this setup can be used to run
an engine.
15Szilard invented the concept of a bit of information and associated the entropy ∆S = k·
ln2 with the acquisition of one bit. However, the name “bit” was introduced only in 1946
by the statistician John Tukey.

Quantum World
189
4.6
Quantum World
On 19 October 1900, Max Planck, a 42 year old physicist, presented to a
meeting of the German Physical Society a ground-breaking solution to a
long-standing problem: Why does the colour of radiation from any glowing
body change from red to orange and ultimately to blue as its temperature
increases? Planck based his answer on the assumption that radiation (like
matter) comes in discrete quantities of energy, the so-called “quanta”.16 The
quantization of energy proved to be a revolutionary and fundamental rule of
nature, with very strange conclusions contradicting what common sense or
classical Newtonian mechanics imposed: things change because you “look”
at them, an action can precede its cause (Compton eﬀect), travelling faster
than light might be possible (EPR), objects have “random” behaviour, etc.17
How come that an innocuous statement about energy can determine such
an alarming behaviour as the uncertainty principle (one can never measure
anything as accurately as one would like, to put it in a rather misleading
way)?
Here is a simple example suggested by Feynman. Consider a mirror which
reﬂects a light, say in a proportion of 95% : 5% of light passes through and
it is absorbed or lost while 95% bounces oﬀthe mirror’s surface.
If light is seen as a continuous stream of energy then we don’t have any
diﬃculty in explaining the above phenomenon. However, if light is a stream
of indivisible quanta (photons), then we get a problem: each photon is either
reﬂected or absorbed in its entirety, but one cannot accept that 5% of a
photon goes into one direction and the remainder of 95% goes into a diﬀerent
direction! Consequently, one is led to the conclusion that out of 20 photons,
19 bounce oﬀthe mirror’s surface and just one photon goes in a diﬀerent
direction. OK, this is not diﬃcult to accept, but who decides which photon
goes astray? This is the crucial question!
Quantum theory claims that we cannot know the answer to the above
question: what happens to any individual photon is completely unpredictable!
We can only talk about chance, about probability: a photon will have a 95%
chance of bouncing oﬀand 5% chance of being absorbed/transmitted. One
cannot say more. The unpredictability/randomness is part of the game, it’s
innate! In opposition with classical physics, in quantum theory one can only
describe a quantum “state” of a photon in terms of its probabilities and
probabilities themselves change depending upon what one plans to do with
the photon.
Confusing? Alarming? Shocking? Dangerous?
16The Latin for amount.
17Planck felt very uncomfortable himself with the idea that quantization is a law of
nature. In this connection, he is notorious for saying that “new scientiﬁc theories supplant
previous ones not because people change their minds, but simply because old people die.”

190
Quantum Computing
From the time of original discoverers to the present day people expressed
wonder and disbelief:
Quantum mechanics is very impressive. But an inner voice tells
me that it is not the real thing. The theory produces a great deal
but hardly brings us closer to the secrets of the old one.
A. Ein-
stein.
. . . we always have had (secret, secret, close the doors!) . . . a great
deal of diﬃculty in understanding the world view that quantum
mechanics represents. At least I do, because I’m an old enough
man that I haven’t got to the point that this stuﬀis obvious to
me. Okay, I still get nervous with it.
R. Feynman.
In spite of all of this, quantum theory is one of the most successful theories:
it makes unbelievable good predictions which were tested to an unprecedented
degree of accuracy, it underpins modern technology, from supermarket laser
scanners to all goodies of microelectronics and . . . quantum computers.
4.7
Bits and Qubits
A classical bit (e.g. the position of gear teeth in Babbage’s diﬀerential en-
gine, a memory element or wire carrying a binary signal, in contemporary
machines) is a system comprising many atoms. Typically, the system is de-
scribed by one or more continuous parameters, for example, voltage. Such a
parameter is used to separate the space into two well-deﬁned regions chosen
to represent 0 and 1. Manufacturing imperfections and local perturbations
may aﬀect the signal, so signals are periodically restored near these regions
to prevent them from drifting away. An n-bit register of memory can exist
in any of 2n logical states, from 00 . . . 0 (n zeros) to 11 . . . 1 (n ones).
A quantum event in which we have two possible mutually exclusive out-
comes is the elementary act of observation: all knowledge of the physical
world is based upon such acts. An elementary act of observation is simul-
taneously like a coin-toss and not like a coin-toss. The information derived
from an elementary act of observation is no more than a single bit, but there
is more on it than that. To mark this diﬀerence Schumaker [265] has coined
the name qubit. A quantum bit, qubit, is typically a microscopic system,
such as an atom or nuclear spin or polarized photon. For example, the state
of a spin- 1
2 particle, when measured, is always found to be in one of two
possible states, represented as
| + 1
2⟩(spin-up) or | −1
2⟩(spin-down).
The intrinsic discrete character is a consequence of quantization discussed
in Section 4.6. Consequently, one can use one spin state to represent 0, and

Quantum Calculus
191
the other spin state to represent 1. There is nothing special about spin sys-
tems – any 2-state quantum system can be equally used to represent 0 and
1. What is really special here is the existence of a continuum of intermedi-
ate states which are superpositions of 0s and 1s.18 Unlike the intermediate
states of a classical bit (for example, any voltages between the “standard”
representations of 0 and 1) which can be distinguished from 0 and 1, but do
not exist from an informational point of view, quantum intermediate states
cannot be reliably distinguished, even in principle, from the basis states, but
do have an informational “existence”.
An n-qubit system can exist in any superposition of the form
Ψ =
11...1

x=00...0
cx|x⟩,
(4.6)
where cx are (complex) numbers such that 
x |cx|2 = 1.19 The exponential
“explosion” represented by formula (4.6) distinguishes quantum systems from
classical ones: in a classical system a state is described by a number of pa-
rameters growing only linearly with the size of the system,20 but, as we shall
see in the next section, quantum systems may not admit such a description
(because quantum states may be “entangled”).
4.8
Quantum Calculus
For a better understanding we need some rudiments of (ﬁnite dimensional)
Hilbert space theory. A Hilbert space is a mathematical model for repre-
senting vectors.21
The state of a quantum system can be described by a
column vector in a Hilbert space of wave functions;22 as the system evolves,
its state vector rotates with its base anchored to the origin of axes. Vectors
can be added and multiplied by (complex) numbers. State vectors are typi-
cally written with a special angular bracket notation, the “ket vector” |Ψ⟩.23
Row vectors, such as ⟨Ψ|, are known as “bra” vectors; when you put together
a column and a bra vector, you get a bracket, that is the inner product of
the two vectors, ⟨Ψ||Ψ⟩, also written as ⟨Ψ|Ψ⟩.
A simple 2-state quantum system (the basic block of a quantum memory
register) can, by deﬁnition, be in one of two possible states. To model it
18Mathematically, as we will see, they are just linear combinations of the basis states.
19Re-phrased, a quantum state of n qubits is just a direction in a Hilbert space of
dimension equal to the number of classical states, i.e. 2n.
20Reason: classical systems are completely described locally, that is, via each state in
part.
21Formally, a Hilbert space is a complex linear vector space, with an inner product which
is complete with respect to the induced norm.
22To be precise, a state is a ray in a Hilbert space, i.e. an equivalence class of vectors
that diﬀer by multiplication by a non-zero complex number.
23The word “ket” was invented by Paul Dirac [90].

192
Quantum Computing
we need the smallest non-trivial Hilbert space C2, a two dimensional space.
Assume that a particular complete orthonormal basis, denoted by {|0⟩, |1⟩},
has been ﬁxed.24 These vectors, |0⟩and |1⟩, correspond to the classical bit
values 0 and 1, respectively.
A qubit is a unit vector in the space C2, so for each qubit |x⟩, there are
two (complex) numbers a, b ∈C such that
|x⟩= a|0⟩+ b|1⟩=
	
a
b

,
(4.7)
where
|0⟩=
	
1
0

, |1⟩=
	
0
1

,
and |a|2 + |b|2 = 1.
The angle which a qubit makes with the vertical axis describes the rel-
ative contributions of |0⟩and |1⟩. The angle through which the vector is
rotated about the vertical axis induces the so-called “phase”. So, diﬀerent
qubits may have the same proportion of |0⟩and |1⟩, but with diﬀerent phase
factors. Phase is irrelevant for the whole states but it’s crucial for “quantum
interference eﬀects”.
We can perform a measurement that projects the qubit onto the basis
{|0⟩, |1⟩}. Then we will obtain the outcome |1⟩with probability |b|2, and the
outcome |0⟩with probability |a|2. With the exception of limit cases a = 0
and b = 0, the measurement irrevocably disturbs the state: If the value of the
qubit is initially unknown, then there is no way to determine a and b with
any conceivable measurement. However, after performing the measurement,
the qubit has been prepared in a known state (either |0⟩or |1⟩); this state is
typically diﬀerent from the previous state.
The above facts point out an important diﬀerence between qubits and
classical bits. There is no problem in measuring a classical bit without dis-
turbing it, so we can decode all of the information that it encodes. If we have
a classical bit with a ﬁxed, but unknown value (0 or 1), then we can only
say that there is a probability that the bit has the value 0, and a probability
that the bit has the value 1, and these two probabilities add up to 1. When
we measure the bit, we acquire additional information; after measurement,
we will know completely the value of the bit.
The ability of quantum systems to exist in a “blend” of all their al-
lowed states simultaneously is known as the Principle of Superposition. Even
though a qubit can be put in a superposition (4.7), it contains no more in-
formation than a classical bit, in spite of its having inﬁnitely many states.
The reason is that information can be extracted only by measurement. But,
24Here “complete” refers to the fact that every state vector in the Hilbert space can be
represented in the form (4.7), and “orthonormal” means that vectors are perpendicular to
one another, and normalized. For example, the vectors |0⟩and |1⟩may correspond to the
horizontal polarization | →⟩and the vertical polarization | ↑⟩of a photon, respectively.

Quantum Calculus
193
as we have argued, for any measurement of a qubit with respect to a given
orthonormal basis, there are only two possible results, corresponding to the
two vectors of the basis. On the other hand, it is not possible to capture
more information measuring in two diﬀerent bases because the measurement
changes the state. Even worse, quantum states cannot be cloned, hence it’s
impossible to measure a qubit in two diﬀerent ways (even, indirectly, by using
a copy trick, that is copying and measuring the copy).
Is a qubit identical to a probabilistic classical bit? The answer is negative
and an argument is that the numbers a and b in (4.7) encode more than just
the probabilities of the outcomes of a measurement in the {|0⟩, |1⟩} basis.
For example, the relative phase of a and b is crucial.
Systems of more than one qubit need a Hilbert space which captures the
interaction of the qubits. A two qubit system can be represented by a unit
vector in the tensor product of two copies of C2, i.e. the space C2 ⊗C2.
Using Dirac notation, if |0⟩and |1⟩are the vectors of a basis in C2 then, the
set
{|0⟩⊗|0⟩, |0⟩⊗|1⟩, |1⟩⊗|0⟩, |1⟩⊗|1⟩} = {|00⟩, |01⟩, |10⟩, |11⟩}
is a basis in C2 ⊗C2; more precisely,
|00⟩=




1
0
0
0



,
|01⟩=




0
1
0
0



,
|10⟩=




0
0
1
0



,
|11⟩=




0
0
0
1



.
In general, a system containing exactly n ≥2 qubits is represented by n
copies of C2 tensored together. Therefore, the state space is 2n dimensional.
A natural basis for this space consists of 2n tensor products:
|0⟩⊗|0⟩⊗. . . ⊗|0⟩,
|0⟩⊗|0⟩⊗. . . ⊗|1⟩,
...
|1⟩⊗|1⟩⊗. . . ⊗|1⟩.
A classical string of bits i1i2 . . . in with ik ∈{0, 1}, 1 ≤k ≤n, corresponds
to the quantum state |i1⟩⊗|i2⟩⊗. . . ⊗|in⟩which is simply denoted by
|i1i2 . . . in⟩. If |0⟩and |1⟩are orthogonal unit vectors in C2, then the set
{|i1i2 . . . in⟩|ik ∈{0, 1}, 1 ≤k ≤n}

194
Quantum Computing
is an orthonormal basis in C2 ⊗C2 ⊗. . . ⊗C2.
In contrast with the classical physics, where the state of a system is com-
pletely deﬁned by describing the state of each of its component pieces sepa-
rately, in a quantum system the state cannot always be described considering
only the component pieces. For instance, the state
1
√
2(|00⟩+ |11⟩)
cannot be decomposed into separate states for each of the two bits. This
means that we cannot express this state as a tensor product of two single
qubits. Indeed, let’s assume for the sake of a contradiction, that there exist
two kets |x⟩and |y⟩in C2 such that
1
√
2(|00⟩+ |11⟩) = |x⟩⊗|y⟩.
Since each single qubit is in a superposition of |0⟩and |1⟩, there exist four
complex numbers a1, b1, a2, b2 such that
|x⟩= a1|0⟩+ b1|1⟩
and
|y⟩= a2|0⟩+ b2|1⟩.
It follows that
|x⟩⊗|y⟩
=
(a1|0⟩+ b1|1⟩) ⊗(a2|0⟩+ b2|1⟩)
=
a1a2|00⟩+ a1b2|01⟩+ b1a2|10⟩+ b1b2|11⟩,
hence a1b2 = 0 and a1a2 =
1
√
2 = b1b2, which is impossible.
A state that cannot be expressed as a tensor product is called an entan-
gled state. Since the space C2 ⊗C2 is spanned by the set {|x⟩⊗|y⟩|
x, y ∈
C2}, the existence of entangled states proves that the previous set is not a
linear space. One can easily ﬁnd entangled states in an n qubit system, for
any integer n ≥2.
Note that it would require vast resources to simulate even a small quan-
tum system on a conventional computer, as such a simulation would require
keeping track of exponentially many states: the dimension of the cartesian
product of multiple classical particles grows linearly with the number of par-
ticles, while the dimension of the tensor product of quantum systems grows
exponentially.
A reason for the (potential) power of quantum computers
is the ability of exploiting the quantum state evolution as a computational
mechanism.

Qubit Evolution
195
4.9
Qubit Evolution
The quantum evolution (quantum transformation, operator) of (on) a qubit
is described by a “unitary operator”, that is an operator induced by a unitary
matrix.25
We will present some simple examples of single qubit quantum state trans-
formations. Any unitary operator U : C2 →C2 can be viewed as a single
qubit gate. Considering the basis {|0⟩, |1⟩}, the transformation is fully spec-
iﬁed by its eﬀect on the basis vectors.
In order to obtain the associated
matrix of an operator U, we put the coordinates of U|0⟩in the ﬁrst column
and the coordinates of U|1⟩in the second one. So, the general form of a
transformation that acts on a single qubit is a 2 × 2 matrix
A =
	
a
b
c
c

,
which transforms the qubit state α|0⟩+ β|1⟩into the state (αa + βb)|0⟩+
(cα + dβ)|1⟩:
	
a
b
c
d

 	
α
β

=
	
αa + βb
cα + dβ

.
For an arbitrary real number θ ∈[0, 2π), the rotation Rθ is given by
Rθ =
	
cos θ
−sin θ
sin θ
cos θ

.
Hence, Rθ acts as follows:
|0⟩&→cos θ|0⟩+ sin θ|1⟩, |1⟩&→−sin θ|0⟩+ cos θ|1⟩.
One can easily verify that RθR†
θ = RθRT
θ = I, hence Rθ is unitary. Note
that in the special case θ = 0 we get the identity transformation of C2:
R0 = I =
	
1
0
0
1

.
We may think of logic gates as transformations. For example, the NOT
transformation which interchanges the vectors |0⟩and |1⟩, is given by Rπ,
that is the matrix
NOT =
	
0
1
1
0

.
It ﬂips that state of its input,
NOT |0⟩=
	
0
1
1
0

 	
1
0

=
	
0
1

= |1⟩,
25A quadratic matrix A of order n over C is unitary if AA† = I (the identity n × n
matrix); A† is the transposed conjugate matrix of A.

196
Quantum Computing
and
NOT |1⟩=
	
0
1
1
0

 	
0
1

=
	
1
0

= |0⟩.
The phase shift gate Shift is deﬁned by the following operator:
Shift |0⟩= |0⟩, Shift |1⟩= −|1⟩, so
Shift =
	
1
0
0
−1

.
Since NOT · NOT † = I and Shift · Shift† = I, the operators NOT
and Shift are also unitary.
The operator Shift · NOT is also a unitary
transformation and we have:
Shift · NOT |0⟩= Shift |1⟩= −|1⟩,
Shift · NOT |1⟩= Shift |0⟩= |0⟩.
Therefore, its associated matrix is
R3π/2 =
	
0
1
−1
0

.
The square-root of NOT (introduced by Deutsch [85]) is the transforma-
tion
√
NOT :
|0⟩
→
1
2(1 + i)|0⟩+ 1
2(1 −i)|1⟩,
|1⟩
→
1
2(1 −i)|0⟩+ 1
2(1 + i)|1⟩,
√
NOT = 1
2
	
1 + i
1 −i
1 −i
1 + i

.
A routine check shows that
√
NOT ·
√
NOT = NOT ,
(4.8)
and
√
NOT ·
√
NOT
† = 1
4
	 1 + i
1 −i
1 −i
1 + i

 	 1 −i
1 + i
1 + i
1 −i

= I.
The square-root of NOT is a typical “quantum” gate in the sense that it
is impossible to have a single-input/single-output classical binary logic gate
that satisﬁes (4.8). Indeed, any classical binary
√
NOT classical

Qubit Evolution
197
gate is going to output a 0 or a 1 for each possible input 0/1.
Assume
that we have such a classical square-root of NOT gate acting as a pair of
transformations
√
NOT classical(0) = 1,
√
NOT classical(1) = 0.
Then, two consecutive applications of it will not ﬂip the input!26
Finally we consider the Hadamard transformation H is deﬁned by
H :
|0⟩
→
1
√
2(|0⟩+ |1⟩)
|1⟩
→
1
√
2(|0⟩−|1⟩)
, H =
1
√
2
	
1
1
1
−1

.
This transformation has a number of important applications. When ap-
plied to |0⟩, H creates a superposition state
1
√
2(|0⟩+ |1⟩).
Applied to n bits individually, H generates a superposition of all 2n possible
states. To see this we need some rudiments on tensor products.
Consider two operators A : Cn →Cm and B : Cq →Cp. The tensor
product of A and B is the operator A ⊗B : Cn ⊗Cq →Cm ⊗Cp, with the
property A⊗B(x⊗y) = Ax⊗By, for any x ∈Cn and y ∈Cq. A convenient
way is, again, to work with matrices. Let A be a (m × n) matrix and B a
(p × q) matrix. The (right) Kronecker product of A and B is the (mp × nq)
matrix deﬁned as follows:
A ⊗B =





a11B
a12B
· · ·
a1nB
a21B
a22B
· · ·
a2nB
...
...
...
am1B
am2B
· · ·
amnB




.
For example, if
A =
	
a11
a12
a21
a22

, B =
	
b11
b12
b21
b22

,
are two 2 × 2 matrices, then we have:
A ⊗B =




a11b11
a11b12
a12b11
a12b12
a11b21
a11b22
a12b21
a12b22
a21b11
a21b12
a22b11
a22b12
a21b21
a21b22
a22b21
a22b22




Two important mathematical results are useful:
26See Williams and Clearwater [296], Chapter 4, for a detailed analysis of the square-root
of NOT computation.

198
Quantum Computing
(a) If A and B are matrices associated to the operators A and
B, then the matrix associated to A ⊗B is the Kronecker product
of A and B.
(b) The tensor products of two unitary transformations is also
unitary.
Consequently, considering the tensor product of n single qubit transfor-
mations, we can obtain examples of unitary transformations acting on n
qubits.
For instance, let (|00⟩, |01⟩, |10⟩, |11⟩) be the basis in C2⊗C2 and consider
the following transformations:
I ⊗I :
|00⟩
→
|00⟩
|01⟩
→
|01⟩
|10⟩
→
|10⟩
|11⟩
→
|11⟩
, I ⊗I =




1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1



.
I ⊗NOT :
|00⟩
→
|01⟩
|01⟩
→
|00⟩
|10⟩
→
|11⟩
|11⟩
→
|10⟩
, I ⊗NOT =




0
1
0
0
1
0
0
0
0
0
0
1
0
0
1
0



.
NOT ⊗I :
|00⟩
→
|10⟩
|01⟩
→
|11⟩
|10⟩
→
|00⟩
|11⟩
→
|01⟩
, NOT ⊗I =




0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0



.
NOT ⊗NOT :
|00⟩
→
|11⟩
|01⟩
→
|10⟩
|10⟩
→
|01⟩
|11⟩
→
|00⟩
, NOT ⊗NOT =




0
0
0
1
0
0
1
0
0
1
0
0
1
0
0
0



.
Shift ⊗NOT :
|00⟩
→
|01⟩
|01⟩
→
|00⟩
|10⟩
→
−|11⟩
|11⟩
→
−|10⟩
, Shift ⊗NOT =




0
1
0
0
1
0
0
0
0
0
0
−1
0
0
−1
0



.
We are ready to come back to binary representations of the numbers from
0 to 2n −1 via Hadamard operator. The Walsh–Hdamard transformation is
deﬁned recursively by
Wn = H, if n = 1 and Wn = H ⊗Wn−1, for any n ≥2.

Qubit Evolution
199
For example, if n = 2 then
W2|00⟩
=
(H ⊗H)|00⟩
=
H|0⟩⊗H|0⟩
=
1
√
2(|0⟩+ |1⟩) ⊗1
√
2(|0⟩+ |1⟩)
=
1
2(|00⟩+ |01⟩+ |10⟩+ |11⟩),
W2|01⟩
=
(H ⊗H)|01⟩
=
H|0⟩⊗H|1⟩
=
1
√
2(|0⟩+ |1⟩) ⊗1
√
2(|0⟩−|1⟩)
=
1
2(|00⟩−|01⟩+ |10⟩−|11⟩),
W2|10⟩
=
(H ⊗H)|10⟩
=
H|1⟩⊗H|0⟩
=
1
√
2(|0⟩−|1⟩) ⊗1
√
2(|0⟩+ |1⟩)
=
1
2(|00⟩+ |01⟩−|10⟩−|11⟩),
W2|11⟩
=
(H ⊗H)|11⟩
=
H|1⟩⊗H|1⟩
=
1
√
2(|0⟩−|1⟩) ⊗1
√
2(|0⟩−|1⟩)
=
1
2(|00⟩−|01⟩−|10⟩+ |11⟩),
so the associated matrix is
W2 = 1
2




1
1
1
1
1
−1
1
−1
1
1
−1
−1
1
−1
−1
1



.

200
Quantum Computing
For n = 3, we have
W3|000⟩
=
(H ⊗W2)|000⟩
=
H|0⟩⊗W2|00⟩
=
1
√
2(|0⟩+ |1⟩) ⊗1
2(|00⟩+ |01⟩+ |10⟩+ |11⟩)
=
1
2
√
2(|000⟩+ |001⟩+ |010⟩+ |011⟩+ |100⟩+ |101⟩+ |110⟩
+|111⟩),
W3|001⟩
=
(H ⊗W2)|001⟩
=
H|0⟩⊗W2|01⟩
=
1
√
2(|0⟩+ |1⟩) ⊗1
2(|00⟩−|01⟩+ |10⟩−|11⟩)
=
1
2
√
2(|000⟩−|001⟩+ |010⟩−|011⟩+ |100⟩−|101⟩+ |110⟩
−|111⟩),
and so on. The associated matrix is W3 = H ⊗W2:
W3 =
1
√
2
	
1
1
1
−1

⊗1
2




1
1
1
1
1
−1
1
−1
1
1
−1
−1
1
−1
−1
1




=












1
1
1
1
1
1
1
1
1
−1
1
−1
1
−1
1
−1
1
1
−1
−1
1
1
−1
−1
1
−1
−1
1
1
−1
−1
1
1
1
1
1
−1
−1
−1
−1
1
−1
1
−1
−1
1
−1
1
1
1
−1
−1
−1
−1
1
1
1
−1
−1
1
−1
1
1
−1












.
If we apply the Walsh–Hdamard transformation to |00 . . . 0⟩we get a
superposition of all possible states:

Qubit Evolution
201
Wn|00 . . . 0⟩
=
(H2 ⊗H2 ⊗. . . ⊗H2)|00 . . . 0⟩
=
1
√
2n ((|0⟩+ |1⟩) ⊗(|0⟩+ |1⟩) ⊗. . . ⊗(|0⟩+ |1⟩))
=
1
√
2n
2n−1

i=0
|i⟩.
For many quantum algorithms the state
|ψ⟩=
1
√
2n
2n−1

i=0
|i⟩
(4.9)
is very convenient to be an “initial state” because it contains an equal
weighted distribution of all basis states. An “empty” register can be “set”
in the above state by an application of the Walsh–Hdamard transformation.
In this way, using a linear number of operations we can transform one basis
state into an exponentially large, equally weighted superposition of all basis
states.
Another useful transformation on C2 ⊗C2 is the “controlled-NOT” gate,
CNOT deﬁned as follows:
CNOT :
|00⟩
→
|00⟩
|01⟩
→
|01⟩
|10⟩
→
|11⟩
|11⟩
→
|10⟩
, CNOT =




1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0



.
Given the input state |ij⟩, i, j ∈{0, 1}, the output state produced by
CNOT is |ik⟩, where k = i ⊕j (mod 2). The ﬁrst bit is not disturbed (it is
a control bit) and the second one interchanges 0 and 1 if and only if the ﬁrst
bit is 1, which corresponds to the logical exclusive-OR (XOR).
The controlled-NOT gate CNOT can be represented by a circuit of the
form speciﬁed in Figure 4.2.
The “oplus” sign indicates the control bit; the opposite symbol indicates
the conditional negation of the second bit. If the input states at a and b are
in base states |0⟩or |1⟩, then the output state at x is the same as the input
state at a, and the output state at y is the exclusive-OR of the two input
states.
The transformation CNOT is unitary since C†
NOT
=
CNOT and
C2
NOT = I4 (the 4 × 4 identity matrix). On the other hand,
CNOT cannot be written as a tensor product of two operators.

202
Quantum Computing
a
x
b
y
❤
Figure 4.2: The controlled-NOT gate.
Indeed, assume the contrary, and take two operators A, B such that
CNOT = A ⊗B. Assume that the associated matrices are
A =
	
a11
a12
a21
a22

, B =
	
b11
b12
b21
b22

so, the matrix A ⊗B corresponds to CNOT and we have
A ⊗B =




a11b11
a11b12
a12b11
a12b12
a11b21
a11b22
a12b21
a12b22
a21b11
a21b12
a22b11
a22b12
a21b21
a21b22
a22b21
a22b22




= CNOT =




1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0



.
Since a11b11 = 1 and a11b12 = 0 it follows that a11 ̸= 0 and b12 = 0, which
is impossible because a22b12 = 1.
Similarly, one can deﬁne the “controlled-controlled-NOT” transformation,
CCNOT, operating on three qubits, which negates the rightmost bit if and
only if the ﬁrst two are both 1:
CCNOT :
|000⟩
→
|000⟩
|001⟩
→
|001⟩
|010⟩
→
|010⟩
|011⟩
→
|011⟩
|100⟩
→
|100⟩
|101⟩
→
|101⟩
|110⟩
→
|111⟩
|111⟩
→
|110⟩
, CCNOT =












1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0












.

No Cloning Theorem
203
We have CCNOT · CC†
NOT = CC†
NOT · CCNOT = I8, hence CCNOT
is also unitary.
4.10
No Cloning Theorem
Quantum states cannot be cloned, as Wooters and Zurek [301], and Dieks [89]
have proved as an application of the linearity of unitary transformations. It
is not possible to create the state (a|0⟩+b|1⟩)⊗(a|0⟩+b|1⟩) from an unknown
state a|0⟩+ b|1⟩.
In other words, there is no unitary transformation U such that U|ϕ0⟩=
|ϕϕ⟩for all quantum states |ϕ⟩.
Indeed, assume the contrary and let |ϕ⟩and |ψ⟩be two orthogonal vectors
in C2 and take |x⟩=
1
√
2(|ϕ⟩+ |ψ⟩). Then, U|ϕ0⟩= |ϕϕ⟩and U|ψ0⟩= |ψψ⟩.
On the one hand,
U|x0⟩
=
|xx⟩
=
1
√
2(|ϕ⟩+ |ψ⟩) ⊗1
√
2(|ϕ⟩+ |ψ⟩)
=
1
2(|ϕϕ⟩+ |ϕψ⟩+ |ψϕ⟩+ |ψψ⟩).
On the other hand,
U|x0⟩
=
U( 1
√
2(|ϕ0⟩+ |ψ0⟩))
=
1
√
2(U|ϕ0⟩+ U|ψ0⟩)
=
1
√
2(|ϕϕ⟩+ |ψψ⟩).
Since the vectors ϕ and ψ are orthogonal, the vectors |ϕϕ⟩, |ϕψ⟩, |ψϕ⟩,
|ψψ⟩constitute a basis in C2 ⊗C2 and the vector |xx⟩= U|x0⟩has been
written in two diﬀerent ways as a linear combination of this basis vectors, an
impossibility.
It’s important to understand that the no cloning principle states the im-
possibility of reliably cloning an unkown quantum state: it is possible to clone
a known quantum state. It is possible to obtain n particles in an entangled
state a|00 . . . 0⟩+ b|11 . . .⟩from an unknown state a|0⟩+ b|1⟩. Each particle
will behave in exactly the same way when measured with respect to the basis
{|00 . . . 0⟩, |00 . . . 01⟩, . . . |11 . . . 1⟩}, but not when measured with respect to
other bases. It is not possible to create the n particle state
(a|0⟩+ b|1⟩) ⊗(a|0⟩+ b|1⟩) ⊗. . . (a|0⟩+ b|1⟩)
from an unkown state a|0⟩+ b|1⟩, cf. Rieﬀel and Polak [250].

204
Quantum Computing
In a sense, the no cloning principle seems to announce “bad news”: we lose
one of the most important facilities of classical computation, the unlimited
possibility to copy. There is “good news” derived from this principle, for
example, the possibility of unconditional secure key generation (see Section
6.2 in Gruska [119]). New techniques (see, for example, Buˇzek, Braunstein,
Hillery, Bruβ, [40]) open possibilites to produce “approximate” copies of
qubits: imperfect, but very close to real copies of qubits can be produced
with a “quality” not depending upon the qubits to be copied. Of course,
there is a price to be paid: copies produced in this way are entangled.
4.11
Measurements
As we have already seen, the measurement of one or more particles in a
quantum system results in a projection of the state of the system prior to
measurement onto the subspace of the state space compatible with the mea-
sured values. The amplitude of the projection is rescaled to make sure that
the resulting state vector has length one. The probability that the result of
the measurement is a given value is the sum of the squares of the absolute
values of the amplitudes of all components compatible with that value of the
measurement.
A simple example of measurement in a two qubit system will illustrate the
above points. Let’s ﬁx the basis {|0⟩, |1⟩}, and assume that all measurements
of individual qubits will be done with respect to this basis. An arbitrary state
of a two qubit system can be written as
a|00⟩+ b|01⟩+ c|10⟩+ d|11⟩,
where a, b, c and d are complex numbers such that
|a|2 + |b|2 + |c|2 + |d|2 = 1.
When the ﬁrst qubit is measured, then the probability that the result is |0⟩
is |a|2 + |b|2. Assume now that the measurement gives the ﬁrst qubit exactly
that value, that is, |0⟩. Consequently, the state is projected onto the subspace
compatible with the measurement which is the subspace spanned by |00⟩and
|01⟩and the result of this projection is a|00⟩+ b|01⟩. Renormalizing we get:
1

|a|2 + |b|2 · (a|00⟩+ b|01⟩).
In general, consider a system containing n qubits (n ≥2). Any state |x⟩
of the system can be expressed as

i1,i2,...,in=0,1
ci1i2...in|i1i2 . . . in⟩,

Measurements
205
where

i1,i2,...,in=0,1
|ci1i2...in|2 = 1.
When the ﬁrst qubit is measured with respect to the basis {|0⟩, |1⟩}, then
the result |0⟩is obtained with probability
P =

i2,...in=0,1
|0i2 . . . in⟩|2.27
After rescaling, the new state obtained after the measurement is
1

i2,...,in=0,1 |c0i2...in|2 ·



i2,...,in=0,1
c0i2...in|0i2 . . . in⟩

.
Similarly, the measurement gives the outcome |1⟩with the probability
1 −P =

i2,...,in=0,1
|c1i2...in|2,
and the state changes correspondingly.
What is the price of measurement? According to Landauer [159],
If it [measurement] is simply information transfer, that is done
all the time inside the computer, and can be done with arbitrarily
little dissipation.
There are many speculations about the “collapse of the wave function
(state)” due to an irreversible interaction of the microphysical quantum sys-
tem with the macroscopic measurement apparatus. Some authors (see, for ex-
ample, Greenberg and YaSin [117] or Herzog, Kwiat, Weinfuter and Zeilinger
[132]) have argued that it is, in fact, possible to reconstruct the state of the
physical system before the measurement, that is, to “reverse the collapse of
the wave function” if the process of measurement is reversible. After “recon-
struction” no information about the measurement is left.
The act of measurement gives another perspective about entangled par-
ticles. Particles are not entangled if the measurement of one has no eﬀect on
the other. For instance, the state
1
√
2(|00⟩+ |11⟩)
is entangled since the probability that the ﬁrst bit is measured to be |0⟩is
1/2 if the second bit has not been measured. However, if the second bit had
27We used the projection onto the space spanned by {|0i2 . . . in⟩|ik ∈{0, 1}, 2 ≤k ≤n}.

206
Quantum Computing
been measured, then the probability that the ﬁrst bit is measured as |0⟩is
diﬀerent from 1/2, it is either 1 or 0, depending on whether the second bit
was measured as |0⟩or |1⟩, respectively. Hence, the probability of measuring
the ﬁrst bit has been changed by the measurement of the second bit.
In contrast, the state
1
√
2(|00⟩+ |01⟩) = |0⟩⊗1
√
2(|00⟩+ |11⟩)
is not entangled. Reason: any measure of the ﬁrst qubit will produce the
result |0⟩independently whether a measurement is performed or not on the
second qubit, and the second qubit has probability 1
2 to be measured to |0⟩
regardless of whether the ﬁrst qubit was measured or not.
In a sense, entangled states can be equivalently presented in mathematical
terms (they cannot be represented as a tensor product of two states) or in
physical terms (the measurement on one aﬀects the other); however, the
physical meaning is richer than the mathematical formalism.
An important consequence of the existence of entangled states is the fact
that if a quantum memory register exists in an entangled state, one can
change the state of one part of the register simply by measuring another part
of it. This is a unique feature of quantum physics28 which has no parallel in
classical physics. Entanglement is one of the most important features which
distinguishes Quantum from Conventional Computing.
4.12
Zeno Machines
A Zeno machine is a Turing machine which computes with “increased speed”.
Two time scales act simultaneously: the intrinsic time scale of the process
of computation approaches the inﬁnity in a ﬁnite extrinsic (or proper) time
of some outside observer, cf. Svozil [284]. As a consequence, certain uncom-
putable functions (i.e. functions which cannot be computed by any Turing
machine) become Zeno computable. For example, the halting problem – the
most notorious unsolvable problem in classical computation theory (see, for
example, Odifreddi [198]) is Zeno solvable.
Zeno machines have been introduced by Weyl [290] (see Svozil [284] for
a bibliography on this subject). Already Weyl raised the question whether
it is kinematically feasible for a machine to carry out an inﬁnite sequence of
operations in a ﬁnite time. He wrote [290], p. 42:
Yet, if the segment of length 1 really consists of inﬁnitely
many subsegments of length 1/2, 1/4, 1/8, . . ., as of ‘chopped-oﬀ’
wholes, then it is incompatible with the character of the inﬁnite
28Which is crucial in many quantum algorithms, teleportation, information transmission,
etc.

Zeno Machines
207
as the ‘incompletable’ that Achilles should have been able to tra-
verse them all. If one admits this possibility, then there is no
reason why a machine should not be capable of completing an in-
ﬁnite sequence of distinct acts of decision within a ﬁnite amount
of time; say, by supplying the ﬁrst result after 1/2 minute, the
second after another 1/4 minute, the third 1/8 minute later than
the second, etc. In this way it would be possible, provided the
receptive power of the brain would function similarly, to achieve a
traversal of all natural numbers and thereby a sure yes-or-no de-
cision regarding any existential question about natural numbers!
A possible construction of a Zeno machine starts with a normal Turing
machine and considers two time scales, τ and t as follows:
• The proper time τ measures the physical system time by clocks in an
usual way.
• A discrete cycle time t = 0, 1, 2, . . . characterizes an “intrinsic” time
scale for a process running on the machine.
• For some unspeciﬁed reason we assume that the machine allows us to
“squeeze” its intrinsic time t with respect to the proper time τ by a
geometric progression. For k < 1 we let any time cycle of t, if measured
in terms of τ, to be “squeezed” by a factor of k with respect to the
foregoing time cycle. More precisely,
τ0 = 0, τ1 = k, τt+1 −τt = k(τt −τt−1),
that is
τt = k(kt −1)
k −1
.
In the limit when t approaches the inﬁnity, the proper time τ∞approaches
k/(1 −k), so it remains ﬁnite.
There is no commonly accepted classical physical principle which would,
a priori, forbid such a behaviour.29 One might argue that such an “oracle”
would require a geometric energy increase resulting in an inﬁnite consumption
of energy. Yet, no currently accepted classical physical principle excludes us
from assuming that every geometric decrease in cycle time could be associated
with a geometric progression in energy consumption, at least up to some
limiting (e.g. Planck) scale.
So, classical physics doesn’t forbid the existence of Zeno machines. How-
ever, classical logic does. A simple diagonalization argument, which mimics
the undecidability of the halting problem, shows that Zeno machines are log-
ically impossible.
Consider an arbitrary algorithm B(x) whose input is a
29Classical mechanics postulates space and time continua as a foundational principle.

208
Quantum Computing
binary string x. Assume, for the sake of a contradiction, that there exists an
eﬀective halting algorithm HALT, implementable on a Zeno machine, which is
able to decide whether B eventually stops on x or not. Using HALT(B(x)) we
shall construct another Zeno machine A, which has as an input a program B
and which proceeds as follows: Upon reading the program B as an input, A
makes a copy of it.30 In the next step, our machine uses the code #(B) as
an input string for B itself, that is, A forms B(#(B)), henceforth denoted
by B(B). The machine hands B(B) over to its subroutine HALT. Then, A
proceeds as follows:
• if HALT(B(B)) decides that B(B) eventually halts, then A does not
halt,31
• if HALT(B(B)) decides that B(B) never halts, then A halts.
What about using A on its own code as input? Notice that B is arbitrary,
so there is no restriction to prevent us for doing this! Consequently, A, which
is representable by its code #(A) will be applied to itself.
Assume that classically A is restricted to classical bits of information.
Then, whenever A(A) halts, HALT(A(A)) forces A(A) not to halt, and con-
versely, whenever A(A) does not halt, then HALT(A(A)) steers A(A) into the
halting state. In both cases one arrives at a contradiction, therefore, Zeno
machines are logically inconsistent.
What about the case when A is allowed a qubit of information. Assume
that |0⟩and |1⟩are the halting and nonhalting states, respectively.
The
computation can be performed if A receives as an input a qubit corresponding
to the ﬁxed point state |⋆⟩of the NOT operator32:
NOT =
	
0
1
1
0

,
NOT|⋆⟩= |⋆⟩.
A simple computation shows that
|⋆⟩= | 1
√
2, 1
√
2⟩.
The qubit solution |⋆⟩proves the impossibility of A to control the output
as the probability to reach a halting (nonhalting) state is exactly one half. At
the level of probability amplitudes, quantum theory permits Zeno machines,
but at the level of observable probabilities, this super-power is nulliﬁed, as
the result of the computation appears to be random.
30This can be readily achieved, since the program B is presented to A in some encoded
form #(B), i.e. as a string of symbols.
31This can be realized by an inﬁnite loop.
32Diagonalization operator.

Inexhaustible Uncertainty
209
4.13
Inexhaustible Uncertainty
In 1927 Werner Heisenberg discovered a fundamental limitation of quantum
mechanics: a bound on the accuracies with which certain complementary
pairs of observables can be measured.
The “canonical” understanding of
complementarity is expressed in Messiah [190, p. 154]
The description of properties of microscopic objects in classical
terms requires pairs of complementary variables; the accuracy in
one member of the pair cannot be improved without a correspond-
ing loss in the accuracy of the other member.
...
It is impossible to perform measurements of position x and mo-
mentum p with uncertainties (deﬁned by the root-mean square
deviations) ∆x and ∆p such that the product of ∆x∆p is smaller
than a constant unit of action ¯h
2.33
In Prigogine’s words [244, p. 51],
the world is richer than it is possible to express in any single
language.
Next we will follow Moore [196] to illustrate uncertainty using simple
automata “Gedanken” experiments (see more in Conway [65], Brauer [38],
Svozil [282], Calude [56]).
A (simple) Moore experiment can be described as follows: a copy of the
machine will be experimentally observed, i.e. the experimenter will input a
string of input symbols to the machine and will observe the sequence of output
symbols. The correspondence between input and output symbols depends on
the particular chosen machine and on its initial state. The experimenter will
study the sequences of input and output symbols and will try to conclude
that “the machine being experimented on was in state q at the beginning of
the experiment”.34
In what follows we will work with ﬁnite deterministic automata with a
ﬁnite set K of states, an input alphabet V , and a transition function δ :
33Many other instances of complementarity are well known. A simple example is oﬀered
by the so-called two-slit experiment. A source is “shooting” electrons towards a wall which
has two tiny holes (slits), each of them just enough for one electron to get through at a
time. A second wall has a detector, that can be moved up and down, with the aim to count
the number of electrons reaching a given position of the second wall. Experimentally one
can determine the probabilities that electrons reach some positions on the second wall,
depending upon the number of open slits, one or two. Contrary to common intuition, due
to interference, there are places where one counts fewer electrons in the case when both
slits are open than in the case when only one slit is open! Detecting through which slit
an electron went (a particle measurement) or recording the interference pattern (a wave
measurement) cannot be done in the same experiment.
34This is often referred to as a state identiﬁcation experiment.

210
Quantum Computing
K × V →K. Instead of ﬁnal states we will consider an output function
f : K →{0, 1}. At each time the automaton is in a given state q and is
continuously emitting the output f(q). The automaton remains in state q
until it receives an input signal σ, when it assumes the state δ(q, σ) and
starts emitting f(δ(q, σ)). As we will discuss only the simplest case when the
alphabet V = {0, 1}, an automaton will be just a triple M = (K, δ, f).
The transition function δ can be extended to a function δ : K × V ∗→K,
as follows: δ(q, λ) = q, for all q ∈K, and δ(q, σw) = δ(δ(q, σ), w), for all
q ∈K, σ ∈V, w ∈V ∗.
The output produced by an experiment started in state q with input
sequence w ∈V ∗is described by E(q, w), where E is the function E : Q ×
V ∗−→V ∗deﬁned by the following equations:
E(q, λ) = f(q),
E(q, σw) = f(q)E(δ(q, σ), w),
for all q ∈K, σ ∈V, w ∈V ∗; recall that f : K −→V is the output function.
Consider, for example, Moore’s automaton, in which K = {1, 2, 3, 4}, the
transition is given by the following tables:
q
σ
δ(q, σ)
1
0
4
1
1
3
2
0
1
2
1
3
q
σ
δ(q, σ)
3
0
4
3
1
4
4
0
2
4
1
2
Table 4.3: Moore’s automaton transition.
and the output function is deﬁned by f(1) = f(2) = f(3) = 0, f(4) = 1. A
graphical display appears in Figure 4.13.
The experiment starting in state 1 with input sequence 000100010 leads
to the output 0100010001. Indeed,
E(1, 000100010)
=
f(1)f(4)f(2)f(1)f(3)f(4)f(2)f(1)f(3)f(4)
=
0100010001.
Consider now an automaton M = (K, δ, f), and following Moore [196]
say that a state q is “indistinguishable” from a state q′ (with respect to M)
if every experiment performed on M starting in state q produces the same
outcome as it would starting in state q′. Formally, E(q, x) = E(q′, x), for all
strings x ∈V +.

Inexhaustible Uncertainty
211
t
t
t
t
1/0
2/0
3/0
4/1
0
1
1
0,1
0
✻










✒
✛
✻
✛
❅
❅
❅
❅
❅
❅
❅
❅
❅
❅
❅
❘
0,1
Figure 4.3: Moore’s automaton.
An equivalent way to express the indistinguishability of the states q and
q′ is to require, following Conway [65, p. 3], that for all w ∈V ∗,
f(δ(q, w)) = f(δ(q′, w)).
Indeed,
E(q, x1x2 . . . xn) = f(q)f(δ(q, x1))f(δ(q, x1x2)) · · · f(δ(q, x1x2 . . . xn)),
for all q ∈K, x1x2 . . . xn ∈V ∗.
A pair of states will be said to be “distinguishable” if they are not “in-
distinguishable”, i.e. if there exists a string x ∈V +, such that E(q, x) ̸=
E(q′, x).
Moore [196] has proven the following important theorem:
There exists an automaton M such that any pair of its distinct
states are distinguishable, but there is no experiment which can
determine what state the machine was in at the beginning of the
experiment.
Moore used the automaton displayed in Figure 4.13 and his argument is
simple. Indeed, each pair of distinct states can be distinguished by an exper-
iment: 1, 2 by x = 0, 1, 3 by x = 1, 1, 4 by x = 0, 2, 3 by x = 0, 2, 4 by x = 0,
and 3, 4 by x = 0.
However, there is no (unique) experiment capable to distinguish between
every pair of arbitrary distinct states. Two cases have to be examined:

212
Quantum Computing
(A) The experiment starts with 1, i.e.
x = 1u, u ∈V ∗.
In this case
E(1, x) = E(2, x), that is x cannot distinguish between the states 1, 2 as
E(1, x) = E(1, 1u)
=
f(1)f(δ(1, 1))E(δ(1, 1), u)
=
f(1)f(3)E(3, u) = 00E(3, u),
and
E(2, x) = E(2, 1u)
=
f(2)f(δ(2, 1))E(δ(2, 1), u)
=
f(2)f(3)E(3, u) = 00E(3, u).
(B) The experiment starts with 0, i.e. x = 0v, v ∈V ∗. In this case
E(1, x) = E(2, x),
that is x cannot distinguish between the states 1, 3 as
E(1, x) = E(1, 0v)
=
f(1)f(δ(1, 0))E(δ(1, 0), v)
=
f(1)f(4)E(4, v) = 01E(4, v),
and
E(2, x) = E(3, 0v)
=
f(3)f(δ(3, 0))E(δ(3, 0), v)
=
f(3)f(4)E(4, v) = 01E(4, v).
Moore’s result can be thought of as being a discrete analogue of the
Heisenberg uncertainty principle. The state of an electron E is considered
speciﬁed if both its velocity and its position are known. Experiments can be
performed with the aim of answering either of the following:
1. What was the position of E at the beginning of the experiment?
2. What was the velocity of E at the beginning of the experiment?
For an automaton, experiments can be performed with the aim of answer-
ing either of the following:
1. Was the automaton in state 1 at the beginning of the experiment?
2. Was the automaton in state 2 at the beginning of the experiment?
In either case, performing the experiment to answer question 1 changes the
state of the system, so that the answer to question 2 cannot be obtained. This
means that it is only possible to gain partial information about the previous
history of the system, since performing experiments causes the system to
“forget” about its past.
An exact quantum mechanical analogue has been given by Foulis and
Randall [100, Example III]: Consider a device which, from time to time, emits

Randomness
213
a particle and projects it along a linear scale. We perform two experiments.
In experiment A, the observer determines if there is a particle present. If there
is no particle, the observer records the outcome of A as the outcome {4}. If
there is a particle, the observer measures its position coordinate x. If x ≥1,
the observer records the outcome {2}, otherwise {3}. A similar procedure
applies for experiment B: If there is no particle, the observer records the
outcome of B as {4}. If there is, the observer measures the x-component px
of the particle’s momentum. If px ≥1, the observer records the outcome
{1, 2}, otherwise the outcome {1, 3}.35
Moore’s automaton is a simple model featuring an “uncertainty principle”
(cf.
Conway [65, p.
21]), later termed “computational complementarity”
by Finkelstein and Finkelstein [99]; for a detailed analysis see Svozil [282],
Calude, Calude, Svozil and Yu [43], Calude and Lipponen [53], Jurvanen and
Lipponen [145].
4.14
Randomness
Randomness is at the very heart of quantum physics. When a physical state
that is in a superposition of states is measured, then it collapses into one of
its possible states in a completely unpredictable way – we can only evaluate
the probability of obtaining various possible outcomes. An extreme view is
to claim with Peres [236] that
in a strict sense quantum theory is a set of rules allowing the
computation of probabilities for the outcomes of tests which follow
speciﬁc preparations.
According to Milburn [193], p. 1, a quantum priciple is
physical reality is irreducible random.
We are talking about “true” randomness, not the “randomness” which,
at times, nature appears to exhibit and for which classical physics blames our
ignorance: meteorologists cannot predict accurately the path of a hurricane,36
for example.
A mathematical deﬁnition of randomness is provided by algorithmic in-
formation theory, see Chaitin [59, 60], Calude [42].
The idea is to deﬁne
(algorithmic) randomness as incompressibility. The length of the smallest
program (say, for a universal Turing machine37) generating a binary string
35Another quantum mechanical analogue has been proposed by Giuntini [114], pp. 159-
162.
36The explanation is not diﬃcult to obtain: the equations governing the motion of the
atmosphere are nonlinear and tiny errors in the initial conditions can immensely amplify.
This behaviour is known as “deterministic chaos”.
37For technical reasons, we use self-delimiting Turing machines, machines having a
“preﬁx-free” domain: no proper extension of a program that eventually halts has that
property.

214
Quantum Computing
is the program-size complexity of the string. This idea can be extended in
an appropriate way to inﬁnite sequences. A random string/sequence is in-
compressible as the smallest program for generating it is the string/sequence
itself! Strings/sequences that can be generated by small programs are deemed
to be less random than those requiring longer programs. For example, the
digits of π(3.1415926...) can be computed one by one; nonetheless, if exam-
ined locally, without being aware of their provenance, they appear “random”.
People have calculated π out to a billion or more digits. A reason for doing
this is the question of whether each digit occurs the same number of times,
a sympton of randomness. It seems, but remains unproven, that the digits
0 through 9 each occur 10% of the time in a decimal expansion of π.
If
this turns out to be true, then π would be a so-called simply normal real
number. But although π may be random in so far as it’s “normal”, it is far
from (algorithmic) random, because its inﬁnity of digits can be compressed
into a concise program for calculating them. The numbers generated by the
so-called logistic map,
xn+1 = rxn(1 −xn),
(4.10)
where r is an arbitrary constant and the process starts at some state x0 = c,
may appear “random” for some values, say x0 = 0.1 and r = 3.98; however,
they are not, because of the succinctness of the rule (4.10) describing them.
In general, a long string of pseudo-random bits produced by a program may
pass all practical statistical tests for randomness, but it is not (algorithmic)
random: its program-size complexity is bounded by the size of the generating
program plus a few extra bits which specify the random number seed.
Similarly, a long string of binary bits produced by any classical physical
system, of which a Turing machine or a Java program is just an instance, is
not (algorithmic) random. The program-size complexity of such a string is
bounded by the size of the program generating it, that is, the physical law
which governs its evolution, plus the size of the initial conditions on which
the law acts. Any classical computer can only feign randomness; thinking
otherwise is not only wrong, but as von Neumann said,
Anyone who considers arithmetical methods of producing random
digits is, of course, in a state of sin.
Note that human beings are not doing a better job in generating “random”
bits as Shannon [268] has argued.38
Is there any physical system that can generate arbitrarily long (algorith-
mic) random strings?
It is not diﬃcult to destroy randomness. For example, start with a random
sequence x1x2 . . . xn . . . over the alphabet {0, 1} and deﬁne a new sequence
y1y2 . . . yn . . ., over the alphabet {0, 1, 2}, by
y1 = x1, yn = xn−1 + xn, n ≥2.
38Biases observed in people’s preferences for popular lottery numbers are manifest.

Randomness
215
Then, the new sequence is not random. The motivation is simple: the strings
02 and 20 (and, inﬁnitely many more others) never appear, so the sequence
has clear regularities (which can, actually, be detected by simple statistical
randomness tests).
It is much more demanding to “generate” a truly random long string
starting from an initial state with a simple description. Note that the condi-
tion of simplicity of the initial state is crucial: starting from a random string
one can generate, in a pure algorithmic way, many other random strings.
For example, if x1x2 . . . x2n−1x2n is a random binary string, then break the
string into pairs and then code 00, 01, 10, 11 by a, b, c, d: the result is again
a random sequence. So, the problem is to start from an initial state which
can be precisely controlled and has a low program-size complexity and pro-
duce measurements of unbounded program-size complexity out its natural
dynamical evolution.
Quantum mechanics seems capable to produce, with probability one, truly
random strings. Here is a way to do it. Consider the operator
Rθ =
	
cos θ
−sin θ
sin θ
cos θ

,
and recall that it rotates a qubit a|0⟩+b|1⟩through an angle θ. In particular,
R π
4 transforms that state |0⟩into an equally weighted superposition of 0 and
1:
1
√
2|0⟩+ 1
√
2|1⟩.
(4.11)
So, to make a quantum device to produce random bits one needs to place
a 2-state quantum system in the |0⟩state, apply the operator R π
4 to rotate
the state into the superposition (4.11), and the observe the superposition.
The act of observation produces the collapse into either |0⟩or |1⟩, with equal
chances. Consequently, one can use the quantum superposition and indeter-
minism to simulate, with probability one, a “fair” coin toss. Random digits
produced with quantum random generators of the type described above are,
with probability one, free of subtle correations that haunt classical pseudo-
random number generators. Of course, the problem of producing algorithmic
random strings is still open. Indeed, let’s assume that we have a classical sili-
con computer that simulates, using a high-quality pseudo-random generator,
the quantum mechanics dynamics and quantum measurement of a 2-state
quantum system. The simulated world will be statistically almost identical
(up to some degree) with the “real” quantum system. However, all simu-
lated bits will be, in the long run, highly compressible. How can we be sure
that the “real” quantum system is not just a superpowerful pseudo-random
generator?

216
Quantum Computing
4.15
The EPR Conundrum and Bell’s
Theorem
According to the philosophical view called realism, reality exists and has
deﬁnite properties irrespective of whether they are observed by some agent.
Motivated by this view point, Einstein, Podolsky and Rosen [91] suggested
a classical argument to “show” that quantum mechanics is incomplete. EPR
assumed (a) the non-existence of action-at-a-distance, (b) that some of the
statistical predictions of quantum mechanics are correct, and (c) a reasonable
criterion deﬁning the existence of “an element of physical reality”.39 They
considered a system of two spatially separated but quantum mechanically
correlated particles. A “mysterious” feature appears: By counterfactual rea-
soning, quantum mechanical experiments yield outcomes which cannot be
predicted by quantum theory; hence the quantum mechanical description of
the system is incomplete!
One possibility to complete the quantum mechanical description is to
postulate additional “hidden-variables” in the hope that completeness, de-
terminism and causality will be thus restored. But then, another conundrum
occurs: Using basically the same postulates as those of EPR, Bell [20, 21]
showed that no deterministic local hidden-variables theory can reproduce all
statistical predictions of quantum mechanics.
We will present ﬁrst Mermin’s [187, 188] two simple devices that explain
EPR conundrum (see [44] for an automata-theoretic analysis). Later, we will
concentrate on Bell’s result.
4.15.1
Mermin’s EPR Device
Mermin’s EPR device [187] has three “completely unconnected”40 parts, two
detectors (D1) and (D2) and a source (S) emitting particles. The source is
placed between the detectors: whenever a button is pushed on (S), shortly
thereafter two particles emerge, moving oﬀtoward detectors (D1) and (D2).
Each detector has a switch that can be set in one of three possible positions
– labelled 1,2,3 – and a bulb that can ﬂash a red (R) or a green (G) light.
The purpose of lights is to “communicate” information to the observer. Each
detector ﬂashes either red or green whenever a particle reaches it. Because
of the lack of any relevant connections between any parts of the device, the
link between the emission of particles by (S), i.e. as a result of pressing a
button, and the subsequent ﬂashing of detectors, can only be provided by
the passage of particles from (S) to (D1) and (D2). Additional tools can be
used to check and conﬁrm the lack of any communication, cf. [187], p. 941.
39If, without in any way disturbing a system, we can predict with certainty (i.e. with
probability equal to unity) the value of a physical quantity, then there exists an element of
physical reality corresponding to this physica quantity. See [91], p. 777.
40There are no relevant connections, neither mechanical nor electromagnetic.

The EPR Conundrum and Bell’s Theorem
217
The device is repeatedly operated as follows:
1. the switch of either detector (D1) and (D2) is set randomly to 1 or 2 or
3, i.e. the settings or states 11, 12, 13, 21, 22, 23, 31, 32, 33 are equally
likely,
2. pushing a button on (S) determines the emission toward both (D1) and
(D2),
3. sometime later, (D1) and (D2) ﬂash one of their lights, G or R,
4. every run is recorded in the form ijXY , meaning that (D1) was set to
state i and ﬂashed X and (D2) was set to j and ﬂashed Y.
For example, the record 31GR means “(D1) was set to 3 and ﬂashed G
and (D2) was set to 1 and ﬂashed R”.
Long recorded runs show the following pattern:
(a) For records starting with ii, i.e. 11, 22, 33, both (D1) and (D2) ﬂash
the same colours, RR, GG, with equal frequency; RG and GR are never
ﬂashed.
(b) For records starting with ij, i ̸= j, i.e. 12, 13, 21, 23, 31, 32, both (D1)
and (D2) ﬂash the same colour only 1/4 of the time (RR and GG come
with equal frequencies); the other 3/4 of the time, they ﬂash diﬀerent
colours (RG, GR), occurring again with equal frequencies.
Of course, the above patterns are statistical, that is they are sub-
ject to usual ﬂuctuations expected in every statistical prediction: patterns
are more and more “visible” as the number of runs becomes larger and larger.
The conundrum posed by the existence of Mermin’s device reveals as
soon as we notice that the seemingly simplest physical explanation of the
pattern (a) is incompatible with pattern (b). Indeed, as (D1) and (D2) are
unconnected there is no way for one detector to “know”, at any time, the state
of the other detector or which colour the other is ﬂashing. Consequently, it
seems plausible to assume that the colour ﬂashed by detectors is determined
only by some property, or group of properties of particles, say speed, size,
shape, etc. What properties determine the colour does not really matter;
only the fact that each particle carries a “program” which determines which
colour a detector will ﬂash in some state is important. So, we are led to the
following two hypotheses:
H1 Particles are classiﬁed into eight categories:
GGG, GGR, GRG, GRR, RGG, RGR, RRG, RRR.41
41A particle of type XY Z will cause a detector in state 1 to ﬂash X; a detector in state
2 will ﬂash Y and a detector in state 3 will ﬂash Z.

218
Quantum Computing
H2 Two particles produced in a given run carry identical programs.
According to H1–H2, if particles produced in a run are of type RGR,
then both detectors will ﬂash R in states 1 and 3; they will ﬂash G if both
are in state 2.
Detectors ﬂash the same colours when being in the same
states because particles carry the same programs.
It is clear that from H1–H2 it follows that programs carried by particles
do not depend in any way on the speciﬁc states of detectors:
they are
properties of particles not of detectors. Consequently, both particles carry
the same program whether or not detectors (D1) and (D2) are in the same
states.42
We are ready to argue that
[L] For each type of particle, in runs of type (b) both detectors ﬂash the
same colour at least one third of the time.
If both particles are of types GGG or RRR, then detectors will ﬂash
the same colour all the time.
For particles carrying programs containing
one colour appearing once and the other colour appearing twice, only in two
cases out of six possible combinations both detectors will ﬂash the same light.
For example, for particles of type RGR, both detectors will ﬂash R if (D1)
is in state 1 and (D2) is in state 3 and vice versa. In all remaining cases
detectors will ﬂash diﬀerent lights. The argument remains the same for all
combinations as the conclusion was solely based on the fact that one colour
appears once and the other twice. So, the lights are the same one third of
the time.
The conundrum reveals as a signiﬁcant diﬀerence appears between the
data dictated by particle programs (colours agree at least one third of the
time) and the quantum mechanical prediction (colours agree only one quarter
of the time):
under H1–H2, the observed pattern (b) is incompatible with [L].
4.15.2
Mermin’s GHZ Device
Based on Greenberg, Horne and Zeilinger’s [116] version of EPR experiment,
Mermin [188] imagined a new device, let’s call it GHZ, to show quantum
nonlocality. The device has a source and three widely separated detectors
(A), (B), (C), each of which has only two switch settings, 1 and 2.
Any
detector, when triggered, ﬂashes red (R) or green (G). Again, detectors are
42The emitting source (S) has no knowledge about the states of (D1) and (D2) and there
is no communication among any parts of the device.

The EPR Conundrum and Bell’s Theorem
219
supposed to be far away from the source and there are no connections between
the source and detectors (except those induced by a group of particles ﬂying
from the source to each detector).
The experiment runs as follows. Each detector is in a randomly chosen
state (1 or 2) and then by pressing a button at the source a trio of parti-
cles are released towards detectors; each particle will reach a detector and,
consequently, each detector will ﬂash a light, green or red. There are eight
possible states, but for the argument we need to take into consideration only
those for which the number of 1’s is odd, i.e. 111, 122, 212, 221.
According to [116], (a) if one detector is set to 1 (and the others to 2), then
an odd number of red lights always ﬂash, i.e. RRR, RGG, GRG, GGR, and
they are equally likely, (b) if all detectors are set to 1, then an odd number
of red lights is never ﬂashed: GRR, RGR, RRG, GGG.
It is immediate that in case (a) knowing the colour ﬂashed by two detec-
tors, say (A) and (B), determines uniquely the colour ﬂashed by the third
detector, (C). The explanation can come only because particles are emitted
by the same source (there are no connections between detectors). A similar
conclusion as in the case of EPR device reveals: particles carry programs in-
structing their detectors what colour to ﬂash. Any particle carries a program
of the form XY telling its detector to ﬂash colour X if in state 1 and colour
Y if in state 2. There are four types of programs: GG, GR, RG, RR. A run
in which programs carried by the trio of particles are of types (RG, GR, GG)
will result in RRG if the states were 122, in GGG if the states were 212,
and in GRG if the states were 221. This is an illegal set of programs as the
number of R’s is not odd (in RRG, for example). A legal set of programs is
(RG, GR, GR) as it produces RRR, GGR, GRG on 122, 212, 221. There are
eight legal programs,
(RR, RR, RR), (RR, GG, GG), (GG, RR, GG), (GG, GG, RR),
(RG, GR, GR), (RG, RG, RG), (GR, GR, RG), (GR, RG, GR)
out of 64 possible programs.
The conundrum reveals again as none of the above programs respects (b),
i.e. it is compatible with the case 111. A single 111 run suﬃces to prove
inconsistency! Particle programs require an odd number of R’s to be ﬂashed
on 111, but quantum mechanics prohibits this in every 111 run.
4.15.3
Bell’s Theorem
Bell [20, 21] showed, using basically the same postulates as those of EPR,
that no deterministic local hidden-variables theory can reproduce all statis-
tical predictions of quantum mechanics. Initially, Bell’s argument has been
applied to an EPR-type Gedanken experiment of Bohm; later, Bell’s analy-
sis was extended to actual systems, and experimental tests were suggested
and performed (see, for example, Clauser and Shimony [61]). Essentially, the

220
Quantum Computing
particles on either side appear to be “more correlated” than can be expected
by a classical analysis assuming locality (i.e. the impossibility of any kind of
information or correlation transfer faster than light).
In what follows we will use Odifreddi [199] in presenting an elementary
analysis of Bell’s result. The setting is the following. We consider two physi-
cal systems; on one two types of measurements are made (A, B), and on the
other one two other types (C, D). The results are binary, so they will be
denoted by “+” and “−”. We will repeat these measurements to ensure sta-
tistically relevant results. Correlations appear when measurements give the
same outcome, that is, “++” and “−−”. The basic result is that in almost
all cases, more “++” and “−−” (and less “+−” and “−−”) coincidences are
recorded than one can explain by any local classical analysis.
Let p(x|i) be the probability that, by taking the measure i ∈{A, B} on the
ﬁrst system, the outcome will be x ∈{+, −}; p(x|ij) is the probability that by
taking the measure i ∈{A, B} on the ﬁrst system and the measure j ∈{C, D}
on the second, the outcome of the ﬁrst system alone will be x; p(xy|ij) is
the probability that by taking the measure i on the ﬁrst system and measure
j on the second system, the outcomes will be respectively, x ∈{+, −} and
y ∈{+, −}; ﬁnally, p(x|ijy) is the probability that when taking the measures
i ∈{A, B} on the ﬁrst system and j ∈{C, D} on the second one, and having
outcome y on the second, the outcome of the ﬁrst will be x.
The main result can be stated as follows:
If the outcomes of the experiments on both systems are indepen-
dent, that is
p(xy|ij) = p(x|i) · p(y|j),
then the lack of correlation in one of the two types of measures
cannot exceed the lack of correlation in the remaining types, that
is, the following quadrangular inequality holds true:
p(+ −|AC) + p(−+ |AC)
≤
p(+ −|AD) + p(−+ |AC)
+
p(+ −|BD) + p(−+ |AC)
+
p(+ −|BC) + p(−+ |BC).
(4.12)
It is remarkable that this inequality43 can be obtained with just an ele-
mentary manipulation of binary variables. To see this, let’s denote p(+|A)
by a, p(−|A) by 1 −a (due to the bivalence nature of measurements we have
p(+|A) + p(−|A) = 1), and so on. Using the independence hypothesis, that
is,
p(+ −|AC) = p(+|A) · p(−|C) = a(1 −c),
and the like, the inequality (4.12) can be re-written as
43And, of course, all inequalities obtained by systematic permutations.

The EPR Conundrum and Bell’s Theorem
221
a (1 −c) + (1 −a) c
≤
a (1 −d) + (1 −a) d + b (1 −d)
+(1 −b) d + b (1 −c) + (1 −b) c,
or, equivalently,
ab + bd + bc ≤ac + b + d,
where a, b, c, d ∈[0, 1]. To ﬁnish we consider the following three cases:
• if b ≤a, then c(b −a) ≤0, so ad + bd + c(b −a) ≤ad + bd, and (4.12)
follows as ad ≤d and bd ≤b;
• if d ≤c, then a(d −c) + bd + bc ≤b + d, so (4.12) follows;
• if a ≤b and c ≤d, then either b ≤d and in this case d(a+b)+c(b−a) ≤
b + d, or d ≤b and in this case a(d −c) + b(d + c) ≤b + d, and in each
case we deduce (4.12).
The probabilistic hypothesis of independence can actually be decomposed
in the conjunction of two hypotheses with more physical signiﬁcance (see
Jarett [144]):
Separability: The statistical outcomes performed on one system are indepen-
dent of the outcomes performed on the other system:
p(x|ijy) = p(x|ij) and p(y|ijx) = p(y|ij).
Locality: The statistical outcomes performed an experiment on one system
are independent of the types of experiments performed on the other system:
p(x|ij) = p(x|i) and p(y|ij) = p(y|j).
Separability says that the spatio-temporal separation between the two
systems makes them reducible to individual parts, the “whole” is no more
than the “sum of parts”; locality forbids any instantaneous interaction.
Separability and locality implies independence as
p(xy|ij) = p(xy|ijy) · p(y|ij) = p(x|ij) · p(y|ij) = p(x|i) · p(y|j).
Consequently, if the outcomes of the experiments on both systems are separa-
ble and local, then the lack of correlation in one of the two types of measures
cannot exceed the lack of correlation in the remaining types.
Probabilities can be interpreted as truth-values of elementary proposi-
tions, so the above analysis can be reformulated in the language of “classical
logic”. Indeed, let’s write A for p(+|A) and ¬A for p(−|A), and similarly
for B, C. Further on, let’s notice that the elementary operations with proba-
bilities can be reformulated as logical operations, namely, conjunction ∧will
correspond to product, disjunction ∨to sum, and implication →to ≤.
A “logical” version of the quadrangular inequality can be deduced:

222
Quantum Computing
If the conjunction is distributive with respect to disjunction for all
propositions A, ¬A, B, ¬B, C, ¬C, that is,
α ∧(β ∨γ) →(α ∧β) ∨(α ∧γ),
then the following quadrangular implication holds true:
(A ∧¬C) ∨(¬A ∧C)
→
(A ∧¬D) ∨(¬A ∧D)
∨
(D ∧¬B) ∨(¬D ∧B)
∨
(B ∧¬C) ∨(¬B ∧C).
First, use the following weak form of distributivity
α ∧(¬β ∨β) →(α ∧¬β) ∨(α ∧β),
for α = X ∧¬Y , and β = Z:
(X ∧¬Y ) ∧(¬Z ∨Z) →(X ∧¬Y ∧¬Z) ∨(X ∧¬Y ∧Z),
so by the law of excluded middle we get:
(X ∧¬Y ) →(X ∧¬Y ∧¬Z) ∨(X ∧¬Y ∧Z).
Weakening the conclusion we get:
(X ∧¬Y ) →(X ∧¬Z) ∨(Z ∧¬Y ).
(4.13)
Using (4.13) for the triples (X, Y, Z) = (A, C, D), (D, C, B) we get
(A ∧¬C) →(A ∧¬D) ∨(D ∧¬C),
and
(D ∧¬C) →(D ∧¬B) ∨(B ∧¬C),
which imply
(A ∧¬C) →(A ∧¬D) ∨(D ∧¬B) ∨(B ∧¬C).
Similarly, we obtain the implication
(¬A ∧C) →(¬A ∧D) ∨(¬D ∧B) ∨(¬B ∧C),
which concludes the argument.
Both quadrangular inequality and implication have been experimentally
falsiﬁed, hence no theory satisfying their hypotheses can be physically correct.
So, locality and separability cannot be simultaneously adopted.44 The failure
of independence aﬀects Reichenbach’s [248] causality principle: two correlated
(non independent) events have a common cause, that there exists an event
in their “past” with respect to which they are independent. So, we arrive
at the idea of synchronicity that has important implication for Quantum
Computation:
44Quantum mechanics has chosen to drop separability.

Quantum Logic
223
there exist events which are correlated in a way which is neither
casual nor causal.
Finally, the failure of distributivity – the “mark” of quantum logic, has
been proved to be more pervasive than the universe of quantum mechanics
statements: it is excluded from any logic aiming to describe the physical
world. Is any hope to rescue classical logic, which seems to be so brutally
excluded . . .
4.16
Quantum Logic
Quantum logic pioneered by Birkhoﬀand von Neumann [33] (see also, Mackey
[175], Jauch [143], Kalmbach [146], Cohen [62], Pulmannov´a [245], Svozil
[283]) deals with propositions expressing properties of quantum systems. To
every physical property P we can associate in a natural way the proposition
“the physical system has property P”,
which means
“if the observable is measured, then the property P is observed”.
Such a proposition is always true or false.
Technically, quantum logic identiﬁes logical entities with Hilbert space
entities. In particular, elementary propositions p, q, . . . are associated with
closed linear subspaces of a Hilbert space through the origin (zero vector); the
implication relation ≤is associated with the set-theoretical subset relation ⊂,
and the logical or ∨, and ∧, and not ′ operations are associated with the set-
theoretic intersection ∩, the linear span ⊕of subspaces and the orthogonal
subspace ⊥, respectively. The negation of p ≤q is denoted by p ̸≤q. The
logical statement 1 which is always true is identiﬁed with the entire Hilbert
space H, and its complement ∅with the zero-dimensional subspace (zero
vector).
Two propositions p and q are orthogonal if p ≤q′.
Two propositions
p, q are co-measurable (compatible, commuting) if there exist three mutually
orthogonal propositions a, b, c such that
p = a ∨b and q = a ∨c.
Intuitively, propositions p and q consist of an “identical part” a and two
orthogonal parts b, c. Clearly, orthogonality implies co-measurability, since if
p and q are orthogonal, we may take a, b, c to be 0, p, q, respectively.
A simple example is the propositional structure encountered in the quan-
tum mechanics of spin state measurements of a spin one-half particle. As-
sume that the associated Hilbert space is two-dimensional and real-valued,

224
Quantum Computing
and consider measurements of the spin-component along one particular direc-
tion, say the x-axis.45 There are two possible spin components of the particle,
−1
2, + 1
2, which can be codiﬁed as −and +. The corresponding elementary
propositions are:
• p−= “the particle is in state −” = one-dimensional subspace spanned
by the vector (1, 0) = (1, 0), and
• p+ = “the particle is in state +” = one-dimensional subspace spanned
by the vector (0, 1) = (0, 1).
The tautology 1 is the proposition “the particle is in state −or in state
+”, which is the whole space, and the absurdity 0 is the proposition “the
particle is neither in state −nor in state +”, which is the zero-dimensional
subspace of (0, 0) which is (0, 0). The propositional structure obtained in this
case is the classical Boolean algebra with two elements; its Hasse diagram
appears in Figure 4.4.46
t
t
t
t
❅
❅
❅
❅
❅
❅
❅






❅
❅
❅
❅
❅
❅
❅






0 = 1′
1 = 0′ = p−∨p+
p−
p+
Figure 4.4: Hasse diagram of the spin one-half state co-measurable proposi-
tions.
So far we have discussed only about co-measurable observables. The cor-
responding propositions form a Boolean algebra with 2n elements, so the
so-called blocks47 in case of a Hilbert space of dimension n. However, non-
co-measurable observables should be treated as well! Of course, we may take
the view that non-co-measurable observables make no physical sense, so we
45This can be obtained with a Stern-Gerlach type of experiment.
46Recall that in a Hasse diagram propositions are represented by dots, implication is
represented “vertically”, that is q is drawn higher than p if p ≤q, and the propositions p
and q are connected by a line.
47Blocks are maximal in the sense that no additional observable, which is co-measurable
with all observables in the block, can be added.

Quantum Logic
225
should forget about them (at least, with respect to current day knowledge;
maybe, a “more complete” theory could make sense of them!). This legiti-
mate, but somewhat “minority” position goes beyond our aim, so we will con-
centrate on the mainstream approach which considers that non-co-measurable
observables make physical sense at least as theoretical constructions.48
A simple formalism to deal with non-co-measurable observables is via
the so-called pasting construction. Consider a collection of blocks and note
that some of them may have a common non-trivial observable. A “logical”
structure can be extracted as follows:
• identify all tautologies in all blocks,
• identify all absurdities in all blocks,
• identify identical elements in diﬀerent blocks,
• keep intact the logical structure of all blocks.
As a simple example let’s paste together observables of the spin one-half
systems, see Figure 4.4. Then we have two propositional systems, the ﬁrst
corresponding to the outcomes of a measurement of the spin states along the
x–axis
L(x) = {0, p−, p+, 1},
and another one, corresponding to the outcomes of a measurement of the
spin states along a diﬀerent spatial direction, say x ̸= x (mod π), an identical
propositional system,
L(x) = {0, p−, p+, 1}.
So, we identify tautologies 1 = 1 and absurdities 0 = 0 and keep all other
propositions intact. The result is the M02 propositional structure49 presented
in Figure 4.5. It is easy to see that M02 is not any longer a Boolean algebra,
since distributivity is not satisﬁed, as the following example shows:
p−= p−∨0 = p−∨(p−∧p ′
−) ̸= (p−∨p) ∧(p−∨p ′
−) = 1 ∧1 = 1.
Algebraically, M02 is an orthocomplemented lattice, that is, any two ele-
ments have a least upper bound and a greatest lower bound,
if a ≤b, a ≤c, then a ≤(b ∧c),
if b ≤a, c ≤a, then (b ∨c) ≤a,
and for all a ≤c, the modular law is satisﬁed:
(a ∨b) ∧c = a ∨(b ∧c).
48This attitude is common also in pure mathematics, where i = √−1 makes no “direct”
sense, but proves to be extremely useful.
49“0” comes from orthocomplemented, “M” comes from modular.

226
Quantum Computing
t
t
t
t
❅
❅
❅
❅
❅
❅
❅






❅
❅
❅
❅
❅
❅
❅






0 = 1′ = p−∨p+
1 = 0′
p+
p−
❜
❜
❜
❜
❜
❜
❜
❜
❜
❜
❜
❜
✧✧✧✧✧✧✧✧✧✧✧✧❜❜❜❜❜❜❜❜❜❜❜❜
✧
✧
✧
✧
✧
✧
✧
✧
✧
✧
✧
✧
t
p−
t
p+
L(x)
L(x)
Figure 4.5: Hasse diagram of M02.
4.17
Have
Quantum
Propositions
Classical
Meaning?
Since measurement destroys quantum information, how do you actually get
the results of your calculations? This question leads to the possible “classical
meaning” of quantum propositions.
Einstein, Podolsky and Rosen [91] speculated that “elements of physical
reality” exist irrespective of whether they are actually observed. Moreover,
they conjectured that the quantum formalism can be “completed” or “embed-
ded” into a larger theoretical framework which would reproduce the quantum
theoretical results but would otherwise be classical and deterministic from an
algebraic and logical point of view.
A proper formalization of the term “element of physical reality” can be
given in terms of two-valued states or valuations, which can take on only one
of the two values 0 and 1, and which are interpretable as the classical logical
truth assignments false and true, respectively. Kochen and Specker’s results
[152] (cf. also Specker [275], Svozil [283]) state that
for quantum systems representable by Hilbert spaces of dimension
higher than two, there does not exist any valuation s : L →{0, 1}
deﬁned on the set of closed linear subspaces of the space L50 pre-
serving the lattice operations and the orthocomplement, even if
one restricts the attention to lattice operations carried out among
commuting (orthogonal) elements.
As a consequence,
there exist diﬀerent quantum propositions which cannot be distin-
guished by any classical truth assignment.
50These subspaces are interpretable as quantum mechanical propositions.

Have Quantum Propositions Classical Meaning?
227
The Kochen and Specker’s result, as it is commonly argued, e.g. by Peres
[236] and Mermin [189], is directed against the non-contextual hidden pa-
rameter program envisaged by [91]. Indeed, if one takes into account the
entire Hilbert logic (of dimension larger than two) and if one considers all
states thereon, any truth value assignment to quantum propositions prior to
the actual measurement yields a contradiction.51 But, the Kochen-Specker
argument continues, it is always possible to prove the existence of separable
valuations or truth assignments for classical propositional systems identiﬁable
with Boolean algebras. Hence, there does not exist any injective morphism
from a quantum logic into some Boolean algebra.
Are there natural (weaker) conditions under which embeddings do exist?
We will show that, if one is willing to abandon the preservation of some
commonly used logical functions, then it is possible to give a classical meaning
to quantum physical statements, thus suggestiong a possible “understanding”
of quantum mechanics.
4.17.1
A gallery of embeddings
An embedding of a quantum logical structure L of propositions into a classical
universe represented by a Boolean algebra B should preserve as much logico-
algebraic structure as possible. Such an embedding can be formalized as a
mapping ϕ : L →B with the following properties.52 Let p, q ∈L.
(i) Injectivity: two diﬀerent quantum logical propositions are mapped into
two diﬀerent propositions of the Boolean algebra: if p ̸= q, then ϕ(p) ̸=
ϕ(q).
(ii) Preservation of the order relation: if p ≤q, then ϕ(p) ≤ϕ(q).
(iii) Preservation of ortholattice operations, i.e. preservation of the
(ortho-) complement: ϕ(p′) = ϕ(p)′,
or operation: ϕ(p ∨q) = ϕ(p) ∨ϕ(q),
and operation: ϕ(p ∧q) = ϕ(p) ∧ϕ(q).
We cannot have an embedding from the quantum universe to the clas-
sical universe satisfying all three requirements (i)–(iii).
In particular, the
nonpreservation of ortholattice operations among non-co-measurable propo-
sitions is quite evident, because of the nondistributive structure of quantum
logics.
51This can be proven by ﬁnitistic means, that is, with a ﬁnite number of one-dimensional
closed linear subspaces (see Havlicek and Svozil [126]).
52These properties are not independent.

228
Quantum Computing
4.17.2
Injective embeddings
We start with the simple fact that there does not exist an injective lattice
morphism from any nondistributive lattice into a Boolean algebra. A good
example is the lattice M02 drawn in Figure 4.5. Recall that M02 is a non-
distributive lattice, in fact, the smallest orthocomplemented nondistributive
lattice.
The requirement (iii) that the embedding ϕ preserves all ortholattice op-
erations (even for non-co-measurable and non-orthogonal propositions) would
mean that
ϕ(p−) ∧(ϕ(q−) ∨ϕ(q+)) ̸= (ϕ(p−) ∧ϕ(q−)) ∨(ϕ(p−) ∧ϕ(q+)), 53
which is impossible because the range of ϕ is a subset of a Boolean algebra
and for any Boolean algebra the distributive law is satisﬁed. Consequently, a
lattice embedding in the form of an injective lattice morphism from Hilbert
lattices into Boolean algebras is not possible even for two-dimensional Hilbert
spaces. Could we still hope for a reasonable kind of embedding of a quantum
universe into a classical one by weakening our requirements, most notably
(iii)?
Let us follow Zierler and Schlessinger [307] and Kochen and Specker [152]
and weaken (iii) by requiring that the ortholattice operations need only to be
preserved among co-measurable propositions.54 Again, the result is negative;
for a proof see Calude, Hertling and Svozil [51].
An even stronger weakening of condition (iii) would be to require
preservation of ortholattice operations merely among the centre C, i.e.
among those propositions which are co-measurable (commuting) with all
other propositions. It is not diﬃcult to prove that in the case of complete
Hilbert lattices (and not mere subalgebras thereof), the center consists of
just the least lower and the greatest upper bound C = {0, 1} and thus is
isomorphic to the two-element Boolean algebra {0, 1}. The requirement is
trivially fulﬁlled and its implications are quite trivial as well.
Is it possible to embed quantum logic into a Boolean algebra when one
does not demand preservation of all ortholattice operations but merely of
complementation? This is indeed possible and here is a sketch of the argu-
ment.
Recall that an orthoposet55 (or orthocomplemented poset) (L, ≤, 0, 1,′ ) is a
set L which is endowed with a partial ordering ≤,56 contains two distinguished
elements 0 and 1 satisfying 0 ≤p ≤1, for all p ∈L, and is endowed with a
53We have written p−for (p+)′ and q−for (q+)′.
54Mathematically, this is equivalent to the requirement of separability by the set of
valuations or two-valued probability measures or truth assignments on L.
55Poset is an abbreviation for partial ordered set.
56That is, a subset ≤of L × L satisfying the following three conditions: (1) p ≤p, (2)
if p ≤q and q ≤r, then p ≤r, (3) if p ≤q and q ≤p, then p = q, for all p, q, r ∈L.

Have Quantum Propositions Classical Meaning?
229
function ′ (orthocomplementation) from L to L satisfying the following three
conditions
(1) p′′ = p,
(2) if p ≤q, then q′ ≤p′,
(3) the least upper bound of p and p′ exists and is 1, for all p, q ∈L.57
For example, an arbitrary sublattice of the lattice of all closed linear
subspaces of a Hilbert space is an orthoposet if it contains the subspace {0}
and the full Hilbert space and is closed under the orthogonal complement
operation.
Namely, the subspace {0} is the 0 in the orthoposet, the full
Hilbert space is the 1, the set-theoretic inclusion is the ordering ≤, and the
orthogonal complement operation is the orthocomplementation ′.
Let L be a ﬁxed arbitrary orthoposet.
We shall construct a Boolean
algebra B and an injective mapping ϕ : L →B which preserves the order
relation and the orthocomplementation. To this aim we will use the notion
of maximal ideal, that is, a subset I of L such that for all p, q ∈L,
1. p ∈I if and only if p′ ̸∈I,
2. if p ≤q and q ∈I, then p ∈I.
Let I be the set of all maximal ideals in L, and let B be the Boolean algebra
which consists of all subsets of I. The order relation in B is the set-theoretic
inclusion, the ortholattice operations complement, or, and are given by the
set-theoretic complement, union, and intersection, and the elements 0 and 1
of the Boolean algebra are just the empty set and the full set I. Consider
the map ϕ : L →B which maps each element p ∈L to the set ϕ(p) = {I ∈
I | p ̸∈I} of all maximal ideals which do not contain p. One can prove that
the map ϕ has the following three properties (cf. Calude, Hertling and Svozil
[51]):
(i) is injective,
(ii) preserves the order relation,
(iii) preserves complementation,
that is,
every orthoposet can be embedded into a Boolean algebra where the
embedding preserves the order relation and the complementation.
57Note that these conditions imply 0′ = 1, 1′ = 0, and that the greatest lower bound of
p and p′ exists and is 0, for all p ∈L.

230
Quantum Computing
A diﬀerent embedding has been suggested by Malhas [177, 178]. Consider
an orthocomplemented lattice (L, ≤, 0, 1,′ ), i.e. a lattice (L, ≤, 0, 1) with 0 ≤
x ≤1 for all x ∈L, with orthocomplementation. Furthermore, assume that
L is atomic, that is, for every x ∈L \ {0}, there is an atom a ∈L such that
a ≤x. An atom is an element a ∈L with the property that if 0 ≤y ≤a,
then y = 0 or y = a, and satisﬁes the following additional property: for all
x, y ∈L,
x ≤y if and only if for every atom a ∈L, a ≤x implies a ≤y.
(4.14)
Every atomic Boolean algebra and the lattice of closed subspaces of a sepa-
rable Hilbert space satisfy the above conditions.
Consider next a set U 58 and let W(U) be the smallest set of strings
over the alphabet U ∪{′, →} which contains U and is closed under negation
(if A ∈W(U), then A′ ∈W(U)) and implication (if A, B ∈W(U), then
A →B ∈W(U)).59 The elements of U are called simple propositions and
the elements of W(U) are called (compound) propositions.
A valuation is a mapping t : W(U) →{0, 1} such that t(A) ̸= t(A′) and
t(A →B) = 0 if and only if t(A) = 1 and t(B) = 0. Clearly, every assignment
s : U →{0, 1} can be extended to a unique valuation ts.
We continue with some natural deﬁnitions. A tautology is a proposition A
which is true under every possible valuation, i.e. t(A) = 1, for every valuation
t. A set K ⊆W(U) is consistent if there is a valuation making true every
proposition in K. Let A ∈W(U) and K ⊆W(U). We say that A derives
from K, and write K |= A, in case t(A) = 1 for each valuation t which makes
true every proposition in K (that is, t(B) = 1, for all B ∈K). Let
Con(K) = {A ∈W(U) | K |= A}.
In fact, the function Con is a ﬁnitary closure operator, i.e. it satisﬁes the
following four properties:
• K ⊆Con(K),
• if K ⊆˜K, then Con(K) ⊆Con( ˜K),
• Con(Con(K)) = Con(K),
• Con(K) = 
{X⊆K,X ﬁnite} Con(X).
Finally, we say that a set K is a theory if K is a ﬁxed-point of the operator
Con: Con(K) = K, that is, Con(K) is the set of all propositions A which can
be derived from K.
58Not containing the logical symbols ∪,′ , →.
59We deﬁne in a natural way A ∪B = A′ →B, A ∩B = (A →B′)′, A ↔B = (A →
B) ∩(B →A).

Have Quantum Propositions Classical Meaning?
231
The main example of a theory can be obtained by taking a set X of val-
uations and constructing the set of all propositions true under all valuations
in X:
Th(X) = {A ∈W(U) | t(A) = 1, for all t ∈X}.
In fact, every theory is of the above form, that is,
for every theory K there exists a set of valuations X (depending
upon K) such that K = Th(X).
In other words, theories are those sets of propositions which are true
under a certain set of valuations (interpretations).
Let now T be a theory. Two elements p, q ∈U are T -equivalent, written
p ≡T q, in case p ↔q ∈T . The relation ≡T is an equivalence relation. The
equivalence class of p is [p]T = {q ∈U | p ≡T q} and the factor set is denoted
by U≡T ; for brevity, we will sometimes write [p] instead of [p]T . The factor
set comes with a natural partial order: [p] ≤[q] if p →q ∈T . Note that in
general, (U≡T , ≤) is not a Boolean algebra.60
In a similar way we can deﬁne the ≡T -equivalence of two propositions:
A ≡T B if A ↔B ∈T . Denote by [[A]]T (shortly, [[A]]) the equivalence
class of A and note that for every p ∈U, [p] = [[p]]∩U. The resulting Boolean
algebra W(U)≡T is the Lindenbaum algebra of T .
Fix now an atomic orthocomplemented lattice (L, ≤, 0, 1,′ ) satisfying
(4.14).
Let U be a set of cardinality greater or equal to the cardinality
of L and ﬁx a surjective mapping f : U →L. For every atom a ∈L, let
sa : U →{0, 1} be the assignment deﬁned by
sa(p) = 1 if and only if a ≤f(p).
Take
X = {tsa | a is an atom of L}61 and T = Th(X).
Malhas [177, 178] has proven that
the lattice (U≡T , ≤) is orthocomplemented, and, in fact, isomor-
phic to L.
In particular,
there exists a theory whose induced orthoposet is isomorphic to
the lattice of all closed subspaces of a separable Hilbert space.
How does this relate to the Kochen-Specker impossibility result?
60For instance, in case T = Con({p}), for some p ∈U. If U has at least three elements,
then (U≡T , ≤) does not have a minimum.
61Recall that ts is the unique valuation extending s.

232
Quantum Computing
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
t
◗
◗
◗
◗
◗
◗
◗
◗
❅
❅
❅
❅
❅





✑✑✑✑✑✑✑✑◗◗◗◗◗◗◗◗
❅
❅
❅
❅
❅





✑
✑
✑
✑
✑
✑
✑
✑
❆
❆
❆
❆
❆
❍
❍
❍
❍
❍
❍
❍
❍
❍
❍
✟✟✟✟✟✟✟✟✟✟❍❍❍❍❍❍❍❍❍❍
✟
✟
✟
✟
✟
✟
✟
✟
✟
✟









❍
❍
❍
❍
❍
❍
❍
❍
❍
❍
✁
✁
✁
✁
✁❆
❆
❆
❆
❆










❍❍❍❍❍❍❍❍❍❍
✁
✁
✁
✁
✁
❍
❍
❍
❍
❍
❍
❍
❍
❍
❍
✟✟✟✟✟✟✟✟✟✟❍❍❍❍❍❍❍❍❍❍
✟
✟
✟
✟
✟
✟
✟
✟
✟
✟
0 ≡[[A]]
1 ≡[[G]] = [[B ∨D ∨E ∨F]]
[[B]]
[[D]]
[[E]]
[[F]]
[[B ∨D]]
[[B ∨E]] [[B ∨F]]
[[D ∨E]]
[[D ∨F]] [[E ∨F]]
[[D ∨E ∨F]]
[[B ∨E ∨F]]
[[B ∨D ∨F]]
[[B ∨D ∨E]]
n
n
n
n
n
n
Figure 4.6: Hasse diagram of an embedding of the quantum logic M02. Con-
centric circles indicate the embedding.
Let us choose
U = {A, B, C, D, E, F, G, H}.
Since U contains more elements than M02, we can map U surjectively onto
M02, e.g. f(A) = 0, f(B) = p−, f(C) = p−, f(D) = p+, f(E) = q−, f(F) =
q+, f(G) = 1, f(H) = 1.
For every atom a ∈MO2, let us introduce the truth assignment sa : U →
{0, 1} as deﬁned above (i.e. sa(r) = 1 if and only if a ≤f(r)) and thus a
valuation on W(U) separating it from the rest of the atoms of M02. That is,
for instance, associate with p−∈MO2 the function sp−as follows:
sp−(A) = sp−(D) = sp−(E) = sp−(F) = 0,
sp−(B) = sp−(C) = sp−(G) = sp−(H) = 1.
The truth assignments associated with all the atoms are listed in Table 4.4.
The theory T we are thus dealing with is determined by the union of all
the truth assignments:
X = {tsp−, tsp+, tsq−, tsq+} and T = Th(X).
By virtue of construction, U splits into six equivalence classes with respect
to the theory T ; i.e.
U≡T = {[A], [B], [D], [E], [F], [G]}.

Have Quantum Propositions Classical Meaning?
233
A
B
C
D
E
F
G
H
sp−
0
1
1
0
0
0
1
1
sp+
0
0
0
1
0
0
1
1
sq−
0
0
0
0
1
0
1
1
sq+
0
0
0
0
0
1
1
1
Table 4.4: Truth assignments on U corresponding to atoms p−, p+, q−, q+ ∈
MO2 .
Since [p] →[q] if and only if (p →q) ∈T , we obtain a partial order on U≡T
induced by T which isomorphically reﬂects the original quantum logic M02.
The Boolean Lindenbaum algebra W(U)≡T = {0, 1}4 is obtained by forming
all the compound propositions of U and imposing a partial order with respect
to T . It is represented in Figure 4.6. The embedding is given by
ϕ(0) = [[A]], ϕ(p−) = [[B]], ϕ(p+) = [[D]],
ϕ(q−) = [[E]], ϕ(q+) = [[F]], ϕ(1) = [[G]].
It is order-preserving but does not preserve operations such as the com-
plement.
Although, in this particular example, f(B) = (f(D))′ implies
(B →D′) ∈T , the converse is not true in general. For example, there is no
s ∈X for which s(B) = s(E) = 1. Thus, (B →E′) ∈T, but f(B) ̸= (f(E))′.
4.17.3
Surjective extensions
The original program of completion of quantum mechanics naturally leads to
injective embeddings because the physical intuition behind an embedding is
that the “actual physics” is a classical one, but because of some yet unknown
reason, some part of this “hidden arena” becomes observable while other part
remains hidden.
Nevertheless, there exists at least one other alternative to complete quan-
tum mechanics, namely by a surjective map φ : B →L of a classical Boolean
algebra onto a quantum logic, such that B “has more elements than L”.
An extension would be just a Boolean algebra B such that every element
of L corresponds to at least two elements of B, one being the negation of
the other. In such a situation the Kochen-Specker argument does not apply,
because every element of L can be mapped by φ onto either one of its two or
more correspondents. That is, φ depends upon the context of measurement.
See more in Svozil [283].
Let us conclude with an observation about terminology. We have used
the term “observables” for quantum propositions. Some “observables” (that
is, the complementary ones) might not be co-measurable, so it seems ap-
propriate to look at these “observables” as “potential observables”. After a

234
Quantum Computing
particular measurement has been chosen, some of these “potential observ-
ables” are actually determined and others (the complementary ones) become
“counterfactuals” by quantum mechanical means.
4.18
Quantum Computers
A classical computer bogs down when it is made to simulate a quantum sys-
tem; Feynman [95] suggested that a quantum computer might do a better
job. In fact earlier, Benioﬀ[22] devised an elaborate quantum-mechanical
simulation of a Turing machine, the ﬁrst half-classical, half-quantum Tur-
ing machine; his construction was as powerful as a Turing machine. In the
same line of research Feynman [95, 96] proposed quantum versions of log-
ical circuits. Albert [5] has described the ﬁrst “truly” quantum computer,
a “quantum automaton”; this machine has properties not shared by any
classical automata. Quantum automata are, of course, not universal com-
puters. In 1985 Deutsch [84] published a model of a quantum computer, the
ﬁrst “true” quantum universal Turing machine, one which is relying directly
on the interference of quantum states. In the same year, 1985, Peres [235]
improved Benniof and Feynman models.
In 1992 Deutsch and Jozsa [88]
dicussed a few problems that could be solved faster with quantum comput-
ers than conventional Turing machines. This was the ﬁrst indication that
quantum computing might be superior to classical computing.
4.18.1
Benioﬀ’s computer
Benioﬀ[22] devised a hybrid Turing machine, in which a classical part co-
exists with a quantum part.
His model is a Turing machine with a tape
consisting of a sequence of qubits (spin states), each one being in one of the
basis state |0⟩or |1⟩. The head of the machine was replaced by a quantum
mechanical interaction that could “change” the values of qubits. The transi-
tion rules came from a speciﬁc Schr¨odinger equation satisfying the following
property: the initial conﬁguration of spins evolves into a ﬁnal set of spins
which, when decoded as bits, will represent the result of the calculation.
The program was encoded in the speciﬁc form of the Schr¨odinger equation.
The computation was done in steps of ﬁxed duration so that at the end of
each step the tape was back in one of the basis states |0⟩(representing 0)
or |1⟩(representing 1), but during a computational step the machine could
temporarily be in a superposition of spin states.
At the end of each step the head measured the state of the tape which
destroyed all superpositions on the tape. Consequently, quantum interference
was only partially used, in fact it was destroyed at the end of each step.
Hence, a classical Turing machine could simulate easily such a computation.
Benioﬀ’s machine faces at least two major problems, (a) the design of
the Hamiltonian that mimics a speciﬁc Turing machine has to incorporate all

Quantum Computers
235
computational paths performed by that speciﬁc machine (which, in a sense,
amounts to knowing the answer of the problem you want to solve before
actually running the computation), (b) the interactions between the head of
the machine and its tape are diﬃcult to realize because these physical objects
can be far apart. The ﬁrst problem was theoretically solved by Benioﬀ, but
the second one could not be solved.
4.18.2
Feynman’s computer
Motivated by the construction of universal reversible Boolean gates, Feynman
[95, 96] proposed a quantum universal simulator based on quantum versions
of Boolean circuits. A quantum circuit is composed of quantum wires and
elementary quantum gates. Initially, Feynman’s approach was seen as re-
strictive (see Deutsch [84]), but in fact these two approaches are equivalent.
However, it took almost a decade to get the proof, see Yao [302]:
every computation which can be performed eﬃciently on a quan-
tum Turing machine can be done by a quantum circuit and con-
versely.
Feynman’s construction uses a serial connection of k quantum gates, each
performing a unitary transformation. There are n input/output qubits and
k + 1 extra qubits as program counter sites, k “creation” operators, ci and
k “annihilation” operators, ci. A creation operator ci sets the ith counter
qubit to 1 and the annihilation operator ci sets the ith counter qubit to 0.
These extra qubits are used to track the progress of a computation. Only
one program counter site is ever occupied, by the “cursor”.
A computation starts by assigning the input bits into the input register
and the cursor to site 0. One checks if the site k is empty or it has the cursor.
When the cursor is found at the kth site, we know that the entire circuit has
been used in the computation. At that time the state of the n qubits has the
answer to the computation.
The quantum computer does not take care itself of termination! The time
when the measurement is to be performed has to be determined from outside.
Peres [235] has improved Feynman’s design in (a) timing the end of cal-
culation, (b) the analysis of possible errors (in the program as well in mea-
surements), and (c) extending the computation with general qubit states
a|0⟩+ b|1⟩.
4.18.3
Deutsch’s computer
Deutsch’s [84] starting point is a challenge to the Church–Turing Thesis. Any
classical computation evolves from a set of input states to a set of output
states; states are “labelled” in some standard way. For a classical Turing
machine, the measured output is a deﬁnite function of the prepared input;

236
Quantum Computing
the measurement can, at least in principle, be done by an outside observer
and will be the same if the measurement is repeated. In this way one can
deﬁne, in a coherent way, the notion of Turing computable function. Two
Turing machines are computationally equivalent when they compute the same
function.
Quantum machines (and probabilistic or stochastic machines) do not com-
pute functions in the above sense! The output state of a stochastic machine
is “random” and only the probability distribution function for possible out-
puts depends on the input state. The quantum state of a quantum machine,
although completely determined by the input state, is not an observable and,
consequently, the user has no access to it. There exists, however, various ways
to generalize the notion of “computational equivalence” for such machines.
One possible way is to look carefully at labels. Like in the classical case,
labels should be provided for all possible combinations of input states. As
measurements cannot in general determine the output state, labels for output
states should be done for the set of pairs consisting of an output observable
and a possible measured value of that observable. Such a pair contains the
speciﬁcation of a possible experiment that could in principle carry on the
output together with a possible result of the experiment. We can now say
that two computing machines are computationally equivalent under given
labellings if any possible experiment (or sequence of experiments) in which
their inputs were prepared equivalently (with respect to the input labellings)
and observables corresponding to each other under the output labellings are
measured, the measured values of these observables are statistically indis-
tinguishable. According to the above criterion, a given computing machine
“computes” a unique function.
The Church–Turing Thesis is manisfestly non-physical. There is, however,
a subtle physical principle deriving from it, which, according to Deutsch [84],
p. 99, reads:
Every ﬁnitely realizable physical system can be perfectly simu-
lated by a universal model computing machine operating by ﬁnite
means.
This statement asks for some deﬁnitions. A ﬁnitely realizable physical
system includes any physical object on which experimentation is possible. A
computing machine M is capable of perfectly simulating a physical system
S, under given labelling of inputs and outputs, if there is a program for M
that makes M computationally equivalent to S under that labelling. Re-
phrased, M becomes under a certain program and a ﬁxed labelling, a system
“computationally indistinguishable” from S.
Deutsch [84], p.100, argued
that the above principle is stronger than the Church–Turing Thesis: it is not
satisﬁed by Turing machines in classical physics, but it is compatible with
quantum theory. This observation motivates the interest, and the urgency, in
seeking a “truly quantum” model of computation.

Quantum Computers
237
Deutsch’s quantum Turing machine has an inﬁnite sequence of qubits (the
tape), t, and a control consisting of a ﬁnite sequence of qubits, m. In addi-
tion, Deutsch uses an observable, x, which has an integer as possible value.
The state of the quantum computer is a unit vector in the space spanned by
basis vectors |x, t, m⟩. The dynamics is given by a constant unitary operator
U. With these ingredients, Deutsch [84] has described a universal quantum
computer, that is one capable of simulating perfectly every ﬁnitely realiz-
able physical system (hence, every quantum computer). As applications he
proposed the random generation procedure described in Section 4.14.
4.18.4
Reversible computation revisited
Recall that a classical computation is irreversible, so dissipative. If a com-
puter operates reversibly, then at least in principle there need be no dissipa-
tion, so no power requirement. So, we may compute, in principle, for free.
In what follows we will describe quantum realizations of sets of reversible
gates which are universal for all Boolean circuits. Recall that a quantum
circuit is composed of quantum wires and elementary quantum gates; each
wire represents a path of a single qubit and is described by a state in the two
dimensional Hilbert space C2.
First we start by observing that
the transformation
T = |0⟩⟨0| ⊗U1 + |1⟩⟨1| ⊗U2,
(4.15)
where U1 and U2 are unitary transformations acting on Cn, is
also unitary.
Indeed, by considering the associated matrices, we have:
|0⟩⟨0| =
	
1
0

(1, 0) =
	
1
0
0
0

,
|1⟩⟨1| =
	
0
1

(0, 1) =
	
0
0
0
1

,
hence
|0⟩⟨0|⊗U1 +|1⟩⟨1|⊗U2 =
	
U1
0U1
0U1
0U1

+
	
0U2
0U2
0U2
U2

=
	
U1
0
0
U2

.
On the other hand,
	
U1
0
0
U2

 	
U1
0
0
U2

†
=
	
U1U †
1
0
0
U2U †
2

=
	
In
0
0
In

= I2n,

238
Quantum Computing
where Ik denotes the identity k × k matrix.
The controlled-U gate
|0⟩⟨0| ⊗I + |1⟩⟨1| ⊗U,
where I is the single-qubit identity and U is another single-qubit gate which
can be used in the particular case of
U =
	
0
1
1
0

= NOT,
to obtain the controlled-NOT gate Cnot.
Let’s stop a moment and try to use Cnot to “copy” a qubit in an unknown
state. Assume that the qubit was in the state |ψ⟩= a|0⟩+ b|1⟩. We use as
input |ψ⟩and |0⟩, that is, a(|0⟩+ b|1⟩) ⊕|0⟩= a|00⟩+ b|10⟩, and we apply
Cnot to it. The result is a|00⟩+ b|11⟩. The two qubits are apparently in
the same state, so we have contradicted the no cloning theorem discussed is
Section 4.10!
The explanation is simple. When we measure one of the qubits we get 0
or 1 with probabilities | a |2 and | b |2. But, once we measure one qubit, the
state of the other qubit is completely determined, so no extra information
can be obtained about a, b, so the hidden information carried by |ψ⟩is lost
because it was not copied. Although the two qubits appear to be identical,
they are not independent copies from |ψ⟩: they are two entangled qubits
carrying together only one qubit of information.
The controlled-controlled-NOT (Toﬀoli) gate can be obtained from (4.15)
by taking U1 = I ⊗I = I4 and U2 = Cnot :
T = |0⟩⟨0| ⊗I ⊗I + |1⟩⟨1| ⊗Cnot.
The Fredkin gate FREDKIN can be deﬁned by
FREDKIN = |0⟩⟨0| ⊗I ⊗I + |1⟩⟨1| ⊗S,
where S is the swap operation
S = |00⟩⟨00| + |01⟩⟨10| + |10⟩⟨01| + |11⟩⟨11|.
To prove that S is a unitary transformation we note that
|00⟩=
	
1
0

⊗
	
1
0

=




1
0
0
0



, |01⟩=
	
1
0

⊗
	
0
1

=




0
1
0
0



,

Quantum Computers
239
|10⟩=
	
0
1

⊗
	
1
0

=




0
0
1
0



, |11⟩=
	
0
1

⊗
	
0
1

=




0
0
0
1



,
and
|00⟩⟨00| =




1
0
0
0



(1, 0, 0, 0) =




1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0



,
|01⟩⟨10| =




0
1
0
0



(0, 0, 1, 0) =




0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0



,
|10⟩⟨01| =




0
0
1
0



(0, 1, 0, 0) =




0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0



,
|11⟩⟨11| =




1
0
0
0



(0, 0, 0, 1) =




0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1



.
It follows that
S =




1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1



and SS† = S2 = I4,
hence S is unitary. According to (4.15), the transformation FREDKIN is
also unitary. The matrix associated to FREDKIN is
	
1
0
0
0

⊗




1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1



+
	
0
0
0
1

⊗




1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1





240
Quantum Computing
=












1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0












+












0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1












=












1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1












.
Any quantum gate can also be represented as a truth table: for each
input basis vector we give the output of the gate. In this way, truth tables for
Toﬀoli and Fredkin gates from Table 4.1 in Section 4.3 re-appear in Table 4.5.
Universality is an important consequence of the above analysis:
The transformation T given by (4.15) is universal for all Boolean
circuits.
We already know that the gates AND and NOT form a universal set,
hence it suﬃces to represent them in terms of the transformation T given by
(4.15).62 We have T|110⟩= |111⟩and T|111⟩= |110⟩, hence the third bit is
changed, that is
T|1, 1, x⟩= |1, 1, ¬x⟩.
On the other hand, for x, y ∈{0, 1}, T|xy0⟩= |xy1⟩if and only if x = y = 1,
and T|xy0⟩= |xy0⟩, otherwise. Consequently, the last bit of T|xy0⟩is x ∧y,
T|x, y, 0⟩= |x, y, x ∧y⟩.
While the T or F quantum gates are universal for Boolean circuits, they
cannot achieve any quantum state transformation. Universality for quantum
transformation is deﬁned diﬀerently as we are dealing with continuous, not
discrete, transformations, and the maximum one can hope for is an arbitrarily
good approximation. A matrix M is ϵ-close to a unitary matrix U if ||U −
M|| ≤ϵ.63 A set of quantum gates S is universal for quantum transformations
62Recall that the basis states |0⟩and |1⟩encode the classical bit values 0 and 1, respec-
tively.
63Recall that ||ψ|| =

⟨ψ|ψ⟩is the norm.

Quantum Algorithms
241
Toﬀoli quantum gate
Fredkin quantum gate
Input
Output
Input
Output
|000⟩
|000⟩
|000⟩
|000⟩
|010⟩
|010⟩
|010⟩
|010⟩
|100⟩
|100⟩
|100⟩
|100⟩
|110⟩
|111⟩
|110⟩
|101⟩
|001⟩
|001⟩
|001⟩
|001⟩
|011⟩
|011⟩
|011⟩
|011⟩
|101⟩
|101⟩
|101⟩
|110⟩
|111⟩
|110⟩
|111⟩
|111⟩
Table 4.5: Toﬀoli/Fredkin quantum gates.
if every unitary transformation U can be performed with arbitrary precision
ϵ > 0 by a quantum circuit CU,ϵ consisting of gates from S. Barenco and
co-authors (see [13, 14]) have proved that
1. there is no one-qubit universal gate,
2. no classical gate can be universal for quantum computing,
3. Cnot together with all single-bit quantum gates form a universal set of
gates for quantum computing.
Deutsch, Barenco and Ekert [87] and Lloyd [171] have proven that almost
any non-trivial two-qubit gate is universal for quantum computing.
4.19
Quantum Algorithms
Is quantum computing oﬀering theoretically any substantial beneﬁt over clas-
sical computing?
4.19.1
Deutsch’s problem
The simplest way to illustrate the power of quantum computing is to solve the
so-called Deutsch’s problem. Consider a Boolean function f : {0, 1} →{0, 1}
and suppose that we have a black box to compute it. We would like to know
whether f is constant (that is, f(0) = f(1)) or balanced (f(0) ̸= f(1)). To
make this test classically, we need two computations of f, f(0) and f(1) and
one comparison. Is it possible to do it better? The answer is aﬃrmative, and
here is a possible solution.
Suppose that we have a quantum black box to compute f. Consider the
transformation Uf which applies to two qubits, |x⟩and |y⟩and produces
|x⟩|y ⊕f(x)⟩.64 This transformation ﬂips the second qubit if f acting on the
64By ⊕we denote, as usual, the sum modulo 2.

242
Quantum Computing
ﬁrst qubit is 1, and does nothing if f acting on the ﬁrst qubit is 0.
The black box is “quantum”, so we can choose the input state to be a
superposition of |0⟩and |1⟩. Assume ﬁrst that the second qubit is initially
prepared in the state
1
√
2(|0⟩−|1⟩). Then,
Uf
	
|x⟩1
√
2(|0⟩−|1⟩)

=
|x⟩1
√
2(|0 ⊕f(x)⟩−|1 ⊕f(x)⟩)
=
(−1)f(x)|x⟩1
√
2(|0⟩−|1⟩).
Next take the ﬁrst qubit to be
1
√
2(|0⟩+ |1⟩). The black box will produce
Uf
	 1
√
2(|0⟩+ |1⟩) 1
√
2(|0⟩−|1⟩)

=
1
√
2((−1)f(0)|0⟩+ (−1)f(1)|1⟩) 1
√
2(|0⟩−|1⟩)
= 1
2(−1)f(0)(|0⟩+ (−1)f(0)⊕f(1)|1⟩)(|0⟩−|1⟩).
Next we will perform a measurement that projects the ﬁrst qubit onto the
basis
1
√
2(|0⟩+ |1⟩),
1
√
2(|0⟩−|1⟩): we will obtain
1
√
2(|0⟩+ |1⟩) if the function
f is balanced and
1
√
2(|0⟩−|1⟩) in the opposite case. So, Deutsch’s problem
was solved with only one computation of f. The explanation consists in the
ability of a quantum computer to be in a blend of states: we can compute f(0)
and f(1), but also, and more importantly, we can extract some information
about f which tells us whether f(0) is equal or not to f(1).
Can any function f : {0, 1} →{0, 1} be implemented by a quantum
gate array Uf? The answer is aﬃrmative. Identifying the values 0 and 1
with the kets |0⟩respectively |1⟩, Uf may be deﬁned as the linear operator
Uf : C4 →C4, which satisﬁes, for any x, y ∈{0, 1}, the equality
Uf|x, y⟩= |x, y ⊕f(x)⟩.
(4.16)
To compute f(x) we apply Uf to |x0⟩. Graphically, the transformation Uf is
presented in Figure 4.7. We shall argue that
for any function f : {0, 1} →{0, 1}, Uf is a unitary transforma-
tion.
We have
UfUf|x, y⟩= Uf|x, y ⊕f(x)⟩= |x, (y ⊕f(x)) ⊕f(x)⟩= |x, y⟩,
hence, in view of the equality UfUf = I, it suﬃces to prove that U †
f = Uf.

Quantum Algorithms
243
|x⟩
|x⟩
Uf
|y⟩
|y ⊕f(x)⟩
Figure 4.7: Quantum gate array Uf.
The function f can be deﬁned in four ways: 1.
f(0) = f(1) = 0; 2.
f(0) = 0, f(1) = 1; 3. f(0) = 1, f(1) = 0; and 4. f(0) = f(1) = 1.
We will investigate the matrix Uf in each situation, taking into account
the correspondences:
0 →|0⟩=
	
1
0

, 1 →|1⟩=
	
0
1

.
In the ﬁrst case, we have Uf|x, y⟩= |x, y⊕0⟩= |x, y⟩, hence Uf = I = U †
f.
In the second case, Uf|00⟩= |00⟩, Uf|01⟩= |01⟩, Uf|10⟩= |11⟩, Uf|11⟩=
|10⟩, so it follows that
Uf =




1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0



= U †
f.
A direct computation shows that in the third case, Uf|00⟩= |01⟩,
Uf|01⟩= |00⟩, Uf|10⟩= |10⟩and Uf|11⟩= |11⟩, therefore,
Uf =




0
1
0
0
1
0
0
0
0
0
1
0
0
0
0
1



= U †
f.
Finally, Uf|x, y⟩= |x, y ⊕1⟩, i.e. Uf|x0⟩= |x1⟩and Uf|x1⟩= |x0⟩, hence
Uf =




0
1
0
0
1
0
0
0
0
0
0
1
0
0
1
0



= U †
f.

244
Quantum Computing
4.19.2
Quantum parallelism
Can the transformation Uf, discussed in the above section, be extended for
arbitrary functions f : {0, 1}n →{0, 1}? Note that to compute a complete
set of values for such a function we need to calculate f in all 2n points, a
huge task if n is big (say, n = 100).
The answer is mathematically easy, but computationally extremely power-
ful, it points out to one of the most important sources of strength of Quantum
Computing.
Each vector x = (i1, i2, . . . , in) ∈{0, 1}n can be identiﬁed with the state
|x⟩= |i1⟩⊗|i2⟩⊗. . . ⊗|in⟩= |i1i2 . . . in⟩,
so keeping in mind that Uf is a linear transformation, deﬁne Uf to be the
linear operator acting on C2n+2 which satisﬁes the equality (4.16),
Uf|x, y⟩= |x, y ⊕f(x)⟩,
for any |x⟩= |i1i2 . . . in⟩, ik ∈{0, 1}, 1 ≤k ≤n and y ∈{0, 1}. One can
prove that
UfU †
f = UfUf = I and Uf|x, 0⟩= |x, f(x)⟩.
If the transformation Uf is applied to an input which is in a superposition
then, taking into account the linearity, Uf is applied simultaneously
to all
basis vectors in the superposition and generates a superposition of the results.
Thus it is possible to compute f(x) for all the 2n values of x in a single
application of Uf. This eﬀect is called quantum parallelism.
Typically, a quantum algorithm starts by preparing the function of inter-
est in a superposition on all values. Starting with an n-qubit state |00 . . . 0⟩
and applying the Walsh–Hdamard transformation W, we get the superposi-
tion
1
√
2n (|00 . . . 0⟩+ |00 . . . 1⟩+ . . . + |11 . . . 1⟩) =
1
√
2n
2n−1

x=0
|x⟩.
This superposition is a compact “encoding” of all integers x in the interval
[0, 2n]. Using linearity, we get, again, a compact “encoding” of all values of
f, the function we are interested in:
Uf(
1
√
2n
2n−1

x=0
|x0⟩) =
1
√
2n
2n−1

x=0
Uf|x0⟩=
1
√
2n
2n−1

x=0
|x, f(x)⟩.
Consequently, n qubits are enough to work simultaneously with 2n states,
in a “strange” compact form; this gives quantum computing the ability to
perform an exponential amount of computation in a linear amount of physical
space.

Quantum Algorithms
245
❣
❣
|x⟩
|y⟩
|0⟩
|x⟩
|y⟩
|x ∧y⟩
Figure 4.8: Quantum parallel computation of controlled-controlled-NOT.
Let’s apply the above technique to the simple example of controlled-
controlled-NOT gate TOFOLLI that computes the conjunction.
We have:
W2|00⟩= 1
2(|00⟩+ |01⟩+ |10⟩+ |11⟩)
and
TOFOLLI(W|00⟩⊗|0⟩)
=
1
2 TOFOLLI(|000⟩+ |010⟩+ |100⟩+ |110⟩)
=
1
2(|000⟩+ |010⟩+ |100⟩+ |111⟩).
The resulting superposition can be viewed as a truth table for conjunction.
4.19.3
Quantum implementations
Recall that for any function f : {0, 1}n →{0, 1}m there exists a quantum
function F : C2n+m →C2n+m working on an n-qubits input and an m-qubits
output register with F|x, 0⟩= |x, f(x)⟩. Invertible functions f : {0, 1}n →
{0, 1}n can be directly realized by quantum functions F : |i⟩→|f(i)⟩. While
a direct implementation of F is possible with any universal set of quantum
gates, the implementation can be substantially more eﬃcient. For example,
if we have eﬃcient implementations of the quantum functions Uf : |i, 0⟩→
|i, f(i)⟩and Uf −1 : |i, 0⟩→|i, f −1(i)⟩, then an overwriting operator F ′ can
be constructed by using an n-qubit scratch register:
F ′ : |i, 0⟩
Uf
−→|i, f(i)⟩
SWAP
−→|f(i), i⟩
U †
f−1
−→|f(i), 0⟩.
Bennett’s “uncomputing” procedure to control the amount of “junk” bits
necessary to compute reversibly non-reversible functions can be presented as
follows:

246
Quantum Computing
|x, 0, 0⟩
G
−→|x, g(x), 0⟩
H
−→|x, g(x), h(g(x))⟩
G†
−→|x, 0, f(x)⟩.
Here f is the composition of two non-reversible functions f(x) = h(g(x))
(G and H are the quantum functions for g and h). The last step is merely
the inversion of the ﬁrst step and uncomputes the intermediate result. The
second register can then be reused for further computations.
If the computation of a function f(x) ﬁlls a scratch register with the junk
bits j(x) (i.e. |x, 0, 0⟩→|x, f(x), j(x)⟩), a similar procedure can free the
register again:
|x, 0, 0, 0⟩−→|x, f(x), j(x), 0⟩
FANOUT
−→
|x, f(x), j(x), f(x)⟩−→|x, 0, 0, f(x)⟩.
Again, the last step is the inversion of the ﬁrst. The intermediate step is
a FANOUT operation which copies the function result into an additional
empty register. Possible implementations of FANOUT operation are,
|x, x ⊕y⟩,
or
|x, y⟩→|x, (x + y)( mod 2n)⟩.
Conditional branching is a powerful tool used by conventional programs.
A unitary operator, on the other hand, is static and has no internal ﬂow-
control. Nevertheless, we can conditionally apply an n qubit operator U to a
quantum register by using an enable qubit and deﬁne an n+1 qubit operator
U ′
U ′ =
	
In
0
0
U

.
So U is only applied to basis vectors where the enable bit is set. This can be
easily extended to enable-registers of arbitrary length.
A conditional operator U[[e]] – with enable register e – is a unitary operator
of the form
U[[e]] : |i, ϵ⟩= |i⟩|ϵ⟩e →

(U |i⟩) |ϵ⟩e,
if ϵ = 111 . . .
|i⟩|ϵ⟩e,
otherwise.
If the architecture allows the eﬃcient implementation of the controlled-
NOT gate
CNOT : |x, y1, y2 . . .⟩→|(x ⊕

i
yi), y1, y2 . . .⟩,
then conditional operators can be realized by simply adding the enable string
to the control register of all controlled-not operations.

Quantum Algorithms
247
4.19.4
Quantum programming
Many quantum algorithms seem diﬃcult to understand because of the heavy
physics formalism (Dirac notation, matrices, gates, operators).
In what
follows we will brieﬂy introduce the programming language QCL65 invented
by ¨Omer [202] and we will use it to present a few important quantum
algorithms.
QCL is a high level, architecture independent programming
language for quantum computing; its syntax is derived from classical proce-
dural languages, like C or Pascal. QCL allows a complete implementation
and simulation of quantum algorithms (including classical components) in
a single, consistent formalism. The interpreter qcl can simulate quantum
computers with arbitrary numbers of qubits.
All numerical simulations
are handled by the QC library, cf.
¨Omer [202].
The command include
"ﬁlename" tells the interpreter to process the ﬁle ﬁlename.qcl, before
continuing with the current input ﬁle or command line; qcl looks for the
ﬁle in the current directory and in the default include directory, which can
be changed with the option include-path.
In interactive use include
"ﬁlename" can be abbreviated by <<ﬁlename.
The syntactic structure of a QCL program is described by a context free
grammar. Syntactic expressions are deﬁned as:
expression-name
←
expression-def 1
←
expression-def 2
· · ·
· · ·
We will use keywords and other literal text in Courier, subexpressions in
Italic, optional expressions, repeated 0 or 1 times, in square brakets [, ],
multiple expressions, repeated 0, 1 or n times, in braces {, }, alternatives
written as alt 1| alt 2| . . ., grouping of expressions forced by round brackets
(, ), character classes including digits, digit ←decimal digit from 0 to 9,
letters, letter ←alphabetic letter form a to z or A to Z, and characters,
char ←printable character except ‘"’.
A QCL Program is a sequence of statements and deﬁnitions:66
qcl-input ←{ stmt | def }
Statements, from simple commands to complex control-structures, are
executed as they are encountered. For example,
qcl> if random()>=0.5 { print "red"; } else { print "black"; }
: red
65QCL stands for Quantum Computation Language.
66Read from a ﬁle or from the shell prompt. In the latter case, input is restricted to one
line which is implicitly terminated by ‘;’.

248
Quantum Computing
Deﬁnitions are not executed but bind a value (variable or constant) or a
block of code (routine-deﬁnition) to a symbol (identiﬁer). Consequently, each
symbol has an associated type, which can either be a data type or a routine
type. The type deﬁnes the symbol access mode, by reference or by call. Here
are two examples:
qcl> int counter=5;
qcl> int fac(int n) {if n<=0 {return 1;} else {return n*fac(n-1);}}
Expressions can be composed of literals, variable references and sub-
expressions combined by operators and function calls. Some examples are:
qcl> print "5 out of 10:",fac(10)/fac(5)^2,"combinations."
: 5 out of 10: 252 combinations.
Classic data-types are: arithmetic types int, real and complex and the
general types boolean and string.
QCL deﬁnes two unconventional operators, which are mainly used with
quantum expressions:
Concatenation The concatenation operator & combines two quantum reg-
isters or two strings. Its precedence is equal to the arithmetic operators
+ and -.
Size The size-of operator # gives the length (i.e. the number of qubits) of any
quantum expression. This is the operator with the highest precedence.
QCL provides external operators for general unitary 2×2, 4×4 and 8×8
matrices, which can be used directly to implement sets of 1, 2 and 3 qubit
gates.
extern operator Matrix2x2(
complex u00,complex u01,
complex u10,complex u11,
qureg q);
extern operator Matrix4x4(...,qureg q);
extern operator Matrix8x8(...,qureg q);
Matrix operators are checked for unitarity before they are applied:
qcl> const i=(0,1);
qcl> qureg q[1];
qcl> Matrix2x2(i*cos(pi/6),i*sin(pi/6),(0,0),(1,0),q);
! external error: matrix operator is not unitary
We continue with some details about the implementation of a few impor-
tant transformations.
The rotation of a single qubit deﬁned by the transformation matrix Rotθ
Rotθ =


cos θ
2
sin θ
2
−sin θ
2
cos θ
2


is implemented by

Quantum Algorithms
249
extern operator Rot(real theta,qureg q);
Hadamard gate of n-qubit registers is deﬁned by67
H : |i⟩→

j∈{0,1}n
(−1)(i,j) |j⟩.
The vectors B′ = {i ∈{0, 1}n | |i′⟩= H |i⟩} form the Hadamard (dual, parity)
base of B = {i ∈{0, 1}n | |i⟩}. Since B′ only contains uniform superpositions
that just diﬀer by the signs of the basis vectors, the external implementation
of H is called Mix:
extern operator Mix(qureg q);
The conditional phase gate is a conditional operator for the zero-qubit
phase operator eiφ:
V (φ) : |ϵ⟩→

eiφ, |ϵ⟩,
if ϵ = 111 . . . ,
|ϵ⟩,
otherwise,
extern operator CPhase(real phi,qureg q);
The external FANOUT operator of QCL is deﬁned by
FANOUT : |x, y⟩→|x, x ⊕y⟩,
extern qufunct Fanout(quconst a,quvoid b);
The SWAP operator, changing two qubits of equal sized registers
(SWAP : |x, y⟩→|y, x⟩) is deﬁned by the transformation matrix:
SWAP =




1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1



,
and implemented as:
extern qufunct Swap(qureg a,qureg b);
The controlled-NOT operator C[[e]] is the conditional operator to C68
with the enabled register e:
C[[e]] : |b⟩|ϵ⟩e →

|1 −b⟩|ϵ⟩e,
if ϵ = 111 . . . ,
|b⟩|ϵ⟩e,
otherwise,
is implemented as:
67Recall that H =
1
√
2
 1
1
1
−1

.
68Recall that the NOT operator is deﬁned by the matrix C =
 0
1
1
0

.

250
Quantum Computing
extern qufunct Not(qureg q);
extern qufunct CNot(qureg q,quconst c);
The QCL versions of NOT and controlled-NOT work also on target reg-
isters:
qcl> qureg q[4]; qureg p[4];
qcl> Not(q);
[8/8] 1 |00001111>
qcl> CNot(p,q);
[8/8] 1 |11111111>
4.19.5
Quantum Fourier Transform
A common mathematical trick in solving a problem given by a “function” is to
transform the function, work with the transform, and ﬁnally “un-transform”
the result to deduce properties of the original function. Fourier transform is
such a signiﬁcant transformation (see Kronsj¨o [157] for more details). Fourier
transform linearity is particular useful for Quantum Computing.
In general, Fourier transforms map from the time domain to the frequency
domain. So, Fourier transforms map functions of period r to functions which
have non-zero period values only at multiples of the frequency 1
r. The Discrete
Fourier Transform maps a q dimensional vector {f(0), f(1), . . . , f(q−1)} into
the vector { ¯f(0), ¯f(1), . . . , ¯f(q −1)} as follows:
DFT : ¯f(c) =
1
√q
q−1

y=0
e
2πi
q
cyf(y),
(4.17)
for all c ∈{0, 1, . . . , q −1}.
The fastest algorithm for computing DFT is the Fast Fourier Transform
(FFT) invented by Cooley and Tukey [66]; it uses a binary decomposition of
the exponent to perform the transformation in O(n2n) steps, where q = 2n.
In analogy, the Quantum Fourier Transform QFT can be deﬁned by:
QFT q : |x⟩→
1
√q
q−1

y=0
e
2πi
q
xy |y⟩.
When applied to a quantum superposition, QFT q transforms the state
1
√q
q−1

y=0
f(y)|y⟩,
into the state
QFT q :
q−1

y=0
f(y)|y⟩→
q−1

c=0
¯f(c)|c⟩,

Quantum Algorithms
251
where ¯f(c) comes from (4.17).
Note that
QFT q : |0⟩→
1
√q
q−1

c=0
|c⟩,
so it has the same eﬀect as a Hadamard transformation. Coppersmith [67] has
used a combination of Hadamard transformations H and conditional phase
gates V 69 to obtain the transformation:
DFT ′ =
n−1

i=1

Hn−i−1(π
2 )
i−1

j=0
Vn−i−1,n−j−1(
2π
2i−j+1 )

Hn−1.
The basis vectors of the transformed state | ˜ψ′⟩= DFT ′q |ψ⟩are given
in reverse bit order, so to get the actual DFT q, the bits have to be ﬂipped.
Here is a QCL program implementing this transformation:
operator dft(qureg q) { // main operator
const n=#q;
// set n to length of input
int i; int j;
// declare loop counters
for i=0 to n-1 {
for j=0 to i-1 {
// apply conditional phase gates
CPhase(2*pi/2^(i-j+1),q[n-i-1] & q[n-j-1]);
}
Mix(q[n-i-1]);
// qubit rotation
}
flip(q);
// swap bit order of the output
}
The program contains two loops: the outer loop performs a Hadamard
transformation from the highest to the lowest qubit (Mix), while the inner
loop performs conditional phase shifts (CPhase) between the qubits.
The dft operator takes a quantum register (qureg) q as argument.70
The number of qubits of a register can be determined at runtime by the size
operator #; this permits register size independent operator deﬁnitions. Inside
the operator deﬁnition, sub-operators are called just as sub-procedures in
conventional languages: the actual sequence of operators is fully determined
at runtime.71
The execution is context dependent: DFT(q) called from top level works
as expected; the diﬀerence appears when we have a !
call, !DFT(q): all
operators within DFT are inverted and applied in reverse order.
Inverse
execution can also take place implicitly, e.g. Bennett’s trick is used.
For illustration, let’s start the QCL interpreter and prepare a test state
(see [202]):
69Indices indicate the qubits operated on.
70A quantum register is not a quantum state by itself, but a pointer indicating the target
qubits in the overall machine state.
71This includes loops, in our speciﬁc example, for-loops, conditional statements, etc.

252
Quantum Computing
$ qcl -b5 -i dft.qcl
[0/5] 1 |00000>
qcl> qureg q[5];
// allocate a 5 qubit register
qcl> Mix(q[3:4]);
// rotate qubits 3 and 4
[5/5] 0.5 |00000> + 0.5 |10000> + 0.5 |01000> + 0.5 |11000>
qcl> Not(q[0]);
// invert first qubit
[5/5] 0.5 |00001> + 0.5 |10001> + 0.5 |01001> + 0.5 |11001>
We now have a periodic state with period 23 = 8 and an oﬀset of 1 composed
of 4 basis vectors to which we can apply the DFT:
qcl> dft(q);
[5/5] 0.353553 |00000> + -0.353553 |10000> + 0.353553i |01000> +
-0.353553i |11000> + (0.25,0.25) |00100> + (-0.25,-0.25) |10100> +
(-0.25,0.25) |01100> + (0.25,-0.25) |11100>
The DFT “inverts” the period to 25/8 = 4 and a periodic distribution with
oﬀset 0 is obtained. The information about the original oﬀset is in the phase
factors, and has no inﬂuence on the probability distribution:
qcl> dump q;
: SPECTRUM q: |43210>
0.125 |00000> + 0.125 |00100> + 0.125 |01000> + 0.125 |01100> +
0.125 |10000> + 0.125 |10100> + 0.125 |11000> + 0.125 |11100>
“Uncomputing” the DFT, that is calling !DFT(q), brings back the initial
conﬁguration:
qcl> !dft(q);
[5/5] 0.5 |00001> + 0.5 |10001> + 0.5 |01001> + 0.5 |11001>
qcl> exit;
4.19.6
Shor’s algorithm
The factoring problem requires ﬁnding a non-trivial factor of a given a com-
posite integer. More precisely, if N is given and known to be composite, ﬁnd
a non-trivial factor of it, that is, 1 < n1 < N such that n1|N.72
Multiplying large numbers is computationally easy. In contrast, no con-
ventional polynomial73 algorithm for the factorization of large numbers is
known.
The best known74 conventional algorithm, the so-called quadratic sieve,
needs
O
	
e( 64
9 )
1
3 (ln N)
1
3 (ln ln N)
2
3

72n1 divides N.
73In the length of its input measured in bits.
74Published; cf. Lenstra and Lenstra [164], Gruska [119].

Quantum Algorithms
253
operations for factoring a binary number of N bits;75 this is exponential
with the input size.
There exist sub-exponential O(2(ln N)
1
3 ) randomized
algorithms for factoring.76
Why are we concerned with these subtleties regarding such a “simple”
problem as factoring of integers? The multiplication of large prime numbers
is a “one-way function” i.e. a function which can easily be computed, while
computing its inversion is “thought” (but not proven) to be practically im-
possible. One-way functions play a major role in digital cryptography and
are essential to public key crypto-systems where the key for encoding is pub-
lic and only the key for decoding remains secret. An example is the RSA
method invented in Rivest, Shamir and Adleman [251]. Their cryptographic
algorithm is based on the “one-way” character of multiplying two large prime
numbers p, q (typically of 512 or 1024 digits). Encription seems to be secure
provided it is not feasible to get p, q from their product N = pq.77 The RSA
method is one of the most popular public key systems and is implemented in
many communication programs.
According to Hughes [141] conventional factoring perspectives with the
best available method in 1997 (quadratic sieve) on a network of 1,000 work-
stations is presented in Table 4.6.
Year/Number of bits
1024
2048
4096
2006
1015 years
5 × 1015 years
3 × 1029 years
2024
38 years
1012 years
7 × 1025 years
2042
3 days
3 × 108 years
2 × 1022 years
Table 4.6: Perspectives of factoring on conventional computers.
While it is generally believed (although not proven) that eﬃcient prime
factorization on a conventional computer is impossible, an eﬃcient algorithm
for quantum computers has been proposed in 1994 by Shor [269]; see also
Beckman, Shari, Devabhaktuni and Preskill [19], Ekert and Jozsa [94].
A key idea is to reduce factoring to the problem of ﬁnding the period of
an integer function. Consider ﬁrst a composite N and let’s ﬁnd an integer
x such that x2 ≡1( mod N ), and x ̸≡1( mod N ), x ̸≡−1( mod N ). If x is
an integer satisfying the above conditions, then x2 −1 = (x −1)(x + 1) is a
multiple of N, but x −1 is not divisible by N nor is x + 1, so there exists
a non-trivial factor n1 of N that divides say x + 1 or x −1. A factor of
75( 64
9 )
1
3 ≈1.9.
76Cf. Vazirani [292].
77It is not known whether breaking RSA is as hard as factoring.

254
Quantum Computing
N can then be found by Euclid’s algorithm78 in O(log n) time: we compute
gcd(N, x + 1) and gcd(N, x −1).
For example, if N = 15, then x = 4 is a solution of x2 ≡1( mod 15) that
veriﬁes the above requirements; gcd(15, 4 + 1) = 5, gcd(15, 4 −1) = 3, so 5
and 3 are non-trivial divisors of 15.
Let G be the set of all integers x in the interval [1, N] co-primes with N,
that is gcd(x, N) = 1.79 Pick x in G and deﬁne the function
fN,x(a) = xa ( mod N ).
The function fN,x is periodic, that is there exists an integer r such that
fN,x(a) = fN,x(a + r), for every a ∈G.
The smallest r such that xr ≡
1 ( mod N ) is called the order of x and is denoted by ord(x).
If the period r is even, then the equation
xr ≡1 ( mod N )
can be written in the following equivalent forms:

x
r
2 2 ≡1 ( mod N ),

x
r
2 2 −12 ≡0 ( mod N ),

x
r
2 −1
 
x
r
2 + 1

≡0 ( mod N ).
The product (x
r
2 −1)(x
r
2 + 1) is a multiple of N, hence, unless x
r
2 ≡
1 ( mod N ), or x
r
2 ≡−1 ( mod N ), both factors of the product must have a
non-trivial factor in common with N. Consequently, the following algorithm
can be used:
Step 1.
Choose a uniformly distributed integer 1 ≤x ≤N.
Step 2.
If gcd(N, x) > 1, then we have a factor of N and stop.
Step 3.
Compute the period r of the function fN,x.
Step 4.
If r is odd or x
r
2 ≡1 ( mod N ) or x
r
2 ≡−1 ( mod N ),
then go to Step 1; otherwise, compute gcd(N, x + 1),
gcd(N, x −1) to get a factor of N, and stop.
Note that in case N = pi, where p is a prime and i > 1, if gcd(x, p) = 1
and r = ord(x), then r is odd or x
r
2 ≡1 ( mod N ) or x
r
2 ≡−1 ( mod N ).
78The greatest common divisor gcd of a, b can be computed by the scheme:
gcd(a, b) =

b,
if a = 0 (mod b),
gcd(a, a mod b),
if a ̸= 0 (mod b), a > b.
.
79This set is a group under the multiplication ( mod N ).

Quantum Algorithms
255
So, this case is excluded by the above algorithm. However, we don’t lose gen-
erality because power of primes can be factorized by conventional computers
in polynomial time.
What are the chances of ﬁnding a factor of N by computing gcd(N, x+1)
and gcd(N, x −1)? This question is answered by the following evaluations
(see, for example, Shor [269] or Gruska [119]):
• Assume that N is not a power of a prime. If x is selected uniformly
distributed in G,80 then the probability to ﬁnd a even r such that
x
r
2 ̸≡1 ( mod N ) and x
r
2 ̸≡−1 ( mod N ) is greater or equal to 1
4.
• If N is odd and x is selected uniformly distributed in G, then the
probability that ord(x) is even is greater or equal to 1
2.
• The probability of picking an element x, uniformly distributed in G,
such that r = ord(x) is even, x
r
2 ̸≡1 ( mod N ) and x
r
2 ̸≡−1 ( mod N )
is greater or equal to 1
2.
This analysis leads to the following conclusion:
If there is a polynomial time deterministic/probabilistic/quantum
algorithm to compute the period of the function fN,x, for every
N, x, then there exists a deterministic/probabilistic/quantum al-
gorithm to factorize any integer N.
Unfortunately,
there
is
no
known
polynomial
time
determinis-
tic/probabilistic algorithm to compute the period of the function fN,x! Next
we will follow Shor [269] in presenting a polynomial time quantum algorithm
to compute the period of the function fN,x.
Let F be quantum function F : |x, 0⟩→|x, f(x)⟩of the integer function
f : Z →{1, 2, . . . , 2m} with the unknown period r < 2n. To determine r, we
need two registers, with the sizes of 2n and m qubits, which should be reset
to |0, 0⟩.
As a ﬁrst step we produce, using the Hadamard transform H, a homoge-
nous superposition of all basis vectors in the ﬁrst register by applying an
operator U:
U|0, 0⟩=
22n−1

i=0
ci|i, 0⟩
with
|ci| = 1
2n .
Applying F to the resulting state gives:
|ψ⟩= F 1
2n
22n−1

i=0
|i, 0⟩= 1
2n
22n−1

i=0
|i, f(i)⟩.
80Recall that G is the set of all integers in the interval [1, N] co-primes with N.

256
Quantum Computing
A measurement of the second register with the result k = f(s) with s < r
reduces the state to
|ψ′⟩=
⌈q/r⌉−1

j=0
c′
j|rj + s, k⟩
with
q = 22n
and
c′
j =
r
q
 
.
The measurement selects the values s, s + r, s + 2r, . . . , s + rα, where α
is the largest integer such that s + rα ≤q, and s ≤r is chosen randomly
(by measurement).
So, α ≈
q
r.
The post-measurement state |ψ′⟩of the
ﬁrst register consists only of basis vectors of the form |rj + s⟩.
Reason:
f(rj + s) = f(s), for all j.
Assume now that α =
q
r −1; later we will brieﬂy discuss the general
case. It is not possible to directly extract the period r (or a multiple of it)
by measurement of the ﬁrst register because of the random oﬀset s. The
result of a Fourier transform, however, is invariant81 to oﬀsets of a periodic
distribution. We have:
| ˜ψ′⟩
=
QFT q|ψ′⟩
=
1
√q
q−1

y=0
!r
q
⌈
q
r⌉−1

j=0
e2πi y(rj+s)
q
|y, k⟩
=
q−1

y=0
˜c′
y|y, k⟩,
where
˜c′
y =
√r
q
p−1

j=0
e( 2πi
q
y(jr+s)) =
√r
q eφy
p−1

j=0
e(2πi yjr
q ),
φy = 2πi ys
q
and
p =
"q
r
#
.
If q = 22n is a multiple of r, then ˜c′
y = eφi/√r if y is a multiple of q/r, and
0 otherwise. But even if r is not a power of 2, the spectrum of | ˜ψ′⟩shows
distinct peaks with a period of q/r because
lim
n→∞
1
n
n−1

k=0
e2πikα =
 1,
if α ∈Z,
0,
if α ̸∈Z.
This is also the reason we use a register of 2n qubits when r < 2n: it
guarantees at least 2n elements in the above sum and thus a peak width of
order O(1).
81Except for phase factors which don’t eﬀect the probability.

Quantum Algorithms
257
Now measure the ﬁrst register: we will get a value c close to λq/r with
λ ∈Zr. This can be written as c/q = c · 2−2n ≈λ/r. We can think of this
result as a rational approximation a/b with a, b < 2n for dyadic number c ·
2−2n. An eﬃcient classical algorithm for solving this problem using continued
fractions is described in Hardy and Wright [122] and is implemented in the
denominator function.
Since the form of a rational number is not unique, λ and r are only
determined by a/b = λ/r if gcd(λ, r) = 1. The probability that λ and r are
co-prime is greater then 1/ln r, so only O(n) trials are necessary to achieve a
probability of success as close to one as desired; in fact, as observed by Shor
[269], the expected number of trials can be decreased to a constant.82
The implementation of the Shor algorithm uses the following functions:
• boolean testprime(int n): Tests whether n is a prime number.83
• boolean testprimepower(int n): Tests whether n is a prime power.
• int powmod(int x,int a,int n): Calculates xa ( mod n).
• int denominator(real x,int qmax): Returns the denominator q of
the best rational approximation p
q ≈x with p, q < qmax.
The procedure shor checks whether the integer number is suitable for
quantum factorization, and then repeats Shor’s algorithm until a factor has
been found ([202]).
procedure shor(int number) {
int width=ceil(log(number,2));
// size of number in bits
qureg reg1[2*width];
// first register
qureg reg2[width];
// second register
int qmax=2^width;
int factor;
// found factor
int m; real c;
// measured value
int x;
// base of exponentiation
int p; int q;
// rational approximation p/q
int a; int b;
// possible factors of number
int e;
// e=x^(q/2) mod number
if number (mod 2) == 0 { exit "number must be odd"; }
if testprime(number) { exit "prime number"; }
if testprimepower(number) { exit "prime power"; };
82If the supposed period r′ = b derived form the rational approximation a/b ≈c 2−2m
is odd or gcd(xr′/2 ± 1, n) = 1, then one could try to expand a/b by some integer factor k
in order to guess the actual period r = kb.
83Since both test functions are not part of the algorithm itself, short but ineﬃcient
implementations with O(√n) have been used in [202].

258
Quantum Computing
{
{
// generate random base
x=floor(random()*(number-3))+2;
} until gcd(x,number) == 1;
print "chosen random x =",x;
Mix(reg1);
// Hadamard transform
expn(x,number,reg1,reg2);
// modular exponentiation
measure reg2;
// measure 2nd register
dft(reg1);
// Fourier transform
measure reg1,m;
// measure 2st register
reset;
// clear local registers
if m==0 {
// failed if measured 0
print "measured zero in 1st register. trying again ...";
} else {
c=m*0.5^(2*width);
// fixed point form of m
q=denominator(c,qmax);
// find rational approximation
p=floor(q*m*c+0.5);
print "measured",m,", approximation for",c,"is",p,"/",q;
if q (mod 2) == 1 and 2*q<qmax { // odd q ? try expanding p/q
print "odd denominator, expanding by 2";
p=2*p; q=2*q;
}
if q (mod 2) == 1 {
// failed if odd q
print "odd period. trying again ...";
} else {
print "possible period is",q;
e=powmod(x,q/2,number);
// calculate candidates for
a=(e+1) mod number;
// possible common factors
b=(e+number-1) mod number; // with number
print x,"^",q/2,"+ 1 mod",number,"=",a,",",
x,"^",q/2,"- 1 mod",number,"=",b;
factor=max(gcd(number,a),gcd(number,b));
}
}
} until factor>1 and factor<number;
print number,"=",factor,"*",number/factor;
}
Shor’s algorithm is probabilistic, so it may fail. The smallest number that
can be factorized with Shor’s algorithm is 15, as it’s the product of smallest
odd prime numbers 3 and 5.
¨Omer’s [202] implementation of the modular
exponentiation needs 2l + 1 qubits scratch space with l = ⌈ld(15 + 1)⌉= 4.
The algorithm itself needs 3l qubits, so a total of 21 qubits must be provided.
$ qcl -b21 -i shor.qcl
qcl> shor(15)
: chosen random x = 4
: measured zero in 1st register. trying again ...
: chosen random x = 11
: measured 128 , approximation for 0.500000 is 1 / 2
: possible period is 2
: 11 ^ 1 + 1 (mod 15( = 12 , 11 ^ 1 - 1 (mod 15) = 10
: 15 = 5 * 3
The ﬁrst try failed because 0 was measured in the ﬁrst register of |ψ′⟩and

Quantum Algorithms
259
λ/r = 0 which gives no information about the period r.
One might argue that this is not likely to happen, since the ﬁrst register
has 8 qubits and 256 possible basis vectors; however, if a number n is to
be factored, one might expect a period about √n assuming that the prime
factors of n are of the same order of magnitude. This leads to a period q/√n
after the DFT and the probability p = 1/√n to accidentally pick the vector
|0⟩is about p = 25.8%.
The
second
try
also
had
the
same
probability
of
failure
since
112( mod 15) = 1, but this time, the measurement picked the second
peak in the spectrum at |128⟩. With 128/28 = 1/2 = λ/r, the period r = 2
was correctly identiﬁed and the factors gcd(112/2 ± 1 , 15) = {3, 5} to 15
have been found.
Shor’s algorithm shows that factoring can be checked in polynomial time
by a quantum algorithm. However, there is no proof that factoring cannot
be checked in polynomial time by a deterministic conventional algorithm.
If eﬃcient ways to simulate quantum computers on conventional machines
will be found, then Shor’s algorithm will be transformed into an eﬃcient
conventional procedure for factoring. However, this possibility seems very
improbable.
We ﬁnish this section by citing again Hughes [141] with estimations dis-
played in Table 4.7 on factoring on quantum computers (with minimal clock
speed of 100 MHz).
Number of bits
1, 024
2, 048
4,096
number of qubits
5, 124 years
10, 244 years
20, 484 years
number of gates
3 × 109 years
2 × 1011 years
2 × 1012 years
factoring time
4.5 mins
36 min
4.8 hours
Table 4.7: Perspectives of factoring on a (hypothetical) quantum computer.
So, according to Tables 4.6, 4.7, using RSA with 2,048-bit numbers may
be safe for the next 50 years if no quantum computer will be constructed by
then; in the opposite case, even working with 4,096-bit numbers may not be
safe if quantum computers become available!
4.19.7
Grover’s algorithm
Many problems, ranging from sorting and graph colouring to database search,
can be phrased as search problems of the form “ﬁnd some x such that P(x)
is true”, where P is an appropriate predicate. For example:

260
Quantum Computing
(a) Given an n element vector A, ﬁnd a permutation π of {1, 2, . . . , n} such
that for all 1 ≤i < n we have Aπ(i) < Aπ(i+1).
(b) Given a graph (V ; E) with n vertices V and e edges E ⊂V × V , and a
set of k colours C, ﬁnd a colouring map c from V to C such that for
all (v1, v2) in E, c(v1) ̸= c(v2).
Some problems, such as 3-SAT or graph colourability, have an “inner
structure” that can be exploited to construct full solutions from (smaller)
partial solutions; in such cases, eﬃcient algorithms are known. But, in gen-
eral, the search space has no special structure. To see the diﬀerence between
a structured and an unstructured search space think, with Brassard [36], of
the task of ﬁnding someone’s (you know her name) phone number in a city’s
directory versus the task of ﬁnding the name of a stranger whose phone num-
ber you happen to know. To search a simple unstructured ﬁle, a computer
would have to run through, on average, half of the data to locate an entry x
satisfying P(x). It is easily proven that there can be, in general, no short-
cuts, so randomly testing the predicate P seems the best strategy that can
be adopted on a conventional computer. For a search space of size N, the
general unstructured search problem is of complexity O(N), once the time it
takes to test the predicate P is factored out. When we said “no shortcuts
are possible in general” we meant “no shortcuts are possible if we use con-
ventional computers”. However, Grover [118] showed that the unstructured
search problem can be solved with bounded probability within O(
√
N) time
with a quantum computer.
This advantage is not dramatic for our example of locating a name in the
phone directory of a city; however, things are diﬀerent if we are interested
in databases that are so large that they would not ﬁt in the memory of all
the world’s (conventional) computers put altogether. Such databases cannot
“exist” explicitly, of course, but they can be speciﬁed by a rule that allows the
construction of any required record on demand. As an example consider one
of the most widely used systems to protect the conﬁdentiality of information,
the Data Encryption Standard (DES). Encipher and decipher are controlled
by a 56-bit key, which the legitimate participants must share in secret. The
goal of an eavesdropper, that has intercepted matching pairs of clear and
enciphered text, is to ﬁnd the key that maps one into the other. This problem
can be described by a virtual “phone directory” in which each possible key
is a name and the enciphered text with that key is the corresponding phone
number. Given the intercepted enciphered text, our name-ﬁnding problem
corresponds to searching for the required secret key to decode the rest of the
enciphered text. An exhaustive search needs to try an average of 255 keys
before hitting the right one, an operation that takes more than 1 year even if
one billion keys are checked every second. By comparison, Grover’s algorithm
can solve the problem, after quantum-DES enciphering the known clear text,
in just 185 million times. Hence, Grover’s quantum searching algorithm can
“in principle” be used to break classical cryptographic systems such as DES.

Quantum Algorithms
261
Grover’s search algorithm is more eﬃcient than any known conventional
algorithm searching a completely unstructured solution space.
More pre-
cisely, Grover’s algorithm is optimal for completely unstructured searches,
see Zalka [305]. But, many search problems involve searching a structured
solution space. One would expect that this extra information would enable
more eﬃcient searching strategies. Hogg [139] has developed quantum algo-
rithms that use the problem structure in a similar way to classical heuristic
search algorithms. Small cases indicate that Hogg’s algorithms are more eﬃ-
cient than Grover’s algorithm applied to structured search problems, but the
speed up is likely to be only polynomial.
As we already pointed out, Grover’s algorithm searches an unstructured
list of size N to ﬁnd one item satisfying a given condition. It is assumed that
it is easy to check whether an arbitrary element satisﬁes the given condition.
Let n be such that 2n ≥N. Assume that predicate P is implemented by a
quantum gate
UP |x, 0⟩→|x, P(x)⟩,
where true is encoded as 1. Grover’s algorithm consists of the following steps:
Step 1.
Prepare a register containing a superposition of all
of the possible values xi ∈{0, 1, . . . , 2n −1}.
Step 2.
Compute P(xi) on this register.
Step 3.
Change amplitude aj to −aj, for all xj such that
P(xj) = 1.
Step 4.
Apply inversion about the average to increase
amplitude of xj with P(xj) = 1.
Step 5.
Repeat steps 2 through 4
π
4 2
n
2 times.
Step 6.
Read the result.
Computing P for all possible inputs xi can be done by applying UP to a
register containing the superposition
1
2n/2
2n−1

x=0
|x⟩
with a register set to 0:
UP :
1
2n/2
2n−1

x=0
|x⟩→
1
2n/2
2n−1

x=0
|x, P(x)⟩.
This is done with the classical technique in steps 1 and 2.
The diﬃculty is, as usual, to obtain a useful result from this superposi-
tion. For any xi such that P(xi) is true, |xi, 1⟩will be part of the result
superposition, but chances are very slim to get it via measurement, 2−n,
since its amplitude is
1
2n/2 . The “trick” is to change the resulting quantum

262
Quantum Computing
state in such a way to greatly increase the amplitude of all vectors xi, 1⟩,
for which the predicate is true, and decrease the amplitude of vectors xj, 0⟩,
for which the predicate is false. This change in amplitudes is obtained using
the inversion about the average transformation; it can be accomplished with
O(log(N)) quantum gates.
The transformation
N−1

i=0
ai|xi⟩→
N−1

i=0
(2A −ai)|xi⟩
is performed by the N × N unitary matrix
D =




2
N −1
2
N
. . .
2
N
2
N
2
N −1
. . .
2
N
. . .
. . .
. . .
. . .
2
N
2
N
. . .
2
N −1



.
The matrix D can be deﬁned as D = WRW where W is the Walsh–
Hdamard transformation and R is the matrix:
R =




1
0
0
. . .
0
0
1
0
. . .
0
. . .
. . .
. . .
. . .
. . .
0
. . .
0
0
1



.
Next we explain how the inversion of amplitudes works. Consider the
gate array
UP : |x, b⟩→|x, b ⊕P(x)⟩.
Apply to UP the superposition
|ψ⟩=
1
2n/2
2n−1

x=0
|x⟩
and choose
b =
1
√
2(|0⟩−|1⟩)
to obtain a state where the sign of all x with P(x) = 1 has been changed, but
b is unchanged. Indeed, let Xi = {x | P(x) = i}, i = 0, 1 and let’s compute
UP |ψ, b⟩:
1
2(n+1)/2 UP (

x∈X0
|x, 0⟩+

x∈X1
|x, 1 −

x∈X0
|x, 1⟩−

x∈X1
|x, 1⟩)
=
1
2(n+1)/2 (

x∈X0
|x, 0 ⊕0⟩+

x∈X1
|x, 0 ⊕1⟩−

x∈X0
|x, 1 ⊕0⟩

Quantum Complexity
263
−

x∈X1
|x, 1 ⊕1⟩)
=
1
2(n+1)/2 (

x∈X0
|x, 0⟩+

x∈X1
|x, 1⟩−

x∈X0
|x, 1⟩−

x∈X1
|x, 0⟩)
=
1
2n/2 (

x∈X0
|x⟩−

x∈X1
|x⟩) ⊕b,
so the amplitude of the states in X1 has been inverted.
Grover’s algorithm is optimal up to a constant factor; no quantum algo-
rithm can perform an unstructured search faster. If there is only a unique x0
such that P(x0) is true, then after π
8 2
n
2 iterations of steps 2 through 4 the
failure rate is 1/2. After iterating π
4 2
n
2 times the failure rate drops to 2−n.
For a more detailed analysis see Gruska [119].
Additional iterations will increase the failure rate: for example, after π
2 2
n
2
iterations, the failure rate is close to 1. This is an important feature of many
quantum algorithms, which has little counterpart in conventional comput-
ing. Repeating quantum procedures may improve results for a while, but
after some repetitions the results will get worse again. Quantum procedures
are unitary transformations, which are rotations of complex space; repeated
applications of a quantum transform may rotate the state closer and closer
to the desired state for a while, but eventually it will rotate past the desired
state to get further and further from the desired state. Thus to obtain use-
ful results from a repeated application of a quantum transformation, it is
paramount to know when to stop.84
4.20
Quantum Complexity
4.20.1
Probabilistic computation
Probabilistic methods of computation have become increasingly popular, not
only in theoretical studies (where they have been successfully applied to the
study of average case complexity of deterministic algorithms), but in many
practical applications. A probabilistic algorithm is a procedure that behaves
deterministically except in some cases when it takes “decisions” pseudo-
randomly, according to a ﬁxed probability distribution.
To illustrate this
type of computation consider Rabin’s [246] test, a polynomial probabilistic
algorithm for checking the primality of an integer:
input n
choose pseudo-randomly m integers, x1, x2, . . . , xm,
such that 1 ≤xj ≤n
84This is a sensible attitude, in general.

264
Quantum Computing
for each xi perform the test W(xi, n)
if W(xj, n) holds true for some j, then accept
else reject
end
For an integer 1 ≤x < n, the test W(x, n) is passed if either xn−1 ̸≡
1 (mod n), or there exists an integer m = (n −1)2−i such that xm −1 and
n have a common divisor diﬀerent from 1 and n. Rabin [246] proved that
if W(x, n) holds true, then n is composite, and, if n is composite, then at
least half of integers 1 ≤x < n satisfy W(x, n). So, if Rabin’s algorithm
accepts, then n is composite; if it rejects, then n is “probably” prime, as n is
composite only in case every chosen integer x1, x2, . . . , xm is not a witness to
the compositeness of n. As no less than half of the integers less than n are
such witnesses, picking m pseudo-randomly chosen integers and not ﬁnding
among them a witness to the compositeness of n implies a mistake whose
probability is smaller than 2−m.
Consider a non-deterministic Turing machine and assume that for every
s ∈K, a ∈V , card(δ(s, a)) = 2, for every input x the machine M executes
the same number of steps, and every computation ends in a ﬁnal state, which
accepts or rejects. In this way, the set of all possible computations of M on
an input x is a complete binary tree. The error probability of a probabilistic
Turing machine M is the function which assigns to every input x the ratio of
the number of computations of M on x giving the wrong answer to the total
number of computations of M on x.
A probabilistic Turing machine M is a non-deterministic Turing machine
satisfying the above conditions plus a rule for acceptance. There are two
important rules: (a) the simple majority rule, and (b) the clear majority
rule.
According to the simple majority rule M accepts the input x when more
than half of the computations of M on x end in an accepting state. Two
equivalent ways to state this rule are: M accepts the input x if (1) the
probability of ﬁnding an accepting computation starting with x is greater
than one half,85 (2) the ratio of accepting computations starting with x to
the total number of computations starting with x is greater than one half.
As an example we transform the non-deterministic algorithm solving SAT
into a probabilistic algorithm testing whether a Boolean formula F is satisﬁed
by more than half of the possible assignments:86
input e(F)
check that e(F) encodes a correct Boolean formula
for each variable x occurring in F do
choose in a non-deterministic manner
F = F|x=0 or
85Note that all computations have the same probability.
86This problem is called MAJ.

Quantum Complexity
265
F = F|x=1
simplify the resulting formula without variables
if the result is 1, then accept and halt
end
The above machine accepts exactly those Boolean formulae for which more
than half of the possible computations are accepted. Each computation cor-
responds to a unique assigment satisfying F, thus F probabilistically accepts
x if and only if F is satisﬁed by more than half of the possible assignments.
Let’s turn our attention to the clear majority accepting rule, the one when
the error probability is bounded. A non-deterministic machine M works with
the clear majority rule if there exists an ϵ > 0 such that M accepts the input
x when more than half plus ϵ of the computations of M on x end in an
accepting state. Equivalently: M accepts the input x if the probability of
ﬁnding an accepting computation starting with x is greater than one half plus
ϵ. Note that Rabin’s test of primality is of this form as the error probability
is at most 2−m.
Probabilistic machines can be simulated by deterministic Turing ma-
chines.
As one can expect, the fact that the full computation tree must
be constructed results in a “blow up” of the running time of the simulation:
the running time is exponential. So, we turn our attention to polynomial time
bounded probabilistic machines, that is, probabilistic machines whose running
times are polynomials. Every computation of such a machine halts in exactly
p(n) steps on inputs of length n. By PP87 we denote the class of languages
accepted by polynomial time probabilistic Turing machines. It is easy to see
that (cf. Balc´azar, D´ıaz, Gabarr´o [11]):
NP ⊆PP ⊆PSPACE.
Let BPP88 denote the class of languages accepted by polynomial time prob-
abilistic Turing machines whose error probability is bounded from above by
some constant ϵ > 0. It is clear that
BPP ⊆PP,
but it’s open whether the inclusion is proper. The importance of BPP consists
in the possibility of iterating the algorithm as many times as you need. The
result is the reduction of the error probability; the error can be made as small
as you need and the time cost for this reduction is reasonably low.
There is an alternative, equivalent, way to look at probabilistic Turing
machines.
The root corresponds to the machine’s starting conﬁguration,
and each node corresponds to a diﬀerent conﬁguration reachable, with non-
zero probability, from the conﬁguration represented by its parent node, in
87PP stands for Probabilistic Polynomial time.
88BPP stands for Bounded error Probabilistic Polynomial time.

266
Quantum Computing
one computation step. Each edge from parent to child is associated with a
probability, and the probability of following a particular path from the root
to a node is the product of probabilities along the edge. Probabilities depend
only on the conﬁguration associated with the parent node, regardless of the
node’s position in the tree; they have to obey the local probability condition
stating that
the sum of probabilities on edges leaving any single node is always
one.
In this way we can associate a probability to each node: simply compute
the product of probabilities assigned to the edges on the path from the root
(having probability 1) to the node. This is the probability that a computation
starting from the root reaches the node.
It may happen that at a certain level of a computation tree the same
conﬁguration duplicates, or even appears several times. The probability that
a particular conﬁguration is reached at a certain step in the computation is
the sum of the probabilities of all nodes corresponding to that conﬁguration
at that step. For example, the probability of a particular ﬁnal conﬁguration
is the sum of the probabilities of all leaf nodes corresponding to that con-
ﬁguration. So, if c1, c2, . . . ck are all distinct conﬁgurations at a certain level
of the computation tree and p1, p2, . . . pk are their “global” probabilities of
occurrence at that level, then the following global probability condition has to
be satisﬁed:
k

j=1
pj = 1.
It is easy to show that if the local probability condition is satisﬁed, then
the global probability condition is also satisﬁed.
4.20.2
Simon’s problem
As we have already mentioned, Deutsch and Jozsa [88] imagined a simple
“promise problem” that can be solved “eﬃciently” without error on a quan-
tum Turing machine, but, classically, one can perform very “ineﬃciently”.
Unfortunately, this problem, as well as some other related ones suggested
by various authors, can be eﬃciently solved by classical probabilistic Turing
machines with exponential small error probability.
In 1994 Simon [270] imagined a simple problem that can be solved in
polynomial time on a quantum Turing machine, but cannot be solved in
polynomial time on any probabilistic Turing machine. Here is Simon’s prob-
lem: Suppose we are given a function f : {0, 1}n →{0, 1}n and we are
promised that either f is one-to-one or there exists a non-trivial n-bit string
s such that for all distinct n-bit strings x, x′ we have
f(x) = f(x′) if and only if x′ = x ⊕s,

Quantum Complexity
267
that is, f(x) = f(x′) if and only if the bits of x and x′ diﬀer in exactly those
positions where the bits of s are 1.
We wish to determine which of these two conditions holds for f, and, in
the second case, to ﬁnd s.
Let a = (a1, . . . , an), b = (b1, . . . , bn) be two n-bit strings regarded as
n-bit vectors in the Zn
2 = {0, 1}n. We say that a < b in case
n

i=1
ai2i−1 >
n

i=1
bi2i−1.
The inner product of a, b is
a · b =
n

i=1
aibi (mod 2).
A set B ⊂{0, 1}n is linearly independent if for every b ∈B and every subset
B′ ⊂B \ {b} we have
(0, . . . , 0) ̸=
$
b′∈B′
b′.
Recall the quantum gate array Uf and the Walsh–Hadamard transforma-
tion W:
Uf(|x, y⟩) = |x, f(x) ⊕y⟩,
W(|x⟩) =
1
√
2n

y∈{0,1}n
(−1)x·y|y⟩.
Simon’s solution is the following. Use two quantum registers, both with
n qubits and the initial states |0, . . . , 0⟩, |0, . . . , 0⟩. Then, apply the Walsh–
Hdamard transformation on the ﬁrst register, then apply Uf, then again the
Walsh–Hdamard transformation on the ﬁrst register, and, ﬁnally, observe the
resulting pair of states to get a pair (y, f(x)). More formally, the algorithm
can be presented in the following form:
|(0, . . . , 0), (0, . . . , 0)⟩
W
→
1
√
2n

x∈{0,1}n
|x, (0, . . . , 0)⟩
Uf
→
1
√
2n

x∈{0,1}n
|x, f(x)⟩
W
→
	
1
√
2n

2

x,y∈{0,1}n
(−1)x·y|y, f(x)⟩
= 1
2n

x,y∈{0,1}n
(−1)x·y|y, f(x)⟩.

268
Quantum Computing
If f is one-to-one, then f is bijective so all possible states |y, f(x)⟩are distinct,
so the result of applying the above scheme n −1 times consists of n −1 pairs
(y1, f(x1)), . . . , (yn−1, f(xn−1)), uniformly distributed over all pairs (y, f(x)).
However, if there is a non-trivial n-bit string s such that f(x) =
f(x′) if and only if x′ = x ⊕s, for all x ̸= x′, then for each y, x we have
|y, f(x)⟩= |y, f(x ⊕s)⟩.
In this case we have:
1
2n

x,y∈{0,1}n
(−1)x·y|y, f(x)⟩
=
1
2n+1

x,y∈{0,1}n

(−1)x·y + (−1)(x⊕s)·y
|y, f(x)⟩
=
1
2n+1

x,y∈{0,1}n
(−1)x·y (1 + (−1)s·y) |y, f(x)⟩
Consequently, after n −1 independent applications of the scheme we will
get n −1 independent pairs (y1, f(x1)), . . . , (yn−1, f(xn−1)), such that for
every 1 ≤i ≤n −1, yi · s ≡0 (mod 2).
In both cases, after n−1 repetitions of the scheme we will get n−1 vectors
yi, i = 1, 2, . . . n −1. There are two possibilities according to whether the set
{yi | 1 ≤i ≤n −1} is linearly independent or not.
In the ﬁrst case, the linear system of n −1 equations yi · s = 0 can be
solved in Z2 to obtain s. There are two cases:
• if f is one-to-one, then the solution s is irrelevant.
• if f is two-to-one, then the solution s is the one required by Simon’s
problem.
To distinguish between these two cases we need to compute and compare the
values of f(0, . . . , 0) and f(s).
In the second case, that is when the set {yi | 1 ≤i ≤n−1} is not linearly
independent, we have to repeat the whole process. However, with probability
at least 1
4, the set of vectors {yi | 1 ≤i ≤n −1} is linearly independent.89
So, after an expected O(n) repetitions, suﬃciently many linearly independent
vectors yi will have been collected such that s is uniquely determined.
Consequently, the time of the computation is O(nTf(n) + G(n)), where
Tf(n) is the time required to compute f on an n-bit string and G(n) is the
89If u is a non-zero n-bit vector, then by choosing n −1 uniformly distributed n-bit
vectors yi, 1 ≤i ≤n, such that yi · u ≡0 (mod 2) we obtain a linearly independent set of
vectors {yi | 1 ≤i ≤n} with probability at least 1
4 .

Quantum Complexity
269
time required to solve an n×n linear system of equations in Z2. Can we do it
equally better with a probabilistic Turing machine? The answer is negative
as Simon [270] has shown. Let us construct an oracle, corresponding to a
“hard probability distribution”, as follows: for each n uniformly generated
two bit strings s(n) ∈{0, 1}n, b(n) ∈{0, 1}. If b(n) = 0, then the function
fn : {0, 1}n →{0, 1}n is uniformly generated from the set of all permutations
of {0, 1}n; if b(n) = 1, then fn is uniformly generated from the set of two-
to-one functions such that fn(x) = fn(x ⊕s(n)), for all n-bit strings x.
Then, any probabilistic Turing machine that queries the above oracle no
more than 2−n/4 times cannot correctly guess b(n) with probability greater
than 1
2 + 2−n/2. So,
any probabilistic Turing machine needs an exponential time to
solve Simon’s problem on inﬁnitely many inputs.
A recent paper by Hemaspaandra, Hemaspaandra and Zimand [130] shows
that a variant of the Simon’s problem that is still solvable in quantum poly-
nomial time needs on a classical machine an exponential time on almost every
input.
Simon’s quantum algorithm works in polynomial time in the expected
time90 and there is no upper bound for the time required in the worst
case. Brassard and Høyer [37] improved Simon’s algorithm (using Grover’s
database search algorithm) and showed that Simon’s problem can be solved
in polynomial time in the worst case. We will follow Mihara and Sung [191] to
present a simpler polynomial time algorithm (in the worst case) for Simon’s
problem.
First let f be as in Simon’s problem and let g be a non-zero n-bit vector
diﬀerent from s. The following quantum algorithm returns an n-bit vector y
such that
s · y = 0, and g · y = 1.
The idea is to use Simon’s algorithm with UF instead of Uf, where F is an
appropriately constructed function. To deﬁne F we construct two functions
φg(x) = max{f(x), f(x ⊕g)},
ψg(x) =

0,
if f(x) > f(x ⊕g),
1,
otherwise,
and we put
F(x, y) = (−1)ψg(x)|x, φg(x) ⊕y⟩.
It is seen that φg(x) = φg(x ⊕g), and f(x) ̸= f(x ⊕g) if and only if
ψg(x) ̸= ψg(x ⊕g).
The algorithm is the following:
90Recall, we need an expected O(n) repetitions to collect the linearly independent vectors
yi.

270
Quantum Computing
|(0, . . . , 0), (0, . . . , 0)⟩
W
→
1
√
2n

x∈{0,1}n
|x, (0, . . . , 0)⟩
UF
→
1
√
2n

x∈{0,1}n
(−1)ψg(x)|y, φg(x)⟩
W
→
	
1
√
2n

2

x,y∈{0,1}n
(−1)x·y(−1)ψg(x)|y, φg(x)⟩
=
1
2n+2

x,y∈{0,1}n
(−1)x·y(−1)ψg(x)((−1)x·y
+ (−1)(x⊕s)·y −(−1)(x⊕g)·y
+ (−1)(x⊕g⊕s)·y)|y, φg(x)⟩
=
1
2n+2

x,y∈{0,1}n
(−1)x·y(1 + (−1)s·y
−(−1)g·y −(−1)(g⊕s)·y)|y, φg(x)⟩
=
1
2n+2

x,y∈{0,1}n
(−1)x·y(1 + (−1)s·y)(1 −
(−1)g·y)|y, φg(x)⟩.
Hence, (1 + (−1)s·y)(1 −(−1)g·y) ̸= 0 if and only if s · y = 0 and g · y = 1.
So, by measuring the ﬁrst register we can obtain y such that s · y = 0 and
g · y = 1. The total running time of the algorithm is O(n + Tf(n)), where
Tf(n) is the time required to compute f on an n-bit string.
We could solve Simon’s problem if one could produce enough y’s such
that s · y = 0. The above algorithm shows how to produce such a y when we
use a diﬀerent, non-zero g. We next show that with certitude we can ﬁnd a
g for obtaining y. To this aim we need two simple mathematical facts:
(1) There exists a polynomial time algorithm that for every linearly inde-
pendent set B ⊂{0, 1}n, returns a non-zero n-bit string g ∈{0, 1}n
such that g · y = 0, for every y ∈B. If B has exactly n −1 elements,
then g is unique.
(2) Let B be a linearly independent set and g ∈{0, 1}n such that g · y = 0,
for every y ∈B. Then, for every y′ such that g ·y′ = 1, the set B ∪{y′}
is linearly independent.
Using the above facts we can write the following quantum algorithm:
Put B = ∅.
Select a non-zero n-bit vector g.

Quantum Complexity
271
Repeat the following steps while g ̸= s:
Use the above quantum algorithm to find a non-zero
n-bit vector y such that s · y = 0
and g · y = 1 and put B = B ∪{y}.
Use 1) to find a non-zero g′ such that g′ · y = 0,
for every y ∈B and set g = g′.
Return g.
In view of (2), the set B is linearly independent, therefore we can ﬁnd
a non-zero s such that s · y = 0, for all y ∈B. The running time of the
algorithm is O(n2 + nTf(n) + nG(n)) in the worst case: here Tf(n) is the
time required to compute f on an n-bit string and G(n) is the time required
to produce g as in (1), a polynomial in n.
4.20.3
Complexity
A computation on a quantum Turing machine QTM as described by Deutsch
[84] can be represented by a tree in much the same way we did it for proba-
bilistic computation. The major change required by quantum mechanics is to
replace probabilities with amplitudes. For complexity issues it is enough to
consider only real amplitudes in the interval [−1, 1]. The amplitude of a node
is the product of the amplitudes of the edges on the path from the root to that
node. The amplitude of a conﬁguration at any step in the computation is the
sum of the amplitudes of all nodes corresponding to that conﬁguration at the
level in the tree corresponding to that step. When an observation is made,
the probability associated with each conﬁguration is not the conﬁguration’s
amplitude in the superposition, but rather the squared magnitude of its am-
plitude. Hence, the probability of a conﬁguration at any step is the square of
its amplitude. For example, the probability of a conﬁguration is the square of
the sum of the amplitudes of all leaf nodes corresponding to that conﬁgura-
tion. Some speciﬁc properties follow. For instance, a particular conﬁguration
c may correspond to two leaf nodes with conjugate amplitudes, α and −α,
and the probability of c being the ﬁnal conﬁguration will be zero. Still the
parent nodes of these two nodes might both have non-zero probabilities.
The computation produces c with probability α2 if the conﬁguration of
one leaf is diﬀerent from the other. If both leaves have amplitude α, then
the probability of c being the ﬁnal conﬁguration is 4α2 (not just 2α2). This
mutual inﬂuence between branches is a consequence of quantum interference.
A quantum computation tree must obey the property that the sum of the
probabilities of conﬁgurations at any level add up to one. Note that it is not
enough to ask that for each node the sum of the squares of the amplitudes
on edges leading to its children is one! Computation steps should be unitary,
so reversible. A quantum computation results, in just one single step, in a
superposition of all branches of its tree simultaneously.

272
Quantum Computing
A classical probabilistic computation tree has to be well-deﬁned and local,
with probablities adding up to one. A quantum computation tree has to be
well-deﬁned, local, and unitary.
Quantum variations of time and complexity classes have been intensively
studied. For time complexity, one-tape multitrack QTM are considered; for
space complexity, oﬀ-line multitape QTM with one-way, read-only, input
tape, a working tape, and one-way, write-only, out-tape are used. For time
complexity it is enough to consider computations in which the measurement is
done only after the machine halts; to study space complexity, a measurement
is done each time a symbol is written on the output tape. Many variations
of models and approaches have been considered; see Gruska [119].
There are no essential diﬀerences between conventional and quantum
computation as concerns the space eﬃciency. Things are diﬀerent for time
complexity. Quantum versions of classes P and BPP are classes EQP and
BQP. The class BQP, which is regarded as the class of languages (problems)
that can be decided eﬃciently on QTMs, is deﬁned as the family of languages
L such that there exists a QTM that can decide, with probability at least
2/3, for each string x whether x ∈L.
The following basic relations hold true:
P ⊆EQP ⊆BQP,
and
BPP ⊆BQP ⊆PP ⊆PSPACE.
It is an open problem to decide which inclusion is proper. Quantum com-
plexity classes are intimately related to conventional complexity classes; in
particular, showing that QTMs are more powerful than PTMs needs a break-
through result in classical complexity theory.
4.21
Quantum Cryptography
Quantum systems can be used to achieve cryptographic tasks, such as secret
(secure) communication. In cryptography (see, for example, Salomaa [263]) it
is very diﬃcult, if not impossible, to prove by experiment that a cryptographic
protocol is secure: who knows whether an eavesdropper (spy, competitor)
managed to beat the system? For example, the bit-commitment method,
thought for a while to be secure through quantum methods, was proven to
be insecure, cf. Mayers [186] and Lo, Chau [173].91 The only conﬁdence one
can hope to achieve relies on mathematical arguments, the so-called proofs
of security.
91Cheating is possible through a clever use of quantum entanglement.

Quantum Cryptography
273
As usual in quantum mechanics scenarios, Alice and Bob are widely sep-
arated and wish to communicate. They are connected by an ordinary bi-
directional open channel and a uni-directional quantum channel, directed
from Alice to Bob. The quantum channel allows Alice to send single qubits
(e.g. photons) to Bob who can measure their quantum state. An eavesdrop-
per, Eve, is able to intercept and measure the qubits, then pass them on to
Bob.
Given two orthonormal bases
{| ↑⟩, | →⟩}, {| ↖⟩, | ↗⟩},
where | ↖⟩=
1
√
2(| ↑⟩−| →⟩) and | ↗⟩=
1
√
2(| ↑⟩+ | →⟩), Alice and
Bob can agree to associate | ↑⟩and | ↖⟩with 0, | →⟩and | ↗⟩with 1.
For each bit, Alice pseudo-randomly uses one of these bases and Bob also
pseudo-randomly selects a basis for measuring the received qubit. After the
bits have been transmitted, Alice and Bob inform each other (using the open
channel) of the basis they used to prepare and measure each qubit. In this
way, they ﬁnd out when they used the same basis, which happens on average
half of the time, and retain only those results.
If Eve measures the qubits transmitted by Alice, then she uses the correct
basis on average half of the time. Therefore, assuming that 2n qubits are sent
by Alice, n bits are received by Bob without any disturbance. In n/2 cases,
Bob will also use the correct basis.
What about the other n qubits sent by Alice? Since
| ↖⟩=
1
√
2(| ↑⟩−| →⟩),
and
| ↗⟩=
1
√
2(| ↑⟩+ | →⟩),
the probability to ﬁnd a qubit represented by the state | ↖⟩or | ↗⟩in the
state |x⟩∈{| ↑⟩, | →⟩} is
P = ( 1
√
2)2 + ( 1
√
2)2 = 1.
Taking into account that
| ↑⟩=
1
√
2(| ↗⟩+ | ↖⟩) and | →⟩=
1
√
2(| ↗⟩−| ↖⟩),
the same result will be obtained by interchanging the two bases.
Conse-
quently, if n qubits are disturbed by Eve, then half of them are measured
by Bob with the correct basis and n/4 qubits will be projected by Bob’s
measurement back onto original state. Eve’s tentative interception disturbs

274
Quantum Computing
only a quarter of the message retained by Bob. Alice and Bob can now de-
tect Eve’s presence by pseudo-randomly choosing n/2 bits of the string and
announcing over the open channel the values they have. If they agree on all
these bits, then the probability that no eavesdropper was present is (3/4)n/2.
The undisclosed bits represent the “secret key”.
The above scenario is extremely “theoretical”, as it assumes that one
possible strategy for Eve – she may deliberately not intercept all qubits (after
all, she knows everything about quantum key distribution, doesn’t she?).
Noise may inﬂuence the trio “communication” as well. There are various
subtle methods to address these issues, but we are not going to enter into
details (see, for example, Gruska [119]).
It is important to observe that
variants of the quantum key distribution are feasible with current technology.
4.22
Information and Teleportation
4.22.1
Dense coding
Dense coding and quantum teleportation can be used to further illustrate the
use of quantum gates. The initial scenario is the same for both processes:
Alice and Bob wish to communicate. They use a pair of entangled qubits,
ψ0 =
1
√
2(|00⟩+ |11⟩).
Alice and Bob need never have communicated: just use a “facility” generating
entangled pairs of qubits, and then sending one qubit to Alice and one qubit
to Bob, which they store. Bennett and Wiesner [30] observed that Alice can
communicate two classical bits by sending Bob just a single qubit, that is,
her qubit of the entangled pair. This is the reason the method is called dense
coding.
This entangled state can be obtained from |00⟩by applying the unitary
transformation Cnot ◦(Rπ/4 ⊗I). We have
(Rπ/4 ⊗I)|00⟩= Rπ/4|0⟩⊗I|0⟩=
1
√
2(|0⟩+ |1⟩) ⊗|0⟩=
1
√
2(|00⟩+ |10⟩)
and
Cnot(Rπ/4 ⊗I)|00⟩=
1
√
2(|00⟩+ |11⟩).
Alice keeps the ﬁrst qubit and gives the second particle to Bob. So until a
particle is transmitted, only Alice can perform transformations on her qubit,
and only Bob can perform transformations on his.
Alice receives two classical bits, encoding the numbers 0, 1, 2 and 3.
Depending on this number Alice performs one of the unitary transformations
I, X, Y or Z, on her qubit of the entangled pair ψ0.
If a single qubit
transformation T acts on the ﬁrst bit and the second one is left unchanged,

Information and Teleportation
275
then the transformation T ⊗I performs on the pair ψ0.
The results for
(T ⊗I)|ψ0⟩are presented in Table 4.8.
Encoded value
T
New state (T ⊗I)|ψ0⟩
0
I
ψ0 =
1
√
2(|00⟩+ |11⟩)
1
X
ψ1 =
1
√
2(|10⟩+ |01⟩)
2
Y
ψ2 =
1
√
2(−|10⟩+ |01⟩)
3
Z
ψ3 =
1
√
2(|00⟩−|11⟩)
Table 4.8: Application of (T ⊗I)|ψ0⟩.
Alice sends her qubit to Bob who applies the controlled-NOT transfor-
mation to the pair ψi (i ∈{0, 1, 2, 3}). The resulting state is shown in Table
4.9.
Initial state
Result
ψ0 =
1
√
2(|00⟩+ |11⟩)
1
√
2(|00⟩+ |10⟩) =
1
√
2(|0⟩+ |1⟩) ⊗|0⟩
ψ1 =
1
√
2(|10⟩+ |01⟩)
1
√
2(|11⟩+ |01⟩) =
1
√
2(|0⟩+ |1⟩) ⊗|1⟩
ψ2 =
1
√
2(−|10⟩+ |01⟩)
1
√
2(−|11⟩+ |01⟩) =
1
√
2(|0⟩−|1⟩) ⊗|1⟩
ψ3 =
1
√
2(|00⟩−|11⟩)
1
√
2(|00⟩−|10⟩) =
1
√
2(|0⟩−|1⟩) ⊗|0⟩
Table 4.9: Application of controlled-NOT transformation.
Since the resulting state is not entangled, Bob can now measure the sec-
ond qubit without disturbing the quantum state. If the measurement returns
|0⟩then the encoded value was either 0 or 3. If the result of the measurement
is |1⟩, then the encoded value was either 1 or 2. Bob now applies the trans-
formation H to the ﬁrst qubit (see Table 4.10) and ﬁnally Bob measures the

276
Quantum Computing
resulting bit. Now he is able to distinguish between 0 and 3, respectively 1
and 2, as it is shown in Table 4.11.
Initial state
First bit
Result
ψ0 or ψ1
1
√
2(|0⟩+ |1⟩)
1
√
2( 1
√
2(|0⟩+ |1⟩) +
1
√
2(|0⟩−|1⟩)) = |0⟩
ψ2 or ψ3
1
√
2(|0⟩−|1⟩)
1
√
2( 1
√
2(|0⟩+ |1⟩) −
1
√
2(|0⟩−|1⟩)) = |1⟩
Table 4.10: Application of transformation H.
First measurement
Second measurement
Encoded value
|0⟩
|0⟩
0
|0⟩
|1⟩
3
|1⟩
|0⟩
1
|1⟩
|1⟩
2
Table 4.11: Final results.
Dense coding is diﬃcult to implement, so has very little practical utility
(at least at the time of writing). However, it has the advantage of revealing
in a particular simple way the relation between classical information, qubits
and the information provided by a pair of entangled qubits.
4.22.2
Quantum teleportation
It is possible to transmit qubits without sending qubits!
What does this mean? Is it a pun? According to Bennett,92 “It’s a means
by which you can take apart an unknown quantum state into classical infor-
mation and purely quantum information, send them through two separate
channels, put them back together, and get back the original quantum state”.
Teleportation, as it is commonly understood, is a ﬁctional procedure of
transferring an object from one location to another location in a three stage
process: (a) dissociation, (b) information transmission, (c) reconstitution.
The point is that, in contrast with fax transmission – where the original
object remains intact at the initial location, only an approximate replica
92A co-author of a 1993 paper that proposed quantum teleportation, [28].

Information and Teleportation
277
is constructed at destination,93 in teleportation the original object is dis-
troyed after enough information about it has been extracted, the object is
not traversing in any way the space between locations, but it is reconstructed,
as an exact replica, at the destination.
Quantum teleportation allows for the transmission of quantum informa-
tion to a distant location. The objective is to transmit the quantum state of
a particle using classical bits and reconstruct the state at the receiver.
Let’s assume that Alice wishes to communicate with Bob a single qubit
in an unkown state ϕ = a|0⟩+ b|1⟩; she wants to make the transmission
through classical channels. Alice cannot know with certainty the state as any
measurement she may perform may change it; she cannot clone it because of
the no cloning result! So, it seems that the only way to send Bob the qubit
is to send him the physical qubit, or to swap the state into another quantum
system and then send Bob that system.
As with dense coding, they use an entangled pair
ψ0 =
1
√
2(|00⟩+ |11⟩).
Alice controls the ﬁrst half of the pair and Bob controls the second one. The
input state is
ϕ ⊗ψ0
=
(a|0⟩+ b|1⟩) ⊗1
√
2(|00⟩+ |11⟩)
=
1
√
2(a|0⟩⊗|00⟩+ a|0⟩⊗|11⟩+ b|1⟩⊗|00⟩+ b|1⟩⊗|11⟩)
=
1
√
2(a|000⟩+ a|011⟩+ b|100⟩+ b|111⟩).
Alice now applies the transformation (H ⊗I ⊗I) ◦(Cnot ⊗I) to this
state. The third bit is left unchanged; only the ﬁrst two bits belong to Alice
and the rightmost one belongs to Bob.
Applying now H ⊗I ⊗I, we have:
(H ⊗I ⊗I) ◦(Cnot ⊗I)(ϕ ⊗ψ0)
=
1
√
2H ⊗I ⊗I(a|000⟩+ a|011⟩+ b|110⟩+ b|101⟩)
=
1
√
2(aH|0⟩⊗(I ⊗I)|00⟩+ aH|0⟩⊗(I ⊗I)|11⟩
+ bH|1⟩⊗(I ⊗I)|10⟩+ bH|1⟩⊗(I ⊗I)|01⟩)
=
1
√
2(a 1
√
2(|0⟩+ |1⟩) ⊗|00⟩+ a 1
√
2(|0⟩+ |1⟩) ⊗|11⟩
93At the end, two “identical” versions of the original object result.

278
Quantum Computing
+ b 1
√
2(|0⟩−|1⟩) ⊗|10⟩+ b 1
√
2(|0⟩−|1⟩) ⊗|01⟩)
=
1
2(a(|000⟩+ |100⟩+ |011⟩+ |111⟩) + b(|010⟩−|110⟩
+ |001⟩−|101⟩)).
This state may be re-written by regrouping terms:
(H ⊗I ⊗I) ◦(Cnot ⊗I)(ϕ ⊗ψ0)
=
1
2(|00⟩(a|0⟩+ b|1⟩)
+|01⟩(a|1⟩+ b|0⟩) + |10⟩(a|0⟩−b|1⟩)
+|11⟩(a|1⟩−b|0⟩)).
Alice then measures her two qubits, obtaining four possible results: |00⟩,
|01⟩, |10⟩, or |11⟩with equal probability 1/4. Depending on the result of the
measurement, the quantum state of Bob’s qubit is projected to a|0⟩+ b|1⟩,
a|1⟩+ b|0⟩, a|0⟩−b|1⟩, a|1⟩−b|0⟩, respectively. Alice sends the result of her
measurement as two classical bits to Bob. He will know what has happened,
and can apply the decoding transformation T ∈{I, X, Y, Z} to ﬁx his qubit.
Received bits
State
Transformation
Result
00
a|0⟩+ b|1⟩
I
a|0⟩+ b|1⟩
01
a|1⟩+ b|0⟩
X
a|0⟩+ b|1⟩
10
a|0⟩−b|1⟩
Z
a|0⟩+ b|1⟩
11
a|1⟩−b|0⟩
Y
a|0⟩+ b|1⟩
Table 4.12: An illustration of quantum teleportation.
The ﬁnal output state is ϕ = a|0⟩+ b|1⟩, which, as desired, is the
unknown qubit that Alice wanted to send.
Recently, important teleportation experiments have been performed in
Innsbruck and Caltech: for a recent report see Bouwmeester, Pan, Wein-
furter, Zeilinger [35]. There is a lot of controversy about the nature of quan-
tum teleportation and what criteria should be met by a successful experiment.
The following criteria for evaluating a quantum teleportation procedure have
been proposed in [35]:
• How well can it teleport any arbitrary quantum state it is intended to
teleport? (ﬁdelity of teleportation)

Computing the Uncomputable?
279
• How often does it succeed to teleport, when it is given an input state
within the set of states it is designed to teleport? (eﬃciency of telepor-
tation)
• If given a state the scheme is not intended to teleport, how well does it
reject such a state? (cross-talk rejection eﬃciency)
Let us close this section with another controversial statement of the same
Bennett: “I think it’s quite clear that anything approximating teleportation
of complex living beings, even bacteria, is so far away technologically that
it’s not really worth thinking about it.”
4.23
Computing the Uncomputable?
One fundamental result of theoretical computer science is Alan Turing’s proof
(in [289]) that it is undecidable to determine whether a computer program will
halt or not. This is formally known as the halting problem. We can restrict
our attention to Turing machines since they are equivalent in computational
power to any “conventional” computer [41, 16]. In what follows we present
an attempt to trespass the Turing barrier (see Calude, Dinneen and Svozil
[50]). The method discussed might in principle allow to “solve” the halting
problem (for another proposal see Mitchison and Josza [195]; for an approach
based on counterfactual computation (cf. [297]) see [49]). Thereby we are
well aware of the fact that for all practical purposes (Bell [20]) this goal will
remain unreachable, at least within Quantum Computing.
Assume that it is possible to design a halting qubit which indicates whether
a computation has actually reached a state associated with a halting condi-
tion. Assume further that the halting qubit starts in its non-halting state
and, since the evolution is unitary, the buildup of the amplitude is continuous
in time.
In such a case, the halting qubit acquires a halting component which is
non-zero even in ﬁnite time. Therefore, a detection of a halting computation
at small time scales is conceivable even if the associated classical computation
lasts “very” long. The price to be paid is the “very small” amplitude and,
associated with it, a corresponding chance of detection.
To be a little bit more precise we need some rudiments of algorithmic
information theory (see Chaitin [57, 60], Calude [42]). We will work with
programs with no input which produce binary strings as outputs. For any n
we denote by Pn a program of length n that halts and produces the longest
string among all outputs produced by all programs of length n that eventually
stop. We denote by Σ(n) the length of the output produced by Pn. Here Σ
is the busy beaver function [247, 58]: it grows faster than every computable
function of n. Let H be the program-size complexity, that is the length of
the smallest universal program generating a binary string.

280
Quantum Computing
Assume that any program which halts requires a running time at least
proportional to the length of its output. If an n-bit program p halts, then
the time t it takes to halt satisﬁes H(t) ≤n + c. So if p has run for time T
without halting, and T has the property that t ≥T =⇒H(t) > n + c, then
p will never halt. This shows that the running times of the programs in the
sequence P1, P2, . . . Pn, . . . grow faster than any computable function.
We are now ready to present the argument. Let us assume the halting
qubit is represented by
|Halt⟩= ch(t)|h⟩+ cn(t)|n⟩,
where |h⟩, |n⟩represent the halting state and non-halting state and ch(t),
cn(t) are time dependent amplitudes thereof, respectively.
Initially, let |ch(t)| = |cn(t)| −1 = 0. As a worst-case scenario derived
from the above analysis, for a linear buildup of the amplitude we obtain
|ch(t)|2 ∝(Σ(H(n) + O(1)))−1.
The setup of a detection of |Halt⟩is a simple transmission measurement
of the halting qubit. Although the buildup may be very slow, there is a non-
vanishing chance to obtain a solution of the halting problem in ﬁnite time.
Of course, the solution is probabilistic (one can argue that all mathematical
proofs or computer programs are ultimately probabilistic, see Davis [80], De
Millo, Lipton and Perlis [83]), but goes beyond the capability of any classical
computation: even the best probabilistic algorithms are not able to achieve
this computational power (by a classical result [82], probabilistic algorithms
are equivalent to Turing machines).
Let us ﬁnally notice that by virtue of the same information-theoretic
argument, the possibility of time-travel (see, for example, Nahin [197]) would
not solve the halting problem unless one could travel back and at a pace
exceeding the growth of any computable function.
4.24
Bibliographical Notes
We made no attempt to be comprehensive in any sense of the word.
Feynman [98] contains a reprint of the lecture “Quantum Mechanical
Computers” [96] which began the ﬁeld of Quantum Computing; an impor-
tant continuation is [134] which includes papers written by many authors
who pioneered the ﬁeld.
Williams and Clearwater’s book [296] is the ﬁrst monograph on the sub-
ject; Mathematica programs simulate a few quantum algorithms including
Shor’s algorithm. Other books of interest are Milburn [193] and Berman,
Doolen, Mainieri, Tsifrinovich [31]. Calude, Dinneen and Casti [47], Brooks
[39], Lo, Spiller, Popescu [174], Macchiavello, Palma, Zeilinger [176] contain
recent papers in the subject. The most comprehensive treatment of Quantum
Computing is Gruska [119].

Bibliographical Notes
281
The articles referenced in this chapter, and many more, have been an-
nounced at the Los Alamos preprint server:
http://xxx.lanl.gov/archive/quant-ph.
Most papers on Quantum Computing can be found on the web, for exam-
ple at the Caltech-MIT-USC Quantum Information and Computation Project
http://theory.caltech.edu/ quic/index.html,
the Centre for Quantum Computation at Oxford University
http://www.qubit.org/,
the Quantum Computation-Cryptography at Los Alamos
http://qso.lanl.gov/qc/,
the Southwest Quantum Information and Technology (SQuInT) Network
http://www.squint.org/.
These sites have a fair amount of information plus lots of links to other sites
of interest.
¨Omer’s
Quantum
Computation
Language
can
be
found
at
http://tph.tuwien.ac.at/ oemer/. Senko Corporation oﬀers a Quantum
Computing Simulator: http://www.senko-corp.co.jp.

282
Quantum Computing

Chapter 5
Final Remarks
The idea of unconventional Computing has ﬁred many imaginations and
many researchers regard it as a new revolution in information processing.
The typical advertisement for unconventional Computing includes two items:
(a) miniaturization, as a basis for massive parallelism and huge data bases,
(b) Moore’s law.1 The fact that some day the gate is going to be too small for
the carrier to tunnel through does not imply that the only solution will come
from biology or quantum mechanics. But even the hypothesis that computa-
tional components will get smaller and smaller is not indisputable! According
to Landauer [162] miniaturization is slowing down for economic not physical
reasons (of course, the semiconductor industry will not remain still, uncon-
ventional Computing has to ﬁght a moving target) and attention could move
together new directions, e.g., three dimensional integration. What will hap-
pen in 5, 10 or 20 years? People have drastically diﬀerent opinions. For
example, Dennis Bushill, chief scientist at Langley Research Centre of NASA
is cited2 on the front page of the site http://www.eiqc.org/ as saying that
NASA are now planning on the basis that Quantum Computing
will be mainstream within ﬁve years.
In February 1999, during the constituting meeting of the European Molec-
ular Computing Consortium3, Erik Winfree, one of the ﬁrst computer scien-
tists who have completed a PhD dissertation in DNA Computing, declared
that he expects that in a few years real life applications will be reported
– maybe based on his two-dimensional self-assembly procedure. The most
promising area is cryptography, where some degree of error is acceptable.4
For Landauer [162],
1The energetic eﬃciency is also mentioned with respect to bio-computers, but this
criterion is not an urgent one.
2On 28 November 1999.
3Leiden, The Netherlands.
4The problem with cryptography is that we may learn about applications only . . . a
dozen of years later, when the matter is no longer classiﬁed. . .
283

284
Final Remarks
. . . the thrust of much of what has been discussed in quantum com-
putation is pet inventions.. . . I think that there are important ques-
tions that get suppressed by premature pitching of what we might
be able to do in ﬁve or ten years with various approaches.
We tend to agree with the last opinion. For example, Quantum Com-
puting is not going to replace classical Computing for similar reasons that
quantum physics does not replace classical physics: no one takes into con-
sideration Heisenberg in order to build a house, and cars are not mended by
quantum mechanical garages. If large quantum/DNA/membrane computers
are ever constructed, they will probably be used to address just special tasks
which beneﬁt from quantum/DNA/membrane information processing, in an
intertwined way with conventional methods.
There are several more solid reasons to be excited about unconventional
Computing, not directly dealing with applications.
We mention some of
them: (a) new methods to design algorithms for conventional computers or
hybrid computers (for comparison, remember the success of neural networks
and genetic algorithms, both of them inspired from nature/life and imple-
mented in silicon), (b) new and insightful ways to think about the funda-
mental laws of physics, (c) new measures of information, (d) new methods of
data encryption and security, (e) the challenge of Turing barrier, (f) the cul-
tural quest to bring together Turing machines, information, biology, number
theory and quantum physics. At least from the point of view of fundamental
research, of general human knowledge, unconventional Computing is already
a successful adventure of the spirit.

Bibliography
[1] L. M. Adleman, Molecular computation of solutions to combinatorial
problems, Science, 226 (November 1994), 1021–1024.
[2] L. M. Adleman, On constructing a molecular computer, in [170], 1–22.
[3] L. M. Adleman, P. W. K. Rothemund, S. Roweiss, E. Winfree, On
applying molecular computation to the Data Encryption Standard, in
[18], 31–44.
[4] A. V. Aho, J. D. Ullman, The Theory of Parsing, Translation, and
Compiling, Prentice Hall, Englewood Cliﬀs, N.J., Vol. I: 1971, Vol. II:
1973.
[5] D. Z. Albert, On quantum-mechanical automata, Physical Letters, A
98 (1983), 249–252.
[6] B. Alberts, D. Bray, J. Lewis, M. Raﬀ, K. Roberts, J. D. Watson,
Molecular Biology of the Cell, 3rd ed., Garland Publishing, New York,
1994.
[7] R. B. Altman, A. K. Dunker, L. Hunter, T. E. Klein, eds., Paciﬁc
Symp. on Biocomputing, Hawaii, 1998, World Sci., Singapore, 1998.
[8] M. Amos, DNA Computing, PhD Thesis, Univ. of Warwick, Dept. of
Computer Sci., 1997.
[9] M. Amos, A. Gibbons, D. Hodgson, Error-resistant implementation of
DNA computations, in L. F. Landweber, E. B. Baum, eds., DNA Based
Computers II, American Mathematical Society, 1998, 151–161.
[10] M. Amos, S. Wilson, D. A. Hodgson, G. Owenson, A. Gibbons, Prac-
tical implementation of DNA computations, in [47], 1–18.
[11] J. L. Balc´azar, J. D´ıaz, J. Gabarr´o, Structural Complexity I, Springer-
Verlag, Berlin, 1988.
285

286
Bibliography
[12] J. P. Banˆatre, A. Coutant, D. Le Metayer, A parallel machine for multi-
set transformation and its programming style, Future Generation Com-
puter Systems, 4 (1988), 133–144.
[13] A. Barenco, C. H. Bennett, R. Cleve, D. P. DiVincenzo, N. Margoluous,
P. W. Schnor, T. Sleator, J. A. Smolin, H. Weinfurter, Elementary gates
of quantum computation, Physical Review, A 52 (1995), 3457–3467.
[14] A. Barenco, Quantum Computation, PhD Thesis, Oxford University,
1996.
[15] J.
M.
Barreiro,
J.
Rodrigo,
A.
Rodriguez-Paton,
Evolutionary
biomolecular computing, Romanian J. of Information Sci. and Tech-
nology, 1, 4 (1998), 289–294.
[16] J. Barrow, Impossibility–The Limits of Science and the Science of Lim-
its, Oxford University Press, Oxford, 1998.
[17] E. B. Baum, A DNA associative memory potentially larger than the
brain, in [170], 23–28.
[18] E. Baum, D. Boneh, P. Kaplan, R. Lipton, J. Reif, N. Seeman, eds.,
DNA Based Computers, Proc. of the Second Annual Meeting, Prince-
ton, 1996.
[19] D. Beckman, A. N. Shari, S. Devabhaktuni, J. Preskill, Eﬃcient net-
works for quantum factoring, Physical Review A, 54 (1996), 1034–1063.
[20] J. S. Bell, On the Einstein Podolsky Rosen paradox, Physics, 1 (1964),
195–200. Reprinted in [291], 403–408, and in [21], 14–21.
[21] J. S. Bell, Speakable and Unspeakable in Quantum Mechanics, Cam-
bridge University Press, Cambridge, 1987.
[22] P. Benioﬀ, The computer as a physical system: A microscopic quantum
mechanical Hamiltonian model of computers as represented by Turing
machines, Journal of Statistical Physics, 22 (1980), 563–591.
[23] C. H. Bennett, Logical reversibility of computation, IBM J. Res. Dev.,
17 (1973), 525–532.
[24] C. H. Bennett, The thermodynamics of computation, International
Journal of Theoretical Physics, 21 (1982), 905–940.
[25] C. H. Bennett, Demons, engines and the second law, Scientiﬁc Ameri-
can, November (1987), 108–116.
[26] C. H. Bennett, Notes on the history of reversible computation, IBM J.
Res. Dev., 32 (1988), 281–288.

Bibliography
287
[27] C. H. Bennett, Time/space trade-oﬀs for reversible computation, SIAM
J. on Computing, 18 (1989), 766–776.
[28] C. H. Bennett, G. Brassard, C. Crepeau, R. Jozsa, A. Peres, W. K.
Wootters, Teleporting an unknown quantum state via dual classical and
Einstein-Podolsky-Rosen channels, Phys. Rev. Lett., 70 (1993), 1895–
1898.
[29] C. H. Bennett, R. Landauer, The fundamental physical limits of com-
putation, Scientiﬁc American, July (1985), 48–56.
[30] C. H. Bennett, S. J. Wiesner, Communication via one- and two-particle
operations on Einstein-Podolsky-Rosen states, Phys. Rev. Lett., 69
(1992), 2881–2884.
[31] G. P. Berman, G. D. Doolen, R. Mainieri, V. I. Tsifrinovich, Introduc-
tion to Quantum Computers, World Scientiﬁc, Singapore, 1993.
[32] G. Berry, G. Boudol, The chemical abstract machine, Theoretical Com-
puter Sci., 96 (1992), 217–248.
[33] G. Birkhoﬀ, J. von Neumann, The logic of quantum mechanics, Annals
of Mathematics, 37(4) (1936), 823–843.
[34] D. Boneh, C. Dunworth, R. J. Lipton, Breaking DES using a molecular
computing, in [170], 37–66.
[35] D. Bouwmeester, J.-W. Pan, H. Weinfurter, A. Zeilinger, High-ﬁdelity
teleportation of independent qubits, quant-ph/9910043.
[36] G. Brassard, Searching a quantum phone book, Science, 31 January
(1997), 627–628.
[37] G. Brassard, P. Høyer, An exact quantum polynomial-time algorithm
for Simon’s problem, Proceedings of Israeli Symposium on Theory of
Computing and System, 1997, 12–23.
[38] W. Brauer, Automatentheorie, Teubner, Stuttgart, 1984.
[39] M. Brooks, ed., Quantum Computing and Communications, Springer-
Verlag, Berlin, 1999.
[40] V. Buˇzek, S. L. Braunstein, M. Hillery, D. Bruβ, Quantum copying: a
network, Physical Review A, 56,5 (1997), 3446–3452.
[41] C. Calude, Theories of Computational Complexity, North-Holland, Am-
sterdam, 1988.
[42] C. Calude, Information and Randomness. An Algorithmic Perspective,
Springer-Verlag, Berlin, 1994.

288
Bibliography
[43] C. S. Calude, E. Calude, K. Svozil, S. Yu, Physical versus computa-
tional complementarity I, International Journal of Theoretical Physics,
36, 7 (1997), 1495–1523.
[44] C. S. Calude, E. Calude, K. Svozil. Quantum correlations conundrum:
An automaton-theoretic approach, in G. P˘aun, ed., Recent Topics
in Mathematical and Computational Linguistics, Romanian Academy
Publishing Company, Bucharest, 2000, in press.
[45] C. S. Calude, J. L. Casti, Parallel thinking, Nature 392, 9 April (1998),
549–551.
[46] C. S. Calude, J. L. Casti, Silicon, molecules, or photons? Complexity,
4, 1 (1998), 13.
[47] C. S. Calude, J. Casti, M. J. Dinneen, eds., Unconventional Models of
Computation, Springer-Verlag, Singapore, 1998.
[48] C. S. Calude, G. J. Chaitin, Randomness everywhere, Nature, 400, 22
July (1999), 319–320.
[49] C.S.
Calude,
M.
J.
Dinneen,
K.
Svozil,
Counterfactual
Ef-
fect,
the
Halting
Problem,
and
the
Busy
Beaver
Function
(Preliminary
Version),
CDMTCS
Research
Report
107,
1999,
(www.cs.auckland.ac.nz/CDMTCS).
[50] C.
S.
Calude,
M.
J.
Dinneen,
K.
Svozil,
Reﬂections
on
Quantum
Computing,
CDMTCS
Research
Report
130,
2000,
(www.cs.auckland.ac.nz/CDMTCS).
[51] C. S. Calude, P. H. Hertling, K. Svozil, Embedding quantum universes
into classical ones, Foundations of Physics, 29, 3 (1999), 349–379.
[52] C. S. Calude, J. Hromkovic, Complexity: A language-theoretic point
of view, in [258], Vol. 2, 1–60.
[53] C. S. Calude, M. Lipponen, Computational complementarity and soﬁc
shifts, in X. Lin, ed., Theory of Computing 98, Proceedings of the
4th Australasian Theory Symposium, CATS’98, Springer-Verlag, Sin-
gapore, 1998, 277–290.
[54] C. S. Calude, Gh. P˘aun, Global syntax and semantics for recursively
enumerable languages, Fundamenta Informaticae, 4, 2 (1981), 245–254.
[55] C. S. Calude, F. W. Meyerstein, Is the universe lawful? Chaos, Solitons
& Fractals, 10, 6 (1999), 1075–1084.
[56] E. Calude, Automata-Theoretic Models for Computational Complemen-
tarity, PhD Thesis, The Univ. of Auckland, 1998.

Bibliography
289
[57] G. J. Chaitin, Information, Randomness and Incompleteness, Papers
on Algorithmic Information Theory, World Scientiﬁc, Singapore, 1987.
(2nd ed., 1990).
[58] G. J. Chaitin, A. Arslanov, C. Calude, Program-size complexity com-
putes the halting problem, Bulletin of the EATCS 57 (1995), 198–200.
[59] G. J. Chaitin, The Limits of Mathematics, Springer-Verlag, Singapore,
1997.
[60] G. J. Chaitin, The Unknowable, Springer-Verlag, Singapore, 1999.
[61] J. F. Clauser, A. Shimony, Bell’s theorem: experimental tests and im-
plications, Rep. Prog. Phys., 41 (1978), 1821–1927.
[62] D. W. Cohen, An Introduction to Hilbert Space and Quantum Logic,
Springer-Verlag, New York, 1989.
[63] M. Conrad, Information processing in molecular systems, Currents in
Modern Biology, 5 (1972), 1–14.
[64] M. Conrad, The price of programmability, in R. Herken, ed., The Uni-
versal Turing Machine: A Half-Century Survey, Kammerer and Un-
verzagt, Hamburg, 1988, 285–307.
[65] J. H. Conway, Regular Algebra and Finite Machines, Chapman and
Hall, London, 1971.
[66] J. W. Cooley, J. W. Tukey, An algorithm for the machine calculation
of complex Fourier series, Math. Comput., 19 (1965), 297–301.
[67] D. Coppersmith, An approximate Fourier transform useful in quantum
factoring, IBM Research Report No. RC19642, 1994.
[68] E. Csuhaj-Varju, J. Dassow, On cooperating distributed grammar sys-
tems, J. Inf. Process. Cybern., EIK, 26, 1–2 (1990), 49–63.
[69] E. Csuhaj-Varju, J. Dassow, J. Kelemen, Gh. P˘aun, Grammar Systems.
A Grammatical Approach to Distribution and Cooperation, Gordon and
Breach, London, 1994.
[70] E. Csuhaj-Varju, L. Kari, Gh. P˘aun, Test tube distributed systems
based on splicing, Computers and AI, 15, 2–3 (1996), 211–232.
[71] E. Csuhaj-Varju, A. Salomaa, Networks of language processors. Parallel
communicating systems, Bulletin of the EATCS, 66 (October 1998),
122–138.
[72] K. Culik II, A purely homomorphic characterization of recursively enu-
merable sets, Journal of the ACM, 26 (1979), 345–350.

290
Bibliography
[73] K. Culik II, T. Harju, Splicing semigroups of dominoes and DNA, Dis-
crete Appl. Math., 31 (1991), 261–277.
[74] J. Dassow, V. Mitrana, Splicing grammar systems, Computers and AI,
15, 2–3 (1996), 109–122.
[75] J. Dassow, Gh. P˘aun, Regulated Rewriting in Formal Language Theory,
Springer-Verlag, Berlin, 1989.
[76] J. Dassow, Gh. P˘aun, On the power of membrane computing, J. Univer-
sal Computer Sci., 5, 2 (1999), 33–49, (http://www.iicm.edu/jucs).
[77] J. Dassow, Gh. P˘aun, Concentration controlled P systems, submitted,
1999.
[78] M. Davis, Computability and Unsolvability, McGraw-Hill, New York,
1958.
[79] M. D. Davis, E. J. Weyuker, Computability, Complexity, and Lan-
guages, Academic Press, New York, 1983.
[80] P. J. Davis, Fidelity in mathematical discourse: Is one and one really
two? Amer. Math. Monthly, 79 (1972), 252–263.
[81] M. T. Dawson, R. Powell, F. Gannon, Gene Technology, BIOS Scientiﬁc
Publishers, Oxford, 1996.
[82] K. De Leeuw, E. F. Moore, C. E. Shannon, N. Shapiro. Computabil-
ity by probabilistic machines, in C. E. Shannon, J. McCarthy (eds.),
Automata Studies, Princeton University Press, Princeton, N.J., 1956,
183–212.
[83] R. De Millo, R. Lipton, A. Perlis, Social processes and proofs of theo-
rems and programs, Comm. ACM, 22 (1979), 271–280.
[84] D. Deutsch, Quantum theory, the Church-Turing principle and the uni-
versal quantum computer, Proceedings of the Royal Society London, A
400 (1985), 97–119.
[85] D. Deutsch, Quantum computation, Physics World, 5 (1992), 57–61.
[86] D. Deutsch, The Fabric of Reality, Allen Lane, Penguin Press, 1997.
[87] D. Deutsch, A. Barenco, A. Ekert, Universality in quantum computa-
tion, Proceedings of the Royal Society of London, A 449 (1995), 669–
677.
[88] D. Deutsch, R. Josza, Rapid solution of problems by quantum compu-
tation, Proceedings of the Royal Society London, A 439 (1992), 553–558.

Bibliography
291
[89] D. Dieks, Communication by EPR devices, Physical Letters A, 92
(1982), 271–272.
[90] P. Dirac, The Principles of Quantum Mechanics, 4th ed., Oxford Uni-
versity Press, Oxford, 1958.
[91] A. Einstein, B. Podolsky, N. Rosen, Can quantum-mechanical descrip-
tion of physical reality be considered complete? Physical Review, 47
(1935), 777–780. Reprinted in [291], 138–141.
[92] J. Engelfriet, Reverse twin-shuﬄes, Bulletin of the EATCS, 60 (1996),
144.
[93] J. Engelfriet, G. Rozenberg, Fixed point languages, equality languages,
and representations of recursively enumerable languages, Journal of the
ACM, 27 (1980), 499–518.
[94] A. Ekert, R. Jozsa, Shor’s quantum algorithm for factoring numbers,
Rev. Modern Physics 68 (3), (1996), 733–753.
[95] R. P. Feynman, Simulating physics with computers, International Jour-
nal of Theoretical Physics, 11 (1985), 11–20.
[96] R. P. Feynman, Quantum mechanical computers, Optics News 21
(1982), 467–488.
[97] R. P. Feynman, in D. H. Gilbert, ed., Miniaturization, Reinhold, New
York, 1961, 282–296.
[98] Feynman Lectures on Computation, J. G. Hey and R. W. Allen, eds.,
Addison-Wesley, Reading, Massachusetts, 1996.
[99] D. Finkelstein, S. R. Finkelstein, Computational complementarity, In-
ternational Journal of Theoretical Physics, 22, 8 (1983), 753–779.
[100] D. J. Foulis, C. H. Randall, Operational statistics. I. Basic concepts,
Journal of Mathematical Physics, 13 (1972), 1667–1675.
[101] E. Fredkin, T. Toﬀoli, Conservative logic, International Journal of The-
oretical Physics, 21 (1982), 219–253.
[102] R. Freund, Generalized P systems, Proc. of FCT’99, Ia¸si, Romania (G.
Ciobanu, Gh. P˘aun, eds.), Lecture Notes in Computer Science, 1684,
Springer-Verlag, 1999, 281–292.
[103] R.
Freund,
Generalized
P
systems
with
splicing
and
cut-
ting/recombination, Grammars, 2, 3 (1999), 189–199
[104] R. Freund, L. Kari, Gh. P˘aun, DNA computing based on splicing. The
existence of universal computers, Theory of Computing Systems, 32
(1999), 69–112.

292
Bibliography
[105] R. Freund, Gh. P˘aun, G. Rozenberg, A. Salomaa, Bidirectional sticker
systems, in [7], 535–546.
[106] R. Freund, Gh. P˘aun, G. Rozenberg, A. Salomaa, Watson-Crick ﬁnite
automata, in [300], 305–317.
[107] R. Freund, Gh. P˘aun, G. Rozenberg, A. Salomaa, Watson-Crick au-
tomata, Techn. Report 97-13, Dept. of Computer Sci., Leiden Univ.,
1997.
[108] B. Fu, R. Beigel, On molecular approximation algorithms for NP-
optimization problems, in [300], 93–101.
[109] T. J. Fu, N. C. Seeman, DNA double-crossover molecules, Biochemistry,
32 (1993), 3211–3220.
[110] V. Geﬀert, Normal forms for phrase-structure grammars, RAIRO. Th.
Inform. and Appl., 25 (1991), 473–496.
[111] G. Georgescu, On the generative capacity of splicing grammar systems,
in [228], 330–345.
[112] A. Gibon, M. Amos, D. Hodgson, Models of DNA computing, Proc. of
21st MFCS Conf., 1996, Cracow, Lect. Notes in Computer Sci. 1113,
Springer-Verlag, Berlin, 1996, 18–36.
[113] S. Ginsburg, The Mathematical Theory of Context-free Languages, Mc-
Graw Hill, New York, 1966.
[114] R.
Giuntini,
Quantum
Logic
and
Hidden
Variables,
BI
Wis-
senschaftsverlag, Mannheim, 1991.
[115] T. Gramss, S. Bornholdt, M. Gross, M. Mitchell, Th. Pellizzari, Non-
Standard Computation. Molecular Computation, Cellular Automata,
Evolutionary Algorithms, Quantum Computers, Willey-VCH, Wein-
heim, New York, etc, 1998.
[116] D. M. Greenberger, M. A. Horne, A. Zeilinger, Going beyond Bell’s
theorem, in M. Kafatos, ed., Bell’s Theorem, Quantum Theory, and
Conceptions of the Universe, Kluwer Academic Publishers, Dordrecht,
1989, 73–76.
[117] D. M. Greenberger, A. YaSin, “Haunted” measurements in quantum
theory, Foundation of Physics, 19, 6 (1989), 679–704.
[118] L. K. Grover, A fast quantum mechanical algorithm for database
search, Proceedings of the Twenty-Eighth Annual ACM Symposium on
the Theory of Computing, 1996, 212–219.
[119] J. Gruska, Quantum Computing, McGraw-Hill, London, 1999.

Bibliography
293
[120] F. Guarnieri, M. Fliss, C. Bancroft, Making DNA add, Science, 273
(July 1996), 220–223.
[121] V. Gupta, S. Parthasarathy, M. J. Zaki, Arithmetic and logic operations
with DNA, in [300], 212–220.
[122] G. H. Hardy, E. M. Wright, An Introduction to the Theory of Numbers,
Oxford University Press, Oxford (4th edition), 1965.
[123] M. Harrison, Introduction to Formal Language Theory, Addison-
Wesley, Reading, Mass., 1978.
[124] J. Hartmanis, About the nature of computer science, Bulletin of the
EATCS, 53 (June 1994), 170–190.
[125] J. Hartmanis, On the weight of computation, Bulletin of the EATCS,
55 (1995), 136–138.
[126] H. Havlicek, K. Svozil, Density conditions for quantum propositions,
Journal of Mathematical Physics, 37(11) (1996), 5337–5341.
[127] T. Head, Formal language theory and DNA: An analysis of the genera-
tive capacity of speciﬁc recombinant behaviors, Bulletin of Mathemat-
ical Biology, 49 (1987), 737–759.
[128] T. Head, Hamiltonian paths and double stranded DNA, in [217], 80–92.
[129] T. Head, Gh. P˘aun, D. Pixton, Language theory and molecular genet-
ics. Generative mechanisms suggested by DNA recombination, in [258],
Vol. 2, 295–360.
[130] E.
Hemaspaandra,
L.
A.
Hemaspaandra,
M.
Zimand,
Almost-
everywhere
superiority
for
quantum
polynomial
time,
quant-
ph/9910033.
[131] G. T. Herman, G. Rozenberg, Developmental Systems and Languages,
North-Holland, Amsterdam, 1975.
[132] T. J. Herzog, P. G. Kwiat, H. Weinfurter, A. Zeilinger, Complemen-
tarity and the quantum eraser, Physical Review Letters, 75, 17 (1995),
3034–3037.
[133] A. Heyting, ed., Constructivity in Mathematics, North-Holland, Ams-
terdam, 1959.
[134] J. G. Hey, ed., Feynman and Computation. Exploring the Limits of
Computers, Perseus Books, Reading, Massachusetts, 1999.
[135] M. Hirvensalo, Copying quantum computer makes NP-complete prob-
lems tractable, in Proc. of MCU Conference, Metz, 1998 (M. Margen-
stern, ed), also as a TUCS Report No 161 (1998).

294
Bibliography
[136] M. Hirvensalo, An introduction to quantum computing, Bulletin of the
EATCS, 66 (1998), 100–121.
[137] J. Hoﬀmeyer, Semiosis and living membranes, 10 Seminario Avan¸cado
de Comunica¸cao e Semiotica: Biosemiotica e Semiotica Cognitiva, Sao
Paolo, Brasil, 1998, 9–23.
[138] M. Hogarth, Predicting the future in relativistic spacetimes, Studies in
History and Philosophy of Science. Studies in History and Philosophy
of Modern Physics 24, 5 (1993), 721–739.
[139] T. Hogg, Highly structured searches with quantum computers, Physical
Review Letters, 80 (1998) 2473–2476.
[140] J. E. Hopcroft, J. D. Ullman, Introduction to Automata Theory, Lan-
guages and Computation, Addison-Wesley, Reading, Mass., 1979.
[141] R. J. Hughes, Cryptography, quantum computation and trapped ions,
Technical Report LA-UR-97-4986, Los Alamos National Laboratory,
1997.
[142] L. Hunter, Molecular biology for computer scientists, in L. Hunter, ed.,
Artiﬁcial Intelligence and Molecular Biology, AAAI Press/MIT Press,
Menlo Park, Calif., 1993, 1–46.
[143] J. M. Jauch, Foundations of Quantum Mechanics, Addison-Wesley,
Reading, MA., 1968.
[144] J. Jarett, On the physical signiﬁcance of the locality condition in Bell
argument, Noˆus, 18 (1984), 569–589.
[145] E. Jurvanen, M. Lipponen, Distinguishability, simulation and univer-
sality of Moore tree automata, Fundamenta Informaticae, 34 (1999),
1–13.
[146] Gudrun Kalmbach, Orthomodular Lattices, Academic Press, New York,
1983.
[147] L. Kari, DNA computing: tomorrow’s reality, Bulletin of the EATCS,
59 (June 1996), 256–266.
[148] L. Kari, G. Gloor, S. Yu, Using DNA to solve the bounded Post corre-
spondence problem, Proc. of Second Intern. Colloq. Universal Machines
and Computations, Metz, 1998, vol. I, 51–65.
[149] L. Kari, Gh. P˘aun, G. Rozenberg, A. Salomaa, S. Yu, DNA computing,
sticker systems, and universality, Acta Informatica, 35, 5 (1998), 401–
420.

Bibliography
295
[150] L. Kari, Gh. P˘aun, G. Thierrin, S. Yu, At the crossroads of DNA
computing and formal languages: Characterizing RE using insertion-
deletion systems, in [300], 318–333.
[151] L. Kari, H. Rubin, D. H. Wood, eds., Preliminary Proceedings of
the Fourth International Meeting on DNA Based Computing, Univ. of
Pennsylvania, Philadelphia, June 1998.
[152] S. Kochen, E. P. Specker, The problem of hidden variables in quantum
mechanics, Journal of Mathematics and Mechanics, 17(1) (1967), 59–
87. Reprinted in [276], 235–263.
[153] L. Kalmar, An argument against the plausibility of Church’s thesis, in
[133], 72–80.
[154] G. Kreisel, A notion of mechanistic theory, Synthese, 29 (1974), 11–16.
[155] S. N. Krishna, R. Rama, A variant of P systems with active membranes.
Solving NP-complete problems, Romanian J. of Information Science
and Technology, 2, 4 (1999).
[156] S. N. Krishna, R. Rama, Computing with P systems, submitted, 2000.
[157] L. Kronsj¨o, Algorithms: Their Complexity and Eﬃciency, 2nd ed., Wi-
ley, New York, 1987.
[158] R. Landauer, Irreversibility and heat generation in the computing pro-
cess, IBM J. Res. Develop., 5 (1961), 183–191.
[159] R. Landauer, Computation, measurement, communication and energy
dissipation, in S. Haykin, ed., Selected Topics in Signal Processing,
Prentice Hall, Englewood Cliﬀs, New Jersey, 1989, 18.
[160] R. Landauer, Information is physical, Physics Today, 44 (1991), 23–29.
[161] R. Landauer, Information is inevitably physical, in [134], 76–92.
[162] R. Landauer, Information is physical, but slippery, in [39], 59–62.
[163] H. S. Leﬀ, A. F. Rex, Maxwell’s Demon: Entropy, Information, Com-
puting, Princeton University Press, Princeton, 1990.
[164] A. Lenstra, H. Lenstra, eds, The Development of the Number Field
Sieve, Springer-Verlag, New York, 1993.
[165] L. F. Landweber, R. J. Lipton, DNA to DNA: A potential “killer app”?,
in [300], 59–68.
[166] E. Laun, K. J. Reddy, Wet splicing systems, in [300], 115–126.

296
Bibliography
[167] W.-H. Li, D. Graur, Fundamentals of Molecular Evolution, Sinauer
Ass., Sunderland, Mass., 1991.
[168] R. J. Lipton, Using DNA to solve NP-complete problems. Science, 268
(April 1995), 542–545.
[169] R. J. Lipton, Speeding up computations via molecular biology, in [170],
67–74.
[170] R. J. Lipton, E. B. Baum, eds., DNA Based Computers, Proc. of a
DIMACS Workshop, Princeton, 1995, Amer. Math. Soc., 1996.
[171] S. Lloyd, Almost any quantum gate is universal. Phys. Rev. Letters, 75
(1995), 346–349.
[172] S. Lloyd, Quantum-mechanical Maxwell’s demon, Phys. Rev., A56
(1997), 3374–3382.
[173] H-K. Lo, H. F. Chau, Is quantum bit commitment really possible?
Phys. Rev. Lett., 78 (1997), 3410–3413.
[174] H-K. Lo, T. Spiller, S. Popescu, eds., Introduction to Quantum Com-
putation and Information, World Scientiﬁc, Singapore, 1999.
[175] G. W. Mackey, Quantum mechanics and Hilbert space, Amer. Math.
Monthly, Supplement, 64 (1957), 45–57.
[176] C. Macchiavello, G. M. Palma, A. Zeilinger, eds., Quantum Computa-
tion and Quantum Information Theory, Collected Papers and Notes,
World Scientiﬁc, Singapore, 1999.
[177] O. Q. Malhas, Quantum logic and the classical propositional calculus,
Journal of Symbolic Logic, 52(3) (1987), 834–841.
[178] O. Q. Malhas, Quantum theory as a theory in a classical propositional
calculus, International Journal of Theoretical Physics, 31(9) (1992),
1699–1714.
[179] M. Malit¸a,
Membrane Computing in Prolog,
in C. S. Calude,
M. J. Dinneen,
G. P˘aun,
eds.,
Pre-Proceedings of the Work-
shop on Multiset Processing, CDMTCS Research Report 140, 2000,
(www.cs.auckland.ac.nz/CDMTCS).
[180] V. Manca, String rewriting and metabolism: A logical perspective, in
[217], 36–60.
[181] V. Manca, C. Martin-Vide, Gh. P˘aun, New computing paradigms sug-
gested by DNA computing: Computing by carving, in [151], 41–56.

Bibliography
297
[182] V. Manca, C. Martin-Vide, Gh. P˘aun, Iterated GSM mappings: A col-
lapsing hierarchy, in J. Karhumaki, H. Maurer, Gh. P˘aun, G. Rozen-
berg, eds., Jewels are Forever, Springer-Verlag, Berlin, 1999, 182–193.
[183] M. Margenstern, Y. Rogozhin, An universal time-varying distributed
H system of degree 2, in L. Kari, H. Rubin, D. H. Wood, eds., Proc.
of the 4th Intern. Meeting on DNA Based Computers, Pennsylvania
Univ., June 1998, and BioSystems, 52, 1–3 (1999).
[184] C. Martin-Vide, Gh. P˘aun, Cooperating distributed splicing systems,
J. Automata, Languages, Combinatorics, 4, 1 (1999), 3–16.
[185] J. C. Maxwell, Theory of Heat, Longman’s, Green, & Co., London,
1985, 328–329.
[186] D. Mayers, Unconditionally secure quantum bit commitment is impos-
sible, Phys. Rev. Lett., 78 (1997), 3414–3417.
[187] N. D. Mermin, Bringing home the atomic world: Quantum mysteries
for anybody, American Journal of Physics, 49 (1981), 940–943.
[188] N. D. Mermin, Quantum mysteries revisited, American Journal of
Physics, 58 (1990), 731–734.
[189] N. D. Mermin, Hidden variables and the two theorems of John Bell,
Reviews of Modern Physics, 65 (1993), 803–815.
[190] A. Messiah, Quantum Mechanics, Volume 1, North-Holland, Amster-
dam, 1961.
[191] T. Mihara, S. S. Sung, A quantum polynomial time algorithm in worst
case for Simon’s problem (extended abstract), Proc. 9th Annual Sym-
posium on Algorithms and Computation, Taejon, 1998, 229–236.
[192] G. Milburn, Quantum optical Fredkin gate, Physical Review, 62 (1989),
2124–2127.
[193] G. Milburn, The Feynamn Processor. An Introduction to Quantum
Computation, Allen & Unwin, St. Leonards, 1998.
[194] M. Minsky, Computation. Finite and Inﬁnite Machines, Prentice Hall,
Englewood Cliﬀs, N.J., 1967.
[195] G.
Mitchison,
R.
Josza,
Counterfactual
computation,
quant-
ph/9907007.
[196] E. F. Moore, Gedanken-experiments on sequential machines, in C. E.
Shannon and J. McCarthy, eds., Automata Studies, Princeton Univer-
sity Press, Princeton, 1956, 128–153.

298
Bibliography
[197] P. J. Nahin, Time Machines, Springer-Verlag, New York, 1999.
[198] P. Odifreddi, Classical Recursion Theory, North-Holland, Amsterdam,
New York, Vol. 1, 1989, Vol. 2, 1999.
[199] P. Odifreddi, Indiscrete applications of discrete mathematics, in D. S.
Bridges, C. S. Calude, J. Gibbons, S. Reeves, I. Witten, eds., Combi-
natorics, Complexity, Logic, Springer-Verlag, Singapore, 1996, 52–65.
[200] M. Ogihara, A. Ray, DNA-based parallel computation by “counting”,
in [300], 265–274.
[201] M. Ogihara, A. Ray, The minimum DNA computation model and its
computational power, in [47], 309–322.
[202] B. ¨Omer, A Procedural Formalism for Quantum Computing, Masters
Thesis, Department of Theoretical Physics, Technical University of Vi-
enna, 1998, (http://tph.tuwien.ac.at/~oemer).
[203] Q. Ouyang, P. D. Kaplan, S. Liu, A. Libchaber, DNA solution of the
maximal clique problem, Science, 278 (1997), 446–449.
[204] A. P˘aun, Extended H systems with permitting contexts of small radius,
Fundamenta Informaticae, 31, 2 (1997), 185–193.
[205] A. P˘aun, On time-varying H systems, Bulletin of the EATCS, 67 (1999),
157–164.
[206] A. P˘aun, On the diameter of extended H systems, in J. Dassow, D.
Wotschke, eds., Workshop on Descriptional Complexity of Automata,
Grammars and Related Structures, Magdeburg, 1999, 165–174.
[207] Gh. P˘aun, On the iteration of gsm mappings, Revue Roum. Math. Pures
Appl., 23, 4 (1978), 921–937.
[208] Gh. P˘aun, On the power of the splicing operation, Intern. J. Computer
Math., 59 (1995), 27–35.
[209] Gh. P˘aun, Regular extended H systems are computationally universal,
J. Automata, Languages, Combinatorics, 1, 1 (1996), 27–36.
[210] Gh. P˘aun, On the power of splicing grammar systems, Ann. Univ. Buc.,
Matem.-Inform. Series, 45, 1 (1996), 93–106.
[211] Gh. P˘aun, DNA computing; Distributed splicing systems, in J. Myciel-
ski, G. Rozenberg, A. Salomaa, eds., Structures in Logic and Computer
Science. A Selection of Essays in Honor of A. Ehrenfeucht, Lect. Notes
in Computer Sci. 1261, Springer-Verlag, Berlin, 1997, 351–370.

Bibliography
299
[212] Gh. P˘aun, Marcus Contextual Grammars, Kluwer Academic Publ.,
Boston, 1997.
[213] Gh. P˘aun, Two-level distributed H systems, in S. Bozapalidis, ed., Proc.
of the Third Conf. on Developments in Language Theory, Thessaloniki,
1997, Aristotle Univ. of Thessaloniki, 1997, 309–327.
[214] Gh. P˘aun, (DNA) Computing by carving, Research Report CTS-97-
17, Center for Theoretical Study of the Czech Academy of Sciences,
Prague, 1997, and (in a revised form) Soft Computing, 3, 1 (1999),
30–36.
[215] Gh. P˘aun, Distributed architectures in DNA computing based on splic-
ing: Limiting the size of components, in [47], 323–335.
[216] Gh. P˘aun, DNA computing based on splicing: universality results,
Proc. of Second Intern. Colloq. Universal Machines and Computations,
Metz, 1998, Vol. I, 67–91.
[217] Gh. P˘aun, ed., Computing with Bio-Molecules. Theory and Experi-
ments, Springer-Verlag, Singapore, 1998.
[218] Gh. P˘aun, Computing with membranes, J. Computer System Sciences,
61 (2000), in press, and TUCS Research Report No. 208, November
1998, (http://www.tucs.fi).
[219] Gh. P˘aun, Computing with membranes: An introduction, Bulletin of
the EATCS, 68 (1999), 139–152.
[220] Gh. P˘aun, Computing with membranes – A variant: P systems with
polarized membranes, Intern. J. of Foundations of Computer Science,
11, 1 (2000), 167–182, and CDMTCS Research Report No. 098, 1999,
(www.cs.auckland.ac.nz/CDMTCS).
[221] Gh.
P˘aun,
P
systems
with
active
membranes:
Attacking
NP
complete
problems,
J.
Automata,
Languages,
Combina-
torics, 5 (2000), and CDMTCS Research Report No. 102, 1999,
(www.cs.auckland.ac.nz/CDMTCS).
[222] Gh. P˘aun, G. Rozenberg, Sticker systems,Theoretical Computer Sci.,
204 (1998), 183–203.
[223] Gh. P˘aun, G. Rozenberg, A. Salomaa, Computing by splicing, Theo-
retical Computer Sci., 168, 2 (1996), 321–336.
[224] Gh. P˘aun, G. Rozenberg, A. Salomaa, DNA Computing. New Comput-
ing Paradigms. Springer-Verlag, Berlin, 1998.
[225] Gh. P˘aun, G. Rozenberg, A. Salomaa, Complementarity versus univer-
sality: Keynotes on DNA computing, Complexity, 4, 1 (1998), 14–19.

300
Bibliography
[226] Gh.
P˘aun,
G.
Rozenberg,
A.
Salomaa,
Membrane
computing
with
external
output,
Fundamenta Informaticae,
41,
3
(2000),
259–266,
and TUCS Research Report No. 218,
December 1998,
(http://www.tucs.fi).
[227] Gh. P˘aun, Y. Sakakibara, T. Yokomori, P systems on graphs of re-
stricted forms, submitted, 1999.
[228] Gh. P˘aun, A. Salomaa, eds., New Trends in Formal Languages. Con-
trol, Cooperation, Combinatorics, Lect. Notes in Computer Sci., 1218,
Springer-Verlag, Berlin, 1997.
[229] Gh. P˘aun, L. Sˆantean, Parallel communicating grammar systems: the
regular case, Ann. Univ. Buc., Series Matem.-Inform., 38 (1989), 55–
63.
[230] Gh. P˘aun, G. Thierrin, Multiset processing by means of systems
of ﬁnite state transducers, in O. Boldt, H. J¨urgensen, L. Robbins,
eds., Workshop on Implementing Automata WIA99, Potsdam, Au-
gust 1999, XV (1–18), and CDMTCS Research Report No. 101, 1999,
(www.cs.auckland.ac.nz/CDMTCS).
[231] Gh. P˘aun, T. Yokomori, Membrane computing based on splicing, in
E. Winfree, D. Giﬀord, eds., Fifth Intern. Workshop on DNA Based
Computers, MIT, 1999, 213–227.
[232] Gh. P˘aun,
T. Yokomori,
Simulating H systems by P systems,
Journal of Universal Computer Science,
6,
1 (2000),
178–193,
(www.iicm.edu/jucs).
[233] Gh. P˘aun, S. Yu, On synchronization in P systems, Fundamenta Infor-
maticae, 38, 4 (1999), 397–410.
[234] R. Penrose, Shadows of the Mind, Oxford University Press, Oxford,
1994.
[235] A. Peres, Einstein, G¨odel, Bohr, Foundations of Physics, 15 (1985),
201–205.
[236] A. Peres, Quantum Theory: Concepts and Methods, Kluwer Academic
Publishers, Dordrecht, 1993.
[237] I. Petre, A normal form for P systems, Bulletin of the EATCS, 67
(1999), 165–172.
[238] I. Petre, L. Petre, Mobile ambients and P systems, Workshop on Formal
Languages, FCT’99, Ia¸si, Romania, 1999.
[239] N. Pisanti, DNA computing: a survey, Bulletin of the EATCS, 64
(February 1998), 188–216.

Bibliography
301
[240] D. Pixton, Regular splicing systems, manuscript, 1995.
[241] D. Pixton, Regularity of splicing languages, Discrete Appl. Math., 69
(1996), 101–124.
[242] D. Pixton, Splicing in abstract families of languages, Technical Report
of SUNY Univ. at Binghamton, New York, 1997.
[243] L. Priese, Y. Rogozhin, M. Margenstern, Finite H systems with 3 tubes
are not predictable, in [7], 547–558.
[244] I. Prigogine, From Being to Becoming, W. H. Freeman, San Francisco,
1980.
[245] P. Pt´ak, S. Pulmannov´a, Orthomodular Structures as Quantum Logics,
Kluwer Academic Publishers, Dordrecht, 1991.
[246] M. O. Rabin, Probabilistic algorithms, in [288], 21–39.
[247] T. Rado, On non-computable functions, Bell System Technical Journal
41 (1962), 877–884.
[248] H. Reichenbach, The Direction of Time, University of California Press,
LA, 1956.
[249] J. H. Reif, Paradigms for biomolecular computation, in [47], 72–93.
[250] E. Rieﬀel, W. Polak, An introduction to quantum computing for non-
physicists, quant-ph/9809016.
[251] R. Rivest, A. Shamir, L. Adleman, A method for obtaining digital
signatures and public key cryptosystems, Communications of ACM, 21
(1978), 120–126.
[252] Y. Rogozhin, Small universal Turing machines, Theoretical Computer
Sci., 168 (1996), 215–240.
[253] Y. Rogozhin, A universal Turing machine with 22 states and 2 symbols,
Romanian J. of Information Science and Technology, 1, 3 (1998), 259–
265.
[254] R. Rosen, Eﬀective processes and natural law, in R. Herken, ed., The
Universal Turing Machine. A Half-Century Survey, Kammerer & Un-
verzagt, Hamburg, 1988, p. 523.
[255] B. Rovan, A framework for studying grammars, Proc. MFCS 81, Lect.
Notes in Computer Sci. 118, Springer-Verlag, Berlin, 1981, 473–482.
[256] G. Rozenberg, A. Salomaa, The Mathematical Theory of L Systems,
Academic Press, New York, 1980.

302
Bibliography
[257] G. Rozenberg, A. Salomaa, Watson-Crick complementarity, universal
computations and genetic engineering, Techn. Report 96-28, Dept. of
Computer Science, Leiden Univ., Oct. 1996.
[258] G. Rozenberg, A. Salomaa, eds., Handbook of Formal Languages, 3
volumes, Springer-Verlag, Berlin, 1997.
[259] A. Salomaa, Formal Languages, Academic Press, New York, 1973.
[260] A. Salomaa, Computation and Automata, Cambridge Univ. Press,
Cambridge, 1985.
[261] A. Salomaa, Equality sets for homomorphisms of free monoids, Acta
Cybernetica, 4 (1978), 127–139.
[262] A. Salomaa, Jewels of Formal Language Theory, Computer Science
Press, Rockville, Md., 1981.
[263] A. Salomaa, Public-Key Cryptography, Springer-Verlag, Berlin, 1996.
[264] A. Salomaa, Turing, Watson–Crick, and Lindenmayer. Aspects of DNA
complementarity, in [47], 94–107.
[265] B. Schumaker, Quantum coding, Physical Review A, 51, 4 (1995), 2738–
2747.
[266] D. B. Searls, The linguistics of DNA, American Scientist, 80 (1992),
579–591.
[267] C. E. Shannon, A universal Turing machine with two internal states,
Automata Studies, Annals of Mathematical Studies, 34, Princeton Univ.
Press, 1956, 157–165.
[268] C. E. Shannon, Computers and automata, Proceedings of the I. R. E.
41 (1953), 1235–1241.
[269] P. W. Shor, Algorithms for quantum computation: discrete log and
factoring, Proceedings of the 35th IEEE Annual Symposium on Foun-
dations of Computer Science, 1994, 124–134.
[270] D. R. Simon, On the power of quantum computation, SIAM J. Comput.
26, 5 (1997), 1474–1483; ﬁrst published in Proc. of the 35th IEEE
Symposium on Foundations of Computer Science (FOCS), Santa Fe,
New Mexico, IEEE Computer Society Press, Los Alamitos, CA, 1994,
116–123.
[271] H. T. Siegelmann, Computation beyond the Turing limit, Science, 268
(April 1995), 545–548.

Bibliography
303
[272] H. T. Siegelmann, Neural Networks and Analog Computation: Beyond
the Turing Limit, Birkhauser, Boston, MA, 1999.
[273] R. I. Soare, Recursively Enumerable Sets and Degrees, Springer-Verlag,
Berlin, 1987.
[274] W. M. Sofer, Introduction to Genetic Engineering, Butterworth-
Heinemann, Boston, 1991.
[275] E. Specker, Die Logik nicht gleichzeitig entscheidbarer Aussagen, Di-
alectica, 14 (1960), 175–182. Reprinted in [276], 175–182.
[276] E. Specker, Selecta, Birkh¨auser Verlag, Basel, 1990.
[277] A. Steane, Quantum computing, quant-ph/9708022.
[278] Y. Suzuki, H. Tanaka, On a LISP implementation of a class of P sys-
tems, Romanian J. of Information Science and Technology, 3, 2 (2000).
[279] Y. Suzuki, H. Tanaka, Order parameter for a symbolic chemical system,
Proc. of Artiﬁcial Life Conf. VI, MIT Press, 1998, 130–139.
[280] Y. Suzuki, H. Tanaka, Symbolic chemical systems based on an abstract
rewriting system and its behavior pattern, Journal of Artiﬁcial Life
and Robotics, 1 (1997), 211–219.
[281] Y. Suzuki, S. Tsumoto, H. Tanaka, Analysis of cycles in symbolic chem-
ical systems based on abstract rewriting systems on multisets, Prof. of
Artiﬁcial Life Conf. V, MIT Press, 1996, 522–528.
[282] K. Svozil, Randomness & Undecidability in Physics, World Scientiﬁc,
Singapore, 1993.
[283] K. Svozil, Quantum Logic, Springer-Verlag, Singapore, 1998.
[284] K. Svozil, The Church-Turing Thesis as a guiding principle for physics,
in [47], 371–385.
[285] K Svozil, One-to-one, Complexity, 4 (1) (1998), 25–29.
[286] L. Szilard, On the decrease of entropy in a thermodynamic system by
the intervention of intelligent beings, Zeitschrift f¨ur Physics, 53 (1929),
840–856.
[287] T. Toﬀoli, N. Margolus, Cellular Automata Machines, MIT Press, Cam-
bridge, MA, 1987.
[288] J. Traub, Algorithms and Complexity: New Directions and Results,
Academic Press, London, 19976.

304
Bibliography
[289] A. M. Turing, On computable numbers, with an application to the
Entscheidungsproblem, Proc. London Math. Soc., Ser. 2, 42 (1936),
230–265; a correction, 43 (1936), 544–546.
[290] H. Weyl, Philosophy of Mathematics and Natural Science, Princeton
University Press, Princeton, 1949.
[291] J. A. Wheeler, W. H. Zurek, Quantum Theory and Measurement,
Princeton University Press, Princeton, 1983.
[292] U. Vazirani, Quantum computing, 1997, (http://www.cs.berkeley.
edu/~vazirani.)
[293] E. Winfree, Complexity of restricted and unrestricted models of molec-
ular computability, in [170], 187–198.
[294] E. Winfree, On the computational power of DNA annealing and liga-
tion, in [170], 199–210.
[295] E. Winfree, Algorithmic Self-Assembly of DNA, PhD Thesis, California
Institute of Technology, Pasadena, CA, 1998.
[296] C. P. Williams, S. H. Clearwater, Explorations in Quantum Computing,
Springer-Verlag, New York, 1997.
[297] C. P. Williams, S. H. Clearwater, Ultimate Zero and One: Computing
at the Quantum Frontier, Springer-Verlag, Heidelberg, 2000.
[298] E. Winfree, X. Yang, N. Seeman, Universal computation via self-
assembly of DNA; some theory and experiments, in [18], 191–213.
[299] D. Wood, Iterated NGSM maps and Γ-systems, Inform. Control, 32
(1976), 1–26.
[300] D. Wood, L. Kari, R. Lipton, J. Reif, N. Seeman, E. Winfree, eds.,
Preliminary Proceedings of the 3rd DIMACS Workshop on DNA Based
Computers, Pennsylvania Univ., Philadelphia, June 1997.
[301] W. K. Wooters, W. H. Zurek, A single quantum cannot be cloned,
Nature, 299 (1992), 802–803.
[302] A. Yao, Quantum circuit complexity, Proceedings of the 34th IEEE
Symposium on Foundations of Computer Science (FOCS), IEEE Com-
puter Science Press, Los Alamitos, 1993, 352–260.
[303] T. Yokomori, YAC: Yet another computation model of self-assembly, in
E. Winfree, D. Giﬀord, eds., Preliminary Proc. 5th Intern. Workshop
on DNA Based Computers, MIT, 1999, 153–168.

Bibliography
305
[304] T. Yokomori, Computation = self-assembly + conformational change:
Toward new computing paradigm, Preproceedings of DLT 99 (Invited
Lecture), Aachen, 1999, 21–30.
[305] C. Zalka, Grover’s quantum searching algorithm is optimal, quant-
ph/9711070.
[306] C. Zandron, C. Ferretti, G. Mauri, A reduced distributed splicing sys-
tem for RE languages, in [228], 346–366.
[307] N. Zierler, M. Schlessinger, Boolean embeddings of orthomodular sets
and quantum logic, Duke Mathematical Journal, 32 (1965), 251–262.
[308] W. H. Zurek, Algorithmic randomness, physical entropy, measure-
ments, and the Demon of choice, in [134], 393–410.

306
Bibliography

Index
adenine (A), 24
alphabet, 1
ampliﬁcation, 34, 90
amplify, 40
annealing, 30, 65
asymetric graph, 154
Basic Universality Lemma, 83
Bell’s Theorem, 216
bounded language, 127
strictly, 127
BPP, 265
BQP, 272
Bridge Theorem, 122
catalyst, 112
cellular automata, 51
chemical abstract machine, 110
Chomsky grammar, 8
context-free, 9
context-sensitive, 9
linear, 9
monotonous, 9
regular, 9
Chomsky hierarchy, 9
Church–Turing Thesis, 15, 57
coding, 2
complementation, 56
complete data pool, 50
computably enumerable language,
9, 56, 72, 83, 123
computably
enumerable
lan-
guages, 146
computational completeness, 16
computing by carving, 50, 55
copy, 47
concatenation, 2
controlled-U gate, 238
controlled-controlled-NOT, 202
controlled-NOT gate, 201
cytosine (C), 24
DAE construct, 51
deletion, 34
denaturation, 30
deoxyribonucleic acid, 23
detect, 40
Discrete Fourier Transform, 250
DNA, see deoxyribonucleic acid,
23
double helix, 24
endonuclease, 31, 32
equality set, 15
equivalent grammars, 8
error probability, 264
Euclid’s algorithm, 254
evolutionary computing, 41
exonuclease, 31
Fast Fourier Transform, 250
ﬁltering, 35
ﬁnite automaton, 11, 81
deterministic, 11
Watson–Crick, 75
reverse, 76
gel elecctrophoresis, 36
gel electrophoresis, 39
generalized
sequential
machine
(gsm), 12, 58
grammar scheme, 18
grammar system, 9, 95
307

308
Index
cooperating distributed, 9
parallel communicating, 10,
95
splicing, 95
Grover’s search algorithm, 261
gsm
deterministic, 12
guanine (G), 24
H scheme, 80
H system, 77
communicating distributed,
97
distributed, 95
extended, 85
ordered, 89
sequential distributed, 102
time-varying, 98
with forbidding contexts, 88
with global target, 89
with local targets, 89
with multisets, 91
with permitting contexts, 87
halting problem, 206
Hamiltonian path problem, 37, 47
insertion, 34
Kleene closure, 2
Landauer’s principle, 182
language, 1
length-separate, 41
ligase, 34, 77
ligation, 34
Lindenmayer system, 10
deterministic, 10
extended, 11
propagating, 10
tabled, 11
matrix grammar, 122
in binary normal form, 123
with
appearance
checking,
122
matrix languages, 146
maximal ideal, 229
maximum clique problem, 47, 49
Maxwell demon, 187
membrane, 111
elementary, 111
membrane structure, 110
index of, 111
merge, 40
menmology, 43
metabolic system, 110
mirror image, 2
morphism, 2
λ-free, 2
multiset, 2, 40, 90, 112
M02, 226
normal form, 10
Geﬀert, 10
Kuroda, 10
nuclease, 31
nucleotide, 23
oligonucleotide (oligo), 31
orthocomplemented lattice, 225
orthoposet, 228
P system, 112
Computational Completeness
Lemma, 123, 133, 139,
165
conﬁguration of, 114
rewriting, 132
splicing, 168
with
polarized
membranes,
136
P’ system, 154
Parikh set, 1
Parikh vector, 1
Peres, 213, 227, 234, 235
polymerase (DNA), 31
Polymerase
Chain
Reaction
(PCR), 34
polynomial time bounded proba-
bilistic machine, 265
position-separate, 41

Index
309
PP, 265
primer, 31
Principle of Superposition, 192
probabilistic Turing machine, 264
program-size complexity, 214
projection, 2
protein channel, 157
pumping lemma, 63
pure grammar, 8
purine, 24
pyrimidine, 24
quanta, 189
Quantum Fourier Transform, 250
quantum parallelism, 244
quantum Turing machine, 271
radius, 80, 113
region, 111
regular approximation, 62
regular language, 69
regular sequence of languages, 58
Regularity Preserving Lemma, 81
regulated rewriting, 87
Reichenbach, 222
remove, 47
restriction enzyme, 32, 77
rewriting system, 8
SAT problem, 44, 157, 160
satisﬁability problem, 20
select, 47
sentential form, 8
separate, 40
sequential transducer, 12
Watson–Crick, 77
Shor’s algorithm, 252
shuﬄe, 2
Simon’s problem, 266
space complexity, 19
splicing, 77, 79
iterated, 81
rule, 80
splicing rule, 80
sticker operation, 67
sticker system, 68
one-sided, 69
regular, 69
simple, 69
sticky end, 25
subgraph isomorphism problem,
47
super-cell, 112
template, 31
test tube programming language,
40
three-vertex-colourability, 47
thymine (T), 24
time complexity, 19
Turing machine, 13, 15
deterministic, 13
universal, 16
twin-shuﬄe language, 23, 26
reverse, 29
type-0 grammar, 9
universal, 18, 87
uncertainty principle, 189
union, 47
universal type-0 grammar, 18
universality, 16
Uf, 241
Walsh–Hdamard
transformation,
198
Watson–Crick
complementarity,
24, 39, 65, 78
Watson–Crick domain, 65

