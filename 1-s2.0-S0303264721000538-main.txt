BioSystems 204 (2021) 104396
Available online 13 March 2021
0303-2647/© 2021 Elsevier B.V. All rights reserved.
From axiomatic systems to the Dogmatic gene and beyond 
Enka Blanchard a,*, Giuseppe Longo b,c 
a Digitrust Consortium, Loria, Universit´e de Lorraine, Nancy, France 
b Centre Cavaill´es, R´epublique des Savoirs, CNRS and ´Ecole Normale Sup´erieure, Paris, France 
c School of Medicine, Tufts University, Boston, MA, USA   
A R T I C L E  I N F O   
Keywords: 
Formalist projects 
Genocentric approaches 
Organismal views 
Historicity 
Darwin in ontogenesis 
Heterogenesis 
A B S T R A C T   
The positivistic views that dominated the early debate on the foundations of mathematics, at the beginning of the 
20th century, survived the “negative results” that have shown the limits of the axiomatic approach since the 
1930s. Rigour, abstraction and symbolism have been confused with formalism, based on finite strings of signs, 
pre-given axioms, and potentially mechanisable rewriting rules. This contributed to major clarifications in the 
mathematical praxes but obscured the limits of formalisms due to the exclusion of the historical creation of sense 
proper to any science. We expand on this sometimes fruitful confusion with some case studies. We then hint to 
the historical creation of sense as a component of an epistemology of mathematics. We continue with an analogy 
with genocentric approaches in biology, as similar positivistic views resurfaced there fifty years later. Finite 
sequences of letters in the DNA would completely determine ontogenesis and phylogenesis, according to the 
Central Dogma of molecular biology. Limits and “negative evidence” have been disregarded while searching for 
the “gene for” everything. Alternative perspectives require a reconstruction of the sense of history as locus for the 
constitution of any object of biological knowledge. In particular, the historicity of biological evolution will be 
understood in terms of changing phase spaces and of the role of rare events in all phylogenetic trajectories. The 
analysis of the evolutionary production of variability, adaptivity and ecosystemic diversity is a key component of 
the project we hint to, as part of a renewed relation to the biological environment.   
1. Proofs, meanings and invariants 
With integers and their ratios mapping perfectly to musical forms 
and aesthetic criteria, ancient Greek thought easily associates intimate 
knowledge of reality and production of beauty. As early Greek thinkers 
lay the groundwork for Western philosophy, Pythagoras thought that 
not only rigour but the very essence of reality was to be found in 
numbers, within the rules governing arithmetic (Huffman, 2019; 
Chiurazzi, 2017). Ancient Greeks had a reason to give such importance 
to integers: their role was central in their societies. This went from the 
political rule — with strict quorums for assemblies and some tediously 
recorded voting tallies (Canevaro, 2018) — to the currency systems used 
in one of the first highly commercial societies with complex 
import-export networks (Ober, 2011). 
But suddenly, after decades of detailed examination of integers and 
their ratios, the crash. The perfect logos of integers creates in its midst 
the irrational, the “a-logos”, inside the unitary square itself: in its diag­
onal one finds a number, 
̅̅̅
2
√
, lying outside of the bounds of arithmetic 
reasoning and integer ratios. 
How to make sense of this nonsensical evidence, this first negative 
result? As it often does when confronted with powerful negative results, 
reason itself expands. At the time, it did so by including drawing mo­
tions, producing lines, evoking meaning through the use of diagrams at 
the core of geometry, in Euclid’s account (Longo, 2009). The breadthless 
border of plane figures that is the line is not an approximation, but a new 
foundation to reground geometric reasoning away from arithmetic 
principles. 
This new geometrical foundation lies in diagrams (Panza, 2012), but 
not simply in the final diagrams themselves. Instead, it is the common 
element between the diagram, the motions that create the diagram, and 
the oral description of both diagram and motions to one’s peers. The 
motion and the description seek to capture that which is not in the 
particular physical instantiation. Following Proclus, the discoveries in 
geometry are measured, in a physical sense, through perception 
(αἰσθητικός). To truly grasp mathematical concepts which reside in the 
intellect, one has to extract their essence, using one’s imagination while 
inspired by the diagrams (Helmig and Steel, 2020). 
* Corresponding author. 
E-mail address: enka.blanchard@gmail.com (E. Blanchard).  
Contents lists available at ScienceDirect 
BioSystems 
journal homepage: http://www.elsevier.com/locate/biosystems 
https://doi.org/10.1016/j.biosystems.2021.104396 
Received 31 December 2020; Received in revised form 25 February 2021; Accepted 26 February 2021   

BioSystems 204 (2021) 104396
2
Geometry then arises in the interplay that allows the tracing motion 
in the sand to show a continuous line — a trajectory, we could say — and 
the language that then allows one to say “the line I’m drawing is 
breadthless”. By this, Greek geometry invented at once a notion of tra­
jectory independent of the object moving along it and the difficult 
notion of border, which served as foundation for later developments, up 
to Thom’s general theory (Thom, 1954). 
Symmetries also play a central role in this new non-numerical 
foundation for geometry. Looking at the five Euclidean postulates, 
they all implicitly try to maximise symmetries in the constructions they 
allow, with the central elements being straight lines between points 
(forming the most symmetric and simplest trajectories) and circles 
(Longo, 2009). And although this was not formalised and might have 
gone unnoticed until Desargues or even Joseph Diaz Gergonne, the 
postulates — and the first theorem — themselves explicit some con­
ceptual symmetry in how they allow a duality between points and lines: 
if any line is defined by a pair of points, a point is also defined by a pair 
of lines1 (Pedoe, 1975). Symmetries will continue to play a key role in 
the foundations of mathematics from Klein’s Erlangen program (1872) 
to Weyl’s approach to mathematical knowledge (Weyl, 1952). 
There is a common pattern behind the use of such elements as basis 
for mathematical thoughts: a search for abstraction. As the very concept 
of abstraction is hard to define and manipulate, let us focus instead on 
the construction of explicit invariants. What is numerosity but the 
abstraction that seeks to find the common invariant between seven coins 
and seven sheep? Between three lions and three roars? What is sym­
metry but the search for patterns that remain the same after simple 
transformations? What is continuity but the common property shared by 
all motions and trajectories? 
The fact that we human mathematicians decided to collectively focus 
on invariants such as those — maybe unconsciously, through the process 
of finding them easy to work with — is grounded in these pre-human 
practices. Each of the early ones apparently corresponds to a pattern 
that our brain is able to notice and then stabilise as invariants of action: 
being able to iterate an action, anticipate the trajectory of a prey or of a 
predator, independently of changing environments, forces the selective 
memory of the invariant properties of trajectories. Our early invariants 
are practiced in actions before becoming generalised and abstract forms 
of small set of intuitions — that result from praxes — for which we now 
even have neuronal evidence, as traces of the interfacing of the brain 
with the world, through action. The brain allows ecosystemic activities 
while shaping itself in them. Like the practice of trajectories, counting 
small amounts of distinct objects is a practice shared with many animals 
— again a practical invariant, in this case in comparing quantities. What 
is the sense of numerosity for larger numbers except the extension of our 
inner capacity for subitising — instantly knowing the number of objects 
in a small set, of size generally less than five. This primary sense can also 
be isolated and traced back to — and perhaps produced by — some 
dedicated neuronal structures in our brains, where initial simple con­
nections are reinforced by their use (Dehaene, 2011; Anobile et al., 
2019; Longo and Viarouge, 2010). There is also evidence that the integer 
number line itself, with the numbers sorted from left to right in our 
writing cultures,2 could be a common invariant on how humans can 
write and “see” numbers internally, linking arithmetic and geometric 
intuitions. 
What is the common element in all lines and curves we draw? The 
pattern that is independent of the thickness, colour, or medium in which 
we draw — be it sand or paper — but the continuity of the gesture? Is it a 
coincidence that the essence of the line might be in the continuity of the 
artist’s hand’s trajectory, and that we chose this as one of the central 
invariants of early geometry, when we as a species share such an un­
canny innate ability to visualise and predict such trajectories effort­
lessly? Following Poincar´e’s intuition, our inner representations of the 
distance of an object in space might be related to the set of muscular 
tensions necessary to reach it. This is facilitated by our saccadic move­
ments — or eye jerks — which trace in advance the paths needed to 
reach an object, to capture a prey. More precisely, tracing in advance a 
trajectory by our eyes allows to anticipate the next position where they 
look to follow a prey, escape a predator, capture a ball, and to move 
there following “the pursuit line” also traced by eye jerks — fractions of 
a second in advance (Berthoz, 1997). The Poincar´e-Berthoz isomor­
phism relates our visual perception of continuity and our vestibular 
perception of trajectories that are continuous in not only space but also 
time (Teissier, 2012). Attuned with our perspective, Teissier observed, 
in a lecture which became famous at the Philosophy and Mathematics 
seminar at ENS Paris: “I understand a theorem when, for sure, I can 
follow the logical argument, yet the actual understanding and mastering 
of the argument comes only when it becomes evident to my monkey’s 
brain”. 
Finally, symmetries contain an aesthetic component. They please our 
brains and we often recognise them from a distance, even if they are 
imperfect — and can now teach artificial neural networks to do the 
same, starting with the simplest patterns of continuity in the early stages 
of AI and going to complex symmetries more recently (Scharcanski and 
Venetsanopoulos, 1997; Cao et al., 2016). If their design shows any­
thing, it is that it is relatively easy to create neural networks that detect 
exactly those simple properties: continuity, borders, symmetry, and 
sometimes even numerosity. The symmetries of the world, those of our 
animal bodies may have forced our brain structures and perpetuate this 
influence on any new brain in a similar environment. An extreme case is 
seen in the empirical evidence from studies where cats where raised in 
environments consisting entirely of stripes (horizontal or oblique). Once 
adults, the cats had visual deficiencies in their capacity to recognise 
shapes that differed from the pattern they were used to, visible in both 
behaviour and neural cell networks (Blasdel et al., 1977; Maffei, 1998). 
When explaining axiomatic systems to young students, we often find 
ourselves describing axioms as self-evident properties and neglecting 
their evolutionary and historical formation, the way they result from a 
friction between our brains and the world by our living bodies, later 
enriched and specified by language and writing. It is then reasonable to 
wonder whether many early axioms are exactly this: attempts to abstract 
a felt intuition, a pattern perceived by most human minds, the proposal 
of a relevant invariance property. We hinted at the role of active tracing 
and symmetries in the proposal of Euclid’s axioms. Their universality — 
as understandable by any human — is due to the pre-linguistic nature of 
this construction of sense that precedes human languages. The proof is 
then inextricably linked to the discourse that allows generalisation of 
this early intuition, as a result of an active relation to the world, into 
rigorous knowledge. It then becomes a social object that relies, by lan­
guage, on rhetoric and sometimes esoteric knowledge (often in a mas­
ter/disciple setting3). Despite the relative fascination of Greek 
philosophers for the written word, proofs were still mostly oral objects, 
with people learning from each other instead of using textbooks. Every 
mathematician experiences the huge efficiency gains one obtains when 
discussing complex results face to face as opposed to in a textbook, and 
how many proofs are elaborated during discussions in front of black- or 
white-boards. Proofs are still now mostly oral objects, supported by 
sketched computations or very informal drawings just hinting to struc­
tures of imagination by gestures. 
We then reach a kind of informal rigour. A proof is whatever con­
vinces its audience — sometimes one well-versed in rhetorical tricks — 
1 To be more accurate, this requires the two lines not to be parallel — as seen 
in the reference to the first theorem of Euclid’s Elements below — or for one to 
be in a projective space.  
2 From right to left in Arabic, as any Arabic mathematised person can witness. 
3 Legends abound on Hippasus of Metapondum and his potential murder as 
punishment for revealing the irrationality of 
̅̅̅
2
√
to the public outside of the 
Pythagorean school (Huffman, 2019). 
E. Blanchard and G. Longo                                                                                                                                                                                                                   

BioSystems 204 (2021) 104396
3
of the veracity of a proposition. Explicit references to previously proved 
results contribute to such rigour. But certain central statements can 
remain unexamined. For example, the consideration of a line as 
breadthless border can be formally interpreted as taking the asymptotic 
limit of a sequence of geometric objects, but this limit is never formally 
defined — and would have been impossible with the tools present at the 
Greeks’ time. Teissier’s very argument about the Poincar´e-Berthoz 
isomorphism makes use of this peculiarity: his isomorphism is not a true, 
formal isomorphism, but only a linguistic trick to give his readers an 
intuition of the relationship he infers between the visual and vestibular 
lines — and he says as much in his article (Teissier, 2012). 
Getting back to Greek mathematicians, let’s consider Euclid’s Ele­
ments again. On one side, it is correct to say that they figure a systematic 
approach with an axiomatisation that was not present in earlier texts. 
The results in the Elements indeed have proofs, albeit lacking some ex­
amination of what proofs are exactly.4 On the other side, a mathematical 
proof is also — or even mainly — a historical construction of meaning, 
through a dialogue in the informal rigour proper to the praxis of 
mathematics. If it is too complex, it must be — and is in practice — re- 
written over time, by distilling lemmas, and by proposing more general 
underlying invariants, thus making the corresponding intuition easier to 
grasp. It might then be ahistorical to interpret Euclid’s project as a 
predecessor to Hilbert’s that was only lacking the proper tools to found 
mathematics in a formal way (see the remarks on Euclid’s first theorem 
in chapter one and on other critiques in Heath (1908)). 
2. The role of formalism 
Looking at the last two millennia, we can observe that formalism and 
codification of thought both appear over time, and are steeped in the 
evolution of language. As in other fields, there is a desire to simplify 
patterns that everyone observes. If most mathematicians talk about 
powers of an unknown number, it is more efficient to use x7 than a 
complex sentence. Common examples can be found in older works such 
as Muḥammad ibn M¯us¯a al-Khw¯arizm¯ı’s treatise on algebra (as trans­
lated by Rosen (1831)): 
“I have divided ten into two parts, and having multiplied each part 
by itself, I have subtracted the smaller from the greater, and the 
remainder was forty. Then the computation is — you multiply ten 
minus thing by itself, it is a hundred plus one square minus twenty 
things and you also multiply thing by thing, it is one square. Subtract 
this from a hundred and a square minus twenty things, and you have 
a hundred, minus twenty things, equal to forty dirhems.” 
Or, equivalently: 
|(10 −x)2 −x2| = 40 →100 −20x = 40  
Here, formalism has the advantage of making proofs more intelligible by 
making them more concise, through the adoption of a common lan­
guage. And this process of formalisation is in itself social: with dozens of 
potential formalisms proposed, the selection of one of them depends on 
their relative popularity among other authors until one reaches hege­
mony. The choice of the symbol’s calligraphy itself is not neutral: quite a 
few of them evoke meaning through both the motion necessary to draw 
them and the mental associations this creates. For example, the choice of 
using ̂
ABC — and sometimes ∠ABC — to denote an angle is the result of 
a non-trivial evolution. As related by Florian Cajori, the symbol < was 
used from for angles in 1634 and continued being used until the eigh­
teenth century, in parallel with the “smaller than” signification (Cajori, 
1928). The period in between saw a multiplication of options appear: ∠ 
for an angle, and ∠∠ for plural angles, ̂
a b for an angle between lines a 
and b, or even ∢, among dozens of other possibilities, each carrying a 
possibly different reference to meaning5). 
Going back to early Greek mathematicians, the representation of 
numbers was dual, with on one hand numbers written as letters, and on 
the other manipulation through pebbles (ψῆφος) on a table — or maybe 
an early abacus. Thus, the representation was not only through an ab­
stract symbol that was drawn, but mentally incorporated the manipu­
lation of a number of dots, linking the concept and the numerosity 
(Lang, 1957). Hence, the process of sharing the initial meaning that one 
ascribes to a symbol is an essential part: without it, the symbol cannot be 
used and remains unknown. Hence, before the symbol acquires any 
autonomy and becomes stable through the same rules as general lin­
guistic terms,6 it must be used by a small community for which it rep­
resents a shared meaning born through common interactions — oral or 
written.7 
The symbol — from the Greek συμβάλλω, “I throw together” — can 
then be said to unite multiple sources of meaning. It represents the 
invariant at the heart of the abstraction process, and its multiple 
meanings interact. As such, it is not just a formal sign that can be 
manipulated by automated syntactic rules without losing something — 
some form of meaning — in the process. In this context, the initial 
invariant is not just a symbol, but the motion that allows one to draw 
said symbol. The meaning evoked arises from both motion and associ­
ation with other symbol-concepts. 
In the historical mathematical practice, after one removes or sim­
plifies redundant passages of proofs to make them clearer to check (and 
sometimes to understand), it seems natural to hope that we can reach 
some point where the whole process becomes quasi-mechanical. In this 
context, Hilbert’s project initially appears reasonable, as is the general 
objective of finding a common axiomatisation of mathematics that can 
lead to a fully formal, fully verified, and ideally all-encompassing tower 
of mathematical knowledge. In this way, the stabilisation of symbols is a 
prerequisite for the axiomatisation project, providing both the tools and 
the motivation for the latter. The issue is when one goes from this initial 
formalisation of simple concepts and towards the formalisation of our 
actions on concept-symbols themselves, in a self-referential way. 
Mathematics is rigorous, but true mathematical rigour can only be 
meaning-making, and in this way can only be “in-formal”. A formalism 
without this meaning-making can only manipulate symbols as signs — 
and not signifiers — and easily falls into the absurd. Shortly after Frege 
explicitly formalised Cantor’s set theory, Russell shows an easy 
contradiction. 
The solution, which uses both sets and classes, illustrates how 
important types are in mathematics — although this is not always fully 
apparent. Types contribute greatly to the creation of meaning in proofs, 
by allowing us to state on which set a function is defined and in which 
set it has its values. 
Thirty years after this first paradox, Alonzo Church proposes a theory 
4 For example, the analysis of modus ponens and the examination of the rules 
of deduction that are used within the text are far from the consideration present 
in the text itself, at least from the evidence subsisting today. 
5 One curious effect is that old notations had a symbol for an indeterminate 
angle (∠), but few possibilities for one that goes through given points or lines. 
Reciprocally, contemporary notations allow for a specific angle given three 
points: ̂
ABC, but do not directly allow the representation of a generic angle. Of 
course, it is still possible to denote a generic angle by using ̂
ABC with generic A, 
B, and C. Some use ∠ABC for specific angles, but the symbol ∠ by itself has 
become rare. Questions on the nature of angles are still debated today, espe­
cially as they relate to physical measurements, as in Krystek (2021).  
6 For example, the most commonly used terms are those that are most stable 
and evolve more slowly (Pagel et al., 2007).  
7 For what is probably the first time in history, mathematicians interact 
mainly through papers and most of the exchanges on what could be shared 
meanings happen via impersonal publications, instead of oral exchanges — or 
even regular epistolary correspondence. It remains to be seen how this will 
impact the evolution of symbol-concepts. 
E. Blanchard and G. Longo                                                                                                                                                                                                                   

BioSystems 204 (2021) 104396
4
of functions that could be applied to themselves. As this allows a brand 
of pure formalism with no intrinsic meaning, it leads to Curry’s paradox, 
a more technical rewriting of the initial circular contradiction. Two 
paths avoid this contradiction: bringing meaning into the formalism by 
reintroducing types (Church, 1941) or limiting the simultaneous inter­
play of self-application and negation (Barendregt, 1984). This happens 
once again with Martin-L¨of and his formal universe of Types, which is a 
Type in itself. Once Girard points out this paradox in the theory, illus­
trating both its expressive power and a profound flaw (Coquand, 1986), 
Martin-L¨of repairs it by restoring the link to meaning-making in a novel 
theory of types (Martin-L¨of and Sambin, 1984). 
Those paradoxes do not negate the importance or usefulness of 
formalism, but underline its limits when it comes to the creation of 
meaningful statements. Studying the paradoxes themselves is a fruitful 
activity even outside of theoretical contexts, as is shown by the de­
velopments of early computer science but also contemporary develop­
ment paradigms inspired by intuitionistic logic. The system that rose out 
of this thanks to Curry’s fixed-point operator is expressive and led to new 
meaningful insights in topology (Amadio and Curien, 1998; Longo, 
2001). 
That said, almost each time mathematicians tried to avoid the re­
quirements of only considering meaningful statements — by trying to 
replace it with the rigour of a pure formalism — they ended up with 
flaws in the system leading to a syntactic contradiction. 
A common feature of many such paradoxes is that they rely on some 
form of encoding. This is extremely present in G¨odel’s initial incom­
pleteness theorems: the goal was after all to encode provability of 
arithmetic statements in arithmetic itself. This is powerful, but the 
process tends to remove all links to intuition and meaning to achieve 
certain results, and was already decried by Girard (2001): “the practice 
of artificial codings corresponds to a logical ideology for which the 
language is a mere bureaucratic device, with no intrinsic properties”. 
The intuition-removing effect of encoding can also be found in another 
object commonly studied by set theory: bijections. There are many — 
discontinuous — bijections between R and R2, and creating one is trivial 
by “mixing” the numerical representations of two numbers, a very 
simple encoding indeed. Here, the encodings typically destroys conti­
nuity, one of our main sources of intuition, or what “gives meaning” to 
the real line and spaces. This makes such objects harder to handle than 
ones related by stronger relations (ideally an isomorphism). The only 
operations doable on such structures are syntactic operations: rigorous, 
but rarely meaningful. 
Thus, despite a century of debate over the role of formalism and 
numerous attempts to push mathematicians to follow stricter rules, 
proofs remain social objects — the most stable of our conceptual con­
structions, yet the result of a historical inter-subjective activity. 
Let us give three examples of diverging reactions to objects that look 
like proofs, where the interplay between intuition, sense and formal 
(even computational) rigour has had a major role. First, we can take the 
first “proof” of the four-colour theorem in 1976, which relied one the 
automatised checking of a disjunction of 1834 cases, as well as extensive 
manual checking8 (Wilson, 2003). Its status as proof was disputed for at 
least two decades, as it left most unsatisfied with the methodology and 
the reliance on automated checking. This lasted at least until further 
proofs appeared and the automated checking of the proof was for­
malised in Coq, requiring mathematicians to have to accept a much 
smaller codebase.9 In a way, the proof did not fully respect the norms of 
being both manually checkable and of bringing meaning and new 
intuitions to mathematicians who would check the proof10. 
One key issue in this direction is that program correctness is not 
decidable, as a consequence of Rice’s theorem Rogers (1987) — more­
over “few” properties of programs are even semidecidable (Giannini and 
Longo, 1984). That is, no program can be guaranteed to find a proof of 
the correctness of other programs, and no general method can perfectly 
rigorously prove the correctness of very long programs, without special 
insights. So, the solution comes from the robustness and transparency of 
the programming style: Coq is such an example, thanks to its robust 
foundation in Type Theory. This is thus a matter of the historical for­
mation of a programming style after avoiding logical paradoxes (Cur­
ry’s, Girard’s …). By this, we can trust a proof written and deduced in 
Coq, even though a proof of correctness is impossible in the general case. 
On the opposite side, we have the article on Quantum Bit Commit­
ment published in 1993 by Brassard, Cr´epeau, Josza and Langlois 
(Brassard et al., 1993). This article featured a short proof (the whole 
article being 9 pages long) of a surprising result, and was quite well 
received initially — and published in what might have been the most 
prestigious venue in the field at the time. It had all the required elements 
to be accepted as a proof: it was formal, made sense, and gave new in­
tuitions about the field. And yet it was shown to be invalid three years 
later, with the central counterargument fitting in a single line (Lo and 
Chau, 1997). One common explanation for this gap is that the field has 
very little reliance on intuition (or rather, offers very little in the way of 
intuition for people to work with), and that most proofs depend on 
formal sign manipulation. Non-visual formalisation as a tool of creation 
(and not only of verification) might then be easily misled. 
Our last example is contemporary, and illustrates the fuzzy boundary 
between proof and non-proof. In 2012, Shinichi Mochizuki put online a 
string of four preprints proposing a proof of the abc conjecture,11 
totalling 646 pages in the versions last updated in December 2020 — 
and requiring the assimilation of nearly 2000 pages of his previous 
research on anabelian geometry. After eight years, the community is still 
undecided about the solidity of the proof, due to both its length and its 
complexity. After the first few years of general evaluation, the debate 
has now focused on Corollary 3.12 of the fourth article. This corollary 
states that the theory developed over all the preceding works applies to 
the problem at hand. The problem is then one of semantics, and not one 
of syntax: does the new theory apply to the object considered, and how 
would that make sense? Interestingly, quite a few arguments against the 
proof are linked to heuristics developed over time and not to formal 
considerations of what proofs are. Central to this is the fact that the new 
theory has so far found no other application or usable intermediate 
result, which would be a first for a theory of this scale. So far, the 
mathematical community is still debating the issue12 (Castelvecchi, 
2015; Roberts, 2019). 
Considering proofs — or at least verified knowledge — as social 
objects is not a new idea, as Henri Poincar´e wrote in 1905: “Anything 
objective must be common to multiple minds, and consequentially be 
8 Ironically, there was a small — and fixable — gap in the original proof, 
located in the “human” side of the proof and not in the massive disjunction 
(Wilson, 2003). 
9 One could then interpret proof checks as force multipliers, allowing math­
ematicians to only have to check a small kernel that then bootstraps by building 
upon itself up to the point where it can check the whole proof. 
10 One could wonder about the status of matrix multiplication algorithms, 
whose complexity grows further and further away from most mathematicians’ 
capacity to review as they get steadily closer to the ̃O(n2) bound, with proofs 
now regularly sporting more than 30 pages of formalism (Gall, 2014; Alman 
and Williams, 2021). One wonders if the proofs of future ̃O(n2+ε) algorithms 
will have lengths in Ω(1
ε).  
11 The abc conjecture was proposed in 1985, and constrains the relative sizes 
of c and abc, where a, b, and c are three positive integers who are relative 
primes and satisfy a + b = c.  
12 There are social factors making this issue more complex. First, the author 
refused to leave his home country of Japan to talk about his solution abroad, 
limiting the opportunities for others to exchange with him. Second, he recently 
published the four articles in a journal for which he is chief editor, which is 
unorthodox and has been badly received — although he recused himself from 
the process. 
E. Blanchard and G. Longo                                                                                                                                                                                                                   

BioSystems 204 (2021) 104396
5
transmitted from one to the other”.13 We could then follow Ren´e Thom 
and his provocation — “anything rigorous is insignificant” — and 
wonder about the relationships between abstractness, rigour, symbol­
ism, and their links to formalism, being careful not to assume one is the 
other (Thom, 1973). 
The “linguistic turn” at the beginning of the 20th century made us 
think we could identify these notions, with formalised axioms and well 
chosen strings of symbols handled by appropriate rewriting rules trying 
to replace the abstract symbolic and rigorous practice of mathematics, 
beyond the question of meaning and context. This attempt to limit the 
legitimate expressions of meaning in non formal but rigorous settings 
has consequences on other fields, which we will now investigate. 
3. Tempered expectations 
After the failure of the end-goal of the Hilbertian project, mostly 
through G¨odel’s incompleteness theorems, mathematicians had to revise 
the objective of mathematics themselves. More concretely, and well 
beyond the fantastic diagonal trick of G¨odel’s, interesting unprovable 
statements of Formal Number Theory (Arithmetic) of the 1970s and 
1980s more blatantly show the “concrete or mathematical” incom­
pleteness of formalisms. One can even point out where a “geometric 
judgements” or the use of “genericity” of a considered number, not 
expressible by induction, steps in the proof of these number theoretic 
examples. A close analysis of the formal unprovability of Kruskal- 
Friedman “tree-theorem” (Harrington et al., 1985) and Girard’s 
Normalization theorems (Girard et al., 1989) is developed in Longo 
(2011). 
Then, gone was the ideal of finding a complete set of formal axioms 
and theorems and their associated proofs in a search for “complete 
knowledge”. Hence Hilbert’s speech of “We must know — We will 
know”,14 in response to du Bois-Reymond’s ignorabimus speech on the 
limits of scientific knowledge in 1872 (McCarty, 2004) and in search for 
the “definitive solution” to the problem of foundation, as said by Hilbert 
of mathematics in the 1920s. Fortunately, in science, there are rarely 
definitive solutions. So, the focus went to the interplay of axioms and 
theories, to find their relationships with one another, to find which re­
sults depend on which ones, continuing, in a sense, Hilbert’s 1898 
seminal work in the foundations of Geometry and the diversity of 
axiomatic systems (Hilbert, 1902). Thus, the field went from a search for 
a unique set of rigorous “truths” to a relativising framework and many 
think about mathematical proofs in a broader sense - in reference to the 
historicity and contextuality of mathematical activity “generated in our 
space of humanity from a human activity”, as written by Husserl in 1933 
while stressing the “crisis” of the formalist approach to knowledge 
(Husserl, 1970). 
One of the few common formal invariants is the use of the axiom of 
choice. This axiom plays an interesting role as it tends to help our in­
tuitions and be a boon when it comes to proving new results. That said, 
in many cases, this is only a shortcut, and alternative proofs that do not 
rely on it are often found in the following years — or the result is shown 
to imply or be equivalent to the axiom of choice. 
Mathematics show us that almost any important proof, even if it is of 
an a posteriori fully formalisable statement, requires new concepts and 
structures, and often an investigation of their properties, in a dynamic 
construction of meaning. This is true from Euclid’s first theorem to 
Wiles’ proof of Fermat’s conjecture. The first requires a reference to the 
meaning of breadthless continuous lines: two continuous trajectories 
intersecting in good conditions produce one and only one point, assumes 
the first theorem in the first book of Euclid’s — and this also defines 
continuity for a breadthless line. Its foundation is anchored in lines as 
(continuous) motion, as “produced” as Euclid says, and on the Greek 
metaphysics of pure ideas. The second refers to the historical meaning 
construction of the Galois representations of the semi-stable modular 
curves and the corresponding original notions and properties that 
emerge in Wiles’ proof. 
There still exists a dynamic struggle on the question of formalism, 
which still plays a role, but mostly to rigorously check proofs. On one 
side, we have the hard border made by impossibility results that limit 
any attempt at a full formalisation of non-trivial theories. On the other 
side, we have a frustration with the limits of mathematics as they are 
performed, with the many errors that are eventually found in published 
work despite the peer-review.15 A human example of this struggle can be 
found in the career of Fields medalist Vladimir Voevodsky. His efforts on 
formal verification — through the development of univalent founda­
tions — might have been motivated by the doubts related to potential 
errors in his own published work, one of which was confirmed only after 
15 years (for more information, see Andrei Rodin’s article on “Voe­
vodsky’s unfinished project: filling the gap between pure and applied 
mathematics” in this very issue or Rodin (2020)). 
In recent years automatic proof assistance has been making miracles, 
particularly in very abstract areas of mathematics, Voevodsky’s work 
being just one example. Robust systems help mathematicians develop 
incredibly complex computations, by formally implementing some 
fragments of proofs that can be fully carried on by a machine. Note 
though, that the formal provability of a statement is undecidable, thus it 
is up to the mathematician experience and good taste to understand 
which fragments of its reasoning may be soundly passed over to a ma­
chine. When it works, the mathematician is then free to “think”, while 
passing to a machine some boring computations, that is, to invent new 
concepts and structures, what really matters in mathematics since the 
invention of the breadthless line. The other advantage is also that errors 
tend to appear in the computational parts of a proof, hidden in “minor” 
lemmas, as it was in Poincar´e’s early version of the Three Body problem 
— avoiding them by machines is just fantastic. 
The search for certainty, as pursued by Voevodsky — but this was 
also Martin-L¨of’s quest after Girard’s paradox demolished his early Type 
Theoretic construction — is a sound concern, in particular for the cer­
tainty in the robustness of a proof. Proving theorems, and doing so 
correctly, is surely essential to mathematics, and it is the mathemati­
cian’s job. However, it is even more important to “understand” which 
are the theorems worth proving. 
The philosophical teachings of the G¨odelian limits and of more 
recent formal impossibility results in number theory have, alas, been 
often ignored beyond the horizons of their initial field. Interestingly, 
despite the limited communication between the fields when it comes to 
the philosophical underpinnings of formalism, physics has also greatly 
evolved over the 20th century, with a similar direction. The initial end 
goal of physics16 was to obtain a predictive power over dynamic systems 
that was hindered only by instrumental capacity — and hence 
unboundedly growing. 
This idea played well with technological advances, from logarithm 
tables for artillery ballistic computations that are performed manually to 
the more intricate computational physical models designed in the sec­
ond half of the 20th century. Representing objects as single points with 
finite-precision arithmetic means that one can be exact and rigorous, 
and running the same predictive program over the same inputs gives the 
exact same results. Some elements were considered to be random in 
13 “Ce qui est objectif doit ˆetre commun `a plusieurs esprits, et par cons´equent 
pouvoir ˆetre transmis de l’un `a l’autre.” as quoted in Thomas (2015).  
14 Wir müssen wissen — wir werden wissen. 
15 Although many errors can be found in work before the 20th century, there is 
a perceived recent increase among some mathematicians — such as those of the 
Xena project — sometimes blamed on the working conditions which increase 
the workload of reviewers because of extreme publishing pressure.  
16 And, it could be argued, of mathematics too in some of their earliest 
incarnation as a computational tool for Babylonian and Indian astronomy 
(Rochberg, 2002). 
E. Blanchard and G. Longo                                                                                                                                                                                                                   

BioSystems 204 (2021) 104396
6
appearance, as the product of continuous non-linear dynamics, but this 
randomness was a classical one, linked to physical fluctuations/pertur­
bations and mathematical non-linearity, not to a mathematical limit of 
what is knowable. 
However, this project also ran into an immovable object with the 
elaboration of Heisenberg’s uncertainty principle, nearly simulta­
neously with mathematics — being published in 1927, barely four years 
before G¨odel’s incompleteness theorems. The discrete character of the 
black-body radiation discovered in 1900 by Max Planck could have 
bolstered this interpretation, limiting the impact of continuity (and the 
corresponding unavoidable noise). As it turned out, what physicists 
found instead was a completely new framework where non- 
deterministic fluctuations made impossible any prediction on a small 
scale. The “rigour” of exact computation and repeated iterations fails, as 
Schr¨odinger’s equation does not give us any single answer, but at best a 
superposition of results or a distribution of probabilities. 
In a way, this is much worse than the initial measuring problems of 
classical physics, but it forces the redefinition of physics end-goals 
themselves. Instead of trying to obtain unbounded predictive power, 
rigour becomes knowing which elements behave deterministically, to 
which precision they can be known, and to understand the probability 
distributions when they cannot. 
The nature of noise is then questioned, and split into a randomness 
due to classical physical phenomena — the random fluctuations of a 
double pendulum, say, and its non-linear dynamics, against quantum 
uncertainty. Knowing a system then acquires a different meaning, even 
randomness depends on the intended theory, on the frame of physical 
meaning. 
4. Foundational back and forth, biology versus mathematics 
4.1. The Dogmatic gene 
Hilbert’s program was rediscovered by molecular biology, after 
World War II. The genocentric approach, though, grounded on the 
isolation of “chromosomes” by colouring them, is due to several authors 
and goes back to the end of the 19th century (Keller, 2009). Soon, 
research focused on them as an immensely important chemical structure 
at the core of all biological activity — which luckily was also easy to see. 
A high moment of their role in the assumption of their “completeness” 
with respect to ontogenesis is surely due to the discovery in Beadle and 
Tatum (1941) that a mutation in an “eye colour gene” was responsible 
for a change in an enzyme acting in the metabolic pathway of pigment 
synthesis (Horowitz, 1995) The “one-gene/one enzyme” hypothesis 
became then a founding principle of molecular biology, that spanned at 
least 60 years. Still in 1999, F. Collins claimed (Collins, 1999), on these 
grounds and in view of the correlations to phenotypes attributed to 
genes, that he, as head of the Human Genome Project, expected to find 
about 80,000 genes in the Human DNA, a lot more than the 16,000 genes 
found in C. Elegans, a microscopic worm of 1,000 cells — a significant 
difference, on the behalf of our human dignity. Two years later human 
genes were claimed to be 25,000, soon downplayed to 20,000. The 
number of C. Elegans’ genes remained stable. In between these 60 years, 
the grandiose march of the completeness assumption had its strong point 
in the “Central Dogma” (CD) of molecular biology: 
The transfer of information from nucleic acid to nucleic acid, or from 
nucleic acid to protein may be possible, but transfer from protein to 
protein, or from protein to nucleic acid is impossible. Information 
means here the precise determination of sequence, either of bases in 
the nucleic acid or of amino acid residues in the — Francis Harry 
Crick (1958). 
In spite of some subtle interpretations still trying to justify the CD, 
enriched by some recursive cycle from RNA to DNA, it remains an 
instructive perspective on life with a predominantly unidirectional flow 
of coded information from DNA to RNA and then to proteins (Istrail 
et al., 2007). 
Of course, even in the perspective of the CD, more is needed to un­
derstand the DNA as the “complete book of life” containing the in­
structions for ontogenesis — “written by God”, as stated by F. Collins in 
2001 upon announcing the decoding of the DNA. One needs proteins to 
completely describe phenotypes. They do so, since the bio-chemical 
cascades taking place in a cell, are “a Cartesian mechanism”, a “bool­
ean algebra” as “found in computers”, gears based on “the exact, ste­
reospecific” interactions (geometric and chemical affinities) of 
macromolecules in the cell (Monod, 1970). The old “key-lock para­
digm”, proposed for small molecules at the end of 19th century, was thus 
transferred to huge ones, an essential feature of the computational 
model of the organism (Istrail et al., 2007; Karr et al., 2012). 
We recognise in the CD, enriched with the assumption of exact, 
mechanical macro-molecular cascades in the cell, the hypothesis of 
completeness of formal axioms: these are finite strings of signs, from 
which “potentially mechanisable” deduction rules, as association of 
strings to strings (gears of a boolean algebra), would allow to deduce all 
true assertions of mathematics. Thus, life dynamics originate in finite 
strings of G, C, A and T, four chemicals, an alphabet: “The surprise is that 
the genetic specificity is written down, not with ideograms as in Chinese, 
but with an alphabet as in French, or rather in Morse” (Jacob, 1965). We 
also write mathematical axioms in an finite alphabet (some logograms or 
special symbols are also required, such as ˝→′′). This is rarely done with 
ideograms as these need to be interpreted, are highly context dependent, 
rich of ambiguities, far from the “unshakable certainties” that the axi­
omatic/alphabetic approach (and the focus on DNA) may give to 
knowledge. Both the analysis of axioms and the decoded DNA would 
give the Hilbertian “definitive solution” to the main problems of biology, 
as claimed by the decoders of the human genome (Gilbert, 1992), 
promising in 2001 to defeat genetic diseases in a few years, to “Eliminate 
Suffering And Death Due To Cancer By 2015” (von Eschenbach, 2003) 
and a lot more (see Longo (2018b) for references and a discussion). The 
complete driving role of genes, as programming feature of DNA (Dan­
chin, 2002, 2008), lead to invent “genes for” such things as language 
(Cohen, 1998) and aggressive behavior (Perbal, 2013). 
4.2. Beyond axioms and dogmas 
Let’s go back to mathematics. In Type Theoretic programming lan­
guages, a program is a (formal) proof — this is the rigorous frame that 
makes Coq a very reliable proof assistant: Coq brings in programming 
the rigour of (constructive) Proof Theory. Conversely, a formal proof, in 
Coq, is a program. Yet, we learned form the story we told above that the 
notion of mathematical proof must be complemented by “human sense”, 
more precisely by the invention of concepts and structures, beginning 
with the first and most fundamental one of western mathematics: 
Euclid’s breadthless line. Moreover, the “concrete or mathematical 
incompleteness” of proofs as programs even applies to pure combina­
torial statements of Number Theory, as mentioned above: geometric 
judgements step in even along these proofs, as part of a shared creation 
of meaning, shared in history (Longo, 2011). This meaning is transfer­
able to other human contexts as it is based — in mathematics more than 
in any other form of knowledge — in action in space, in pre-linguistic 
constructions that single out invariant trajectories, in subitising. The 
construction of the concept, in language, is a further stabilisation in 
humans of those pre-linguistic experiences, made definitely robust by 
the invention of writing. But this (finite, alphabetic) writing does not 
allow even a posteriori to reconstruct the constitutive path that gives 
meaning to it, since reference to meaning, action, symmetries and ges­
tures provably enter into the proof. A historical context thus gives 
meaning to the practice of mathematics: its human universality and 
historical stability is due to its basis in prelinguistic activities in the 
ecosystem, as hinted above. The aim of mathematics though is to turn 
these historical constructions into maximally invariant concepts and 
E. Blanchard and G. Longo                                                                                                                                                                                                                   

BioSystems 204 (2021) 104396
7
proofs. In a sense, historicity, as origin of meaning making, enables and 
constrains the mathematical invention, later stabilised by forgetting 
history as much as possible — though without ever reaching a 
meaning-independent complete reconstruction. 
The formal approach to foundation did contribute to a rich debate 
during the 20th century: it gave us a robust notion of rigor, highly 
required after a century rich of mathematics as well as of mistakes due to 
fuzzy definitions and proofs — such as the famous mistakes concerning 
the continuity of sum of series of continuous functions, of left and right 
derivability — mostly due to wrong alternation of quantifiers and 
imprecise writing. The result has been a monument of rigour and results, 
the detection of errors and paradoxes and their fixing, lively and pro­
ductive controversies. More importantly, it set the guidelines to the in­
vention of formal computing machines, since the “negative results” of 
the 1930s. As hinted in Section 1, since the irrationality of 
̅̅̅
2
√
, negative 
results opened new ways for knowledge construction, from Poincar´e’s 
Three Body Theorem, to G¨odel’s theorem to Einstein’s EPR “paradox”.17 
The myth of the “positive” construction of knowledge, typical of Hil­
bertian views or of unbounded genocentrism is unfounded: mathematics 
is not done by formally deriving theorems from pre-given axioms, nor 
biology by discovering new “genes of …” — i.e. by deducing phenotypes 
from finite sequences of letters of DNA. Even within Computer Science, 
the limits of formalisms stimulated the debate between “syntax ori­
ented” and “semantic oriented” approaches in Theory of Programming 
(Scott, 1980; Longo, 2001). 
As for biology, mainstream molecular biology highlighted many 
fundamental molecular “mechanisms” and proposed enormous amounts 
of data. Longo and Mossio (2020), compares the geo-centric (Ptolemaic) 
approach in astronomy and the geno-centric one in biology. The 
fantastic astronomy in the Arabic world (7th to 14th centuries) provided 
us with fantastic data and correlations: all visible celestial bodies were 
named, classified and their movements correlated. The Alphonsine 
Tables of the late 15th century allowed geographic explorations that 
changed the world. Yet, the theoretical frame was totally wrong. This 
means that any assertion correlating data, by causality typically, such as 
“the position of Jupiter in this constellation causes Mars to be there …“, 
was wrong. 
Similarly, the “genes for everything”, from intelligence (Stewart, 
2004) to marital fidelity (Young et al., 1999) or the ad hoc “oncogenes”, 
one for each cancer type, diverted research from understanding and 
fighting this life threatening disease (Sonnenschein and Soto, 1999; 
Weinberg, 2014). Typically, it distorted analyses from the investigation 
of carcinogenes, such as endocrine disruptors, since these mostly small 
molecules would not fit the exact, boolean, key-lock paradigm of 
macromolecular interactions — thus, they could not interfere, by prin­
ciple, with molecular cascades and cellular receptors. Instead they do, 
with lower probabilities, but sufficient to disrupt cellular processes 
(Diamanti-Kandarakis et al., 2009). This is a catastrophic consequence 
of a bad philosophy of biology. A change of perspective is emerging from 
within molecular biology itself, in particular since the results on sto­
chasticity of gene expression in Elowitz et al. (2002). Similarly, proteins 
in the cell’s cytoplasm have large enthalpic random movements and at 
least 30 percent of them are considered to be “intrinsically unstruc­
tured”. Moreover, their shape and biological functionality depends on 
the context, from PH levels, to pressures and torsions on their physical 
structure, including on the DNA chromatin, to the presence of other 
proteins in their subcellular locales (Uversky, 2011). The new views 
belong still a minority and Prusiner’s ideas, who discovered prions in 
early 1980’s, were ousted for a decade as these badly folded proteins 
modify the conformation of close ones, against the CD (Prusiner, 1982). 
A philosophical bias made many ignore the evidence accumulated by 
some as above and by many others — the marginalisation of the 
epigenetic approach by C. Waddington and B. McClintock is yet another 
example (Keller, 1984). Today, an increasing trend in research closely 
considers the crucial role of the enthalpic quasi chaotic movements of 
macro-molecules, with changing chemical affinities according to the 
context. This justifies the richness of the interactions in the quasi tur­
bulence of the proteome, while stressing that they are canalised, con­
strained and selected by the context and the cell structure (Kupiec, 
2010). It is the context that “gives sense” by canalising and allowing to 
function many highly improbable macro-molecular networks that, in 
turn, contribute to the construction of the cellular, tissular, organismal 
and ecosystemic constraints (for a general role of constraints formation 
and dynamics in organismal biology, see Mont´evil and Mossio (2015)). 
In spite of the paradigm shift which is taking place, immense in­
terests forbid a faster pace for it. For example, Genetically Modified 
Organisms (GMOs) in agriculture are yet another consequence of the 
mechanistic philosophy of biology. They are a direct application of the 
Central Dogma: we can drive the plant in the ecosystem by manipulating 
the genetic information and program, somehow like formal axioms and 
rules completely drive a mathematical theory. The biological conse­
quences though are severe: typically, a major decrease of biodiversity 
(Bizzarri, 2012) and a destruction of the microbioma of roots (Kowal­
chuk et al., 2003). 
5. Historicity 
What is, in biology, the analogue of the way out from formalist 
frames (the complete coding of an organism in the DNA) by “contexts of 
historical creation of meaning”, as hinted above in reference to the role 
of meaning and of mathematical invention in proofs? 
The key is again in the role of the historical dynamics of life. Nothing 
in biology can be understood unless in a temporal perspective, both 
phylogenetic and ontogenetic. We stressed above the historical creation 
of meaning in mathematics, while acknowledging that at each epoch, 
mathematics’ aim is to give a “universal” definition of new concepts and 
structures, to pass beyond history by searching the maximal conceptual 
— and historical — invariance. In biology, historicity is even more 
radical. There is no way to define a “mouse” without referring to a 
history, no way to propose a historically stable concept of mouse. The 
notion of mouse is not a historical invariant. That is, no detailed 
description of its phenotypes is ever complete nor stable: other species 
may fit it or some phenotype may differ or chance over time and still you 
may have a mouse. Mice do change, as they evolve, typically they 
differentiate in strains, a major concern as for the diversity of their re­
actions in experimental work, where measurement strictly depend on 
the animal’s history, in the broadest sense (Mont´evil, 2019). In short, the 
only way to define a mouse is by its phylogenetic history (Lecointre and 
Le Guyader, 2001). 
Similarly, homologies — common evolutionary origins of an organ, 
as in Gould (2002) — allow us to understand organ structures and 
functions: the strange structure of the wings of bats is only understood as 
homologous to our hands, while the wings of birds are homologous to 
our arms. Insects’ wings are not homologous to birds’, yet they have the 
same function (they are analogous). Some birds have wings that do not 
allow to fly (penguins), in some cases this function has been lost in 
relatively recent evolutionary times (the New Zealand kiwi). Most of 
evolutionary novelty comes from historical readjustment and adaptation 
of previous functions and structures that “make sense” in new contexts 
— they acquire new functions and change structure and may thus be 
understood (West-Eberhard, 2003). Ex-aptation, as adaptation ex-post, 
is a key notion in evolutionary biology Gould (2002). No formal defi­
nition of a structure nor of a function allows to characterize a phenotype: 
only its history may help to understand both structure and function. 
Consider also the physical nonsense of the blood vessels and nerves 
passing in front of the eye, between cellular sensors and the incoming 
light (as Helmotz allegedly observed, we should fire God for making 
17 See Longo (2019) for a comparative analysis and their creative fall-out. 
Note also that the EPR paradox of entanglement is at the core of Quantum 
Computing (Zorzi, 2016). 
E. Blanchard and G. Longo                                                                                                                                                                                                                   

BioSystems 204 (2021) 104396
8
such an engineering mistake), can only be understood by the vertebrates 
eye’s phylo- and onto-genesis. Cephalopods have more suitably 
back-vascularised and innervated eyes, because of the very different 
phylogenetic and ontogenetic paths of their very modular brains, of 
which the eyes are an extension (Serb and Eernisse, 2008). 
In synthesis, a mathematical proof is also a historical creation of 
meaning, a biological organism is only a historical formation of an 
individual. 
A sense of history, both phylogenetic and ontogenetic, motivates 
alternative knowledge paths in biology. The formation of constraints to 
processes that evolve over time is at the core of the novel theory of or­
ganisms in Mont´evil and Mossio (2015); Mont´evil (2020). Constraints 
act upon processes, are conserved at the relevant time scales and are 
based on mutual dependence such that they both depend on and 
contribute to maintaining each other. In particular, cellular constraints 
canalise and enable, within cells, biochemical networks that are chem­
ically highly improbable (with almost zero probabilities as physical 
phenomena). These chemical processes then co-construct the constraints 
that make them possible. Variability is probably the main invariant in 
biology, canalised by organismal and ecosystemic historical contraints. 
A unified perspective is proposed in Soto et al. (2016). 
Let us note finally that the notion of gene is not “well-defined”, to put 
it in mathematical terms — in contrast to axioms and to the very notion 
of “axiom”. More precisely, the latter has had some historical plasticity, 
yet in each conceptual frame, mostly in continuity with previous ones, it 
found a rigorous frame. The notion of “gene”, instead, not only changed 
at least five times in the 20th century (Keller, 2009), but each time 
turned out to be fuzzy. First, early in the century, their were meant to be 
some parts of the chromosomes acting as “Mendelian genes” (directly 
related to phenotypes, like in the colors of peas). Later, they were 
associated to proteins and enzymes but this gradually turned out more of 
the many-to-many type, in contrast to the one-to-one claim in Beadle 
and Tatum (1941) — so that no linear nor functional deduction is in 
general possible from genes to proteins. More recently, alternative 
splicing and overlapping genes (Pavesi et al., 2018) destroy any myth of 
the complete encoding of the “information” for phenotypes in genes, 
viewed as well delimited segments of DNA, as well as of the leading role 
of genes in development and evolution: these dynamics largely depend 
on the epigenetic contexts, in the broadest sense. For example, pressure 
and torsions on the chromatin, as already mentioned, modify gene 
expression (Lesne et al., 2012) and this has a major role in embryo­
genesis in view of the physically changing constraints acting on cells. 
Genes have more the role of followers than of programmers in either 
process (West-Eberhard, 2003). So, hybrids of cave-fish — more than a 
hundred species of fish that have lost their eyes by evolving in caverns — 
do have eyes (Gatenby et al., 2011). The corresponding “genes”, what­
ever this notion may mean, or the parts of the DNA used in the formation 
of eyes, are still there, but no longer used by the organism in ontogen­
esis, in that context. 
Organisms are not self-organised processes, like flames or hurri­
canes, but a historically changing formation of constraints to flows of 
energy and matter, that enable the use of these flows to reconstruct the 
constraints (for example, the vascular systems constrains flows, thus 
enables metabolism, which contributes to the reconstruction of the 
constraint). Note that flames and hurricanes have an irreversible proc­
essual time, not a historical time: their physico-mathematical structure 
did not change in the last four billions years, in contrast to life. The time 
of history is a time of changing phase spaces (the space of pertinent 
observables and parameters) and marked by rare events (Longo, 2018a). 
The Darwinian principles of reproduction with variation and selection, 
can be seen as the a priori of life (reproduction with variation), in 
co-constituted contexts that may enable (as “positive selection”) the 
(new) possible or may exclude it as incompatible (do not enable it, or 
“negative selection”). These principles guide the intelligibility of life, 
both in phylo- and onto-genesis, as proposed in Soto et al. (2016). 
DNA is a major constraint and a physico-chemical trace of evolution 
inside each cell; it constrains Brownian flows of molecules and, thus, the 
type of possible proteins in a given moment and context. So, as the 
foundation of mathematics is the clarification of principles in a historical 
praxis — possibly by trying to write axioms — where invariance plays a 
major role, similarly the foundation of biology must refer to (Darwinian) 
principles of reproduction with modifications and reconstruction with 
correlated variations, enablement and negative selection, while taking 
the point of view of the organism and its dynamics in the historical time 
of an ecosystem. The role of axioms in mathematics and of DNA in 
biology, while preserving their relevance, are then re-understood in 
their historical process of formation, use and activity. 
6. Concluding remarks 
Besides the reference to Mont´evil and Mossio (2015), Soto et al. 
(2016) and correlated work in an organismal/evolutionary (historical) 
perspective, we did not venture in details in the ongoing re-inventions of 
foundational analysis in mathematics for biology as there are many 
growing projects, such as the work by Ehresmann (2017). A sort of 
Manifesto of the new trends, recovering the role of meaning and struc­
tures in meta-mathematical analyses may be found in the Editorial 
Policy of the ’’Annals of Mathematics and Philosophy’’(https://spartac 
us-idh.com/amp/MxPhi_en.pdf). A reference book along similar per­
spectives, with particular emphasis on contemporary mathematics, is 
Zalamea (2012). In biology, we point to the papers and new openings in 
“Organisms, a journal in biological sciences” and its Editorial. Time is 
required to appreciate the philosophical (and mathematical) relevance 
of the new ideas in mathematics proposed by “heterogenesis” in Sarti 
et al. (2019), among new bridging perspectives. They propose an anal­
ysis of the “genesis of and from diversity” in the changing space of ob­
servables and parameters: the historical frame of semiotics (their 
original field of interest) and of the Darwinian ecosystem along evolu­
tion. By this, they aim to describe by mathematics the dynamics of 
changing phase spaces, typical to historical sciences such as biology, 
which has no counterpart in physical morphogeneses. Our common 
perspective (see Longo (2018a); Mont´evil (2020)) sets a new founda­
tional bridge between mathematics and biology as it changes the most 
fundamental mathematical invariants of physical theories, the under­
lying symmetries in the “background spaces” (or in the “conditions of 
possibilities”) of all existing systems of equations for physics. As Weyl 
observes in 1952: “All a priori statements in physics have their origin in 
symmetry”, a remark justified by Noether’s theorems. These a priori 
symmetries characterize the pregiven structure of observables and pa­
rameters — an invariance unsuitable for theorising biological evolution 
as well as any historical science. 
In the approach hinted here, we first observe that Darwin’s first 
principle, the a priori of heredity, that is descent with modification, is a 
“non-conservation principle” for phenotypes. Thus we propose a 
fundamental breaking of symmetry, the non conservation of the space of 
all possible dynamics or phase space, at the core of biological evolution, 
as production of variability and diversity. This may also help to modify 
our bugged, mechanistic relation to the ecosystem, its diversity and its 
historicity. 
Conflicts of interest 
None. 
Acknowledgements 
We’re grateful to Lˆe Th`anh D˜ung (Tito) Nguỹˆen for his useful re­
marks and comments. This work was supported partly by the French PIA 
project “Lorraine Universit´e d’Excellence”, reference ANR-15-IDEX-04- 
LUE. 
Giuseppe Longo is Directeur de Recherche (DRE) CNRS, Centre 
E. Blanchard and G. Longo                                                                                                                                                                                                                   

BioSystems 204 (2021) 104396
9
Interdisciplinaire Cavaill`es, Ecole Normale Sup´erieure, Paris and 
Adjunct Professor, School of Medicine, Tufts University, Boston. He is a 
former Professor of Mathematical Logic and, later, of Computer Science 
at the University of Pisa. In the 1980s, he spent 3 years in the USA (U.C. 
Berkeley, M.I.T., Carnegie Mellon U.) as researcher and Visiting Pro­
fessor. Giuseppe Longo is founder and director (1990–2015) of Mathe­
matical Structures in Computer Science, a Cambridge University Press 
journal. Since the 1990s, he extended his research interests to the 
Epistemology of Mathematics and Theoretical Biology. He (co-)authored 
more than 100 papers and three books: with A. Asperti, on Categories, 
Types and Structures (M.I.T. Press, 1991); with F. Bailly, Mathematics and 
the natural sciences: The Physical Singularity of Life (Hermann, Paris, 2006; 
Imperial College Press, London, 2011); with M. Mont´evil, Perspectives on 
Organisms: Biological Time, Symmetries and Singularities (Springer, Berlin, 
2014). With A. Soto and D. Noble, Longo edited (and co-authored six 
papers of) a special issue of Progress in Biophysics and Molecular Biology, 
From the century of the genome to the century of the organism: New theo­
retical approaches, 2016. He directed a research project at IEA-Nantes 
(2014-18) on the concept of law, in human and natural sciences, see 
the book: Lois des dieux, des hommes et de la nature, G. Longo (Editeur), 
Spartacus IDH, Paris, 2017. Web: Association Cardano: http://cardano. 
visions-des-sciences.eu/ 
Enka Blanchard is a post-doctoral researcher currently working on 
the Digitrust project at the University of Lorraine. They initially studied 
mathematics and computer science at ENS Paris and University Paris 
Diderot. In 2019, they defended their PhD on human aspects of security 
systems, with a focus on authentication and voting systems, which 
received the PSL award for best PhD at the Science/Humanities inter­
face. They currently work on several different transdisciplinary issues 
such as usability of security, institution design, and the interactions 
between crip theory and queer theory. Web: http://koliaza.com. 
References 
Alman, J., Williams, V.V., 2021. A refined laser method and faster matrix multiplication. 
In: Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA). 
SIAM, pp. 522–539. 
Amadio, R.M., Curien, P.L., 1998. Domains and Lambda-Calculi, vol. 46. Cambridge 
University Press. 
Anobile, G., Arrighi, R., Burr, D.C., 2019. Simultaneous and sequential subitizing are 
separate systems, and neither predicts math abilities. J. Exp. Child Psychol. 178, 
86–103. https://doi.org/10.1016/j.jecp.2018.09.017. http://www.sciencedirect.co 
m/science/article/pii/S0022096518300870. 
Barendregt, H., 1984. The lambda calculus, its syntax and semantics. In: Studies in Logic 
and the Foundations of Mathematics, vol. 103. 
Beadle, G.W., Tatum, E.L., 1941. Genetic control of developmental reactions. Am. Nat. 
75, 107–116. 
Berthoz, A., 1997. Le sens du mouvement. Odile Jacob. 
Bizzarri, M., 2012. The New Alchemists: the Risks of Genetic Modification. Wit Press. 
Blasdel, G.G., Mitchell, D.E., Muir, D.W., Pettigrew, J.D., 1977. A physiological and 
behavioural study in cats of the effect of early visual experience with contours of a 
single orientation. J. Physiol. 265, 615–636. https://doi.org/10.1113/jphysiol.1977. 
sp011734. 
Brassard, G., Crepeau, C., Jozsa, R., Langlois, D., 1993. A quantum bit commitment 
scheme provably unbreakable by both parties. Proceedings of 1993 IEEE 34th 
Annual Foundations of Computer Science, pp. 362–371. https://doi.org/10.1109/ 
SFCS.1993.366851. 
Cajori, F., 1928. A History of Mathematical Notations, vol. 1. Open Court Publishing 
Company. 
Canevaro, M., 2018. Majority Rule vs. Consensus: the Practice of Democratic 
Deliberation in the Greek Poleis. Edinburgh University Press, pp. 101–156. https:// 
doi.org/10.3366/edinburgh/9781474421775.003.0005. 
Cao, J., Pang, Y., Li, X., 2016. Pedestrian detection inspired by appearance constancy and 
shape symmetry. In: Proceedings of the IEEE Conference on Computer Vision and 
Pattern Recognition, pp. 1316–1324. 
Castelvecchi, D., 2015. The biggest mystery in mathematics: Shinichi Mochizuki and the 
impenetrable proof. Nature News 526, 178. https://doi.org/10.1038/526178a, 
18509. http://www.nature.com/news/the-biggest-mystery-in-mathematics-shi 
nichi-mochizuki-and-the-impenetrable-proof-1. 
Chiurazzi, G., 2017. Dynamis. Ontologia Dell’incommensurabile, vol. 9. Guerini & 
Associati. 
Church, A., 1941. The Calculi of Lambda Conversion. Princeton University Press, USA.  
Cohen, P., 1998. Excited researchers think they have found a gene for language. New Sci. 
2119, 77. 
Collins, F.S., 1999. Medical and societal consequences of the human genome project. 
N. Engl. J. Med. 341, 28–37. PMID: 10387940.  
Coquand, T., 1986. An analysis of Girard’s paradox. In: Symposium on Logic in Computer 
Science. IEEE Computer Society Press, pp. 227–236. 
Crick, F.H., 1958. On protein synthesis. In: Symposia of the Society for Experimental 
Biology, Number XII: the Biological Replication of Macromolecules. Cambridge 
University Press, pp. 138–163. 
Danchin, A., 2002. The Delphic Boat: what Genomes Tell Us. Harvard University Press. 
Danchin, A., 2008. Bacteria as computers making computers. FEMS Microbiol. Rev. 33, 
3–26. 
Dehaene, S., 2011. The Number Sense: How the Mind Creates Mathematics. Oxford 
University Press, USA.  
Diamanti-Kandarakis, E., Bourguignon, J.P., Giudice, L.C., Hauser, R., Prins, G.S., 
Soto, A.M., Zoeller, R.T., Gore, A.C., 2009. Endocrine-disrupting chemicals: an 
endocrine society scientific statement. Endocr. Rev. 30, 293–342. 
Ehresmann, A., 2017. Applications of Categories in Biology and Cognition, Categories for 
the Working Philosopher. Oxford University Press. 
Elowitz, M.B., Levine, A.J., Siggia, E.D., Swain, P.S., 2002. Stochastic gene expression in 
a single cell. Science 297, 1183–1186. 
von Eschenbach, A.C., 2003. NCI sets goal of eliminating suffering and death due to 
cancer by 2015. J. Natl. Med. Assoc. 95, 637. 
Gall, F.L., 2014. Powers of tensors and fast matrix multiplication. In: Nabeshima, K., 
Nagasaka, K., Winkler, F., Sz´ant´o, ´A. (Eds.), International Symposium on Symbolic 
and Algebraic Computation, ISSAC ’14. ACM, Kobe, Japan, pp. 296–303. https:// 
doi.org/10.1145/2608628.2608664. July 23-25, 2014.  
Gatenby, R.A., Gillies, R.J., Brown, J.S., 2011. Of cancer and cave fish. Nat. Rev. Canc. 
11, 237–238. 
Giannini, P., Longo, G., 1984. Effectively given domains and lambda-calculus models. 
Inf. Contr. 62, 36–63. https://doi.org/10.1016/S0019-9958(84)80009-1. http:// 
www.sciencedirect.com/science/article/pii/S0019995884800091. 
Gilbert, W., 1992. A Vision of the Grail. Harvard University Press. 
Girard, J.Y., 2001. Locus solum: from the rules of logic to the logic of rules. Math. Struct. 
Comput. Sci. 11, 301–506. https://doi.org/10.1017/S096012950100336X. 
Girard, J.Y., Taylor, P., Lafont, Y., 1989. Proofs and Types, vol. 7. Cambridge University 
Press. 
Gould, S.J., 2002. The Structure of Evolutionary Theory. Harvard University Press. 
Harrington, L.A., Morley, M.D., ˇScedrov, A., Simpson, S.G., 1985. Harvey Friedman’s 
Research on the Foundations of Mathematics. Elsevier. 
Heath, T.L., 1908. The Thirteen Books of Euclid’s Elements. Cambridge University Press. 
Helmig, C., Steel, C., 2020. Proclus. In: Zalta, E.N. (Ed.), The Stanford Encyclopedia of 
Philosophy. Fall 2020. Metaphysics Research Lab, Stanford University. 
Hilbert, D., 1902. The Foundations of Geometry. Open court publishing Company. 
Horowitz, N.H., 1995. One-gene-one-enzyme: remembering biochemical genetics. 
Protein Sci. 4, 1017–1019. 
Huffman, C., 2019. Pythagoreanism. In: Zalta, E.N. (Ed.), The Stanford Encyclopedia of 
Philosophy. Fall 2019. Metaphysics Research Lab, Stanford University. 
Husserl, E., 1970. The Crisis of European Sciences and Transcendental Phenomenology: 
an Introduction to Phenomenological Philosophy. Northwestern University Press. 
Istrail, S., De-Leon, S.B.T., Davidson, E.H., 2007. The regulatory genome and the 
computer. Dev. Biol. 310, 187–195. 
Jacob, F., 1965. G´en´etique cellulaire: Leçon inaugurale prononc´ee le vendredi 7 mai 
1965. Coll`ege de France, Paris.  
Karr, J.R., Sanghvi, J.C., Macklin, D.N., Gutschow, M.V., Jacobs, J.M., Bolival Jr., B., 
Assad-Garcia, N., Glass, J.I., Covert, M.W., 2012. A whole-cell computational model 
predicts phenotype from genotype. Cell 150, 389–401. 
Keller, E.F., 1984. A Feeling for the Organism, 10th Aniversary Edition: the Life and 
Work of Barbara McClintock. Macmillan. 
Keller, E.F., 2009. The Century of the Gene. Harvard University Press. 
Kowalchuk, G.A., Bruinsma, M., van Veen, J.A., 2003. Assessing responses of soil 
microorganisms to gm plants. Trends Ecol. Evol. 18, 403–410. 
Krystek, M.P., 2021. The Term ‘angle’ in the International System of Units, 01023 arXiv: 
2101.  
Kupiec, J.J., 2010. On the lack of specificity of proteins and its consequences for a theory 
of biological organization. Prog. Biophys. Mol. Biol. 102, 45–52. 
Lang, M., 1957. Herodotos and the abacus. Hesperia: J. Am. School Classical Stud. 
Athens 26, 271–288. http://www.jstor.org/stable/147100. 
Lecointre, G., Le Guyader, H., 2001. Classification phylog´en´etique du vivant. Belin. 
Lesne, A., Becavin, C., Victor, J.M., 2012. The condensed chromatin fiber: an allosteric 
chemo-mechanical machine for signal transduction and genome processing. Phys. 
Biol. 9, 013001. 
Lo, H.K., Chau, H.F., 1997. Is quantum bit commitment really possible? Phys. Rev. Lett. 
78, 3410. 
Longo, G., 2001. Some topologies for computations. In: Invited paper, proccedings of the 
Conference G´eom´etrie au XX`e si`ecle. 
Longo, G., 2009. Theorems as constructive visions. In: Proceedings of ICMI 19 
Conference on Proof and Proving. Springer. 
Longo, G., 2011. Reflections on concrete incompleteness. Philos. Math. 19, 255–280. 
Longo, G., 2018a. How future depends on past and rare events in systems of life. Found. 
Sci. 23, 443–474. 
Longo, G., 2018b. Information and causality: mathematical reflections on cancer biology. 
Organisms. J. Biol. Sci. 2, 83–104. 
Longo, G., 2019. Interfaces of incompleteness. In: Systemics of Incompleteness and 
Quasi-Systems. Springer, pp. 3–55. 
Longo, G., Mossio, M., 2020. Geocentrism vs genocentrism: theories without metaphors, 
metaphors without theories. Interdiscipl. Sci. Rev. 45, 380–405. 
Longo, G., Viarouge, A., 2010. Mathematical intuition and the cognitive roots of 
mathematical concepts. Topoi 29, 15–27. 
Maffei, L., 1998. Il mondo del cervello. Laterza. 
E. Blanchard and G. Longo                                                                                                                                                                                                                   

BioSystems 204 (2021) 104396
10
Martin-L¨of, P., Sambin, G., 1984. Intuitionistic Type Theory, vol. 9. Bibliopolis Naples. 
McCarty, D.C., 2004. David Hilbert and Paul du Bois-Reymond: Limits and Ideals. 
Gruyter. 
Monod, J., 1970. Le hasard et la n´ecessit´e. Le Seuil. 
Mont´evil, M., 2019. Measurement in biology is methodized by theory. Biol. Philos. 34, 
35. 
Mont´evil, M., Mossio, M., 2015. Closure of constraints in biological organisation. 
J. Theor. Biol. 372, 179–191. 
Mont´evil, M., 2020. Historicity at the hearth of biology. Theor. Biosci. https://doi.org/ 
10.1007/s12064-020-00320-8. 
Ober, J., 2011. Wealthy hellas. J. Econ. Asymmetries 8, 1–38. 
Pagel, M., Atkinson, Q.D., Meade, A., 2007. Frequency of word-use predicts rates of 
lexical evolution throughout indo-european history. Nature 449, 717–720. https:// 
doi.org/10.1038/nature06176, 10.1038/nature06176.  
Panza, M., 2012. The twofold role of diagrams in euclid’s plane geometry. Synthese 186, 
55–102. 
Pavesi, A., Vianelli, A., Chirico, N., Bao, Y., Blinkova, O., Belshaw, R., Firth, A., 
Karlin, D., 2018. Overlapping genes and the proteins they encode differ significantly 
in their sequence composition from non-overlapping genes. PloS One 13, e0202513. 
Pedoe, D., 1975. Notes on the history of geometrical ideas ii. the principle of duality. 
Math. Mag. 48, 274–277 arXiv: https://doi.org/10.1080/0025570X.1975.11976511 
. 
Perbal, L., 2013. The ’warrior gene’ and the ma˜ori people: the responsibility of the 
geneticists. Bioethics 27, 382–387. 
Prusiner, S.B., 1982. Novel proteinaceous infectious particles cause scrapie. Science 216, 
136–144. 
Roberts, D.M., 2019. A crisis of identification. Inference. Int. Rev. Sci. 4. 
Rochberg, F., 2002. A consideration of babylonian astronomy within the historiography 
of science. In: Studies in History and Philosophy of Science Part A, vol. 33, 
pp. 661–684. https://doi.org/10.1016/S0039-3681(02)00022-5. http://www.scienc 
edirect.com/science/article/pii/S0039368102000225. 
Rodin, A., 2020. Axiomatic Architecture of Scientific Theories, Habilitation Thesis. State 
University, St. Petersburg.  
Rogers, H., 1987. Theory of Recursive Functions and Effective Computability. MIT Press, 
Cambridge, MA, USA.  
Rosen, F.A., 1831. The Algebra of Mohammed Ben Musa and Translated by Frederic 
Rosen. Oriental translation fund.  
Sarti, A., Citti, G., Piotrowski, D., 2019. Differential heterogenesis and the emergence of 
semiotic function. Semiotica 2019, 1–34. 
Scharcanski, J., Venetsanopoulos, A.N., 1997. Edge detection of color images using 
directional operators. IEEE Trans. Circ. Syst. Video Technol. 7, 397–401. https://doi. 
org/10.1109/76.564116. 
Scott, D., 1980. Lambda calculus: some models, some philosophy. In: Studies in Logic 
and the Foundations of Mathematics, vol. 101. Elsevier, pp. 223–265. 
Serb, J.M., Eernisse, D.J., 2008. Charting evolution’s trajectory: using molluscan eye 
diversity to understand parallel and convergent evolution. Evolution: Educ. 
Outreach 1, 439. 
Sonnenschein, C., Soto, A.M., 1999. The Society of Cells: Cancer and Control of Cell 
Proliferation. Bios Scientific Pub Limited. 
Soto, A., Longo, G., Noble, D., 2016. From the century of the genome to the century of 
the organism: new theoretical approaches. Spec. Issue Progr. Biophys. Mol. Biol. 
122. 
Stewart, J., 2004. La vie existe-t-elle. Vuibert, Paris.  
Teissier, B., 2012. Why are stories and proofs interesting ?. In: Circles Disturbed. The 
Interplay of Mathematics and Narrative. 
Thom, R., 1954. Quelques propri´et´es globales des vari´et´es diff´erentiables. Comment 
Math. Helv. 17–86. 
Thom, R., 1973. La Science Malgr´e Tout. 
Thomas, E., 2015. L’objectivit´e scientifique, hier et aujourd’hui. Catalogue de 
l’exposition ”Boîtes noires - Empreintes du monde et paysages int´erieurs”. 
Uversky, V.N., 2011. Intrinsically disordered proteins from a to z. Int. J. Biochem. Cell 
Biol. 43, 1090–1103. 
Weinberg, R.A., 2014. Coming full circle—from endless complexity to simplicity and 
back again. Cell 157, 267–271. 
West-Eberhard, M.J., 2003. Developmental Plasticity and Evolution. Oxford University 
Press. 
Weyl, H., 1952. Symmetry. Princeton, New Jersey.  
Wilson, R., 2003. Four Colours Suffice: How the Map Problem Was Solved. Allen Lane, 
London, UK. http://oro.open.ac.uk/7716/.  
Young, L.J., Nilsen, R., Waymire, K.G., MacGregor, G.R., Insel, T.R., 1999. Increased 
affiliative response to vasopressin in mice expressing the v 1a receptor from a 
monogamous vole. Nature 400, 766–768. 
Zalamea, F., 2012. Synthetic Philosophy of Contemporary Mathematics. MIT Press. 
Zorzi, M., 2016. On quantum lambda calculi: a foundational perspective. Math. Struct. 
Comput. Sci. 26, 1107. 
E. Blanchard and G. Longo                                                                                                                                                                                                                   

