GAUSSIAN-BERNOULLI RBMS WITHOUT TEARS
Renjie Liao∗1, Simon Kornblith2, Mengye Ren3, David J. Fleet2,4,5, Geoffrey Hinton2,4,5
University of British Columbia1, Google Research, Brain Team2,
New York University3, University of Toronto4, Vector Institute5
rjliao@ece.ubc.ca, mengye@cs.nyu.edu
{skornblith, davidfleet, geoffhinton}@google.com
ABSTRACT
We revisit the challenging problem of training Gaussian-Bernoulli restricted
Boltzmann machines (GRBMs), introducing two innovations. We propose a novel
Gibbs-Langevin sampling algorithm that outperforms existing methods like Gibbs
sampling. We propose a modiﬁed contrastive divergence (CD) algorithm so that
one can generate images with GRBMs starting from noise. This enables direct
comparison of GRBMs with deep generative models, improving evaluation pro-
tocols in the RBM literature. Moreover, we show that modiﬁed CD and gradient
clipping are enough to robustly train GRBMs with large learning rates, thus re-
moving the necessity of various tricks in the literature. Experiments on Gaussian
Mixtures, MNIST, FashionMNIST, and CelebA show GRBMs can generate good
samples, despite their single-hidden-layer architecture. Our code is released at:
https://github.com/lrjconan/GRBM
1
INTRODUCTION
Restricted Boltzmann machines (RBMs) (Smolensky, 1986; Freund & Haussler, 1991; Hinton,
2002) are energy-based generative models with stochastic binary units. A variant of Boltzmann
machines (Ackley et al., 1985), they have a bipartite graphical structure that enables efﬁcient proba-
bilistic inference, and they can be stacked to form deep belief networks (DBNs) (Hinton & Salakhut-
dinov, 2006; Bengio et al., 2006; Hinton et al., 2006). Gaussian-Bernoulli RBMs (GRBMs) (Welling
et al., 2004; Hinton & Salakhutdinov, 2006) extend RBMs to model continuous data by replacing
the binary visible units of the RBM with Gaussian random variables.
GRBMs remain challenging to learn, however, despite many proposed modiﬁcations to the model
or training algorithm. For instance, Lee et al. (2007) add a regularization term to encourage sparsely
activated binary hidden units. Krizhevsky et al. (2009) attribute the difﬁculties in learning to high-
frequency noise present in natural images. Factorized high-order terms were introduced in (Ranzato
& Hinton, 2010; Ranzato et al., 2010) to allow GRBMs to explicitly learn the covariance structure
among pixels. Nair & Hinton (2010) suggest that binary hidden units are problematic, and proposed
model variants with real-valued hidden units. Cho et al. (2011a; 2013) advocate the use of parallel
tempering sampling (Earl & Deem, 2005), adaptive learning rate, and enhanced gradient (Cho et al.,
2011b) to improve GRBM learning. Melchior et al. (2017) conclude that difﬁculties in GRBM
training are due to training algorithms rather than the model itself; they advocate the use of gradient
clipping, specialized weight initialization, and contrastive divergence (CD) (Hinton, 2002) rather
than persistent CD (Tieleman, 2008). Upadhya & Sastry (2021) propose a stochastic difference of
convex functions programming (S-DCP) algorithm to replace CD in training GRBMs.
An important motivation for seeking to improve GRBM learning is so that a GRBM can be used to
convert real-valued data to stochastic binary data. This would make it easy for researchers to explore
novel ways of implementing stochastic binary Boltzmann machines to model real-valued data. To
that end, we propose improved GRBM learning methods for image data. Speciﬁcally,
• We propose a hybrid Gibbs-Langevin sampling algorithm that outperforms predominant
use of Gibbs sampling. To the best of our knowledge this is the ﬁrst use of Langevin
sampling for GRBM training (with or without Metropolis adjustment).
∗Work done partially as a visiting faculty researcher at Google Brain.
1
arXiv:2210.10318v1  [cs.LG]  19 Oct 2022

• We propose a modiﬁed CD algorithm so that one can generate images with learned GRBMs
starting from Gaussian noise. This enables a fair and direct comparison of GRBMs with
deep generative models, something beyond the reach of existing GRBM learning methods.
• We show that the modiﬁed CD with gradient clipping is sufﬁcient to train GRBMs, thus
removing the need for heuristics that have been crucial for existing approaches.
• We empirically show that GRBMs can generate good samples on Gaussian Mixtures,
MNIST, FashionMNIST, and CelebA, despite they have a single hidden layer.
2
RELATED WORK
Learning the variances Learning the variance of visible units in GRBMs is necessary for gen-
erating sharp and realistic images. But small variances tend to cause the energy function and its
gradient to have large values, thus making the stochastic gradient estimates returned by CD numeri-
cally unstable. Most existing methods ﬁx the variance (e.g., to one) to avoid this issue. Krizhevsky
et al. (2009); Cho et al. (2011a) consider learning the variance using a smaller learning rate than for
other parameters, obtaining much better reconstruction, thus supporting the importance of learning
variances. However, many of the learned ﬁlters are still noisy and point-like. Melchior et al. (2017)
learn a shared variance across all visible units, yielding improved performance, especially with large
numbers of hidden units. In this work, we learn one variance parameter per visible unit and achieve
much lower learned variances than existing methods, e.g., approximately 1e−5 on MNIST.
Stochastic gradient estimation and learning rate Due to the intractable log partition function of
GRBMs, one often estimates the gradients of the log likelihood w.r.t. parameters via Monte Carlo.
Gibbs sampling is predominant in CD learning due to its simplicity, but it mixes slowly in practice.
This yields noisy gradient estimates which often cause training instabilities and prohibits using
large learning rates. Cho et al. (2011a) explore parallel tempering with adaptive learning rates to
obtain better reconstruction. Cho et al. (2013) propose enhanced gradients that are invariant to
bit-ﬂipping in hidden units. Melchior et al. (2017) show that gradient clipping and special weight
initialization support robust CD learning with large learning rates. We advocate Langevin MC to
improve gradients, and validate that gradient clipping does enable training with large learning rates.
Model capacity Theis et al. (2011) empirically show that GRBMs are outperformed even by sim-
ple mixture models in estimating likelihoods for image data. Wang et al. (2012); Melchior et al.
(2017) demonstrate that GRBMs can be expressed as either a product of experts or a constrained
Gaussian mixture in the visible domain, hinting that GRBMs need more hidden units than the true
number of components to ﬁt additive mixture densities well. Krause et al. (2013); Gu et al. (2022)
provide theoretical guarantees on GRBMs for universal approximation of mixtures and smooth den-
sities. Although this shows that GRBMs are expressive, they do not lead directly to practical GRBM
learning algorithms.
Model Evaluation Like many deep generative models, evaluating GRBMs is difﬁcult, as the log
likelihood is intractable. To date, GRBMs have been evaluated by visually inspecting reconstructed
images, ﬁlters and hidden activation (i.e., features), and sampled images during CD training. Quanti-
tative metrics include reconstruction errors, and error rates of post-hoc trained classiﬁers on learned
features. However, these metrics do not necessarily indicate if GRBMs are good generative mod-
els Melchior et al. (2017). Unlike existing work, we sample from learned GRBMs, starting from
Gaussian noise, enabling direct comparisons with other generative models, qualitatively (visually
inspecting samples) and quantitatively (e.g., Frechet Inception distance (FID) (Heusel et al., 2017).
3
GAUSSIAN-BERNOULLI RESTRICTED BOLTZMANN MACHINES
A Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) (Welling et al., 2004; Krizhevsky
et al., 2009; Cho et al., 2011a; Melchior et al., 2017) is a Markov Random Field (MRF) with contin-
uous stochastic visible units and binary stochastic hidden units. Denoting N visible units as v ∈RN
and M hidden units as h ∈{0, 1}M, the energy function associated with a GRBM is deﬁned to be
Eθ(v, h) = 1
2
v −µ
σ
⊤v −µ
σ

−
 v
σ2
⊤
Wh −b⊤h ,
(1)
2

Algorithm 1 Langevin Sampling for GRBMs
1: Input: v(0), step size α0, total step T, burn-in step ˜T, adjust step η
2: For t = 1, . . . , T
3:
αt = CosineScheduler(t, T, α0)
4:
v = v(t−1) −αt
∂˜
E(v(t−1))
∂v
+ √2αtξt , ξt ∼N(0, I)
▷Use marginal energy in Eq. (5)
5:
If t <= η or
 t > η and u ∼U(0, 1) < A(v, v(t−1))

6:
v(t) = v
7:
Else
8:
v(t) = v(t−1)
9: Return: {v( ˜T +1:T )}
▷i : j indexes consecutive samples from i-th to j-th
with weight matrix W ∈RN×M, bias b ∈RM, mean µ ∈RN, and variance σ2 ∈RN
+, where,
unless stated otherwise, x
y denotes element-wise division between vectors x and y, as is convention
in the GRBM literature. We denote the set of learnable parameters as θ = {W, b, µ, σ2}. To
ensure the variance remains non-negative during learning, we adopt a reparameterization, directly
learning log σ2 rather than σ2 or σ. Finally, given the energy function, one can deﬁne the Boltzmann
distribution, over visible and hidden states, as
pθ(v, h) = 1
Z exp (−Eθ(v, h)) , where Z =
Z +∞
−∞
X
h
exp (−Eθ(v, h)) dv
(2)
is the normalization constant, which is intractable for even moderately large M.
The underlying graphical model, like an RBM, is a bipartite graph with edges only connecting visible
units to hidden units. This entails conditional independence of the form p(v|h) = Q
i p(vi|h) and
p(h|v) = Q
j p(hj|v). One can also derive the following conditional distributions for GRBMs,
p(v|h) = N
 v|Wh + µ, diag(σ2)

(3)
p(hj = 1|v) =
h
Sigmoid

W ⊤v
σ2 + b
i
j ,
(4)
where N
 v|Wh + µ, diag(σ2)

is the multivariate Gaussian distribution with mean Wh + µ, and
the diagonal covariance matrix diag(σ2). Here, Sigmoid(x) = 1/(1 + exp(−x)) is applied to the
vector x in an element-wise manner, and [·]j denotes the j-th element of the corresponding vector.
Given the Boltzmann distribution, one can derive the marginal distribution over visible units, i.e.,
p(v) = 1
Z exp

−˜Eθ(v)

,
(5)
˜Eθ(v) = 1
2
v −µ
σ
⊤v −µ
σ

−Softplus

W ⊤v
σ2 + b
⊤
1 ,
where Softplus(x) = log(1 + exp(x)) is applied in an element-wise manner, and 1 is the all-one
vector of size M. We call ˜Eθ(v) the marginal energy to distinguish it from the GRBM energy in
Eq. (1). We leave the derivation to Appendix A.1. As shown in Melchior et al. (2017), one can also
rewrite the marginal distribution p(v) as a constrained Gaussian mixture.
3.1
INFERENCE
When performing probabilistic inference, e.g., computing the marginal distribution or the maxi-
mum a posterior (MAP) estimation, one often chooses between variational inference (Hinton &
Van Camp, 1993; Jordan et al., 1999) and Markov chain Monte Carlo (MCMC) methods (Neal,
1993; Andrieu et al., 2003). We focus on MCMC as common variational methods have been less
effective with RBMs and GRBMs (Gabri´e et al., 2015; Takahashi & Yasuda, 2016). From the gen-
erative modelling perspective, we wish to draw samples of visible units during inference. There
are two natural approaches to this: 1) sample from the joint distribution in Eq. (2) and discard the
samples of hidden units, or 2) directly sample from the marginal distribution.
3

Gibbs sampling (Geman & Geman, 1984) is perhaps the predominant approach, due to its simplicity.
In the context of GRBMs, one alternates between sampling hidden units given visible units, and
sampling visible units given hidden units. This produces samples from the joint distribution in Eq.
(2). The detailed Gibbs sampling algorithm is given in Appendix A.2.
Langevin Sampling
Langevin Monte Carlo (Grenander & Miller, 1994; Roberts & Tweedie,
1996; Welling & Teh, 2011) is a class of MCMC methods that generate samples from a proba-
bility distribution of continuous random variables by simulating Langevin dynamics. Since GRBMs
are hybrid graphical models, i.e., comprising continuous and discrete random variables, we have at
least two ways to leverage Langevin sampling. One is to directly apply Langevin sampling to the
marginal distribution of visible units in Eq. (5). Suppose at time step t −1, we have sample vt−1
and want to draw a new sample vt. The proposal distribution corresponding to one-step Langevin
dynamics is given by
q(v|v(t−1)) = N
 
v
v(t−1) −αt
∂˜E(v(t−1))
∂v
, 2αtI
!
,
(6)
where the gradient the of marginal energy ˜E w.r.t. the visible units is given in Appendix A.3. If
we use the Metropolis-Hastings algorithm to accept or reject proposed samples, the acceptance
probability of a proposal vt, given the previous state, vt−1, is (see Appendix A.3 for derivation):
A(v(t), v(t−1)) = min



1,
exp

−˜Eθ(v(t)) −
1
4αt
v(t−1) −v(t) + αt
∂˜
E(v(t))
∂v

2
exp

−˜Eθ(v(t−1)) −
1
4αt
v(t) −v(t−1) + αt
∂˜
E(v(t−1))
∂v

2



.
(7)
Alg. 1 shows the Metropolis-adjusted Langevin Algorithm (MALA) for the marginal GRBM. Com-
pared to generic MALA, it also includes an extra hyperparameter, namely, the adjust step η. If η is
set to 0, then we perform a Metropolis adjustment at every sampling step, as prescribed in the generic
MALA. If η is set to K > 0, then we skip the Metropolis adjustment for the ﬁrst K steps. The adjust
step effectively controls a trade-off between sampling accuracy1 and computational efﬁciency. Since
we do not hope to see Gaussian noise in our ﬁnal-sampled images (i.e., beyond the level of intrinsic
noise in the observations), it is beneﬁcial to decay the noise level, as in score-based models (Song &
Ermon, 2019). For certain step-size-annealing schedules and energy functions, there are theoretical
guarantees on the convergence of Langevin sampling (Durmus & Moulines, 2019). For simplicity,
we use the cosine scheduler and ﬁnd it works well in practice. More details about the scheduler are
provided in Appendix A.3.
Gibbs-Langevin Sampling
We also introduce a new hybrid sampler for GRBMs (see Alg. 2).
Like the Gibbs sampler, it alternates between sampling hidden units conditioned on visible units,
and sampling visible units given the hidden units. Unlike generic Gibbs, which directly samples
from the Gaussian p(v|h(t)), we instead use Langevin MC to sample the continuous visible units
given the hidden state. The use of Langevin MC may seem unnecessary because the Gaussian con-
ditional permits a one-step sampling algorithm. The subtlety comes from the fact that the ﬁnite-step
Langevin sampler explicitly depends on the initial sample. Speciﬁcally, the proposal distribution of
one complete outer-loop step in Alg. 2, e.g., at iteration t −1, can be expressed as
q(v, h|v(t−1), h(t−1)) = q(h|v) q(v|v(t−1), h(t−1)) ,
(8)
where q(h|v) is given by Eq. (4), and q(v|v(t−1), h(t−1)) is the proposal distribution of a K-step
Langevin sampler (i.e. from the inner loop). This proposal distribution explicitly depends on the
initial visible sample, v(t−1) from iteration t −1. By contrast, the generic Gibbs sampler does
not have such dependence, i.e., q(v|v(t−1), h(t−1)) = q(v|h(t−1)). This dependence allows us to
construct a persistent Markov chain in the space of visible units. Moreover, the Langevin sampler
leverages the informative gradient of log density whereas Gibbs sampler does not. We ﬁnd that our
new sampler performs signiﬁcantly better than the vanilla Gibbs sampler in practice.
1Here sampling accuracy means the closeness between the underlying distribution of samples and the target
distribution measured in, e.g., total variation or Wasserstein distances.
4

Algorithm 2 Gibbs-Langevin Sampling for GRBMs
1: Input: v(0), h(0), step size α0, total step T, burn-in step ˜T, adjust step η, Langevin step K
2: Function Langevin(˜v(0), h, α0, K):
3:
For k = 1, . . . , K
4:
αk = CosineScheduler(k, K, α0)
5:
˜v(k) = ˜v(k−1) −αk
∂E(˜v(k−1),h)
∂v
+ √2αkξk ,
ξk ∼N(0, I)
6: Return ˜v(K)
7:
8: For t = 1, . . . , T
9:
v = Langevin(v(t−1), h(t−1), α0, K)
10:
h ∼p(h|v)
11:
If t <= η or

t > η and u ∼U(0, 1) < ˜A
 (v, h), (v(t−1), h(t−1))

12:
v(t), h(t) = v, h
13:
Else
14:
v(t), h(t) = v(t−1), h(t−1)
15: Return: {(v( ˜T +1:T ), h( ˜T +1:T ))}
The Metropolis adjustment for these Gibbs-Langevin proposals is, however, somewhat more in-
volved. Following Alg. 2, with ˜v(0) = v(t−1) and ˜v(K) = v, by marginalizing out the intermediate
states on the Markov chain, we obtain the proposal
q(˜v(K)|˜v(0), h(t−1)) =
Z
· · ·
Z  K
Y
k=1
q(˜v(k)|˜v(k−1), h(t−1))
!
d˜v(1) · · · d˜v(K−1).
(9)
The integrand in Eq. 9 comprises K one-step Langevin updates, each of which is given by
q(v|˜v(k−1), h(t−1)) = N

v
˜v(k−1) −αk
∂E(˜v(k−1), h(t−1))
∂v
, 2αkI

,
(10)
for which the energy gradient is given in Appendix A.4. Although the multiple integral in Eq. (9)
appears intractable, one can use reparameterization to derive the following analytical form,
q(˜v(K)|˜v(0), h(t−1)) = N
 
β0˜v(0) +
 K
X
k=1
βkαk
!
µ + Wh(t−1)
σ2
, diag
 K
X
k=1
2αkβ2
k
!!
,
(11)
where βk = QK
j=k+1
 1 −αj
σ2

, ∀k ∈{0, . . . , K −1} and βK = 1. Based on this result, one can
show that the acceptance probability for the Metropolis adjustment is
˜A((v(t), h(t)), (v(t−1), h(t−1))) =
min



1,
exp

−Eθ(v(t), h(t)) −
 v(t−1)−β0v(t)−a(µ+W h(t))
√
2˜σ

2
q(h(t−1)|v(t−1))
exp

−Eθ(v(t−1), h(t−1)) −
 v(t)−β0v(t−1)−a(µ+W h(t−1))
√
2˜σ

2
q(h(t)|v(t))



,
(12)
where q(hj = 1|v) =

Sigmoid
 W ⊤v
σ2 + b

j, a =
PK
k=1 βkαk
σ2
, and ˜σ2 = PK
k=1 2αkβ2
k. We
leave derivations to Appendix A.4.
3.2
LEARNING
To learn GRBMs, we maximize the log likelihood of the observed data using stochastic gradient-
based methods, e.g., contrastive divergence (CD). Depending on whether we use the joint (Eq. (2))
or the marginal (Eq. (5)) distribution, we have two possible gradient estimators.
Learning with the Joint Distribution
When optimizing the GRBM with the joint distribution,
one can express the general form of the gradient of the log likelihood w.r.t. parameters θ as
∇θ =

−∂Eθ(v, h)
∂θ

d
−

−∂Eθ(v, h)
∂θ

m
.
(13)
5

Algorithm 3 Modiﬁed CD Learning Algorithm for GRBMs with Joint Density
1: Input: CD-step K, burn-in step M, learning Rate η, Langevin step size α0, SGD step T
2: For t = 1, · · · , T
3:
v+ = vdata
4:
h+ ∼p(h|v+)
5:
∇θ+ =
D
∂E(v+,h+)
∂θ
E
d
▷Compute Positive Gradient
6:
v−
0 ∼N(0, I), h−
0 ∼p(h|v−
0 )
▷Modiﬁed CD to start with noise
7:
{v−
M:K, h−
M:K} ∼Sampler(v−
0 , h−
0 , α0 ¯σ)
▷Alg. 5 or Alg. 2, ¯σ is current mean variance
8:
∇θ−=
D
∂E(v−,h−)
∂θ
E
m
▷Compute Negative Gradient
9:
θ = θ −η(∇θ+ −∇θ−)
▷Compute Update
10: Return θ
Here, following the notation in the RBM literature, we denote expectation under the data distribu-
tion, i.e., pθ(h|v) pdata(v), as ⟨·⟩d = Epθ(h|v)pdata(v) [·]. Similarly, we denote the expectation under
the model distribution, pθ(v, h) as ⟨·⟩m = Epθ(v,h) [·]. The expected gradients under the data and
model distributions are called positive and negative gradients respectively. Based on Eq. (13), we
can formulate the gradients of speciﬁc parameters as follows,
∇Wij =
 vi
σ2
i
hj

d
−
 vi
σ2
i
hj

m
(14)
∇µi =
vi −µi
σ2
i

d
−
vi −µi
σ2
i

m
(15)
∇log σ2
i =
(vi −µi)2
2σ2
i
−
P
j viWijhj
σ2
i

d
−
(vi −µi)2
2σ2
i
−
P
j viWijhj
σ2
i

m
(16)
∇bi = ⟨hi⟩d −⟨hi⟩m .
(17)
Since the expectations in these gradients are generally intractable, we use Monte Carlo methods to
approximate them. To sample from the joint density, we can use Gibbs or Gibbs-Langevin samplers
as described in Sec. 3.1. The overall learning algorithm is outlined in Alg. 3. An important detail is
that we multiply the initial Langevin step size by the average variance at each gradient update step
and then feed it to the sampler. Since the variance is decreasing (the energy function and its gradient
are increasing) as learning goes on, keeping the step size roughly invariant to such scaling would
make the sampling more effective.
Learning with the Marginal Distribution
Now we turn to learning the model under the marginal
distribution in Eq. (5). Since we have the marginal distribution of visible units, we can directly get
the gradients of log likelihood w.r.t. model parameters θ as,
∇θ =
*
−∂˜Eθ(v)
∂θ
+
d
−
*
−∂˜Eθ(v)
∂θ
+
m
.
(18)
Since the gradient ∂˜
Eθ(v)
∂θ
does not depend on h anymore, we have
*
−∂˜Eθ(v)
∂θ
+
d
= Epdata(v)
"
−∂˜Eθ(v)
∂θ
#
,
*
−∂˜Eθ(v)
∂θ
+
m
= Epθ(v)
"
−∂˜Eθ(v)
∂θ
#
.
(19)
Based on above results, we can work out the detailed gradients which are the same as those in Eq.
(14) to Eq. (17) but with h replaced with Sigmoid
 W ⊤v
σ2 + b

. More details are left to Appendix
A.5. We use the Langevin sampler in Sec. 3.1 to sample from the marginal density to approximate
the intractable expectation. The overall learning algorithm is outlined in Alg. 4.
Modiﬁed Contrastive Divergence
The above two learning algorithms resemble CD if one ig-
nores the speciﬁc sampler used. There exists a subtle yet important difference however. For most
deep generative models one generates samples starting from noise. But this does not work well for
6

Algorithm 4 Modiﬁed CD Learning Algorithm for GRBMs with Marginal Density
1: Input: CD-step K, burn-in step M, learning rate η, Langevin step size α0, SGD step T
2: For t = 1, · · · , T
3:
v+ = vdata
4:
∇θ+ =
D
∂˜
E(v+)
∂θ
E
d
▷Compute Positive Gradient
5:
v−
0 ∼N(0, I)
▷Modiﬁed CD to start with noise
6:
{v−
i |i = M, · · · , K} ∼Sampler(v−
0 , α0 ¯σ)
▷Alg. 1, ¯σ is current mean variance
7:
∇θ−=
D
∂˜
E(v−)
∂θ
E
m
▷Compute Negative Gradient
8:
θ = θ −η(∇θ+ −∇θ−)
▷Compute Update
9: Return θ
(a)
(b)
(c)
(d)
(e)
(f)
Figure 1: Density modelling using GRBMs on data from a Gaussian mixtures with isotropic (rows
1 and 2) and anisotropic variances (rows 3 and 4). Rows 1 and 3 show normalized GMM densities
and (unnormalized) negative energy values for GRBMs. Rows 2 and 4 show samples drawn un-
der different models and methods; i.e., (a) Ground Truth; (b) Gibbs; (c) Langevin wo. Adjust; (d)
Langevin w. Adjust; (e) Gibbs-Langevin wo. Adjust; (f) Gibbs-Langevin w. Adjust.
models trained with CD, where sampling starts from observed data. This discrepancy of the starting
sample between training and testing would be a signiﬁcant issue if the Markov chain does not mix
sufﬁciently quickly. We therefore modify CD by running two Markov chains to collect samples for
positive and negative gradients respectively. The positive Markov chain is the same as in CD, i.e.,
starting from observed data. The negative Markov chain now starts from a sample of standard Nor-
mal noise rather than the reconstructed data2. Since the positive chain starting from data will usually
stay close to the data distribution, this modiﬁcation pushes the negative Markov chain, starting from
noise, toward the data distribution. Moreover, the discrepancy between training and testing ceases to
be important as we can start from standard Normal noise while sampling from the learned model.
4
EXPERIMENTS
We examine the empirical behavior of our new GRBM algorithms on benchmark image datasets,
namely, MNIST, Fashion-MNIST (Xiao et al., 2017), and CelebA (Liu et al., 2015).
2The reconstructed data is typically obtained by running one complete step of Gibbs sampler from the
observed data, thus being highly likely close to observed data.
7

Implementation Details
We found that training with modiﬁed CD alone occasionally diverges,
necessitating careful tuning of the learning rate. However, adding gradient clipping (e.g., clip gradi-
ent norm to 10) enables stable training with all aforementioned sampling methods. We therefore set
learning rate to 0.01 for all experiments. Such a large learning rate almost never works in the litera-
ture. Melchior et al. (2017) used gradient clipping and similarly large learning rates, but they had to
set the learning rate for the variances 100 times smaller than that for the weights and biases during
CD training. But thanks to the modiﬁed CD and gradient clipping, we found this special treatment
of variances is unnecessary. We do not use momentum, weight decay, PCD, or other tricks.
4.1
MODELING GAUSSIAN MIXTURE DENSITIES
We ﬁrst evaluate density modelling by GRBMs when the data density is known, i.e., Gaussian
mixture models (GMMs) in our case. This is challenging for GRBMs as the marginal distribution
of visible units of GRBMs is essentially a constrained Gaussian mixture, i.e., the weights of mixture
components depend on one another (Melchior et al., 2017). As such, the mixture components in
GRBMs can not be freely placed in the visible domain so one actually needs more hidden units
than the log of the number of mixture components to ﬁt GMMs well. We consider the 2D case for
simplicity and better visibility. We generate 1,000 samples from two types (isotropic and anisotropic
variances) of GMMs with 3 components as shown in Fig. 1, and learn GRBMs using our modiﬁed
CD with different sampling algorithms, from which we can draw samples. Here all samplers run for
100 steps during both CD training and testing (see Appendix B.1 for more detail). Density plots and
samples are shown in Fig. 1. Notice that Gibbs manages to recover the three modes in the isotropic
case but fails in the anisotropic case. Both Langevin and Gibbs-Langevin sampling collapse when
the adjustment is absent. We believe the cosine step size schedule contributes to the collapse as it
removes more stochasticity of Langevin dynamics with small step sizes, thus making sampling more
similar to gradient descent. But as we will see later, in image modelling, this may not be so severe;
there are more modes so that the sampling may collapse to different modes, and the diversity of
images remains acceptable. Finally, both Langevin and Gibbs-Langevin do recover all three modes
with the adjustment, which shows the adjustment helps the mixing in this synthetic case.
4.2
IMAGE GENERATION
We learn GRBMs to ﬁt image datasets including MNIST, FashionMNIST, and CelebA. To the best
of our knowledge, this is the ﬁrst time that GRBMs have been shown to (unconditionally) generate
good images. We provide the ablation study in Appendix B.2 and more results in Appendix B.3.
Figure 2:
Intermediate samples from Gibbs-
Langevin sampling.
Methods
FID
VAE
16.13
2sVAE (Dai & Wipf, 2019)
12.60
PixelCNN++ (Salimans et al.)
11.38
WGAN (Arjovsky et al., 2017)
10.28
NVAE (Vahdat & Kautz, 2020)
7.93
GRBMs
Gibbs
47.53
Langevin wo. Adjust
43.80
Langevin w. Adjust
41.24
Gibbs-Langevin wo. Adjust
17.49
Gibbs-Langevin w. Adjust
19.27
Table 1: Results on MNIST dataset.
MNIST
We train GRBMs with hidden size 4096 and 100 sampling steps on MNIST. We compare
FID scores of GRBMs with other deep generative models in Table 1. From the table, we can see that
Gibbs-Langevin family works signiﬁcantly better than the Langevin family. The Metropolis adjust-
ment improves Langevin slightly but degrades Gibbs-Langevin slightly, which is different from what
we observed on synthetic data. This is likely because the image distribution is so complicated (e.g.,
having signiﬁcantly more modes) that the adjustment rejects proposed moves more frequently than
before. Some sophisticated strategy may be needed to increase the acceptance probability. Never-
8

(a)
(b)
(c)
Figure 3: (a) Learning curve of (natural) log variances, (b) learned ﬁlters, and (c) samples on MNIST.
(a)
(b)
(c)
Figure 4: Samples from GRBMs on (a) FashionMNIST, (b) CelebA-32, and (c) CelebA-2K-64.
theless, GRBMs trained with Gibbs-Langevin without adjustment achieve FID scores comparable to
other deep generative models, which is impressive given the single-hidden-layer architecture. The
learning curve of (natural) log variance is shown in Fig. 3a. The learned variance converges to
around 1e−5 which is signiﬁcantly smaller than those reported in the literature. The learned ﬁlters
are shown in Fig. 3b. Although some point-like ﬁlters still exist, stroke-like ﬁlters are common,
thus indicating GRBMs indeed learn meaningful features. We show samples drawn from the best
GRBM in Fig. 3c. The intermediate samples from Gibbs-Langevin are shown in Fig. 2. Since
Gibbs-Langevin without adjustment works the best, we use it for remaining experiments.
FashionMNIST
We then train GRBMs on FahsionMNIST which is more challenging than
MNIST. We set hidden size to 10,000 and the sampling step to 100. Samples drawn from learned
GRBMs are shown in Fig. 4a. GRBMs successfully learn the shapes of clothes, shoes, bags, and
so on. However, they fail to capture ﬁne textures. Since many images in this dataset look similar in
shape but differ in texture, the resulting samples look similar to each other.
CelebA
Last, we consider the even more challenging CelebA dataset. In particular, we explore two
versions of this dataset: 1) CelebA-32 where we center-crop (140×140) and downsample images to
32×32; 2) CelebA-2K-64 where randomly select 2,000 images from the original CelebA and apply
the same center crop and downsampling to 64×64. We set hidden size to 10,000 and explore the
number of 100 and 200 sampling steps. Generated samples are shown in Fig. 4b and 4c. From the
ﬁgure, we can see that GRBMs can learn to generate reasonably good face images.
5
CONCLUSION
In this paper, we revisit learning Gaussian-Bernoulli restricted Boltzmann machines. We investigate
Langevin Monte Carlo and propose a novel Gibbs-Langevin sampling method. Furthermore, we
modify the contrastive divergence (CD) algorithm so that one can sample data from learned GRBMs
starting from noise. Modiﬁed CD along with gradient clipping enables robust training of GRBMs
with large learning rates. Finally, we show that GRBMs can unconditionally generate images with
good qualities, despite its single-hidden-layer architecture. In the future, it would be beneﬁcial to
9

extend the current GRBMs to convolutional GRBMs which should be able to learn better localized
ﬁlters. Meanwhile, it would be interesting to explore Gaussian deep belief networks (GDBNs),
which are deeper than GRBMs and should be superior. At last, investigating our Gibbs-Langevin
sampling for hybrid deep energy based models could be a fruitful direction.
ACKNOWLEDGMENTS
This work was funded, in part, by the NSERC Discovery Grant. Resources used in preparing this
research were provided, in part, by Google, Vector Institute, and UBC.
REFERENCES
David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for boltzmann
machines. Cognitive science, 9(1):147–169, 1985.
Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An introduction to
mcmc for machine learning. Machine Learning, 50(1):5–43, 2003.
Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214–223. PMLR, 2017.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training
of deep networks. Advances in neural Information Processing Systems, 19, 2006.
Kyung Hyun Cho, Tapani Raiko, and Alexander Ilin. Gaussian-bernoulli deep boltzmann machine.
In The 2013 International Joint Conference on Neural Networks (IJCNN), pp. 1–7. IEEE, 2013.
KyungHyun Cho, Alexander Ilin, and Tapani Raiko. Improved learning of gaussian-bernoulli re-
stricted boltzmann machines. In International Conference on artiﬁcial Neural networks, pp. 10–
17. Springer, 2011a.
KyungHyun Cho, Tapani Raiko, and Alexander Ilin. Enhanced gradient and adaptive learning rate
for training restricted boltzmann machines. In ICML, 2011b.
Bin Dai and David Wipf. Diagnosing and enhancing vae models. arXiv preprint arXiv:1903.05789,
2019.
Alain Durmus and Eric Moulines. High-dimensional bayesian inference via the unadjusted langevin
algorithm. Bernoulli, 25(4A):2854–2882, 2019.
David J Earl and Michael W Deem. Parallel tempering: Theory, applications, and new perspectives.
Physical Chemistry Chemical Physics, 7(23):3910–3916, 2005.
Yoav Freund and David Haussler. Unsupervised learning of distributions on binary vectors using
two layer networks. Advances in Neural Information Processing Systems, 4, 1991.
Marylou Gabri´e, Eric W Tramel, and Florent Krzakala. Training restricted boltzmann machine via
the thouless-anderson-palmer free energy. Advances in Neural Information Processing Systems,
28, 2015.
Stuart Geman and Donald Geman.
Stochastic relaxation, gibbs distributions, and the bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, (6):
721–741, 1984.
Ulf Grenander and Michael I Miller. Representations of knowledge in complex systems. Journal of
the Royal Statistical Society: Series B (Methodological), 56(4):549–581, 1994.
Linyan Gu, Lihua Yang, and Feng Zhou. Approximation properties of gaussian-binary restricted
boltzmann machines and gaussian-binary deep belief networks. Neural Networks, 2022.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
Neural Information Processing Systems, 30, 2017.
10

Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
Computation, 14(8):1771–1800, 2002.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504–507, 2006.
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the de-
scription length of the weights. In Proceedings of the sixth annual Conference on Computational
Learning theory, pp. 5–13, 1993.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief
nets. Neural Computation, 18:1527–1554, 2006.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine Learning, 37(2):183–233, 1999.
Oswin Krause, Asja Fischer, Tobias Glasmachers, and Christian Igel. Approximation properties
of dbns with binary hidden units and real-valued visible units. In International Conference on
Machine Learning, pp. 419–426. PMLR, 2013.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Honglak Lee, Chaitanya Ekanadham, and Andrew Ng. Sparse deep belief net model for visual area
v2. Advances in Neural Information Processing Systems, 20, 2007.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Jan Melchior, Nan Wang, and Laurenz Wiskott. Gaussian-binary restricted boltzmann machines for
modeling natural image statistics. PloS one, 12(2):e0171015, 2017.
Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.
In Icml, 2010.
Radford M Neal. Probabilistic inference using Markov chain Monte Carlo methods. Department of
Computer Science, University of Toronto Toronto, ON, Canada, 1993.
Marc’Aurelio Ranzato and Geoffrey E Hinton. Modeling pixel means and covariances using factor-
ized third-order boltzmann machines. In 2010 IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2551–2558. IEEE, 2010.
Marc’Aurelio Ranzato, Alex Krizhevsky, and Geoffrey Hinton. Factored 3-way restricted boltzmann
machines for modeling natural images. In Proceedings of the thirteenth International Conference
on Artiﬁcial Intelligence and Statistics, pp. 621–628. JMLR Workshop and Conference Proceed-
ings, 2010.
Gareth O Roberts and Richard L Tweedie. Exponential convergence of langevin distributions and
their discrete approximations. Bernoulli, pp. 341–363, 1996.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: A pixelcnn imple-
mentation with discretized logistic mixture. ICLR.
P Smolensky. Information processing in dynamical systems: foundations of harmony theory. In Par-
allel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations,
pp. 194–281. 1986.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in Neural Information Processing Systems, 32, 2019.
Chako Takahashi and Muneki Yasuda. Mean-ﬁeld inference in gaussian restricted boltzmann ma-
chine. Journal of the Physical Society of Japan, 85(3):034001, 2016.
Lucas Theis, Sebastian Gerwinn, Fabian Sinz, and Matthias Bethge. In all likelihood, deep belief is
not enough. The Journal of Machine Learning Research, 12:3071–3096, 2011.
11

Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood
gradient. In Proceedings of the 25th International Conference on Machine Learning, pp. 1064–
1071, 2008.
Vidyadhar Upadhya and PS Sastry. Learning gaussian-bernoulli rbms using difference of convex
functions optimization. IEEE Transactions on Neural Networks and Learning Systems, 2021.
Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. Advances in Neural
Information Processing Systems, 33:19667–19679, 2020.
Nan Wang, Jan Melchior, and Laurenz Wiskott. An analysis of gaussian-binary restricted boltzmann
machines for natural images. In ESANN, 2012.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th International Conference on Machine Mearning (ICML-11), pp. 681–
688. Citeseer, 2011.
Max Welling, Michal Rosen-Zvi, and Geoffrey E Hinton. Exponential family harmoniums with
an application to information retrieval. Advances in Neural Information Processing Systems, 17,
2004.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
12

A
DERIVATIONS
A.1
MARGINAL PROBABILITY DISTRIBUTION OF VISIBLE UNITS OF GRBMS
We derive the marginal distribution of visible units as follows,
p(v) =
X
h
p(v, h)
= 1
Z
X
h
exp(−Eθ(v, h))
= 1
Z exp
 
−1
2
v −µ
σ
⊤v −µ
σ
! X
h
exp
 v
σ2
⊤
Wh + b⊤h

= 1
Z exp
 
−1
2
v −µ
σ
⊤v −µ
σ
! Y
i

1 + exp
 v
σ2
⊤
W

i
+ bi

= 1
Z exp
 
−1
2
v −µ
σ
⊤v −µ
σ
! Y
i
exp

Softplus
 v
σ2
⊤
W

i
+ bi

= 1
Z exp
 
−1
2
v −µ
σ
⊤v −µ
σ
!
exp

Softplus

W ⊤v
σ2 + b
⊤
1

= 1
Z exp
 
−1
2
v −µ
σ
⊤v −µ
σ

+ Softplus

W ⊤v
σ2 + b
⊤
1
!
.
(20)
A.2
GIBBS SAMPLING
Algorithm 5 Gibbs Sampling for GRBMs
1: Input: number of steps T, burn-in step ˜T
2: v(0) ∼N(0, I)
3: For t = 1, . . . , T
4:
h(t) ∼p(h|v(t−1))
▷following Eq. (4)
5:
v(t) ∼p(v|h(t))
▷following Eq. (3)
6: Return: {(v(t), h(t))|t = ˜T + 1, · · · , T}
Gibbs sampling (Geman & Geman, 1984) is perhaps the most popular approach due to its simplicity.
In the context of GRBMs, we can alternate between sampling hidden units given visible units and
sampling visible units given hidden units. Alg. 5 is a blocked Gibbs sampler; it samples all visible
units (a block of random variables) at once given all hidden units (the other block) and vice versa.
Given the conditional independence in the bipartite graphical model, this block Gibbs sampler is
equivalent to a univariate Gibbs sampler that updates one variable at a time given the others follow-
ing some schedule. In fact, any schedule comprising a sequence of all hidden units followed by all
visible units or vice versa would make the equivalence hold. In other words, it preserves the conver-
gence of the original univariate Gibbs sampler and runs as fast as a blocked Gibbs sampler. Relying
on this Gibbs sampler, we can get samples of visible and hidden units from the joint distribution in
Eq. (2). We can then discard the samples within the burn-in stage and treat remaining ones as the
ﬁnal set of samples.
A.3
LANGEVIN SAMPLING
The gradient of the marginal energy w.r.t. visible units is,
∂˜E(v)
∂v
= v −µ
σ2
−W Sigmoid
 W ⊤v
σ2 + b

σ2
.
(21)
13

The cosine scheduler for annealing the step size is,
αk = CosineScheduler(k, K, α0) = 1
2α0

1 + cos
 k
K π

(22)
where αk is the k-th step size, α0 is the initial step size, and K is the total number of steps.
The derivation of the Metropolis adjustment for Langevin sampling is as follows,
˜A(v(t), v(t−1)) = min

1, p(v(t))q(v(t−1)|v(t))
p(v(t−1))q(v(t)|v(t−1))

= min



1,
exp

−˜Eθ(v(t))

exp

−1
4αt
v(t−1) −v(t) + αt
∂˜
E(v(t))
∂v

2
exp

−˜Eθ(v(t−1))

exp

−1
4αt
v(t) −v(t−1) + αt
∂˜
E(v(t−1))
∂v

2




= min



1,
exp

−˜Eθ(v(t)) −
1
4αt
v(t−1) −v(t) + αt
∂˜
E(v(t))
∂v

2
exp

−˜Eθ(v(t−1)) −
1
4αt
v(t) −v(t−1) + αt
∂˜
E(v(t−1))
∂v

2



.
(23)
A.4
GIBBS-LANGEVIN SAMPLING
We now derive the Metropolis Adjustment for Gibbs-Langevin sampling. At time step t-1, the
proposal distribution in Alg. 2 is
q(v, h|v(t−1), h(t−1)) = q(h|v)q(v|v(t−1), h(t−1)),
(24)
where
q(h|v) = Sigmoid

W ⊤ v
σ2

+ b

.
(25)
Denoting v(t−1) = ˜v(0) and v = ˜v(K), we have,
q(v|v(t−1), h(t−1)) = q(˜v(K)|˜v(0), h(t−1))
=
Z
· · ·
Z  K
Y
k=1
q(˜v(k)|˜v(k−1), h(t−1))
!
d˜v(1) · · · d˜v(K−1),
(26)
where
q(v|˜v(k−1), h(t−1)) = N

v
˜v(k−1) −αk
∂E(˜v(k−1), h(t−1))
∂v
, 2αkI

(27)
∂E(v, h)
∂v
= v −µ −Wh
σ2
.
(28)
The key question here is how to derive the analytical form of q(˜v(K)|˜v(0), h(t−1)). The most
straightforward way is to compute the multiple integral directly. By ﬁxing all variables except
for ˜v(k) in Eq. (8), we can integrate out ˜v(k) analytically via the Gaussian integral trick, i.e.,
R ∞
−∞exp(−ax2 + bx + c)dx = p π
a exp( b2
4a + c). Then by applying the same trick recursively,
one can ideally integrate out all ˜v(1), . . . , ˜v(K−1) in an analytical manner. However, this process is
quite involved due to the fact that the integral of ˜v(k) depends on both ˜v(k+1) and ˜v(k−1).
We instead resort to the reparameterization trick. In particular, at the outer loop step t, the k-th inner
loop step of Langevin sampling is as follows,
˜v(k) = ˜v(k−1) −αk
∂E(˜v(k−1), h(t−1))
∂v
+
√
2αkξk
= ˜v(k−1) −αk
˜v(k−1) −µ −Wh(t−1)
σ2
+
√
2αkξk
=

1 −αk
σ2

˜v(k−1) + αk
µ + Wh(t−1)
σ2
+
√
2αkξk,
(29)
14

where ∀k ∈{1, . . . , K}, ξk ∼N(0, I). This discretization of Langevin dynamics gives a sample
path of the distribution q(˜v(K)|˜v(0), h(t−1)). We now show that this sample path could be repa-
rameterized as a simpler one which gives the desirable analytical form of q(˜v(K)|˜v(0), h(t−1)). To
simplify the derivation, we introduce βk = QK
j=k+1
 1 −αj
σ2

, ∀k ∈{0, . . . , K −1} and βK = 1.
Therefore, after K steps, we have,
˜v(K) =

1 −αK
σ2

˜v(K−1) + αK
µ + Wh(t−1)
σ2
+
√
2αKξK
=
 K
Y
k=1

1 −αk
σ2
!
˜v(0) +
K
X
k=1


K
Y
j=k+1

1 −αj
σ2




αk
µ + Wh(t−1)
σ2
+
√
2αkξk

= β0˜v(0) +
 K
X
k=1
βkαk
!
µ + Wh(t−1)
σ2
+
K
X
k=1
βk
√
2αkξk
(30)
Here {ξk|k = 1, . . . , K} are independent random variables from the standard Normal distribution.
Since we know that the linear combination of several independent Gaussian random variables leads
to another Gaussian random variable, we have
q(˜v(K)|˜v(0), h(t−1)) = N
 
β0˜v(0) +
 K
X
k=1
βkαk
!
µ + Wh(t−1)
σ2
,
K
X
k=1
2αkβ2
k
!
.
(31)
We can compute the acceptance probability,
A((v(t), h(t)), (v(t−1), h(t−1)))
= min

1,
p(v(t), h(t))q(v(t−1), h(t−1)|v(t), h(t))
p(v(t−1), h(t−1))q(v(t), h(t)|v(t−1), h(t−1))

= min

1, p(v(t), h(t))q(h(t−1)|v(t−1))q(v(t−1)|v(t), h(t))
p(v(t−1), h(t−1))q(h(t)|v(t))q(v(t)|v(t−1), h(t−1))

= min



1,
exp

−Eθ(v(t), h(t)) −
 v(t−1)−β0v(t)−a(µ+W h(t))
√
2˜σ

2
q(h(t−1)|v(t−1))
exp

−Eθ(v(t−1), h(t−1)) −
 v(t)−β0v(t−1)−a(µ+W h(t−1))
√
2˜σ

2
q(h(t)|v(t))



,
(32)
where q(hj = 1|v) =

Sigmoid
 W ⊤v
σ2 + b

j, a =
PK
k=1 βkαk
σ2
, and ˜σ2 = PK
k=1 2αkβ2
k.
A.5
LEARNING
We derive the detailed gradients of the marginalized log likelihood of visible units w.r.t. model
parameters as below.
∇Wij =
 vi
σ2
i
h
Sigmoid

W ⊤v
σ2 + b
i
j

d
−
 vi
σ2
i
h
Sigmoid

W ⊤v
σ2 + b
i
j

m
(33)
∇µi =
vi −µi
σ2
i

d
−
vi −µi
σ2
i

m
(34)
∇log σ2
i =
*
(vi −µi)2
2σ2
i
−
P
j viWij

Sigmoid
 W ⊤v
σ2 + b

j
σ2
i
+
d
−
*
(vi −µi)2
2σ2
i
−
P
j viWij

Sigmoid
 W ⊤v
σ2 + b

j
σ2
i
+
m
(35)
∇bi = ⟨
h
Sigmoid

W ⊤v
σ2 + b
i
i⟩d −⟨
h
Sigmoid

W ⊤v
σ2 + b
i
i⟩m.
(36)
15

B
MORE EXPERIMENTAL RESULTS
For all experiments, we set the initial variances of GRBMs to be 1, clip the gradient norm to be no
larger than 10, and use SGD with neither momentum nor weight decay. We divide the total energy
of a mini-batch by the batch size so that we are minimizing the average negative log likelihood. We
also decay the learning rate of SGD from the initial value 0.01 to 0 using the same cosine scheduler
as described in Eq. (22). The burn-in step in CD learning is set to 0, i.e., we do not discard any
samples from any Markov chains. For all experiments involving images, we standardize the input
image by subtracting the pixel-wise mean and dividing by the pixel-wise standard deviation. For
color images, the subtraction and division is performed channel-wise.
B.1
GAUSSIAN MIXTURE DENSITIES
The batch size and the hidden size are set to 100 and 256 respectively. We adjust at every step
whenever Metropolis adjustmentment is used as the experiments with Gaussian mixture densities
are fast. Although smaller hidden size could work, but we found this size makes learning converges
stably for all sampling algorithms. To ensure a fair comparison, we train all GRBMs for 50K epochs
and use the last model to draw the density plots and samples, despite the learning processes with
most of inference algorithms converge within 5K to 10K epochs.
B.2
ABLATION STUDY ON MNIST
In this part, we perform ablation study on MNIST to investigate the effect of several important
factors. In all experiments on MNIST, we set batch size to 512 and the number of epochs to 3000.
First, we vary the CD step and the hidden size while ﬁxing the other hyperparameters. The results
are shown in Table 2. We found that 4096 hidden size and 100 CD steps work the best on MNIST.
More CD steps would potentially be better but take longer time to train. Then we turn to study
the number of Langevin sampling steps, the adjust step size, the initial Langevin step size, and its
annealing. Here annealing means we decay the initial Langevin step size to 0 following the cosine
scheduler as training goes on. The results are shown in Table 3. We can see that the larger the
initial Langevin step size, the better the performance. But values larger than 0.04 would sometimes
make the sampling numerically fail. The more the Langevin steps, the better the performance would
be. Again, it comes with more computational cost with more Langevin steps. We also ﬁnd that it
may not be necessary to adjust at every step and annealing the initial step size slightly improves the
performance of Gbbis-Langevin with adjustment.
B.3
MORE VISUAL RESULTS
We train 3K epochs for experiments on both FashionMNIST and CelebA-32 datasets. For CeleA-
2K-64, we train 4K epochs. The batch size on FashionMNIST and CelebA-32 is 512 whereas the
batch size on CeleA-2K-64 is 100.
We show the samples drawn from the best GRBMs learned with different sampling methods in Fig.
5. It is clear that samples corresponding to Gibbs-Langevin have better visual qualities than those
from Langevin and Gibbs. We also show more results of GRBMs learned with Gibbs-Langevin in
Fig. 6, Fig. 7, Fig. 8, and Fig. 9.
Methods
CD Step
Hidden Size
FID
Gibbs-Langevin wo. Adjust
50
2048
32.33
Gibbs-Langevin wo. Adjust
50
4096
21.02
Gibbs-Langevin wo. Adjust
50
8192
22.05
Gibbs-Langevin wo. Adjust
100
4096
17.49
Table 2: Ablation study of the hidden size and the number of CD steps on MNIST dataset.
16

Methods
Langevin
Step K
Langevin
Step Size α0
Anneal α0
Adjust
Step η
FID
Gibbs-Langevin wo. Adjust
1
20

-
35.08
Gibbs-Langevin wo. Adjust
5
20

-
19.00
Gibbs-Langevin wo. Adjust
10
20

-
17.49
Gibbs-Langevin wo. Adjust
10
10

-
21.05
Gibbs-Langevin wo. Adjust
10
5

-
25.67
Gibbs-Langevin w. Adjust
10
20

0
21.31
Gibbs-Langevin w. Adjust
10
20

25
21.25
Gibbs-Langevin w. Adjust
10
20

50
20.64
Gibbs-Langevin w. Adjust
10
20

50
19.27
Table 3: Ablation study of the number of Langevin steps K, the initial Langevin step size α0,
annealing of the initial Langevin step size, and the Metropolis adjust step η on MNIST dataset. All
runs use 100 CD steps.
(a) Gibbs
(b) Langevin
(c) Langevin
w. adjust
(d) Gibbs-Langevin
(e) Gibbs-Langevin
w. adjust
Figure 5: Samples from GRBMs learned with different sampling algorithms on MNIST.
Figure 6: More samples from the learned GRBM (Gibbs-Langevin) on MNIST.
17

Figure 7: More samples from the learned GRBM (Gibbs-Langevin) on FashionMNIST.
Figure 8: More samples from the learned GRBM (Gibbs-Langevin) on CelebA-32.
Figure 9: More samples from the learned GRBM (Gibbs-Langevin) on CelebA-2K-64.
18

