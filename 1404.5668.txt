An Adversarial Interpretation of Information-Theoretic Bounded Rationality
Pedro A. Ortega and Daniel D. Lee
School of Engineering and Applied Sciences
University of Pennsylvania
Philadelphia, PA 19104, USA
{ope,ddlee}@seas.upenn.edu
Abstract
Recently, there has been a growing interest in modeling
planning with information constraints. Accordingly, an
agent maximizes a regularized expected utility known
as the free energy, where the regularizer is given by
the information divergence from a prior to a posterior
policy. While this approach can be justiﬁed in various
ways, including from statistical mechanics and informa-
tion theory, it is still unclear how it relates to decision-
making against adversarial environments. This connec-
tion has previously been suggested in work relating the
free energy to risk-sensitive control and to extensive
form games. Here, we show that a single-agent free en-
ergy optimization is equivalent to a game between the
agent and an imaginary adversary. The adversary can,
by paying an exponential penalty, generate costs that
diminish the decision maker’s payoffs. It turns out that
the optimal strategy of the adversary consists in choos-
ing costs so as to render the decision maker indiffer-
ent among its choices, which is a deﬁnining property of
a Nash equilibrium, thus tightening the connection be-
tween free energy optimization and game theory.
Keywords: bounded rationality, free energy, game theory,
Legendre-Fenchel transform.
1
Introduction
Recently, there has been a renewed interest in bounded ra-
tionality, originally proposed by Herbert A. Simon as an
alternative account to the model of perfect rationality in
the face of complex decisions (Simon 1972). In artiﬁcial
intelligence (AI), this development has been largely mo-
tivated by the intractability of exact planning in complex
and uncertain environments (Papadimitriou and Tsitsiklis
1987) and the difﬁculty of ﬁnding domain-speciﬁc sim-
pliﬁcations or approximations that would render planning
tractable despite the latest theoretical advancements in un-
derstanding the AI problem (Hutter 2004). Newly proposed
techniques based on Monte Carlo simulations such as Monte
Carlo Tree Search (Coulom 2006; Browne et al. 2012),
Thompson sampling (Strens 2000) and Free Energy (Kap-
pen 2005b) have become the focus of much AI and rein-
forcement learning research due to their wide applicability
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
and their unprecedented success in difﬁcult problems such
as computer Go (Gelly et al. 2006), universal planning (Ve-
ness et al. 2011), human-level video game playing (Mnih
et al. 2013) and robot learning (Theodorou, Buchli, and
Schaal 2010). Roughly, these efforts can be broadly classi-
ﬁed into two groups: (a) randomized approximations to the
perfect rationality model and (b) exact statistical-mechanical
or information-theoretic (IT) approaches. The latter group
is of special theoretical interest, as it rests on a model of
bounded rationality1 that yields randomized optimal poli-
cies as a consequence of resource-boundedness (Ortega and
Braun 2013).
The difference between the perfectly rational and the IT
bounded rational model lies in the choice of the objective
function: the IT approach adds a regularization term in the
form of a Kullback-Leibler divergence (KL-divergence) to
the expected utility of the perfectly rational model. The re-
sulting objective function goes under various names in the
literature, such as KL-control cost and free energy, and has
been motivated in numerous ways. The initial inspiration
comes from the maximum entropy principle of statistical
mechanics as a way to model stochastic control problems
that are linearly-solvable (Fleming 1977; Kappen 2005a;
Todorov 2006). The methods from robust control were also
adopted by economists to characterize model misspeciﬁca-
tion (Hansen and Sargent 2008). Later, it was shown that
the free energy can be derived from axioms that treat util-
ities and information as commensurable quantities (Ortega
and Braun 2013). Then in robot reinforcement learning,
researchers proposed the free energy as a way to control
the information-loss resulting from policy updates (Peters,
M¨ulling, and Alt¨un 2010). Finally, the free energy has also
been motivated from arguments that parallel rate-distortion
theory and information geometry by showing that the opti-
mal policy minimizes the decision complexity subject to a
constraint on the expected utility (Tishby and Polani 2011).
It is also worth mentioning that similar approaches arise in
the context of neuroscience (Friston 2009) and in game the-
1There are several approaches to bounded rationality in the lit-
erature, most notably those coming from the ﬁeld of behavioral
economics (Rubinstein 1998), which put emphasis on the proce-
dural elements of decision making. Here we have settled on the
qualiﬁer “information-theoretic” as a way to distinguish from these
approaches.
arXiv:1404.5668v1  [cs.AI]  22 Apr 2014

ory (Wolpert 2004).
An important yet intriguing property of the free energy
is that it is a universal aggregator of value. More precisely,
it can implement the minimum, expectation, maximization,
and all the “soft” aggregations in between by varying a
single parameter. This has at least two important conse-
quences. First, the free energy corresponds, in economic jar-
gon, to the certainty-equivalent, i.e. the value a risk-sensitive
decision-maker is thought to assign to an uncertain choice
(van den Broek, Wiegerinck, and Kappen 2010). Second, it
has been pointed out that decision trees are nested certainty-
equivalent operations (Ortega and Braun 2012). This is im-
portant, as decision making in the face of an adversarial,
an indifferent, and a cooperative environment, which have
previously been treated as unrelated modeling assumptions,
are actually instantiations of a single decision rule. Previous
work explores this property in multi-agent settings, showing
that solutions change from being risk-dominant to payoff-
dominant (Kappen, G´omez, and Opper 2012).
Aim of this Work
Despite the identiﬁcation of the free energy with the
certainty-equivalent, the connection to adversarial environ-
ments is as yet not fully understood. Our work makes an
important step into understanding this connection. By ap-
plying a Legendre-Fenchel transformation to the regulariza-
tion term of the free energy, it is revealed that a single-agent
free energy optimization can be thought of as representing
a game between the agent and an imaginary adversary. Fur-
thermore, this result explains stochastic policies as a strategy
that guards the agent from adversarial reactions of the envi-
ronment.
Structure of this Article
This article is organized into four sections. The aim of the
next section (Section 2) is to familiarize the reader with the
mathematical foundations underlying the planning problem
in AI, and to give a basic introduction into IT bounded ra-
tionality necessary in order to contextualize the results of
our work. Section 3 contains our central contribution. It ﬁrst
brieﬂy reviews Legendre-Fenchel transformations and then
applies them to the free energy functional in order to unveil
the adversarial assumptions implicit in the objective func-
tion. Finally, Section 4 discusses the results and concludes.
2
Preliminaries
We review the abstract foundations of planning under ex-
pected utility theory and IT bounded rationality.
Variational Principles
The behavior of an agent is typically characterized in one
of two ways: (a) by directly describing its policy, or (b) by
specifying a variational problem that has the policy as its
solution. While the former is a direct speciﬁcation of the
agent’s actions under any contingency, the latter has the ad-
vantage that it provides an explicit (typically convex) objec-
tive function that the policy has to optimize. Thus, a varia-
tional principle has additional explanatory power: not only
does it single out an optimal policy, but it also encodes a
preference relation over the set of feasible policies. Cru-
cially, the qualiﬁer “optimal” only holds relative to the ob-
jective function and is by no means absolute: because given
any policy, one can always engineer a variational principle
that is extremized by it. In AI, virtually all planning algo-
rithms are framed (either explicitly or implicitly) as maxi-
mum expected utility problems. This encompasses popular
problem classes such as multi-armed bandits, Markov de-
cision processes and partially observable Markov decision
processes (Legg 2008, Chapter 3).
Sequential versus Single-Step Decisions
We brieﬂy recall a basic theoretical result that will simplify
our analysis. In planning, sequential decision problems can
be rephrased as single-step decision problems: instead of let-
ting the agent choose an action in each turn, one can equiv-
alently let the agent choose a single policy in the beginning
that it then must follow during its interactions with the en-
vironment2. This observation was ﬁrst made in game the-
ory, where it was proven that every extensive form game can
always be re-expressed as a normal form game (Von Neu-
mann and Morgenstern 1944). Consequently, we abstract
away from the sequential nature of the general problem by
limiting our discussion to single-step decisions involving a
single action and observation. The price we pay is to hide
the dynamical structure and to increase the complexity of
the policy, but the mathematical results can stated more con-
cisely.
(Subjective) Expected Utility
Expected utility (Von Neumann and Morgenstern 1944;
Savage 1954) is the de facto standard variational principle in
artiﬁcial intelligence (Russell and Norvig 2010). It is based
on a utility function, that is, a real-valued mapping of the out-
comes encoding the desirability of each possible realization
of the interactions between the agent and the environment.
The qualiﬁers “subjective” and “expected” in its name derive
from the fact that the desirability of a stochastic realization
is calculated as the expectation over the utilities of the par-
ticular realizations measured with respect to the subjective
beliefs of the agent.
Formally, let X and Y be two ﬁnite sets, the former cor-
responding to the set of actions and the latter to the set
of observations. A realization is a pair (x, y) ∈X × Y.
Furthermore, let U : X × Y →R be a utility function,
such that U(x, y) represents the desirability of the realiza-
tion (x, y) ∈X × Y; and let q(·|·) be a conditional proba-
bility distribution characterizing the environmental response
where q(y|x) represents the probability of the observation
y ∈Y given the action x ∈X. Then, the agent’s optimal
2Note that this reduction encompasses policies that appear to
change during its execution as a function of the history: the meta-
policy controlling the changes is a compressed, yet sufﬁcient de-
scription of the changing policy.

max
min
Figure 1: Expected Utility Theory. The decision problem
(left) boils down to choosing a policy—a member of the
simplex over actions x ∈X—maximizing the convex com-
bination over conditional expected utilities E[U|x] (right).
policy p ∈∆(X) is chosen so as to maximize the functional
E[U] =
X
x
p(x)E[U|x]
=
X
x
p(x)
X
y
q(y|x)U(x, y).
(1)
This is illustrated in Figure 1. An optimal policy is any dis-
tribution p∗with no support over suboptimal actions, that is,
p∗(x) = 0 whenever E[U|x] ≤maxz E[U|z]. In particular,
because the expected utility is linear in the policy probabili-
ties, one can always choose a solution that is a vertex of the
probability simplex ∆(X):
p∗(x) = δx
x∗=
1
if x = x∗:= arg maxx E[U|x]
0
otherwise,
(2)
where δ is the Kronecker delta. Hence, there always exists a
deterministic optimal policy.
Once the optimal policy has been chosen, the utility U be-
comes a well-deﬁned random variable with probability dis-
tribution
P(U = u) = P

(x, y) : U(x, y) = u
	
=
X
U −1(u)
p∗(x)q(y|x).
(3)
Even though the optimal policy might be deterministic, the
utility of the ensuing realization is in general stochastic.
Let us now consider the case when the environment is
another agent. Formally, this means that the agent lacks
the conditional probability distribution q(·|·) over observa-
tions that it requires in order the evaluate the expected util-
ities. Instead, the agent possesses a second utility function
V : X × Y →R characterizing the desires of the environ-
ment. Game theory then invokes a solution concept, most
notably the Nash equilibrium, in order to obtain the miss-
ing distribution q(·|·) from U and V that renders the original
description as a well-deﬁned decision problem. Thus, con-
ceptually, game theory starts from slightly weaker assump-
tions than expected utility theory. For simplicity, we restrict
ourselves to two particular cases: (a) the fully adversarial
case U(x, y) = −V (x, y); and (b) the fully cooperative case
U(x, y) = V (x, y). The Nash equilibrium then yields the
decision rules (Osborne and Rubinstein 1999):
p∗= arg max
p
X
x
p(x)

min
q
q(y|x)U(x, y)

,
(4)
p∗= arg max
p
X
x
p(x)

max
q
q(y|x)U(x, y)

(5)
for the two cases respectively. Comparing these to (1) we
immediately see that, for planning purposes, it is irrelevant
whether the decision is made by the agent or not—what mat-
ters is the degree to which an outcome (be it an action or an
observation) contributes to the agent’s objective function (as
encoded by one of the three possible aggregation operators).
Thus, equations (1), (4) and (5) can be regarded as consisting
of not one, but two nested “decision steps” of the form
max
x {U(x)},
expp[U(X)]
or
min
x {U(x)}.
We end this brief review by summarizing the main properties
of expected utility:
1. Linearity: The objective function is linear in the policy.
2. Existence of Deterministic Solution: Although there are
optimal stochastic policies, there always exists an equiva-
lent deterministic optimal policy.
3. Indifference to Higher-Order Moments: Expected util-
ity places constraints on the ﬁrst, but not on the higher-
moments of the probability distribution of the utility.
4. Exhaustive Search: Finding the optimal deterministic pol-
icy requires, in the worst-case, an exhaustive evaluation of
all the expected utilities.
It is important to note that these properties only apply to
expected utility, but not to game theory.
Free Energy
How does an agent make decisions when it cannot exhaus-
tively evaluate all the alternatives? IT bounded rationality
addresses this question by deﬁning a new objective function
that trades off utilities versus information costs. Planning is
conceptualized as a process that transforms a prior policy
into a posterior policy that incurs a cost measured in utiles.
Importantly, it is assumed that the agent is not allowed itself
to reason about the costs of this transformation3; rather, it
tries to optimize the original expected utility but then runs
out of resources. From the point of view of an external ob-
server, this “interrupted” planning process will appear as if
it were explicitly optimizing the free energy.
Formally, let X be a ﬁnite set corresponding to the set of
realizations and U : X →R is the utility function. Fur-
thermore, let p0 ∈∆(X) be a prior policy and β ∈R be
a boundedness parameter. Then, the agent’s optimal policy
p ∈∆(X) is chosen so as to extremize the free energy func-
tional
Fβ[p] :=
X
x
p(x)U(x)
|
{z
}
Expected Utility
−1
β
X
x
p(x) log p(x)
p0(x)
|
{z
}
Information Costs
,
(6)
3The inability to reason about resource costs renders IT
bounded rationality fundamentally different from the meta-
reasoning approach of Stuart J. Russell (Russell 1995).

x1
x2
x3
x1
x2
x3
x1
x2
x3
Expected Utility
KL-Divergence
Free Energy
Figure 2: Free energy consists of the expected utility reg-
ularized by the KL-divergence between a given prior and
posterior policy. The resulting objective function is convex
and has an optimum p∗that is in general in the interior of
the probability simplex ∆(X).
where it is seen that β controls the conversion from units of
information to utiles (Figure 2). The optimal policy is given
by the “softmax”-like distribution
p∗(x) =
p0(x)eβU(x)
P
x p0(x)eβU(x)
(7)
which is known as the equilibrium distribution. Notice that
β controls how much the agent is in control of the choice: a
value β ≈0 falls back to the prior policy and represents lack
of inﬂuence; and values far away from zero yield either ad-
versarial or cooperative choices depending on the sign of β.
An important property of the equilibrium distribution is that
it can be simulated exactly without evaluating all the utili-
ties using Monte Carlo methods (Ortega, Braun, and Tishby
2014).
As we have mentioned before, the free energy (6) corre-
sponds to the certainty-equivalent. This is seen as follows:
the extremum of (6) given by
1
β log Zβ,
where
Zβ :=
X
x
p0(x)eβU(x),
(8)
but now seen as a function of β, can be thought of as an in-
terpolation of the maximum, expectation and minimum op-
erator, since
1
β log Zβ = max
x {U(x)}
β →+∞
1
β log Zβ =
Ep0[U]
β →
0
1
β log Zβ = min
x {U(x)}
β →−∞.
We end this brief review by summarizing the main properties
of IT bounded rationality:
1. Nonlinearity: The objective function is nonlinear in the
policy.
2. Stochastic Solutions: Optimal policies are inherently
stochastic.
3. Sensitive to Higher-Order Moments: Because the free en-
ergy is nonlinear in the policy, it places constraints on the
higher-order moments of the policy too. In particular, a
Taylor expansion of the KL-divergence term reveals that
the free energy is sensitive to all the moments of the pol-
icy.
Figure 3: The Legendre-Fenchel Transform. Given a func-
tion f(x), the convex conjugate f ⋆(s) corresponds to the
intercept of a tangent line to the curve with slope s.
4. Randomized Search: Obtaining a decision from the opti-
mal policy does not require the exhaustive evaluation of
the utilities. Rather, it can be sampled using Monte Carlo
techniques.
3
Adversarial Interpretation
Before presenting our main results, we give a brief deﬁnition
of Legendre-Fenchel transforms.
Legendre-Fenchel Transforms
The Legendre-Fenchel transformation is a transformation
that yields an alternative encoding of a given functional re-
lationship. More precisely, the information contained in the
function is expressed using the derivative as the independent
variable. Because of this, Legendre-Fenchel transformations
play an important role in thermodynamics and optimization.
For a function f(x) : Rn →R, it is deﬁned by the varia-
tional formula
f ⋆(s) = sup
x∈X
{⟨s, x⟩−f(x)}
where f ⋆(s) : Rn →R is the convex conjugate and ⟨s, x⟩
is the inner product of two vectors s, x in Rn. The following
are common examples:
1. Afﬁne function:
f(x) = ax −b
f ⋆(s) =
b
if s = a,
+∞
if s ̸= a.
2. Power function:
f(x) = 1
α|x|α
f ⋆(s) = 1
α′ |s|α′
where 1
α + 1
α′ = 1.
3. Exponential function:
f(x) = ex
f ⋆(s) =



s log s −s
if s > 0,
0
if s = 0,
+∞
if s < 0.
We refer the reader to Touchette’s article (Touchette 2005)
for a brief introduction or Boyd and Vanderberghe’s book
(Boyd and Vandenberghe 2004) for a thorough treatment.

Unveiling the Adversarial Environment
Using a Legendre-Fenchel transformation, one can show
that the regularization term of the free energy can be rewrit-
ten as a variational problem.
Lemma 1.
−1
β
X
x
p(x) log p(x)
p0(x)
= min
C
X
x
−p(x)C(x) + p0(x)eβC(x) −1
β (log β + 1)
(9)
Proof. Since n = |X| is ﬁnite, p(x), C(x) are vectors in Rn.
Essentially, the lemma says that the l.h.s. is the convex con-
jugate of the function
f(C) = −
X
x
p0(x)eβC(x) + 1
β (log β + 1),
because the r.h.s. can be re-expressed as
min
C
X
x
−p(x)C(x) + f(C) = max
C
X
x
p(x)C(x) −f(C).
Setting the derivative w.r.t. C to zero yields the optimality
condition
p(x) = βp0(x)eβC(x).
Isolating C(x) and substituting into P
x p(x)C(x) −f(C)
proves the lemma.
Consequently, the regularization term of the free energy
encapsulates a second variational problem over an auxiliary
vector C. In particular, the logarithmic form encodes an ob-
jective function that is exponential in C. Equipped with this
lemma, we can state our main result.
Theorem 2. The maximization of the free energy (6) is
equivalent to the maximization of
min
C
X
x
p(x)[U(x) −C(x)]
|
{z
}
Expected Net Utility
+
X
x
p0(x)eβC(x)
|
{z
}
Penalty of Adversary
(10)
w.r.t. the policy p ∈∆(X).
Proof. The
proof
is
an
immediate
consequence
of
Lemma 1:
arg max
p
X
x
p(x)U(x) −1
β
X
x
p(x) log p(x)
p0(x)

= arg max
p
X
x
p(x)U(x) + min
C
X
x
−p(x)C(x)
+ p0(x)eβC(x) −1
β (log β + 1)

= arg max
p
min
C
X
x
p(x)[U(x) −C(x)] +
X
x
p0(x)eβC(x)

.
This result can be interpreted as follows. When a single
agent maximizes the free energy, it is implicitly assuming a
situation where it is playing against an imaginary adversary.
In this situation, the agent ﬁrst chooses its policy p ∈∆(X),
and then the adversary attempts to decrease the agent’s net
utilities by subtracting costs C(x). However, the adversary
cannot choose these costs arbitrarily; instead, it must pay
exponential penalties. In fact, its optimal strategy can be di-
rectly calculated from Lemma 1.
Corollary 3. The imaginary adversary’s optimal strategy is
given by
C∗(x) = 1
β log
p(x)
βp0(x).
(11)
Inspecting (11), we see that the optimal costs scale rela-
tively with the agent’s deviation from its prior probabilities.
This leads to an interesting interaction between the agent’s
and the imaginary adversary’s choices.
Indifference
Our second main result characterizes the solution to this ad-
versarial setup. It turns out that the adversary’s best strategy
is to choose costs such that the agent’s net payoffs are uni-
form.
Theorem 4. The solution to
max
p
min
C
X
x
p(x)[U(x) −C(x)] +
X
x
p0(x)eβC(x)
has the property that for all x ∈X,
U(x) −C(x) = constant.
Proof. We ﬁrst note that we can exchange the maximum and
minimum operations,
max
p
min
C
X
x
p(x)[U(x) −C(x)] +
X
x
p0(x)eβC(x)
= min
C max
p
X
x
p(x)[U(x) −C(x)] +
X
x
p0(x)eβC(x)
because there is no duality gap due to the concavity of the
exponential function. We can thus maximize ﬁrst w.r.t. the
policy p. Deﬁne X ∗⊂X as the subset of elements maxi-
mizing the penalized expected utility, that is, for all x∗∈X
and x ∈X,
U(x∗) −C(x∗) ≥U(x) −C(x).
(12)
Maximizing w.r.t. to p yields optimal probabilities p∗(x)
given by
p∗(x) =
q(x)
if x ∈X ∗,
0
otherwise,
where q is any distribution over X ∗. Given this, the worst
case costs C∗(x) are
C∗(x) =
(
1
β log
q(x)
βp0(x)
if x ∈X ∗,
−∞
otherwise.
(13)
However, if X ∗̸= X, then we get a contradiction, since
U(x∗) −C∗(x∗) ̸≥U(x) −C∗(x)

1
2
3
0
0.5
1
1
2
3
0
0.5
1
1
2
3
0
0.5
1
1
2
3
0
0.5
1
1
2
3
0
0.5
1
1
2
3
0
8
1
2
3
0
8
1
2
3
0
8
1
2
3
0
8
1
2
3
0
8
Figure 4: Given an agent’s policy p, the environment chooses
costs that change the net utilities U(x)−C(x) of the realiza-
tions with support. The agent can protect itself from adver-
sarial costs by randomization. The last column corresponds
to the agent’s optimal policy.
for all x /∈X ∗, violating (12). Hence, it must be that for all
x ∈X,
U(x) −C(x) = constant,
concluding the proof.
To get a better understanding on the meaning of this re-
sult, it is helpful to consider an example. Figure 4 illustrates
four choices of policies and the corresponding optimal ad-
versarial costs. Here it is seen that an agent can protect itself
by spreading the probability mass of its policy over many
realizations.
4
Discussion
Starting from the free energy functional, we have shown how
to construct an alternative adversarial interpretation that con-
stitutes an equivalent problem. Conceptually, our ﬁndings
can be summarized as follows:
1. A regularization of the expected utility encodes assumpi-
ons about deviations from the expected utility.
2. The Legendre-Fenchel transformation reinterprets the
free energy as a game against an environmental adversary.
3. In this transformation, it is seen that the regularization is
equivalent to the penalization of adversarial costs.
4. Stochastic policies guard against adversarial costs. Ex-
pected utility alone is linear in the policy and thus encodes
deterministic optimal policies.
Indifference and Nash Equilibrium
Our results establish an interesting relation to game theory.
Theorem 4 and (11) immediately imply
U(x) −C∗(x) = U(x) −1
β log
p(x)
βp0(x) = constant
for all x ∈X. This turns out to be an known characteriza-
tion of the equilibrium distribution (7). However, in light of
our present results, it acquires a different twist; it ﬁts with
the well-known result that a Nash equilibrium is a strategy
proﬁle such that each player chooses a (mixed) strategy that
renders the others players indifferent to their choices (Os-
borne and Rubinstein 1999).
Other Regularizers
The presented method is general and can also be applied to
other regularizers in order to make the assumptions about
the imaginary adversary explicit. For instance, the following
list enumerates some examples.
1. Expected Utility: Because the regularization term is null,
the resulting adversary’s objective function is
max
p
min
C
X
x
p(x)[U(x) −C(x)] −δC(x)
0
.
Here we see that a null regularization implies an adver-
sary devoid of power, i.e. one that cannot alter the utilities
chosen by the agent.
2. Power Function: If the regularizer is a power function,
then we have the dual power function
max
p
min
C
X
x
p(x)[U(x) −C(x)] + |C(x)|α.
This case is interesting because it encompases ridge and
lasso as special cases.
3. Modern Portfolio Theory: One notable case that is widely
used in practice is Modern Portfolio Theory. Here, an
investor trades off asset returns versus portfolio risk,
encoded into a regularization term that is quadratic in
the policy (Markowitz 1952). The transformed objective
function is given by
max
p
min
C
X
x
p(x)[U(x) −C(x)] + 1
2λCT ΣC.
Conclusions
Our central result shows how to extract the adversarial cost
function implicitly assumed in the agent’s objective function
by means of an appropriately chosen Legendre transforma-
tion. Conversely, one can also start from the cost function
of an adversarial environment and reexpress it as a regular-
ized optimization. While our main motivation was to apply
this to the free energy functional, the transformation is gen-
eral and works also in other well-known frameworks such
as in modern portfolio theory. The immediate application is
that we can switch between the two representations and pick
the one that is easier to solve. This suggests novel algorithms
for solving the single-agent planning problem based on ideas
from differential game theory (Dockner et al. 2001) and con-
vex programming (Zinkevich 2003). Furthermore, this also
suggests that agents that randomize their decisions, are es-
sentially expressing their information limitations by protect-
ing themselves from undesired outcomes. As such, we be-
lieve this is a basic result that opens up a series of additional
questions regarding the nature of stochastic policies, such as
the conditions under which they arise and how they encode
risk sensitivity, which need to be further explored in the fu-
ture.

Acknowledgements
We thank the anonymous reviewers for their valuable com-
ments and suggestions for improving this manuscript. This
study was funded by grants from the U.S. National Science
Foundation, Ofﬁce of Naval Research and Department of
Transportation.
References
Boyd, S., and Vandenberghe, L. 2004. Convex Optimization.
Cambridge Univeristy Press.
Browne, C.; Powley, E.; Whitehouse, D.; Lucas, S.; Cowl-
ing, P.; Rohlfshagen, P.; Travener, S.; Perez, D.; Samoth-
rakis, S.; and Colton, S. 2012. A Survey of Monte Carlo
Tree Search Methods. IEEE Transactions on Computational
Intelligence and AI in Games 4(1).
Coulom, R. 2006. Efﬁcient Selectivity and Backup Opera-
tors in Monte-Carlo Tree Search. In Computer and Games.
Dockner, E.; Jorgensen, S.; Long, N.; and Sorger, G. 2001.
Differential Games in Economics and Management Science.
Cambridge University Press.
Fleming, W. 1977. Exit Probabilities and Optimal Stochastic
Control. Applied Mathematics and Optimization 4:329–346.
Friston, K. 2009. The free-energy principle: a rough guide
to the brain? Trends in Cognitive Science 13:293–301.
Gelly, S.; Wang, Y.; Munos, R.; and Teytaud, O. 2006. Mod-
iﬁcation of UCT with Patterns in Monte-Carlo Go. Techni-
cal report, Inst. Nat. Rech. Inform. Auto. (INRIA).
Hansen, L., and Sargent, T. 2008. Robustness. Princeton:
Princeton University Press.
Hutter, M. 2004. Universal Artiﬁcial Intelligence: Sequen-
tial Decisions based on Algorithmic Probability.
Berlin:
Springer.
Kappen, H.; G´omez, V.; and Opper, M.
2012.
Optimal
control as a graphical model inference problem. Machine
Learning 1:1–11.
Kappen, H. 2005a. A linear theory for control of non-linear
stochastic systems. Physical Review Letters 95:200201.
Kappen, H. 2005b. Path integrals and symmetry breaking
for optimal control theory. Journal of Statistical Mechanics:
Theory and Experiment.
Legg, S. 2008. Machine Super Intelligence. Ph.D. Disserta-
tion, Department of Informatics, University of Lugano.
Markowitz, H. 1952. Portfolio Selection. The Journal of
Finance 7:77–91.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.;
Antonoglou, I.; Wierstra, D.; and Riedmiller, M.
2013.
Playing Atari with Deep Reinforcement Learning.
ArXiv
(1312.5602).
Ortega, P., and Braun, D. 2012. Free Energy and the Gener-
alized Optimality Equations for Sequential Decision Mak-
ing.
In European Workshop on Reinforcement Learning
(EWRL’10).
Ortega, P. A., and Braun, D. A. 2013. Thermodynamics as
a Theory of Decision-Making with Information Processing
Costs. Proceedings of the Royal Society A 20120683.
Ortega, P.; Braun, D.; and Tishby, N. 2014. Monte Carlo
Methods for Exact & Efﬁcient Solution of the Generalized
Optimality Equations. In IEEE International Conference on
Robotics and Automation (ICRA).
Osborne, M., and Rubinstein, A. 1999. A Course in Game
Theory. MIT Press.
Papadimitriou, C., and Tsitsiklis, J. 1987. The Complexity
of Markov Decision Processes. Mathematics of Operations
Research 12(3):441–450.
Peters, J.; M¨ulling, K.; and Alt¨un, Y. 2010. Relative entropy
policy search. In AAAI.
Rubinstein, A. 1998. Modeling Bounded Rationality. Cam-
bridge, MA: MIT Press.
Russell, S., and Norvig, P. 2010. Artiﬁcial Intelligence: A
Modern Approach. Prentice-Hall, Englewood Cliffs, NJ, 3rd
edition edition.
Russell, S. 1995. Rationality and Intelligence. In Mellish,
C., ed., Proceedings of the Fourteenth International Joint
Conference on Artiﬁcial Intelligence, 950–957. San Fran-
cisco: Morgan Kaufmann.
Savage, L. 1954. The Foundations of Statistics. New York:
John Wiley and Sons.
Simon, H. 1972. Theories of Bounded Rationality. In Rad-
ner, C., and Radner, R., eds., Decision and Organization.
Amsterdam: North Holland Publ. 161–176.
Strens, M. 2000. A Bayesian Framework for Reinforcement
Learning. In ICML.
Theodorou, E.; Buchli, J.; and Schaal, S. 2010. A general-
ized path integral approach to reinforcement learning. Jour-
nal of Machine Learning Research 11:3137–3181.
Tishby, N., and Polani, D. 2011. Perception-Action Cycle.
Springer New York. chapter Information Theory of Deci-
sions and Actions, 601–636.
Todorov, E. 2006. Linearly solvable Markov decision prob-
lems. In Advances in Neural Information Processing Sys-
tems, volume 19, 1369–1376.
Touchette, H. 2005. Legendre-Fenchel Transforms in a Nut-
shell. Technical report, Rockefeller University.
van den Broek, B.; Wiegerinck, W.; and Kappen, H. 2010.
Risk Sensitive Path Integral Control. In UAI, 615–622.
Veness, J.; Ng, M.; Hutter, M.; Uther, W.; and Silver, D.
2011. A Monte-Carlo AIXI Approximation. Journal of Ar-
tiﬁcial Intelligence Research 40:95–142.
Von Neumann, J., and Morgenstern, O. 1944. Theory of
Games and Economic Behavior. Princeton: Princeton Uni-
versity Press.
Wolpert, D. 2004. Complex Engineering Systems. Perseus
Books. chapter Information theory - the bridge connecting
bounded rational game theory and statistical physics.
Zinkevich, M. 2003. Online Convex Programming and Gen-
eralized Inﬁnitesimal Gradient Ascent. In ICML, 928–936.

