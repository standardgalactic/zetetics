Draft version March 7, 2023
Typeset using LATEX twocolumn style in AASTeX63
Deep symbolic regression for physics guided by units constraints:
toward the automated discovery of physical laws
Wassim Tenachi
,1 Rodrigo Ibata
,1 and Foivos I. Diakogiannis
2
1Universit´e de Strasbourg, CNRS, Observatoire astronomique de Strasbourg, UMR 7550, F-67000 Strasbourg, France
2Data61, CSIRO, Kensington, WA 6155, Australia
(Received March 03, 2023)
Submitted to ApJ
ABSTRACT
Symbolic Regression is the study of algorithms that automate the search for analytic expressions that
ﬁt data. While recent advances in deep learning have generated renewed interest in such approaches,
eﬀorts have not been focused on physics, where we have important additional constraints due to the
units associated with our data. Here we present Φ-SO, a Physical Symbolic Optimization framework
for recovering analytical symbolic expressions from physics data using deep reinforcement learning
techniques by learning units constraints. Our system is built, from the ground up, to propose solutions
where the physical units are consistent by construction. This is useful not only in eliminating physically
impossible solutions, but because it restricts enormously the freedom of the equation generator, thus
vastly improving performance. The algorithm can be used to ﬁt noiseless data, which can be useful for
instance when attempting to derive an analytical property of a physical model, and it can also be used
to obtain analytical approximations to noisy data. We showcase our machinery on a panel of examples
from astrophysics.
Keywords: Neural networks (1933), Regression (1914), Astronomy data modeling (1859)
1. INTRODUCTION
Galileo famously intuited in Opere Il Saggiatore
(Galilei 1623) that the book of the Universe “`e scritto
in lingua matematica”. Ever since, it has been a central
concern of physics to attempt to explain the properties
of nature in mathematical terms, by proposing or deriv-
ing mathematical expressions that encapsulate our mea-
surements from experiment and observation. This ap-
proach has proven to be immensely powerful. Through
trial and error over the centuries, the great masters of
physics have developed and bequeathed us a rich tool-
box of techniques that have allowed us to understand
the world and build our modern technological civiliza-
tion. But now, thanks to the development of modern
deep learning networks, there is hope that this endeavor
could be accelerated, by making use of the fact that
machines are able to survey a vastly larger space of trial
solutions than an unaided human.
Of course, since the beginning of the computer rev-
olution, many methods have been developed to ﬁt co-
eﬃcients of linear or non-linear functions to data (see,
e.g., Press et al. 2007). While such approaches are un-
doubtedly very useful, the procedures we wish to dis-
cuss in the present contribution are more general, in the
sense that they aim to ﬁnd the functions themselves, as
well as any necessary ﬁtting coeﬃcients. In particular,
we wish to infer a free-form symbolic analytical func-
tion f : Rn −→R that ﬁts y = f(x) given (x, y) data.
In computer science, these procedures are generally re-
ferred to as “Symbolic Regression” (SR).
1.1. Motivations from physics and Big Data
Although there are multiple demonstrations of the ca-
pabilities of SR in physics (e.g., Wu & Tegmark 2019;
Liu & Tegmark 2021; Liu et al. 2021; Lemos et al.
2022; DiPietro & Zhu 2022; Bartlett et al. 2022; Rein-
bold et al. 2021) and astrophysics (e.g., Wadekar et al.
2022; Matchev et al. 2022; Shao et al. 2022; Delgado
et al. 2022; Wong & Cranmer 2022; Wadekar et al. 2020;
Desmond et al. 2023), to date, symbolic regression has
never been used to discover new physical laws from as-
trophysical measurements. Yet this may change thanks
to new observational missions and surveys such as Gaia
(Gaia Collaboration et al. 2016), Euclid (Laureijs et al.
2011), LSST (Ivezi´c et al. 2019; LSST Science Collabo-
arXiv:2303.03192v1  [astro-ph.IM]  6 Mar 2023

2
Tenachi et al.
ration et al. 2009) and SKA (Carilli & Rawlings 2004).
With these and other large surveys, our ﬁeld is entering
a new era of data abundance, and there is considerable
excitement at the possibility of discovering new physics
from these unprecedentedly rich datasets. However, the
colossal amount of data also presents signiﬁcant con-
ceptual challenges. Although deep learning will allow
us to extract valuable information from the large sur-
veys, it is both blessed and plagued by the underlying
neural networks that are one of its most potent compo-
nents. Neural networks are ﬂexible and powerful enough
to model any physical system (that can be described as a
Lebesgue integrable function Lu et al. 2017) and work in
high dimensions, but they unfortunately largely consist
of non-interpretable black boxes. Clearly, interpretabil-
ity and intelligibility are of great importance in physics,
which begs the question: how can one harness infor-
mation from these large datasets while retaining their
ability to interpret? After training a deep neural net-
work to ﬁt a dataset, can one open the black box, to
understand the physics modelled inside?
1.2. Symbolic regression
Symbolic regression addresses these issues by produc-
ing compact, interpretable and generalizable models. In-
deed, the goal is to ﬁnd very simple prescriptions such as
Newton’s law of universal gravitation that can explain
well a vast number of experiments and observations.
There are many advantages to discovering physical laws
in the form of succinct mathematical expressions rather
than large numerical models:
• Compactness:
The models produced by SR are
extremely compact compared to most numerical
models.
This makes the models computation-
ally inexpensive to run and in principle also en-
ables SR to correctly recover the exact underly-
ing mathematical expression of a dataset using
much less data than traditional machine learning
approaches and with a robustness towards noise
even for perfect model recovery (Wilstrup & Kasak
2021; Reinbold et al. 2021; La Cava et al. 2021).
• Generalization: In addition, the expressions pro-
duced by symbolic regression are less prone to
overﬁtting on measurement errors and are much
more robust and reliable outside of the ﬁtting
range provided by the data than large numerical
models, showing overall much better generaliza-
tion capabilities (Wilstrup & Kasak 2021) (we will
provide an example of this in Section 5).
This
makes SR a potentially powerful tool to discover
the most concise and general representation of the
measurements.
• Intelligibility & interpretability: Since the models
produced by SR consist of mathematical expres-
sions, their behavior is intelligible to us, contrary
to the case of large numerical models. This is of
enormous value in physics (Wu & Tegmark 2019)
as SR models may enable one to connect newly dis-
covered physical laws with theory and make sub-
sequent theoretical developments. More broadly,
this approach ﬁts into the increasing push towards
intelligible (Sabbatini & Calegari 2022), explain-
able (Arrieta et al. 2020) and interpretable (Mur-
doch et al. 2019) machine learning models, which
is especially important in ﬁelds where such mod-
els can aﬀect human lives (European Commission
2021; 117th US Congress 2022).
However, although the prospect of using SR for dis-
covering new physical laws may be very appealing, it
is also extremely challenging to implement. It is useful
to consider the diﬃculty of this problem if one were to
approach it in a naive way. Suppose in the trial ana-
lytic expressions, we allow for an expression length of
30 symbols (as we will do below), and that there are
15 diﬀerent variables or operations (e.g. x, +, −, ×, /,
sin, log, ...) to chose from for each symbol. A naive
brute-force attempt to ﬁt the dataset might then have
to consider up to 1530 ≈1.9 × 1035 trial solutions which
is obviously vastly beyond our computational means to
test against the data at the present day or at any time in
the foreseeable future. This consideration does not even
account for the optimization of free constants, which
makes SR an “NP hard” (nondeterministic polynomial
time) problem (Virgolin & Pissis 2022).
The obvious
conclusion one draws from these considerations is that
symbolic regression requires one to develop highly eﬃ-
cient strategies to prune poor guesses.
1.3. Physical Symbolic Regression
There are multiple approaches to SR (detailed in Sec-
tion 2) which are capable of generating accurate ana-
lytical models. However, in the context of physics, we
have the additional requirement that our equations must
be balanced in terms of their physical units, as other-
wise the equation is simply non-sensical, irrespective of
whether it gives a good ﬁt to the numerical values of the
data. Although powerful, to the best of our knowledge,
all of the available SR approaches spend most of their
time exploring a search space where the immense major-
ity of candidate expressions are unphysical in terms of
units and thus often end up producing unphysical mod-
els. A very simple solution to this problem would have
been to use an existing SR code, and check post hoc
whether the proposed solutions obey that constraint.

Physical Symbolic Optimization
3
Search space
Search space with our in situ physical units prior
+
cos
v0
x
t
cos
v0
+
/
cos
v0
x
t
v0
x
t
v0
x
t
v0
x
t
v0
x
t
v0
x
t
v0
x
t
v0
x
t
v0
x
t
cos
v0
x
t
v0
x
t
cos
t
v0
x
t
cos
v0
x
t
x
v0
x
t
cos
v0
x
t
v0
v0
x
t
cos
v0
x
t
v0
x
t
v0
x
t
v0
x
t
v0
x
t
+
cos
x
v0
x
t
cos
v0
x
t
v0
x
t
v0
v0
x
t
x
v0
x
t
t
v0
x
t
/
v0
v0
x
t
x
v0
x
t
t
v0
x
t
+
cos
t
v0
x
t
cos
v0
x
t
v0
x
t
v0
v0
x
t
x
v0
x
t
t
v0
x
t
9
cos
v0
x
t
/
cos
v0
x
t
+
v0
x
t
/
v0
x
t
cos
v0
x
t
v0
x
t
15
15
27
27
+
/
26
27
x
t
v0
+
v0
v0
+
/
v0
v0
v0
x
t
x
/
t
/
x
t
v0
Figure 1.
Illustration of the symbolic expression search space reduction enabled by our in situ physical units prior.
We
represent paths (in preﬁx notations) leading to expressions with physically-possible units (in red) and a small sample of the
paths that lead to expressions with unphysical units (in black). Here we consider the recovery of a velocity v using a library of
symbols {+, /, cos, v0, x, t} where v0 is a velocity, x is a length, and t is a time (limiting ourselves to 5 symbol long expressions
for readability). This reduces the search space from 268 expressions to only 6.
But not only does that constitute an immense waste of
time and computing resources, which could render many
interesting SR tasks impossible, it also makes a signif-
icant fraction of the resulting “best” analytical models
unusable and uninterpretable.
At ﬁrst glance, one could think of the units constraints
as severe restrictions that limit the capabilities of SR.
However, in this work we show that respecting physi-
cal constraints actually helps improve SR performance
not only in terms of interpretability but also in accuracy
by guiding the exploration of the space of solutions to-
wards exact analytical laws. This is consistent with the
studies of Petersen et al. (2019, 2021); Kammerer et al.
(2020) who found that using in situ constraints during
analytical expression generation is much more eﬃcient
as it vastly reduces the search space of trial expressions
(though we note their machinery is not capable of incor-
porating units constraints as it requires one to compute
and exploit the whole graph describing an expression as
a relational tree).
Here we present our physical symbolic optimization
framework (Φ-SO) which was built from the ground up
to incorporate and take full advantage of physical units
information during symbolic regression. This addresses
in part the combinatorial challenge discussed above in
subsection 1.2. Our Φ-SO framework includes the units
constraints in situ during the equation generation pro-
cess, such that only equations with balanced units are
proposed by construction, thus also greatly reducing the
search space as illustrated in Figure 1.
Although our framework could be applied to virtu-
ally any one of the SR approaches described in Sec-
tion 2, we chose to implement our algorithm in pytorch
(Paszke et al. 2019, currently the most popular deep
learning library) building upon key strategies pioneered
in the state-of-the-art Deep Symbolic Regression frame-
work proposed in Petersen et al. (2019) and Landajuela
et al. (2021a) which relies on reinforcement learning via
a risk-seeking policy gradient.
In the present study, we develop a foundational sym-
bolic embedding for physics that enables the entire ex-
pression tree graph to be tackled, as well as local units
constraints. This approach allows us to anticipate the
required units for the subsequent symbol to be gener-
ated in a partially composed mathematical expression.
By adopting this approach, we not only focus on training
a neural network to generate increasingly precise expres-
sions, as in Petersen et al. (2019), but we also generate

4
Tenachi et al.
labels of the necessary units and actively train our neu-
ral network to adhere to such constraints. In essence,
our method equips the neural network with the ability
to learn to select the appropriate symbol in line with
local units constraints.
To the best of our knowledge such a framework was
never built before.
This constitutes a ﬁrst step in
our planned research program of building a powerful
general-purpose symbolic regression algorithm for astro-
physics and other physical sciences. Our aim here is to
present the algorithm to the community, show its work-
ings and its potential, while leaving concrete astrophys-
ical research applications to future studies.
The layout of this study is as follows. We ﬁrst provide
a brief overview of the recent SR literature in Section 2.
Our Φ-SO framework is described in detail in Section 3,
its capabilities are showcased on a panel of astrophysical
test cases presented in Section 4, with results given in
Section 5, and ﬁnally in Sections 6 and 7 we discuss the
results and draw our conclusions.
2. RELATED WORKS – A BRIEF SURVEY OF
MODERN SYMBOLIC REGRESSION
SR has traditionally been tackled using genetic algo-
rithms where a population of candidate mathematical
expressions are iteratively improved through operations
inspired by natural evolution such as natural selection,
crossover, and mutation. This type of approach includes
the well known Eureqa software (Schmidt & Lipson
2009) (see Graham et al. 2013 for a benchmark of Eu-
reqa’s capabilities on astrophysical test cases), as well as
more recent works (Cranmer 2020; Virgolin & Bosman
2022; Stephens 2015; Kommenda et al. 2020; Keren et al.
2023). In addition, SR has been implemented using var-
ious methods ranging from brute force to (un-)guided
Monte-Carlo, all the way to probabilistic searches (Mc-
Conaghy 2011; Kammerer et al. 2020; Bartlett et al.
2022; Brence et al. 2021; Jin et al. 2019), as well as
through problem simpliﬁcation algorithms (Luo et al.
2017; Tohme et al. 2022).
Given the great successes of deep learning techniques
in many other ﬁelds, it is not surprising that they have
now been applied to symbolic regression, and now chal-
lenge the reign of Eureqa-type approaches (La Cava
et al. 2021; Matsubara et al. 2022). Multiple methods
for incorporating neural networks into SR have been de-
veloped, ranging from powerful problem simpliﬁcation
schemes (Udrescu & Tegmark 2020; Udrescu et al. 2020;
Cranmer et al. 2020; Keren et al. 2023), to end-to-end
symbolic regression methods where a neural network is
trained in a supervised manner to map the relationship
between datasets and their corresponding symbolic func-
tions (Kamienny et al. 2022; Vastl et al. 2022; d’Ascoli
et al. 2022; Becker et al. 2022; Biggio et al. 2020; Al-
nuqaydan et al. 2022; Ar´echiga et al. 2021), all the way
to incorporating symbols into neural networks in place
of activation functions to enable interpretability or to
recover a mathematical expression (Martius & Lampert
2016; Zheng et al. 2022; Sahoo et al. 2018; Valle & Had-
dadin 2021; Kim et al. 2020; Panju & Ghodsi 2020).
See La Cava et al. (2021) & Makke & Chawla (2022) for
recent reviews of symbolic regression algorithms.
While some of the aforementioned algorithms excel at
generating very accurate symbolic approximations, the
reinforcement learning based deep symbolic regression
framework proposed in Petersen et al. (2019) is the new
standard for exact symbolic function recovery, partic-
ularly in the presence of noise (La Cava et al. 2021;
Matsubara et al. 2022). This has resulted in a number
of studies in the literature built on this framework (e.g.,
Du et al. 2022; DiPietro & Zhu 2022; Zheng et al. 2022;
Landajuela et al. 2021b; Usama & Lee 2022).
3. METHOD
Considering the success of deep reinforcement learn-
ing methods in accurately recovering exact symbolic ex-
pressions, which is particularly important in the ﬁeld
of physics where precise physical law recovery is cru-
cial, we have chosen to incorporate this methodology
into the machine learning component of our physical
symbolic regression approach.
In Subsection 3.1, we
describe how we generate analytical expressions from
a recurrent neural network (RNN). Subsection 3.2 pro-
vides details about the algorithm we use to generate in
situ units constraints, which are used to teach the RNN
units rules and help to reduce the search space. And
ﬁnally, in Subsection 3.3, we describe the reinforcement
learning strategy we adopted to make our RNN not only
produce accurate expressions but also physically mean-
ingful ones.
3.1. Generating symbolic expressions
Symbolic expressions can be regarded as binary trees
where each node represents a symbol of the expression
in the library of available symbols, i.e., an input vari-
able (e.g., x, t), a constant (e.g., v0) or an operation
(e.g., +, −, ×, /, sin, log, ...). In this representation,
input variables and constants can be referred to as termi-
nal nodes or symbols (having no child node), operations
taking a single argument (e.g., sin, log, ...) are unary
symbols (having one child node) and operations taking
two arguments (e.g., +, −, ×, /, ...) are binary sym-
bols. By considering each node ﬁrst in depth and then
left to right, one can compute a one dimensional list i.e.

Physical Symbolic Optimization
5
Expression (prefix notation)
/
L.T-1
-
-
L.T-1
# dangling = 1
L.T-1
-
L.T-1
L.T-1
# dangling = 2
L.T-1
L.T-1
L.T-1
L.T-1
# dangling = 1
L.T-1
-
L.T-1
-
# dangling = 2
t
/
L.T-1
L
L
T
# dangling = 1
/
RNN
Categorical
distribution
Physical units prior
(+ other priors)
Sampled
token
Contextual information
around next token
# dangling
sibling and units
parent and units
previous token and units
1
2
3
4
5
Expression tree
Expression
Library of choosable
tokens
current required units
Figure 2. Expression generation sketch. The process starts at the top left RNN block. For each token, the RNN is given the
contextual information regarding the surroundings of the next token to generate, namely: the parent, sibling and previously
sampled token along with their units, the required units for the token to be generated and the dangling number (i.e. the
minimum number of tokens needed to obtain a valid expression). Based on this information, the RNN produces a categorical
distribution over the library of available tokens (top histograms) as well as a state which is transmitted to the RNN on its next
call. The generated distribution is then masked based on local units constraints (bottom histograms), forbidding tokens that
would lead to nonsensical expressions. The resulting token is sampled from this distribution, leading to the token ‘+’ in this
example. Repeating this process, from left to right, allows one to generate a complete physical expression, here [+, v0, /, x, t]
which translates into v0 + x/t in the inﬁx notation we are more familiar with.
a preﬁx1 notation in which operators are placed before
the corresponding operands in the expression, alleviat-
ing the need for parentheses. Using the preﬁx notation
and treating symbols, referred to as tokens, as categories
allows us to treat any expression as a mere sequence of
categorical vectors. E.g., considering short toy library
of tokens {+, cos, x}, the operator + can be encoded as
[1, 0, 0], the function cos as [0, 1, 0] and the variable x as
[0, 0, 1].
As in previous deep symbolic regression studies (e.g.,
(Kamienny et al. 2022; Vastl et al. 2022; Petersen et al.
2019; Du et al. 2022; DiPietro & Zhu 2022), treating
mathematical expressions as sequences allows us to em-
ploy traditional natural language processing techniques
to sample them. Token sequences are generated by using
an RNN, which, in a nutshell, is a neural network that
can be called multiple times i < N to generate a time
1 This is also called “Polish” notation, and can be converted to
a tree representation or the “inﬁx” notation which we are more
familiar with, as there is a one-to-one relationship between them.
dependent output as well as a time dependent Si mem-
ory state. The RNN takes as input some time dependent
observations Oi as well as the state of the previous call
Si−1. In practice, we use the RNN to generate a categor-
ical probability distribution over the library of available
tokens, which we then simply sample to draw a deﬁnite
token. Once a token is generated, we feed its properties
and the properties of its surroundings as observations
for the next RNN call.
We also feed as observations
the minimum number of tokens still needed to obtain a
valid analytical expression (i.e. the number of dangling
nodes), the nature of the token which was sampled at
the previous step, the sibling and parent tokens of the
token to be generated in a tree representation, to which
in the context of our Φ-SO framework we add the phys-
ical units of all of these tokens and the units required
for the token to be generated so as to respect the units
rules. This allows the inner mechanisms of the neural
network to take into account not only the local structure
of the expression for generating the next token, but also
to take into account the local units constraints.
The

6
Tenachi et al.
process described above can be repeated multiple times
until a whole token function is generated in preﬁx nota-
tion, as illustrated in Figure 2.
It is important to note here that one can artiﬁcially
tune the generated categorical distribution to incor-
porate prior knowledge in situ while expressions are
being generated.
One can for example zero-out the
probability of some token depending on the context
encoded in the expression tree being generated, thus
greatly reducing search space (Petersen et al. 2019,
2021). We therefore adopt priors that force expressions
sizes to be < 30 tokens long, to contain no more than
2 levels of nested trigonometric operations (e.g., for-
bidding cos(f.t + sin(x/x0 + tan(□))) but still allowing
cos(f.t+sin(x/x0))), contain no self nesting of exponent
and log operators (e.g., forbidding ee□) and forbid use-
less inverse unary operations (e.g., forbidding elog □). It
is worth noting that the combination of priors we em-
ploy can conﬂict in some cases, in which case we discard
the resulting candidate.
In addition to the above priors whose formulation de-
pends on the local tree structure (parent, sibling, ances-
tors), we are able to create priors that take into account
the entire tree structure. This is rendered possible by
the fact that contrary to most other SR algorithms (in-
cluding the state-of-the-art Deep Symbolic Regression
Petersen et al. 2019), in the Φ-SO framework we com-
pute and keep track of the full graph of the tree repre-
sentation while the expression is being generated, as it
is an essential ingredient to compute units constraints
as detailed in the following subsection.
3.2. In situ physical units constraints
As discussed above, in physics we already know that
some combinations of tokens are not possible due to
units constraints. For example, if the algorithm is in
the process of generating an expression in which a ve-
locity (v0) is summed with a length (x) divided by a
token or sub-expression which is still to be generated
(□):
v0 + x
□,
(1)
then based on the expression tree (as shown in Figure
2), we already know that that □must be a time variable
or a more complicated sub-tree that eventually ends up
having units of time, but that it is deﬁnitely not a length
or a dimensionless operator such as the log function.
Computing such constraints in situ i.e. in incomplete,
only partially sampled trees (containing empty place-
holder nodes) is much harder than simply checking post
hoc if the units of a given equation make sense, because
in some situations it is impossible to compute such con-
straints until later on in the sequence, leaving the units
of some nodes free (i.e. compatible with any units at this
point in the sequence). For example, it is impossible to
compute the units requirement in the left child node of
a (binary) multiplication operator token □× △, as any
units in the □left child node could be compensated by
units in the △right child node. Algorithm 1 shows the
pseudo-code of the procedure we devised to compute
the required units whenever possible and leaving them
as free otherwise. The procedure is applied to a token
at position i < N in an incomplete sequence of tokens
{τj}j<N of size N, knowing the units of terminal nodes
and of the root node (e.g., respectively {v0, x, t} and
{v} in the example of Figure 2). The sequence may be
partially made up of placeholder tokens of yet undeter-
mined nature (representing dangling nodes). Running
algorithm 1 before each token generation step allows one
to have a maximally informed expression tree graph in
terms of units.
Having access in situ to the (required) physical units
of tokens allows us to not only to inform the neural net-
work of our expectations in terms of units as well as to
feed it units of surrounding tokens, thus allowing the
model to leverage such information, but also to express
a prior distribution over the library. This enables the
algorithm to zero-out the probability of forbidden sym-
bols that would result in expressions that violate units
rules. Combining this prior distribution with the cate-
gorical distribution given by the RNN while expressions
are being generated results in a system where by con-
struction only correct expressions with correct physical
units can be formulated and learned on by the neural
network.
3.3. Learning
One might naively imagine that symbolic regression
problems could be solved by directly optimizing the
choice of symbols to ﬁt the problem, using the auto-
diﬀerentiation capabilities of modern machine learning
frameworks2.
Unfortunately this approach cannot be
used for symbolic regression because the cost function
is non diﬀerentiable (the choice of selecting say the sin
function over log is not diﬀerentiable with respect to
the data), which prevents one from using gradient de-
scent. A practical solution is to use a neural network as
a “middle man” to generate a categorical distribution
from which we can sample symbols. One can then opti-
mize the parameters of this neural network whose task
2 Most machine learning tasks use the diﬀerentiability of the imple-
mented model with respect to the data to implement a (stochas-
tic) gradient descent towards an optimal model solution that ﬁts
the data best

Physical Symbolic Optimization
7
Algorithm 1: In situ units requirements algo-
rithm.
Input: (In)-complete expression {τj}j<N, Position of
token i
Output: Required physical units Φi of token at i
1 function ComputeRequiredUnits({τj}j<N, i)
2 p ←PositionOfParent(i)
3 s ←PositionOfSibling(i)
4 Φp ←Units(τp)
5 Φs ←Units(τs)
6 AdditiveTokens ←{+, −}
7 MultiplicativeTokens ←{×, /}
8 PowerTokens ←{1/□,
√
□, □n}
9 DimensionlessTokens ←{cos, sin, tan, exp, log}
10 if τp is in AdditiveTokens and Φs is known then
11
Φi ←Φs
12 else if τp is in AdditiveTokens and Φp is free and
NodeRank(τi) is 2 and Φs is free then
13
BottomUpUnitsAssignement(start = s, end =
i −1)
14
Φi ←Φs
15 else if Φp is free and τp is not in
MultiplicativeTokens and τs is not a placeholder
then
16
Φi ←free;
17 else if i = 0 then
18
Φi ←Units(root)
19 else if τp is in AdditiveTokens then
20
Φi ←Φp
21 else if τp is in PowerTokens then
22
n ←Power(τp)
23
Φi ←Φp/n
24 else if Φp = 0 or τp is in DimensionlessTokens then
25
Φi ←0
26 else if τp is in MultiplicativeTokens then
27
if τi is a placeholder and τs is a placeholder
then
28
Φi ←free
29
else if NodeRank(τi) is 1 then
30
Φi ←free
31
else if Φp is free then
32
Φi ←free
33
else
34
BottomUpUnitsAssignement(start = s, end =
i −1)
35
if τi is {×} then
36
Φi ←Φp −Φs;
37
else if τi is {/} then
38
Φi ←Φs −Φp;
39 return Φi
is to generate these symbols according to ﬁt quality and
physical units constraints.
The training of the network that generates the dis-
tribution of symbols relies on the “reinforcement learn-
ing” strategy (Sutton & Barto 2018), which is a com-
mon method used to train artiﬁcial intelligence agents
to navigate virtual worlds such as video games3, or mas-
ter open-ended tasks (Adaptive Agent Team et al. 2023).
In the present context, the idea is to generate a set (usu-
ally called a “batch” in machine learning) of trial sym-
bolic functions, and compute a scalar reward for each
function by confronting it to the data. We can then re-
quire the neural network to generate a new batch of trial
functions, encouraging it to produce better results by
reinforcing behavior associated with high reward values,
approximating gradients via a so-called “policy” (i.e., a
quantitative strategy). The hope is that, by trial and
error, the learnable parameters of the network will con-
verge to values that are able to generate a symbolic func-
tion that ﬁts the data well.
Following the insight by Petersen et al. (2019), we
adopt the risk-seeking policy gradient along with the en-
tropy regularization scheme found by Landajuela et al.
(2021a). In essence, we only reinforce the best 5 % of
candidate solutions, not penalizing the neural network
for proposing the 95 % of other candidates, therefore
maximizing the reward of the few best performing can-
didates rather than the average reward. This enables an
eﬃcient exploration of the search space at the expense
of average performance, which is of particular interest
in SR as we are only concerned in ﬁnding the very best
candidates and do not care if the neural network per-
forms well on average4. This novel risk-seeking policy,
ﬁrst proposed by Petersen et al. (2019), has signiﬁcantly
boosted performance in symbolic regression.
It is worth noting that our approach reinforces can-
didates which are sampled based on not only the out-
put of the RNN, but also the local units constraints
derived from the units prior distribution, which en-
sures the physical correctness of token choices.
As a
result, our approach eﬀectively trains the RNN to make
appropriate symbolic choices in accordance with local
units constraints, in a quasi supervised learning manner.
This combined with the general reinforcement learning
paradigm enables us to produce both accurate and phys-
ically relevant symbolic expressions.
We allow the candidate functions f to also contain
“constants” with ﬁxed units, but with free numerical
values. These free constants allow us the possibility to
model situations where the problem has some unknown
physical scales. A (somewhat contrived) example from
3 See, e.g., https://www.youtube.com/watch?v=QilHGSYbjDQ
4 This is contrary to many other applications of reinforcement
learning (e.g., robotic automation, video games) which can even
sometimes require risk-adverse gradient policies (e.g., self driving
cars) (Rajeswaran et al. 2016).

8
Tenachi et al.
galactic dynamics could be if we were provided a set
of potential values Φ, and cylindrical coordinate values
(R, z) of some mystery function that was actually a sim-
ple logarithmic potential model:
Φ = 1
2v2
0 ln

R2
c + R2 + z2
q2

,
(2)
whose parameters are the velocity parameter v0, the core
radius Rc and the potential ﬂattening q. Of course, we
will generally not know in advance either the number
of such parameters that the correct solution requires,
or their numerical values.
Yet to be able to evaluate
the loss of the trial functions f, we need to assign val-
ues to all such free “constants” they may contain. We
accomplish this task by processing each trial function,
with the L-BFGS (Zhu et al. 1997) optimization routine
in pytorch, leveraging the fact that we can encode the
symbols of f using pytorch functions. Since pytorch has
in-built auto-diﬀerentiation, ﬁnding the optimal value
of the constants via gradient descent is extremely eﬃ-
cient. However, due to the number of trial expressions to
evaluate and considering that each expression must be
evaluated multiple times to optimize its free constants,
this optimization step is one of the main performance
bottlenecks of the Φ-SO algorithm.
Then, as in Petersen et al. (2019) for each candidate
f, we compute a reward R that is representative of ﬁt
quality: R = 1/(1+NRMSE) where NRMSE is the root
mean squared error normalized by the deviation of the
target σy: NRMSE =
1
σy
q
1
N
PN
i=1(yi −f(xi))2.
We
apply the policy gradients by means of an Adam opti-
mizer Kingma & Ba (2014) and use a long-short term
memory (LSTM) type RNN (Hochreiter & Schmidhuber
1997). Our additional learning hyper-parameters can be
found in Table 1. It is worth noting that the empirically
tuned batch size we found (10k) is larger than the one
found by Petersen et al. (2019) which was of 1k. We at-
tribute this to the very strong constraints oﬀered by our
Φ-SO setup which require a strong exploration counter-
part to avoid getting stuck in local minima.
It is also worth noting that in the reinforcement learn-
ing framework, the the reward function can be consid-
ered as as a black box, which does not have to be dif-
ferentiable, therefore one could use anything as the re-
ward. For example, we can also include the complexity
of the symbolic function in the reward function, so as
to have a criterion akin to Occam’s razor. But actually
one could in principle implement many ideas into the re-
ward function: symmetries, constraints on primitives or
derivatives, ﬁtness in a diﬀerential equation, the results
of some symbolic computation using external packages
such as Mathematica (Wolfram 2003) or SymPy (Meurer
Learning parameters
Batch size
10 000
Learning rate
0.0025
Entropy coeﬃcient
0.005
Risk factor
5 %
Table 1. Learning parameters.
et al. 2017), behavior of the function when implemented
an n-body simulation, and so on.
4. CASE STUDIES
We showcase our Φ-SO method on a panel of astro-
physical test cases: the relativistic energy of a particle
is examined in subsection 4.1, the law describing the ex-
pansion of the Universe in subsection 4.2, the isochrone
action from galactic dynamics in subsection 4.3 and ad-
ditional toy test cases given in 4.4. These examples show
that the method can successfully recover physical laws
and relations from real or synthetic data. We limit our-
selves to the exploration of 10 million trial expressions
which roughly takes ∼4 hours on a modern laptop com-
puter and is only necessary for the most diﬃcult case
(the relativistic energy).
In addition, for some of these cases, we give the Pareto
front which shows the most accurate expression based
on RMSE (root mean squared error) for each level of
complexity. Although we note that there are sophisti-
cated schemes inspired by information theory to deﬁne
the complexity (Udrescu & Tegmark 2020; Bartlett et al.
2022; Schmidt & Lipson 2009), we defer such considera-
tions to a future study and simply deﬁne the complexity
of each token to be of 1 except for input variables which
we take to have complexity zero.
We also deﬁne the successful exact symbolic recovery
of an expression by its symbolic equivalence using the
SymPy symbolic simpliﬁcation subroutine (Meurer et al.
2017) whenever possible and by a numerical criterion
akin to the one used in (La Cava et al. 2021; Matsubara
et al. 2022) R > 0.9999 along with manual veriﬁcations
in cases where it is impossible to automatically simplify
(e.g., when free constants are involved).
Finally,
we
agnostically
rely
on
the
same
li-
brary
of
choosable
tokens
for
all
test
cases:
{+, −, ×, /, 1/□,
√
□, □2, exp, log, cos, sin, 1} to which
we only add input variables and free or ﬁxed constants
depending on the test cases.
4.1. Relativistic energy of a particle

Physical Symbolic Optimization
9
2
4
6
8
10
12
14
complexity
−20
−10
0
10
20
log(RMSE)
E = mv2
E = mc2
E = m
 c2 + v2
E =
mc2
cos2

v2
c2

E =
mc2
q
1−v2
c2
Figure 3. Pareto-front encoding accuracy-complexity trade oﬀof recovered physical formulae typically recovered using our
Φ-SO method when applied to data for the relativistic energy of a particle. We recover the relativistic expression as well as the
classical approximation.
Let us consider the expression for the relativistic en-
ergy of a particle:
E =
mc2
q
1 −v2
c2
,
(3)
where m, v and c are respectively the mass of the par-
ticle, its velocity and the speed of light.
Using the aforementioned library of tokens as well as
the {m, v} input variables and a free constant {c}, Φ-
SO is able to successfully recover this expression 100% of
the time. Figure 3 contains the Pareto front of recovered
expressions where similarly to Udrescu et al. (2020), we
showcase that we are able to recover the relativistic en-
ergy of a particle as well as the classical approximation
which has a lower complexity.
However, we note that contrary to the system pro-
posed in (Udrescu et al. 2020) which mostly relies on
powerful problem simpliﬁcation schemes (in particular,
the identiﬁcation of symmetries as well as the identiﬁ-
cation of additive and multiplicative separability), our
system is able to recover the exact expression for the
relativistic energy test case without any simpliﬁcation
and without needing to simplify further the problem by
treating c as a variable taking a range of diﬀerent values
as in (Udrescu et al. 2020). To the best of our knowledge
our algorithm is the only one at present able to crack
this case under these more stringent conditions.
4.2. Expansion of the Universe
The next case study we examine is the Hubble Dia-
gram of supernovae type Ia, namely the change in the
observed luminosity of these important standard can-
dles as a function of redshift z. This is one of the major
pieces of evidence that indicates that the Universe is
experiencing an accelerating expansion, and it is also
one of the observational pillars underlying Λ Cold Dark
Matter (ΛCDM) cosmology in which Dark Energy dom-
inates the energy-density budget of the Universe.
We will use the so-called Pantheon state-of-the-art
compilation dataset (Scolnic et al. 2018), shown in Fig-
ure 4.
We follow an almost identical methodology as
Bartlett et al. (2022), to ﬁnd the Hubble parameter
H(z) from the measured supernova magnitude and red-
shift pairs. Following Bartlett et al. (2022), we use the
auxiliary function
y(x ≡1 + z) ≡H(z)2 ,
(4)
which for ΛCDM in a ﬂat Universe with negligible radi-
ation pressure is
yΛCDM(x) = H2
0(Ωmx3 + (1 −Ωm)) ,
(5)
where Ωm is the matter density parameter and H0 is
the Hubble constant. In this ﬂat Universe model the

10
Tenachi et al.
35
40
45
µb
Pantheon sample
Φ-SO expression (best ﬁt)
Φ-SO expression (ΛCDM)
0
1
2
z
0
100
counts
Figure 4. Symbolic regression results when applying the Φ-
SO algorithm (allowing two free parameters) to the Hubble
Diagram of supernovae Ia from the Pantheon sample. Φ-SO
rediscovers the ΛCDM relation (in red) as well as another
relation (in blue) which has a slightly better ﬁt than ΛCDM
when naively considering only Pantheon’s observational con-
straints due to the over-abundance of low z SNe.
cosmological luminosity distance is
dL(z) = (1 + z)
Z z
0
c dz′
H(z′) ,
(6)
where c is the speed of light.
We adapt our machinery to the Hubble diagram prob-
lem by integrating numerically the H(z) =
p
y(z) func-
tions proposed by the algorithm under Eqn. 6 to ob-
tain the implied luminosity distance dL.
These are
then trivially converted into a distance modulus µ(z) =
5 log10(dL(z)/10 pc), which can be compared to the Pan-
theon data.
This Hubble Diagram example showcases the capabil-
ity of the software to include free “constants” (here we
include one having the units of H0 and the other being
dimensionless as Ωm) in the expression search, whose
values are found thanks to auto-diﬀerentiation via L-
BFGS optimization, as mentioned in Section 3.3. The
optimal values of these constants need to be calculated
after being passed through the numerical integration
step (Eqn. 6), which turns out to be the main bottleneck
of the problem in terms of computational cost. However,
this also shows that the algorithm allows one to derive
expressions that are subsequently passed through com-
plicated operations before being compared to data.
Although we are able to successfully recover the
ΛCDM expression given in equation 5 during the SR
exploration using either synthetic data or the real Pan-
theon observational data, we note that as Bartlett et al.
(2022), using observational data our system ﬁnds more
accurate solutions at lower complexities than the ΛCDM
model. We attribute that to the fact that our system is
only given the chance to confront its trial model of H(z)
to a dataset of standard candles where there is an over
abundance of low z events but not to other observational
constraints such as the cosmic microwave background
which might tilt the balance in favor of ΛCDM as the
most accurate model at its level of complexity.
We note that contrary to the exhaustive symbolic re-
gression approach proposed in Bartlett et al. (2022), we
are able to recover this expression by typically explor-
ing < 50k expressions (which takes a few minutes on
a modern laptop) rather than 5 × 106 even given the
fact that in this study we are allowing more functions
(cos, sin, exp, log), demonstrating the eﬀectiveness of our
approach.
4.3. Isochrone action from galactic dynamics
Another interesting application of symbolic regression
is to derive perfect analytical properties of analytical
models of physical systems. To this end, we chose to
attempt to ﬁnd the radial action Jr of the spherical
isochrone potential.
Φ(r) = −
GM
b +
√
b2 + r2 ,
(7)
where G is the gravitational constant, M is the mass
of the model, b is a length scale of the model, and r is
a spherical radius (Binney & Tremaine 2011). Action
variables are special integrals of motion in integrable
potentials which can be used to describe the orbit of an
object in a system, and they are of particular interest
in Galactic Archaeology as they are adiabatic invari-
ants, so they are preserved if a galaxy or stellar system
has evolved slowly. The isochrone is the only potential
model to have actions known in analytic form in terms
of elementary functions5. For the case of the isochrone
model, the radial component of the action of a particle
can be expressed as
Jr =
GM
√
−2E −1
2

L + 1
2
p
L2 −4GMb

,
(8)
where E and L are, respectively, the particle energy and
total angular momentum (Binney & Tremaine 2011).
We provide our algorithm numerical values of Jr
(which has units of angular momentum) given L and
5 We have recently shown that actions can be calculated numeri-
cally from samples of points along orbits in realistic galaxy po-
tentials using deep learning techniques (Ibata et al. 2021).

Physical Symbolic Optimization
11
E, and leave b as a free scaling parameter.
Since we
expect each occurrence of M to be accompanied by an
occurrence of the gravitational constant, we provide the
algorithm with GM as a single variable.
This expression (Eqn. 8) could not be solved either
by the standard DSR algorithm (Petersen et al. 2019),
or by the AIFeynman algorithm (Udrescu & Tegmark
2020). Our algorithm was also not able to identify the
equation in 10 million guesses. However, one of the steps
of the AIFeynman algorithm is a test for additive and
multiplicative separability of the mystery function, and
it creates new datasets for each separable part.
For
the case of additive separability, the units remain un-
changed, and so it is trivial to simply provide our Φ-SO
algorithm these separated data to ﬁt in turn, one at a
time. Thus the ﬁrst term of the right hand side of Eqn. 8
(with an E dependence) was easily solved together with
a ﬁtted additive free constant. We then subtracted the
ﬁtted constant from the second dataset, and Φ-SO cor-
rectly recovered the second term on the right hand side
of Eqn. 8 (with an L dependence).
4.4. Supplementary cases
In addition to the cases above, we test our algorithm
on a set of textbook equations. We include Newton’s
law of universal gravitation:
F = Gm1m2
r2
,
(9)
where G is the universal gravitational constant, m1 and
m2 are the masses of the attracting bodies and r is the
distance separating them.
For this test case, we use
{m1, m2, r} as input variables and leave G as a free con-
stant.
We also include a damped harmonic oscillator which
appears in a wide range of (astro)-physical contexts:
y = e−αt cos(ft + Φ) ,
(10)
where α and f are respectively the damping parameter
and the frequency of oscillations (both homogeneous to
the inverse of a time) and Φ is the (dimensionless) phase.
We leave these three parameters as free constants and
use t as our input variable.
Finally,
we
consider
the
Navarro–Frenk–White
(NFW) proﬁle (Navarro et al. 1996) which is an em-
pirical relation that describes the density proﬁle ρ(r) of
halos of collisionless dark matter in cosmological N-body
simulations:
ρ =
ρ0
r
Rs

1 +
r
Rs
2 ,
(11)
where r is the radius which we use as an input variable
and ρ0 and Rs are respectively the density and radius
scale parameters which we leave as free constants.
5. RESULTS
In physics, we often seek to build approximate models,
such as might be obtained via a polynomial function or
a Fourier series ﬁt to some data.
In those instances,
the root mean square error is usually the criterion of
relevance to determine whether the procedure worked
well or not. However, here we wish to recover the “true”
underlying model, in which case the recovery rate should
be the criterion of success.
The performance of Φ-SO on the test cases detailed
above is summarized in the ablation study reported in
Table 2.
There we also report SR performance after
disabling the units prior (only using the units informed
RNN), disabling the RNN’s ability to be informed of lo-
cal units units constraints (only using the units prior and
a standard SR RNN), disabling both the units prior and
units information (only using a standard RNN which is
equivalent to the Petersen et al. 2019 setup), doing a
units guided random search by using a random number
generator in lieu of the RNN, and ﬁnally doing a purely
random search.
We show that merely constraining the choice of sym-
bols using the external units prior distribution scheme
(described in 3.2) is not enough to ensure perfect sym-
bolic recovery of physical laws, but that informing the
RNN of local units constraints (as described in 3.1) is
essential as it allows the RNN to actively learn units
rules. In addition, we show that our system does not
only rely on a mere brute force approach combined with
units constraints, but that the deep reinforcement learn-
ing setup described in 3.3 is an essential ingredient of the
success of Φ-SO.
It should be noted that in the NFW test case, sim-
ply expressing the inverse of a third-degree polynomial
is suﬃcient to solve the problem. However, using the
units prior without enabling the RNN to observe local
units constraints or utilizing the units prior in conjunc-
tion with a random number generator can result in a
lower recovery rate compared to the use of a standalone
random number generator. This is due to the highly re-
strictive nature of the units prior which in a simple case
like this can actually slow down the convergence toward
the solution.
Due to its very constraining nature, using a yet un-
trained RNN or a random number generator, our in situ
units prior often conﬂicts with the length prior which is
essential to avoid the expression generation phase going
on forever. This typically results in the majority of ex-
pressions being discarded due to this conﬂict during the
ﬁrst epochs of training. However, enabling the RNN to
learn on physically correct expressions, and enabling it
able to observe local units constraints, allows it to ac-

12
Tenachi et al.
Expression
# Trial expressions
Φ-SO ≡{Φ-prior, Φ-RNN}
{Φ-RNN}
{Φ-prior, RNN}
{RNN}
{Φ-prior, RNG}
{RNG}
E =
mc2
√
1−v2/c2
10M
100 %
0 %
60 %
0 %
20 %
0 %
Jr =
GM
√−2E −1
2

L + 1
2
√
L2 −4GMb

4M
100 %
0 %
80 %
0 %
60 %
0 %
ρ = ρ0/

r
Rs (1 +
r
Rs )2
2M
100 %
100 %
40 %
100 %
20 %
100 %
y = e−αt cos(ft + Φ)
1M
100 %
0 %
0 %
0 %
0 %
0 %
F = Gm1m2
r2
100K
100 %
80 %
100 %
20 %
80 %
0 %
H2(x ≡1 + z) = H0
2(Ωmx3 + (1 −Ωm))
100K
100 %
100 %
100 %
100 %
40 %
40 %
Table 2. Exact symbolic recovery rate summary and ablation study on our panel of astrophysical examples (input variables and
free parameters are colored in red and blue respectively). The acronymns are as follows. Φ-prior : physical units prior; Φ-RNN
: physical units informed RNN; RNG : random number generator. By studying the performance in combinations of ablations
of the in situ units prior, the RNN’s ability to be informed of local units constraints, and of the RNN itself (i.e., replaced by a
random number generator), we show that all three are essential ingredients of the success of our Φ-SO algorithm.
tively learn units rules as shown in Figure 5 which gives
the fraction of physical expressions successfully gener-
ated over iterations of learning.
Finally, we also illustrate the generalization capabili-
ties oﬀered by virtue of ﬁnding the exact analytical ex-
pression compared to a good approximation in Figure
6, where we show that such analytical expressions can
vastly outperform a multilayer perceptron (MLP) neural
network.
6. DISCUSSION
Since the Deep Symbolic Regression framework (Pe-
tersen et al. 2019) and most other SR methods work
by maximizing ﬁt quality, there are few constraints on
the arrangement of symbols. However, the paths in ﬁt
quality and the paths in symbol arrangement toward the
global minima (perfect ﬁt quality and perfect symbol ar-
rangement) are not necessarily correlated. This results
in the curse of accuracy guided SR, as small changes in
ﬁt quality can hide dramatic changes in functional form
and vice-versa. In essence, one can improve ﬁt quality
of candidates over learning iterations while getting fur-
ther away from the correct solution in symbolic arrange-
ment.
Therefore strong constraints on the functional
form, such as the one we are proposing in our setup, are
of great value for guiding SR algorithms in the context of
physics. This is an advantage that physics has and that
Φ-SO leverages by: (i) reducing the search space and (ii)
0M
5M
10M
# trial expressions
20
40
physical expressions (%)
Figure 5. RNN learning units rules. The black line shows
the evolution of the fraction of expressions that have bal-
anced units. As the training progresses, the algorithm learns
to respect the units constraints better. Here, we showcase
this trend on the relativistic energy of a particle test case
(averaged over multiple runs).
enabling the neural network to actively learn units rules
and leverage them to explore the space of solutions more
eﬃciently. Although the possibility of making a physical

Physical Symbolic Optimization
13
0
20
40
time [s]
0.5
0.0
0.5
amplitude [m]
Training points
Training range
Target function
Φ-SO expression
MLP
Figure 6. Example of the generalization capability of SR.
Here we show randomly drawn data points (black dots) from
a damped harmonic oscillator model (black line). The data
are well ﬁtted by an MLP (green line), which however fails in
regions beyond the range of the training data (vertical dotted
lines). In contrast, our SR algorithm Φ-SO (red dashed line)
manages to provide much more reliable extrapolation.
units prior was hinted by Petersen et al. (2021), to the
best of our knowledge such a framework was never built
before.
For example, we note that in Wadekar et al. (2022),
the relation is only in part generated by their genetic
SR algorithm which is used to capture “a ﬁrst order”
approximation and that it had to be tinkered with by
hand to make the expression physically meaningful and
more accurate.
We hope that our in situ units prior
which can also be paired with alternative approaches
will contribute to solving such issues.
The guidance oﬀered by the units constraints gives
Φ-SO an edge over other methods for ﬁnding the ex-
act symbolic solutions, improving performance from a
purely predictive standpoint. This makes Φ-SO a po-
tentially useful tool for opening up black-box physics
models such as neural networks ﬁtted on data of physi-
cal phenomena. In addition, we note that in the con-
text of physics, components of our Φ-SO framework
can not only be used to improve the performance of
algorithms built upon Petersen et al. (2019)’s frame-
work (DiPietro & Zhu 2022; Du et al. 2022), but can
also be used in tandem with other approaches. For in-
stance, our in situ units prior can be used to reduce
search space in the context of probabilistic or exhaus-
tive searches (Bartlett et al. 2022; Kammerer et al. 2020;
Brence et al. 2021; Jin et al. 2019), during the seeding
phases of genetic algorithms (Schmidt & Lipson 2009;
Cranmer 2020; Cranmer et al. 2020; Virgolin & Bosman
2022; Stephens 2015; Kommenda et al. 2020) or for mak-
ing a physically motivated dataset of expressions, which
in conjunction with enabling the RNN to be informed of
local units constraints, could improve the performance
of supervised end-to-end approaches (Kamienny et al.
2022; Vastl et al. 2022; Becker et al. 2022; Biggio et al.
2020).
Our approach is based on a deep reinforcement learn-
ing methodology, where the neural network is reinitial-
ized at the start of each SR task. It is therefore trained
independently for each speciﬁc problem, and so does
not beneﬁt from past experience nor is it pre-trained
on a dataset of well known physical functional forms.
One could argue that this makes our approach “unbi-
ased” akin to unsupervised learning setups and there-
fore well suited for discovering new physics (Karagiorgi
et al. 2022). However, this also intrinsically limits SR
capabilities as exploiting such prior knowledge is of great
value for resolving the curse of accuracy guided SR de-
scribed above.
One can exploit such prior knowledge
by formulating it as an in situ prior (Kim et al. 2021)
or by learning on it in a supervised manner using trans-
formers learning techniques (Kamienny et al. 2022; Vastl
et al. 2022). However, although state-of-the-art super-
vised SR methods, as of now, shine in providing accu-
rate approximations, they show poorer exact symbolic
expression recovery rates than other methods. Future
work attempting to incorporate supervised learning with
reinforcement learning as in Fan et al. (2022), so as to
map the relationship between data points and mathe-
matical expressions, may allow the recovery of expres-
sions of substantially greater complexity than those we
have explored in Section 4.
As we have shown in Section 4.3, it is straightforward
to improve our method by combining it with the power-
ful problem simpliﬁcation schemes devised in (Udrescu
& Tegmark 2020; Udrescu et al. 2020; Luo et al. 2017;
Tohme et al. 2022; Cranmer et al. 2020). The results of
the separability procedures implemented in the (Udrescu
et al. 2020) algorithm are conveniently recorded in sep-
arate dataﬁles, which makes it completely straightfor-
ward to use their approach as a pre-processing step for
Φ-SO. We expect that it will be relatively easy to repro-
duce their method inside our algorithm which should
lead to improved performances for Φ-SO.
It could be argued that we could have tackled the
physical units validity of expressions in SR by using the
Buckingham Π theorem (Buckingham 1914), with vari-

14
Tenachi et al.
ables and constants rendered dimensionless by means
of multiplicative operations amongst them (such an ap-
proach has recently been proposed by Matchev et al.
2022; Keren et al. 2023). However, working on so called
Π groups can make SR much more diﬃcult in practice
as it makes the dimensionless solutions often more com-
plex. It is interesting to note that nature (or at least
physics) is not dimensionless, so information is lost dur-
ing the process of making variables and constants dimen-
sionless. This would have prevented us from leveraging
the powerful constraints provided by the units.
7. CONCLUSIONS
We have presented a new symbolic regression algo-
rithm, built from the ground up to make use of the
highly restrictive constraint that we have in the physical
sciences that our equations must have balanced units.
The heart of the algorithm is an embedding that gen-
erates a sequence of mathematical symbols while cumu-
latively keeping track of their physical units. We adopt
the very successful deep reinforcement learning strategy
of Petersen et al. (2019), which we use to train our RNN
to not only produce accurate expressions but physically
sound ones by making it learn local units constraints.
The algorithm was applied to several test cases from
astrophysics. The ﬁrst was a simple search for the en-
ergy of a particle in Special Relativity (Section 4.1),
which our algorithm was easily able to ﬁnd, yet is a
problem that the standard Petersen et al. (2019) code
fails on. The second test case applied the algorithm to
the famous Hubble diagram of supernovae of type Ia.
While the form of the Hubble parameter H(z) in stan-
dard ΛCDM cosmology was indeed recovered, the algo-
rithm ﬁnds that other simpler solutions ﬁt the super-
nova data (in isolation) better. This result is consistent
with the ﬁndings of Bartlett et al. (2022). Another test
examined a relatively complicated function in galactic
dynamics, where we searched for the functional form of
the radial action coordinate in an isochrone stellar po-
tential model. Although our algorithm initially fails in
this test, we managed to recover the correct equation by
ﬁrst splitting the dataset using the additive separability
criterion as implemented by Udrescu & Tegmark (2020).
No algorithm other than Φ-SO was able to recover this
equation.
These tests have demonstrated the applicability of the
algorithm to model data of the real world as well as
to derive non-obvious analytic expressions for proper-
ties of perfect mathematical models of physical systems.
Although we realise that the physical laws potentially
discovered by our method will depend on data range,
choice of priors, etc, this is a step toward a full agnostic
method for connecting observational data to theory. Fu-
ture contributions in this research program will extend
the algorithm to allow for diﬀerential and integral opera-
tors, potentially permitting the solution of ordinary and
partial diﬀerential equations with physical units con-
straints. However, our primary goal will be to use the
new machinery to discover as yet unknown physical re-
lationships from the state-of-the-art large surveys that
the community has at its disposal.
CODE AVAILABILITY
The documented code for the Φ-SO algorithm along
with demonstration notebooks are available on GitHub
github.com/WassimTenachi/PhySO §.
ACKNOWLEDGMENTS
RI acknowledges funding from the European Research
Council (ERC) under the European Unions Horizon
2020 research and innovation programme (grant agree-
ment No. 834148).
REFERENCES
117th US Congress. 2022, Algorithmic Accountability Act.
https://www.congress.gov/bill/117th-congress/
house-bill/6580/
Adaptive Agent Team, Bauer, J., Baumli, K., et al. 2023,
arXiv e-prints, arXiv:2301.07608,
doi: 10.48550/arXiv.2301.07608
Alnuqaydan, A., Gleyzer, S., & Prosper, H. 2022, Machine
Learning: Science and Technology
Ar´echiga, N., Chen, F., Chen, Y.-Y., et al. 2021, arXiv
preprint arXiv:2112.04023
Arrieta, A. B., D´ıaz-Rodr´ıguez, N., Del Ser, J., et al. 2020,
Information fusion, 58, 82
Bartlett, D. J., Desmond, H., & Ferreira, P. G. 2022, arXiv
preprint arXiv:2211.11461
Becker, S., Klein, M., Neitz, A., Parascandolo, G., &
Kilbertus, N. 2022, arXiv preprint arXiv:2211.02830
Biggio, L., Bendinelli, T., Lucchi, A., & Parascandolo, G.
2020, in Learning Meets Combinatorial Algorithms at
NeurIPS2020
Binney, J., & Tremaine, S. 2011, Galactic dynamics, Vol. 13
(Princeton university press)
Brence, J., Todorovski, L., & Dˇzeroski, S. 2021,
Knowledge-Based Systems, 224, 107077
Buckingham, E. 1914, Physical review, 4, 345

Physical Symbolic Optimization
15
Carilli, C., & Rawlings, S. 2004, arXiv preprint
astro-ph/0409274
Cranmer, M. 2020, PySR: Fast & Parallelized Symbolic
Regression in Python/Julia, Zenodo,
doi: 10.5281/zenodo.4041459
Cranmer, M., Sanchez Gonzalez, A., Battaglia, P., et al.
2020, Advances in Neural Information Processing
Systems, 33, 17429
d’Ascoli, S., Kamienny, P.-A., Lample, G., & Charton, F.
2022, arXiv preprint arXiv:2201.04600
Delgado, A. M., Wadekar, D., Hadzhiyska, B., et al. 2022,
Monthly Notices of the Royal Astronomical Society, 515,
2733
Desmond, H., Bartlett, D. J., & Ferreira, P. G. 2023, arXiv
preprint arXiv:2301.04368
DiPietro, D. M., & Zhu, B. 2022, arXiv preprint
arXiv:2209.01521
Du, M., Chen, Y., & Zhang, D. 2022, arXiv preprint
arXiv:2210.02181
European Commission. 2021, The Artiﬁcial Intelligence
Act. https://artiﬁcialintelligenceact.eu/
Fan, L., Wang, G., Jiang, Y., et al. 2022, arXiv preprint
arXiv:2206.08853
Gaia Collaboration, Prusti, T., de Bruijne, J. H. J., et al.
2016, A&A, 595, A1, doi: 10.1051/0004-6361/201629272
Galilei, G. 1623, Il saggiatore
Graham, M. J., Djorgovski, S., Mahabal, A. A., Donalek,
C., & Drake, A. J. 2013, Monthly Notices of the Royal
Astronomical Society, 431, 2371
Hochreiter, S., & Schmidhuber, J. 1997, Neural
computation, 9, 1735
Ibata, R., Diakogiannis, F. I., Famaey, B., & Monari, G.
2021, ApJ, 915, 5, doi: 10.3847/1538-4357/abfda9
Ivezi´c, ˇZ., Kahn, S. M., Tyson, J. A., et al. 2019, ApJ, 873,
111, doi: 10.3847/1538-4357/ab042c
Jin, Y., Fu, W., Kang, J., Guo, J., & Guo, J. 2019, arXiv
preprint arXiv:1910.08892
Kamienny, P.-A., d’Ascoli, S., Lample, G., & Charton, F.
2022, arXiv preprint arXiv:2204.10532
Kammerer, L., Kronberger, G., Burlacu, B., et al. 2020, in
Genetic Programming Theory and Practice XVII
(Springer), 79–99
Karagiorgi, G., Kasieczka, G., Kravitz, S., Nachman, B., &
Shih, D. 2022, Nature Reviews Physics, 4, 399
Keren, L. S., Liberzon, A., & Lazebnik, T. 2023, Scientiﬁc
Reports, 13, 1249, doi: 10.1038/s41598-023-28328-2
Kim, J. T., Landajuela, M., & Petersen, B. K. 2021, arXiv
preprint arXiv:2104.05930
Kim, S., Lu, P. Y., Mukherjee, S., et al. 2020, IEEE
transactions on neural networks and learning systems, 32,
4166
Kingma, D. P., & Ba, J. 2014, arXiv preprint
arXiv:1412.6980
Kommenda, M., Burlacu, B., Kronberger, G., & Aﬀenzeller,
M. 2020, Genetic Programming and Evolvable Machines,
21, 471
La Cava, W., Orzechowski, P., Burlacu, B., et al. 2021,
arXiv preprint arXiv:2107.14351
Landajuela, M., Petersen, B. K., Kim, S. K., et al. 2021a,
arXiv preprint arXiv:2107.09158
—. 2021b, arXiv preprint arXiv:2107.09158
Laureijs, R., Amiaux, J., Arduini, S., et al. 2011, arXiv
e-prints, arXiv:1110.3193.
https://arxiv.org/abs/1110.3193
Lemos, P., Jeﬀrey, N., Cranmer, M., Ho, S., & Battaglia, P.
2022, arXiv preprint arXiv:2202.02306
Liu, Z., & Tegmark, M. 2021, Physical Review Letters, 126,
180604
Liu, Z., Wang, B., Meng, Q., et al. 2021, Physical Review
E, 104, 055302
LSST Science Collaboration, Abell, P. A., Allison, J., et al.
2009, arXiv e-prints, arXiv:0912.0201.
https://arxiv.org/abs/0912.0201
Lu, Z., Pu, H., Wang, F., Hu, Z., & Wang, L. 2017,
Advances in neural information processing systems, 30
Luo, C., Chen, C., & Jiang, Z. 2017, arXiv preprint
arXiv:1705.08061
Makke, N., & Chawla, S. 2022, arXiv preprint
arXiv:2211.10873
Martius, G., & Lampert, C. H. 2016, arXiv preprint
arXiv:1610.02995
Matchev, K. T., Matcheva, K., & Roman, A. 2022, The
Astrophysical Journal, 930, 33
Matsubara, Y., Chiba, N., Igarashi, R., & Ushiku, Y. 2022,
in NeurIPS 2022 AI for Science: Progress and Promises.
https://openreview.net/forum?id=oKwyEqClqkb
McConaghy, T. 2011, in Genetic Programming Theory and
Practice IX (Springer), 235–260
Meurer, A., Smith, C. P., Paprocki, M., et al. 2017, PeerJ
Computer Science, 3, e103
Murdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asl, R., &
Yu, B. 2019, Proceedings of the National Academy of
Sciences, 116, 22071
Navarro, J. F., Frenk, C. S., & White, S. D. M. 1996, ApJ,
462, 563, doi: 10.1086/177173
Panju, M., & Ghodsi, A. 2020, arXiv preprint
arXiv:2011.02415

16
Tenachi et al.
Paszke, A., Gross, S., Massa, F., et al. 2019, Advances in
neural information processing systems, 32
Petersen, B. K., Larma, M. L., Mundhenk, T. N., et al.
2019, arXiv preprint arXiv:1912.04871
Petersen, B. K., Santiago, C. P., & Landajuela, M. 2021,
arXiv preprint arXiv:2107.09182
Press, W. H., Teukolsky, S. A., Vetterling, W. T., &
Flannery, B. P. 2007, Numerical recipes 3rd edition: The
art of scientiﬁc computing (Cambridge university press)
Rajeswaran, A., Ghotra, S., Ravindran, B., & Levine, S.
2016, arXiv preprint arXiv:1610.01283
Reinbold, P. A., Kageorge, L. M., Schatz, M. F., &
Grigoriev, R. O. 2021, Nature communications, 12, 1
Sabbatini, F., & Calegari, R. 2022, arXiv preprint
arXiv:2211.00238
Sahoo, S., Lampert, C., & Martius, G. 2018, in
International Conference on Machine Learning, PMLR,
4442–4450
Schmidt, M., & Lipson, H. 2009, science, 324, 81
Scolnic, D. M., Jones, D. O., Rest, A., et al. 2018, ApJ,
859, 101, doi: 10.3847/1538-4357/aab9bb
Shao, H., Villaescusa-Navarro, F., Genel, S., et al. 2022,
The Astrophysical Journal, 927, 85
Stephens, T. 2015, GPLearn.
https://gplearn.readthedocs.io/en/stable/index.html
Sutton, R. S., & Barto, A. G. 2018, Reinforcement learning:
An introduction (MIT press)
Tohme, T., Liu, D., & Youcef-Toumi, K. 2022, arXiv
preprint arXiv:2205.15569
Udrescu, S.-M., Tan, A., Feng, J., et al. 2020, Advances in
Neural Information Processing Systems, 33, 4860
Udrescu, S.-M., & Tegmark, M. 2020, Science Advances, 6,
eaay2631
Usama, M., & Lee, I.-Y. 2022, Sensors, 22, 8240
Valle, C. M. C., & Haddadin, S. 2021, arXiv preprint
arXiv:2105.14396
Vastl, M., Kulh´anek, J., Kubal´ık, J., Derner, E., &
Babuˇska, R. 2022, arXiv preprint arXiv:2205.15764
Virgolin, M., & Bosman, P. A. 2022, arXiv preprint
arXiv:2204.12159
Virgolin, M., & Pissis, S. P. 2022, arXiv preprint
arXiv:2207.01018
Wadekar, D., Villaescusa-Navarro, F., Ho, S., &
Perreault-Levasseur, L. 2020, arXiv preprint
arXiv:2012.00111
Wadekar, D., Thiele, L., Villaescusa-Navarro, F., et al.
2022, arXiv preprint arXiv:2201.01305
Wilstrup, C., & Kasak, J. 2021, arXiv preprint
arXiv:2103.15147
Wolfram, S. 2003, The mathematica book, Vol. 1 (Wolfram
Research, Inc.)
Wong, K. W., & Cranmer, M. 2022, arXiv preprint
arXiv:2207.12409
Wu, T., & Tegmark, M. 2019, Physical Review E, 100,
033311
Zheng, W., Sharan, S., Fan, Z., et al. 2022, arXiv preprint
arXiv:2212.14849
Zhu, C., Byrd, R. H., Lu, P., & Nocedal, J. 1997, ACM
Transactions on mathematical software (TOMS), 23, 550

