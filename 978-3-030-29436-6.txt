Pascal Fontaine (Ed.)
 123
LNAI 11716
27th International Conference on Automated Deduction
Natal, Brazil, August 27–30, 2019
Proceedings
Automated Deduction – 
CADE 27

Lecture Notes in Artiﬁcial Intelligence
11716
Subseries of Lecture Notes in Computer Science
Series Editors
Randy Goebel
University of Alberta, Edmonton, Canada
Yuzuru Tanaka
Hokkaido University, Sapporo, Japan
Wolfgang Wahlster
DFKI and Saarland University, Saarbrücken, Germany
Founding Editor
Jörg Siekmann
DFKI and Saarland University, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/1244

Pascal Fontaine (Ed.)
Automated Deduction –
CADE 27
27th International Conference on Automated Deduction
Natal, Brazil, August 27–30, 2019
Proceedings
123

Editor
Pascal Fontaine
University of Lorraine
Villers-lès-Nancy, France
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Artiﬁcial Intelligence
ISBN 978-3-030-29435-9
ISBN 978-3-030-29436-6
(eBook)
https://doi.org/10.1007/978-3-030-29436-6
LNCS Sublibrary: SL7 – Artiﬁcial Intelligence
© Springer Nature Switzerland AG 2019
Chapters 20, 25 and 30 are licensed under the terms of the Creative Commons Attribution 4.0 International
License (http://creativecommons.org/licenses/by/4.0/). For further details see license information in the
chapters.
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, expressed or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This volume contains the proceedings of the 27th International Conference on
Automated Deduction (CADE 27). The conference was hosted by the Universidade
Federal do Rio Grande do Norte, in Natal, Brazil, during August 27–30, 2019. CADE is
the major forum for the presentation of research in all aspects of automated deduction,
including foundations, applications, implementations, and practical experience.
The Program Committee accepted 34 papers (27 full papers and 7 system
descriptions) out of 65 submissions (53 full papers and 12 system descriptions). Each
submission was reviewed by at least three Program Committee (PC) members or
external reviewers appointed by the PC members in charge. The main criteria for
evaluation were originality and signiﬁcance, technical quality and completeness,
comparison with related work and completeness of references, quality of presentation,
clarity, and readability. All papers containing experimental data were also evaluated
with respect to reproducibility.
The technical program of the conference included three invited talks:
– Cas Cremers (CISPA Helmholtz Center for Information Security, Saarbrücken,
Germany): “Automated Reasoning for Security Protocols”
– Assia Mahboubi (Inria, LS2N, Université de Nantes, France and Vrije Universiteit
Amsterdam, the Netherlands): “Computer Deduction and (Formal) Proofs in
Mathematics”
– Cesare Tinelli (Department of Computer Science, The University of Iowa, USA):
“From Counter-Model-based Quantiﬁer Instantiation to Quantiﬁer Elimination in
SMT”
During the conference, the Herbrand Award for Distinguished Contributions to
Automated Reasoning was presented to Nikolaj Bjørner and Leonardo de Moura in
recognition of their numerous and important contributions to SMT solving, including
its theory, implementation, and application to a wide range of academic and industrial
needs. The Selection Committee for the Herbrand Award consisted of Bruno Dutertre,
Juergen Giesl, Dale Miller (chair), and Larry Paulson.
The Thoralf Skolem Awards were conferred this year to reward CADE papers that
have passed the test of time by being most inﬂuential papers in the ﬁeld for 1979, 1990,
1999, and 2009. The authors receiving an award were:
– Peter Andrews for the paper entitled “General Matings,” published in the CADE 4
proceedings in 1979.
The paper is recognized for its invention of the generalized mating method for
constructing refutations of formulas in negative normal form. This development
paved the way for the subsequent construction of many non-resolution methods in
automated deduction, including the well-known connection method.

– Leo Bachmair and Harald Ganzinger for the paper entitled “On Restrictions of
Ordered Paramodulation with Simpliﬁcation,” published in the CADE 10
proceedings in 1990.
The paper is recognized for its development of the superposition calculus for
equational ﬁrst-order clauses alongside a new and powerful framework for proving
completeness and accommodating redundancy. This framework forms the basis of
many advanced modern theorem provers, and has been highly inﬂuential in
accelerating progress in the area of automated deduction.
– Christoph Weidenbach for the paper “Towards an Automated Analysis of Security
Protocols,” published in the CADE 16 proceedings in 1999.
The paper is recognized for two main contributions to automated deduction: ﬁrst,
its novel application of general theorem proving techniques to a key-exchange
security protocol; and, second, its development of new decidability and undecid-
ability results for fragments of monadic Horn theories.
– Rajeev Goré and Florian Widmann for the paper entitled “An Optimal On-the-Fly
Tableau-Based Decision Procedure for PDL-Satisﬁability,” published in the CADE
22 proceedings in 2009.
The paper is recognized for presenting the ﬁrst decision procedure for propo-
sitional dynamic logic which is both theoretically optimal and effective in practice.
Previous decision procedures are either suboptimal in the worst case, or worst-case
optimal but with poor average-case performance. The solution in this paper thereby
closed a problem that had been open for almost 30 years.
The conference issued a call for workshops out of which the following ﬁve
proposals were approved:
– Automated
Reasoning:
Challenges,
Applications,
Directions,
Exemplary
Achievements (ARCADE)
– Deduction Mentoring Workshop (DeMent 2019)
– Logical and Semantic Frameworks, with Applications (LSFA)
– Proof eXchange for Theorem Proving (PxTP)
– Theorem Prover Components for Educational Software (ThEdu 2019).
In addition, the conference included a two-day program of introductory tutorials.
The ﬁrst day was dedicated to tutorials given by local organizers, with an aim to
promote among local and foreign students the research on automated reasoning carried
out locally:
– Cláudia Nalon: “Machine Oriented Reasoning”
– Carlos Olarte: “Building Theorem Provers Using Rewriting Logic”
– Giselle Reis: “Intuitionistic Logic”
A one-day tutorial, titled “Build Your Own First-Order Prover,” was given by Jens
Otten on the second day.
During the conference, the CADE 27 ATP System Competition (CASC 27) was
held, organized by Geoff Sutcliffe. The description of the competition is available as an
abstract in these proceedings.
vi
Preface

I would like to thank the many people without whom the conference would not have
been possible. First, I would like to thank all authors who submitted papers, all
participants of the conference as well as the invited keynote speakers, the tutorial
speakers, and the workshop organizers for their contributions. I am very grateful to the
members of the PC and the external reviewers for carefully reviewing and selecting the
papers. In particular, I would like to thank Philipp Rümmer and Roberto Sebastiani
who acted as chairs for the papers I was conﬂicting with. Many thanks to Andrei
Voronkov for providing the EasyChair system which greatly facilitated the reviewing
process, the electronic PC meeting, and the preparation of the proceedings. I also thank
the Trustees of CADE Inc. for their advice and support. Special thanks go to Elaine
Pimentel, who as conference chair was involved in almost every aspect of the
organization of the conference, and the members of the local organization team, Carlos
Olarte, João Marcos, Cláudia Nalon, and Giselle Reis, for the tremendous effort they
devoted to the organization of the conference. I am extremely grateful to Geoff
Sutcliffe, for organizing CASC 27 and being the publicity chair, and to Giles Reger, the
workshop chair.
CADE 27 received support from many organizations. On behalf of all organizers,
I would like to thank the Universidade Federal do Rio Grande do Norte, DMAT,
ProEx, PPG, PROPESQ, the Universidade de Brasilia, CAPES, CNPq, CMU Qatar,
the Association for Symbolic Logic, IBM, Imandra, Microsoft, and Springer.
August 2019
Pascal Fontaine
Preface
vii

Organization
Program Committee
Carlos Areces
FaMAF, Universidad Nacional de Córdoba, Argentina
Franz Baader
TU Dresden, Germany
Clark Barrett
Stanford University, USA
Jasmin Christian Blanchette
Vrije Universiteit Amsterdam, The Netherlands
Maria Paola Bonacina
Università degli Studi di Verona, Italy
Leonardo de Moura
Microsoft, USA
Hans de Nivelle
School of Science and Technology,
Nazarbayev University, Kazakhstan
Clare Dixon
University of Liverpool, UK
Mnacho Echenim
University of Grenoble, France
Marcelo Finger
University of São Paulo, Brazil
Pascal Fontaine
University of Lorraine, CNRS, Inria, LORIA, France
Silvio Ghilardi
Dipartimento di Matematica, Università degli Studi
di Milano, Italy
Jürgen Giesl
RWTH Aachen University, Germany
Rajeev Gore
The Australian National University, Australia
Stefan Hetzl
Vienna University of Technology, Austria
Marijn Heule
The University of Texas at Austin, USA
Nao Hirokawa
JAIST, Japan
Moa Johansson
Chalmers University of Technology, Sweden
Cezary Kaliszyk
University of Innsbruck, Austria
Deepak Kapur
University of New Mexico, USA
Benjamin Kiesl
CISPA Helmholtz Center for Information Security,
Germany
Konstantin Korovin
The University of Manchester, UK
Laura Kovacs
Vienna University of Technology, Austria
Ramana Kumar
DeepMind, UK
Cláudia Nalon
University of Brasília, Brazil
Vivek Nigam
Federal University of Paraíba, Brazil
and Fortiss, Germany
Carlos Olarte
Universidade Federal do Rio Grande do Norte, Brazil
Jens Otten
University of Oslo, Norway
André Platzer
Carnegie Mellon University, USA
Andrew Reynolds
University of Iowa, USA
Philipp Rümmer
Uppsala University, Sweden
Renate A. Schmidt
The University of Manchester, UK
Stephan Schulz
DHBW Stuttgart, Germany
Roberto Sebastiani
University of Trento, Italy

Natarajan Shankar
SRI International, USA
Viorica
Sofronie-Stokkermans
University Koblenz-Landau, Germany
Martin Suda
Czech Technical University, Czech Republic
Geoff Sutcliffe
University of Miami, USA
René Thiemann
University of Innsbruck, Austria
Uwe Waldmann
Max Planck Institute for Informatics, Germany
Christoph Weidenbach
Max Planck Institute for Informatics, Germany
Sarah Winkler
University of Innsbruck, Austria
Conference Chair
Elaine Pimentel
Universidade Federal do Rio Grande do Norte, Brazil
Local Organization
Carlos Olarte
Universidade Federal do Rio Grande do Norte, Brazil
João Marcos
Universidade Federal do Rio Grande do Norte, Brazil
Cláudia Nalon
Universidade de Brasilia, Brazil
Giselle Reis
CMU, Qatar
Workshop Chair
Giles Reger
The University of Manchester, UK
Publicity Chair
Geoff Sutcliffe
University of Miami, USA
System Competition
Geoff Sutcliffe
University of Miami, USA
Additional Reviewers
Aoto, Takahito
Aravantinos, Vincent
Atig, Mohamed Faouzi
Avanzini, Martin
Backeman, Peter
Barbosa, Haniel
Benedikt, Michael
Bentkamp, Alexander
Berger, Gerald
Bottesch, Ralph
Brown, Chad
Cordwell, Katherine
Cruanes, Simon
Das, Anupam
Dawson, Jeremy
Ebner, Gabriel
El Ouraoui, Daniel
Fervari, Raul
Fiorentini, Camillo
Fiorino, Guido
Fleury, Mathias
Frohn, Florian
x
Organization

Fulton, Nathan
Gauthier, Thibault
Gianola, Alessandro
Graham-Lengrand, Stephane
Hark, Marcel
Haslbeck, Max W.
Hensel, Jera
Hladik, Jan
Hoffmann, Guillaume
Holden, Edvard
Hustadt, Ullrich
Immler, Fabian
Irfan, Ahmed
Kamburjan, Eduard
Kaminski, Mark
Kojima, Kensuke
Kop, Cynthia
Kutsia, Temur
Lammich, Peter
Liang, Chencheng
Libal, Tomer
Lisitsa, Alexei
Löding, Christof
Maric, Filip
Marshall, Andrew M.
Middeldorp, Aart
Moguillansky, Martin
Mover, Sergio
Nishida, Naoki
Noetzli, Andres
Peltier, Nicolas
Ponce-De-Leon, Hernan
Rawson, Michael
Riener, Martin
Ringeissen, Christophe
Robillard, Simon
Rocha, Camilo
Rowe, Reuben
Sakai, Masahiko
Schurr, Hans-Jörg
Smallbone, Nicholas
Sogokon, Andrew
Steigmiller, Andreas
Sternagel, Christian
Stoilos, Giorgos
Traytel, Dmitriy
van Gool, Sam
van Oostrom, Vincent
Verma, Rakesh
Vierling, Jannik
Voigt, Marco
Wernhard, Christoph
Zamansky, Anna
Zeljic, Aleksandar
Zohar, Yoni
Board of Trustees of CADE Inc.
Christoph Benzmüller
(Vice-president)
Freie Universität Berlin, Germany
and University of Luxembourg, Luxembourg
Jasmin Blanchette
Vrije Universiteit Amsterdam, The Netherlands
Pascal Fontaine (PC Chair)
University of Lorraine, CNRS, Inria, LORIA, France
Jürgen Giesl
RWTH Aachen University, Germany
Marijn Heule
The University of Texas at Austin, USA
Laura Kovács
Vienna University of Technology, Austria
Neil Murray (Treasurer)
University at Albany - State University of New York,
USA
Andrew Reynolds
University of Iowa, USA
Philipp Rümmer (Secretary)
Uppsala University, Sweden
Renate Schmidt
The University of Manchester, UK
Stephan Schulz
DHBW Stuttgart, Germany
Christoph Weidenbach
(President)
Max Planck Institute for Informatics, Germany
Organization
xi

Board of the Association for Automated Reasoning
Christoph Benzmüller
(CADE)
Freie Universität Berlin, Germany
and University of Luxembourg, Luxembourg
Uli Furbach
(Vice-president)
Universität Koblenz-Landau, Germany
Jürgen Giesl (CADE)
RWTH Aachen University, Germany
Philipp Rümmer (Secretary)
Uppsala University, Sweden
Sophie Tourret
(Newsletter Editor)
Max Planck Institute for Informatics, Germany
Larry Wos (President)
Argonne National Laboratory, USA
xii
Organization

Abstracts

Automated Reasoning for Security Protocols
Cas Cremers
CISPA Helmholtz Center for Information Security, Saarbrücken, Germany
Abstract. Security protocols are a prime example of seemingly simple algo-
rithms for which it would be highly desirable, and possibly feasible, to provide
formal proofs of their security. Yet despite several decades of active research,
this goal has remained elusive. In this talk we will revisit the security protocol
problem, why it is so crucial, and how forms of automated reasoning have
helped advance the state of the art, using the TLS 1.3 protocol as an example.
We highlight some of the many open general questions and how future
advancements in automated reasoning might help towards the ultimate goal of
deploying provably secure protocols.

Computer Deduction and (Formal) Proofs
in Mathematics
Assia Mahboubi
Inria, LS2N, Université de Nantes, Vrije Universiteit Amsterdam
Abstract. In 1976, K. Appel and W. Haken announced a computer-assisted
proof of the Four Color theorem, solving a long standing open question in graph
theory. Since, experimental mathematics have gained momentum and computers
have even changed the very nature of peer-reviewed mathematical proofs. This
phenomenon can be observed in a broad spectrum of ﬁelds, including number
theory, dynamical systems, combinatorics, etc.
A vast variety of software is available today for doing computer-aided
mathematics. Tools, and the algorithms they implement, are usually grouped in
two partially overlapping categories: the symbolic ones, typically computer
algebra systems, and the numerical ones. This talk is about a different ﬂavor of
software for doing mathematics with a computer: proof assistants. So far proof
assistants have been mostly used for research projects in computer science, often
related to program veriﬁcation. But proof assistants, and in particular those
based on dependent type theory, are receiving these days an increased attention
from users with a background in mathematics. Designing formal libraries about
contemporary mathematics raises speciﬁc issues and challenges for proof
assistants, that we will discuss.

From Counter-Model-Based Quantiﬁer
Instantiation to Quantiﬁer Elimination in SMT
Andrew Reynolds and Cesare Tinelli
The University of Iowa, Iowa City, USA
Abstract. Despite decades of research, reasoning efﬁciently about formulas
containing both quantiﬁers and built-in symbols for a given background theory
remains a challenge in automated deduction. Nevertheless, several exciting
advances have been made in the last few years, mainly in two directions:
(i) integrating theory reasoning in saturation-based calculi for ﬁrst-order logic
and (ii) integrating quantiﬁed reasoning into frameworks for ground Satisﬁa-
bility Modulo Theories (SMT). Focusing on the latter, this talk provides an
overview of a general, refutation-based approach for reasoning about quantiﬁed
formulas in SMT. The approach maintains a set S of ground formulas that is
incrementally expanded with selected instances of quantiﬁed input formulas,
with the selection based on counter-models of S. In addition to being quite
effective in practice, for several logical theories that admit quantiﬁer elimination
and have a decidable universal fragment this approach also leads to practically
efﬁcient decision procedures for the full theory. While the approach applies to
traditional theories with quantiﬁer elimination such as linear real and integer
arithmetic, this talk will present new promising developments for the theory of
ﬁxed-sized bit vectors and the theory of ﬂoating point arithmetic whose
full-fragments are notoriously difﬁcult to reason about.

The CADE-27 ATP System
Competition - CASC-27
Geoff Sutcliffe
University of Miami, Miami, USA
http://www.cs.miami.edu/~geoff
The CADE ATP System Competition (CASC) [4] is the annual evaluation of fully
automatic, classical logic Automated Theorem Proving (ATP) systems – the world
championship for such systems. One purpose of CASC is to provide a public evalu-
ation of the relative capabilities of ATP systems. Additionally, CASC aims to stimulate
ATP research, motivate development and implementation of robust ATP systems that
are useful and easily deployed in applications, provide an inspiring environment for
personal interaction between ATP researchers, and expose ATP systems within and
beyond the ATP community. Fulﬁllment of these objectives provides insight and
stimulus for the development of more powerful ATP systems, leading to increased and
more effective use. CASC-27 was held on 29th August 2019 in Natal, Brazil, as part
of the 27th International Conference on Automated Deduction (CADE-27). CASC-27
was the twenty-fourth competition in the CASC series; see [6] and citations therein
for information about individual previous competitions. The CASC-27 web site pro-
vides access to all competition resources: http://www.tptp.org/CASC/27. (Information
about previous competitions is available from http://www.tptp.org/CASC.)
CASC is divided into divisions according to problem and system characteristics.
The divisions reﬂect active areas of research and application of ATP. Each division
uses problems that have certain logical, language, and syntactic characteristics, so that
the systems that compete in the division are, in principle, able to attempt all the
problems in the division. Some divisions are further divided into problem categories
that make it possible to analyze, at a more ﬁne-grained level, which systems work well
for what types of problems. Table 1 catalogs the divisions and problem categories of
CASC-27.
Problems for the THF, TFA, FOF, FNT, UEQ, and EPR divisions are taken from
the TPTP Problem Library [5]. The TPTP version used for CASC is not released until
after the competition has started, so that new problems have not been seen by the
entrants. In order to ensure that no system receives an advantage or disadvantage due to
the speciﬁc presentation of the problems in the TPTP, the problems are obfuscated by
stripping out all comment lines, randomly reordering the formulae/clauses, randomly
swapping the arguments of associative connectives, randomly reversing implications,
and randomly reversing equalities. The problems have to meet certain criteria to be
eligible for selection: they may not be designed speciﬁcally to be suited or ill-suited to
some ATP system, calculus, or control strategy; they must be syntactically
non-propositional; generally they must have a TPTP difﬁculty rating in the range 0.21
to 0.99 (some exceptions are permitted). The problems used are randomly selected

from the eligible problems based on a seed supplied by the competition panel. The
selection is constrained so that no division or category contains an excessive number of
very similar problems, and is biased to select problems that are new in the TPTP
version used. The problems are given to the systems in TPTP format, with include
directives, and are given to the systems in increasing order of TPTP difﬁculty rating.
A CPU time limit is imposed for each problem.
The problems for the CASC-27 LTB division were taken from the HOL4 [2]
library. For CASC-27 multiple exports [1] of the library were used, so that multiple
versions of each problem were available – two FOF versions, two TF0 versions, one
TF1 version, two TH0 versions, and one TH1 version. Systems could attempt as many
of the versions as they want, in any order including in parallel, and a solution to any
version counted as a solution to the problem. The LTB problems are not obfuscated,
thus allowing the systems to take advantage of natural structure that occurs in the
The CADE-27 ATP System Competition - CASC-27
xix
Table 1. Divisions and problem categories
Div’n
Problems
Problem Categories
THF
Monomorphic Typed Higher-order
Form theorems (axioms with a
provable conjecture).
TNE – THF with No Equality
TEQ – THF with EQuality
TFA
Monomorphic Typed First-order form
theorems with Arithmetic (axioms
with a provable conjecture).
TFI – TFA with only Integer arithmetic
TFE – TFA with only rEal arithmetic
FOF
First-Order Form theorems (axioms
with a provable conjecture).
FNE – FOF with No Equality
FEQ – FOF with EQuality
FNT
FOF Non-Theorems (axioms with a
countersatisﬁable conjecture, and
satisﬁable axioms sets).
FNN – FNT with No equality
FNQ – FNT with eQuality
EPR
Effectively PRopositional theorems and
non-theorems in clause normal form
(unsatisﬁable and satisﬁable clause
sets). Effectively propositional means
that the problems are known to be
reducible to propositional form.
EPT – Effectively Propositional
Theorems (unsatisﬁable clause sets)
EPS – Effectively Propositional
non-theorems (Satisﬁable clause sets)
UEQ
Clause normal form non-propositional
Unit EQuality theorems (unsatisﬁable
clause sets).
LTB
Theorems (axioms with a provable
conjecture) from Large Theories,
presented in Batches. A large theory
typically has many functors and
predicates, and many axioms of
which only a few are required for the
proof of a theorem. The problems in
each batch all use a common core set
of axioms, and the problems in each
batch are given to the ATP systems all
at once.
HL4 – Problems exported from the
HOL4 library.
A set of training problems and their
solutions, taken from the same exports
as the competition problems, is
provided. The training data can be
used for system tuning during
(typically at the start of) the
competition.

problems. The batch presentation of the LTB division allows the systems to load and
process the common core set of axioms just once. The set of training problems and
solutions facilitates system tuning by learning from previous proofs. The batch pre-
sentation allows proofs and other control information to be used between proof sear-
ches for further incremental tuning. The problems are given to the systems in TPTP
format, with include directives, and the problems are given to the systems in the natural
order of their creation (i.e., for CASC-27, in their natural HOL4 order). Systems are
allowed to attempt the problems in any order, and to make multiple attempts on each
problem. An overall wall clock time limit is imposed for all proof attempts and system
tuning. No CPU time limits are imposed.
The THF, TFA, FOF, FNT, and UEQ, and LTB divisions are ranked according to
the number of problems solved with an acceptable proof/model output (where
“acceptable” includes criteria such as derivations starting from problem formulae and
ending at the conjecture/refutation, derivations documenting the input, output, and
inference rule of each inference step, derivation inference steps being reasonably
ﬁne-grained, and models documenting the domain, function maps, and predicate
maps). The EPR division is ranked according to the number of problems solved, but
not necessarily accompanied by a proof or model. Ties are broken according to the
average time taken over problems solved (CPU time or wall clock time, depending on
the type of limit in the division).
The competition was run on computers provided by the StarExec project [3].
References
1. Brown, C., Gauthier, T., Kaliszyk, C., Sutcliffe, G., Urban, J.: GRUNGE: a grand uniﬁed ATP
challenge. In: Fontaine, P. (ed.) CADE 2019. LNCS, vol. 11716, pp. 123–141. Springer,
Heidelberg (2019). https://doi.org/10.1007/978-3-030-29436-6_8
2. Slind, K., Norrish, M.: A brief overview of HOL4. In: Mohamed O.A., Muñoz C., Tahar S.
(eds.) TPHOLs 2008. LNCS, vol. 5170, pp. 28–32. Springer, Heidelberg (2008). https://doi.
org/10.1007/978-3-540-71067-7_6
3. Stump A., Sutcliffe G., Tinelli C.: StarExec: a cross-community infrastructure for logic
solving. In: Demri S., Kapur D., Weidenbach C. (eds.) IJCAR 2014. LNCS (LNAI), vol.
8562, pp. 367–373. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08587-6_28
4. Sutcliffe, G.: The CADE ATP system competition - CASC. AI Mag. 37(2), 99–101 (2016)
5. Sutcliffe, G.: The TPTP problem library and associated infrastructure. From CNF to TH0,
TPTP v6.4.0. J. Autom. Reason. 59(4), 483–502 (2017)
6. Sutcliffe, G.: The 9th IJCAR automated theorem proving system competition - CASC-29. AI
Commun. 31(6), 495–507 (2018)
xx
G. Sutcliffe

Contents
Unification Modulo Lists with Reverse Relation with Certain
Word Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Siva Anantharaman, Peter Hibbs, Paliath Narendran,
and Michael Rusinowitch
On the Width of Regular Classes of Finite Structures . . . . . . . . . . . . . . . . .
18
Alexsander Andrade de Melo and Mateus de Oliveira Oliveira
Extending SMT Solvers to Higher-Order Logic. . . . . . . . . . . . . . . . . . . . . .
35
Haniel Barbosa, Andrew Reynolds, Daniel El Ouraoui, Cesare Tinelli,
and Clark Barrett
Superposition with Lambdas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
Alexander Bentkamp, Jasmin Blanchette, Sophie Tourret,
Petar Vukmirović, and Uwe Waldmann
Restricted Combinatory Unification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
Ahmed Bhayat and Giles Reger
dLi: Deﬁnite Descriptions in Differential Dynamic Logic . . . . . . . . . . . . . . .
94
Rose Bohrer, Manuel Fernández, and André Platzer
SPASS-SATT: A CDCL(LA) Solver . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
Martin Bromberger, Mathias Fleury, Simon Schwarz,
and Christoph Weidenbach
GRUNGE: A Grand Unified ATP Challenge . . . . . . . . . . . . . . . . . . . . . . .
123
Chad E. Brown, Thibault Gauthier, Cezary Kaliszyk, Geoff Sutcliffe,
and Josef Urban
Model Completeness, Covers and Superposition . . . . . . . . . . . . . . . . . . . . .
142
Diego Calvanese, Silvio Ghilardi, Alessandro Gianola, Marco Montali,
and Andrey Rivkin
A Tableaux Calculus for Default Intuitionistic Logic . . . . . . . . . . . . . . . . . .
161
Valentin Cassano, Raul Fervari, Guillaume Hoffmann, Carlos Areces,
and Pablo F. Castro
NIL: Learning Nonlinear Interpolants . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
Mingshuai Chen, Jian Wang, Jie An, Bohua Zhan, Deepak Kapur,
and Naijun Zhan

ENIGMA-NG: Efficient Neural and Gradient-Boosted Inference
Guidance for E . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
Karel Chvalovský, Jan Jakubův, Martin Suda, and Josef Urban
Towards Physical Hybrid Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
Katherine Cordwell and André Platzer
SCL Clause Learning from Simple Models. . . . . . . . . . . . . . . . . . . . . . . . .
233
Alberto Fiori and Christoph Weidenbach
Names Are Not Just Sound and Smoke: Word Embeddings
for Axiom Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
250
Ulrich Furbach, Teresa Krämer, and Claudia Schon
Computing Expected Runtimes for Constant Probability Programs . . . . . . . .
269
Jürgen Giesl, Peter Giesl, and Marcel Hark
Automatic Generation of Logical Models with AGES . . . . . . . . . . . . . . . . .
287
Raúl Gutiérrez and Salvador Lucas
Automata Terms in a Lazy WSkS Decision Procedure . . . . . . . . . . . . . . . . .
300
Vojtěch Havlena, Lukáš Holík, Ondřej Lengál, and Tomáš Vojnar
Confluence by Critical Pair Analysis Revisited . . . . . . . . . . . . . . . . . . . . . .
319
Nao Hirokawa, Julian Nagele, Vincent van Oostrom,
and Michio Oyamaguchi
Composing Proof Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
337
Christina Kohl and Aart Middeldorp
Combining ProVerif and Automated Theorem Provers for Security
Protocol Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354
Di Long Li and Alwen Tiu
Towards Bit-Width-Independent Proofs in SMT Solvers . . . . . . . . . . . . . . .
366
Aina Niemetz, Mathias Preiner, Andrew Reynolds, Yoni Zohar,
Clark Barrett, and Cesare Tinelli
On Invariant Synthesis for Parametric Systems . . . . . . . . . . . . . . . . . . . . . .
385
Dennis Peuter and Viorica Sofronie-Stokkermans
The Aspect Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
406
David A. Plaisted
Uniform Substitution at One Fell Swoop . . . . . . . . . . . . . . . . . . . . . . . . . .
425
André Platzer
xxii
Contents

A Formally Verified Abstract Account of Gödel’s
Incompleteness Theorems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
442
Andrei Popescu and Dmitriy Traytel
Old or Heavy? Decaying Gracefully with Age/Weight Shapes . . . . . . . . . . .
462
Michael Rawson and Giles Reger
Induction in Saturation-Based Proof Search . . . . . . . . . . . . . . . . . . . . . . . .
477
Giles Reger and Andrei Voronkov
Faster, Higher, Stronger: E 2.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
495
Stephan Schulz, Simon Cruanes, and Petar Vukmirović
Certified Equational Reasoning via Ordered Completion . . . . . . . . . . . . . . .
508
Christian Sternagel and Sarah Winkler
JGXYZ: An ATP System for Gap and Glut Logics . . . . . . . . . . . . . . . . . . .
526
Geoff Sutcliffe and Francis Jeffry Pelletier
GKC: A Reasoning System for Large Knowledge Bases . . . . . . . . . . . . . . .
538
Tanel Tammet
Optimization Modulo the Theory of Floating-Point Numbers . . . . . . . . . . . .
550
Patrick Trentin and Roberto Sebastiani
FAME(Q): An Automated Tool for Forgetting in Description Logics
with Qualified Number Restrictions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
568
Yizheng Zhao and Renate A. Schmidt
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
581
Contents
xxiii

Uniﬁcation Modulo Lists with Reverse Relation
with Certain Word Equations
Siva Anantharaman1(B), Peter Hibbs2,4P, Paliath Narendran2,
and Michael Rusinowitch3
1 LIFO - Universit´e d’Orl´eans, Orl´eans, France
siva@univ-orleans.fr
2 University at Albany–SUNY, Albany, USA
peter.s.hibbs@gmail.com, pnarendran@albany.edu
3 Loria-Inria Universit´e de Lorraine, Nancy, France
rusi@loria.fr
4 Google Inc., Mountain View, USA
Abstract. Decision procedures for various list theories have been investigated in
the literature with applications to automated veriﬁcation. Here we show that the
uniﬁability problem for some list theories with a reverse operator is NP-complete.
We also give a uniﬁability algorithm for the case where the theories are extended
with a length operator on lists.
1
Introduction
Reasoning about data types such as lists and arrays is an important research area with
many applications, such as formal program veriﬁcation [13,19]. Early work on this [10]
focused on proving inductive properties. Important outcomes of this work include sat-
isﬁability modulo theories (SMT), starting with the pioneering work of Nelson and
Oppen [21] and of Shostak [25]. (See [3] for a more recent syntactic, inference-rule
based approach to developing SMT algorithms for lists and arrays.)
In this paper, we investigate the uniﬁcation problem modulo two simple equational
theories for lists. The constructors we shall use are the usual ‘nil’ and ‘cons’. We only
consider nil-terminated lists, or equivalently, only ﬁnite lists that are proper in the sense
of LISP. (All our lists can actually be visualized as ﬂat-lists in the sense of LISP.) We
ﬁrst examine lists with right cons (rcons) as the only operator (observer), and propose
an algorithm for the uniﬁcation problem modulo this theory (Sect. 2). We then consider
the theory extended with a second operator reverse (named rev) and develop an algo-
rithm to solve the uniﬁcation problem over rev (Sect. 3). In both cases, the algorithm is
based on a suitable reduction of the uniﬁcation problem to solving equations on ﬁnite
words over a ﬁnite alphabet, where every equation of the problem has at most one word
variable on either side. Further reductions will then lead us to the case where the equa-
tions will be ‘independent,’ and each equation will involve a single word variable; they
can be solved by the techniques presented in [8]. All of this can be done in NP with
respect to the lengths of the equations of the initial problem. In Sect. 4 we show how
the considerations of length of words can be built into the uniﬁcation algorithms for the
c⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 1–17, 2019.
https://doi.org/10.1007/978-3-030-29436-6_1

2
S. Anantharaman et al.
theories rcons and reverse. These could be of use in formal techniques based on word
constraints (e.g., [2,11,16,17]) or in constraint programming [7]. Several examples are
given in Sect. 5, to illustrate how the method we have developed in this paper operates.
Related Work. Motivated by constraint logic programming [7], some existential theo-
ries of list concatenation have been investigated in [26]. But these works do not consider
any list reverse operator. With a view to derive NP decision procedures we reduce our
uniﬁcation problems, on lists with a reverse operator but without concatenation, to sys-
tems of word equations that are special case of quadratic word equations. It is stated
in [24] that solving systems of quadratic word equations is in NP if a simple expo-
nential bound can be obtained on their shortest solution; however, to our knowledge
this simple exponential bound has not yet been proved. In [11] it is shown that if word
equations can be converted to a solved form, then satisﬁability of word equations with
length constraints is decidable. Satisﬁability of quadratic regular-oriented word equa-
tions with length constraints is shown decidable in [11]. Again these results do not
consider a reverse operator.
2
List Theory with rcons
The reader is assumed to be familiar with the concepts and notation used in [4]. For
terminology and a more in-depth treatment of uniﬁcation the reader is referred to [6].
The signature underlying our study below, will be 2-sorted with two disjoint types:
element and list. We assume there are ﬁnitely many constants (at least 2) of type ele-
ment, while nil will be the unique constant of type list. The uniﬁcation problems we
consider are instances of uniﬁcation with constants in the terminology of [6].
For better comprehension, we shall use in general the lower-case letters
x,y,z,u,v,... for the variables to which are assigned terms of type element, and the
upper-case letters X,Y,Z,U,V,..., for the variables to which are assigned terms of
type list; possibly with sufﬁxes or indices, in both cases.
We introduce now the equational axioms of List theory with rcons:
rcons(nil,x) ≈cons(x,nil)
rcons(cons(x,Y),z) ≈cons(x,rcons(Y,z))
where nil and cons are constructors; and cons, rcons are typed respectively as:
cons : element×list →list
rcons : list×element →list
We refer to this equational theory as RCONS. Orienting these from left to right pro-
duces a convergent system:
(1)
rcons(nil,x) →cons(x,nil)
(2) rcons(cons(x,y),z) →cons(x,rcons(y,z))
The following result helps simplifying equations in RCONS:
Lemma 1. Let s1, s2, t1, t2 be terms such that rcons(s1, t1) ≈RCONS rcons(s2, t2).
Then
s1 ≈RCONS s2
and
t1 ≈RCONS t2.

Uniﬁcation Modulo Lists with Reverse Relation with Certain Word Equations
3
2.1
Uniﬁability Complexity Analysis
Theorem 1. Uniﬁability modulo RCONS is NP-hard.
Proof. We will show this by reduction from 1-in-3-SAT. Given an instance of 1-in-3-
SAT, we will construct a uniﬁcation problem in our theory such that a uniﬁer exists
if and only if the instance of 1-in-3-SAT is satisﬁable. The set of equations thus con-
structed will be referred to as S.
For each clauseCi = (ai∨bi∨ci) in the instance of 1-in-3-SAT, we add the following
equation into S:
Si : cons(0,cons(0,cons(1,Li))) ≈? rcons(rcons(rcons(Li,xi),yi),zi)
where 0 and 1 are constants. Note that this equation has the following three solutions:
1. Li →nil, xi →0, yi →0, zi →1
2. Li →cons(0,nil), xi →0, yi →1, zi →0
3. Li →cons(0,cons(0,nil)), xi →1, yi →0, zi →0.
it also has the following solution:
Li →cons(0,cons(0,cons(1,rcons(rcons(rcons(Mi,xi),yi),zi))))
but if we substitute this solution back into equation Si and apply a series of decomposi-
tions, this gives us the following equation:
cons(0,cons(0,cons(1,Mi))) ≈? rcons(rcons(rcons(Mi,xi),yi),zi)
Therefore, clearly {Mi,xi,yi,zi} has the same solution set as {Li,xi,yi,zi} and
must ultimately terminate in a solution of type 1, 2, or 3. If it does not termi-
nate, then the uniﬁer for Li must be inﬁnitely large and is thus not a valid unify-
ing assignment. We associate the solutions of type 1, 2, and 3 with the truth assign-
ments {ai = false, bi = false, ci = true}, {ai = false, bi = true, ci = false}, and
{ai = true, bi = false, ci = false} respectively. Thus, if the constructed uniﬁcation
problem has a set of ﬁnite uniﬁers for these variables, then the original 1-in-3-SAT
problem has a solution (which is given by the previous associations.) Similarly, if there
is some satisfying assignment of the 1-in-3-SAT, then a set of ﬁnite uniﬁers for the
uniﬁcation problem can be constructed from that assignment by running the previous
associations backward.
⊓⊔
To show that the problem is in NP, we ﬁrst consider the set of variables of type
element in the problem, and guess equivalence classes in this set. We then select one
representative element from each equivalence class, and replace all instances of the
other variables in that class with the chosen representative; whenever possible, choose
a constant as representative. Clearly, no equivalence class may contain more than one
constant. For every representative x of type element that is not a constant, introduce a
fresh (symbolic) constant cx to act as the representative of its class. This guessing step is
clearly in NP. (If a uniﬁer is found involving cx, then all instances of cx may be replaced
by x once again.)

4
S. Anantharaman et al.
Once this guessing step is done, all equations of the given uniﬁcation problem will
be of the following form (after RCONS-normalization if necessary):
cons(a1,...(cons(ak,rcons(rcons(...rcons(X,bl)...,b1)))))
≈? cons(c1,...(cons(cm,rcons(rcons(...rcons(Y,dn),...,d1)))))
with X and Y not necessarily distinct. We will represent the sequences {ai}, {bi}, {ci},
{di} as ﬁnite words α, β, γ, δ respectively, over the constants. Such an equation can
then be expressed as a word equation as follows:
αXβ ≈? γYδ
Clearly, this equation does not have a solution unless either α is a preﬁx of γ or vice-
versus. Without loss of generality, let α be a preﬁx of γ and let α−1γ denote the sufﬁx of
γ after α is removed. The equation may be simpliﬁed to the following: Xβ ≈? α−1γYδ.
Similarly, either β or δ is a sufﬁx of the other; there are two cases:
β is a sufﬁx of δ. Let δβ −1 denote the remaining preﬁx of δ.
The equation is then simpliﬁed to X ≈? α−1γYδβ −1
δ is a sufﬁx of β. Let βδ −1 denote the remaining preﬁx of β.
The equation is thus simpliﬁed to Xβδ −1 ≈? α−1γY
A word equation αXβ ≈? γYδ, is said to be pruned, if all common (non-empty)
preﬁxes and sufﬁxes from the two sides of the equation have been removed. If this
cannot be done, the equation is unsolvable. Every pruned 1-variable equation is either
of the form αX ≈? Xβ, or of the form X ≈? γ , and every pruned 2-variable equation is
either of the form X ≈? αYβ, or of the form αX ≈? Yβ, for words α, β,γ. Equations
of the form X ≈? γ or of the form X ≈? αYβ are said to be in solved form. Those of
the other types are said to be unsolved.
In the following subsection, we present a nondeterministic algorithm to solve any
set of such equations, on ﬁnite words over a ﬁnite alphabet, each equation involving at
most two variables, one on either side, appearing at most once. Such a set of equations
will be said to be a simple system, or a simple set, of word equations. The following
notions will be useful for presenting and analyzing our algorithm.
Deﬁnition 1. Let U be a simple set of word equations.
(i) The relation graph GU of U is the undirected graph G = (V , E ) where the set of
vertices V is the set of variables in U and the set of edges E contains (X, Y) iff
there is an equation of the form αXβ ≈? γYδ in U.
(ii) For any two variables X,Y in U, the variable Y is said to be dependent on X iff the
graph GU has an edge deﬁned by an equation of the form Y ≈? αXβ, with α or
β (or both) non-empty; such a dependency is denoted as Y ≻U X, or as X ≺U Y.
(iii) The graph GU is said to present a dependency cycle from a variable Y in U, iff for
some variables X1,X2,...Xp in U, we have: Y ≻U X1 ≻U ··· ≻U Xp ≻U Y.
Given a dependency relation Y ≈? αXβ on the variables X,Y in U, the variable Y
is said to be the ‘lhs’ (left-hand-side) of this dependency; the edge on GU between Y
and X is called a directed dependency edge from Y to X. (By deﬁnition, at least one of
α,β is supposed to be non-empty.) A dependency path from a node V to a node W is a
sequence of dependency edges on GU, from V to W.

Uniﬁcation Modulo Lists with Reverse Relation with Certain Word Equations
5
2.2
NP-Solvability of Simple Sets: Algorithm A
Algorithm A presented below is nondeterministic. We will show that, for any run of
Algorithm A (successful or not) on any given simple set U of word equations, the total
number of steps is polynomial w.r.t. inputs. Moreover the equations generated in a run
will be shown to have polynomial size w.r.t. inputs (Sect. 2.3). Consequently, Algorithm
A will produce, when successful on U, a system containing only a polynomial number
of dependencies and 1-variable equations, each of them of polynomial size. By applying
to this resulting system of 1-variable equations, (Lemma 3 followed by) a polynomial
solvability check from [8], we will deduce that solvability of simple systems of word
equations is in NP.
Under the runs of Algorithm A, dependencies chosen in Step 2 get marked; we
assume that, initially, none of the dependencies in the given set U is marked.
Step 1. (Pruning) For each equation in U of the form αXβ ≈? γYδ, remove all
common preﬁxes and sufﬁxes from the two sides of that equation.
(i) If the two sides of some equation have non-common preﬁxes or sufﬁxes, then
EXIT with failure.
(ii) If for some variable X in U, there is a dependency cycle at X in the graph GU,
then EXIT with failure.
Step 2. Choose an unmarked dependency X ≈? αYβ in U; replace all instances of X
in all the other equations by αYβ; mark the chosen dependency. GOTO Step 1.
Step 3.a. Select an arbitrary equation such that the variables on the right and left
hand sides of the equation are distinct. If no such equation is available, EXIT.
Step 3.b. Let the selected equation be of the form αX ≈? Yβ.
Guess a word u in Prefixes(α),
(i) If α = uv and β = vw, with v ̸= λ, then replace the selected equation on X,Y
by the two equations {X ≈? w, Y ≈? u} and propagate this substitution through
GU; GOTO Step 1.
(ii) (Splitting) Otherwise, let Z be a fresh variable; and replace the selected equation
on X,Y by the two solved forms: X ≈? Zβ and Y ≈? αZ; GOTO Step 1.
Proposition 1. Let U be a simple set of word equations. The number of steps needed
for Algorithm A to halt is bounded by 5n where n is the initial number of variables in
the given problem U.
Proof. Let d be the number of unsolved variables (i.e., that are not the lhs of an equa-
tion). Initially d ≤n where n is the initial number of variables inU. Let d(k) be the value
of d when we enter for the kth time in Step 3. Since Step 3 generates one fresh variable
and two solved variables (that were not solved at previous steps: otherwise they would
have been replaced at Step 2) we have d(k + 1) < d(k). Therefore Step 3 is applied at
most n times. Hence the number of fresh variables generated (under Splitting) is at most
n, and the maximum number of variables at any stage is at most 2n. Therefore Step 2
can be applied at most 2n times, and the same holds also for Step 1.
⊓⊔
When a fresh variable ‘Z’ is introduced in Step 3.b. (ii) the graph GU will be dynam-
ically extended by the addition of a fresh node labelled with the variable Z; we also

6
S. Anantharaman et al.
introduce two dependency edges from the nodes X and Y to the node Z, corresponding
respectively to the two solved forms X ≈? Zβ, and Y ≈? αZ. Similarly, each time an
equation derived under this step turns out (after Pruning) to be a solved form, a depen-
dency edge will be added on the extended graph, between the corresponding nodes.
We note that when Algorithm A halts (without failure) on a given problem, we
will be left with a set of equations each being either in solved form, or a simple 1-
variable equation. Note also that the variables that are lhs of solved forms have a unique
occurrence. Hence the resulting system is solvable iff each subsystem of 1-variable
equations, on a given variable, is solvable.
We prove in Lemma 3 below, that every subsystem of the resulting 1-variable equa-
tions, on a given variable, can be replaced by a single equation (that may not be nec-
essarily simple) on that variable, at polynomial cost; each such 1-variable equation can
be checked for solvability, by a known polynomial algorithm from [8]. Prior to that we
need to show that, when A halts without failure on any problem U, the length of any
resulting simple 1-variable equation is polynomially bounded, w.r.t. the size of U. In
the following section we shall actually show more.
2.3
Lengths of Preﬁxes/Sufﬁxes of Equations Are Polynomially Bounded
Note that in Steps 2 and 3.b of Algorithm A, when a dependency X = μ is selected, then
every other equation e containing at least one occurrence of X is replaced by e[X ←μ]
and immediately simpliﬁed by Pruning (Step 1). After these operations the resulting
equation e′ replaces e.
Suppose now, that a derived equation e′ replaces an equation e under the propagation
of a dependency (and after Pruning); let α,β denote respectively the preﬁx and sufﬁx of
the equation e, and α′,β ′ those of e′. The replacing equation e′ is said to be in ‘excess-
size’ w.r.t. the equation it replaces, iff |α′| > |α|, or |β ′| > |β|, or both.
It is easy to see that the propagation of solved forms of the type Y ≈? X, or of the
type Y ≈? γ, cannot lead to replacing equations in excess-size. We can also check that,
in any run of A, a 1-variable equation is never replaced by an equation in excess-size;
this follows from a simple case analysis (cf. [1], Appendix A). It can also be checked
(cf. loc. cit), that the cases of Steps 2 and 3.b of A that can lead to replacing equations
possibly in excess-size, are as follows:
– a 2-variable equation in excess-size can get derived, when a dependency is applied
to the lhs (or the rhs) of a 2-variable equation.
– a 1-variable equation in excess-size can get derived, when a dependency is propa-
gated onto a 2-variable equation on the same two variables.
– a solved form equation in excess-size can get derived when a dependency is propa-
gated onto a solved form for the same variable, or on a 2-variable equation.
We already know that Algorithm A halts in polynomially many steps w.r.t. the num-
ber of variables n of the given problem, and that the number of equations in U when A
halts, is also polynomially bounded w.r.t. n (each step generates at most one equation).
We show now, that in the equations derived under A, even when they are in excess-size,
the lengths of the preﬁxes/sufﬁxes remain polynomially bounded w.r.t. U.

Uniﬁcation Modulo Lists with Reverse Relation with Certain Word Equations
7
Let us consider a 2-variable equation that gets derived under A: for instance, the
2-variable equation α1Y ≈? Wβ1, on which is propagated the dependency Y ≈? αXβ.
The 2-variable equation will be replaced (after Pruning) by a 2-variable equation of the
form α′
1X ≈? Wβ ′
1, where: |α′
1 = α1 α| ≤|α1| + |α|, and |β ′
1| ≤|β1|. To the variable
X, brought in by the substitution in the equation derived α′
1X ≈? Wβ ′
1, we attach the
singleton sequence [Y ≻X], and refer to it as the ‘preﬁx-tag’ (or ‘ptag’) of X in this
equation. (Remember: by deﬁnition, either the preﬁx or the sufﬁx of a dependency must
be non-empty.) This ptag is to be seen as a tag, to notify that the unique dependency
with Y as lhs, has served in the derivation of this fresh equation.
The replacing equation (in the example) will be in excess-size, iff α is non-empty.
In the preﬁx α1 α of X in the equation, α1 is contributed by the 2-variable equation
α1Y ≈? Wβ1, and α is contributed by the dependency Y ≈? αXβ that is applied to
that 2-variable equation. In other words, if the equation derived is in excess-size, the
ptag also carries the information that the excess in the length of the preﬁx, is due to a
portion contributed by the preﬁx of the dependency.
The ptag sequences grow incrementally, when a fresh equation derived gets replaced
in turn, under a subsequent step of A, by a new fresh equation. For instance, suppose on
the same example, that we have a second dependency of the form X ≈? θVδ. The fresh
(replacing) 2-variable equation derived would then be of the form: α′
1θ V ≈? Wβ ′
1. The
ptag ofV (the variable brought in) in this equation (not necessarily in excess-size) would
then be, by deﬁnition, the sequence [Y ≻X,X ≻V].
Sufﬁx-tags (‘stags’) are deﬁned analogously: on the same example above, suppose
for instance that we have a dependency W ≈? τ Z η. We would then derive an equation
of the form α′′V ≈? Zβ ′′; the ptag of V in this equation would still be [Y ≻X,X ≻V],
while the stag of Z (the variable brought in by the substitution) would be [W ≻Z].
The ptags and stags can be deﬁned, in formal terms, (recursively,) as follows:
Deﬁnition 2. (i) For any variable in an equation of the given problem U, the ptag
attached, w.r.t. that equation, is set to be the empty sequence [].
(ii) Suppose that, under some step of the algorithm A:
– a dependency of the form Y ≈? αXβ is propagated onto an equation of the form
γ1Y ≈? Wδ1 (resp. of the form X ≈? γ1 Z δ1);
– that the variable Y in γ1Y ≈? Wδ1 (resp. Z in X ≈? γ1 Z δ1) has an attached
ptag of the form [c], where c is a (possibly empty) ﬁnite sequence of dependency
relations on the graph GU;
– and that from the propagation of the dependency (after Pruning), we derive an
equation of the form γ′
1X ≈? W δ ′
1 (resp. of the form Y ≈? γ′
1 Z δ ′
1).
Then, the ptag attached to the variable X (resp. the variable Z), brought in by the
substitution in the replacing equation, is set to be [c,Y ≻X]
(iii) stags are deﬁned analogously.
Note that the ptags/stags deﬁne a dependency chain of the form Y ≻X ≻V ≻W ...,
on the variables of equations that get derived, under the Steps 2 and 3.b of A.

8
S. Anantharaman et al.
Lemma 2.
Assume that algorithm A halts without failure on a given problem U. Then,
no variable can appear more the once in the dependency chain deﬁned by the ptag, or
stag, of any equation derived under the runs of A on U.
Proof. If we assume the contrary, then we get a dependency cycle on the (extended)
relation graph of U; but then A would have exited with failure on U.
⊓⊔
Corollary 1. Assume that algorithm A halts without failure, on a given problem U.
Then the length of the preﬁx of any resulting equation is polynomially bounded, w.r.t.
N s, where N is the total number of equations in U, and s is the maximum size of the
preﬁxes or sufﬁxes of the equations in U.
Proof. By the above Lemma, the number of dependency relations in any ptag or stag
is at most the number N1 of dependencies, initial or derived under the runs of A; and
we also know that N1 is polynomial on N. On the other hand, the maximal (or ‘worst’)
growth in the preﬁx size of any derived equation e, when A halts, would be when each
dependency relation in its ptag sequence corresponds to a derived equation in excess-
size. But, as observed above, this means, that the preﬁx α (or sufﬁx β) of a dependency
of the form Y ≈? αXβ whose propagation led to the derivation of the equation e, has
contributed to the excess-size in the preﬁx (or sufﬁx) of the variable X in e. On the
other hand, we know that the length of the preﬁx (or sufﬁx) of any dependency in the
problem, initial or derived under Steps 2 and 3.b on a ﬁrst run of A, is polynomial on
N s (cf. [1], Appendix A); an inductive argument, on the number of steps of A before
it halts, proves that the same bound holds also for all derivations under the subsequent
runs of A. That proves the corollary.
⊓⊔
Note however, that when A halts without failure on a given problem, the resulting 1-
variable equations may not be all independent. So, to be able to apply [8] and conclude,
it remains now to replace every subsystem formed of the resulting simple 1-variable
equations on the same variable, by an equivalent single equation (which may not be
simple) on that variable, but of polynomial size w.r.t. the size of U. This is the objective
of our next lemma:
Lemma 3. Any system S of 1-variable equations, of size m, on a given variable X, is
equivalent to a single 1-variable equation of size p(m) for some ﬁxed polynomial p
(where X can appear more than once on either side).
Proof. We ﬁrst recall the well-known ‘trick’ (see [15]) to build such a single equation
from two equations:

u = v
u′ = v′ ≡
uau′ubu′ = vav′vbv′
where a,b are two distinct constants. The resulting equation is of size 2|S| + 4. Since
the initial system S is of size |S| ≥4, we deduce that the resulting single equation has
size ≤3|S|. To iterate the process on a system W of n equations (indexed from 1 to n)
we consider an integer k such that k −1 ≤logn < k; by adding to the system 2k −n
trivial equations X = X, we get an extended system V = (Vi) with equations indexed

Uniﬁcation Modulo Lists with Reverse Relation with Certain Word Equations
9
from 1 to 2k. We shall show by induction, that V is equivalent to a single equation of
size ≤3k|V|.
Assume (as inductive hypothesis) that we have derived, for the two systems V ′ =
(Vi)2k−1
1
and V ′′ = (Vi)2k
2k−1+1 two equivalent single equations e′ and e′′ respectively, of
size ≤3k−1|V ′| and ≤3k−1|V ′′| respectively. Now if we combine e′ and e′′ we obtain
an equivalent single equation of size bounded by ≤3(3k−1|V ′|+3k−1|V ′′|) = 3k(|V ′|+
|V ′′|) = 3k(|V|). Getting back to system W, this means that W is equivalent to a single
equation of size ≤3k(|W| + 2k −n). Since k ≤logn + 1 we have 3k(|W| + 2k −n) ≤
3logn+1(|W| + 2logn+1 −n) ≤3nlog3(|W| + 2nlog2 −n). Since n is bounded by |W|, we
deduce the assertion of the lemma.
⊓⊔
Theorem 2. Solvability of a simple set U of word equations is in NP.
Proof. Assume that Algorithm A halts without failure on the given problem U. We shall
then be left with a ﬁnal system of solved form equations, along with several (simple)
1-variable equations. Moreover (see Lemma 2, and Corollary 1) the size of these 1-
variable equations is polynomially bounded w.r.t. the size of U. Thanks to Lemma 3,
every subsystem of these 1-variable equations involving a given variable X is equivalent
to a single 1-variable equation in X (that may not be simple). Each of these resulting
1-variable equations can then be solved, independently, in polynomial time (see [8]). ⊓⊔
We can now conclude:
Theorem 3. Uniﬁability modulo RCONS is NP-complete.
3
List Theory with rev
The axioms of this theory are
rcons(nil,x) ≈cons(x,nil)
rcons(cons(x,Y),z) ≈cons(x,rcons(Y,z))
rev(nil) ≈nil
rev(cons(x,Y)) ≈rcons(rev(Y),x)
where nil and cons are constructors. Orienting each of the above equations to the right
yields a convergent rewrite system (with 4 rules). But the term rewriting system we
shall consider here, for the theory rev, is the following system of six rewrite rules:
(1)
rcons(nil,x) →cons(x,nil)
(2)
rcons(cons(x,Y),z) →cons(x,rcons(Y,z))
(3)
rev(nil) →nil
(4)
rev(cons(x,Y)) →rcons(rev(Y),x)
(5)
rev(rcons(X,y)) →cons(y,rev(X))
(6)
rev(rev(X)) →X
which is again convergent. We shall refer to this equational theory as REV.
Actually, the two added rules (5) and (6) are derivable as inductive consequences
of the ﬁrst four rules. We shall prove this by induction on the length of the list-term X,

10
S. Anantharaman et al.
where by ‘length’ of any ground list-term X, we shall mean the number of applications
of cons in X at the outermost level.
We ﬁrst prove the claim for the added rule (5): rev(rcons(X,y)) →cons(y,rev(X)).
Suppose we have some ground term X. If X = nil, then
rev(rcons(X,y)) ≈rev(rcons(nil,y)) →+ cons(y,nil) and
cons(y,rev(X)) ≈cons(y,rev(nil)) →+ cons(y,nil)
If X = cons(a,Y) for some term a and some term Y of length n, then
rev(rcons(X,y)) ≈rev(rcons(cons(a,Y),y)) →+ rcons(rev(rcons(Y,y)),a) and
cons(y,rev(X)) ≈cons(y,rev(cons(a,Y))) →+ rcons(cons(y,rev(Y)),a)
By the inductive assumption rev(rcons(Y,y)) = cons(y,rev(Y)) and we are done.
We prove then the claim for the added rule (6): rev(rev(X)) →X.
Clearly rev(rev(nil)) →+ nil. Let X = cons(a,Y) for some term a and some term Y
of length n. Then rev(rev(X)) ≈rev(rev(cons(a,Y))) →rev(rcons(rev(Y),a))
→cons(a,rev(rev(Y))) →cons(a,Y) = X.
From this point on, without loss of generality, we will consider all terms to be in
normal form modulo this term rewrite system.
Lemma 4. Let S1, S2, t1, t2 be terms such that: rcons(S1, t1) =REV rcons(S2, t2).
Then
S1 =REV S2
and
t1 =REV t2.
Lemma 5. Let S1, S2 be terms such that: rev(S1) =REV rev(S2). Then S1 =REV S2.
Lemma 6. Uniﬁability modulo REV is NP-hard.
Proof. The NP-hardness proof for uniﬁability modulo RCONS (as given in Sect. 2)
remains valid for uniﬁability modulo REV as well.
⊓⊔
Theorem 4. Uniﬁability modulo REV is in NP and is therefore NP-Complete.
Proof. After normalization with the rules of REV, we can assume that, for every equa-
tion in the given uniﬁcation problem, its lhs as well as its rhs are of one of the following
two types:
cons(x1,cons(x2,...(rcons(rcons(...(rcons(X,y1)...))),
or
cons(x1,cons(x2,...(rcons(rcons(...(rcons(rev(Y),z1)...)))
If the lhs and the rhs of an equation are both of the ﬁrst type, or both of the second
type, then we can associate with it a word equation of the form αXβ ≈? α′Yβ ′, as in
Sect. 2; we deal with all such equations ﬁrst, exactly as we did in Sect. 2.
Once done with such equations, we consider equations (in the uniﬁcation prob-
lem) whose lhs are of the ﬁrst type, while their rhs are of the second type, or vice
versa. To each such equation we can associate either a word equation of the form
αXβ ≈? α′Y Rβ ′, or a word equation of the form αXβ ≈? α′XRβ ′, where Y R (resp.

Uniﬁcation Modulo Lists with Reverse Relation with Certain Word Equations
11
XR) is a variable that stands for rev(Y) (resp. for rev(X)); naturally, all these will be
duly pruned.
The (pruned) word equations of a ‘mixed’ type, of the form αX ≈? Y Rβ involving
two different variables X,Y will be handled by the addition of an extra splitting infer-
ence step to the algorithm A, say between its Steps 2 and 3. In concrete terms, such
an equation will ﬁrst get split by writing: X ≈? Zβ, and Y R ≈? αZ, where Z is a fresh
variable, then ‘solving it locally’ as X ≈? Zβ ′, Y ≈? ZRαR; this substitution will then
be propagated to all the other equations of the problem involving X or Y; the result-
ing equations derived thereby, will be treated similarly, and by the procedure that we
present below for the equations involving a single variable.
We present now the part of the algorithm that deals with all the (pruned) word
equations of the form αXβ ≈? α′XRβ ′, on a given variable X of the problem. This part
will be referred to as the palindrome discovery step of the algorithm. We will use the
word palindrome to refer to a variable X that has to satisfy X = XR. We maintain a
list of variables that are known to be palindromes in our algorithm, which is initially
empty. Clearly, if X is known to be a palindrome, then αXβ ≈? α′XRβ ′ is the same as
αXβ ≈? α′Xβ ′ and need not be considered at this step in the algorithm.
In this part, we have two cases to consider:
Case 1: X ≈? α′′XRβ ′′. In this case, if |α′′β ′′| = 0, then we conclude that X is a
palindrome. Else, if |α′′β ′′| ̸= 0, then there is clearly no solution and we terminate
with failure.
Case 2: α′′X ≈? XRβ ′′. In this case, we check for the existence of words u,v such
that α′′ = uRv, β ′′ = vu. If such a pair exists, we may conclude that X = u, and
this solution can be propagated through the dependency graph, as in the ﬂat-list
case. Again, there cannot be more than min(|α|,|β|) of these solutions. If all such
pairs are checked without ﬁnding a solution, then we resort to splitting and write
X = Zβ ′′, XR = α′′Z, where Z is fresh. This second equation gives us X = ZRα′′R
and therefore Zβ ′′ = ZRα′′R. If β ′′ ̸= α′′R, then there is no solution and we may
terminate with failure. Otherwise we may conclude that Z = ZR (and is therefore a
palindrome) and replace all occurrences of X with Zβ ′′.
Once we have ﬁnished this, we have to check with the equations of the form αX ≈? Xβ
involving the same variable X studied above. If X is not a palindrome, then we may use
(possibly after grouping several equations with Lemma 3) the algorithm given in [8] to
ﬁnd a solution. If X is known to be a palindrome, then we still run the algorithm given
in [8] to check for a solution, but we ﬁrst check that the preﬁxes and sufﬁxes of each
equation (i.e., α,β) meet certain criteria.
In the case where |α| or |β| ≥|X|, the equation αX ≈? Xβ implies that X has to
be a preﬁx of α and a sufﬁx of β. Therefore we may exhaustively check all palindrome
preﬁxes and sufﬁxes of α and β respectively for validity.
Remains to consider the case where |α| and |β| < |X|; then, according to the follow-
ing Lemma 7, X is a solution if and only if there exist palindromes u, v, and a positive
integer k such that α = uv, β = vu and X = (uv)ku.
Lemma 7. Let α, β and A be non-empty words such that A is a palindrome and |α| =
|β| < |A|. Then αA = Aβ if and only if there exist palindromes u, v, and a positive
integer k such that α = uv, β = vu and A = (uv)ku.

12
S. Anantharaman et al.
Proof. If α = uv, β = vu and A = (uv)ku for palindromes u, v then
αA = uv(uv)ku = (uv)k+1u = (uv)kuvu = Aβ
Also, AR = ((uv)ku)R = uR(vRuR)k = u(vu)k = (uv)ku = A. So, A is indeed a
palindrome and satisﬁes αA = Aβ.
It is well-known that for any equation αA = Aβ where 0 < |α| = |β| < |A|, α and
β must be conjugates. That is, there must exist some pair of words u, v, such that α = uv
and β = vu. Furthermore, A must be (uv)ku for some k. If A is also a palindrome, then
α must be a preﬁx of A and β must be a sufﬁx of A. Because A is a palindrome, αR is
therefore also a sufﬁx of A. So, because |α| = |β|, we may conclude that αR = β. The
proof proceeds as follows:
αR = β implies (uv)R = vu implies vRuR = vu implies (vR = v and uR = u)
Thus u and v are palindromes, α = uv, β = vu, and A = (uv)ku for some k > 0.
⊓⊔
4
List Theories with length
In many cases of practical interest, list data types are ‘enriched’ with a length operator,
under which, e.g., the list cons(a,nil) will have length 1, the list cons(a,cons(b,nil))
will have length 2, etc. Solving equations on list terms in these cases will need to take
into account length constraints. For instance cons(a,X) = cons(a,Y) cannot be solved if
length(X) = s(length(Y) (where s stands for the successor function on natural integers).
We shall be assuming in this section that a length operator is deﬁned on the lists we
consider, and that this operator is formally deﬁned in terms of a (typed) convergent
rewrite system, presented below in Sects. 4.1 and 4.2. Our objective will be to solve
equations on list terms subject to certain given length constraints. We will again reduce
the problem to solving some word equations. It seems appropriate here to quote [17]:
“The problem of solving a word equation with a length constraint (i.e., a constraint
relating the lengths of words in the word equation) has remained a long-standing open
problem”. However, thanks to the special form of the word equations we deal with, we
will be able to provide a decision algorithm in our case.
4.1
length with rcons
The Term Rewrite System: The (typed) rewrite rules for rcons with length are given
below, where the unary functions s and length are typed as s : nat →nat, length : list →
nat; the constant 0 is typed 0 →nat;. This rewrite system is convergent:
length(nil) →0
length(cons(x,Y)) →s(length(Y))
rcons(nil,x) →cons(x,nil)
rcons(cons(x,Y),z) →cons(x,rcons(Y,z))
length(rcons(X,y)) →s(length(X))
Variables of type nat will be denoted (in general) by the lower case letters i, j,k,m,n,...
The last rule above, length(rcons(x,y)) →s(length(x)), is derived easily from the pre-
ceding rules, by induction.

Uniﬁcation Modulo Lists with Reverse Relation with Certain Word Equations
13
The Uniﬁcation Algorithm. We shall assume that all instances of s and 0 in the equa-
tions of the given problem have been removed by successive applications of the infer-
ence rules below, where E Q is a set of equations and ⊎is the disjoint union:
E Q ⊎{n ≈? 0}
E Q ∪{n ≈? length(X′), X′ ≈? nil}
E Q ⊎{n ≈? s(m)}
E Q ∪{n ≈? length(X′), m ≈? length(Y ′), X′ ≈? cons(x′,Y ′)}
Assume further that our given uniﬁcation problem U has been transformed into
word equations of the form αXβ ≈? γYδ and that those equations have been reduced
to a set of mutually independent systems Si of equations on one variable Xi. Let us call
this set S = Si where each Si is a set of equations on one variable. These independent
systems of equations Si may be solved, each producing a solution-set which forms a
regular language [8]. We shall use Li to refer to the solution-set to the system of equa-
tions Si. By Theorem 3 in [8] either Li = Fi or Li = Fi ∪(uivi)+ui for some words ui,vi
and some ﬁnite set of words Fi.
In the version of this problem without length, the problem of uniﬁability is now
solved by simply checking each element of {Li} for (non-)emptiness. However, here
the solutions may still be related by length equations. We must ﬁnd a set of words
{wi | wi ∈Li} which satisfy length constraints of the form |wi| = |wj|+ci j for a constant,
non-negative integer ci j. Note that not all pairs i, j need have a constraint of this form.
For wi we either try elements of Fi or a word of type: (uivi)niui. For the latter case
constraints of the type |wi| = |wj| + ci j are equivalent to (|ui| + |vi|)ni + |ui| = (|uj| +
|vj|)nj + |uj| + ci j, where ni,nj are non-negative integer variables; so we can always
reduce our problem to solving a ﬁnite number of linear diophantine equations.
4.2
length with rcons and rev
The Term Rewrite System: We now add the rewrite rules of rev as deﬁned in Sect. 3.
The resulting rewrite system (where 0,s,length are typed as in Sect. 4.1) is convergent:
length(nil) →0
length(cons(x,y)) →s(length(y))
rcons(nil,x) →cons(x,nil)
rcons(cons(x,y),z) →cons(x,rcons(y,z))
rev(nil) →nil
rev(cons(x,y)) →rcons(rev(y),x)
rev(rev(x)) →x
length(rcons(x,y)) →s(length(x))
length(rev(x)) →length(x)
The last rule, length(rev(x)) →length(x), is not originally in the theories of RCONS,
REV or LENGTH but can be easily derived by induction from the other rules.

14
S. Anantharaman et al.
The Uniﬁcation Algorithm. We again assume that all instances of s and 0 have been
removed from the equations of the given problem, by successive applications of the
same two inference rules presented above in Sect. 4.1, for rcons.
We thus assume once more that our uniﬁcation problem has been transformed into
word equations of the form αXβ ≈? γYδ and that those equations have been reduced
to a set of mutually independent systems of equations on one variable (without reverse)
which may be required to be a palindrome. As before, let S = {Si} where each Si is a
set of equations on one variable which are respectively solved by the languages Li.
Now suppose that Si has variable X that is not required to be a palindrome, then
its solution-set is either Fi or Fi ∪(uivi)+ui for some words ui,vi and some ﬁnite set of
words Fi according to Theorem 3 of [8]. If X is required to be a palindrome then its
solution-set contains either a ﬁnite set of palindromes Fi or Fi ∪(uivi)+ui, where ui, vi,
are such that α = uivi, β = viui according to Lemma 7 and Theorem 3 of [8].
The algorithm now continues as in the previous length with rcons case: if two
solution-sets are related by length equations, satisﬁability may be checked by solving a
ﬁnite set of linear diophantine equations.
5
Some Illustrative Examples
The following simple examples illustrate how our methods presented above will operate
(either directly, or indirectly) in concrete situations.
Example 1. cof two ‘list equations’:
cons(x,X) ≈? rev((cons(y,Y)), cons(a,X) ≈? rev(cons(a,rev(X)))
As described in Sect. 3, this system will ﬁrst get transformed to a system of two word
equations: xX ≈? Y R y, aX ≈? X a.
We can apply the Splitting step of algorithm A to the ﬁrst word equation, and derive:
X ≈? Z y, Y R ≈? xZ, where Z is fresh; the latter of these two will get transformed to the
solved form: Y ≈? ZRx. We have thus derived two solved forms: X ≈? Z y, Y ≈? ZRx.
Propagating for X from the ﬁrst of these solved forms in the second word equation
would a priori give: aZ y ≈? Z ya, so Pruning would imply: y = a. And the variable Z
has to satisfy: aZ ≈? Z a; which is true for any of the assignments: Z = nil, Z = a, Z =
aa, Z = aaa, etc. If we choose Z = nil, and x = a, we get the following solution for
the given list equations: X = rcons(nil,a), Y = rcons(nil,a).
Example 2. Consider the single 1-variable word equation abX ≈? X ba.
For solving this 1-variable equation, (instead of appealing to the general result
of [8]) we could choose to see the equation as an instance of a 2-variable equation,
and use Splitting, as an ad hoc technique: i.e., replace the X on the lhs by Z ba, and the
X on the rhs by abZ.
The equation would then become: abZ ba ≈? abZ ba, on the single variable Z,
which admits any value of Z as a solution. Now, each of the assignments Z = a, Z =
aba, Z = ababa satisﬁes the equalities X = abZ = Z ba. So each of the assignments
X = aba,X = ababa,X = abababa, is a solution for the given problem.

Uniﬁcation Modulo Lists with Reverse Relation with Certain Word Equations
15
Example 3. We consider now the set of word equations: abX ≈? Y ba, Y ≈? XR.
The equations are ﬁrst replaced as: abX ≈? XR ba and the equality Y = XR. For
solving the former we use Splitting, and write: X ≈? Z ba and XR ≈? abZ. The fresh
variable Z has then to satisfy the condition that Z ba = ZR ba. That is to say: Z must
be a palindrome. Any palindrome (on the given alphabet) is actually a solution. We
thus deduce that the assignments X = Z ba, Y = abZ, where Z is any palindrome, is a
solution for the given set of equations.
Note: This also shows that uniﬁcation modulo the theory rev is inﬁnitary. (It is not
difﬁcult to see that uniﬁcation modulo the theory rcons is not ﬁnitary either.)
Example 4. We consider now the set of word equations, subject to a length constraint:
abX ≈? X ba, X ≈? ababY, length(Y) = 1
This set would be transformed into a set of word equations:
{abX ≈? X ba, X ≈? ababY, Y ≈? yZ, Z ≈? nil}.
Propagation of the ﬁrst dependency would give us the 1-variable equation
abababY ≈? ababY ba, which, once pruned, would become: abY ≈? Y ba. Propa-
gation of the other dependencies would give us the equation aby ≈? yba; which admits
as solution y = a. Thus the given problem admits as solution: X = ababa, Y = a, Z = nil.
Suppose now, that the length constraint given is either length(Y) = 0, or
length(Y) = 3, instead of the one given above. Then, by what we have seen in the
previous two examples (we know the forms of the possible solutions for Y, after the
propagation of the ﬁrst dependency; therefore) the problem thus modiﬁed would be
unsatisﬁable.
6
Conclusion and Future Work
We have shown that uniﬁability modulo the two theories RCONS, REV are both NP-
complete. For that we have identiﬁed a new class of word equations, simple sets, which
can be solved in NP. One possible direction for our future work would be to investigate
other problems for these list theories; for instance we can show that the uniform word
problem for RCONS is undecidable (cf. [1], Appendix B). A second direction of future
work would be to identify a class of non-simple sets of word equations, which can be
solved by a suitable adaptation (and extension) of the algorithm A.
We also plan to investigate the interesting question of whether the results, such as
membership in NP, hold with the addition of linear constant restrictions (as in [5,26]),
to the theory of REV. This could lead to a method to solve the positive fragment of REV.
Disuniﬁcation modulo REV is another interesting problem to investigate, and that may
be reducible to the previous one.

16
S. Anantharaman et al.
References
1. Anantharaman, S., Hibbs, P., Narendran, P., Rusinowitch, M.: Uniﬁcation of lists with reverse
as solving simple sets of word equations. Research-Report. https://hal.archives-ouvertes.fr/
hal-02123648
2. Abdulla, P.A., et al.: String constraints for veriﬁcation. In: Biere, A., Bloem, R. (eds.) CAV
2014. LNCS, vol. 8559, pp. 150–166. Springer, Cham (2014). https://doi.org/10.1007/978-
3-319-08867-9 10
3. Armando, A., Bonacina, M.P., Ranise, S., Schulz, S.: New results on rewrite-based satisﬁa-
bility procedures. ACM Trans. Comput. Log. 10(1), 4:1–4:51 (2009)
4. Baader, F., Nipkow, T.: Term Rewriting and All That. Cambridge University Press, Cam-
bridge (1999)
5. Baader, F., Schulz, K.U.: Uniﬁcation in the union of disjoint equational theories: combining
decision procedures. J. Symb. Comput. 21(2), 211–243 (1996)
6. Baader, F., Snyder, W.: Uniﬁcation theory. In: Robinson, J.A., Voronkov, A. (eds.) Handbook
of Automated Reasoning, pp. 445–532. Elsevier and MIT Press (2001)
7. Colmerauer, A.: An introduction to Prolog III. Commun. ACM 33(7), 69–90 (1990)
8. Dabrowski, R., Plandowski, W.: On word equations in one variable. Algorithmica 60(4),
819–828 (2011)
9. Diekert, V., Jez, A., Plandowski, W.: Finding all solutions of equations in free groups and
monoids with involution. Inf. Comput. 251, 263–286 (2016)
10. Guttag, J.V., Horowitz, E., Musser, D.R.: Abstract data types and software validation. Com-
mun. ACM 21(12), 1048–1064 (1978)
11. Ganesh, V., Minnes, M., Solar-Lezama, A., Rinard, M.: Word equations with length con-
straints: what’s decidable? In: Biere, A., Nahir, A., Vos, T. (eds.) HVC 2012. LNCS, vol.
7857, pp. 209–226. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-39611-
3 21
12. Hibbs, P.: Uniﬁcation modulo common list functions. Doctoral Dissertation. University at
Albany–SUNY (2015)
13. Kapur, D.: Towards a theory for abstract data types. Doctoral Dissertation. Massachusetts
Institute of Technology (1980)
14. Kapur, D., Musser, D.R.: Proof by consistency. Artif. Intell. 31(2), 125–157 (1987)
15. Karhum¨aki, J.: Combinatorics on Words: A New Challenging Topic, p. 12. Turku Centre for
Computer Science, Turku (2004)
16. Liang, T., Tsiskaridze, N., Reynolds, A., Tinelli, C., Barrett, C.: A decision procedure for
regular membership and length constraints over unbounded strings. In: Lutz, C., Ranise, S.
(eds.) FroCoS 2015. LNCS, vol. 9322, pp. 135–150. Springer, Cham (2015). https://doi.org/
10.1007/978-3-319-24246-0 9
17. Lin, A.W., Majumdar, R.: Quadratic word equations with length constraints, counter systems,
and Presburger arithmetic with divisibility. arXiv preprint arXiv:1805.06701 (2018)
18. Morita, K.: Universality of a reversible two-counter machine. Theoret. Comput. Sci. 168(2),
303–320 (1996)
19. Musser, D.R.: Abstract data type speciﬁcation in the AFFIRM system. IEEE Trans. Softw.
Eng. 6(1), 24–32 (1980)
20. Musser, D.R.: On proving inductive properties of abstract data types. In: Proceedings of the
Seventh Annual ACM Symposium on Principles of Programming Languages (POPL), pp.
154–162 (1980)
21. Nelson, G., Oppen, D.C.: Simpliﬁcation by cooperating decision procedures. ACM Trans.
Program. Lang. Syst. 1(2), 245–257 (1979)

Uniﬁcation Modulo Lists with Reverse Relation with Certain Word Equations
17
22. Oppen, D.C.: Reasoning about recursively deﬁned data structures. J. ACM 27(3), 403–411
(1980)
23. Plandowski, W.: An efﬁcient algorithm for solving word equations. In: Proceedings of the
ACM Symposium on the Theory of Computing, pp. 467–476 (2006)
24. Robson, J.M., Diekert, V.: On quadratic word equations. In: Meinel, C., Tison, S. (eds.)
STACS 1999. LNCS, vol. 1563, pp. 217–226. Springer, Heidelberg (1999). https://doi.org/
10.1007/3-540-49116-3 20
25. Shostak, R.E.: Deciding combinations of theories. J. ACM 31(1), 1–12 (1984)
26. Schulz, K.U.: On existential theories of list concatenation. In: Pacholski, L., Tiuryn, J. (eds.)
CSL 1994. LNCS, vol. 933, pp. 294–308. Springer, Heidelberg (1995). https://doi.org/10.
1007/BFb0022264

On the Width of Regular Classes
of Finite Structures
Alexsander Andrade de Melo1 and Mateus de Oliveira Oliveira2(B)
1 Federal University of Rio de Janeiro, Rio de Janeiro, Brazil
aamelo@cos.ufrj.br
2 University of Bergen, Bergen, Norway
mateus.oliveira@uib.no
Abstract. In this work we introduce the notion of decisional width of
a ﬁnite relational structure and the notion of regular-decisional width
of a regular class of ﬁnite structures. Our main result states that the
ﬁrst-order theory of any regular-decisional class of ﬁnite structures is
decidable. Building on the proof of this decidability result, we show that
the problem of counting satisfying assignments for a ﬁrst-order logic for-
mula in a structure of constant width is ﬁxed parameter tractable when
parameterized by the width parameter and can be solved in quadratic
time with respect to the length of the input representation of the struc-
ture.
Keywords: Automatic structures · Width measures ·
First order logic
1
Introduction
The notion of relational structure can be used to formalize a wide variety
of mathematical constructions—such as graphs, hypergraphs, groups, rings,
databases, etc. Not surprisingly, relational structures are a central object of
study in several subﬁelds of computer science, such as database theory, learn-
ing theory [15], constraint satisfaction theory [7,22], and automated theorem
proving [9,10,25,28,30].
Let C be a class of ﬁnite structures and let L be a logic, such as ﬁrst-order
logic (FO) or monadic second-order logic (MSO). The L theory of C is the set
T (L, C) of all logical sentences from L that are satisﬁed by at least one structure
in C. We say that T (L, C) is decidable if the problem of determining whether a
given sentence ϕ belongs to T (L, C) is decidable.
Showing that the L theory of a particular class of structures C is decidable
is an endeavour of fundamental importance because many interesting mathe-
matical statements can be formulated as the problem of determining whether
some/every/no structure in a given class C of ﬁnite structures satisﬁes a given
logical sentence ϕ. For instance, when considering the celebrated 4-color theo-
rem, which states that every planar graph is 4-colorable, the class C is the set
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 18–34, 2019.
https://doi.org/10.1007/978-3-030-29436-6_2

On the Width of Regular Classes of Finite Structures
19
of all planar graphs, while formula ϕ is a particular formula in the monadic sec-
ond order logic of graphs expressing the 4-colorability property. While it can be
shown that the monadic second-order theory of planar graphs is undecidable, one
can show using automata-theoretic techniques that for each ﬁxed k, the MSO1
theory1 of graphs of clique width at most k is decidable [10,30], and that the
MSO2 theory of the class of graphs of treewidth at most k is decidable [9,30].
The problem of deciding logical properties of relational structures has also
been studied extensively in the ﬁeld of automatic structure theory. Intuitively,
a relational structure is automatic if its domain and each of its relations can be
deﬁned by ﬁnite automata. Although the idea of representing certain relational
structures using automata dates back to seminal work of B¨uchi and Rabin, and it
has been well studied in the realm of automatic group theory [14], Khoussainov
and Nerode were the ﬁrst to deﬁne and investigate a general notion of automatic
structure [21]. Among other results that can be proved, automatic structures
can be used to provide an elegant proof of a celebrated theorem of Presburger
stating that the ﬁrst order theory of (N, +) is decidable. In [24] Kruckman et
al. introduced the notion of structure with advice and used it to show that the
ﬁrst-order theory of (Q, +) is decidable. The theory of automatic structures with
advice was recently extended by Zaid, Gr¨adel and Reinhardt and used to deﬁne
classes of structures. They show that the notion of automatic classes of structures
with advice has decidable ﬁrst-order logic theory if and only if the set of used
advices has a decidable monadic second-order logic theory [30].
In this work we introduce a notion of width for relational structures based
on the notion of ordered decision diagrams (ODD’s). ODD’s may be regarded as
a special case of acyclic ﬁnite automata where states are split into a sequence of
levels, and transitions are split into layers which send states in level i to states
in level i + 1. We note that ODD’s over binary alphabets are widespread in the
literature and usually called OBDDs. This formalism has been used extensively
in the ﬁled of symbolic computation due to its ability to represent in a concise
way combinatorial structures with exponentially many elements in the domain
[3,4,18,26,29]. An important complexity measure when dealing with ODD’s is
the notion of width. This is simply the maximum number of states in a level of
the ODD. We say that a relational structure is (Σ, w)-decisional if its domain
and each of its relations can be represented by an ODD of width w over an
alphabet Σ. We then use ﬁnite automata over alphabets of tuples of layers to
deﬁne inﬁnite classes of ﬁnite structures. We call these classes regular-decisional.
We show that, given a ﬁrst order formula ψ and a constant w, the class of ﬁnite
structures of decisional width at most w that satisfy ψ is regular-decisional. From
this result, it follows that the ﬁrst-order theory of regular-decisional classes of
ﬁnite structures is decidable. Finally, we show that the problem of counting the
number of satisfying assignments for a ﬁxed ﬁrst-order logic formula ψ with
respect to a decisional ﬁnite τ-structure A is ﬁxed parameter tractable when
1 MSO1 denotes the MSO logic of graphs where edge-set quantiﬁcations are not
allowed, while MSO2 is the extension of MSO1 where we can also quantify over
sets of edges.

20
A. Andrade de Melo and M. de Oliveira Oliveira
parameterized by the width of the input representation D of A, and that such a
problem can be solved in time quadratic in the length of D.
An interesting feature of our width measure for classes of structures is that
it behaves very diﬀerently from usual complexity measures for graphs studied in
structural complexity theory. As an example of this fact, we note that the family
of hypercube graphs has regular-decisional width 2, while it has unbounded
width in many of the studied measures, such as treewidth and cliquewidth. This
class of graphs has also unbounded degeneracy, and therefore it is not no-where
dense. As a consequence, we have that most algorithmic metatheorems proved
so far dealing with the interplay between ﬁrst-order logic and structural graph
theory [16,17,23] fail on graphs of constant decisional-width.
2
Preliminaries
We denote by N .= {0, 1, . . .} the set of natural numbers (including zero), and
by N+ .= N \ {0} the set of positive natural numbers. For each c ∈N+, we let
[c] .= {1, 2, . . . , c} and c .= {0, 1, . . . , c−1}. Given a set S and a number a ∈N+,
we let S×a be the set of all a-tuples of elements from S.
Relational Structures. A relational vocabulary is a tuple τ = (R1, . . . , Rl)
of relation symbols where for each i ∈[l], the relation symbol Ri is associated
with an arity ai ∈N+. Let τ = (R1, . . . , Rl) be a relational vocabulary. A ﬁnite
τ-structure is a tuple A = (R0(A), R1(A), . . . , Rl(A)) such that
1. R0(A) is a non-empty ﬁnite set, called the domain of A;
2. for each i ∈[l], Ri(A) ⊆R0(A)×ai is an ai-ary relation.
Let A and A′ be τ-structures. An isomorphism from A to A′ is a bijection
π: R0(A) →R0(A′) such that, for each i ∈[l], (u1, . . . , uai) ∈Ri(A) if and only if
(π(u1), . . . , π(uai)) ∈Ri(A′). If there exists an isomorphism from A to A′, then
we say that A is isomorphic to A′, and we denote this fact by A ≃A′.
First-Order Logic. Now, we brieﬂy recall some basic concepts from ﬁrst-order
logic. Extensive treatments of this subject can be found in [11,12].
Let τ = (R1, . . . , Rl) be a relational vocabulary. We denote by FO{τ} the set
of all ﬁrst-order logic formulas over τ, i.e. the logic formulas comprising: variables
to be used as placeholders for elements from the domain of a τ-structure; the
Boolean connectives ∨, ∧, ¬, →and ↔; the quantiﬁers ∃and ∀that can be
applied to the variables; and the two following atomic logic formulas x = y, where
x and y are variables, and Ri(x1, . . . , xai) for some i ∈[l], where x1, . . . , xai are
variables.
A variable x is said to be free in a formula ψ ∈FO{τ}, if x is not within
the scope of any quantiﬁer in ψ. The set of free variables of ψ is denoted by
freevar(ψ). We write ψ(x1, . . . , xt) to indicate that freevar(ψ) ⊆{x1, . . . , xt}. A
sentence is a formula without free variables.
Let A = (R0(A), R1(A), . . . , Rl(A)) be a ﬁnite τ-structure, ψ(x1, . . . , xt) ∈
FO{τ} and u1, . . . , ut ∈R0(A). We write A |= ψ[u1, . . . , ut] to mean that A

On the Width of Regular Classes of Finite Structures
21
satisﬁes ψ when all the free occurrences of the variables x1, . . . , xt are interpreted
by the values u1, . . . , ut, respectively. In particular, if ψ is a sentence, then we
may write A |= ψ to mean that A satisﬁes ψ. In this case, we also say that A is
a model of ψ. If ψ(x1, . . . , xt) ≡Ri(xβ(1), . . . , xβ(ai)) for some mapping β : [ai] →
[t] and some i ∈[l], then A |= ψ[u1, . . . , ut] if and only if (uβ(1), . . . , uβ(ai)) ∈
Ri(A). The semantics of the equality symbol, of the quantiﬁers ∃and ∀, and of
the Boolean connectives ∨, ∧, ¬, →and ↔are the usual ones.
Languages. An alphabet Σ is any ﬁnite, non-empty set of symbols. A string
over Σ is any ﬁnite sequence of symbols from Σ. We denote by Σ+ the set of
all (non-empty) strings over Σ. A language over Σ is any subset L of Σ+. In
particular, for each k ∈N+, we let Σk be the language of all strings of length
k over Σ, and we let Σ≤k .= Σ1 ∪· · · ∪Σk be the language of all (non-empty)
strings of length at most k over Σ.
For each alphabet Σ, we let □be a special symbol, called the padding symbol,
such that □̸∈Σ, and we write Σ ⊎{□} to denote the disjoint union between Σ
and {□}. For each σ ∈Σ ⊎{□} and each c ∈N+, we let σ×c denote the tuple
(σ, . . . , σ) composed by c copies of the symbol σ.
Tensor Product. Let Σ1, . . . , Σc be c alphabets, where c ∈N+. The tensor
product of Σ1, . . . , Σc is deﬁned as the alphabet
Σ1 ⊗· · · ⊗Σc .= {(σ1, . . . , σc): σi ∈Σi, i ∈[c]}.
In particular, for each c ∈N+ and each alphabet Σ, we deﬁne the c-th tensor
power of Σ as the alphabet Σ⊗c .=
c times



Σ ⊗· · · ⊗Σ . Note that, the tensor product of
Σ1, . . . , Σc (the c-th tensor power of Σ) may be simply regarded as the Cartesian
product of Σ1, . . . , Σc (the c-ary Cartesian power of Σ, respectively). Also, note
that Σ⊗1 = Σ.
For each i ∈[c], let si = σi,1 · · · σi,ki be a string of length ki, where ki ∈N+,
over the alphabet Σi. The tensor product of s1, . . . , sc is deﬁned as the string
s1 ⊗· · · ⊗sc .= (σ1,1, . . . , σc,1) · · · (σ1,k, . . . , σc,k)
of length k = max{k1, . . . , kc} over the alphabet (Σ1 ⊎{□}) ⊗· · · ⊗(Σc ⊎{□})
such that, for each i ∈[c] and each j ∈[k],
σi,j .=
σi,j
if j ≤ki
□
otherwise.
For instance, the tensor product of the strings aabab and abb over the alphabet
{a, b} is the string (a, a)(a, b)(b, b)(a, □)(b, □) over the alphabet ({a, b}⊎{□})⊗2.
For each i ∈[c], let Li ⊆Σ+
i be a language over the alphabet Σi. The tensor
product of L1, . . . , Lc is deﬁned as the language
L1 ⊗· · · ⊗Lc .= {s1 ⊗· · · ⊗sc : si ∈Li, i ∈[c]}.
We remark that, in the literature [1,2,20,30], the tensor product of strings (as
well as the tensor product of languages) is also commonly called of convolution.

22
A. Andrade de Melo and M. de Oliveira Oliveira
Finite Automata. A ﬁnite automaton is a tuple F = (Σ, Q, T, I, F), where Σ
is an alphabet, Q is a ﬁnite set of states, T ⊆Q × Σ × Q is a set of transitions,
I ⊆Q is a set of initial states and F ⊆Q is a set of ﬁnal states. The size of a
ﬁnite automaton F is deﬁned as |F| .= |Q| + |T|.
Let k ∈N+ and s = σ1 · · · σk ∈Σk. We say that F accepts s if there
exists a sequence of transitions ⟨(q0, σ1, q1), (q1, σ2, q2), . . . , (qk−1, σk, qk)⟩, called
accepting sequence for s in F, such that q0 ∈I, qk ∈F and, for each i ∈[k],
(qi−1, σi, qi) ∈T. The language of F, denoted by L(F), is deﬁned as the set of
all strings accepted by F, i.e. L(F) .= {s ∈Σ+ : s is accepted by F}.
Regular Relations. Let Σ be an alphabet. A language L ⊆Σ+ is called regular
if there exists a ﬁnite automaton F over Σ such that L(F) = L.
For each a ∈N+ and each language L ⊆(Σ⊗a)+ over the alphabet Σ⊗a, we
let rel(L) .= {(s1, . . . , sa): s1 ⊗· · · ⊗sa ∈L} be the relation associated with L.
On the other hand, for each a ∈N+ and each a-ary relation R ⊆(Σ+)×a, we let
lang(R) .= {s1 ⊗· · · ⊗sa : (s1, . . . , sa) ∈R} be the language associated with R.
We say that such a relation R is regular if its associated language lang(R) is a
regular subset of (Σ⊗a)+.
3
Ordered Decision Diagrams
Let Σ be an alphabet and w
∈
N+. A (Σ, w)-layer is a tuple B
.=
(ℓ, r, T, I, F, ι, φ), where ℓ⊆w is a set of left states, r ⊆w is a set of right
states, T ⊆ℓ×(Σ ⊎{□})×r is a set of transitions, I ⊆ℓis a set of initial states,
F ⊆r is a set of ﬁnal states and ι, φ ∈{0, 1} are Boolean ﬂags satisfying the
following conditions: i) I = ∅if ι = 0, and ii)F = ∅if φ = 0. We remark that,
possibly, ι = 1 and I = ∅. Similarly, it might be the case in which φ = 1 and
F = ∅.
In what follows, we may write ℓ(B), r(B), T(B), I(B), F(B), ι(B) and φ(B)
to refer to the sets ℓ, r, T, I and F and to the Boolean ﬂags ι and φ, respectively.
We let B(Σ, w) denote the set of all (Σ, w)-layers. Note that, B(Σ, w) is non-
empty and has at most 2O(|Σ|·w2) elements. Therefore, B(Σ, w) may be regarded
as an alphabet.
Let k ∈N+. A (Σ, w)-ordered decision diagram (or simply, (Σ, w)-ODD) of
length k is a string D .= B1 · · · Bk ∈B(Σ, w)k of length k over the alphabet
B(Σ, w) satisfying the following conditions:
1. for each i ∈[k −1], ℓ(Bi+1) = r(Bi);
2. ι(B1) = 1 and, for each i ∈{2, . . . , k}, ι(Bi) = 0;
3. φ(Bk) = 1 and, for each i ∈[k −1], φ(Bi) = 0.
The width of D is deﬁned as ω(D) .= max{|ℓ(B1)|, . . . , |ℓ(Bk)|, |r(Bk)|}. We
remark that ω(D) ≤w.
For each k ∈N+, we denote by B(Σ, w)◦k the set of all (Σ, w)-ODDs of
length k. And, more generally, we let B(Σ, w)⊛be the set of all (Σ, w)-ODDs,
i.e. B(Σ, w)⊛.= 
k∈N+ B(Σ, w)◦k.

On the Width of Regular Classes of Finite Structures
23
Let D = B1 · · · Bk ∈B(Σ, w)◦k and s = σ1 · · · σk′ ∈Σ≤k. A valid sequence
for s in D is a sequence of transitions ⟨(p1, σ1, q1), . . . , (pk, σk, qk)⟩satisfying the
following conditions:
1. for each i ∈[k], σi = σi if i ≤k′, and σi = □otherwise;
2. for each i ∈[k], (pi, σi, qi) ∈T(Bi);
3. for each i ∈[k −1], pi+1 = qi.
Such a sequence is called accepting for s if, in addition to the above conditions,
p1 ∈I(B1) and qk ∈F(Bk). We say that D accepts s if there exists an accepting
sequence for s in D. The language of D, denoted by L(D), is deﬁned as the set
of all strings accepted by D, i.e. L(D) .=
	
s ∈Σ≤k : s is accepted by D

.
Proposition 3.1. Let Σ be an alphabet, w, k ∈N+ and D ∈B(Σ, w)◦k. For
each natural number k′ ≥k, there exists an ODD D′ ∈B(Σ, w)◦k′ such that
L(D′) = L(D).
4
Regular-Decisional Classes of Finite Relational
Structures
In this section we combine ODDs with ﬁnite automata in order to deﬁne inﬁnite
classes of ﬁnite structures. We start by deﬁning the notion of (Σ, w, τ)-structural
tuples of ODDs as a way to encode single ﬁnite τ-structures. Subsequently, we
use ﬁnite automata over a suitable alphabet in order to uniformly deﬁne inﬁnite
families of ﬁnite τ-structures.
4.1
Decisional Relations and Decisional
Relational Structures
Let Σ be an alphabet and a, w ∈N+. A ﬁnite a-ary relation R ⊆(Σ+)×a is said
to be strongly (Σ, w)-decisional if there exists an ODD D in B(Σ⊗a, w)⊛such
that rel(L(D)) = R. Given a set S, we say that a relation R ⊆S×a is (Σ, w)-
decisional if R is isomorphic to some strongly (Σ, w)-decisional structure.
Let τ = (R1, . . . , Rl) be a relational vocabulary, Σ be an alphabet and w ∈
N+. We say that a tuple D = (D0, D1, . . . , Dl) of ODDs is (Σ, w, τ)-structural
if there exists a positive natural number k ∈N+, called the length of D, such
that the following conditions are satisﬁed:
1. D0 is an ODD in B(Σ, w)◦k;
2. for each i ∈[l], Di is an ODD in B(Σ⊗ai, w)◦k;
3. for each i ∈[l], L(Di) ⊆L(D0)⊗ai.
Let D = (D0, D1, . . . , Dl) be a (Σ, w, τ)-structural tuple. The τ-structure
derived from D is deﬁned as the ﬁnite τ-structure
s(D) .= (R0(s(D)), R1(s(D)), . . . , Rl(s(D))),

24
A. Andrade de Melo and M. de Oliveira Oliveira
with domain R0(s(D)) = L(D0), such that Ri(s(D)) = rel(L(Di)) for each i ∈[l].
We say that a ﬁnite τ-structure A is strongly (Σ, w)-decisional if there exists
some (Σ, w, τ)-structural tuple D such that A = s(D). We say that a ﬁnite
τ-structure A—whose domain is not necessarily a subset of Σ+—is (Σ, w)-
decisional if A is isomorphic to some strongly (Σ, w)-decisional structure.
The Σ-decisional width of a ﬁnite τ-structure A, denoted by ω(Σ, A), is
deﬁned as the minimum w ∈N+ such that A is (Σ, w)-decisional. The following
proposition states that if a relation is (Σ, w)-decisional, then it is also ({0, 1}, w′)-
decisional for a suitable w′ ∈N+.
Proposition 4.1. Let τ = (R1, . . . , Rl) be a relational vocabulary, Σ be an
alphabet and w ∈N+. If A is a ﬁnite τ-structure with ω(Σ, A) ≤w for some
w ∈N+, then ω({0, 1}, A) ≤w2 · |Σ|max{1,a1,...,al}.
The Hypercube Graph Hk. Let k ∈N+. The k-dimensional hypercube graph
is the graph Hk = (R0(Hk), R1(Hk)) whose vertex set R0(Hk) = {0, 1}k is the
set of all k-bit binary strings, and whose edge set R1(Hk) is the set of all pairs
of k-bit strings that diﬀer in exactly one position.
R1(Hk) = {(b1b2 · · · bk, b′
1b′
2 · · · b′
k) : There is a unique i ∈[k] such that bi ̸= b′
i}.
Therefore, the k-dimensional hypercube is equal to the structure derived from
the structural pair Dk = (D0, D1), where the ODD D0 ∈B({0, 1}, 2)◦k accepts
all k-bit strings of length k, and D1 ∈B({0, 1}⊗2, 2)◦k is the ODD that accepts
all strings of the form b1 · · · bk ⊗b′
1 · · · b′
k such that b1 · · · bk and b′
1 · · · b′
k diﬀer in
exactly one entry. In other words, we have Hk = s(Dk). This structural pair is
depicted in Fig. 1.
Fig. 1. An example of a ({0, 1}, 2, τ)-structural pair D5 = (D0, D1), where τ is the rela-
tional vocabulary of directed graphs. The ODD D0 accepts all binary strings of length
5. The odd D1 accepts all strings of the form b1 · · · b5 ⊗b′
1 · · · b′
5 = (b1, b′
1) · · · (b5, b′
5)
such that b1 · · · b5 and b′
1 · · · b′
5 diﬀer in exactly one bit. The structure derived from D5
is the 5-dimensional directed hypercube H5 = s(D5).

On the Width of Regular Classes of Finite Structures
25
4.2
Regular-Decisional Classes
Let Σ be an alphabet, w ∈N+ and τ = (R1, . . . , Rl) be a relational vocabu-
lary. We let R(Σ, w, τ) denote the relation constituted by all (Σ, w, τ)-structural
tuples. Note that the relation R(Σ, w, τ) is a subset of the following set of tuples
of ODDs

k∈N+
B(Σ, w)◦k × B(Σ⊗a1, w)◦k × · · · × B(Σ⊗al, w)◦k.
Since each (Σ, w, τ)-structural tuple D ∈R(Σ, w, τ) corresponds to a
(Σ, w)-decisional structure s(D), we can associate with each sub-relation
R ⊆R(Σ, w, τ) a class s(R) = {A: D ∈R, s(D) ≃A} of (Σ, w)-decisional τ-
structures.
Deﬁnition 4.2. Let Σ be an alphabet, w ∈N+, and τ be a relational vocab-
ulary. We say that a class C of ﬁnite τ-structures is (Σ, w)-regular-decisional if
there exists a regular sub-relation R ⊆R(Σ, w, τ) such that C = s(R).
The Σ-regular-decisional width of a class C of ﬁnite τ-structures, denoted
by ω(Σ, C), is deﬁned as the minimum w ∈N+ such that C is (Σ, w)-regular-
decisional. We note that this minimum w may not exist. In this case, we set
ω(Σ, C) = ∞.
Now, consider the alphabet
B(Σ, w, τ) .= B(Σ, w) ⊗B(Σ⊗a1, w) ⊗· · · ⊗B(Σ⊗al, w).
(1)
Then a class C of ﬁnite τ-structures is (Σ, w)-decisional if and only if there
exists a ﬁnite automaton F over B(Σ, w, τ) such that rel(L(F)) ⊆R(Σ, w, τ),
and C = s(rel(L(F))). This implies, in particular, that in order to show that a
class C of ﬁnite τ-structures is (Σ, w)-decisional, it is enough to construct a ﬁnite
automaton F over B(Σ, w, τ) such that a string of the form D0 ⊗D1 ⊗. . . ⊗Dl
belongs to L(F) if and only if D = (D0, D1, . . . , Dl) is a (Σ, w, τ)-structural tuple
and s(D) ∈C. To illustrate this type of construction, we show in Proposition 4.3
that the class H of hypercubes is ({0, 1}, 2)-regular-decisional. Since it can be
easily shown that this class is not ({0, 1}, 1)-regular-decisional, we have that the
{0, 1}-regular-decisional width of H is 2.
Proposition 4.3. Let H
.= {Hk : k ∈N+} be the class of all hypercube graphs.
The class H is ({0, 1}, 2)-regular-decisional.
Proof. In Fig. 2 we depict an automaton F which accepts a string D0 ⊗D1 of
length k if and only if the pair Dk = (D0, D1) is a structural pair whose derived
structure s(Dk) is the hypercube graph Hk.
⊓⊔
An interesting aspect of Proposition 4.3 is that it states that the class H of
hypercube graphs has regular-decisional width 2, while this class has unbounded
width with respect to most traditional width measures studied in structural

26
A. Andrade de Melo and M. de Oliveira Oliveira
Fig. 2. An automaton F over the alphabet B({0, 1}, 2, τ), where τ is the vocabulary
of directed graphs. This automaton accepts exactly one string of length k for each
k ∈N+. For each such k, if D0 ⊗D1 is the unique string of length k accepted by F,
then the pair Dk = (D0, D1) is structural, and s(Dk) is the hypercube graph Hk. In
particular, the string D0 ⊗D1 represented in Fig. 1 is accepted by F upon following
the sequence of states q0q1q1q1q1q2.
graph theory. For instance, it can be shown that the hypercube graph Hk has
treewidth Θ(2k/
√
k) [8] and cliquewidth Ω(2k/
√
k) [5]. Therefore, Hk has also
exponential bandwidth, carving width, pathwidth, treedepth and rank-width.
Additionally, since all vertices of Hk have degree k, the degeneracy of the family
H is Θ(k). Therefore, H is not a nowhere dense class of graphs. This also implies
that the graphs in H have unbounded genus, unbounded local treewidth, etc.
5
First-Order Deﬁnable Classes of Constant Width
For each FO{τ}-sentence ψ, we let R(Σ, w, τ, ψ) denote the sub-relation of
R(Σ, w, τ) consisting of all tuples D ∈R(Σ, w, τ) whose associated structure
s(D) satisﬁes ψ, i.e. R(Σ, w, τ, ψ) .= {D ∈R(Σ, w, τ): s(D) |= ψ}. The next
theorem (Theorem 5.1), which is the main technical result of this section, states
that the relation R(Σ, w, τ, ψ) is regular.
Theorem 5.1. Let Σ be an alphabet, w ∈N+ and τ = (R1, . . . , Rl) be a rela-
tional vocabulary. For each FO{τ}-sentence ψ, the relation R(Σ, w, τ, ψ) is reg-
ular.
A constructive proof of Theorem 5.1 will be given in Sect. 6. As a consequence
of Theorem 5.1, we have that the ﬁrst-order theory of any regular-decisional class
of ﬁnite structures is decidable.

On the Width of Regular Classes of Finite Structures
27
Theorem 5.2. Let Σ be an alphabet, w ∈N+, τ = (R1, . . . , Rl) be a relational
vocabulary and C be a ﬁxed (Σ, w, τ)-regular-decisional class of ﬁnite structures.
The following problem is decidable: Given an FO{τ}-sentence ψ is there a τ-
structure A ∈C that satisﬁes ψ?
Proof. Since C is (Σ, w)-regular-decisional, there exists a regular relation R ⊆
R(Σ, w, τ) such that C = s(R) = {A: D ∈R, s(D) ≃A}. Let F be a ﬁnite
automaton over the alphabet B(Σ, w, τ) such that L(F) = lang(R). From The-
orem 5.1 it follows that there exists a ﬁnite automaton F′ over the alphabet
B(Σ, w, τ) such that L(F′) = lang(R(Σ, w, τ, ψ)). Additionally, since the proof
of Theorem 5.1 is constructive, the automaton F′ can be eﬀectively constructed
from the input representation of the FO sentence ψ. Therefore, one can decide
whether there exists a ﬁnite τ-structure A ∈C that satisﬁes ψ by simply checking
whether L(F) ∩L(F′) ̸= ∅.
⊓⊔
Let τ = (R1, . . . , Rl) be a relational vocabulary. We denote by MSO{τ} the
set of all monadic second-order logic formulas over τ, i.e. the extension of FO{τ}
that, additionally, allows variables to be used as placeholders for sets of elements
from the domain of a ﬁnite τ-structure and allows quantiﬁcation over such vari-
ables. We note that neither Theorem 5.1 nor Theorem 5.2 can be generalized to
the logic MSO{τ} for an arbitrary relational vocabulary τ. Indeed, it is well
known that the MSO theory of unlabeled grids is already undecidable [19,27]2.
For a given alphabet Γ, we let ϱ(Γ) be the vocabulary of strings over Γ. A
celebrated result of B¨uchi [6] and Elgot [13] states that a language L ⊆Γ + can
be represented by a ﬁnite automaton over Γ if and only if L can be deﬁned by
an MSO{ϱ(Γ)}-sentence. In particular, let B(Σ, w, τ) be the alphabet deﬁned
in Eq. 1, and let B(Σ, w, τ)⊛be the set of strings D0 ⊗D1 ⊗. . . ⊗Dl over
B(Σ, w, τ) such that (D0, D1, . . . , Dl) is a (Σ, w, τ)-structural tuple. Then, we
have that a language L ⊆B(Σ, w, τ)⊛is regular if and only if L can be deﬁned
by an MSO{ϱ(B(Σ, w, τ))}-sentence.
Theorem 5.3. Let Σ be an alphabet, w ∈N+ and τ = (R1, . . . , Rl) be a rela-
tional vocabulary. Given an MSO{ϱ(B(Σ, w, τ))}-sentence ϕ and an FO{τ}-
sentence ψ, one can decide whether there exists some string S = D0 ⊗D1 ⊗. . .⊗
Dl ∈B(Σ, w, τ)⊛such that S |= ϕ and s(D) |= ψ, where D = (D0, D1, . . . , Dl).
Proof. By using B¨uchi-Elgot’s Theorem, one can construct a ﬁnite automaton
F1 over B(Σ, w, τ) that accepts a string S ∈B(Σ, w, τ)+ if and only if S |= ϕ.
Now, from Theorem 5.1, we can construct a ﬁnite automaton F2 over B(Σ, w, τ)
which accepts a string D0 ⊗D1 ⊗. . . ⊗Dl ∈B(Σ, w, τ)+ if and only if D =
(D0, D1, . . . , Dl) is a (Σ, w, τ)-structural tuple and s(D) |= ψ. Let F∩be a ﬁnite
automaton that accepts the language L(F1)∩L(F2). Then, we have that L(F∩)
is non-empty if and only if there exists some string S = D0 ⊗D1 ⊗. . . ⊗Dl ∈
2 Note that Theorem 5.2 implies that the ﬁrst-order theory of unlabeled grids is decid-
able, since unlabeled grids have constant decisional-width. Nevertheless, it is known
that the ﬁrst-order theory of labeled grids is undecidable. In this latter case, we may
have labeled grids that require ODDs of arbitrarily high width to be represented.

28
A. Andrade de Melo and M. de Oliveira Oliveira
B(Σ, w, τ)⊛such that S |= ϕ and s(D) |= ψ, where D = (D0, D1, . . . , Dl). Since
emptiness is decidable for ﬁnite automata, the theorem follows.
⊓⊔
6
Proof of Theorem 5.1
We dedicate this section to the proof of Theorem 5.1. The proof follows a tra-
ditional strategy combined with new machinery for the implicit manipulation
of ODDs. More precisely, given an alphabet Σ, w ∈N+, a relational vocabu-
lary τ = (R1, . . . , Rl) and an FO{τ}-formula ψ(x1, . . . , xt) with free variables
freevar(ψ) ⊆Xt = {x1, . . . , xt}, we deﬁne R(Σ, w, τ, ψ, Xt) as the relation con-
taining precisely the tuples of the form (D0, D1, . . . , Dl, u1, . . . , ut) such that
D = (D0, D1, . . . , Dl) is (Σ, w, τ)-structural and s(D) |= ψ[u1, . . . , ut]. The
Boolean connectives ∧, ∨and ¬ and the existential quantiﬁcation ∃are handled
using closure properties from regular languages.
The technically involved part of the proof however will be the construction
of an “initial” automaton which accepts precisely those strings
D0 ⊗D1 ⊗. . . ⊗Dl ⊗u1 . . . ut
such that (D0, D1, . . . , Dl) is a (Σ, w, τ)-structural tuple, and u1, . . . , ut belong
to the domain L(D0). The diﬃculty lies in the fact that we need to guarantee
that, for each i ∈[l], the language L(Di) is contained in the tensored language
L(D0)⊗ai. This will require the introduction of new machinery for the implicit
manipulation of ODDs that may be of independent interest.
6.1
Basic General Operations
In this section, we introduce some basic low-level operations that will be used
repeatedly in the proof of Theorem 5.1. More precisely, we consider the following
operations: projection, identiﬁcation, permutation of coordinates, fold, unfold,
direct sum, union, intersection and complementation.
Let Σ be an alphabet, a ∈N+ and R ⊆(Σ+)×a be an a-ary relation.
For each permutation π: [a] →[a], we let perm(R, π) be the relation obtained
from R by permuting the coordinates of each tuple in R according to π. In other
words, perm(R, π) .=
	
sπ(1), . . . , sπ(a)

: (s1, . . . , sa) ∈R

.
For each i ∈[a], the projection of the i-th coordinate of R is deﬁned as the
(a −1)-ary relation proj(R, i) .= {(s1, . . . , si−1, si+1, . . . , sa): (s1, . . . , sa) ∈R}
obtained from R by removing the i-th coordinate of each tuple in R. More
generally, for each J ⊆[a], we let proj(R, J) denote the relation obtained from
R by removing all the i-th coordinates of each tuple in R, where i ∈J.
For each i, j ∈[a], the identiﬁcation of the i-th and j-th coordinates of R is
deﬁned as the relation ident(R, i, j) .= {(s1, . . . , sa) ∈R: si = sj} obtained from
R by removing each tuple (s1, . . . , sa) ∈R such that si ̸= sj. More generally, for
each J ⊆[a] × [a], we let ident(R, J) .= {(s1, . . . , sa) ∈R: si = sj, (i, j) ∈J}.
For each i, j ∈[a], with i ≤j, we let
fold(R, i, j) .= {(s1, . . . , si−1, si ⊗· · · ⊗sj, sj+1, . . . , sa): (s1, . . . , sa) ∈R}.

On the Width of Regular Classes of Finite Structures
29
On the other hand, if R = fold(R′, i, j) for some relation R′ and some i, j ∈[a],
with i ≤j, then we let unfold(R, i) = R′, i.e. the inverse operation of fold.
Let Σ1 and Σ2 be two alphabets, a1, a2 ∈N+, R1 ⊆(Σ+
1 )×a1 be an a1-ary
relation and R2 ⊆(Σ+
2 )×a2 be an a2-ary relation. If R1 and R2 are non-empty,
then we deﬁne the direct sum of R1 with R2 as the (a1 + a2)-ary relation
R1 ⊕R2 .=
	
s1, . . . , sa1, s′
1, . . . , s′
a2

: (s1, . . . , sa1) ∈R1,

s′
1, . . . , s′
a2

∈R2

.
Otherwise, we let R1 ⊕∅.= R1 and ∅⊕R2 .= R2.
Proposition 6.1. Let Σ1 and Σ2 be two alphabets, a1, a2 ∈N+, R1 ⊆(Σ+
1 )×a1
be a regular a1-ary relation and R2 ⊆(Σ+
2 )×a2 be a regular a2-ary relation. The
following closure properties are held:
1. for each permutation π: [a1] →[a1], perm(R1, π) is regular;
2. for each J ⊆[a1], proj(R1, J) is regular;
3. for each J ⊆[a1] × [a1], ident(R1, J) is regular;
4. for each i, j ∈[a1], with i ≤j, fold(R1, i, j) is regular;
5. if R1 = fold(R′
1, i, j) for some relation R′
1 and some i, j ∈[a1], with i ≤j,
then unfold(R1, i) = R′
1 is regular;
6. R1 ⊕R2 is regular.
Besides the operations described above, it is worth noting that, if R1 and R2
have the same arity, i.e. a1 = a2, then the union R1 ∪R2 and the intersection
R1 ∩R2 of R1 and R2 are regular relations. Moreover, if R ⊆(Σ+)×a is a regular
a-ary relation, then the complement ¬R .= (Σ+)×a \ R of R is also a regular
a-ary relation.
6.2
Core Relations
In this subsection we introduce some non-standard relations and prove that these
relations are regular. Intuitively, these relations will be used to implicitly manip-
ulate tuples of ODDs, and in particular to construct a ﬁnite automaton accepting
a string D0 ⊗D1 ⊗. . . ⊗Dl ⊗u1 . . . ut if and only if the tuple (D0, D1, . . . , Dl)
is (Σ, w, τ)-structural and u1, . . . , ut belong to the domain L(D0).
Proposition 6.2. For each alphabet Σ and each w
∈
N+, the language
B(Σ, w)⊛is regular.
Let Σ be an alphabet and w ∈N+. We let R∈(Σ, w) be the relation deﬁned
as follows: R∈(Σ, w) .=
	
(D, s): D ∈B(Σ, w)⊛, s ∈L(D)

.
Proposition 6.3. For each alphabet Σ and each w ∈N+, the relation R∈(Σ, w)
is regular.
Proof. Consider the ﬁnite automaton F over the alphabet B(Σ, w) ⊗(Σ ⊎{□})
deﬁned in the following way:
– Q(F) = {qI} ∪{qB,[p,σ,q] : B ∈B(Σ, w), (p, σ, q) ∈T(B)};

30
A. Andrade de Melo and M. de Oliveira Oliveira
– T(F) =

(qI, (B, σ), qB,[p,σ,q]): qB,[p,σ,q] ∈Q(F) \ {qI}, p ∈I(B)

∪

(qB′,[p′,ν,q′], (B, σ), qB,[p,σ,q]): qB′,[p′,ν,q′], qB,[p,σ,q] ∈Q(F) \ {qI},
ℓ(B) = r(B′), φ(B′) = 0, ι(B) = 0, p = q′
;
– I(F) = {qI}; F(F) =
	
qB,[p,σ,q] ∈Q(F) \ {qI}: q ∈F(B)

.
One can verify that L(F) = {D ⊗s: D ∈B(Σ, w)⊛, s ∈L(D)}. In other words,
rel(L(F)) = R∈(Σ, w). Therefore, R∈(Σ, w) is a regular relation.
⊓⊔
Lemma 6.4. For each alphabet Σ and each w, c ∈N+, the following relations
are regular:
1.
R∈(Σ, w, c) .=
	
(D, s1, . . . , sc): D ∈B(Σ, w)⊛, s1, . . . , sc ∈L(D)

;
2.
R(Σ, w, c) .= {(D, s1, . . . , sc): D ∈B(Σ, w)◦k, s1, . . . , sc ∈Σ≤k, k ∈N+};
3.
R̸∈(Σ, w, c) .=
	
(D, s1, . . . , sc): D ∈B(Σ, w)◦k, s1, . . . , sc ∈Σ≤k,
si ̸∈L(D) for some i ∈[c], k ∈N+

.
Lemma 6.5. For each two alphabets Σ1 and Σ2 and each w ∈N+, the relation
R(Σ1, Σ2, w) .= {(D, D′): D ∈B(Σ1, w)⊛, D′ ∈B(Σ2, w)⊛} is regular.
Proof. It follows from Proposition 6.2 that B(Σ1, w)⊛and B(Σ2, w)⊛are regu-
lar languages. In other words, R1 = B(Σ1, w)⊛and R2 = B(Σ2, w)⊛are regu-
lar unary relations. Therefore, R(Σ1, Σ2, w) is regular, since it may be simply
regarded as the regular relation R1 ⊕R2.
⊓⊔
Let Σ be an alphabet and w, a ∈N+. We let R⊆(Σ, w, a) be the relation
deﬁned as follows:
R⊆(Σ, w, a) .=
	
(D, D′): D ∈B(Σ, w)⊛, D′ ∈B(Σ⊗a, w)⊛, L(D′) ⊆L(D)⊗a
.
Proposition 6.6. For each alphabet Σ and each w, a ∈N+, the relation
R⊆(Σ, w, a) is regular.
Proof. Consider the relation R = fold( R̸∈(Σ, w, a), 2, a+1)⊕R∈(Σ⊗a, w). Note
that, R consists of all tuples of the form (D, s1 ⊗· · · ⊗sa, D′, s′
1 ⊗· · · ⊗s′
a) sat-
isfying the following conditions:
– D ∈B(Σ, w)◦k, s1, . . . , sa ∈Σ≤k but, for some i ∈[a], si ̸∈L(D);
– D′ ∈B(Σ, w)◦k′, and s′
1 ⊗· · · ⊗s′
a ∈L(D′),
where k, k′ ∈N+. Let R′ = ident(R, 2, 4). By deﬁnition, R′ is the sub-relation of
R comprised of all tuples (D, s1 ⊗· · ·⊗sa, D′, s′
1 ⊗· · ·⊗s′
a) ∈R such that si = s′
i
for each i ∈[a]. Thus, proj(R′, {2, 4}) consists of all tuples (D, D′) ∈B(Σ, w)◦k ×
B(Σ⊗a, w)◦k′ such that there exist s1, . . . , sa ∈Σ≤k with s1 ⊗· · · ⊗sa ∈L(D′)
but, for some i ∈[a], si ̸∈L(D), where k, k′ ∈N+. In other words, we have that
proj(R′, {2, 4}) =
	
(D, D′): D ∈B(Σ, w)⊛, D′ ∈B(Σ⊗a, w)⊛, L(D′) ⊈L(D)⊗a
,
Now, let R(Σ, Σ⊗a, w) = {(D, D′): D ∈B(Σ, w)⊛, D′ ∈B(Σ⊗a, w)⊛}. By
Lemma 6.5, R(Σ, Σ⊗a, w) is regular. Therefore, R⊆(Σ, w, a) is regular, since it
may be simply regarded as the relation R(Σ, Σ⊗a, w) ∩¬proj(R′, {2, 4}).
⊓⊔

On the Width of Regular Classes of Finite Structures
31
6.3
Regular-Structural Relations
Let Σ be an alphabet, w ∈N+, τ = (R1, . . . , Rl) be a relational vocabulary and
t ∈N. We let R(Σ, w, τ, t) be the relation deﬁned as follows:
R(Σ, w, τ, t) .=
	
(D0, D1, . . . , Dl, u1, . . . , ut):
(D0, D1, . . . , Dl) ∈R(Σ, w, τ), u1, . . . , ut ∈L(D0)

.
In particular, note that, if t = 0, then R(Σ, w, τ, t) = R(Σ, w, τ).
Lemma 6.7. Let Σ be an alphabet, w ∈N+, τ = (R1, . . . , Rl) be a relational
vocabulary and t ∈N. The relation R(Σ, w, τ, t) is regular.
Proof. Consider the relations R = R⊆(Σ, w, a1)⊕· · ·⊕R⊆(Σ, w, al)⊕R∈(Σ, w, t)
and R′ = ident(R, {(2i + 1, 2i + 3): i ∈l}). One can verify that R′ consists of
tuples of the form (D0, D1, . . . , D0, Dl, D0, u1, . . . , ut) satisfying the conditions:
(D0, D1, . . . , Dl) ∈R(Σ, w, τ) and u1, . . . , ut ∈L(D0). Therefore, R(Σ, w, τ, t)
is regular, since it may regarded as the relation proj(R′, {2i + 1: i ∈[l]}).
⊓⊔
Let ψ(x1, . . . , xt) be an FO{τ}-formula, with t ∈N. If Xt = {x1, . . . , xt},
then we let R(Σ, w, τ, ψ, Xt) denote the relation deﬁned as follows:
R(Σ, w, τ, ψ, Xt) .=
	
(D0, D1, . . . , Dt, u1, . . . , ut) ∈R(Σ, w, τ, t):
s(D0, D1, . . . , Dt) |= ψ[u1, . . . , ut]}.
We remark that, if t = 0, then R(Σ, w, τ, ψ, Xt) coincides with the relation
R(Σ, w, τ, ψ), in which ψ is an FO{τ}-sentence.
Finally, we show in Theorem 6.8 that, for each FO{τ}-formula ψ(x1, . . . , xt),
the relation R(Σ, w, τ, ψ, Xt) is regular. As a result, we obtain that R(Σ, w, τ, ψ)
is also a regular relation, concluding the proof of Theorem 5.1. We note that
Theorem 6.8 is proven by induction on the structure of the input formula
ψ(x1, . . . , xt). The base case follows straightforwardly from the results proven
in this section, the induction step follows from the fact that regular languages
are closed under, negation, union, intersection and projection.
Theorem 6.8. Let Σ be an alphabet, w ∈N+, τ = (R1, . . . , Rl) be a relational
vocabulary and ψ(x1, . . . , xt) be an FO{τ}-formula, with t ∈N. The relation
R(Σ, w, τ, ψ, Xt) is regular, where Xt = {x1, . . . , xt}.
7
Counting Satisfying Assignments
Let τ = (R1, . . . , Rl) be a relational vocabulary, ψ(x1, . . . , xt) be an FO{τ}-
formula with freevar(ψ) ⊆{x1, . . . , xt}, and let A be a ﬁnite τ-structure. We
say that an assignment (u1, . . . , ut) of elements from the domain R0(A) to the
free occurrences of the variables x1, . . . , xt satisﬁes ψ with respect to A if A |=
ψ[u1, . . . , ut].

32
A. Andrade de Melo and M. de Oliveira Oliveira
The next theorem states that, if we are given a (Σ, w, τ)-structural tuple D
such that s(D) = A, then the problem of counting the number of assignments
to the free occurrences of the variables x1, . . . , xt that satisfy ψ with respect to
A is ﬁxed parameter tractable when parameterized by Σ, w, τ, ψ and t. More
speciﬁcally, such a problem can be solved in time quadratic in the length of the
(Σ, w, τ)-structural tuple D. We remark that, in most applications, the param-
eters Σ, τ, ψ, t are naturally already ﬁxed, and therefore the only complexity
parameter that is indeed relevant in these situations is the width w.
Theorem 7.1. Let Σ be an alphabet, w ∈N+, τ = (R1, . . . , Rl) be a relational
vocabulary and ψ(x1, . . . , xt) be an FO{τ}-formula. Given a (Σ, w, τ)-structural
tuple D = (D0, D1 . . . Dl) of length k, one can count in time f(Σ, w, τ, ψ, t) · k2,
for some computable function f, the number of assignments that satisfy ψ with
respect to s(D).
8
Conclusion
In this work we have introduced the notion of decisional-width of a relational
structure, as a measure that intuitively tries to provide a quantiﬁcation of how
diﬃcult it is to deﬁne the relations of the structure. Subsequently we provided
a suitable way of deﬁning inﬁnite classes of structures of small width. Interest-
ingly, there exist classes of structures of constant width that have very high width
with respect to most width measures deﬁned so far in structural graph theory.
As an example, we have shown that the class of hypercube graphs has regular-
decisional width 2, while it is well known that they have unbounded treewidth
and cliquewidth. Additionally, this family is not nowhere-dense. Therefore, ﬁrst-
order model-checking and validity-testing techniques developed for these well
studied classes of graphs do not generalize to graphs of constant decisional
width. Other examples of families of graphs of constant decisional width are
paths, cliques (which have unbounded treewidth), unlabeled grids (which have
unbounded treewidth and cliquewidth) and many others. It is interesting to
note that these mentioned classes have all a regular structure and, therefore, are
“easy” to describe.
Acknowledgments. Alexsander Andrade de Melo acknowledges support from the
Brazilian agencies CNPq/GD 140399/2017-8 and CAPES/PDSE 88881.187636/2018-
01; and Mateus de Oliveira Oliveira acknowledges support from the Bergen Research
Foundation and from the Research Council of Norway.
References
1. Blumensath, A.: Automatic structures. Diploma thesis. Rheinisch-Westf¨alische
Technische Hochschule Aachen (1999)
2. Blumensath, A., Gradel, E.: Automatic structures. In: Proceedings of the 15th
Annual IEEE Symposium on Logic in Computer Science, pp. 51–62. IEEE (2000)

On the Width of Regular Classes of Finite Structures
33
3. Bollig, B.: On symbolic OBDD-based algorithms for the minimum spanning tree
problem. Theoret. Comput. Sci. 447, 2–12 (2012)
4. Bollig, B.: On the width of ordered binary decision diagrams. In: Zhang, Z., Wu, L.,
Xu, W., Du, D.-Z. (eds.) COCOA 2014. LNCS, vol. 8881, pp. 444–458. Springer,
Cham (2014). https://doi.org/10.1007/978-3-319-12691-3 33
5. Bonomo, F., Grippo, L.N., Milaniˇc, M., Safe, M.D.: Graph classes with and without
powers of bounded clique-width. Discrete Appl. Math. 199, 3–15 (2016)
6. B¨uchi, J.R.: Weak second order arithmetic and ﬁnite automata. Z. Math. Logik
Grundl. Math. 6, 66–92 (1960)
7. Bulatov, A.A.: Graphs of relational structures: restricted types. In: Proceedings
of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science, pp.
642–651. ACM (2016)
8. Chandran, L.S., Kavitha, T.: The treewidth and pathwidth of hypercubes. Discrete
Math. 306(3), 359–365 (2006)
9. Courcelle, B.: The monadic second-order logic of graphs. I. Recognizable sets of
ﬁnite graphs. Inf. Comput. 85(1), 12–75 (1990)
10. Courcelle, B., Makowsky, J.A., Rotics, U.: Linear time solvable optimization prob-
lems on graphs of bounded clique-width. Theory Comput. Syst. 33(2), 125–150
(2000)
11. Ebbinghaus, H.D., Flum, J., Thomas, W.: Mathematical Logic. Springer, New York
(2013)
12. Ebbinghaus, H.D., Flum, J.: Finite Model Theory. Springer, Heidelberg (2005)
13. Elgot, C.C.: Decision problems of ﬁnite automata and related arithmetics. Trans.
Am. Math. Soc. 98, 21–52 (1961)
14. Farb, B.: Automatic groups: a guided tour. Enseign. Math. (2) 38(3–4), 291–313
(1992)
15. Getoor, L., Friedman, N., Koller, D., Taskar, B.: Learning probabilistic models of
relational structure. In: ICML, vol. 1, pp. 170–177 (2001)
16. Grohe, M.: Algorithmic meta theorems. In: Broersma, H., Erlebach, T., Friedetzky,
T., Paulusma, D. (eds.) WG 2008. LNCS, vol. 5344, p. 30. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-92248-3 3
17. Grohe, M.: Algorithmic meta theorems for sparse graph classes. In: Hirsch, E.A.,
Kuznetsov, S.O., Pin, J.´E., Vereshchagin, N.K. (eds.) CSR 2014. LNCS, vol. 8476,
pp. 16–22. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-06686-8 2
18. Hachtel, G.D., Somenzi, F.: A symbolic algorithm for maximum ﬂow in 0-1
networks. In: Proceedings of the 1993 IEEE/ACM International Conference on
Computer-Aided Design, pp. 403–406. IEEE Computer Society Press (1993)
19. Hlinˇen`y, P., Seese, D.: Trees, grids, and MSO decidability: from graphs to matroids.
Theoret. Comput. Sci. 351(3), 372–393 (2006)
20. Khoussainov, B., Minnes, M.: Three lectures on automatic structures. In: Proceed-
ings of Logic Colloquium, pp. 132–176 (2007)
21. Khoussainov, B., Nerode, A.: Automatic presentations of structures. In: Leivant,
D. (ed.) LCC 1994. LNCS, vol. 960, pp. 367–392. Springer, Heidelberg (1995).
https://doi.org/10.1007/3-540-60178-3 93
22. Kolaitis, P.G., Vardi, M.Y.: A game-theoretic approach to constraint satisfaction.
In: AAAI/IAAI, pp. 175–181 (2000)
23. Kreutzer, S.: Algorithmic meta-theorems. In: Grohe, M., Niedermeier, R. (eds.)
IWPEC 2008. LNCS, vol. 5018, pp. 10–12. Springer, Heidelberg (2008). https://
doi.org/10.1007/978-3-540-79723-4 3

34
A. Andrade de Melo and M. de Oliveira Oliveira
24. Kruckman, A., Rubin, S., Sheridan, J., Zax, B.: A Myhill-Nerode theorem for
automata with advice. In: Proceedings of GANDALF 2012. Electronic Proceedings
in Theoretical Computer Science, vol. 96, pp. 238–246 (2012)
25. Poon, H., Domingos, P.M., Sumner, M.: A general method for reducing the com-
plexity of relational inference and its application to MCMC. In: AAAI, vol. 8, pp.
1075–1080 (2008)
26. Sawitzki, D.: Implicit ﬂow maximization by iterative squaring. In: Van Emde
Boas, P., Pokorn´y, J., Bielikov´a, M., ˇStuller, J. (eds.) SOFSEM 2004. LNCS,
vol. 2932, pp. 301–313. Springer, Heidelberg (2004). https://doi.org/10.1007/978-
3-540-24618-3 26
27. Seese, D.: The structure of the models of decidable monadic theories of graphs.
Ann. Pure Appl. Log. 53(2), 169–195 (1991)
28. Sutskever, I., Tenenbaum, J.B., Salakhutdinov, R.R.: Modelling relational data
using Bayesian clustered tensor factorization. In: Advances in Neural Information
Processing Systems, pp. 1821–1828 (2009)
29. Woelfel, P.: Symbolic topological sorting with OBDDs. J. Discrete Algorithms 4(1),
51–71 (2006)
30. Zaid, F.A., Gr¨adel, E., Reinhardt, F.: Advice automatic structures and uniformly
automatic classes. In: 26th EACSL Annual Conference on Computer Science Logic
(CSL 2017). LIPIcs, vol. 82, pp. 35:1–35:20 (2017)

Extending SMT Solvers to Higher-Order
Logic
Haniel Barbosa1, Andrew Reynolds1, Daniel El Ouraoui2(B), Cesare Tinelli1,
and Clark Barrett3
1 The University of Iowa, Iowa City, USA
2 University of Lorraine, CNRS, Inria, and LORIA, Nancy, France
daniel.elouraoui@gmail.com
3 Stanford University, Stanford, USA
Abstract. SMT solvers have throughout the years been able to cope
with increasingly expressive formulas, from ground logics to full ﬁrst-
order logic (FOL). In contrast, the extension of SMT solvers to higher-
order logic (HOL) is mostly unexplored. We propose a pragmatic exten-
sion for SMT solvers to support HOL reasoning natively without com-
promising performance on FOL reasoning, thus leveraging the extensive
research and implementation eﬀorts dedicated to eﬃcient SMT solving.
We show how to generalize data structures and the ground decision pro-
cedure to support partial applications and extensionality, as well as how
to reconcile quantiﬁer instantiation techniques with higher-order vari-
ables. We also discuss a separate approach for redesigning an HOL SMT
solver from the ground up via new data structures and algorithms. We
apply our pragmatic extension to the CVC4 SMT solver and discuss a
redesign of the veriT SMT solver. Our evaluation shows they are com-
petitive with state-of-the-art HOL provers and often outperform the tra-
ditional encoding into FOL.
1
Introduction
Higher-order (HO) logic is a pervasive setting for reasoning about numerous real-
world applications. In particular, it is widely used in proof-assistants (also known
as interactive theorem provers) to provide trustworthy, formal, and machine-
checkable proofs of theorems. A major challenge in these applications is to auto-
mate as much as possible the production of these formal proofs, thereby reduc-
ing the burden of proof on the users. An eﬀective approach to achieve stronger
automation in proof assistants is to rely on less expressive but more automatic
theorem provers to discharge some of the proof obligations. Systems such as
HOLyHammer, MizAR, Sledgehammer, and Why3, which provide a one-click
connection from proof-assistants to ﬁrst-order (FO) provers, have led in recent
This work was partially supported by the National Science Foundation under award
1656926.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 35–54, 2019.
https://doi.org/10.1007/978-3-030-29436-6_3

36
H. Barbosa et al.
years to considerable improvements in proof-assistant automation [14]. A simi-
lar layered approach is also used by automatic HO provers such as Leo-III [43]
and Satallax [17], which regularly invoke FO provers to discharge intermedi-
ate goals that depend solely on FO reasoning. However, as noted in previous
work [12,30,48], in both cases the reduction to FOL has its own disadvantages:
full encodings into FO, such as those performed by the hammers, may lead to
issues with performance, soundness, or completeness. On the other hand, the
combination of FO and HO reasoning in automatic HO provers may suﬀer from
the HO prover itself having to perform substantial FO reasoning, since it is not
optimized for FO proving. This would be the case in HO problems with a large
FO component, which occur often in practice. We aim to overcome these short-
comings by extending Satisﬁability Modulo Theories (SMT) [8] solvers, a class
of highly successful automatic FO provers, to natively support HOL.
The two main challenges for extending SMT solvers to HOL lie in dealing
with partial function applications and with functional variables, i.e., quantiﬁer
variables of higher-order type. The former mainly aﬀects term representation and
core algorithms, which in FOL are based on the fact that all function symbols
are fully applied. The latter impacts quantiﬁer instantiation techniques, which
must now account for quantiﬁed variables occurring in function symbol positions.
Moreover, often HO problems can only be proven if functional variables are
instantiated with synthesized λ-terms, typically via HO uniﬁcation [23], which
is undecidable in general.
Contributions.
We present two approaches for extending SMT solvers to
natively support HO reasoning (HOSMT). The ﬁrst one, the pragmatic app-
roach (Sect. 3), targets existing state-of-the-art SMT solvers with large code
bases and complex data structures optimized for the FO case. In this approach,
we extend a solver with only minimal modiﬁcations to its core data structures
and algorithms. In the second approach, the redesign approach (Sect. 4), we
rethink a solver’s data structures and develop new algorithms aimed speciﬁcally
at HO reasoning. This approach may lead to better results but is better suited to
lightweight solvers, i.e., less optimized solvers with a smaller code base. Moreover,
this approach provides more ﬂexibility to later develop new techniques especially
suited for higher-order reasoning. A common theme of both approaches is that
the instantiation algorithms are not extended with HO uniﬁcation. This is a sig-
niﬁcant enough challenge that we plan to explore in a later phase of this work.
We include proofs, more examples, and related work in a technical report [5].
We present an extensive experimental evaluation (Sect. 5) of our pragmatic
and redesign approaches as implemented respectively in the state-of-the-art
SMT solver CVC4 [6] and the lightweight solver veriT [16]. Besides compar-
isons against state-of-the-art HO provers, we also evaluate these solvers against
themselves, comparing a native HO encoding using the extensions in this paper
to the base versions of the solvers with the more traditional FO encoding (not
using the extensions).

Extending SMT Solvers to Higher-Order Logic
37
Related Work. The pioneering work of Robinson [41] on using a translation to
reduce higher-order reasoning to ﬁrst-order logic inspired the successful tools
such as Sledgehammer [36] and CoqHammer [19] that build on this idea by
automating HO reasoning via automatic FO provers. Earlier works on native HO
proving are, e.g., Andrews’s higher-order resolution [1] and Kohlhase’s higher-
order tableau [29], inspire the modern day HO provers such as LEO-II [11] and
Leo-III [43], implementing variations of HO resolution, and Satallax [17], based
on a HO tableau calculus guided by a SAT solver. Our approach however is
conceptually closer to recent work by Blanchette et al. [9,48] on gracefully gen-
eralizing the superposition calculus [2,33] to support higher-order reasoning.
As a ﬁrst step, they have targeted the λ-free fragment of higher-order logic,
presenting a refutationally complete calculus [9] and an initial implementation
as a prototype extension of the Zipperposition prover [18]. More recently they
integrated their approach into the state-of-the-art FO prover E [48], showing
competitive results against state-of-the-art HO provers. Their next step, as is
ours, is to extend their calculus to superposition with λ-terms while preserving
their completeness guarantees.
2
Preliminaries
Our monomorphic higher-order language L
is deﬁned in terms of right-
associative binary sort constructors →, × and pairwise-disjoint countably inﬁnite
sets S, X and F, of atomic sorts, variables, and function symbols, respectively.
We use the notations ¯an and ¯a to denote the tuple (a1, . . . , an) or the cross
product a1 × · · · × an, depending on context, with n ≥0. We extend this nota-
tion to pairwise binary operations over tuples in the natural way. A sort τ is
either an element of S or a functional sort ¯τn →τ from sorts ¯τn = τ1 × · · · × τn
to sort τ. The elements of X and F are annotated with sorts, so that x : τ is a
variable of sort τ and f : ¯τn →τ is an n-ary function symbol of sort ¯τn →τ. We
identify function symbols of sort ¯τ0 →τ with function symbols of sort τ, which
we call constants when τ is not a functional sort. Whenever convenient, we drop
the sort annotations when referring to symbols.
The set of terms is deﬁned inductively: every variable x : τ is a term of sort
τ. For variables ¯xn : ¯τn and a term t : τ of sort τ, the expression λ¯xn. t is a term
of sort ¯τn →τ, called a λ-abstraction, with bound variables ¯xn and body t. A
variable occurrence is free in a term if it is not bound by a λ-abstraction. For a
term t : ¯τn →τ and terms t1 : τ1, . . . , tm : τm with m ≤n, the expression f(¯tn) is
a term, called an application of f, the head of the application, to the arguments
¯tm. The application is total and has sort τ if m = n; it is partial and has sort
τm+1 × · · · × τn →τ if m < n. A λ-application is an application whose head is a
λ-abstraction. The subterm relation is deﬁned recursively: a term is a subterm of
itself; if a term is an application, all subterms of its arguments are also its sub-
terms. Note this is not the standard deﬁnition of subterms in HOL, which also
includes application heads and all partial applications. The set of all subterms
in a term t is denoted by T(t). We assume S contains a sort o, the Boolean

38
H. Barbosa et al.
sort, and that F contains Boolean constants ⊤, ⊥, a Boolean unary function ¬,
Boolean binary functions ∧, ∨, and, for every sort τ, a family of equality sym-
bols ≃: τ × τ →o and a family of symbols ite : o × τ × τ →τ. These symbols
are interpreted in the usual way as, respectively, logical constants, connectives,
identity, and if-then-else (ITE). We refer to terms of sort o as formulas and to
terms of sort ¯τ →o as predicates. An atom is a total predicate application. A
literal or constraint is an atom or its negation. We assume the language con-
tains the ∀and ∃binders over formulas, deﬁned as usual, in addition to the λ
binder. A formula or a term is ground if it is binder-free. We use the symbol =
for syntactic equality on terms. We reserve the names a, b, c, f, g, h, p for function
symbols; w, x, y, z for variables in general; F, G for variables of functional sort;
r, s, t, u for terms; and ϕ, ψ for formulas. The notation t[¯xn] stands for a term
whose free variables are included in the tuple of distinct variables ¯xn; t[¯sn] is the
term obtained from t by a simultaneous substitution of ¯sn for ¯xn.
We assume F contains a family @ : (¯τn →τ) × τ1 →(τ2 × · · · × τn →τ) of
application symbols for all n > 1. We use it to model (curried) applica-
tions of terms of functional sort ¯τn →τ. For example, given a function sym-
bol f : τ1 × τ2 →τ3 and application symbols @ : (τ1 × τ2 →τ3) × τ1 →(τ2 →τ3)
and @ : (τ2 →τ3) × τ2 →τ3, @(f, t1) and @(@(f, t1), t2) have, respectively, the
same denotation as λx2 : τ2.f(t1, x2) and f(t1, t2).
An applicative encoding is a well-known approach for performing HO rea-
soning using FO provers. This encoding converts every functional sort into an
atomic sort, every n-ary symbol into a nullary symbol, and uses @ to encode
applications. Thus, all applications, partial or not, become total, and quantiﬁca-
tion over functional variables becomes quantiﬁcation over regular FO variables.
We adopt Henkin semantics [10,27] with extensionality and choice, as is standard
in automatic HO theorem proving.
2.1
SMT Solvers and Quantiﬁed Reasoning
SMT solvers that process quantiﬁed formulas can be seen as containing three
main components: a preprocessing module, a ground solver, and an instantiation
module. Given an input formula ϕ, the preprocessing module applies various
transformations (for instance, Skolemization, clausiﬁcation and so on) to it to
obtain another, equisatisﬁable, formula ϕ′. The ground solver operates on the
formula ϕ′. It abstracts all of its atoms and quantiﬁed formulas and treats them
as if they were propositional variables. The solver for ground formulas provides
an assignment E ∪Q, where E is a set of ground literals and Q is a set of
quantiﬁed formulas appearing in ϕ′, such that E ∪Q propositionally entails
ϕ′. The ground solver then determines the satisﬁability of E according to a
decision procedure for a combination of background theories. If E is satisﬁable,
the instantiation module of the solver generates new instances, ground formulas
of the form ¬(∀¯x. ψ) ∨ψσ where ∀¯x. ψ is a quantiﬁed formula in Q and σ is
a substitution from the variables in ¯x to ground terms. These instances will
be, after preprocessing, added conjunctively to the input of the ground solver,
which will proceed to derive a new assignment E′ ∪Q′, if possible. This interplay

Extending SMT Solvers to Higher-Order Logic
39
may terminate either if ϕ′ is proven unsatisﬁable or if a model is found for an
assignment E ∪Q that is also a model of ϕ′.
Extending SMT solvers to HOL can be achieved by extending these three
components so that: (1) the preprocessing module eliminates λ-abstractions; (2)
the ground decision procedure supports a ground extensional logic with partial
applications, which we denote QF HOSMT; and (3) the instantiation module
instantiates variables of functional type and takes into account partial applica-
tions and equations between functions. We can perform each of these tasks prag-
matically without heavily modifying the solver, which is useful when extending
highly optimized state-of-the-art SMT solvers (Sect. 3). Alternatively, we can
perform these extensions in a more principled way by redesigning the solver,
which better suits lightweight solvers (Sect. 4).
3
A Pragmatic Extension for HOSMT
We pragmatically extend the ground SMT solver to QF HOSMT by removing
λ-expressions (Sect. 3.1), checking ground satisﬁability (Sect. 3.2), and generat-
ing models (Sect. 3.3). Extensions to the instantiation module are discussed in
Sect. 3.4.
3.1
Eliminating λ-Abstractions and Partial Applications
of Theory Symbols
To ensure that the formulas that reach the core solving algorithm are λ-free, a
preprocessing pass is used to ﬁrst eliminate λ-applications and then eliminate
any remaining λ-abstractions. The former are eliminated via β-reduction, with
each application (λ¯x. t[¯x])¯u replaced by the equivalent term t[¯u]. The substitution
renames bound variables in t as needed to avoid capture.
Two main approaches exist for eliminating (non-applied) λ-abstractions:
reduction to combinators [35] and λ-lifting [28]. Combinators allow λ-terms to be
synthesized during solving without the need for HO uniﬁcation. This translation,
however, introduces a large number of quantiﬁers and often leads to performance
loss [13, Sect. 6.4.2]. We instead apply λ-lifting in our pragmatic extension.
In λ-lifting, each λ-abstraction is replaced by a fresh function symbol, and
a quantiﬁed formula is introduced to deﬁne the symbol in terms of the original
expression. Note this is similar to the typical approach used for eliminating ITE
expressions in SMT solvers. The new function takes as arguments the variables
bound by the respective λ-abstraction and the free variables occurring in its
body. More precisely, λ-abstractions of the form λ¯xn. t[¯xn, ¯ym] of type ¯τn →τ
with ¯ym : ¯υm occurring in a formula ϕ are lifted to (possibly partial) applications
f(¯ym) where f is a fresh function symbol of type ¯υm × ¯τn →τ. Moreover, the
formula ∀¯ym¯xn. f(¯ym, ¯xn) ≃t[¯xn, ¯ym] is added conjunctively to ϕ. To minimize
the number of new functions and quantiﬁed formulas introduced, eliminated
expressions are cached so that the same deﬁnition can be reused.

40
H. Barbosa et al.
In the presence of a background theory T, the norm in SMT, a previous
preprocessing step is also needed to make all applications of theory, or inter-
preted, symbols total: each term of the form h(¯tm), where h : ¯τn →τ is a sym-
bol of T and m < n, is converted to λ¯xn−m. h(¯tm, ¯xn−m), which is then λ-
lifted as above to an uninterpreted symbol f, deﬁned by the quantiﬁed formula
∀¯y∀¯xn−m. f(¯xn−m) ≃h(¯tm, ¯xn−m), with ¯y collecting the free variables of ¯tm.
We stress that careful engineering is required to perform λ-lifting correctly
in an SMT solver not originally designed for it. For instance, using the existing
machinery for ITE removal may be insuﬃcient, since this may not properly
handle instances occurring inside binders or as the head of applications.
3.2
Extending the Ground Solver to QF HOSMT
Since we operate after preprocessing in a λ-free setting in which only unin-
terpreted functions may occur partially applied, lifting the ground solver to
QF HOSMT amounts to extending the solver for ground literals in the theory
of Equality and Uninterpreted Functions (EUF) to handle partial applications
and extensionality.
The decision procedure for ground EUF adopted by SMT solvers is based
on classical congruence closure algorithms [24,31]. While the procedure is easily
extensible to HOL (with partial applications but no λ-abstractions) via a uni-
form applicative encoding [32], many SMT solvers require that function symbols
occurring in (FO) terms be fully applied. Instead of redesigning the solver to
accommodate partial applications, we apply a lazy applicative encoding where
only partial applications are converted.
Concretely, during term construction, all partial applications are converted
to total applications by means of the binary symbol @, while fully applied terms
are kept in their regular representation. Determining the satisﬁability of a set
of EUF constraints E containing terms in both representations is done in two
phases: if E is determined to be satisﬁable by the regular ﬁrst-order procedure,
we introduce equalities between regular terms (i.e., fully applied terms without
the @ symbol) and their applicative counterpart and recheck the satisﬁability
of the resulting set of constraints. However, we only introduce these equalities
for regular terms which interact with partially applied ones. This interaction is
characterized by function symbols appearing as members of congruence classes
in the E-graph, the congruence closure of E built by the EUF decision procedure.
A function symbol occurs in an equivalence class if it is an argument of an @
symbol or if it appears in an equality between function symbols. The equalities
between regular terms and their applicative encodings are kept internal to the
E-graph, therefore not aﬀecting other parts of the ground decision procedure.
Example 1. Given f : τ × τ →τ, g, h : τ →τ and a : τ, consider the set of con-
straints E = {@(f, a) ≃g, f(a, a) ̸≃g(a), g(a) ≃h(a)}. We have that E is
initially found to be satisﬁable. However, since f and g occur partially applied,
we augment the set of constraints with a correspondence between the HO and
FO applications of f, g:

Extending SMT Solvers to Higher-Order Logic
41
E′ = E ∪{@(@(f, a), a) ≃f(a, a), @(g, a) ≃g(a)}
When determining the satisﬁability of E′, the equality @(@(f, a), a) ≃@(g, a)
will be derived by congruence and hence, f(a, a) ≃g(a) will be derived by tran-
sitivity, leading to a conﬂict. Notice that we do not require equalities between
fully applied terms whose functions do not appear in the E-graph and their
equivalent in the applicative encoding. In particular, the equality h(a) ≃@(h, a)
is not introduced in this example.
•
Fig. 1. Derivation rules for checking satisﬁability of QF HOSMT constraints in EUF.
We formalize the above procedure via the calculus in Fig. 1. The derivation
rules operate on a current set E of constraints. A derivation rule can be applied
if its premises are met. A rule’s conclusion either adds an equality literal to E
or replaces it by ⊥to indicate unsatisﬁability. A rule application is redundant if
its conclusion leaves E unchanged. A constraint set is saturated if it admits only
redundant rule applications.
Rules Refl, Sym, Trans, Cong and Conflict are standard for EUF deci-
sion procedures based on congruence closure, i.e., the smallest superset of a set
of equations that is closed under entailment in the theory of equality. The rule
App-encode equates a full application to its applicative encoding equivalent,
and it is applied only to applications of functions which occur as subterms in E.
As mentioned above, this can only be the case if the function itself appears as
an argument of an application, which happens when it is partially applied (as
argument of @ or ≃).
Rule Extensionality is similar to how extensionality is handled in decision
procedures for extensional arrays [21,44]. If two non-nullary functions are dise-
qual in E, then a witness of their disequality is introduced. The extensionality
property is characterized by the axiom ∀¯xn. f(¯xn) ≃g(¯xn) ⇔f ≃g, for all func-
tions f and g of the same type. The rule ensures the left-to-right direction of the

42
H. Barbosa et al.
axiom (the opposite one is ensured by App-encode together with the congru-
ence closure rules). To simplify the presentation we assume that, for every term
@(. . . (@(f, t1), . . .), tm) : ¯τn →τ ∈T(E), there is a fresh symbol f′ : ¯τn →τ
such that @(. . . (@(f, t1), . . .), tm) ≃f′ ∈E.
Example 2. Consider the function symbols f, g : τ →τ, a : τ, and the set of con-
straints E = {f ≃g, f(a) ̸≃g(a)}. The constraints are initially satisﬁable with
respect to the congruence closure rules, however, since f, g ∈T(E), the rule
App-encode will be applied twice to derive f(a) ≃@(f, a) and g(a) ≃@(g, a).
Then, via Cong, from f ≃g we infer @(f, a) ≃@(g, a), which leads to a conﬂict
via transitivity.
•
Decision Procedure. Any derivation strategy for the calculus that does not
stop until it saturates or generates ⊥yields a decision procedure for the satisﬁa-
bility of QF HOSMT constraints in the EUF theory, according to the following
results for the calculus.
Proposition 1 (Termination). Every sequence of non-redundant rule appli-
cations is ﬁnite.
Proposition 2 (Refutation Soundness). A constraint set is unsatisﬁable if
⊥is derivable from it.
Proposition 3 (Solution Soundness). Every saturated constraint set is sat-
isﬁable.
Even though we could apply the rules in any order, for better performance
we only apply App-Encode and Extensionality once other rules have only
redundant applications. Moreover, App-Encode has precedence over Exten-
sionality.
3.3
Model Generation for Ground Formulas
When our decision procedure for QF HOSMT saturates, it can produce a ﬁrst-
order model M as a witness for the satisﬁability of its input. Typically, the
models generated by SMT solvers for theories in ﬁrst-order logic map uninter-
preted functions f : ¯τn →τ to functions, denoted M(f), of the form
λ¯xn. ite(x1 ≃t1
1∧. . . xn ≃t1
n, s1, . . . , ite(x1 ≃tm−1
1
∧. . . xn ≃tm−1
n
, sm−1, sm) . . .)
in which every entry but the last corresponds to an application f(ti
1, . . . , ti
n),
modulo congruence, occurring in the problem. In other words, functions are
interpreted in models M as almost constant functions.
In the presence of partial applications, this scheme can sometimes lead to
functions with exponentially many entries. For example, consider the satisﬁable
formula
f1(a) ≃f1(b) ∧f1(b) ≃f2 ∧f2(a) ≃f2(b) ∧f2(b) ≃f3 ∧f3(a) ≃f3(b) ∧f3(b) ≃c

Extending SMT Solvers to Higher-Order Logic
43
in which f1 : τ × τ × τ →τ, f2 : τ × τ →τ, f3 : τ →τ, and a, b, c : τ. To produce
the model values of f1 as a list of total applications with three arguments into
an element of the interpretation of τ, we would need to account for 8 cases.
In other words, we require 8 ite cases to indicate f1(x, y, z) ≃c for all inputs
where x, y, z ∈{a, b}. The number of entries in the model is exponential on the
“depth” of the chain of functions that each partial application is equal to, which
can make model building unfeasible if just a few functions are chained as in the
above example.
To avoid such an exponential behavior, model building assigns values for func-
tions in terms of the other functions that their partial applications are equated
to. In the above example f1 would have only two model values, depending on its
application’s ﬁrst argument being a or b, by using the model values of f2 applied
on its two other arguments. In other words, we construct M(f1) as the term:
λxyz. ite(x ≃a, M(f2)(y, z), ite(x ≃b, M(f2)(y, z), ))
where M(f2) is the model for f2 and
is an arbitrary value. The model value of
f2 would be analogously built in terms of the model value of f3. This guarantees
a polynomial construction for models in terms of the number of constraints in
the problem in the presence of partial applications.
Extensionality and Finite Sorts. Model construction assigns diﬀerent values to
terms not asserted equal. Therefore, if non-nullary functions f, g : ¯τn →τ occur
as terms in diﬀerent congruence classes but are not asserted disequal, we ensure
they are assigned diﬀerent model values by introducing disequalities of the form
f( ¯skn) ̸≃g( ¯skn) for fresh ¯skn. This is necessary because model values for functions
are built based on their applications occurring in the constraint set. However,
such disequalities are only always guaranteed to be satisﬁed if ¯τn, τ are inﬁnite
sorts.
Example 3. Let E be a saturated set of constraints s.t. p1, p2, p3 : τ →o ∈T(E)
and E ̸|= p1 ≃p2 ∨p1 ≃p3 ∨p2 ≃p3 ∨p1 ̸≃p2 ∨p1 ̸≃p3 ∨p2 ̸≃p3. In
the congruence closure of E the functions p1, p2, p3 each occur in a diﬀerent
congruence class but are not asserted disequal, so a naive model construction
would, in order to build their model values, introduce disequalities p1(sk1) ̸≃
p2(sk1), p1(sk2) ̸≃p3(sk2), and p2(sk3) ̸≃p3(sk3), for fresh sk1, sk2, sk3 : τ.
However, if τ has cardinality one these disequalities make E unsatisﬁable, since
sk1, sk2, sk3 must be equal and o has cardinality 2.
•
To prevent this issue, whenever the set of constraints E is saturated, we
introduce, for every pair of functions f, g : ¯τn →τ ∈T(E) s.t. n > 0 and
E ̸|= f ≃g ∨f ̸≃g, the splitting lemma f ≃g ∨f ̸≃g. In the above example this
would amount to add the lemmas p1 ≃p2∨p1 ̸≃p2, p1 ≃p3∨p1 ̸≃p3, and p2 ≃
p3 ∨p2 ̸≃p3, thus ensuring that the decision procedure detects the inconsistency
before saturation.

44
H. Barbosa et al.
3.4
Extending the Quantiﬁer Instantiation Module to HOMST
The main quantiﬁer instantiation techniques in SMT solving are trigger-
based [22], conﬂict-based [4,38], model-based [26,40], and enumerative [37]. Lift-
ing any of them to HOSMT presents its own challenges. We focus here on extend-
ing the E-matching [20] algorithm, the keystone of trigger-based instantiation,
the most commonly used technique in SMT solvers. In this technique, instan-
tiations are chosen for quantiﬁed formulas ϕ based on triggers. A trigger is a
term (or set of terms) containing the free variables occurring in ϕ. Matching a
trigger term against ground terms in the current set of assertions E results in a
substitution that is used to instantiate ϕ.
The presence of higher-order constraints poses several challenges for E-
matching. First, notice that the @ symbol is an overloaded operator. Applications
of this symbol can be selected as terms that appear in triggers. Special care must
be taken so that applications of @ are not matched with ground applications of
@ whose arguments have diﬀerent types. Second, functions can be equated in
higher-order logic. As a consequence, a match may involve a trigger term and a
ground term with diﬀerent head symbols. Third, since we use a lazy applicative
encoding, our ground set of terms may contain a mixture of partially and fully
applied function applications. Thus, our indexing techniques must be robust to
handle combinations of the two. The following example demonstrates the last
two challenges.
Example 4. Consider E with the equality @(f, a) ≃g and the term f(a, b) where
f : τ × τ →τ and g : τ →τ. Notice that g(x) is equivalent modulo E to the
term f(a, b) under the substitution x →b. Such a match is found by indexing all
terms that are applications of either @(f, a) or g in a common term index. This
ensures when matching g(x), the term f(a, b), whose applicative counterpart is
@(@(f, a), b), is considered.
We extended the regular ﬁrst-order E-matching algorithm of CVC4 as described
in this section. Extensions to the other instantiation techniques of CVC4, such
as model-based quantiﬁer instantiation, are left as future work.
Extending Expressivity via Axioms. Even though not synthesizing λ-abstractions
prevents us from fully lifting the above instantiation techniques to HOL, we
remark that, as we see in Sect. 5, this pragmatic extension very often can prove
HO theorems, many times even at higher rates than full-ﬂedged HO provers.
Success rates can be further improved by using well-chosen axioms to prove
problems that otherwise cannot be proved without synthesizing λ-abstractions.
Example 5. Consider the ground formula ϕ = a ̸≃b with a, b of sort τ and
the quantiﬁed formula ψ = ∀F, G : τ →τ. F ≃G. Intuitively ψ states that all
functions of sort τ →τ are equal. However, this is inconsistent with ϕ, which
forces τ to contain at least two elements and therefore τ →τ to contain at
least four functions. For a prover to detect this inconsistency it must apply an

Extending SMT Solvers to Higher-Order Logic
45
instantiation like {F →(λw. a), G →(λw. b)} to ψ, which would need HO
uniﬁcation. However, adding the axiom
∀F : τ →τ. ∀x, y : τ. ∃G : τ →τ. ∀z : τ. G(z) ≃ite(z ≃x, y, F(z))
(SAX)
makes the problem provable without the need to synthesize λ-abstractions.
•
We denote the above axiom as the store axiom (SAX) because it simulates
how arrays are updated via the store operation. As we note in Sect. 5, introducing
this axiom for all functional sorts occurring in the problem often allows our
pragmatically extended solver to prove problems it would not be able to prove
otherwise. Intuitively, the reason is that instances can be generated not only
from terms in the original problem, but also from the larger set of functions
representable in the formula signature.
4
Redesigning a Solver for HOSMT
In the previous section we discussed how to address the challenges of HO rea-
soning in SMT while minimally changing the SMT solver. Alternatively, we
can redesign the solver to support HO features directly. However, this requires
a redesign of the core data structures and algorithms. We propose one such
redesign below. We again assume that the solver operates on formulas with
no λ-abstraction and no partial applications of theory symbols, which can be
achieved via preprocessing (Sect. 3.1).
4.1
Redesigning the Core Ground Solver for HOSMT
Eﬃcient implementations of the congruence closure (CC) procedure for EUF
reasoning operate on Union-Find data structures and have asymptotic time
complexity O(n log n). To accommodate partial applications, we propose a sim-
pler algorithm which operates on an E-graph where nodes are terms, and edges
are relations (equality, congruence, disequality) between them. An equivalence
class is a connected component without disequality edges. All operations on the
graph (incremental addition of new constraints, backtracking, conﬂict analysis,
proof production) are implemented straightforwardly. This simpler implementa-
tion comes at the cost of higher worse-case time complexity (the CC algorithm
becomes quadratic) but integrates better with various other features such as
term addition, support of injective functions, rewriting or even computation,
in particular for β- and η-conversion, which now can be done during solving
rather than as preprocessing. In the redesigned approach, the solver keeps two
term representations, a curried representation and a regular one. In the regular
one, partial and total applications are distinguished by type information. The
curried representation is used only by the congruence closure algorithm. It is
integrated with the rest of the solver via an interface with translation functions
curry and uncurry between the two diﬀerent representations. For conciseness,
instead of writing @(. . . (@(f, t1), . . .), tn) below, we use the curried notation
(· · · ((f t1) · · · ) tn), omitting parenthesis when unambiguous.

46
H. Barbosa et al.
Example 6. Given f : τ × τ →τ, g, h : τ →τ and a : τ, consider the constraints
{f(a) ≃g, f(a, a) ̸≃g(a), g(a) ≃h(a)}. The congruence closure module will
operate on {f a ≃g, f a a ̸≃g a, g a ≃h a}, thanks to the curry translation.
•
SMT solvers generally perform theory combination via equalities over terms
shared between diﬀerent theories. Given the diﬀerent term representations kept
between the CC procedure and the rest of the solver, to ensure that theory
combination is done properly, the redesigned core ground solver keeps track of
terms shared with other theory solvers. Whenever an equality is inferred on a
term whose translation is shared with another theory, a shared equality is sent
out in terms of the translation.
Example 7. Consider the function symbols f : Int →Int, p : Int →o, a, b,
c1, c2, c3, c4 : Int, the set of arithmetic constraints {a ≤b, b ≤a, p(f(a) −
f(b)), ¬p(0), c1 ≃c3 −c4, c2 ≃0}, and the set of curried equality constraints
E = {p c1, ¬(p c2), c3 ≃f a, c4 ≃f b}. The equalities c3 ≃f a and c4 ≃f b keep
track of the fact that f a and f b are shared. The arithmetic module deduces a ≃b,
which is added to E′ = E ∪{a ≃b}. By congruence, f a ≃f b is derived, which
propagates c3 ≃c4 to the arithmetic solver. With this new equality, arithmetic
reasoning derives c1 ≃c2, whose addition to the equality constraints produces
the unsatisﬁable constraint set E′ ∪{c1 ≃c2}.
•
Extensionality. The Extensionality rule (Fig. 1) is suﬃcient for handling
extensionality at the ground level. However, it has shortcomings when quan-
tiﬁers, even just ﬁrst-order ones, are considered, as shown in the example below.
In the redesigned solver, extensionality is better handled via axioms.
Example 8. Consider the constraints E = {h f ≃b, h g ̸≃b, ∀x. f(x) ≃a,
∀x. g(x) ≃a}, with h : τ →τ →τ, f, g : τ →τ, a, b : τ. The pragmatic solver
could prove this problem unsatisﬁable only with a ground decision procedure
that derives consequences of disequalities, since deriving f ̸≃g is necessary to
derive f(sk) ̸≃g(sk), via extensionality, which then leads to a conﬂict. But SMT
solvers are well known not to propagate all disequalities for eﬃciency consider-
ations. In contrast, with the axiom ∀F, G : ¯τn →τ. F ̸≃G ⇒F(sk1, . . . , skn) ̸≃
G(sk1, . . . , skn), the instantiation {F →f, G →g} (which may be derived, e.g.,
via enumerative instantiation, since f, g ∈T(E)), provides the splitting lemma
f ≃g ∨f(sk) ̸≃g(sk). The case E ∪{f ≃g} leads to a conﬂict by pure ground
reasoning, while the case E ∪{f sk ̸≃g sk} leads to a conﬂict from the instances
f(sk) ≃a, g(sk) ≃a of the quantiﬁed formulas in E.
•
4.2
Quantiﬁer Instantiation Module
In the pragmatic approach, the challenges for the E-matching procedure lied
in properly accounting for the @ symbol, functional equality, and the mixture
of partial and total applications, all of which lead to diﬀerent term representa-
tions, in the term indexing data structure. In the redesign approach, the second

Extending SMT Solvers to Higher-Order Logic
47
challenge remains the same, and term indexing is extended in the same manner
of Sect. 3.4 to cope with it. The ﬁrst and third challenge present themselves in
a diﬀerent way, however, since the curried representation of terms is only used
inside the E-graph of the new CC procedure. To apply E-matching properly,
term indexing is extended to perform query by types, returning all the subterms
of a given type that occur in the E-graph, but translated back to the uncurried
representation.
Example 9. Consider E = {f(a, g(b, c)) ≃a, ∀F. F(a) ≃h, ∀y. h(y) ̸≃a} and
the set of triggers {F(a), h(y)} where a, b, c : τ, h : τ →τ and f, g : τ × τ →τ.
The set of ground curried terms in E is {f a (g b c), f a, g b, g b c, f, g, a, b, c}. To
do E-matching with F(a) and h(y) the index returns the sets of uncurried sub-
terms {f(a, g(b, c)), a, g(b, c), b, c} and {f(a), g(b)} for the types τ and τ →τ,
respectively.
•
Since we do not perform HO uniﬁcation, to instantiate functional variables it
suﬃces to extend the standard E-matching algorithm applied by SMT solvers by
accounting for function applications with variable heads. When matching a term
F(¯sn) with a ground term t the procedure essentially matches F with the head
of ground terms f(¯tn) congruent to t, as long as each si in ¯sn can be matched
with each ti in ¯tn. In the above example, matching the trigger F(a) with the
term f(a) yields the substitution {F →f}.
5
Evaluation
We have implemented the above techniques in the state-of-the-art CVC4 solver
and in the lightweight veriT solvers. We distinguish between two main versions
of each solver: one that performs a full applicative encoding (Sect. 2) into FOL
a priori, denoted @cvc and @vt, and another that implements the pragmatic
(Sects. 3) or redesigned (Sect. 4) extensions to HOL within the solvers, denoted
cvc and vt. Both CVC4 modes eliminate λ-abstractions via λ-lifting. Neither
veriT conﬁguration supports benchmarks with λ-abstractions. The CVC4 con-
ﬁgurations that employ the “store axiom” (Sect. 3.4) are denoted by having the
suﬃx -sax.
We use the state-of-the-art HO provers Leo-III [43], Satallax [17,25] and
Ehoh [42,48] as baselines in our evaluation. The ﬁrst two have refutationally
complete calculi for extensional HOL with Henkin semantics, while the third
only supports λ-free HOL without ﬁrst-class Booleans. For Leo-III and Satallax
we use their conﬁgurations from the CASC competition [47], while for Ehoh we
report on their best non-portfolio conﬁguration from Vukmirovi´c et al., Ehoh
hb, [48].
We split our account between the case of proving HO theorems and that
of producing countermodels for HO conjectures since the two require diﬀerent
strengths from the system considered. We discus only two of them, CVC4 and
Satallax, for the second evaluation. The reason is that Leo-III and veriT do not
provide models and Ehoh is not model-sound with respect to Henkin semantics,

48
H. Barbosa et al.
only with respect to λ-free Henkin semantics. We ran our experiments on a
cluster equipped with Intel E5-2637 v4 CPUs running Ubuntu 16.04, providing
one core, 60 s, and 8 GB RAM for each job. The full experimental data is publicly
available.1
We consider the following sets2 of HO benchmarks: the 3,188 monomorphic
HO benchmarks in TPTP [46], split into three subsets: the 530 problems that
are both λ-free and without ﬁrst-class Booleans (TH0); the 743 that are only
λ-free (oTH0); and the 1,915 that are neither (λoTH0). The next sets are Sledge-
hammer (SH) benchmarks from the Judgment Day test harness [15], consisting
of 1,253 provable goals manually chosen from diﬀerent Isabelle theories [34] and
encoded into λ-free monomorphic HOL problems without ﬁrst-class Booleans.
The encoded problems are such that they are provable only if the original goal is.
These problems are split into four subsets, JD32
lift, JD32
combs, JD512
lift , and JD512
combs
depending, respectively, on whether they have 32 or 512 Isabelle lemmas, or
facts, and whether λ-abstractions are removed via λ-lifting or via SK-style com-
binators. The last set, λoSH1024, has 832 SH benchmarks from 832 provable goals
randomly selected from diﬀerent Isabelle theories, encoded with 1,024 facts and
preserving λs and ﬁrst-class Booleans. Considering a varying number of facts in
the SH benchmarks emulates the needs of increasingly larger problems in inter-
active veriﬁcation, while diﬀerent λ handling schemes allow us to measure from
which alternative each particular solver beneﬁts more.
We point out that our extensions of CVC4 and veriT do not signiﬁcantly
compromise their performance on FO benchmarks. The pragmatic extension of
CVC4 has virtually the same performance as the original solver on SMT-LIB [7],
the standard SMT test suite. The redesigned veriT does have a considerably
lower performance. However, while it is, for example, three times slower on the
QF UF category of SMT-LIB due to its slower ground solver for EUF, it still
performs better on this category than CVC4. This shows that despite the added
cost of supporting higher-order reasoning, the FO performance of veriT is still
on par with the state of the art.
5.1
Proving HO Theorems
The number of theorems proved by each solver conﬁguration per benchmark
set is given in Table 1. Grayed out cells represent unsupported benchmark sets.
Figure 2 compares benchmarks solved per time. It only includes benchmark sets
supported by all solvers (namely TH0 and the JD benchmarks).
As expected, the results vary signiﬁcantly between benchmark sets. Leo-
III and Satallax have a clear advantage on TPTP, which contains a signiﬁcant
number of small logical problems meant to exercise the HO features of a prover.
Considering the TPTP benchmarks from less to more expressive, i.e., including
ﬁrst-class Booleans and then λs, we see the advantages of these systems only
1 http://matryoshka.gforge.inria.fr/pubs/hosmt/.
2 Since veriT does not parse TPTP, its reported results are on the equivalent bench-
marks as translated by CVC4 into the HOSMT language [3].

Extending SMT Solvers to Higher-Order Logic
49
Table 1. Proved theorems per benchmark set. Best results are in bold.
Solver
Total
TH0
oTH0
λoTH0
JD32
lift
JD32
combs
JD512
lift
JD512
combs
λoSH1024
#
9032
530
743
1915
1253
1253
1253
1253
832
4318
384
344
940
457
459
655
667
412
4348
390
373
937
456
457
655
668
412
cvc
4232
389
342
865
463
447
667
654
405
cvc-sax
4275
389
376
883
458
443
667
654
405
Leo-III
4410
402
452
1178
491
482
609
565
231
Satallax
3961
392
457
1215
394
390
407
404
302
370
332
404
396
525
529
vt
369
346
426
424
550
556
Ehoh
394
489
481
637
630
increase. We also observe that both @cvc and cvc, but especially the latter,
beneﬁt from -sax as more complex benchmarks are considered in TPTP, showing
that the disadvantage of not synthesizing λ-abstractions can sometimes be oﬀset
by well-chosen axioms. Nevertheless, the results on λoTH0 show that this axiom
alone is far from enough to oﬀset the gap between @cvc and cvc, with cvc giving
up more often from lack of instantiations to perform.
Fig. 2. Execution times in secs on 5,543 benchmarks, from TH0 and JD, supported by
all solvers.
Sledgehammer-generated problems stem from formalization eﬀorts across dif-
ferent applications. As others note [45,48], the bottleneck in solving these prob-
lems is often scalability and eﬃcient FO reasoning, rather than a reﬁned handling
of HO constructs, especially as more facts are considered. Thus, the ability to
synthesize λ-abstractions is not suﬃcient for scalability as more facts are con-
sidered, and Ehoh and the CVC4 extensions eventually surpass the native HO
provers. In particular, in the largest set we considered, λoSH1024, both @cvc and
cvc have signiﬁcant advantages. As in λoTH0, @cvc also solves more problems

50
H. Barbosa et al.
than cvc in λoSH1024, which we attribute again to @cvc being able to perform
more instantiations than cvc On commonly solved problems, however, cvc is
often faster than @cvc, albeit by a small margin: 15% on average.
Both CVC4 conﬁgurations dominate JD512 with a signiﬁcantly margin over
Ehoh and Leo-III. Comparing the results between using λ-lifting or combinators,
the former favors cvc and the latter, @cvc. These results, as well as the previ-
ously discussed ones, indicate that for unsatisﬁable benchmarks the pragmatic
extension of CVC4 should not, in its current state, substitute an encoding-based
approach but complement it. In fact, a virtual best solver of all the CVC4 conﬁg-
urations, as well as others employing interleaved enumerative instantiation [37],
in portfolio, would solve 703 problems in JD512
lift , 702 in JD512
combs, 453 in λoSH1024,
and 408 in TH0, the most in these categories, even also considering a virtual best
solver of all Ehoh conﬁgurations from [48]. The CVC4 portfolio would also solve
482 problems in JD32
lift, and 482 in JD32
combs, doing almost as well as Leo-III, and
1,001 problems in λoTH0, The virtual best CVC4 has a success rate 3% points
higher than @cvc on Sledgehammer benchmarks, as well as overall, which rep-
resents a signiﬁcant improvement when considering the usage of these solvers as
backends for interactive theorem provers.
Diﬀerently from the pragmatic extension in CVC4, which provides more of an
alternative to the full applicative encoding, the redesigned veriT is an outright
improvement, with vt consistently solving more problems and with better solving
times than @vt, especially on harder problems, as seen by the wider separation
between them after 10s in Fig. 2. Overall, veriT’s performance, consistently with
it being a lightweight solver, lags behind CVC4 and Ehoh as bigger benchmarks
are considered. However, it is respectable compared with Leo-III’s and ahead of
Satallax’s performance, thus validating the eﬀort of redesigning the solver for
a more reﬁned handling of higher-order constructs and suggesting that further
extensions should be beneﬁcial.
5.2
Providing Countermodels to HO Conjectures
The number of countermodels found by each solver conﬁguration per benchmark
set is given in Table 2. We consider the two CVC4 extension, @cvc and cvc, run
in ﬁnite-model-ﬁnding mode (-fmf) [39]. The builtin HO support in cvc is vastly
superior to @cvc when it comes to model ﬁnding, as cvc-fmf greatly outperforms
@cvc-fmf-sax. We note that @cvc-fmf is only model-sound if combined with -sax.
Table 2. Conjectures with found countermodels per benchmark set. Best results in
bold.
Solver
Total TH0 oTH0 λoTH0 JD32
lift
JD32
combs
JD512
lift
JD512
combs
λoSH1024
#
9032
530
743
1915
1253
1253
1253
1253
832
@cvc-fmf-sax
224
58
43
80
20
18
1
1
3
cvc-fmf
482
90
17
205
93
73
1
1
2
Satallax
186
72
15
98
0
0
0
0
1

Extending SMT Solvers to Higher-Order Logic
51
Diﬀerently from cvc-fmf, which fails to provide a model as soon as it is faced with
quantiﬁcation over a functional sort, in @cvc-fmf functional sorts are encoded
as atomic sorts. Thus it needs the extra axiom to ensure model soundness. For
example, @cvc-fmf considers Example 5 satisﬁable while @cvc-fmf-sax properly
reports it unsatisﬁable.
The high number of countermodels in JD32 indicates, not surprisingly, that
providing few facts makes several SH goals unprovable. Nevertheless, it is still
useful to know where exactly the Sledgehammer generation is being “incomplete”
(i.e., making originally provable goals unprovable), something that is diﬃcult to
determine without eﬀective model ﬁnding procedures.
6
Concluding Remarks
We have presented extensions for SMT solvers to handle HOSMT problems. The
pragmatic extension of CVC4, which can be implemented in other state-of-the-
art SMT solver with similar level of eﬀort, performs similarly to the standard
encoding-based approach despite its limited support for HO instantiation. More-
over, it allows numerous new problems to be solved by CVC4, with a portfolio
approach performing very competitively and often ahead of state-of-the-art HO
provers. The redesigned veriT on the other hand consistently outperforms its
standard encoding-based counterpart, showing it can be the basis for future
advancements towards stronger HO automation.
Acknowledgments. We are grateful to Jasmin Blanchette and Pascal Fontaine for
numerous discussions throughout the development of this work, for providing funding
for research visits and for suggesting many improvements. We also thank Jasmin for
generating several of the benchmarks with which we evaluate our approach; Simon
Cruanes and Martin Riener for many fruitful discussions on the intricacies of HOL;
Andres N¨otzli for help with the table and plot scripts; Mathias Fleury, Hans-J¨org
Schurr and Sophie Tourret for suggesting many improvements. This work was partially
supported by the National Science Foundation under Award 1656926 and the European
Research Council (ERC) under starting grant Matryoshka (713999).
References
1. Andrews, P.B.: Resolution in type theory. J. Symb. Log. 36(3), 414–432 (1971)
2. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with selec-
tion and simpliﬁcation. J. Log. Comput. 4(3), 217–247 (1994)
3. Barbosa, H., Blanchette, J.C., Cruanes, S., El Ouraoui, D., Fontaine, P.: Language
and proofs for higher-order SMT (work in progress). In: Dubois, C., Paleo, B.W.
(eds.) PXTP 2017. EPTCS, vol. 262, pp. 15–22 (2017)
4. Barbosa, H., Fontaine, P., Reynolds, A.: Congruence closure with free variables.
In: Legay, A., Margaria, T. (eds.) TACAS 2017. LNCS, vol. 10206, pp. 214–230.
Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-662-54580-5 13
5. Barbosa, H., Reynolds, A., El Ouraoui, D., Tinelli, C., Barrett, C.: Extending SMT
solvers to higher-order logic. Technical report. The University of Iowa, May 2019

52
H. Barbosa et al.
6. Barrett, C., et al.: CVC4. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011.
LNCS, vol. 6806, pp. 171–177. Springer, Heidelberg (2011). https://doi.org/10.
1007/978-3-642-22110-1 14
7. Barrett, C., Fontaine, P., Tinelli, C.: The SMT-LIB standard: version 2.6. Technical
report. Department of Computer Science, The University of Iowa (2017)
8. Barrett, C., Sebastiani, R., Seshia, S., Tinelli, C.: Satisﬁability modulo theories,
Chap. 26. In: Biere, A., Heule, M.J.H., van Maaren, H., Walsh, T. (eds.) Handbook
of Satisﬁability. FAIA, vol. 185, pp. 825–885. IOS Press (2009)
9. Bentkamp, A., Blanchette, J.C., Cruanes, S., Waldmann, U.: Superposition for
lambda-free higher-order logic. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.)
IJCAR 2018. LNCS, vol. 10900, pp. 28–46. Springer, Cham (2018). https://doi.
org/10.1007/978-3-319-94205-6 3
10. Benzm¨uller, C., Miller, D.: Automation of higher-order logic. In: Siekmann, J.H.
(ed.) Computational Logic. Handbook of the History of Logic, vol. 9, pp. 215–254.
Elsevier (2014)
11. Benzm¨uller, C., Sultana, N., Paulson, L.C., Theiss, F.: The higher-order prover
LEO-II. J. Autom. Reason. 55, 389–404 (2015)
12. Bhayat, A., Reger, G.: Set of support for higher-order reasoning. In: Konev, B.,
Urban, J., R¨ummer, P. (eds.) PAAR-2018. CEUR Workshop Proceedings, vol.
2162, pp. 2–16. CEUR-WS.org (2018)
13. Blanchette, J.C.: Automatic proofs and refutations for higher-order logic. Ph.D.
thesis. Technical University Munich (2012)
14. Blanchette, J.C., Kaliszyk, C., Paulson, L.C., Urban, J.: Hammering towards QED.
J. Formaliz. Reason. 9(1), 101–148 (2016)
15. B¨ohme, S., Nipkow, T.: Sledgehammer: judgement day. In: Giesl, J., H¨ahnle, R.
(eds.) IJCAR 2010. LNCS, vol. 6173, pp. 107–121. Springer, Heidelberg (2010).
https://doi.org/10.1007/978-3-642-14203-1 9
16. Bouton, T., Caminha B. de Oliveira, D., D´eharbe, D., Fontaine, P.: veriT: an open,
trustable and eﬃcient SMT-solver. In: Schmidt, R.A. (ed.) CADE 2009. LNCS,
vol. 5663, pp. 151–156. Springer, Heidelberg (2009). https://doi.org/10.1007/978-
3-642-02959-2 12
17. Brown, C.E.: Satallax: an automatic higher-order prover. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS, vol. 7364, pp. 111–117. Springer, Hei-
delberg (2012). https://doi.org/10.1007/978-3-642-31365-3 11
18. Cruanes, S.: Superposition with structural induction. In: Dixon, C., Finger, M.
(eds.) FroCoS 2017. LNCS, vol. 10483, pp. 172–188. Springer, Cham (2017).
https://doi.org/10.1007/978-3-319-66167-4 10
19. Czajka, L., Kaliszyk, C.: Hammer for Coq: automation for dependent type theory.
J. Autom. Reason. 61, 423–453 (2018)
20. de Moura, L., Bjørner, N.: Eﬃcient E-matching for SMT solvers. In: Pfenning,
F. (ed.) CADE 2007. LNCS, vol. 4603, pp. 183–198. Springer, Heidelberg (2007).
https://doi.org/10.1007/978-3-540-73595-3 13
21. de Moura, L., Bjørner, N.: Generalized, eﬃcient array decision procedures. In:
FMCAD 2009, pp. 45–52. IEEE (2009)
22. Detlefs, D., Nelson, G., Saxe, J.B.: Simplify: a theorem prover for program check-
ing. J. ACM 52, 365–473 (2005)
23. Dowek, G.: Higher-order uniﬁcation and matching. In: Robinson, J.A., Voronkov,
A. (eds.) Handbook of Automated Reasoning, vol. II, pp. 1009–1062. Elsevier and
MIT Press (2001)
24. Downey, P.J., Sethi, R., Tarjan, R.E.: Variations on the common subexpression
problem. J. ACM 27, 758–771 (1980)

Extending SMT Solvers to Higher-Order Logic
53
25. F¨arber, M., Brown, C.: Internal guidance for Satallax. In: Olivetti, N., Tiwari, A.
(eds.) IJCAR 2016. LNCS, vol. 9706, pp. 349–361. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-40229-1 24
26. Ge, Y., de Moura, L.: Complete instantiation for quantiﬁed formulas in satisﬁ-
abiliby modulo theories. In: Bouajjani, A., Maler, O. (eds.) CAV 2009. LNCS,
vol. 5643, pp. 306–320. Springer, Heidelberg (2009). https://doi.org/10.1007/978-
3-642-02658-4 25
27. Henkin, L.: Completeness in the theory of types. J. Symb. Log. 15(2), 81–91 (1950)
28. Hughes, R.J.M.: Super combinators: a new implementation method for applicative
languages. In: Symposium on LISP and Functional Programming, pp. 1–10 (1982)
29. Kohlhase, M.: Higher-order tableaux. In: Baumgartner, P., H¨ahnle, R., Possega,
J. (eds.) TABLEAUX 1995. LNCS, vol. 918, pp. 294–309. Springer, Heidelberg
(1995). https://doi.org/10.1007/3-540-59338-1 43
30. Meng, J., Paulson, L.C.: Translating higher-order clauses to ﬁrst-order clauses. J.
Autom. Reason. 40(1), 35–60 (2008)
31. Nelson, G., Oppen, D.C.: Fast decision procedures based on congruence closure. J.
ACM 27, 356–364 (1980)
32. Nieuwenhuis, R., Oliveras, A.: Fast congruence closure and extensions. Inf. Com-
put. IC 2005(4), 557–580 (2007)
33. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving. In: Robinson,
A., Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. 1, pp. 371–443.
Elsevier Science (2001)
34. Nipkow, T., Wenzel, M., Paulson, L.C.: Isabelle/HOL: A Proof Assistant for
Higher-Order Logic. LNCS, vol. 2283. Springer, Heidelberg (2002). https://doi.
org/10.1007/3-540-45949-9
35. Noshita, K.: Translation of Turner combinators in O(n log n) space. IPL 20, 71–74
(1985)
36. Paulson, L.C., Blanchette, J.C.: Three years of experience with Sledgehammer, a
practical link between automatic and interactive theorem provers. In: Sutcliﬀe, G.,
Schulz, S., Ternovska, E. (eds.) IWIL-2010. EPiC, vol. 2, pages 1–11. EasyChair
(2012)
37. Reynolds, A., Barbosa, H., Fontaine, P.: Revisiting enumerative instantiation. In:
Beyer, D., Huisman, M. (eds.) TACAS 2018. LNCS, vol. 10806, pp. 112–131.
Springer, Cham (2018). https://doi.org/10.1007/978-3-319-89963-3 7
38. Reynolds, A., Tinelli, C., de Moura, L.: Finding conﬂicting instances of quantiﬁed
formulas in SMT. In: FMCAD 2014, pp. 195–202. IEEE (2014)
39. Reynolds, A., Tinelli, C., Goel, A., Krsti´c, S.: Finite model ﬁnding in SMT. In:
Sharygina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 640–655. Springer,
Heidelberg (2013). https://doi.org/10.1007/978-3-642-39799-8 42
40. Reynolds, A., Tinelli, C., Goel, A., Krsti´c, S., Deters, M., Barrett, C.: Quantiﬁer
instantiation techniques for ﬁnite model ﬁnding in SMT. In: Bonacina, M.P. (ed.)
CADE 2013. LNCS, vol. 7898, pp. 377–391. Springer, Heidelberg (2013). https://
doi.org/10.1007/978-3-642-38574-2 26
41. Robinson, J.A.: Mechanizing higher order logic. Mach. Intell. 4, 151–170 (1969)
42. Schulz, S.: E - a brainiac theorem prover. AI Commun. 15, 111–126 (2002)
43. Steen, A., Benzm¨uller, C.: The higher-order prover Leo-III. In: Galmiche, D.,
Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS, vol. 10900, pp. 108–116.
Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94205-6 8
44. Stump, A., Barrett, C.W., Dill, D.L., Levitt, J.R.: A decision procedure for an
extensional theory of arrays. In: LICS 2001, pp. 29–37. IEEE Computer Society
(2001)

54
H. Barbosa et al.
45. Sultana, N., Blanchette, J.C., Paulson, L.C.: LEO-II and Satallax on the Sledge-
hammer test bench. J. Appl. Log. 11, 91–102 (2013)
46. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure. J. Autom.
Reason. 43, 337–362 (2009)
47. Sutcliﬀe, G.: The CADE ATP system competition - CASC. AI Mag. 37, 99–101
(2016)
48. Vukmirovi´c, P., Blanchette, J.C., Cruanes, S., Schulz, S.: Extending a brainiac
prover to lambda-free higher-order logic. In: Vojnar, T., Zhang, L. (eds.) TACAS
2019. LNCS, vol. 11427, pp. 192–210. Springer, Cham (2019). https://doi.org/10.
1007/978-3-030-17462-0 11

Superposition with Lambdas
Alexander Bentkamp1(B), Jasmin Blanchette1,2, Sophie Tourret2,
Petar Vukmirović1, and Uwe Waldmann2
1 Vrije Universiteit Amsterdam, Amsterdam, The Netherlands
{a.bentkamp,j.c.blanchette,p.vukmirovic}@vu.nl
2 Max-Planck-Institut für Informatik,
Saarland Informatics Campus, Saarbrücken, Germany
{jblanche,stourret,uwe}@mpi-inf.mpg.de
Abstract. We designed a superposition calculus for a clausal frag-
ment of extensional polymorphic higher-order logic that includes anony-
mous functions but excludes Booleans. The inference rules work on βη-
equivalence classes of λ-terms and rely on higher-order uniﬁcation to
achieve refutational completeness. We implemented the calculus in the
Zipperposition prover and evaluated it on TPTP and Isabelle bench-
marks. The results suggest that superposition is a suitable basis for
higher-order reasoning.
1
Introduction
Superposition [5] is widely regarded as the calculus par excellence for reasoning
about ﬁrst-order logic with equality. To increase automation in proof assistants
and other veriﬁcation tools based on higher-order formalisms, we propose to
generalize superposition to an extensional, polymorphic, clausal version of higher-
order logic (also called simple type theory). Our ambition is to achieve a graceful
extension, which coincides with standard superposition on ﬁrst-order problems
and smoothly scales up to arbitrary higher-order problems.
Bentkamp, Blanchette, Cruanes, and Waldmann [10] recently designed a fam-
ily of superposition-like calculi for a λ-free fragment of higher-order logic, with
currying and applied variables. We adapt their “extensional nonpurifying” calcu-
lus to also support λ-expressions (Sect. 3). Our calculus does not support ﬁrst-
class Booleans; it is conceived as the penultimate milestone towards a superposi-
tion calculus for full higher-order logic. If desired, Booleans can be encoded in our
logic fragment using an uninterpreted type and uninterpreted “proxy” symbols
corresponding to equality, the connectives, and the quantiﬁers.
Designing a higher-order superposition calculus poses three main challenges:
1. In ﬁrst-order logic, superposition is parameterized by a ground-total simpli-
ﬁcation order ≻, but such orders do not exist for λ-terms considered equal
up to β-conversion. The relations designed for proving termination of higher-
order term rewriting systems, such as HORPO [39] and CPO [21], lack many
of the desired properties (e.g., transitivity, stability under substitution).
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 55–73, 2019.
https://doi.org/10.1007/978-3-030-29436-6_4

56
A. Bentkamp et al.
2. Higher-order uniﬁcation is undecidable and may give rise to an inﬁnite set
of incomparable uniﬁers. For example, the constraint f (y a)
?= y (f a) admits
inﬁnitely many independent solutions of the form {y →λx. fn x}.
3. In ﬁrst-order logic, to rewrite into a term s using an oriented equation t ≈t′,
it suﬃces to ﬁnd a subterm of s that is uniﬁable with t. In higher-order
logic, this is insuﬃcient. Consider superposition from f c ≈a into y c ̸≈y b.
The left-hand sides can obviously be uniﬁed by {y →f}, but the more general
substitution {y →λx.zx(fx)} also gives rise to a subterm fc after β-reduction.
The corresponding inference generates the clause z c a ̸≈z b (f b).
To address the ﬁrst challenge, we adopt η-short β-normal form to repre-
sent βη-equivalence classes of λ-terms. In the spirit of Jouannaud and Rubio’s
early joint work [38], we state requirements on the term order only for ground
terms (i.e., closed monomorphic βη-equivalence classes); the nonground case is
connected to the ground case via stability under substitution. Even on ground
terms, it is impossible to obtain all desirable properties. We sacriﬁce compatibil-
ity with arguments (the property that s′ ≻s implies s′ t ≻s t) and compensate
for it with an argument congruence rule (ArgCong), as in Bentkamp et al. [10].
For the second challenge, we accept that there might be inﬁnitely many
incomparable uniﬁers and enumerate a complete set (including the notorious
ﬂex–ﬂex pairs [36]), relying on heuristics to keep the combinatorial explosion
under control. The saturation loop must also be adapted to interleave this enu-
meration with the theorem prover’s other activities (Sect. 6). Despite its repu-
tation for explosiveness, higher-order uniﬁcation is a conceptual improvement
over SK combinators, because it can often compute the right uniﬁer. Consider
the conjecture ∃z. ∀x y. z x y ≈f y x. After negation, clausiﬁcation, and skolem-
ization, it becomes z (skx z) (sky z) ̸≈f (sky z) (skx z). Higher-order uniﬁcation
quickly computes the unique uniﬁer: {z →λx y. f y x}. In contrast, an encoding
approach based on combinators, similar to the one implemented in Sledgeham-
mer [48], would blindly enumerate all possible SK terms for z until the right one,
S (K (S f)) K, is found. Given the deﬁnitions S z y x ≈z x (y x) and K x y ≈x,
the E prover [55] in auto mode needs to perform 3756 inferences to derive the
empty clause.
For the third challenge, when applying t ≈t′ to perform rewriting inside a
higher-order term s, the idea is to encode an arbitrary context as a fresh higher-
order variable z, unifying s with z t; the result is (z t′)σ, for some uniﬁer σ. This
is performed by a dedicated ﬂuid subterm superposition rule (FluidSup).
Functional extensionality (the property that ∀x. y x ≈z x implies y ≈z) is
also considered a challenge for higher-order reasoning [13], although similar dif-
ﬁculties arise with the ﬁrst-order theories of sets and arrays [33]. Our approach
is to add extensionality as an axiom and provide optional rules as optimizations
(Sect. 5). With this axiom, our calculus is refutationally complete with respect
to extensional Henkin semantics (Sect. 4). Detailed proofs are included in a tech-
nical report [11], together with more explanations, examples, and discussions.
We implemented the calculus in the Zipperposition prover [27] (Sect. 6). Our
empirical evaluation includes benchmarks from the TPTP [59] and interactive

Superposition with Lambdas
57
veriﬁcation problems exported from Isabelle/HOL [22] (Sect. 7). The results
appear promising and suggest that an optimized implementation inside a com-
petitive prover such as E [55], SPASS [64], or Vampire [44] would outperform
existing higher-order automatic provers.
2
Logic
Our extensional polymorphic clausal higher-order logic is a restriction of full
TPTP THF [15] to rank-1 polymorphism, as in TH1 [40]. In keeping with
standard superposition, we consider only formulas in conjunctive normal form.
Booleans can easily be axiomatized [11, Sect. 2.3]. We use Henkin semantics
[14,31,34].
We ﬁx a set Σty of type constructors with arities and a set Vty of type vari-
ables. We require a binary function type constructor →∈Σty to be present. A
type τ, υ is either a type variable α ∈Vty or has the form κ(¯τn) for an n-ary
type constructor κ ∈Σty and types ¯τn. We use the notation ¯an or ¯a to stand for
the tuple (a1, . . . , an) or product a1 × · · · × an, where n ≥0. We write κ for κ()
and τ →υ for →(τ, υ). A type declaration is an expression of the form Π¯αm. τ
(or simply τ if m = 0), where all type variables occurring in τ belong to ¯αm.
We ﬁx a nonempty set Σ of (function) symbols a, b, c, f, g, h, . . . , with type
declarations, written as f : Π¯αm. τ or f, and a set V of term variables with asso-
ciated types, written as x : τ or x. The sets (Σty, Vty, Σ, V ) form the signature.
The set of raw λ-terms is deﬁned inductively as follows. Every x : τ ∈V is a raw
λ-term of type τ. If f : Π¯αm. τ ∈Σ and ¯υm is a tuple of types, called type argu-
ments, then f⟨¯υm⟩(or simply f if m = 0) is a raw λ-term of type τ{¯αm →¯υm}.
If x : τ and t : υ, then the λ-expression λx. t is a raw λ-term of type τ →υ. If
s : τ →υ and t : τ, then the application s t is a raw λ-term of type υ.
The α-renaming rule is deﬁned as (λx. t) →α (λy. t{x →y}), where y does
not occur free in t and is not captured by a λ in t. Raw λ-terms form equivalence
classes modulo α-renaming, called λ-terms. A variable occurrence is free in a λ-
term if it is not bound by a λ-expression. A λ-term is ground if it is built without
using type variables and contains no free term variables. Using the spine notation
[25], λ-terms can be decomposed in a unique way as a non-application head t
applied to zero or more arguments: t s1 . . . sn or t ¯sn (abusing notation).
The β- and η-reduction rules are deﬁned on λ-terms as (λx.t)u →β t{x →u}
and (λx.tx) →η t. For β, bound variables in t are renamed to avoid capture; for
η, the variable x must not occur free in t. The λ-terms form equivalence classes
modulo βη-reduction, called βη-equivalence classes or simply terms. When deﬁn-
ing operations that need to analyze the structure of terms, we use the η-short
β-normal form t↓βη, obtained by applying →β and →η exhaustively, as a rep-
resentative of the equivalence class t. Many authors prefer the η-long β-normal
form [36,38,47], but in a polymorphic setting it has the drawback that instan-
tiating a type variable by a function type can lead to η-expansion. We reserve
the letters s, t, u, v for terms and w, x, y, z for variables, and write : τ to indicate
their type.

58
A. Bentkamp et al.
An equation s ≈t is formally an unordered pair of terms s and t. A literal is
an equation or a negated equation, written ¬ s ≈t or s ̸≈t. A clause L1∨· · ·∨Ln
is a ﬁnite multiset of literals Lj. The empty clause is written as ⊥.
In general, a substitution {¯αm, ¯xn →¯υm, ¯sn}, where each xj has type τj
and each sj has type τj{¯αm →¯υm}, maps m type variables to m types and
n term variables to n terms. The letters θ, ρ, σ are reserved for substitutions.
Substitutions are lifted to terms and clauses in a capture-avoiding way. The
composition ρσ applies ρ ﬁrst: tρσ = (tρ)σ. A complete set of uniﬁers on a
set X of variables for s and t is a set U of uniﬁers of s and t such that for every
uniﬁer ρ of s and t there exists a member σ ∈U and a substitution θ such that
xσθ = xρ for all x ∈X. We use CSUX(s, t) to denote a ﬁxed complete set of
uniﬁers on X for s and t. The set X will consist of the free variables of the
clauses in which s and t occur and will be left implicit.
3
The Calculus
Our superposition calculus for clausal higher-order logic is inspired by the λ-free
extensional nonpurifying calculus described by Bentkamp et al. [10]. The text
of this section is partly based on that paper (with Cruanes’s permission). The
central idea is that superposition inferences are restricted to unapplied subterms
occurring in the “ﬁrst-order outer skeleton” of the superterm—that is, outside λ-
expressions and outside the arguments of applied variables. We call these “green
subterms.” Thus, an equation g ≈(λx. f x x) cannot be used directly to rewrite
g a to f a a, because g is applied in g a. A separate inference rule, ArgCong,
takes care of deriving g x ≈f x x, which can be oriented independently of its
parent clause and used to rewrite g a or f a a.
A term (i.e., a βη-equivalence class) t is deﬁned to be a green subterm of a
term s if either s = t or s = f⟨¯τ⟩¯s for some function symbol f, types ¯τ and
terms ¯s, where t is a green subterm of si for some i. In f (g a) (y b) (λx. h c (g x)),
the green subterms are a, g a, y b, λx. h c (g x), and the entire term. We write
t = s<u> to express that u is a green subterm of t and call s< > a green context.
Another key notion is that of a “ﬂuid” term. A subterm t of s[t] is called
ﬂuid if (1) t↓βη is of the form y ¯un, where y is not bound in s[t] and n ≥1,
or (2) t↓βη is a λ-expression and there exists a substitution σ such that tσ↓βη
is not a λ-expression (due to η-reduction). A necessary condition for case (2)
is that t↓βη contains an applied variable that is not bound in s[t]. Intuitively,
ﬂuid subterms are terms whose η-short β-normal form can change radically as a
result of instantiation. For example, applying the substitution {z →(λx. x)} to
the ﬂuid term λx. y a (z x) makes the λ-expression vanish: (λx. y a x) = y a.
Term Order. The calculus is parameterized by a well-founded strict total order
≻on ground terms satisfying the following properties:
• green subterm property: t<s> ⪰s (i.e., t<s> ≻s or t<s> = s);
• compatibility with green contexts: s′ ≻s implies t<s′> ≻t<s>.

Superposition with Lambdas
59
The literal and clause orders are deﬁned as multiset extensions in the standard
way [5]. Two properties that are not required are compatibility with λ-expressions
(s′ ≻s implies (λx. s′) ≻(λx. s)) and compatibility with arguments (s′ ≻s
implies s′ t ≻s t). The latter would even be inconsistent with totality. To see
why, consider the symbols c ≻b ≻a and the terms λx. b and λx. x. Owing to
totality, one of the terms must be larger than the other, say, (λx. b) ≻(λx. x).
By compatibility with arguments, we get (λx. b) c ≻(λx. x) c, i.e., b ≻c, a
contradiction. A similar line of reasoning applies if (λx. b) ≺(λx. x), using a
instead of c.
For nonground terms, ≻is extended to a strict partial order so that t ≻s if
and only if tθ ≻sθ for all grounding substitutions θ. We also introduce a qua-
siorder ≿such that t ≿s if and only if tθ ⪰sθ for all grounding substitutions θ,
and similarly for literals and clauses. The quasiorder ≿is more precise than ⪰;
for example, given a, b : ι with b ≻a, we can have xb ≿xa even though xb ̸⪰xa.
Our approach to derive a suitable order is to encode η-short β-normal forms
into untyped λ-free higher-order terms and apply an order ≻base such as the λ-
free Knuth–Bendix order (KBO) [8], the λ-free lexicographic path order (LPO)
[20], or the embedding path order (EPO) [9]. The encoding, denoted by ⌊⌈⌋⌉,
translates λx : τ. t to lam ⌊⌈τ⌋⌉⌊⌈t⌋⌉and uses De Bruijn symbols dbi to represent
bound variables x [24]. It replaces ﬂuid terms t by fresh variables zt and maps
type arguments to term arguments; thus, ⌊⌈λx:ι.λy:ι.x⌋⌉= lamι(lamι(db1ι)) and
⌊⌈f⟨ι⟩(ya)⌋⌉= f ιzy a. We then deﬁne the metaorder ≻meta induced by ≻base in such
a way that t ≻meta s if and only if ⌊⌈t⌋⌉≻base ⌊⌈s⌋⌉. The use of De Bruijn indices
and the monolithic encoding of ﬂuid terms ensure stability under α-renaming
and under substitution.
The Inference Rules. The calculus is parameterized by a selection function,
which maps each clause to a subclause consisting of negative literals. A literal
L<y> must not be selected if y ¯un, with n > 0, is a ≿-maximal term of the clause.
A literal L is (strictly) eligible in C if it is selected in C or if there are no
selected literals in C and L is (strictly) maximal in C. A variable is deep in a
clause C if it occurs inside a λ-expression or inside an argument of an applied
variable; these cover all occurrences that may correspond to positions inside
λ-expressions after applying a substitution.
We regard positive and negative superposition as two cases of a single rule
D



D′ ∨t ≈t′
C



C′ ∨[¬] s<u> ≈s′
Sup
(D′ ∨C′ ∨[¬] s<t′> ≈s′)σ
with the following side conditions:
1. u is not a ﬂuid subterm;
2. u is not a deep variable in C;
3. if u is a variable y, there must exist a grounding θ such that tσθ ≻t′σθ and
Cσθ ≺C{y →t′}σθ;
4. σ ∈CSU(t, u);
5. tσ ̸≾t′σ;
6. s<u>σ ̸≾s′σ;
7. Cσ ̸≾Dσ;

60
A. Bentkamp et al.
8. (t ≈t′)σ is strictly eligible in Dσ;
9. ([¬] s<u> ≈s′)σ is eligible in Cσ, and strictly eligible if it is positive.
There are four main diﬀerences with the statement of the standard superposition
rule: Contexts s[ ] are replaced by green contexts s< >. The standard condition
u /∈V is generalized by conditions 2 and 3. Most general uniﬁers are replaced
by complete sets of uniﬁers. And ̸⪯is replaced by the more restrictive ̸≾.
The second rule is a variant of Sup that focuses on ﬂuid subterms occurring
in green contexts. Its statement is
D



D′ ∨t ≈t′
C



C′ ∨[¬] s<u> ≈s′
FluidSup
(D′ ∨C′ ∨[¬] s<z t′> ≈s′)σ
with the following side conditions, in addition to Sup’s conditions 5 to 9:
1. u is either a deep variable in C or a ﬂuid subterm;
2. z is a fresh variable;
3. σ ∈CSU(z t, u);
4. z t′ ̸= z t.
The next two rules are almost identical to their standard counterparts:
C′ ∨u ̸≈u′
EqRes
C′σ
C′ ∨u′ ≈v′ ∨u ≈v
EqFact
(C′ ∨v ̸≈v′ ∨u ≈v′)σ
For EqRes: σ ∈CSU(u, u′) and (u ̸≈u′)σ is eligible in the premise. For EqFact:
σ ∈CSU(u, u′), u′σ ̸≾v′σ, uσ ̸≾vσ, and (u ≈v)σ is eligible in the premise.
Argument congruence, a higher-order concern, is embodied by the rule
C′ ∨s ≈s′
ArgCong
C′σ ∨(sσ) ¯xn ≈(s′σ) ¯xn
where σ is the most general type substitution that ensures well-typedness of the
conclusion. In particular, if the result type of s is not a type variable, σ is the
identity substitution; and if the result type is a type variable, it is instantiated
with ¯αn →β, where ¯αn and β are fresh type variables, yielding inﬁnitely many
conclusions, one for each n. The literal sσ ≈s′σ must be strictly eligible in
(C′ ∨s ≈s′)σ, and ¯xn is a nonempty tuple of distinct fresh variables.
The rules are complemented by an axiom expressing functional extensional-
ity:
y (diﬀ⟨α, β⟩y z) ̸≈z (diﬀ⟨α, β⟩y z) ∨y ≈z
The symbol diﬀ: Πα, β. (α →β) →(α →β) →α is a Skolem symbol.
Rationale for the Rules. The calculus realizes the following division of labor:
Sup and FluidSup are responsible for green subterms, which are outside λs,
ArgCong indirectly gives access to the remaining positions outside λs, and the
extensionality axiom takes care of subterms occurring inside λs.

Superposition with Lambdas
61
Example 1. Applied variables give rise to subtle situations with no counter-
parts in ﬁrst-order logic. Consider the clauses
f a ≈c
h (y b) (y a) ̸≈h (g (f b)) (g c)
where f a ≻c. It is easy to see that the clause set is unsatisﬁable, by grounding
the second clause with θ = {y →(λx. g (f x))}. However, to mimic the super-
position inference that can be performed at the ground level, it is necessary to
superpose at an imaginary position below the applied variable y and yet above
its argument a, namely, into the subterm f a of g (f a) = (λx. g (f x)) a = (y a)θ.
FluidSup’s z variable eﬀectively transforms f a ≈c into z (f a) ≈z c, whose
left-hand side can be uniﬁed with y a by taking {y →(λx.z (f x))}. The resulting
clause is h (z (f b)) (z c) ̸≈h (g (f b)) (g c), which has the right form for EqRes.
Example 2. Third-order clauses in which variables are applied to λ-expressions
can be even more stupefying. The clause set
f a ≈c
h (y (λx. g (f x)) a) y ̸≈h (g c) (λw x. w x)
is unsatisﬁable. To see this, apply θ = {y →(λw x. w x)} to the second clause:
h(g (f a))(λw x.w x) ̸≈h(g c)(λw x.w x). Let f a ≻c. A Sup inference is possible
between the two ground clauses. But at the nonground level, the subterm fa is not
clearly localized: g(fa) = (λx.g(fx))a = (λwx.wx)(λx.g(fx))a = (y(λx.g(fx))a)θ.
FluidSup can cope with this. One of the uniﬁers of z (f a) and y (λx. g (f x)) a
will be {y →(λw x. w x), z →g}, yielding h (g c) (λw x. w x) ̸≈h (g c) (λw x. w x).
Because it gives rise to ﬂex–ﬂex pairs (uniﬁcation constraints where both
sides are applied variables), FluidSup can be very proliﬁc. The extensionality
axiom is another prime source of ﬂex–ﬂex pairs.
Due to order restrictions and fairness, we cannot postpone solving ﬂex–ﬂex
pairs indeﬁnitely. Thus, we cannot use Huet’s pre-uniﬁcation procedure [36] and
must instead choose a complete procedure such as Jensen and Pietrzykowski’s
[37] or Snyder and Gallier’s [57]. On the positive side, optional inference rules can
eﬃciently cover many cases where FluidSup or the extensionality axiom would
otherwise be needed, and heuristics can help keep the explosion under control.
Moreover, ﬂex–ﬂex pairs are not always as bad as their reputation; for example,
y a b
?= z c d admits a most general uniﬁer: {y →(λw x. y′ w x c d), z →y′ a b}.
The calculus is a graceful generalization of standard superposition, except
for the extensionality axiom. From g x ≈f x x, the axiom can be used to derive
clauses such as (λx. y x (g x)) ≈(λx. y x (f x x)), which are useless if the problem
is ﬁrst-order.
Redundancy Criterion. A redundant (or composite) clause is usually deﬁned
as a clause whose ground instances are entailed by smaller (≺) ground instances
of existing clauses. This would be too strong for our calculus; for example, it
would make ArgCong inferences redundant. Our solution is to base the redun-
dancy criterion on a weaker ground logic in which argument congruence and
extensionality are not guaranteed to hold.

62
A. Bentkamp et al.
The weaker logic is deﬁned via an encoding ⌊⌋of ground λ-terms into ﬁrst-
order terms. The ⌊⌋encoding indexes each symbol occurrence with its type
arguments and argument count. Thus, ⌊f⌋= f0, ⌊f a⌋= f1(a0), and ⌊g⟨ι⟩⌋= gι
0.
In addition, it conceals λs by replacing them with fresh symbols. These measures
eﬀectively disable argument congruence and extensionality. For example, the
clause sets {g0 ≈f0, g1(a0) ̸≈f1(a0)} and {b0 ≈a0, c0 ̸≈d0} are satisﬁable, even
though {g ≈f, g a ̸≈f a} and {b ≈a, (λx. b) ̸≈(λx. a)} are unsatisﬁable.
Given a ground higher-order signature (Σty, {}, Σ, {}), we deﬁne a ﬁrst-order
signature (Σty, {}, Σ↓, {}) as follows. The type constructors Σty are the same in
both signatures, but →is uninterpreted in ﬁrst-order logic. For each ground
instance f⟨¯υ⟩: τ1 →· · · →τn →τ of a symbol f ∈Σ, we introduce a ﬁrst-order
symbol f ¯υ
j ∈Σ↓with argument types ¯τj and result type τj+1 →· · · →τn →τ, for
each j. Moreover, for each ground term λx.t, we introduce a symbol ⌊λx.t⌋∈Σ↓
of the same type.
The ⌊⌋encoding is deﬁned on ground η-short β-normal forms so that λx.t is
mapped to the symbol ⌊λx.t⌋and ⌊f⟨¯υ⟩¯sj⌋= f ¯υ
j (⌊¯sj⌋) recursively. The encoding
is extended to literals and clauses elementwise. Using the inverse mapping ⌈⌉, the
order ≻can be transferred to the ﬁrst-order level by deﬁning t ≻s as ⌈t⌉≻⌈s⌉.
A crucial property of ⌊⌋is that green subterms of a term t correspond to ﬁrst-
order subterms of ⌊t⌋. Thus, the subterms considered by Sup and FluidSup
coincide with the subterms exposed to the redundancy criterion.
In standard superposition, redundancy employs the entailment relation |= on
ground clauses. We deﬁne redundancy of higher-order clauses in the same way,
but using |= on the ⌊⌋-encoded clauses. This deﬁnition gracefully generalizes the
standard ﬁrst-order notion of redundancy. Formally, a clause C is redundant with
respect to a set of clauses N if for each ground instance Cθ, ⌊Cθ⌋is entailed by
ground instances of clauses in ⌊GΣ(N)⌋that are smaller than ⌊Cθ⌋. Here, GΣ(N)
denotes the set of ground instances of clauses in N. We call N saturated up to
redundancy if for each inference from clauses in N, its premise is redundant with
respect to N or its conclusion is contained in N or redundant with respect to N.
The saturation procedures of superposition-based provers aggressively delete
clauses that are strictly subsumed by other clauses. A clause C subsumes D if
there exists a substitution σ such that Cσ ⊆D. A clause C strictly subsumes D if
C subsumes D but D does not subsume C. For example, x ≈c strictly subsumes
both a ≈c and b ̸≈a ∨x ≈c. The proof of refutational completeness of
resolution and superposition provers relies on the well-foundedness of the strict
subsumption relation [54, Section 7]. Unfortunately, this property does not hold
for higher-order logic, where f x x ≈c is strictly subsumed by f (x a) (x b) ≈c,
which is strictly subsumed by f (x a a) (x b b′) ≈c, and so on. Subsumption must
be restricted to prevent such inﬁnite chains—for example, by requiring that the
subsumer is syntactically smaller than or of the same size as the subsumee.
4
Refutational Completeness
Besides soundness, the most important property of the higher-order superposi-
tion calculus introduced in Sect. 3 is refutational completeness:

Superposition with Lambdas
63
Theorem 3. Let N ̸∋⊥be a clause set that is saturated up to redundancy and
that contains the extensionality axiom. Then N has a Henkin model.
The proof is adapted from Bentkamp et al. [10]. We present a brief outline
in this section and point to our technical report [11] for the details. Let N ̸∋⊥
be a higher-order clause set saturated up to redundancy with respect to the
inference rules and that contains the extensionality axiom. The proof proceeds
in two steps:
1. Construct a model of the ﬁrst-order grounded clause set ⌊GΣ(N)⌋, where ⌊⌋
is the encoding of ground terms used to deﬁne redundancy.
2. Lift this ﬁrst-order model to a higher-order interpretation and show that it
is a model of GΣ(N) and hence of N.
The ﬁrst step follows the same general idea as the completeness proof for
standard superposition [5,50,63]. We construct a term rewriting system R∞and
use it to deﬁne a candidate interpretation that equates all terms that share
the same normal form with respect to R∞. At this level, expressions λx. t are
regarded as uninterpreted symbols ⌊λx. t⌋.
As in the standard proof, it is the set N, and not its grounding GΣ(N),
that is saturated. We must show that there exist nonground inferences corre-
sponding to all necessary ground Sup, EqRes, and EqFact inferences. We face
two speciﬁcally higher-order diﬃculties. First, in standard superposition, we can
avoid Sup inferences into variables x by exploiting the order’s compatibility with
contexts: If t′ ≺t, we have C{x →t′} ≺C{x →t}, which allows us to invoke
the induction hypothesis at a key point in the argument to establish the truth of
C{x →t′}. This technique fails for higher-order variables x that occur applied
in C, because the order lacks compatibility with arguments. Hence, our Sup rule
must perform some inferences into variables. The other diﬃculty also concerns
applied variables. We must show that any necessary ground Sup inference into a
position corresponding to a ﬂuid term or a deep variable on the nonground level
can be lifted to a FluidSup inference. This involves showing that the z variable
in FluidSup can represent arbitrary contexts around a term t.
For the ﬁrst-order model construction, βη-normalization is the proverbial
dog that did not bark. At the ground level, the rules Sup, EqRes, and EqFact
preserve η-short β-normal form, and so does ﬁrst-order term rewriting. Thus, we
can completely ignore →β and →η. At the nonground level, β- and η-reduction
can arise only through instantiation. This poses no diﬃculties thanks to the
order’s stability under substitution.
The second step of the completeness proof consists of constructing a higher-
order interpretation and proving that it is a model of GΣ(N), and hence of N.
The diﬃculty is to show that the symbols representing λ-expressions behave like
the λ-expressions they represent. This step relies on saturation with respect to
the ArgCong rule—which connects a λ-expression with its value when applied
to an argument x—and on the presence of the extensionality axiom.

64
A. Bentkamp et al.
5
Extensions
The calculus can be extended to make it more practical. The familiar simpliﬁca-
tion machinery can be adapted to higher-order terms by considering green con-
texts instead of arbitrary contexts. Optional inference rules provide lightweight
alternatives to the extensionality axiom.
Two of the rules below are based on “orange subterms.” A λ-term t is an
orange subterm of a λ-term s if s = t; or if s = f⟨¯τ⟩¯s and t is an orange subterm
of si for some i; or if s = x ¯s and t is an orange subterm of si for some i; or if
s = (λx. u) and t is an orange subterm of u. In f (g a) (y b) (λx. h c (g x)), the
orange subterms include b, c, x, g x, h c (g x), and all the green subterms. This
notion is lifted to βη-equivalence classes via representatives in η-short β-normal
form. We write t = s<<¯xn.u>> to indicate that u is an orange subterm of t, where
¯xn are the variables bound in the orange context around u.
Once a term s<<¯xn. u>> has been introduced, we write s<<¯xn. u′>>η to denote
the same context with a diﬀerent subterm u′ at that position. The η subscript
is a reminder that u′ is not necessarily an orange subterm of s<<¯xn. u′>>η due to
potential applications of η-reduction. For example, if s<<x. g x x>> = (λx. g x x),
then s<<x. f x>>η = (λx. f x) = f.
Demodulation, which destructively rewrites using an equality t ≈t′, is avail-
able at green positions. A variant rewrites inside λ-expressions:
t ≈t′
C<s<<¯x. tσ>>>
λDemodExt
t ≈t′
C<s<<¯x. t′σ>>η>
s<<¯x. tσ>> ≈s<<¯x. t′σ>>η
where s<<¯x. tσ>>↓βη is a λ-expression or an applied variable. The term tσ may
refer to the bound variables ¯x. Side condition: The second premise is larger than
(≻) the second and third conclusion. This ensures that this premise is redundant
with respect to these conclusions and may be removed. The double bar indicates
that the conclusions collectively make the premises redundant and can replace
them. An instance of the rule, where g z is rewritten to f z z under a λ, follows:
g x ≈f x x
k (λz. h (g z)) ≈c
λDemodExt
g x ≈f x x
k (λz. h (f z z)) ≈c
(λz. h (g z)) ≈(λz. h (f z z))
The next simpliﬁcation rule can be used to prune arguments to variables
that can be expressed as functions of the remaining arguments. For example,
the clause C[y a b (f b a), y b d (f d b)], in which y occurs twice, can be simpliﬁed
to C[y′ a b, y′ b d]. The rule can also be used to remove the repeated arguments
in y b b ̸≈y a a, the static argument a in y a c ̸≈y a b, and all four arguments in
y a b ̸≈z b d. It is stated as
C
PruneArg
C{y →(λ¯xj. y′ ¯xj−1)}
where y′ is a fresh variable, the minimum number k of arguments passed to
any occurrence of y in the clause C ↓βη is at least j, and there exists a term t

Superposition with Lambdas
65
containing no variables bound in the clause such that sj = t ¯sj−1 sj+1 . . . sk for
all terms of the form y ¯sk occurring in the clause. For example, clauses with a
static argument correspond to the case t := (λ¯xj−1 xj+1 . . . xk. u), where u is
the static argument (containing no variables bound in t) and j is its index in y’s
argument list.
Following the literature [33,58], we provide a rule for negative extensionality:
C ∨s ̸≈s′
NegExt
C ∨s (sk⟨¯αm⟩¯yn) ̸≈s′ (sk⟨¯αm⟩¯yn)
where sk is a fresh Skolem symbol, ¯αm, ¯yn are the variables occurring free in the
literal s ̸≈s′, and s ̸≈s′ is eligible in the premise. Negative extensionality can
also be applied as a simpliﬁcation rule to all literals in the initial problem.
Superposition can be generalized to orange subterms as follows:
D′ ∨t ≈t′
C′ ∨[¬] s<<¯x. u>> ≈s′
λSup
(D′ ∨C′ ∨[¬] s<<¯x. t′>>η ≈s′)σρ
Sup’s side conditions apply. We also require that ¯xσ = ¯x and that the variables
¯x do not occur in yσ for all variables y in u. Moreover, let Py = {y} for all
type and term variables y ̸∈¯x. For each i, let Pxi be recursively deﬁned as
the union of all Py such that y occurs free in the λ-expression that binds xi
in s<<¯x. u>>σ or that occurs free in the corresponding subterm of s<<¯x. t′>>ησ.
The substitution ρ is deﬁned as {xi →ski⟨¯αi⟩¯yi for each i}, where ¯yi are the
term variables in Pxi and ¯αi are the type variables in Pxi and the type variables
occurring in the type of the λ-expression binding xi. The rule can be justiﬁed
in terms of paramodulation and extensionality, with the Skolem terms standing
for diﬀterms. An instance of the rule follows:
n ≈zero ∨div n n ≈one
prod K (λk. div (succ k) (succ k)) ̸≈one
λSup
succ sk ≈zero ∨prod K (λk. one) ̸≈one
Intuitively, the term prod K (λk. u) is intended to denote the product 
k∈K u,
where k ranges over a ﬁnite set K of natural numbers.
6
Implementation
Zipperposition [26,27] is an open source superposition prover written in OCaml.1
Originally designed for polymorphic ﬁrst-order logic (TF1 [19]), it was later
extended with an incomplete higher-order mode based on pattern uniﬁcation
[49]. Bentkamp et al. [10] extended it further with a complete λ-free higher-order
mode. As a prototype, we have now implemented a Boolean-free higher-order
mode based on our calculus.
1 https://github.com/c-cube/zipperposition.

66
A. Bentkamp et al.
We use a metaorder induced by a λ-free KBO [8]. We currently use ⪰as the
nonstrict term order but could improve precision by employing a more precise
computable approximation of ≿.
Except for FluidSup, the core calculus rules already existed in Zipperposi-
tion in a similar form. To retrieve candidate right premises for FluidSup, we
created an index of all ﬂuid green subterms in the active clause set. Among the
proposed higher-order optional rules, we implemented NegExt, λSup, a mildly
incomplete variant of λDemodExt without the third conclusion, and a variant
of the PruneArg rule that removes most functional dependencies that occur in
practice.
For uniﬁcation, we started with Jensen and Pietrzykowski’s procedure [37].
The procedure is not ideal because it computes a nonminimal set of uniﬁers; for
example, given the ﬂex–ﬂex constraint y a
?= z b, it generates not only the most
general uniﬁer {y →(λw. y′ w b), z →y′ a} but also inﬁnitely many superﬂuous
uniﬁers. It is not clear whether Snyder and Gallier’s procedure [57] would behave
better. To support polymorphism, we extended Jensen and Pietrzykowski’s pro-
jection rule to check type uniﬁability instead of equality and their iteration rule
to consider the possibility that a type variable is instantiated with a function
type. On the other hand, polymorphism allows us to avoid the enumeration of
types in the iteration rule.
To interleave the uniﬁcation with other computation, our uniﬁcation proce-
dure returns a possibly inﬁnite stream of subsingletons (sets of cardinality 0
or 1) computed on demand. It can even cope with nonterminating uniﬁcation
problems that do not yield any uniﬁers, by representing them as an inﬁnite
stream of empty sets. We use this procedure for inference rules, keeping simpler
pattern-style uniﬁcation for simpliﬁcation rules. The inference rules turn the
possibly inﬁnite streams of uniﬁers into possibly inﬁnite streams of clauses—the
conclusions of inferences. To consume these streams fairly while giving ﬂexibility
to heuristics, we designed a priority queue that associates a weight with each
stream. This queue is used in the main given clause loop to store new streams
resulting from inferences and to extract clauses, which are then moved to the
passive clause set.
Based on informal experiments, we developed or tuned a few general heuris-
tics of Zipperposition. Deﬁnition unfolding, in conjunction with β-reduction,
transforms many higher-order TPTP problems into ﬁrst-order problems. We
also modiﬁed KBO’s weight generation scheme to take symbol frequencies into
account and modiﬁed other heuristics to prioritize clauses containing symbols
present in the conjecture.
7
Evaluation
We evaluated our prototype implementation of the calculus in Zipperposition
with other higher-order provers and with Zipperposition’s modes for less expres-
sive logics. All of the experiments presented in this section were performed on
StarExec nodes equipped with Intel Xeon E5-2609 0 CPUs clocked at 2.40 GHz.

Superposition with Lambdas
67
Fig. 1. Number of proved problems
Provers were invoked with a CPU time limit of 300 s. The raw data are available
online.2
We used both standard TPTP benchmarks [59] and Sledgehammer-generated
benchmarks. From the TPTP, we selected all 709 TFF (monomorphic and poly-
morphic ﬁrst-order) problems without arithmetic and all 597 TH0 (monomorphic
higher-order) problems without ﬁrst-class Booleans and arithmetic. We parti-
tioned the TH0 problems into those containing no λs (TH0λf, 545 problems)
and those containing λs (TH0λ, 52 problems). The Sledgehammer benchmarks,
corresponding to Isabelle’s Judgment Day suite [22], were regenerated to tar-
get Boolean-free higher-order logic. They comprise 5012 problems, divided in
two groups based on the number of Isabelle facts (lemmas, deﬁnitions, etc.)
selected for inclusion in each problem: either 256 (SH256) or 16 facts (SH16).
Each group is further divided into two subgroups based on the processing of λ-
expressions: SH256-λ and SH16-λ preserve λ-expressions, whereas SH256-ll and
SH16-ll encode them as λ-lifted supercombinators [48] to make the problems
accessible to λ-free higher-order provers.
We chose Leo-III 1.3 and Satallax 3.3 as representatives of the state of
the art. These are cooperative higher-order provers that can be set up to reg-
ularly invoke ﬁrst-order provers as terminal proof procedures. Leo-III can be
used on its own or as a metaprover (Leo-III-meta) with CVC4, E, and iProver
as backends. Satallax can be used on its own or as a metaprover (Satallax-
meta) with E. We also included Ehoh [62], the λ-free higher-order mode of E
2.3. For Zipperposition, we included its ﬁrst-order and λ-free modes (FOZip
and λfreeZip) as well as a mode that performs an applicative encoding [62,
Section 2] before invoking the ﬁrst-order mode (@+FOZip). We experimented
with three variants of our calculus implementation. λZip-full is designed to be
refutationally complete. λZip-pragmatic disables FluidSup and the extension-
ality axiom, and uses a lightweight higher-order uniﬁcation algorithm instead of
2 http://matryoshka.gforge.inria.fr/pubs/lamsup_results.tgz.

68
A. Bentkamp et al.
Jensen and Pietrzykowski’s procedure. Finally, λZip-competitive is a variant of
λZip-pragmatic that is further tuned for small problems requiring a substantial
amount of higher-order reasoning.
A summary of our experiments is presented in Fig. 1. To enhance readabil-
ity, we highlight in bold the winning system for each column excluding the
metaprovers. We observe that Leo-III-meta emerges as winner on all benchmark
sets, but λZip-pragmatic and λZip-competitive compare very well with Leo-III
and Satallax. In contrast, λZip-full cannot seem to keep its FluidSup rule and
extensionality under control. More research into heuristics design appears neces-
sary.
It is disappointing that on Sledgehammer problems (SH256 and SH16), we
obtain better performance by using λfreeZip with λ-lifting than using λZip with
native λs. On TH0λf problems, the situation is reversed. This seems to suggest
that λ reasoning is rarely needed for Sledgehammer problems. Clearly, this is
another area where research into heuristics design could be beneﬁcial.
8
Discussion and Related Work
Bentkamp et al. [10] introduced four calculi for λ-free higher-order logic orga-
nized along two axes: intensional versus extensional, and nonpurifying versus
purifying. The purifying calculi ﬂatten the clauses containing applied variables,
thereby eliminating the need for superposition into variables. As we extended
their work to support λs, we found the puriﬁcation approach problematic and
quickly gave it up because it needs x to be smaller than x t, which is impossible
to achieve with a term order on βη-equivalence classes. As for extensionality, it
is the norm for higher-order uniﬁcation [30] and is employed in the TPTP THF
format [60] and in proof assistants such as HOL4, HOL Light, Isabelle/HOL,
Lean, Nuprl, and PVS. Bentkamp et al. viewed their approach as “a stepping
stone towards full higher-order logic.” It already included a notion analogous to
green subterms and an ArgCong rule, which help cope with the complications
occasioned by β-reduction.
Our superposition calculus joins the family of proof systems for higher-
order logic. Closely related are Andrews’s higher-order resolution [1], Huet’s con-
strained resolution [35], Jensen and Pietrzykowski’s ω-resolution [37], Snyder’s
higher-order E-resolution [56], Benzmüller and Kohlhase’s extensional higher-
order resolution [13], and Benzmüller’s higher-order unordered paramodulation
and RUE resolution [12]. A noteworthy variant is Steen and Benzmüller’s higher-
order ordered paramodulation [58], whose order restrictions undermine refuta-
tional completeness but yield good empirical results. Other approaches are based
on analytic tableaux [6,42,43,52], connections [2], sequents [46], and satisﬁabil-
ity modulo theories [7]. Andrews [3] and Benzmüller and Miller [14] provide
excellent surveys.
The main advantage of our calculus is that it gracefully generalizes the highly
successful ﬁrst-order superposition rules without sacriﬁcing refutational com-
pleteness. It also includes a powerful simpliﬁcation rule, PruneArg, that could

Superposition with Lambdas
69
be useful in other provers. Among the drawbacks of our approach are the need
to solve ﬂex–ﬂex pairs eagerly and the explosion caused by the extensionality
axiom. We believe that this is a reasonable trade-oﬀ, especially for large prob-
lems with a substantial ﬁrst-order component, such as those originating from
proof assistants.
Our prototype λZipperposition joins the league of higher-order automatic the-
orem provers. We brieﬂy list some of its rivals. TPS [4] is based on the connection
method and expansion proofs. LEO [13] and Leo-II [16] implement variants of
RUE resolution. Leo-III [58] is based on higher-order paramodulation. Satallax
[23] implements a higher-order tableau calculus guided by a SAT solver. Leo-II,
Leo-III, and recent versions of Satallax integrate ﬁrst-order provers as terminal
procedures. AgsyHOL [46] is based on a focused sequent calculus guided by nar-
rowing. Finally, there is ongoing work by the developers of CVC4, veriT, and
Vampire to extend their provers to higher-order logic [7,17].
Half a century ago, Robinson [53] proposed to reduce higher-order logic to
ﬁrst-order logic via a translation. Tools such as Sledgehammer [51], MizAR [61],
HOLyHammer [41], and CoqHammer [28] have since popularized this approach.
Such translations must eliminate the λ-expressions, typically using SKBCI com-
binators or λ-lifting [48], and encode typing information [18]. Most translations
are implemented outside provers, but hybrid approaches are also possible [17,29].
9
Conclusion
We presented a superposition calculus for a Boolean-free fragment of extensional
polymorphic higher-order logic. With the notable exception of a functional exten-
sionality axiom, it gracefully generalizes standard superposition. Our prototype
prover Zipperposition shows promising results on TPTP and Isabelle bench-
marks. In future work, we plan to pursue ﬁve main avenues of investigation.
We ﬁrst plan to extend the calculus to support Booleans and Hilbert choice.
Booleans are notoriously explosive. We want to experiment with both axiomati-
zations and native support in the calculus. Native support would likely take the
form of a primitive substitution rule that enumerates predicate instantiations [2],
delayed clausiﬁcation rules [32], and rules for reasoning about Hilbert choice.
We want to investigate techniques to curb the explosion caused by functional
extensionality. The extensionality axiom reintroduces the search space explosion
that the calculus’s order restrictions aim at avoiding.
We will also look into approaches to curb the explosion caused by higher-order
uniﬁcation. Our calculus suﬀers because it needs to solve ﬂex–ﬂex pairs. Existing
procedures [37,57] enumerate redundant uniﬁers. This can probably be avoided
to some extent. It could also be interesting to investigate uniﬁcation algorithms
that would delay imitation/projection choices via special schematic variables,
inspired by Libal’s concise representation of regular uniﬁers [45].
We clearly need to ﬁne-tune and develop heuristics. We expect heuristics to be
a fruitful area for future research in higher-order reasoning. Proof assistants are
an inexhaustible source of easy-looking benchmarks that are beyond the power

70
A. Bentkamp et al.
of today’s provers. Whereas “hard higher-order” may remain forever out of reach,
there is a substantial “easy higher-order” fragment that awaits automation.
Finally, we plan to implement the calculus in a state-of-the-art prover. A suit-
able basis for an optimized implementation of our calculus would be Ehoh, the
λ-free higher-order version of the E prover developed by Vukmirović et al. [62].
Acknowledgment. Simon Cruanes patiently explained Zipperposition’s internals and
allowed us to continue the development of his prover. Christoph Benzmüller and Alexan-
der Steen shared insights and examples with us, guiding us through the literature and
clarifying how the Leos work. Maria Paola Bonacina and Nicolas Peltier gave us some
ideas on how to treat the extensionality axiom as a theory axiom, ideas we have yet to
explore. Mathias Fleury helped us set up regression tests for Zipperposition. Ahmed
Bhayat, Tomer Libal, and Enrico Tassi shared their insights on higher-order uniﬁca-
tion. Andrei Popescu and Dmitriy Traytel explained the terminology surrounding the
λ-calculus. Haniel Barbosa, Daniel El Ouraoui, Pascal Fontaine, and Hans-Jörg Schurr
were involved in many stimulating discussions. Christoph Weidenbach made this col-
laboration possible. Ahmed Bhayat, Mark Summerﬁeld, and the anonymous reviewers
suggested several textual improvements. We thank them all.
Bentkamp, Blanchette, and Vukmirović’s research has received funding from the
European Research Council (ERC) under the European Union’s Horizon 2020 research
and innovation program (grant agreement No. 713999, Matryoshka). Bentkamp and
Blanchette also beneﬁted from the Netherlands Organization for Scientiﬁc Research
(NWO) Incidental Financial Support scheme. Blanchette has received funding from
the NWO under the Vidi program (project No. 016.Vidi.189.037, Lean Forward).
References
1. Andrews, P.B.: Resolution in type theory. J. Symb. Log. 36(3), 414–432 (1971)
2. Andrews, P.B.: On connections and higher-order logic. J. Autom. Reason. 5(3),
257–291 (1989)
3. Andrews, P.B.: Classical type theory. In: Robinson, J.A., Voronkov, A. (eds.) Hand-
book of Automated Reasoning, vol. II, pp. 965–1007. Elsevier and MIT Press (2001)
4. Andrews, P.B., Bishop, M., Issar, S., Nesmith, D., Pfenning, F., Xi, H.: TPS: a
theorem-proving system for classical type theory. J. Autom. Reason. 16(3), 321–
353 (1996)
5. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with selec-
tion and simpliﬁcation. J. Log. Comput. 4(3), 217–247 (1994)
6. Backes, J., Brown, C.E.: Analytic tableaux for higher-order logic with choice. J.
Autom. Reason. 47(4), 451–479 (2011)
7. Barbosa, H., Reynolds, A., Fontaine, P., El Ouraoui, D., Tinelli, C.: Higher-order
SMT solving (work in progress). In: Dimitrova, R., D’Silva, V. (eds.) SMT 2018
(2018)
8. Becker, H., Blanchette, J.C., Waldmann, U., Wand, D.: A transﬁnite Knuth–
Bendix order for lambda-free higher-order terms. In: de Moura, L. (ed.) CADE
2017. LNCS (LNAI), vol. 10395, pp. 432–453. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-63046-5_27
9. Bentkamp, A.: Formalization of the embedding path order for lambda-free higher-
order terms. Archive of Formal Proofs (2018). http://isa-afp.org/entries/Lambda_
Free_EPO.html

Superposition with Lambdas
71
10. Bentkamp, A., Blanchette, J.C., Cruanes, S., Waldmann, U.: Superposition for
lambda-free higher-order logic. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.)
IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 28–46. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-94205-6_3
11. Bentkamp, A., Blanchette, J., Tourret, S., Vukmirović, P., Waldmann, U.: Superpo-
sition with lambdas (technical report). Technical report (2019). http://matryoshka.
gforge.inria.fr/pubs/lamsup_report.pdf
12. Benzmüller, C.: Extensional higher-order paramodulation and RUE-resolution. In:
Ganzinger, H. (ed.) CADE 1999. LNCS (LNAI), vol. 1632, pp. 399–413. Springer,
Heidelberg (1999). https://doi.org/10.1007/3-540-48660-7_39
13. Benzmüller, C., Kohlhase, M.: Extensional higher-order resolution. In: Kirchner,
C., Kirchner, H. (eds.) CADE 1998. LNCS (LNAI), vol. 1421, pp. 56–71. Springer,
Heidelberg (1998). https://doi.org/10.1007/BFb0054248
14. Benzmüller, C., Miller, D.: Automation of higher-order logic. In: Siekmann, J.H.
(ed.) Computational Logic, Handbook of the History of Logic, vol. 9, pp. 215–254.
Elsevier (2014)
15. Benzmüller, C., Paulson, L.C.: Multimodal and intuitionistic logics in simple type
theory. Log. J. IGPL 18(6), 881–892 (2010)
16. Benzmüller, C., Sultana, N., Paulson, L.C., Theiss, F.: The higher-order prover
Leo-II. J. Autom. Reason. 55(4), 389–404 (2015)
17. Bhayat, A., Reger, G.: Set of support for higher-order reasoning. In: Konev, B.,
Urban, J., Rümmer, P. (eds.) PAAR-2018. CEUR Workshop Proceedings, vol. 2162,
pp. 2–16. CEUR-WS.org (2018)
18. Blanchette, J.C., Böhme, S., Popescu, A., Smallbone, N.: Encoding monomorphic
and polymorphic types. Log. Meth. Comput. Sci. 12(4) (2016)
19. Blanchette, J.C., Paskevich, A.: TFF1: the TPTP typed ﬁrst-order form with rank-
1 polymorphism. In: Bonacina, M.P. (ed.) CADE 2013. LNCS (LNAI), vol. 7898, pp.
414–420. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-38574-
2_29
20. Blanchette, J.C., Waldmann, U., Wand, D.: A lambda-free higher-order recursive
path order. In: Esparza, J., Murawski, A.S. (eds.) FoSSaCS 2017. LNCS, vol.
10203, pp. 461–479. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-
662-54458-7_27
21. Blanqui, F., Jouannaud, J.P., Rubio, A.: The computability path ordering. Log.
Meth. Comput. Sci. 11(4) (2015)
22. Böhme, S., Nipkow, T.: Sledgehammer: Judgement Day. In: Giesl, J., Hähnle, R.
(eds.) IJCAR 2010. LNCS (LNAI), vol. 6173, pp. 107–121. Springer, Heidelberg
(2010). https://doi.org/10.1007/978-3-642-14203-1_9
23. Brown, C.E.: Satallax: an automatic higher-order prover. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 111–117. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3_11
24. de Bruijn, N.G.: Lambda calculus notation with nameless dummies, a tool for
automatic formula manipulation, with application to the Church-Rosser theorem.
Indag. Math. 75(5), 381–392 (1972)
25. Cervesato, I., Pfenning, F.: A linear spine calculus. J. Log. Comput. 13(5), 639–688
(2003)
26. Cruanes, S.: Extending superposition with integer arithmetic, structural induction,
and beyond. Ph.D. thesis, École polytechnique (2015)
27. Cruanes, S.: Superposition with structural induction. In: Dixon, C., Finger, M.
(eds.) FroCoS 2017. LNCS (LNAI), vol. 10483, pp. 172–188. Springer, Cham (2017).
https://doi.org/10.1007/978-3-319-66167-4_10

72
A. Bentkamp et al.
28. Czajka, Ł., Kaliszyk, C.: Hammer for Coq: automation for dependent type theory
(2018)
29. Dougherty, D.J.: Higher-order uniﬁcation via combinators. Theor. Comput. Sci.
114(2), 273–298 (1993)
30. Dowek, G.: Higher-order uniﬁcation and matching. In: Robinson, J.A., Voronkov,
A. (eds.) Handbook of Automated Reasoning, vol. II, pp. 1009–1062. Elsevier and
MIT Press (2001)
31. Fitting, M.: Types, Tableaus, and Gödel’s God. Kluwer (2002)
32. Ganzinger, H., Stuber, J.: Superposition with equivalence reasoning and delayed
clause normal form transformation. Inf. Comput. 199(1–2), 3–23 (2005)
33. Gupta, A., Kovács, L., Kragl, B., Voronkov, A.: Extensional crisis and proving
identity. In: Cassez, F., Raskin, J.-F. (eds.) ATVA 2014. LNCS, vol. 8837, pp.
185–200. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11936-6_14
34. Henkin, L.: Completeness in the theory of types. J. Symb. Log. 15(2), 81–91 (1950)
35. Huet, G.P.: A mechanization of type theory. In: Nilsson, N.J. (ed.) IJCAI 1973, pp.
139–146. William Kaufmann (1973)
36. Huet, G.P.: A uniﬁcation algorithm for typed lambda-calculus. Theor. Comput.
Sci. 1(1), 27–57 (1975)
37. Jensen, D.C., Pietrzykowski, T.: Mechanizing ω-order type theory through uniﬁca-
tion. Theor. Comput. Sci. 3(2), 123–171 (1976)
38. Jouannaud, J.P., Rubio, A.: Rewrite orderings for higher-order terms in eta-long
beta-normal form and recursive path ordering. Theor. Comput. Sci. 208(1–2), 33–
58 (1998)
39. Jouannaud, J.P., Rubio, A.: Polymorphic higher-order recursive path orderings. J.
ACM 54(1), 2:1–2:48 (2007)
40. Kaliszyk, C., Sutcliﬀe, G., Rabe, F.: TH1: the TPTP typed higher-order form
with rank-1 polymorphism. In: Fontaine, P., Schulz, S., Urban, J. (eds.) PAAR
2016. CEUR Workshop Proceedings, vol. 1635, pp. 41–55. CEUR-WS.org (2016)
41. Kaliszyk, C., Urban, J.: HOL(y)Hammer: online ATP service for HOL Light. Math.
Comput. Sci. 9(1), 5–22 (2015)
42. Kohlhase, M.: Higher-order tableaux. In: Baumgartner, P., Hähnle, R., Possega, J.
(eds.) TABLEAUX 1995. LNCS, vol. 918, pp. 294–309. Springer, Heidelberg (1995).
https://doi.org/10.1007/3-540-59338-1_43
43. Konrad, K.: Hot: a concurrent automated theorem prover based on higher-order
tableaux. In: Grundy, J., Newey, M. (eds.) TPHOLs 1998. LNCS, vol. 1479, pp.
245–261. Springer, Heidelberg (1998). https://doi.org/10.1007/BFb0055140
44. Kovács, L., Voronkov, A.: First-order theorem proving and Vampire. In: Sharygina,
N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidelberg
(2013). https://doi.org/10.1007/978-3-642-39799-8_1
45. Libal, T.: Regular patterns in second-order uniﬁcation. In: Felty, A.P., Middeldorp,
A. (eds.) CADE 2015. LNCS (LNAI), vol. 9195, pp. 557–571. Springer, Cham
(2015). https://doi.org/10.1007/978-3-319-21401-6_38
46. Lindblad, F.: A focused sequent calculus for higher-order logic. In: Demri, S.,
Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS (LNAI), vol. 8562, pp. 61–75.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08587-6_5
47. Mayr, R., Nipkow, T.: Higher-order rewrite systems and their conﬂuence. Theor.
Comput. Sci. 192(1), 3–29 (1998)
48. Meng, J., Paulson, L.C.: Translating higher-order clauses to ﬁrst-order clauses. J.
Autom. Reason. 40(1), 35–60 (2008)
49. Miller, D.: A logic programming language with lambda-abstraction, function vari-
ables, and simple uniﬁcation. J. Log. Comput. 1(4), 497–536 (1991)

Superposition with Lambdas
73
50. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving. In: Robinson,
J.A., Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. I, pp. 371–443.
Elsevier and MIT Press (2001)
51. Paulson, L.C., Blanchette, J.C.: Three years of experience with Sledgehammer,
a practical link between automatic and interactive theorem provers. In: Sutcliﬀe,
G., Schulz, S., Ternovska, E. (eds.) IWIL-2010. EPiC, vol. 2, pp. 1–11. EasyChair
(2012)
52. Robinson, J.: Mechanizing higher order logic. In: Meltzer, B., Michie, D. (eds.)
Machine Intelligence, vol. 4, pp. 151–170. Edinburgh University Press (1969)
53. Robinson, J.: A note on mechanizing higher order logic. In: Meltzer, B., Michie, D.
(eds.) Machine Intelligence, vol. 5, pp. 121–135. Edinburgh University Press (1970)
54. Schlichtkrull, A., Blanchette, J.C., Traytel, D., Waldmann, U.: Formalizing Bach-
mair and Ganzinger’s ordered resolution prover. In: Galmiche, D., Schulz, S., Sebas-
tiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 89–107. Springer, Cham
(2018). https://doi.org/10.1007/978-3-319-94205-6_7
55. Schulz, S.: System description: E 1.8. In: McMillan, K., Middeldorp, A., Voronkov,
A. (eds.) LPAR 2013. LNCS, vol. 8312, pp. 735–743. Springer, Heidelberg (2013).
https://doi.org/10.1007/978-3-642-45221-5_49
56. Snyder, W.: Higher order E-uniﬁcation. In: Stickel, M.E. (ed.) CADE 1990. LNCS
(LNAI), vol. 449, pp. 573–587. Springer, Heidelberg (1990). https://doi.org/10.
1007/3-540-52885-7_115
57. Snyder, W., Gallier, J.H.: Higher-order uniﬁcation revisited: complete sets of trans-
formations. J. Symb. Comput. 8(1/2), 101–140 (1989)
58. Steen, A., Benzmüller, C.: The higher-order prover Leo-III. In: Galmiche, D.,
Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 108–
116. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94205-6_8
59. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure-from CNF
to TH0, TPTP v6.4.0. J. Autom. Reason. 59(4), 483–502 (2017)
60. Sutcliﬀe, G., Benzmüller, C., Brown, C.E., Theiss, F.: Progress in the development
of automated theorem proving for higher-order logic. In: Schmidt, R.A. (ed.) CADE
2009. LNCS (LNAI), vol. 5663, pp. 116–130. Springer, Heidelberg (2009). https://
doi.org/10.1007/978-3-642-02959-2_8
61. Urban, J., Rudnicki, P., Sutcliﬀe, G.: ATP and presentation service for Mizar
formalizations. J. Autom. Reason. 50(2), 229–241 (2013)
62. Vukmirović, P., Blanchette, J.C., Cruanes, S., Schulz, S.: Extending a brainiac
prover to lambda-free higher-order logic. In: Vojnar, T., Zhang, L. (eds.) TACAS
2019. LNCS, vol. 11427, pp. 192–210. Springer, Cham (2019). https://doi.org/10.
1007/978-3-030-17462-0_11
63. Waldmann, U.: Automated reasoning II. Lecture notes, Max-Planck-Institut für
Informatik
(2016).
http://resources.mpi-inf.mpg.de/departments/rg1/teaching/
autrea2-ss16/script-current.pdf
64. Weidenbach, C., Dimova, D., Fietzke, A., Kumar, R., Suda, M., Wischnewski,
P.: SPASS version 3.5. In: Schmidt, R.A. (ed.) CADE 2009. LNCS (LNAI), vol.
5663, pp. 140–145. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-
642-02959-2_10

Restricted Combinatory Uniﬁcation
Ahmed Bhayat(B) and Giles Reger
University of Manchester, Manchester, UK
ahmed.bhayat@manchester.ac.uk
Abstract. First-order theorem provers are commonly utilised as back-
ends to proof assistants. In order to improve eﬃciency, it is desirable
that such provers can carry out some higher-order reasoning. In his
1991 paper, Dougherty proposed a combinatory uniﬁcation algorithm
for higher-order logic. The algorithm removes the need to deal with
λ-binders and α-renaming, making it attractive to implement in ﬁrst-
order provers. However, since publication it has garnered little interest
due to a number of characteristics that make it unsuitable for a practi-
cal implementation. It fails to terminate on many trivial instances and
requires polymorphism. We present a restricted version of Dougherty’s
algorithm that is incomplete, terminating and does not require polymor-
phism. Further, we describe its implementation in the Vampire theorem
prover, including a novel use of a substitution tree as a ﬁltering index
for higher-order uniﬁcation. Finally, we analyse the performance of the
algorithm on two benchmark sets and show that it is competitive.
1
Introduction
Higher-order logic has many applications from the formalisation of mathematics
through to uses in verifying the safety and security of computer systems. This
has led to a growing interest in the automation of reasoning in higher-order
logic. A successful step in this direction has been via translation to ﬁrst-order
logic and utilisation of ﬁrst-order theorem provers, made possible by the high
level of maturity and sophistication of such provers. Proof assistants such as
Isabelle [23] and Coq [9] along with automated provers such as Leo-III [29],
interact with ﬁrst-order provers by translating their native logic into ﬁrst-order
logic [21]. These translations tend to be incomplete and suﬀer from a number
of problems of which two of the most important are highlighted below. This
paper addresses these problems with a higher-order uniﬁcation algorithm for
combinatory logic and its pragmatic realisation within the ﬁrst-order Vampire
theorem prover [17].
The translation of nameless or λ-functions is often carried out using combi-
nators. However, when translating to monomorphic ﬁrst-order logic, supported
by most ﬁrst-order provers, an inﬁnite set of combinators is required to guar-
antee completeness. Thus, most translation schemes suﬃce with including the
combinators necessary to translate the λ-functions present in the input. Consider
the somewhat contrived conjecture ∃X : X b a = a. On negation, this becomes
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 74–93, 2019.
https://doi.org/10.1007/978-3-030-29436-6_5

Restricted Combinatory Uniﬁcation
75
X b a ̸= a. As there are no ‘λ’s there would be no combinators present in the
translation. Accordingly, the prover would be unable to synthesise the combi-
natory equivalent (which is CK) of the λ-term λxy.y and would be unable to
ﬁnd a proof. Now consider the same conjecture, but assume that some com-
binator axioms are present in the ﬁrst-order translation (this could be via the
heuristic addition of combinator axioms, an option in Vampire). In this case, the
axioms can superpose amongst themselves. For example, the C combinator axiom
C X Y Z = X Z Y could superpose onto the right hand side of the S combinator
axiom S X′ Y ′ Z′ = X′ Z′(Y ′ Z′) with uniﬁer {X →C X′, Y →Z′, Z →Y ′ Z′}
to produce the equation S (C X) Y ′ Z′ = X (Y ′ Z′)Z′. A consequence of the
combinator axioms has been derived that is of no use in proving the goal.
Both of these problems stem from attempting to achieve what the goal-
oriented procedure of higher-order uniﬁcation (HOU) does using the non-goal-
oriented superposition calculus. Thus, there is a strong argument that introduc-
ing some form of higher-order uniﬁcation into ﬁrst-order provers would signif-
icantly improve their performance on problems generated by proof-assistants.
This is particularly so if this can be achieved without harming performance on
the ﬁrst-order portion of the problems. We are not attempting to solve problems
which require complex higher-order uniﬁers. Rather, the aim is to introduce lim-
ited HOU into a ﬁrst-order prover to allow it to deal with ‘nearly ﬁrst-order’
problems.
Consider for example, the TPTP problem NUM020ˆ1. The problems posits
the existence of the Church numeral 2 and requires provers to synthesise the
lambda function λXY.X(XY ). Despite its simple nature, no current ﬁrst-order
prover would be able to solve the problem unless provided with the deﬁnition of
the Church numeral 2.
An option is to convert combinatory terms used in the ﬁrst-order prover into
λ-terms at the point of uniﬁcation and then run a HOU algorithm on these
terms. However, the usage of λ-binders adds complications and subtleties to the
implementation of higher-order uniﬁcation. It is precisely to deal with such issues
that explicit substitution calculi [10,12] have been investigated.
First-order provers are generally not able to handle binders, so rather than
explicit substitution calculi, we focus on higher-order uniﬁcation in the setting
of combinatory logic. The only existing algorithm in this setting is Dougherty’s
algorithm. The algorithm is a complete uniﬁcation procedure for polymorphic
higher-order terms. It is unattractive for implementation because it produces
many redundant uniﬁers and does not terminate in many cases.
Contribution. Our main contributions in this paper are:
• A modiﬁcation of Dougherty’s algorithm that works on monomorphic higher-
order terms (Sect. 4). Our algorithm is incomplete, but terminating and has
shown strong experimental results.
• A method of imperfect ﬁltering that facilitates the implementation of
higher-order uniﬁcation without harming performance on ﬁrst-order prob-
lems (Sect. 5).

76
A. Bhayat and G. Reger
These techniques are implemented in the Vampire theorem prover [17] (along
with other extensions reported elsewhere for higher-order reasoning) and exper-
imental results (Sect. 6) show that combinatory uniﬁcation can help solve previ-
ously unsolved problems.
2
Preliminaries
In this paper some knowledge of ﬁrst-order uniﬁcation and substitution tree
indexing is assumed. The reader is referred to [15] and [14] for further details. We
present the logical terminology used throughout the rest of the paper. We work
with the combinatory-logic (CL) ﬁrst developed by Sch¨onﬁnkel, but popularised
by Curry. As Dougherty’s original algorithm works with polymorphic terms and
our modiﬁcation works with monomorphic terms, both are presented here.
Terms are built over a set of types. Let S be a set of sort symbols that act
as syntactic identiﬁers for the base types of the logic and Vty be a set of sort
variables. The set of types is deﬁned as:
Monomorphic Types
τ ::= σ | τ →τ
where σ ∈S
Polymorphic Types
τ ::= σ | α | τ →τ
where σ ∈S, α ∈Vty
A polymorphic type declaration is of the form Παm.τ where each αi is a type
variable and τ is a potentially polymorphic type containing type variables from
αm (αm is a list of type variables). A monomorphic type declaration is simply
τ for some monomorphic type τ.
For each type τ let Vτ be a set of term variables of type τ and let V =

τ∈T Vτ. Further, let Σ be a set of typed constant symbols. When working in
monomorphic CL, for every type τ, there exists a constant I : τ →τ ∈Σ. For
every pair of types τ, ρ, there exists a constant K : τ →ρ →τ ∈Σ and for every
triple of types τ, ρ, σ, there exists a constant S : (τ →ρ →σ) →(τ →ρ) →τ →
σ ∈Σ. The constants I, K and S are known as basic combinators. When working
in polymorphic CL, the existence of only three polymorphic basic combinators is
required. From now on, unless required for clarity, type subscripts are omitted.
We deﬁne monomorphic and polymorphic terms together as follows. Let f :
Παm.τ be a member of Σ. In the monomorphic case, m = 0. Then, f applied
to m type arguments: f⟨σm⟩is a term of type τ{αm →σm} for some tuple of
types σm. For all X ∈Vτ, X is a term of type τ. If t is a term of type τ →σ
and t′ is a term of type τ, then tt′ is a term of type σ. Where type arguments
are irrelevant, they are dropped from the presentation.
Terms of the form tt′ are called applications. Non-applicative terms are called
heads. A term can be decomposed uniquely into a head and n arguments, e.g.,
ζ t1 . . . tn or in shorter form ζ tn. By head(t) the unique head of t is intended,
e.g., head(f a b) = f or head(g⟨α⟩a) = g⟨α⟩. A head is ﬁrst-order if it is not a
variable or combinator. A term is passive if it does not have a combinator head.
The positions pos(t) of term t are deﬁned in the standard fashion; we write t|p

Restricted Combinatory Uniﬁcation
77
for the subterm of t at position p. Recall the partial ordering < on positions such
that p < p′ if t|p′ is a subterm of t|p. The set of all positions over t is denoted
pos(t) and the size of t, denoted |t|, is the cardinality of pos(t). A higher-order
subterm1 is a subterm with a variable or combinator head. A term that contains
no higher-order subterms is called ﬁrst-order. The set of ﬁrst-order positions
over a term t is deﬁned as all p ∈pos(t) such that, for all p′ < p, head(t|p′) is
not a variable or combinator. In a term ζ tn, subterms of the form ζ ti for i < n
are known as preﬁx subterms.
In what follows, capital letters such as X, Y, Z . . . are used to denote variables,
s, t, u denote arbitrary terms, a, b, c . . . denote constants.
Uniﬁcation. Uniﬁcation involves substituting terms for (free) variables in order
to make two or more terms equal. This equality could be syntactic equality, as
is generally the case in ﬁrst-order theorem proving, or equality modulo a set
of axioms. In classic HOU, the goal is to ﬁnd substitution(s) θ1 . . . θn for terms
t1, t2 . . . tn such that t1θi =βη t2θi =βη . . . =βη tnθi for all i in {1 . . . n} where
=βη is equality modulo the axioms of β and η reduction. In this paper, we are
interested in the relationship =c (deﬁned below) on terms of the combinatory
logic. A substitution that uniﬁes two or more terms is known as a uniﬁer. When
working with polymorphic terms, a substitution θ is a pair, a term substitution θ0
and a type substitution θ1. By an abuse of notation, the same symbols are used
to refer to these dual substitutions and standard monomorphic substitutions.
For two uniﬁers σ and θ, σ is more general than θ (σ ≤θ) iﬀthere exists a
substitution γ such that σγ = θ. In this case, θ is redundant. If neither σ ≤θ,
nor θ ≤σ then σ and θ are independent. In syntactic ﬁrst-order uniﬁcation, if
two terms have a uniﬁer then they have a unique (up to variable naming) most
general uniﬁer (mgu). This is not the case with HOU and the notion of mgu is
generalised to that of complete set of uniﬁers (csu). Let Γ be a set of uniﬁers of
terms t1 . . . tn. Γ is a csu iﬀfor all σ ∈unifiers(tn) such that σ /∈Γ, we have
∃σ′ ∈Γ such that σ′ ≤σ. Γ is a minimal csu iﬀfor all σ1, σ2 ∈Γ, σ1 and σ2
are independent. With respect to HOU, all minimal csus may be inﬁnite.
The HOU problem is undecidable and any complete algorithm must produce
redundant uniﬁers [16]. Let
up
= be the least congruence relation on combinatory
terms which contains {(t1, t2)|head(t1), head(t2) ∈V }. The pre-uniﬁcation prob-
lem is to ﬁnd substitutions θ1 . . . θn such that for terms t1 . . . tn, tiθk
up
= tjθk for
all i, j and k. Huet devised a famous complete algorithm for pre-uniﬁcation [16]
that is irredundant.
Deﬁnition 1. For CL terms t1 and t2, t1 =c t2 or equivalently t1 is C-equal to
t2, iﬀΛ(t1) =βη Λ(t2) where Λ is the following translation between combinatory
terms and terms of the λ-calculus.
Λ(a) = a
for a not a combinator
Λ(S) = λXY Z.XZ(Y Z)
Λ(I) = λX.X
Λ(K) = λXY.X
Λ(t1t2) = Λ(t1)Λ(t2)
1 Note that the deﬁnition of higher-order subterm here is diﬀerent to its usage in [3].

78
A. Bhayat and G. Reger
The translation Λ can be used to derive uniﬁers of λ-terms from uniﬁers of CL
terms. The details can be found in Dougherty’s paper [11]. If θ is a substitution
such that θ uniﬁes two or more terms with respect to =c, θ is referred to as a
C-uniﬁer.
Following Dougherty, a system is deﬁned as a multiset of pairs of CL terms. A
pair is trivial if its components are identical and it is valid if its components are
C-equal. The deﬁnition of ﬂex-ﬂex, ﬂex-rigid and rigid-rigid pairs is as common in
HOU literature [27]. The deﬁnitions of trivial and valid are extended to systems
in the obvious way. The extensional combinatory uniﬁcation problem is to ﬁnd,
for any given system S, a set of uniﬁers U such that ∀⟨t1, t2⟩∈S and ∀θ ∈U
t1θ =c t2θ. A system S is simple if for all pairs ⟨t, t′⟩in S, terms t and t′ do not
have identical rigid heads and both are passive.
Deﬁnition 2 (Solved System). For some system S, a pair ⟨X, t2⟩∈S is
solved if X doesn’t occur in t2 or in any other pair in S. A system S is solved
if ∀p ∈S, either p is trivial or solved.
The importance of the concept of solved systems can be seen from the fact that
the solved pairs of a solved system S form a most general uniﬁer of S (see [11]).
3
Dougherty’s Combinatory Uniﬁcation Algorithm
Dougherty’s algorithm is a complete, ﬁnitely-branching, polymorphic HOU algo-
rithm. It is presented here as a set of non-deterministic transformation rules
that act on a system of uniﬁcation pairs. Let =⇒represent the application of a
transformation rule to a system, =⇒+ the transitive closure of =⇒and =⇒∗its
reﬂexive transitive closure. We use ⊎for the multiset sum of two multisets. It
assumed that the order of the terms in the pairs is immaterial.
Polymorphism is an essential feature in Dougherty’s algorithm even if the
initial terms are monomorphic. In the SXX’-narrow transform (given below)
an applied variable head is replaced with the term S X′ X′′ and then reduced.
Applying this transform to the system {⟨Xι→ι aι, bι⟩} results in {⟨X′a(X′′a), b⟩}.
Because the type of a is ι the type of the fresh variable X′′ must be ι →?, but
there is no way to determine what type ? should be. Similarly the type of X′
must be ι →? →ι, but again ? cannot be determined. Thus, in both cases ? is set
to a type variable which may be instantiated during subsequent uniﬁcation. In
only this transformation are the types of the introduced variables not deducible.
Our presentation diﬀers from that of Dougherty’s by having WeakReduce
as a separate transformation, rather than a special case of HeadNarrow.
1. AddArg
{⟨t1, t2⟩} ⊎S =⇒{⟨t1θ d, t2θ d⟩} ⊎Sθ
Where either t1 or t2 has an under applied combinator as its head, d is a fresh
constant and θ is the type-uniﬁer of the types of t1, t2 and α →τ for fresh
type variables α and τ (in case t1 and t2 are both of atomic type).

Restricted Combinatory Uniﬁcation
79
2. Split:
{⟨X tn, h s′m s′′n⟩} ⊎S
=⇒
{⟨h X′m tn, h s′m s′′n⟩}θ ⊎Sθ
Where θ = (θ0, θ1), θ0 = {X →h X′m} and θ1 is the mgu of type(X) and
type(h s′m). Each X′
i is a fresh variable.
3. WeakReduce:
{⟨I t tn, s′⟩} ⊎S
=⇒{⟨t tn, s′⟩} ⊎S
(I-reduce)
{⟨K t t′ tn, s′⟩} ⊎S
=⇒{⟨t tn, s′⟩} ⊎S
(K-reduce)
{⟨S t t′ t′′ tn, s′⟩} ⊎S
=⇒{⟨t t′′ (t′t′′) tn, s′⟩} ⊎S
(S-reduce)
4. HeadNarrow: The variables introduced by the rules are assumed to be fresh
for the system in all cases. In all rules, θ = (θ0, θ1) where θ0 is the syntactic
uniﬁer of a non-variable preﬁx subterm and the left-hand side of a suitably
renamed combinator axiom and θ1 is the relevant type uniﬁer. For example,
in the ﬁrst rule θ0 = {X →I} and θ1 = mgu(type(X), type(I)).
{⟨X t tn, s′⟩} ⊎S
=⇒{⟨t tn, s′⟩}θ ⊎Sθ
(I-narrow)
{⟨X t tn, s′⟩} ⊎S
=⇒{⟨X′ tn, s′⟩}θ ⊎Sθ
(KX-narrow)
{⟨X t t′ tn, s′⟩} ⊎S
=⇒{⟨t tn, s′⟩}θ ⊎Sθ
(K-narrow)
{⟨X t tn, s′⟩} ⊎S
=⇒{⟨X′ t (X′′ t) tn, s′⟩}θ ⊎Sθ
(SXX’-narrow)
{⟨X t t′ tn, s′⟩} ⊎S
=⇒{⟨X′ t′ (t t′) tn, s′⟩}θ ⊎Sθ
(SX-narrow)
{⟨X t t′ t′′ tn, s′⟩} ⊎S
=⇒{⟨t t′′ (t′ t′′) tn, s′⟩}θ ⊎Sθ
(S-narrow)
These four transformation rules are collectively known as the HUT-
transformations. They are used alongside syntactic transformations Decomp,
Eliminate and TypeUnify. For a system S, its derived system is the system
of type pairs formed by replacing each term in S with its type.
5. Decomp:
{⟨f tn, f sn⟩} ⊎S =⇒{⟨t1, s1⟩. . . ⟨tn, sn⟩} ⊎S
6. Eliminate:
{⟨X, t⟩} ⊎S =⇒{⟨X, t⟩} ⊎Sθ
where θ = {X →t} and X does not occur in t.

80
A. Bhayat and G. Reger
7. TypeUnify
S =⇒Sθ
Where θ is the most general type uniﬁer of the derived system of S.
For any system S, an exhaustive application of WeakReduce, Decomp and
AddArg results in a simple system S′ with the same uniﬁers as S as proved
by Dougherty. Dougherty [11] proves the following non-deterministic algorithm,
called U, for enumerating C-uniﬁers to be sound and complete.
1. Reduce the system to a simple system then apply some HUT-transformation
out of an unsolved pair.
2. If at any point the system is syntactically uniﬁable by a pure substitution
then optionally return a most general uniﬁer of the system.
A substitution σ is pure if ∀x ∈dom(σ), σ(x) does not contain any constants
introduced by AddArg. Unfortunately U contains inﬁnite computation paths in
many cases where Huet’s classical algorithm does terminate. Worse, even when
restricted to pre-uniﬁcation, the algorithm produces redundant uniﬁers.
Lemma 1. If, for a system of uniﬁcation pairs Σ, there exists a computation
path of U that includes a HeadNarrow step, then there exists an inﬁnite com-
putation path of U on S.
Proof. Let ty1 and ty2 be meta-type variables, standing for arbitrary types. We
show that if a pair of the form ⟨Xty1→ty2 tty1 tn, t′⟩(*) is part of a simple system,
then an SXX’-narrow step can be applied and the resulting system transformed
to a simple system containing a pair of the same form.
For the application of a HeadNarrow step, there must exist a simple system
S1 such that S =⇒∗S1 by a series of U-steps and S1 includes a pair p =
⟨Xα→τ tα tn, t′⟩. Assume that t′ has a rigid head or a ﬂexible head diﬀerent to X
(if the head of t′ is X, the proof still holds, but is slightly more complex). The
pair p is of the form (*). The following HeadNarrow step can then be applied:
S1 = p ⊎S2 =⇒SXX′−narrow {⟨X′
α→γ→τ tα(X′′
α→γ tα) tn, t′⟩}θ ⊎S2θ = S3
The algorithm proceeds by reducing S3 to a simple system S4. As head(t′) ̸= X,
we have that head(t′) = head(t′θ). Since neither X′ nor head(t′) is a combina-
tor, no WeakReduce or AddArg rules can be applied to the pair during the
reduction phase. As X′ is not rigid, Decomp is not applicable either. Therefore,
the pair ⟨(X′
α→γ→τ tα (X′′ t) tn)θσ, tθσ⟩is a part of S4 where σ is a possibly
empty type substitution introduced by AddArg steps. By taking ty1 = αθσ
and ty2 = (γ →τ)θσ, we have that S4 is a simple system that contains a pair
of form (*).
Lemma 2. Even if U is restricted, such that no transformation steps are carried
out on ﬂex-ﬂex pairs, U can still produce redundant uniﬁers.

Restricted Combinatory Uniﬁcation
81
Proof. Consider the simple system {⟨Xa, a⟩}. Then by a single application of
I-narrow the uniﬁer {X →I} can be produced. Alternatively the derivation
path {⟨Xa, a⟩} =⇒SXX′−narrow {⟨X′a(X′′a), a⟩} =⇒K−narrow =⇒
{⟨a, a⟩}
can be followed leading to the uniﬁer {X →SKX′′}. SKX′′ =c I and is thus
a redundant uniﬁer.
Lemmas 1 and 2 show that Dougherty’s algorithm, whilst interesting from
a theoretical aspect, is not suitable for a practical implementation. In as yet
unpublished work [4], Bentkamp et al. present a modiﬁcation of the given-clause
algorithm that deals with possibly inﬁnite streams of uniﬁers. But even such a
method would be unable to handle Dougherty’s algorithm as almost all uniﬁca-
tion problems are likely to be non-terminating quickly leading to memory issues
on diﬃcult problems. Instead, we propose a modiﬁcation to the algorithm that
eliminates these unpleasant properties at the cost of completeness.
4
Restricted Combinatory Uniﬁcation
The pair of problems with Dougherty’s algorithm identiﬁed in the previous
section are both linked to the SXX’-narrow transform. As this step introduces
type variables, typing cannot be used to restrict its application. In our modiﬁ-
cation of Dougherty’s algorithm, we remove this head-narrow step. As this step
was the only one to introduce type variables, polymorphism can now be elim-
inated. The three polymorphic combinator axioms used in the HeadNarrow
step now become an inﬁnite set of monomorphic axioms. To this set the C and
B combinator axioms schemas are added. These schemas are BX Y Z = X(Y Z)
and CX Y Z = X Z Y . The C and B combinators are redundant, in the sense
that they can be deﬁned in terms of S, K and I, yet their usage often makes
combinatory terms smaller.
Below, the modiﬁcations to Dougherty’s algorithm are presented. The result-
ing algorithm is referred to as Restricted Combinatory Uniﬁcation or RCU.
In the calculation of the uniﬁer θ in the steps below, no type uniﬁcation is
required. Further Dougherty’s syntactic transformation step TypeUnify is no
longer required resulting in Decomp and Eliminate being the only syntactic
transforms needed.
Deﬁnition 3. The set of all variables contained in a system S, denoted vars(S),
is divided into two disjoint subsets R and B. Members of R are referred to as red
variables and members of B as blue variables. We deﬁne R = {X ∈vars(S) | X
introduced by CX-narrow transform} and B = vars(S) −R.
1. The following WeakReduce rules are added in addition to the three in
Dougherty’s algorithm.
{⟨B t t′ t′′ tn, s′⟩} ⊎S
=⇒{⟨t (t′t′′) tn, s′⟩} ⊎S
(B-reduce)
{⟨C t t′ t′′ tn, s′⟩} ⊎S
=⇒{⟨t t′′ t′ tn, s′⟩} ⊎S
(C-reduce)

82
A. Bhayat and G. Reger
2. The SXX’-narrow step is removed from Dougherty’s rules and the following
HeadNarrow steps are added. In all cases, θ is the syntactic (ﬁrst-order)
uniﬁer of a non-variable preﬁx subterm and the left-hand side of a suitably
renamed combinator axiom.
{⟨X t t′ tn, s′⟩} ⊎S
=⇒{⟨X′ (t t′) tn, s′⟩}θ ⊎Sθ
(BX-narrow)
{⟨X t t′ t′′ tn, s′⟩} ⊎S
=⇒{⟨t (t′ t′′) tn, s′⟩}θ ⊎Sθ
(B-narrow)
{⟨X t t′ tn, s′⟩} ⊎S
=⇒{⟨X′ t′ t tn, s′⟩}θ ⊎Sθ
(CX-narrow)
Where X is not a red variable
{⟨X t t′ t′′ tn, s′⟩} ⊎S
=⇒{⟨t t′′ t′ tn, s′⟩}θ ⊎Sθ
(C-narrow)
The reason for the restriction on the CX-narrow step is to prevent inﬁnite
computation paths such as ⟨Xab, s⟩=⇒⟨X′ba, s⟩=⇒⟨X′′ab, s⟩=⇒. . ..
The convention that no transformations are carried out on solved or trivial pairs
is adopted.
Restricting Dougherty’s algorithm would be of little interest if the restricted
version suﬀered the same problems as the original. This is not the case with
RCU.
Theorem 1. Every sequence of RCU transformations terminates.
Proof. A proof sketch is provided here (see Appendix A for full proof). Dougherty
proves that the set of transforms WeakReduce, Decomp and AddArg are
terminating. We reduce the proof of termination of RCU to this proof by showing
that the remaining transformation can only appear ﬁnitely many times on a
computation path. Intuitively, for any type σ, size(σ) is the number of ‘→’s in
σ.
• Each of transforms Eliminate and Split, reduce the number of unsolved
variables in a system by 1, whilst all other transforms maintain or reduce this
measure.
• For HeadNarrow, consider the measure (
v∈vars(S) size(type(v)), #S)
where #S is the number of blue variables in a system S. HeadNarrow
reduces this measure whilst WeakReduce, Decomp and AddArg main-
tain or reduce it.
An obvious corollary of Theorem 1 is that RCU is not a complete HOU
algorithm. It does not, in general, ﬁnd a csu. It would be of interest to compare
RCU with other restricted forms of higher-order uniﬁcation such as pattern
uniﬁcation [22]. However, it is not readily comparable with pattern uniﬁcation
and similar restrictions, as these tend to be restrictions on the input terms whilst
RCU is a restriction on the transformations. As such, it most closely resembles a
depth-bound version of Huet’s algorithm. We have not studied the complexity of
RCU, but in Sect. 6, empirical evidence is presented that suggests its performance
is reasonable.

Restricted Combinatory Uniﬁcation
83
5
Imperfect Filtering
In ﬁrst-order provers, uniﬁcation is generally carried out via term-indexes. Let
R be a relationship on terms. A term-indexing data-structure stores terms in a
manner that facilitates the rapid retrieval of all terms l such that R(l, t) for some
query term t. In the context of theorem-proving, the relationship R could be “is
uniﬁable with”, “is a generalisation of” etc. Indexing structures for ﬁrst-order
theorem proving have been intensively studied and eﬃcient indexing structures
such as substitution trees, ﬁngerprint indexes and perfect discrimination trees
have been developed.
These structures are either perfect or imperfect depending on whether they
return all and only those terms that match the query, or they return some
sub/superset of the same. Substitution trees store substitutions in their nodes.
Variables of the form ∗i are used to denote substitutions in the tree. The equation
∗i = t represents the substitution of the variable ∗i by the term t. A path from
the root to a leaf represents a term formed by the composition of substitutions
on the path (view Fig. 1). Substitution trees act as perfect ﬁlters for ﬁrst-order
uniﬁcation. This is achieved by traversing the tree left-right depth-ﬁrst. In the
root, the query term is uniﬁed with ∗0. Each time we move down to a node
∗i = t the current uniﬁer is extended with the uniﬁer of (∗i, t) in what is known
as incremental uniﬁcation. On backtracking, the uniﬁer is reset to its previous
value.
With respect to higher-order uniﬁcation, some work has been carried out on
developing indexing data structures [20,24], but has gained little acceptance.
Steen [28] suggests that the reason for this is the complexity or undecidability
of many of the operations required to build and maintain higher-order indexing
structures. For example, higher-order uniﬁcation, anti-uniﬁcation and matching
are all either undecidable or have large complexities. This is a daunting obsta-
cle to developing perfect higher-order indexing structures. However, it does not
preclude the development of imperfect ﬁlters.
We have modiﬁed substitution trees to act as imperfect ﬁlters for higher-order
uniﬁcation. The insights behind this modiﬁcation are two-fold:
1. If two terms disagree on a function symbol that is not below an applied
variable or combinator, then the terms have no higher-order uniﬁer
2. Two terms that are ﬁrst-order can only have a ﬁrst-order uniﬁer
Description of Filtering Algorithm. Prior to any term being inserted into
the tree all higher-order subterms are replaced with special sort-correct constants
#τ not appearing in the input. We call this process hashing and its inverse de-
hashing. When performing incremental uniﬁcation, ‘#’s unify with all terms.
Let t be a query term and T a hashed substitution tree. The ﬁltering algorithm
works as follows:
1. Calculate t′ = hash(t).

84
A. Bhayat and G. Reger
2. Run the standard ﬁrst-order algorithm to ﬁnd all uniﬁcation partners of t′
in T. Let U = {⟨σi, ti⟩|ti uniﬁes with t′ with uniﬁer σi} be the output of this
algorithm.
3. For all ⟨σi, ti⟩in U, if both t and dehash(ti) are ﬁrst-order then return σi as
the only uniﬁer of t and dehash(ti).
4. Otherwise the pair ⟨t, ti⟩has passed ﬁltering and is handed over to RCU.
Correctness of Algorithm. To show that the algorithm is correct, we need to
show that (1) if incremental uniﬁcation at a node ∗i = t fails due to Clash or
OccursCheck (terminology from [1]), then for all terms sharing this substitu-
tion, none of them can have a higher-order uniﬁer with the query term. We also
need to show that (2) if two terms are ﬁrst-order then higher-order uniﬁcation
can only produce the ﬁrst-order uniﬁer. The following two lemmas are used to
show (1).
Lemma 3. If t|p = s for ﬁrst-order position p, then (tθ|p) = sθ for all substi-
tutions θ.
Proof. Proof is by induction on the length of p. If p = ϵ then tθ|ϵ = tθ = sθ.
In the inductive case, tθ|p = tθ|p′.i
IH
= (ζ s′
1 . . . s′
i−1, s, s′
i+1 . . . s′
n)θ|i = (ζ s′
1θ . . .
s′
i−1θ, sθ, s′
i+1θ . . . s′
nθ)|i = sθ.
Lemma 4. Let p be a ﬁrst-order position in terms t1 and t2. Then t1 and t2
have no higher-order uniﬁers if:
1. Both head(t1|p) and head(t2|p) are ﬁrst order and head(t1|p) ̸= head(t2|p),
or
2. t1|p = X, X occurs in t2|p at a ﬁrst-order position and X ̸= t2|p or vice
versa.
Proof. Assume that θ is a uniﬁer of t1 and t2. Let t1|p be s1 and t2|p be s2.
By Lemma 3, (t1|p)θ = s1θ and (t2|p)θ = s2θ. Therefore, we must have that
s1θ = s2θ. In case (1), s1 = f s′n, s2 = g t′m and f ̸= g. However, s1θ = (f s′n)θ =
f (s′nθ) ̸= g (t′mθ) = (g t′m)θ and thus θ cannot be a uniﬁer. In case (2), s1 = X
and s2 = ζ tn such that for some ti and position p′, ti|p′ = X. By Lemma 3 and
the fact i.p′ is a ﬁrst-order position in s2, we have that s2θ|i.p′ = Xθ. But then
we cannot have that s1θ = s2θ since |s1θ| = |Xθ| ≤|tiθ| < |ζ (tn)θ| = |s2θ|.
Now consider incremental uniﬁcation at a node ∗i = t. Assume that ∗i is
bound to a subterm t′ of the hashed query term. If a Clash occurs in unifying t
with t′, then the query term and all terms sharing the substitution ∗i = t must
disagree on a head symbol occurring at a ﬁrst-order position. By Lemma 4, the
query term and all terms sharing the substitution ∗i = t can have no higher-
order uniﬁers. The case where uniﬁcation of t and t′ fails due to OccursCheck
is similar.
To show (2) we prove the following lemma.

Restricted Combinatory Uniﬁcation
85
Fig. 1. Example of substitution tree being used as a ﬁlter for higher-order uniﬁcation.
The query will return terms (1), (3) and (4). Higher-order uniﬁcation only needs to be
run on the pairs ⟨g(f X)b, g(X a)b⟩and ⟨g(f X)b, g(f(Z d))b⟩
Lemma 5. Let t1 and t2 be ﬁrst-order terms. Then, for any computation path
{⟨t1, t2⟩} =⇒∗S′, we have:
1. All terms in S′ are ﬁrst-order.
2. In the computation path, only Decomp and Eliminate are used.
Proof. Proof by induction on the length of the =⇒∗path. The base case is
trivial. In the inductive case, after p steps the original system is transformed into
{⟨s1, s2⟩. . . ⟨sn−1, sn⟩} where each si is ﬁrst-order by the induction hypothesis.
In the p+1 step, an arbitrary pair ⟨si, si+1⟩is transformed. If either si or si+1 is
a variable than the transformation is Eliminate and the resulting system again
contains only ﬁrst-order terms. Otherwise si and si+1 are of the form ζ tm and
ζ rm and the only applicable step is Decomp. After performing Decomp, the
resulting system is {⟨s1, s2⟩. . . ⟨tm, rm⟩. . . ⟨sn−1, sn⟩} and again all terms are
ﬁrst-order.
Since the usage of Decomp and Eliminate is deterministic, and these two
transforms form a sound and complete uniﬁcation algorithm for syntactic ﬁrst-
order uniﬁcation [1], we have that if two terms are ﬁrst-order and uniﬁable then
running RCU on the terms will result in the single mgu produced by ﬁrst-order
uniﬁcation. The result is presented in terms of RCU, but is in reality general.
Example. Consider searching the tree in Fig. 1 for uniﬁers of g(f X)b. The origi-
nal substitution is σ0 = {∗0 →g(f X)b}. In the root, the substitution is extended
to unify ∗0 with g ∗1 ∗2 resulting in σ1 = σ0 ∪{∗1 →f X, ∗2 →b}. In the left
child, ∗1 is uniﬁed with #. This succeeds without adding anything to the uniﬁer,
so σ3 = σ2. Finally, in the left-most leaf, ∗2 is uniﬁed with b, again succeeding
with the empty substitution. The search then backtracks and attempts to enter
the second-to-left leaf. This requires unifying ∗2 which is bound to b, with c
and fails due to Clash. The terms g(f X)b and g(I a)c can have no uniﬁers as

86
A. Bhayat and G. Reger
symbols b and c which are not below a variable or combinator disagree. The set
of terms eventually returned by the query is {g(X a)b, g(f a)b, g(f(Z d))b}. As
both g(f X)b and g(f a)b are ﬁrst-order, RCU does not need to run on this pair.
RCU is run on the pairs ⟨g(f X)b, g(X a)b⟩and ⟨g(f X)b, g(f(Z d))b⟩
Note that if the input problem is ﬁrst-order, then all terms in the index and
all query terms will be ﬁrst-order. Thus, in this case, uniﬁcation will always be
ﬁrst-order uniﬁcation, and the addition of RCU to Vampire is graceful in the
sense of [3].
6
Experimental Results
As with most successful ﬁrst-order provers, Vampire is a portfolio prover. In
ﬁnding a proof, it runs a set of strategies known as a schedule. Each strategy is a
predeﬁned set of proof search parameters. Normally, if a problem is solvable, it is
solvable within a short space of time using a particular strategy. In this section
we examine whether the new RCU option can be used to complement the existing
set of options available for higher-order reasoning in Vampire. To test this we ﬁrst
heuristically created a custom higher-order schedule that includes various options
(including RCU) linked to higher-order proof search. Some details regarding
these options can be found in [5]. Two options in particular are relevant to the
experiments below:
comb unif: this option can be set to on or off toggling combinatory uniﬁcation
as described in this paper
combinator elimination: this option can be set to axioms in which case
combinators are axiomatised. It can be set to inference rules to enable a
set of inferences that rewrite fully applied combinators. Axioms and rewriting
can be used together by setting the option to both or the option can be set
to off
The higher-order schedule represents our current best-eﬀort prior to these exper-
iments. It is Vampire trying its hardest rather than a default baseline.
Experiments were run across two benchmark sets. The ﬁrst set consists of
TPTP library benchmarks [31]. From the TPTP library we selected higher-
order monomorphic problems that are designated as theorems, unsatisﬁable or
unknown giving a total of 2727 problems. Satisﬁable problems were excluded
as both Vampire and Leo-III are incomplete. The second benchmark set was
produced by the Isabelle theorem prover’s Sledgehammer system. It contains
1253 benchmarks kindly made available to us by the Matryoshka team and is
called SH-λ following their naming convention.
We ran Vampire’s higher-order schedule across both benchmark sets multi-
ple times with comb unif and combinator elimination forced to various
values across the whole schedule. A (wall-clock) time limit of 520s was used for
experiments over the TPTP problem set and 300s for experiments over the SH-λ
problem set. Experiments were performed on StarExec [30] nodes equipped with
four 2.40 GHz Intel Xeon CPUs.

Restricted Combinatory Uniﬁcation
87
Table 1. Number of problems proved theorem or unsat
TPTP problems
SH-λ
Number
Uniques
Number
Uniques
Solved
All
A vs B Solved
All
A vs B
Vampire-HOL-RCUon
1907
7
713
0
Vampire-HOL-RCUon-CEaxs
1538
2
708
0
Vampire-HOL-RCUon-CEinf (A)
1728
0
276
714
0
16
Vampire-HOL-RCUon-CEoﬀ
1500
0
700
0
Vampire-HOL-RCUoﬀ
1920
30
725
1
Vampire-HOL-RCUoﬀ-CEaxs (B) 1637
15
175
715
0
17
Vampire-HOL-RCUoﬀ-CEinf
1672
0
720
0
Vampire-HOL-RCUoﬀ-CEoﬀ
1186
0
692
0
Union
2018
734
Vampire-HOL
1958
31
719
55
Leo-III
2097
58
668
31
Satallax
2095
92
513
8
The results of two sets of experiments can be found in Table 1. In the ﬁrst set,
Vampire-HOL-RCUxx-CEyyy refers to Vampire running its higher-order
schedule, with comb unif forced to value xx and combinator elimination
forced to value yyy. The second set compares Vampire-HOL (higher-order
schedule with nothing forced) to the CASC-2018 versions of leading higher-order
theorem provers, Leo-III version 1.3 and Satallax version 3.3 [6]. The number of
problems solved uniquely are given separately for the two sets of experiments.
As explained below, we make an additional comparison in the ﬁrst set. Our
experimental data is publicly available.2
Firstly, we consider the choice between reasoning solely with axioms or solely
with RCU. To do this we compare Vampire-HOL-RCUon-CEinf (A) and
Vampire-HOL-RCUoﬀ-CEaxs (B). In (B) the only method of synthesising
higher-order functions is via combinators and in (A) it is via RCU, making these
suitable versions for comparing the two methods. RCU is run in conjunction with
inference rules in our comparison due to the implementation of RCU which does
not weak reduce terms on the application of a combinatory uniﬁer. Across the
TPTP benchmark set, RCU signiﬁcantly outperforms axioms. Across the SH-λ
problem set, the performance of the two is similar.
Next we consider the other combinations of options more broadly. The strate-
gies that solve the most problems do not force RCU on. Indeed, between them,
strategies forcing RCU oﬀsolve 93 problems unsolved by forcing RCU on, con-
versely 68 are solved by forcing it on as opposed to oﬀ. This is expected as RCU
is expensive and many problems do not require it. It is the 68 problems gained
with the new option that are of interest. We also draw attention to the fact that
2 https://github.com/vprover/vampire publications/tree/master/experimental data/
CADE-2019-RCU.

88
A. Bhayat and G. Reger
Table 2. Eﬃciency of RCU
Problem
category
Average time spent on combinatory
uniﬁcation (as % of total time)
Average number of uniﬁers
produced per problem
SYO
11.71%
63764
SEU
9.5%
42968
SET
9.89%
57339
NUM
9.55%
22779
ALG
11.08%
54723
for both problem sets, the union of all problems solved by the various Vampire
versions exceeds the number of problems solved by our current schedule. This
suggests that the schedule could easily be improved by incorporating some of
these strategies.
It is interesting to note that on the SH-λ problems Vampire-HOL-RCUoﬀ-
CEoﬀwhich treats combinators as uninterpreted symbols and cannot synthesise
higher-order functions outperforms Leo-III suggesting that this problem set is
highly ﬁrst-order in nature. This may well explain why RCU performs poorly
across the problem set in general (adding it as an option to the schedule actually
reduces the number of proofs). We suspect that RCU is becoming trapped in the
high-order portions of the problems which do not need to be explored to ﬁnd a
proof. The 16 problems solved by RCU that cannot be solved with combinators
show that RCU is still a valuable option to include in the schedule. However, for
these mostly ﬁrst-order problems, the schedule requires tweaking to reduce the
prominence of RCU.
Amongst the TPTP benchmarks, Vampire solves 31 problems not solved by
Leo-III and Satallax.3 Out of these, 2 (SEV016ˆ54 and SEV032ˆ5) are diﬃculty
rating 1.00 problems, meaning that they are unsolvable by any current theorem
prover. Amongst the 31 problems 17 are solved by strategies that utilise combi-
natory uniﬁcation, showing its value. On the SH-λ benchmarks, Vampire solves
55 problems that Leo-III and Satallax cannot. Out of these, 33 are solved by
strategies that use combinatory uniﬁcation.
Finally, we investigated the eﬃciency of combinatory uniﬁcation. It would
have been interesting to compare it against ﬁrst-order uniﬁcation, but this is
not possible in Vampire as ﬁrst order uniﬁcation is carried out incrementally
whilst RCU is carried out term-to-term. Instead, the amount of time spent on
RCU in each run was recorded as well as the number of uniﬁers produced. The
results can be found in Table 2. It can be seen that combinatory uniﬁcation is
not dominating the running time. Further the number of combinatory uniﬁers
3 Some of these problems are marked as being solvable by Satallax on the TPTP
website. However, the CASC-2018 version of Satallax that we used in our tests was
unable to ﬁnd a proof.
4 This problem can be solved by the new version of E, Ehoh developed by Vukmirovi´c
et al. [32].

Restricted Combinatory Uniﬁcation
89
produced suggests that whatever the worst case complexity is, in practice the
procedure is eﬃcient.
7
Conclusion and Related Work
Pragmatic approaches to higher-order theorem proving include Otter-λ, a prover
for lambda logic [2] developed by Michael Beeson. Lambda logic is a relatively
weak extension of ﬁrst-order logic. Also included is Cruanes’ prover Zipperposi-
tion [8] which he extended to HOL. The prover rewrites using the combinator
deﬁnitions which resembles our approach, but is less goal directed.
Dowek et al. developed the deduction modulo framework [7,13] which can be
used to turn higher-order logic into a ﬁrst-order theory. It involves replacing uni-
ﬁcation with uniﬁcation modulo and adding rewrite rules for literals. Their proof
methods don’t extend to superposition and in the absence of an eﬃcient, com-
plete uniﬁcation algorithm modulo the combinator axioms, their work remains
of academic interest.
The approach we present here resembles that taken by the Leo-III higher-
order prover [29]. Leo-III implements Huet’s complete algorithm for higher-order
uniﬁcation, but then imposes a depth bound, thereby losing completeness. We
feel that RCU is more amenable to implementation within a ﬁrst-order prover
than depth-bound Huet’s algorithm and therefore our approach is complemen-
tary. By retaining the ordering restrictions of the superposition calculus, our
approach further resembles that of Leo-III which makes use of the computa-
tional path order (CPO) further losing completeness.
Related to our work is the Matryoshka team’s extension of superposition to
lambda-free higher-order logic [3] and, in as yet unpublished work, to full higher-
order logic [4]. Their approach aims for completeness and thus has to deal with
possible inﬁnite sets of uniﬁers. Due to the incompleteness of our method, there
will certainly be problems provable by a complete calculus that are inaccessible
to us. On the other hand, it appears likely that, at least on some problems,
the overhead of dealing with inﬁnite streams of possibly redundant uniﬁers will
allow lightweight but incomplete solutions such as RCU to outperform complete
methods. However, what promises to be an interesting empirical comparison of
the two approaches is future work.
As far as restrictions to higher-order uniﬁcation are concerned, our app-
roach adds to a long list of attempts to devise useful restrictions. Foremost
amongst these are pattern uniﬁcation [22] and its generalisation by Libal and
Miller [19]. Pattern uniﬁcation has gained popularity because it is decidable and
mgus exist. Bounding the number of ‘λ’s that can appear in a uniﬁer has been
shown to make higher-order uniﬁcation decidable [26]. Various other restrictions
have been shown to either be decidable or undecidable in [18] and [25]. As far as
we are aware, we are the ﬁrst to address restrictions to combinatory higher-order
uniﬁcation algorithms.
On the mildly higher-order Sledgehammer benchmarks, Vampire with RCU
outperformed full higher-order solvers. It remains to be seen whether this results

90
A. Bhayat and G. Reger
holds across larger proof assistant generated benchmark sets. Other lines of
investigation, include implementing Dougherty’s algorithm, but utilising it in
a limited form. Consider the following inference which we call HyperEqRes:
t1 ̸= t2 ∨t3 ̸= t4 . . . ∨tn−1 ̸= tn HyperEqRes
[]
where Dougherty’s algorithm is used to ﬁnd a uniﬁer θ that simultaneously
uniﬁes each pair of terms. As this leads directly to a refutation, the problem of
redundant uniﬁers is circumvented. Likewise, polymorphism can be restricted to
the uniﬁcation algorithm. Finally, it would be interesting to evaluate the usage
of substitution trees as imperfect ﬁlters. On average what percentage of terms
in the index are discarded? In practice, how often does ﬁrst-order uniﬁcation
suﬃce?
Acknowledgements. Thanks to Jasmin Blanchette, Alexander Bentkamp, Simon
Cruanes and Petar Vukmirovi´c for many discussions on aspects of this research. A
big thanks to the Matryoshka team as a whole for sharing their benchmarks. We would
also like to thank Andrei Voronkov, Michael Rawson, Alexander Steen, the maintainers
of StarExec and the anonymous paper reviewers. Special thanks to Martin Riener for
proof-reading the paper. The ﬁrst author thanks the family of James Elson for funding
his research.
A
Restricted Combinatory Uniﬁcation Is Terminating
We prove the termination of an algorithm identical to RCU, except that reduc-
tions can be performed at all positions. That RCU is terminating follows imme-
diately. A straight-forward corollary of this is that RCU is incomplete.
Lemma 6. For any (ﬁnite) system S, if there exists an inﬁnite RCU compu-
tation path on S, then there exists a system S′ such that S =⇒∗S′ and there
exists an inﬁnite computation path on S′ that does not include the Eliminate
or Split transforms.
Proof. Both Eliminate and Split reduce the number of unsolved variables in
S. As S is ﬁnite, it can only contain a ﬁnite number of unsolved variables. A case
analysis of the other rules shows that either they reduce the number of unsolved
variables or leave it unchanged. Thus Split and Eliminate can only be carried
out a ﬁnite number of times.
⊓⊔
Based on Lemma 6, to prove that RCU is terminating, it suﬃces to prove
that RCU without the eliminate and split transforms is terminating. Call the
resulting set of transformation rules RCU−. Next a result analogous to Lemma 6
is proved with respect to the HeadNarrow transformation.
Deﬁnition 4 (Size). The size of a type σ is deﬁned inductively as follows:
• σ is atomic, the size(σ) = 0

Restricted Combinatory Uniﬁcation
91
• σ = α →β, then size(σ) = size(α) + size(β) + 1
For a term t, its type is denoted by τ(t)
Intuitively, size(σ) is the number of ‘→’s in σ.
Lemma 7. For any (ﬁnite) system S, if there exists an inﬁnite RCU−compu-
tation path on S, then there exists a system S′ such that S =⇒∗S′ and there
exists an inﬁnite computation path on S′ that does not include the head narrow
transform.
Proof. Consider the following measure on systems:
(

v∈vars(S)
size(τ(v)), #S)
Where the pairs are compared lexicographically and #S denotes the number
of blue variables in S or equivalently, the cardinality of B. The transformation
rules of RCU−, other than head narrow, keep this measure constant or reduce
it. HeadNarrow reduces the measure, so there can only be a ﬁnite number of
applications of head narrow. The former claim is demonstrated for a number of
HeadNarrow steps:
1. KX-narrow. For a variable X to be eligible for a KX-narrow step, it must
have type α →β. It is replaced by a term Kβ→α→βX′
β. Clearly size(τ(X′)) <
size(τ(X)) and so the ﬁrst item of the measure is reduced.
2. BX-narrow. For a variable X to be eligible for a BX-narrow step, it must have
type (α →β) →α →γ. It is replaced by a term B(β→γ)→(α→β)→α→γX′
β→γ.
Again size(τ(X′)) < size(τ(X)) and so the ﬁrst item of the measure is
reduced.
3. CX-narrow. In this case, the size of the type of the variable being narrowed is
not reduced. For a variable X to be eligible for a CX-narrow step, it must have
type α →β →γ. It is replaced by a term C(β→α→γ)→α→β→γX′
β→α→γ. Here
size(τ(X′)) = size(τ(X)). However, each CX-narrow step replaces a blue
variable with a red variable and therefore the second item of the measure is
reduced.
⊓⊔
Based on Lemma 7 to prove that RCU−is terminating, it suﬃces to prove
that RCU−without the HeadNarrow transformation is terminating. This is
precisely Dougherty’s VT transformations which he has proven to be terminating
in [11]. Therefore we have:
Theorem 2. Every sequence of RCU transformations terminates.
References
1. Baader, F., Nipkow, T.: Term Rewriting and All That. Cambridge University Press,
Cambridge (1999)

92
A. Bhayat and G. Reger
2. Beeson, M.: Lambda logic. In: Basin, D., Rusinowitch, M. (eds.) IJCAR 2004.
LNCS (LNAI), vol. 3097, pp. 460–474. Springer, Heidelberg (2004). https://doi.
org/10.1007/978-3-540-25984-8 34
3. Bentkamp, A., Blanchette, J.C., Cruanes, S., Waldmann, U.: Superposition for
lambda-free higher-order logic. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.)
IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 28–46. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-94205-6 3
4. Bentkamp, A., Blanchette, J.C., Tourret, S., Vukmirovi´c, P., Waldmann, U.: Super-
position with lambdas (2019, submitted for publication)
5. Bhayat, A., Reger, G.: Set of support for higher-order reasoning. In: PAAR 2018.
CEUR Workshop Proceedings, vol. 2162, pp. 2–16 (2018)
6. Brown, C.E.: Satallax: an automatic higher-order prover. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 111–117. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 11
7. Burel, G.: Embedding deduction modulo into a prover. In: Dawar, A., Veith,
H. (eds.) CSL 2010. LNCS, vol. 6247, pp. 155–169. Springer, Heidelberg (2010).
https://doi.org/10.1007/978-3-642-15205-4 15
8. Cruanes, S.: Superposition with structural induction. In: Dixon, C., Finger, M.
(eds.) FroCoS 2017. LNCS (LNAI), vol. 10483, pp. 172–188. Springer, Cham
(2017). https://doi.org/10.1007/978-3-319-66167-4 10
9. Czajka, L., Kaliszyk, C.: Hammer for coq: automation for dependent type theory.
J. Autom. Reason. 61(1), 423–453 (2018)
10. de Moura, F.L.C., Ayala-Rinc´on, M., Kamareddine, F.: Higher-order uniﬁcation:
a structural relation between Huet’s method and the one based on explicit substi-
tutions. J. Appl. Logic 6(1), 72–108 (2008)
11. Dougherty, D.J.: Higher-order uniﬁcation via combinators. Theor. Comput. Sci.
114(2), 273–298 (1993)
12. Dowek, G.: Higher order uniﬁcation via explicit substitutions. Inf. Comput. 157(1–
2), 183–235 (2000)
13. Dowek, G., Hardin, T., Kirchner, C.: Theorem proving modulo. J. Autom. Reason.
31(1), 33–72 (2003)
14. Graf, P.: Substitution tree indexing. In: Hsiang, J. (ed.) RTA 1995. LNCS, vol. 914,
pp. 117–131. Springer, Heidelberg (1995). https://doi.org/10.1007/3-540-59200-
8 52
15. Hoder, K., Voronkov, A.: Comparing uniﬁcation algorithms in ﬁrst-order theorem
proving. In: Mertsching, B., Hund, M., Aziz, Z. (eds.) KI 2009. LNCS (LNAI),
vol. 5803, pp. 435–443. Springer, Heidelberg (2009). https://doi.org/10.1007/978-
3-642-04617-9 55
16. Huet, G.: A uniﬁcation algorithm for typed λ-calculus. Theor. Comput. Sci. TCS
1(1), 27–57 (1975)
17. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
18. Levy, J.: Decidable and undecidable second-order uniﬁcation problems. In: Nip-
kow, T. (ed.) RTA 1998. LNCS, vol. 1379, pp. 47–60. Springer, Heidelberg (1998).
https://doi.org/10.1007/BFb0052360
19. Libal, T., Miller, D.: Functions-as-constructors higher-order uniﬁcation. In: 1st
International Conference on Formal Structures for Computation and Deduction
(FSCD 2016). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik (2016)

Restricted Combinatory Uniﬁcation
93
20. Libal, T., Steen, A.: Towards a substitution tree based index for higher-order res-
olution theorem provers. In: PAAR 2016. CEUR Workshop Proceedings, vol. 1635
(2016)
21. Meng, J., Paulson, L.C.: Translating higher-order clauses to ﬁrst-order clauses. J.
Autom. Reason. 40(1), 35–60 (2008)
22. Miller, D.: Uniﬁcation of simply typed lambda-terms as logic programming. In:
Logic Programming Conference, pp. 255–269. MIT Press (1991)
23. Paulson, L.C., Blanchette, J.C.: Three years of experience with sledgehammer, a
practical link between automatic and interactive theorem provers. In: IWIL 2010,
vol. 2, pp. 1–11 (2010)
24. Pientka, B.: Higher-order term indexing using substitution trees. ACM Trans.
Comput. Logic 11(1), 6:1–6:40 (2009)
25. Prehofer, C.: Decidable higher-order uniﬁcation problems. In: Bundy, A. (ed.)
CADE 1994. LNCS, vol. 814, pp. 635–649. Springer, Heidelberg (1994). https://
doi.org/10.1007/3-540-58156-1 46
26. Schmidt-Schauß, M., Schulz, K.U.: Decidability of bounded higher-order uniﬁca-
tion. J. Symb. Comput. 40(2), 905–954 (2005)
27. Snyder, W., Gallier, J.: Higher-order uniﬁcation revisited: complete sets of trans-
formations. J. Symb. Comput. 8(1–2), 101–140 (1989)
28. Steen, A.: Extensional Paramodulation for Higher-Order Logic and its Eﬀective
Implementation Leo-III. Ph.D. thesis, Freie Universit¨at Berlin (2018)
29. Steen, A., Benzm¨uller, C.: The higher-order prover Leo-III. In: Galmiche, D.,
Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 108–
116. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94205-6 8
30. Stump, A., Sutcliﬀe, G., Tinelli, C.: StarExec: a cross-community infrastructure
for logic solving. In: Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014.
LNCS (LNAI), vol. 8562, pp. 367–373. Springer, Cham (2014). https://doi.org/10.
1007/978-3-319-08587-6 28
31. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure, from CNF
to TH0, TPTP v6.4.0. J. Autom. Reason. 59(4), 483–502 (2017)
32. Vukmirovi´c, P., Blanchette, J.C., Cruanes, S., Schulz, S.: Extending a brainiac
prover to lambda-free higher-order logic. In: Vojnar, T., Zhang, L. (eds.) TACAS
2019. LNCS, vol. 11427, pp. 192–210. Springer, Cham (2019). https://doi.org/10.
1007/978-3-030-17462-0 11

dLι: Deﬁnite Descriptions in Diﬀerential
Dynamic Logic
Rose Bohrer1(B)
, Manuel Fern´andez1
, and Andr´e Platzer1,2
1 Computer Science Department, Carnegie Mellon University,
Pittsburgh, USA
rose.bohrer.cs@gmail.com, {manuelf,aplatzer}@andrew.cmu.edu
2 Fakult¨at f¨ur Informatik, Technische Universit¨at M¨unchen,
Munich, Germany
Abstract. We introduce dLι, which extends diﬀerential dynamic logic
(dL) for hybrid systems with deﬁnite descriptions and tuples, thus
enabling its theoretical foundations to catch up with its implementation
in the theorem prover KeYmaera X. Deﬁnite descriptions enable partial,
nondiﬀerentiable, and discontinuous terms, which have many examples
in applications, such as divisions, nth roots, and absolute values. Tuples
enable systems of multiple diﬀerential equations, arising in almost every
application. Together, deﬁnite description and tuples combine to support
long-desired features such as vector arithmetic.
We overcome the unique challenges posed by extending dL with these
features. Unlike in dL, deﬁnite descriptions enable non-locally-Lipschitz
terms, so our diﬀerential equation (ODE) axioms now make their conti-
nuity requirements explicit. Tuples are simple when considered in isola-
tion, but in the context of hybrid systems they demand that diﬀerentials
are treated in full generality. The addition of deﬁnite descriptions also
makes dLι a free logic; we investigate the interaction of free logic and the
ODEs of dL, showing that this combination is sound, and characterize its
expressivity. We give an example system that can be deﬁned and veriﬁed
using these extensions.
Keywords: Dynamic logic · Deﬁnite description · Hybrid systems ·
Theorem proving · Uniform substitution · Partial functions
1
Introduction
Cyber-physical systems (CPSs) such as self-driving cars, trains, and airplanes
combine discrete control and continuous physical dynamics and are often safety-
critical because they operate around humans. Thus, it is essential to achieve the
highest possible conﬁdence in their correctness, e.g., using formal methods with
strong theoretical foundations. Diﬀerential dynamic logic (dL) [18,22,23] is a
This research was sponsored by NDSEG, the AFOSR under grant number FA9550-16-
1-0288, and the Alexander von Humboldt Foundation.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 94–110, 2019.
https://doi.org/10.1007/978-3-030-29436-6_6

dLι: Deﬁnite Descriptions in Diﬀerential Dynamic Logic
95
logic for formal veriﬁcation of hybrid systems [10], widely-used models of CPSs
that incorporate both their discrete and continuous behaviors. Among formal
methods for CPSs, dL is notable both for its case studies [12,15,16] using the
KeYmaera X [9] theorem prover, and for its strong foundations, as evidenced by
its completeness results [18,22,23,25] and a formal proof of soundness in both
Isabelle/HOL and Coq [4].
However, there is a tension between the goals of practical applicability
and rigorous foundations. In practice, theorem prover implementations often
demand new features which were not anticipated in theory. Formalizations of
KeYmaera X [5], Coq [2], and NuPRL [1] all omit or simplify whichever practi-
cal features are most theoretically challenging for their speciﬁc logic: discontinu-
ous and partial terms in KeYmaera X, termination-checking in Coq, or context
management in NuPRL. When formalizations of theorem provers do succeed in
reﬂecting the implementation [13], they owe a credit to the generality of the
underlying theory: it is much more feasible to formalize a general base theory
than to formalize multiple ad-hoc extensions as they arise.
This paper introduces dLι, a new, generalized foundation for dL where deﬁnite
description ιx φ denotes the unique x for which φ holds, enabling practical exten-
sions like divisions θ1/θ2, roots
n√
θ, and the functions min(θ1, θ2), max(θ1, θ2),
and |θ|, while pairs (θ1, θ2) enable diﬀerential equation (ODE) systems. Useful
new features like trigonometric functions and vectors are also deﬁnable, and
existing features like diﬀerentials (θ)′ have elegant new axiomatizations in dLι.
The term ιx φ is the deﬁnite (i.e., requiring unique existence) counter-
part of Hilbert’s choice εx φ; both have seen success in HOL-style theorem
provers [17,26]. We chose deﬁnite ιx φ over εx φ because uniqueness signiﬁcantly
simpliﬁes continuity and diﬀerential reasoning. In adopting deﬁnite descriptions
and tuples in dL, we solve the novel challenges of integrating them with diﬀeren-
tial equations, dL’s distinguishing feature. Deﬁnite descriptions allow partiality,
discontinuity, and nondiﬀerentiability, all of which interact subtly with sound
ODE reasoning. Multidimensional systems, enabled by tuples, demand a gen-
eral treatment of diﬀerentials and expose subtle variable dependencies in some
advanced ODE reasoning principles.
An example demonstrates the power of deﬁnite description: deﬁnite descrip-
tions allow non-polynomial terms and thus non-polynomial ODEs, which need
not have unique solutions. While non-polynomial ODEs (and all of dLι) are
reducible to dL in theory, the reduction of ιx φ is completely impractical [3].
Expressivity comes with deep semantic changes: supporting partiality makes dLι
a free logic, for which we adopt a 3-valued Lukasiewicz semantics. We show this
profound change in foundations needs only small changes to the proof calculus
with additional deﬁnedness conditions. We develop the theory of dLι, show that
the proof calculus is sound and show the nontrivial reduction from dLι to dL.
2
Syntax
We present the core syntax of dLι, which extends dL with deﬁnite descriptions
and tuples. We describe the constructs informally here, deferring formal seman-

96
R. Bohrer et al.
tics to Sect. 3. As a free logic [8], dLι contains terms that do not denote and
formulas whose truth values are unknown (truth is indicated ⊕, falsehood by
⊖, and unknown by ⊘), a major point of diﬀerence between our semantics and
proof calculus vs. those of dL. Our calculus uses uniform substitution [6, §35,§40],
where symbols ranging over predicates, programs, etc. are explicitly represented
in the syntax, because it has simpliﬁed the construction of dL calculi [23], imple-
mentations [9], and machine-checked correctness proofs [4]. This will ease imple-
menting dLι and mechanizing the soundness proof in future work. The syntax of
dLι is divided into terms, programs, and formulas, whose deﬁnitions, unlike in
dL, are all mutually recursive. The terms θ of dLι extend the terms of dL with
deﬁnite descriptions, pairs, and reductions:
θ ::= q | x | f(θ) | θ + θ | θ · θ | (θ)′ | ιx φ | (θ, θ) | red(θ, s θ, lr θ)
for literal q ∈Q and variable x ∈V, where V is the set of all variable names,
f is a function symbol, and φ is a formula. The ﬁrst six cases, polynomials,
diﬀerentials, and function symbols, are as in dL. Variables are ﬂexible: they are
modiﬁed by quantiﬁers and programs. Variables x always denote some value
and so assignments succeed only when the RHS denotes a value. In contrast,
f(θ) is an uninterpreted function f applied to term θ, but both θ and f(θ) are
allowed to be non-denoting. While function symbols f rarely appear in theorem
statements, they are essential for the axioms of Sect. 5. The deﬁnite description
ιx φ denotes the unique value of x that makes formula φ true, if exactly one such
value exists, else it does not denote (since description is deﬁnite). Pairs (θ1, θ2)
can be nested to arbitrary ﬁnite depth, so their eliminator is primitive recursion
on binary trees with values at the leaves. Reduction red(θ1, s θ2, lr θ3) reduces
every leaf t ∈R to θ2
t
s and reduces every pair a, b of recursive results to θ3
a
l
b
r,
where ey
x is the capture-avoiding substitution of y for every x in e. For example,
if θ1 = ((−1, 2), −3), then the reduction red(θ1, s s2, lr (r, l)) is the elementwise
square of the reverse tree, (9, (4, 1)).
The programs α, β of dLι are hybrid programs, a program syntax for hybrid
systems combining discrete and continuous dynamics. Hybrid programs of dLι are
identical to those of dL with the exception that any formula or term contained
therein is again any formula or term of dLι, not necessarily just dL. For any
starting state, a program α might transition to zero, one, or many ﬁnal states.
Whenever a program transitions to zero states, we say it aborts.
α, β :: = x := θ | x′ = θ & ψ | ?φ | α ∪β | α; β | α∗| a
Assignments x := θ assign the value of term θ to variable x, if θ denotes a value,
else they abort. Tests ?φ abort execution if formula φ is not true, else they are
no-ops. Nondeterministic choices α ∪β behave as either α or β, nondetermin-
istically. Sequential composition α; β performs β in any state resulting from α.
Loops α∗repeat α sequentially any number of times, nondeterministically. The
deﬁning construct of hybrid programs are the diﬀerential equations x′ = θ & ψ,

dLι: Deﬁnite Descriptions in Diﬀerential Dynamic Logic
97
which continuously evolve x according to the diﬀerential equation x′ = θ for any
duration such that term θ denotes and formula ψ is true throughout. Note the
core syntax of dLι need only contain systems of a single variable x: in Sect. 4 we
will derive systems with multiple variables from systems of one variable. Unin-
terpreted program constants a range over programs. We parenthesize programs
α as {α} with braces for disambiguation and readability. The formulas φ, ψ of
dLι are deﬁned inductively:
φ, ψ :: = φ ∧ψ | ¬φ | ∀x φ | θ1 ≥θ2 | [α]φ | p(θ)
Conjunctions φ ∧ψ, negations ¬φ, and quantiﬁers ∀x φ are as is standard in
ﬁrst-order Lukasiewicz [14] logic. The quantiﬁer ∃x φ is also as in ﬁrst-order
Lukasiewicz logic and can be derived ∃x φ ≡¬∀x ¬φ. In comparing θ1 ≥θ2,
if terms θ1 and θ2 both denote reals, those reals are compared, if they both
denote tuples they are compared elementwise, in all other cases the result is
unknown (⊘). The deﬁning construct of dynamic logics is [α]φ, which says φ
holds in all states reachable by running α. Its dual, ⟨α⟩φ, says there exists a state
reachable by running α where φ holds, and can be derived by the equivalence
⟨α⟩φ ≡¬[α]¬φ. Uninterpreted predicate symbols p expect terms θ, which are
also allowed not to denote, as arguments, and are allowed truth value unknown
(⊘). We write P, Q for predicates which take all variables as arguments. We
sometimes write the implication φ →ψ as ψ ←φ for emphasis on ψ.
Example 1 (Robot Water Cooler).
The textbook examples of non-Lipschitz
ODEs are those of form h′ = k ·
√
h for constant k. In dLι, in contrast to dL,
non-Lipschitz terms simplify describing a hybrid system with such ODEs, which
we base on Hubbard’s leaky bucket [11, §4.2]. Consider a water cooler of height
h and an opening of surface area a in its bottom of surface area A, where g is
acceleration due to gravity. Suppose an enterprising student has equipped the
cooler’s valve with robotic control. We could then model the cooler as:
αB ≡

{?h > 0; a := 1} ∪a := 0

; h′ = −

2gh a
A & h ≥0
∗
This says that so long as there is water in the cooler (?h > 0) we can choose
to open the valve (a := 1), but we can always close the valve (a := 0). Then
the water drains out the cooler at a rate proportional to the square root of the
current volume by Torricelli’s Law [7], or rate 0 if the valve is closed. This control
process repeats arbitrarily often. The constructs √2gh (root) and
a
A (division)
are not core dL, but we can rewrite αB using deﬁnite descriptions:

{?h > 0; a := 1} ∪a := 0

; h′ = −(ιy y2 = 2gh ∧y ≥0)(ιz zA = a) & h ≥0
∗
This example is representative because the ODE is non-Lipschitz: the solution
is unique at h = 0 only within the constraint h ≥0. The terms √2gh and a
A are
also both partial: deﬁned only assuming gh ≥0 and A ̸= 0, respectively. The
interactions between partiality, uniqueness, and the constraint combine to make
the proof subtle, even if short.

98
R. Bohrer et al.
Common dL (and likewise, dLι) theorems include safety assertions of the form
φ →[α]ψ which say that if φ holds initially, then ψ will necessarily hold after α.
For example, we might wish to prove the ﬁnal water height of αB never exceeds
the initial height, so it is actually leaky (or at least is not ﬁlling up):
Proposition 1 (Leakiness). This is valid (deﬁnitely true ⊕in all states):
g > 0 ∧h = h0 ∧h0 > 0 ∧A > 0 →[αB](h ≤h0)
We will prove Proposition 1 after we have introduced a proof calculus for dLι in
Sect. 5.
3
Denotational Semantics
We now formally deﬁne the semantics of dLι terms, formulas, and programs. Due
to the presence of deﬁnite descriptions ιx φ(x), not every dLι term denotes in
every state, i.e., dLι is a free logic [8]. We write ⊥for the interpretation of a
term that does not denote any value. When a term denotes, it denotes a ﬁnite,
binary tree with real values at the leaves: a scalar denotes a singleton tree, while
(arbitrarily nested) pairs denote non-singleton trees. We refer to the set of all
real trees as Tree(R), where for any S, Tree(S) is the smallest set such that:
(i) S ⊆Tree(S), and (ii) for any l and r ∈Tree(S), (l, r) ∈Tree(S). Typing
is extrinsic, i.e., we do not make typing distinctions between R and Tree(R)
in the semantics; typing constraints will be expressed explicitly as predicates.
To account for non-denoting terms, formulas can take on three truth values: ⊕
(deﬁnitely true), ⊘(unknown), and ⊖(deﬁnitely false). Thus dLι is a 3-valued
logic, and ﬁrst-order connectives use the Lukasiewicz [14] interpretation.
The interpretation functions are parameterized by state ω : V →Tree(R)
mapping variables to values, and by an interpretation I mapping function sym-
bols, predicate symbols, and program constants to their interpretation, including
the possibility of not denoting a value. Writing S for the set of all states, we
have I(f) : (Tree(R) ∪⊥) →(Tree(R) ∪⊥), I(p) : (Tree(R) ∪⊥) →{⊕, ⊘, ⊖},
and I(a) : ℘(S × S), where ℘(U) is the power set of a set U. Below, ωt
x is the
state that is equal to ω except at x, where ωt
x(x) = t.
Deﬁnition 1 (Term semantics). The denotation of a term is either a tree or
undeﬁned, i.e. Iω[[θ]] : Tree(R) ∪{⊥}, and is inductively deﬁned as:
Iω[[q]] = q
Iω[[x]] = ω(x)
Iω[[f(θ)]] = I(f)(Iω[[θ]])
Iω[[θ1 + θ2]] = Iω[[θ1]] + Iω[[θ2]] if Iω[[θ1]], Iω[[θ2]] ∈R
Iω[[θ1 · θ2]] = Iω[[θ1]] · Iω[[θ2]]
if Iω[[θ1]], Iω[[θ2]] ∈R

dLι: Deﬁnite Descriptions in Diﬀerential Dynamic Logic
99
Iω[[ιx φ]] =

t
if a unique t ∈Tree(R) has Iωt
x[[φ]] = ⊕
⊥
otherwise
Iω[[(θ1, θ2)]] = (Iω[[θ1]], Iω[[θ2]]) if Iω[[θ1]], Iω[[θ2]] ̸= ⊥
Iω[[red(θ1, s θ2, lr θ3)]] = Fold(Iω[[θ1]], s θ2, lr θ3, Iω) if Iω[[θ1]] ̸= ⊥
Iω[[(θ)′]] =

x∈V
ω(x′)∂Iω[[θ]]
∂x
if I[[θ]] totally diﬀerentiable at ω
Iω[[θ]] = ⊥in all other cases
where ω(x′) ∂Iω[[θ]]
∂x
abuses notation: when ω(x) is a tuple, the partial is taken
w.r.t. each leaf in x, scaled by the corresponding component of x′; the subtleties
of semantics for diﬀerentials are discussed at greater length in the companion
report [3]. In previous formalisms for dL [23] the semantics of (θ)′ do not explicitly
require that θ is totally diﬀerentiable because all pure dL terms are already
smooth, thus totally diﬀerentiable. In contrast, not all dLι terms are smooth,
thus we require total diﬀerentiability explicitly, as it is required for soundness
(speciﬁcally of DI≥, Sect. 5). If diﬀerentiability conditions are not met, then
Iω[[(θ)′]] = ⊥. Reductions Fold(t, s θR, lr θT , Iω) recurse on t:
Fold(t, s θR, lr θT , Iω) = Iωt
s[[θR]] when t ∈R
Fold((L, R), s θR, lr θT , Iω) = IωK
l
S
r [[θT ]] where
K = Fold(L, s θR, lr θT , Iω), S = Fold(R, s θR, lr θT , Iω)
That is, they reduce singletons t by binding s to t in θR, and reduce node (L, R)
by binding l, r to the reductions of the respective branches in θT .
Deﬁnition 2 (Formula semantics). The formula semantics are 3-valued:
Iω[[φ ∧ψ]] = Iω[[φ]] ⊓Iω[[ψ]]
Iω[[¬φ]] = Iω[[φ]]
Iω[[∀x φ]] =

t∈Tree(R)
Iωt
x[[φ]]
Iω[[[α]φ]] =

(ω,ν)∈I[[α]]
Iν[[φ]]
Iω[[θ1 ≥θ2]] = Geq(Iω[[θ1]], Iω[[θ2]])
Iω[[p(θ)]] = I(p)(Iω[[θ]])
Geq(r1, r2) = r1 ≥r2 if r1, r2 ∈R
Geq((l1, r1), (l2, r2)) = Geq(l1, l2) ⊓Geq(r1, r2)
Geq(v1, v2) = ⊘otherwise

100
R. Bohrer et al.
p ⊓q
q = ⊕⊘⊖
p = ⊕
⊕⊘⊖
p = ⊘
⊘⊘⊖
p = ⊖
⊖⊖⊖
p p = ⊕⊘⊖
⊖⊘⊕
p →q q = ⊕⊘⊖
p = ⊕
⊕⊘⊖
p = ⊘
⊕⊕⊘
p = ⊖
⊕⊕⊕
p ↔q q = ⊕⊘⊖
p = ⊕
⊕⊘⊖
p = ⊘
⊘⊕⊘
p = ⊖
⊖⊘⊕
Implication p →q can be intuited as p ≤q, (where ⊖< ⊘< ⊕) so (p →q)
is ⊕even when p = q = ⊘. Conjunction p ⊓q takes the minimum value of the
arguments, and is unknown ⊘when the least conjunct is ⊘. Equivalence p ↔q is
reﬂexive (even (⊘↔⊘) is ⊕), but is ⊘in all other cases where some argument is
⊘. We say a formula φ is valid if Iω[[φ]] = ⊕for all ω and I. Comparisons θ1 ≥θ2
are taken elementwise and are unknown (⊘) for diﬀering shapes. Predicates p are
interpreted by the interpretation I. The meaning of quantiﬁers ∀x φ and [α]φ are
taken as conjunctions ⊓S over potentially-uncountable index sets S. The value
of ⊓S is the least value of any conjunct, one of {⊖, ⊘, ⊕}.
Deﬁnition 3 (Program semantics). Program semantics generalize those of
dL as conservatively as possible so that veriﬁcation ﬁnds as many bugs as possible:
e.g. assignments of non-denoting terms and tests of unknown formulas abort. The
denotation of a program α is a relation I[[α]] where (ω, ν) ∈I[[α]] whenever ﬁnal
state ν is reachable from initial state ω by running α.
I[[x := θ]] = {(ω, ωIω[[θ]]
x
) | Iω[[θ]] ̸= ⊥}
I[[?φ]] = {(ω, ω) | Iω[[φ]] = ⊕}
I[[α ∪β]] = I[[α]] ∪I[[β]]
I[[α; β]] = I[[α]] ◦I[[β]]
I[[α∗]] = I[[α]]∗=
	
n∈N
I[[α; · · · ; α



n times
]]
I[[x′ = θ & ψ]] = {(ω, ν) | ω = ϕ(0) on {x′}∁and ν = ϕ(r) for some ϕ : [0, r] →S
which solves x′ = θ & ψ, i.e., for s ∈[0, r], ∂ϕ(t)(x)
∂t
(s) = ϕ(s)(x′)
and Iϕ(s)[[x′ = θ ∧ψ]] = ⊕and ϕ(s) = ϕ(0) on {x, x′}∁}
where X∁is the complement of set X. ODEs x′ = θ & ψ are initial value prob-
lems: (ω, ν) ∈I[[x′ = θ & ψ]] if some solution ϕ of some duration r ∈R≥0 takes ω
to ν while satisfying ψ throughout. A solution ϕ must satisfy x′ = θ as an equa-
tion, satisfy constraint ψ, and assign the time-derivative of each x to each x′.
The initial value of x′ is overwritten and variables except x, x′ are not changed.
Assignments x := θ are strict: they store the value of θ in variable x, or abort
if θ does not denote a value. Tests ?φ succeed if φ is deﬁnitely true (⊕); both
the unknown (⊘) and deﬁnitely false (⊖) cases abort execution. Likewise, the
domain constraint ψ of a diﬀerential equation x′ = θ & ψ must be deﬁnitely-true
(⊕) throughout the entire evolution and the term θ implicitly must denote values
throughout the evolution, since Iϕ(s)[[x′ = θ ∧ψ]] = ⊕.

dLι: Deﬁnite Descriptions in Diﬀerential Dynamic Logic
101
4
Derived Constructs
A key beneﬁt of dLι is extensibility: Many term constructs can be deﬁned with
deﬁnite descriptions ιx φ and tuples which otherwise require unwieldy encodings
as formulas. In this section we reap the beneﬁts of extensibility by deﬁning such
new term constructs.
Arithmetic Operations. In practice, we often wish to use arithmetic operations
beyond the core dL operations. Figure 1 demonstrates basic arithmetic operations
which have simple deﬁnitions in dLι but not as terms in dL: Of these, max, min,
and | · | preserve Lipschitz-continuity but not diﬀerentiability. Roots
√
θ can vio-
late even Lipschitz-continuity and both roots and divisions are non-total. In
practice (as in Example 1), these operators are used in ODE models, making
their continuity properties essential. Since pure dL requires smooth terms [23],
even functions max and min would be encoded as formulas in pure dL.
(if(φ)(θ1)else(θ2)) = ιx (φ ∧x=θ1) ∨(¬φ ∧x=θ2)
max(θ1, θ2) = ιx (θ1 ≥θ2 ∧x = θ1) ∨(θ2 ≥θ1 ∧x = θ2)
min(θ1, θ2) = ιx (θ1 ≥θ2 ∧x = θ2) ∨(θ2 ≥θ1 ∧x = θ1)
|θ| = max(θ, −θ)
√
θ = ιx (x2=θ ∧x ≥0)
θ1/θ2 = ιx (x · θ2=θ1)
(sin θ, cos θ) = ιz [t := 0; s := 0; c := 1; s′=c, c′=−s, t′=1; ?t=θ]z=(s, c)
Fig. 1. Derived arithmetic operations (for fresh x, t, c, s, z)
Tuples. We make tuples ﬁrst-class in dLι to simultaneously simplify the treat-
ment of ODEs compared to prior work [18] and provide support for data struc-
tures such as vectors, widely used in physical computations. In contrast to
the ﬂexible function symbols (think: unbounded arrays) of QdL [20], they are
equipped with a primitive recursion operator, making it easier to write sophis-
ticated functional computations. These structures can be used in systems with
non-scalar inputs, for example a robot which avoids a list of obstacles [16].
While pairs (θ1, θ2) are core dLι constructs, the left and right projections π1θ
and π2θ are derivable, as are convenience predicates inR(θ) and isT(θ) which
hold exactly for scalars and tuples, respectively:
π1θ ≡ιl ∃r (θ = (l, r))
π2θ ≡ιr ∃l (θ = (l, r))
inR(θ) ≡(red(θ, s 1, lr 0) = 1)
isT(θ) ≡(red(θ, s 1, lr 0) = 0)
When combined with the reduce operation on trees, these operations can
be used to implement a variety of data structures. Figure 2 shows an example
library of operations on lists. Lists are represented as nested pairs, with no
special terminator. We name an argument L to indicate its intended use as a
list rather than an arbitrary tree. Lists are trees whose left-projections are never
pairs. Additional data structures are shown in the report [3].

102
R. Bohrer et al.
Systems of ODEs. Tuples reduce ODE systems to individual ODEs, e.g.:
{x′
1=θ1, x′
2=θ2} ≡

z := (x1, x2); {z′ = (θ1
πjz
xj , θ2
πjz
xj )}; x1 := π1z; x2 := π2z

While this encoding is simple, it will enable us in Sect. 5 to support systems
of any ﬁnite dimension in axiom DG, which implementation experience [9] has
shown challenging due to the variable dependencies involved.
map2(T, f(x, y)) = red(T, s s, lr if(inR(r))(f(l, r))else{(f(π1l, π2l), r)})
snoc(L, x) = red(L, s (s, x), lr (π1l, r))
rev(L) = red(L, s s, lr snoc(r, l))
zip(L1, L2) = π1red(rev(L1), s ((s, π1L2), π2L2), lr (((π1π1l, π1π2r), π1r), π2π2r))
(L1+L2) = map2(zip(L1, L2), x + y)
L1·L2 = red(map2(zip(L1, L2), x · y), s s, lr l + r)
Fig. 2. Example vector functions
Types and Deﬁnedness. Many of the operations in dLι expect, for example, reals
or terms that denote values. For simplicity, we make these type distinctions
extrinsically: core dL terms are untyped, and proposition inR(θ) says θ belongs
to type R. Typed quantiﬁers are deﬁnable, e.g., ∀x : R φ ≡∀x (inR(x) →φ).
Whether a term denotes is also treated extrinsically. Formula E(θ) ≡D(θ = θ)
only holds for terms that denote, where D(φ) says φ is deﬁnitely true, which has
truth value ⊕when φ has truth value ⊕and has value ⊖otherwise. We give its
truth table and a deﬁnition:
p
⊕⊘⊖
D(p) ⊕⊖⊖
D(φ) ≡¬(φ →¬φ)
That is, D(φ) collapses ⊘into ⊖. These constructs are used in the axioms of
Sect. 5. In the same spirit, we sometimes need to know that a function f(x) (of
any dimension) is continuous, but derive this notion. We write Con(f(x)) to say
that f(x) is continuous as x varies around its current value:
Con(f(x)) ≡D(∀ξ ∃δ ∀y (0 < ∥y −x∥< δ →∥f(y) −f(x)∥< ξ))
Note that when Con(f(x)) holds, the shape of f(x) is constant in a neighborhood
of x, since the Euclidean norm ∥f(y)−f(x)∥does not exist when f(y) and f(x)
diﬀer in shape. Likewise, Con(f(x)) requires only continuity on y whose shape
agrees with that of x, since the Euclidean norm ∥y−x∥does not otherwise exist.
5
dLι Axioms
Our proof system is given in the Hilbert style, with a minimum number of
proof rules and larger number of axioms, each of which is an individual concrete
formula. The core proof rule is uniform substitution [23][6, §35,§40]: from the

dLι: Deﬁnite Descriptions in Diﬀerential Dynamic Logic
103
validity of φ we can conclude validity of σ(φ) where the uniform substitution σ
speciﬁes concrete replacements for some or all predicates, functions, and program
constants in a formula φ:
(US)
φ
σ(φ)
The soundness side-conditions to US about σ are non-trivial, and make up much
of its soundness proof in Sect. 6. The payoﬀis that uniform substitution enables
a modular design where such subtle arguments need only be done once in the
soundness proof of the US rule, and every axiom, which is now an individual
concrete dLι formula, is signiﬁcantly simpler to prove valid and to implement.
[·] ⟨a⟩P ↔¬[a]¬P
[:=] ([x := f]p(x) ↔p(f)) ←E(f)
[?] [?Q]P ↔(D(Q) →P)
[∪] [a ∪b]P ↔[a]P ∧[b]P
[; ] [a; b]P ↔[a][b]P
[∗] [a∗]P ↔P ∧[a][a∗]P
∀i (∀x p(x)) →(E(f) →p(f))
∀→∀x (p(x) →q(x)) →∀x p(x) →∀x q(x)
K [a] (P →Q) →([a]P →[a]Q)
I [a∗]D(P →[a]P)→D(P →[a∗]P)
V p →[a]p
G
P
[a]P
∀
p(x)
∀x p(x)
MP P →Q
P
Q
V∀p →∀x p
Fig. 3. Discrete dL axioms
Figure 3 gives axioms and rules for the discrete programming constructs,
which are generalizations of corresponding axioms [23] for dL to account for non-
denoting terms and unknown formulas. Axioms are augmented with deﬁnedness
conditions whenever multiple occurrences of terms or formulas diﬀer in their
tolerance for partiality. The conclusion (in canonical usage) of each axiom is
highlighted in blue, while any diﬀerence from the dL axioms is highlighted in red.
Recall the operator D(φ) says φ is deﬁnitely true. For example, axiom [?] says
that a test ?Q succeeds when Q is deﬁnitely true. The induction axiom I requires
the inductive step proved deﬁnitely true, but concludes deﬁnite truth. The other
axioms for program constructs ([·], [∪], [; ], [∗]) carry over from dL without mod-
iﬁcation, since partiality primarily demands changes when mediating between
formulas and programs or between terms and program variables. As is standard
in free logics, axiom ∀i says that since quantiﬁers range over values, they must
be instantiated only to terms that denote values. Assignments [:=] require the
assigned term to denote a value, since program variables x range over values.
Figure 4 gives the dLι generalizations of dL’s axioms for reasoning about dif-
ferential equations: DC is generalized by analogy to [?] to require deﬁnite truth

104
R. Bohrer et al.
DW [x′ = f(x)&q(x)]q(x)
DC [x′ = f(x)&q(x)]p(x) ↔[x′ = f(x)&q(x) ∧r(x)]p(x)

←D([x′ = f(x)&q(x)]r(x))
DE [x′ = f(x)&q(x)][x′ := f(x)]p(x, x′) ↔[x′ = f(x)&q(x)]p(x, x′)
DI≥
[x′ = h(x)&q(x)]f(x) ≥g(x) ↔[?q(x)]f(x) ≥g(x)

←[x′ = h(x)&q(x)](f(x))′ ≥(g(x))′
DG
∀x (q(x) →Con(a(x)) ∧Con(b(x)))
→
[x′ = f(x)&q(x)]p(x) ↔∃y : R [x′=f(x), y′ = a(x)y + b(x)&q(x)]p(x)

DS ∀t : R ((∀0≤s≤t q(x + fs)) →[x := x + ft]p(x))

→[x′ = f & q(x)]p(x)
(θ)′ (f(x))′ = x′ · ιM ∀ξ>0 ∃δ ∀y D(0<∥y−x∥<δ →f(y)−f(x)−M(y−x)<ξ∥y−x∥)
←E((f(x))′)
E(′) E((f(x))′)←E(ιM ∀ξ>0 ∃δ ∀y D(0<∥y−x∥<δ →f(y)−f(x)−M(y−x)<ξ∥y−x∥))
Fig. 4. Diﬀerential equation axioms and diﬀerential axioms
and DG is generalized to require continuity, otherwise the axioms carry over
unchanged. DW says the constraint of an ODE always holds as a postcondi-
tion. DC says any postcondition which is proven (deﬁnitely) true may be added
to the constraint. DE says the ODE holds as an equation in the postcondi-
tion. DI≥is the diﬀerential induction [19] axiom for proving nonstrict inequal-
ities f(x) ≥g(x) follow from their diﬀerential formula (f(x))′ ≥(g(x))′. The
strict case f(x) > g(x) is analogous; axioms for equality, inequality, conjunc-
tion, and disjunction can be derived from these. Note the assumptions in DI≥
hold only when f(x) and g(x) are totally diﬀerentiable within the constraint, as
required for soundness. DG allows extending a system with an additional ghost
dimension, and is used for everything from solving systems to reasoning about
exponentially-decaying systems [25]. The new dimension is required to be Lip-
schitz so that solutions exist and is required to be linear in the new variables
so that the solutions of the extended system exist as long as those of the initial
system. DS says the solution of a constant ODE system is linear. To solve mul-
tidimensional systems with DS, interpret x + fs and x + ft as pairwise vector
sums per Fig. 2. Axiom (θ)′ expands a diﬀerential (f(x))′ according to the deﬁni-
tion of total diﬀerential. It assumes E((f(x))′) because equalities are not allowed
to hold between non-denoting terms; proving these assumptions is enabled by
axiom E(′). In practice, axioms are derived from E(′) for each case and applied
recursively to automatically prove existence, for example:
E((f(x))′) ∧E((g(x))′) →E(((f + g)(x))′)
is used to show diﬀerentials of sums exist. Likewise, axiom (θ)′ is long-winded
for practical proving, so we will use it to implement simpler special-case axioms
in Example 2. The deﬁnition of (θ)′ above only supports real-valued x and f(x),

dLι: Deﬁnite Descriptions in Diﬀerential Dynamic Logic
105
because scalar diﬀerences f(y) −f(x) and y −x only denote a value when
x, y, f(x), and f(y) are reals. The report [3] discusses its generalization to tree-
valued functions of tree-valued arguments.
ι p(ιz p(z)) ↔∃x p(x) ∧∀y (p(y) →y = x)

=T l1=l2 ∧r1=r2 ↔(l1, r1)=(l2, r2)
QE
∗

x∈V(φ) inR(x)

→φ
(where φ is valid in ﬁrst-order real arithmetic)
redT red((L, R), s f(s), lr g(l, r)) = g red(L, s f(s), lr g(l, r)), red(R, s f(s), lr g(l, r))

redR inR(r) →red(r, s f(s), lr g(l, r)) = f(r)
TreeI D

p(ιx 0 = 1) ∧∀s inR(s) →p(s)

∧∀lr p(l) ∧p(r) →p((l, r))

→D(p(t))
Fig. 5. Axioms for datatypes
Figure 5 gives axioms for deﬁnite descriptions and tuples. Axiom ι fully char-
acterizes deﬁnite descriptions, and it is used to derive axioms for deﬁned term
constructs like those in Example 2. Axiom =T enables comparisons on tuples.
Quantiﬁer elimination rule QE uses that ﬁrst-order real arithmetic, a fragment,
is decidable [27]. Since variables of dLι may range over tuples, which are not
part of ﬁrst-order arithmetic, it must ﬁrst check that all variables of the formula
(written V(φ)) are indeed real-valued. Axioms redT and redR evaluate reduc-
tions when their shape is known, and axiom TreeI allows proving a property
of an arbitrary value by induction on its shape, including a second base case
p(ιx 0 = 1) where the argument to p does not denote.
Example 2 (Derived axioms). The following are examples of derived axiom
schemata that have been proved from those above. Proofs are in the report [3].
π1(l, r) = l
π2(l, r) = r
inR(f) ∨isT(f) ←E(f)
(x)′ = x′
(f(x) + g(x))′ = (f(x))′ + (g(x))′ ←E((f(x))′) ∧E((g(x))′)
(f)′ = 0 ←E(f)
(f(x) · g(x))′ = (f(x))′ · g(x) + (g(x))′ · f(x) ←E((f(x))′) ∧E((g(x))′)
It is signiﬁcant that the diﬀerential axioms of Example 2 are derived: when new
term constructs are added in the future, we expect to derive their diﬀerential
axioms as well, so that these extensions lie entirely outside the core dLι calculus.
Note that these axioms also conclude (by applying axiom E(′)) that the diﬀer-
ential of the larger term exists, because it equals something. Thus, these axioms
are suitable both for showing diﬀerentials exist and what form diﬀerentials take.
Example 3 (Proof of leakiness). Proposition 1 of Sect. 2 is provable in dL.

106
R. Bohrer et al.
Proof (Sketch). By axiom I with loop invariant P ≡(g > 0 ∧A > 0 ∧0 ≤h ≤h0).
The ﬁrst two conditions are trivially invariant by axiom V because g and A are
constant throughout αB. Proceed by cases with axiom [U]. In each case, show
h ≤h0 to be an invariant of the ODE by DI≥. Because h ≤h0 holds initially and
the ODE is locally Lipschitz-continuous given constraint h ≥0, it suﬃces to show
(h)′ ≤(h0)′ = 0 throughout. Then (h)′ ≤0 ⇐⇒−√2gh a
A ≤0 ⇐⇒
√
h ≥0
by algebra and DE, which is true by DW, showing h ≤h0.
⊓⊔
6
Theory
Proofchecking is decidable, and provable formulas are valid.
Theorem 1 (Proofchecking decidability). There exists an algorithm which
decides whether a derivation D is a proof of a given dLι formula φ.
Theorem 2 (Soundness of dLι). If φ is provable in dLι, then φ is valid.
The proof of soundness proceeds by induction on the structure of derivations.
That is, we prove each axiom (which is an individual formula) to be valid and
prove every proof rule to be sound (producing valid conclusions from valid pre-
misses). Because dLι supports the formula and program connectives of dL, many
of the axioms are extensions of corresponding dL axioms. The axiom validity
proofs also have a similar ﬂavor to those of dL: each axiom is proven valid by
direct proof, showing truth of the axiom according to the denotational semantics
in an arbitrary state. The full proofs for each axiom and rule are given in the
report [3]; Lemma 1 gives an example.
Lemma 1 (Assignment axiom is valid). The following formula is valid:
([x := f]p(x) ↔p(f)) ←E(f)
Proof. Assume (1) Iω[[E(f)]]
=
⊕for some state ω and interpretation
I, then observe Iω[[[x := f]p(x)]]
=
Iω[[p(f)]] by the chain of equalities
Iω[[[x := f]p(x)]] = 
ν | (ω,ν)∈{(ω,ωIω[[f]]
x
)},Iω[[f]]̸=⊥Iν[[p(x)]] = IωIω[[f]]
x
[[p(x)]] =
I(p)(I(f)) = Iω[[p(f)]]
⊓⊔
6.1
Uniform Substitution
The uniform substitution proof rule in dLι is analogous to that in dL:
(US)
φ
σ(φ)
In dL, the US rule is sound when the substitution σ does not introduce free
references to bound variables, in a sense made precise elsewhere [23]. Such sub-
stitutions are called admissible, a condition which can be checked syntactically.
We show that the same holds of dLι when adding terms ιx φ, (θ1, θ2) and
red(θ1, s θ2, lr θ3) and generalizing dL to a three-valued semantics. As in dL, we
formulate admissibility in terms of U-admissibility (Deﬁnition 4) checks.

dLι: Deﬁnite Descriptions in Diﬀerential Dynamic Logic
107
Deﬁnition 4 (Admissible uniform substitution). A substitution σ is U-
admissible for φ (or θ or α) with respect to a set U ⊆V∪V′ iﬀFV(σ|Σ(φ))∩U = ∅
where σ|Σ(φ) is the restriction of σ that only replaces symbols that occur in φ
and FV(σ) = 
f∈σ FV(σf(·)) ∪
p∈σ FV(σp(·)) are the free variables that σ
introduces, and where V′ = {x′ | x ∈V}. The substitution σ is admissible for φ
(or θ or α) if all such checks during its applications hold, per Fig. 6.
n
e
h
w
el
b
is
si
m
d
A
t
n
e
m
e
c
a
l
p
e
R
e
s
a
C
σ((θ1, θ2)) = (σ(θ1), σ(θ2))
σ(red(θ1, s θ2, lr θ3)) = red(σ(θ1), s σ(θ2), lr σ(θ3))
σ is {s}-admissible for θ2
σ is {l, r}-admissible for θ3
σ(ιx φ) = ιx σ(φ)
σ is {x}-admissible for φ
σ(∀x φ) = ∀x σ(φ)
σ is {x}-admissible for φ
σ([α]φ) = [σ(α)]σ(φ)
σ is BV(σ(α))-admissible for φ
σ(f(θ)) = f(σ(θ)), if f /∈σ, else σf(σ(θ))
Fig. 6. Uniform substitution algorithm (selected cases)
In Fig. 6, σf denotes the replacement for symbol f provided by σ. We give
the new cases of FV(·) here and the full static semantics in the report [3]:
FV((θ1, θ2)) = FV(θ1) ∪FV(θ2)
FV(ιx φ) = FV(φ)\{x}
FV(red(θ1, s θ2, lr θ1)) = FV(θ1) ∪(FV(θ2)\{s}) ∪(FV(θ3)\{l, r})
Admissibility checks employ static semantics consisting of free-variable (FV(·)),
may-bound-variable (BV(·)), and must-bound-variable (MBV(·)) computations.
Generally speaking, the free variables of a compound expression θ are the free
variables of its immediate subexpressions, minus any variables that it binds
Formally, FV(θ) (or φ, α) contains all variables that inﬂuence meaning:
Lemma 2 (Coincidence). The interpretation of an expression depends only
on the values of its free variables and constants, e.g. for any term θ, any inter-
pretations I and J that agree on the signature (mentioned predicate symbols,
function symbols, and program constants) Σ(θ) of θ, and any states ω and ˜ω
that agree on FV(θ), we have Iω[[θ]] = J ˜ω[[θ]].
The substitution result for a compound expression is found by substituting in
each immediate subexpression, and is deﬁned so long as all admissibility checks
hold recursively. In general, the admissibility check for each constructor says that
the substitution result must not contain any new occurrences of the variables
bound at that constructor.
Theorem 3 (Uniform substitution). Rule US is sound.
Soundness of the proof system then follows from validity of the axioms and
soundness of US and of the other proof rules.

108
R. Bohrer et al.
6.2
Expressive Power
After showing soundness of dLι, we explore its expressive power: can dLι express
formulas that are inexpressible in dL, or is its advantage the ease with which
certain formulas are expressed? Conversely, are all dL formulas expressible in
dLι? Because dLι is an extension of dL, it is unsurprising that it can express all
dL formulas. However, a valid dL formula φ is not always valid in dLι.
Remark 1 (Conservativity counterexample). There exist valid formulas of dL that
are not valid formulas of dLι.
Proof. The formula φ ≡(x · x ≥0) is not conserved, because it is true for all
real values of x, but fails when x is a tuple such as (0, 0), outside the domain of
multiplication. This is why rule QE requires inR(x) for each mentioned x.
⊓⊔
We transform dL quantiﬁers to real-valued dLι quantiﬁers to close the gap:
Theorem 4 (Converse reducibility). There exists a linear-time transforma-
tion T such that for all φ in dL, T(φ) is valid in dLι iﬀφ is valid in dL.
The greater challenge is to show that dL also suﬃces to express all dLι formulas
and thus dL and dLι are equiexpressive:
Theorem 5 (Reducibility). There is a computable T s.t. for all formulas φ,
interpretations I, and states ω in dLι, Iω[[φ]] = ⊕in dLι iﬀIω[[T(φ)]] = ⊕in dL.
While this result might be misread to suggest that dLι is not truly necessary,
deﬁnite descriptions enable us to deﬁne constructs that have no description as
terms in dL, even if they can be expressed through a suﬃciently complex formula
translation. The key is that the reduction from dLι to dL is indeed complex,
exploiting for example G¨odel encodings for tuples and continuous functions [21,
24]. On the contrary, the complexity of the reduction shows that native support
for deﬁnite descriptions is essential for practical proving. The equiexpressiveness
result is of theoretical interest because it allows us to inherit results from dL [18]:
Theorem 6 (Completeness and decidability). dLι is reducible to dL, and
therefore semidecidable relative to properties of diﬀerential equations.
While the reduction gives a semi-decision procedure for dLι in principle, it is
infeasible for implementation, especially since deciding even core dL is hard in
practice. Moreover, this would defeat our purpose: easing implementation of
practical term language extensions in dL, where interactive proof is common.
7
Conclusion and Future Work
In this paper we developed dLι, an extension to diﬀerential dynamic logic (dL)
for formal veriﬁcation of hybrid systems models of safety-critical cyber-physical
systems. The key feature of dLι is deﬁnite description ιx φ, which provides a
foundation for deﬁning new term language constructs from their characteristic

dLι: Deﬁnite Descriptions in Diﬀerential Dynamic Logic
109
formulas. We develop the theory of dLι, including semantics, a proof calculus, and
soundness and expressiveness proofs. We apply dLι to verify a classic example of
a non-Lipschitz ODE, which could not be directly veriﬁed in dL.
In particular, we give a novel axiomatization that accounts for the interac-
tions between non-diﬀerentiable and partially deﬁned operators with systems of
diﬀerential equations, an interaction which does not occur for dL’s simpler lan-
guage where all terms are smooth. More generally, example applications abound:
almost every serious case study of dL employs these constructs in practice; we
give a fully rigorous foundation to these case studies. In future work, imple-
menting dLι in KeYmaera X would enable case studies to soundly employ the
constructs given herein and to deﬁne their own. We expect few core changes
would be needed, thanks to our use of uniform substitution, rather the challenge
is to eﬃciently prove and track the new assumptions on existence and continuity.
Acknowledgments. We thank Martin Giese for discussions on the use of deﬁnite
descriptions in theorem provers and the referees for their thoughtful feedback.
References
1. Anand, A., Rahli, V.: Towards a formally veriﬁed proof assistant. In: Klein, G.,
Gamboa, R. (eds.) ITP 2014. LNCS, vol. 8558, pp. 27–44. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-08970-6 3
2. Barras, B.: Sets in Coq, Coq in sets. J. Formaliz. Reason. 3(1), 29–48 (2010).
https://doi.org/10.6092/issn.1972-5787/1695
3. Bohrer, R., Fern´andez, M., Platzer, A.: dLι: deﬁnite descriptions in diﬀerential
dynamic logic. Technical report. CMU-CS-19-111, School of Computer Science,
Carnegie Mellon University, Pittsburgh, PA (2019)
4. Bohrer, R., Rahli, V., Vukotic, I., V¨olp, M., Platzer, A.: Formally veriﬁed diﬀer-
ential dynamic logic. In: Bertot, Y., Vafeiadis, V. (eds.) CPP, pp. 208–221. ACM
(2017). https://doi.org/10.1145/3018610.3018616
5. Bohrer, R., Tan, Y.K., Mitsch, S., Myreen, M.O., Platzer, A.: VeriPhy: veriﬁed con-
troller executables from veriﬁed cyber-physical system models. In: Grossman, D.
(ed.) PLDI, pp. 617–630. ACM (2018). https://doi.org/10.1145/3192366.3192406
6. Church, A.: Introduction to Mathematical Logic. Princeton University Press,
Princeton (1956)
7. Driver, R.: Torricelli’s law: an ideal example of an elementary ODE. Am. Math.
Mon. 105(5), 453–455 (1998)
8. Fitting, M., Mendelsohn, R.L.: First-Order Modal Logic. Kluwer, Norwell (1999)
9. Fulton, N., Mitsch, S., Quesel, J.-D., V¨olp, M., Platzer, A.: KeYmaera X: an
axiomatic tactical theorem prover for hybrid systems. In: Felty, A.P., Middeldorp,
A. (eds.) CADE 2015. LNCS (LNAI), vol. 9195, pp. 527–538. Springer, Cham
(2015). https://doi.org/10.1007/978-3-319-21401-6 36
10. Henzinger, T.A.: The theory of hybrid automata. In: LICS. IEEE (1996). https://
doi.org/10.1109/LICS.1996.561342
11. Hubbard, J.H., West, B.H.: Diﬀerential Equations: A Dynamical Systems App-
roach. Springer, Heidelberg (1991). https://doi.org/10.1007/978-1-4612-4192-8
12. Jeannin, J., et al.: A formally veriﬁed hybrid system for safe advisories in the
next-generation airborne collision avoidance system. STTT 19(6), 717–741 (2017).
https://doi.org/10.1007/s10009-016-0434-1

110
R. Bohrer et al.
13. Kumar, R., Arthan, R., Myreen, M.O., Owens, S.: Self-formalisation of higher-order
logic: semantics, soundness, and a veriﬁed implementation. J. Autom. Reason.
56(3), 221–259 (2016). https://doi.org/10.1007/s10817-015-9357-x
14. Lukasiewicz, J.: O logice tr ojwarto´sciowej (on 3-valued logic). Ruch Filozoﬁczny
5, 169–171 (1920)
15. Mitsch, S., Gario, M., Budnik, C.J., Golm, M., Platzer, A.: Formal veriﬁcation of
train control with air pressure brakes. In: Fantechi, A., Lecomte, T., Romanovsky,
A. (eds.) RSSRail. LNCS, vol. 10598, pp. 173–191. Springer, Cham (2017). https://
doi.org/10.1007/978-3-319-68499-4 12
16. Mitsch, S., Ghorbal, K., Vogelbacher, D., Platzer, A.: Formal veriﬁcation of obsta-
cle avoidance and navigation of ground robots. Int. J. Robot. Res. 36(12), 1312–
1340 (2017). https://doi.org/10.1177/0278364917733549
17. Nipkow, T., Paulson, L.C., Wenzel, M.: Isabelle/HOL - A Proof Assistant for
Higher-Order Logic. LNCS, vol. 2283. Springer, Heidelberg (2002). https://doi.
org/10.1007/3-540-45949-9
18. Platzer, A.: Diﬀerential dynamic logic for hybrid systems. J. Autom. Reason. 41(2),
143–189 (2008). https://doi.org/10.1007/s10817-008-9103-8
19. Platzer, A.: Diﬀerential-algebraic dynamic logic for diﬀerential-algebraic programs.
J. Log. Comput. 20(1), 309–352 (2010). https://doi.org/10.1093/logcom/exn070
20. Platzer, A.: A complete axiomatization of quantiﬁed diﬀerential dynamic logic
for distributed hybrid systems. Log. Method Comput. Sci. 8(4), 1–44 (2012).
https://doi.org/10.2168/LMCS-8(4:17)2012. Special issue for selected papers from
CSL2010
21. Platzer, A.: The complete proof theory of hybrid systems. In: LICS, pp. 541–550.
IEEE (2012). https://doi.org/10.1109/LICS.2012.64
22. Platzer, A.: Logics of dynamical systems. In: LICS, pp. 13–24. IEEE (2012).
https://doi.org/10.1109/LICS.2012.13
23. Platzer, A.: A complete uniform substitution calculus for diﬀerential dynamic logic.
J. Autom. Reason. 59(2), 219–265 (2017). https://doi.org/10.1007/s10817-016-
9385-1
24. Platzer, A.: Diﬀerential hybrid games. ACM Trans. Comput. Log. 18(3), 19:1-19:44
(2017). https://doi.org/10.1145/3091123
25. Platzer, A., Tan, Y.K.: Diﬀerential equation axiomatization: the impressive power
of diﬀerential ghosts. In: Dawar, A., Gr¨adel, E. (eds.) LICS, pp. 819–828. ACM,
New York (2018). https://doi.org/10.1145/3209108.3209147
26. Slind, K., Norrish, M.: A brief overview of HOL4. In: Mohamed, O.A., Mu˜noz, C.,
Tahar, S. (eds.) TPHOLs 2008. LNCS, vol. 5170, pp. 28–32. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-71067-7 6
27. Tarski, A.: A decision method for elementary algebra and geometry. In: Cavi-
ness, B.F., Johnson, J.R. (eds.) Quantiﬁer Elimination and Cylindrical Algebraic
Decomposition. Texts and Monographs in Symbolic Computation (A Series of the
Research Institute for Symbolic Computation, Johannes-Kepler-University, Linz,
Austria), pp. 24–84. Springer, Vienna (1998). https://doi.org/10.1007/978-3-7091-
9459-1 3

SPASS-SATT
A CDCL(LA) Solver
Martin Bromberger1,2,3(B), Mathias Fleury1,2,3, Simon Schwarz1,2,
and Christoph Weidenbach1,2
1 Max Planck Institute for Informatics, Saarland Informatics Campus,
Saarbr¨ucken, Germany
{mbromber,mfleury,sschwarz,weidenb}@mpi-inf.mpg.de
2 Saarland University, Saarland Informatics Campus, Saarbr¨ucken, Germany
3 Graduate School of Computer Science, Saarland Informatics Campus,
Saarbr¨ucken, Germany
Abstract. SPASS-SATT is a CDCL(LA) solver for linear rational and
linear mixed/integer arithmetic. This system description explains its spe-
ciﬁc features: fast cube tests for integer solvability, bounding transforma-
tions for unbounded problems, close interaction between the SAT solver
and the theory solver, eﬃcient data structures, and small-clause-normal-
form generation. SPASS-SATT is currently one of the strongest systems
on the respective SMT-LIB benchmarks.
Keywords: Linear arithmetic · Integer arithmetic · SMT ·
Preprocessing
1
Introduction
SPASS-SATT (v1.1) is a sound and complete CDCL(LA) solver for quantiﬁer-
free linear rational and linear mixed/integer arithmetic. It is a from-scratch
implementation except for some basic data structures taken from the SPASS [32]
superposition theorem prover. It is available through the SPASS-Workbench [3].
We participated with SPASS-SATT in the main track of the 13th International
Satisﬁability Modulo Theories Competition (SMT-COMP 2018) and ranked ﬁrst
in the category QF LIA (quantiﬁer-free linear integer arithmetic) [1] and second
in the category QF LRA (quantiﬁer-free linear rational arithmetic) [2]. This
system description explains the main features that led to the success of SPASS-
SATT. We do not only describe the relevant techniques, but also show their
speciﬁc impact on dedicated groups of examples from the SMT-LIB by experi-
ments.
By far not all techniques presented in this system description are unique
features of SPASS-SATT. The techniques that appeared ﬁrst in SPASS-SATT
are the unit cube test and bounding transformations explained in Sect. 2. Con-
cerning preprocessing, SPASS-SATT is the ﬁrst SMT solver implementing the
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 111–122, 2019.
https://doi.org/10.1007/978-3-030-29436-6_7

112
M. Bromberger et al.
Fig. 1. Impact of our BnB extensions on the QF LIA benchmarks (a) QF LIA with(out)
unit cube tests (b) QF LIA with(out) bounding transf.
small-clause-normal-form algorithm, see Sect. 4. Further important techniques
implemented in SPASS-SATT have already been available in other SMT solvers
such as CVC4 [4], MathSAT [12], Yices [16], and Z3 [14], but not all in one tool:
(i) the implementation of branch and bound as a separate theory solver and a
number of improvements to the simplex implementation such as a priority queue
for pivot selection, integer coeﬃcients instead of rational coeﬃcients, dynami-
cally switching between native and arbitrary precision integers, and backing-up
versus recalculating simplex states, all in Sect. 2, (ii) decision recommendations,
unate propagations, and bound reﬁnements for the interaction between the SAT
and theory solver, in Sect. 3, and (iii) preprocessing techniques for if-then-else
operators and pseudo-boolean inequalities, in Sect. 4. Although these techniques
are contained in existing SMT solvers, not all have been described in the respec-
tive literature. The paper ends with a discussion of future extensions to SPASS-
SATT in Sect. 5.
The benchmark experiments with SPASS-SATT consider the 6947 SMT-LIB
benchmarks for quantiﬁer-free linear integer arithmetic (QF LIA) [5]. For the
experiments, we used a Debian Linux cluster and gave SPASS-SATT for each
problem one core of an Intel Xeon E5620 (2.4 GHz) processor, 8 GB RAM, and
40 min. The results are depicted as scatter plots and in each of them we compare
the default conﬁguration (i.e., without any command line options) of SPASS-
SATT (horizontal axis) with an alternative conﬁguration of SPASS-SATT (verti-
cal axis). (The SMT-COMP results were obtained with the default conﬁguration;
by default all presented techniques are turned on.)
2
SPASS-IQ: An LA Theory Solver
SPASS-SATT’s theory solver, called SPASS-IQ, decides conjunctions of linear
arithmetic inequations. It is divided into two main components: a simplex imple-

SPASS-SATT
113
mentation for handling linear rational arithmetic and a branch-and-bound imple-
mentation for handling linear mixed/integer arithmetic.
However, the division between the two components is in all truth not that
strict. The branch-and-bound implementation is more of a supervisor for the
simplex implementation. To be more precise, the branch-and-bound implemen-
tation coordinates the search for a mixed or integer solution, but the majority
of the actual search/calculation is still done by the simplex implementation. For
most QF LIA benchmark instances (4894 out of 6947 instances), this super-
vision is not even necessary; i.e., SPASS-SATT solves these instances with just
the simplex implementation as its theory solver. This means that SPASS-SATT’s
eﬃciency on the QF LIA benchmarks also highly depends on the eﬃciency of
our simplex implementation and not just on the extensions and optimizations
to our branch-and-bound implementation.
The simplex implementation inside SPASS-IQ is based on a speciﬁc ver-
sion [17] of the dual simplex algorithm [29]. The overall eﬃciency of our simplex
implementation is heavily inﬂuenced by the eﬃciency of the data structures that
we use. Our most important data structure features are:
(1) Priority Queue for Pivot Selection: Instead of iterating over all basic vari-
ables when searching for violated basic variables, we collect the basic vari-
ables in a priority queue as soon as they become violated.
(2) Integer Coeﬃcients Instead of Rational Coeﬃcients: We avoid rational coef-
ﬁcients in our simplex tableau by multiplying each equation in the tableau
with the common denominator of the equations coeﬃcients. As a result, each
basic variable also has a coeﬃcient, but all coeﬃcients are integers. This
transformation roughly halves the cost of most tableau operations because
we do not need to consider rationals which are typically represented by two
integers (the numerator and the denominator).
(3) Dynamically Switching between Native and Arbitrary-Precision Integers: We
use the arbitrary-precision arithmetic library FLINT to represent our inte-
gers [21]. It dynamically switches between native C integer and arbitrary-
precision types.
(4) Backup vs. Recalculation: In contrast to Dutertre and de Moura’s version of
the simplex algorithm, our simplex backtrack function recalculates a satisﬁ-
able assignment instead of loading a backup of the last satisﬁable assignment.
SPASS-IQ’s second set of decision procedures revolves around an implementation
of the branch-and-bound (BnB) algorithm [29]. Most SMT solvers implement
branch and bound through a technique called splitting-on-demand [6], which
delegates some of the branch-and-bound reasoning to the SAT solver. In order
to keep more control over the branch-and-bound reasoning, we decided against
splitting-on-demand and implemented branch and bound as a theory solver sep-
arate from the SAT solver. This also made it easier to complement branch and
bound with other decision procedures:
The ﬁrst two extensions that we discuss here are simple rounding (turn oﬀ
with −LASR 0) and bound propagation (turn oﬀwith −LABP 0) [29], which are

114
M. Bromberger et al.
both classical additions to most branch-and-bound implementations. For sim-
ple rounding, we round any rational solution computed during the branch-and-
bound search to the closest integer assignment and check whether this is already
an integer solution. For bound propagation, we propagate new bounds from
existing bounds at every node in our branching tree. Although both techniques
are very popular, we could only measure a minor impact on SPASS-SATT’s
performance on the QF LIA benchmarks. With simple rounding we solve only
one instance faster and with bound propagations we solve only 10 additional
instances. In part, this is due to our next two extensions that make simple
rounding and bound propagation in many cases unnecessary.
The next extension we discuss is the unit cube test (turn oﬀwith −C 0). It
determines in polynomial time whether a polyhedron, i.e., the geometric repre-
sentation of a system of inequalities, contains a hypercube parallel to the coor-
dinate axes with edge length one [9,10]. The existence of such a hypercube
guarantees a mixed/integer solution for the system of inequalities.
The unit cube test is only a suﬃcient and not a necessary test for the existence
of a solution. There is at least one class of inequality systems, viz., absolutely
unbounded inequality systems [9,10], where the unit cube test is also a necessary
test and which are much harder for many complete decision procedures.
The plot in Fig. 1(a) shows that SPASS-SATT employing the unit cube test
solves 56 additional benchmark instances from the QF LIA benchmarks and
solves 705 instances more than twice as fast.1 Moreover, the unit cube test causes
only a minor overhead on problems where it is not successfully applicable.
The ﬁnal extension that we discuss are bounding transformations (turn oﬀ
with −B 0). Branch and bound alone is an incomplete decision procedure and
only guarantees termination on bounded problems, i.e., problems where all vari-
ables have an upper and a lower bound. For this reason, we developed two
transformations that reduce any unbounded problem into an equisatisﬁable prob-
lem that is bounded [7]. The transformed problem can then be solved with our
branch-and-bound implementation because it is complete for bounded problems.
The plot in Fig. 1(b) shows that SPASS-SATT employing the bounding trans-
formation solves 169 additional benchmark instances from the QF LIA bench-
marks and solves 167 instances more than twice as fast.2 Moreover, the bounding
transformation causes only a minor, almost immeasurable overhead on problem
instances where it is not successfully applicable.
3
CDCL(LA): SAT and Theory Solver Interaction
SPASS-SATT uses at its core a CDCL(LA) implementation that combines our
CDCL (conﬂict-driven-clause-learning)-based SAT solver SPASS-SAT with our
1 These
instances
belong
to
the
dillig
[15],
CAV-2009
[15],
slacks
[22],
20180326-Bromberger [7], and prime-cone benchmark families [22], which together
contain more than 1483 instances of absolutely unbounded problems.
2 These instances belong to the 20180326-Bromberger [7], arctic-matrix [13],
cut lemmas [19], slacks [22], and tropical-matrix [13] benchmark families.

SPASS-SATT
115
RatCheck()
Assert(L)
IntCheck()
IncLvl ()
LABacktrack(D)
GetConflict()
LearnClauses()
GetPhase(L’)
Decide(L’,P)
UnitPropagate(N)
AnalyzeConflict(C)
SelectDecLit()
SATBacktrack(C’)
Assert(
)
L
P
S
L’
C’
D
SAT Solver
Theory Solver
C
N
CDCL(LA) Input: 
CDCL(LA) Output: 
unsat
or
sat
N
unsat
sat
Decide(L,P): if P is ⊤, then L is added
to the model; otherwise, ¬L is added.
SelectDecLit(): returns ∅if the model
satisﬁes all clauses; otherwise, returns a
literal L that is undeﬁned in the model.
UnitPropagate(N): returns ∅if no lite-
ral can be unit propagated; otherwise, se-
lects a literal L that can be unit propaga-
ted and adds it to the model; if a clause
C ∈N evaluates to ⊥under the new mo-
del, then returns C; otherwise, returns L.
AnalyzeConﬂict(C): derives a clause C′
which is the negation of the literals that
led to the conﬂict in C; ends CDCL(LA)
and returns unsat if C′ is empty; other-
wise, returns C′.
SATBacktrack(C′):
adds
C′
to
the
clause set N and backtracks to the maxi-
mum decision level D where C′ is still sa-
tisﬁable; retuns D.
Assert(L): returns ⊥if the literal L contradicts another asserted literal.
IncLvl(): notiﬁes the theory solver that a new decision level was reached.
GetPhase(L): selects the phase P for the decision literal L.
LearnClauses(): Adds a set S of clauses to N that correspond to unate propagations
and bound reﬁnements; returns ∅if there are no clauses to learn.
RatCheck(): determines a rational solution for the asserted literals; returns ⊤if a
rational solution exists; otherwise, returns ⊥.
IntCheck(): determines an integer solution for the asserted literals; ends CDCL(LA)
and returns sat if an integer solution exists; otherwise, returns ⊥.
GetConﬂict(): returns a clause C that explains the theory conﬂict.
LABacktrack(D): removes all asserted literals that were added after decision level
D; recalculates a rational solution for the remaining asserted literals.
L’
Fig. 2. CDCL(LA) as implemented in SPASS-SATT
LA theory solver SPASS-IQ. The result is a decision procedure for ground linear-
arithmetic formulae in clause normal form. In this section, we quickly explain
how our theory solver and SAT solver interact. To this end, we list in Fig. 2 the
main interface functions of our SAT solver and theory solver and show through
a ﬂow graph how they interact. The main focus of this section, however, is to
explain in which way our implementation of CDCL(LA) diﬀers from more general
frameworks for CDCL(T), also called DPLL(T) [6,18,26,27].
There are three key points that we have changed compared to the more
general frameworks for CDCL(T). First of all, we rely on “weakened early prun-
ing” [30], i.e., we only use a weaker but faster check to determine theory sat-
isﬁability for partial (propositionally abstracted) models. We do so because
IntCheck(), i.e., checking for an integer solution, is too expensive and not
incrementally eﬃcient enough to be checked more than once per complete

116
M. Bromberger et al.
Fig. 3. Impact of decision recommendations on the QF LIA benchmarks (a) QF LIA
with(out) decision recom. (b) convert with(out) decision recom.
(propositionally abstracted) model. As a compromise, we at least check the par-
tial model with RatCheck(), i.e., we check for a rational solution, before we add
a(nother) decision literal to the model with Decide(L,P).
As our second key change, we let the theory solver select via GetPhase(L) the
phase of the next decision literal L, i.e., whether Decide(L,P) will add the pos-
itive or the negated version of L to the model. We call this technique a decision
recommendation (turn oﬀwith −p 0).3 Finally, we use theory reasoning via the
function LearnClauses() to ﬁnd and learn new clauses implied by the input for-
mula. The reasoning techniques we use for this purpose are unate propagations
and bound reﬁnements as proposed in [17].
In Fig. 3(a), we examine the impact that decision recommendations have on
SPASS-SATT’s performance on the QF LIA benchmarks. With decision recom-
mendations it can solve 129 additional problems. Moreover, it becomes more than
twice as fast on 389 problems, but only twice as slow on 58 problems. The bench-
mark family that is impacted the most by decision recommendations is convert
with 116 additionally solved problems (see Fig. 3(b)). Although SPASS-SATT
frequently and regularly performs unate propagations on the QF LIA bench-
marks, we are unable to observe any consistent beneﬁt or drawback from this
interaction technique. The impact of bound reﬁnements is also relatively minor
and SPASS-SATT solves only 24 additional problem instances if they are acti-
vated.
3 In our own theory solver, GetPhase(L) = ⊤if the current theory assignment satisﬁes
L or if Assert(¬L) would return ⊥. Otherwise, GetPhase(L) = ⊥. (If unate prop-
agations are enabled, then the case that Assert(¬L) would return ⊥is impossible
because L would have been unate propagated.).

SPASS-SATT
117
Fig. 4. Impact of our preprocessing techniques on (a) the QF LIA benchmarks and
more speciﬁcally (b) nec smt, (c) rings, and (d) pb2010
4
Preprocessing
Many real world applications can be encoded as linear integer arithmetic formu-
las, and some of those applications are too specialized to be eﬃciently handled
by our rather general CDCL(LA) implementation. To resolve this, we have com-
plemented SPASS-SATT with several specialized preprocessing techniques.4
Complex input formulas are typically transformed into CNF by a Tseitin-
style renaming [31] using a static criterion which subformula to replace by a
fresh propositional variable. SPASS-SATT includes the small clause normal form
4 All preprocessing techniques are also contained in CVC4 [4] with the exception of
the small CNF. The implementation is so eﬃcient because we employ a shared term
representation and cache all intermediate results.

118
M. Bromberger et al.
algorithm [28]. Instead of a static criterion, the number of clauses with or without
a renaming is compared and a fresh propositional variable is introduced only if
a renaming eventually yields fewer clauses. This results in a more compact CNF
with strictly fewer additional propositional variables. To this end we extended
the small clause normal form algorithm to ITE formulas. For an ITE formula
(ite t1 t2 t3), simpliﬁed by the below techniques and potentially contained in
some formula f, we compare the number of clauses generated out of replacing the
formula with a fresh variable P in f and adding P ↔[(t1 →t2)∧(¬t1 →t3)] with
a direct replacement of the ITE formula by the before mentioned conjunction
of two implications. This test can be carried out in constant time after having
once ﬁlled respective data structures. The set up of the data structures needs
one run on the overall formula, i.e., can be computed in linear time [28].
SPASS-SATT also has ﬁve specialized preprocessing techniques for if-then-
else expressions (ITE). Our ﬁrst technique, if-then-else reconstruction (turn oﬀ
with −PPIR 0), rebuilds if-then-else operations that were already preprocessed-
away by the creators of the input problem. The reconstruction then allows
us to apply simpliﬁcations missed during the creation of the input prob-
lem. To this end, we check whether the ﬁrst conjunctive layer of our formula
f := (and . . . ti . . . t′
i . . .) contains any pair of clauses ti, t′
i that match
the clauses added by the standard if-then-else elimination, i.e., ti ∈Ti and
t′
i ∈Ti\{ti}, where Ti = {(or ti1 (= yi ti3)), (or (not ti1) (= yi ti2)), (or (=
yi ti2) (= yi ti3))} and yi is an arithmetic variable. If we ﬁnd such pairs ti, t′
i,
then we remove them from f and replace all remaining occurrences of yi in f
with (ite ti1 ti2 ti3).
The next three techniques are all dedicated to so-called constant if-then-else
expressions (CITEs). A CITE is either a leaf, i.e., an arithmetic expression that
can be simpliﬁed to a number aij ∈Q, or a branch, i.e., an if-then-else expression
(ite t1 t2 t3) where t2 and t3 are again CITEs.
The ﬁrst CITE technique is called shared monomial lifting (turn oﬀwith
−PPIL 0) and we use it to increase the number of CITEs in our formula. It
traverses the subterms in our formula in bottom-up order and transforms all
subterms t := (ite t1 (+ q q′) (+ q ˆq)) into (+ q (* 1 (ite t1 (+ q′) (+ ˆq)))).
(We assume here for simplicity that the shared part q appears after the unshared
parts q′ and ˆq. In reality SPASS-SATT has to ﬁnd and extract the shared parts.)
The second technique is called CITE simpliﬁcation (turn oﬀwith −PPIS 0)
and it simpliﬁes atoms (o t1 t2), where t1 and t2 are CITEs and o is one of
the operators <=, =, >=. To be more precise, the technique essentially pushes
the comparison operator o recursively down the CITE branches and greedily
simpliﬁes any branch to true or false if possible. For more details, see Sect. 4 in
the paper by Kim et al. on eﬃcient term-ITE conversion [24].
The third technique is called CITE bounding (turn oﬀwith −PPIB 0) and
eliminates the remaining CITEs. Thanks to it, we only introduce one new vari-
able for each topmost CITE instead of one variable for each CITE branch.
Moreover, this new variable describes only a small set of values (often equiv-
alent to just the CITE leaves) because it is bounded as tightly as possible.

SPASS-SATT
119
As its ﬁrst step, CITE bounding creates one new integer variable xj for each
topmost CITE expression tj and replaces the occurrences of tj in our for-
mula f with xj. As its second step, CITE bounding extends f to the for-
mula f ′ := (and f f1 f ′
1
. . .
fm f ′
m), where (i) fj is equivalent to tj
except that all leaves aij of tj are replaced by the equations (=
xj aij),
(ii) f ′
j := (and (>=
xj aminj) (<=
xj amaxj)), and (iii) aminj is the small-
est leaf in tj and amaxj is the largest leaf in tj. As its last step, CITE bounding
replaces all occurrences of xj in f ′ with (∗agj xj), where agj is the greatest
common divisor of the leaves aij in tj.
The ﬁnal ITE technique handles nested conjunctive if-then-else expressions
(AND-ITEs). An AND-ITE is a series of nested if-then-else expressions that
can be simpliﬁed to a conjunction. For instance, if ti := (ite t′
i ti+1 false) for
i = 1, . . . , n, then t1 is an AND-ITE equivalent to (and t′
1 . . . t′
n tn+1). Naturally,
we transform these AND-ITEs into actual conjunctions and we call this process
compression. However, we compress an AND-ITE t1 only if all of its actual
AND-ITE subterms ti appear only inside t1 and only once. If this is the case,
then we ﬁrst replace all occurrences of t1 in f by a new propositional variable
pj and extend our formula f to the formula (and f (=
pj t′
1)), where t′
1 is
the compressed form of t1. (If this is not the case, then we simply replace and
compress the AND-ITE subterms in ti ﬁrst.) We do the compression in this way
to strengthen the connection of the AND-ITEs that have multiple occurrences
in f. The above described technique is called if-then-else compression (turn oﬀ
with −PPIC 0) and it was ﬁrst presented by Burch in [11]. However, Burch used
it to simplify control circuits and not SMT input problems.
Last but not least, SPASS-SATT also provides a preprocessing technique for
pseudo-boolean problems [20], i.e., linear arithmetic problems where all integer
variables xj have bounds 0 ≤xj ≤1, which we call pseudo-boolean variables.
To be more precise, SPASS-SATT recognizes clauses that are encoded as lin-
ear pseudo-boolean inequalities (i.e., inequalities containing just pseudo-boolean
variables) and turns them into actual clauses (turn oﬀwith −PPCC 0). (This
technique goes back to the NP-hardness proof of 0-1 programming [23].) How-
ever, SPASS-SATT only transforms inequalities containing at most three vari-
ables because it would otherwise fail to solve some of the problems from the
pidgeons benchmark family.
The convert benchmark family contains problems that are relatively hard
unless SPASS-SATT uses the right combination of techniques. To be more pre-
cise, SPASS-SATT solves only two-thirds of the 319 instances if one of the fol-
lowing three techniques is missing: (i) recalculating simplex states during back-
tracking (Sect. 2), (ii) decision recommendations (Sect. 3), or (iii) the small CNF
transformation. SPASS-SATT with all three techniques solves all instances in less
than 2 s (see Fig. 3(b)).
The nec smt benchmark family contains problems with many nested if-then-
else and let expressions. SPASS-SATT can handle most of them if we ﬁrst
apply our constant if-then-else simpliﬁcations and our conjunctive if-then-else
compression. In Fig. 4(b), we see that SPASS-SATT without our preprocessing

120
M. Bromberger et al.
techniques solves only 1422 out of the 2800 benchmark instances and is by far
slower on the instances it can solve. SPASS-SATT with our preprocessing tech-
niques solves 2782 out of the 2800 benchmark instances.
The rings benchmark family encodes associative properties on modular
arithmetic with the help of if-then-else expressions. With a combination of shared
monomial lifting and constant if-then-else bounding, these problems become
almost trivial to solve. In fact, SPASS-SATT needs less than one second for
each problem instance and needs only techniques for linear rational arithmetic
to solve each of them (Fig. 4(c)).
The rings preprocessed benchmark family is equivalent to the rings
benchmark family except that all if-then-else-operations were eliminated by stan-
dard if-then-else elimination [19]. We can use the same trick as for the rings
benchmark family if we ﬁrst use our if-then-else reconstruction technique that
reverses the standard if-then-else elimination.
The pb2010 benchmark family is a set of industrial problems taken from
the pseudo-boolean competition 2010. With its pseudo-boolean preprocessing,
SPASS-SATT solves 22 additional benchmark instances from the 81 instances
in the pb2010 benchmark family (see Fig. 4(e)). Moreover, SPASS-SATT solves
all of these instances without its branch-and-bound implementation.
5
Conclusion and Future Work
We have presented SPASS-SATT our complete solver for ground linear arith-
metic and have explained which techniques make it so eﬃcient in practice. To
summarize, SPASS-SATT is so eﬃcient because (i) we have optimized the data
structures in our simplex implementation, (ii) we have combined branch and
bound with the unit cube test and the bounding transformation, (iii) we have
added decision recommendations to our CDCL(LA) framework, (iv) we have
added a small CNF transformation, and (v) we have added specialized prepro-
cessing techniques for if-then-else expressions and pseudo-boolean inequalities.
Almost all of the presented techniques can be applied incrementally, however
this is not always useful. For the partial models computed by the SAT solver,
we only apply the simplex method, unate propagation, and bound reﬁnements
incrementally. If we ever extend SPASS-SATT to handle theory combinations or
incremental SMT-LIB problems, then we would also apply branch-and-bound,
unit cubes, and bounding transformations incrementally; but only on the models
generated during Nelson-Oppen combination or between two (check-sat) calls.
For future research, we plan to extend SPASS-SATT to quantiﬁed linear
arithmetic. Moreover, we plan to complement SPASS-SATT with several spe-
cialized decision procedures. For instance, SPASS-SATT could handle (almost)
pseudo-boolean problems (e.g., benchmark families pb2010, miplib2003) much
more eﬃciently if we extended branch and bound with a SAT based arithmetic
decision procedure [8,22,25].

SPASS-SATT
121
References
1. SMT-COMP 2018 results for QF LIA (main track). http://smtcomp.sourceforge.
net/2018/results-QF LIA.shtml
2. SMT-COMP 2018 results for QF LRA (main track). http://smtcomp.sourceforge.
net/2018/results-QF LRA.shtml
3. The SPASS workbench. https://www.spass-prover.org/
4. Barrett, C., Conway, C.L., Deters, M., Hadarean, L., Jovanovi´c, D., King, T.,
Reynolds, A., Tinelli, C.: CVC4. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV
2011. LNCS, vol. 6806, pp. 171–177. Springer, Heidelberg (2011). https://doi.org/
10.1007/978-3-642-22110-1 14
5. Barrett, C., Fontaine, P., Tinelli, C.: The satisﬁability modulo theories library
(SMT-LIB) (2016). www.SMT-LIB.org
6. Barrett, C., Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Splitting on demand in
SAT modulo theories. In: Hermann, M., Voronkov, A. (eds.) LPAR 2006. LNCS
(LNAI), vol. 4246, pp. 512–526. Springer, Heidelberg (2006). https://doi.org/10.
1007/11916277 35
7. Bromberger, M.: A reduction from unbounded linear mixed arithmetic problems
into bounded problems. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR
2018. LNCS (LNAI), vol. 10900, pp. 329–345. Springer, Cham (2018). https://doi.
org/10.1007/978-3-319-94205-6 22
8. Bromberger, M., Sturm, T., Weidenbach, C.: Linear integer arithmetic revisited.
In: Felty, A.P., Middeldorp, A. (eds.) CADE 2015. LNCS (LNAI), vol. 9195, pp.
623–637. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-21401-6 42
9. Bromberger, M., Weidenbach, C.: Fast cube tests for LIA constraint solving. In:
Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol. 9706, pp. 116–132.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-40229-1 9
10. Bromberger, M., Weidenbach, C.: New techniques for linear arithmetic: cubes and
equalities. Formal Methods Syst. Des. 51(3), 433–461 (2017). https://doi.org/10.
1007/s10703-017-0278-7
11. Burch, J.R.: Techniques for verifying superscalar microprocessors. In: DAC, pp.
552–557. ACM Press (1996). https://doi.org/10.1145/240518.240623
12. Cimatti, A., Griggio, A., Schaafsma, B.J., Sebastiani, R.: The MathSAT5 SMT
solver. In: Piterman, N., Smolka, S.A. (eds.) TACAS 2013. LNCS, vol. 7795, pp.
93–107. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36742-7 7
13. Codish, M., Fekete, Y., Fuhs, C., Giesl, J., Waldmann, J.: Exotic semi-ring con-
straints. In: SMT@IJCAR. EPiC Series in Computing, vol. 20, pp. 88–97. Easy-
Chair (2012). https://doi.org/10.29007/qqvt
14. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
15. Dillig, I., Dillig, T., Aiken, A.: Cuts from proofs: a complete and practical technique
for solving linear inequalities over integers. In: Bouajjani, A., Maler, O. (eds.) CAV
2009. LNCS, vol. 5643, pp. 233–247. Springer, Heidelberg (2009). https://doi.org/
10.1007/978-3-642-02658-4 20
16. Dutertre, B.: Yices 2.2. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559,
pp. 737–744. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08867-
9 49
17. Dutertre, B., de Moura, L.: A fast linear-arithmetic solver for DPLL(T). In: Ball,
T., Jones, R.B. (eds.) CAV 2006. LNCS, vol. 4144, pp. 81–94. Springer, Heidelberg
(2006). https://doi.org/10.1007/11817963 11

122
M. Bromberger et al.
18. Ganzinger, H., Hagen, G., Nieuwenhuis, R., Oliveras, A., Tinelli, C.: DPLL(T):
fast decision procedures. In: Alur, R., Peled, D.A. (eds.) CAV 2004. LNCS, vol.
3114, pp. 175–188. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-
540-27813-9 14
19. Griggio, A.: A practical approach to satisﬁability modulo linear integer arithmetic.
JSAT 8(1/2), 1–27 (2012)
20. Hammer, P.L., Rudeanu, S.: Boolean Methods in Operations Research and Related
Areas. Econometrics and Operations Research, vol. 7. Springer, Heidelberg (2012).
https://doi.org/10.1007/978-3-642-85823-9
21. Hart, W., Johansson, F., Pancratz, S.: FLINT: fast library for number theory,
version 2.4.0 (2013). http://ﬂintlib.org
22. Jovanovi´c, D., de Moura, L.: Cutting to the chase solving linear integer arithmetic.
In: Bjørner, N., Sofronie-Stokkermans, V. (eds.) CADE 2011. LNCS (LNAI), vol.
6803, pp. 338–353. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-
642-22438-6 26
23. Karp, R.M.: Reducibility among combinatorial problems. In: Complexity of Com-
puter Computations. The IBM Research Symposia Series, pp. 85–103. Plenum
Press, New York (1972). https://doi.org/10.1007/978-1-4684-2001-2 9
24. Kim, H., Somenzi, F., Jin, H.S.: Eﬃcient term-ITE conversion for satisﬁability
modulo theories. In: Kullmann, O. (ed.) SAT 2009. LNCS, vol. 5584, pp. 195–208.
Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-02777-2 20
25. Nieuwenhuis, R.:
The IntSat method
for
integer
linear
programming. In:
O’Sullivan, B. (ed.) CP 2014. LNCS, vol. 8656, pp. 574–589. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-10428-7 42
26. Nieuwenhuis, R., Oliveras, A.: DPLL(T) with exhaustive theory propagation and
its application to diﬀerence logic. In: Etessami, K., Rajamani, S.K. (eds.) CAV
2005. LNCS, vol. 3576, pp. 321–334. Springer, Heidelberg (2005). https://doi.org/
10.1007/11513988 33
27. Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Solving SAT and SAT modulo theories:
from an abstract Davis-Putnam-Logemann-Loveland procedure to DPLL(T). J.
ACM 53(6), 937–977 (2006). https://doi.org/10.1145/1217856.1217859
28. Nonnengart, A., Weidenbach, C.: Computing small clause normal forms. In: Hand-
book of Automated Reasoning, vol. 1. Elsevier (2001). https://doi.org/10.1016/
b978-044450813-3/50008-4
29. Schrijver, A.: Theory of Linear and Integer Programming. Wiley, New York (1986)
30. Sebastiani, R.: Lazy satisability modulo theories. JSAT 3(3–4), 141–224 (2007)
31. Tseitin, G.S.: On the complexity of derivation in propositional calculus. In: Siek-
mann, J.H., Wrightson, G. (eds.) Automation of Reasoning: Classical Papers on
Computational Logic, vol. 2, pp. 466–483. Springer, Heidelberg (1983). https://
doi.org/10.1007/978-3-642-81955-1 28. First Published. In: Slisenko, A.O. (ed.)
Studies in Constructive Mathematics and Mathematical Logic (1968)
32. Weidenbach, C., Dimova, D., Fietzke, A., Kumar, R., Suda, M., Wischnewski,
P.: SPASS version 3.5. In: Schmidt, R.A. (ed.) CADE 2009. LNCS (LNAI), vol.
5663, pp. 140–145. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-
642-02959-2 10

GRUNGE: A Grand Uniﬁed ATP
Challenge
Chad E. Brown1, Thibault Gauthier1, Cezary Kaliszyk2,3, GeoﬀSutcliﬀe4,
and Josef Urban1(B)
1 Czech Technical University in Prague, Prague, Czech Republic
josef.urban@gmail.com
2 University of Innsbruck, Innsbruck, Austria
3 University of Warsaw, Warsaw, Poland
4 University of Miami, Coral Gables, USA
Abstract. This paper describes a large set of related theorem prov-
ing problems obtained by translating theorems from the HOL4 standard
library into multiple logical formalisms. The formalisms are in higher-
order logic (with and without type variables) and ﬁrst-order logic (possi-
bly with types, and possibly with type variables). The resultant problem
sets allow us to run automated theorem provers that support diﬀerent
logical formalisms on corresponding problems, and compare their perfor-
mances. This also results in a new “grand uniﬁed” large theory bench-
mark that emulates the ITP/ATP hammer setting, where systems and
metasystems can use multiple formalisms in complementary ways, and
jointly learn from the accumulated knowledge.
Keywords: Theorem proving · Higher-order logic ·
First-order logic · Many-sorted logic
1
Introduction
A hammer [7] for an interactive theorem prover (ITP) [22] typically translates an
ITP goal into a formalism used by an automated theorem prover (ATP). Since
the most successful ATPs have so far been for untyped ﬁrst-order logic, the focus
has been on ﬁrst-order translations. There is also interest in ATPs working in
richer formalisms, such as monomorphic and polymorphic, typed ﬁrst-order, and
higher-order logics. The TPTP formats for various formalisms have been adopted
for this work, viz. FOF [46] TF0 [47], TF1 [8], TH0 [5], and TH1 [26]. An interest-
ing related task is the creation of a (grand) uniﬁed large-theory benchmark that
allows fair comparison of such ATP systems, their combination and integration
with premise selectors and machine learners [1], across diﬀerent formalisms. As
Supported by the ERC grant no. 649043 AI4REASON and no. 714034 SMART, by
the Czech project AI&Reasoning CZ.02.1.01/0.0/0.0/15 003/0000466, the European
Regional Development Fund, and the National Science Foundation Grant 1730419 -
“CI-SUSTAIN: StarExec: Cross-Community Infrastructure for Logic Solving”.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 123–141, 2019.
https://doi.org/10.1007/978-3-030-29436-6_8

124
C. E. Brown et al.
a step towards creating such benchmarks we present two families of translations
from the language of HOL4 [42] to the various TPTP formats. We have imple-
mented these translations and plan to use them as the ﬁrst “GRand UNiﬁed ATP
challenGE” (GRUNGE) benchmarks, generalizing existing benchmarks such as
the CakeML export [31] that was used in the large-theory benchmark (LTB)
division of the CASC-J9 ATP competition [48].
The rest of the paper is structured as follows. Section 2 introduces notation
and the HOL syntax. Section 3 introduces the problems – the HOL4 standard
library. Section 4 introduces the ﬁrst family of translations, and Sect. 5 intro-
duces the second family of translations. Section 6 discusses and compares the
translations on an example, and Sect. 7 evaluates the translations using existing
ATPs. Section 8 describes the CASC-27 LTB division, which is based on these
translations. Related work is discussed in Sect. 9.
2
Preliminaries
Since this work is based on the HOL4 standard library, it is necessary to start with
brief comments about the syntax and notion of proof in HOL4. More detailed
information is in [19,42]. HOL4, like several other ITPs (e.g., Isabelle/HOL [36],
HOL Light [20] or ProofPower [28]), is based on an extension of Church’s simple
type theory [12] that includes preﬁx polymorphism and type deﬁnitions [19].
HOL4 includes a type o of propositions, a type ι of individuals and a type (σ →
τ) of functions from a type σ to a type τ. Parentheses are omitted, with →
associating to the right. In addition, there are type variables α and deﬁned
types. At each point in the development of the HOL4 library there is a ﬁnite set
of (previously deﬁned) constants c, and a ﬁnite set of (previously deﬁned) type
constructors κ giving a type κ(σ1, . . . , σn) for types σ1, . . . , σn. For simplicity
we consider the signature to be ﬁxed, to avoid the need to specify the types and
terms relative to an evolving signature.
Terms are generated from constants c and variables x using application (s t)
and λ-abstractions (λx : σ.t) in the expected way, for terms s and t. Parentheses
are omitted, with application associating the left. Binders have scope as far to
the right as possible, consistent with parentheses. Multiple binders over the same
type can be written in a combined form, e.g., λxy : σ.t means λx : σ.λy : σ.t.
Constants may be polymorphic. There are two primitive polymorphic logical
constants: =α is polymorphic with type α →α →o and εα is polymorphic
with type (α →o) →α, where α is a type variable. When terms are deﬁned,
such constants are used with a ﬁxed type for α written as a superscript. New
polymorphic constants can be deﬁned within a HOL4 theory.
Aside from =α and εα, implication ⇒of type o →o →o is primitive. From
these primitive logical constants it is possible to deﬁne ∧, ∨and ¬, as well as
polymorphic operators ∀α and ∃α. The usual notation is used for these logical
connectives, so that the binder notation ∀σ(λx : σ.t) is written as ∀x : σ.t, using
the same binding conventions as for λ-abstractions. Similarly, ∃σ(λx : σ.t) is
written as ∃x : σ.t.

GRUNGE: A Grand Uniﬁed ATP Challenge
125
Terms of type o are called propositions, and we use ϕ and ψ to range over
propositions. A sequent is a pair Γ ⊢ϕ where Γ is a ﬁnite set of propositions and
ϕ is a proposition. There is a notion of HOL4 provability for sequents. While our
translations map HOL4 sequents to TPTP formulae, it is not our intention to
mirror HOL4 provability in the target format. The intention, roughly speaking,
is to gain information about when a HOL4 theorem is a consequence of previous
HOL4 theorems, in some logic weaker than HOL4.
From the simplest perspective, each translation translates HOL4 types and
HOL4 terms (including propositions) to terms in the target format. A type, term
or sequent with no type variables is called monomorphic. As an optimization,
some of the translations translate some monomorphic HOL4 types to types in
the target language. As a common notation throughout this paper, a HOL4 type
σ translated as a term is written as ˆσ, and σ translated as a type is written as
˜σ. Another optimization is to translate HOL4 propositions (and sequents) to the
level of formulae in the target language.
3
Problem Set: The HOL4 Standard Library
Version Kananaskis-12 of the HOL4 standard library contains 15733 formulae:
8 axioms, 2294 deﬁnitions, and 13431 theorems. If most of the formulae were
monomorphic and fell into a natural ﬁrst-order fragment of HOL4, then there
would be a natural translation into the FOF format. However, many formulae are
either polymorphic or higher-order (or both), as Table 1 shows (note that the
numbers are not cumulative, e.g., the 2232 monomorphic ﬁrst-order formulae
do not include the 1632 uni-sorted ﬁrst-order formulae, which could also be
processed by an ATP that can handle the monomorphic types). The problem set
consists of 12140 theorems proven in the HOL4 standard library1, in the context
of a ﬁnite set of dependencies used in the HOL4 proof [16].
Table 1. Number of HOL4 formulae in each category.
First-order
Higher-order Combined
Uni-sorted
1632 (FOF) 0
1632
Monomorphic 2232 (TF0)
3683 (TH0)
5915
Polymorphic
1536 (TF1)
6650 (TH1)
8186
Combined
5400
10333
15733
1 1291 theorems were not included due to dependencies being erased during the build
of the HOL4 library.

126
C. E. Brown et al.
4
Syntactic Translations via First-Order Encodings
There already exist many families of translations for HOL to the TPTP format,
usually developed for hammers [27,35]. We have adopted and adapted them:
(i) We have made the translations more local, by making them independent
for each theorem, i.e., unaﬀected by other theorems. In particular, this means
that when the same lambda function appears in two theorems, lambda-lifting
will produce two new functions. (ii) We have made more problems provable
(in principle) by introducing additional axioms and relying on an embedding of
polymorphic types instead of relying on heuristic monomorphization. (iii) For the
TF0 and TH0 formats, we have made use of their polysortedness by expressing
the type of monomorphic constants directly using TF0 and TH0 types.
The translations are described in the order TH1 →TF1 →FOF →TF0 →
TH0, as translations to the later formats take advantage of translation techniques
used for earlier formats.
4.1
Translating to TH1
TH1 is a language that is strictly more expressive than HOL4. Therefore HOL4
formulae can be represented in TH1 with minimal eﬀort. This produces the TH1-I
collection of ATP problems.
Alignment of Logical Constructions. The TPTP format contains a set of deﬁned
constructs that have implicit deﬁnitions, and HOL4 objects are mapped to their
TPTP counterparts in a natural way. The boolean type o of HOL4 is mapped to
the deﬁned TPTP type $o. The arrow type operator is mapped to the TPTP
arrow >. All other type operators are declared to take n types and give a type,
using the TPTP “type of types” $tType. For example, the type operator list has
type $tType > $tType.
The TPTP logical connectives ∧, ∨, ⇒, ¬, =, ∀, ∃are used at the top-level of
the translated formula whenever possible, but the corresponding HOL4 constants
are used when necessary. Equivalences relating HOL4 logical constants to TPTP
connectives are included.
Explicit Type Arguments. A HOL4 constant c carries a type ν′. This type is an
instance of type ν that was given to c when c was created. By matching ν with ν′,
a type substitution s can be inferred. Ordering the type variables in the domain
of s, the implicit type arguments of c are deduced. Making the quantiﬁcation of
type variables and the type arguments explicit is required in the TH1 format.
The eﬀect that this requirement has on a constant declaration and a formula is
shown in Example 1.
Example 1. Explicit type arguments and type quantiﬁcations

GRUNGE: A Grand Uniﬁed ATP Challenge
127
HOL4
TH1
Type of I α →α
∀α : $tType. α →α
Formula
∀x : α. (I x) = x ∀α : $tType. ∀x : α. ((I α) x) = x
4.2
Translating to TF1
To produce the TF1-I collection of ATP problems, all the higher-order features
of the HOL4 problems have to be eliminated. This is done in a sequence of steps.
Lambda-Lifting and Boolean-Lifting. One of the higher-order features is the pres-
ence of lambda-abstraction. The translation ﬁrst uses the extensionality property
to add extra arguments to lambdas appearing on either side of an equality, and
then beta-reduce the formula. Next lambda-lifting [14,35] is used. Lambda-lifting
creates a constant f for the leftmost outermost lambda-abstractions appearing at
the term-level. This constant f replaces the lambda-abstraction in the formula. A
deﬁnition is given for f, which may involve some variable capture - see Example 2.
This procedure is repeated until all the atoms are free of lambda-abstractions.
The deﬁnitions are part of the background theory, and are considered to be
axioms in the TF1 problem even if they were created from the conjecture.
Example 2. Lambda-lifting with variable capture
Original formula: ∀k. linear (λx. k × x)
Additional deﬁnition: ∀k x. f k x = k × x
New formula: ∀k. linear (f k)
A similar method can be applied to move some logical constants from the
term-level to the formula-level - see Example 3 (this optimization is not applied
in our second family of translations).
Example 3. Boolean-lifting
Original formula: ∀x. x = COND (x = 0) 0 x
Additional deﬁnition: ∀x. f x ⇔(x = 0)
New formula: ∀x. x = COND (f x) 0 x
To allow the ATPs to create their own encoded lambda-abstractions, axioms
for the combinators S, K and I are added to every problem. These axioms are
omitted in the second family of translations. In the TH0-II versions of the prob-
lem, combinators are not needed since all simply typed λ-calculus terms are
already representable. In the TF0-II and FOF-II versions only an axiom for I
and a partially applied axiom for K are included. Combinator axioms enlarge
the search space, which hinders the ATPs unnecessarily because they are not
needed for proving most of the theorem.

128
C. E. Brown et al.
Apply Operator and Arity Equations. As functions cannot be passed as argu-
ments in ﬁrst-order logic, an explicit apply operator ap is used to apply a function
to an argument. This way all objects (constants and variables) have arity zero
except for the apply operator which has arity two. The HOL4 functional exten-
sionality axiom is added to all problems, as it expresses the main property of
the apply operator:
∀f g (∀x. ap f x = ap g x) ⇒f = g
This axiom also demonstrates how the higher-order variables f and g become
ﬁrst-order variables after the introduction of ap.
To limit the number of apply operators in a formula, versions of each constant
are deﬁned for all its possible arities, in terms of the zero-arity version. These
constants are used in the translated formula - see Example 4.
Example 4. Using constants with their arity
Original formula: SUC 0 = 1 ∧∃y. MAP SUC y ̸= y
Arity equations: SUC1 x = ap SUC0 x, . . .
New formula: SUC1 00 = 10 ∧∃y. MAP2 SUC0 y ̸= y
If the return type of a constant is a type variable then some of its instances
can expect an arbitrarily large numbers of arguments. In the case where the
number of arguments n′ of an instance exceeds the number of arguments n of
the primitive constant, variants of this constant are not created for this arity.
Instead, the apply operator is used to reduce the number of arguments to n. For
example, the term I I x is not translated to I2 I0 x but instead to ap (I1 I0) x.
TF1 types. As a ﬁnal step, type arguments and type quantiﬁcations are added as
in Sect. 4.1. Moreover, the boolean type of HOL4 is replaced by $o at the formula-
level, and by o at the term-level (because $o is not allowed at the term-level in
a ﬁrst-order formula). This causes a mismatch between the type of the atom
o and the type of the logical connective $o. Therefore an additional operator
p : o →$o is applied on top of every atom. The following properties of p and o
are added to every translated problem (written here in the ﬁrst-order style for
function application):
∀xy:o. (p(x) ⇔p(y)) ⇒(x = y)
p(true), ¬p(false), ∀x:o. x = true ∨x = false
In a similar manner, the TPTP arrow type cannot be used whenever a func-
tion appears as an argument. Instead the type constructor fun is used, as illus-
trated by the following constant declaration:
ap : ∀α : $tType β : $tType. ((fun(α, β) × α) →β)).

GRUNGE: A Grand Uniﬁed ATP Challenge
129
4.3
Translating to FOF
The translation to FOF, which produces the FOF-I collection of ATP problems,
follows exactly the same path as the translation to TF1 except that the types are
encoded as ﬁrst-order terms. To represent the fact that a ﬁrst-order term t has type
ν, the tagging function s, introduced by Hurd [23], is used: every term t of type ν
is replaced by s(ˆν, t). Going from the type ν to the term ˆν eﬀectively transforms
type variables into term variables, and type operators into ﬁrst-order functions
and constants. Type arguments are unnecessary as the tags contain enough infor-
mation. In practice, the s tagging function prevents terms of diﬀerent types from
unifying, and allows instantiation of type variables - see Example 5.
Example 5. Type instantiation
TF1
FOF
∀x:α. I(α, x) = x
∀ˆα ˆx. s(ˆα,ˆI(s(ˆα, ˆx))) = s(ˆα, ˆx)
∀x:num. I(num, x) = x ∀ˆx. s( 
num,ˆI(s( 
num, ˆx))) = s( 
num, ˆx)
4.4
Translating to TF0
An easy way to translate HOL4 formulae to TF0, which produces the TF0-I
collection of ATP problems, is to take the translation to FOF and inject it into
TF0.
Trivial Injection from FOF to TF0. The ﬁrst step is to give types to all the
constants and variables appearing the FOF formula. A naive implementation
would be to give the type ιn →ι to symbols with arity n. However, since it
is known that the ﬁrst argument comes from the universe of non-empty types,
and the second argument comes from the universe of untyped terms, an explicit
distinction can be made. The type of s is deﬁned to be δ × μ →ι, with δ being
the universe of non-empty types, μ being the universe of untyped terms, and ι
being the universe of typed terms. After this translation a type operator (or type
variable) with m arguments has type δm →δ, and a function (or term variable)
with n arguments has type ιn →μ. The type of p is p : ι →$o. Declaring the
type of all these objects achieves a trivial translation from FOF with tags to TF0.
Using Special Types. To take full advantage of the polysortedness of TF0, a con-
stant ˜cn
ν is declared for every constant cn, with arity n and monomorphic type
ν. The type of ˜cn
ν is declared to be ( ˜ν1 × . . . × ˜νn) →˜ν0, where ˜ν1, . . . , ˜νn, and
˜ν0 are basic types. A basic type constructs a single type from a monomorphic
type, e.g., list real for list[real], fun o o for fun(o, o). The basic types are special
types, and are declared using $tType. Thanks to these new constants monomor-
phic formulae can be expressed in a natural way, without type encodings in the
formula. Nevertheless, an ATP should still be able to perform a type instanti-
ation if necessary. That is why we relate the monomorphic representation with
its tagged counterpart.

130
C. E. Brown et al.
If a term has a basic type then it lives in the monomorphic world where
as a term of type ι it belongs to the tagged world. All monomorphic terms
(constructed from monomorphic variables and constants) can be expressed in
the monomorphic world. To relate the two representations of the same HOL4
term an “injection” iν and a “surjection” jν are deﬁned for each basic type ˜ν.
The constants iν : ˜ν →μ and jν : ι →˜ν must respect the following properties,
which are included as axioms in the translated problems:
∀x:μ. s(ˆν, iν(jν(s(ˆν, x)))) = s(ˆν, x)
∀x: ˜ν. jν(s(ˆν, iν(x))) = x
Whenever ˜cn
ν is an instance of a polymorphic function ˆcn, the following
equation is included in the TF0 problem, which relates the two representatives:
∀x1 : ˜ν1...xn : ˜νn. s(ν0, iν0( ˜cn
ν(x1,..., xn))) = s(ν0, ˆcn(s(ν1, iν1(x1)),..., s(νn, iνn(xn)))
Example 6 shows how “injections”, “surjections”, and special types can be
used to translate a theorem mixing polymorphic and monomorphic elements.
Example 6.
Special types
The polymorphic function I is applied to the monomorphic variable x. Using
special types, type tags can be dropped for x.
FOF : ∀ˆx. s( 
num,ˆI( s( 
num, ˆx) )) = s( 
num, ˆx)
TF0 : ∀˜x : 
num. jnum(ˆI( s( 
num, inum(˜x)) )) = ˜x
Eﬀect on Deﬁned Operators. The ap operator is treated in the same way as
every other constant. In particular, a diﬀerent version of ap is created for each
monomorphic type. The type of p becomes ˜o →$o, and the projection jo is used
to transfer atoms from the tagged world to the monomorphic world.
If the presence of the p predicate and the inclusion of additional equations
are ignored, our translation of a HOL4 ﬁrst-order monomorphic formula using
special types to TF0 is simply the identity transformation.
4.5
Translating to TH0
Translating from HOL4 to TH0, which produces the TH0-I collection of ATP
problems, is achieved in a way similar to the translation to TF0. The HOL4
formulae are ﬁrst translated to FOF and then trivially injected into TH0. Special
types are used for basic types extracted from monomorphic types. The set of
higher-order basic types is slightly diﬀerent from the ﬁrst-order one, where we
recursively remove arrow types until a non-arrow constructor is found. In the
higher-order setting a single monomorphic constant ˜cν is used to replace all arity
versions of c: ∀f x. ˜apν f x = f x. Another beneﬁt of the expressivity of TH0 is
that the basic type ˜o can be replaced by $o, and the predicate p can be omitted.
The eﬀect of the previous steps is illustrated in Example 7.

GRUNGE: A Grand Uniﬁed ATP Challenge
131
Example 7. Translations of ∃f. f 0 = 0
In this example ˜apν has type (fun num num × 
num) →
num where fun num num
is the special type corresponding to num →num.
TF0 : ∃˜f :fun num num. ˜apν ( ˜f, ˜0num) = ˜0num
TH0 : ∃˜f : 
num →
num. ˜f ˜0num = ˜0num
In order to have the same shallowness result for TH0 as for TF0, it would
be necessary to replace monomorphic constants created by the lifting procedure
by their lambda-abstractions. We chose to keep the deﬁnitions for the lifted
constants, as they allow some term-level logical operators to be pushed to the
formula level.
5
Semantic Translations via Set Theory Encodings
The second family of translations into TH0, TF0, and FOF is semantically moti-
vated [38]: we make use of constructors known to be deﬁnable in set theory. Types
and terms are translated to sets, where types must translate to non-empty sets.
The translation may optionally use other special types for monomorphic types in
the HOL4 source. In the TH0 case the builtin type $o can be used for the HOL4
type o. In the ﬁrst-order cases HOL4 terms of type o are sometimes translated to
terms, and sometimes to formulae, depending on how the HOL4 term is used. In
the TF0 case a separate type ˜o of booleans is declared, which is used as the type
of terms translated from HOL4 terms of type o. In the FOF case this approach is
not possible, as all terms have the same type (intuitively representing sets). The
other main diﬀerence between the translation to TH0 and the translations to
the ﬁrst-order languages is that the ﬁrst-order translations make use of lambda
lifting [14,35]. As a result of the translations we obtain three new collections of
ATP problems are produced: TH0-II, TF0-II and FOF-II.
5.1
Translating to TH0
The base type o for propositions is written as $o in TH0, and ι for individuals
is written as $i. In addition a base type δ is declared. The translation treats
elements of type ι as sets, and elements of type δ as non-empty sets. The basic
constants used in the ATP problems are as follows:
– bool : δ is used for a ﬁxed two element set.
– ind : δ is used for a ﬁxed non-empty set corresponding to HOL4’s type of
individuals.
– arr : δ →δ →δ is used to construct the function space of two sets.
– mem : ι →δ →o corresponds to the membership relation on sets, where the
second set is known to be non-empty. The term mem s t is written as s ∈t,
and the term ∀x.x ∈s →t is written as ∀x ∈s.t.

132
C. E. Brown et al.
– ap : ι →ι →ι corresponds to set theory level application (represented as a
set).
– lam : δ →(ι →ι) →ι is used to build set bounded λ-abstractions as sets.
– p : ι →o is a predicate that indicates whether or not an element of bool is
true or not.
– io : o →ι is an injection of o into ι, essentially translating false to a set and
true to a diﬀerent set.
The basic axioms included in each ATP problem are:
Injo: ∀X : o.ioX ∈bool.
Iso1
o: ∀X : o.p(ioX) = X.
Iso2
o: ∀X ∈bool.io(pX) = X.
aptp: ∀AB : δ.∀f ∈(arr A B).∀x ∈A.(ap f x) ∈B.
lamtp: ∀AB : δ.∀F : ι →ι.(∀x ∈A.F x ∈B) →(lam A F) ∈(arr A B).
FunExt: ∀AB : δ.∀f ∈(arr A B).∀g ∈(arr A B).
(∀x ∈A.ap f x = ap g x) →f = g.
beta: ∀A : δ.∀F : ι →ι.∀x ∈A.(ap (lam A F) x) = F x.
If ι is interpreted using a model of ZFC and δ using a copy of the non-empty
sets in this model, then the constants above can be interpreted in an obvious
way so as to make the basic axioms true.
Given this theory, a basic translation from HOL4 to TH0 is as follows. Each
HOL4 type α (including type variables) is mapped to a term ˆα of type δ. HOL4
type variables (constants) are mapped to TH0 variables (constants) of type δ.
For the remaining cases bool, ind, and arr are used. Each HOL4 term s : α is
mapped to a TH0 term ˆs of type ι, for which the context ˆs ∈ˆα is always known.
The invariant can be maintained by including the hypothesis ˆx ∈ˆα whenever
x is a variable or a constant. The ap and lam constants are used to handle
HOL4 applications and λ-abstractions. The axioms aptp and lamtp ensure the
invariant is maintained. Finally HOL4 propositions (which may quantify over
type variables) are translated to TH0 propositions in an obvious way, using p to
go from ι to o, and io to go from o to ι, when necessary. As an added heuristic, the
translation makes use of TH0 connectives and quantiﬁers as deeply as possible,
delaying the use of p whenever possible.
Using Special Types. As with the ﬁrst family of translations, the second family
optimizes by using special types for HOL4 types with no type variables, e.g., num
and list num. Unlike the ﬁrst family, special types are not used for monomorphic
function types. As a result it is not necessary to consider alternative ap operators.
A basic monomorphic type is a monomorphic type that is not of the form α →β.
If special types are used, then for each basic monomorphic type occurring in
a proposition a corresponding TH0 type γ is declared, mappings and axioms
relating γ to the type ι of sets are declared, and the type γ is used to translate
terms of the type and quantiﬁers over the type when possible. For example, if
a basic monomorphic type ν (e.g., num) occurs in a HOL4 proposition, then in
addition to translating ν as a term ˆν : ι we also declare a TH0 type ˜ν, iν : ˜ν →ι

GRUNGE: A Grand Uniﬁed ATP Challenge
133
and jν : ι →˜ν along with axioms ∀x : ˜ν.jν(iνx) = x and ∀x : ι.x ∈ˆν →iν(jνx) =
x.
One obvious basic monomorphic type is o. In the case of o a new type is not
declared, but instead the TH0 type $o is used. That is, ˜o denotes $o. Note that
io : ˜o →ι is already declared. Additionally, jo is used as shorthand for p, which
has the desired type ι →˜o.
Suppose a HOL4 constant c has type α1 →. . . →αn →β, where α1, . . . , αn, β
are basic monomorphic types with corresponding TH0 types
˜α1, . . . , ˜
αn, ˜β.
Instead of translating a term c t1 · · · tn as a term of type ι, each ti is trans-
lated to a term ˆti of type ˜αi, and a ﬁrst order constant ˜c : ˜α1 →· · · →˜
αn →˜β
is used to translate to the term ˜c ˆt1 · · · ˆtn of type ˜β. In such a case an equation
relating ˆc to ˜c is also included. Since the translation may return a term of type ι
or ˜α, where α is a basic monomorphic type, iα and jα are used to obtain a term
of type ˜α or ι when one is required. If a quantiﬁer ranges over a monomorphic
type α, a quantiﬁer over type ˜α is used instead of using a quantiﬁer over type ι
and using ∈to guard the quantiﬁer.
5.2
Translating to TF0
There are two main modiﬁcations to the translation to TH0 when targeting TF0.
Firstly, propositions cannot be treated as special kinds of terms in TF0. In order
to deal with this o is treated like other special types by declaring a new type ˜o
and functions io : ˜o →ι and jo : ι →˜o along with corresponding axioms as above.
Note that unlike the TH0 case, jo diﬀers from p. In TF0 p is a unary predicate
on ι, and jo is a function from ι to ˜o. In the TF0 versions of the axioms Iso1
o and
Iso2
o, p is replaced with jo. Secondly, the background theory cannot include the
higher-order lam operator. Therefore the lam operator is omitted, and lambda
lifting is used to translate (most) HOL4 λ-abstractions. The two higher-order
axioms lamtp and beta are also omitted.
In the TH0 case, the background axioms are enough to infer the following
(internal) propositional extensionality principle
∀Q ∈bool.∀R ∈bool.(p Q ↔p R) →Q = R
from the corresponding extensionality principle ∀QR : o.(Q ↔R) →Q = R
valid in TH0. This is no longer the case in TF0, so propositional extensionality
is added as an axiom.
There are two special cases where lambda lifting can be avoided: identity and
constant functions. For this purpose a new unary function I on sets and a new
binary function K on sets are added. Two new basic axioms are added to the
ATP problem for these functions:
Id: ∀A : δ.∀X ∈A.(ap (I A) X) = X.
Const: ∀A : δ.∀Y : ι.∀X ∈A.(ap (K A Y ) X) = Y .
A HOL4 term λx : α.x is translated as I ˆα. For a HOL4 term λx : α.t, where
x is not free in t, t is translated to a ﬁrst-order term ˆt of type ι, and the λ-term

134
C. E. Brown et al.
is translated to K ˆα ˆt. If there is already a function deﬁned for λx : α.t (with the
same variable names), then that function is reused. Otherwise, lambda lifting
of λx : α.t proceeds as follows. Let α1, . . . , αm be type variables occurring in
λx : α.t and y1 : β1, . . . , yn : βn be the free variables occurring in λx : α.t.
Assume ˆt is a ﬁrst-order term translation of t, with ˆx of type ι corresponding
to the variable x. (Note that this may have involved some lambda lifting.) Let f
be a new m + n-ary function returning sets. If special types are not being used,
then each argument of f is a set. If special types are used, then each argument is
a set unless it corresponds to yi : βi, where βi is a monomorphic type in which
case the argument has type ˜βi. The following axioms about f are added to the
ATP problem:
ftp: ∀A1 · · · An : δ.∀Y1 · · · Ym : ι. · · · (f A1 · · · An Y1 · · · Ym) ∈ˆα.
fbeta: ∀A1 · · · An : δ.∀Y1 · · · Ym : ι. · · · ∀X ∈ˆα.ap (f A1 · · · An Y1 · · · Ym) X = ˆt.
In these axioms the preconditions that each Yi must be in ˆβi if Yi has type ι
have been elided (otherwise special types are being used, βi is monomorphic, Yi
has type ˜βi, and no guard is required).
5.3
Translating to FOF
In order to translate to FOF, all terms must be translated to the same type, eﬀec-
tively the type ι. This requires omission of any special treatment of monomorphic
types, and instead all HOL4 terms are translated to terms of type ι. The type δ of
non-empty sets is also omitted. Instead, ι is used wherever δ was used in the TF0
setting, and quantiﬁers that were over δ are guarded by a new non-emptiness
predicate ne : ι →o. Aside from these changes, the translation proceeds using
lambda lifting as in the TF0 case.
6
Case Study
A very simple HOL4 theorem is ∀f : α →β.∀x : α.LETα,β f x = f x, where
LETα,β is deﬁned to be λf : α →β.λx : α.fx. Informally the proof is clear:
expand the deﬁnition of LET and perform two β-reductions. However, proving
various translated versions of the problem range from trivial to challenging.
The ﬁrst family of translations make use of a preprocessing step (Sect. 4.2)
that changes the deﬁnition of LET from LETα,β = λf : α →β.λx : α.fx to
∀x : α →β.∀x′ : α.LETα,β x x′ = x x′.
This step makes the deﬁnition of LET the same (up to α-conversion) as the
theorem. Even if further encodings are applied to obtain a ﬁrst-order problem,
the axiom will still be the same as the conjecture. Consequently all versions
resulting from the ﬁrst family of translations are trivially provable.
The TH0-II version has conjecture
∀AB : δ.∀f ∈(arr A B).∀x ∈A.ap (ap (LET A B) f) x = ap f x

GRUNGE: A Grand Uniﬁed ATP Challenge
135
and the axiom (corresponding to the deﬁnition of LET)
∀AB : δ.LET A B = lam (arr A B) (λf : ι.lam A (λx : ι.ap f x)).
The axiom deﬁning LET combined with the basic axiom beta is enough to prove
the theorem. However, the TH0-II version also includes all the other basic axioms
along with internal versions of the logical constants for universal quantiﬁcation
and equality. The extra axioms make the problem hard for ATP systems, but
if only the necessary axioms are provided the problem is easy. In TF0-II and
FOF-II the conjecture is the same as in the TH0-II version, but the deﬁnition of
LET is split into two functions declared when lambda lifting:
∀A · · · ∀B · · · LET A B = f14 A B,
∀A · · · ∀B · · · ∀f ∈(arr A B).ap (f14 A B) f = f13 A B f
and
∀A · · · ∀B · · · ∀f ∈(arr A B).∀x ∈A.ap (f13 A B f) x = ap f x.
All the ﬁrst-order versions of this problem are easy for current ATP systems.
7
Results
Since the HOL4 library has a natural order of the problems, each translation
can generate two versions of each problem. The bushy (small) version contains
only the (translated) library facts that were needed for the HOL4 proof of the
theorem. The chainy (large) version contains all the facts that precede the the-
orem in the library order, i.e., the real task faced by hammer systems. Chainy
problems typically include thousands of axioms, requiring the use of premise
selection algorithms [1] as a front-end in the ATP systems. Thus, in order to
maintain the focus on ATP system performance, the results of running the ATP
systems on the bushy problems are presented here.
Nineteen ATPs were run on the 12140 problems in each of the bushy problem
sets, according to the ATPs’ support for the various TPTP formats. In each case
we ran the ATP with a CPU time limit of 60s per problem. Table 2 summarizes
the results. In union, more proofs were found in the ﬁrst family of translations
than in the second family, in all formats. However, some provers like Vampire
4.3 and SPASS 3.9 do better on FOF-II than on FOF-I. This indicates that these
provers are probably better at reasoning with type guards than with type tags.
Of the 12140 problems 7412 (61.1%) were solved by some ATP in one of the
representations.
The TacticToe [17,18] prover built into HOL4 has been tested as a base-
line comparison, and it (re)proves 5327 of 8855 chainy versions of the problems
(60.2%). TacticToe is a machine-learning guided prover that searches for a tacti-
cal proof by selecting suitable tactics and theorems learned from human-written
tactical proofs. By design, this system works in the chainy setting. In total 8840
(72.8%) of the 12140 problems can be proved by either TacticToe or one of the
ATPs using one of the translations.

136
C. E. Brown et al.
Table 2. Number of theorems proved, out of 12140. Each ATP is evaluated on all its
supported TPTP formats.
System
TH1-I
TH0-I
TH0-II
TF1-I
TF0-I
TF0-II
FOF-I
FOF-II
Union
agsyHOL 1.0 [32]
1374
1187
1605
Beagle 0.9.47 [3]
2008
2047
2449
2498
3183
cocATP 0.2.0
899
599
1000
CSE E 1.0 [51]
4251
3102
4480
CVC4 1.6 [2]
4851
3991
5030
3746
5709
E 2.2 [41]
4277
3622
4618
3844
5118
HOLyHammer 0.21 [27]
5059
5059
iProver 2.8 [29]
2778
2894
3355
iProverModulo 2.5-0.1 [11]
2435
1639
1433
1263
2852
LEO-II 1.7.0 [4]
2579
1923
2119
1968
3702
Leo-III 1.3 [43,44]
6668
5018
3485
3458
4032
3421
3986
3185
7090
Metis 2.4 [24]
2353
474
2356
Princess 170717 [39,40]
3646
2138
3162
2086
4096
Prover9 1109a [33]
2894
1742
3128
Satallax 3.3 [10]
2207
1292
2494
SPASS 3.9 [50]
2850
3349
3821
Vampire 4.3 [30]
4837
4693
4008
4928
5929
ZenonModulo [15]
1071
1038
1041
1026
1198
1751
Zipperposition 1.4 [13]
2252
2161
3771
3099
2576
2531
1795
4251
Union
6824
5209
3771
4663
5732
5074
5909
5249
7412
8
GRUNGE as CASC LTB Division
The CADE ATP System Competition (CASC) [45] is the annual evaluation of
fully automatic, classical logic Automated Theorem Proving (ATP) systems – the
world championship for such systems. CASC is divided into divisions according
to problem and system characteristics. Each competition division uses problems
that have certain logical, language, and syntactic characteristics, so that the
systems that compete in the division are, in principle, able to attempt all the
problems in the division. For example, the First-Order Form (FOF) division
uses problems in full ﬁrst-order logic, with each problem having axioms and a
conjecture to be proved.
While most of the CASC divisions present the problems to the ATP systems
one at a time, with an individual CPU or wall clock time limit per problem,
the Large Theory Batch (LTB) division presents the problems in batches, with
an overall wall clock time limit on the batch. As the name also suggests, the
problems in each batch come from a “large theory”, which typically has many
functors and predicates, and many axioms of which only a few are required for the
proof of a theorem. The problems in a batch typically have a common core set of
axioms used by all problems, and each problem typically has additional axioms
that are speciﬁc to the problem. The batch presentation allows the ATP systems
to load and preprocess the common core set of axioms just once, and to share
logical and control results between proof searches. Each batch is accompanied

GRUNGE: A Grand Uniﬁed ATP Challenge
137
by a set of training problems and their solutions, taken from the same source as
the competition problems. The training data can be used for ATP system tuning
and learning during (typically at the start of) the competition.
In CASC-J9 [48] – the most recent edition of the competition – the LTB
division used FOF problems exported from CakeML [31]. At the time there was
growing interest in an LTB division for typed higher-order problems, and it
became evident that a multi-format LTB division would add a valuable dimen-
sion to CASC. For the CASC-27 LTB division each problem was presented in
multiple formats: TH1, TH0, TF1, TF0, and FOF. The work described in this
paper provides the problems. Systems were able to attempt whichever versions
they support, and a solution to any version constitutes a solution to the problem.
For example, Leo-III is able to handle all the formats, while E can attempt only
the tﬀzero and FOF formats.
The batch presentation of problems in the LTB division provides interesting
opportunities for ATP systems, including making multiple attempts on problems
and learning search heuristics from proofs found. The multi-format LTB division
extends these possibilities by allowing multiple attempts on problems by virtue
of the multiple formats available, and learning from proofs found in one format
to improve performance on problems in another format. The latter is especially
interesting, with little known research in this direction.
9
Related Work
The HOL4 library already has translations for SMT solvers such as Yices [49],
Z3 [9] and Beagle. A link to ﬁrst-order ATPs is also available thanks to exports
[16] of HOL4 theories to the HOL(y)Hammer framework [27]. Another notable
project that facilitates the export of HOL4 theories is Open Theory [25]. The gen-
eral approach for higher-order to ﬁrst-order translations is laid out in Hurd [23].
An evaluation of the eﬀect of diﬀerent translations on ATP-provability was per-
formed in [35]. A further study shows the potential improvements provided by
the use of supercombinators [14]. In our work, the use of lambda-lifting (or
combinators) is not necessary in TH0-II thanks to the use of the higher-order
operator lam. This is similar to using higher-order abstract syntax to model
syntax with binders [37].
A method for encoding of polymorphic types as terms through type tags (as in
our ﬁrst translation) or type guards (as in our second translation) is described in
[6]. Translations [21,44] from a polymorphic logic to a monomorphic poly-sorted
logic without encoding typically rely on heuristic instantiations of type variables.
However, heuristics may miss useful instantiations, and make the translation less
modular (i.e., context dependent). Our translations to TH0 and TF0 try to get
the best of both worlds by using a type encoding for polymorphic types and
special types for basic monomorphic types.

138
C. E. Brown et al.
10
Conclusion
This work has deﬁned, compared, and evaluated ATP performance on two fam-
ilies of translations of the HOL4 logic to a number of ATP formalisms, and
described a new uniﬁed large-theory ATP benchmark (GRUNGE) based on
them. The ﬁrst family is designed to play to the strengths of the calculi of
most ATP systems, while the second family is based on more straightforward
semantics rooted in set theory. The case study shows how diﬀerent the trans-
lated problems may be, even in a simple example. A number of methods and
optimizations have been used, however it is clear that the translations can be
further optimized and that diﬀerent encodings favour diﬀerent provers. Out of
12140 HOL4 theorems, the ATP systems can solve 7412 problems in one or more
of the formats. The TacticToe system that works directly in the HOL4 formalism
and uses HOL4 tactics could solve 5327 problems. Together the total number
of problems solved is 8840. Leo-III was the strongest system in the higher-order
representations. In the ﬁrst-order representations the strongest systems were Zip-
perposition, CVC4, E and Vampire. A pre-release of the bushy versions of the
problems was provided before CASC-272, to allow system developers to adapt
and tune their systems before the competition.
References
1. Alama, J., Heskes, T., K¨uhlwein, D., Tsivtsivadze, E., Urban, J.: Premise selection
for mathematics by corpus analysis and kernel methods. J. Autom. Reason. 52(2),
191–213 (2014). https://doi.org/10.1007/s10817-013-9286-5
2. Barrett, C., et al.: CVC4. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011.
LNCS, vol. 6806, pp. 171–177. Springer, Heidelberg (2011). https://doi.org/10.
1007/978-3-642-22110-1 14
3. Baumgartner, P., Waldmann, U.: Hierarchic superposition with weak abstraction.
In: Bonacina, M.P. (ed.) CADE 2013. LNCS (LNAI), vol. 7898, pp. 39–57. Springer,
Heidelberg (2013). https://doi.org/10.1007/978-3-642-38574-2 3
4. Benzm¨uller, C., Paulson, L.C., Theiss, F., Fietzke, A.: LEO-II - a cooperative
automatic theorem prover for classical higher-order logic (system description). In:
Armando, A., Baumgartner, P., Dowek, G. (eds.) IJCAR 2008. LNCS (LNAI),
vol. 5195, pp. 162–170. Springer, Heidelberg (2008). https://doi.org/10.1007/978-
3-540-71070-7 14
5. Benzm¨uller, C., Rabe, F., Sutcliﬀe, G.: THF0 – the core of the TPTP
language
for
higher-order
logic.
In:
Armando,
A.,
Baumgartner,
P.,
Dowek,
G.
(eds.)
IJCAR
2008.
LNCS
(LNAI),
vol.
5195,
pp.
491–506.
Springer,
Heidelberg
(2008).
https://doi.org/10.1007/978-3-540-71070-7 41.
http://christoph-benzmueller.de/papers/C25.pdf
6. Blanchette, J.C., B¨ohme, S., Popescu, A., Smallbone, N.: Encoding monomorphic
and polymorphic types. In: Piterman, N., Smolka, S.A. (eds.) TACAS 2013. LNCS,
vol. 7795, pp. 493–507. Springer, Heidelberg (2013). https://doi.org/10.1007/978-
3-642-36742-7 34
2 http://www.tptp.org/CASC/27/TrainingData.HL4.tgz.

GRUNGE: A Grand Uniﬁed ATP Challenge
139
7. Blanchette, J.C., Kaliszyk, C., Paulson, L.C., Urban, J.: Hammering towards QED.
J. Formalized Reason. 9(1), 101–148 (2016). https://doi.org/10.6092/issn.1972-
5787/4593
8. Blanchette, J.C., Paskevich, A.: TFF1: the TPTP typed ﬁrst-order form with
rank-1 polymorphism. In: Bonacina, M.P. (ed.) CADE 2013. LNCS (LNAI), vol.
7898, pp. 414–420. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-
642-38574-2 29
9. B¨ohme, S., Weber, T.: Fast LCF-style proof reconstruction for Z3. In: Kaufmann,
M., Paulson, L.C. (eds.) ITP 2010. LNCS, vol. 6172, pp. 179–194. Springer, Hei-
delberg (2010). https://doi.org/10.1007/978-3-642-14052-5 14
10. Brown, C.E.: Satallax: an automatic higher-order prover. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 111–117. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 11
11. Burel, G.: Experimenting with deduction modulo. In: Bjørner, N., Sofronie-
Stokkermans, V. (eds.) CADE 2011. LNCS (LNAI), vol. 6803, pp. 162–176.
Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-22438-6 14
12. Church, A.: A formulation of the simple theory of types. J. Symb. Logic 5, 56–68
(1940)
13. Cruanes, S.: Extending superposition with integer arithmetic, structural induction,
and beyond. (Extensions de la Superposition pour l’Arithm´etique Lin´eaire Enti`ere,
l’Induction Structurelle, et bien plus encore). Ph.D. thesis, ´Ecole Polytechnique,
Palaiseau, France (2015). https://tel.archives-ouvertes.fr/tel-01223502
14. Czajka, L.: Improving automation in interactive theorem provers by eﬃcient encod-
ing of lambda-abstractions. In: Avigad, J., Chlipala, A. (eds.) Proceedings of the
5th ACM SIGPLAN Conference on Certiﬁed Programs and Proofs, Saint Peters-
burg, FL, USA, 20–22 January 2016, pp. 49–57. ACM (2016). https://doi.org/10.
1145/2854065.2854069
15. Delahaye, D., Doligez, D., Gilbert, F., Halmagrand, P., Hermant, O.: Zenon mod-
ulo: when achilles outruns the tortoise using deduction modulo. In: McMillan et al.
[34], pp. 274–290. https://doi.org/10.1007/978-3-642-45221-5 20
16. Gauthier, T., Kaliszyk, C.: Premise selection and external provers for HOL4. In:
Certiﬁed Programs and Proofs (CPP 2015). ACM (2015). https://doi.org/10.1145/
2676724.2693173
17. Gauthier, T., Kaliszyk, C., Urban, J.: TacticToe: learning to reason with HOL4
tactics. In: Eiter, T., Sands, D. (eds.) 21st International Conference on Logic for
Programming, Artiﬁcial Intelligence and Reasoning, LPAR-21, Maun, Botswana,
7–12 May 2017. EPiC Series in Computing, vol. 46, pp. 125–143. EasyChair (2017).
http://www.easychair.org/publications/paper/340355
18. Gauthier, T., Kaliszyk, C., Urban, J., Kumar, R., Norrish, M.: Learning to prove
with tactics. CoRR (2018). http://arxiv.org/abs/1804.00596
19. Gordon, M.J.C., Melham, T.F. (eds.): Introduction to HOL: A Theorem Proving
Environment for Higher Order Logic. Cambridge University Press (1993). http://
www.cs.ox.ac.uk/tom.melham/pub/Gordon-1993-ITH.html
20. Harrison, J.: HOL light: a tutorial introduction. In: Srivas, M., Camilleri, A. (eds.)
FMCAD 1996. LNCS, vol. 1166, pp. 265–269. Springer, Heidelberg (1996). https://
doi.org/10.1007/BFb0031814
21. Harrison, J.: Optimizing proof search in model elimination. In: McRobbie, M.A.,
Slaney, J.K. (eds.) CADE 1996. LNCS, vol. 1104, pp. 313–327. Springer, Heidelberg
(1996). https://doi.org/10.1007/3-540-61511-3 97

140
C. E. Brown et al.
22. Harrison, J., Urban, J., Wiedijk, F.: History of interactive theorem proving. In:
Siekmann, J.H. (ed.) Computational Logic, Handbook of the History of Logic,
vol. 9, pp. 135–214. Elsevier (2014). https://doi.org/10.1016/B978-0-444-51624-4.
50004-6
23. Hurd, J.: First-order proof tactics in higher-order logic theorem provers. Design
and Application of Strategies/Tactics in Higher Order Logics, number NASA/CP-
2003-212448 in NASA Technical reports, pp. 56–68 (2003)
24. Hurd, J.: System description: the metis proof tactic. In: Benzmueller, C., Harrison,
J., Schurmann, C. (ed.) Workshop on Empirically Successful Automated Reasoning
in Higher-Order Logic (ESHOL), pp. 103–104 (2005). https://arxiv.org/pdf/cs/
0601042
25. Hurd, J.: The opentheory standard theory library. In: Bobaru, M., Havelund,
K., Holzmann, G.J., Joshi, R. (eds.) NFM 2011. LNCS, vol. 6617, pp. 177–191.
Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-20398-5 14
26. Kaliszyk, C., Sutcliﬀe, G., Rabe, F.: TH1: the TPTP typed higher-order form with
rank-1 polymorphism. In: Fontaine, P., Schulz, S., Urban, J. (eds.) Proceedings of
the 5th Workshop on Practical Aspects of Automated Reasoning. CEUR Workshop
Proceedings, vol. 1635, pp. 41–55 (2016)
27. Kaliszyk, C., Urban, J.: Learning-assisted automated reasoning with Flyspeck. J.
Autom. Reason. 53(2), 173–213 (2014). https://doi.org/10.1007/s10817-014-9303-
3
28. King, D., Arthan, R., Winnersh, I.: Development of practical veriﬁcation tools.
ICL Syst. J. 11, 106–122 (1996)
29. Korovin, K.: iProver – an instantiation-based theorem prover for ﬁrst-order logic
(system description). In: Armando, A., Baumgartner, P., Dowek, G. (eds.) IJCAR
2008. LNCS (LNAI), vol. 5195, pp. 292–298. Springer, Heidelberg (2008). https://
doi.org/10.1007/978-3-540-71070-7 24
30. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
31. Kumar, R., Myreen, M.O., Norrish, M., Owens, S.: CakeML: a veriﬁed implementa-
tion of ML. In: Jagannathan, S., Sewell, P. (eds.) The 41st Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages, POPL 2014, San
Diego, CA, USA, 20–21 January 2014, pp. 179–192. ACM (2014). https://doi.org/
10.1145/2535838.2535841
32. Lindblad, F.: A focused sequent calculus for higher-order logic. In: Demri, S.,
Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS (LNAI), vol. 8562, pp. 61–
75. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08587-6 5
33. McCune, W.: Prover9 and Mace4 (2005–2010). http://www.cs.unm.edu/∼mccune/
prover9/
34. McMillan, K.L., Middeldorp, A., Voronkov, A. (eds.): LPAR 2013. LNCS, vol.
8312. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-45221-5
35. Meng, J., Paulson, L.C.: Translating higher-order clauses to ﬁrst-order clauses. J.
Autom. Reason. 40(1), 35–60 (2008)
36. Nipkow, T., Wenzel, M., Paulson, L.C. (eds.): Isabelle/HOL. LNCS, vol. 2283.
Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-45949-9
37. Pfenning, F., Elliot, C.: Higher-order abstract syntax. In: Proceedings of the ACM
SIGPLAN 1988 Conference on Programming Language Design and Implementa-
tion, PLDI 1988, pp. 199–208. ACM, New York (1988). https://doi.org/10.1145/
53990.54010

GRUNGE: A Grand Uniﬁed ATP Challenge
141
38. Pitts, A.: The HOL logic. In: Gordon and Melham [19]. http://www.cs.ox.ac.uk/
tom.melham/pub/Gordon-1993-ITH.html
39. R¨ummer, P.: A constraint sequent calculus for ﬁrst-order logic with linear integer
arithmetic. In: Cervesato, I., Veith, H., Voronkov, A. (eds.) LPAR 2008. LNCS
(LNAI), vol. 5330, pp. 274–289. Springer, Heidelberg (2008). https://doi.org/10.
1007/978-3-540-89439-1 20
40. R¨ummer, P.: E-matching with free variables. In: Bjørner, N., Voronkov, A. (eds.)
LPAR 2012. LNCS, vol. 7180, pp. 359–374. Springer, Heidelberg (2012). https://
doi.org/10.1007/978-3-642-28717-6 28
41. Schulz, S.: System description: E 1.8. In: McMillan et al. [34], pp. 735–743. https://
doi.org/10.1007/978-3-642-45221-5 49
42. Slind, K., Norrish, M.: A brief overview of HOL4. In: Mohamed, O.A., Mu˜noz, C.,
Tahar, S. (eds.) TPHOLs 2008. LNCS, vol. 5170, pp. 28–32. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-71067-7 6
43. Steen, A., Benzm¨uller, C.: The higher-order prover Leo-III. In: Galmiche, D.,
Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp.
108–116. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94205-6 8.
http://christoph-benzmueller.de/papers/C70.pdf
44. Steen, A., Wisniewski, M., Benzm¨uller, C.: Going polymorphic - TH1 reasoning for
Leo-III. In: Eiter, T., Sands, D., Sutcliﬀe, G., Voronkov, A. (eds.) IWIL@LPAR
2017 Workshop and LPAR-21 Short Presentations, Maun, Botswana, 7–12 May
2017, vol. 1. Kalpa Publications in Computing, EasyChair (2017). http://www.
easychair.org/publications/paper/346851
45. Sutcliﬀe, G.: The CADE ATP system competition - CASC. AI Mag. 37(2), 99–101
(2016)
46. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure. From CNF
to TH0, TPTP v6.4.0. J. Autom. Reason. 59(4), 483–502 (2017)
47. Sutcliﬀe, G., Schulz, S., Claessen, K., Baumgartner, P.: The TPTP typed ﬁrst-
order form with arithmetic. In: Bjørner, N., Voronkov, A. (eds.) LPAR 2012. LNCS,
vol. 7180, pp. 406–419. Springer, Heidelberg (2012). https://doi.org/10.1007/978-
3-642-28717-6 32
48. Sutcliﬀe, G.: The 9th IJCAR automated theorem proving system competition
- CASC-J9. AI Commun. 31(6), 495–507 (2018). https://doi.org/10.3233/AIC-
180773
49. Weber, T.: SMT solvers: new oracles for the HOL theorem prover. Int. J. Softw.
Tools Technol. Transfer 13(5), 419–429 (2011). https://doi.org/10.1007/s10009-
011-0188-8
50. Weidenbach, C., Dimova, D., Fietzke, A., Kumar, R., Suda, M., Wischnewski,
P.: SPASS version 3.5. In: Schmidt, R.A. (ed.) CADE 2009. LNCS (LNAI), vol.
5663, pp. 140–145. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-
642-02959-2 10
51. Xu, Y., Liu, J., Chen, S., Zhong, X., He, X.: Contradiction separation based
dynamic multi-clause synergized automated deduction. Inf. Sci. 462, 93–113 (2018)

Model Completeness, Covers
and Superposition
Diego Calvanese1, Silvio Ghilardi2, Alessandro Gianola1(B), Marco Montali1,
and Andrey Rivkin1
1 Faculty of Computer Science, Free University of Bozen-Bolzano, Bolzano, Italy
{calvanese,gianola,montali,rivkin}@inf.unibz.it
2 Dipartimento di Matematica, Universit`a degli Studi di Milano, Milan, Italy
silvio.ghilardi@unimi.it
Abstract. In ESOP 2008, Gulwani and Musuvathi introduced a notion
of cover and exploited it to handle inﬁnite-state model checking prob-
lems. Motivated by applications to the veriﬁcation of data-aware pro-
cesses, we show how covers are strictly related to model completions, a
well-known topic in model theory. We also investigate the computation
of covers within the Superposition Calculus, by adopting a constrained
version of the calculus, equipped with appropriate settings and reduction
strategies.
1
Introduction
Declarative approaches to inﬁnite state model checking [40] need to manip-
ulate logical formulae in order to represent sets of reachable states. To pre-
vent divergence, various abstraction strategies have been adopted, ranging from
interpolation-based [33] to sophisticated search via counterexample elimina-
tion [26]. Precise computations of the set of reachable states require some form
of quantiﬁer elimination and hence are subject to two problems, namely that
quantiﬁer elimination might not be available at all and that, when available, it
is computationally very expensive.
To cope with the ﬁrst problem, [25] introduced the notion of a cover and
proved that covers exist for equality with uninterpreted symbols (EUF) and its
combination with linear arithmetic; also, it was shown that covers can be used
instead of quantiﬁer elimination and yield a precise computation of reachable
states. Concerning the second problem, in [25] it was observed (as a side remark)
that computing the cover of a conjunction of literals becomes tractable when only
free unary function symbols occur in the signature. It can be shown (see [10])
that the same observation applies when also free relational symbols occur.
In [11,12] we propose a new formalism for representing read-only database
schemata towards the veriﬁcation of integrated models of processes and data
[9], in particular so-called artifact systems [7,15,31,43]; this formalism (brieﬂy
recalled in Sect. 4.1 below) uses precisely signatures comprising unary function
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 142–160, 2019.
https://doi.org/10.1007/978-3-030-29436-6_9

Model Completeness, Covers and Superposition
143
symbols and free n-ary relations. In [11,12] we apply model completeness tech-
niques for verifying transition systems based on read-only databases, in a frame-
work where such systems employ both individual and higher order variables.
In this paper we show (see Sect. 3 below) that covers are strictly related
to model completions and to uniform interpolation [39], thus building a bridge
between diﬀerent research areas. In particular, we prove that computing covers
for a theory is equivalent to eliminating quantiﬁers in its model completion.
Model completeness has other well-known applications in computer science. It
has been applied: (i) to reveal interesting connections between temporal logic
and monadic second order logic [22,23]; (ii) in automated reasoning to design
complete algorithms for constraint satisﬁability in combined theories over non
disjoint signatures [1,17,20,34–36] and theory extensions [41,42]; (iii) to obtain
combined interpolation for modal logics and software veriﬁcation theories [18,19].
In the last part of the paper (Sect. 5 below), we prove that covers for EUF can
be computed through a constrained version of the Superposition Calculus [38]
equipped with appropriate settings and reduction strategies; the related com-
pleteness proof requires a careful analysis of the constrained literals generated
during the saturation process. Not all proofs could be included here: for the
missing ones, we refer to the online available extended version [10] (the proofs
of our results from Sect. 5 are however reported in full detail).
2
Preliminaries
We adopt the usual ﬁrst-order syntactic notions of signature, term, atom,
(ground) formula, and so on; our signatures are multi-sorted and include equality
for every sort. Hence variables are sorted as well. For simplicity, some basic deﬁ-
nitions will be supplied for single-sorted languages only (the adaptation to multi-
sorted languages is straightforward). We compactly represent a tuple ⟨x1, . . . , xn⟩
of variables as x. The notation t(x), φ(x) means that the term t, the formula φ
has free variables included in the tuple x. We assume that a function arity can
be deduced from the context. Whenever we build terms and formulae, we always
assume that they are well-typed, i.e., that the sorts of variables, constants, and
function sources/targets match. A formula is said to be universal (resp., exis-
tential) if it has the form ∀x(φ(x)) (resp., ∃x(φ(x))), where φ is a quantiﬁer-free
formula. Formulae with no free variables are called sentences. From the semantic
side, we use the standard notion of Σ-structure M and of truth of a formula in
a Σ-structure under a free variables assignment. The support |M| of M is the
disjoint union of the interpretations of the sorts in Σ. The interpretation of a
(sort, function, predicate) symbol σ in M is denoted σM.
A Σ-theory T is a set of Σ-sentences; a model of T is a Σ-structure M where
all sentences in T are true. We use the standard notation T |= φ to say that φ
is true in all models of T for every assignment to the variables occurring free in
φ. We say that φ is T-satisﬁable iﬀthere is a model M of T and an assignment
to the variables occurring free in φ making φ true in M.
We now focus on the constraint satisﬁability problem and quantiﬁer elimina-
tion for a theory T. A Σ-formula φ is a Σ-constraint (or just a constraint) iﬀit is

144
D. Calvanese et al.
a conjunction of literals. The constraint satisﬁability problem for T is the follow-
ing: we are given a constraint (equivalently, a quantiﬁer-free formula) φ(x) and
we are asked whether there exist a model M of T and an assignment I to the free
variables x such that M, I |= φ(x). A theory T has quantiﬁer elimination iﬀfor
every formula φ(x) in the signature of T there is a quantiﬁer-free formula φ′(x)
such that T |= φ(x) ↔φ′(x). Since we are in a computational logic context,
when we speak of quantiﬁer elimination, we assume that it is eﬀective, namely
that it comes with an algorithm for computing φ′ out of φ. It is well-known that
quantiﬁer elimination holds in case we can eliminate quantiﬁers from primitive
formulae, i.e., formulae of the kind ∃y φ(x, y), with φ a constraint.
We recall also some basic notions from logic and model theory. Let Σ be a
ﬁrst-order signature. The signature obtained from Σ by adding to it a set a of
new constants (i.e., 0-ary function symbols) is denoted by Σa. Analogously, given
a Σ-structure M, the signature Σ can be expanded to a new signature Σ|M| :=
Σ ∪{¯a | a ∈|M|} by adding a set of new constants ¯a (the name for a), one for
each element a in M, with the convention that two distinct elements are denoted
by diﬀerent “name” constants. M can be expanded to a Σ|M|-structure M :=
(M, a)a∈|M| just interpreting the additional constants over the corresponding
elements. From now on, when the meaning is clear from the context, we will
freely use the notation M and M interchangeably: in particular, given a Σ-
structure M and a Σ-formula φ(x) with free variables that are all in x, we will
write, by abuse of notation, M |= φ(a) instead of M |= φ(¯a).
A Σ-homomorphism (or, simply, a homomorphism) between two Σ-structu-
res M and N is a map μ : |M| −→|N| among the support sets |M| of M and
|N| of N satisfying the condition (M |= ϕ
⇒
N |= ϕ) for all Σ|M|-atoms
ϕ (M is regarded as a Σ|M|-structure, by interpreting each additional constant
a ∈|M| into itself and N is regarded as a Σ|M|-structure by interpreting each
additional constant a ∈|M| into μ(a)). In case the last condition holds for all
Σ|M|-literals, the homomorphism μ is said to be an embedding and if it holds for
all ﬁrst order formulae, the embedding μ is said to be elementary. If μ : M −→N
is an embedding which is just the identity inclusion |M| ⊆|N|, we say that M
is a substructure of N or that N is an extension of M.
Let M be a Σ-structure. The diagram of M, written ΔΣ(M) (or just Δ(M)),
is the set of ground Σ|M|-literals that are true in M. An easy but important
result, called Robinson Diagram Lemma [13], says that, given any Σ-structure
N, the embeddings μ : M −→N are in bijective correspondence with expansions
of N to Σ|M|-structures which are models of ΔΣ(M). The expansions and the
embeddings are related in the obvious way: ¯a is interpreted as μ(a).
3
Covers, Uniform Interpolation and Model Completions
We report the notion of cover taken from [25]. Fix a theory T and an existential
formula ∃e φ(e, y); call a residue of ∃e φ(e, y) any quantiﬁer-free formula belong-
ing to the set of quantiﬁer-free formulae Res(∃e φ) = {θ(y, z) | T |= φ(e, y) →
θ(y, z)}. A quantiﬁer-free formula ψ(y) is said to be a T-cover (or, simply, a

Model Completeness, Covers and Superposition
145
cover) of ∃e φ(e, y) iﬀψ(y) ∈Res(∃e φ) and ψ(y) implies (modulo T) all the
other formulae in Res(∃e φ). The following Lemma (to be widely used through-
out the paper) supplies a semantic counterpart to the notion of a cover:
Lemma 1. A formula ψ(y) is a T-cover of ∃e φ(e, y) iﬀit satisﬁes the following
two conditions: (i) T |= ∀y (∃e φ(e, y) →ψ(y)); (ii) for every model M of T,
for every tuple of elements a from the support of M such that M |= ψ(a)
it is possible to ﬁnd another model N of T such that M embeds into N and
N |= ∃e φ(e, a).
◁
Proof. Suppose that ψ(y) satisﬁes conditions (i) and (ii) above. Condition (i)
says that ψ(y) ∈Res(∃e φ), so ψ is a residue. In order to show that ψ is also a
cover, we have to prove that T |= ∀y, z(ψ(y) →θ(y, z)), for every θ(y, z) that
is a residue for ∃e φ(e, y). Given a model M of T, take a pair of tuples a, b of
elements from |M| and suppose that M |= ψ(a). By condition (ii), there is
a model N of T such that M embeds into N and N |= ∃eφ(e, a). Using the
deﬁnition of Res(∃e φ), we have N |= θ(a, b), since θ(y, z) ∈Res(∃x φ). Since M
is a substructure of N and θ is quantiﬁer-free, M |= θ(a, b) as well, as required.
Suppose that ψ(y) is a cover. The deﬁnition of residue implies condition (i).
To show condition (ii) we have to prove that, given a model M of T, for every
tuple a of elements from |M|, if M |= ψ(a), then there exists a model N of T
such that M embeds into N and N |= ∃xφ(x, a). By reduction to absurdity,
suppose that this is not the case: this is equivalent (by using Robinson Diagram
Lemma) to the fact that Δ(M) ∪{φ(e, a)} is a T-inconsistent Σ|M|∪{e}-theory.
By compactness, there is a ﬁnite number of literals ℓ1(a, b), ..., ℓm(a, b) (for some
tuple b of elements from |M|) such that M |= ℓi (for all i = 1, . . . , m) and T |=
φ(e, a) →¬(ℓ1(a, b)∧· · ·∧ℓm(a, b)), which means that T |= φ(e, y) →(¬ℓ1(y, z)∨
· · · ∨¬ℓm(y, z)), i.e. that T |= ∃e φ(e, y) →(¬ℓ1(y, z) ∨· · · ∨¬ℓm(y, z)). By
deﬁnition of residue, clearly (¬ℓ1(y, z)∨· · ·∨¬ℓm(y, z)) ∈Res(∃x φ); then, since
ψ(y) is a cover, T |= ψ(y) →(¬ℓ1(y, z) ∨· · · ∨¬ℓm(y, z)), which implies that
M |= ¬ℓj(a, b) for some j = 1, . . . , m, which is a contradiction. Thus, ψ(y)
satisﬁes conditions (ii) too.
⊣
We say that a theory T has uniform quantiﬁer-free interpolation iﬀevery
existential formula ∃e φ(e, y) (equivalently, every primitive formula ∃e φ(e, y))
has a T-cover.
It is clear that if T has uniform quantiﬁer-free interpolation, then it has ordi-
nary quantiﬁer-free interpolation [8], in the sense that if we have T |= φ(e, y) →
φ′(y, z) (for quantiﬁer-free formulae φ, φ′), then there is a quantiﬁer-free for-
mula θ(y) such that T |= φ(e, y) →θ(y) and T |= θ(y) →φ′(y, z). In fact,
if T has uniform quantiﬁer-free interpolation, then the interpolant θ is inde-
pendent on φ′ (the same θ(y) can be used as interpolant for all entailments
T |= φ(e, y) →φ′(y, z), varying φ′).
We say that a universal theory T has a model completion iﬀthere is a stronger
theory T ∗⊇T (still within the same signature Σ of T) such that (i) every Σ-
constraint that is satisﬁable in a model of T is satisﬁable in a model of T ∗;
(ii) T ∗eliminates quantiﬁers. Other equivalent deﬁnitions are possible [13]: for

146
D. Calvanese et al.
instance, (i) is equivalent to the fact that T and T ∗prove the same quantiﬁer-
free formulae or again to the fact that every model of T can be embedded into
a model of T ∗. We recall that the model completion, if it exists, is unique and
that its existence implies the amalgamation property for T [13]. The relationship
between uniform interpolation in a propositional logic and model completion of
the equational theory of the variety algebraizing it was extensively studied in [24].
In the context of ﬁrst order theories, we prove an even more direct connection:
Theorem 1. Suppose that T is a universal theory. Then T has a model com-
pletion T ∗iﬀT has uniform quantiﬁer-free interpolation. If this happens, T ∗
is axiomatized by the inﬁnitely many sentences ∀y (ψ(y) →∃e φ(e, y)), where
∃e φ(e, y) is a primitive formula and ψ is a cover of it.
◁
The proof (via Lemma 1, by iterating a chain construction) is in [10].
4
Model-Checking Applications
In this section we supply old and new motivations for investigating covers and
model completions in view of model-checking applications. We ﬁrst report the
considerations from [11,12,25] on symbolic model-checking via model comple-
tions (or, equivalently, via covers) in the basic case where system variables are
represented as individual variables (for more advanced applications where sys-
tem variables are both individual and higher order variables, see [11,12]). Sim-
ilar ideas (‘use quantiﬁer elimination in the model completion even if T does
not allow quantiﬁer elimination’) were used in [41] for interpolation and symbol
elimination.
Deﬁnition 1. A (quantiﬁer-free) transition system is a tuple
S = ⟨Σ, T, x, ι(x), τ(x, x′)⟩
where: (i) Σ is a signature and T is a Σ-theory; (ii) x = x1, . . . , xn are individual
variables; (iii) ι(x) is a quantiﬁer-free formula; (iv) τ(x, x′) is a quantiﬁer-free
formula (here the x′ are renamed copies of the x).
◁
A safety formula for a transition system S is a further quantiﬁer-free formula
υ(x) describing undesired states of S. We say that S is safe with respect to υ if
the system has no ﬁnite run leading from ι to υ, i.e. (formally) if there are no
model M of T and no k ≥0 such that the formula
ι(x0) ∧τ(x0, x1) ∧· · · ∧τ(xk−1, xk) ∧υ(xk)
(1)
is satisﬁable in M (here xi’s are renamed copies of x). The safety problem for S
is the following: given υ, decide whether S is safe with respect to υ.

Model Completeness, Covers and Superposition
147
Suppose now that the theory T men-
tioned in Deﬁnition 1(i) is universal, has
decidable constraint satisﬁability problem
and admits a model completion T ∗. Algo-
rithm 1 describes the backward reachabil-
ity algorithm for handling the safety prob-
lem for S (the dual algorithm working
via forward search is described in equiva-
lent terms in [25]). An integral part of the
algorithm is to compute preimages. For
that purpose, for any φ1(x, x′) and φ2(x),
we deﬁne Pre(φ1, φ2) to be the formula
∃x′(φ1(x, x′) ∧φ2(x′)). The preimage of
the set of states described by a state for-
mula φ(x) is the set of states described by Pre(τ, φ). The subprocedure QE(T ∗, φ)
in Line 6 applies the quantiﬁer elimination algorithm of T ∗to the existential for-
mula φ. Algorithm 1 computes iterated preimages of υ and applies to them quan-
tiﬁer elimination, until a ﬁxpoint is reached or until a set intersecting the initial
states (i.e., satisfying ι) is found. Inclusion (Line 2) and disjointness (Line 3)
tests produce proof obligations that can be discharged thanks to the fact that T
has decidable constraint satisﬁability problem.
The proof of Proposition 1 consists just in the observation that, thanks to
quantiﬁer elimination in T ⋆, (1) is a quantiﬁer-free formula and that a quantiﬁer-
free formula is satisﬁable in a model of T iﬀso is it in a model of T ∗:
Proposition 1. Suppose that the universal Σ-theory T has decidable constraint
satisﬁability problem and admits a model completion T ∗. For every transition
system S = ⟨Σ, T, x, ι, τ⟩, the backward search algorithm is eﬀective and partially
correct for solving safety problems for S.1
◁
Despite its simplicity, Proposition 1 is a crucial fact. Notice that it implies
decidability of the safety problems in some interesting cases: this happens, for
instance, when in T there are only ﬁnitely many quantiﬁer-free formulae in which
x occur, as in case T has a purely relational signature or, more generally, T is
locally ﬁnite2. Since a theory is universal iﬀit is closed under substructures [13]
and since a universal locally ﬁnite theory has a model completion iﬀit has the
amalgamation property [44], it follows that Proposition 1 can be used to cover
the decidability result stated in Theorem 5 of [7] (once restricted to transition
systems over a ﬁrst-order deﬁnable class of Σ-structures).
1 Partial correctness means that, when the algorithm terminates, it gives a correct
answer. Eﬀectiveness means that all subprocedures in the algorithm can be eﬀec-
tively executed.
2 We say that T is locally ﬁnite iﬀfor every ﬁnite tuple of variables x there are only
ﬁnitely many non T-equivalent atoms A(x) involving only the variables x.

148
D. Calvanese et al.
4.1
Database Schemata
In this subsection, we provide a new application for the above explained model-
checking techniques [11,12]. The application relates to the veriﬁcation of inte-
grated models of business processes and data [9], referred to as artifact systems
[43], where the behavior of the process is inﬂuenced by data stored in a relational
database (DB) with constraints. The data contained therein are read-only: they
can be queried by the process and stored in a working memory, which in the
context of this paper is constituted by a set of system variables. In this con-
text, safety amounts to checking whether the system never reaches an undesired
property, irrespectively of what is contained in the read-only DB.
We deﬁne next the two key notions of (read-only) DB schema and instance,
by relying on an algebraic, functional characterization.
Deﬁnition 2. A DB schema is a pair ⟨Σ, T⟩, where: (i) Σ is a DB signature,
that is, a ﬁnite multi-sorted signature whose function symbols are all unary; (ii)
T is a DB theory, that is, a set of universal Σ-sentences.
◁
We now focus on extensional data conforming to a given DB schema.
Deﬁnition 3. A DB instance of DB schema ⟨Σ, T⟩is a Σ-structure M such
that M is a model of T.3
◁
One might be surprised by the fact that signatures in our DB schemata
contain unary function symbols, beside relational symbols. As shown in [11,12],
the algebraic, functional characterization of DB schema and instance can be
actually reinterpreted in the classical, relational model so as to reconstruct the
requirements posed in [31]. Deﬁnition 2 naturally corresponds to the deﬁnition
of relational database schema equipped with single-attribute primary keys and
foreign keys. To see this connection, we adopt the named perspective, where
each relation schema is deﬁned by a signature containing a relation name and
a set of typed attribute names. Let ⟨Σ, T⟩be a DB schema. Each sort S from
Σ corresponds to a dedicated relation RS with the following attributes: (i) one
identiﬁer attribute idS with type S; (ii) one dedicated attribute af with type S′
for every function symbol f from Σ of the form f : S −→S′.
The fact that RS is constructed starting from functions in Σ naturally
induces corresponding functional dependencies within RS, and inclusion depen-
dencies from RS to other relation schemas. In particular, for each non-id
attribute af of RS, we get a functional dependency from idS to af. Altogether,
such dependencies witness that idS is the primary key of RS. In addition, for
each non-id attribute af of RS whose corresponding function symbol f has id
sort S′ as image, we get an inclusion dependency from af to the id attribute
idS′ of RS′. This captures that af is a foreign key referencing RS′.
3 One may restrict to models interpreting sorts as ﬁnite sets, as customary in database
theory. Since the theories we are dealing with usually have ﬁnite model property for
constraint satisﬁability, assuming such restriction turns out to be irrelevant, as far
as safety problems are concerned (see [11,12] for an accurate discussion).

Model Completeness, Covers and Superposition
149
Given a DB instance M of ⟨Σ, T⟩, its corresponding relational instance
R[M] is the minimal set satisfying the following property: for every id
sort S
from Σ, let f1, . . . , fn
be all functions in Σ
with domain S;
then, for every identiﬁer o ∈SM, R[M] contains a labeled fact of the
form RS(idS : oM, af1 : f M
1 (o), . . . , afn : f M
n (o)). In addition, R[M] contains the
tuples from rM, for every relational symbol r from Σ (these relational symbols
represent plain relations, i.e. those not possessing a key).
We close our discussion by focusing on DB theories. Notice that EUF suﬃces
to handle the sophisticated setting of database-driven systems from [12] (e.g.,
key dependencies). The role of a non-empty DB theory is to encode background
axioms to express additional constraints. We illustrate a typical background
axiom, required to handle the possible presence of undeﬁned identiﬁers/values
in the diﬀerent sorts. This, in turn, is essential to capture artifact systems whose
working memory is initially undeﬁned, in the style of [16,31]. To accommodate
this, we add to every sort S of Σ a constant undefS (written by abuse of notation
just undef from now on), used to specify an undeﬁned value. Then, for each
function symbol f of Σ, we can impose additional constraints involving undef,
for example by adding the following axioms to the DB theory:
∀x (x = undef ↔f(x) = undef)
(2)
This axiom states that the application of f to the undeﬁned value produces an
undeﬁned value, and it is the only situation for which f is undeﬁned. A slightly
diﬀerent approach may handle many undeﬁned values for each sort; the reader is
referred to [11,12] for examples of concrete database instances formalized in our
framework. We just point out that in most cases the kind of axioms that we need
for our DB theories T are just one-variable universal axioms (like Axioms 2), so
that they ﬁt the hypotheses of Proposition 2 below.
We are interested in applying the algorithm of Proposition 1 to what we
call simple artifact systems, i.e. transition systems S = ⟨Σ, T, x, ι(x), τ(x, x′)⟩,
where ⟨Σ, T⟩is a DB schema in the sense of Deﬁnition 2. To this aim, it is
suﬃcient to identify a suitable class of DB theories having a model completion
and whose constraint satisﬁability problem is decidable. A ﬁrst result in this
sense is given below. We associate to a DB signature Σ the edge-labeled graph
G(Σ) whose nodes are the sorts in Σ, and such that G(Σ) contains a labeled
edge S
f−→S′ if and only if Σ contains a function symbol whose source sort is S
and whose target sort is S′. We say that Σ is acyclic if G(Σ) is so.
Proposition 2. A DB theory T has decidable constraint satisﬁability problem
and admits a model completion in case it is axiomatized by ﬁnitely many uni-
versal one-variable formulae and Σ is acyclic.
◁
The proof is given in [10]. Since acyclicity of Σ yields local ﬁniteness, we
immediately get as a Corollary the decidability of safety problems for transitions
systems based on DB schema satisfying the hypotheses of the above theorem.

150
D. Calvanese et al.
5
Covers via Constrained Superposition
Of course, a model completion may not exist at all; Proposition 2 shows that it
exists in case T is a DB theory axiomatized by universal one-variable formulae
and Σ is acyclic. The second hypothesis is unnecessarily restrictive and the
algorithm for quantiﬁer elimination suggested by the proof of Proposition 2 is
highly impractical: for this reason we are trying a diﬀerent approach. In this
section, we drop the acyclicity hypothesis and examine the case where the theory
T is empty and the signature Σ may contain function symbols of any arity.
Covers in this context were shown to exist already in [25], using an algorithm
that, very roughly speaking, determines all the conditional equations that can
be derived concerning the nodes of the congruence closure graph. An algorithm
for the generation of interpolants, still relying on congruence closure [28] and
similar to the one presented in [25], is supplied in [29].
We follow a diﬀerent plan and we want to produce covers (and show that
they exist) using saturation-based theorem proving. The natural idea to proceed
in this sense is to take the matrix φ(e, y) of the primitive formula ∃e φ(e, y)
we want to compute the cover of: this is a conjunction of literals, so we con-
sider each variable as a free constant, we saturate the corresponding set of
ground literals and ﬁnally we output the literals involving only the y. For
saturation, one can use any version of the superposition calculus [38]. This
procedure however for our problem is not suﬃcient. As a trivial counterex-
ample consider the primitive formula ∃e (R(e, y1) ∧¬R(e, y2)): the set of lit-
erals {R(e, y1), ¬R(e, y2)} is saturated (recall that we view e, y1, y2 as con-
stants), however the formula has a non-trivial cover y1 ̸= y2 which is not
produced by saturation. If we move to signatures with function symbols, the
situation is even worse: the set of literals {f(e, y1) = y′
1, f(e, y2) = y′
2} is sat-
urated but the formula ∃e (f(e, y1) = y′
1 ∧f(e, y2) = y′
2) has the conditional
equality y1 = y2 →y′
1 = y′
2 as cover. Disjunctions of disequations might
also arise: the cover of ∃e h(e, y1, y2) ̸= h(e, y′
1, y′
2) (as well as the cover of
∃e f(f(e, y1), y2) ̸= f(f(e, y′
1), y′
2), see Example 1 below) is y1 ̸= y′
1 ∨y2 ̸= y′
2. 4
Notice that our problem is diﬀerent from the problem of producing ordi-
nary quantiﬁer-free interpolants via saturation based theorem proving [30]:
for ordinary Craig interpolants, we have as input two quantiﬁer-free formulae
φ(e, y), φ′(y, z) such that φ(e, y) →φ′(y, z) is valid; here we have a single for-
mula φ(e, y) in input and we are asked to ﬁnd an interpolant which is good for
all possible φ′(y, z) such that φ(e, y) →φ′(y, z) is valid. Ordinary interpolants
can be extracted from a refutation of φ(e, y) ∧¬φ′(y, z), here we are not given
any refutation at all (and we are not even supposed to ﬁnd one).
What we are going to show is that, nevertheless, saturation via superposition
can be used to produce covers, if suitably adjusted. In this section we consider
signatures with n-ary function symbols (for all n ≥1). For simplicity, we omit
4 This example points out a problem that needs to be ﬁxed in the algorithm presented
in [25]: that algorithm in fact outputs only equalities, conditional equalities and single
disequalities, so it cannot correctly handle this example.

Model Completeness, Covers and Superposition
151
n-ary relation symbols (you can easily handle them by rewriting R(t1, . . . , tn) as
R(t1, . . . , tn) = true, as customary in the paramodulation literature [38]).
We are going to compute the cover of a primitive formula ∃e φ(e, y) to be ﬁxed
for the remainder of this section. We call variables e existential and variables
y parameters. By applying abstraction steps, we can assume that φ is primitive
ﬂat. i.e. that it is a conjunction of e-ﬂat literals, deﬁned below. [By an abstraction
step we mean replacing ∃e φ with ∃e ∃e′(e′ = u ∧φ′), where e′ is a fresh variable
and φ′ is obtained from φ by replacing some occurrences of a term u(e, y) by e′].
A term or a formula are said to be e-free iﬀthe existential variables do not
occur in it. An e-ﬂat term is an e-free term t(y) or a variable from e or again
it is of the kind f(u1, . . . , un), where f is a function symbol and u1, . . . , un are
e-free terms or variables from e. An e-ﬂat literal is a literal of the form
t = a,
a ̸= b
where t is an e-ﬂat term and a, b are either e-free terms or variables from e.
We assume the reader is familiar with standard conventions used in rewriting
and paramodulation literature: in particular s|p denotes the subterm of s in
position p and s[u]p denotes the term obtained from s by replacing s|p with u.
We use ≡to indicate coincidence of syntactic expressions (as strings) to avoid
confusion with equality symbol; when we write equalities like s = t below, we
may mean both s = t or t = s (an equality is seen as a multiset of two terms).
For information on reduction ordering, see for instance [2].
We ﬁrst replace variables e = e1, . . . , en and y = y1, . . . , ym by free constants -
we keep the names e1, . . . , en, y1, . . . , ym for these constants. Choose a reduction
ordering > total for ground terms such that e-ﬂat literals t = a are always
oriented from left to right in the following two cases: (i) t is not e-free and a is
e-free; (ii) t is not e-free, it is not equal to any of the e and a is a variable from e.
To obtain such properties, one may for instance choose a suitable Knuth-Bendix
ordering taking weights in some transﬁnite ordinal, see [32].
Given two e-ﬂat terms t, u, we indicate with E(t, u) the following procedure:
• E(t, u) fails if t is e-free and u is not e-free (or vice versa);
• E(t, u) fails if t ≡ei and (either t ≡f(t1, . . . , tk) or u ≡ej for i ̸= j);
• E(t, u) = ∅if t ≡u;
• E(t, u) = {t = u} if t and u are diﬀerent but both e-free;
• E(t, u) fails if none of t, u is e-free, t ≡f(t1, . . . , tk) and u ≡g(u1, . . . , ul) for
f ̸≡g;
• E(t, u) = E(t1, u1) ∪· · · ∪E(tk, uk) if none of t, u is e-free, t ≡f(t1, . . . , tk),
u ≡f(u1, . . . , uk) and none of the E(ti, ui) fails.
Notice that, whenever E(t, u) succeeds, the formula  E(t, u) →t = u is uni-
versally valid. The deﬁnition of E(t, u) is motivated by the next lemma.
Lemma 2. Let R be a convergent (i.e. terminating and conﬂuent) ground
rewriting system, whose rules consist of e-free terms. Suppose that t and u are
e-ﬂat terms with the same R-normal form. Then E(t, u) does not fail and all
pairs from E(t, u) have the same R-normal form as well.
◁

152
D. Calvanese et al.
Proof. This is due to the fact that if t is not e-free, no R-rewriting is possible at
root position because rules from R are e-free.
⊣
In the following, we handle constrained ground ﬂat literals of the form L ∥C
where L is a ground ﬂat literal and C is a conjunction of ground equalities among
e-free terms. The logical meaning of L ∥C is the Horn clause  C →L.
In the literature, various calculi with constrained clauses were considered,
starting e.g. from the non-ground constrained versions of the Superposition Cal-
culus of [4,37]. The calculus we propose here is inspired by such versions and
it has close similarities with a subcase of hierarchic superposition calculus [5],
or rather to its “weak abstraction” variant from [6] (we thank an anonymous
referee for pointing out this connection).
The rules of our Constrained Superposition Calculus follow; each rule applies
provided the E subprocedure called by it does not fail. The symbol ⊥indicates
the empty clause. Further explanations and restrictions to the calculus are given
in the Remarks below.
Superposition Right
(Constrained)
l = r ∥C
s = t ∥D
s[r]p = t ∥C ∪D ∪E(s|p, l)
if l > r and s > t
Superposition Left
(Constrained)
l = r ∥C
s ̸= t ∥D
s[r]p ̸= t ∥C ∪D ∪E(s|p, l)
if l > r and s > t
Reﬂexion
(Constrained)
t ̸= u ∥C
⊥∥C ∪E(t, u)
Demodulation
(Constrained)
L ∥C,
l = r ∥D
L[r]p ∥C
if l > r, L|p ≡l
and C ⊇D
Remark 1. The ﬁrst three rules are inference rules: they are non-deterministic-
ally selected for application, until no rule applies anymore. The selection strategy
for the rule to be applied is not relevant for the correctness and completeness of the
algorithm (some variant of a ‘given clause algorithm’ can be applied). An infer-
ence rule is not applied in case one premise is e-free (we have no reason to apply
inferences to e-free premises, since we are not looking for a refutation).
◁
Remark 2. The Demodulation rule is a simpliﬁcation rule: its application not
only adds the conclusion to the current set of constrained literals, but it also
removes the ﬁrst premise. It is easy to see (e.g., representing literals as multisets
of terms and extending the total reduction ordering to multisets), that one cannot
have an inﬁnite sequence of consecutive applications of Demodulation rules.
◁
Remark 3. The calculus takes {L∥∅
|
L is a ﬂat literal from the matrix of
φ} as the initial set of constrained literals. It terminates when a saturated set of
constrained literals is reached. We say that S is saturated iﬀevery constrained
literal that can be produced by an inference rule, after being exhaustively sim-
pliﬁed via Demodulation, is already in S (there are more sophisticated notions
of ‘saturation up to redundancy’ in the literature, but we do not need them).
When it reaches a saturated set S, the algorithm outputs the conjunction of the
clauses  C →L, varying L ∥C among the e-free constrained literals from S. ◁

Model Completeness, Covers and Superposition
153
We need some rule application policy to ensure termination: without any such
policy, a set like {e = y ∥∅, f(e) = e∥∅} may produce by Right Superposition the
inﬁnitely many literals (all oriented from right to left) f(y) = e ∥∅, f(f(y)) =
e ∥∅, f(f(f(y))) = e ∥∅, etc. The next Remark explains the policy we follow.
Remark 4. First, we apply Demodulation only in case the second premise is
of the kind ej = t(y) ∥D, where t is e-free. Demodulation rule is applied with
higher priority with respect to the inference rules. Inside all possible applications
of Demodulation rule, we give priority to the applications where both premises
have the form ej = t(y) ∥D (for the same ej but with possibly diﬀerent D’s -
the D from the second premise being included in the D of the ﬁrst). In case
we have two constrained literals of the kind ej = t1(y) ∥D, ej = t2(y) ∥D inside
our current set of constrained literals (notice that the ej’s and the D’s here are
the same), among the two possible applications of the Demodulation rule, we
apply the rule that keeps the smallest ti. Notice that in this way two diﬀerent
constrained literals cannot simplify each other.
◁
We say that a constrained literal L ∥C belonging to a set of constrained
literals S is simpliﬁable in S iﬀit is possible to apply (according to the above
policy) a Demodulation rule removing it. A ﬁrst eﬀect of our policy is:
Lemma 3. If a constrained literal L ∥C is simpliﬁable in S, then after applying
to S any sequence of rules, it remains simpliﬁable until it gets removed. After
being removed, if it is regenerated, it is still simpliﬁable and so it is eventually
removed again.
◁
Proof. Suppose that L ∥C can be simpliﬁed by e = t ∥D and suppose that a rule
is applied to the current set of constrained literals. Since there are simpliﬁable
constrained literals, that rule cannot be an inference rule by the priority stated
in Remark 4. For simpliﬁcation rules, keep in mind again Remark 4. If L ∥C
is simpliﬁed, it is removed; if none of L ∥C and e = t ∥D get simpliﬁed, the
situation does not change; if e = t ∥D gets simpliﬁed, this can be done by some
e = t′∥D′, but then L ∥C is still simpliﬁable - although in a diﬀerent way - using
e = t′∥D′ (we have that D′ is included in D, which is in turn included in C).
Similar observations apply if L ∥C is removed and re-generated.
⊣
Due to the above Lemma, if we show that a derivation (i.e. a sequence of
rule applications) can produce terms only from a ﬁnite set, it is clear that when
no new constrained literal is produced, saturation is reached. First notice that
Lemma 4. Every constrained literal L ∥C produced during the run of the algo-
rithm is e-ﬂat.
◁
Proof. The constrained literals from initialization are e-ﬂat. The Demodulation
rule, applied according to Remark 4, produces an e-ﬂat literal out of an e-ﬂat
literal. The same happens for the Superposition rules: in fact, since both the
terms s and l from these rules are e-ﬂat, a Superposition may take place at root
position or may rewrite some l ≡ej with r ≡ei or with r ≡t(y).
⊣

154
D. Calvanese et al.
There are in principle inﬁnitely many e-ﬂat terms that can be generated
out of the e-ﬂat terms occurring in φ (see the above counterexample). We show
however that only ﬁnitely many e-ﬂat terms can in fact occur during saturation
and that one can determine in advance the ﬁnite set they are taken from.
To formalize this idea, let us introduce a hierarchy of e-ﬂat terms. Let D0 be
the e-ﬂat terms occurring in φ and let Dk+1 be the set of e-ﬂat terms obtained
by simultaneous rewriting of an e-ﬂat term from 
i≤k Di via rewriting rules of
the kind ej →tj(y) where the tj are e-ﬂat e-free terms from 
i≤k Di. The degree
of an e-ﬂat term is the minimum k such that it belongs to set Dk (it is necessary
to take the minimum because the same term can be obtained in diﬀerent stages
and via diﬀerent rewritings).5
Lemma 5. Let the e-ﬂat term t′ be obtained by a rewriting ej →u(y) from the
e-ﬂat term t; then, if t has degree k > 1 and u has degree at most k −1, we have
that t′ has degree at most k.
◁
Proof. This is clear, because at the k-stage one can directly produce t′ instead
of just t: in fact, all rewriting producing directly t′ replace an occurrence of some
ei by an e-free term, so they are all done in parallel positions.
⊣
Proposition 3. The saturation of the initial set of e-ﬂat constrained literals
always terminates after ﬁnitely many steps.
◁
Proof. We show that all e-ﬂat terms that may occur during saturation have at
most degree n (where n is the cardinality of e). This shows that the saturation
must terminate, because only ﬁnitely many terms may occur in a derivation (see
the above observations). Let the algorithm during saturation reach the status S;
we say that a constraint C allows the explicit deﬁnition of ej in S iﬀS contains
a constrained literal of the kind ej = t(y) ∥D with D ⊆C. Now we show by
mutual induction two facts concerning a constrained literal L ∥C ∈S:
(1) if an e-ﬂat term u of degree k occurs in L, then C allows the explicit deﬁnition
of k diﬀerent ej in S;
(2) if L is of the kind ei = t(y), for an e-ﬂat e-free term t of degree k, then either
ei = t ∥C can be simpliﬁed in S or C allows the explicit deﬁnition of k + 1
diﬀerent ej in S (ei itself is of course included among these ej).
Notice that (1) is suﬃcient to exclude that any e-ﬂat term of degree bigger than
n can occur in a constrained literal arising during the saturation process.
We prove (1) and (2) by induction on the length of the derivation leading
to L ∥C ∈S. Notice that it is suﬃcient to check that (1) and (2) hold for the
ﬁrst time where L ∥C ∈S because if C allows the explicit deﬁnition of a certain
variable in S, it will continue to do so in any S′ obtained from S by continuing
the derivation (the deﬁnition may be changed by the Demodulation rule, but the
fact that ei is explicitly deﬁned is forever). Also, by Lemma 3, a literal cannot
become non simpliﬁable if it is simpliﬁable.
5 Notice that, in the above deﬁnition of degree, constraints (attached to the rewriting
rules occurring in our calculus) are ignored.

Model Completeness, Covers and Superposition
155
(1) and (2) are evident if S is the initial status. To show (1), suppose that
u occurs for the ﬁrst time in L ∥C as the eﬀect of the application of a certain
rule: we can freely assume that u does not occur in the literals from the pre-
misses of the rule (otherwise induction trivially applies) and that u of degree k is
obtained by rewriting in a non-root position some u′ occurring in a constrained
literal L′ ∥D′ via some ej →t ∥D. This might be the eﬀect of a Demodulation or
Superposition in a non-root position (Superpositions in root position do not pro-
duce new terms). If u′ has degree k, then by induction D′ contains the required k
explicit deﬁnitions, and we are done because D′ is included in C. If u′ has lower
degree, then t must have degree at least k −1 (otherwise u does not reach degree
k by Lemma 5). Then by induction on (2), the constraint D (also included in C)
has (k −1) + 1 = k explicit deﬁnitions (when a constraint ej →t ∥D is selected
for Superposition or for making Demodulations in a non-root position, it is itself
not simpliﬁable according to the procedure explained in Remark 4).
To show (2), we analyze the reasons why the non simpliﬁable constrained
literal ei = t(y) ∥C is produced (let k be the degree of t). Suppose it is produced
from ei = u′ ∥C via Demodulation with ej = u(y) ∥D (with D ⊆C) in a non-
root position; if u′ has degree at least k, we apply induction for (1) to ei = u′ ∥C:
by such induction hypotheses, we get k explicit deﬁnitions in C and we can add
to them the further explicit deﬁnition ei = t(y) (the explicit deﬁnitions from C
cannot concern ei because ei = t(y) ∥C is not simpliﬁable). Otherwise, u′ has
degree less than k and u has degree at least k −1 by Lemma 5 (recall that t has
degree k): by induction, ej = u ∥D is not simpliﬁable (it is used as the active
part of a Demodulation in a non-root position, see Remark 4) and supplies k
explicit deﬁnitions, inherited by C ⊇D. Note that ei cannot have a deﬁnition
in D, otherwise ei = t(y) ∥C would be simpliﬁable, so with ei = t(y) ∥C we get
the required k + 1 deﬁnitions.
The remaining case is when ei = t(y) ∥C is produced via Superposition Right.
Such a Superposition might be at root or at a non-root position. We ﬁrst analyse
the case of a root position. This might be via ej = ei ∥C1 and ej = t(y) ∥C2
(with ej > ei and C = C1 ∪C2 because E(ej, ej) = ∅), but in such a case one
can easily apply induction. Otherwise, we have a diﬀerent kind of Superposition
at root position: ei = t(y) ∥C is obtained from s = ei ∥C1 and s′ = t(y) ∥C2,
with C = C1 ∪C2 ∪E(s, s′). In this case, by induction for (1), C2 supplies k
explicit deﬁnitions, to be inherited by C. Among such deﬁnitions, there cannot
be an explicit deﬁnition of ei otherwise ei = t(y) ∥C would be simpliﬁable, so
again we get the required k + 1 deﬁnitions.
In case of a Superposition at a non root-position, we have that ei = t(y) ∥C
is obtained from u′ = ei ∥C1 and ej = u(y) ∥C2, with C = C1 ∪C2; here t is
obtained from u′ by rewriting ej to u. This case is handled similarly to the case
where ei = t(y) ∥C is obtained via Demodulation rule.
⊣
Having established termination, we now prove that our calculus computes
covers; to this aim, we rely on refutational completeness of unconstrained Super-
position Calculus (thus, our technique resembles the technique used [5,6] in order
to prove refutational completeness of hierarchic superposition, although it is not

156
D. Calvanese et al.
clear whether Theorem 2 below can be derived from the results concerning hier-
archic superposition - we are not just proving refutational completeness and we
need to build proper superstructures):
Theorem 2. Suppose that the above algorithm, taking as input the primitive
e-ﬂat formula ∃e φ(e, y), gives as output the quantiﬁer-free formula ψ(y). Then
the latter is a cover of ∃e φ(e, y).
◁
Proof. Let S be the saturated set of constrained literals produced upon termina-
tion of the algorithm; let S = S1 ∪S2, where S1 contains the constrained literals
in which the e do not occur and S2 is its complement. Clearly ∃e φ(e, y) turns
out to be logically equivalent to

L ∥C∈S1
(

C →L) ∧∃e

L ∥C∈S2
(

C →L)
so, as a consequence, in view of Lemma 1 it is suﬃcient to show that every model
M satisfying 
L ∥C∈S1( C →L) via an assignment I to the variables y can
be embedded into a model M′ such that for a suitable extension I′ of I to the
variables e we have that (M′, I′) satisﬁes also 
L ∥C∈S2( C →L).
Fix M, I as above. The diagram Δ(M) of M is obtained as follows. We take
one free constant for each element of the support of M (by L¨owenheim-Skolem
theorem you can keep M at most countable, if you like) and we put in Δ(M)
all the literals of the kind f(c1, . . . , ck) = ck+1 and c1 ̸= c2 which are true in M
(here the ci are names for the elements of the support of M). Let R be the set of
ground equalities of the form yi = ci, where ci is the name of I(yi). Extend our
reduction ordering in the natural way (so that yi = ci and f(c1, . . . , ck) = ck+1
are oriented from left to right). Consider now the set of clauses
Δ(M) ∪R ∪{

C →L | (L ∥C) ∈S}
(3)
(below, we distinguish the positive and the negative literals of Δ(M) so that
Δ(M) = Δ+(M) ∪Δ−(M)). We want to saturate the above set in the stan-
dard Superposition Calculus. Clearly the rewriting rules in R, used as reduction
rules, replace everywhere yi by ci inside the clauses of the kind  C →L. At
this point, the negative literals from the equality constraints all disappear: if
they are true in M, they Δ+(M)-normalize to trivial equalities ci = ci (to be
eliminated by standard reduction rules) and if they are false in M they become
part of clauses subsumed by true inequalities from Δ−(M). Similarly all the
e-free literals not coming from Δ(M) ∪R get removed. Let ˜S be the set of sur-
vived literals involving the e (they are not constrained anymore and they are
Δ+(M)∪R-normalized): we show that they cannot produce new clauses. Let in
fact (π) be an inference from the Superposition Calculus [38] applying to them.
Since no superposition with Δ(M) ∪R is possible, this inference must involve
only literals from ˜S; suppose it produces a literal ˜L from the literals ˜L1, ˜L2 (com-
ing via Δ+(M) ∪R-normalization from L1 ∥C1 ∈S and L2 ∥C2 ∈S) as parent
clauses. Then, by Lemma 2, our constrained inferences produce a constrained

Model Completeness, Covers and Superposition
157
literal L ∥C such that the clause  C →L normalizes to ˜L via Δ+(M) ∪R.
Since S is saturated, the constrained literal L ∥C, after simpliﬁcation, belongs
to S. Now simpliﬁcations via our Constrained Demodulation and Δ(M)+ ∪R-
normalization commute (they work at parallel positions, see Remark 4), so the
inference (π) is redundant because ˜L simpliﬁes to a literal already in ˜S ∪Δ(M).
Thus the set of clauses (3) saturates without producing the empty clause. By
the completeness theorem of the Superposition Calculus [3,27,38] it has a model
M′. This M′ by construction ﬁts our requests by Robinson Diagram Lemma. ⊣
Theorem 2 also proves the existence of the model completion of EUF.
Example 1. We compute the cover of the primitive formula ∃e f(f(e, y1), y2) ̸=
f(f(e, y′
1), y′
2) (one more example, taken from [25], is analyzed in [10]). Flat-
tening gives the set of literals { f(e, y1) = e1, f(e1, y2) = e′
1, f(e, y′
1) =
e2, f(e2, y′
2) = e′
2, e′
1 ̸= e′
2 }. Superposition Right produces the constrained
literal e1 = e2 ∥{y1 = y′
1}; supposing that we have e1 > e2, Superposition Right
gives ﬁrst f(e2, y2) = e′
1 ∥{y1 = y′
1} and then also e′
1 = e′
2 ∥{y1 = y′
1, y2 = y′
2}.
Superposition Left and Reﬂexion now produce ⊥∥{y1 = y′
1, y2 = y′
2}. Thus the
clause y1 = y′
1 ∧y2 = y′
2 →⊥will be part of the output (actually, this will be
the only clause in the output).
◁
In the special case where the signature Σ contains only unary function sym-
bols, only empty constraints can be generated; in case Σ contains also relation
symbols of arity n > 1, the only constrained clauses that can be generated have
the form ⊥∥{t1 = t′
1, . . . , tn−1 = t′
n−1}. Also, it is not diﬃcult to see that in a
derivation at most one explicit deﬁnition ei = t(y) || ∅can occur for every ei: as
soon as this deﬁnition is produced, all occurrences of ei are rewritten to t. This
shows that Constrained Superposition computes covers in polynomial time for
the empty theory, whenever the signature Σ matches the restrictions of Deﬁni-
tion 2 for DB schemata. More details on complexity are given in [10] (where a
quadratic bound is obtained).
6
Conclusions and Future Work
As evident from Subsect. 4.1, our main motivation for investigating covers orig-
inated from the veriﬁcation of data-aware processes. Such applications require
database (DB) signatures to contain only unary function symbols (besides rela-
tions of every arity). We observed that computing covers of primitive formulae
in such signatures requires only polynomial time. In addition, if relation sym-
bols are at most binary, the cover of a primitive formula is a conjunction of
literals: this is crucial in applications, because model checkers like mcmt [21]
and cubicle [14] represent sets of reachable states as primitive formulae. This
makes cover computations a quite attractive technique in database-driven model
checking.
Our cover algorithm for DB signatures has been implemented in the model
checker mcmt. A ﬁrst experimental evaluation (based on the existing benchmark

158
D. Calvanese et al.
provided in [31], which samples 32 real-world BPMN workﬂows taken from the
BPMN oﬃcial website http://www.bpmn.org/) is described in [11]. The bench-
mark set is available as part of the last distribution 2.8 of mcmt http://users.
mat.unimi.it/users/ghilardi/mcmt/ (see the subdirectory /examples/dbdriven
of the distribution). The user manual, also included in the distribution, contains
a dedicated section giving essential information on how to encode relational arti-
fact systems (comprising both ﬁrst order and second order variables) in mcmt
speciﬁcations and how to produce user-deﬁned examples in the database driven
framework. Although an extensive experimentation is outside the focus of this
paper, we mention that the ﬁrst experiments were very encouraging: the tool was
able to solve in few seconds all the proposed benchmarks and the cover compu-
tations generated automatically during model-checking search were discharged
instantaneously.
This experimental setup motivates new research to extend Proposition 2 to
further theories axiomatizing integrity constraints used in DB applications. Com-
bined cover algorithms (along the perspectives in [25]) could be crucial also in
this setting. Practical algorithms for the computation of covers in the theories
falling under the hypotheses of Proposition 2 need to be designed: as a little ﬁrst
example, in [10] we show how to handle Axiom (2) by light modiﬁcations to our
techniques. Symbol elimination of function and predicate variables should also
be combined with cover computations.
Acknowledgements. This research has been partially supported by the UNIBZ
CRC projects REKAP: Reasoning and Enactment for Knowledge-Aware Processes and
PWORM: Planning for Workﬂow Management.
References
1. Baader, F., Ghilardi, S., Tinelli, C.: A new combination procedure for the word
problem that generalizes fusion decidability results in modal logics. Inf. Comput.
204(10), 1413–1452 (2006)
2. Baader, F., Nipkow, T.: Term Rewriting and All That. Cambridge University Press,
Cambridge (1998)
3. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with selec-
tion and simpliﬁcation. J. Log. Comput. 4(3), 217–247 (1994)
4. Bachmair, L., Ganzinger, H., Lynch, C., Snyder, W.: Basic paramodulation. Inf.
Comput. 121(2), 172–192 (1995)
5. Bachmair, L., Ganzinger, H., Waldmann, U.: Refutational theorem proving for
hierarchic ﬁrst-order theories. Appl. Algebra Eng. Commun. Comput. 5, 193–212
(1994)
6. Baumgartner, P., Waldmann, U.: Hierarchic superposition with weak abstraction.
In: Bonacina, M.P. (ed.) CADE 2013. LNCS (LNAI), vol. 7898, pp. 39–57. Springer,
Heidelberg (2013). https://doi.org/10.1007/978-3-642-38574-2 3
7. Boja´nczyk, M., Segouﬁn, L., Toru´nczyk, S.: Veriﬁcation of database-driven systems
via amalgamation. In: Proceedings of PODS, pp. 63–74 (2013)
8. Bruttomesso, R., Ghilardi, S., Ranise, S.: Quantiﬁer-free interpolation in combina-
tions of equality interpolating theories. ACM Trans. Comput. Log. 15(1), 5:1–5:34
(2014)

Model Completeness, Covers and Superposition
159
9. Calvanese, D., De Giacomo, G., Montali, M.: Foundations of data aware process
analysis: a database theory perspective. In: Proceedings of PODS (2013)
10. Calvanese, D., Ghilardi, S., Gianola, A., Montali, M., Rivkin, A.: Quantiﬁer elim-
ination for database driven veriﬁcation. CoRR, abs/1806.09686 (2018)
11. Calvanese, D., Ghilardi, S., Gianola, A., Montali, M., Rivkin, A.: Veriﬁcation of
data-aware processes via array-based systems (extended version). Technical report
arXiv:1806.11459, arXiv.org (2018)
12. Calvanese, D., Ghilardi, S., Gianola, A., Montali, M., Rivkin, A.: From model
completeness to veriﬁcation of data aware processes. In: Lutz, C., Sattler, U.,
Tinelli, C., Turhan, A.Y., Wolter, F. (eds.) Description Logic, Theory Combination,
and All That. LNCS, vol. 11560, pp. 212–239. Springer, Cham (2019). https://doi.
org/10.1007/978-3-030-22102-7 10
13. Chang, C.-C., Keisler, J.H.: Model Theory, 3rd edn. North-Holland Publishing Co.,
Amsterdam (1990)
14. Conchon, S., Goel, A., Krsti´c, S., Mebsout, A., Za¨ıdi, F.: Cubicle: a parallel SMT-
based model checker for parameterized systems. In: Madhusudan, P., Seshia, S.A.
(eds.) CAV 2012. LNCS, vol. 7358, pp. 718–724. Springer, Heidelberg (2012).
https://doi.org/10.1007/978-3-642-31424-7 55
15. Deutsch, A., Hull, R., Patrizi, F., Vianu, V.: Automatic veriﬁcation of data-centric
business processes. In: Proceedings of ICDT, pp. 252–267 (2009)
16. Deutsch, A., Li, Y., Vianu, V.: Veriﬁcation of hierarchical artifact systems. In:
Proceedings of PODS, pp. 179–194. ACM Press (2016)
17. Ghilardi, S.: Model theoretic methods in combined constraint satisﬁability. J.
Autom. Reason. 33(3–4), 221–249 (2004)
18. Ghilardi, S., Gianola, A.: Interpolation, amalgamation and combination (the non-
disjoint signatures case). In: Dixon, C., Finger, M. (eds.) FroCoS 2017. LNCS
(LNAI), vol. 10483, pp. 316–332. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-66167-4 18
19. Ghilardi, S., Gianola, A.: Modularity results for interpolation, amalgamation and
superamalgamation. Ann. Pure Appl. Log. 169(8), 731–754 (2018)
20. Ghilardi, S., Nicolini, E., Zucchelli, D.: A comprehensive combination framework.
ACM Trans. Comput. Log. 9(2), 54 p. (2008). Article no. 8
21. Ghilardi, S., Ranise, S.: MCMT: a model checker modulo theories. In: Giesl, J.,
H¨ahnle, R. (eds.) IJCAR 2010. LNCS (LNAI), vol. 6173, pp. 22–29. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-14203-1 3
22. Ghilardi, S., van Gool, S.J.: Monadic second order logic as the model companion
of temporal logic. In: Proceedings of LICS, pp. 417–426 (2016)
23. Ghilardi, S., van Gool, S.J.: A model-theoretic characterization of monadic second
order logic on inﬁnite words. J. Symb. Log. 82(1), 62–76 (2017)
24. Ghilardi, S., Zawadowski, M.: Sheaves, Games, and Model Completions: A Cat-
egorical Approach to Nonclassical Propositional Logics. Trends in Logic-Studia
Logica Library, vol. 14. Kluwer Academic Publishers, Dordrecht (2002)
25. Gulwani, S., Musuvathi, M.: Cover algorithms and their combination. In:
Drossopoulou, S. (ed.) ESOP 2008. LNCS, vol. 4960, pp. 193–207. Springer, Hei-
delberg (2008). https://doi.org/10.1007/978-3-540-78739-6 16
26. Hoder, K., Bjørner, N.: Generalized property directed reachability. In: Cimatti, A.,
Sebastiani, R. (eds.) SAT 2012. LNCS, vol. 7317, pp. 157–171. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-31612-8 13
27. Hsiang, J., Rusinowitch, M.: Proving refutational completeness of theorem-proving
strategies: the transﬁnite semantic tree method. J. ACM 38(3), 559–587 (1991)

160
D. Calvanese et al.
28. Kapur, D.: Shostak’s congruence closure as completion. In: Comon, H. (ed.) RTA
1997. LNCS, vol. 1232, pp. 23–37. Springer, Heidelberg (1997). https://doi.org/10.
1007/3-540-62950-5 59
29. Kapur, D.: Nonlinear polynomials, interpolants and invariant generation for sys-
tem analysis. In: Proceedings of the 2nd International Workshop on Satisﬁability
Checking and Symbolic Computation Co-Located with ISSAC (2017)
30. Kov´acs, L., Voronkov, A.: Interpolation and symbol elimination. In: Schmidt, R.A.
(ed.) CADE 2009. LNCS (LNAI), vol. 5663, pp. 199–213. Springer, Heidelberg
(2009). https://doi.org/10.1007/978-3-642-02959-2 17
31. Li, Y., Deutsch, A., Vianu, V.: VERIFAS: a practical veriﬁer for artifact systems.
PVLDB 11(3), 283–296 (2017)
32. Ludwig, M., Waldmann, U.: An extension of the knuth-bendix ordering with LPO-
like properties. In: Dershowitz, N., Voronkov, A. (eds.) LPAR 2007. LNCS (LNAI),
vol. 4790, pp. 348–362. Springer, Heidelberg (2007). https://doi.org/10.1007/978-
3-540-75560-9 26
33. McMillan, K.L.: Lazy abstraction with interpolants. In: Ball, T., Jones, R.B. (eds.)
CAV 2006. LNCS, vol. 4144, pp. 123–136. Springer, Heidelberg (2006). https://doi.
org/10.1007/11817963 14
34. Nicolini, E., Ringeissen, C., Rusinowitch, M.: Data structures with arithmetic con-
straints: a non-disjoint combination. In: Ghilardi, S., Sebastiani, R. (eds.) FroCoS
2009. LNCS (LNAI), vol. 5749, pp. 319–334. Springer, Heidelberg (2009). https://
doi.org/10.1007/978-3-642-04222-5 20
35. Nicolini, E., Ringeissen, C., Rusinowitch, M.: Satisﬁability procedures for combi-
nation of theories sharing integer oﬀsets. In: Kowalewski, S., Philippou, A. (eds.)
TACAS 2009. LNCS, vol. 5505, pp. 428–442. Springer, Heidelberg (2009). https://
doi.org/10.1007/978-3-642-00768-2 35
36. Nicolini, E., Ringeissen, C., Rusinowitch, M.: Combining satisﬁability procedures
for unions of theories with a shared counting operator. Fundam. Inform. 105(1–2),
163–187 (2010)
37. Nieuwenhuis, R., Rubio, A.: Theorem proving with ordering and equality con-
strained clauses. J. Symb. Comput. 19(4), 321–351 (1995)
38. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving. In: Handbook
of Automated Reasoning, vol. 2, pp. 371–443. MIT Press (2001)
39. Pitts, A.M.: On an interpretation of second order quantiﬁcation in ﬁrst order intu-
itionistic propositional logic. J. Symb. Log. 57(1), 33–52 (1992)
40. Rybina, T., Voronkov, A.: A logical reconstruction of reachability. In: Broy, M.,
Zamulin, A.V. (eds.) PSI 2003. LNCS, vol. 2890, pp. 222–237. Springer, Heidelberg
(2004). https://doi.org/10.1007/978-3-540-39866-0 24
41. Sofronie-Stokkermans, V.: On interpolation and symbol elimination in theory
extensions. In: Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol.
9706, pp. 273–289. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
40229-1 19
42. Sofronie-Stokkermans, V.: On interpolation and symbol elimination in theory
extensions. Log. Methods Comput. Sci. 14(3), 1–41 (2018)
43. Vianu, V.: Automatic veriﬁcation of database-driven systems: a new frontier. In:
Proceedings of ICDT, pp. 1–13 (2009)
44. Wheeler, W.H.: Model-companions and deﬁnability in existentially complete struc-
tures. Isr. J. Math. 25(3–4), 305–330 (1976)

A Tableaux Calculus for Default
Intuitionistic Logic
Valentin Cassano1, Raul Fervari1, Guillaume Hoﬀmann1, Carlos Areces1(B),
and Pablo F. Castro2
1 CONICET and Universidad Nacional de C´ordoba, C´ordoba, Argentina
carlos.areces@gmail.com
2 CONICET and Universidad Nacional de R´ıo Cuarto, R´ıo Cuarto, Argentina
Abstract. We build a Default Logic variant on Intuitionistic Proposi-
tional Logic and develop a sound, complete, and terminating, tableaux
calculus for it. We also present an implementation of the calculus. We
motivate and illustrate the technical elements of our work with examples.
1
Introduction
Non-monotonic formalisms have traditionally been deﬁned with a classical
semantics [20]. More recently –following the seminal work of Gabbay [19]– there
is an interest in the interplay between non-monotonic formalisms and Intuition-
istic Logic (IL). E.g., in [38] a formalization of a notion of non-monotonic impli-
cation capturing non-monotonic consequence based on IL is proposed; in [39]
the work of Gabbay in [19] is revised; in [30] a characterization of answer sets
for logic programs with nested expressions is oﬀered in terms of provability in IL
(generalizing earlier proposals of Pearce in [32,33]). The articles just mentioned
have in common a study of the interplay between non-monotonic formalisms
and IL from a theoretical perspective. Another interesting take on this interplay
can be found in the area of Normative Systems or Legal Artiﬁcial Intelligence.
E.g., in [22] a Description Logic built on IL is presented as a way to deal with
conﬂicts present in laws and normative systems. These conﬂicts usually lead to
logical inconsistencies when they are formally analyzed, bringing to the fore the
need for an adequate semantics for negation in such a context. Another example
is [31], where a construction of an I/O Logic –a general framework to study and
reason about conditional norms [28]– is carried out on IL.
The interplay between non-monotonic formalisms and IL in normative sys-
tems is succinctly illustrated by the following motivating example (adapted
from [26]). Let the possible outcomes of a trial be the verdicts of guilty or not
guilty. A verdict of guilty is obtained when the evidence presented by the pros-
ecution meets the so-called “beyond reasonable doubt” standard of proof. A
verdict of not guilty is obtained when the evidence fails to meet said standard
of proof; say because the defense manages to pinpoint contradictions in what
the prosecution has presented. In such a context, the proposition guilty or not
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 161–177, 2019.
https://doi.org/10.1007/978-3-030-29436-6_10

162
V. Cassano et al.
guilty is not understood as plainly true. Associated to it there is a proof of guilti-
ness; or a proof that this leads to contradictions. This intuitive understanding
of guilty or not guilty departs from its classical interpretation and better ﬁts in
an intuitionistic setting.
Furthermore, as stated in [26], a proposition such as: a verdict of guilty implies
not innocent is intuitively correct. The reason for this is that a verdict of guilty,
as mentioned, is backed up by evidence meeting a standard of proof “beyond
reasonable doubt”, and such a proof can be used to convert any proof of innocent
into a contradiction. But we might be more reluctant to accept the contrapo-
sition: innocent implies not guilty as intuitively correct. Being innocent, as a
concept, is not backed up by any notion of evidence, neither it has to meet any
standard of proof. A common starting point of a trial is the so-called principle of
presumption of innocence, whereby someone accused of committing a crime is a
priori innocent. In other words, some care needs to be taken in an intuitionistic
setting for the law of contraposition does not necessarily hold.
The principle of presumption of innocence is clearly defeasible. If we only
know that a person has been accused of committing a crime, we must conclude
that this person is innocent. However, if additional information is brought up,
e.g., a credible witness, the murder weapon, etc., the principle ceases to apply
and the conclusion that the person is innocent is withdrawn. In other words, the
principle of presumption of innocence behaves non-monotonically.
Here, we build a default logic over Intuitionistic Propositional Logic (IPL).
Our aim is to formally reason about scenarios such as the one presented above.
The choice of a default logic is not arbitrary. Since their introduction in [37], it
has become clear that default logics have a special status in the literature on
non-monotonic logic due to a relatively simple syntax and semantics, natural
representation capabilities, and direct connections to other non-monotonic log-
ics [2,5]. From a logic engineering point of view default logics are also interesting
since they can be modularly built on an underlying logic having some minimal
properties [8]. Moreover, we develop a tableaux calculus for our default logic
taking some ideas from [7,9]. The resulting calculus is sound, complete, and ter-
minating. Not many proof calculi for default logics built over IL exist. Important
works are [1,14]. In [1], a tableaux method is presented for a default logic built on
IPL which allows for the computation of extensions, but not for checking default
consequence. The latter is covered in [14], where a sequent calculus is presented.
The calculi introduced in [1,14] are related to ours but diﬀer in some important
aspects, in particular, in their construction. In addition, we present a prototype
implementation which enables automated reasoning, a feature missing in [1,14].
Structure. In Sect. 2 we introduce preliminary deﬁnitions and results. More pre-
cisely, we present Intuitionistic Propositional Logic (IPL), and a default logic
built over IPL (DIPL). In Sect. 3 we recall tableaux for IPL and develop a tableaux
calculus for default consequence in DIPL. In Sect. 4 we present an implementa-
tion of our calculus. In Sect. 5 we report on a preliminary empirical evaluation of
our implementation. In Sect. 6 we conclude the paper and discuss future research.

A Tableaux Calculus for Default Intuitionistic Logic
163
2
Basic Deﬁnitions
This section introduces basic deﬁnitions to make the paper self-contained.
Intuitionistic Logic. The syntax and semantics of Intuitionistic Propositional
Logic (IPL) is deﬁned below.
Deﬁnition 1 (Syntax). The set F of wﬀs of IPL is deﬁned on an enumerable
set P = { pi | 0 ≤i } of proposition symbols, and is determined by the grammar
ϕ ::= pi | ϕ ∧ϕ | ϕ ∨ϕ | ¬ϕ | ϕ ⊃ϕ.
We write ⊥as an abbreviation for p ∧¬p, and ⊤for ¬⊥.
As in [35], we deﬁne the semantics for IPL via intuitionistic Kripke models.
Deﬁnition 2 (Models). A Kripke model M is a tuple ⟨W, ≼, V ⟩where: W is
a non-empty set of elements (a.k.a. worlds); ≼⊆W × W is the accessibility
relation; and V : W →2P is the valuation function. An intuitionistic Kripke
model is a Kripke model M in which ≼is reﬂexive and transitive, and in which V
satisﬁes the so-called heredity condition: for all w ≼w′, if w ∈V (p), w′ ∈V (p).
Deﬁnition 3 (Semantics). Let M = ⟨W, ≼, V ⟩be an intuitionistic Kripke
model, w ∈W, and ϕ ∈F, we deﬁne the satisﬁability relation M, w |= ϕ
s.t.:
M, w |= p
iﬀp ∈V (w)
M, w |= ϕ ∧ψ
iﬀM, w |= ϕ and M, w |= ψ
M, w |= ϕ ∨ψ
iﬀM, w |= ϕ or M, w |= ψ
M, w |= ¬ϕ
iﬀfor all w ≼w′, M, w′ ⊭ϕ
M, w |= ϕ ⊃ψ iﬀfor all w ≼w′, if M, w′ |= ϕ then M, w′ |= ψ.
Notice that, unlike Classical Propositional Logic, M, w ⊭ϕ is not equivalent to
M, w |= ¬ϕ. For any Φ ⊆F, we say that M, w |= Φ iﬀM, w |= ϕ, for all
ϕ ∈Φ.
Next, we introduce the deﬁnition of consequence for IPL.
Deﬁnition 4 (Consequence). Let Φ ⊆F and ϕ ∈F; we say that ϕ is a logi-
cal consequence of Φ, notation Φ ⊨ϕ, iﬀfor every M and w in M, if M, w |= Φ,
then M, w |= ϕ1. We use ⊨ϕ as an abbreviation for ∅⊨ϕ. We say that Φ is
consistent if Φ ̸⊨⊥, otherwise it is inconsistent.
It is well known that ⊨satisﬁes reﬂexivity, monotonicity, cut, structurality
and compactness (see e.g. [18] for details). In Deﬁnition 4 consequence in IPL is
characterized semantically in terms of Kripke models. In Sect. 3 we present a
syntactic characterization based on a proof system.
1 This notion is referred to as local consequence in the literature on Modal Logic.

164
V. Cassano et al.
Default Logic. First introduced in [37], Default Logic comprises a sub-class
of non-monotonic logics, characterized by so-called defaults and extensions. A
default is a 3-tuple of formulas, notation π
ρ=⇒χ. Intuitively, we can think of
a default as a defeasible conditional which given some conditions on π and ρ
enables us to obtain χ. Extensions formalize what are these conditions. Defaults
and extensions are introduced below.
Deﬁnition 5 (Defaults and Default Theories). We call D = F 3 the set of
all defaults. Let Δ ⊆D, ΔΠ = { π | π
ρ=⇒χ ∈Δ }, ΔP = { ρ | π
ρ=⇒χ ∈Δ } and
ΔX = { χ | π
ρ=⇒χ ∈Δ }. A default theory Θ is a pair (Φ, Δ) where Φ ⊆F and
Δ ⊆D. For any default theory Θ = (Φ, Δ), we deﬁne ΦΘ = Φ and ΔΘ = Δ.
In what follows we restrict our attention to ﬁnite default theories, i.e., default
theories Θ in which both ΦΘ and ΔΘ are ﬁnite sets. Though our deﬁnitions extend
directly to inﬁnite default theories, there are some subtleties involved in dealing
with inﬁnite sets of defaults which we wish to avoid here (see [8] for details).
Deﬁnition 6 (Triggered). Let Θ be a default theory, and Δ ∪{δ} ⊆ΔΘ; we
say that δ is triggered by Δ iﬀ(ΦΘ ∪ΔX) ⊨δΠ.
Deﬁnition 7 (Blocked). Let Θ be a default theory, and Δ ∪{δ} ⊆ΔΘ; we
say that δ is blocked by Δ iﬀthere is ρ ∈(Δ ∪δ)P s.t. ΦΘ ∪(Δ ∪δ)X ∪ρ is
inconsistent.
Deﬁnition 8 (Detached). Let Θ be a default theory and Δ ∪{δ} ⊆ΔΘ; we
say that δ is detached by Δ if δ is triggered and not blocked by Δ.
Intuitively, for a default π
ρ=⇒χ in the context of a default theory Θ, the
notion of detachment in Deﬁnition 8 tells us under which conditions on π and ρ
we can obtain χ. The deﬁnition of detachment is an intermediate step towards
the deﬁnition of an extension.
Deﬁnition 9 (Generating Set). Let Θ be a default theory and Δ ⊆ΔΘ; we
call Δ a generating set iﬀthere is a total ordering ⋖on ΔΘ s.t. Δ = D⋖
Θ(n) for
n = |ΔΘ|, and D⋖
Θ is deﬁned as:
D
⋖
Θ(0) = ∅
D
⋖
Θ(i+1) =
⎧
⎪
⎨
⎪
⎩
D⋖
Θ(i) ∪δ
if δ ∈ΔΘ\D⋖
Θ(i) is detached by D⋖
Θ(i), and
for all η ̸= δ ∈ΔΘ\D⋖
Θ(i), if η is detached by D⋖
Θ(i), δ ⋖η
D⋖
Θ(i)
otherwise.
Deﬁnition 10 (Extension). Let Θ be a default theory, we say that E ⊆F is
an extension of Θ iﬀE = ΦΘ ∪ΔX where Δ ⊆ΔΘ is a generating set.
Intuitively, we can think of an extension of a default theory Θ as a set of
formulas which contains ΦΘ and which is closed under detachment. We are now
in a position to deﬁne our default logic.

A Tableaux Calculus for Default Intuitionistic Logic
165
Deﬁnition 11 (DIPL). The default logic DIPL is the 3-tuple ⟨F, ⊨, E ⟩where:
(i) F is the set of all formulas of IPL; (ii) ⊨is the consequence relation of
IPL; and (iii) E : (2F × 2D ) →2(2F ) is a function which maps every default
theory Θ to its set of extensions, i.e., E ∈E (Θ) iﬀE is an extension of Θ
(see Deﬁnition 10).
The notion of consequence for DIPL is introduced below.
Deﬁnition 12 (Default Consequence). We say that a formula ϕ is a default
consequence of a default theory Θ, notation Θ |≈ϕ, iﬀfor all E ∈E (Θ), E ⊨ϕ2.
Some Comments and Easily Established Properties of DIPL. Deﬁnition 10 corre-
sponds to extensions as deﬁned by Lukaszewicz in [27]. This deﬁnition of exten-
sions is better behaved than Reiter’s original proposal [37]. In particular, it
guarantees existence, i.e., for any default theory Θ, E (Θ) ̸= ∅. Deﬁnition 10 also
guarantees semi-monotonicity. For default theories Θ1 and Θ2, deﬁne Θ1 ⊑Θ2 iﬀ
ΦΘ1 ⊆ΦΘ2 and ΔΘ1 ⊆ΔΘ2. Semi-monotonicity implies that for any two default
theories Θ1 ⊑Θ2, if ΦΘ1 = ΦΘ2, then for all E1 ∈E (Θ1), there is E2 ∈E (Θ2)
s.t. E1 ⊆E2. As we will see in Sect. 3, semi-monotonicity is important because
it allows us to deﬁne a tableaux calculus for default consequence that can take
advantage of a partial use of default theories. DIPL is non-monotonic; in the
sense that there are default theories Θ1 and Θ2 s.t. Θ1 ⊑Θ2, Θ1 |≈ϕ, and
Θ2 ̸|≈ϕ.
We conclude this section with an example illustrating some of the features
and technical elements of DIPL. The following terminology and notation is useful.
A default π
ρ=⇒χ is normal iﬀρ = χ. Normal defaults are written π =⇒χ. Let
Θ1 = (Φ1, Δ1) and Θ2 = (Φ2, Δ2), deﬁne Θ1 ⊔Θ2 = (Φ1 ∪Φ2, Δ1 ∪Δ2). If Θ1
is a default theory, Φ a set of formulas, and Δ a set of defaults, we use Θ1 ⊔Φ
to mean Θ ⊔(Φ, ∅), and Θ ⊔Δ to mean Θ ⊔(∅, Δ).
Example 1 (Presumption of Innocence). Consider the following propositions:
(1) ‘accused’. (2) ‘guilty or not guilty’. (3) ‘guilty implies not innocent’.
(4) ‘the aﬃdavit of a credible witness, the murder weapon, and the results
of forensic tests, imply suﬃcient evidence’. (5) ‘suﬃcient evidence implies
a verdict of guilty’.
In IPL, we would typically formalize (1) to (5) as:
(1’) a.
(2’) g ∨¬g.
(3’) g ⊃¬i.
(4’) (c ∧w ∧f) ⊃e.
(5’) e ⊃g.
In turn, consider the principle of presumption of innocence, i.e., ‘an accused of
committing a crime is a priori innocent’; because of its defeasible status, we
choose to formalize it as the (normal) default (6’) a =⇒i.
Let Θ = ({g ∨¬g, g ⊃¬i, (c ∧w ∧f) ⊃e, e ⊃g}, {a =⇒i}); then:
2 This notion is referred to as sceptical consequence in the literature on Default Logics.

166
V. Cassano et al.
(a) Θ |≈g ∨¬g
(e) Θ ⊔{a} |≈a
(h) Θ ⊔{a, c, w, f} |≈a
(b) Θ |≈g ⊃¬i
(f) Θ ⊔{a} |≈i
(i) Θ ⊔{a, c, w, f} ̸|≈i
(c) Θ |≈(c ∧w ∧f) ⊃e
(g) Θ ⊔{a} ̸|≈g
(j) Θ ⊔{a, c, w, f} |≈g
(d) Θ |≈e ⊃g
Intuitively, the default theory Θ captures the basic set of assumptions dis-
cussed in the example in Sect. 1. These assumptions include the possible out-
comes of the trial, i.e., the verdict of guilty or not guilty, i.e., g ∨¬g; the consid-
eration that guilty implies not innocent, i.e., g ⊃¬i; what constitutes suﬃcient
evidence, i.e., (c ∧w ∧f) ⊃e; and the claim that suﬃcient evidence leads to a
verdict of guilty, i.e., e ⊃g. Each of these assumptions is a default consequence
of Θ. Θ ⊔{a} considers the particular situation at the beginning of a trial, i.e.,
someone is accused of committing a crime. It follows that a and i are default
consequences of Θ⊔{a}; i.e., if the only thing we know is that someone is accused
of committing a crime, we must conclude that said person is innocent, as per
the principle of presumption of innocence. In turn, Θ ⊔{a, c, w, f} captures the
idea that if we acquire suﬃcient evidence, in the form of a credible witness aﬃ-
davit, the murder weapon, and the results of forensic tests, we obtain a verdict
of guilt, and in such a situation the principle of presumption of innocence no
longer holds (i.e., no longer can be used as a basis for establishing the innocence
of the accused).
3
Tableaux Proof Calculus
We develop a tableaux proof calculus for DIPL based on one for IPL. The calculus
captures default consequence in DIPL and it is sound, complete, and terminating
(using loop-checks).
Intuitionistic Tableaux. We begin by recalling the basics of a tableaux calcu-
lus for IPL. We follow closely the style of presentation of [35].
A tableau is a tree whose nodes are of two diﬀerent kinds. The ﬁrst kind
corresponds to a pair of a formula ϕ and a natural number i, called a label,
appearing in positive form, notation @+
i ϕ, or negative form, notation @−
i ϕ3. The
second kind corresponds to a pair of labels i and j, notation (i, j). Intuitively,
@+
i ϕ means “ϕ holds at world i”; and @−
i ϕ means “ϕ does not hold at world
i”4. Intuitively, (i, j) means that world j is accessible from world i.
A tableau for ϕ is a tableau having @−
0 ϕ as its root. A tableau for ϕ is
well-formed if it is constructed according to the expansion rules in Fig. 1. In
this ﬁgure, (∧+), (∧−), (∨+) and (∨−), are rules for the logical connectives of
conjunction and disjunction. The “positive rules” (⊃+) and (¬+) for the logical
connectives of implication and negation are applied for every j in the branch,
whereas the “negative rules” (⊃−) and (¬−) for these logical connectives create
3 The ‘@’ notation is borrowed from Hybrid Logic [3].
4 The signs + and −are necessary since in IPL we cannot use the symbol ¬ of negation
for expressing that a formula does not hold in a world.

A Tableaux Calculus for Default Intuitionistic Logic
167
a “new” label j. The latter implies that the positive rules might need to be re-
applied, e.g., if a new (i, j) is introduced in the branch. The rules (ref) and (trans)
correspond to the reﬂexivity and transitivity constraints for the accessibility
relation. The rule (her) corresponds to the heredity condition in Kripke models,
propagating the valuation of a positive proposition symbol from a world to all its
successors. The rule (A) occupies a special place in the construction of a tableau
and will be discussed immediately below. Rules are applied as usual: premisses
must belong to the branch; side conditions must be met (if any); the branch is
extended at the level of leaves according to the consequents.
Fig. 1. Tableau rules for IPL
Deﬁnition 13 (Closedness and Saturation). A branch is closed, tagged (▲),
if @+
i ϕ and @−
i ϕ occur in the branch; otherwise it is open, tagged (▼). A branch
is saturated, tagged (♦), if the application of any expansion rule is redundant.
Deﬁnition 14 (Provability). A tableau τ for ϕ is an attempt at proving ϕ.
We call τ a proof of ϕ if all branches in τ are closed. We write ⊢ϕ if there is
a proof of ϕ.
Deﬁnition 13 introduces standard conditions of closedness and saturation for
a tableau. Given these conditions, we deﬁne a tableau proof in Deﬁnition 14. The
resulting proof calculus is sound and complete, i.e., ⊢ϕ iﬀ⊨ϕ (see [35]). Ter-
mination is ensured using loop-checks. Loop-checks are a standard termination
technique in tableaux systems that require the re-application of expansion rules
[17,23].
Tableaux constructed without the rule (A) formulate a proof calculus for
provability, i.e., proofs without assumptions. Including the rule (A) in the con-
struction of a tableau gives us a proof calculus for deducibility (proofs from a set

168
V. Cassano et al.
Φ of assumptions). Intuitively, (A) can be understood as stating that assump-
tions are always true in the “current” world. This rule is not strictly necessary:
Φ ⊨ϕ iﬀ⊨∧Φ ⊃ϕ, for Φ ﬁnite. Nonetheless, incorporating a primitive rule for
assumptions simpliﬁes the deﬁnitions and understanding of tableaux for DIPL.
When (A) is involved, we talk about a tableau for ϕ from Φ. Such a tableau is
well-formed if: its root is @−
0 ϕ; the rules in Fig. 1 are applied as usual; and (A)
is applied w.r.t. the formulas in Φ. The precise deﬁnition of the proof calculus
for deducibility is given in Deﬁnition 15. By adapting the argument presented in
[35], it is possible to prove that the calculus for deducibility is also sound and
complete, i.e., Φ ⊢ϕ iﬀΦ ⊨ϕ. Termination is also guaranteed with loop-checks.
Deﬁnition 15 (Deducibility). A tableau τ for ϕ from Φ is an attempt at
proving that ϕ follows from Φ. We call τ a proof of ϕ from Φ if all branches in
τ are closed. We write Φ ⊢ϕ if there is a proof of ϕ from Φ.
An interesting feature of the tableaux calculus of Deﬁnition 15 is that not
only it allows us to ﬁnd proofs, but also lack of proofs. The latter is done by
inspecting particular proof attempts, i.e., tableaux having a branch that is both
open and saturated. These tableaux serve as counter-examples (i.e., they result
in a description of a model which satisﬁes the assumptions but invalidates the
formula we are trying to prove). This claim is made precise in Prop. 1. We resort
to this feature as a way of checking consistency of a set of formulas. This check
will be used in the deﬁnition of an expansion rule for tableaux for DIPL.
Proposition 1. If a tableau for ϕ from Φ has an open and saturated branch,
then, Φ ̸⊨ϕ.
Corollary 1. Φ ̸⊢⊥iﬀΦ ̸⊨⊥.
Summing up, Deﬁnition 15 characterizes proof theoretically, via tableaux, the
semantically deﬁned notion of logical consequence in Deﬁnition 4. We repeat that
termination of the proof calculus is not ensured by a simple exhaustive applica-
tion of rules, and loop-checks are required. Intuitively, a loop-check restricts the
application of an expansion rule ensuring that only “genuinely new worlds” are
created. This technique, traced back to [17,23], is nowadays standard in tableaux
systems.
Default Tableaux. Default tableaux extend tableaux for IPL with the addition
of a new kind of node corresponding to the use of defaults. More precisely, a
default tableau is a tree whose nodes are as in tableaux for IPL, together with a
third kind that corresponds to the use of a default π
ρ=⇒χ. By a default tableau
for ϕ from Θ, where ϕ is a formula and Θ is a default theory, we mean a default
tableau having @−
0 ϕ at its root. Such a default tableau is well-formed if it is
constructed according to the expansion rules in Figs. 1 and 2. Rules in Fig. 1 are
applied as before, with rule (A) being applied w.r.t. formulas in ΦΘ. Rule (D) in
Fig. 2 is applied w.r.t. the defaults in ΔΘ. Note that rule (D) is applicable only if
its side condition is met. This side condition can be syntactically decided using
auxiliary tableaux in IPL to verify detachment, i.e., to prove that a default is
triggered and not blocked.

A Tableaux Calculus for Default Intuitionistic Logic
169
Fig. 2. Tableau rule for defaults
Deﬁnition 16 (Default Deducibility). Any well-formed default tableau τ for
ϕ from Θ is an attempt at proving that ϕ follows from Θ by default. We call τ
a default proof of ϕ from Θ if all branches of τ are closed. We write Θ |∼ϕ if
there is a default proof of ϕ from Θ.
The proof calculus just introduced is sound and complete. Termination is
guaranteed by the termination of tableaux for IPL. Figure 3 shows a default
proof.
Fig. 3. Default tableau for u1 ∧u2 from ⟨{b1 ∨b2}, { ⊤
ui∧¬bi
=====⇒ui | i ∈{1, 2} }⟩.
Theorem 1. Θ |∼ϕ iﬀΘ |≈ϕ. Since ⊢terminates, |∼also terminates.
Proof (Sketch). We make use of the already known soundness and completeness
of deducibility of the tableaux calculus for IPL. The proof of soundness and com-
pleteness of default deducibility depends on the following two observations: (i)
a branch of a default tableau contains a branch of a tableau for deducibility in
IPL (just remove default nodes); and (ii) defaults appearing in an open and satu-
rated branch of a default tableau deﬁne a generating set (as per Deﬁnition 9) by
construction. (i) and (ii) implies that if there is an open and saturated branch of

170
V. Cassano et al.
a default tableau, then, there is an extension which serves as a counter-example.
This proves that if Θ |≈ϕ, then Θ |∼ϕ. For the converse, suppose that Θ ̸|≈ϕ;
hence there is an extension E ∈E (Θ) s.t. E ̸⊨ϕ. Let τ be any saturated default
tableau for ϕ from Θ; by construction, there is an open branch of τ which con-
tains a set of defaults which is a generating set for E. The result follows from
the completeness of deducibility for IPL.
4
Implementation
Overview. DefTab is an implementation of the default tableau calculus pre-
sented in Sect. 3. It is available at http://tinyurl.com/deftab0.
Given Θ and ϕ as input, DefTab builds proof attempts of Θ |∼ϕ by searching
for Kripke models for ϕ, and subsequently restricting these models with the use
of sentences from ΦΘ and defaults from ΔΘ. DefTab reports whether or not a
default proof has been found. In the latter case, DefTab exhibits an extension of
Θ from which ϕ does not follow.
Defaults Detachment Rule and Sub-tableaux. The following is a brief
explanation of how defaults are dealt with in DefTab. At any given moment,
DefTab maintains defaults in three lists: available, triggered, and detached. The
available list contains the defaults of the input default theory. An available
default π
ρ=⇒χ is triggered if π is deduced from the set ΦΘ of the default the-
ory Θ under question, together with the consequents of the defaults already in
the branch of the tableau. Once triggered, available defaults are moved to the
list of triggered defaults. DefTab uses the latter list to apply rule (D) in Fig. 2
and generates a (temporary) sub-list of non-blocked defaults. This sub-list of
non-blocked defaults corresponds to the branching part of rule (D). DefTab then
takes defaults from the sub-list of non-blocked defaults and moves them from
the triggered list to the detached list, expanding the default tableau accordingly.
Since the application of rule (D) requires checking consequence and consistency
in IPL, for each (D)-step, DefTab builds a corresponding number of intuitionistic
tableaux.
Blocking and Optimizations. For loop-checking intuitionistic rules that cre-
ate new labels, DefTab uses a blocking technique called pattern-based block-
ing [25], initially designed for modal logics. In the present tableau system, when
rule (¬−) can be applied to some formula @−
i ¬ϕ, it is ﬁrst checked that no label
k exists such that the set of formulas {@−
k ϕ} ∪@kC(i) hold, where C(i) are the
constraints that formulas at label i forces on all its successors. If such a label
exists, then the rule is not applied. The same occurs for rule (⊃−).
DefTab does not include semantic branching, or any other optimization based
on Boolean negation and the excluded middle (which are usually unsound in an
intuitionistic setup). Backjumping [24], on the other hand, is intuitionistically
sound, and preliminary testing shows that it greatly improves performance.
We take special care of tracking dependencies of the consequent formulas
introduced by the application of rule (D). That is, once a default π
ρ=⇒χ is

A Tableaux Calculus for Default Intuitionistic Logic
171
triggered, we bookkeep it along with the set of formulas that triggered it. Con-
cretely, this bookkeeping is the union of the dependencies of all defaults Δ s.t.
ΦΘ ∪ΔX |= π. Note that this set can overestimate the set of dependencies of the
triggered rule. This is a trade-oﬀbetween being precise and limiting the number
of sub-tableaux runs.
Usage. DefTab takes as input a ﬁle indicating the underlying logic to be used,
a default theory partitioned into its set of formulas and set of defaults, and the
formula to be checked. The structure of this input ﬁle is illustrated in the fol-
lowing example ﬁle presumption.dt. This ﬁle corresponds to Example 1 where:
g →P1, i →P2, a →P3, c →P4, w →P5, f →P6, e →P7.
intuitionistic
facts:
P1 v !P1; P1 -> !P2;
(P4 ^ P5 ^ P6) -> P7;
P7 -> P1; P3;
defaults:
P3 --> P2;
consequence:
P2
– The keyword intuitionistic indicates that the
prover will work over IPL as the underlying logic.
– The keyword facts indicates the beginning of the
set of formulas of the default theory.
– The keyword defaults indicates the beginning of
the set of defaults. The syntax for a default π
ρ=⇒χ is
π --- ρ --> χ. Normal default rules can be written
as π --> χ.
– The keyword consequence indicates the formula to
be proven.
DefTab is executed from the command line as
$ ./deftab -f presumption.dt
----------------------------------
Indeed a sceptical consequence.
Total time: 8.01513e-4
The output indicates that P2 is a sceptical consequence of the default theory.
If we add the facts P4; P5; P6;, we obtain that P2 is not a sceptical conse-
quence, indicated by the output: Not a sceptical consequence, found bad
extension: []. The list [] indicates the defaults in the extension, in this case
none. DefTab includes in the output a counter-example for disproving the candi-
date default consequence. Such counter-example consists of the generator set of
the corresponding extension. In this case, the empty list indicates that the set
of facts itself (without any default) is enough to generate the extension.
5
Preliminary Testing
To our knowledge, there is no standard test set for automated reasoning for
default logic, and less so (if possible) for default reasoning based on IL. Moreover,
our tool is a ﬁrst prototype which still needs the implementation of many natu-
ral optimizations (e.g., catching for sub-tableaux results). Hence, any empirical
testing is, by force, very preliminary, and should be taken only as a ﬁrst eval-
uation of the initial performance of the tool, and of its current usability. Still,

172
V. Cassano et al.
the results are encouraging and the prover seems to be able to handle examples
which are well beyond what can be computed by hand, making it already a use-
ful tool. We discuss below the tests sets we evaluated. All tests were performed
on a machine running Ubuntu 16.04 LTS, with 8 GB of memory and an Intel
Core i7-5500U CPU @ 2.40 Ghz.
Purely Intuitionistic Problems. When no defaults are speciﬁed in the input
ﬁle, DefTab behaves as a prover for consequence in IPL (but it has not particular
optimization for the case where the input is just an intuitionistic formula). Even
though DefTab is still a prototype, we carried out a comparison of its performance
w.r.t. existing provers for IPL.
We extend the comparison of provers for IPL carried out in [11] which com-
pares their own prover (intuit) that implements satisﬁability checking in IPL
by an SMT (Satisﬁability Modulo Theories) reasoner implemented over Min-
iSAT [13], with IntHistGC [21] and fCube [16]. These two last provers perform a
backtracking search directly on a proof calculus, and are rather diﬀerent from the
approach taken by intuit, and closer to DefTab. IntHistGC implements clever back-
tracking optimizations that avoid recomputations in many cases. fCube imple-
ments several pruning techniques on a tableau-based calculus. Tests are drawn
from three diﬀerent benchmark suites.
1. ILTP [36] includes 12 problems parameterized by size. The original bench-
mark is limited, and hence it was extended as follows: two problems were
generated up to size 38 and all other problems up to size 100, leading up to
a total of 555 problem instances.
2. Benchmarks crafted by IntHistGC developers. These are 6 parameterized prob-
lems. They are carefully constructed sequences of formulas that separate clas-
sical and intuitionistic logic. The total number of instances is 610.
3. API solving; these are 10 problems where a rather large API (set of functions
with types) is given, and the problem is to construct a new function of a
given type. Each problem has variants with API sizes that vary in size from a
dozen to a few thousand functions. These problems were constructed by the
developers of intuit in an attempt to create practically useful problems. The
total number of instances is 35.
Figure 4 shows a scatterplot (logscale) of the performance of DefTab with the
other three provers, discriminating between valid and not valid formulas. Times
shown are in seconds, and the timeout was set to 300 seconds. Point below the
diagonal show cases where DefTab performance is worse that the other provers.
The empirical tests show that specialized provers for IPL (and in particular
intuit) outperforms DefTab, specially on valid, complex formulas. On simple, not
valid formulas, the performance of DefTab in this test is better than fCube and
comparable to IntHistGC.
Broken Arms [34]. Consider an assembly line with two mechanical arms. We
assume that an arm is usable (ui), but sometimes it can be broken (bi), although
a broken arm is an exception. In the literature on Default Logic such default
assumptions have been formalized either as:

A Tableaux Calculus for Default Intuitionistic Logic
173
Fig. 4. Comparison on IPL: □fCube, ♦intuit, △IntHistGC.
Δ1 =

⊤
ui∧¬bi
====⇒ui
 i ∈{1, 2}

or Δ2 =

(bi ∨¬bi)
ui∧¬bi
====⇒ui
 i ∈{1, 2}

.
Δ1 can be found in Poole’s original discussion of the example (see [34]), whereas
Δ2 is found in, e.g., [1,14]. Suppose that we know as a fact that one of the arms
is broken, but we do not know which one, i.e., b1 ∨b2. Let Θ1 = ({b1 ∨b2}, Δ1)
and Θ2 = ({b1 ∨b2}, Δ2); it is possible to prove that Θ1 |∼u1 ∧u2 and that
Θ2 ̸|∼u1 ∧u2. Poole introduces this example to argue that having u1 ∧u2 as a
default consequence of Θ1 is counter-intuitive; as we have as a fact that one of the
arms is broken. This counter-intuitive result has inspired some important work
on Default Logic [6,12,29]. Here, we choose this example, and Θ1 in particular,
merely as a test case. First, observe that Θ1 can easily be made parametric on
the number of arms (the number of defaults grows linearly with the number of
arms). Second, observe that since defaults do not block each other, they can all
be detached at any given time in a default proof attempt. The latter means that
the prover needs, a priori, to consider a large number of combinations, leading
to a potentially large search space. On the other hand, the intuitionistic reason-
ing needed is controlled, and as a result, the test case should mostly highlight
how default are handled by the prover. These observations make this example a
good candidate for testing the implementation of our proof calculus in order to
evaluate its performance in a small, but non-trivial case. The results of running
DefTab with n defaults are reported below.
No. of defaults 10
20
40
60
80
run time
0.038 s 0.499 s 8.667 s 49.748 s 177.204 s
Abstract. The following example, actually, an example template, is a variation
of Broken Arms, where defaults do not block one another, but in which the

174
V. Cassano et al.
detachment of defaults involves “non-trivial” intuitionistic formulas. This exam-
ple is built on the ILTP library. To provide a bit of context, the ILTP library has
two kinds of problems: those testing consequence in IPL, i.e., problems Φ ⊢ϕ;
and those testing not-consequence in IPL, i.e., problems Φ ̸⊢ϕ. Of the problems
testing not-consequence, we are interested in a particular sub-class, i.e., those
of the form Φ ̸⊢¬ϕ. Problems in this sub-class can directly be used for testing
blocking of defaults. Now, for each pair Φ1 ⊢ϕ1 and Φ2 ̸⊢¬ϕ2, we construct a
default theory Θ = (Φ1 ∪Φ2, {ϕ1
ϕ2
=⇒p}). We carry out this construction guar-
anteeing that the languages of Φi ∪ϕi are disjoint via renaming of proposition
symbols and that p is new. Then, we consider a family { Θi | i ∈[1, n] } of
default theories constructed in the way just described. It can be proven that
(n
i=1 Θi) |∼(n
i=1 pi). This construction serves as a way to test our implemen-
tation on increasingly more diﬃcult default theories built on the ILTP library.
The next table reports the results of running DefTab on Θm
n |∼(n
i=1 ri) where:
Θa
b = a
i=1(Φb
i ∪Γ b
i , {pi0
qi0
=⇒ri})
Φa
b = {
pba, a
i=1(pbi ⊃(pbi ⊃pb(i−1)))}
Γ a
b = {¬¬qba, a
i=1(qbi ⊃(qbi ⊃qb(i−1)))}.
n\m 1
2
3
4
5
1
2.20e−3 s 1.50e−2 s 0.15 s 1.15 s 11.29 s
2
1.99e−3 s 2.23e−2 s 0.25 s 2.17 s 17.17 s
3
2.8e−3 s
3.7e−2 s
0.39 s 3.29 s 26.08 s
Intuitively, Θm
n contains m instances of sub-problems Φn
m ⊢pm0 and Γ n
m ̸⊢¬qm0
of size n taken from the ILTP. The sub-indices in Φn
m and Γ n
m ensure languages
are disjoint. It follows that every default pi0
qi0
=⇒ri is detached by Φn
i ∪Γ n
i ; and
also that no default blocks any other. Thus, Θm
n
|∼(m
i=1 ri). The times show
the progression of running DefTab on increasingly larger default theories Θm
n .
Note the increase on complexity is both on m and n.
6
Final Remarks
We introduced DIPL, a Default Logic to reason non-monotonically over Propo-
sitional Intuitionistic Logic. This logic is motivated by Normative Systems and
Legal Artiﬁcial Intelligence in order to deal, e.g., with legal conﬂicts due to logical
inconsistencies. Our contribution is twofold. First, we present a sound, complete
and terminating tableaux calculus for DIPL, which decides if a formula ϕ is a
logical consequence of a default theory Θ. The calculus is based on tableaux for
IPL as presented in [35], combined with the treatment for defaults of [7]. Second,
we provide a prototype implementation for our calculus in the DefTab prover.

A Tableaux Calculus for Default Intuitionistic Logic
175
To the best of our knowledge, this is the ﬁrst prover combining Intuitionis-
tic Logic with Non-monotonic Reasoning. For instance, DeReS [10] is a default
logic reasoner with an underlying propositional tableaux calculus. It is designed
to check logical consequence by combining defaults reasoning and the underly-
ing logic reasoning as ‘black boxes’. This contrasts with DefTab which integrates
these two reasonings in a same tableaux calculus. On the other hand DefTab only
supports sceptical consequence checking, while DeReS also supports credulous
consequence checking. In [15], a tableaux calculus for Intuitionistic Propositional
Logic is presented, with a special treatment for nested implications. The imple-
mentation is no longer available, but it would be interesting to implement their
specialized rules in DefTab. More directly related to our work are [1], where
the authors present a sequent calculus for a Non-monotonic Intuitionistic Logic;
and [14], where a tableaux method is presented but only for the computation of
extensions, and with no implementation.
We ran an empirical evaluation of our prover and obtained preliminary results
concerning the implementation. To test purely intuitionistic reasoning we used
the ILTP problem library [36]. We tested non-monotonic features in two, very
small, case examples (broken arms and abstract). Clearly further testing is nec-
essary. In particular, we are interested in crafting examples that combine the
complexities of non-monotonic and intuitionistic reasoning.
For future work there are several interesting lines of research. As we men-
tioned, the treatment of defaults in the calculus is almost independent from the
underlying logic. As a consequence, it would be interesting to deﬁne a calculus
which is parametric on the rules for the underlying logic (see, e.g., [8] for such
a proposal). We believe that (modulo some refactoring in the current source
code) the implementation of DefTab can be generalized to handle defaults over
diﬀerent underlying logics obtaining a prover for a wide family of Default Logics.
Ackowledgements. This work was partially supported by ANPCyT-PICTs-2017-
1130 and 2016-0215, MinCyT C´ordoba, SeCyT-UNC, the Laboratoire International
Associ´e INFINIS and the European Union’s Horizon 2020 research and innovation
programme under the Marie Skodowska-Curie grant agreement No. 690974 for the
project MIREL: MIning and REasoning with Legal texts.
References
1. Amati, G., Aiello, L., Gabbay, D., Pirri, F.: A proof theoretical approach to default
reasoning I: tableaux for default logic. J. Log. Comput. 6(2), 205–231 (1996)
2. Antoniou, G.: Nonmonotonic Reasoning. The MIT Press, Cambridge (1997)
3. Areces, C., ten Cate, B.: Hybrid logics. In: Blackburn, et al. [4], pp. 821–868
4. Blackburn, P., van Benthem, J., Wolter, F. (eds.): Handbook of Modal Logic.
Elsevier, Amsterdam (2007)
5. Bochman, A.: Non-monotonic reasoning. In: Gabbay and Woods [20], pp. 555–632
6. Brewka, G.: Cumulative default logic: in defense of nonmonotonic inference rules.
Artif. Intell. 50(2), 183–205 (1991)

176
V. Cassano et al.
7. Cassano, V., Areces, C., Castro, P.: Reasoning about prescription and description
using prioritized default rules. In: Barthe, G., Sutcliﬀe, G., Veanes, M. (eds.) 22nd
International Conference on Logic for Programming, Artiﬁcial Intelligence and
Reasoning (LPAR-22), EPiC Series in Computing, vol. 57, pp. 196–213. EasyChair
(2018)
8. Cassano, V., Fervari, R., Areces, C., Castro, P.F.: Interpolation and beth deﬁn-
ability in default logics. In: Calimeri, F., Leone, N., Manna, M. (eds.) JELIA 2019.
LNCS (LNAI), vol. 11468, pp. 675–691. Springer, Cham (2019). https://doi.org/
10.1007/978-3-030-19570-0 44
9. Cassano, V., Pombo, C.G.L., Maibaum, T.S.E.: A propositional tableaux based
proof calculus for reasoning with default rules. In: De Nivelle, H. (ed.) TABLEAUX
2015. LNCS (LNAI), vol. 9323, pp. 6–21. Springer, Cham (2015). https://doi.org/
10.1007/978-3-319-24312-2 2
10. Cholewinski, P., Marek, V., Truszczynski, M.: Default reasoning system DeReS.
In: 5th International Conference on Principles of Knowledge Representation and
Reasoning (KR 1996), pp. 518–528. Morgan Kaufmann (1996)
11. Claessen, K., Ros´en, D.: SAT modulo intuitionistic implications. In: Davis, M.,
Fehnker, A., McIver, A., Voronkov, A. (eds.) LPAR 2015. LNCS, vol. 9450, pp.
622–637. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48899-
7 43
12. Delgrande, J., Schaub, T., Jackson, W.: Alternative approaches to default logic.
Artif. Intell. 70(1–2), 167–237 (1994)
13. E´en, N., S¨orensson, N.: An extensible SAT-solver. In: Giunchiglia, E., Tacchella,
A. (eds.) SAT 2003. LNCS, vol. 2919, pp. 502–518. Springer, Heidelberg (2004).
https://doi.org/10.1007/978-3-540-24605-3 37
14. Egly, U., Tompits, H.: A sequent calculus for intuitionistic default logic. In: 12th
Workshop Logic Programming (WLP 1997), pp. 69–79 (1997)
15. Ferrari, M., Fiorentini, C., Fiorino, G.: A tableau calculus for propositional intu-
itionistic logic with a reﬁned treatment of nested implications. J. Appl. Non-Class.
Log. 19, 149–166 (2009)
16. Ferrari, M., Fiorentini, C., Fiorino, G.: fCube: an eﬃcient prover for intuitionistic
propositional logic. In: Ferm¨uller, C.G., Voronkov, A. (eds.) LPAR 2010. LNCS,
vol. 6397, pp. 294–301. Springer, Heidelberg (2010). https://doi.org/10.1007/978-
3-642-16242-8 21
17. Fitting, M.: Proof Methods for Modal and Intuitionistic Logics. Springer, Dor-
drecht (1983). https://doi.org/10.1007/978-94-017-2794-5
18. Font, J.: Abstract Algebraic Logic: An Introductory Textbook, 1st edn. College
Publications (2016)
19. Gabbay, D.M.: Intuitionistic basis for non-monotonic logic. In: Loveland, D.W.
(ed.) CADE 1982. LNCS, vol. 138, pp. 260–273. Springer, Heidelberg (1982).
https://doi.org/10.1007/BFb0000064
20. Gabbay, D., Woods, J. (eds.): Handbook of the History of Logic: The Many Valued
and Nonmonotonic Turn in Logic, vol. 8. North-Holland, Amsterdam (2007)
21. Gor´e, R., Thomson, J., Wu, J.: A history-based theorem prover for intuitionistic
propositional logic using global caching: IntHistGC system description. In: Demri,
S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS (LNAI), vol. 8562, pp.
262–268. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08587-6 19
22. Haeusler, E., de Paiva, V., Rademaker, A.: Intuitionistic logic and legal ontologies.
In: 23rd International Conference on Legal Knowledge and Information Systems
(JURIX 2010), Frontiers in Artiﬁcial Intelligence and Applications, vol. 223, pages
155–158. IOS Press (2010)

A Tableaux Calculus for Default Intuitionistic Logic
177
23. Hughes, G., Cresswell, M.: An Introduction to Modal Logic. Methuen, London
(1968)
24. Hustadt, U., Schmidt, R.A.: Simpliﬁcation and backjumping in modal tableau.
In: de Swart, H. (ed.) TABLEAUX 1998. LNCS (LNAI), vol. 1397, pp. 187–201.
Springer, Heidelberg (1998). https://doi.org/10.1007/3-540-69778-0 22
25. Kaminski, M., Smolka, G.: Terminating tableau systems for hybrid logic with dif-
ference and converse. J. Log. Lang. Inf. 18(4), 437–464 (2009)
26. Kapsner, A.: The logic of guilt, innocence and legal discourse. In: Urbaniak, R.,
Payette, G. (eds.) Applications of Formal Philosophy. LARI, vol. 14, pp. 7–24.
Springer, Cham (2017)
27. Lukaszewicz, W.: Considerations on default logic: an alternative approach. Com-
put. Intell. 4, 1–16 (1988)
28. Makinson, D., van der Torre, L.: What is input/output logic? Input/output logic,
constraints, permissions. In: Boella, G., van der Torre, L., Verhagen, H. (eds.)
Normative Multi-agent Systems, Dagstuhl Seminar Proceedings, vol. 07122. Inter-
nationales Begegnungsund Forschungszentrum f¨ur Informatik (2007)
29. Mikitiuk, A., Truszczynski, M.: Constrained and rational default logics. In: 14th
International Joint Conference on Artiﬁcial Intelligence (IJCAI 1995), pp. 1509–
1517 (1995)
30. Osorio, M., Navarro P´erez, J., Arrazola, J.: Applications of intuitionistic logic in
answer set programming. TPLP 4(3), 325–354 (2004)
31. Parent, X., Gabbay, D., Torre, L.: Intuitionistic basis for input/output logic. In:
Hansson, S.O. (ed.) David Makinson on Classical Methods for Non-Classical Prob-
lems. OCL, vol. 3, pp. 263–286. Springer, Dordrecht (2014). https://doi.org/10.
1007/978-94-007-7759-0 13
32. Pearce, D.: Stable inference as intuitionistic validity. J. Log. Program. 38(1), 79–91
(1999)
33. Pearce, D., Sarsakov, V., Schaub, T., Tompits, H., Woltran, S.: A polynomial
translation of logic programs with nested expressions into disjunctive logic pro-
grams: preliminary report. In: Stuckey, P.J. (ed.) ICLP 2002. LNCS, vol. 2401, pp.
405–420. Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-45619-8 28
34. Poole, D.: What the lottery paradox tells us about default reasoning. In: 1st Inter-
national Conference on Principles of Knowledge Representation and Reasoning
(KR 1989), pp. 333–340 (1989)
35. Priest, G.: An Introduction to Non-Classical Logic: From If to Is. Cambridge Uni-
versity Press, Cambridge (2000)
36. Raths, T., Otten, J., Kreitz, C.: The ILTP problem library for intuitionistic logic.
J. Autom. Reason. 38(1–3), 261–271 (2007)
37. Reiter, R.: A logic for default reasoning. Artif. Intell. 13(1–2), 81–132 (1980)
38. Servi, G.: Nonmonotonic consequence based on intuitionistic logic. J. Symb. Log.
57(4), 1176–1197 (1992)
39. Wansing, H.: Semantics-based nonmonotonic inference. Notre Dame J. Formal Log.
36(1), 44–54 (1995)

NIL: Learning Nonlinear Interpolants
Mingshuai Chen1,2(B)
, Jian Wang1,2
, Jie An3
, Bohua Zhan1,2(B)
,
Deepak Kapur4
, and Naijun Zhan1,2(B)
1 State Key Laboratory of Computer Science,
Institute of Software, CAS, Beijing, China
{chenms,bzhan,znj}@ios.ac.cn
2 University of Chinese Academy of Sciences, Beijing, China
3 School of Software Engineering, Tongji University, Shanghai, China
4 Department of Computer Science, University of New Mexico, Albuquerque, USA
Abstract. Nonlinear interpolants have been shown useful for the veri-
ﬁcation of programs and hybrid systems in contexts of theorem proving,
model checking, abstract interpretation, etc. The underlying synthesis
problem, however, is challenging and existing methods have limitations
on the form of formulae to be interpolated. We leverage classiﬁcation
techniques with space transformations and kernel tricks as established
in the realm of machine learning, and present a counterexample-guided
method named NIL for synthesizing polynomial interpolants, thereby
yielding a uniﬁed framework tackling the interpolation problem for the
general quantiﬁer-free theory of nonlinear arithmetic, possibly involving
transcendental functions. We prove the soundness of NIL and propose
suﬃcient conditions under which NIL is guaranteed to converge, i.e.,
the derived sequence of candidate interpolants converges to an actual
interpolant, and is complete, namely the algorithm terminates by pro-
ducing an interpolant if there exists one. The applicability and eﬀective-
ness of our technique are demonstrated experimentally on a collection
of representative benchmarks from the literature, where in particular,
our method suﬃces to address more interpolation tasks, including those
with perturbations in parameters, and in many cases synthesizes simpler
interpolants compared with existing approaches.
Keywords: Nonlinear Craig interpolant ·
Counterexample-guided learning · Program veriﬁcation ·
Support vector machines (SVMs)
1
Introduction
Interpolation-based technique provides a powerful mechanism for local and mod-
ular reasoning, thereby improving scalability of various veriﬁcation techniques,
This work has been supported through grants by NSFC under grant No. 61625206
and 61732001, by the CAS Pioneer Hundred Talents Program under grant No.
Y9RC585036, and by the National Science Foundation Award DMS-1217054.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 178–196, 2019.
https://doi.org/10.1007/978-3-030-29436-6_11

NIL: Learning Nonlinear Interpolants
179
e.g., theorem proving, model checking and abstract interpretation, to name just
a few. The study of interpolation was pioneered by Kraj´ıˇcek [27] and Pudl´ak [35]
in connection with theorem proving, by McMillan [30] in the context of model
checking, by Graf and Sa¨ıdi [17], McMillan [31] and Henzinger et al. [20] per-
taining to abstraction like CEGAR [8], and by Wang et al. [24] in the context
of learning-based invariant generation. Developing eﬃcient algorithms for gen-
erating interpolants for various theories and their combination has become an
active research area, see e.g., [7,25,26,31,32,36,46].
Though established methods addressing interpolant generation for Pres-
burger arithmetic, decidable fragments of ﬁrst-order logic, theory of equality
over uninterpreted functions (EUFs) as well as their combination have been
extensively studied in the literature, there appears to be little work on syn-
thesizing nonlinear interpolants. Dai et al. proposed an algorithm in [11] for
generating interpolants for nonlinear polynomial inequalities based on the exis-
tence of a witness guaranteed by Stengle’s Positivstellensatz [16] that can be
computed using semi-deﬁnite programming (SDP). A major limitation of this
method is that the two mutually contradictory formulas to be interpolated must
share the same set of variables. Okudono et al. extended [11] in [33] to cater
for the so-called sharper and simpler interpolants by developing a continuous
fraction-based algorithm that rounds oﬀnumerical solutions. In [14], Gan et al.
considered the interpolation for inequalities combined with EUFs by employ-
ing the hierarchical calculus framework proposed in [38] (and its extension [39]),
while the inequalities are limited to be of the concave quadratic form. In [15], Gao
and Zuﬀerey transformed proof traces from δ-complete decision procedures into
interpolants, composed of Boolean combinations of linear constraints, which can
deal with certain transcendental functions beyond polynomials. The techniques
of encoding interpolants as logical combinations of linear constraints, includ-
ing [15,28,37], however, yield potentially large interpolants (requiring even an
inﬁnite length in the worst case) and their usage thus becomes diﬃcult in prac-
tical applications (cf. Example 1).
Interpolants can be viewed as classiﬁers that distinguish, in the context of
program veriﬁcation for instance, positive program states from negative ones
(unreachable/error states) and consequently the state-of-the-art classiﬁcation
algorithms can be leveraged for synthesizing interpolants. The universal appli-
cability of classiﬁcation techniques substantially extends the scope of theories
admitting interpolant generation. This idea was ﬁrst employed by Sharma et al.
in [37], which infers linear interpolants through hyperplane-classiﬁers generated
by support vector machines (SVMs) [3,45] whilst handles superﬁcial nonlinear-
ities by assembling interpolants in the form purely of conjunctions (or dually,
disjunctions) of linear half-spaces, which addresses only a limited category of
formulae featuring nonlinearities. The learning-based paradigm has also been
exploited in the context of nonlinear constraint solving, see e.g., [12].
In this paper, we present a classiﬁcation-based learning method for the syn-
thesis of polynomial interpolants for the quantiﬁer-free theory of nonlinear arith-
metic. Our approach is based on techniques of space transformations and ker-
nel tricks pertinent to SVMs that have been well-developed in the realm of

180
M. Chen et al.
machine learning. Our method is described by an algorithm called NIL (and
its several variants) that adopts the counterexample-guided inductive synthesis
framework [22,40]. We prove the soundness of NIL and propose suﬃcient condi-
tions under which NIL is guaranteed to converge, that is, the derived sequence
of classiﬁers (candidate interpolants) converges to an actual interpolant, and is
complete, i.e., if an interpolant exists, the method terminates with an actual
interpolant. In contrast to related work on generation of nonlinear interpolants,
which restrict the input formulae, our technique provides a uniform framework,
tackling the interpolation problem for the general quantiﬁer-free theory of non-
linear arithmetic, possibly involving transcendental functions. The applicabil-
ity and eﬀectiveness of NIL are demonstrated experimentally on a collection of
representative benchmarks from the literature; as is evident from experimental
results, our method is able to address more demands on the nature of inter-
polants, including those with perturbations in parameters (due to the robust-
ness inherited from SVMs); in many cases, it synthesizes simpler interpolants
compared with other approaches, as shown by the following example.
Example 1 ([15]). Consider two mutually contradictory inequalities φ = y ≥x2
and ψ = y ≤−cos(x) + 0.8. Our NIL algorithm constructs a single polynomial
inequality I = 15x2 < 4 + 20y as the interpolant, namely, φ |= I and I ∧ψ
is unsatisﬁable; while the interpolant generated by the approach in [15], only
when provided with suﬃciently large ﬁnite domains, e.g., x ∈[−π, π] and y ∈
[−0.2, π2], is y > 1.8 ∨(0.59 ≤y ≤1.8 ∧−1.35 ≤x ≤1.35) ∨(0.09 ≤y <
0.59 ∧−0.77 ≤x ≤0.77) ∨(y ≥0 ∧−0.3 ≤x ≤0.3). As will be discussed later,
we do not need to provide a priori information to our algorithm such as bounds
on variables.
The rest of the paper is organized as follows. Section 2 introduces some pre-
liminaries on Craig interpolants and SVMs. In Sect. 3, we present the NIL algo-
rithm dedicated to synthesizing nonlinear interpolants, followed by the analysis
of its soundness, conditional completeness and convergence in Sect. 4. Section 5
reports several implementation issues and experimental results on a collection
of benchmarks (with the robustness discussed in Sect. 6). The paper is then
concluded in Sect. 7.
2
Preliminaries
Let N, Q and R be the set of natural, rational and real numbers, respectively.
We denote by R[x] the polynomial ring over R with variables x = (x1, . . . , xn),
and ∥x∥denotes the ℓ2-norm [4]. For a set X ⊆Rn, its convex hull is denoted by
conv(X). For x, x′ ∈X, dist(x, x′) = ∥x −x′∥denotes the Euclidean distance
between two points, which generalizes to dist(x, X′) = minx′∈X′ dist(x, x′).
Given δ ≥0, deﬁne B(x, δ) = {x′ ∈Rn|∥x′ −x∥≤δ} as the closed ball of radius
δ centered at x. Consider the quantiﬁer-free fragment of a ﬁrst-order theory of
polynomials over the reals, denoted by TP , in which a formula ϕ is of the form
ϕ = p(x) ⋄0 | ϕ ∧ϕ | ϕ ∨ϕ | ¬ϕ

NIL: Learning Nonlinear Interpolants
181
where p(x) ∈R[x] and ⋄∈{<, >, ≤, ≥, =}. A natural extension of our method
to cater for more general nonlinearities involving transcendental functions will
be demonstrated in subsequent sections. In the sequel, we use ⊥to stand for false
and ⊤for true. Let R[x]m consist of all polynomials p(x) of degree ≤m ∈N.
We abuse the notation ϕ ∈R[x]m to abbreviate ϕ = p(x) ⋄0 and p(x) ∈R[x]m
if no ambiguity arises.
Given formulas φ and ψ in a theory T , φ is valid w.r.t. T , written as |=T φ,
iﬀφ is true in all models of T ; φ entails ψ w.r.t. T , written as φ |=T
ψ, iﬀ
every model of T that makes φ true makes ψ also true; φ is satisﬁable w.r.t. T ,
iﬀthere is a model of T in which φ is true; otherwise unsatisﬁable. It follows
that φ is unsatisﬁable iﬀφ |=T ⊥. The set of all the models that make φ true
is denoted by [[φ]]T .
2.1
Craig Interpolant
Craig showed in [10] that given two formulas φ and ψ in a ﬁrst-order logic T s.t.
φ |=T ψ, there always exists an interpolant I over the common symbols of φ
and ψ s.t. φ |=T I and I |=T ψ. In the veriﬁcation literature, this terminology
has been abused by [31], which deﬁned an interpolant over the common symbols
of φ and ψ as
Deﬁnition 1 (Interpolant). Given φ and ψ in a theory T s.t. φ∧ψ |=T ⊥, a
formula I is a (reverse) interpolant of φ and ψ if (i) φ |=T I; (ii) I ∧ψ |=T ⊥;
and (iii) I contains only common symbols shared by φ and ψ.
It is immediately obvious that φ |=T ψ iﬀφ ∧¬ψ |=T ⊥, namely, I is an
interpolant of φ and ψ iﬀI is a reverse interpolant in McMillan’s sense of φ and
¬ψ. We follow McMillan in continuing to abuse the terminology.
2.2
Support Vector Machines
In machine learning, support vector machines [3,45] are supervised learning
models for eﬀective classiﬁcation based on convex optimization. In a binary
setting, we are given a training dataset X = X+ ⊎X−of n sample points
{(x1, y1), (x2, y2), . . . , (xn, yn)}, where xi ∈Rd, and yi is either 1, indicating a
positive sample xi ∈X+, or −1, indicating a negative one in X−. The goal of
classiﬁcation here is to ﬁnd a potential hyperplane (a.k.a. linear classiﬁer) to
separate the positive samples from the negative ones. There however might be
various or even inﬁnite number of separating hyperplanes, and an SVM aims
to construct a separating hyperplane that yields the largest distance (so-called
functional margin) to the nearest positive and negative samples. Such a clas-
siﬁcation hyperplane is called the optimal-margin classiﬁer while the samples
closest to it are called the support vectors.
Linear SVMs. Assume that X+ and X−are linearly separable, meaning that
there exists a linear separating hyperplane wTx+b = 0 such that yi(wTxi+b) >
0, for all (xi, yi) ∈X. Then the functional margin can be formulated as

182
M. Chen et al.
γ = 2 min1≤i≤n 1/∥w∥|wTxi + b|.
Linear SVMs are committed to ﬁnding appropriate parameters (w, b) that max-
imize the functional margin while adhering to the constraints of separability,
which reduces equivalently to the following convex quadratic optimization prob-
lem [2] that can be eﬃciently solved by oﬀ-the-shelf packages for quadratic pro-
gramming:
minimize
w,b
1
2wTw
subject to yi(wTxi + b) ≥1,
i = 1, 2, . . . , n.
(1)
Lemma 1 (Correctness of SVMs [37]). Given positive samples X+ which are
linearly separable from negative samples X−, SVMs produce, under computations
of inﬁnite precision, a half-space h s.t. ∀x ∈X+. h(x) > 0 and ∀x ∈X−. h(x) <
0.
Corollary 1 (Separation of Convex Hulls [1]). The half-space h in Lemma 1
satisﬁes that ∀x ∈conv(X+). h(x) > 0 and ∀x ∈conv(X−). h(x) < 0.
Φ
Fig. 1. Mapping from a two-dimensional input space into a three-dimensional feature
space with linear separation thereof.
Nonlinear SVMs. When φ and ψ are formulas over nonlinear arithmetic, often
after sampling X, it is not possible to ﬁnd a linearly separable hyperplane in the
common variables. However, a nonlinear surface that can be described as a linear
hyperplane in the space of monomials of bounded degree may separate X+ and
X−. The above construction is generalized by introducing a transformation from
Rd to R ˜d, the vector space of monomials in the common variables up to some
bounded degree, with yi(wTxi + b) ≥1 in (1) replaced by yi(wTΦ(xi) + b) ≥1,
where Φ is a linear expression in monomials in the common variables up to a
bounded degree. Here, the vectors Φ(x) span the feature space.
Consider the Lagrangian dual [3] of the modiﬁed optimization problem:
minimize
α
1
2
n
i=1
n
j=1 αiαjyiyjΦ(xi)TΦ(xj) −
n
i=1 αi
subject to
n
i=1 αiyi = 0, and αi ≥0 for i = 1, 2, . . . , n.
A kernel function κ: Rd × Rd →R is deﬁned as κ(x, x′) = Φ(x)TΦ(x′). The
introduction of the dual problem and the kernel function [3] reduces the compu-
tational complexity essentially from O( ˜d) down to O(d). For the sake of post-

NIL: Learning Nonlinear Interpolants
183
verifying a candidate interpolant given by SVMs, we adopt an inhomogeneous
polynomial kernel function of the form
κ(x, x′) = (βxTx′ + θ)m,
where m is the polynomial degree describing complexity of the feature space, θ ≥
0 is a parameter trading oﬀthe inﬂuence of higher-order versus lower-order terms
in the polynomial, and β is a scalar parameter. Henceforth, the optimal-margin
classiﬁer (if there exists one) can be derived as wTΦ(x) = n
i=1 αiκ(xi, x) = 0,
with xi being a support vector iﬀαi > 0. In practice, usually a large amount of
αis turn out to be zero and this leads to a simple representation of a classiﬁer.
Figure 1 illustrates the intuitive idea of the transformation from the original
input space to the feature space. We will show in the sequel that the resulting
classiﬁer can be viewed as a candidate interpolant, while its optimal-margin
feature contributes to a certain “medium” logical strength of the interpolant,
which is thus robust to perturbations (in the feature space) in the formulae to
be interpolated.
3
Learning Interpolants
In this section, we present the NIL algorithm for synthesizing nontrivial (reverse)
Craig interpolants for the quantiﬁer-free theory of nonlinear arithmetic. It takes
as input a pair ⟨φ, ψ⟩of formulas in TP as well as a positive integer m, and
aims to generate an interpolant I of maximum degree m, i.e., I ∈R[x]m, if it
exists, such that φ |=TP I and I ∧ψ |=TP ⊥. Here, ⟨φ, ψ⟩can be decorated as
⟨φ(x, y), ψ(x, z)⟩with variables involved in the predicates, and thus x denotes
variables that are common to φ and ψ. In the sequel, we drop the subscript TP
in |=TP and [[·]]TP wherever the context is unambiguous.
Due to the decidability of the ﬁrst-order theory of real-closed ﬁelds estab-
lished by Tarski [44], TP admits quantiﬁer elimination (QE). This means that
the satisﬁability of any formula in TP can be decided (in doubly exponential
time in the number of variables for the worst case). If the formula is satisﬁable,
models satisfying the formula can also be constructed algorithmically (follow-
ing the same time complexity). Though the introduction of general forms of
transcendental functions renders the underlying theory undecidable, there does
exist certain extension of TP with transcendental functions (involving exponen-
tial functions, logarithms and trigonometric functions), e.g. that identiﬁed by
Strzebo´nski in [43] and references therein, which still admits QE. This allows a
straightforward extension of NIL to such a decidable fragment involving tran-
scendental functions. Speciﬁcally, the decidability remains when the transcen-
dental functions involved are real univariate exp-log functions [41] or tame ele-
mentary functions [42] which admit a real root isolation algorithm.

184
M. Chen et al.
Algorithm NIL: Learning nonlinear interpolant
input : φ and ψ in TP over common variables x;
m, degree of the polynomial kernel, and hence
maximum degree of the interpolant.
/* checking unsatisfiability */
1 if φ ∧ψ ̸|= ⊥then
/* no interpolant exists */
2
abort;
/* generating initial sample points */
3 ⟨X+, X−⟩←Sampling(φ, ψ);
/* counterexample-guided learning */
4 while ⊤do
/* generating a classifier by SVMs */
5
C ←SVM(X+, X−, m);
/* checking classification result */
6
if C = Failed then
/* no interpolant exists in R[x]m */
7
abort;
/* classifier as candidate interpolant */
8
else
9
I ←C;
/* valid interpolant found */
10
if φ |= I and I ∧ψ |= ⊥then
11
return I;
/* adding counterexamples */
12
else
13
X+ ←X+ ⊎FindInstance(φ ∧¬I);
14
X−←X−⊎FindInstance(I ∧ψ);
Fig. 2.
In NIL, a candidate inter-
polant (black line as its boundary) is
reﬁned to an actual one (red line as its
boundary) by adding a counterexample
(red dot). (Color ﬁgure online)
Fig. 3.
In NILδ, a counterexample
(red
dot)
stays
at
least
a
distance
of δ away from the candidate inter-
polant (black line as its boundary) to
be reﬁned, leading to an interpolant
(red line as its boundary) with toler-
ance δ. (Color ﬁgure online)
3.1
The Core Algorithm
The basic idea of NIL is to view interpolants as classiﬁers and use SVMs with
the kernel trick to perform eﬀective classiﬁcation. The algorithm is based on the
sampling-guessing-reﬁning technique: in each iteration, it is fed with a classi-
ﬁer (candidate interpolant) for a ﬁnite set of sample points from [[φ]] and [[ψ]]
(line 5), and verify the candidate (line 10) by checking the entailment problem
that deﬁnes an interpolant (as in Deﬁnition 1). If the veriﬁcation succeeds, the
interpolant is returned as the ﬁnal result. Otherwise, a set of counterexamples is
obtained (line 13 and 14) as new sample points to further reﬁne the classiﬁer. In
what follows, we explain the steps of the interpolation procedure in more detail.
Initial Sampling. The algorithm begins by checking the satisﬁability of φ ∧ψ.
If the formula is satisﬁable, it is then impossible to ﬁnd an interpolant, and the
algorithm stops declaring no interpolant exists.
Next, the algorithm attempts to sample points from both [[φ]] and [[ψ]]. This
initial sampling stage can usually be done eﬃciently using the Monte Carlo
method, e.g. by (uniformly) scattering a number of random points over certain
bounded range and then selecting those fall in [[φ]] and [[ψ]] respectively. However,
this method fails when one or both of the predicates is very unlikely to be sat-
isﬁed. One common example is when the predicate involves equalities. For such
situations, solving the satisﬁability problem using QE is guaranted to succeed
in producing the sample points.

NIL: Learning Nonlinear Interpolants
185
To meet the condition that the generated interpolant can only involve sym-
bols that are common to φ and ψ, we can project the points sampled from [[φ]]
(resp. [[ψ]]) to the space of x by simply dropping the components that pertain
to y (resp. z) and thereby obtain sample points in X+ (resp. X−).
Entailment Checking. The correctness of SVM given in Lemma 1 only guaran-
tees that the candidate interpolant separates the ﬁnite set of points sampled from
[[φ]] and [[ψ]], not necessarily the entirety of the two sets. Hence, post-veriﬁcation
by checking the entailment problem (line 10) is needed for the candidate to be
claimed as an interpolant of φ and ψ. This can be achieved by solving the equiv-
alent QE problems ∀x. φ(x, y)|x =⇒I(x) and ∀x. I(x) ∧ψ(x, z)|x =⇒⊥,
where ·|x is the projection to the common space over x. The candidate will be
returned as an actual interpolant if both formulae reduce to ⊤after eliminat-
ing the universal quantiﬁers. The satisﬁability checking at line 1 can be solved
analogously. Granted, the entailment checking can also be encoded in SMT tech-
niques by asking the satisﬁability of the negation of the universally quantiﬁed
predicates, however, limitations of current SMT solvers in nonlinear arithmetic
hinders them from being practically used in our framework, as demonstrated
later in Sect. 5.
Counterexample Generation. If a candidate interpolant cannot be veriﬁed as
an actual one, then at least one witness can be found as a counterexample to that
candidate, which can be added to the set of sample points in the next iteration to
reﬁne further candidates (cf. Fig. 2). Multiple counterexamples can be obtained
at a time thereby eﬀectively reducing the number of future iterations.
In general, we have little control over which counterexample will be returned
by QE. In the worst case, the counterexample can lie almost exactly on the
hyperplane found by SVM. This poses issues for the termination of the algorithm.
We will address this theoretical issue by slightly modifying the algorithm, as
explained in Sects. 3.3 and 4.
3.2
Comparison with the Na¨ıve QE-Based Method
Simply performing QE on ∃y. φ(x, y) yields already an interpolant for mutu-
ally contradictory φ and ψ. Such an interpolant is actually the strongest in the
sense of [13], which presents an ordered family of interpolation systems due to
the logical strength of the synthesized interpolants. Dually, the negation of the
result when performing QE over ∃z. ψ(x, z) is the weakest interpolant. However,
as argued by D’Silva et al. in [13], a good interpolant (approximation of φ or
ψ) –when computing invariants of transition systems using interpolation-based
model checking– should be coarse enough to enable rapid convergence but strong
enough to be contained within the weakest inductive invariant. In contrast, the
advantages of NIL are two-fold: ﬁrst, it produces better interpolants (in the above
sense) featuring “medium” strength (due to the way optimal-margin classiﬁer is
deﬁned) which are thus more eﬀective in practical use and furthermore resilient
to perturbations in φ and ψ (i.e., the robustness shown later in Sect. 6); second,
NIL always returns a single polynomial inequality as the interpolant which is

186
M. Chen et al.
often simpler than that derived from the na¨ıve QE-based method, where the
direct projection of φ(x, y) onto the common space over x can be as complex as
the original φ.
These issues can be avoided by combining this method with a template-based
approach, which in turn introduces fresh quantiﬁers over unknown parameters
to be eliminated. Note that in NIL the candidate interpolants I ∈R[x]m under
veriﬁcation are polynomials without unknown parameters, and therefore, in con-
trast to performing QE over an assumed template, the learning-based technique
can practically generate polynomial interpolants of higher degrees (with accept-
able rounds of iterations). For example, NIL is able to synthesize an interpolant
of degree 7 over 2 variables (depicted later in Fig. 4(b)), which would require a
polynomial template with
7+2
2

= 36 unknown parameters that goes far beyond
the capability of QE procedures.
On the other hand, performing QE within every iteration of the learning
process, for entailment checking and generating counterexamples, limits the eﬃ-
ciency of the proposed method, thereby conﬁning NIL currently to applications
only of small scales. Potential solutions to the eﬃciency bottleneck will be dis-
cussed in Sect. 5.
3.3
Variants of NIL
While the above basic algorithm is already eﬀective in practice (as demonstrated
in Sect. 5), it is guaranteed to terminate only when there is an interpolant with
positive functional margin between [[φ]] and [[ψ]]. In this section, we present two
variants of the algorithm that have nicer theoretical properties in cases where
the two sets are only separated by an interpolant with zero functional margin,
e.g., cases where [[φ]] and [[ψ]] share adjacent or even coincident boundaries.
Entailment Checking with Tolerance δ. When performing entailment check-
ing for a candidate interpolant I, instead of using, e.g., the formula p(x) ≥0
for I, we can introduce a tolerance of δ. That is, we check the satisﬁability of
φ ∧(p(x) < −δ) and (p(x) ≥δ) ∧ψ instead of the original φ ∧(p(x) < 0) and
(p(x) ≥0) ∧ψ. This means that a candidate that is an interpolant “up to a
tolerance of δ” will be returned as a true interpolant, which may be acceptable
in some applications. If the candidate interpolant is still not veriﬁed, the coun-
terexample is guaranteed to be at least a distance of δ away from the separating
hyperplane. Note the distance δ is taken in the feature space R ˜d, not in the origi-
nal space. We let NILδ(φ, ψ, m) denote the version of NIL with this modiﬁcation
(cf. Fig. 3). In the next section, we show NILδ(φ, ψ, m) terminates as long as
[[φ]] and [[ψ]] are bounded, including the case where they are separated only by
interpolants of functional margin zero.
Varying Tolerance During the Execution. A further reﬁnement of the algo-
rithm can be made by varying the tolerance δ during the execution. We also
introduce a bounding box B of the varying size to handle unbounded cases.
Deﬁne algorithm NIL∗
δ,B(φ, ψ, m) as follows. Let δ1 = δ and B1 = B. For each
iteration i, execute the core algorithm, except that the counterexample must be

NIL: Learning Nonlinear Interpolants
187
a distance of at least δi away from the separating boundary, and have absolute
value in each dimension at most B (both in R ˜d). After the termination of iter-
ation i, begin iteration i + 1 with δi+1 = δi/2 and Bi+1 = 2Bi. This continues
until an interpolant is found or until a pre-speciﬁed cutoﬀ. For any [[φ]] and [[ψ]]
(without the boundedness condition), this variant of the algorithm converges to
an interpolant in the limit, which will be made precise in the next section.
4
Soundness, Completeness and Convergence
In this section, we present theoretical results obtained for the basic NIL algo-
rithm and its variants. Proofs are available in the appendix of [6].
First, the basic algorithm is sound, as captured by Theorem 1.
Theorem 1 (Soundness of NIL). NIL(φ, ψ, m) terminates and returns I if
and only if I is an interpolant in R[x]m of φ and ψ.
Under certain conditions, the algorithm is also terminating (and hence com-
plete). We prove two such situations below. In both cases, we require bounded-
ness of the two sets that we want to separate. In the ﬁrst case, there exists an
interpolant with positive functional margin between the two sets.
Theorem 2 (Conditional Completeness of NIL). If [[φ]] and [[ψ]] are
bounded and there exists an interpolant in R[x]m of φ and ψ with positive func-
tional margin γ when mapped to R ˜d, then NIL(φ, ψ, m) terminates and returns
an interpolant I of φ and ψ.
The standard algorithm is not guaranteed to terminate when [[φ]] and [[ψ]] are
only separated by interpolants of functional margin zero. However, the modiﬁed
algorithm NILδ(φ, ψ, m) does terminate (with the cost that the resulting answer
is an interpolant with tolerance δ).
Theorem 3 (Completeness of NILδ with zero margin). If [[φ]] and [[ψ]] are
bounded, and δ > 0, then NILδ(φ, ψ, m) terminates. It returns an interpolant I
of φ and ψ with tolerance δ whenever such an interpolant exists.
By iteratively decreasing δ during the execution of the algorithm, as well as
introducing an iteratively increasing bounding box, as in NIL∗
δ,B(φ, ψ, m), we
can obtain more and more accurate candidate interpolants. We now show that
this algorithm converges to an interpolant without restrictions on φ and ψ. We
ﬁrst make this convergence property precise in the following deﬁnition.
Deﬁnition 2 (Convergence of a sequence of equations to an inter-
polant). Given two sets [[φ]] and [[ψ]] that we want to separate, and an inﬁnite
sequence of equations I1, I2, . . . , we say the sequence In converges to an inter-
polant of φ and ψ if, for each point p in the interior of [[φ]] or [[ψ]], there exists
some integer Kp such that Ik classiﬁes p correctly for all k ≥Kp.
Theorem 4 (Convergence of NIL∗
δ,B). Given two regions [[φ]] and [[ψ]]. Sup-
pose there exists an interpolant of φ and ψ, then the inﬁnite sequence of can-
didates produced by NIL∗
δ,B(φ, ψ, m) converges to an interpolant in the sense of
Deﬁnition 2.

188
M. Chen et al.
5
Implementation and Experiments
5.1
Implementation Issues
We have implemented the core algorithm NIL as a prototype1 in Wolfram Math-
ematica with LIBSVM [5] being integrated as an engine to perform SVM clas-
siﬁcations. Despite featuring no completeness for adjacent [[φ]] and [[ψ]] nor con-
vergence for unbounded [[φ]] or [[ψ]], the standard NIL algorithm yields already
promising results as shown later in the experiments. Key Mathematica functions
that are utilized include Reduce, for entailment checking, e.g., the unsatisﬁa-
bility checking of φ ∧ψ and the post-veriﬁcation of a candidate interpolant,
and FindInstance, for generating counterexamples and sampling initial points
(when the random sampling strategy fails). The Reduce command implements a
decision procedure for TP and its appropriate extension to catering for transcen-
dental functions (cf. [43]) based on cylindrical algebraic decomposition (CAD),
due to Collins [9]. The underlying quantiﬁer-elimination procedure, albeit induc-
ing rather high computation complexity, cannot in practice be replaced by SMT-
solving techniques (by checking the negation of a universally quantiﬁed predi-
cate) as in the linear arithmetic. For instance, the oﬀ-the-shelf SMT solver Z3
fails to accomplish our tasks particularly when the coeﬃcients occurring in the
entailment problem to be checked get larger2.
Numerical Errors and Rounding. LIBSVM conducts ﬂoating-point compu-
tations for solving the optimization problems induced by SVMs and consequently
yields numerical errors occurring in the candidate interpolants. Such numerical
errors may block an otherwise valid interpolant from being veriﬁed as an actual
one and additionally bring down the simplicity and thereby the eﬀectiveness of
the synthesized interpolant, thus not very often proving humans with clear-cut
understanding. This is a common issue for approaches that reduce the inter-
polation problem to numerical solving techniques, e.g. SDP solvers exploited
in [11,14,33], while an established method to tackle it is known as rational
recovery [29,47], which retrieves the nearest rational number from the continued
fraction representation of its ﬂoating-point approximation at any given accuracy
(see e.g. [47] for theoretical guarantees and [33] for applications in interpola-
tion). The algorithm implementing rational recovery has been integrated in our
implementation and the consequent beneﬁts are two-fold: (i) NIL can now cope
with interpolation tasks where only exact coeﬃcients suﬃce to constitute an
actual interpolant while any numerical error therein will render the interpolant
invalid, e.g., cases where [[φ]] and [[ψ]] share parallel, adjacent, or even coincident
boundaries, as demonstrated later by examples with ID 10–17 in Table 1; (ii)
rationalizing coeﬃcients moreover facilitates simpliﬁcations over all of the can-
didate interpolants and therefore practically accelerating the entailment checking
and counterexample generation processes, which in return yields simpler inter-
polants, as shown in Table 2 in the following section.
1 Available at http://lcs.ios.ac.cn/∼chenms/tools/NIL.tar.bz2.
2 As can be also observed at https://github.com/Z3Prover/z3/issues/1765.

NIL: Learning Nonlinear Interpolants
189
5.2
Benchmark and Experimental Results
Table 1 collects a group of benchmark examples from the literature on synthe-
sizing nonlinear interpolants as well as some geometrically contrived ones. All of
the experiments have been evaluated on a 3.6 GHz Intel Core-i7 processor with
8 GB RAM running 64-bit Ubuntu 16.04.
In Table 1, we group the set of examples into four categories comprising 20
cases in total. For each example, ID numbers the case, φ, ψ and I represent the
two formulas to be interpolated and the synthesized interpolant by our method
respectively, while Time/s indicates the total time in seconds for interpolation.
The categories are described as follows, and the visualization of a selected set of
typical examples thereof is further depicted in Fig. 4.
Cat. I: with/without rounding. This category includes 9 cases, for which
our method generates the polynomial interpolants correctly with or without
the rounding operation.
Cat. II: with rounding. For cases 10 to 17 in this category, where [[φ]] and [[ψ]]
share parallel, adjacent, or even coincident boundaries, our method produces
interpolants successfully with the rouding process based on rational recovery.
Cat. III: beyond polynomials. This category encloses two cases beyond the
theory TP of polynomials: for case 18, a veriﬁed polynomial interpolant is
obtained in spite of the transcendental term in ψ; while for case 19, the SVM
classiﬁcation fails since [[φ]] and [[ψ]] are not linearly separable in any ﬁnite-
dimensional feature space and hence no polynomial interpolant exists for this
example. Note that our counterexample-guided learning framework admits a
straightforward extension to a decidable fragment of more general nonlinear
theories involving transcendental functions, as investigated in [43].
Cat. IV: unbalanced. The case 20, called Unbalanced, instantiates a particular
scenario where φ and ψ have extraordinary “unbalanced” number of models
that make them true respectively. For this example, there are an inﬁnite num-
ber of models satisfying φ yet one single model (i.e., x = 0) satisfying ψ. The
training process in SVMs may fail when encountering extremely unbalanced
number of positive/negative samples. This is solved by specifying a weight
factor for the positive set of samples as the number of negative ones, and
dually for the other way around, to balance biased number of training sam-
ples before triggering the classiﬁcation. Such a balancing trick is supported
in LIBSVM.
Remark that examples named CAV13-1/3/4 are taken from [11] (and the lat-
ter two originally from [28] and [18] respectively), where interpolation is applied
to discovering inductive invariants in the veriﬁcation of programs and hybrid
systems. For instance, CAV13-3 is a program fragment describing an acceler-
ating car and the synthesized interpolant by NIL suﬃces to prove the safety
property of the car concerning its velocity.
Applicability and Comparison with Existing Approaches. As shown in
Table 1, our learning-based technique succeeds in all of the benchmark examples

190
M. Chen et al.
Table 1. Benckmark examples for synthesizing nonlinear interpolants.

NIL: Learning Nonlinear Interpolants
191
Fig. 4. Visualization in NIL on a selected set of examples. Legends: gray region: [[φ]],
blue region: [[ψ]], pink region: [[I]] with a valid interpolant I, red dots: X+, blue dots:
X−, circled dots: support vectors. Sample points are hidden in 3D-graphics for a clear
presentation.
that admit polynomial interpolants. Due to theoretical limitations of existing
approaches as elaborated in Sect. 1, none of the aforementioned methods can
cope with as many cases in Table 1 as NIL can. For instances, the Twisted
example as depicted in Fig. 4(c) falls beyond the scope of concave quadratic
formulas and thus cannot be addressed by the approach in [14], while the Parallel
parabola example as shown in Fig. 4(d) needs an inﬁnite combination of linear
constraints as an interpolant when performing the technique in [15] and hence not
of practical use, to name just a few. Moreover, we list in Table 2 a comparison
of the synthesized interpolants against works where the benchmark examples
are collected from. As being immediately obvious from Table 2, our technique

192
M. Chen et al.
Table 2. Comparison of the synthesized interpolants.
often produces interpolants of simpler forms, particularly for examples CAV13-
2, CAV13-4 and TACAS16. Such a simplicity beneﬁts from both the rounding
eﬀect and the form of interpolant (i.e., a single polynomial inequality) that we
tend to construct.
Bottleneck of Eﬃciency and Potential Solutions. The current implemen-
tation of NIL works promisingly for small examples; it does not scale to interpo-
lation problems with large numbers of common variables, as reported in Table 1.
The bottleneck stems from quantiﬁer eliminations performed within every iter-
ation of the learning process, for entailment checking and generating counterex-
amples. We pose here several potential solutions that are expected to signiﬁ-
cantly reduce computational eﬀorts: (i) substitute general purpose QE procedure
that perform CAD by the so-called variant quantiﬁer-elimination (VQE) algo-
rithm [21], which features singly-exponential complexity in the number of vari-
ables. This however requires a careful inspection of whether our problem meets
the geometric conditions imposed by VQE; (ii) incorporate relaxation schemes,
e.g., Lagrangian relaxation and sum-of-squares decompositions [34], and com-
plement with QE only when the relaxation fails to produce desired results.

NIL: Learning Nonlinear Interpolants
193
Fig. 5. ϵ-Face: introducing perturbations (with ϵ up to 0.5) in the Face example. The
synthesized interpolant is resilient to any ϵ-perturbation in the radii satisfying −0.5 ≤
ϵ ≤0.5.
6
Taming Perturbations in Parameters
An interpolant synthesized by the SVM-based technique features inherent
robustness due to the way optimal-margin classiﬁer is deﬁned (Sect. 2). That
is, the validity of such an interpolant is not easily perturbed by changes (in
the feature space) in the formulae to be interpolated. It is straightforward in
NIL to deal with interpolation problems under explicitly speciﬁed perturba-
tions, which are treated as constraints over fresh variables. An example named
ϵ-Face is depicted in Fig. 5, which perturbs ⟨φ, ψ⟩in the Face example as
φ = −0.5 ≤ϵ1 ≤0.5∧((x+4)2 +y2 −(1+ϵ1)2 ≤0∨(x−4)2 +y2 −(1+ϵ1)2 ≤0)
and ψ = −0.5 ≤ϵ2 ≤0.5 ∧x2 + y2 −64 ≤0 ∧(x + 4)2 + y2 −(3 + ϵ2)2 ≥
0 ∧(x −4)2 + y2 −(3 + ϵ2)2 ≥0. The synthesized interpolant over common
variables of φ and ψ is
x4
139 + x3y
268 + x2 
y2
39 −11
36

+ x

−y3
52 −y2
157 −y
52 −
1
116

+
y4
25 −
y3
182 + 2y2
19 −
y
218 + 1 < 0 which is hence resilient to any ϵ-perturbation in
the radii satisfying −0.5 ≤ϵ ≤0.5, as illustrated in Fig. 5(b).
7
Conclusions
We have presented a uniﬁed, counterexample-guided method named NIL for gen-
erating polynomial interpolants over the general quantiﬁer-free theory of non-
linear arithmetic. Our method is based on classiﬁcation techniques with space
transformations and kernel tricks as established in the community of machine-
learning. We proved the soundness of NIL and proposed suﬃcient conditions for
its completeness and convergence. The applicability and eﬀectiveness of our tech-
nique are demonstrated experimentally on a collection of representative bench-
marks from the literature, including those extracted from program veriﬁcation.
Experimental results indicated that our method suﬃces to address more inter-
polation tasks, including those with perturbations in parameters, and in many
cases synthesizes simpler interpolants compared with existing approaches.

194
M. Chen et al.
For future work, we would like to improve the eﬃciency of NIL by substitut-
ing the general purpose quantiﬁer-elimination procedure with alternative meth-
ods previously discussed in Sect. 5. An extension of our approach to cater for
the combination of nonlinear arithmetic with EUFs, by resorting to predicate-
abstraction techniques [23], will be of particular interest. Additionally, we plan to
investigate the performance of NIL over diﬀerent classiﬁcation techniques, e.g.,
the widespread regression-based methods [19], though SVMs are expected to be
more competent concerning the robustness and predictability, as also observed
in [37].
References
1. Bennett, K.P., Bredensteiner, E.J.: Duality and geometry in SVM classiﬁers. In:
ICML 2000, pp. 57–64 (2000)
2. Bishop, C.M.: Pattern Recognition and Machine Learning, pp. 326–328. Springer,
New York (2006)
3. Boser, B.E., Guyon, I., Vapnik, V.: A training algorithm for optimal margin clas-
siﬁers. In: COLT 1992, pp. 144–152 (1992)
4. Bourbaki, N.: Topological Vector Spaces. Elements of Mathematics. Springer, Hei-
delberg (1987). https://doi.org/10.1007/978-3-642-61715-7
5. Chang, C., Lin, C.: LIBSVM: a library for support vector machines. ACM TIST
2(3), 27:1–27:27 (2011)
6. Chen, M., Wang, J., An, J., Zhan, B., Kapur, D., Zhan, N.: NIL: learning nonlinear
interpolants (full version). http://lcs.ios.ac.cn/∼chenms/papers/CADE-27 FULL.
pdf
7. Cimatti, A., Griggio, A., Sebastiani, R.: Eﬃcient interpolant generation in satis-
ﬁability modulo theories. In: Ramakrishnan, C.R., Rehof, J. (eds.) TACAS 2008.
LNCS, vol. 4963, pp. 397–412. Springer, Heidelberg (2008). https://doi.org/10.
1007/978-3-540-78800-3 30
8. Clarke, E., Grumberg, O., Jha, S., Lu, Y., Veith, H.: Counterexample-guided
abstraction reﬁnement. In: Emerson, E.A., Sistla, A.P. (eds.) CAV 2000. LNCS,
vol. 1855, pp. 154–169. Springer, Heidelberg (2000). https://doi.org/10.1007/
10722167 15
9. Collins, G.E.: Quantiﬁer elimination for real closed ﬁelds by cylindrical algebraic
decompostion. In: Brakhage, H. (ed.) GI-Fachtagung 1975. LNCS, vol. 33, pp.
134–183. Springer, Heidelberg (1975). https://doi.org/10.1007/3-540-07407-4 17
10. Craig, W.: Linear reasoning. A new form of the Herbrand-Gentzen theorem. J.
Symb. Log. 22(3), 250–268 (1957)
11. Dai, L., Xia, B., Zhan, N.: Generating non-linear interpolants by semideﬁnite pro-
gramming. In: Sharygina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp.
364–380. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-39799-
8 25
12. Dathathri, S., Arechiga, N., Gao, S., Murray, R.M.: Learning-based abstractions
for nonlinear constraint solving. In: IJCAI 2017, pp. 592–599 (2017)
13. D’Silva, V., Kroening, D., Purandare, M., Weissenbacher, G.: Interpolant strength.
In: Barthe, G., Hermenegildo, M. (eds.) VMCAI 2010. LNCS, vol. 5944, pp. 129–
145. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-11319-2 12

NIL: Learning Nonlinear Interpolants
195
14. Gan, T., Dai, L., Xia, B., Zhan, N., Kapur, D., Chen, M.: Interpolant synthesis
for quadratic polynomial inequalities and combination with EUF. In: Olivetti, N.,
Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol. 9706, pp. 195–212. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-40229-1 14
15. Gao, S., Zuﬀerey, D.: Interpolants in nonlinear theories over the reals. In: Chechik,
M., Raskin, J.-F. (eds.) TACAS 2016. LNCS, vol. 9636, pp. 625–641. Springer,
Heidelberg (2016). https://doi.org/10.1007/978-3-662-49674-9 41
16. Gilbert, S.: A nullstellensatz and a positivstellensatz in semialgebraic geometry.
Math. Ann. 207(2), 87–97 (1974)
17. Graf, S., Saidi, H.: Construction of abstract state graphs with PVS. In: Grum-
berg, O. (ed.) CAV 1997. LNCS, vol. 1254, pp. 72–83. Springer, Heidelberg (1997).
https://doi.org/10.1007/3-540-63166-6 10
18. Gulavani, B.S., Chakraborty, S., Nori, A.V., Rajamani, S.K.: Automatically reﬁn-
ing abstract interpretations. In: Ramakrishnan, C.R., Rehof, J. (eds.) TACAS 2008.
LNCS, vol. 4963, pp. 443–458. Springer, Heidelberg (2008). https://doi.org/10.
1007/978-3-540-78800-3 33
19. Hastie, T., Tibshirani, R., Friedman, J.: The Elements of Statistical Learning:
Data Mining, Inference, and Prediction. SSS, 2nd edn. Springer, New York (2009).
https://doi.org/10.1007/978-0-387-84858-7
20. Henzinger, T.A., Jhala, R., Majumdar, R., McMillan, K.L.: Abstractions from
proofs. In: POPL 2004, pp. 232–244 (2004)
21. Hong, H., Din, M.S.E.: Variant quantiﬁer elimination. J. Symb. Comput. 47(7),
883–901 (2012)
22. Jha, S., Gulwani, S., Seshia, S.A., Tiwari, A.: Oracle-guided component-based
program synthesis. In: ICSE 2010, pp. 215–224 (2010)
23. Jhala, R., Podelski, A., Rybalchenko, A.: Predicate abstraction for program ver-
iﬁcation. In: Clarke, E., Henzinger, T., Veith, H., Bloem, R. (eds.) Handbook of
Model Checking, pp. 447–491. Springer, Cham (2018). https://doi.org/10.1007/
978-3-319-10575-8 15
24. Jung, Y., Lee, W., Wang, B.-Y., Yi, K.: Predicate generation for learning-based
quantiﬁer-free loop invariant inference. In: Abdulla, P.A., Leino, K.R.M. (eds.)
TACAS 2011. LNCS, vol. 6605, pp. 205–219. Springer, Heidelberg (2011). https://
doi.org/10.1007/978-3-642-19835-9 17
25. Kapur, D., Majumdar, R., Zarba, C.G.: Interpolation for data structures. In: FSE
2006, pp. 105–116 (2006)
26. Kov´acs, L., Voronkov, A.: Interpolation and symbol elimination. In: Schmidt, R.A.
(ed.) CADE 2009. LNCS (LNAI), vol. 5663, pp. 199–213. Springer, Heidelberg
(2009). https://doi.org/10.1007/978-3-642-02959-2 17
27. Kraj´ıˇcek, J.: Interpolation theorems, lower bounds for proof systems, and indepen-
dence results for bounded arithmetic. J. Symb. Log. 62(2), 457–486 (1997)
28. Kupferschmid, S., Becker, B.: Craig interpolation in the presence of non-linear
constraints. In: Fahrenberg, U., Tripakis, S. (eds.) FORMATS 2011. LNCS, vol.
6919, pp. 240–255. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-
642-24310-3 17
29. Lang, S.: Introduction to Diophantine Approximations: New Expanded Edition.
Springer, New York (2012)
30. McMillan, K.L.: Interpolation and SAT-based model checking. In: Hunt, W.A.,
Somenzi, F. (eds.) CAV 2003. LNCS, vol. 2725, pp. 1–13. Springer, Heidelberg
(2003). https://doi.org/10.1007/978-3-540-45069-6 1

196
M. Chen et al.
31. McMillan, K.L.: An interpolating theorem prover. In: Jensen, K., Podelski, A.
(eds.) TACAS 2004. LNCS, vol. 2988, pp. 16–30. Springer, Heidelberg (2004).
https://doi.org/10.1007/978-3-540-24730-2 2
32. McMillan, K.L.: Quantiﬁed invariant generation using an interpolating saturation
prover. In: Ramakrishnan, C.R., Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp.
413–427. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-78800-
3 31
33. Okudono, T., Nishida, Y., Kojima, K., Suenaga, K., Kido, K., Hasuo, I.: Sharper
and simpler nonlinear interpolants for program veriﬁcation. In: Chang, B.-Y.E.
(ed.) APLAS 2017. LNCS, vol. 10695, pp. 491–513. Springer, Cham (2017).
https://doi.org/10.1007/978-3-319-71237-6 24
34. Parrilo, P.A.: Semideﬁnite programming relaxations for semialgebraic problems.
Math. Program. 96(2), 293–320 (2003)
35. Pudl´ak, P.: Lower bounds for resolution and cutting plane proofs and monotone
computations. J. Symb. Log. 62(3), 981–998 (1997)
36. Rybalchenko, A., Sofronie-Stokkermans, V.: Constraint solving for interpolation.
In: Cook, B., Podelski, A. (eds.) VMCAI 2007. LNCS, vol. 4349, pp. 346–362.
Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-69738-1 25
37. Sharma, R., Nori, A.V., Aiken, A.: Interpolants as classiﬁers. In: Madhusudan, P.,
Seshia, S.A. (eds.) CAV 2012. LNCS, vol. 7358, pp. 71–87. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-31424-7 11
38. Sofronie-Stokkermans, V.: Interpolation in local theory extensions. In: Furbach, U.,
Shankar, N. (eds.) IJCAR 2006. LNCS (LNAI), vol. 4130, pp. 235–250. Springer,
Heidelberg (2006). https://doi.org/10.1007/11814771 21
39. Sofronie-Stokkermans, V.: On interpolation and symbol elimination in theory
extensions. In: Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol.
9706, pp. 273–289. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
40229-1 19
40. Solar-Lezama, A., Rabbah, R.M., Bod´ık, R., Ebcioglu, K.: Programming by sketch-
ing for bit-streaming programs. In: PLDI 2005, pp. 281–294 (2005)
41. Strzebo´nski, A.W.: Real root isolation for exp-log functions. In: ISSAC 2008, pp.
303–314 (2008)
42. Strzebo´nski, A.W.: Real root isolation for tame elementary functions. In: ISSAC
2009, pp. 341–350 (2009)
43. Strzebo´nski, A.W.: Cylindrical decomposition for systems transcendental in the
ﬁrst variable. J. Symb. Comput. 46(11), 1284–1290 (2011)
44. Tarski, A.: A Decision Method for Elementary Algebra and Geometry. University
of California Press, Berkeley (1951)
45. Vladimir, V.: Pattern recognition using generalized portrait method. Autom.
Remote Control 24, 774–780 (1963)
46. Yorsh, G., Musuvathi, M.: A combination method for generating interpolants. In:
Nieuwenhuis, R. (ed.) CADE 2005. LNCS (LNAI), vol. 3632, pp. 353–368. Springer,
Heidelberg (2005). https://doi.org/10.1007/11532231 26
47. Zhang, J., Feng, Y.: Obtaining exact value by approximate computations. Sci.
China Ser. A Math. 50(9), 1361 (2007)

ENIGMA-NG: Eﬃcient Neural
and Gradient-Boosted Inference
Guidance for E
Karel Chvalovsk´y(B), Jan Jakub˚uv, Martin Suda, and Josef Urban
Czech Technical University in Prague, Prague, Czech Republic
karel@chvalovsky.cz
Abstract. We describe an eﬃcient implementation of given clause selec-
tion in saturation-based automated theorem provers, extending the pre-
vious ENIGMA approach. Unlike in the ﬁrst ENIGMA implementation
where a fast linear classiﬁer is trained and used together with manually
engineered features, we have started to experiment with more sophisti-
cated state-of-the-art machine learning methods such as gradient boosted
trees and recursive neural networks. In particular, the latter approach
poses challenges in terms of eﬃciency of clause evaluation, however, we
show that deep integration of the neural evaluation with the ATP data-
structures can largely amortize this cost and lead to competitive real-time
results. Both methods are evaluated on a large dataset of theorem prov-
ing problems and compared with the previous approaches. The resulting
methods improve on the manually designed clause guidance, providing
the ﬁrst practically convincing application of gradient-boosted and neu-
ral clause guidance in saturation-style automated theorem provers.
1
Introduction
Automated theorem provers (ATPs) have been developed for decades by manu-
ally designing proof calculi and search heuristics. Their power has been growing
and they are already very useful, e.g., as parts of large interactive theorem prov-
ing (ITP) veriﬁcation toolchains (hammers) [5]. On the other hand, with small
exceptions, ATPs are still signiﬁcantly weaker than trained mathematicians in
ﬁnding proofs in most research domains.
Recently, machine learning over large formal corpora created from ITP
libraries [24,32,42] has started to be used to develop guidance of ATP sys-
tems [2,30,44]. This has already produced strong systems for selecting rele-
vant facts for proving new conjectures over large formal libraries [1,4,13]. More
recently, machine learning has also started to be used to guide the internal search
of the ATP systems. In sophisticated saturation-style provers this has been done
Supported by the ERC Consolidator grant no. 649043 AI4REASON, and by the Czech
project AI&Reasoning CZ.02.1.01/0.0/0.0/15 003/0000466 and the European Regional
Development Fund.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 197–215, 2019.
https://doi.org/10.1007/978-3-030-29436-6_12

198
K. Chvalovsk´y et al.
by feedback loops for strategy invention [21,38,43] and by using supervised learn-
ing [19,31] to select the next given clause [35]. In the simpler connection tableau
systems such as LeanCoP [34], supervised learning has been used to choose
the next tableau extension step [25,45] and ﬁrst experiments with Monte-Carlo
guided proof search [12] and reinforcement learning [26] have been done.1
In this work, we add two state-of-the-art machine learning methods to the
ENIGMA [19,20] algorithm that eﬃciently guides saturation-style proof search.
The ﬁrst one trains gradient boosted trees on eﬃciently extracted manually
designed (handcrafted) clause features. The second method removes the need
for manually designed features, and instead uses end-to-end training of recursive
neural networks. Such architectures, when implemented naively, are typically
expensive and may be impractical for saturation-style ATP. We show that deep
integration of the neural evaluation with the ATP data-structures can largely
amortize this cost, allowing competitive performance.
The rest of the paper is structured as follows. Section 2 introduces saturation-
based automated theorem proving with the emphasis on machine learning.
Section 3 brieﬂy summarizes our previous work with handcrafted features in
ENIGMA and then extends the previously published ENIGMA with addi-
tional classiﬁers based on decision trees (Sect. 3.3) and simple feature hashing
(Sect. 3.4). Section 4 presents our new approach of applying neural networks for
ATP guidance. Section 5 provides experimental evaluation of our work. We con-
clude in Sect. 6.
2
Automated Theorem Proving with Machine Learning
State-of-the-art saturation-based automated theorem provers (ATPs) for ﬁrst-
order logic (FOL), such as E [40] and Vampire [28] are today’s most advanced
tools for general reasoning across a variety of mathematical and scientiﬁc
domains. Many ATPs employ the given clause algorithm, translating the input
FOL problem T ∪{¬C} into a refutationally equivalent set of clauses. The search
for a contradiction is performed maintaining sets of processed (P) and unpro-
cessed (U) clauses. The algorithm repeatedly selects a given clause g from U,
moves g to P, and extends U with all clauses inferred with g and P. This process
continues until a contradiction is found, U becomes empty, or a resource limit is
reached. The size of the unprocessed set U grows quickly and it is a well-known
fact that the selection of the right given clause is crucial for success. Machine
learning from a large number of proofs and proof searches may help guide the
selection of the given clauses.
E allows the user to select a proof search strategy S to guide the proof search.
An E strategy S speciﬁes parameters such as term ordering, literal selection
function, clause splitting, paramodulation setting, premise selection, and, most
importantly for us, the given clause selection mechanism. The given clause selec-
tion in E is implemented using a list of priority queues. Each priority queue stores
1 Other, less immediately relevant, previous work on combining machine learning with
automated theorem proving includes, e.g., [6,8,9,11,39].

ENIGMA-NG: Eﬃcient Neural and Gradient-Boosted Inference
199
all the generated clauses in a speciﬁc order determined by a clause weight func-
tion. The clause weight function assigns a numeric (real) value to each clause,
and the clauses with smaller weights (“lighter clauses”) are prioritized. To select
a given clause, one of the queues is chosen in a round robin manner, and the
clause at the front of the chosen queue gets processed. Each queue is additionally
assigned a frequency which amounts to the relative number of clause selections
from that particular queue. Frequencies can be used to prefer one queue over
another. We use the following notation to denote the list of priority queues with
frequencies fi and weight functions Wi:
(f1 ∗W1, . . . , fk ∗Wk).
To facilitate machine learning research, E implements an option under which
each successful proof search gets analyzed and outputs a list of clauses annotated
as either positive or negative training examples. Each processed clause that is
present in the ﬁnal proof is classiﬁed as positive. On the other hand, processing
of clauses not present in the ﬁnal proof was redundant, hence they are classiﬁed
as negative. Our goal is to learn such classiﬁcation (possibly conditioned on the
problem and its features) in a way that generalizes and leads to solving previously
unsolved related problems.
Given a set of problems P, we can run E with a strategy S and obtain positive
and negative training data T from each of the successful proof searches. In this
work, we use three diﬀerent machine learning methods to learn the clause classi-
ﬁcation given by T , each method yielding a classiﬁer or model M. The concrete
structure of M depends on the machine learning method used, as explained in
detailed below. With any method, M provides a function to compute the weight
of an arbitrary clause. This weight function is then used in E to guide further
proof runs.
A model M can be used in E in diﬀerent ways. We use two methods to
combine M with a strategy S. Either (1) we use M to select all the given clauses,
or (2) we combine M with the given clause guidance from S so that roughly half
of the clauses are selected by M. We denote the resulting E strategies as (1)
S ⊙M, and (2) S ⊕M. The two strategies are equal up to the priority queues
for given clause selection which are changed (⇝) as follows.
in S ⊙M : (f1 ∗W1, . . . , fk ∗Wk) ⇝(1 ∗M),
in S ⊕M : (f1 ∗W1, . . . , fk ∗Wk) ⇝(( fi) ∗M, f1 ∗W1, . . . , fk ∗Wk).
The strategy S ⊕M usually performs better in practice as it helps to counter
overﬁtting by combining powers with the original strategy S. The strategy S⊙M
usually provides additional proved problems, gaining additional training data,
and it is useful for the evaluation of the training phase. When S ⊙M performs
better than S, it indicates that M has learned the training data well. When it
performs much worse, it indicates that M is not very well-trained. The strategy
S ⊕M should always perform better than S, otherwise the guidance of M is
not useful. Additional indication of successful training can be obtained from the
number of clauses processed during a successful proof search. The strategy S⊙M

200
K. Chvalovsk´y et al.
should run with much fewer processed clauses, in some cases even better than
S ⊕M, as the original S might divert the proof search. In the best case, when
M would learn to guide for certain problem perfectly, the number of processed
clauses would not need to exceed the length of the proof.
It is important to combine a model M only with a “compatible” strategy
S. For example, let us consider a model M trained on samples obtained with
another strategy S0 which has a diﬀerent term ordering than S. As the term
ordering can change term normal forms, the clauses encountered in the proof
search with S might look quite diﬀerent from the training clauses. This may be
an issue unless the trained models are independent of symbol names, which is
not (yet) our case. Additional complications might arise as term orderings and
literal selection might change the proof space and the original proofs might not
be reachable. Hence we only combine M with the strategy S which provided the
examples on which M was trained.
3
ATP Guidance with Handcrafted Clause Features
In order to employ a machine learning method for ATP guidance, ﬁrst-order
clauses need to be represented in a format recognized by the selected learning
method. A common approach is to manually extract a ﬁnite set of various prop-
erties of clauses called features, and to encode these clause features by a ﬁxed-
length numeric vector. Various machine learning methods can handle numeric
vectors and their success heavily depends on the selection of correct clause fea-
tures. In this section, we work with handcrafted clause features which, we believe,
capture information important for ATP guidance.
ENIGMA [19,20] is our eﬃcient learning-based method for guiding given
clause selection in saturation-based ATPs. Sections 3.1 and 3.2 brieﬂy summa-
rizes our previous work. Sections 3.3 and 3.4 describe extensions, ﬁrst presented
in this work.
3.1
ENIGMA Clause Features
So far the development of ENIGMA has focused on fast and practically usable
methods, allowing E users to directly beneﬁt from our work. Various possible
choices of eﬃcient clause features for theorem prover guidance have been exper-
imented with [19,20,26,27]. The original ENIGMA [19] uses term-tree walks
of length 3 as features, while the second version [20] reaches better results by
employing various additional features. In particular, the following types of fea-
tures are used (see [19, Sec. 3.2] and [20, Sec. 2] for details):
Vertical Features are (top-down-)oriented term-tree walks of length 3. For
example, the unit clause P(f(a, b)) contains only features (P, f, a) and
(P, f, b).
Horizontal Features are horizontal cuts of a term tree. For every term
f(t1, . . . , tn) in the clause, we introduce the feature f(s1, . . . , sn) where si
is the top-level symbol of ti.

ENIGMA-NG: Eﬃcient Neural and Gradient-Boosted Inference
201
Symbol Features are various statistics about clause symbols, namely, the num-
ber of occurrences and the maximal depth for each symbol.
Length Features count the clause length and the numbers of positive and
negative literals.
Conjecture Features embed information about the conjecture being proved
into the feature vector. In this way, ENIGMA can provide conjecture-
dependent predictions.
Since there are only ﬁnitely many features in any training data, the features
can be serially numbered. This numbering is ﬁxed for each experiment. Let n
be the number of diﬀerent features appearing in the training data. A clause C
is translated to a feature vector ϕC whose i-th member counts the number of
occurrences of the i-th feature in C. Hence every clause is represented by a sparse
numeric vector of length n.
With conjecture features, instead of using the vector ϕC of length n, we use
a vector (ϕC, ϕG) of length 2n where ϕG contains the features of the conjecture
G. For a training clause C, G corresponds to the conjecture of the proof search
where C was selected as a given clause. When classifying a clause C during
proof search, G corresponds to the conjecture currently being proved. When
the conjecture consists of several clauses, their vectors are computed separately
and then summed (except for features corresponding to maxima, such as the
maximal symbol depth, where maximum is taken instead).
3.2
ATP Guidance with Fast Linear Classiﬁers
ENIGMA has so far used simple but fast linear classiﬁers such as linear SVM
and logistic regression eﬃciently implemented by the LIBLINEAR open source
library [10]. In order to employ them, clause representation by numeric feature
vectors described above in Sect. 3.1 is used. Clausal training data T are trans-
lated to a set of ﬁxed-size labeled vectors. Each (typically sparse) vector of length
n is labeled either as positive or negative.
The labeled numeric vectors serve as an input to LIBLINEAR which, after
the training, outputs a model M consisting mainly of a weight vector w of length
n. The main cost in classifying a clause C consists in computing its feature vector
ϕC and its dot product with the weight vector p = ϕC · w. ENIGMA the assigns
to the positively classiﬁed clauses (i.e., p ≥0) a chosen small weight (1.0) and a
higher weight (10.0) to the negatively classiﬁed ones (i.e., p < 0). This weight is
then used inside E to guide given clause selection as described in Sect. 2.
The training data obtained from the proof runs are typically not balanced
with respect to the number of positive and negative examples. Usually, there
are many more negative examples and the method of Accuracy-Balancing Boost-
ing [20] was found useful in practice to improve precision on the positive training
data. This is done as follows. Given training data T we create a LIBLINEAR
classiﬁer M, test M on the training data, and collect the positives mis-classiﬁed
by M. We then repeat (boost) the mis-classiﬁed positives in the training data,
yielding updated T1 and an updated classiﬁer M1. We iterate this process, and

202
K. Chvalovsk´y et al.
with every iteration, the accuracy on the positive samples increases, while the
accuracy on the negatives typically decreases. We ﬁnish the boosting when the
positive accuracy exceeds the negative one. See [20, Sec. 2] for details.
3.3
ATP Guidance with Gradient Boosted Trees
Fast linear classiﬁers together with well-designed features have been used with
good results for a number of tasks in areas such as NLP [23]. However, more
advanced learning models have been recently developed, showing improved per-
formance on a number of tasks, while maintaining eﬃciency. One such method is
gradient boosted trees and, in particular, their implementation in the XGBoost
library [7]. Gradient boosted trees are ensembles of decision trees trained by tree
boosting.
The format of the training and evaluation data used by XGBoost is the
same as the input used by LIBLINEAR (sparse feature vectors). Hence, we use
practically the same approach for obtaining the positive and negative training
examples, extracting their features, and clause evaluation during proof runs as
described in Sects. 3.1 and 3.2. XGBoost, however, does not require the accuracy-
balancing boosting. This is because XGBoost can deal with unbalanced training
data by setting the ratio of positive and negative examples.2
The model M produced by XGBoost consists of a set (ensemble [37]) of deci-
sion trees. The inner nodes of the decision trees consist of conditions on feature
values, while the leafs contain numeric scores. Given a vector ϕC representing a
clause C, each tree in M is navigated to a unique leaf using the values from ϕC,
and the corresponding leaf scores are aggregated across all trees. The ﬁnal score
is translated to yield the probability that ϕC represents a positive clause. When
using M as a weight function in E, the probabilities are turned into binary clas-
siﬁcation, assigning weight 1.0 for probabilities ≥0.5 and weight 10.0 otherwise.
Our experiments with scaling of the weight by the probability did not yet yield
improved functionality.
3.4
Feature Hashing
In the previous version of ENIGMA, the vectors representing clauses had always
length n where n is the total number of features in the training data T (or 2n with
conjecture features). Experiments revealed that both LIBLINEAR and XGBoost
are capable of dealing with vectors up to the length of 105 with a reasonable
performance. This might be enough for smaller benchmarks, but with the need
to train on bigger training data, we might need to handle much larger feature
sets. In experiments with the whole translated Mizar Mathematical Library [42],
the feature vector length can easily grow over 106. This signiﬁcantly increases
both the training and the clause evaluation times. To handle such larger data
sets, we have implemented a simple hashing method to decrease the dimension
of the vectors.
2 We use the XGBoost parameter scale pos weight.

ENIGMA-NG: Eﬃcient Neural and Gradient-Boosted Inference
203
Instead of serially numbering all features, we represent each feature f by a
unique string and apply a general-purpose string hashing function to obtain a
number nf within a required range (between 0 and an adjustable hash base).
The value of f is then stored in the feature vector at the position nf. If diﬀerent
features get mapped to the same vector index, the corresponding values are
summed up.
We use the following hashing function sdbm coming from the open source
SDBM project. Given a string s, the value hi is computed for every character as
follows:
hi = si + (hi−1 ≪6) + (hi−1 ≪16) −hi−1
where h0 = 0, si is the ASCII code of the character at the i-th position, and the
operation ≪stands for a bit shift. The value for the last character is computed
with a ﬁxed-size data type (we use 64-bit unsigned integers) and this value
modulo the selected hash base is returned. We evaluate the eﬀect of the selected
hashing function later in Sect. 5.
4
Neural Architecture for ATP Guidance
Although the handcrafted clause features described in Sect. 3.1 lead to very good
results, they naturally have several limitations. It is never clear whether the
selected set of features is the best available given the training data. Moreover,
a rich set of features can easily lead to long sparse vectors and thus using them
for large corpora requires the use of dimensionality reduction techniques (c.f.
Sect. 3.4). Hence selecting the features automatically is a natural further step.
Among various techniques used to extract features fully automatically, neural
networks (NN) have recently become the most popular thanks to many successful
applications in, e.g., computer vision and natural language processing. There
have been several attempts to use NNs for guiding ATPs. However, such attempts
have so far typically suﬀered from a large overhead needed to evaluate the used
NN [31], making them impractical for actual proving.
A popular approach for representing tree-structured data, like logical formu-
lae, is based on recursive NNs [16]. The basic idea is that all objects (tree nodes,
subterms, subformulas) are represented in a high dimensional vector space and
these representations are subject to learning. Moreover, the representation of
more complex objects is a function of representations of their arguments. Hence
constants, variables, and atomic predicates are represented as directly learned
vectors, called vector embeddings. Assume that all such objects are represented by
n-dimensional vectors. For example, constants a and b are represented by learned
vectors va and vb, respectively. The representation of f(a, b) is then produced by
a learned function (NN), say vf, that has as an input two vectors and returns
a vector; hence vf(va, vb) ∈Rn. Moreover, the representation of P(f(a, b), a)
is obtained similarly, because from our point of view a representation is just a
function of arguments. Therefore we have

204
K. Chvalovsk´y et al.
vf : Rn × · · · × Rn



k-times
→Rn
for every k-ary, k ≥0, function symbol f,
vP : Rn × · · · × Rn



k-times
→Rn
for every k-ary, k ≥0, predicate symbol P,
in our language. We treat all variables as a single symbol, i.e., we always represent
a variable by a ﬁxed learned vector, and similarly for Skolem names. This is
in line with how the ENIGMA features are constructed and thus allows for a
more straightforward comparison. We also replace symbols that appear rarely
(fewer than 10 times) in our training set by a representative, e.g., all rare binary
functions become the same binary function. Loosely speaking, we learn a general
binary function this way. Because we treat equality and negation as learned
functions, we have described how a representation of a literal is produced.
We could now produce the representation of clauses by assuming that dis-
junction is a binary connective, however, we instead use a more direct approach
and we treat clauses directly as sequences of literals. Recurrent neural networks
(RNN) are commonly used to process arbitrary sequences of vectors. Hence we
train an RNN, called Cl, that consumes the representations of literals in a clause
and produces the representation of the clause, Cl : Rn × · · · × Rn →Rn.
Given a representation of a clause we could learn a function that says whether
the clause is a good given clause. However, without any context this may be hard
to decide. As in ENIGMA and [31], we introduce more context into our setting
by using the problem’s conjecture. The negated conjecture is translated by E
into a set of clauses. We combine the vector representations of these clauses by
another RNN, called Conj and deﬁned by Conj : Rn × · · · × Rn →Rn.
Now that we know how to represent a conjecture and a given clause by a
vector, we can deﬁne a function that combines them into a decision, called Fin
and deﬁned by Fin : Rn × Rn →R2. The two real values can later be turned
into probabilities of whether the clause will (will not) be useful, see Sect. 4.2.
Although all the representations have been vectors in Rn, this is an unnec-
essary restriction. It suﬃces if the objects of the same type are represented by
vectors of the same length. For example, we have experimented with Conj where
outputs are shorter (and inputs to Fin are changed accordingly) with the aim
to decrease overﬁtting to a particular problem.
4.1
Neural Model Parameters
The above mentioned neural model can be implemented in many ways. Although
we have not performed an extensive grid search over various variants, we can
discuss some of them shortly. The basic parameter is the dimension n of the
vectors. We have tried various models with n ∈{8, 16, 32, 64, 128}. The functions
used for vf and vP can be simple linear transformations (tensors), or more
complex combinations of linear and nonlinear layers. An example of a frequently
used nonlinearity is the rectiﬁed linear unit (ReLU), deﬁned by max(0, x).3
3 Due to various numerical problems with deep recursive networks we have obtained
better results with ReLU6, deﬁned by min(max(0, x), 6), or tanh.

ENIGMA-NG: Eﬃcient Neural and Gradient-Boosted Inference
205
For Cl and Conj we use (multi-layer) long short-term memory (LSTM) RNNs
[18]. We have tried to restrict the output vector of Conj to m = n
2 or m = n
4 to
prevent overﬁtting with inconclusive results. The Fin component is a sequence of
alternating linear and nonlinear layers (ReLU), where the last two linear layers
are Rn+m →R
n
2 and R
n
2 →R2.
4.2
ATP Guidance with Pytorch
We have created our neural model using the Pytorch library and integrated it
with E using the library’s C++ API.4 This API allows to load a previously
trained model saved to a ﬁle in a special TorchScript format. We use a separate
ﬁle for each of the neural parts described above. This includes computing of
the vector embeddings of terms, literals, and clauses, as well as the conjecture
embedding Conj summarizing the conjecture clauses into one vector, and ﬁnally
the part Fin, which classiﬁes clauses into those deemed useful for proving the
given conjecture and the rest.
We have created a new clause weight function in E called TorchEval which
interfaces these parts and can be used for evaluating clauses based on the neural
model. One of the key features of the interface, which is important for ensuring
reasonable evaluation speed, is caching of the embeddings of terms and literals.
Whenever the evaluation encounters a term or a literal which was evaluated
before, its embedding is simply retrieved from the memory in constant time
instead of being computed from the embeddings of its subterms recursively. We
use the fact that terms in E are perfectly shared and thus a pointer to a particular
term can be used as a key for retrieving the corresponding embedding. Note
that this pervasive caching is possible thanks to our choice of recursive neural
networks (that match our symbolic data) and it would not work with naive use of
other neural models such as convolutional or recurrent networks without further
modiﬁcations.
The clause evaluation part of the model returns two real outputs x0 and x1,
which can be turned into a probability that the given clause will be useful using
the sigmoid (logistic) function:
p =
1
1 + e(x0−x1) .
(1)
However, for classiﬁcation, i.e. for a yes-no answer, we can just compare the two
numbers and “say yes” whenever
x0 < x1.
(2)
After experimenting with other schemes that did not perform so well,5 we made
TorchEval return 1.0 whenever condition (2) is satisﬁed and 10.0 otherwise.
4 https://pytorch.org/cppdocs/.
5 For instance, using the probability (1) for a more ﬁne-grained order on clauses dic-
tated by the neural model.

206
K. Chvalovsk´y et al.
This is in accord with the standard convention employed by E that clauses with
smaller weight should be preferred and also corresponds to the ENIGMA app-
roach. Moreover, E implicitly uses an ever-increasing clause id as a tie breaker, so
among the clauses within the same class, both TorchEval and ENIGMA behave
as FIFO.
Another performance improvement was obtained by forcing Pytorch to use
just a single core when evaluating the model in E. The default Pytorch setting
was causing degradation of performance on machines with many cores, probably
by assuming by default that multi-threading will speed up frequent numeric
operations such as matrix multiplication. It seems that in our case, the overhead
for multi-threading at this point might be higher than the gain.
5
Experimental Evaluation
We experimentally evaluated the three learning-based ATP guidance methods
on the MPTP2078 benchmark [1].6 MPTP2078 contains 2078 problems coming
from the MPTP translation [42] of the Mizar Mathematical Library (MML)
[3] to FOL. The consistent use of symbol names across the MPTP corpus is
crucial for our symbol-based learning methods. We evaluated ATP performance
with a good-performing baseline E strategy, denoted S, which was previously
optimized [22] on Mizar problems (see Appendix A for details).
Section 5.1 provides details on model training and the hyperparameters used,
and analyzes the most important features used by the tree model. The model
based on linear regression (Sect. 3.2) is denoted Mlin, the model based on deci-
sion trees (Sect. 3.3) is denoted Mtree, and the neural model (Sect. 4) is denoted
Mnn. Sections 5.2 and 5.3 evaluate the performance of the models both by stan-
dard machine learning metrics and by plugging them into the ATPs. Section 5.4
evaluates the eﬀect of the feature hashing described in Sect. 3.4.
All experiments were run on a server with 36 hyperthreading Intel(R)
Xeon(R) Gold 6140 CPU @ 2.30 GHz cores, with 755 GB of memory available
in total. Each problem is always assigned one core. For training of the neural
models we used NVIDIA GeForce GTX 1080 Ti GPUs. As described above, nei-
ther GPU nor multi-threading was, however, employed when using the trained
models for clause evaluation inside the ATP.
5.1
Model Training, Hyperparameters and Feature Analysis
We evaluated the baseline strategy S on all the 2078 benchmark problems with
a ﬁxed CPU time limit of 10 s per problem.7 This yielded 1086 solved problems
and provided the training data for the learning methods as described in Sect. 2.
For Mlin and Mtree, the training data was translated to feature vectors (see
6 The benchmark can be found at https://github.com/JUrban/MPTP2078. For all the
remaining materials for reproducing the experiments please check out the repository
https://github.com/ai4reason/eprover-data/tree/master/CADE-19.
7 This appears to be a reasonable waiting time for, e.g., the users of ITP hammers [5].

ENIGMA-NG: Eﬃcient Neural and Gradient-Boosted Inference
207
Sect. 3) which were then fed to the learner. For Mnn we used the training data
directly without any feature extraction.
Training Data and Training of Linear and Tree Models: The training
data consisted of around 222 000 training samples (21 000 positives and 201
000 negatives) with almost 32 000 diﬀerent ENIGMA features. This means that
the training vectors for Mlin and Mtree had dimension close to 64 000,8 and
so had the output weight vector of Mlin. For Mtree, we reused the parameters
that performed well in the ATPBoost [36] and rlCoP [26] systems and pro-
duced models with 200 decision trees, each with maximal depth 9. The resulting
models—both linear and boosted trees—were about 1MB large in their native
representation. The training time for Mlin was around 8 min (ﬁve iterations of
accuracy-balancing boosting), and approximately 5 min for Mtree. Both of them
were measured on a single CPU core. During the boosting of Mlin, the positive
samples were extended from 21k to 110k by repeating the mis-classiﬁed vectors.
Learned Tree Features: The boosted tree model Mtree allows computing
statistics of the most frequently used features. This is an interesting aspect that
goes in the direction of explainable AI. The most important features can be ana-
lyzed by ATP developers and compared with the ideas used in standard clause
evaluation heuristics. There were 200 trees in Mtree with 20215 decision nodes
in total. These decision nodes refer to only 3198 features out of the total 32000.
The most frequently used feature was the clause length, used 3051 times, followed
by the conjecture length, used 893 times, and by the numbers of the positive
and negative literals in the clauses and conjectures. In a crude way, the machine
learning here seems to conﬁrm the importance assigned to these basic metrics
by ATP researchers. The set of top ten features additionally contains three sym-
bol counts (including “∈” and “⊆”) and a vertical feature corresponding to a
variable occurring under negated set membership ∈(like in “x ̸∈·” or “· ̸∈x”).
This seems plausible, since the Mizar library and thus MPTP2078 are based on
set theory where membership and inclusion are key concepts.
Neural Training and Final Neural Parameters: We tried to improve the
training of Mnn by randomly changing the order of clauses in conjectures, literals
in clauses, and terms in equalities. If after these transformations a negative
example pair (C, G) was equivalent to a positive one, we removed the negative
one from the training set. This way we reduced the number of negative examples
to 198k. We trained our model in batches9 of size 128 and used the negative log-
likelihood as a loss function (the learning rate is 10−3), where we applied log-
softmax on the output of Fin. We weighted positive examples more to simulate
a balanced training set. All symbols of the same type and arity that have less
than 10 occurrences in the training set were represented by one symbol. We set
the vector dimension to be n = 64 for the neural model Mnn and we set the
output of Conj to be m = 16. All the functions representing function symbols
8 Combining the dimensions for the clause and the conjecture.
9 Moreover, we always try to put examples with the same conjecture G into the same
batch to share the time for recomputing the representation of G.

208
K. Chvalovsk´y et al.
Table 1. True Positive Rate (TPR) and True Negative Rate (TNR) on training data.
Mlin
Mtree
Mnn
TPR 90.54% 99.36% 97.82%
TNR 83.52% 93.32% 94.69%
and predicates were composed of a linear layer and ReLU6. Fin was set to be a
sequence of linear, ReLU, linear, ReLU, and linear layers. The training time for
Mnn was around 8 min per epoch and the model was trained for 50 epochs. Note
that we save a model after each epoch and randomly test few of these generated
models and select the best performing one (in our case the model generated after
35 epochs).
5.2
Evaluation of the Model Performance
Training Performance of the Models: We ﬁrst evaluate how well the indi-
vidual models managed to learn the training data. Due to possible overﬁtting,
this is obviously used only as a heuristic and the main metric is provided by
the ultimate ATP evaluation. Table 1 shows for each model the true positive
and true negative rates (TPR, TNR) on the training data, that is, the percent-
age of the positive and negative examples, classiﬁed correctly by each model.10
The highest TPR, also called sensitivity, is achieved by Mtree while the highest
TNR, also called speciﬁcity, by Mnn. As expected, the accuracy of the linear
model is lower. Its main strength seems to come from the relatively high speed
of evaluation (see below).
ATP Performance of the Models: Table 2 shows the total number of prob-
lems solved by the four methods. For each learning-based model M, we always
consider the model alone (S ⊙M) and the model combined equally with S
(S ⊕M). All methods are using the same time limit, i.e., 10 s. This is our
ultimate “real-life” evaluation, conﬁrming that the boosted trees indeed outper-
form the guidance by the linear classiﬁer and that the recursive neural network
and its caching implementation is already competitive with these methods in
real time. The best method S ⊕Mtree solves 15.7% more problems than the
original strategy S, and 3.8% problems more than the previously best linear
strategy S ⊕Mlin.11 Table 2 provides also further explanation of these aggre-
gated numbers. We show the number of unique solutions provided by each of
the methods and the diﬀerence to the original strategy. Table 3 shows how useful
are the particular methods when used together. Both the linear and the neural
10 For Mlin, we show the numbers after ﬁve iterations of the boosting loop (see
Sect. 3.2). The values in the ﬁrst round were 40.81% for the positive and 98.62%
for the negative rate.
11 We have also measured how much S beneﬁts from increased time limits. It solves
1099 problems in 20 s and 1137 problems in 300 s.

ENIGMA-NG: Eﬃcient Neural and Gradient-Boosted Inference
209
Table 2. Number of problems solved (and uniquely solved) by the individual models.
S+ and S−indicate the number of problems gained and lost w.r.t. the baseline S.
S
S ⊙Mlin S ⊕Mlin S ⊙Mtree S ⊕Mtree S ⊙Mnn S ⊕Mnn
Solved
1086 1115
1210
1231
1256
1167
1197
Unique
0
3
7
10
15
3
2
S+
0 +119
+138
+155
+173
+114
+119
S−
0
−90
−14
−10
−3
−33
−8
Table 3. The greedy sequence—methods sorted by their greedily computed contribu-
tion to all the problems solved.
S ⊕Mtree S ⊕Mlin S ⊙Mnn S ⊙Mtree S ⊙Mlin S ⊕Mnn S
Addition 1256
33
13
11
3
2
0
Total
1256
1289
1302
1313
1316
1318
1318
models complement the boosted trees well, while the original strategy is made
completely redundant.
Testing Performance of the Models on Newly Solved Problems: There
are 232 problems solved by some of the six learning-based methods but not by
the baseline strategy S. To see how the trained models behave on new data,
we again extract positive and negative examples from all successful proof runs
on these problems. This results in around 31 000 positive testing examples and
around 300 000 negative testing examples.
Table 4 shows again for each of the previously trained models the true positive
and true negative rates (TPR, TNR) on these testing data. The highest TPR
is again achieved by Mtree and the highest TNR by Mnn. The accuracy of
the linear model is again lower. Both the TPR and TNR testing scores are
signiﬁcantly lower for all methods compared to their training counterparts. TPR
decreases by about 15% and TNR by about 20%. This likely shows the limits of
our current learning and proof-state characterization methods. It also points to
the very interesting issue of obtaining many alternative proofs [29] and learning
from them. It seems that just using learning or reasoning is not suﬃcient in our
AI domain, and that feedback loops combining the two multiple times [36,44]
are really necessary for building strong ATP systems.
5.3
Speed of Clause Evaluation by the Learned Models
The number of generated clauses reported by E can be used as a rough estimate
of the amount of work done by the prover. If we look at this statistic for those
runs that timed out—i.e., did not ﬁnd a proof within the given time limit—we
can use it to estimate the slowdown of the clause processing rate incurred by
employing a machine learner inside E. (Note that each generated clause needs
to be evaluated before it is inserted on the respective queue.)

210
K. Chvalovsk´y et al.
Table 4. True Positive Rate (TPR) and True Negative Rate (TNR) on testing data
from the newly solved 232 problems.
Mlin
Mtree
Mnn
TPR 80.54% 83.35% 82.00%
TNR 62.28% 72.60% 76.88%
Table 5. The ASRPA and NSRGA ratios. ASRPA are the average ratios (and standard
deviations) of the relative number of processed clauses with respect to S on problems
on which all runs succeeded. NSRGA are the average ratios (and standard deviations)
of the relative number of generated clauses with respect to S on problems on which all
runs timed out. The numbers of problems were 898 and 681, respectively.
S
S ⊙Mlin
S ⊕Mlin
S ⊙Mtree
S ⊕Mtree
S ⊙Mnn
S ⊕Mnn
ASRPA
1 ± 0
2.18 ± 20.35
0.91 ± 0.58
0.60 ± 0.98
0.59 ± 0.36
0.59 ± 0.75
0.69 ± 0.94
NSRGA
1 ± 0
0.61 ±
0.52
0.56 ± 0.35
0.42 ± 0.38
0.43 ± 0.35
0.06 ± 0.08
0.07 ± 0.09
Complementarily, the number of processed clauses when compared across
those problems on which all runs succeeded may be seen as an indicator of how
well the respective clause selection guides the search towards a proof (with a
perfect guidance, we only ever process those clauses which constitute a proof).12
Table 5 compares the individual conﬁgurations of E based on the seven eval-
uated models with respect to these two metrics. To obtain the shown values,
we ﬁrst normalized the numbers on per problem basis with respect to the result
of the baseline strategy S and computed an average across all relevant prob-
lems. The comparison of thus obtained All Solved Relative Processed Average
(ASRPA) values shows that, except for S⊙Mlin (which has a very high standard
deviation), all other conﬁgurations on average manage to improve over S and
ﬁnd the corresponding proofs with fewer iterations of the given clause loop. This
indicates better guidance towards the proof on the selected benchmarks.
The None Solved Relative Generated Average (NSRGA) values represent
the speed of the clause evaluation. It can be seen that while the linear model
is relatively fast (approximately 60% of the speed of S), followed closely by the
tree-based model (around 40%), the neural model is more expensive to evaluate
(achieving between 6% and 7% of S).
We note that without caching, NSRGA of S ⊕Mnn drops from 7.1% to 3.6%
of the speed of S. Thus caching currently helps to approximately double the
speed of the evaluation of clauses with Mnn.13 It is interesting and encouraging
that despite the neural method being currently about ten times slower than the
linear method—and thus generating about ten times fewer inferences within the
12 This metric is similar in spirit to given clause utilization introduced by Schulz and
M¨ohrmann [41].
13 Note that more global caching (of, e.g., whole clauses and frequent combinations
of literals) across multiple problems may further amortize the cost of the neural
evaluation. This is left as future work here.

ENIGMA-NG: Eﬃcient Neural and Gradient-Boosted Inference
211
10 s time limit used for the ATP evaluation—the neural model already manages
to outperform the linear model in the unassisted setting. I.e., S ⊙Mnn is already
better than S ⊙Mlin (recall Table 2), despite the latter being much faster.
5.4
Evaluation of Feature Hashing
Finally, we evaluate the feature hashing described in Sect. 3.4. We try diﬀerent
hash bases in order to reduce dimensionality of the vectors and to estimate the
inﬂuence on the ATP performance. We evaluate on 6 hash bases from 32k (215),
16k (214), down to 1k (210). For each hash base, we construct models Mlin and
Mtree, we compute their prediction rates, and evaluate their ATP performance.
With the hash base n, each feature must fall into one of n buckets. When the
number of features is greater than the base—which is our case as we intend to
use hashing for dimensionality reduction—collisions are inevitable. When using
hash base of 32000 (ca 215) there are almost as many hashing buckets as there are
features in the training data (31675). Out of these features, ca 12000 features are
hashed without a collision and 12000 buckets are unoccupied. This yields a 40%
probability of a collision. With lower bases, the collisions are evenly distributed.
Lower hash bases lead to larger loss of information, hence decreased perfor-
mance can be expected. On the other hand, dimensionality reduction sometimes
leads to better generalization (less overﬁtting of the learners). Also, the evalua-
tion in the ATP can be done more eﬃciently in a lower dimension, thus giving
the ATP the chance to process more clauses. The prediction rates and ATP
performance for models with and without hashing are presented in Table 6. We
compute the true positive (TPR) and negative (TNR) rates as in Sect. 5.1, and
we again evaluate E’s performance based on the strategy S in the two ways (⊙
and ⊕) as in Sect. 5.2. The best value in each row is highlighted. Both models
perform comparably to the version without hashing even when the vector dimen-
sion is reduced to just 25%, i.e. 8k. With reduction to 1000 (32x), the models
still provide a decent improvement over the baseline strategy S, which solved
1086 problems. The Mtree model deals with the reduction slightly better.
Table 6. Eﬀect of feature hashing on prediction rates and ATP performance.
Model\hash size Without
32k
16k
8k
4k
2k
1k
Mlin
TPR [%] 90.54
89.32 88.27
89.92 82.08 91.08 83.68
TNR [%] 83.52
82.40 86.01 83.02 81.50 76.04
77.53
S ⊙M
1115
1106
1072
1078
1076
1028
938
S ⊕M
1210
1218 1189
1202
1189
1183
1119
Mtree TPR [%] 99.36
99.38 99.38
99.51 99.62 99.65
99.69
TNR [%] 93.32
93.54 93.29
93.69 93.90 94.53
94.88
S ⊙M
1231
1231
1233
1232
1223
1227
1215
S ⊕M
1256
1244
1244
1256 1245
1236
1232

212
K. Chvalovsk´y et al.
Interestingly, the classiﬁcation accuracy of the models (again, measured only
on the training data) seems to increase with the decrease of hash base (especially
for Mtree). However, with this increased accuracy, the ATP performance mildly
decreases. This could be caused by the more frequent collisions and thus learning
on data that has been made less precise.
6
Conclusions and Future Work
We have described an eﬃcient implementation of gradient-boosted and recursive
neural guidance in E, extending the ENIGMA framework. The tree-based mod-
els improve on the previously used linear classiﬁer, while the neural methods
have, for the ﬁrst time, been shown practically competitive and useful, by using
extensive caching corresponding to the term sharing implemented in E. While
this is clearly not the last word in this area, we believe that this is the ﬁrst prac-
tically convincing application of gradient-boosted and neural clause guidance in
saturation-style automated theorem provers.
There are a number of future directions. For example, research in better proof
state characterization of saturation-style systems has been started recently [14,
15] and it is likely that evolving vectorial representations of the proof state will
further contribute to the quality of the learning-based guidance. Our recursive
neural model is just one of many, and a number of related and combined models
can be experimented with.
A
Strategy S from Experiments in Sect. 5
The following E strategy has been used to undertake the experimental evalu-
ation in Sect. 5. The given clause selection strategy (heuristic) is deﬁned using
parameter “-H”.
--definitional-cnf=24 --split-aggressive --simul-paramod -tKBO6 -c1 -F1
-Ginvfreq -winvfreqrank --forward-context-sr --destructive-er-aggressive
--destructive-er --prefer-initial-clauses -WSelectMaxLComplexAvoidPosPred
-H’(1*ConjectureTermPrefixWeight(DeferSOS,1,3,0.1,5,0,0.1,1,4),
1*ConjectureTermPrefixWeight(DeferSOS,1,3,0.5,100,0,0.2,0.2,4),
1*Refinedweight(ConstPrio,4,300,4,4,0.7),
1*RelevanceLevelWeight2(PreferProcessed,0,1,2,1,1,1,200,200,2.5,
9999.9,9999.9),
1*StaggeredWeight(DeferSOS,1),
1*SymbolTypeweight(DeferSOS,18,7,-2,5,9999.9,2,1.5),
2*Clauseweight(ConstPrio,20,9999,4),
2*ConjectureSymbolWeight(DeferSOS,9999,20,50,-1,50,3,3,0.5),
2*StaggeredWeight(DeferSOS,2))’

ENIGMA-NG: Eﬃcient Neural and Gradient-Boosted Inference
213
References
1. Alama, J., Heskes, T., K¨uhlwein, D., Tsivtsivadze, E., Urban, J.: Premise selection
for mathematics by corpus analysis and kernel methods. J. Autom. Reason. 52(2),
191–213 (2014)
2. Alemi, A.A., Chollet, F., E´en, N., Irving, G., Szegedy, C., Urban, J.: DeepMath -
deep sequence models for premise selection. In: Lee, D.D., Sugiyama, M., Luxburg,
U.V., Guyon, I., Garnett, R. (eds.) Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016,
Barcelona, Spain, 5–10 December 2016, pp. 2235–2243 (2016)
3. Bancerek, G., et al.: Mizar: state-of-the-art and beyond. In: Kerber, M., Carette, J.,
Kaliszyk, C., Rabe, F., Sorge, V. (eds.) CICM 2015. LNCS (LNAI), vol. 9150, pp.
261–279. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-20615-8 17
4. Blanchette, J.C., Greenaway, D., Kaliszyk, C., K¨uhlwein, D., Urban, J.: A learning-
based fact selector for Isabelle/HOL. J. Autom. Reason. 57(3), 219–244 (2016)
5. Blanchette, J.C., Kaliszyk, C., Paulson, L.C., Urban, J.: Hammering towards QED.
J. Formalized Reason. 9(1), 101–148 (2016)
6. Bridge, J.P., Holden, S.B., Paulson, L.C.: Machine learning for ﬁrst-order theorem
proving - learning to select a good heuristic. J. Autom. Reason. 53(2), 141–172
(2014)
7. Chen, T., Guestrin, C.: XGBoost: a scalable tree boosting system. In: KDD, pp.
785–794. ACM (2016)
8. Denzinger, J., Fuchs, M., Goller, C., Schulz, S.: Learning from previous proof expe-
rience. Technical report AR99-4, Institut f¨ur Informatik, Technische Universit¨at
M¨unchen (1999)
9. Ertel, W., Schumann, J.M.P., Suttner, C.B.: Learning heuristics for a theorem
prover using back propagation. In: Retti, J., Leidlmair, K. (eds.) 5. ¨Osterreichische
Artiﬁcial Intelligence-Tagung. INFORMATIK, vol. 208, pp. 87–95. Springer, Hei-
delberg (1989). https://doi.org/10.1007/978-3-642-74688-8 10
10. Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., Lin, C.-J.: LIBLINEAR: a
library for large linear classiﬁcation. J. Mach. Learn. Res. 9, 1871–1874 (2008)
11. F¨arber, M., Brown, C.: Internal guidance for satallax. In: Olivetti and Tiwari [33],
pp. 349–361
12. F¨arber, M., Kaliszyk, C., Urban, J.: Monte Carlo tableau proof search. In: de
Moura, L. (ed.) CADE 2017. LNCS (LNAI), vol. 10395, pp. 563–579. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-63046-5 34
13. Gauthier, T., Kaliszyk, C.: Premise selection and external provers for HOL4.
In: Certiﬁed Programs and Proofs (CPP 2015) (2015). https://doi.org/10.1145/
2676724.2693173
14. Goertzel, Z., Jakub˚uv, J., Schulz, S., Urban, J.: ProofWatch: watchlist guidance
for large theories in E. In: Avigad, J., Mahboubi, A. (eds.) ITP 2018. LNCS, vol.
10895, pp. 270–288. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-
94821-8 16
15. Goertzel, Z., Jakubuv, J., Urban, J.: ProofWatch meets ENIGMA: ﬁrst experi-
ments. In: Barthe, G., Korovin, K., Schulz, S., Suda, M., Sutcliﬀe, G., Veanes, M.
(eds.) LPAR-22 Workshop and Short Paper Proceedings. Kalpa Publications in
Computing, vol. 9, pp. 15–22. EasyChair (2018)
16. Goller, C., K¨uchler, A.: Learning task-dependent distributed representations by
backpropagation through structure. In: Proceedings of International Conference
on Neural Networks (ICNN 1996), vol. 1, pp. 347–352, June 1996

214
K. Chvalovsk´y et al.
17. Gottlob, G., Sutcliﬀe, G., Voronkov, A. (eds.): Global Conference on Artiﬁcial
Intelligence, GCAI 2015, Tbilisi, Georgia, 16–19 October 2015. EPiC Series in
Computing, vol. 36. EasyChair (2015)
18. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9,
1735–1780 (1997)
19. Jakub˚uv, J., Urban, J.: ENIGMA: eﬃcient learning-based inference guiding
machine. In: Geuvers, H., England, M., Hasan, O., Rabe, F., Teschke, O. (eds.)
CICM 2017. LNCS (LNAI), vol. 10383, pp. 292–302. Springer, Cham (2017).
https://doi.org/10.1007/978-3-319-62075-6 20
20. Jakub˚uv, J., Urban, J.: Enhancing ENIGMA given clause guidance. In: Rabe, F.,
Farmer, W.M., Passmore, G.O., Youssef, A. (eds.) CICM 2018. LNCS (LNAI),
vol. 11006, pp. 118–124. Springer, Cham (2018). https://doi.org/10.1007/978-3-
319-96812-4 11
21. Jakub˚uv, J., Urban, J.: Hierarchical invention of theorem proving strategies. AI
Commun. 31(3), 237–250 (2018)
22. Jakubuv, J., Urban, J.: BliStrTune: hierarchical invention of theorem proving
strategies. In: Bertot, Y., Vafeiadis, V. (eds.) Proceedings of the 6th ACM SIG-
PLAN Conference on Certiﬁed Programs and Proofs, CPP 2017, Paris, France,
16–17 January 2017, pp. 43–52. ACM (2017)
23. Joulin, A., Grave, E., Bojanowski, P., Mikolov, T.: Bag of tricks for eﬃcient text
classiﬁcation. In: Proceedings of the 15th Conference of the European Chapter of
the Association for Computational Linguistics, Short Papers, vol. 2, pp. 427–431.
Association for Computational Linguistics, April 2017
24. Kaliszyk, C., Urban, J.: Learning-assisted automated reasoning with Flyspeck. J.
Autom. Reason. 53(2), 173–213 (2014)
25. Kaliszyk, C., Urban, J.: FEMaLeCoP: fairly eﬃcient machine learning connection
prover. In: Davis, M., Fehnker, A., McIver, A., Voronkov, A. (eds.) LPAR 2015.
LNCS, vol. 9450, pp. 88–96. Springer, Heidelberg (2015). https://doi.org/10.1007/
978-3-662-48899-7 7
26. Kaliszyk, C., Urban, J., Michalewski, H., Ols´ak, M.: Reinforcement learning of
theorem proving. In: Bengio, S., Wallach, H.M., Larochelle, H., Grauman, K.,
Cesa-Bianchi, N., Garnett, R. (eds.) Advances in Neural Information Processing
Systems 31: Annual Conference on Neural Information Processing Systems 2018,
NeurIPS 2018, Canada, Montr´eal, 3–8 December 2018, pp. 8836–8847 (2018)
27. Kaliszyk, C., Urban, J., Vyskocil, J.: Eﬃcient semantic features for automated
reasoning over large theories. In: IJCAI, pp. 3084–3090. AAAI Press (2015)
28. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
29. Kuehlwein, D., Urban, J.: Learning from multiple proofs: ﬁrst experiments. In:
Fontaine, P., Schmidt, R.A., Schulz, S. (eds.) PAAR-2012. EPiC Series, vol. 21,
pp. 82–94. EasyChair (2013)
30. K¨uhlwein, D., van Laarhoven, T., Tsivtsivadze, E., Urban, J., Heskes, T.: Overview
and evaluation of premise selection techniques for large theory mathematics. In:
Gramlich, B., Miller, D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol.
7364, pp. 378–392. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-
642-31365-3 30
31. Loos, S.M., Irving, G., Szegedy, C., Kaliszyk, C.: Deep network guided proof search.
In: Eiter, T., Sands, D. (eds.) LPAR-21, 21st International Conference on Logic for
Programming, Artiﬁcial Intelligence and Reasoning, Maun, Botswana, 7–12 May
2017. EPiC Series in Computing, vol. 46, pp. 85–105. EasyChair (2017)

ENIGMA-NG: Eﬃcient Neural and Gradient-Boosted Inference
215
32. Meng, J., Paulson, L.C.: Translating higher-order clauses to ﬁrst-order clauses. J.
Autom. Reason. 40(1), 35–60 (2008)
33. Olivetti, N., Tiwari, A. (eds.): IJCAR 2016. LNCS (LNAI), vol. 9706. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-40229-1
34. Otten, J., Bibel, W.: leanCoP: lean connection-based theorem proving. J. Symb.
Comput. 36(1–2), 139–161 (2003)
35. Overbeek, R.A.: A new class of automated theorem-proving algorithms. J. ACM
21(2), 191–200 (1974)
36. Piotrowski, B., Urban, J.: ATPboost: learning premise selection in binary setting
with ATP feedback. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018.
LNCS (LNAI), vol. 10900, pp. 566–574. Springer, Cham (2018). https://doi.org/
10.1007/978-3-319-94205-6 37
37. Polikar, R.: Ensemble based systems in decision making. IEEE Circuits Syst. Mag.
6(3), 21–45 (2006)
38. Sch¨afer, S., Schulz, S.: Breeding theorem proving heuristics with genetic algorithms.
In: Gottlob et al. [17], pp. 263–274
39. Schulz, S.: Learning search control knowledge for equational deduction. DISKI,
vol. 230. Inﬁx Akademische Verlagsgesellschaft (2000)
40. Schulz, S.: E - a brainiac theorem prover. AI Commun. 15(2–3), 111–126 (2002)
41. Schulz, S., M¨ohrmann, M.: Performance of clause selection heuristics for saturation-
based theorem proving. In: Olivetti and Tiwari [33], pp. 330–345
42. Urban, J.: MPTP 0.2: design, implementation, and initial experiments. J. Autom.
Reason. 37(1–2), 21–43 (2006)
43. Urban, J.: BliStr: the blind strategymaker. In: Gottlob et al. [17], pp. 312–319
44. Urban, J., Sutcliﬀe, G., Pudl´ak, P., Vyskoˇcil, J.: MaLARea SG1 - machine learner
for automated reasoning with semantic guidance. In: Armando, A., Baumgartner,
P., Dowek, G. (eds.) IJCAR 2008. LNCS (LNAI), vol. 5195, pp. 441–456. Springer,
Heidelberg (2008). https://doi.org/10.1007/978-3-540-71070-7 37
45. Urban, J., Vyskoˇcil, J., ˇStˇep´anek, P.: MaLeCoP machine learning connection
prover. In: Br¨unnler, K., Metcalfe, G. (eds.) TABLEAUX 2011. LNCS (LNAI),
vol. 6793, pp. 263–277. Springer, Heidelberg (2011). https://doi.org/10.1007/978-
3-642-22119-4 21

Towards Physical Hybrid Systems
Katherine Cordwell1(B)
and Andr´e Platzer1,2
1 Computer Science Department, Carnegie Mellon University, Pittsburgh, USA
{kcordwel,aplatzer}@cs.cmu.edu
2 Fakult¨at f¨ur Informatik, Technische Universit¨at M¨unchen, Munich, Germany
Abstract. Some hybrid systems models are unsafe for mathematically
correct but physically unrealistic reasons. For example, mathematical
models can classify a system as being unsafe on a set that is too small
to have physical importance. In particular, diﬀerences in measure zero
sets in models of cyber-physical systems (CPS) have signiﬁcant mathe-
matical impact on the mathematical safety of these models even though
diﬀerences on measure zero sets have no tangible physical eﬀect in a
real system. We develop the concept of “physical hybrid systems” (PHS)
to help reunite mathematical models with physical reality. We modify
a hybrid systems logic (diﬀerential temporal dynamic logic) by adding
a ﬁrst-class operator to elide distinctions on measure zero sets of time
within CPS models. This approach facilitates modeling since it admits
the veriﬁcation of a wider class of models, including some physically real-
istic models that would otherwise be classiﬁed as mathematically unsafe.
We also develop a proof calculus to help with the veriﬁcation of PHS.
Keywords: Hybrid systems · Almost everywhere ·
Diﬀerential temporal dynamic logic · Proof calculus
1
Introduction
Hybrid systems [1,24], which have interacting discrete and continuous dynam-
ics, provide all the necessary mathematical precision to describe and verify the
behavior of safety-critical cyber-physical systems (CPS), such as self-driving cars,
surgical robots, and drones. Ironically, however, hybrid systems provide so much
mathematical precision that they can distinguish models that exhibit no phys-
ically measurable diﬀerence. More speciﬁcally, since mathematical models are
This material is based upon work supported by the National Science Foundation Grad-
uate Research Fellowship under Grant No. DGE-1252522. Any opinions, ﬁndings, and
conclusions or recommendations expressed in this material are those of the authors and
do not necessarily reﬂect the views of the National Science Foundation. This research
was also sponsored by the AFOSR under grant number FA9550-16-1-0288 and by the
Alexander von Humboldt Foundation. The views and conclusions contained in this
document are those of the authors and should not be interpreted as representing the
oﬃcial policies, either expressed or implied, of any sponsoring institution, the U.S.
government or any other entity.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 216–232, 2019.
https://doi.org/10.1007/978-3-030-29436-6_13

Towards Physical Hybrid Systems
217
minutely precise, models can classify systems as being unsafe on minutely small
sets—even when these sets have no physical signiﬁcance. For example, a mathe-
matical model could classify a system as being mathematically unsafe at a single
instant in time—but why should the safety of a model give more weight to such
glitches than even the very notion of solutions of diﬀerential equations, which is
unaﬀected [30] by changes on sets of measure zero in time? Practically speaking,
a physical system is only unsafe at a single instant of time if it is also already
unsafe at a signiﬁcantly larger set of times. In the worst case, such degenerate
counterexamples could detract attention from real unsafeties in a model.
That is why this paper calls for a shift in perspective toward physical hybrid
systems (PHS) that are more attuned to the limitations and necessities of physics
than pure mathematical models. PHS are hybrid systems that behave safely
“almost everywhere” (in a measure theoretic sense) and thus, physically speak-
ing, are safe systems. While diﬀerent ﬂavors of attaining PHS are possible and
should be pursued, we propose arguably the tamest one, which merely disre-
gards diﬀerences in safety on sets of time of measure zero. As our ultimate hope
is that models of PHS can be (correctly) formally veriﬁed without introducing
any burden on the user, we introduce the ability to rigorously ignore sets of time
of measure zero into logic. A major diﬃculty is that there is a delicate trade-
oﬀbetween the physical practicality of a deﬁnition (what real-world behavior
it captures) and the logical practicality of a deﬁnition (what logical reasoning
principles it supports). Our notion of safety almost everywhere in time not only
enjoys a direct link with well-established mathematical principles of diﬀerential
equations, but also satisﬁes key logical properties, such as compositionality.
We modify diﬀerential temporal dynamic logic (dTL) [15,25] to capture the
notion of safety time almost everywhere (tae) along the execution trace of a
hybrid system. dTL extends the hybrid systems logic diﬀerential dynamic logic
(dL) with the ability to analyze system behavior over time. We call our new
logic physical diﬀerential temporal dynamic logic (PdTL) to reﬂect its purpose.
While PdTL is closely related to dTL in style and development, the formaliza-
tion of safety tae is entirely new, and thus requires new reasoning. Guiding the
development of PdTL is the following motivating example: Consider a train and
a safety condition v<100 on the velocity of the train. Physically speaking, it is
ﬁne to allow v = 100 for a split-second, because this has no measurable impact.
PdTL is designed to classify the situation where the train continuously acceler-
ates until v = 100 and immediately brakes whenever it reaches v = 100 as tae
safe.
2
Related Work
Since all systems inherently suﬀer from imprecision, several approaches develop
robust hybrid systems, which are stable up to small perturbations—for example,
in the contexts of decidability [2,10,11], runtime monitoring [8,9], and controls
[18,19,22]. If systems are robust, they provide a fair amount of automation
[11,17]. Both our approach and robustness hinge on building in an awareness

218
K. Cordwell and A. Platzer
of physics to hybrid systems veriﬁcation. However, robustness is fundamentally
diﬀerent from our deductive veriﬁcation approach. The analysis of robust systems
often relies on a reachability analysis (see, e.g., [21]) which loses much of the
logical precision present in deductive veriﬁcation, e.g., decidability of diﬀerential
equation invariants [27]. Further, by building on dL, which is a general purpose
hybrid systems logic, we are able to handle a wide class of models, whereas tools
like dReach [17] tend to be slightly more limited in scope. In particular, our
deductive approach for PHS admits an induction principle—making it possible to
verify safety properties of controllers that run in loops for any amount of time—
whereas robustness approaches are presently limited to bounded model checking.
We advocate for robustness in that models can, and should be, written with an
awareness of imprecision. However, we recognize that modeling is diﬃcult, and
even well-intentioned models can suﬀer from nonphysical glitches—hence PHS.
Non-classical solutions of ODEs [4,6], especially Filippov and Carath´eodory
solutions, align well with the PHS intuition as they often inherently ignore sets of
measure zero. Filippov solutions consider vector ﬁelds equivalent up to diﬀerences
on sets of measure zero. Carath´eodory solutions satisfy a diﬀerential equation
everywhere except on a set of measure zero. Hybrid systems models do not
usually make use of non-classical solutions, since admitting them would require a
relaxed notion of safety. Non-classical solutions are sometimes used in the context
of controls: Goebel et al. [13] generalized the notion of a solution to a hybrid
system by using non-classical solutions of ODEs by Filippov and Krasovskii,
with a view towards obtaining robustness properties, and a later work [12] allows
solutions that ﬁt a system of ODEs almost everywhere in a time interval. The
temporal approach of PdTL naturally admits Carath´eodory solutions.
Eliding sets of measure zero can be computationally signiﬁcant—notably, in
quantiﬁer elimination, which arises in the last step of hybrid systems proofs.
Despite having no physical meaning, measure zero sets have a signiﬁcant impact
on the eﬃciency of real arithmetic, and thus on the overall hybrid systems proofs.
The enabling factor behind eﬃcient arithmetic [20] is to ignore sets of measure
zero and thus remove the need to compute with irrational algebraic numbers. A
potential computational beneﬁt is an encouraging motivation for PHS.
3
Syntax of PdTL
In pursuit of enabling the statement of physical safety properties of hybrid sys-
tems, we develop PdTL, which builds on concepts from dTL [15,25] to introduce
an “almost everywhere in time” operator, □tae, which makes it possible to dis-
regard minor glitches violating safety conditions on sets of time of measure zero.
When possible, we keep our notation consistent with that of dTL [25], so that
the syntax of PdTL is very similar to the syntax of dTL—the key diﬀerence
being that we eschew dTL trace formulas □φ and ♦φ in favor of □taeφ.
The new PdTL formula [α]□taeφ expresses that along each run of the hybrid
system α, the formula φ is true at almost every time (“tae” stands for “time
almost everywhere”). This formula remains true even in cases where φ is false at

Towards Physical Hybrid Systems
219
only a measure zero set of points in time along α. Because a hybrid system may
exhibit diﬀerent behaviors, the particular measure zero set of points in time at
which φ is false is allowed to depend on the particular run of α.
Fix a set Σ containing real-valued variables, function symbols, and predicate
symbols. In particular, Σ contains the symbols needed for ﬁrst-order logic of real
arithmetic (FOL). We use Σvar to denote the set of real-valued variables in Σ,
and let Trm(Σ) denote the set of (polynomial) terms over Σ (as in FOL).
We now deﬁne the syntax of hybrid programs (which model hybrid sys-
tems) and formulas capable of expressing physical properties of hybrid programs.
Hybrid programs [25,26] are allowed to assign values to variables (with the :=
operator), test the truth of formulas (with the ? operator), evolve along systems
of diﬀerential equations, and branch nondeterministically (with the ∪operator).
Hybrid programs are also sequentially composable with the ; operator, and can
be run in loops with the ∗operator.
Deﬁnition 1. Hybrid programs are given by a grammar, where α and β are
hybrid programs, e ∈Trm(Σ), x is a variable, and P and R are FOL formulas:
α, β ::= x := e | ?P | x′ = f(x) & R | α ∪β | α; β | α∗
As in CTL∗[7] and dTL, we split PdTL formulas into state formulas that
are true or false in a state (i.e., at a snapshot in time) and trace formulas that
are true or false along a ﬁxed trace that keeps track of the behavior of a system
over time.
Deﬁnition 2. The state formulas are given by the following grammar, where
p ∈Σ is a predicate symbol of arity n ≥0, e1, . . . , en ∈Trm(Σ), φ and ψ are
state formulas, α is a hybrid program, κ is a trace formula, and x is a variable:
φ, ψ ::= p(e1, . . . , en) | ¬φ | φ ∧ψ | ∀x φ | [α]κ | ⟨α⟩κ
Trace formulas are given by the following grammar, where φ is a state formula:
κ ::= φ | □taeφ
We will also allow the use of the standard logical operators ∨, →, and ↔,
which are deﬁned in terms of ¬ and ∧as usual in classical logic.
Our motivating example can be modeled in PdTL as follows:
a=0 ∧v=0 →[

((?(v<100); a := 1) ∪(?(v=100); a := −1));
{x′ = v, v′ = a & 0≤v≤100}
∗]□taev<100
This claims that if the initial velocity and acceleration are both 0, then along
any run of the system, v<100 holds at almost all times. The train accel-
erates if v<100 and brakes if v=100; it moves according to the system of
ODEs x′ = v, v′ = a. The evolution domain constraint v≤100 indicates an
event-triggered controller [26].

220
K. Cordwell and A. Platzer
A natural question is why we choose to build in reasoning about □tae by
developing PdTL instead of having the user edit the model by, for example,
making the postcondition v≤100 instead of v<100. Indeed, in this particular
case that would make the program safe at every moment in time. However, in
other examples, editing the postcondition in a similar way may be unwise. For
example, although [x := 0; y := 0; {x′ = 0, y′ = 1}]□tae (y>0 →x>1 ∨x<1)
is valid, if we relax the inequalities, [x := 0; y := 0; {x′ = 0, y′ = 1}]
□tae(y ≥0 →x ≥1 ∨x ≤1) is not. As reasoning about hybrid systems is
so subtle, the most user-friendly approach to eliding sets of measure zero is to
speciﬁcally introduce the rigorous ability to ignore sets of measure zero into the
logic.
4
Semantics of PdTL
We now report a trace semantics for hybrid programs, based on which we give
meaning to the informal concept of formulas being true almost everywhere in
time, ﬁrst along an individual trace and then along all traces of a hybrid program.
4.1
Semantics of State Formulas
State formulas are evaluated at states, which capture the behavior of the hybrid
program at an instant in time. Each state contains the values of all relevant
variables at a given instant. We formalize this in the following deﬁnition.
Deﬁnition 3. A state is a map ω : Σvar →R. We distinguish a separate state
Λ to indicate the failure of a system run. The set of all states is Sta(Σvar).
We now give the semantics of state formulas. The val(ω, φ) operator deter-
mines whether state formula φ is true or false in state ω. The valuations of the
state formulas [α]κ and ⟨α⟩κ depend on the semantics of traces σ (especially the
notion of ﬁrst σ), which is explained in Deﬁnition 5, and on the semantics of the
trace formula κ (i.e., val(ω, κ)) which is given later, in Deﬁnition 12, and may
be undeﬁned.
Deﬁnition 4 ([25]). The valuation of state formulas with respect to state ω is
deﬁned inductively:
1. val(ω, p(θ1, . . . , θn)) is pℓ(val(ω, θ1), . . . , val(ω, θn)) where pℓis the relation
associated with p under the semantics of real arithmetic
2. val(ω, ¬φ) is true iﬀval(ω, φ) is false
3. val(ω, φ ∧ψ) is true iﬀval(ω, φ) is true and val(ω, ψ) is true
4. val(ω, ∀x φ) is true iﬀval(ωd
x, φ) is true for all d ∈R, where ωd
x is the state
that is identical to ω, except x has the value d.
5. val(ω, [α]κ) is true iﬀfor every trace σ of α that starts in ﬁrst σ = ω, if
val(σ, κ) is deﬁned, then val(σ, κ) is true
6. val(ω, ⟨α⟩κ) is true iﬀthere is some trace σ of α where ﬁrst σ = ω and
val(σ, κ) is true
We write ω |= φ when val(ω, φ) is true. We write ω ̸|= φ when val(ω, φ) is
false.

Towards Physical Hybrid Systems
221
4.2
Traces of Hybrid Programs
Trace formulas are evaluated with respect to an execution trace of a hybrid pro-
gram. Intuitively, a trace of a hybrid program views its behavior over time as a
sequence of functions, where each function corresponds to a particular discrete
or continuous portion of the dynamics. Most hybrid systems are associated with
multiple traces (to reﬂect the variety of behaviors that a given program can
exhibit). Each function within a trace maps from a time interval to states of the
hybrid program. Continuous portions of traces are functions from an uncountable
time interval, and thus are associated to uncountably many states, whereas dis-
crete portions involve just a single state. Signiﬁcantly, traces are allowed to end
in an abort state Λ, which indicates an unsuccessful run of a program. Aborts are
incurred when tests fail and when evolution domain constraints are not initially
satisﬁed. No program can run past Λ. We review the formal deﬁnition below.
Deﬁnition 5 ([25]). A trace σ of a hybrid program α is a sequence of functions
σ = (σ0, σ1, . . . , σn) where σi : [0, ri] →Sta(Σvar). We will denote the length of
the interval associated to σi by |σi| (so that if σi maps from [0, ri] to states of α,
|σi| = ri). A position of σ is a tuple (i, ζ) where i ∈N, ζ ∈[0, ri]. Each position
(i, ζ) is associated with the corresponding state σi(ζ). A trace (σ0, σ1, . . . , σn) is
said to terminate if it does not end in the abort state, i.e. if σn(|σn|) ̸= Λ, and
we write last σ ≡σn(|σn|) in that case. We write ﬁrst σ ≡σ0(0) for the ﬁrst
state.
We now modify the trace semantics [25] using Carath´eodory solutions for
ODEs [30] using notation as in [26, Deﬁnition 2.6].
Deﬁnition 6. The state ν is reachable in the extended sense from initial state
ω by x′
1 = θ1, . . . , x′
n = θn & R iﬀthere is a function ϕ : [0, r] →Sta(Σvar) s.t.:
1. Initial and ﬁnal states match: ϕ(0) = ω, ϕ(r) = ν.
2. ϕ is absolutely continuous.
3. ϕ respects the diﬀerential equations almost everywhere: For each variable xi,
ϕ(z)(xi) is continuous in z on [0, r] and if r > 0, ϕ(z)(xi) has a time-
derivative of value ϕ(z)(θi) at all z ∈[0, r] \ U, for some set U ⊂[0, r]
that has Lebesgue measure zero.
4. The value of other variables y ̸∈{x1, . . . , xn} remains constant throughout
the continuous evolution, that is ϕ(z)(y) = ω(y) for all times z ∈[0, r];
5. ϕ respects the evolution domain at all times: ϕ(z) |= R for all z ∈[0, r].
If such a ϕ exists, we say that ϕ |= x′
1= θ1 ∧· · ·∧x′
n= θn & R almost everywhere.
This change highlights how the PHS intuition aligns with the intuition behind
Carath´eodory solutions. However, we have left the syntax of hybrid programs
unchanged, and any system of diﬀerential equations in this syntax has a unique
classical solution by Picard-Lindel¨of [26]. We believe that in order to determine
a suitable generalization of the syntax of hybrid programs to allow systems
of ODEs with Carath´eodory solutions, one should ﬁrst develop strategies for

222
K. Cordwell and A. Platzer
reasoning about ODEs in PdTL that are beyond the scope of this work (for
example, a notion of diﬀerential invariants—see [26,27]).
Note that in condition 5 of Deﬁnition 6, the solution is required to stay within
the evolution domain constraint at all times. This is because evolution domain
constraints, when used correctly, are nonnegotiable—for example, a correct use
of evolution domain constraints is to reﬂect some underlying property of physics,
like that the speed of a decelerating system is always nonnegative.
We deﬁne the trace semantics [25], with the above change for ODEs. Like
evolution domain constraints, we treat tests as nonnegotiable, so as not to inter-
fere with a user’s ability to write precise models. We do not intend to secretly
change the meaning of models, but rather to identify physically correct models.
Deﬁnition 7. The trace semantics, τ(α), of a hybrid program α is the set of all
its possible hybrid traces and is deﬁned inductively as follows (where e ∈Trm(Σ),
x′=f(x) is a vectorial ODE, R and P are FOL formulas, β is a hybrid program,
and where for a state ω, ˆω is the function from [0, 0] →Sta(Σvar) with ˆω(0) = ω):
1. τ(x := e) = {(ˆω, ˆν) : ν = ωval(ω,e)
x
for ω ∈Sta(Σvar)}
2. τ(x′=f(x) & R) = {(ϕ) : ϕ |= x′=f(x) & R almost everywhere} ∪{(ˆω, ˆΛ) :
ω ̸|= R}
3. τ(α ∪β) = τ(α) ∪τ(β)
4. τ(?P) = {(ˆω) : val(ω, P) = true} ∪{(ˆω, ˆΛ) : val(ω, P) = false}
5. τ(α; β) = {σ ◦ζ : σ ∈τ(α), ζ ∈τ(β) when σ ◦ζ is deﬁned}; the composition
of σ = (σ0, σ1, σ2, . . . ) and ζ = (ζ0, ζ1, ζ2, . . . ) is
σ ◦ζ :=
⎧
⎪
⎨
⎪
⎩
(σ0, . . . , σn, ζ0, ζ1, . . . )
if σ terminates at σn and last σ = ﬁrst ζ
σ
if σ does not terminate
not deﬁned
otherwise
6. τ(α∗) = 
n∈N τ(αn), where αn+1 := (αn; α) for n ≥1, and α0 := ?(true)
As an important remark, notice that if we have a trace σ = (σ0, . . . , σn) and
|σi| > 0, then σi ∈τ(x′ = f(x) & R) for some (vectorial) ODE x′ = f(x) and
some evolution domain constraint R. In other words, only continuous portions
of a trace have nonzero duration, and continuous portions are only introduced
when our system is evolving subject to a system of diﬀerential equations.
We are almost ready to give the semantics of trace formulas, but ﬁrst we
need to take a slight detour to discuss what formulas make “physical sense”.
4.3
Physical Formulas
One feature of our motivating example is that v<100 is not a physically mean-
ingful postcondition: If v is allowed to get arbitrarily close to 100, v=100 should
also be allowed, since there is no physically measurable diﬀerence between v<100
and v≤100. The postcondition v≤100 is, mathematically speaking, less restric-
tive than v<100, but also practically speaking, the same as v<100. Motivated
by this intuition, we deﬁne “physical formulas”.

Towards Physical Hybrid Systems
223
Physical Formulas and φ. Geometrically, the set of states in which a
state formula φ in n variables is true is a subset, φ = {(x1, . . . , xn) ∈
Rn | φ(x1, . . . , xn)}, of Rn. We use this correspondence to deﬁne the physical
version of φ.
Deﬁnition 8. A state formula φ is called physical iﬀφ is topologically closed.
If φ is a formula in n variables x1, . . . , xn, then the physical version of φ is the
closure, denoted by φ, and is, indeed, deﬁnable [3] by:
∀ϵ>0 ∃y1, . . . , yn

φ(y1, . . . , yn) ∧(x1 −y1)2 + · · · + (xn −yn)2 < ϵ2
This satisﬁes φ = φ, where φ is the topological closure of φ. Quantiﬁer
elimination can compute a quantiﬁer-free equivalent of φ that is often preferable.
Associating a state formula to a subset of Rn is useful for identifying which
points are “almost included”, which will be crucial knowledge for our temporal
approach. In the train example, v=100 is “almost included” in the postcondition
v<100. These points that are “almost included” in formula φ are exactly the limit
points of the set associated to φ, so φ adds in all of these limit points. We will
make use of the following properties of the physical version of φ. Both are proved
in a companion report [5].
Proposition 9. For any state formula φ, φ →φ is valid (i.e., true in all states).
Proposition 10. The following proof rule is sound for state formulas φ, ψ (i.e.,
the validity of all premises implies the validity of the conclusion):
φ →ψ TopCl
φ →ψ
4.4
Semantics of Trace Formulas
Intuitively, we want to say that, for a trace σ, σ |= □t.a.eφ when there is only a
“small” set of positions (i, ζ) where σi(ζ) ̸|= φ and where the discrete portions
of σ satisfy a reasonable constraint. To formalize the notion of a “small” set of
positions, we map positions of σ to R, since R admits the Lebesgue measure.
Deﬁnition 11. Given a trace σ = (σ0, . . . , σn) of a hybrid program α with
|σi| = ri, map each position (i, ζ) of σ to ζ + i + i−1
k=0 |σk|, so that the positions
(0, 0), . . . , (0, r0) cover the interval [0, r0], the positions (1, 0), . . . , (1, r1) cover
the interval [r0 + 1, r0 + r1 + 1], and so on. In this way we have an injection,
which we call f, from positions of a trace σ to (a subset of) R.
Figure 1 illustrates the mapping f, which is obtained by ﬁrst concatenating
the positions between each discrete step (i.e. the positions for each continuous
function σi) and then projecting these concatenations onto a single time axis

224
K. Cordwell and A. Platzer
Fig. 1. Injectively mapping positions of a trace to times in R
so that the images of the states for σi and the states for σj are disjoint when
i ̸= j. To ensure disjointness, our mapping places an open interval of unit length
between the images of the states of σi and the states of σi+1 (for all i). The
unit length was chosen arbitrarily—any nonzero length would work—but it is
important that the positions (i, ri) and (i+1, 0) have diﬀerent projections, since
discrete changes can cause their states σi(ri) and σi+1(0) to be diﬀerent.
Deﬁnition 12. The valuation of a trace formula κ with respect to trace σ is
deﬁned as:
1. val(σ, φ) = val(last σ, φ) if σ terminates. If σ does not terminate, then
val(σ, φ) is undeﬁned. We write σ |= φ when val(σ, φ) is true. We write
σ ̸|= φ when val(σ, φ) is false.
2. Let U be the set of positions (i, ζ) where corresponding states σi(ζ) satisfy
σi(ζ) ̸|= φ and σi(ζ) ̸= Λ. We say that val(σ, □taeφ) is true iﬀthe following
two conditions are satisﬁed:
(a) (Discrete condition) For all i, if |σi| = 0 and σi(0) ̸= Λ, then σi(0) |= φ.
(b) (Continuous condition) f(U) ⊆R has measure zero with respect to the
Lebesgue measure, where f is the mapping deﬁned in Deﬁnition 11 for σ;
i.e., for all ϵ > 0 there exist intervals Ip = [ap, bp] so that f(U) ⊆∞
p=0 Ip
and ∞
p=0 |bp −ap| < ϵ (see [16, Section 1-1013], or [28]).
We write σ |= □taeφ when val(σ, □taeφ) is true. We write σ ̸|= □taeφ when
val(σ, □taeφ) is false.
A short primer on the measure theory we need is in a companion report [5].
With this mapping, the condition that f(U) has measure zero with respect to
the Lebesgue measure enforces the t.a.e. constraint for the continuous portions
of our program. The image of the states for σi is an interval of length ri. In order
for the t.a.e constraint to be satisﬁed, σi cannot go wrong except at a “small
set” of states, where we use our mapping to formalize the notion of a “small set”
in terms of measure zero.
Next, the condition that σi(0) |= φ whenever |σi| = 0 constrains the discrete
portions of a program. This constraint will be important for induction; it also
ensures that discrete programs behave reasonably within our logic. For example,

Towards Physical Hybrid Systems
225
let α be the fully discrete program x := 5; (x := x + 1)∗, and take a trace σ
of α. The states of σ map to the points 5, 6, 7, . . . , n, and without the discrete
condition, we would be able to show that [x := 5; (x := x + 1)∗]□taex<5 is valid,
even though x is never less than 5 along the trace.
To understand why the discrete condition speciﬁes σi(0) |= φ when |σi| = 0
instead of σi(0) |= φ when |σi| = 0, recall the motivating train control example.
We want to allow the velocity of the train to evolve from a safe state where
v<100 to an unsafe state where v=100, as long as the train then immediately
brakes (sets its acceleration to a negative value). If we speciﬁed that σi(0) |= φ,
the train would not be allowed to accelerate from v<100 to v=100 and then
brake, because at the discrete braking point, the train would be in a state that
is unsafe mathematically (though still safe physically).
Given a hybrid program α, postcondition φ, and a state ω, we are interested
in determining whether ω |= [α]□taeφ holds, because when such a formula is true
for a given hybrid program, that indicates that no matter how that particular
hybrid program runs, it will be “safe almost everywhere”. Following Deﬁnition 4,
this is true iﬀfor each trace σ ∈τ(α) with ﬁrst σ = ω, σ |= □taeφ.
5
Discussion
Now that we have developed the semantics, we step back to consider how PdTL
makes progress towards PHS, and why PdTL is a good way to introduce PHS.
Impact on Modeling. PdTL allows the veriﬁcation of several classes of physically
realistic models that are mathematically not quite safe. For example, in Sect. 7
we will explain how PdTL allows the veriﬁcation of the train control model.
This simple train example is representative of a greater class of examples—
tiny glitches are common in event-triggered controllers, since the event that is
being detected is often an almost unsafe event that requires the controller to
immediately change behavior.
Other examples do not involve time-triggered controllers, but rather suf-
fer from tiny glitches at handover points between the discrete and continuous
dynamics within a hybrid program. Consider the safety postcondition x2+y2 < 1
and the hybrid program x := 0; y := 1; {x′ = −x, y′ = −y}. The only glitch is
that the program starts ever so slightly outside the safe set. Since it immediately
moves into the safe set, all runs of this hybrid program are safe tae. PdTL is
designed to classify [x := 0; y := 1; {x′ = −x, y′ = −y}]□taex2 + y2<1 as valid.
In another class of examples, our approach handles tiny glitches within the
continuous portion of the program. This can easily happen if the postcondition
is missing some small regions. For example, consider two robots that are moving,
one in front of the other. Since we do not want the robots to collide, it is unsafe for
the second robot to accelerate while the ﬁrst robot is braking. Say we model this
with safety postcondition ¬(a1 ≤0 ∧a2 ≥0). This is a small modeling mistake,
because we should allow the point where a1 = 0 and a2 = 0. Now, if our controller
is a1 := −1; a2 := −1; {a′
1 = 1, a′
2 = 1}, any run of this hybrid program is tae

226
K. Cordwell and A. Platzer
safe, but not safe at all points in time (as some runs will contain the origin).
Notably, the very similar controller a1 := −1; a2 := −1; {a′
1 = 1, a′
2 = 2} is not
tae safe. PdTL is designed to distinguish between these two controllers.
Why tae? The tae safety notion along the trace of a hybrid system is a natural
approach with strong mathematical underpinnings (e.g., from the invariance of
Lebesgue integrals up to sets of measure zero, and from Carath´eodory solutions),
and with physical motivation from examples like those just discussed. Introduc-
ing tae is a good way to begin PHS, because it may be the closest possible PHS
construct to the canonical notion of “safety everywhere”. However, this closeness
to safety everywhere does make tae more restrictive than some other possible
PHS notions—for example, a notion of safety “space almost everywhere”, or sae.
Consider a self-driving car moving in R3. The ﬁnal states of a hybrid program
α modeling the car correspond to positions in R3, so here we may wish to consider
safety almost everywhere with respect to the Lebesgue measure on the set of all
possible ﬁnal states of α as follows: Given a hybrid program α and precondition
ψ, let F = {last σ s.t. σ ∈τ(α), ﬁrst σ |= ψ}. Say that ψ ⊢[α]□μsaeφ if μ({ω ∈
F | ω ̸|= φ}) = 0, where μ is the Lebesgue measure on R3. This modality has
an intuitive geometric interpretation and is more permissive than tae.
However, □μsae applies only when there is a natural measure μ on the set
of all possible ﬁnal states of α. Furthermore, □μsae is not compositional. Given
P ⊢[α]□μsaeP and P ⊢[β]□μsaeP, in order to conclude that P ⊢[α; β]□μsaeP,
one needs to know that β is sae safe when starting in P ∪Q, for any measure
zero set Q reachable from α. This is not always true—for example, let P be
x2 + y2 < 1, α be {x′ = 1, y′ = 1 & x2 + y2 ≤1} and β be ?(x2 + y2 = 1);
{x′ = 1, y′ = 1}. Further, given α, it is unclear how to syntactically classify such
sets Q—as sae is more relaxed than tae, it also seems to be less well-behaved.
Thus, although □μsae has some advantages, it is not clear how to constrain it
to achieve desirable logical properties like compositionality. Although we hope
that future work will develop a notion of safety sae, the challenges therein are
no small matter. In contrast, tae satisﬁes many nice logical properties, which we
now turn our attention to.
6
Proof Calculus and Properties of PdTL
Before developing the proof calculus for PdTL, we discuss some key properties.
First, PdTL is a conservative extension of dL, i.e. all valid formulas of dL are
still valid in PdTL. The proof of this, discussed in a companion report [5], is
essentially the same as the proof that dTL is a conservative extension of dL
[25, Proposition 4.1]. This conservativity property is useful, since if we are able
to reduce temporal PdTL formulas to dL formulas, we can use the extensive
machinery built for dL to close proofs. Indeed, our proof calculus is designed
to reduce temporal PdTL formulas into nontemporal formulas to rely on dL’s
capabilities for the latter.

Towards Physical Hybrid Systems
227
Key dL axioms are proved sound for PdTL in a companion report [5], which
is useful as sometimes a dL axiom is needed to reduce the goal in a proof, as we
will see when we analyze the train example in Sect. 7.
Next, we state three properties of temporal PdTL formulas which underlie
some of the soundness proofs for rules in the proof calculus. These properties
hold by construction. All proofs are in the companion report [5].
Lemma 13. If σ |= □taeφ for a terminating σ, then last σ |= φ.
Corollary 14. The formula [α]□taeφ →[α]φ is valid.
Lemma 15. If ξ and η are traces of hybrid programs where ξ terminates and
last ξ = ﬁrst η, then ξ ◦η |= □taeφ iﬀboth ξ |= □taeφ and η |= □taeφ.
6.1
Proof Calculus
The proof calculus for PdTL is shown in Fig. 2. Intuitively, all of the axioms
are designed to successively decompose complicated formulas into structurally
simpler formulas while successively reducing trace formulas into state formulas.
The test axiom, assignment axiom, solution axiom, and solution with evolution
domain constraint axiom ([?]tae, [:=]tae, [′]tae, and [′&]tae) remove instances of
□tae. The nondeterministic choice axiom [∪]tae reduces a choice between two
hybrid programs to two separate programs. The induction axiom Itae reduces
a loop property involving a trace formula to a loop property involving a state
formula; Itae also allows us to derive two very useful proof rules.
Fig. 2. Proof calculus for PdTL (Here, α and β are hybrid programs, φ and ψ are
state formulas, P is a FOL formula, y(t) is the unique global polynomial solution to
the diﬀerential equation x′ = f(x), and the formula Q in [′]tae and [′&]tae is the
FOL formula constructed by Proposition 16 for P(y(t)). Although the “for almost
all” quantiﬁer is in general not deﬁnable in FOL [23], Proposition 16 justiﬁes that
“for almost all t≥0[x := y(t)]P” is logically equivalent to “∀t≥0 Q”.)

228
K. Cordwell and A. Platzer
The G¨odel generalization rule (Gtae) proves that if formula φ is valid, then it
is also true, tae, along the trace of any hybrid program. The modal modus ponens
rule (Ktae) allows us to derive a monotonicity property. Our approach occasion-
ally introduces extra premises; for example, the modal modus ponens rule (Ktae)
has an extra goal φ →ψ due to the discrete condition of Deﬁnition 12. Many of
these extra premises will be easy to prove—if our models make use of physical
formulas, which are closed, then these extra cases will prove immediately.
Soundness proofs are in the report [5]. We discuss a few key high-level ideas.
Soundness of [; ]tae and Itae. Sequential composition and induction are sub-
tly challenging for PHS—since we are allowed to leave the safe set, handover
points between hybrid programs are no longer guaranteed to be safe points.
However, sequential composition and induction are crucial for the practicality of
veriﬁcation, which is predicated on having a good way of breaking down compli-
cated formulas into simpler components. The soundness proof of [; ]tae exploits
Lemma 15. The soundness proof of Itae is based on Lemma 13, which in turn
relies on the discrete condition of Deﬁnition 12.
Diﬀerential Equations. Reasoning about diﬀerential equations is one of the most
challenging aspects of hybrid systems. In this work, we focus on relatively simple
reasoning principles for diﬀerential equations, as justifying even simple principles
is made much more challenging by introducing the notion of “safety almost every-
where”. We leave the development of more complicated reasoning (for example,
a notion of diﬀerential invariants for □tae) to future work.
For “suﬃciently tame” systems of ODEs x′ = f(x), we might hope to replace
[x′ = f(x)]□taeP with an equivalent expression without the □tae modality. The
cleanest case is when x′ = f(x) has a unique global polynomial solution, y(t).
Although we think of y as being a polynomial in t, y can involve any of the other
parameters, call them x1, . . . , xn, in f, from its dependency on initial values.
We require that y is also polynomial in x1, . . . , xn. This is the case handled
by axiom [′]tae: [x′ = f(x)]□taeP ↔P ∧∀t≥0 Q, where Q is a FOL formula
constructed so that “∀t≥0 Q” expresses “for almost all t≥0 [x := y(t)]P”. Axiom
[′&]tae generalizes [′]tae to ODEs with evolution domain constraints.
In particular, we get Q by applying Proposition 16 to P(y(t)), which is the
formula obtained using the assignment axiom [:=] of dL on [x := y(t)]P (it is a
FOL formula because y(t) is polynomial and polynomials are closed under com-
position). Given any FOL formula P, Proposition 16 constructs a FOL formula
Q so that “for almost all t≥0 P” is semantically equivalent to ∀t≥0 Q (i.e., “for
almost all t≥0 P” is true in a state ω iﬀ∀t≥0 Q is true in ω).
Proposition 16. Let P be a FOL formula. Using quantiﬁer elimination [29],
put it into one of the following normal forms: e = 0, e ≥0, e < 0, P1 ∧P2,
and P1 ∨P2, where e is a polynomial and P1, P2 are FOL formulas. Construct
the FOL formula Q = g(P) by structural induction on P as follows: g(e = 0) is
e = 0, g(e ≥0) is e ≥0, g(e < 0) is e ≤0 ∧((an = 0 ∧· · · ∧a1 = 0) →e < 0),
g(P1 ∧P2) is g(P1) ∧g(P2), and g(P1 ∨P2) is g(P1) ∨g(P2).

Towards Physical Hybrid Systems
229
Then, for any state ω, the following hold:
1. Locally false: If ωk
t ̸|= Q for some k ≥0, then there is a nonempty interval
[k, ℓ) so that for all q ∈[k, ℓ), ωq
t ̸|= P. Further, if k > 0, then there is an
interval (ℓ1, ℓ2) with ℓ1 < k < ℓ2 so that for all q ∈(ℓ1, ℓ2), ωq
t ̸|= P.
2. Finite diﬀerence: There are only ﬁnitely many values k ≥0 where ωk
t
|=
Q ∧¬P.
The proof is by induction on the structure of P; details are in the report [5].
6.2
Derived Rules
We highlight some of the most useful derived rules for PdTL formulas in Fig. 3.
Monotonicity properties are fundamental in logic. Our rule Mtae intuitively says
that if ψ →φ is valid, then if ψ is true almost everywhere along every trace
of a hybrid program, then φ is also true almost everywhere along every trace
of that hybrid program. The rule Indtae reduces proving a safety property of
hybrid program α∗to proving a safety property of program α. When its premise
proves, it eﬀectively removes the need to reason about loops. The rule looptae
provides us with a loop invariant rule. The rule Comptae reduces a property of
α; β to individual properties of α and β. The derivations are given in the report
[5].
Fig. 3. Derived rules for PdTL
7
Proof of Motivating Example
We now apply our proof calculus to the model of the train example (Sect. 3).
Full details are in the report [5]. Using structural rule →R and our induc-
tion proof rule looptae with invariant v<100, the proof reduces to showing
a=0∧v=0 ⊢v≤100 (which holds by real arithmetic), v<100 ⊢v<100 (identi-
cally true), and
v≤100 ⊢[

(?(v<100); a := 1) ∪(?(v=100); a := −1)

;
{x′ = v, v′ = a & 0≤v≤100}]□taev<100.
Axiom [; ]tae splits this into goals (1) and (2):
v≤100 ⊢[(?(v<100); a := 1) ∪(?(v=100); a := −1)]□taev<100
(1)

230
K. Cordwell and A. Platzer
v≤100 ⊢[(?(v<100); a := 1) ∪(?(v=100); a := −1)]
[{x′ = v, v′ = a & 0≤v≤100}]□taev<100.
(2)
(1) is straighforward. (2) is more complicated because it involves ODEs rea-
soning. The dL axioms [∪] and ∧R split the proof of (2) into (3) and (4):
v≤100 ⊢[?(v<100); a := 1][{x′ = v, v′ = a & 0≤v≤100}]□taev<100
(3)
v≤100 ⊢[?(v=100); a := −1][{x′ = v, v′ = a & 0≤v≤100}]□taev<100
(4)
(3) and (4) require similar reasoning, so we focus on (3). The dL axioms [; ], [:=],
and [?] reduce (3) to
v≤100, v<100 ⊢[{x′ = v, v′ = 1 & 0≤v≤100}]□taev<100
(5)
To prove (5), we need to use axiom [′&]tae, which says:
[x′ = f(x)&R]□taeP ↔P ∧∀t>0 ((∀0≤s≤t [x := y(s)]R) →Q)
For clarity, we use v0 for the value of v in the initial state before it starts
evolving along the ODEs, and similarly we use x0 for the value of x in the
initial state. Following Proposition 16, Q is (1 = 0 →t + v0<100) ∧t + v0≤100.
Since applying [′&]tae reduces our goal to a dL formula, and since PdTL is a
conservative extension of dL, we can use the contextual equivalence rules of dL
to replace Q with the logically equivalent formula t + v0≤100, obtaining:
v0≤100, v0<100 ⊢v0≤100 ∧∀t>0

∀0≤s≤t [x := .5s2 + v0s + x0][v := s + v0]0≤v≤100

→t + v0≤100

(6)
After using the dL axiom [:=], the proof closes by real arithmetic.
8
Conclusions and Future Work
We introduce PHS to help narrow the gap between mathematical models and
physical reality. To enable logic to begin to distinguish between true unsafeties
of systems and physically unrealistic unsafeties, we develop the notion of safety
tae along the execution trace of a system. Our new logic, PdTL, contains the
logical operator □tae, which elides sets of time that have measure zero.
A cornerstone of our approach is its logical practicality—in order to support
veriﬁcation, we develop a proof calculus for PdTL. We demonstrate the capabil-
ity of the proof calculus by applying it to a motivating example. We think it is
an interesting and challenging problem for future work to develop new ways of
thinking about PHS, such as the notion of space almost everywhere discussed in
Sect. 5, while maintaining this logical practicality.
Future work could continue to develop PdTL. It would be especially interest-
ing to develop further diﬀerential equations reasoning, including an appropriate
generalization of the syntax of hybrid programs to admit Carath´eodory solutions.
Acknowledgements. We very much appreciate Yong Kiam Tan and Rose Bohrer
for many useful discussions and for feedback on the paper. Thank you also to the
anonymous CADE’19 reviewers for their thorough feedback.

Towards Physical Hybrid Systems
231
References
1. Alur, R., Courcoubetis, C., Henzinger, T.A., Ho, P.H.: Hybrid automata: an algo-
rithmic approach to the speciﬁcation and veriﬁcation of hybrid systems. In: Gross-
man et al. [14], pp. 209–229
2. Asarin, E., Bouajjani, A.: Perturbed turing machines and hybrid systems. In: Pro-
ceedings of the 16th Annual IEEE Symposium on Logic in Computer Science,
Boston, Massachusetts, USA, 16–19 June 2001, pp. 269–278. IEEE Computer Soci-
ety (2001). https://doi.org/10.1109/LICS.2001.932503
3. Bochnak, J., Coste, M., Roy, M.F.: Real Algebraic Geometry. Springer, Heidelberg
(1998). https://doi.org/10.1007/978-3-662-03718-8
4. Ceragioli, F.: Some remarks on stabilization by means of discontinuous feed-
backs. Syst. Control Lett. 45(4), 271–281 (2002). https://doi.org/10.1016/S0167-
6911(01)00185-2
5. Cordwell, K., Platzer, A.: Towards physical hybrid systems. CoRR abs/1905.09520
(2019). http://arxiv.org/abs/1905.09520
6. Cortes, J.: Discontinuous dynamical systems. IEEE Control Syst. 28(3), 36–73
(2008). https://doi.org/10.1109/MCS.2008.919306
7. Dam, M.: CTL* and ECTL* as fragments of the modal μ-calculus. Theor. Comput.
Sci. 126(1), 77–96 (1994). https://doi.org/10.1016/0304-3975(94)90269-0
8. Donz´e, A., Ferr`ere, T., Maler, O.: Eﬃcient robust monitoring for STL. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 264–279. Springer,
Heidelberg (2013). https://doi.org/10.1007/978-3-642-39799-8 19
9. Fainekos, G.E., Pappas, G.J.: Robustness of temporal logic speciﬁcations for
continuous-time signals. Theor. Comput. Sci. 410(42), 4262–4291 (2009). https://
doi.org/10.1016/j.tcs.2009.06.021
10. Fr¨anzle, M.: Analysis of hybrid systems: an ounce of realism can save an inﬁnity of
states. In: Flum, J., Rodriguez-Artalejo, M. (eds.) CSL 1999. LNCS, vol. 1683, pp.
126–139. Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48168-0 10
11. Gao, S., Avigad, J., Clarke, E.M.: Delta-decidability over the reals. In: Proceedings
of the 27th Annual IEEE Symposium on Logic in Computer Science, LICS 2012,
Dubrovnik, Croatia, 25–28 June 2012, pp. 305–314. IEEE Computer Society (2012).
https://doi.org/10.1109/LICS.2012.41
12. Goebel, R., Sanfelice, R.G., Teel, A.R.: Hybrid dynamical systems. IEEE Control
Syst. Mag. 29(2), 28–93 (2009). https://doi.org/10.1109/MCS.2008.931718
13. Goebel, R., Hespanha, J., Teel, A.R., Cai, C., Sanfelice, R.: Hybrid systems: gener-
alized solutions and robust stability. IIFAC Proc. Vol. 37(13), 1–12 (2004). https://
doi.org/10.1016/S1474-6670(17)31194-1. 6th IFAC Symposium on Nonlinear Con-
trol Systems 2004 (NOLCOS 2004), Stuttgart, Germany, 1–3 September 2004
14. Grossman, R.L., Nerode, A., Ravn, A.P., Rischel, H. (eds.): HS 1991–1992. LNCS,
vol. 736. Springer, Heidelberg (1993). https://doi.org/10.1007/3-540-57318-6
15. Jeannin, J.-B., Platzer, A.: dTL2: diﬀerential temporal dynamic logic with nested
temporalities for hybrid systems. In: Demri, S., Kapur, D., Weidenbach, C. (eds.)
IJCAR 2014. LNCS (LNAI), vol. 8562, pp. 292–306. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-08587-6 22
16. Jeﬀreys, H., Swirles, B.: Methods of Mathematical Physics, 3rd edn. Cambridge
University Press, Cambridge (1999). https://doi.org/10.1017/CBO9781139168489
17. Kong, S., Gao, S., Chen, W., Clarke, E.: dReach: δ-reachability analysis for hybrid
systems. In: Baier, C., Tinelli, C. (eds.) TACAS 2015. LNCS, vol. 9035, pp. 200–
205. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-46681-0 15

232
K. Cordwell and A. Platzer
18. Manthanwar, A., Sakizlis, V., Dua, V., Pistikopoulos, E.: Robust model-based pre-
dictive controller for hybrid system via parametric programming. In: Puigjaner, L.,
Espu˜na, A. (eds.) European Symposium on Computer-Aided Process Engineering-
15, 38th European Symposium of the Working Party on Computer Aided Process
Engineering, Computer Aided Chemical Engineering, vol. 20, pp. 1249–1254. Else-
vier (2005). https://doi.org/10.1016/S1570-7946(05)80050-1
19. Mayhew, C.G., Sanfelice, R.G., Teel, A.R.: Robust source-seeking hybrid con-
trollers for autonomous vehicles. In: 2007 American Control Conference, pp. 1185–
1190, July 2007. https://doi.org/10.1109/ACC.2007.4283016
20. McCallum, S.: Solving polynomial strict inequalities using cylindrical alge-
braic decomposition. Comput. J. 36(5), 432–438 (1993). https://doi.org/10.1093/
comjnl/36.5.432
21. Moggi, E., Farjudian, A., Duracz, A., Taha, W.: Safe & robust reachability analysis
of hybrid systems. Theor. Comput. Sci. 747, 75–99 (2018). https://doi.org/10.
1016/j.tcs.2018.06.020
22. Moor, T., Davoren, J.M.: Robust controller synthesis for hybrid systems using
modal logic. In: Di Benedetto, M.D., Sangiovanni-Vincentelli, A. (eds.) HSCC 2001.
LNCS, vol. 2034, pp. 433–446. Springer, Heidelberg (2001). https://doi.org/10.
1007/3-540-45351-2 35
23. Morgenstern, C.F.: The measure quantiﬁer. J. Symb. Log. 44(1), 103–108 (1979).
https://doi.org/10.2307/2273708
24. Nerode, A., Kohn, W.: Models for hybrid systems: automata, topologies, control-
lability, observability. In: Grossman et al. [14], pp. 317–356
25. Platzer, A.: Logical Analysis of Hybrid Systems - Proving Theorems for Complex
Dynamics. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-14509-
4
26. Platzer, A.: Logical Foundations of Cyber-Physical Systems. Springer, Cham
(2018). https://doi.org/10.1007/978-3-319-63588-0
27. Platzer, A., Tan, Y.K.: Diﬀerential equation axiomatization: the impressive power
of diﬀerential ghosts. In: Dawar, A., Gr¨adel, E. (eds.) LICS, pp. 819–828. ACM,
New York (2018). https://doi.org/10.1145/3209108.3209147
28. Royden, H.L., Fitzpatrick, P.M.: Real Analysis (Classic Version), 4th edn. Pearson,
London (2018)
29. Tarski, A.: A decision method for elementary algebra and geometry. In: Cavi-
ness, B.F., Johnson, J.R. (eds.) Quantiﬁer Elimination and Cylindrical Algebraic
Decomposition. TEXTSMONOGR, pp. 24–84. Springer, Vienna (1998). https://
doi.org/10.1007/978-3-7091-9459-1 3
30. Walter, W.: Ordinary Diﬀerential Equations. Graduate Texts in Mathematics, vol.
182. Springer, New York (1998). https://doi.org/10.1007/978-1-4612-0601-9

SCL
Clause Learning from Simple Models
Alberto Fiori1,2 and Christoph Weidenbach1(B)
1 Max Planck Institute for Informatics, Saarland Informatics Campus,
Saarbr¨ucken, Germany
weidenbach@mpi-inf.mpg.de
2 Graduate School of Computer Science, Saarbr¨ucken, Germany
Abstract. Several decision procedures for the Bernays-Schoenﬁnkel
(BS) fragment of ﬁrst-order logic rely on explicit model assumptions. In
particular, the procedures diﬀer in their respective model representation
formalisms. We introduce a new decision procedure SCL deciding the
BS fragment. SCL stands for clause learning from simple models. Simple
models are solely built on ground literals. Nevertheless, we show that
SCL can learn exactly the clauses other procedures learn with respect to
more complex model representation formalisms. Therefore, the overhead
of complex model representation formalisms is not always needed. SCL
is sound and complete for full ﬁrst-order logic without equality.
1
Introduction
There has been intensive research into the development of decision procedures
for the Bernays-Schoenﬁnkel (BS) ﬁrst-order clause fragment without equal-
ity [1,3,4,7,9,14,17]. Even classical tableau can be turned into a decision pro-
cedure for BS [2]. The procedures follow three diﬀerent paradigms. They either
employ an explicit CDCL-style [12] partial model assumption [1,3,4,14], or they
implement an abstraction-reﬁnement approach [9,17], or merely rely on syntactic
restrictions on inferences [7] yielding ﬁnite saturations.
The BS fragment is a natural generalization of propositional logic but still
enjoys the ﬁnite-model property. Furthermore, any ﬁnite BS clause set can be
transformed into a satisﬁability equivalent SAT problem by ﬁnite instantia-
tion at the price of a, worst case, exponentially larger clause set. For example,
this relationship is used as a reasoning principle in Answer Set Programming
(ASP) [8]. The exponential “overhead” is, in the worst case, unavoidable for any
decision procedure, because BS satisﬁability is NEXPTIME-complete [11,15].
This means, worst case, that an explicit model representation gets exponentially
large, or satisﬁability testing with respect to the model representation cannot
be done in polynomial time. This justiﬁes and motivates the research for pro-
cedures with diﬀerent model representation formalisms as well as alternative
approaches through abstraction or saturation. Actually, the leading systems at
recent CASCs [16] have implemented a portfolio containing procedures from all
of the aforementioned paradigms.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 233–249, 2019.
https://doi.org/10.1007/978-3-030-29436-6_14

234
A. Fiori and C. Weidenbach
One contribution of this paper is a CDCL-style calculus deciding the BS
fragment and being sound and complete for full ﬁrst-order logic without equality.
The model representation is simple: it consists of a sequence of ground literals.
It is therefore properly contained in known model representation formalisms [1,
3,4,14]. However, we show that this model representation formalism together
with the respective inference rules is suﬃcient to learn the very same clauses
as in NRCL [1]. NRCL has one of the most expressive model representation
formalisms. We call the procedure SCL for clause learning from simple models.
The most important computations with respect to a model representation are
the consistent extension of the current trail and the detection of a propagating
literal or a false clause. The currently available procedures can be further divided
into procedures where these computations can be done in polynomial time [3,6]
and procedures where such computations are worst case NP-complete [1,14].1
The advantages of the latter two formalisms are exponentially more compact
model representations where the model representation language of the NRCL
calculus [1] is more general than [3,6,14]. One contribution of this paper is
that for model-driven clause learning, sophisticated model representations are
not needed (Theorem 24). More precisely, we prove that any clause learned
by the NRCL calculus can also be learned by our new SCL calculus, where the
model representation consists of ground literals only. One of the simplest but also
most eﬃcient model representations known with respect to computations. This
result holds for full ﬁrst-order logic without equality. Model representations for
BS clause sets with respect to ground literals can become exponentially larger
compared to the above-mentioned more sophisticated model representations.
The model size is exponential in the maximal arity of a predicate, which we
will discuss in detail in Sect. 5. The implication of our result is that SCL can be
eﬃciently used on problems where the ground literal model representation does
not become “too large”, further discussed in Sect. 6.
Another contribution in addition to SCL being sound, complete, and a deci-
sion procedure for the BS fragment is the fact that it only learns non-redundant
clauses with respect to so-called reasonable strategies, see Sect. 3. A clause is
redundant with respect to a clause set, if it is implied by smaller clauses, see
Sect. 2. Non-redundancy is a powerful property: in the BS context, we prove it
NEXPTIME-complete, Theorem 14. Practically, this implies that a clause gen-
erated by SCL with a reasonable strategy does not need to be tested for forward
redundancy, e.g., forward Subsumption. Saturation-based theorem provers spent
a substantial share of their run time on testing forward redundancy.
A third contribution concerning SCL is its ability to simulate resolution,
Sect. 4, Theorem 20. Arbitrary resolution steps may generate redundant clauses,
hence giving up a reasonable strategy is a prerequisite for the simulation. In this
context we also discuss the performance of SCL with respect to proof length,
Sect. 4, following [13].
Finally, we investigate a so called weakly-reasonable strategy, where prop-
agations need not to be exhaustive, speciﬁcally unit clauses need not to be
1 For [4] no complexity result has been published so far.

SCL Clause Learning from Simple Models
235
propagated. Although propagating unit clauses is typically a good strategy for
SAT, for the BS fragment this depends already on the actual problem, because
one unit clause may cause exponentially many subsequent propagation steps. In
summary, the weakly-reasonable strategy generates non-redundant clauses with
the exception of unit instances, Theorem 12, and allows for exponentially shorter
proofs compared to a reasonable strategy, which exhausts propagation, Exam-
ple 9. We end the paper with a short summary and discussion of the obtained
results, Sect. 6.
2
Preliminaries
We assume a ﬁrst-order language without equality where N denotes a clause
set; C, D denote clauses; L, K, H denote literals; A, B denote atoms; P, Q, R
denote predicates; t, s terms; f, g, h function symbols; a, b, c constants; and x, y, z
variables. Atoms, literals, clauses and clause sets are considered as usual. The
complement of a literal is denoted by the function comp. Semantic entailment
|= is deﬁned as usual where variables in clauses are assumed to be universally
quantiﬁed. Substitutions σ, τ are total mappings from variables to terms, where
dom(σ) := {x | xσ ̸= x} is ﬁnite and codom(σ) := {t | xσ = t, x ∈dom(σ)}.
Their application is extended to literals, clauses, and sets of such objects in the
usual way. A term, atom, clause, or a set of these objects is ground if it does
not contain any variable. A substitution σ is ground if codom(σ) is ground. A
substitution σ is grounding for a term t, literal L, clause C if tσ, Lσ, Cσ is
ground, respectively. A closure is denoted as C · σ and is a pair of a clause C
and a ground substitution σ. The function gnd computes the set of all ground
instances of a literal, clause, or clause set. Note that for BS this set is always
ﬁnite, whereas for ﬁrst-order logic it is inﬁnite, in general. The function mgu
denotes the most general uniﬁer of two terms, atoms, literals. We assume that
any mgu of two terms or literals does not introduce any fresh variables and is
idempotent.
In addition, we assume a well-founded, total, strict ordering ≺on ground
literals. This ordering is then lifted to clauses and clause sets by its respective
multiset extension. We overload ≺for literals, clauses, clause sets if the meaning
is clear from the context. The ordering is lifted to the non-ground case via
instantiation: we deﬁne C ≺D if for all grounding substitutions σ it holds
Cσ ≺Dσ. We deﬁne ⪯as the reﬂexive closure of ≺and N ⪯C := {D | D ∈
N and D ⪯C}.
Deﬁnition 1 (Clause Redundancy). A ground clause C is redundant with
respect to a ground clause set N and an order ≺if N ⪯C |= C. A clause C is
redundant with respect to a clause set N and an order ≺if for all C′ ∈gnd(C)
C′ is redundant with respect to ∪D∈N gnd(D).
3
SCL Rules and Properties
The inference rules of SCL are represented by an abstract rewrite system. They
operate on a problem state, a ﬁve-tuple (Γ; N; U; k; u) where Γ is a sequence of

236
A. Fiori and C. Weidenbach
annotated ground literals, the trail; N and U are the sets of initial and learned
clauses; k counts the number of decisions; and u is a status that is either true
⊤, false ⊥, or a closure C · σ. Literals in Γ are either annotated with a number,
a level; i.e., they have the form Lk meaning that L is the k-th guessed decision
literal, or they are annotated with a closure that propagated the literal to become
true. A ground literal L is of level i with respect to a problem state (Γ; N; U; k; u)
if L or comp(L) occurs in Γ and the ﬁrst decision literal left from L (comp(L))
in Γ, including L, is annotated with i. If there is no such decision literal then
its level is zero. A ground clause D is of level i with respect to a problem state
(Γ; N; U; k; u) if i is the maximal level of a literal in D; the level of the empty
clause ⊥is 0. Recall u is a non-empty closure or ⊤or ⊥.
A literal L is undeﬁned in Γ if neither L nor comp(L) occur in Γ. The initial
state for a ﬁrst-order clause set N is (ϵ, N, ∅, 0, ⊤). The rules for conﬂict search
are
Propagate
(Γ; N; U; k; ⊤) ⇒SCL (Γ, Lσ(C∨L)·σ; N; U; k; ⊤)
provided C ∨L ∈(N ∪U), Cσ is ground and false under Γ, Lσ is undeﬁned in Γ
Decide
(Γ; N; U; k; ⊤) ⇒SCL (Γ, Lk+1; N; U; k + 1; ⊤)
provided L is a ground literal undeﬁned in Γ
Conﬂict
(Γ; N; U; k; ⊤) ⇒SCL (Γ; N; U; k; D · σ)
provided D ∈(N ∪U), Dσ false in Γ for a grounding substitution σ
These rules construct a (partial) model via the trail Γ for N ∪U until a
conﬂict, i.e., a false clause with respect to Γ is found. The above rules always
terminate with respect to the BS fragment, but not for ﬁrst-order logic, in gen-
eral. In the special case of a unit clause L, the rule Propagate actually annotates
the literal L with a closure of itself. So the propagated literals on the trail are
annotated with the respective propagating clause and the decision literals with
the respective level. If a conﬂict is found, it is resolved by the rules below. Before
any Resolve step, we assume that the respective clauses are renamed such that
they do not share any variables and that the grounding substitutions of closures
are adjusted accordingly.
Skip
(Γ, Lδ(C∨L)·δ; N; U; k; D · σ) ⇒SCL (Γ; N; U; k; D · σ)
provided comp(Lδ) does not occur in Dσ
Factorize
(Γ; N; U; k; (D ∨L ∨L′) · σ) ⇒SCL (Γ; N; U; k; (D ∨L)η · σ)
provided Lσ = L′σ, η = mgu(L, L′)
Resolve
(Γ, Lδ(C∨L)·δ; N; U; k; (D∨L′)·σ) ⇒SCL (Γ, Lδ(C∨L)·δ; N; U; k; (D∨
C)η · σδ)
provided Dσ is of level k, Lδ = comp(L′σ), η = mgu(L, comp(L′))

SCL Clause Learning from Simple Models
237
Backtrack (Γ, Ki+1, Γ ′; N; U; k; (D ∨L) · σ) ⇒SCL (Γ; N; U ∪{D ∨L}; i; ⊤)
provided Lσ is of level k and Dσ is of level i.
The clause D∨L added by the rule Backtrack to U is called a learned clause.
The empty clause ⊥can only be generated by rule Resolve or be already present
in N, hence, as usual for CDCL style calculi, the generation of ⊥together with
the clauses in N ∪U represent a resolution refutation. The rules for SCL are
applied in a don’t-care style, hence, the calculus oﬀers freedom with respect to
factorization. Literals in the conﬂict clause can, but do not have to be factorized.
In particular, the Factorize rule may remove duplicate literals. The rule Resolve
does not remove the literal resolved upon from the trail. Actually, Resolve is
applied as long as the rightmost propagated trail literal occurs in the conﬂict
clause. This literal is eventually removed by rule Skip from the trail.
For example, consider the clause set N = {D = Q ∨R(a, y) ∨R(x, b), C =
Q ∨S(x, y) ∨P(x) ∨P(y) ∨¬R(x, y)} and a problem state:
(¬P(a)1, ¬P(b)2, ¬S(a, b)3, ¬Q4, ¬R(a, b)C·{x→a,y→b}, N, ∅, 4, ⊤)
derived by SCL. The rule Conﬂict is applicable and yields the conﬂict state
(¬P(a)1, ¬P(b)2, ¬S(a, b)3, ¬Q4, R(a, b)C·{x→a,y→b}; N; ∅; 4; D · {x →a, y →b})
from which we can either learn the clause
C1 = Q ∨S(x, b) ∨P(x) ∨P(b) ∨S(a, y) ∨P(a) ∨P(y)
or the clause
C2 = Q ∨S(a, b) ∨P(a) ∨P(b)
depending on whether we ﬁrst resolve or factorize. Note that C2 does not sub-
sume C1. Both clauses are non-redundant. In order to learn C1 we need to resolve
twice with R(a, b)C·{x→a,y→b}.
The ﬁrst property we prove about SCL is soundness. We prove it via the
notion of a sound state.
Deﬁnition 2 (Sound States). A state (Γ; N; U; k; u) is sound if the following
conditions hold
1. Γ is a consistent sequence of annotated ground literals,
2. for each decomposition Γ = Γ1, LσC∨L·σ, Γ2 we have that Cσ is false under
Γ1 and Lσ is undeﬁned under Γ1, C ∨L ∈(N ∪U),
3. for each decomposition Γ = Γ1, Lk, Γ2 we have that L is undeﬁned in Γ1,
4. N |= U,
5. if u = C · σ then Cσ is false under Γ and N |= C.
Note that an initial state (ϵ, N, ∅, 0, ⊤) is sound. A rule is sound if it maps
sound states to sound states.

238
A. Fiori and C. Weidenbach
Theorem 3 (Soundness of SCL). The rules of SCL are sound, hence SCL
starting with an initial state is sound.
Proof. (Idea) By induction on the length of an SCL derivation and a case analysis
for the diﬀerent rules preserving soundness of states.
⊓⊔
Next we introduce regular and weakly-regular runs. Regular runs always
generate non-redundant clauses, but require exhaustive propagation. Weakly-
regular runs do not require exhaustive propagation and almost always generate
non-redundant clauses except for instances of unit clauses. However, although
exhaustive propagation is typically done in CDCL style SAT, already for the
BS fragment it should not always be preferred, because unit clauses can have
already exponentially many ground instances.
Deﬁnition 4 (Regular States). A state (Γ; N; U; k; u) is regular if and only
if the following hold:
1. for every decomposition Γ = Γ1, Lk, Γ2 there is no clause in N ∪U that could
propagate from Γ1,
2. for each decomposition Γ = Γ1, L, Γ2 where L may be either propagated or
decided, there is no clause from gnd(N ∪U) false under Γ1.
Deﬁnition 5 (Weakly-Regular States). A state (Γ; N; U; k; u) is weakly-
regular if and only if the following hold:
1. for every decomposition Γ = Γ1, Lk, Γ2 there is no non-unit clause in N ∪U
that could propagate from Γ1,
2. for each decomposition Γ = Γ1, L, Γ2 where L may be either propagated or
decided, there is no clause from gnd(N ∪U) false under Γ1.
Some of the below results hold both for regular and weakly-regular states or
runs. In this case we write “(weakly-) regular” meaning both cases.
Theorem 6 (Correct Termination). If no rules are applicable to a (weakly-)
regular state (Γ; N; U; k; u) then either u = ⊥and N is unsatisﬁable or N is
satisﬁable and Γ |= N.
Proof. For a state (Γ; N; U; k; u) where u ̸∈{⊤, ⊥}, one of the rules Resolve,
Skip, Factorize or Backtrack is applicable. If the top level literal is a propagated
literal then either Resolve or Skip are applicable. If the top level literal is a
decision then one of the rules Backtrack or Factorize is applicable. If u = ⊤and
Propagate, Decide, and Conﬂict are not applicable it means that there are no
undeﬁned ground literals in Γ, so Γ |= N.
⊓⊔
Deﬁnition 7 (Regular Runs). A derivation of regular states is regular or a
regular run if the rules Conﬂict and Propagate are always applied before all other
rules in decreasing order of priority.
Deﬁnition 8 (Weakly-Regular Runs). A derivation of regular states is
weakly-regular or a weakly-regular run if the following conditions hold:

SCL Clause Learning from Simple Models
239
1. Conﬂict has higher priority than all other rules,
2. if Conﬂict is not applicable and we can apply Propagate to a non-unit clause
then Propagate has higher priority than any other rule,
3. Decide never adds a literal L to the trail if comp(L) is a unit clause in gnd(N∪
U),
4. Resolve has higher priority than Backtrack if the current conﬂict clause is
subsumed in N by a unit clause.
Example 9 (Comparing Proof Length of Regular and Weakly-Regular Runs).
Proofs generated by weakly-regular runs can be exponentially shorter than proofs
generated by regular runs. Consider the simple BS clause set
N = {R(x1, . . . , xn, a, b), P ∨Q, P ∨¬Q, ¬P ∨Q, ¬P ∨¬Q}.
A weakly-regular run can ignore generating the 2n diﬀerent ground instances of
R(x1, . . . , xn, a, b) and directly proceed in refuting the propositional part of N in
the usual CDCL style by starting with a decision on P or Q. For the example it
is obvious that the instances of R(x1, . . . , xn, a, b) can be ignored, but in general
it is not. This phenomenon already occurs for NP-complete problems: when
deciding linear integer arithmetic in a CDCL style, exhaustive propagation is
not required by respective calculi for the very same reason [5].
Deﬁnition 10 (State Induced Ordering). Let (L1, L2, . . . , Ln; N; U; k; u) be
a sound state of SCL where the annotations of the Li are ignored. The trail
induces a total well-founded strict order on the deﬁned literals by
L1 ≺Γ comp(L1) ≺Γ L2 ≺Γ comp(L2) ≺Γ · · · ≺Γ Ln ≺Γ comp(Ln).
We extend ≺Γ to a strict total order on all literals where all undeﬁned lit-
erals are larger than comp(Ln). We also extend ≺Γ to a strict total order on
ground clauses by multiset extension and also on multisets of ground clauses and
overload ≺Γ for all these cases. With ⪯Γ we denote the reﬂexive closure of ≺Γ .
Theorem 11 (Learned Clauses in Regular Runs). Let (Γ; N; U; k; C0 ·σ0)
be the state resulting from the application of Conﬂict in a regular run and let C
be the clause learned at the end of the conﬂict resolution, then C is not redundant
with respect to N ∪U and ≺Γ .
Proof. Consider the following fragment of a derivation learning a clause:
⇒Conﬂict
SCL
(Γ; N; U; k; C0 · σ0) ⇒{Skip, Fact., Res.}∗
SCL
(Γ ′; N; U; k; C · σ) ⇒Backtrack
SCL
.
By soundness N∪|= C and Cσ is false under both Γ and Γ ′. We prove that
Cσ is non-redundant.
Assume there is an S ⊆gnd(N ∪U)⪯Γ Cσ s.t. S |= Cσ. There is a clause
D ∈S false under Γ, S ⪯Γ {Cσ} and Cσ ̸∈S. All clauses in S have a deﬁned
truth value (as all undeﬁned literals are greater than all deﬁned literals) and if
Γ |= S then Γ |= Cσ, a contradiction.

240
A. Fiori and C. Weidenbach
We distinguish whether the two trails Γ and Γ ′ are equal or Γ ′ is a strict
preﬁx of Γ.
If Γ ̸= Γ ′ then at least one Skip application was performed during conﬂict
resolution, so Cσ does not contain the rightmost literal of Γ and since D ≺Γ Cσ
neither does D. So at a previous point in the derivation there must be a conﬂict
search state such that D was false under the current trail but was not chosen as
conﬂict instance, a contradiction to the exhaustive application of Conﬂict.
If Γ = Γ ′ we distinguish two sub-cases according to whether the rightmost
literal in Γ is the result of a Decision or a Propagation.
If the rightmost literal of Γ = Γ ′′, Lk is a decision literal, then D is either
true in Γ ′′ or has at least two literals undeﬁned or of level k. Since D must be
false under Γ, D must have two or more occurrences of literals undeﬁned under
Γ or of comp(L). Since Cσ has no undeﬁned literal and exactly one occurrence
of comp(L) we have a contradiction with D ≺Γ Cσ.
If Γ = Γ ′′, LC′·δ then at most one literal in Cσ is of level k and all other
literals, if any, are of level at most k −1. Moreover, D is also either true in Γ ′′
or has at least two literals undeﬁned under Γ ′′ or of level k or k = 0. Backtrack
requires the presence of at least one decision literal on the trail and so k > 0 so
D must have at least two literals of level k. If both of those literal are diﬀerent
from comp(L) then by regularity, we would have applied Conﬂict instead of
Propagate on the trail Γ ′′. So at least one of the literals of level k in D must be
comp(L). Simple case analysis shows that under these conditions Cσ ≺Γ D, a
contradiction.
⊓⊔
Theorem 12 (Learned Clauses in Weakly-Regular Runs). Let the state
(Γ; N; U; k; C0 · σ0) the result of a Conﬂict application in a weakly-regular run
and let C be the clause learned at the end of the conﬂict resolution, then C is
not redundant w.r.t. N ∪U and ≺Γ or C is an instantiation of a unit clause in
N ∪U.
Proof. Consider the following fragment of a derivation learning a clause:
⇒Conﬂict
SCL
(Γ; N; U; k; C0 · σ0) ⇒{Skip, Fact., Res.}∗
SCL
(Γ ′; N; U; k; C · σ) ⇒Backtrack
SCL
.
By soundness N ∪U |= C and Cσ is false under both Γ and Γ ′. We need
to prove that there exists a ground instantiation of C that is non-redundant or
that C is unit; we assume that C is not a unit and prove by contradiction that
Cσ is non-redundant.
Assume there is an S ⊆gnd(N ∪U)⪯Γ Cσ s.t. S |= C. There is a clause
D ∈S false under Γ; indeed since S ≺Γ {Cσ} all clauses in S have a deﬁned
truth value and if all clauses in S were to be true under Γ we would also have
that Cσ would be true under Γ by transitivity of entailment.
We distinguish whether the two trails Γ and Γ ′ are equal or Γ ′ is a strict
preﬁx of Γ.
If Γ ̸= Γ ′ then rule Skip was applied at least once during conﬂict resolution,
so Cσ does not contain the rightmost literal of Γ and since D ≺Γ Cσ neither
does D. So at a previous point in the derivation there must be a conﬂict search

SCL Clause Learning from Simple Models
241
state such that D was false under the current trail but was not chosen as conﬂict
instance, a contradiction to the exhaustive application of Conﬂict.
If Γ = Γ ′ we distinguish two sub-cases according to whether the rightmost
literal in Γ is the result of a Decision or a Propagation.
If the rightmost literal of Γ = Γ ′′, Lk is a decision literal then D is either
true in Γ ′′, a unit clause, or has at least two literals undeﬁned or of level k. If
D is a unit clause then it must be true or undeﬁned in Γ ′′, and since decisions
are restricted to never falsify unit clauses D cannot be false in Γ ′′. Otherwise, D
must have two or more occurrences of literals undeﬁned under Γ ′′ or of comp(L).
Since Cσ has no undeﬁned literal and exactly one occurrence of comp(L), we
have a contradiction with D ≺Γ Cσ.
If Γ = Γ ′′, LC′·δ then at most one literal in Cσ is of level k and all other
literals, if any, are of level at most k −1, moreover D is either true in Γ ′′, a unit
clause, or has at least two literals undeﬁned or of level k. Under these conditions
D ≺Γ Cσ and Γ |= ¬D imply D = ¬L and Cσ = C′′ ∨¬L, but then the next
rule cannot be a Backtrack as by weak regularity Resolve would have higher
priority.
⊓⊔
Proposition 13. All regular runs are weakly-regular runs.
Proof. There are four conditions that a weakly-regular run needs to satisfy. A
regular run clearly respects the ﬁrst two conditions, so we prove that regular runs
respect the last two. In a regular run Propagate has always higher priority than
Decide, so whenever we could decide a literal L s.t. comp(L) ∈gnd(N ∪U) the
literal comp(L) must have already been propagated and so L is not undeﬁned.
By Theorem 11 whenever we can apply Backtrack the conﬂict clause is not
redundant in N and so not subsumed in N.
⊓⊔
Theorem 14 (BS Non-redundancy is NEXPTIME-Complete). Decid-
ing non-redundancy of a BS clause C with respect to a ﬁnite BS clause set N ⪯C
is NEXPTIME-Complete.
Proof. We only show hardness, because containment of the problem in NEXP-
TIME is obvious. To this end, let N = {C1, . . . , Cn} be an arbitrary, ﬁnite BS
clause set. We consider an LPO ordering ≺LPO. Next we add two fresh predicates
of arity zero, P, Q with P ≺LPO Q, where P, Q are larger in the LPO precedence
than any other symbol from N. Then obviously N is satisﬁable iﬀthe ﬁnite BS
clause set N ′ = {P, Q, C1 ∨¬Q, C2 ∨¬P, C3, . . . , Cn} is satisﬁable. Furthermore,
the clause ¬P ∨¬Q is ≺LPO larger than any clause in N ′ \ {P, Q}. The clause
¬P ∨¬Q is non-redundant with respect to N ′ \ {P, Q} iﬀN ′ is satisﬁable.
⊓⊔
Theorem 15 (Termination). If N is a clause set and gnd(N) is ﬁnite then
any (weakly-) regular run of SCL terminates.
Proof. Any inﬁnite run learns inﬁnitely many clauses. Firstly, for a regular run,
by Theorem 11, all learned clauses are non-redundant. The number of diﬀerent
ground clauses and literals is ﬁnite. So there is no inﬁnite regular run. Sec-
ondly, for weakly-regular runs, the learned clause is either non-redundant, or an

242
A. Fiori and C. Weidenbach
instance of a unit clause from N or from the set of learned clauses. However,
there are also only ﬁnitely many instances of unit clauses from N. So there is no
inﬁnite weakly-regular run.
⊓⊔
Theorem 16 (SCL Refutational Completeness).
If N is unsatisﬁable,
then there is a (weakly-) regular run of SCL deriving ⊥.
Proof. If N is unsatisﬁable, then as a consequence of Herbrand’s Theorem there
is a ﬁnite set of ground instances N ′ from N that is unsatisﬁable. Now restrict
the rules Decide and Propagate to ground literals from N ′. By Theorems 15 and
6 any (weakly-) regular run on this restriction derives ⊥.
⊓⊔
Theorem 17 (SCL decides the BS fragment). SCL restricted to weakly-
regular runs decides satisﬁability of a BS clause set.
Proof. There are only ﬁnitely many ground instances of a BS clause set. Follow-
ing the proof of Theorem 15, any SCL (weakly-) regular run will terminate on a
BS clause set.
⊓⊔
4
Simulating Resolution by SCL
It is well-known that resolution inferences may generate redundant clauses.
Therefore, by Theorems 11 and 12 (weakly-) regular runs cannot simulate reso-
lution. However, the SCL calculus is still ﬂexible enough to simulate arbitrary
resolution inferences that do not result in tautologies by a non-regular strategy.
Lemma 18 (SCL Simulates Resolution). Let N be a clause set, C = C′∨H
and D = D′ ∨H′ be clauses in N such that C′ and D′ are non-empty, η =
mgu(H, comp(H′)) exists, the literals H and H′ are not duplicated in either C
or D and the three clauses Cη, Dη and (C′ ∨D′)η are not tautologies. Then,
there is an SCL run starting from the state (ϵ; N; ∅; 0; ⊤) where the ﬁrst learned
clause is (C′ ∨D′)η.
Proof. Let θ be a grounding substitution on the variables of Cη and Dη such that
distinct literals are mapped to distinct ground literals, also let {L1, . . . , Ln} =
(C′ ∨D′)ηθ.
We can start our derivation with n Decisions where at the i-th step we
decide the literal comp(Li) to obtain the state (K1
1, . . . , Kn
n; N; U; n; ⊤), Ki =
comp(Li). This is possible as (C′ ∨D′) is not a tautology.
Hηθ is still undeﬁned but every other literal in C′ηθ is falsiﬁed under the
current trail; so in the next step we can propagate Hηθ on the trail.
(K1
1, . . . , Kn
n; N; ∅; n; ⊤) ⇒Propagate
SCL
(K1
1, . . . , Kn
n, HηθC′∨H·ηθ; N; ∅; n; ⊤)
Now the rule Conﬂict is applicable to (D′ ∨H′)ηθ resulting in
(K1
1, . . . , Kn
n, HηθC′∨H·ηθ; N; ∅; n; D′ ∨H′ · ηθ).

SCL Clause Learning from Simple Models
243
We apply Resolve once and we reach the state
(K1
1, . . . , Kn
n; N; ∅; n; (C′ ∨D′)η · θ).
From this state we can backtrack to
(K1
1, . . . , Kk
k; N; {(C′ ∨D′)η}; k; ⊤).
⊓⊔
Lemma 19 (SCL Simulates Factoring). Let N be a clause set, C = C′ ∨
H ∨H′ a non-tautological clause with uniﬁable literals H and H′. There is a run
starting from the state (ϵ; N; ∅; 0; ⊤) where the ﬁrst learned clause is (C′ ∨H)η
where η = mgu(H, K).
Proof. Let {L1, . . . , Lk}
=
Cη and let ρ be a grounding of C injective
on the literals of Cη. By applying Decide k times we can reach a state
(K1
1ρ, . . . , Kk
kρ; N; ∅; k; ⊤), Ki = comp(Li), from which we can apply rule Con-
ﬂict to Cηρ resulting in (K1
1ρ, . . . , Kk
kρ; N; ∅; k; C · ηρ). Now we can factorize
H and H′, deriving ⇒Factorize
SCL
(K1
1ρ, . . . , Kk
kρ; N; ∅; k; (C′ ∨H)η · ρ) and, ﬁnally,
backtrack and learn (C′ ∨H)η.
⊓⊔
In order to simulate an overall refutation of the resolution calculus, an addi-
tional Restart rule is needed. Note that tautologies can be ignored in refutations
by the resolution calculus.
Restart
(Γ; N; U; k; ⊤) ⇒SCL (ϵ; N; U; 0; ⊤)
provided Γ ̸|= N
Theorem 20 (SCL Simulates the Resolution Calculus).
SCL together
with the Restart rule can simulate a resolution refutation by a non-regular strat-
egy.
Proof. By Lemmas 18 and 19.
⊓⊔
Consider another example, taken from [13], where exhaustive propagation
leads to exponentially longer proofs compared to the shortest resolution proof.
Let i be a positive integer and consider the clause set N i with one predicate P
of arity i consisting of the following clauses, where we write ¯x, ¯0 and ¯1 to denote
sequences of the appropriate length of variables and constants to meet the arity
of P:
P(¯0)
¬P(¯1)
and i clauses of the form
¬P(¯x, 0, ¯1) ∨P(¯x, 1, ¯0)
where the length of ¯1 varies between 0 and i −1. The example encodes an i-bit
counter. A regular run of SCL (NRCL) on this clause set would ﬁnd a conﬂict

244
A. Fiori and C. Weidenbach
after O(2i) propagations without any application of Decide. For example, for
i = 4 we get the clauses of N 4:
1 : P(0, 0, 0, 0)
2 : ¬P(x1, x2, x3, 0) ∨P(x1, x2, x3, 1)
3 : ¬P(x1, x2, 0, 1) ∨P(x1, x2, 1, 0)
4 : ¬P(x1, 0, 1, 1) ∨P(x1, 1, 0, 0)
5 : ¬P(0, 1, 1, 1) ∨P(1, 0, 0, 0)
6 : ¬P(1, 1, 1, 1)
For this clause set a regular SCL (NRCL) generates all unit clauses from
P(0, 0, 0, 0) to P(1, 1, 1, 1) via 24 applications of Propagate, then ﬁnds a conﬂict
with clause 6 and then uses 24 times Resolve to end up in ⊥.
Instead a short resolution refutation can be obtained by
2.2 Res 3.1
7 : ¬P(x1, x2, 0, 0) ∨P(x1, x2, 1, 0)
7.2 Res 2.1
8 : ¬P(x1, x2, 0, 0) ∨P(x1, x2, 1, 1)
8.2 Res 4.1
9 : ¬P(x1, 0, 0, 0) ∨P(x1, 1, 0, 0)
9.2 Res 8.1
10 : ¬P(x1, 0, 0, 0) ∨P(x1, 1, 1, 1)
10.2 Res 5.1 11 : ¬P(0, 0, 0, 0) ∨P(1, 0, 0, 0)
11.2 Res 10.1 12 : ¬P(0, 0, 0, 0) ∨P(1, 1, 1, 1)
12.1 Res 6.1 13 : ⊥
In general, O(2i) many resolution steps are suﬃcient to refute N i. The above
resolution proof cannot be simulated by an SCL weakly-regular run. As soon as
we decide a ground literal [¬]P(. . .) propagation using the two literal clauses
yields a conﬂict with either ¬P(1, 1, 1, 1) or P(0, 0, 0, 0). The above resolution
proof can only be simulated by a non-regular SCL run, Theorem 20. It is an
open problem to ﬁnd a notion of regularity that both guarantees non-redundant
clause learning and can simulate resolution proofs of the above type.
5
Simulating NRCL by SCL
In this section we show that even under the restriction of a ground trail, SCL
can generate any clause learned by NRCL. In the worst case, the trail generated
by SCL may be exponentially longer than the NRCL trail, see Example 25. We
use Γ, Γ1, Γ2 to denote SCL trails and Γ ′, Γ ′
1, Γ ′
2 to denote NRCL trails.
The ordering ≺Γ ′ is deﬁned as in [1] and concerning NRCL we exactly stick
to the notions from [1]. Nevertheless, the most important notions from NRCL
are recalled below.
Deﬁnition 21 (Constrained Clauses [1]). A constrained clause (C · σ; π) is
a pair of a closure C ·σ and a constraint π of the form n
i=0 ⃗si ̸= ⃗ti where ⃗si and
⃗ti are tuples of terms of equal length. The set of ground instances of (C · σ; π)
denoted as gnd(C · σ; π) is {Cσδ | Cσδ ∈gnd(Cσ) and πδ is true}. A constraint
π = n
i=0 ⃗si ̸= ⃗ti is true if for all 0 ≤i ≤n ⃗si and ⃗ti are not uniﬁable. A ground

SCL Clause Learning from Simple Models
245
clause C′ is covered by a constrained clause (C · σ; π) if C′ ∈gnd(C · σ; π). We
similarly deﬁne constrained literals (L · σ; π) and say that a ground literal L′ is
deﬁned true by (L·σ; π) if it is covered by (L·σ; π) and deﬁned false if it covered
by (comp(L) · σ; π).
Deﬁnition 22 (State induced ordering in NRCL [1]). A ground literal
L is deﬁned under a trail Γ ′ if L or comp(L) is covered by some con-
strained literals L′ in Γ ′. Such a literal is necessarily unique and is denoted
by def(L). Given two deﬁned ground literals L1, L2 we say L1 ≺Γ ′ L2 if
Γ ′ = Γ ′
0, def(L1), Γ ′
1, def(L2), Γ ′
2.
The proof of the simulation is done in two steps. Firstly, we show that we
can generate via SCL a suitable ground instance of any NRCL trail, Lemma 23.
Then we show that on this basis, we can actually learn exactly the clause NRCL
learns, Theorem 24.
Lemma 23 (Simulating the NRCL Trail). Let (Γ ′; N; U; k; ⊤) be a regular
state in an NRCL run and let ≺0 be a total order on ground literals compatible
with ≺Γ ′. Then there is an SCL derivation starting from (ϵ; N; U; 0; ⊤) which
produces a trail Γ such that for any ground literal L, L is true, false, undeﬁned
under Γ if and only if it is so under Γ ′, and for all ground literals L1, L2 ∈
gnd(Γ ′): L1 ≺Γ L2 if and only if L1 ≺0 L2. We call Γ a grounding of Γ ′.
Proof. By induction on the length of Γ ′. If Γ ′ = ϵ then we choose Γ = ϵ satisfying
the conjecture. If Γ ′ = Γ ′
1, (L; σ; π)k then all ground literals gnd(L; σ; π) are
undeﬁned in Γ1 so we can simply decide every literal in gnd(L; σ; π) in increasing
order according to ≺Γ ′ to obtain a trail Γ2 that clearly satisﬁes the conjecture.
If the rightmost literal of Γ ′ is a propagation then we apply Propagate instead
of Decide on the SCL trail.
⊓⊔
A consequence of Lemma 23 is that SCL can simulate the derivation of ⊥
from a state without decisions. So this needs not to be considered anymore.
The most sophisticated rule of NRCL to be considered for the simulation is
Backjump, because its side conditions are substantially diﬀerent from the side
conditions of the SCL Backtrack rule.
Backjump (Γ, Kk′, Γ ′; N; U; k; (C · σ; π)) ⇒NRCL (Γ, Kk′; N; U ∪{C}; k′; ⊤)
provided one of the following conditions hold (i) k = 0 and C = ⊥, (ii) k > 0, all
ground clauses covered by (Cσ; π) have exactly one literal of level k and (Cσ; π)
has no false instances under Γ (iii) k > 0, the right-most element of Γ ′ is a
decision, some ground clauses covered by (Cσ; π) have two or more literals of
level k, (Cσ; π) has no false instances under Γ and Factorize cannot be applied.
Theorem 24 (SCL Simulates NRCL). If from an NRCL conﬂict state we
can learn a clause C ̸= ⊥, then we can learn the same clause C from a grounding
of that state by SCL.

246
A. Fiori and C. Weidenbach
Proof. Let w′
1 = (Γ ′
1; N; U; k; (C1; σ1; π1)) be an NRCL conﬂict state from which
we learn the clause C. We prove by induction on the length of conﬂict resolution
that there exists an SCL state w1 = (Γ1; N; U; k; C1 · δ1) obtained by grounding
w′
1 from which we can learn C.
As a base case, we prove that if we can apply Backjump to an NRCL
state (Γ ′
2; N; U; k′; (C2; σ2; π2)) then we can also apply Backtrack to a grounding
(Γ2; N; U; k; C2 · δ2). In particular, we choose δ2 = σ2ρ where ρ is an injective
mapping from the variables of C2 to a set of fresh constants. If we can apply
Backjump in NRCL then one of the three cases of the rule applies. We consider
them separately. The ﬁrst case cannot apply as we have assumed a clause diﬀer-
ent from ⊥. The second case implies that we can backtrack to the very same level
via SCL. For the third case, we note that in SCL there is no equivalent of the
concept of blocking decisions or blocking clauses as they can only arise when a
decision deﬁnes multiple ground literals. In SCL all literals deﬁned by decisions
in a conﬂict clause have diﬀerent levels and thus we can always apply Backtrack
from any grounding of the conﬂict clause if the third case of Backjump applies.
For the inductive step, consider a rule application (Γ ′
1; N; U; k′; (C1; σ1; π1))
⇒NRCL (Γ ′
2; N; U; k′; (C2; σ2; π2)) in a conﬂict resolution. For any ground-
ing (Γ2; N; U; k; C2 · δ2) of (Γ ′
2; N; U; k′; (C2; σ2; π2)) we build a grounding
(Γ1; N; U; k; C1 · δ1) of (Γ ′
1; N; U; k′; (C1; σ1; π1)) from which we can still learn
the clause C. In particular we will need to deﬁne Γ1 and δ1 in terms of
(Γ2; N; U; k; C2 · δ2)
Case Resolve: we consider the NRCL rule application
(Γ ′
1, (L0; σ0; π0)C0∨L0; N; U; k′; (C1 ∨L1; σ1; π1))
⇒Resolve
NRCL (Γ ′
2, (L0; σ0; π0)C0∨L0; N; U; k′; (C2; σ2; π2))
by the conditions of Resolve in NRCL we have
1. there exists η0 = mgu(comp(L0), L1)
2. there exists η = mgu(comp(L0)σ0, L1σ1)
3. η0σ2 = σ1σ0η
4. C2 = (C1 ∨C0)η0
and from the grounding we have δi = σiδ′
i for some δ′
i, i = 0, 1, 2. It is clear that
any grounding substitution δ2 on the variables of (C1 ∨C0)η0 can be induced
by choosing opportune grounding substitutions δ0 and δ1. In particular, we can
deﬁne δi for i = 0, 1 as the restriction of η0δ2ρ to the variables of Ci∨Li where ρ is
a grounding on var(L0η0, L1η0) \ var(C0η0, C1η0). If there is a grounding ρ such
that L0η0δ2ρ is undeﬁned in Γ2 then we deﬁne Γ1 as Γ2, L0η0δ2ρC0∨L0·η0δ2ρ.
If such a substitution ρ does not exist then the literal L0ρ′C0∨L0·ρ′ is already
deﬁned in Γ2 obtained from grounding the same literal (L0; σ0; π0)C0∨L0; we can
then deﬁne Γ1 equal to Γ2 and resolve once more with the literal L0ρ′C0∨L0·ρ′.
Case Factorize: we consider the NRCL rule application
(Γ ′
1, (L0; σ0; π0)C0∨L0; N; U; k′; (C1 ∨L1 ∨L′
1; σ1; π1))
⇒Factorize
NRCL
(Γ ′
2, (L0; σ0; π0)C0∨L0; N; U; k′; (C2 ∨L2; σ2; π2))

SCL Clause Learning from Simple Models
247
by the conditions on Factorize in NRCL we have
1. there exists η0 = mgu(L1, L′
1)
2. C2 = C1η0 and L2 = L1η0
3. there exists η = mgu(comp(L0)σ0, L1σ1, L′
1σ1)
4. η0σ2 = σ1η
and from the grounding we have δi = σiδ′
i. We can deﬁne δ1 = η0δ2 which
produces an acceptable grounding of (C1 ∨L1 ∨L′
1; σ1; π1) as η0δ2 = η2σ2 and
so δ1 = σ1δ′
1 for δ′
1 = ηδ′
2. We can, moreover, deﬁne Γ1 = Γ2 as in SCL Factorize
is not restricted to the rightmost literal.
Case Skip: we consider the NRCL rule application
(Γ ′
1, (L0; σ0; π0)C0∨L0; N; U; k′; (C1; σ1; π1))
⇒Skip
NRCL (Γ ′
2; N; U; k′; (C2 ∨L2; σ2; π2)).
We can simply deﬁne (Γ1; N; U; k; C1 · δ1) = (Γ2; N; U; k; C2 · δ2) where Fac-
torize is independent from the trail in SCL and the induction step for Resolve
can add any needed literal.
⊓⊔
Example 25 (SCL Trails may be Exponentially Longer). Consider an unsatisﬁ-
able clause set
N n =
⎧
⎪
⎨
⎪
⎩
Qn(x1, . . . , xn)
¬Qi+1(x1, . . . , xi, 0) ∨¬Qi+1(x1, . . . , xi, 1) ∨Qi(x1, . . . , xi)
if 0 ≤i < n
¬Q0
NRCL can refute the clause set N n at level k = 0 in linear time by building
through rule Propagate the trail ¬Q0, Qn(x1, . . . , xn), Qn−1(x1, . . . , xn−1), . . . ,
Q1(x1), where we do not show the clauses annotated to the literals, result-
ing in a conﬂict with ¬Q1(1) ∨¬Q1(0) ∨Q0. For SCL all ground instances of
the Qi(x1, . . . , xi) have to be enumerated and there are 2i many such ground
instances for each Qi.
Another relevant aspect is whether the SCL run constructed in Theorem 24
is (weakly-) regular. The example below shows it is not, in general, however at
least for the example below, it is the case that SCL can actually learn a more
general clause by a regular run.
Example 26 (Simulating SCL Runs are not (Weakly-) Regular). Consider the
clauses
Q(x) ∨¬P(x)
P(x) ∨P(a) ∨P(b) ∨P(c)
P(a) ∨¬P(b)
P(b) ∨¬P(c)
P(c) ∨¬P(a)
In NRCL we can decide the literal ¬Q(x), propagate ¬P(x) and result in a
conﬂict with P(x) ∨P(a) ∨P(b) ∨P(c). After factorization and resolution with
Q(x)∨¬P(x), NRCL learns the clause Q(a)∨Q(b)∨Q(c). This clause cannot be
learned with a (weakly-) regular run in SCL. After deciding any ground instance
of ¬Q(x), immediately all ground literals ¬P(a), ¬P(b), ¬P(c) are propagated
through the two literal clauses. Now the conﬂict does not only rely on the ﬁrst two
clauses but also involves the two literal clauses. After resolution and factoring
steps, if SCL started with ¬Q(a) it eventually learns the clause Q(a) which
makes the NRCL learned clause Q(a) ∨Q(b) ∨Q(c) redundant.

248
A. Fiori and C. Weidenbach
6
Conclusion
The contributions of this paper are: (i) a sound, complete, SCL calculus for
full ﬁrst-order logic learning non-redundant clauses with respect to regular runs,
(ii) weakly-regular runs do not exhaustively propagate unit clauses but still learn
non-redundant clauses except for unit instances, (iii) the used notion of non-
redundancy is NEXPTIME-complete for the BS fragment, (iv) SCL simulates
resolution by non-regular runs, (v) SCL simulates NRCL by non-regular runs,
(vi) exhaustive propagation is not always a good strategy for the BS fragment
and beyond, and (vii) SCL is a decision procedure for the BS fragment.
The price for the simple SCL models is that trails can be exponentially
longer compared to trails of calculi with more expressive model representation
languages. For a BS clause set N the overall trail size is bound by mrk where
k is the maximal arity of a predicate in N, m the number of predicates, and r
the number of constant symbols in N. Exploiting the actual recursive structure
of N this bound can be further reﬁned for a speciﬁc problem [10]. So in a
simple preprocessing step, it can be checked whether an SCL trail potentially
becomes “too large”. Then a procedure can either start with a more expressive
trail language [1,3,4,7,9,14,17] or dynamically decide to switch the trail model
representation formalism. In practice, there are many interesting problems where
the maximal predicate arity is not larger than three and there are not “too many”
constants. Recall that all our examples inducing an exponentially growing trail
or an exponentially growing proof length include the encoding of some type of
binary counter.
Although SCL with a regular strategy and also all other calculi with exhaus-
tive propagation cannot simulate resolution, the resolution calculus has also
drawbacks. Worst case, the resolution calculus may generate more clauses, even
in a terminating setting [7], than there are potential ground model assumptions
as they are explored by SCL. Still one open question is whether the advantages
of resolution and SCL can be combined: learning only non redundant clauses via
partial model assumptions and being able to simulate non-redundant resolution
inferences, in general. Such a result would unify both paradigms.
Acknowledgments. This work was funded by DFG grant 389792660 as part of
TRR 248.
References
1. Alagi, G., Weidenbach, C.: NRCL - a model building approach to the Bernays-
Sch¨onﬁnkel fragment. In: Lutz, C., Ranise, S. (eds.) FroCoS 2015. LNCS (LNAI),
vol. 9322, pp. 69–84. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-
24246-0 5
2. Baumgartner, P.: Hyper tableau—the next generation. In: de Swart, H. (ed.)
TABLEAUX 1998. LNCS (LNAI), vol. 1397, pp. 60–76. Springer, Heidelberg
(1998). https://doi.org/10.1007/3-540-69778-0 14

SCL Clause Learning from Simple Models
249
3. Baumgartner, P., Fuchs, A., Tinelli, C.: Lemma learning in the model evolution cal-
culus. In: Hermann, M., Voronkov, A. (eds.) LPAR 2006. LNCS (LNAI), vol. 4246,
pp. 572–586. Springer, Heidelberg (2006). https://doi.org/10.1007/11916277 39
4. Bonacina, M.P., Plaisted, D.A.: Semantically-guided goal-sensitive reasoning:
model representation. J. Autom. Reason. 56(2), 113–141 (2016)
5. Bromberger, M., Sturm, T., Weidenbach, C.: Linear integer arithmetic revisited.
In: Felty, A.P., Middeldorp, A. (eds.) CADE 2015. LNCS (LNAI), vol. 9195, pp.
623–637. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-21401-6 42
6. Ferm¨uller, C.G., Pichler, R.: Model representation over ﬁnite and inﬁnite signa-
tures. J. Log. Comput. 17(3), 453–477 (2007)
7. Hillenbrand, T., Weidenbach, C.: Superposition
for bounded domains. In:
Bonacina, M.P., Stickel, M.E. (eds.) Automated Reasoning and Mathematics.
LNCS (LNAI), vol. 7788, pp. 68–100. Springer, Heidelberg (2013). https://doi.
org/10.1007/978-3-642-36675-8 4
8. Kaufmann, B., Leone, N., Perri, S., Schaub, T.: Grounding and solving in answer
set programming. AI Mag. 37(3), 25–32 (2016)
9. Korovin, K.: Inst-Gen – a modular approach to instantiation-based automated
reasoning. In: Voronkov, A., Weidenbach, C. (eds.) Programming Logics. LNCS,
vol. 7797, pp. 239–270. Springer, Heidelberg (2013). https://doi.org/10.1007/978-
3-642-37651-1 10
10. Korovin, K.: Non-cyclic sorts for ﬁrst-order satisﬁability. In: Fontaine, P., Ringeis-
sen, C., Schmidt, R.A. (eds.) FroCoS 2013. LNCS (LNAI), vol. 8152, pp. 214–228.
Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-40885-4 15
11. Lewis, H.R.: Complexity results for classes of quantiﬁcational formulas. J. Comput.
Syst. Sci. 21(3), 317–353 (1980)
12. Nieuwenhuis, R., Oliveras, A., Tinelli, C.: Solving sat and sat modulo theories:
from an abstract Davis-Putnam-Logemann-Loveland procedure to DPLL (T). J.
ACM 53, 937–977 (2006)
13. Navarro, J.A., Voronkov, A.: Proof systems for eﬀectively propositional logic. In:
Armando, A., Baumgartner, P., Dowek, G. (eds.) IJCAR 2008. LNCS (LNAI),
vol. 5195, pp. 426–440. Springer, Heidelberg (2008). https://doi.org/10.1007/978-
3-540-71070-7 36
14. Piskac, R., de Moura, L.M., Bjørner, N.: Deciding eﬀectively propositional logic
using DPLL and substitution sets. J. Autom. Reason. 44(4), 401–424 (2010)
15. Plaisted, D.A.: Complete problems in the ﬁrst-order predicate calculus. J. Comput.
Syst. Sci. 29, 8–35 (1984)
16. Sutcliﬀe, G.: The CADE ATP system competition - CASC. AI Mag. 37(2), 99–101
(2016)
17. Teucke, A., Weidenbach, C.: First-order logic theorem proving and model building
via approximation and instantiation. In: Lutz, C., Ranise, S. (eds.) FroCoS 2015.
LNCS (LNAI), vol. 9322, pp. 85–100. Springer, Cham (2015). https://doi.org/10.
1007/978-3-319-24246-0 6

Names Are Not Just Sound and Smoke:
Word Embeddings for Axiom Selection
Ulrich Furbach1, Teresa Kr¨amer1, and Claudia Schon2(B)
1 Institute for Computer Science, University of Koblenz-Landau, Koblenz, Germany
{uli,tbergk}@uni-koblenz.de
2 Institute for Web Science and Technologies, University of Koblenz-Landau,
Koblenz, Germany
schon@uni-koblenz.de
Abstract. First-order theorem proving with large knowledge bases
makes it necessary to select those parts of the knowledge base, that
are necessary to prove the theorem at hand. We extend syntactic axiom
selection procedures like SInE to use semantics of symbol names. For
this, not only occurrences of symbol names but also semantically similar
names are taken into account. We use a similarity measure based on word
embeddings. An evaluation of this similarity based SInE is given using
problems from TPTP’s CSR problem class and Adimen-SUMO. This
evaluation is done with two very diﬀerent systems, namely the Hyper
tableau prover and the saturation based system E.
1
Introduction
Automated theorem proving attempts to ﬁnd a proof (for the unsatisﬁability)
of a set of formulae. Most problems, even those from benchmark suites like
TPTP [28], consist of hand-coded sets of logical formulae and hence used to be
relatively small. This situation changed dramatically since automated theorem
provers have been used in contexts with large background knowledge. This can be
large ontologies like SUMO [19], CYC [11], Yago [27] or mathematical libraries,
where the theorem prover has to face millions of formulae that are potentially
necessary to ﬁnd a proof of the problem under consideration. Once a proof is
found, it typically turns out that only a very small part of the vast background
knowledge was necessary to ﬁnd the proof. This leads to the following challenge:
Given a large background knowledge base and a problem, ﬁnd a (preferably
small) subset of the knowledge base with which a proof for the problem can be
found.
One of the most used approaches for this task is the SInE selection method
[9]. It’s basic idea is to determine symbols relevant for a problem at hand and
to select formulae to be included into the proof search based on this relevancy.
This method is very well suited for many problem areas and it is used by many
Work supported by DFG grant CoRg – Cognitive Reasoning. Author names are given
in alphabetical order.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 250–268, 2019.
https://doi.org/10.1007/978-3-030-29436-6_15

Names Are Not Just Sound and Smoke
251
theorem provers nowadays. However, in order to apply this method it is manda-
tory that the symbols are used in a consistent manner throughout all formulae
in the knowledge base and in the axiomatisation of the problem. And, of course,
this is fully consistent with the way the logical formulae are usually processed
by theorem provers. To prove the unsatisﬁability of a set of formulae it does not
matter which symbols are used, it is only important that symbols, e.g., a predi-
cate symbol p, is used consistently in the entire formulae set. If every occurrence
of p is substituted by q the problem to prove the unsatisﬁability of the formulae
remains the same. This is why the SInE selection strategy only counts occur-
rences of symbol names and does not consider the meaning of symbol names.
In certain areas such as commonsense reasoning, however, symbol names in
knowledge bases are anything but random. This is very obvious as soon as we
have to apply reasoning within the context of natural language. Examples are
the commonsense reasoning benchmarks from [13] where the task is to determine
which one of the given alternatives is the most plausible one:
My body cast a shadow over the grass. What was the CAUSE of this?
1. The sun was rising.
2. The grass was cut.
In the approach followed in the project CoRg [24] these sentences are trans-
fered into ﬁrst-order logic as a starting point for reasoning. This is done auto-
matically by KNEWS [3], which is based on Boxer, a system for translating
natural language input into various logical output formats [5]. For the premise
together with the ﬁrst alternative from our example we get:
∃A, B(∃C, D, E, F(rover(D, B)∧∃G(rtopic(G, A)∧arisingC(G))
∧rthat(B, C) ∧rpatient(D, E) ∧ragent(D, F)
∧vcast(D)∧nshadow(E) ∧nbody(F)∧rof (F, E)
∧nperson(E)) ∧nsun(A) ∧ngrassC(B)).
So far, this approach follows very much the traditional paradigm in com-
putational linguistics, namely to translate a natural language sentence into a
metalanguage that represents the meaning of the sentence. In our case, ﬁrst-
order logic is used as a metalanguage, which has the advantage that powerful
automatic reasoning systems can be used for further processing of the mean-
ing of the sentence. The predicates in the resulting ﬁrst-order logic formula are
derived either directly from the words appearing in the English sentence or by
portraying the connections. Examples for the former are nbody or vcast with a
preﬁx indicating the word type (n for noun, v for verb), while the latter can be
exempliﬁed by ragent(D, F) (r for role), which states that F (the noun body)
has the role of the agent of D (the verb cast).
It is easy to see that the COPA task shown above can not be solved by
simply applying a reasoning system to the formula generated from it. To reason
that a shadow can be cast by an object and the sun needs more background
knowledge. We use the ontology SUMO [19] as a knowledge base representing

252
U. Furbach et al.
this background knowledge. The task of axiom selection is to consider those parts
of SUMO for the reasoning process that contain the symbols of the formula or
related ones. There are two problems: Firstly, a symbol might not be used in
the ontology, but a synonym or a semantically similar one. E.g., vcast does not
occur in the ontology (and neither does cast), but the symbol project, which
has a similar meaning occurs in the ontology and can be used instead of vcast.
This can be easily solved by the use of WordNet [17], which immediately gives
us project as a synonym of cast. This is even supported by KNEWS, which
determines WordNet synsets for the symbol names in the generated formulae.
The use of WordNet is further discussed in Sect. 4.3.
The second problem is much more subtle. There is no guarantee that the
symbols of the background knowledge in SUMO ﬁt semantically to the symbols in
the generated predicate logic formulae—even if synonyms are taken into account.
This is why we also want to consider symbols in the background knowledge
that are semantically similar to the symbols in the formulae. This oﬀers the
possibility to select more relevant axioms for the reasoning process from the
background knowledge. For such a similarity measure there is an obvious choice,
namely the distributional semantics of natural language, which is applied in
many statistical natural language processing systems. Firth, one of the founders
of this approach, put it like this:
You shall know a word by the company it keeps [8].
For such a semantics, a large corpus of natural language text is evaluated
to ﬁnd co-occurences of words. Words that occur more frequently together are
more similar compared to words with less frequent co-occurences. Similar to this
co-occurrence information are word embeddings, which we use in our approach.
More speciﬁcally, we use the word embedding ConceptNet Numberbatch [26] to
give us a real-valued vector for a symbol, which can be used to compare symbols
with respect to similarity. In our example Numberbatch gives us a similarity
value for cast and project of 0.19337063, which appears to be pretty low—
the reason certainly is that project is not only a verb but also a noun with a
semantics very diﬀerent from cast, and this is mirrored by the low value. On the
other hand, if we compare sun and shadow the value is 0.25696868 and for grass
and shadow we get 0.09716037, which clearly indicates that the ﬁrst alternative
from the example above is more plausible. There are numerous systems which
use this kind of distributional semantics together with machine learning methods
for solving commonsense reasoning tasks very successfully [12,32,33].
We use both kinds of semantics—translation into logic as a metalanguage and
distributional semantics—together. This oﬀers the possibility of getting insights
into the reasoning process and applying statistical methods at the same time.
For this, we introduce a selection technique that considers the meaning of symbol
names. The main contributions are:
– A selection technique that takes the meaning of symbol names into account
by using word embeddings.

Names Are Not Just Sound and Smoke
253
– A discussion of the need of mapping symbol names from the problem descrip-
tion and the background knowledge base to the vocabulary of the word embed-
ding.
– An evaluation of the methods presented, which demonstrates that these meth-
ods can be helpful for reasoning.
This paper is organized as follows: after discussing related work in Sect. 2,
in Sect. 3 we review the SInE selection method from [9]. Sect. 4 introduces our
approach for combining word embeddings with the SInE method and in Sect. 5
this method is evaluated. Finally we discuss future work.
2
Related Work
There are a number of papers on axiom selection in large theories. Most of them
are of purely syntactic nature, like [14,23] or [9]. In [30] there is a semantic app-
roach for axiom selection. It is based on the computation of models for subsets of
the available axioms and by consecutively extending these sets. This approach is
model based and the meaning of symbol names is not taken into account. A very
similar approach is described in [34], where it is used to search for conjectures
which might be interesting.
In [10] the authors extend SInE by a method which is well known from market
basket analysis, namely frequent item set mining. Instead of co-occurences given
by word embeddings in our case, frequent item set mining is working directly
on axiom sets in order to ﬁnd symbols which frequently occur together. This
information is then used for extending SInE. Frequent item mining is evaluated
with prover E and with SPASS with the MPTP2078 benchmarks and with both
provers there was no increase of performance with the extended SInE. Moreover,
the authors state that in many cases the selection itself was too costly and often
resulted in a timeout.
In [22] similarities of symbol names are used to deﬁne an extended uniﬁcation
method. The similarity is learned by neural networks which allow calculation of
proof success with respect to a vector representation of symbol names. This
method is used to improve the similarity measure and it is shown in this paper
that it is well suited for inductive logic programming.
The observation that names are meaningful is explicitly formulated and eval-
uated in [7]. This approach is based on knowledge graphs as they are used in the
semantic web area. The authors test and evaluate their hypothesis, namely that
the names of IRIs (Internationalized Resource Identiﬁers) carry a kind of social
semantics, and come to the conclusion that semantics encoded in the names of
IRIs signiﬁcantly coincides with the formal meaning of the denoted resources.
Hence the authors prove, that names in RDF graphs encode semantics. The
authors use this insight to motivate the development of semantic web tools that
make use of the meaning of names.

254
U. Furbach et al.
3
Selection Techniques for Large Knowledge Bases
The reasoning task we are considering in this paper consists of a very large
set of axioms called the knowledge base (KB), a small set of further axioms
F1, . . . , Fn called assumptions and a query Q. The task is to show that KB
together with F1, . . . , Fn implies Q. In classical logic, this can be reduced to
showing that F1 ∧. . . ∧Fn →Q is entailed by KB. As suggested in [9], we will
denote F1 ∧. . . ∧Fn →Q as goal.
Given a query and a real-world KB, automated theorem provers (ATP) nowa-
days are able to compute proofs quite eﬃciently. However, once the underlying
KB has reached a certain size, it is not feasible to consider the entire KB when
trying to ﬁnd a proof. Analysis of proofs on large KBs has shown that in the vast
majority of cases only a very small part of the KB has been used for the proof.
This observation is the basis of a strategy for reasoning in large KB: starting
from the goal an attempt is made to determine a subset of the KB that contains
suﬃcient knowledge to construct a proof.
In [9] the relevance-based selection strategy SInE is introduced. This selec-
tion is based on a trigger relation that determines which symbols trigger an
axiom. Assume for a given axiom A and a symbol s occurring in A, a relation
triggers(s, A).
Deﬁnition 1 (Trigger-based selection [9]). Let KB be a knowledge base, A
be an axiom in KB and s be a predicate or function symbol occurring in KB.
Let furthermore g be a goal to be proven from KB.
1. If s is a symbol occurring in the goal g, then s is 0-step triggered.
2. If s is k-step triggered and s triggers A (triggers(s, A)), then A is k + 1-step
triggered.
3. If A is k-step triggered and s occurs in A, then s is k-step triggered, too.
An axiom or a symbol is called triggered if it is k-step triggered for some k ≥0.
In order to obtain a selection strategy, it is necessary to deﬁne which symbols
trigger an axiom. A naive choice for this relation would be to determine that an
axiom is triggered by all symbols occurring in it. Usually there are symbols like
subClass or hasPart in KBs that are very common. Therefore, this naive trigger
relation would result in the selection of almost all axioms. The SInE selection
deﬁnes the triggers relation such that only the least common symbol in an axiom
is allowed to trigger this axiom. This ensures that common symbols do not lead
to the selection of all axioms.
Deﬁnition 2 (Trigger relation for the SInE selection [9]). Let KB be a
knowledge base and s be a symbol. Let furthermore occ(s) denote the number of
axioms in which s occurs in KB. Then the triggers relation is deﬁned as follows:
triggers(s, A) iﬀfor all symbols s′ occurring in A we have occ(s) ≤occ(s′)
(1)

Names Are Not Just Sound and Smoke
255
Deﬁnition 1 together with Deﬁnition 2 deﬁne a trigger-based selection strat-
egy. If a trigger-based selection selects all k-step relevant axioms, we call this
selection SInE with recursion depth k in the following.
The SInE selection is an incomplete selection technique. This means that it
can happen that given a goal, SInE selects a set of axioms with which no proof
for the goal can be found (see Example 1 in Sect. 4.3 for an example). One reason
for the incompleteness is SInE’s fragility w.r.t. the number of occurrences of a
symbol: even if two symbols occurring in an axiom occur almost equally often
in the KB, only the one with the least number of occurrences is allowed to
trigger the axiom. In order to soften this eﬀect, the tolerance parameter, a real
number t ≥1, was introduced. To take this parameter into account, Eq. (1) can
be changed such that
triggers(s, A) iﬀfor all symbols s′ occurring in A we have occ(s) ≤t · occ(s′)
(2)
This allows not only the symbol with the least number of occurrences to trigger
an axiom but also symbols with t times more occurrences.
The SInE selection strategy is successfully used by many provers. One prop-
erty of SInE is that it completely ignores the names of the symbols in the KB.
The behaviour of the selection does not change if a symbol called beer would be
renamed to p. In many areas where the names of symbols are not very mean-
ingful, this is a legitimate approach. In commonsense KBs, the symbols usually
have meaningful names and we suspect that this meaning can be very helpful
for the selection process. In the following, we introduce a selection technique
taking into account the symbol name’s meaning which is implemented as a SInE
extension.
4
Integration of Distributional Semantics
into Axiom Selection
An example of a KB where symbols carry meanings, is SUMO [19,20]. To sim-
plify its use, there is even a mapping oﬀered that relates symbols occurring in
SUMO to WordNet [17] synsets. SUMO is no exception; many other KBs in the
commonsense reasoning area, like Cyc [11], also oﬀer such mappings to Word-
Net or include the WordNet class hierarchy like Yago [27]. This illustrates the
importance of symbol names in this area.
In current selection techniques, however, the meaning of the symbol names
used in a KB is completely ignored. Intuitively, a symbol like beer in the goal
should not only lead to the selection of axioms triggered by the symbol beer, but
also to the selection of axioms triggered by symbols with a similar meaning, like
pilsner for example. This is why we aim at including the meaning of the symbol
names in the selection process. We semantically guide the selection process by
integrating word-embeddings into the selection process by replacing the above
trigger relation with one that takes a word embedding into account. Using this
selection technique together with a theorem prover leads to a hybrid approach
using both reasoning and statistical methods.

256
U. Furbach et al.
4.1
Distributional Semantics
The area of distributional semantics researches theories for quantifying semantic
similarities between words based on their distributional properties in large text
corpora. More speciﬁcally, distributional semantics is based on the distributional
hypothesis [18], which states that linguistic elements with a similar distribution
have a similar meaning. A currently very popular technique in the ﬁeld of distri-
butional semantics is word embeddings [16], which map words or phrases from
a given vocabulary to vectors of real numbers. The rough idea behind the con-
struction of word embeddings, as performed by methods like Word2vec [15], is
to ﬁrst determine word co-occurrences on a large text corpus and store them
in a matrix. If the vocabulary V of the text corpus consists of |V | words, this
step leads to a |V | times |V | matrix. In the next step, Word2vec performs a
dimensionality reduction that leads to a smaller matrix. Each column of the
resulting matrix corresponds to the vector representation of a word. A nice
property of these word embeddings is that relative similarities of the vectors
correlate with semantic similarity. This means that the vector representations of
beer and pilsner are closer together than the representations of beer and stone.
More precisely, the similarity of two words w1 and w2 from the vocabulary can
be calculated by as the cosine similarity of their vector representations xw1 and
xw2:
xw1 · xw2
||xw1|| ||xw2||
(3)
where xw1 · xw2 denotes the dot product and ||xw1|| the magnitude of vector
xw1. The value of this cosine similarity lies between -1 for completely opposite
vectors to 1 for the same vectors.
Surprisingly, simple algebraic operations on the vector representations can
be used to answer questions about analogies. For example the question: Which
word w has the same relationship to king as woman to man? can be answered
by calculating xy = xwoman −xman + xking where xw denotes the vector repre-
sentation of word w normalized to unit form. The vector x′
y with the greatest
cosine similarity to xy yields the expected answer y = queen.
The word embedding toolkits Word2vec [15] and GloVe [21] are able to learn
vector representations for given text corpora. We exploit the possibility of using
word embeddings to provide the k most similar words for a given word, and inte-
grate this information into the selection of axioms. In the following, we assume a
given word embedding. The word embedding we use in our experiments is Con-
ceptNet Numberbatch [26], which was derived using ConceptNet, Word2vec,
GloVe, and OpenSubtitles 2016 [31].
Deﬁnition 3 (Set of k vectors most similar to xv). Let V be a vocabulary
and f a word embedding, i.e. f : V →Rn , k ∈N, |V | > k and xv ∈Rn a vector.
Then simvecsf(xv, k), the set of k vectors in f(V ) most similar to vector xv, is
deﬁned as
simvecsf(xv, k) =

{x1, . . . , xk}
if xv ∈f(V )
∅
else

Names Are Not Just Sound and Smoke
257
such that
– {x1, . . . xk} ⊆f(V ),
– |{x1, . . . , xk}| = k and
– there is no xj ∈f(V ) with xj /∈{x1, . . . , xk} and
xj·xv
||xj|| ||xv|| >
xi·xv
||xi|| ||xv|| for
some xi ∈{x1, . . . , xk}.
Furthermore, simwordsf(w, k), the set of words similar to word w, is deﬁned
as
simwordsf(w, k) =

{w′ ∈V | f(w′) ∈simvecsf(f(w), k)}
if w ∈V
∅
else
Next, we show how to integrate a word embedding into the selection process.
4.2
Using Similarities for Axiom Selection
Using the example of the frequently used selection SInE, we now show how word
embeddings can be integrated into the selection process. For this the trigger
relation must be exchanged. The trigger-based selection presented in Deﬁnition 1
can be used unchanged. In the following deﬁnition we assume the set of predicate
and function symbols coincide with the set of words in the vocabulary of the used
word embedding. We do this only to increase the readability of the deﬁnition.
Since this assumption does not apply in practice, the next section presents details
on how to map the symbols from the KB to the vocabulary of the embedding
and vice versa.
Deﬁnition 4 (Word embedding enhanced trigger relation). Let KB be a
knowledge base with Σ the set of function and predicate symbols occurring in KB,
A an axiom in KB, s ∈Σ be a symbol and k ∈N. Let furthermore f : V →Rn
be a word embedding with vocabulary V = Σ. Then the word embedding enhanced
set of symbols triggering axiom A is deﬁned as follows:
simtriggersf(A, k) =

s∈{s′|triggers(s′,A)}
({s} ∪(simwordsf(s, k)))
Intuitively, not only the rarest symbol s is allowed to trigger an axiom but
also all symbols which are among the k most similar symbols of s according to
word embedding f. This can result in an axiom like subClass(beer, beverage) to
be triggered by pilsner which does not occur in the axiom but is clearly related
to content of the axiom.
Setting triggers(s, A) iﬀs ∈simtriggersf(A, k) in Deﬁnition 1 for some
embedding f and some k ∈N, results in a selection technique that takes word
embedding f into account.
Deﬁnition 4 assumes that the symbols occurring in the KB coincide with the
vocabulary of the word embedding. Next we describe problems occurring if this
assumption is not true and we show how to solve these problems.

258
U. Furbach et al.
4.3
Relating Symbol Names to the Vocabulary
of a Word Embedding
Usually, a KB does not come in combination with a suitable word embedding.
There are a variety of word embeddings that can be downloaded and used
directly. For the integration of a word embedding into axiom selection it is impor-
tant that the word embedding used matches the KB from which the selection is
to be made. In other words, the embedding should be calculated on a text that
matches the KB thematically. Ideally, for example, a word embedding learned on
the text of Wikipedia would be used for selection on Yago, since Yago contains
the knowledge of Wikipedia. Even if there is a word embedding which is close to
the content of the KB, we cannot assume all predicate and function symbols of
the KB to have vector representations in the embedding. A disadvantage of word
embeddings is that they usually do not distinguish between the diﬀerent mean-
ings of a word. This means, for example, that the meaning of the verb project
together with the meaning of the noun project is represented by the same vec-
tor. Therefore, WordNet mappings for word embeddings are usually not oﬀered.
However, when relating symbol names with elements in the embedding’s vocab-
ulary, an existing WordNet mapping for the KB can be used. Such a WordNet
mapping of a KB is a mapping of the symbol names occurring in the KB to
WordNet synsets.1 A WordNet synset represents a set of synonyms, meaning a
set of words with a similar meaning. Figure 1 shows some of the noun senses in
WordNet for the word grass.
– S: (n) grass (narrow-leaved green herbage: grown as lawns; used as pasture for grazing ani-
mals; cut and dried as hay)
– S: (n) supergrass, grass (a police informer who implicates many people)
– S: (n) pot, grass, green goddess, dope, weed, gage, sess, sens, smoke, skunk, locoweed, Mary
Jane (street names for marijuana)
Fig. 1. Some of the noun senses of the word grass in WordNet. Each line represents
one synset of the word grass. The text given in parentheses represents the sense. The
synset shown in the third line contains the synonyms supergrass and grass and has the
sense a police informer who implicates many people.
One way to relate a symbol name s of the KB to elements in the word
embedding’s vocabulary is to check for each symbol name, if the symbol name
occurs as a word in the vocabulary. We create a relation rel from this information
such that rel(s, s) for all symbols names s that are contained in the embedding’s
vocabulary. If the symbols have meaningful names, this can produce a relation
covering many symbol names.
To achieve a higher coverage of the relation between symbol names and ele-
ments in the word embedding’s vocabulary, the WordNet mapping of the KB
1 WordNet mappings usually also contain information about subclass or instance of
relations. Since these relations are not relevant for this paper, they are omitted.

Names Are Not Just Sound and Smoke
259
can be used as follows: the symbol name s is ﬁrst mapped to a WordNet synset
using the WordNet mapping and then for all synonyms l belonging to this synset
it is checked if l is in the word embedding’s vocabulary. Meaning that for all syn-
onyms l occurring in the vocabulary of the embedding rel(s, l) is added to the
relation.
In the following deﬁnition, we omit all further information in WordNet and
consider a synset to be a set of synonyms. Therefore, a WordNet mapping is
assumed to be a mapping w : Σ →2Synsets with Synsets the set of WordNet
synsets and each synset S = {l1, . . . ln} a set of synonyms.
Deﬁnition 5 (Bridging relation between symbol names and a word
embedding’s vocabulary).
Let KB be a knowledge base with Σ the set of
function and predicate symbols occurring in KB and s ∈Σ be a symbol. Let
furthermore f : V →Rn be a word embedding with vocabulary V . Let Synsets
be the set of synsets in WordNet and w : Σ →2Synsets be a WordNet mapping
of KB. Then the bridging relation rel ⊆Σ × V is deﬁned as {(s, s) | s ∈
Σ ∩V } ∪{(s, l) | S ∈w(s) and l ∈S and l ∈V }.
Note that the bridging relation rel is not total. So there may be symbol names
not taking part in the relation and there may be words in the vocabulary not
taking part in the relation. Furthermore, rel is not a function. So there may be
distinct words w1, w2 in the vocabulary with rel(s, w1) and rel(s, w2) for some
symbol s.
Deﬁnition 6 (Word embedding enhanced trigger relation using a
bridging relation).
Let KB be a knowledge base with Σ the set of function
and predicate symbols occurring in KB, A an axiom in KB, s ∈Σ be a symbol
and k ∈N. Let furthermore f : V →Rn be a word embedding with vocabulary
V and rel a bridging relation between symbols in Σ and words in V . Then the
word embedding enhanced set of symbols triggering axiom A using rel is deﬁned
as follows:
simtriggersf(A, k) =

s∈{s′|triggers(s′,A)}
({s}∪{s′′|rel(s, w) and w′∈simwordsf(w, k)
and rel(s′′, w′)})
We call the resulting selection Similarity SInE. Note that for an empty
bridging relation rel
Deﬁnition 6 is identical to Deﬁnition 4. In general,
simtriggersf(A, k) does not necessarily contain k + 1 symbols. In general, it
contains many less than k + 1 symbols, since not all words contained in
simwordsf(w, k) can be mapped by rel to a symbol in Σ. In extreme cases, if no
additional similar symbols can be added to the triggers relation due to an empty
bridging relation, the resulting selection behaves exactly like SInE. Conversely,
it could also happen that more than k +1 symbols end up in simtriggersf(A, k),
since a symbol s can be associated with more than one word from V via rel, and
for all these words the k most similar words are looked up in the word embed-
ding. In Sect. 5, we present the experiences we have made in this respect in our
experiments.

260
U. Furbach et al.
Example 1. We consider the following set of axioms, which is an extended version
of the example given in [9]:
∀X, Y, Z((subClass(X, Y ) ∧subClass(Y, Z)) →subClass(X, Z))
(4)
subClass(stone, liquid) →⊥
(5)
subClass(petrol, liquid)
(6)
subClass(beverage, liquid)
(7)
subClass(beer, beverage)
(8)
subClass(guiness, beer)
(9)
subClass(pilsner, beer)
(10)
subClass(coolant, liquid)
(11)
subClass(lager, beer)
(12)
subClass(ale, beer)
(13)
The second column in Fig. 2 shows for each symbol the number of axioms it
occurs in. This information is used to determine the set of axioms triggered by
each symbol which is shown in the third column. The fourth column presents
for each given symbol the set of similar symbols found by using rel to map the
symbol name to the word embedding ConceptNet Numberbatch’s vocabulary V ,
determining the k = 100 most similar words and using rel to map these words
back (if possible) to symbols in the axiom set. For the example, we assume rel
to be deﬁned as rel = {(s, s) | s ∈V ∩Σ}. Of course ConceptNet Numberbatch
contains many other similar words that do not occur in the axioms. For example
the 100 words most similar to beverage in ConceptNet Numberbatch’s vocabulary
contain tea like drink, beverages, red bull, beer run and in drink all of which can
not be mapped back to the symbols in our example. The last column of Fig. 2
presents for each symbol the axioms that are similarity triggered by this symbol.
We now consider the following conjecture:
?subClass(beer, liquid)
(14)
With regular SInE symbols subClass, beer and liquid are 0-step triggered.
subClass triggers axiom (4) whereas symbol beer and liquid do not trigger further
axioms (see third column Fig. 2). With the one selected axiom a proof cannot
be found.
With Similarity SInE symbols subClass, beer and liquid are 0-step triggered.
Symbol subClass triggers axiom (4), beer triggers axioms (7), (8), (10), (12) and
(13) causing the symbols beverage, liquid, pilsner, lager and ale to be 1-step
triggered. The symbols liquid, beverage, pilsner, lager and ale do not trigger
any further axioms. Using the selected axioms, a proof can be found.
Using a tolerance of 2 for regular SInE causes SInE to select axioms (4),
(7) and (8), which are suﬃcient to ﬁnd a proof. The extent to which tolerance
must be increased, however, depends strongly on the axioms present in the KB.
For example, if we add the information about two more beers and two more
liquids to the KB, the tolerance must be set to 3 for SInE to select all necessary
axioms. If we then add three more beers and three more liquids, SInE selects
all axioms necessary for the proof only at tolerance 4.5. In our example, the

Names Are Not Just Sound and Smoke
261
Fig. 2. SInE’s and Similarity SInE’s trigger relation.
tolerance to be used depends on the relationship between the number of the
occurrence of the beverage and liquid symbols as well as the relationship between
the number of occurrences of the beverage and beer symbols. If the symbols
beer and liquid occur much more frequently than the symbol beverage, a high
tolerance is required to be able to select all necessary axioms with SInE. In
contrast, Similarity SInE selection is less dependent on relationships between
frequencies. Regardless of how many more beers and liquids are added, Similarity
SInE always selects axioms (4), (7), (8), (10), (12) and (13).
5
Experiments
In order to evaluate Similarity SInE, we need KBs in which the symbols have
meaningful names. In addition, the KBs used must be large, otherwise the use
of selection techniques does not make sense. The LTB division of CASC [29]
contains very large problems. Unfortunately, most of the problems in this division
do not have meaningful symbol names. The CSR SUMO problems (CSR075 -
CSR109 and CSR118) are a positive exception here, and therefore considered for
the experiments. In addition to the CSR SUMO problems, we consider Adimen-
SUMO for the experiments. It was obtained by translating a large part of SUMO
into ﬁrst-order logic [2]. The reason for choosing Adimen-SUMO over SUMO is
the fact that the current version of Adimen-SUMO (v2.6), comes with a set
of 8010 automatically generated white-box truth-tests [1]. These problems are
supposed to be entailed by Adimen-SUMO and are therefore ﬁt for the evaluation
of Similarity SInE. Just like SUMO, Adimen-SUMO uses meaningful symbol
names and provides a mapping to WordNet (which corresponds to SUMO’s
WordNet mapping). Since there is no word embedding that ﬁts exactly to CSR
SUMO or Adimen-SUMO, the word embedding ConceptNet Numberbatch was
used for the experiments, which contains a broad general knowledge.
In the experiments, we use SInE and Similarity SInE with diﬀerent parame-
ters as selection techniques. We adapted E’s SInE implementation, which can be

262
U. Furbach et al.
used as a stand-alone program, such that it takes similar symbols into account.
The resulting program is used to perform the Similarity SInE selection2. E’s
implementation of SInE is used in the experiments to perform the SInE selec-
tion. Since the selection of the formulae is completely independent of the ATP
used afterwards, we were able to use two diﬀerent ATPs, Hyper [4] and E [25],
after the selection step. Hyper is a tableau prover that is very well suited for
tasks within cognitive reasoning because of its compact proof structure, eﬃcient
equality handling and conﬂuence property. E is one of the best high performance
saturation-based ﬁrst-order systems.
5.1
Relating Symbol Names to a Word Embedding’s Vocabulary
For the experiments, we use the ConceptNet Numberbatch word embedding.
Adimen-SUMO contains 3,917 symbols. Starting with an empty bridging relation
58% namely 2,279 of these symbols can be mapped to words in the vocabulary by
throwing away preﬁxes and reformatting compound symbol names. E.g., symbol
c FamilyBusiness was mapped to word family business, which occurs in Con-
ceptNet Numberbatch’s vocabulary, and led to adding (c FamilyBusiness, fam-
ily business) to the bridging relation rel. For those 2,279 symbols for which this
brute-force relating of symbols to words in ConceptNet Numberbatch’s vocabu-
lary worked, the k = 100 most similar words were determined. Of these words,
on average, 2.4 (standard deviation 2.6) could be mapped back to symbol names
of Adimen-SUMO. In total, 5,521 similar symbols were found and used in the
triggers relation. The value of k has been determined by small normative exper-
iments. Further values of k will be considered in future work.
The coverage with this brute-force relating of symbol names to the word
embedding’s vocabulary can be improved to 63% using a bridging relation
derived from the SUMO WordNet mapping.
Not all the CSR SUMO problems include the same axiom sets. This is why
it is not possible to present one single number for the coverage of the bridg-
ing relation rel. Depending on the included axiom sets, the CSR SUMO prob-
lems use 3,452 to 34,239 diﬀerent symbols. Using the brute-force method to
create the bridging relation rel, the coverage was between 14% and 21%. Using
SUMO’s WordNet mapping increases coverage to 20% to 31%. Note that even
using SUMO’s WordNet mapping, the coverage of the bridging relation for CSR
SUMO is far below the coverage of the bridging relation for Adimen-SUMO.
In order to evaluate if the coverage of the bridging relation plays an important
role for the selection and the subsequent ATP run, we performed experiments
with both the bridging relation created by brute-force and the improved bridging
relation (using the WordNet mapping).
In our implementation, we take advantage of the fact that WordNet gives
a list of synonyms ordered by relevance. Therefore, for a given symbol name s
we only add (s, l) to rel for the most relevant word l occurring in the list of
synonyms.
2 Implementation (git hash ‘eeee0fc0b46c688ec25e08806d39ec8cea93cbc0’) available at
https://gitlab.uni-koblenz.de/corg/similaritysine.

Names Are Not Just Sound and Smoke
263
proof
model
timeout
0
20
40
60
80
100
2.3
94.88
2.8
35.26
62.48
2.25
Result of Hyper
Percentage of problems with depicted result
SInE
Similarity SInE (rel coverage 58%)
(a) Result of Hyper on the 6,701 problems, it
was not able to solve without selection.
proof
model
timeout
0
20
40
60
80
100
1.56
87.55
10.9
7.78
68.81
23.41
Result of E
Percentage of problems with depicted result
SInE
Similarity SInE (rel coverage 58%)
(b) Result of E on 3,405 problems, it was not
able to solve without selection.
Fig. 3. Percentage of the considered Adimen-SUMO problems where Hyper or E found
a proof, a model or ran into a timeout broken down by the selection technique used.
For all selections, tolerance one and recursion depth 1–6 was used.
5.2
Experimental Results
All tests were carried out on a computer featuring an Intel(R) Xeon(R) CPU
E5-2699 v3 @ 2.30GHz (only two cores were used) with 8GB RAM. For the
provers used in the experiments, we used a timeout of 15 s (cpu time). The
timeout is based on the experimental results from [1], where for the majority of
the Adimen-SUMO problem’s proofs were found in up to 10 s.
Hyper and E on Adimen-SUMO. For the Adimen-SUMO problems we ﬁrst
determined the set of problems for which selection makes sense for the used
provers. For this, Hyper and E were run for all 8,010 Adimen-SUMO problems
without using a selection technique. This resulted in 1,309 problems solved by
Hyper and 4,605 problems solved by E without selection. For the sets of prob-
lems unsolved without selection (6,701 for Hyper and 3,405 for E) SInE and
Similarity SInE (with 58% coverage of rel) were used to select formulae that
were then fed into the respective prover. Figure 3a shows the results for Hyper,
Fig. 3b for E. Hyper was able to ﬁnd a proof for 2.3% of the problems using the
formulae selected by SInE and for 35.26% of the problems using the Similarity
SInE selected formulae. E was able to ﬁnd a proof for 10.9% of the problems
using the SInE selected formulae and for 23.41% using the formulae selected
with Similarity SInE. This 12.51% increase shows that both tableau-based and
saturation-based provers beneﬁt from the Similarity SInE selection technique.
Experiments with Diﬀerent Tolerance Values. To investigate the eﬀect
of the tolerance parameter on selection, we performed experiments with toler-

264
U. Furbach et al.
ance values between 1.0 and 6.0 with SInE, Similarity SInE (rel coverage 58%)
and Similarity SInE (rel coverage 63%) on the CSR SUMO problems and on
1,000 randomly selected Adimen-SUMO problems. Both problem sets consist
only of problems Hyper is not able to solve without selection. A comparison of
Fig. 3a and the values for tolerance 1.0 in Fig. 4b shows that the 1,000 randomly
selected Adimen-SUMO problems are representative for the entire set of prob-
lems. Furthermore Fig. 3 shows that Similarity SInE performs better than SInE
for the given setup no matter the prover. Therefore the formulae selected by the
diﬀerent selection techniques are checked for satisﬁability only with Hyper.
Figure 4 shows that on the formulae selected by Similarity SInE with low
tolerance values, signiﬁcantly more proofs were found than on the formulae
selected by SInE with the same tolerance. This is due to the fact that Sim-
ilarity SInE selects more, but the additionally selected formulae are targeted
towards the goal due to the use of the similar symbols. SInE can catch up by
using higher tolerance values. Starting from a tolerance value of 3.0 for CSR
problems and 6.0 for Adimen-SUMO problems, more proofs can be found on the
formulae selected with SInE than on the formulae selected with Similarity SInE.
For these high tolerance values, too many additional formulae are selected with
Similarity SInE. Figure 4b shows that a higher coverage of the bridging relation
rel further improves the results for low tolerance values. On the CSR SUMO
problems this eﬀect is less clear, which we attribute to the fact that the CSR
SUMO problems are generally more diﬃcult to solve than the Adimen-SUMO
problems. The fact that the improved coverage of the bridging relation for high
tolerance values worsens the proportion of solved problems can be explained
by the additionally selected formulae. For high tolerance values Similarity SInE
with bridging relation with 58% coverage already selects too much. Similarity
SInE with bridging relation with a coverage of 63% selects even more formulae
for these high tolerance values, which exacerbates this problem.
Since the trigger relation of Similarity SInE extends SInE’s trigger relation,
Similarity SInE always selects a superset of the axioms selected by SInE. Figure 4
presents the number of selected axioms for both SInE and Similarity SInE. Even
with a low tolerance, Similarity SInE already selects signiﬁcantly more axioms
than SInE. Nevertheless, there are not many more timeouts with the Similarity
SInE selected axioms than with the SInE selected axioms (see Fig. 3).
5.3
Discussion
The experiments on the Adimen-SUMO problems reveal that the coverage of the
bridging relation of symbol names to the used word embedding’s vocabulary is
crucial. Thus it is worthwhile to put work into a good coverage. Furthermore, the
experiments have shown that Similarity SInE is superior to SInE selection at low
tolerance values. Although SInE provides similar or better results with higher
tolerance values, it must be taken into account that suitable tolerance values are
not known in advance and are heavily dependent on the fact distributions in the
KB.

Names Are Not Just Sound and Smoke
265
To the best of our knowledge, the TPTP [28] does not include problems where
background knowledge, like word embeddings or WordNet mappings, can be used
to ﬁnd a proof. Since background knowledge plays an important role in practice,
we propose an extension of the TPTP by a problem class (domain BGK), where it
is necessary to consult background knowledge to solve the tasks. This background
knowledge could be speciﬁed WordNet mappings, word embeddings or even text
corpora which may be used by provers, when solving problems. The SUMO
problems of the CSR domain together with the SUMO WordNet mapping as
well as problems from Yago [27], Yago-SUMO [6] and Cyc [11] would be a good
starting point for this new domain.
1.0
1.5
3.0
6.0
0
20
40
60
27.68
40.18
47.77
45.09
31.7
41.52
40.18
39.73
32.14
41.52
39.73
38.84
Tolerance
Percentage of problems where Hyper found a proof
Mean number of selected axioms
SInE
Similarity SInE (rel coverage 58%)
Similarity SInE (rel coverage 63%)
0
200
400
600
800
Mean number of selected axioms
(a) Hyper on 224 CSR SUMO problems, it
was not able to solve without selection.
1.0
1.5
3.0
6.0
0
20
40
60
2.1
4.4
37.1
55.1
37.7
48.2
48.3
52.4
40.5
53.9
43.4
49.3
Tolerance
Percentage of problems where Hyper found a proof
Mean number of selected axioms
SInE
Similarity SInE (rel coverage 58%)
Similarity SInE (rel coverage 63%)
0
1,000
2,000
3,000
Mean number of selected axioms
(b) Hyper on 1,000 randomly selected Adimen-
SUMO problems
Fig. 4. Percentage of the considered problems where Hyper found a proof using the
selected formulae broken down by the selection technique and tolerance values used.
For all selections recursion depth 1–6 was used.
6
Conclusion and Future Work
In many areas the symbol names in KBs are not chosen arbitrarily, but have
a meaning. Although this meaning of symbol names is an interesting source
of information, it has been ignored by previous axiom selection techniques. In
this paper, we introduced a selection technique that uses word embeddings to
consider the similarities of symbol names and thus includes their meaning in
the selection process. Since the presented method is based on the SInE selection
technique, it is called Similarity SInE. In future work, we will investigate diﬀerent
values for the number k of similar symbol names considered by Similarity SInE.
Furthermore, we want to determine to what extent Similarity SInE is suitable as

266
U. Furbach et al.
a selection technique for commonsense reasoning benchmarks such as the Choice
of Plausible Alternatives Challenge [13]. More speciﬁcally, we want to analyze
whether for a given problem, Similarity SInE is able to extract thematically
appropriate modules from a background KB.
References
1. ´Alvez, J., Hermo, M., Lucio, P., Rigau, G.: Automatic white-box testing of ﬁrst-
order logic ontologies. CoRR, abs/1705.10219 (2017)
2. ´Alvez, J., Lucio, P., Rigau, G.: Adimen-SUMO: reengineering an ontology for ﬁrst-
order reasoning. Int. J. Seman. Web Inf. Syst. 8, 80–116 (2012)
3. Basile, V., Cabrio, E., Schon, C.: KNEWS: using logical and lexical semantics to
extract knowledge from natural language. In: Proceedings of the European Con-
ference on Artiﬁcial Intelligence (ECAI) (2016)
4. Bender, M., Pelzer, B., Schon, C.: System description: E-KRHyper 1.4. In:
Bonacina, M.P. (ed.) CADE 2013. LNCS (LNAI), vol. 7898, pp. 126–134. Springer,
Heidelberg (2013). https://doi.org/10.1007/978-3-642-38574-2 8
5. Curran, J.R., Clark, S., Bos, J.: Linguistically motivated large-scale NLP with C&C
and boxer. In: Proceedings of the ACL 2007 Demo and Poster Sessions, Prague,
Czech Republic, pp. 33–36 (2007)
6. de Melo, G., Suchanek, F. M., Pease, A.: Integrating YAGO into the suggested
upper merged ontology. In: 20th IEEE International Conference on Tools with
Artiﬁcial Intelligence, ICTAI 2008, Dayton, Ohio, USA, 3–5 November 2008, vol.
1, pp. 190–193. IEEE Computer Society (2008)
7. de Rooij, S., Beek, W., Bloem, P., van Harmelen, F., Schlobach, S.: Are names
meaningful? Quantifying social meaning on the semantic web. In: Groth, P., et al.
(eds.) ISWC 2016. LNCS, vol. 9981, pp. 184–199. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46523-4 12
8. Firth, J.R.: Papers in Linguistics 1934–1951: Rep. Oxford University Press, Oxford
(1991)
9. Hoder, K., Voronkov, A.: Sine qua non for large theory reasoning. In: Bjørner, N.,
Sofronie-Stokkermans, V. (eds.) CADE 2011. LNCS (LNAI), vol. 6803, pp. 299–
314. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-22438-6 23
10. Kuksa, E., Mossakowski, T.: Prover-independent axiom selection for automated
theorem proving in Ontohub. In: Fontaine, P., Schulz, S., Urban, J. (eds.) Pro-
ceedings of the 5th Workshop on Practical Aspects of Automated Reasoning Co-
Located with International Joint Conference on Automated Reasoning, IJCAR
2016, Coimbra, Portugal, 2nd July 2016, volume 1635 of CEUR Workshop Pro-
ceedings, pp. 56–68. CEUR-WS.org (2016)
11. Lenat, D.B.: CYC: a large-scale investment in knowledge infrastructure. Commun.
ACM 38(11), 33–38 (1995)
12. Luo, Z., Sha, Y., Zhu, K.Q., Hwang, S., Wang, Z.: Commonsense causal reasoning
between short texts. In: Baral, C., Delgrande, J.P., Wolter, J. (eds.) Principles
of Knowledge Representation and Reasoning: Proceedings of the Fifteenth Inter-
national Conference, KR 2016, Cape Town, South Africa, 25–29 April 2016, pp.
421–431. AAAI Press (2016)
13. Maslan, N., Roemmele, M., Gordon, A.S.: One hundred challenge problems for log-
ical formalizations of commonsense psychology. In: Twelfth International Sympo-
sium on Logical Formalizations of Commonsense Reasoning, Stanford, CA (2015)

Names Are Not Just Sound and Smoke
267
14. Meng, J., Paulson, L.C.: Lightweight relevance ﬁltering for machine-generated res-
olution problems. J. Appl. Logic 7(1), 41–57 (2009)
15. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Eﬃcient estimation of word repre-
sentations in vector space. CoRR, abs/1301.3781 (2013)
16. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed repre-
sentations of words and phrases and their compositionality. In: Burges, C.J.C.,
Bottou, L., Ghahramani, Z., Weinberger, K.Q. (eds.) Advances in Neural Infor-
mation Processing Systems 26: 27th Annual Conference on Neural Information
Processing Systems 2013. Proceedings of a Meeting Held, Lake Tahoe, Nevada,
United States, 5–8 December 2013, pp. 3111–3119 (2013)
17. Miller, G.A.: WordNet: a lexical database for English. Commun. ACM 38(11),
39–41 (1995)
18. Miller, G.A., Charles, W.G.: Contextual correlates of semantic similarity. Lang.
Cogn. Process. 6(1), 1–28 (1991)
19. Niles, I., Pease, A.: Towards a standard upper ontology. In: Proceedings of the
international conference on Formal Ontology in Information Systems-Volume 2001,
pp. 2–9. ACM (2001)
20. Pease, A.: Ontology: A Practical Guide. Articulate Software Press, Angwin (2011)
21. Pennington, J., Socher, R., Manning, C.D.: Glove: global vectors for word repre-
sentation. In: Moschitti, A., Pang, B., Daelemans, W. (eds.) Proceedings of the
2014 Conference on Empirical Methods in Natural Language Processing, EMNLP
2014, Doha, Qatar, 25–29 October 2014, A Meeting of SIGDAT, A Special Interest
Group of the ACL, pp. 1532–1543. ACL (2014)
22. Rockt¨aschel, T., Riedel, S.: End-to-end diﬀerentiable proving. In: NIPS, pp. 3791–
3803 (2017)
23. Roederer, A., Puzis, Y., Sutcliﬀe, G.: Divvy: an ATP meta-system based on
axiom relevance ordering. In: Schmidt, R.A. (ed.) CADE 2009. LNCS (LNAI),
vol. 5663, pp. 157–162. Springer, Heidelberg (2009). https://doi.org/10.1007/978-
3-642-02959-2 13
24. Schon, C., Siebert, S., Stolzenburg, F.: The CoRg project - cognitive reasoning. KI
33(3) (2019, to appear)
25. Schulz, S.: System description: E 1.8. In: McMillan, K., Middeldorp, A., Voronkov,
A. (eds.) LPAR 2013. LNCS, vol. 8312, pp. 735–743. Springer, Heidelberg (2013).
https://doi.org/10.1007/978-3-642-45221-5 49
26. Speer, R., Chin, J., Havasi, C.: ConceptNet 5.5: an open multilingual graph of
general knowledge. In: Singh, S.P., Markovitch, S. (eds.) Proceedings of the Thirty-
First AAAI Conference on Artiﬁcial Intelligence, San Francisco, California, USA,
4–9 February 2017, pp. 4444–4451. AAAI Press (2017)
27. Suchanek, F.M., Kasneci, G., Weikum, G.: YAGO: a large ontology from Wikipedia
and WordNet. Web Semant. 6(3), 203–217 (2008)
28. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure: from CNF
to TH0, TPTP v6.4.0. J. Autom. Reason. 1–20 (2017)
29. Sutcliﬀe, G.: The 9th IJCAR automated theorem proving system competition
CASC-J9. AI Commun. 31(6), 495–507 (2018)
30. Sutcliﬀe, G., Puzis, Y.: SRASS - a semantic relevance axiom selection system. In:
Pfenning, F. (ed.) CADE 2007. LNCS (LNAI), vol. 4603, pp. 295–310. Springer,
Heidelberg (2007). https://doi.org/10.1007/978-3-540-73595-3 20
31. Tiedemann, J.: Parallel data, tools and interfaces in OPUS. In: Calzolari, N., (Con-
ference Chair) et al. (eds.) Proceedings of the Eight International Conference on
Language Resources and Evaluation, LREC 2012, Istanbul, Turkey. European Lan-
guage Resources Association (ELRA), May 2012

268
U. Furbach et al.
32. Wang, L., Sun, M., Zhao, W., Shen, K., Liu, J.: Yuanfudao at SemEval-2018 task
11: three-way attention and relational knowledge for commonsense machine com-
prehension. In: Apidianaki, M., Mohammad, S.M., May, J., Shutova, E., Bethard,
S., Carpuat, M. (eds.) Proceedings of The 12th International Workshop on Seman-
tic Evaluation, SemEval@NAACL-HLT, New Orleans, Louisiana, 5–6 June 2018,
pp. 758–762. Association for Computational Linguistics (2018)
33. Williams, B., Lieberman, H., Winston, P.H.: Understanding stories with large-
scale common sense. In: Gordon, A.S., Miller, R., Tur´an, G. (eds.) Proceedings of
the Thirteenth International Symposium on Commonsense Reasoning, COMMON-
SENSE 2017, London, UK, 6–8 November 2017, volume 2052 of CEUR Workshop
Proceedings. CEUR-WS.org (2017)
34. Zhang, J.: System description: MCS: model-based conjecture searching. CADE
1999. LNCS (LNAI), vol. 1632, pp. 393–397. Springer, Heidelberg (1999). https://
doi.org/10.1007/3-540-48660-7 37

Computing Expected Runtimes
for Constant Probability Programs
J¨urgen Giesl1
, Peter Giesl2
, and Marcel Hark1(B)
1 LuFG Informatik 2, RWTH Aachen University, Aachen, Germany
{giesl,marcel.hark}@cs.rwth-aachen.de
2 Department of Mathematics, University of Sussex, Brighton, UK
p.a.giesl@sussex.ac.uk
Abstract. We introduce the class of constant probability (CP) programs
and show that classical results from probability theory directly yield
a simple decision procedure for (positive) almost sure termination of
programs in this class. Moreover, asymptotically tight bounds on their
expected runtime can always be computed easily. Based on this, we
present an algorithm to infer the exact expected runtime of any CP
program.
Keywords: Probabilistic programs · Expected runtimes ·
(Positive) almost sure termination · Complexity · Decidability
1
Introduction
Probabilistic programs are used to describe randomized algorithms and proba-
bility distributions, with applications in many areas. As an example, consider
the well-known program which models the race between a tortoise and a hare
(see, e.g., [10,24,30]). As long as the tortoise (variable t) is not behind the hare
(variable h), it does one step in each iteration. With probability
1
2, the hare
while (h ≤t) {
t = t + 1;
{h = h + Unif (0, 10)} ⊕1
2
{h = h};
}
stays at its position and with probabil-
ity 1
2 it does a random number of steps
uniformly chosen between 0 and 10. The
race ends when the hare is in front of the
tortoise. Here, the hare wins with proba-
bility one and the technique of [30] infers the upper bound 2
3 ·max(t−h+9, 0) on
the expected number of loop iterations. Thus, the program is positively almost
surely terminating.
Section 2 recapitulates preliminaries on probabilistic programs and on the
connection between their expected runtime and their corresponding recurrence
equation. Then we show in Sects. 3 and 4 that classical results on random walk
theory directly yield a very simple decision procedure for (positive) almost sure
Supported by the DFG Research Training Group 2236 UnRAVeL and the London
Mathematical Society (Grant 41662, Research in Pairs).
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 269–286, 2019.
https://doi.org/10.1007/978-3-030-29436-6_16

270
J. Giesl et al.
termination of CP programs like the tortoise and hare example. In this way,
we also obtain asymptotically tight bounds on the expected runtime of any CP
program. Based on these bounds, in Sect. 5 we develop the ﬁrst algorithm to
compute closed forms for the exact expected runtime of such programs. In Sect. 6,
we present its implementation in our tool KoAT [9] and discuss related and future
work. We refer to [20] for a collection of examples to illustrate the application
of our algorithm and for all proofs.
2
Expected Runtimes of Probabilistic Programs
Example 1 (Tortoise and Hare). The program
Prace on the right formulates the race of the
tortoise and the hare as a CP program. In
the loop guard, we use the scalar product
(1, −1) • (t, h) which stands for t −h. Exactly
one of the instructions with numbers in brack-
ets [. . .] is executed in each loop iteration
and the number indicates the probability that
the corresponding instruction is chosen.
while ((1, −1) • (t, h) > −1) {
(t, h) = (t, h) + (1, 0)
[ 6
11];
(t, h) = (t, h) + (1, 1)
[ 1
22];
(t, h) = (t, h) + (1, 2)
[ 1
22];
(t, h) = (t, h) + (1, 3)
[ 1
22];
...
(t, h) = (t, h) + (1, 10)
[ 1
22];
}
We now deﬁne the kind of probabilistic programs considered in this paper.
Deﬁnition 2 (Probabilistic Program). A pro-
gram has the form on the right, where x = (x1,
. . . , xr) for some r ≥1 is a tuple of pairwise diﬀe-
rent program variables, a, c1, . . . , cn ∈Zr are tuples
of integers, the cj are pairwise distinct, b∈Z, • is
the scalar product (i.e., (a1, . . . , ar) • (x1, . . . , xr) =
a1 · x1 + . . . + ar · xr), and d ∈Zr with a • d ≤b.
We require pc1(x), . . . , pcn(x), p′(x)
∈
R≥0
=

r
∈
R
|
r
≥
0

and

1≤j≤n pcj(x) + p′(x) = 1 for all x ∈Zr. It is a program with direct ter-
mination if there is an x ∈Zr with a • x > b and p′(x) > 0. If all probabilities
are constant, i.e., if there are pc1, . . . , pcn, p′ ∈R≥0 such that pcj(x) = pcj and
p′(x) = p′ for all 1 ≤j ≤n and all x ∈Zr, we call it a constant probability
(CP) program.
while (a • x > b) {
x = x + c1
[pc1(x)];
...
x = x + cn
[pcn(x)];
x = d
[p′(x)];
}
Such a program means that the integer variables x are changed to x + cj
with probability pcj(x). For inputs x with a • x ≤b the program terminates
immediately. Note that the program in Example 1 has no direct termination
(i.e., p′(x) = 0 for all x ∈Zr). Since the values of the program variables only
depend on their values in the previous loop iteration, our programs correspond
to Markov Chains [32] and they are related to random walks [16,21,33], cf. [20]
for details.
Clearly, in general termination is undecidable and closed forms for the run-
times of programs are not computable. Thus, decidability results can only
be obtained for suitably restricted forms of programs. Our class nevertheless

Computing Expected Runtimes
271
includes many examples that are often regarded in the literature on probabilistic
programs. So while other approaches are concerned with incomplete techniques
to analyze termination and complexity, we investigate classes of probabilistic pro-
grams where one can decide the termination behavior, always ﬁnd complexity
bounds, and even compute the expected runtime exactly. Our decision procedure
could be integrated into general tools for termination and complexity analysis of
probabilistic programs: As soon as one has to investigate a sub-program that falls
into our class, one can use the decision procedure to compute its exact runtime.
Our contributions provide a starting point for such results and the considered
class of programs can be extended further in future work.
In probability theory (see, e.g., [2]), given a set Ω of possible events, the goal
is to measure the probability that events are in certain subsets of Ω. To this end,
one regards a set F of subsets of Ω, such that F contains the full set Ω and is
closed under complement and countable unions. Such a set F is called a σ-ﬁeld,
and a pair of Ω and a corresponding σ-ﬁeld F is called a measurable space.
A probability space (Ω, F, P) extends a measurable space (Ω, F) by a probabil-
ity measure P which maps every set from F to a number between 0 and 1, where
P(Ω) = 1, P(∅) = 0, and P(
j≥0 Aj) = 
j≥0 P(Aj) for any pairwise disjoint
sets A0, A1, . . . ∈F. So P(A) is the probability that an event from Ω is in the
subset A. In our setting, we use the probability space ((Zr)ω, FZr, PP
x0) arising
from the standard cylinder-set construction of MDP theory, cf. [20]. Here, (Zr)ω
corresponds to all inﬁnite sequences of program states and PP
x0 is the probabil-
ity measure induced by the program P when starting in the state x0 ∈Zr. For
example, if A ⊆(Z2)ω consists of all inﬁnite sequences starting with (5, 1), (6, 1),
(7, 6), then PPrace
(5,1) (A) =
6
11 ·
1
22 =
3
121. So, if one starts with (5, 1), then
3
121 is
the probability that the next two states are (6, 1) and (7, 6). Once a state is
reached that violates the loop guard, then the probability to remain in this state
is 1. Hence, if B contains all inﬁnite sequences starting with (7, 8), (7, 8), then
PPrace
(7,8) (B) = 1. In the following, for any set of numbers M let M = M ∪{∞}.
Deﬁnition 3 (Termination Time). For a program P as in Deﬁnition 2, its
termination time is the random variable T P : (Zr)ω →N that maps every inﬁnite
sequence ⟨z0, z1, . . .⟩to the ﬁrst index j where zj violates P’s loop guard.
Thus, T Prace(⟨(5, 1), (6, 1), (7, 8), (7, 8), . . .⟩) = 2 and T Prace(⟨(5, 1), (6, 1),
(5, 6), (8, 6), (9, 6), . . .⟩) = ∞(i.e., this sequence always satisﬁes Prace’s loop
guard as the jth entry is (5 + j, 6) for j ≥3). Now we can deﬁne the dif-
ferent notions of termination and the expected runtime of a probabilistic pro-
gram. As usual, for any random variable X on a probability space (Ω, F, P),
P(X = j) stands for P(X−1({j})). So PP
x0(T P = j) is the probability that a
sequence has termination time j. Similarly, PP
x0(T P < ∞) = 
j∈N PP
x0(T P = j).
The expected value E(X) of a random variable X : Ω →N for a probability
space (Ω, F, P) is the weighted average under the probability measure P, i.e.,
E(X) = 
j∈N j · P(X = j), where ∞· 0 = 0 and ∞· u = ∞for all u ∈N>0.
Deﬁnition 4 (Termination and Expected Runtime). A program P as in
Deﬁnition 2 is almost surely terminating (AST) if PP
x0(T P < ∞) = 1 for any

272
J. Giesl et al.
initial value x0 ∈Zr. For any x0 ∈Zr, its expected runtime rtP
x0 (i.e., the
expected number of loop iterations) is deﬁned as the expected value of the random
variable T P under the probability measure PP
x0, i.e., rtP
x0 = EP
x0

T P
= 
j∈N j·
PP
x0(T P=j) if PP
x0(T P<∞) = 1, and rtP
x0 = EP
x0

T P
= ∞otherwise.
The program P is positively almost surely terminating (PAST) if for any ini-
tial value x0 ∈Zr, the expected runtime of P is ﬁnite, i.e., if rtP
x0 = EP
x0

T P
<
∞.
Example 5 (Expected Runtime for Prace). By the observations in Sect. 4 we will
infer that 2
3 ·(t−h+1) ≤rtPrace
(t,h) ≤2
3 ·(t−h+1)+ 16
3 holds whenever t−h > −1,
cf. Example 22. So the expected number of steps until termination is ﬁnite (and
linear in the input variables) and thus, Prace is PAST. The algorithm in Sect. 5
will even be able to compute rtPrace
(t,h) exactly, cf. Example 34.
If the initial values x0 violate the loop guard, then the runtime is trivially 0.
Corollary 6 (Expected Runtime for Violating Initial Values). For any
program P as in Deﬁnition 2 and any x0 ∈Zr with a•x0 ≤b, we have rtP
x0 = 0.
To obtain our results, we use an alternative, well-known characterization of
the expected runtime, cf. e.g., [3,8,15,24–27,32,34]. To this end, we search for
the smallest (or “least”) solution of the recurrence equation that describes the
runtime of the program as 1 plus the sum of the runtimes in the next loop
iteration, multiplied with the corresponding probabilities. Here, functions are
compared pointwise, i.e., for f, g : Zr →R≥0 we have f ≤g if f(x) ≤g(x) holds
for all x ∈Zr. So we search for the smallest function f : Zr →R≥0 that satisﬁes
f(x) =

1≤j≤n pcj(x)·f(x+cj)+p′(x)·f(d)+1
for all x with a•x > b. (1)
Equivalently, we can search for the least ﬁxpoint of the “expected runtime trans-
former” LP which transforms the left-hand side of (1) into its right-hand side.
Deﬁnition 7 (LP, cf. [32]). For P as in Deﬁnition 2, we deﬁne the expected
runtime transformer LP : (Zr →R≥0) →(Zr →R≥0), where for any f : Zr →
R≥0:
LP(f)(x) =
	
1≤j≤n pcj(x) · f(x + cj) + p′(x) · f(d) + 1,
if a • x > b
f(x),
if a • x ≤b
Example 8 (Expected Runtime Transformer for Prace). For Prace from Example
1, LPrace maps any function f : Z2 →R≥0 to LPrace(f), where LPrace(f)(t, h) =
	
6
11 · f(t + 1, h) + 1
22 · 
1≤j≤10 f(t + 1, h + j) + 1,
if t −h > −1
f(t, h),
if t −h ≤−1
(2)
Theorem 9 recapitulates that the least ﬁxpoint of LP indeed yields an equiva-
lent characterization of the expected runtime. In the following, let 0 : Zr →R≥0
be the function with 0(x) = 0 for all x ∈Zr.

Computing Expected Runtimes
273
Theorem 9 (Connection Between Expected Runtime and Least Fix-
point of LP, cf. [32]). For any P as in Deﬁnition 2, the expected runtime
transformer LP is continuous. Thus, it has a least ﬁxpoint lfp(LP ) : Zr →R≥0
with lfp(LP) = sup{0, LP(0), (LP)2(0), . . .}. Moreover, the least ﬁxpoint of LP
is the expected runtime of P, i.e., for any x0 ∈Zr, we have lfp(LP)(x0) = rtP
x0.
So the expected runtime rtPrace
(t,h)
can also be characterized as the smallest
function f : Z2 →R≥0 satisfying f(t, h)=(2), i.e., as the least ﬁxpoint of LPrace.
3
Expected Runtime of Programs with Direct
Termination
We start with stating a decidability result for the case where for all x with
a • x > b, the probability p′(x) for direct termination is at least p′ for some p′ > 0.
Intuitively, these programs have a termination time whose distribution is closely
related to the geometric distribution with parameter p′ (which has expected
value
1
p′ ). By using the alternative characterization of rtP
x0 from Theorem 9,
one obtains that such programs are always PAST and their expected runtime
is indeed bounded by the constant
1
p′ . This result will be used in Sect. 5 when
computing the exact expected runtime of such programs. The more involved case
where p′(x) = 0 is considered in Sect. 4.
Theorem 10 (PAST and Expected Runtime for Programs With Direct
Termination). Let P be a program as in Deﬁnition 2 where there is a p′ > 0
such that p′(x) ≥p′ for all x ∈Zr with a • x > b. Then P is PAST and its
expected runtime is at most
1
p′ , i.e., rtP
x0 ≤
1
p′ if a • x0 > b, and rtP
x0 = 0 if
a • x0 ≤b.
Example 11 (Example 1 with Direct Ter-
mination). Consider the variant Pdirect of
Prace on the right, where in each iteration,
the hare either does nothing with probabil-
ity
9
10 or one directly reaches a conﬁgura-
tion where the hare is ahead of the tortoise.
By Theorem 10 the program is PAST and its expected runtime is at most
1
1
10 =
10, i.e., independent of the initial state it takes at most 10 loop iterations on
average. In Sect. 5 it will turn out that 10 is indeed the exact expected runtime,
cf. Example 32.
4
Expected Runtimes of Constant Probability Programs
while ((1, −1) • (t, h) > −1) {
(t, h) = (t, h) + (1, 0)
[ 9
10];
(t, h) = (7, 8)
[ 1
10];
}
Now we present a very simple decision procedure for termination of CP programs
(Sect. 4.2) and show how to infer their asymptotic expected runtimes (Sect. 4.3).
This will be needed for the computation of exact expected runtimes in Sect. 5.

274
J. Giesl et al.
4.1
Reduction to Random Walk Programs
while (x > 0) {
x = x + m
[pm];
...
x = x + 1
[p1];
x = x
[p0];
x = x −1
[p−1];
...
x = x −k
[p−k];
x = d
[p′];
}
As a ﬁrst step, we show that we can restrict ourselves
to random walk programs, i.e., programs with a single
program variable x and the loop condition x > 0.
Deﬁnition 12 (Random Walk Program). A CP pro-
gram P is called a random walk program if there exist
m, k ∈N and d ∈Z with d ≤0 such that P has the form
on the right. Here, we require that m > 0 implies pm > 0
and that k > 0 implies p−k > 0.
Deﬁnition 13 shows how to transform any CP program as in Deﬁnition 2 into
a random walk program. The idea is to replace the tuple x by a single variable
x that stands for a • x −b. Thus, the loop condition a • x > b now becomes
x > 0. Moreover, a change from x to x + cj now becomes a change from x to
x + a • cj.
while (a • x > b) {
x = x + c1
[pc1];
...
x = x + cn
[pcn];
x = d
[p′];
}
Deﬁnition 13 (Transforming CP Programs to Ran-
dom Walk Programs). Let P be the CP program on the
left with x = (x1, . . . , xr) and a • d ≤b. Let rdw P denote
the aﬃne map rdw P : Zr→Z
with rdw P(z) = a • z −b for all
z ∈Zr. Thus, rdw P(d) ≤0. Let
kP, mP ∈N be minimal such
that
−kP ≤a • cj ≤mP holds for all 1 ≤j ≤n. For
−kP ≤j ≤mP, we deﬁne prdw
j
= 
1≤u≤n, a•cu=j pcu.
This results in the random walk program Prdw on the
right.
Example 14 (Transforming Prace). For the program
Prace of Example 1, the mapping rdw Prace : Z2 →Z
is rdw Prace(t, h) = (1, −1) • (t, h) + 1 = t −h + 1.
Hence we obtain the random walk program Prdw
race on
the right, where x = rdw Prace(t, h) represents the dis-
tance between the tortoise and the hare.
while (x > 0) {
x = x + mP [prdw
mP ];
...
x = x −kP
[prdw
−kP];
x = rdw P(d) [p′];
}
while (x > 0) {
x = x + 1
[ 6
11];
x = x
[ 1
22];
x = x −1
[ 1
22];
x = x −2
[ 1
22];
...
x = x −9
[ 1
22];
}
Approaches based on supermartingales (e.g.,
[1,4,10,12,13,17]) use mappings similar to rdw P
in order to infer a real-valued term which over-approximates the expected run-
time. However, in the following (non-trivial) theorem we show that our trans-
formation is not only an over- or under-approximation, but the termination
behavior and the expected runtime of P and Prdw are identical.
Theorem 15 (Transformation
Preserves
Termination
&
Expected
Runtime). Let P be a CP program as in Deﬁnition 2. Then the termination
times T P and T Prdware identically distributed w.r.t. rdw P, i.e., for all x0 ∈Zr

Computing Expected Runtimes
275
with x0 = rdw P(x0) and all j ∈N we have PP
x0(T P=j) = PPrdw
x0
(T Prdw=j). So in
particular, PP
x0(T P<∞)= PPrdw
x0
(T Prdw<∞) and rtP
x0 =EP
x0(T P)=EPrdw
x0
(T Prdw )=
rtPrdw
x0
. Thus, the expected runtimes of P on the input x0 and of Prdw on x0
coincide.
The following deﬁnition identiﬁes pathological programs that can be disre-
garded.
Deﬁnition 16 (Trivial Program). Let P be a CP pro-
gram as in Deﬁnition 2. We call P trivial if a = 0 = (0, 0,
. . . , 0) or if Prdw is the program on the right.
Note that a random walk program P is trivial iﬀit has the form while(x >
0){x = x [1]; }, since P = Prdw holds for random walk programs P. From now
on, we will exclude trivial programs P as their termination behavior is obvious:
for inputs x0 that satisfy the loop condition a • x0 > b, the program never
terminates (i.e., rtP
x0 = ∞) and for inputs x0 with a • x0 ≤b we have rtP
x0 = 0.
Note that if a = 0, then the termination behavior just depends on b: if b < 0,
then rtP
x0 = ∞for all x0 and if b ≥0, then rtP
x0 = 0 for all x0.
4.2
Deciding Termination
while (x > 0) {
x = x
[1];
}
We now present a simple decision procedure for (P)AST of random walk pro-
grams P. By the results of Sect. 4.1, this also yields a decision procedure for
arbitrary CP programs. If p′ > 0, then Theorem 10 already shows that P is
PAST and its expected runtime is bounded by the constant
1
p′ . Thus, in the
rest of Sect. 4 we regard random walk programs without direct termination, i.e.,
p′ = 0.
Deﬁnition 17 introduces the drift of a random walk program, i.e., the
expected value of the change of the program variable in one loop iteration, cf.
[4].
Deﬁnition 17 (Drift). Let P be a random walk program P as in Deﬁnition 12.
Then its drift is μP = 
−k≤j≤m j · pj.
Theorem 18 shows that to decide (P)AST, one just has to compute the drift.
Theorem 18 (Decision Procedure for (P)AST of Random Walk Pro-
grams). Let P be a non-trivial random walk program without direct termination.
• If μP > 0, then the program is not AST.
• If μP = 0, then the program is AST but not PAST.
• If μP < 0, then the program is PAST.
Example 19 (Prace is PAST). The drift of Prdw
race in Example 14 is μPrdw
race = 1 ·
6
11 + 1
22 ·
−9≤j≤0 j = −3
2 < 0. So on average the distance x between the tortoise
and the hare decreases in each loop iteration. Hence by Theorem 18, Prdw
race is
PAST and the following Corollary 20 implies that Prace is PAST as well.

276
J. Giesl et al.
Corollary 20 (Decision Procedure for (P)AST of CP programs). For a
non-trivial CP program P, P is (P)AST iﬀPrdw is (P)AST. Hence, Theorems
15 and 18 yield a decision procedure for AST and PAST of CP programs.
In [20], we show that Theorem 18 follows from classical results on random
walks [33]. Alternatively, Theorem 18 could also be proved by combining several
recent results on probabilistic programs: The approach of [28] could be used
to show that μP = 0 implies AST. Moreover, one could prove that μP < 0
implies PAST by showing that x is a ranking supermartingale of the program
[4,10,13,17]. That the program is not PAST if μP ≥0 and not AST if μP > 0
could be proved by showing that −x is a μP-repulsing supermartingale [12].
While the proof of Theorem 18 is based on known results, the formulation
of Theorem 18 shows that there is an extremely simple decision procedure for
(P)AST of CP programs, i.e., checking the sign of the drift is much simpler than
applying existing (general) techniques for termination analysis of probabilistic
programs.
4.3
Computing Asymptotic Expected Runtimes
It turns out that for random walk programs (and thus by Theorem 15, also for
CP programs), one can not only decide termination, but one can also infer tight
bounds on the expected runtime. Theorem 21 shows that the computation of
the bounds is again very simple.
Theorem 21 (Bounds on the Expected Runtime of CP Programs).
Let P be a non-trivial CP program as in Deﬁnition 2 without direct termina-
tion which is PAST (i.e., μPrdw < 0). Moreover, let kP be obtained according
to the transformation from Deﬁnition 13. If rdw P(x0) ≤0, then rtP
x0 = 0. If
rdw P(x0) > 0, then P’s expected runtime is asymptotically linear and we have
−
1
μPrdw · rdw P(x0)
≤
rtP
x0
≤
−
1
μPrdw · rdw P(x0) + 1−kP
μPrdw .
Example 22 (Bounds on the Runtime of Prace). In Example 19 we saw that
the program Prdw
race from Example 14 is PAST as it has the drift μPrdw
race =
−3
2 < 0. Note that here k = 9. Hence by Theorem 21 we get that whenever
rdw Prace(t, h) = t −h + 1 is positive, the expected runtime rtPrace
(t,h)
is between
−
1
μPrdw
race · rdw Prace(t, h) = 2
3 · (t −h + 1) and −
1
μPrdw
race · rdw Prace(t, h) +
1−k
μPrdw
race =
2
3 · (t −h + 1) + 16
3 . The same upper bound
2
3 · (t −h + 1) + 16
3 was inferred
in [30] by an incomplete technique based on several inference rules and linear
programming solvers. In contrast, Theorem 21 allows us to read oﬀsuch bounds
directly from the program.
Our proof of Theorem 21 in [20] again uses the connection to random walks
and shows that the classical Lemma of Wald [21, Lemma 10.2(9)] directly yields
both the upper and the lower bound for the expected runtime. Alternatively, the
upper bound in Theorem 21 could also be proved by considering that rdw P(x0)+

Computing Expected Runtimes
277
(1 −kP) is a ranking supermartingale [1,4,10,13,17] whose expected decrease in
each loop iteration is μP. The lower bound could also be inferred by considering
the diﬀerence-bounded submartingale −rdw P(x0) [7,19].
5
Computing Exact Expected Runtimes
While Theorems 10 and 21 state how to deduce the asymptotic expected runtime,
we now show that based on these results one can compute the runtime of CP
programs exactly. In general, whenever it is possible, then inferring the exact
runtimes of programs is preferable to asymptotic runtimes which ignore the
“coeﬃcients” of the runtime.
Again, we ﬁrst consider random walk programs and generalize our technique
to CP programs using Theorem 15 afterwards. Throughout Sect. 5, for any ran-
dom walk program P as in Deﬁnition 12, we require that P is PAST, i.e., that
p′ > 0 (cf. Theorem 10) or that the drift μP is negative if p′ = 0 (cf. Theorem
18). Note that whenever k = 0 and P is PAST, then p′ > 0.1
To compute P’s expected runtime exactly, we use its characterization as the
least ﬁxpoint of the expected runtime transformer LP (cf. Theorem 9), i.e., rtP
x
is the smallest function f : Z →R≥0 satisfying the constraint
f(x) =

−k≤j≤m pj · f(x + j) + p′ · f(d) + 1
for all x > 0,
(3)
cf. (1). Since P is PAST, f never returns ∞, i.e., f : Z →R≥0. Note that the
smallest function f : Z →R≥0 that satisﬁes (3) also satisﬁes
f(x) = 0
for all x ≤0.
(4)
Therefore, as d ≤0, the constraint (3) can be simpliﬁed to
f(x) =

−k≤j≤m pj · f(x + j) + 1
for all x > 0.
(5)
In Sect. 5.1 we recapitulate how to compute all solutions of such inhomoge-
neous recurrence equations (cf., e.g., [14, Ch. 2]). However, to compute rtP
x , the
challenge is to ﬁnd the smallest solution f : Z →R≥0 of the recurrence equation
(5). Therefore, in Sect. 5.2 we will exploit the knowledge gained in Theorems
10 and 21 to show that there is only a single function f that satisﬁes both (4)
and (5) and is bounded by a constant (if p′ > 0, cf. Theorem 10) resp. by a
linear function (if p′ = 0, cf. Theorem 21). This observation then allows us to
compute rtP
x exactly. So the crucial prerequisites for this result are Theorem 9
(which characterizes the expected runtime as the smallest solution of the recur-
rence equation (5)), Theorem 18 (which allows the restriction to negative drift
if p′ = 0), and in particular Theorems 10 and 21 (since Sect. 5.2 will show that
the results of Theorems 10 and 21 on the asymptotic runtime can be translated
into suitable conditions on the solutions of (5)).
1 If p′ = 0 and k = 0 then μP ≥0.

278
J. Giesl et al.
5.1
Finding All Solutions of the Recurrence Equation
Example 23 (Modiﬁcation of Prdw
race). To illustrate our
approach, we use a modiﬁed version of Prdw
race from Exam-
ple 14 to ease readability. In Sect. 6, we will consider
the original program Prdw
race resp. Prace from Example 14
resp. Example 1 again and show its exact expected run-
time inferred by the implementation of our approach. In
the modiﬁed program Pmod
race on the right, the distance be-
tween the tortoise and the hare still increases with proba-
bility
6
11, but the probability of decreasing by more than two is distributed to the
cases where it stays the same and where it decreases by two. We have p′ = 0 and
the drift is μPmod
race = 1 ·
6
11 + 0 ·
1
11 −1 ·
1
22 −2 ·
7
22 = −3
22 < 0. So by Theorem
18, Pmod
race is PAST. By Theorem 9, rtPmod
race
x
is the smallest function f : Z →R≥0
satisfying
f(x) =
6
11 ·f(x+1)+ 1
11 ·f(x)+ 1
22 ·f(x−1)+ 7
22 ·f(x−2)+1 for all x > 0. (6)
while (x > 0) {
x = x + 1
[ 6
11];
x = x
[ 1
11];
x = x −1
[ 1
22];
x = x −2
[ 7
22];
}
Instead of searching for the smallest f : Z →R≥0 satisfying (5), we ﬁrst
calculate the set of all functions f : Z →C that satisfy (5), i.e., we also consider
functions returning negative or complex numbers. Clearly, (5) is equivalent to
0 = pm · f(x + m) + . . . + p1 · f(x + 1) + (p0 −1) · f(x) +
p−1 · f(x −1) + . . . + p−k · f(x −k) + 1
for all x > 0.
(7)
The set of solutions on Z →C of this linear, inhomogeneous recurrence equa-
tion is an aﬃne space which can be written as an arbitrary particular solution
of the inhomogeneous equation plus any linear combination of k + m linearly
independent solutions of the corresponding homogeneous recurrence equation.
We start with computing a solution to the inhomogeneous equation (7). To
this end, we use the bounds for rtP
x from Theorems 10 and 21 (where we take
the upper bound
1
p′ if p′ > 0 and the lower bound −1
μP · x if p′ = 0). So we
deﬁne
Cconst = 1
p′ , if p′ > 0
and
Clin = −1
μP , if p′ = 0.
One easily shows that if p′ > 0, then f(x) = Cconst is a solution of the inhomo-
geneous recurrence equation (7) and if p′ = 0, then f(x) = Clin · x solves (7).
Example 24 (Example 23 cont.). In the program Pmod
race of Example 23, we have
p′ = 0 and μPmod
race = −3
22. Hence Clin = 22
3 and Clin · x is a solution of (6).
After having determined one particular solution of the inhomogeneous recur-
rence equation (7), now we compute the solutions of the homogeneous recurrence
equation which results from (7) by replacing the add-on “+ 1” with 0. To this
end, we consider the corresponding characteristic polynomial χP:2
χP(λ) = pm · λk+m + . . . + p1 · λk+1 + (p0 −1) · λk + p−1 · λk−1 + . . . + p−k (8)
2 If m = 0 then χP(λ) = (p0 −1) · λk + p−1 · λk−1 + . . . + p−k, and if k = 0 then
χP(λ) = pm · λm + . . . + p1 · λ + (p0 −1). Note that p0 ̸= 1 since P is PAST and
in Deﬁnition 12 we required that m > 0 implies pm > 0 and k > 0 implies p−k > 0.
Hence, the characteristic polynomial has exactly the degree k + m, even if m = 0 or
k = 0.

Computing Expected Runtimes
279
Let λ1, . . . , λc denote the pairwise diﬀerent (possibly complex) roots of the cha-
racteristic polynomial χP. For all 1 ≤j ≤c, let vj ∈N \ {0} be the multiplicity
of the root λj. Thus, we have v1 + . . . + vc = k + m.
Then we obtain the following k + m linearly independent solutions of the
homogeneous recurrence equation resulting from (7):
λx
j · xu
for all 1 ≤j ≤c and all 0 ≤u ≤vj −1
So f :Z→C is a solution of (5) (resp. (7)) iﬀthere exist coeﬃcients aj,u ∈C with
f(x) = C(x) +

1≤j≤c

0≤u≤vj−1 aj,u · λx
j · xu
for all x > −k,
(9)
where C(x) = Cconst = 1
p′ if p′ > 0 and C(x) = Clin · x = −1
μP · x if p′ = 0. The
reason for requiring (9) for all x > −k is that −k + 1 is the smallest argument
where f’s value is taken into account in (5).
Example 25 (Example 24 cont.). The characteristic polynomial for the program
Pmod
race of Example 23 has the degree k + m = 2 + 1 = 3 and is given by
χPmod
race(λ) =
6
11 · λ3 −10
11 · λ2 + 1
22 · λ + 7
22.
Its roots are λ1 = 1, λ2 = −1
2, and λ3 =
7
6. So here, all roots are real
numbers and they all have the multiplicity 1. Hence, three linearly independent
solutions of the homogeneous part of (6) are the functions 1x = 1, (−1
2)x, and
( 7
6)x. Therefore, a function f : Z →C satisﬁes (6) iﬀthere are a1, a2, a3 ∈C
such that
f(x) = Clin · x + a1 · 1x + a2 · (−1
2)x+ a3 · ( 7
6)x
=
22
3 · x + a1 + a2 · (−1
2)x+ a3 · ( 7
6)x
for x > −2.
(10)
5.2
Finding the Smallest Solution of the Recurrence Equation
In Sect. 5.1, we recapitulated the standard approach for solving inhomogeneous
recurrence equations which shows that any function f : Z →C that satisﬁes
the constraint (5) is of the form (9). Now we will present a novel technique to
compute rtP
x , i.e., the smallest non-negative solution f : Z →R≥0 of (5). By
Theorems 10 and 21, this function f is bounded by a constant (if p′ > 0) resp.
linear (if p′ = 0). So, when representing f in the form (9), we must have aj,u = 0
whenever |λj| > 1. The following lemma shows how many roots with absolute
value less or equal to 1 there are (i.e., these are the only roots that we have
to consider). It is proved using Rouch´e’s Theorem which allows us to infer the
number of roots whose absolute value is below a certain bound. Note that 1 is a
root of the characteristic polynomial iﬀp′ = 0, since 
−k≤j≤m pj = 1 −p′.
Lemma 26 (Number of Roots With Absolute Value ≤1). Let P be a
random walk program as in Deﬁnition 12 that is PAST. Then the characteristic
polynomial χP has k roots λ ∈C (counted with multiplicity) with |λ| ≤1.

280
J. Giesl et al.
Example 27 (Example 25 cont.). In Pmod
race of Example 23 we have k = 2. So by
Lemma 26, χP has exactly two roots with absolute value ≤1. Indeed, the roots of
χP are λ1 = 1, λ2 = −1
2, and λ3 = 7
6, cf. Example 25. So |λ3| > 1, but |λ1| ≤1
and |λ2| ≤1.
Based on Lemma 26, the following lemma shows that when imposing the
restriction that aj,u = 0 whenever |λj| > 1, then there is only a single function of
the form (9) that also satisﬁes the constraint (4). Hence, this must be the function
that we are searching for, because the desired smallest solution f : Z →R≥0 of
(5) also satisﬁes (4).
Lemma 28 (Unique Solution of (4) and (5) when Disregarding Roots
With Absolute Value > 1). Let P be a random walk program as in Deﬁnition
12 that is PAST. Then there is exactly one function f : Z →C which satisﬁes
both (4) and (5) (thus, it has the form (9)) and has aj,u = 0 whenever |λj| > 1.
The main theorem of Sect. 5 now shows how to compute the expected runtime
exactly. By Theorems 10 and 21 on the bounds for the expected runtime and by
Lemma 28, we no longer have to search for the smallest function that satisﬁes
(4) and (5), but we just search for any solution of (4) and (5) which has aj,u = 0
whenever |λj| > 1 (because there is just a single such solution). So one only has
to determine the values of the remaining k coeﬃcients aj,u for |λj| ≤1, which
can be done by exploiting that f(x) has to satisfy both (4) for all x ≤0 and it
has to be of the form (9) for all x > −k. In other words, the function in (9) must
be 0 for −k + 1 ≤x ≤0.
Theorem 29 (Exact Expected Runtime for Random Walk Programs).
Let P be a random walk program as in Deﬁnition 12 that is PAST and
let λ1, . . . , λc be the roots of its characteristic polynomial with multiplicities
v1, . . . , vc. Moreover, let C(x) = Cconst =
1
p′ if p′ > 0 and C(x) = Clin · x =
−1
μP · x if p′ = 0. Then the expected runtime of P is rtP
x = 0 for x ≤0 and
rtP
x = C(x) +

1≤j≤c, |λj|≤1

0≤u≤vj−1 aj,u · λx
j · xu
for x > 0,
where the coeﬃcients aj,u are the unique solution of the k linear equations:
0 = C(x)+

1≤j≤c, |λj|≤1

0≤u≤vj−1 aj,u·λx
j ·xu
for −k+1 ≤x ≤0 (11)
So in the special case where k = 0, we have rtP
x = C(x) = Cconst = 1
p′ for x > 0.
Thus for x > 0, the expected runtime rtP
x can be computed by summing up
the bound C(x) and an add-on 
1≤j≤c, |λj|≤1

0≤u≤vj−1 . . . Since C(x) is an
upper bound for rtP
x if p′ > 0 and a lower bound for rtP
x if p′ = 0, this add-on
is non-positive if p′ > 0 and non-negative if p′ = 0.

Computing Expected Runtimes
281
Example 30 (Example 27 cont.). By Theorem 29, the expected runtime of the
program Pmod
race from Example 23 is rtPmod
race
x
= 0 for x ≤0 and
rtPmod
race
x
=
22
3 · x + a1 + a2 · (−1
2)x
for
x > 0, cf. (10).
The coeﬃcients a1 and a2 are the unique solution of the k = 2 linear equations
0 = 22
3 · 0 + a1 + a2 · (−1
2)0 = a1 + a2
0 = 22
3 · (−1) + a1 + a2 · (−1
2)−1 = −22
3 + a1 −2 · a2
So a1 = 22
9 , a2 = −22
9 , and hence rtPmod
race
x
=
22
3 · x + 22
9 −22
9 · (−1
2)x for
x > 0.
By Theorem 15, we can lift Theorem 29 to arbitrary CP programs P imme-
diately.
Corollary 31 (Exact Expected Runtime for CP Programs). For any CP
program, its expected runtime can be computed exactly.
Note that irrespective of the degree of the characteristic polynomial, its roots
can always be approximated numerically with any chosen precision. Thus, “exact
computation” of the expected runtime in the corollary above means that a closed
form for rtP
x can also be computed with any desired precision.
Example 32 (Exact Expected Runtime of Pdirect). Recon-
sider the program Pdirect of Example 11 with the probabil-
ity p′ =
1
10 for direct termination. Pdirect is PAST and
its expected runtime is at most
1
p′ = 10, cf. Example 11.
The random walk program Prdw
direct on the right is obtained
by the transformation of Deﬁnition 13. As k = 0, by Theorem 29 we obtain
rtPrdw
direct
x
=
1
p′
= 10 for x > 0. By Theorem 15, this implies rtPdirect
(t,h)
=
rtPrdw
direct
rdw Pdirect(t,h) = 10 if rdw Pdirect(t, h) = t −h + 1 > 0, i.e., 10 is indeed the
exact expected runtime of Pdirect.
while (x > 0) {
x = x + 1
[ 9
10];
x = 0
[ 1
10];
}
Note that Theorem 29 and Corollary 31 imply that for any x0 ∈Zr, the
expected runtime rtP
x0 of a CP program P that is PAST and has only ratio-
nal probabilities pc1, . . . , pcn, p′ ∈Q is always an algebraic number. Thus, one
could also compute a closed form for the exact expected runtime rtP
x using a
representation with algebraic numbers instead of numerical approximations.
Nevertheless, Theorem 29 may yield a representation of rtP
x which contains
complex numbers aj,u and λj, although rtP
x is always real. However, one can
easily obtain a more intuitive representation of rtP
x without complex numbers:
Since the characteristic polynomial χP only has real coeﬃcients, whenever
χP has a complex root λ of multiplicity v, its conjugate λ is also a root of
χP with the same multiplicity v. So the pairwise diﬀerent roots λ1, . . . , λc can

282
J. Giesl et al.
be distinguished into pairwise diﬀerent real roots λ1, . . . , λs, and into pairwise
diﬀerent non-real complex roots λs+1, λs+1, . . . , λs+t, λs+t, where c = s + 2 · t.
For any coeﬃcients aj,u, a′
j,u ∈C with j ∈{s + 1, . . . , s + t} and u ∈
{0, . . . , vj −1} let bj,u = 2 · Re(aj,u) ∈R and b′
j,u = −2 · Im(aj,u) ∈R. Then
aj,u · λx
j + a′
j,u · λj
x = bj,u · Re(λx
j ) + b′
j,u · Im(λx
j ). Hence, by Theorem 29 we
get the following representation of the expected runtime which only uses real
numbers:
rtP
x =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
C(x) +

1≤j≤s, |λj|≤1

0≤u≤vj−1
aj,u · λx
j · xu
+

s+1≤j≤s+t, |λj|≤1

0≤u≤vj−1

bj,u·Re(λx
j ) + b′
j,u·Im(λx
j )

· xu, for x > 0
0,
for x ≤0
(12)
To compute Re(λx
j ) and Im(λx
j ), take the polar representation of the non-real
roots λj = wj · eθj·i. Then Re(λx
j ) = wx
j · cos(θj · x) and Im(λx
j ) = wx
j · sin(θj · x).
Therefore, we obtain the following algorithm to deduce the exact expected
runtime automatically.
Algorithm 33 (Computing the Exact Expected Runtime). To infer the
runtime of a CP program P as in Deﬁnition 12 that is PAST, we proceed as
follows:
1. Transform P into Prdw by the transformation of Deﬁnition 13. Thus, Prdw
is a random walk program as in Deﬁnition 12.
2. Compute the solution C(x) = Cconst = 1
p′ resp. C(x) = Clin · x = −
1
μPrdw · x
of the inhomogeneous recurrence equation (7).
3. Compute the k + m (possibly complex) roots of the characteristic polynomial
χPrdw (cf. (8)) and keep the k roots λ with |λ| ≤1.
4. Determine the coeﬃcients aj,u by solving the k linear equations in (11).
5. Return the solution (12) where bj,u = 2·Re(aj,u), b′
j,u = −2·Im(aj,u), and for
λj = wj · eθj·i we have Re(λx
j ) = wx
j · cos(θj · x) and Im(λx
j ) = wx
j · sin(θj · x).
Moreover, x must be replaced by rdw P(x).
6
Conclusion, Implementation, and Related Work
We presented decision procedures for termination and complexity of classes of
probabilistic programs. They are based on the connection between the expected
runtime of a program and the smallest solution of its corresponding recurrence
equation, cf. Sect. 2. For our notion of probabilistic programs, if the probability
for leaving the loop directly is at least p′ for some p′ > 0, then the program
is always PAST and its expected runtime is asymptotically constant, cf. Sect. 3.
In Sect. 4 we showed that a very simple decision procedure for AST and PAST
of CP programs can be obtained by classical results from random walk theory
and that the expected runtime is asymptotically linear if the program is PAST.
Based on these results, in Sect. 5 we presented our algorithm to automatically
infer a closed form for the exact expected runtime of CP programs (i.e., with
arbitrarily high precision). All proofs and a collection of examples to demonstrate
our algorithm can be found in [20].

Computing Expected Runtimes
283
Implementation. We implemented Algorithm 33 in our tool KoAT [9], which
was already one of the leading tools for complexity analysis of (non-probabilistic)
integer programs. The implementation is written in OCaml and uses the Python
libraries MpMath [22] and SymPy [29] for solving linear equations and for ﬁnd-
ing the roots of the characteristic polynomial. In addition to the closed form
for the exact expected runtime, our implementation can also compute the con-
crete number of expected loop iterations if the user speciﬁes the initial values
of the variables. For further details, a set of benchmarks, and to download our
implementation, we refer to https://aprove-developers.github.io/recurrence/.
Example 34 (Computing the Exact Expected Runtime of Prace Automatically).
For the tortoise and hare program Prace from Example 1, our implementation in
KoAT computes the following expected runtime within 0.49 s on an Intel Core
i7-6500 with 8 GB memory (when selecting a precision of 2 decimal places):
rtPrace
(t,h)
= 0.049 · 0.65(t−h+1) · sin (2.8 · (t −h + 1)) −0.35 · 0.65(t−h+1)· cos (2.8 · (t −h + 1))
+0.15 · 0.66(t−h+1)· sin (2.2 · (t −h + 1)) −0.35 · 0.66(t−h+1)· cos (2.2 · (t −h + 1))
+0.3 · 0.7(t−h+1)· sin (1.5 · (t −h + 1)) −0.39 · 0.7(t−h+1)· cos (1.5 (t −h + 1))
+0.62 · 0.75(t−h+1)· sin (0.83 · (t −h + 1)) −0.49 · 0.75(t−h+1)· cos (0.83 · (t −h + 1))
+ 2
3 · (t −h) + 2.3
So when starting in a state with t = 1000 and h = 0, according to our implemen-
tation the number of expected loop iterations is rtPrace
(1000,0) = 670.
Related Work. Many techniques to analyze (P)AST have been developed,
which mostly rely on ranking supermartingales, e.g., [1,4,10,12,13,17,19,28,30].
Indeed, several of these works (e.g., [1,4,17,19]) present complete criteria for
(P)AST, although (P)AST is undecidable. However, the corresponding automa-
tion of these techniques is of course incomplete. In [13] it is shown that for aﬃne
probabilistic programs, a superclass of our CP programs, the existence of a linear
ranking supermartingale is decidable. However, the existence of a linear ranking
supermartingale is suﬃcient but not necessary for PAST or an at most linear
expected runtime.
Classes of programs where termination is decidable have already been stud-
ied for deterministic programs. In [35] it was shown that for a class of linear
loop programs over the reals, the halting problem is decidable. This result was
transferred to the rationals [5] and under certain conditions to integer programs
[5,18,31]. Termination analysis for probabilistic programs is substantially harder
than for non-probabilistic ones [23]. Nevertheless, there is some previous work
on classes of probabilistic programs where termination is decidable and asymp-
totic bounds on the expected runtime are computable. For instance, in [6] it
was shown that AST is decidable for certain stochastic games and [11] presents
an automatic approach for inferring asymptotic upper bounds on the expected
runtime by considering uni- and bivariate recurrence equations.
However, our algorithm is the ﬁrst which computes a general formula (i.e., a
closed form) for the exact expected runtime of arbitrary CP programs. To our
knowledge, up to now such a formula was only known for the very restricted
special case of bounded simple random walks (cf. [16]), i.e., programs of the

284
J. Giesl et al.
while (b > x > 0) {
x = x + 1
[p];
x = x −1
[1 −p];
}
form on the right for some 1≥p≥0 and some b ∈Z.
Note that due to the two boundary conditions x > 0
and b > x, the resulting recurrence equation for the
expected runtime of the program only has a single
solution f : Z →R≥0 that also satisﬁes f(0) = 0
and f(b) = 0. Hence, standard techniques for solving
recurrence equations suﬃce to compute this solution. In contrast, we developed
an algorithm to compute the exact expected runtime of unbounded arbitrary CP
programs where the loop condition only has one boundary condition x > 0, i.e.,
x can grow inﬁnitely large. For that reason, here the challenge is to ﬁnd an
algorithm which computes the smallest solution f : Z →R≥0 of the resulting
recurrence equation. We showed that this can be done using the information on
the asymptotic bounds of the expected runtime from Sects. 3 and 4.
Future Work. There are several directions for future work. In Sect. 4.1 we
reduced CP programs to random walk programs. In future work, we will con-
sider more advanced reductions in order to extend the class of probabilistic
programs where termination and complexity are decidable. Moreover, we want
to develop techniques to automatically over- or under-approximate the runtime
of a program P by the runtimes of corresponding CP programs P1 and P2 such
that rtP1
x
≤rtP
x ≤rtP2
x
holds for all x ∈Zr. Furthermore, we will integrate the
easy inference of runtime bounds for CP programs into existing techniques for
analyzing more general probabilistic programs.
Acknowledgments. We would like to thank Nicos Georgiou and Vladislav Vysotskiy
for drawing our attention to Wald’s Lemma and to the work of Frank Spitzer on
random walks, and Benjamin Lucien Kaminski and Christoph Matheja for many helpful
discussions. Furthermore, we thank Tom K¨uspert who helped with the implementation
of our technique in our tool KoAT.
References
1. Agrawal, S., Chatterjee, K., Novotn´y, P.: Lexicographic ranking supermartingales:
an eﬃcient approach to termination of probabilistic programs. Proc. ACM Pro-
gram. Lang. (POPL) 2, 34:1–34:32 (2018). https://doi.org/10.1145/3158122
2. Ash,
R.B.,
Doleans-Dade,
C.A.:
Probability
and
Measure
Theory.
Else-
vier/Academic Press (2000)
3. Bazzi, L., Mitter, S.: The solution of linear probabilistic recurrence relations. Algo-
rithmica 36(1), 41–57 (2003). https://doi.org/10.1007/s00453-002-1003-4
4. Bournez, O., Garnier, F.: Proving positive almost-sure termination. In: Giesl, J.
(ed.) RTA 2005. LNCS, vol. 3467, pp. 323–337. Springer, Heidelberg (2005). https://
doi.org/10.1007/978-3-540-32033-3 24
5. Braverman, M.: Termination of integer linear programs. In: Ball, T., Jones, R.B.
(eds.) CAV 2006. LNCS, vol. 4144, pp. 372–385. Springer, Heidelberg (2006).
https://doi.org/10.1007/11817963 34
6. Br´azdil, T., Brozek, V., Etessami, K.: One-counter stochastic games. In: Lodaya,
K., Mahajan, M. (eds.) FSTTCS 2010, LIPIcs, vol. 8, pp. 108–119 (2010). https://
doi.org/10.4230/LIPIcs.FSTTCS.2010.108

Computing Expected Runtimes
285
7. Br´azdil, T., Kuˇcera, A., Novotn´y, P., Wojtczak, D.: Minimizing expected termi-
nation time in one-counter Markov decision processes. In: Czumaj, A., Mehlhorn,
K., Pitts, A., Wattenhofer, R. (eds.) ICALP 2012. LNCS, vol. 7392, pp. 141–152.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-31585-5 16
8. Br´azdil, T., Esparza, J., Kiefer, S., Kucera, A.: Analyzing probabilistic pushdown
automata. Formal Methods Syst. Des. 43(2), 124–163 (2013). https://doi.org/10.
1007/s10703-012-0166-0
9. Brockschmidt, M., Emmes, F., Falke, S., Fuhs, C., Giesl, J.: Analyzing runtime
and size complexity of integer programs. ACM Trans. Program. Lang. Syst. 38(4),
13:1–13:50 (2016). https://doi.org/10.1145/2866575
10. Chakarov, A., Sankaranarayanan, S.: Probabilistic program analysis with martin-
gales. In: Sharygina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 511–526.
Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-39799-8 34
11. Chatterjee, K., Fu, H., Murhekar, A.: Automated recurrence analysis for almost-
linear expected-runtime bounds. In: Majumdar, R., Kunˇcak, V. (eds.) CAV 2017.
LNCS, vol. 10426, pp. 118–139. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-63387-9 6
12. Chatterjee, K., Novotn´y, P., Zikelic, D.: Stochastic invariants for probabilistic ter-
mination. In: Castagna, G., Gordon, A.D. (eds.) POPL 2017, pp. 145–160 (2017).
https://doi.org/10.1145/3093333.3009873
13. Chatterjee, K., Fu, H., Novotn´y, P., Hasheminezhad, R.: Algorithmic analysis of
qualitative and quantitative termination problems for aﬃne probabilistic programs.
ACM Trans. Program. Lang. Syst. 40(2), 7:1–7:45 (2018). https://doi.org/10.1145/
3174800
14. Elaydi, S.: An Introduction to Diﬀerence Equations. Springer, New York (2005).
https://doi.org/10.1007/0-387-27602-5
15. Esparza, J., Kucera, A., Mayr, R.: Quantitative analysis of probabilistic pushdown
automata: expectations and variances. In: Panangaden, P. (ed.) LICS 2005, pp.
117–126 (2005). https://doi.org/10.1109/LICS.2005.39
16. Feller, W.: An Introduction to Probability Theory and Its Applications, Probability
and Mathematical Statistics, vol. 1. Wiley, Hoboken (1950)
17. Fioriti, L.M.F., Hermanns, H.: Probabilistic termination: soundness, completeness,
and compositionality. In: Rajamani, S.K., Walker, D. (eds.) POPL 2015, pp. 489–
501 (2015). https://doi.org/10.1145/2676726.2677001
18. Frohn, F., Giesl, J.: Termination of triangular integer loops is decidable. In: Dillig,
I., Tasiran, S. (eds.) CAV 2019. LNCS, vol. 11562, pp. 426–444. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-25543-5 24
19. Fu, H., Chatterjee, K.: Termination of nondeterministic probabilistic programs. In:
Enea, C., Piskac, R. (eds.) VMCAI 2019. LNCS, vol. 11388, pp. 468–490. Springer,
Cham (2019). https://doi.org/10.1007/978-3-030-11245-5 22
20. Giesl, J., Giesl, P., Hark, M.: Computing expected runtimes for constant probability
programs. CoRR abs/1905.09544 (2019). https://arxiv.org/abs/1905.09544
21. Grimmett, G., Stirzaker, D.: Probability and Random Processes. Oxford University
Press, Oxford (2001)
22. Johansson, F., et al.: MpMath: a Python library for arbitrary-precision ﬂoating-
point arithmetic. http://mpmath.org/
23. Kaminski, B.L., Katoen, J.: On the hardness of almost-sure termination. In: Ital-
iano, G.F., Pighizzini, G., Sannella, D. (eds.) MFCS 2015. LNCS, vol. 9234, pp.
307–318. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-48057-
1 24

286
J. Giesl et al.
24. Kaminski, B.L., Katoen, J.-P., Matheja, C., Olmedo, F.: Weakest precondition
reasoning for expected run–times of probabilistic programs. In: Thiemann, P. (ed.)
ESOP 2016. LNCS, vol. 9632, pp. 364–389. Springer, Heidelberg (2016). https://
doi.org/10.1007/978-3-662-49498-1 15
25. Karp, R.M.: Probabilistic recurrence relations. J. ACM 41(6), 1136–1150 (1994).
https://doi.org/10.1145/195613.195632
26. Kozen, D.: Semantics of probabilistic programs. In: Kosaraju, S.R. (ed.) FOCS
1979, pp. 101–114 (1979). https://doi.org/10.1109/SFCS.1979.38
27. McIver, A., Morgan, C.: Abstraction, Reﬁnement and Proof for Probabilistic Sys-
tems. Springer, New York (2005). https://doi.org/10.1007/b138392
28. McIver, A., Morgan, C., Kaminski, B.L., Katoen, J.: A new proof rule for almost-
sure termination. Proc. ACM Program. Lang. (POPL) 2, 33:1–33:28 (2018).
https://doi.org/10.1145/3158121
29. Meurer, A., et al.: SymPy: symbolic computing in Python. Peer J Comput. Sci. 3,
e103 (2017). https://doi.org/10.7717/peerj-cs.103
30. Ngo, V.C., Carbonneaux, Q., Hoﬀmann, J.: Bounded expectations: resource anal-
ysis for probabilistic programs. In: Foster, J.S., Grossman, D. (eds.) PLDI 2018,
pp. 496–512 (2018). https://doi.org/10.1145/3192366.3192394. Extended Version
available at https://arxiv.org/abs/1711.08847
31. Ouaknine, J., Pinto, J.S., Worrell, J.: On termination of integer linear loops.
In: Indyk, P. (ed.) SODA 2015, pp. 957–969 (2015). https://doi.org/10.1137/1.
9781611973730.65
32. Puterman, M.L.: Markov Decision Processes: Discrete Stochastic Dynamic Pro-
gramming. Wiley, New York (1994)
33. Spitzer, F.: Principles of Random Walk. Springer, New York (1964). https://doi.
org/10.1007/978-1-4757-4229-9
34. Tassarotti, J., Harper, R.: Veriﬁed tail bounds for randomized programs. In: Avi-
gad, J., Mahboubi, A. (eds.) ITP 2018. LNCS, vol. 10895, pp. 560–578. Springer,
Cham (2018). https://doi.org/10.1007/978-3-319-94821-8 33
35. Tiwari, A.: Termination of linear programs. In: Alur, R., Peled, D.A. (eds.) CAV
2004. LNCS, vol. 3114, pp. 70–82. Springer, Heidelberg (2004). https://doi.org/10.
1007/978-3-540-27813-9 6

Automatic Generation of Logical Models
with AGES
Ra´ul Guti´errez(B)
and Salvador Lucas
Valencian Research Institute for Artiﬁcial Intelligence (VRAIN),
Universitat Polit`ecnica de Val`encia, Valencia, Spain
{rgutierrez,slucas}@dsic.upv.es
Abstract. We describe a new tool, AGES, which can be used to auto-
matically generate models for order-sorted ﬁrst-order theories. The tool
uses linear algebra techniques to associate ﬁnite or inﬁnite domains to
the diﬀerent sorts. Function and predicate symbols are then interpreted
by means of piecewise interpretations with matrix-based expressions and
inequalities. Relations interpreting binary predicates can be speciﬁed to
be well-founded as an additional requirement for the generation of the
model. The system is available as a web application.
Keywords: Abstraction · Logical models · First-order logic ·
Program analysis · Sorts
1
Introduction
Consider an interpretation A of the function and predicate symbols occurring
in a ﬁrst-order formula ϕ. A valuation α of the free variables occurring in ϕ as
values of the domain of A satisﬁes ϕ in
A if it makes ϕ true. We say that
A is a model of ϕ if all valuations α satisfy ϕ in A . If A is a model of ϕ for
all formulas ϕ in a set S (often called a theory), we say that A is a model
of S . In program analysis and veriﬁcation, the synthesis of models for theories
representing program semantics and properties is useful for several purposes (see
[11] and the references therein for a more detailed motivation). Our tool AGES
(Automatic GEneration of logical modelS)
http://zenon.dsic.upv.es/ages/
gives support to this kind of analysis and veriﬁcation purposes. AGES implements
the methodology developed in [11] to generate a model A for a theory T of
the order-sorted, ﬁrst-order logic [8,18]. The meaning or use of such a model is
up to the user. As a running example, consider the following Maude speciﬁcation
Partially supported by the EU (FEDER), and projects RTI2018-094403-B-C32,
PROMETEO/2019/098, and SP20180225. Ra´ul Guti´errez was also supported by
INCIBE program “Ayudas para la excelencia de los equipos de investigaci´on avan-
zada en ciberseguridad”.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 287–299, 2019.
https://doi.org/10.1007/978-3-030-29436-6_17

288
R. Guti´errez and S. Lucas
(hopefully self-explained, but see [4]) of a many-sorted term rewriting system
(MS-TRS) for the usual arithmetic operations over the naturals (sort N) together
with function head, which returns the head of a list of natural numbers (sort
LN) [10, Section 5.1]:
mod ExAddMulHead is
sorts N LN .
op Z : -> N .
op suc : N -> N . ops add mul : N N -> N .
op head : LN -> N . op nil : -> LN .
op cons : N LN -> LN .
vars x y : N .
var xs : LN .
rl add(Z,x) => x .
rl add(suc(x),y) => suc(add(x,y)) .
rl mul(Z,x) => Z .
rl mul(suc(x),y) => add(y,mul(x,y)) .
rl head(cons(x,xs)) => x .
endm
We may express the claim of add being commutative as follows:
(∀x : N)(∀y : N)(∃z : N) add(x, y) →∗z ∧add(y, x) →∗z
(1)
where →∗is the many-step rewrite relation associated to the system. By [10,
Corollary 1], we can disprove (1) if we ﬁnd a structure A which is a model of
ExAddMulHead ∪HN ∪{¬(1)}
(2)
with ExAddMulHead the corresponding (many-sorted) ﬁrst-order theory in Fig. 1
and
HN = {(∀x : N) x = Z ∨x = suc(Z) ∨x = head(nil)}
(3)
a ﬁrst-order theory which guarantees that the interpretation homomorphism
hN : TΣN →AN is surjective (see [10, Sections 3 and 4] for further details). We
obtain a model A of (2) with AGES, which we describe in Sect. 3. This formally
proves that property (1) does not hold in ExAddMulHead. The main features
of AGES can be summarized as follows: (a) supports order-sorted ﬁrst-order
logic, (b) generates ﬁnite and inﬁnite, multidimensional domains, (c) supports
overloaded function and predicate symbols, including equality, (d) supports the
use of N and Z with most of the usual operations and predicates by means of
predeﬁned sorts Nat and Int, (e) generates piecewise function and predicate
symbols, and (f) supports the deﬁnition of well-founded relations (over ﬁnite or
inﬁnite domains).
Section 2 explains the input format. Section 3 brieﬂy introduces the kind of
interpretations computed by AGES. Section 4 describes the structure of the tool.
Section 5 provides experimental results showing the performance of the tool.
Section 6 concludes.

Automatic Generation of Logical Models with AGES
289
Fig. 1. Horn theory for ExAddMulHead (→and →∗are overloaded)
2
Input Format
The main idea of order-sorted ﬁrst-order logic (OS-FOL [8]) is distinguishing
diﬀerent kinds of objects by giving them a sort s from a set of sorts S which
is ordered by a subsort relation ≤. Variables have sorts and can be bound to
objects of this sort only. The arguments of function and predicate symbols have
sorts and only objects of these sorts are allowed in the arguments. The outcome
of a function also has a sort.
We use Maude [4] as an appropriate basis to specify our order-sorted ﬁrst-
order theories, although we extend Maude syntax in diﬀerent ways to ﬁt our
needs (and dismiss other Maude features which are not necessary). Our tool,
though, is not meant to deal with Maude programs. We beneﬁt from the order-
sorted features of Maude to describe the syntax of the language we use as part
of a system module S [4, Chapter 6]. The speciﬁcation module may just deﬁne
the signature (sorts, subsorts, and function and predicate symbols) with sorts
Bool, Nat and Int, integer and natural numbers, relations =, > and >=, boolean
constants true and false, and + and - operators with the standard meaning.
The speciﬁcation may also include conditional rewrite rules to deﬁne an order-
sorted conditional term rewriting system (OS-CTRS). Rules are introduced as
follows: rl l => r (or crl l => r if c for conditional rules), where l and r
are terms (the conditional part c is of the form s1 => t1 /\ ... /\ sn => tn
for terms s1,t1,...,sn,tn). Then, AGES obtains the order-sorted ﬁrst-order

290
R. Guti´errez and S. Lucas
theory S from the OS-TRS S by specializing the inference rules in Fig. 21 and
then treating inference rules B1···Bn
A
as sentences (∀x : s)B1∧· · ·∧Bn ⇒A where
x are the variables x1, . . . , xm of sorts s1, . . . , sm occurring in A, B1, . . . , Bn.
Fig. 2. Schematic inference rules for Order-Sorted CTRSs R
Example 1. Program ExAddMulHead is a valid AGES speciﬁcation as it is. The
theory which is obtained by AGES corresponds to the one displayed in Fig. 1.
The target theory T = S ∪G for AGES consists of the theory S obtained from
the speciﬁcation S together with a second component G.
Remark 1. The generation of S from S using the inference rules in Fig. 2 can
be disabled if this is not convenient for the analysis at hand. In this way, we can
use the goal part G of the target theory T to provide a full, explicit description
of the sentences to be considered in the generation process.
The set G of goals is given as a set of quantiﬁer-free formulas F1, . . . , Fk so that
G is implicitly deﬁned as follows:

1≤i≤k
(∀x1 : s1, . . . , xqi : sqi)Fi
(4)
where x1, . . . , xqi are the variables occurring in Fi (with sorts s1, . . . , sqi respec-
tively). Formulas Fi are built up using the symbols in the speciﬁcation part.
Predicates →and →∗can be used as -> and ->*, respectively. We can also use
true, false, and = (the equality predicate), conjunction
, disjunction
,
negation ~, implication =>, and equivalence <=>, with the standard meaning.
Remark 2 (Predicate symbols in Maude speciﬁcations). Since Maude does not
provide any speciﬁc means to specify predicate symbols, we assume that every
function p returning values in the predeﬁned sort Bool is a predicate symbol (see
also Sect. 3).
The sort of each variable occurrence in the formula must be explicit. Note,
however, that (4) contains universal quantiﬁers only. If existential quantiﬁers
are necessary to specify a sentence (like in (1)) we need to ﬁrst give it the
appropriate format.
1 Note that, following the semantics of oriented CTRSs [17, Section 7.1], (Rl) imple-
ments the treatment of the conditional part of rewrite rules as reachability tests
(with →∗rather than →).

Automatic Generation of Logical Models with AGES
291
Example 2. In order to ﬁnd a model for (2), we need to write ¬(1) as a valid AGES
goal (HN has the required format). By applying well-known transformations, we
obtain
(∃x : N)(∃y : N)(∀z : N) ¬(add(x, y) →∗z ∧add(y, x) →∗z)
Now we use skolemization to remove the existential quantiﬁers by introduc-
ing new constants declared by ops skX skY : -> N. (in the signature part) to
obtain
(∀z : N) ¬(add(skX, skY) →∗z ∧add(skY, skX) →∗z)
which is in the appropriate format.2 Thus, the model obtained by AGES (dis-
cussed in Sect. 3) is actually obtained for
ExAddMulHead ∪HN ∪{(∀z : N) ¬(add(skX, skY) →∗z ∧add(skY, skX) →∗z)}(5)
In AGES, we would enter module ExAddMulHead (including the declarations for
skX and skY) in the speciﬁcation part and, in the goal part:
x:N = Z \/ x:N = suc(Z) \/ x:N = head(nil)
~(add(skX,skY) ->* z:N /\ add(skY,skX) ->* z:N)
3
Interpretations Generated by AGES
In AGES, the automatic generation of models for a theory T = S ∪G pro-
ceeds bottom-up by (i) associating parametric expressions to sorts, function, and
predicate symbols in the sentences of T, (ii) combining them according to their
syntactic structure to obtain a set of conditional sentences of linear arithmetic,
(iii) transforming the (parametric) sentences into equivalent quantiﬁer-free for-
mulas which actually are constraints over the parameters, and (iv) solving the
constraints, to ﬁnally (v) give value to the initial parameters and then synthe-
size a structure which is (by construction) a model of T. For this purpose, sorts
s ∈S are given parametric matrices Cs (of ms rows and ns columns) and vectors
bs so that each sort s ∈S is interpreted as the set As of vectors x of ns inte-
ger components satisfying the linear inequality Csx ≥bs (for some appropriate
instantiation of the parameters in Cs and bs). Sorts can be given values for ms
and ns in the AGES speciﬁcation S . With
sort S [m=2 n=1].
we make explicit the default values ms = 2 and ns = 1 we use in the tool for all
sorts s without explicit values for ms and ns as above (see [11, Remark 12]).
2 These skolemization transformations are not implemented in AGES yet.

292
R. Guti´errez and S. Lucas
Example 3. In the model of (5) computed by AGES, sorts are interpreted by
AN = {−1, 0} and ALN = {0, 1}. The domains for sorts N and LN are deﬁned by
the inequalities
CN
1
CN
2

x ≥
bN
1
bN
2

and
CLN
1
CLN
2

x ≥
bLN
1
bLN
2

, where, for AN, we have
CN
1 = 1, CN
2 = −1, bN
1 = −1, and bN
2 = 0, and x ranges over Z. For ALN, CLN
1 = 1,
CLN
2
= −1, bLN
1
= 0, and bLN
2
= −1, and x ranges on Z. The interpretation of,
e.g., Nat is predeﬁned, but obtained similarly as the solution of CNatx ≥bNat
(we do not need to use two parameters in the matrices because N is known to
be inﬁnite) by just providing predeﬁned values CNat = 1 and bNat = 0 to the
parameters.
Each predicate symbol P ∈Πw with w = s1 · · · sn is interpreted by piecewise
inequalities
R(x1, . . . , xn)⇔
⎧
⎪
⎪
⎨
⎪
⎪
⎩
R1
1x1 + · · · + R1
nxn ≥R1
0
if R1
1x1 + · · · + R1
nxn ≥R1
0
...
RNP
1
x1 + · · · + RNP
n
xn ≥RNP
0
if RNP
1
x1 + · · · + RNP
n
xn ≥RNP
0
for some NP > 0, where, for all 1 ≤i ≤n and 1 ≤j ≤NP , Rj
i, Rj
i, Rj
i, and Rj
1
are mP × nsi-matrices, Rj
0 and Rj
0 are vectors of mP rows for some mP ∈N. In
AGES, predicates P ∈Πs1···sn can be given values for mP and NP as follows:
op P : S1 ... Sn -> Bool [m=1 N=1].
which actually shows the default values mP = 1 and NP = 1 we use for all
predicates (but see [11, Example 13]). In this case, the generic expression above
boils down into:
R(x1, . . . , xn) ⇔R1x1 + · · · + Rnxn ≥R0
(6)
where Ri is a 2mP ×nsi-matrix, for 1 ≤i ≤n, and R0 is a vector [11, Remark14].
We can also use (6) directly, with matrices Ri of m rows for some m > 0.
Example 4. In the model of (5) computed by AGES, predicates are interpreted
as follows: x →A
N
y ⇔x(→∗
N)A y ⇔x = y, x →A
LN y ⇔x = y, and x(→∗
LN)A y
is true. These are more readable formulations of the AGES output (after some
variable renaming):
x:N -> y:N <=> ((y:N >= x:N) /\ (x:N >= y:N))
x:N ->* y:N <=> ((y:N >= x:N) /\ (x:N >= y:N))
x:LN -> y:LN <=> ((x:LN >= y:LN) /\ (y:LN >= x:LN))
x:LN ->* y:LN <=> (3 + x:LN + y:LN >= 0)
For instance, the interpretation of →N (ﬁrst line), in matrix format (6), corre-
sponds to
x →A
N y ⇔

−1
1

x +

1
−1

y ≥

0
0


Automatic Generation of Logical Models with AGES
293
which is better understood as x = y. Note that since 3 + x:LN + y:LN >= 0
holds for every value x and y in ALN = {0, 1}, x(→∗
LN)A y is equivalent to true.
Unfortunately, these kind of ‘simpliﬁcations’ are not implemented in AGES yet.
However, we do not display repeated members in a conjunction of inequalities.
This explains that the last line shows a single inequality instead of two.
A binary relation R on a set A is well-founded if there is no inﬁnite sequence
a1, a2, . . . such that for all i ≥1, ai ∈A and aiRai+1. We can require that relation
P A
ss interpreting a binary predicate Pss be well-founded [11, Section 8.3.1]:
op P : S S -> Bool [wellfounded].
Each function symbol f : s1 · · · sk →s is interpreted as follows (for some Nf >
0):
F (x1, . . . , xk)=
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
F 1
1 x1 + · · · + F 1
k xk + F 1
0
if 
F 1
1 x1 + · · · + 
F 1
k xk ≥
F 1
0
...
F
Nf −1
1
x1 + · · · + F
Nf −1
k
xk + F
Nf −1
0
if 
F
Nf −1
1
x1 + · · · + 
F
Nf −1
k
xk ≥
F
Nf −1
0
F
Nf
1
x1 + · · · + F
Nf
k
xk + F
Nf
0
otherwise
where
the
otherwise
in
the
last
option
guarantees
that
F
is
total
[11, Remark 9]. The default option is Nf
=
1 (signature declaration
op f : S1 ... Sk -> S [N=1].), i.e., no pieces. Then, the generic expression
above boils down into a linear expression:
F(x1, . . . , xk) = F1x1 + · · · + Fkxk + F0
(7)
where Fi is an ns × nsi-matrix, 1 ≤i ≤k, and F0 a vector of ns rows.
Example 5. In our model of (5), function symbols are interpreted as follows:
ZA = −1
nilA = 0
sucA (x) = x
addA (x, y) = y
mulA (x, y) = x
consA (x, xs) = −x
headA (xs) = −xs
where x and y range on AN and xs on ALN. The AGES output is as follows:
|[add(x_1_1:N,x_2_1:N)]| = x_2_1:N
|[head(x_1_1:LN)]| =
- x_1_1:LN
|[mul(x_1_1:N,x_2_1:N)]| = x_1_1:N
|[Z]| =
- 1
|[cons(x_1_1:N,x_2_1:LN)]| =
- x_1_1:N
|[nil]| = 0
|[suc(x_1_1:N)]| = x_1_1:N

294
R. Guti´errez and S. Lucas
Skolem symbols are interpreted as follows: skXA
= 0 and skYA
= −1.
By [10, Section 6], we could use this information to conclude that terms
add(head(nil), Z) and add(Z, head(nil)) explain why property (1) fails to hold
(but AGES provides no support for such kind of issues yet).
4
User Interface
AGES is a web based application written in Haskell in 13705 lines of code. In order
to use this application, the end user only needs an HTML5 compatible browser.
The main web page is divided in three areas: the input area, the conﬁguration
area and the information area. In the input area, two ﬁelds must be ﬁlled before
generating a model:
– Speciﬁcation: This ﬁeld accepts the extended Maude speciﬁcation as described
in Sect. 2. It can be pasted from a text ﬁle, uploaded using the Browse button,
or written in the text box.
– Goal: This ﬁeld allows to add a goal to the model according to the format in
Sect. 2. The goal can be a single formula or a list of formulas separated by
newlines. If we do not want to add a goal, we can just write true in the ﬁeld.
The conﬁguration area permits to parameterize the search of the solution:
– Predicates: The user can select the interpretation of the relation →and →∗
to well-known relations (i.e. >, ≥or =) or allow the solver to search for it
automatically.
– Inference Rules: The user can deactivate some of the inference rules of the
OS-TRS logic (Fig. 2) in order to redeﬁne them and obtain a new logic.
– Convex Polytopic Domain: permits the deﬁnition of common values ms and
ns of Cs and bs for all sorts s. The values are overridden by any speciﬁc value
given in the speciﬁcation (see Sect. 3).
– Timeout: for the SMT-based constraint solving. Maximum timeout is 300 s.
– SMT Solver: AGES uses external SMT solvers to ﬁnd the model that satis-
ﬁes the generated constraints in SMT-LIB format3 in the QF_NIA domain.
Currently, we can choose between SMT solvers Barcelogic [3], Yices [5] and
Z3 [16].
All conﬁguration ﬁelds have default values. The information area explains how to
use the extended Maude conﬁguration parameters described in Sect. 2. Pressing
the Generate button starts the model generation process depicted in Fig. 3.
3 http://smtlib.cs.uiowa.edu/.

Automatic Generation of Logical Models with AGES
295
Maude Speciﬁcation
+
Set of formulas
Signature
Rules (optional)
Property
First-Order Logic Formulae
Constraints
Sort Domains
Signature Interpretation
Constraint Generator
SMT solver
SMT-LIB format
SMT Solver
Model
Model Generation
Fig. 3. AGES Workﬂow
5
Experimental Results
In order to evaluate the performance of AGES with respect to related tools (a
nonexhaustive list includes FALCON [19], Mace4 [15], SEM [20], and the model
generation subsystems of the theorem provers Alt-Ergo, PDL-tableau, Princess,
or Vampire) we have compared AGES and Mace4, which is a good representative
of what other similar model generation tools do. Mace4 generates (one-sorted)
ﬁnite models only. Actually, this is not rare: to the best of our knowledge, all
aforementioned systems generate models with ﬁnite domains only. Thus, as far
as we known, AGES is the only tool which is able to generate inﬁnite models for
ﬁrst-order theories. Mace4 is a very fast and easy to use tool. It is available as a
desktop application in several platforms.
AGES and Mace4 are based on rather diﬀerent ideas. Zhang [19] provides a
comprehensive description of how Mace4-like tools work and [11] describes the
approach implemented by AGES. Rather than directly comparing them as model
generators, we compare them as auxiliary tools for other purposes. We have pre-
pared three benchmarks suites which can be useful to compare both tools and
evaluate their capabilities. The details of our benchmarks are available here:
http://zenon.dsic.upv.es/ages/benchmarks/cade27/
In the following, we explain each of them and comment on their results.
Proving Infeasibility.
Given a (C)TRS R we say that a sequence s1 →∗
t1, . . . , sn
→∗
tn is R-infeasible if there is no substitution σ such that
σ(si) →∗
R σ(ti) holds for all 1 ≤i ≤n. In [12] it is proved that a sequence

296
R. Guti´errez and S. Lucas
s1 →∗t1, . . . , sn →∗tn is R-infeasible if there is a model of R ∪{¬(∃x) s1 →∗
t1 ∧· · · ∧sn →∗tn}, where R is the ﬁrst-order theory associated to R (like the
theory in Fig. 1 for ExAddMulHead, see [12, Section 3], for more details) and x
refers all variables in s1, t1, . . . , sn, tn. Infeasibility is useful in proofs of conﬂu-
ence and termination of CTRSs (see [12] and references therein).
We implemented a comparison of AGES and Mace4 when used to solve infea-
sibility problems by using the aforementioned satisﬁability approach. We have
considered 129 examples of infeasibility problems from the COPS database.4 On
this test suite,
AGES was able to prove infeasibility of 49 examples and Mace4 proved
infeasibility of 52 examples, including all cases were AGES succeeded.
Proving Termination of TRSs. Modern termination tools implement proofs of ter-
mination of Term Rewriting Systems (TRSs) R by using the DP Framework [7]. In
the DP Framework, proofs of termination proceed by transforming DP problems.
A proof of termination starts with an initial DP Problem (DP(R), R) whose ﬁrst
component DP(R) consists of all the dependency pairs of R. Then, a divide-and-
conquer approach is applied by means of processors Proc mapping a DP problem
τ into a (possibly empty) set Proc(τ) of DP problems {τ1, . . . , τn} (alternatively,
they can return “no”). DP problems τi returned by Proc can now be treated inde-
pendently by using other processors. In this way, a DP proof tree is built. One of the
most important processors is the reduction pair processor ProcRP. With ProcRP
we can remove a dependency pair u →v if we ﬁnd a well-founded relation ⊐such
that u ⊐v holds (among some additional conditions). Such well-founded relations
can be generated by means of a model A of R which also satisﬁes (∀x) uπ⊐v for
a new predicate symbol π⊐representing the relation ⊐. If πA
⊐is guaranteed to be
well-founded, then we can remove u →v from the current DP problem. In this
way we introduce a simpliﬁcation in the termination proof.
We have implemented (as part of our termination tool mu-term 6.0 [1]5)
the use of ProcRP by using AGES and Mace4 to generate the required mod-
els and well-founded relations. As mentioned in Sect. 3, AGES provides explicit
support for well-foundedness through the modiﬁer wellfounded that can be
speciﬁed in the predicate declaration. Mace4 does not provide any support for
well-foundedness. However, we can use the fact that a ﬁnite relation R on a set
A is well-founded iﬀR is not cyclic, i.e., there is no a ∈A such that a R+ a.
As in [9], we instruct Mace4 to obtain a well-founded interpretation for π⊐by
adding the following sentences:
(∀x)(∀y) x π⊐y ⇒x π+
⊐y
(8)
(∀x)(∀y)(∀z) x π⊐y ∧y π+
⊐z ⇒x π+
⊐z
(9)
¬(∃x) x π+
⊐x
(10)
where π+
⊐is a new binary predicate symbol. We have used the Termination Prob-
lems Data Base (TPDB version 10.6) containing examples of TRS termination
problems. On the 1498 examples of this test suite,
4 http://project-coco.uibk.ac.at/2019/categories/infeasibility.php.
5 Available at http://zenon.dsic.upv.es/muterm/.

Automatic Generation of Logical Models with AGES
297
mu-term 6.0+AGES was able to prove termination of 336 examples and
mu-term 6.0+Mace4 proved termination of 109 examples.
The ability of AGES to generate models with inﬁnite domains and diﬀerent inter-
pretations for sorts6 could be essential for these good results. It is also interesting
that Mace4 was able to prove termination of a few examples where AGES could
not be used and that all the examples proved by AGES using Z3 and Yices are
also proved by AGES using Barcelogic.
Proving Operational Termination of CTRSs. When conditional rewrite rules
ℓ→r ⇐s1 ≈t1, . . . , sn ≈tn are considered, besides the usual notion of termi-
nation as the absence of inﬁnite sequences of reductions, a new source of non-
termination arises: the possibility of ‘getting lost’ when trying to prove whether
a single reduction step is possible [13,14]. In this setting, describing rewriting
computations as proofs of goals s →t and s →∗t with respect to an appro-
priate inference system is useful to approach the termination behavior of such
systems as the absence of inﬁnite proof trees [13]. In [14] appropriate notions of
dependency pairs were introduced to capture operational termination (OT) of
oriented7 CTRSs. Given a CTRS R, two new CTRSs DPH (R) and DPV (R)
are introduced to capture the two horizontal and vertical dimensions of opera-
tional termination of CTRSs [14, Section 3]: the usual absence of inﬁnite rewrite
sequences (termination), and the absence of inﬁnite ‘climbings’ on a proof tree
when trying to prove a goal s →t or s →∗t (called V -termination [14, Deﬁni-
tion 13]). Proofs of operational termination in the 2D DP Framework essentially
proceed like in the DP Framework for TRSs. Thus, we have prepared mu-term
6.0 to work with AGES and Mace4 when trying to prove operational termination
of CTRSs (again on the TPDB). On this test suite, over 121 examples,
mu-term 6.0+AGES proved OT of 85 examples and disproved OT of 17.
mu-term 6.0+Mace4 proved OT of 79 examples and disproved OT of 5.
Summary. In the ﬁrst benchmark suite Mace4 slightly outperforms AGES. A pos-
sible explanation is that models with ﬁnite domains are usually well-suited to
solve infeasibility problems. Mace4 computes fully general, extensional represen-
tations of functions and predicates, as sets of equations and atoms, respectively.
AGES can simulate them using piecewise interpretations, but less eﬃciently,
often running out of the timeout. The second and third benchmark suites show
that mu-term+Mace4 proved termination of 23% of the TRSs handled by using
AGES; in contrast, mu-term+Mace4 was able to (dis)prove operational ter-
mination of 80% of the CTRSs solved by using AGES. The ﬁrst benchmark
suite suggests a possible explanation. As remarked in [12], infeasibility (of con-
ditional dependency pairs, of links between nodes in the dependency graph, etc.)
6 Sorts can be used to treat dependency pairs [6, Sect. 5]; this is implemented in
mu-term 6.0.
7 Oriented CTRSs treat conditions si ≈ti in rules ℓ→r ⇐s1 ≈t1, . . . , sn ≈tn as
rewriting goals σ(si) →∗σ(ti) for appropriate substitutions σ.

298
R. Guti´errez and S. Lucas
is important in proofs of operational termination of CTRSs. Infeasibility is tested
in mu-term 6.0 as part of the proofs. Many proofs of operational termination
ﬁnish with no use of well-founded relations. Thus, it is not so strange that Mace4
obtains better results when used in proofs of operational termination of CTRSs.
Overall, the conclusion is that AGES is similar to Mace4 in proofs of infeasibility,
whereas it is much better in the generation of well-founded models.
6
Conclusions
The main motivation to develop AGES was providing an appropriate frame-
work to explore the applicability of the results in [11]: the generation of convex
domains to interpret sorts of order-sorted signatures and the interpretation of
function and predicate symbols by means of piecewise matrix interpretations
which, for binary relations interpreting binary predicates, can be required to be
well-founded. We have shown that this approach is applicable in practice, and
can be competitive, as shown by our experiments (Sect. 5) and also in the 2017,
2018, and 2019 termination competitions,8 where AGES was integrated as a back-
end of mu-term in the conditional TRS category, being the most successful tool
for proving operational termination of CTRSs. Regarding future work, we plan
to improve on eﬃciency, applications and extensions. With regard to eﬃciency,
the generation of matrix-based models could beneﬁt from the ideas in [2]. We will
also improve our treatment of piecewise interpretations regarding the generation
of the CNF formulae submitted to the backend solvers. With regard to applica-
tions, a deeper integration of AGES into mu-term would be necessary to test its
impact in other TRS categories. We will also consider its integration/cooperation
with other conﬂuence and veriﬁcation tools. Finally, with respect to extensions,
we plan to include equational components in our input signature and give sup-
port to other input formats for rewriting-based systems.
Acknowledgments. We thank the anonymous referees for their comments and sug-
gestions. We thank Patricio Reinoso for his work in the initial development of the tool.
References
1. Alarc´on, B., Guti´errez, R., Lucas, S., Navarro-Marset, R.: Proving termination
properties with mu-term. In: Johnson, M., Pavlovic, D. (eds.) AMAST 2010.
LNCS, vol. 6486, pp. 201–208. Springer, Heidelberg (2011). https://doi.org/10.
1007/978-3-642-17796-5 12
2. Bau, A., Lohrey, M., N¨oth, E., Waldmann, J.: Compression of rewriting systems
for termination analysis. In: van Raamsdonk, F. (ed.) Proceedings of the 24th
International Conference on Rewriting Techniques and Applications, RTA 2013,
LIPICS, vol. 21, pp. 97–112 (2013). https://doi.org/10.4230/LIPIcs.RTA.2013.97
8 http://termination-portal.org/wiki/Termination Competition.

Automatic Generation of Logical Models with AGES
299
3. Boﬁll, M., Nieuwenhuis, R., Oliveras, A., Rodr´ıguez-Carbonell, E., Rubio, A.: The
barcelogic SMT solver. In: Gupta, A., Malik, S. (eds.) CAV 2008. LNCS, vol.
5123, pp. 294–298. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-
540-70545-1 27
4. Clavel, M., et al.: All About Maude - A High-Performance Logical Framework.
LNCS, vol. 4350. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-
71999-1
5. Dutertre, B.: Yices 2.2. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559,
pp. 737–744. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08867-
9 49
6. Endrullis, J., Waldmann, J., Zantema, H.: Matrix interpretations for proving termi-
nation of term rewriting. J. Autom. Reasoning 40(2–3), 195–220 (2008). https://
doi.org/10.1007/s10817-007-9087-9
7. Giesl, J., Thiemann, R., Schneider-Kamp, P., Falke, S.: Mechanizing and improving
dependency pairs. J. Autom. Reasoning 37(3), 155–203 (2006). https://doi.org/
10.1007/s10817-006-9057-7
8. Goguen, J.A., Meseguer, J.: Models and equality for logical programming. In:
Ehrig, H., Kowalski, R., Levi, G., Montanari, U. (eds.) TAPSOFT 1987. LNCS, vol.
250, pp. 1–22. Springer, Heidelberg (1987). https://doi.org/10.1007/BFb0014969
9. Lucas, S.: Using Well-Founded Relations for Proving Operational Termination. J.
Autom. Reasoning (2019). https://doi.org/10.1007/s10817-019-09514-2
10. Lucas, S.: Proving program properties as ﬁrst-order satisﬁability. In: Mesnard, F.,
Stuckey, P.J. (eds.) LOPSTR 2018. LNCS, vol. 11408, pp. 3–21. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-13838-7 1
11. Lucas, S., Guti´errez, R.: Automatic synthesis of logical models for order-sorted
ﬁrst-order theories. J. Autom. Reasoning 60(4), 465–501 (2018). https://doi.org/
10.1007/s10817-017-9419-3
12. Lucas, S., Guti´errez, R.: Use of logical models for proving infeasibility in term
rewriting. Inf. Process. Lett. 136, 90–95 (2018). https://doi.org/10.1016/j.ipl.2018.
04.002
13. Lucas, S., March´e, C., Meseguer, J.: Operational termination of conditional term
rewriting systems. Inf. Process. Lett. 95, 446–453 (2005). https://doi.org/10.1016/
j.ipl.2005.05.002
14. Lucas, S., Meseguer, J.: Dependency pairs for proving termination properties of
conditional term rewriting systems. J. Logical Algebraic Methods Program. 86,
236–268 (2017). https://doi.org/10.1016/j.jlamp.2016.03.003
15. McCune, W.: Prover9 and Mace4 (2005–2010). http://www.cs.unm.edu/∼mccune/
prover9/
16. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrishnan, C.R.,
Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 337–340. Springer, Heidelberg
(2008). https://doi.org/10.1007/978-3-540-78800-3 24
17. Ohlebusch, E.: Advanced Topics in Term Rewriting. Springer, New York (2002).
https://doi.org/10.1007/978-1-4757-3661-8
18. Wang, H.: Logic of many-sorted theories. J. Symbolic Logic 17(2), 105–116 (1952).
https://doi.org/10.2307/2266241
19. Zhang, J.: Constructing ﬁnite algebras with FALCON. J. Autom. Reasoning 17,
1–22 (1996). https://doi.org/10.1007/BF00247667
20. Zhang, J., Zhang, H.: System description generating models by SEM. In: McRob-
bie, M.A., Slaney, J.K. (eds.) CADE 1996. LNCS, vol. 1104, pp. 308–312. Springer,
Heidelberg (1996). https://doi.org/10.1007/3-540-61511-3 96

Automata Terms in a Lazy WSkS
Decision Procedure
Vojtˇech Havlena, Luk´aˇs Hol´ık, Ondˇrej Leng´al(B), and Tom´aˇs Vojnar
FIT, IT4I Centre of Excellence,
Brno University of Technology, Brno, Czech Republic
lengal@fit.vutbr.cz
Abstract. We propose a lazy decision procedure for the logic WSkS.
It builds a term-based symbolic representation of the state space of the
tree automaton (TA) constructed by the classical WSkS decision proce-
dure. The classical decision procedure transforms the symbolic represen-
tation into a TA via a bottom-up traversal and then tests its language
non-emptiness, which corresponds to satisﬁability of the formula. On
the other hand, we start evaluating the representation from the top,
construct the state space on the ﬂy, and utilize opportunities to prune
away parts of the state space irrelevant to the language emptiness test.
In order to do so, we needed to extend the notion of language terms
(denoting language derivatives) used in our previous procedure for the
linear fragment of the logic (the so-called WS1S) into automata terms.
We implemented our decision procedure and identiﬁed classes of formu-
lae on which our prototype implementation is signiﬁcantly faster than
the classical procedure implemented in the Mona tool.
1
Introduction
Weak monadic second-order logic of k successors (WSkS) is a logic for describing
regular properties of ﬁnite k-ary trees. In addition to talking about trees, WSkS
can also encode complex properties of a rich class of general graphs by referring
to their tree backbones [1]. WSkS oﬀers extreme succinctness for the price of non-
elementary worst-case complexity. As noticed ﬁrst by the authors of [2] in the
context of WS1S (a restriction that speaks about ﬁnite words only), the trade-
oﬀbetween complexity and succinctness may, however, be turned signiﬁcantly
favourable in many practical cases through a use of clever implementation tech-
niques and heuristics. Such techniques were then elaborated in the tool Mona
[3,4], the best-known implementation of decision procedures for WS1S and WS2S.
Mona has found numerous applications in veriﬁcation of programs with com-
plex dynamic linked data structures [1,5–8], string programs [9], array programs
[10], parametric systems [11–13], distributed systems [14,15], hardware veriﬁca-
tion [16], automated synthesis [17–19], and even computational linguistics [20].
Despite the extensive research and engineering eﬀort invested into Mona,
due to which it still oﬀers the best all-around performance among existing
WS1S/WS2S decision procedures, it is, however, easy to reach its scalability
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 300–318, 2019.
https://doi.org/10.1007/978-3-030-29436-6_18

Automata Terms in a Lazy WSkS Decision Procedure
301
limits. Particularly, Mona implements the classical WS1S/WS2S decision proce-
dures that build a word/tree automaton representing models of the given formula
and then check emptiness of the automaton’s language. The non-elementary com-
plexity manifests in that the size of the automaton is prone to explode, which
is caused mainly by the repeated determinisation (needed to handle negation
and alternation of quantiﬁers) and synchronous product construction (used to
handle conjunctions and disjunctions). Users of WSkS are then forced to either
ﬁnd workarounds, such as in [6], or, often restricting the input of their approach,
give up using WSkS altogether [21].
As in Mona, we further consider WS2S only (this does not change the expres-
sive power of the logic since k-ary trees can be easily encoded into binary ones).
We revisit the use of tree automata (TAs) in the WS2S decision procedure and
obtain a new decision procedure that is much more eﬃcient in certain cases. It is
inspired by works on antichain algorithms for eﬃcient testing of universality and
language inclusion of ﬁnite automata [22–25], which implement the operations
of testing emptiness of a complement (universality) or emptiness of a product
of one automaton with the complement of the other one (language inclusion)
via an on-the-ﬂy determinisation and product construction. The on-the-ﬂy app-
roach allows one to achieve signiﬁcant savings by pruning the state space that
is irrelevant for the language emptiness test. The pruning is achieved by early
termination when detecting non-emptiness (which represents a simple form of
lazy evaluation), and subsumption (which basically allows one to disregard proof
obligations that are implied by other ones). Antichain algorithms and their gen-
eralizations have shown great eﬃciency improvements in applications such as
abstract regular model checking [24], shape analysis [26], LTL model checking
[27], or game solving [28].
Our work generalizes the above mentioned approaches of on-the-ﬂy automata
construction, subsumption, and lazy evaluation for the needs of deciding WS2S.
In our procedure, the TAs that are constructed explicitly by the classical pro-
cedure are represented symbolically by the so-called automata terms. More pre-
cisely, we build automata terms for subformulae that start with a quantiﬁer (and
for the top-level formula) only—unlike the classical procedure, which builds a TA
for every subformula. Intuitively, automata terms specify the set of leaf states of
the TAs of the appropriate (sub)formulae. The leaf states themselves are then
represented by state terms, whose structure records the automata constructions
(corresponding to Boolean operations and quantiﬁcation on the formula level)
used to create the given TAs from base TAs corresponding to atomic formulae.
The leaves of the terms correspond to states of the base automata. Automata
terms may be used as state terms over which further automata terms of an even
higher level are built. Non-leaf states, the transition relation, and root states are
then given implicitly by the transition relations of the base automata and the
structure of the state terms.
Our approach is a generalization of our earlier work [29] on WS1S. Although
the term structure and the generalized algorithm may seem close to [29], the
reasoning behind it is signiﬁcantly more involved. Particularly, [29] is based on

302
V. Havlena et al.
deﬁning the semantics (language) of terms as a function of the semantics of
their sub-terms. For instance, the semantics of the term {q1, . . . , qn} is deﬁned
as the union of languages of the state terms q1, . . . , qn, where the language of a
state of the base automaton consists of the words accepted at that state. With
TAs, it is, however, not meaningful to talk about trees accepted from a leaf
state, instead, we need to talk about a given state and its context, i.e., other
states that could be obtained via a bottom-up traversal over the given set of
symbols. Indeed, trees have multiple leafs, which may be accepted by a number
of diﬀerent states, and so a tree is accepted from a set of states, not from any
single one of them alone. We therefore cannot deﬁne the semantics of a state
term as a tree language, and so we cannot deﬁne the semantics of an automata
term as the union of the languages of its state sub-terms. This problem seems
critical at ﬁrst because without a sensible notion of the meaning of terms, a
straightforward generalization of the algorithm of [29] to trees does not seem
possible. The solution we present here is based on deﬁning the semantics of
terms via the automata constructions they represent rather then as functions of
languages of their sub-terms.
Unlike the classical decision procedure, which builds a TA corresponding to
a formula bottom-up, i.e. from the atomic formulae, we build automata terms
top-down, i.e., from the top-level formula. This approach oﬀers a lot of space
for various optimisations. Most importantly, we test non-emptiness of the terms
on the ﬂy during their construction and construct the terms lazily. In particu-
lar, we use short-circuiting for dealing with the ∧and ∨connectives and early
termination with possible continuation when implementing the ﬁxpoint compu-
tations needed when dealing with quantiﬁers. That is, we terminate the ﬁxpoint
computation whenever the emptiness can be decided in the given computation
context and continue with the computation when such a need appears once the
context is changed on some higher-term level. Further, we deﬁne a notion of sub-
sumption of terms, which, intuitively, compares the terms w.r.t the sets of trees
they represent, and allows us to discard terms that are subsumed by others.
We have implemented our approach in a prototype tool. When experiment-
ing with it, we have identiﬁed multiple parametric families of WS2S formulae
where our implementation can—despite its prototypical form—signiﬁcantly out-
perform Mona. We ﬁnd this encouraging since there is a lot of space for further
optimisations and, moreover, our implementation can be easily combined with
Mona by treating automata constructed by Mona in the same way as if they
were obtained from atomic predicates.
An extended version of this paper including proofs is available as [30].
2
Preliminaries
In this section, we introduce basic notation, trees, and tree automata, and give
a quick introduction to the weak monadic second-order logic of two successors
(WS2S) and its classical decision procedure. We give the minimal syntax of
WS2S only; see, e.g., Comon et al. [31] for more details.

Automata Terms in a Lazy WSkS Decision Procedure
303
Basics, Trees, and Tree Automata. Let Σ be a ﬁnite set of symbols, called
an alphabet. The set Σ∗of words over Σ consists of ﬁnite sequences of symbols
from Σ. The empty word is denoted by ϵ, with ϵ ̸∈Σ. The concatenation of two
words u and v is denoted by u.v or simply uv. The domain of a partial function
f : X →Y is the set dom(f) = {x ∈X | ∃y : x →y ∈f}, its image is the
set img(f) = {y ∈Y | ∃x : x →y ∈f}, and its restriction to a set Z is the
function f|Z = f ∩(Z × Y ). For a binary operator •, we write A [•] B to denote
the augmented product {a • b | (a, b) ∈A × B} of A and B.
We will consider ordered binary trees. We call a word p ∈{L, R}∗a tree
position and p.L and p.R its left and right child, respectively. Given an alphabet Σ
s.t. ⊥/∈Σ, a tree over Σ is a ﬁnite partial function τ : {L, R}∗→(Σ ∪{⊥})
such that (i) dom(τ) is non-empty and preﬁx-closed, and (ii) for all positions
p ∈dom(t), either τ(p) ∈Σ and p has both children, or τ(p) = ⊥and p has no
children, in which case it is called a leaf. We let leaf (τ) be the set of all leaves
of τ. The position ϵ is called the root, and we write Σ
to denote the set of all
trees over Σ1.We abbreviate {a}
as a
for a ∈Σ.
The sub-tree of τ rooted at a position p ∈dom(τ) is the tree τ ′ = {p′ →
τ(p.p′) | p.p′ ∈dom(τ)}. A preﬁx of τ is a tree τ ′ such that τ ′
|dom(τ ′)\leaf (τ ′) ⊆
τ|dom(τ)\leaf (τ). The derivative of a tree τ wrt a set of trees S ⊆Σ
is the set
τ −S of all preﬁxes τ ′ of τ such that, for each position p ∈leaf (τ ′), the sub-tree
of τ at p either belongs to S or it is a leaf of τ. Intuitively, τ −S are all preﬁxes
of τ obtained from τ by removing some of the sub-trees in S. The derivative of
a set of trees T ⊆Σ
wrt S is the set 
τ∈T (τ −S).
A (binary) tree automaton (TA) over an alphabet Σ is a quadruple A =
(Q, δ, I, R) where Q is a ﬁnite set of states, δ : Q2 × Σ →2Q is a transition
function, I ⊆Q is a set of leaf states, and R ⊆Q is a set of root states. We use
(q, r)−{a}→s to denote that s ∈δ((q, r), a). A run of A on a tree τ is a total map
ρ : dom(τ) →Q such that if τ(p) = ⊥, then ρ(p) ∈I, else (ρ(p.L), ρ(p.R))−{a}→ρ(p)
with a = τ(p). The run ρ is accepting if ρ(ϵ) ∈R, and the language L (A) of A
is the set of all trees on which A has an accepting run. A is deterministic
if |I| = 1 and ∀q, r ∈Q, a ∈Σ : |δ((q, r), a)| ≤1, and complete if I ≥1 and
∀q, r ∈Q, a ∈Σ : |δ((q, r), a)| ≥1. Last, for a ∈Σ, we shorten δ((q, r), a) as
δa(q, r), and we use δΓ(q, r) to denote {δa(q, r) | a ∈Γ} for a set Γ ⊆Σ.
Syntax and Semantics of WS2S. WS2S is a logic that allows quantiﬁcation
over second-order variables, which are denoted by upper-case letters X, Y, . . .
and range over ﬁnite sets of tree positions in {L, R}∗(the ﬁniteness of variable
assignments is reﬂected in the name weak). See Fig. 1a for an example of a set
of positions assigned to a variable. Atomic formulae (atoms) of WS2S are of the
form: (i) X ⊆Y , (ii) X = SL(Y ), and (iii) X = SR(Y ). Formulae are constructed
from atoms using the logical connectives ∧, ¬, and the quantiﬁer ∃X where X
1 Intuitively, the [·]
operator can be seen as a generalization of the Kleene star to
tree languages. The symbol
is the Chinese character for a tree, pronounced m`u,
as in English moo-n, but shorter and with a falling tone, staccato-like.

304
V. Havlena et al.
Fig. 1. An example of an assignment ν to a pair of variables {X, Y } s.t. ν(X) =
{LR, R, RLR, RR} and ν(Y ) = {ϵ, L, LL, R, RR} and its encoding into a tree.
is a ﬁnite set of variables (we write ∃X when X is a singleton set {X}). Other
connectives (such as ∨or ∀) and predicates (such as the predicate Sing(X) for
a singleton set X) can be obtained as syntactic sugar.
A model of a WS2S formula ϕ(X) with the set of free variables X is an
assignment ν : X →2{L,R}∗of the free variables of ϕ to ﬁnite subsets of {L, R}∗
for which the formula is satisﬁed, written ν |= ϕ. Satisfaction of atomic formulae
is deﬁned as follows: (i) ν |= X ⊆Y iﬀν(X) ⊆ν(Y ), (ii) ν |= X = SL(Y ) iﬀ
ν(X) = {p.L | p ∈ν(Y )}, and (iii) ν |= X = SR(Y ) iﬀν(X) = {p.R | p ∈ν(Y )}.
Informally, the SL(Y ) function returns all positions from Y shifted to their left
child and the SR(Y ) function returns all positions from Y shifted to their right
child. Satisfaction of formulae built using Boolean connectives and the quantiﬁer
is deﬁned as usual. A formula ϕ is valid, written |= ϕ, iﬀall assignments of its
free variables are its models, and satisﬁable if it has a model. Wlog, we assume
that each variable in a formula either has only free occurrences or is quantiﬁed
exactly once; we denote the set of (free and quantiﬁed) variables occurring in
a formula ϕ as Vars(ϕ).
Representing Models as Trees. We ﬁx a formula ϕ with variables Vars(ϕ) =
X. A symbol ξ over X is a (total) function ξ : X →{0, 1}, e.g., ξ = {X →
0, Y →1} is a symbol over X = {X, Y }. We use ΣX to denote the set of all
symbols over X and ⃗0 to denote the symbol mapping all variables in X to 0, i.e.,
⃗0 = {X →0 | X ∈X}.
A ﬁnite assignment ν : X →2{L,R}∗of ϕ’s variables can be encoded as
a ﬁnite tree τν of symbols over X where every position p ∈{L, R}∗satisﬁes the
following conditions: (a) if p ∈ν(X), then τν(p) contains {X →1}, and (b)
if p /∈ν(X), then either τν(p) contains {X →0} or τν(p) = ⊥(note that the
occurrences of ⊥in τ are limited since τ still needs to be a tree). Observe that ν
can have multiple encodings: the unique minimum one τ min
ν
and (inﬁnitely many)

Automata Terms in a Lazy WSkS Decision Procedure
305
extensions of τ min
ν
with ⃗0-only trees. The language of ϕ is deﬁned as the set of all
encodings of its models L (ϕ) = {τν ∈ΣX | ν |= ϕand τν is an encoding of ν}.
Let ξ be a symbol over X. For a set of variables Y ⊆X, we deﬁne the projection
of ξ wrt Y as the set of symbols πY(ξ) = {ξ′ ∈ΣX | ξ|X\Y ⊆ξ′}. Intuitively, the
projection removes the original assignments of variables from Y and allows them
to be substituted by any possible value. We deﬁne πY(⊥) = ⊥and write πY if Y
is a singleton set {Y }. As an example, for X = {X, Y } the projection of ⃗0 wrt
{X} is given as πX(⃗0) = {{X →0, Y →0}, {X →1, Y →0}}.2 The deﬁnition
of projection can be extended to trees τ over ΣX so that πY(τ) is the set of trees
{τ ′ ∈ΣX | ∀p ∈pos(τ) : if τ(p) = ⊥, then τ ′(p) = ⊥, else τ ′(p) ∈πY(τ(p))}
and subsequently to languages L so that πY(L) = {πY(τ) | τ ∈L}.
The Classical Decision Procedure for WS2S. The classical decision pro-
cedure for the WS2S logic goes through a direct construction of a TA Aϕ having
the same language as a given formula ϕ. Let us brieﬂy recall the automata
constructions used (cf. [31]). Given a complete TA A = (Q, δ, I, R), the com-
plement assumes that A is deterministic and returns A∁= (Q, δ, I, Q \ R),
the projection returns πX(A) = (Q, δπX, I, R) with δπX
a (q, r) = δπX(a)(q, r),
and the subset construction returns the deterministic and complete automaton
AD = (2Q, δD, {I}, RD) where δD
a (S, S′) = 
q∈S,q′∈S′ δa(q, q′) and RD = {S ⊆
Q | S ∩R ̸= ∅}. The binary operators ◦∈{∪, ∩} are implemented through
a product construction, which—given the TA A and another complete TA
A′ = (Q′, δ′, I′, R′)—returns the automaton A◦A′ = (Q×Q′, Δ×, I×, R◦) where
Δ×
a ((q, r), (q′, r′)) = Δa(q, q′) × Δ′
a(r, r′), I× = I × I′, and for (q, r) ∈Q × Q′,
(q, r) ∈R∩⇔q ∈R ∧r ∈R′ and (q, r) ∈R∪⇔q ∈R ∨r ∈R′. The language
non-emptiness test can be implemented through the equivalence L (A) ̸= ∅iﬀ
reachδ(I) ∩R ̸= ∅where the set reachδ(S) of states reachable from a set S ⊆Q
through δ-transitions is computed as the least ﬁxpoint
reachδ(S) = μZ. S ∪

q,r∈Z
δ(q, r).
(1)
The same ﬁxpoint computation is used to compute the derivative wrt a
for
some a ∈Σ as A −a
= (Q, δ, reachδa(I), R): the new leaf states are all those
reachable from I through a-transitions.
The classical WSkS decision procedure uses the above operations to con-
structs the automaton Aϕ inductively to the structure of ϕ as follows: (i) If ϕ
is an atomic formula, then Aϕ is a pre-deﬁned base TA over ΣX (the particular
base automata for our atomic predicates can be found, e.g., in [31], and we list
them also in [30]). (ii) If ϕ = ϕ1 ∧ϕ2, then Aϕ = Aϕ1 ∩Aϕ2. (iii) If ϕ = ϕ1 ∨ϕ2,
then Aϕ = Aϕ1 ∪Aϕ2. (iv) If ϕ = ¬ψ, then Aϕ = A∁
ψ. (v) Finally, if ϕ = ∃X. ψ,
then Aϕ = (πX(Aψ))D −⃗0 .
2 Note that our deﬁnition of projection diﬀers from the usual one, which would in the
example produce a single symbol {Y →0} over a diﬀerent alphabet (the alphabet
of symbols over {Y }).

306
V. Havlena et al.
Points (i) to (iv) are self-explanatory. In point (v), the projection imple-
ments the quantiﬁcation by forgetting the values of the X component of all
symbols. Since this yields non-determinism, projection is followed by determin-
isation by the subset construction. Further, the projection can produce some
new trees that contain ⃗0-only labelled sub-trees, which need not be present
in some smaller encodings of the same model. Consider, for example, a for-
mula ψ having the language L (ψ) given by the tree τν in Fig. 1b and all
its ⃗0-extensions. To obtain L (∃X.ψ), it is not suﬃcient to make the projec-
tion πX(L (ψ)) because the projected language does not contain the minimum
encoding τ min
ν
of ν : Y →{ϵ, L, LL, R, RR}, but only those encodings ν′ such
that ν′(RLR) = {Y →0}. Therefore, the ⃗0-derivative is needed to saturate the
language with all encodings of the encoded models (if some of these encod-
ings were missing, the inductive construction could produce a wrong result, for
instance, if the language were subsequently complemented). Note that the same
eﬀect can be achieved by replacing the set of leaf states I of Aϕ by reachΔ⃗0(I)
where Δ is the transition function of Aϕ. See [31] for more details.
3
Automata Terms
Our algorithm for deciding WS2S may be seen as an alternative implementation
of the classical procedure from Sect. 2. The main innovation is the data struc-
ture of automata terms, which implicitly represent the automata constructed
by the automata operations. Unlike the classical procedure—which proceeds by
a bottom-up traversal on the formula structure, building an automaton for each
sub-formula before proceeding upwards—automata terms allow for constructing
parts of automata at higher levels from parts of automata on the lower levels
even though the construction of the lower level automata has not yet ﬁnished.
This allows one to test the language emptiness on the ﬂy and use techniques of
state space pruning, which will be discussed later in Sect. 4.
Syntax of Automata Terms. Terms are created according to the grammar
in Fig. 2 starting from states q ∈Qi, denoted as atomic states, of a given ﬁnite
set of base automata Bi = (Qi, δi, Ii, Ri) with pairwise disjoint sets of states.
For simplicity, we assume that the base automata are complete, and we denote
by B = (QB, δB, IB, RB) their component-wise union. Automata terms A specify
the set of leaf states of an automaton. Set terms S list a ﬁnite number of the leaf
Fig. 2. Syntax of terms.

Automata Terms in a Lazy WSkS Decision Procedure
307
states explicitly, while derivative terms D specify them symbolically as states
reachable from a set of states S via ⃗0s. The states themselves are represented
by state terms t (notice that set terms S and derivate terms D can both be
automata and state terms). Intuitively, the structure of state terms records the
automata constructions used to create the top-level automaton from states of the
base automata. Non-leaf state terms, the state terms’ transition function, and
root state terms are then deﬁned inductively from base automata as described
below in detail. We will normally use t, u to denote terms of all types (unless the
type of the term needs to be emphasized).
Example 1. Consider a formula ϕ ≡¬∃X. Sing(X) ∧X = {ϵ} and its corre-
sponding automata term tϕ =

{πX({q0} &{p0})} −⃗0

(we will show how
tϕ was obtained from ϕ later). For the sake of presentation, we will con-
sider the following base automata for the predicates Sing(X) and X = {ϵ}:
ASing(X) = ({q0, q1, qs}, δ, {q0}, {q1}) and AX={ϵ} = ({p0, p1, ps}, δ′, {p0}, {p1})
where δ and δ′ have the following sets of transitions (transitions not deﬁned
below go to the sink states qs and ps, respectively):
δ : (q0, q0)−{{X	→0}}→q0, (q0, q1)−{{X	→0}}→q1,
δ′ : (p0, p0)−{{X	→0}}→p0,
(q0, q0)−{{X	→1}}→q1, (q1, q0)−{{X	→0}}→q1
(p0, p0)−{{X	→1}}→p1.
The term tϕ denotes the TA

(πX(ASing(X) ∩AX={ϵ})−⃗0 )D∁constructed
by intersection, projection, derivative, subset construction, and complement. ⊓⊔
Semantics of Terms. We will deﬁne the denotation of an automata term t
as the automaton At = (Q, Δ, I, R). For a set automata term t = S, we deﬁne
I = S, Q = reachΔ(S) (i.e., Q is the set of state terms reachable from the
leaf state terms), and Δ and R are deﬁned inductively to the structure of t.
Particularly, R contains the terms of Q that satisfy the predicate R deﬁned in
Fig. 3, and Δ is deﬁned in Fig. 4, with the addition that whenever the rules in
Fig. 4 do not apply, then we let Δa(t, t′) = {∅}. The ∅here is used as a universal
sink state in order to maintain Δ complete, which is needed for automata terms
representing complements to yield the expected language.
Fig. 3. Root term states.

308
V. Havlena et al.
Fig. 4. Transitions among compatible state terms.
The transitions of Δ for
terms of the type +, &, πX, · ,
and S are built from the tran-
sition function of their sub-
terms analogously to how the
automata operations of the
product union, product inter-
section, projection, comple-
ment, and subset construc-
tion, respectively, build the
transition function from the
transition functions of their
arguments (cf. Sect. 2). The only diﬀerence is that the state terms stay annotated
with the particular operation by which they were made (the annotation of the set
state terms are the set brackets). The root states are also deﬁned analogously as
in the classical constructions. In Figs. 3 and 4, the terms t, t′, u, u′ are arbitrary
terms, S, S′ are set terms, and q, r ∈QB.
Finally, we complete the deﬁnition of the term semantics by adding the def-
inition of semantics for the derivative term S −⃗0 . This term is a symbolic
representation of the set term that contains all state terms upward-reachable
from S in AS over ⃗0. Formally, we ﬁrst deﬁne the so-called saturation of AS as
(S −⃗0 )s = reachΔ⃗0(S)
(14)
(with reachΔ⃗0(S) deﬁned as the ﬁxpoint (1)), and we complete the deﬁnition
of Δ and R in Figs. 3 and 4 with three new rules to be used with a derivative
term D:
Δa(D, u)=Δa(Ds, u) (15)
Δa(u, D)=Δa(u, Ds) (16)
R(D) ⇔R(Ds)
(17)
The automaton AD then equals ADs, i.e., the semantics of a derivative term is deﬁned
by its saturation.
Example 2. Let us consider a derivative term t = {πX({q0} &{p0})}−⃗0
, which occurs
within the nested automata term tϕ of Example 1. The set term representing all terms
reachable upward from t is then the term
ts = {πX({q0} &{p0}), πX({q1} &{p1}), πX({qs} &{ps}),
πX({q1} &{ps}), πX({q0} &{ps})}.
The semantics of t is therefore the TA At with the set of states given by ts.
⊓⊔
Properties of Terms. An implication of the deﬁnitions above, essential for ter-
mination of our algorithm in Sect. 4, is that the automata represented by the terms
indeed have ﬁnitely many states. This is the direct consequence of Lemma 1.
Lemma 1. The size of reachΔ(t) is ﬁnite for any automata term t.

Automata Terms in a Lazy WSkS Decision Procedure
309
Intuitively, the terms are built over a ﬁnite set of states QB, they are ﬁnitely
branching, and the transition function on terms does not increase their depth.
Let us further denote by L (t) the language L (At) of the automaton induced by
a term t. Lemma 2 below shows that languages of terms can be deﬁned from the lan-
guages of their sub-terms if the sub-terms are set terms of derivative terms. The terms
on the left-hand sides are implicit representations of the automata operations of the
respective language operators on the right-hand sides. The main reason why the lemma
cannot be extended to all types of sub-terms and yield an inductive deﬁnition of term
languages is that it is not meaningful to talk about the bottom-up language of an
isolated state term that is neither a set term nor a derivative term (which both are
also automata terms). This is also one of the main diﬀerences from [29] where every
term has its own language, which makes the reasoning and the correctness proofs in
the current paper signiﬁcantly more involved.
Lemma 2. For automata terms A1, A2 and a set term S, the following holds:
L({A1}) = L(A1)
(a)
L({A1 + A2}) = L(A1) ∪L(A2) (b)
L({A1&A2}) = L(A1) ∩L(A2) (c)
L({A1}) = L(A1)
(d)
L({πX(A1)}) = πX(L(A1)) (e)
L(S −⃗0
) = L(S) −⃗0
(f)
Terms of Formulae. Our algorithm in Sect. 4 will translate a WS2S formula ϕ
into the automata term tϕ = {⟨ϕ⟩} representing a deterministic automaton with its
only leaf state represented by the state term ⟨ϕ⟩. The base automata of tϕ include the
automaton Aϕatom for each atomic predicate ϕatom used in ϕ. The state term ⟨ϕ⟩is then
deﬁned inductively to the structure of ϕ as shown in Fig. 5. In the deﬁnition, ϕ0 is an
atomic predicate, Iϕ0 is the set of leaf states of Aϕatom , and ϕ and ψ denote arbitrary
WS2S formulae. We note that the translation rules may create sub-terms of the form
{{t}}, i.e., with nested set brackets. Since {·} semantically means determinisation by
subset construction, such double determinisation terms can be always simpliﬁed to
{t} (cf. Lemma 2a). See Example 1 for a formula ϕ and its corresponding term tϕ.
Theorem 1 establishes the correctness of the formula to term translation.
Fig. 5. From formulae to state-terms.
Theorem 1. Let ϕ be a WS2S formula. Then L (ϕ) = L(tϕ).
The proof of Theorem 1 uses structural induction, which is greatly simpliﬁed by
Lemma 2, but since Lemma 2 does not (and cannot, as discussed above) cover all used
types of terms, the induction step must in some cases still rely on reasoning about the
deﬁnition of the transition relation on terms.

310
V. Havlena et al.
4
An Eﬃcient Decision Procedure
The development in Sect. 3 already implies a na¨ıve automata term-based satisﬁability
check. Namely, by Theorem 1, we know that a formula ϕ is satisﬁable iﬀL(Atϕ) ̸=
∅. After translating ϕ into tϕ using rules (18)–(22), we may use the deﬁnitions of
the transition function and root states of Atϕ = (Q, Δ, I, F) in Sect. 3 to decide the
language emptiness through evaluating the root state test R(reachΔ(I)). It is enough
to implement the equalities and equivalences (8)–(17) as recursive functions. We will
further refer to this algorithm as the simple recursion. The evaluation of reachΔ(I)
induces nested evaluations of the ﬁxpoint (14): the one on the top level of the language
emptiness test and another one for every expansion of a derivative sub-term. The
termination of these ﬁxpoint computations is guaranteed due to Lemma 1.
Such a na¨ıve implementation is, however, ineﬃcient and has only disadvantages in
comparison to the classical decision procedure. In this section, we will discuss how it
can be optimized. Besides an essential memoization needed to implement the recursion
eﬃciently, we will show that the automata term representation is amenable to opti-
mizations that cannot be used in the classical construction. These are techniques of
state space pruning: the fact that the emptiness can be tested on the ﬂy during the
automata construction allows one to avoid exploration of state space irrelevant to the
test. The pruning is done through the techniques of lazy evaluation and subsumption.
We will also discuss an optimization of the transition function of Sect. 3 through product
ﬂattening, which is an analogy to standard implementations of automata intersection.
4.1
Memoization
The simple recursion repeats the ﬁxpoint computations that saturate derivative terms
from scratch at every call of the transition function or root test. This is easily countered
through memoization, known, e.g., from compilers of functional languages, which caches
results of function calls in order to avoid their re-evaluation. Namely, after saturating
a derivative sub-term t = S −⃗0
of tϕ for the ﬁrst time, we simply replace t in tϕ
by the saturation ts = reachΔ⃗0(S). Since a derivative is a symbolic representation
of its saturated version, the replacement does not change the language of tϕ. Using
memoization, every ﬁxpoint computation is then carried out once only.
4.2
Lazy Evaluation
The lazy variant of the procedure uses short-circuiting to optimize connectives ∧
and ∨, and early termination to optimize ﬁxpoint computation in derivative satura-
tions. Namely, assume that we have a term t1 + t2 and that we test whether R(t1 + t2).
Suppose that we establish that R(t1); we can short circuit the evaluation and immedi-
ately return true, completely avoiding touching the potentially complex term t2 (and
analogously for a term of the form t1 & t2 when one branch is false).
Furthermore, early termination is used to optimize ﬁxpoint computations used to
saturate derivatives within tests R(S−⃗0
) (obtained from sub-formulae such as ∃X. ψ).
Namely, instead of ﬁrst unfolding the whole ﬁxpoint into a set {t1, . . . tn} and only then
testing whether R(ti) is true for some ti, the terms ti can be tested as soon as they
are computed, and the ﬁxpoint computation can be stopped early, immediately when
the test succeeds on one of them. Then, instead of replacing the derivative sub-term
by its full saturation, we replace it by the partial result {t1, . . . , ti} −⃗0
for i ≤n.

Automata Terms in a Lazy WSkS Decision Procedure
311
Finishing the evaluation of the ﬁxpoint computation might later be required in order to
compute a transition from the derivative. We note that this corresponds to the concept
of continuations from functional programming, used to represent a paused computation
that may be required to continue later.
Example 3. Let us now illustrate the lazy decision procedure on our running exam-
ple formula ϕ ≡¬∃X. Sing(X) ∧X = {ϵ} and the corresponding automata term
tϕ =

{πX({q0} &{p0})} −⃗0

from Example 1. The task of the procedure is to
compute the value of R(reachΔ(tϕ)), i.e., whether there is a root state reachable
from the leaf state ⟨ϕ⟩of Atϕ. The fact that ϕ is ground allows us to slightly sim-
plify the problem because any ground formula ψ is satisﬁable iﬀ⊥∈L (ψ), i.e.,
iﬀthe leaf state ⟨ψ⟩of Atψ is also a root. It is thus enough to test R(⟨ϕ⟩) where
⟨ϕ⟩= {πX({q0} &{p0})} −⃗0
.
The computation proceeds as follows. First, we use (5) from Fig. 3 to propagate the
root test towards the derivative, i.e., to obtain that R(⟨ϕ⟩) iﬀ¬R({πX({q0} &{p0})} −
⃗0
). Since the R-test cannot be directly evaluated on a derivative term, we need to
start saturating it into a set term, evaluating R on the ﬂy, hoping for early termination.
We begin with evaluating the R-test on the initial element t0 = πX({q0}{p0}) of the
set. The test propagates through the projection πX due to (4) and evaluates as false
on the left conjunct (through, in order, (3), (6), and (7) since the state q0 is not a root
state. As a trivial example of short circuiting, we can skip evaluating R on the right
conjunct {p0} and conclude that R(t0) is false.
The ﬁxpoint computation then continues with the ﬁrst iteration, computing the ⃗0-
successors of the set {t0}. We will obtain Δ⃗0(t0, t0) = {t0, t1} with t1 = πX({q1} &{p1}).
The test R(t1) now returns true because both q1 and p1 are root states. With that,
the ﬁxpoint computation may terminate early, with the R-test on the derivative sub-
term returning true. Memoization then replaces the derivative sub-term in ⟨ϕ⟩by the
partially evaluated version {t0, t1} −⃗0
, and R(⟨ϕ⟩) is evaluated as false due to (5).
We therefore conclude that ϕ is unsatisﬁable (and invalid since it is ground).
⊓⊔
4.3
Subsumption
The next technique we use is based on pruning out parts of a search space that are
subsumed by other parts. In particular, we generalize (in a similar way as we did for
WS1S in our previous work [29]) the concept used in antichain algorithms for eﬃciently
deciding language inclusion and universality of ﬁnite word and tree automata [22–25].
Although the problems are in general computationally infeasible (they are PSPACE-
complete for ﬁnite word automata and EXPTIME-complete for ﬁnite tree automata),
antichain algorithms can solve them eﬃciently in many practical cases.
We apply the technique by keeping set terms in the form of antichains of simulation-
maximal elements and prune out any other simulation-smaller elements. Intuitively,
the notion of a term t being simulation-smaller than t′ implies that trees that might
be generated from the leaf states T ∪{t} can be generated from T ∪{t′} too, hence
discarding t does not hurt. Formally, we introduce the following rewriting rule:
{t1, t2, . . . , tn} ⇝{t2, . . . , tn}
for t1 ⊑t2,
(23)
which may be used to simplify set sub-terms of automata terms. The rule (23) is
applied after every iteration of the ﬁxpoint computation on the current partial result.

312
V. Havlena et al.
Hence the sequence of partial results is monotone, which, together with the ﬁniteness
of reachΔ(t), guarantees termination. The subsumption relation ⊑used in the rule is
deﬁned in Fig. 6 where S ⊑∀∃S′ denotes ∀t ∈
S ∃t′ ∈
S′. t ⊑
t′. Intuitively, on
base TAs, subsumption corresponds to inclusion of the set terms (the left disjunct of
(24). This clearly has the intended outcome: a larger set of states can always simulate
a smaller set in accepting a tree. The rest of the deﬁnition is an inductive extension of
the base case. It can be shown that ⊑for any automata term t is an upward simulation
on At in the sense of [25]. Consequently, rewriting sub-terms in an automata term
according to the new rule (23) does not change its language. Moreover, the ﬁxpoint
computation interleaved with application of rule (23) terminates.
Fig. 6. The subsumption relation ⊑
4.4
Product Flattening
Product ﬂattening is a technique that we use to reduce the size of ﬁxpoint saturations
that generate conjunctions and disjunctions of sets as their elements.Consider a term
of the form D = {πX(S0 & S′
0)} −⃗0
for a pair of sets of terms S0 and S′
0 where the
TAs AS0 and AS′
0 have sets of states Q and Q′, respectively.The saturation generates
the set {πX(S0 & S′
0), . . . , πX(Sn & S′
n)} with Si ⊆Q, S′
i ⊆Q′ for all 0 ≤i ≤n. The size
of this set is 2|Q|·|Q′| in the worst case. In terms of the automata operations, this ﬁxpoint
expansion corresponds to ﬁrst determinizing both AS0 and AS′
0 and only then using
the product construction (cf. Sect. 2). The automata intersection, however, works for
nondeterministic automata too—the determinization is not needed. Implementing this
standard product construction on terms would mean transforming the original ﬁxpoint
above into the following ﬁxpoint with a ﬂattened product: D = {πX(S [&] S′)} −⃗0
where [&] is the augmented product for conjunction. This way, we can decrease the
worst-case size of the ﬁxpoint to |Q| · |Q′|. A similar reasoning holds for terms of the
form {πX(S0 + S′
0)}−⃗0
. Formally, the technique can be implemented by the following
pair of sub-term rewriting rules where S and S′ are non-empty sets of terms:
S + S′ ⇝S [+] S′,
(29)
S & S′ ⇝S [&] S′.
(30)
Observe that for terms obtained from WS2S formulae using the translation from
Sect. 3, the rules are not really helpful as is. Consider, for instance, the term
{πX({r} &{q})} −⃗0
obtained from a formula ∃X. ϕ ∧ψ with ϕ and ψ being atoms.
The term would be, using rule (30), rewritten into the term {πX({r & q})}−⃗0
. Then,
during a subsequent ﬁxpoint computation, we might obtain a ﬁxpoint of the following
form: {πX({r & q}), πX({r & q, r1 & q1}), πX({r1 & q1, r2 & q2})}, where the occurrences

Automata Terms in a Lazy WSkS Decision Procedure
313
of the projection πX disallow one to perform the desired union of the inner sets, and
so the application of rule (30) did not help. We therefore need to equip our procedure
with a rewriting rule that can be used to push the projection inside a set term S:
πX(S) ⇝{πX(t) | t ∈S}.
(31)
In
the
example
above,
we
would
now
obtain
the
term
{πX(r & q)} −
⃗0
(we
rewrote
{{·}}
to
{·}
as
mentioned
in
Sect. 3)
and
the
ﬁxpoint
{πX(r & q), πX(r1 & q1), πX(r2 & q2)}. The correctness of the rules is guaranteed by the
following lemma:
Lemma 3. For sets of terms S, S′ s.t. S ̸= ∅, S′ ̸= ∅we have:
L

{S + S′}

= L

{S [+] S′}

,
(a)
L ({πX(S)}) = L ({πX(t) | t ∈S}) . (c)
L

{S&S′}

= L

{S [&] S′}

,
(b)
However, we still have to note that there is a danger related with the rules (29)–
(31). Namely, if they are applied to some terms in a partially evaluated ﬁxpoint but
not to all, the form of these terms might get diﬀerent (cf. πX({r & q}) and πX(r & q)),
and it will not be possible to combine them as source states of TA transitions when
computing Δa, leading thus to an incorrect result. We resolve the situation such that
we apply the rules as a pre-processing step only before we start evaluating the top-level
ﬁxpoint, which ensures that all terms will subsequently be generated in a compatible
form.
5
Experimental Evaluation
We have implemented the above introduced technique in a prototype tool written in
Haskell.3 The base automata, hard-coded into the tool, were the TAs for the basic
predicates from Sect. 2, together with automata for predicates Sing(X) and X = {p}
for a variable X and a ﬁxed tree position p. As an optimisation, our tool uses the
so-called antiprenexing (proposed already in [29]), pushing quantiﬁers down the for-
mula tree using the standard logical equivalences. Intuitively, antiprenexing reduces
the complexity of elements within ﬁxpoints by removing irrelevant parts outside the
ﬁxpoint.
We have performed experiments with our tool on various formulae and compared
its performance with that of Mona. We applied Mona both on the original form of the
considered formulae as well as on their versions obtained by antiprenexing (which is
built into our tool and which—as we realised—can signiﬁcantly help Mona too). Our
preliminary implementation of product ﬂattening (cf. Sect. 4.4) is restricted to parts
below the lowest ﬁxpoint, and our experiments showed that it does not work well when
applied on this level, where the complexity is not too high, so we turned it oﬀfor the
experiments. We ran all experiments on a 64-bit Linux Debian workstation with the
Intel(R) Core(TM) i7-2600 CPU running at 3.40 GHz with 16 GiB of RAM. We used
a timeout of 100 s.
We ﬁrst considered various WS2S formulae on which Mona was successfully applied
previously in the literature. On them, our tool is quite slower than Mona, which is
not much surprising given the amount of optimisations built into Mona (for instance,
3 The implementation is available at https://github.com/vhavlena/lazy-wsks.

314
V. Havlena et al.
Table 1. Experimental results over the family of formulae ϕpt
n
≡
∀Z1, Z2. ∃
X1, . . . , Xn. edge(Z1, X1) ∧n
i=1 edge(Xi, Xi+1) ∧edge(Xn, Z2) where edge(X, Y ) ≡
edgeL(X, Y ) ∨edgeR(X, Y ) and edgeL/R(X, Y ) ≡∃Z. Z = SL/R(X) ∧Z ⊆Y .
Running time (sec)
# of subterms/states
n Lazy
Mona
Mona+AP
Lazy
Mona
Mona+AP
1
0.02
0.16
0.15
149
216
216
2
0.50
-
-
937
-
-
3
0.83
-
-
2487
-
-
4
34.95
-
-
8391
-
-
5
60.94
-
-
23827
-
-
for the benchmarks from [5], Mona on average took 0.1 s, while we timeouted).4 Next,
we identiﬁed several parametric families of formulae (adapted from [29]), such as, e.g.,
ϕhorn
n
≡∃X. ∀X1. ∃X2, . . . Xn. ((X1 ⊆X ∧X1 ̸= X2) ⇒X2 ⊆X) ∧. . . ∧((Xn−1 ⊆
X∧Xn−1 ̸= Xn) ⇒Xn ⊆X), where our approach ﬁnished within 10 ms, while the time
of Mona was increasing when increasing the parameter n, going up to 32 s for n = 14
and timeouting for k ≥15. It turned out that Mona could, however, easily handle
these formulae after antiprenexing, again (slightly) outperforming our tool. Finally, we
also identiﬁed several parametric families of formulae that Mona could handle only
very badly or not at all, even with antiprenexing, while our tool can handle them much
better. These formulae are mentioned in the captions of Tables 1, 2 and 3, which give
detailed results of the experiments.
Table 2. Experimental results over the family of formulae ϕcnst
n
≡∃X. X = {(LR)4} ∧
X = {(LR)n}.
Running time (sec)
# of subterms/states
n
Lazy
Mona
Mona+AP
Lazy
Mona
Mona+AP
80
14.60
40.07
40.05
1146
27913
27913
90
21.03
64.26
64.20
1286
32308
32308
100
28.57
98.42
98.91
1426
36258
36258
110
38.10
-
-
1566
-
-
120
49.82
-
-
1706
-
-
Table
3.
Experiments
over
the
family
ϕsub
n
=
∀X1, . . . , Xn ∃X. n−1
i=1 Xi ⊆X ⇒(Xi+1 = SL(X) ∨
Xi+1 = SR(X)).
Running time (sec)
# of subterms/states
n
Lazy
Mona
Mona+AP
Lazy
Mona
Mona+AP
3
0.01
0.00
0.00
140
92
92
4
0.04
34.39
34.47
386
170
170
5
0.24
−
−
981
−
−
6
2.01
−
−
2376
−
−
Particularly, Columns 2–
4
give
the
running
times
(in
seconds)
of
our
tool
(denoted Lazy), Mona, and
Mona
with
antiprenexing.
Columns 5–7 characterize the
size of the generated terms
and automata. Namely, for
our approach, we give the
4 Building an optimised and overall competitive implementation is a subject of our
further work. Our results with an implementation of a lazy decision procedure for
WS1S from [29] suggest that this is possible.

Automata Terms in a Lazy WSkS Decision Procedure
315
number of nodes in the ﬁnal term tree (with the leaves being states of the base TAs).
For Mona, we give the sum of the numbers of states of all the minimal deterministic
TAs constructed by Mona when evaluating the formula. The “–” sign means a timeout
or memory shortage.
The formulae considered in Tables 1, 2 and 3 speak about various paths in trees.
We were originally inspired by formulae kindly provided by Josh Berdine, which arose
from attempts to translate separation logic formulae to WS2S (and use Mona to
discharge them), which are beyond the capabilities of Mona (even with antiprenexing).
We were also unable to handle them with our tool, but our experimental results on
the tree path formulae indicate (despite the prototypical implementation) that our
techniques can help one to handle some complex graph formulae that are out of the
capabilities of Mona. Thus, they provide a new line of attack on deciding hard WS2S
formulae, complementary to the heuristics used in Mona. Improving the techniques
and combining them with the classical approach of Mona is a challenging subject for
our future work.
6
Related Work
The seminal works [32,33] on the automata-logic connection were the milestones leading
to what we call here the classical tree automata-based decision procedure for WSkS [34].
Its non-elementary worst-case complexity was proved in [35], and the work [2] presents
the ﬁrst implementation, restricted to WS1S, with the ambition to use heuristics to
counter the high complexity. The authors of [31] provide an excellent survey of the
classical results and literature related to WSkS and tree automata.
The tool Mona [3] implements the classical decision procedures for both WS1S
and WS2S. It is still the standard tool of choice for deciding WS1S/WSkS formulae
due to its all-around most robust performance. The eﬃciency of Mona stems from
many optimizations, both higher-level (such as automata minimization, the encoding
of ﬁrst-order variables used in models, or the use of multi-terminal BDDs to encode the
transition function of the automaton) as well as lower-level (e.g. optimizations of hash
tables, etc.) [36,37]. The M2L(Str) logic, a dialect of WS1S, can also be decided by
a similar automata-based decision procedure, implemented within, e.g., jMosel [38]
or the symbolic ﬁnite automata framework of [39]. In particular, jMosel implements
several optimizations (such as second-order value numbering [40]) that allow it to
outperform Mona on some benchmarks (Mona also provides an M2L(Str) interface
on top of the WS1S decision procedure).
The original inspiration for our work are the antichain techniques for checking uni-
versality and inclusion of ﬁnite automata [22–25] and language emptiness of alternating
automata [22], which use symbolic computation together with subsumption to prune
large state spaces arising from subset construction. This paper is a continuation of our
work on WS1S, which started by [41], where we discussed a basic idea of generalizing
the antichain techniques to a WS1S decision procedure. In [29], we then presented
a complete WS1S decision procedure based on these ideas that is capable to rival
Mona on already interesting benchmarks. The work in [42] presents a decision proce-
dure that, although phrased diﬀerently, is in essence fairly similar to that of [29]. This
paper generalizes [29] to WS2S. It is not merely a straightforward generalization of the
word concepts to trees. A nontrivial transition was needed from language terms of [29],
with their semantics being deﬁned straightforwardly from the semantics of sub-terms,
to tree automata terms, with the semantics deﬁned as a language of an automaton

316
V. Havlena et al.
with transitions deﬁned inductively to the structure of the term. This change makes
the reasoning and correctness proof considerably more complex, though the algorithm
itself stays technically quite simple.
Finally, Ganzow and Kaiser [43] developed a new decision procedure for the weak
monadic second-order logic on inductive structures within their tool Toss. Their app-
roach completely avoids automata; instead, it is based on the Shelah’s composition
method. The paper reports that the Toss tool could outperform Mona on two fam-
ilies of WS1S formulae, one derived from Presburger arithmetics and one formula of
the form that we mention in our experiments as problematic for Mona but solvable
easily by Mona with antiprenexing.
Acknowledgement. We thank the anonymous reviewers for their helpful comments
on how to improve the exposition in this paper. This work was supported by the Czech
Science Foundation project 17-12465S, the FIT BUT internal project FIT-S-17-4014,
and The Ministry of Education, Youth and Sports from the National Programme of
Sustainability (NPU II) project IT4Innovations excellence in science—LQ1602.
References
1. Møller, A., Schwartzbach, M.: The pointer assertion logic engine. In: PLDI 2001.
ACM Press (2001). Also in SIGPLAN Notices 36(5) (2001)
2. Glenn, J., Gasarch, W.: Implementing WS1S via ﬁnite automata. In: Raymond,
D., Wood, D., Yu, S. (eds.) WIA 1996. LNCS, vol. 1260, pp. 50–63. Springer,
Heidelberg (1997). https://doi.org/10.1007/3-540-63174-7 5
3. Elgaard, J., Klarlund, N., Møller, A.: MONA 1.x: new techniques for WS1S and
WS2S. In: Hu, A.J., Vardi, M.Y. (eds.) CAV 1998. LNCS, vol. 1427, pp. 516–520.
Springer, Heidelberg (1998). https://doi.org/10.1007/BFb0028773
4. Klarlund, N., Møller, A.: MONA Version 1.4 User Manual. BRICS, Department of
Computer Science, Aarhus University, January 2001. Notes Series NS-01-1. http://
www.brics.dk/mona/. Revision of BRICS NS-98-3
5. Madhusudan, P., Parlato, G., Qiu, X.: Decidable logics combining heap structures
and data. In: POPL 2011, pp. 611–622. ACM (2011)
6. Madhusudan, P., Qiu, X.: Eﬃcient decision procedures for heaps using STRAND.
In: Yahav, E. (ed.) SAS 2011. LNCS, vol. 6887, pp. 43–59. Springer, Heidelberg
(2011). https://doi.org/10.1007/978-3-642-23702-7 8
7. Chin, W., David, C., Nguyen, H.H., Qin, S.: Automated veriﬁcation of shape, size
and bag properties via user-deﬁned predicates in separation logic. Sci. Comput.
Program. 77(9), 1006–1036 (2012)
8. Zee, K., Kuncak, V., Rinard, M.C.: Full functional veriﬁcation of linked data struc-
tures. In: POPL 2008, 349–361. ACM (2008)
9. Tateishi, T., Pistoia, M., Tripp, O.: Path- and index-sensitive string analysis based
on monadic second-order logic. ACM Trans. Comput. Log. 22(4), 33 (2013)
10. Zhou, M., He, F., Wang, B., Gu, M., Sun, J.: Array theory of bounded elements
and its applications. J. Autom. Reasoning 52(4), 379–405 (2014)
11. Baukus, K., Bensalem, S., Lakhnech, Y., Stahl, K.: Abstracting WS1S systems
to verify parameterized networks. In: Graf, S., Schwartzbach, M. (eds.) TACAS
2000. LNCS, vol. 1785, pp. 188–203. Springer, Heidelberg (2000). https://doi.org/
10.1007/3-540-46419-0 14

Automata Terms in a Lazy WSkS Decision Procedure
317
12. Bodeveix, J.-P., Filali, M.: FMona: a tool for expressing validation techniques over
inﬁnite state systems. In: Graf, S., Schwartzbach, M. (eds.) TACAS 2000. LNCS,
vol. 1785, pp. 204–219. Springer, Heidelberg (2000). https://doi.org/10.1007/3-
540-46419-0 15
13. Bozga, M., Iosif, R., Sifakis, J.: Structural invariants for parametric veriﬁcation of
systems with almost linear architectures. Technical report arXiv:1902.02696 (2019)
14. Klarlund, N., Nielsen, M., Sunesen, K.: A case study in veriﬁcation based on trace
abstractions. In: Broy, M., Merz, S., Spies, K. (eds.) Formal Systems Speciﬁcation.
LNCS, vol. 1169, pp. 341–373. Springer, Heidelberg (1996). https://doi.org/10.
1007/BFb0024435
15. Smith, M.A., Klarlund, N.: Veriﬁcation of a sliding window protocol using IOA
and MONA. In: Bolognesi, T., Latella, D. (eds.) Formal Methods for Distributed
System Development. ITIFIP, vol. 55, pp. 19–34. Springer, Boston, MA (2000).
https://doi.org/10.1007/978-0-387-35533-7 2
16. Basin, D., Klarlund, N.: Automata based symbolic reasoning in hardware veriﬁca-
tion. In: CAV 1998. LNCS, pp. 349–361. Springer (1998)
17. Sandholm, A., Schwartzbach, M.I.: Distributed safety controllers for web services.
In: Astesiano, E. (ed.) FASE 1998. LNCS, vol. 1382, pp. 270–284. Springer, Hei-
delberg (1998). https://doi.org/10.1007/BFb0053596
18. Hune, T., Sandholm, A.: A case study on using automata in control synthesis. In:
Maibaum, T. (ed.) FASE 2000. LNCS, vol. 1783, pp. 349–362. Springer, Heidelberg
(2000). https://doi.org/10.1007/3-540-46428-X 24
19. Hamza, J., Jobstmann, B., Kuncak, V.: Synthesis for regular speciﬁcations over
unbounded domains. In: FMCAD 2010, pp. 101–109. IEEE Computer Science
(2010)
20. Morawietz, F., Cornell, T.: The MSO logic-automaton connection in linguistics. In:
Lecomte, A., Lamarche, F., Perrier, G. (eds.) LACL 1997. LNCS (LNAI), vol. 1582,
pp. 112–131. Springer, Heidelberg (1999). https://doi.org/10.1007/3-540-48975-
4 6
21. Wies, T., Mu˜niz, M., Kuncak, V.: An eﬃcient decision procedure for imperative
tree data structures. In: Bjørner, N., Sofronie-Stokkermans, V. (eds.) CADE 2011.
LNCS (LNAI), vol. 6803, pp. 476–491. Springer, Heidelberg (2011). https://doi.
org/10.1007/978-3-642-22438-6 36
22. Doyen, L., Raskin, J.-F.: Antichain algorithms for ﬁnite automata. In: Esparza, J.,
Majumdar, R. (eds.) TACAS 2010. LNCS, vol. 6015, pp. 2–22. Springer, Heidelberg
(2010). https://doi.org/10.1007/978-3-642-12002-2 2
23. De Wulf, M., Doyen, L., Henzinger, T.A., Raskin, J.-F.: Antichains: a new algo-
rithm for checking universality of ﬁnite automata. In: Ball, T., Jones, R.B. (eds.)
CAV 2006. LNCS, vol. 4144, pp. 17–30. Springer, Heidelberg (2006). https://doi.
org/10.1007/11817963 5
24. Bouajjani, A., Habermehl, P., Hol´ık, L., Touili, T., Vojnar, T.: Antichain-based
universality and inclusion testing over nondeterministic ﬁnite tree automata. In:
Ibarra, O.H., Ravikumar, B. (eds.) CIAA 2008. LNCS, vol. 5148, pp. 57–67.
Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-70844-5 7
25. Abdulla, P.A., Chen, Y.-F., Hol´ık, L., Mayr, R., Vojnar, T.: When simulation meets
antichains (on checking language inclusion of NFAs). In: Esparza, J., Majumdar,
R. (eds.) TACAS 2010. LNCS, vol. 6015, pp. 158–174. Springer, Heidelberg (2010).
https://doi.org/10.1007/978-3-642-12002-2 14
26. Habermehl, P., Hol´ık, L., Rogalewicz, A., ˇSim´aˇcek, J., Vojnar, T.: Forest automata
for veriﬁcation of heap manipulation. Formal Methods Syst. Des. 41(1), 83–106
(2012)

318
V. Havlena et al.
27. De Wulf, M., Doyen, L., Maquet, N., Raskin, J.-F.: Antichains: alternative algo-
rithms for LTL satisﬁability and model-checking. In: Ramakrishnan, C.R., Rehof,
J. (eds.) TACAS 2008. LNCS, vol. 4963, pp. 63–77. Springer, Heidelberg (2008).
https://doi.org/10.1007/978-3-540-78800-3 6
28. De Wulf, M., Doyen, L., Raskin, J.-F.: A lattice theory for solving games of imper-
fect information. In: Hespanha, J.P., Tiwari, A. (eds.) HSCC 2006. LNCS, vol. 3927,
pp. 153–168. Springer, Heidelberg (2006). https://doi.org/10.1007/11730637 14
29. Fiedor, T., Hol´ık, L., Jank˚u, P., Leng´al, O., Vojnar, T.: Lazy automata techniques
for WS1S. In: Legay, A., Margaria, T. (eds.) TACAS 2017. LNCS, vol. 10205, pp.
407–425. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-662-54577-
5 24
30. Havlena, V., Hol´ık, L., Leng´al, O., Vojnar, T.: Automata terms in a lazy WSkS
decision procedure (technical report). Technical report arXiv:1905.08697 (2019)
31. Comon, H., et al.: Tree automata techniques and applications (2008)
32. B¨uchi, J.R.: On a decision method in restricted second-order arithmetic. In: Inter-
national Congress on Logic, Methodology, and Philosophy of Science, pp. 1–11.
Stanford University Press (1962)
33. Rabin, M.O.: Decidability of second order theories and automata on inﬁnite trees.
Trans. Am. Math. Soc. 141, 1–35 (1969)
34. Thatcher, J.W., Wright, J.B.: Generalized ﬁnite automata theory with an applica-
tion to a decision problem of second-order logic. Math. Syst. Theory 2(1), 57–81
(1968)
35. Stockmeyer, L.J., Meyer, A.R.: Word problems requiring exponential time (prelim-
inary report). In: Fifth Annual ACM Symposium on Theory of Computing, STOC
1973, pp. 1–9. ACM, New York (1973)
36. Klarlund, N., Møller, A., Schwartzbach, M.I.: MONA implementation secrets. Int.
J. Found. Comput. Sci. 13(4), 571–586 (2002)
37. Klarlund, N.: A theory of restrictions for logics and automata. In: Halbwachs, N.,
Peled, D. (eds.) CAV 1999. LNCS, vol. 1633, pp. 406–417. Springer, Heidelberg
(1999). https://doi.org/10.1007/3-540-48683-6 35
38. Topnik, C., Wilhelm, E., Margaria, T., Steﬀen, B.: jMosel: a stand-alone tool and
jABC plugin for M2L(Str). In: Valmari, A. (ed.) SPIN 2006. LNCS, vol. 3925, pp.
293–298. Springer, Heidelberg (2006). https://doi.org/10.1007/11691617 18
39. D’Antoni, L., Veanes, M.: Minimization of symbolic automata. In: POPL 2014, pp.
541–554 (2014)
40. Margaria, T., Steﬀen, B., Topnik, C.: Second-order value numbering. In: GraMoT
2010. Volume 30 of ECEASST, pp. 1–15. EASST (2010)
41. Fiedor, T., Hol´ık, L., Leng´al, O., Vojnar, T.: Nested antichains for WS1S. In:
Baier, C., Tinelli, C. (eds.) TACAS 2015. LNCS, vol. 9035, pp. 658–674. Springer,
Heidelberg (2015). https://doi.org/10.1007/978-3-662-46681-0 59
42. Traytel, D.: A coalgebraic decision procedure for WS1S. In: 24th EACSL Annual
Conference on Computer Science Logic (CSL 2015). Volume 41 of Leibniz Interna-
tional Proceedings in Informatics (LIPIcs), pp. 487–503. Schloss Dagstuhl-Leibniz-
Zentrum fuer Informatik, Dagstuhl, Germany (2015)
43. Ganzow, T., Kaiser, L.: New algorithm for weak monadic second-order logic on
inductive structures. In: Dawar, A., Veith, H. (eds.) CSL 2010. LNCS, vol. 6247, pp.
366–380. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-15205-
4 29

Conﬂuence by Critical Pair Analysis
Revisited
Nao Hirokawa1(B)
, Julian Nagele2
, Vincent van Oostrom3
,
and Michio Oyamaguchi4
1 JAIST, Nomi, Japan
hirokawa@jaist.ac.jp
2 Queen Mary University of London, London, UK
j.nagele@qmul.ac.uk
3 University of Innsbruck, Innsbruck, Austria
Vincent.van-Oostrom@uibk.ac.at
4 Nagoya University, Nagoya, Japan
oyamaguchi@za.ztv.ne.jp
Abstract. We present two methods for proving conﬂuence of left-linear
term rewrite systems. One is hot-decreasingness, combining the paral-
lel/development closedness theorems with rule labelling based on a ter-
minating subsystem. The other is critical-pair-closing system, allowing
to boil down the conﬂuence problem to conﬂuence of a special subsystem
whose duplicating rules are relatively terminating.
Keywords: Term rewriting · Conﬂuence · Decreasing diagrams
1
Introduction
We present two results for proving conﬂuence of ﬁrst-order left-linear term
rewrite systems, which extend and generalise three classical results: Knuth and
Bendix’ criterion [19] and strong and parallel closedness due to Huet [17]. Our
idea is to reduce conﬂuence of a term rewrite system R to that of a subsystem
C comprising rewrite rules needed for closing the critical pairs of R. In Sect. 3
we introduce hot-decreasingness, requiring that critical pairs can be closed using
rules that are either below those in the peak or in a terminating subsystem C.
In Sect. 4 we introduce the notion of a critical-pair-closing system and present a
conﬂuence-preservation result based on relative termination Cd/R of the dupli-
cating part Cd of C. For the left-linear systems we consider, our ﬁrst criterion
generalises both Huet’s parallel closedness and Knuth and Bendix’ criterion,
and the second Huet’s strong closedness. In Sect. 5, we assess viability of the
new techniques, reporting on their implementation and empirical results.
Huet’s parallel closedness result relies on the notion of overlap whose geomet-
ric intuition is subtle [1,24], and reasoning becomes intricate for development
Supported by JSPS KAKENHI Grant Number 17K00011 and Core to Core Program.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 319–336, 2019.
https://doi.org/10.1007/978-3-030-29436-6_19

320
N. Hirokawa et al.
closedness as covered by Theorem 2. We factor the classical theory of overlaps
and critical pairs through the encompassment lattice in which overlapping redex-
patterns is taking their join and the amount of overlap between redex-patterns
is computed via their meet, thus allowing to reason algebraically about over-
laps. Methodologically, our contribution here is the introduction of the lattice-
theoretic language itself, relevant as it allows one to reason about occurrences of
patterns1 and their amount of (non-)overlap, omnipresent in deduction. Tech-
nically, whereas Huet’s critical pair lemma [17] is well-suited for proving con-
ﬂuence of terminating TRSs, it is ill-suited to do so for orthogonal TRSs. Our
lattice-theoretic results remedy this, allowing to decompose a reduction R both
horizontally (as R1 · R2) and vertically (as R[x:=R2]
1
), enabling both termination
and orthogonality reasoning in conﬂuence proofs (Theorem 2).
In the last decade various classical conﬂuence results for term rewrite systems
have been factored through the decreasing diagrams method [28,30] for prov-
ing conﬂuence of abstract rewrite systems, often leading to generalisations along
the way: e.g. Felgenhauer’s multistep labelling [13] generalises Okui’s simulta-
neous closedness [27], the layer framework [12] generalises Toyama’s modular-
ity [33], critical pair systems [16] generalise both orthogonality [31] and Knuth
and Bendix’ criterion [19], and Jouannaud and Liu generalise, among others [20],
parallel closedness, but in a way we do not know how to generalise to develop-
ment closedness [29]. This paper ﬁts into this line of research.2
We assume the reader is familiar with term rewriting [1,9,32] in general and
conﬂuence methods [17,19,30] in particular. Notions not explicitly deﬁned in
this paper can all be found in those works.
2
Preliminaries on Decreasingness and Encompassment
We recall the key ingredients of the decreasing diagrams method for proving con-
ﬂuence, see [20,26,30,32], and revisit the classical notion of critical pair, recast-
ing its traditional account [1,17,19] based on redexes (substitution instances of
left-hand sides) into one based on redex-patterns (left-hand sides).
Decreasingness. Consider an ARS comprising an I-indexed relation →=

ℓ∈I →ℓequipped with a well-founded strict order ≻. We refer to {κ ∈I | ℓ≻κ}
by ⋎ℓ, and to ⋎ℓ∪⋎κ by ⋎ℓ, κ. For a subset J of I we deﬁne →J as 
ℓ∈J →ℓ.
Deﬁnition 1. A diagram for a peak b ℓ←a →κ c is decreasing if its closing
conversion has shape b ↔∗
⋎ℓ· →=
κ · ↔∗
⋎ℓ,κ · =
ℓ←· ↔∗
⋎κ c. An ARS in this setting
is called decreasing if every peak can be completed into a decreasing diagram.
One may think of decreasing diagrams as combining the diamond property [25,
Theorem 1] (via the steps in the closing conversion with labels ℓ, κ) at the basis
1 Modelled in various ways, via e.g.: tree homomorphisms (tree automata [7]), term-
operations (algebra), context-variables, labelling (rippling [5]), to name a few.
2 For space reasons we have omitted the proof by decreasing diagrams of Theorem 3.

Conﬂuence by Critical Pair Analysis Revisited
321
of conﬂuence of orthogonal systems [6,31], with local conﬂuence diagrams [25,
Theorem 3] (via the conversions with labels ⋎ℓ, κ) at the basis of conﬂuence of
terminating systems [19,21].
Theorem 1 ([28,30]).
An ARS is conﬂuent if it is decreasing. Conversely,
every countable ARS that is conﬂuent, is decreasing for some set of indices I.
For the converse part it suﬃces that the set of labels I is a doubleton, a result
that can be reformulated without referring to decreasing diagrams, as follows.
Lemma 1 ([11]). A countable conﬂuent rewrite relation has a spanning forest.
Here a spanning forest for →is a relation
⊆→that is spanning (
∗= ↔∗)
and a forest, i.e. deterministic (b
a
c implies b = c) and acyclic.
Critical Peaks Revisited. We introduce clusters as the structures obtained
after the matching of the left-hand side of a rule in a rewrite step, but before its
replacement by the right-hand side. When proving the aforementioned results in
Sects. 3 and 4, we use them as a tool to analyse overlaps and critical peaks. To
illustrate our notions we use the following running example. We refer to [16,32]
for the notion of multistep.
Example 1. In the TRS R with ϱ(x):f(f(x))→g(x) the term t = f(f(f(f(a))))
allows the step f(ϱ(f(a))) : t →f(g(f(a))) and multistep ϱ(ϱ(a)) : t ◦
−→g(g(a)).
Here f(ϱ(f(a))) and ϱ(ϱ(a)) are so-called proofterms, terms representing proofs
of rewritability in rewriting logic [22,32]. The source of a proofterm can be
computed by the 2nd-order substitution src of the left-hand side of the rule
for the rule symbol3 f(ϱ(f(a)))src = f(ϱ(f(a)))ϱ:=λx.f(f(x)) = f(f(f(f(a)))),
and, mutatis mutandis, the same for the target via tgt. Proofclusters, introduced
here, abstract from such proofterms by allowing to represent the matching and
substitution phases of multisteps as well, by means of let-expressions.
Example 2. The multistep in Example 1 comprises three phases [28, Chapter 4]:
1. let X, Y =λx.f(f(x)), λy.f(f(y)) in X(Y (a)) denotes matching f(f(x)) twice;
2. let X, Y =λx.ϱ(x), λx.ϱ(x) in X(Y (a)) denotes replacing by ϱ twice;
3. let X, Y =λx.g(x), λx.g(x) in X(Y (a)) denotes substituting g(x) twice.
To represent these we assume to have proofterms t, s, u, . . . over a signature com-
prising function symbols f, g, h, . . ., rule symbols ϱ, θ, η, . . ., 2nd-order variables
X, Y, Z, . . ., all having natural number arities, and 1st-order variables x, y, z, . . .
(with arity 0).We call proofterms without 2nd-order variables or rule symbols,
1st-order proofterms respectively terms, ranged over by M, N, L, . . ..
3 src can be viewed as tree homomorphism [7], or as a term algebra ϱLhs(t) = ℓ[x:=t].

322
N. Hirokawa et al.
Deﬁnition 2. A proofcluster is a let-expression let X = Q in t, where
– X is a vector X1, . . . , Xn of (pairwise distinct) second-order variables;
– Q is a vector of length n of closed λ-terms Qi = λxi.si, where si is a
proofterm and the length of the vector xi of variables is the arity of Xi;
and
– t is a proofterm, the body, with its 2nd-order variables among X.
Its denotation let X = Q in t is tX :=Q. It is a cluster if s1, . . . , sn, t are terms.
We let ς, ζ, ξ, . . . range over (proof)clusters. They denote (proof)terms.
Example 3. Using ς, ζ, ξ for the three let-expressions in Example 2, each is
a proofcluster and ς, ξ are clusters. Their denotations are the term ς =
f(f(f(f(a)))) = t, proofterm ζ = ϱ(ϱ(a)), and term ξ = g(g(a)).
We assume the usual variable renaming conventions, both for the 2nd-order ones
in let-binders and the 1st-order ones in λ-abstractions. We say a proofcluster ς is
linear if every (let or λ) binding binds exactly once, and canonical [23] if, when
a binding variable occurs to the left of another such (of the same type), then
the ﬁrst bound occurrence of the former occurs before that of the latter in the
pre-order walk of the relevant proofterm.
Example 4. Let ζ′ and ξ′ be the clusters let X = λx.f(f(x)) in X(X(a)) and
let X, Y = λyz.f(f(y)), λx.f(f(x)) in Y (X(a, f(a))). Each of ς, ζ′, ξ′ denotes t
in Example 1. The cluster ς is linear and canonical, ζ′ is canonical but not linear
(X occurs twice in the body), and ξ′ is neither linear (z does not occur in f(y))
nor canonical (Y occurs outside of X in the body).
We adopt the convention that absent λ-binders are inserted linearly, canonically;
let X = f(f(x)) in X(X(a)) is ζ′. Clusters witness encompassment ·⊵[9].
Proposition 1. t ·⊵s iﬀ∃u, X s.t. let X = s in u = t and X occurs once in u.
We deﬁne the size ∥t∥of a proofterm t in a way that is compatible with encom-
passment. Formally, ∥t∥is the pair comprising the number of non-1st-order-
variable symbols in t, and the sum over the 1st-order variables x, of the square
of the number of occurrences of x in t. Then ∥t∥> ∥s∥if t ·▷s, where we (ab)use
> to denote the lexicographic product of the greater-than relation with itself,
e.g. ∥g(a, a)∥= (3,0) > ∥g(x, x)∥= (1,4) > ∥g(x, y)∥= (1,2). For a proofcluster
ς given by let x = s in t its pattern-size ς is 
i∥si∥(adding component-wise,
with empty sum (0,0)) and its body-size ς is ∥t∥. Encompassment ·⊵is at the
basis of the theory of reducibility [7, Section 3.4.2]: t is reducible by a rule ℓ→r
iﬀt ·⊵ℓ. For instance, let X = f(f(x)) in f(X(f(a))) is a witness to reducibility
of t in Example 1. We call it, or simply f(f(x)), a pattern in t.
Deﬁnition 3. Let ς be a canonical linear proofcluster let X = s in t with term t.
We say ς is a multipattern if each si is a non-variable 1st-order term, and ς is
a multistep if each si has shape ϱ(x), i.e. a rule symbol applied to a sequence of
pairwise distinct variables. If X has length 1 we drop the preﬁx ‘multi’.

Conﬂuence by Critical Pair Analysis Revisited
323
We use Φ, Ψ, Ω, . . . to range over multisteps, and φ, ψ, ω, . . . to range over steps.
Taking their denotation yields the usual multistep [16,32] and step ARSs
◦
−→
and →underlying a TRS R. These can be alternatively obtained by ﬁrst applying
src and tgt (of which only the former is guaranteed to yield a multipattern, by
left-linearity) and then taking denotations: Φsrc = Φsrc and Φtgt = Φtgt.
Pattern- and body-sizes of multipatterns are compositional.
Proposition 2. For multipatterns ς,ς if ς = ς[x:=ς]
0
with each variable among
x occurring once in the body of ς0, then ς = 
iςi, and ς ⩾ςi for all i,
with strict inequality holding in case the substitution is not a bijective renaming.
Here multipattern-substitution substitutes in the body and combines let-bindings.
Multipatterns are ordered by reﬁnement ⊑.
Deﬁnition 4. Let ς and ζ be multipatterns let X = s in t and let Y = u in w. We
say ς reﬁnes ζ and write ς ⊑ζ, if there is a 2nd-order substitution σ on Y with
wσ = t and let X = s in Yi(yi)σ = ui for all i, with yi the variables of ui.
Example 5. We have ς ⊑ς′ with ς′ being let Z = f(f(f(f(z)))) in Z(a), and
ς as in Example 3, as witnessed by the 2nd-order substitution mapping Z to
λx.X(Y (x)).
Lemma 2. ⊑is a ﬁnite distributive lattice [8] on multipatterns denoting a 1st-
order term t, with least element ⊥the empty let-expression let = in t, and great-
est element ⊤of shape let X = t′ in X(x) with x the vector of variables in t.
Proof (Idea). Although showing that ⊑is reﬂexive and transitive is easy, showing
anti-symmetry or existence of/constructions for meets ⊓and joins ⊔, directly is
not. Instead, it is easy to see that each multipattern let X = s in t is determined
by the set of the (non-empty, convex,4 pairwise disjoint) sets of node positions
of its patterns si in t, and vice versa. For instance, the multipatterns ς and
ς′ in Example 5 are determined by {{ε, 1}, {1·1, 1·1·1}} and {{ε, 1, 1·1, 1·1·1}}.
Viewing multipatterns as sets in that way ς ⊑ζ iﬀ∀P ∈ς, ∃Q ∈ζ with P ⊆Q.
Saying P, Q ∈ς ∪ζ have overlap if P ∩Q ̸= ∅, denoted by P ≬Q, characterising
meets and joins now also is easy: ς ⊓ζ = {P ∩Q | P ∈ς, Q ∈ζ, and P ≬Q}, and
ς ⊔ζ = { P≬| P ∈ς ∪ζ}, where P≬= {Q∈ς ∪ζ | P ≬∗Q}, i.e. the sets connected
to P by successive overlaps. On this set-representation ⊑can be shown to be a
ﬁnite distributive lattice by set-theoretic reasoning, using that the intersection
of two overlapping patterns is a pattern again5. For instance, ⊥is the empty set
and ⊤is the singleton containing the set of all non-variable positions in t.
⊓⊔
The (proof of the) lemma allows to freely switch between viewing multisteps and
multipatterns as let-expressions and as sets of sets of positions, and to reason
about (non-)overlap of multipatterns and multisteps in lattice-theoretic terms.
4 Here convex means that for each pair of positions p,q in the set, all positions on the
shortest path from p to q in the term tree are also in the set, cf. [32, Deﬁnition 8.6.21].
5 This fails for, e.g., connected graphs; these may fall apart into non-connected ones.

324
N. Hirokawa et al.
We show any multistep Φ can be decomposed horizontally as φ followed by Φ/φ
for any step φ∈Φ [16,29], and vertically as some vector Φ substituted in a preﬁx
Φ0 of Φ, and that peaks can be decomposed correspondingly.
Deﬁnition 5. For a pair of multipatterns ς,ζ denoting the same term its
amount of overlap6 and non-overlap is ς ⋒ζ = ς ⊓ζ respectively ς ⋓ζ = ς ⊔ζ,
we say ς,ζ is overlapping if ς ⊓ζ ̸= ⊥, and critically overlapping if moreover
ς ⊔ζ = ⊤and ς = ζ is linear. This extends to peaks s Φ ◦
←−t ◦
−→Ψ u via Φsrc
and Ψ src.
Note ς,ζ is overlapping iﬀς ⋒ζ ̸= (0,0). Critical peaks s φ←t →ψ u are classiﬁed
by comparing the root-positions pφ, pψ of their patterns with respect to the preﬁx
order ≺o, into being outer–inner (pφ ≺o pψ), inner–outer (pψ ≺o pφ), or overlay
(pψ = pφ), and induce the usual [1,9,17,19,26,32] notion of critical pair (s,u).7
Deﬁnition 6. A pair (ς′,ζ′) of overlapping patterns such that ς′, ζ′ are in the
multipatterns ς, ζ with ⊤= ς ⊔ζ, is called inner, if it is minimal among all
such pairs, comparing them in the lexicographic product of ≺o with itself, via
the root-positions of their patterns, ordering these themselves ﬁrst by ⪯o. This
extends to pairs of steps in peaks of multisteps via src.
Proposition 3. If (φ,ψ) is an inner pair for a critical peak Φ ◦
←−· ◦
−→Ψ, and
φ ∈Φ, ψ ∈Ψ contract redexes at the same position, then φ = Φ and ψ = Ψ.
For patterns and peaks of ordinary steps, their join being top, entails they are
overlapping, and the patterns in a join are joins of their constituent patterns.
Proposition 4. Linear patterns ς,ζ are critically overlapping iﬀς ⊔ζ = ⊤.
Lemma 3. If ξ = ς ⊔ζ and ς, ζ ⊑ξ are witnessed by the 2nd-order substitutions
σ, τ, for multipatterns ς and ζ given by let X = t in M and let Y = s in N, then
for all let-bindings Z = u of ξ, ⊤u = (let X = t in Z(z)σ)⊔(let Y = s in Z(z)τ).
Lemma 4 (Vertical). A peak s Φ ◦
←−t ◦
−→Ψ u of overlapping multisteps either
is critical or it can be vertically decomposed as:
s[x:=s]
0
Φ[x :=Φ ]
0
◦
←−t[x:=t]
0
◦
−→Ψ [x :=Ψ ]
0
u[x:=u]
0
for peaks si Φi ◦
←−ti
◦
−→Ψi ui with Φ⋒Ψ ⩾Φi ⋒Ψi and Φ⋓Ψ > Φi ⋓Ψi, for all i.
Let Φ, Ψ in s Φ ◦
←−t ◦
−→Ψ u be given by let X =ϱ(x) in M and let Y =θ(y) in N,
for rules ϱi(xi) : ℓi →ri and θj(yj) : gj →dj. Lemma 2 entails that if Φ, Ψ are
non-overlapping their patterns are (pairwise) disjoint, so that the join Φsrc ⊔Ψ src
is given by taking the (disjoint) union of the let-bindings: let XY = ℓg in L for
some L such that LY :=g = M and LX :=ℓ = N. We deﬁne the join8 Φ⊔Ψ and
6 For the amount of overlap for redexes in parallel reduction
−→, see e.g. [1,17,24].
7 We exclude neither overlays of a rule with itself nor pairs obtained by symmetry.
8
This does not create ambiguity with joins of multipatterns since if Φ ̸= Ψ, then
Φ ̸= Ψ unless the let-bindings of both are empty, so both are bottom.

Conﬂuence by Critical Pair Analysis Revisited
325
residual Φ/Ψ by let XY = ϱ(x)θ(y) in L respectively let X = ϱ(x) in LY :=d,
where, as substituting the right-hand sides d may lose being linear and canonical,
we implicitly canonise and linearise the latter by reordering and replicating let-
bindings. Then t ◦
−→Φ⊔Ψ · Φ/Ψ ◦
←−u, giving rise to the classical residual theory [2,
4,6,18], see [32, Section 8.7]. We let φ ∈Φ abbreviate ∃Ψ.Φ = φ ⊔Ψ.
Example 6. The steps φ and ψ given by let X = λx.ϱ(x) in X(f(f(a))) respec-
tively let X = λx.ϱ(x) in f(f(Y (a))), are non-overlapping, φ, ψ ∈ζ, φ ⊔ψ = ζ,
and f(f(g(a))) ◦
−→φ/ψ g(g(a)), for ζ and ϱ as in Example 3.
Lemma 5 (Horizontal). A peak t Φ ◦
←−· ◦
−→Ψ s of multisteps either
1. is non-overlapping and then t ◦
−→Ψ/Φ · Φ/Ψ ◦
←−s, with the rule symbols occur-
ring in Ψ/Φ contained in Ψ (and those in Φ/Ψ contained in Φ); or
2. it can be horizontally decomposed: t Φ/φ ◦
←−· φ←· →ψ · ◦
−→Ψ/ψ s for some
peak φ←· →ψ of overlapping steps φ ∈Φ and ψ ∈Ψ.
The above allows to refactor the proof of the critical pair lemma [17, Lemma 3.1]
for left-linear TRSs, as an induction on the amount of non-overlap between the
steps in the peak, such that the critical peaks form the base case:
Lemma 6. A left-linear TRS is locally conﬂuent if all critical pairs are joinable.
Proof. We show every peak =
φ←· →=
ψ of empty or single steps is joinable, by
induction on the amount of non-overlap (φ ⋓ψ) ordered by >. We distinguish
cases on whether φ, ψ are overlapping (φ ⋒ψ ̸= (0,0)) or not. If φ, ψ do not
have overlap, in particular when either φ or ψ is empty, then we conclude by
Lemma 5(1). If φ, ψ do have overlap, then by Lemma 4 the peak either
– is critical and we conclude by assumption; or
– can be (vertically) decomposed into smaller such peaks =
φi←·→=
ψi. Since these
are >-smaller, the induction hypothesis yields them joinable, from which we
conclude by reductions and joins being closed under composition.
⊓⊔
Remark 1. Apart from enabling our proof of Theorem 2 below, we think this
refactoring is methodologically interesting, as it extends to (parallel and) simul-
taneous critical pairs, then yielding, we claim, simple statements and proofs of
conﬂuence results [13,27] based on these and their higher-order generalisations.
3
Conﬂuence by Hot-Decreasingness
Linear TRSs have a critical-pair criterion for so-called rule-labelling [16,30,35]:
If all critical peaks are decreasing with respect to some rule-labelling, then the
TRS is decreasing, hence conﬂuent. We introduce the hot-labelling extending
that result to left-linear TRSs. To deal with non-right-linear rules we make use
of a rule-labelling for multisteps that is invariant under duplication, cf. [13,35].

326
N. Hirokawa et al.
Remark 2. Na¨ıve extensions fail. Non-left-linear TRSs need not be conﬂuent
even without critical pairs [32, Exercise 2.7.20]. That non-right-linear TRSs need
not be conﬂuent even if all critical peaks are decreasing for rule-labelling, is
witnessed by [16, Example 8].
Deﬁnition 7. For a TRS R, terminating subsystem C ⊆R, and labelling of
R −C-rules into a well-founded order ≻, hot-labelling ˚
L maps a multistep Φ :
t ◦
−→R s
– to the term t if Φ contains C-rules only; and
– to the set of ≻-maximal R −C-rules in Φ otherwise.
The hot-order ˚≻relates terms by →+
C , sets by ≻mul, and all sets to all terms.
Note ˚≻is a well-founded order as series composition [3] of →+
C and ≻mul, which
are well-founded orders by the assumptions on C and ≻. Taking the set of
maximal rules in a multistep makes hot-labelling invariant under duplication.
As with the notation ⋎ℓ, we denote {κ | ℓ˚≻κ} by ˚⋎ℓ, and {κ | ℓ˚⪰κ} by ˚⋎ℓ.
Deﬁnition 8. A TRS R is hot-decreasing if its critical peaks are decreasing
for the hot-labelling, for some C and ≻, such that each outer–inner critical peak
ℓ←· →for label ℓ, is decreasing by a conversion of shape (oi): ↔∗
˚
⋎ℓ· ˚
⋎ℓ◦
←−.
Theorem 2. A left-linear TRS is conﬂuent, if it is hot-decreasing.
Before proving Theorem 2, we give (non-)examples and special cases.
Example 7. Consider the left-linear TRS R:
ϱ1: nats →0 : inc(nats) ϱ3:
inc(x : y) →s(x) : inc(y)
ϱ5: hd(x : y) →x
ϱ2: d(x) →x : (x : d(x)) ϱ4: inc(tl(nats)) →tl(inc(nats)) ϱ6: tl(x : y) →y
By taking C = ∅, labelling rules by themselves, and ordering ϱ4 ≻ϱ1, ϱ3, ϱ6 the
only critical peak {ϱ4}←· →{ϱ1} can be completed into the decreasing diagram:
tl(inc(nats))
inc(tl(nats))
inc(tl(0 : inc(nats)))
tl(inc(0 : inc(nats)))
tl(s(0) : inc(inc(nats))
inc(inc(nats))
{ϱ4}
{ϱ1}
{ϱ1}
{ϱ3}
{ϱ6}
{ϱ6}
Since the peak is outer–inner, the closing conversion must be of (oi)-shape
↔∗
˚
⋎{ϱ4} · ˚
⋎{ϱ4} ◦
←−. It is, so the system is conﬂuent by Theorem 2.
Example 8. Consider the left-linear conﬂuent TRS R:
ϱ1: f(a, a) →b
ϱ3: f(c, x) →f(x, x)
ϱ5: f(c, c) →f(a, c)
ϱ2:
a →c
ϱ4: f(x, c) →f(x, x)
Since b is an R-normal form, the only way to join the outer–inner critical peak
b ϱ1←f(a, a) →ϱ2 f(c, a) is by a conversion starting with a step b ϱ1←f(a, a).
As its label must be identical to the same step in the peak, not smaller, whether

Conﬂuence by Critical Pair Analysis Revisited
327
we choose ϱ1 to be in C or not, the peak is not hot-decreasing, so Theorem 2
does not apply.
That hot-decreasingness in Theorem 2 cannot be weakened to (ordinary)
decreasingness, can be seen by considering R′ obtained by omitting ϱ5 from R.
Although R′ is not conﬂuent [16, Example 8], by taking C = ∅and ϱ1 ≻ϱ3, ϱ4,
we can show that all critical peaks of R′ are decreasing for the hot-labelling.
A special case of Theorem 2, is that a left-linear terminating TRS is conﬂu-
ent [19], if each critical pair is joinable, as can be seen by setting C = R.
Corollary 1. A left-linear development closed TRS is conﬂuent [29, Corol-
lary 24].
Proof. A TRS is development closed if for every critical pair (t,s) such that t is
obtained by an outer step, t
◦
←−s holds. Taking C = ∅and labelling all rules the
same, say by 0, yields that each outer–inner or overlay critical peak is labelled as
t {0}←· →{0} s, and can be completed as t {0} ◦
←−s, yielding a hot-decreasing
diagram of (oi)-shape. We conclude by Theorem 2.
⊓⊔
The proof of Theorem 2 uses the following structural properties of decreasing
diagrams speciﬁc to the hot-labelling. The labelling was designed so they hold.
Lemma 7. 1. If the peak s ℓ◦
←−t
◦
−→κ u is hot-decreasing, then it can be
completed into a hot-decreasing diagram of shape s ↔∗
˚
⋎ℓs′
◦
−→κ s′′ ↔∗
˚
⋎ℓκ
u′′ ℓ◦
←−u′ ↔∗
˚
⋎κ u such that the 1st-order variables in all terms in the diagram
are contained in those of t.
2. If the multisteps Φ, Ψ in the peak s Φ ◦
←−t ◦
−→Ψ u are non-overlapping, then
the valley s ◦
−→Ψ/Φ · Φ/Ψ ◦
←−u completes it into a hot-decreasing diagram.
3. If the peak s
◦
←−t
◦
−→u and vector of peaks s
◦
←−t
◦
−→u have hot-
decreasing diagrams, so does the composition s[x:=s]
◦
←−t[x:=t]
◦
−→u[x:=u].
The proof of Theorem 2 reﬁnes our refactored proof (see Lemma 6) of Huet’s
critical pair lemma, by wrapping the induction on the amount of non-overlap
(⋓) between multisteps, into an outer induction on their amount of overlap (⋒).
Proof (of Theorem 2). We show that every peak s Φ ◦
←−t
◦
−→Ψ u of multisteps
Φ and Ψ can be closed into a hot-decreasing diagram, by induction on the pair
(Φ⋒Ψ,Φ⋓Ψ) ordered by the lexicographic product of > with itself. We distinguish
cases on whether or not Φ and Ψ have overlap.
If Φ and Ψ do not have overlap, Lemma 5(1) yields s ◦
−→Ψ/Φ · Φ/Ψ ◦
←−u. This
valley completes the peak into a hot-decreasing diagram by Lemma 7(2).
If Φ and Ψ do have overlap, then we further distinguish cases on whether or
not the overlap is critical.
If the overlap is not critical, then by Lemma 4 the peak can be vertically
decomposed into a number of peaks between multisteps Φi, Ψi that have an
amount of overlap that is not greater, Φ ⋒Ψ ⩾Φi ⋒Ψi, and a strictly smaller
amount of non-overlap Φ ⋓Ψ > Φi ⋓Ψi. Hence the I.H. applies and yields that
each such peak can be completed into a hot-decreasing diagram. We conclude by
vertically recomposing them yielding a hot-decreasing diagram by Lemma 7(3).

328
N. Hirokawa et al.
If the overlap is critical, then by Lemma 5 the peak can be horizontally decom-
posed as s Φ/φ ◦
←−s′ φ←t →ψ u′
◦
−→Ψ/ψ u for some peak s′ φ←t →ψ u′ of
overlapping steps φ ∈Φ and ψ ∈Ψ, i.e. such that Φ = φ ⊔Φ′ Ψ = ψ ⊔Ψ ′ for
some Φ′, Ψ ′. We choose (φ,ψ) to be inner among such overlapping pairs (see
Deﬁnition 6), assuming w.l.o.g. that pφ ⪯o pψ for the root-positions pφ,pψ of
their patterns. We distinguish cases on whether or not pφ is a strict preﬁx of pψ.
If pφ = pψ, then φ = Φ and ψ = Ψ by Proposition 3, so the peak is overlay,
from which we conclude since such peaks are hot-decreasing by assumption.
Suppose pφ ≺o pψ. We will construct a hot-decreasing diagram D for the peak
s Φ ◦
←−t
◦
−→Ψ u out of several smaller such diagrams as illustrated in Fig. 1,
using the multipattern ς = Φsrc ⊔ψsrc as a basic building block; it has as patterns
those of Φ′ and the join of the patterns of φ, ψ. To make ς explicit, unfold Φ and
Ψ to let-expressions let X = ϱ(x) in M respectively let Y = θ(y) in N, for rules
of shapes ϱi(xi) : ℓi →ri and θj(yj) : gj →dj. We let X = X′X and Y = Y ′Y
be such that X and Y are the 2nd-order variables corresponding to φ ∈Φ and
ψ ∈Ψ for rules ϱ(x):ℓ→r and θ(y):g →d. By the choice of (φ,ψ) as inner, φsrc is
the unique pattern in Φsrc overlapping ψsrc. As a consequence we can write ς as
let X′Z = ℓ′ˆt in L, for some pattern ˆt, the join of the patterns of φ,ψ, such that σ
maps Z to a term of shape X(gψ ) as φ is the outer step, and τ maps it to a term
of shape C[Y (ℓφ)],9 where σ, τ witness Φsrc, ψsrc ⊑ς. That the other 2nd-order
variables are X′ follows by σ being the identity on them (their patterns do not
overlap ψ), and that these are bound to the patterns ℓ′ by τ mapping them to
1st-order terms (only Z can be mapped to a non-1st-order term).
Fig. 1. Outer–inner critical peak construction
9 C is a preﬁx of the left-hand side ℓof ϱ. For instance, for a peak from f(g(a)) between
ϱ : f(g(a)) →. . . and θ : g(x) →. . ., Z is mapped by σ to X() and by τ to f(Y (a)).

Conﬂuence by Critical Pair Analysis Revisited
329
We start with constructing a hot-decreasing diagram ˚
D for the critical peak
ˆs ˆφ←ˆt →ˆ
ψ ˆu encompassed by the peak between φ and ψ, as follows. We set ˆφ
and ˆψ to let X = ϱ(x) in Z(z)σ respectively let Y = θ(y) in Z(z)τ. This yields a
peak as desired, which is outer–inner as p ˆφ ≺o p ˆ
ψ by pφ ≺o pψ, and critical by
Lemma 3, hence by the hot-decreasingness assumption, it can be completed into a
hot-decreasing diagram ˆD by a conversion of (oi)-shape: ˆs ↔∗
˚
⋎˚
L( ˆφ) ˆw ˚
⋎˚
L( ˆφ) ◦
←−ˆu.
Below we refer to its conversion and multistep as ˆΨ and ˆΦ. Based on ˆD we
construct a hot-decreasing diagram D′ (Fig. 1, left) for the peak s Φ ◦
←−t →ψ u′
by constructing a conversion ˆΨ↑: s ↔∗w′′ and a multistep Φ′ ⊕ˆΦ : u′
◦
−→w′′,
with their composition (reversing the latter) of (oi)-shape.
The conversion ˆΨ↑: s ↔∗w′′ is constructed by lifting the closing conver-
sion ˆΨ of the diagram ˆD back into ς. Formally, for any multistep ˆΩ given by
let ˆ
Z = η(w) in ˆL for rules ηk(wk), occurring anywhere in ˆΨ , we deﬁne its lifting
ˆΩ↑to be let ˆ
Z = η(w) in L[X ′,Z:=r ′,ˆL]. That is, we update ς by substituting10
both ˆΩ (for Z, instead of binding that to ˆt) and the right-hand sides r′ in its
body. Because right-hand sides r need not be linear, the resulting proofclusters
may have to be linearised (by replicating let-bindings) ﬁrst to obtain multisteps.
This extends to terms p by p↑= (let = in p)↑. That this yields multisteps and
terms that connect into a conversion s = ˆs↑↔∗
ˆΨ↑ˆw↑= w′′ as desired follows by
computation. E.g., s = M [X ′,X:=r ′,r] = L[X ′,Z:=r ′,ˆs] = ˆs↑using that σ witnesses
Φsrc ⊑ς so that M = Lσ and ˆs = let X = r in Z(z)σ. That the labels in ˆΨ↑are
strictly below ˚
L(Φ) follows for set-labels from that lifting clearly does not intro-
duce rule symbols and from that labels of rule symbols in ˆΨ are, by assumption,
strictly below the label of the rule ϱ of φ. In case Φ is term-labelled, by t, it
follows from closure of →C-reduction under lifting (which also contracts Φ′).
The multistep Φ′ ⊕ˆΦ : u′
◦
−→w′′ is the combination of the multisteps Φ′ (the
redex-patterns in Φ other than φ) and ˆΦ, lifting the latter into ς. For ˆΦ : ˆu
◦
−→
ˆw given by let ˆ
X = ˆϱ(ˆx) in ˆ
M, it is deﬁned as let X′ ˆ
X = ϱ(x)′ ˆϱ(ˆx) in L[Z:= ˆ
M].
Per construction it only contracts rules in Φ′, ˆΦ, so has a label in ˚⋎˚
L(Φ) by
Φ = φ ⊔Φ′ and the label of ˆΦ is in ˚⋎˚
L(ˆφ) by the (oi)-assumption. That Φ′ ⊕
ˆΦ : u′
◦
−→w′′ follows again by computation, e.g. let X′ ˆ
X = r′ˆr in L[Z:= ˆ
M] =
L[X ′,Z:=r ′, ˆ
M [ ˆ
X :=ˆ
r ]] = L[X ′,Z:=r ′, ˆ
w] = ˆw↑= w′′.
Finally, applying the I.H. to the peak w′′
Φ′⊕ˆΦ ◦
←−u′
◦
−→Ψ/ψ u yields
some hot-decreasing diagram DIH (Fig. 1, right). Preﬁxing ˆΨ↑to its closing
conversion between w′′ and u, then closes the original peak s Φ ◦
←−t
◦
−→Ψ u
into a hot-decreasing diagram D, because labels of steps in ˆΨ↑are in ˚⋎˚
L(Φ),
˚
L(Φ) ˚⪰
˚
L(Φ′ ⊕ˆΦ) as seen above, and ˚
L(Ψ) ˚⪰
˚
L(Ψ/ψ). The I.H. applies
since Φ ⋒Ψ > (Φ′ ⊕ˆΦ) ⋒(Ψ/ψ): To see this, we deﬁne L′ = L[Z ′:=ℓ′] and
F ′ = let ˆ
X = ˆℓin L′[Z:= ˆ
M] and collect needed ingredients (the joins are disjoint):
10 For this to be a valid 2nd-order substitution, the 1st-order variables of ˆΩ (ˆL) must
be contained in those of ˆt, which we may assume by Lemma 7(1).

330
N. Hirokawa et al.
D
=
Φsrc
= (let X′ = ℓ′ in L[Z:=ˆt]) ⊔φsrc = Φ′src ⊔φsrc
E
=
Ψ src
= (let Y ′ = g′ in N [Y :=g]) ⊔ψsrc = Ψ ′src ⊔ψsrc
D′
=
(Φ′ ⊕ˆΦ)src = (let X′ = ℓ′ in L[Z:=ˆu]) ⊔F ′
E′
=
(Ψ/ψ)src
= let Y ′ = g′ in N [Y :=d]
Using these one may reason with sets of patterns (not let-expressions as t ̸= s′;
the sets are positions in both t,s′) as follows, relying on distributivity:
(D ⊓E) ⊐(D−⊓E) = (D−⊓E′) = (D′
+ ⊓E′) ⊒(D′ ⊓E′)
(1)
where F is the singleton {{p∈φsrc | pψ  p}} having all positions in φ not below
ψ’s root, D−= Φ′src ⊔F, and D′
+ = (D′ −F ′) ⊔F.
⊓⊔
4
Conﬂuence by Critical-Pair Closing Systems
We introduce a conﬂuence criterion based on identifying for a term rewrite sys-
tem R a subsystem C such that every R-critical peak can be closed by means
of C-conversions, rendering the rules used in the peak redundant.
Deﬁnition 9. A TRS C is critical-pair closing for a TRS R, if C is a subsystem
of R (namely C ⊆R) and s ↔∗
C t holds for all critical pairs (s,t) of R.
We phrase the main result of this section as a preservation-of-conﬂuence result.
We write →S/R for ↠R · →S · ↠R, and if it is terminating, S/R is said to be
(relatively) terminating. By Cd we denote the set of all duplicating rules in C.
Theorem 3. If C is a critical-pair-closing system for a left-linear TRS R such
that Cd/R is terminating, then R is conﬂuent if C is conﬂuent.
Any left-linear TRS is critical-pair-closing for itself. However, the power of the
method relies on choosing small C. Before proving Theorem 3, we illustrate it
by some (non-)examples and give a special case.
Example 9. Consider again the TRS R in Example 7. As we observed, the only
critical pair originating from ϱ4 and ϱ1 is closed by →ϱ1 ·→ϱ3 ·→ϱ6 · ϱ6←. So the
subsystem C = {ϱ1, ϱ3, ϱ6} is a critical-pair-closing system for R. As all C-rules
are linear, Cd/R is vacuously terminating. Thus, by Theorem 3 it is suﬃcient
to show conﬂuence of C. Because C has no critical pairs, the empty TRS ∅is a
critical-pair-closing TRS for C. As ∅/C is terminating, conﬂuence of C follows
from that of ∅, which is trivial.
Observe how conﬂuence was shown by successive applications of the theorem.
Remark 3. In our experiments (see Sect. 5), 3
4 of the TRSs proven conﬂuent by
means of Theorem 3 used more than 1 iteration, with the maximum number of
iterations being 6. For countable ARSs (see Corollary 2 below) 1 iteration suﬃces,
which can be seen by setting C to the spanning forest obtained by Lemma 1. This
provides the intuition underlying rule specialisation in Example 15 below.

Conﬂuence by Critical Pair Analysis Revisited
331
Example 10. Although conﬂuent, the TRS R in Example 8 does not have any
conﬂuent critical-pair-closing subsystem C such that Cd/R is terminating, not
even R itself: Because of b being in normal form in the critical pair induced by
b ϱ1←f(a, a) →ϱ2 f(a, c), any such subsystem must contain ϱ4, as one easily
veriﬁes, but ϱ4 is both duplicating and non-terminating (looping).
Note that the termination condition of Cd/R cannot be omitted from The-
orem 3. Although the TRS R′ in Example 8 is not conﬂuent, it admits the con-
ﬂuent critical-pair-closing system {ϱ1, ϱ3, ϱ4}.
Remark 4. The example is taken from [16] where it was used to show that
decreasingness of critical peaks need not imply that of all peaks, for rule
labelling. That example, in turn was adapted from L´evy’s TRS in [17] show-
ing that strong conﬂuence need not imply conﬂuence for left-linear TRSs.
Example 11. For self-joinable rules, i.e. rules that are self-overlapping and whose
critical pairs need further applications of the rule itself to join, Theorem 3 is
not helpful since the critical-pair-closing system C then contains the rule itself.
Examples of self-joinable rules are associativity (x · y) · z →x · (y · z) and self-
distributivity (x·y)·z →(x·z)·(y ·z), with conﬂuence of the latter being known
to be hard (currently no tool can handle it automatically).11
The special case we consider is that of TRSs that are ARSs, i.e. where all
function symbols are nullary. The identiﬁcation is justiﬁed by that any ARS
in the standard sense [26,32] can be presented as →R for the TRS R having a
nullary symbol for each object, and a rule for each step of the ARS. Since ARSs
have no duplicating rules, Theorem 3 specialises to the following result.
Corollary 2. If C is critical-pair-closing for ARS R, R is conﬂuent if C is.
Example 12. Consider the TRS R given by c →a′ →a →b and a →a′ →c. It
is an ARS having the critical-pair-closing system C given by the ﬁrst part c →
a′ →a →b. Since C is orthogonal it is conﬂuent by Corollary 2, so R is conﬂuent
by the same corollary. In general, a conﬂuent ARS may have many non-conﬂuent
critical-pair-closing systems. Requiring local conﬂuence is no impediment to that:
The subsystem C′ of R obtained by removing c →a′ allows to join all R-critical
peaks, but is not conﬂuent; it simply is Kleene’s example [32, Figure 1.2] showing
that local conﬂuence need not imply conﬂuence.
For Cd/R to be vacuously terminating it is suﬃcient that all rules are linear.
Example 13. Consider the linear TRS R consisting of ρ1 : f(x) →f(f(x)),
ρ2 : f(x) →g(x), and ρ3 : g(x) →f(x). The subsystem C = {ρ1, ρ3} is critical-
pair-closing and has no critical pairs, so R is conﬂuent.
From the above it is apparent that, whereas usual redundancy-criteria are based
on rules being redundant, the theorem gives a suﬃcient criterion for peaks of
steps being redundant. This allows one to leverage the power of extant conﬂuence
methods. Here we give a generalisation of Huet’s strong closedness theorem [17]
as a corollary of Theorem 3.
11 See problem 127 of http://cops.uibk.ac.at/results/?y=2019-full-run&c=TRS.

332
N. Hirokawa et al.
Deﬁnition 10. A TRS R is strongly closed [17] if s ↠R · =
R←t and s →=
R
· R↞t hold for all critical pairs (s,t).
Corollary 3. A left-linear TRS R is conﬂuent if there exists a critical-pair-
closing system C for R such that C is linear and strongly closed.
Example 14. Consider the linear TRS R:
ϱ1: h(f(x, y)) →f(h(r(x)), y)
ϱ2: f(x, k(y, z)) →g(p(y), q(z, x))
ϱ3: h(q(x, y)) →q(x, h(r(y)))
ϱ4: q(x, h(r(y))) →h(q(x, y))
ϱ5: h(g(x, y)) →g(x, h(y))
ϱ6: a(x, y, z) →h(f(x, k(y, z)))
ϱ7:
a(x, y, z) →g(p(y), q(z, h(r(x))))
C
=
{ϱ1, . . . , ϱ5} is critical-pair-closing for R, since the R-critical peak
between ϱ6 and ϱ7 can be C-closed: h(f(x, k(y, z))) →ϱ1 f(h(r(x)), k(y, z)) →ϱ2
g(p(y), q(z, h(r(x)))). Because C is strongly closed and also linear, conﬂuence of
R follows by Corollary 3.
Remark 5. Neither of the TRSs in Examples 13 and 14 is strongly closed. The
former not, because f(f(x)) ↠R · =
R←g(x) does not hold, and the latter not
because g(p(y), q(z, h(r(x)))) ↠R · =
R←h(f(x, k(y, z))) does not hold.
Having illustrated the usefulness of Theorem 3, we now present its proof. In
TRSs there are two types of peaks: overlapping and non-overlapping ones. As
Example 10 shows, conﬂuence criteria only addressing the former need not gen-
eralise from ARSs to TRSs. Note that one of the peaks showing non-conﬂuence
of R′, the one between ϱ2 and ϱ3 (ϱ4), is non-overlapping. Therefore, restricting
to a subsystem without ϱ2 can only provide a partial analysis of conﬂuence of
R′; the (non-overlapping) interaction between C and R −C is not accounted for,
and indeed that is fatal here. The intuition for our proof is that the problem is
that the number of such interactions is unbounded due to the presence of the
duplicating and non-terminating rule ϱ3 (and ϱ4) in C, and that requiring ter-
mination of Cd/R bounds that number and suﬃces to regain conﬂuence. This
is veriﬁed by showing that ↠C · ◦
−→R has the diamond property.
Lemma 8. Let →A = 
a∈I →a be a relation equipped with a well-founded order
≻on a label set I, and let →B be a conﬂuent relation with →B ⊆↠A. The
relation →A is conﬂuent if
1. a←· →b ⊆(→A · A←) ∪
{a,b}≻mul{a′,b′}(a′←· ↔∗
B · →b′) for all a, b ∈I; and
2. a←· →B ⊆(↠B · a←) ∪
a≻a′(↠B · a′←· ↔∗
B) for all a ∈I.
Here ≻mul stands for the multiset extension of ≻.
Proof (Sketch). Let ↣= ↠B · →A. We claim that a←· →m
B · n
B←· →b ⊆↣· ↢
holds for all labels a, b and numbers m, n ⩾0. The claim is shown by well-
founded induction on ({a, b}, m + n) with respect to the lexicographic product
of ≻mul and the greater-than order > on N. Thus, the diamond property of ↣
follows from the claim and conﬂuence of B. As →A ⊆↣⊆↠A, we conclude
conﬂuence of A by e.g. [32, Proposition 1.1.11].

Conﬂuence by Critical Pair Analysis Revisited
333
Proof (of Theorem 3 by Lemma 8). Let I comprise pairs of a term and a natural
number, and deﬁne t →(ˆt,n) s if ˆt ↠R t
◦
−→R s with n the maximal length of
a development of the multistep,12 and →B = →C, in Lemma 8. As well-founded
order ≻on indices we take the lexicographic product of Cd/R and greater-than
>. We only present the interesting case, leaving the others to the reader:
– Suppose s (ˆt,n)←t →C u where the steps do not have overlap. Then by
Lemma 5(1), s
◦
−→C · R ◦
←−u, so s ↠C · R ◦
←−u. Distinguish cases on the
type of the C-rule employed in t →C u.
If the rule is duplicating, then s ↠C · (u,m)←u for m the maximal length of a
development of the
◦
−→R-step from u, and condition 2 is satisﬁed as t →Cd u
implies (ˆt,n) ⪰(u,m).
If the rule is non-duplicating, then s ↠C · (ˆt,n)←u as ˆt ↠R t →R u by
assumption and the length of the maximal development of the residual multi-
step does not increase when projecting over a linear rule. Again, condition 2
is satisﬁed.
⊓⊔
5
Implementation and Experiments
The presented conﬂuence techniques have been implemented in the conﬂuence
tool Saigawa version 1.12 [14]. We used the tool to test the criteria on 432 left-
linear TRSs in COPS [15] Nos. 1–1036, where we ruled out duplicated problems.
Out of 432 systems, 224 are known to be conﬂuent and 173 are non-conﬂuent.
We brieﬂy explain how we automated the presented techniques. As illustrated
in Example 9, Theorem 3 can be used as a stand-alone criterion. The condition
s →∗
R · =
R←t of strong closedness is tested by s →⩽5
R
· =
R←t. For a critical
peak s ←· →t of C, hot-decreasingness is tested by s ↠C · C↞t. For any
other critical peak s ℓ←· →t, we test the disjunction of s →⩽5
˚
⋎ℓ· ˚
⋎ℓ◦
←−t and
s ↠C · ˚
⋎ℓ◦
←−t if it is outer–inner one, and if it is overlay, the disjunction of
s →⩽5
˚
⋎ℓ· ˚
⋎ℓ◦
←−t and s ↠C · C↞t is used. Order constraints for hot-labeling are
solved by SMT solver Yices [10]. For proving (relative) termination we employ
the termination tool NaTT version 1.8 [34]. Finally, suitable subsystems C used
in our criteria are searched by enumeration.
Table 1 gives a summary of the results.13 The tests were run on a PC equipped
with Intel Core i7-8500Y CPU (1.5 GHz) and 16 GB memory using a timeout of
60 seconds. For the sake of comparison we also tested Knuth and Bendix’ theorem
(kb), the strong closedness theorem (sc), and development closedness theorem
(dc). As theoretically expected, they are subsumed by their generalizations.
6
Conclusion and Future Work
We presented two methods for proving conﬂuence of TRSs, dubbed critical-pair-
closing systems and hot-decreasingness. We gave a lattice-theoretic characterisa-
tion of overlap. Since many results in term rewriting, and beyond, are based on
12 By the Finite Developments Theorem lengths of such developments are ﬁnite [32].
13 Detailed data are available from: http://www.jaist.ac.jp/project/saigawa/19cade/.

334
N. Hirokawa et al.
Table 1. Experimental results
Theorem 2 Theorem 3 Corollary 3
kb
dc
sc
# proved (# timeouts)
101 (46)
81 (24)
94 (15)
45 (18) 34 (1) 62 (1)
reasoning about overlap, which is notoriously hard [24], we expect that formal-
ising our characterisation could simplify or even enable formalising them. We
expect that both methods generalise to commutation, extend to HRSs [21], and
can be strengthened by considering rule specialisations.
Example 15. Analysing the TRS R of Example 10 one observes that for clos-
ing the critical pairs only (non-duplicating) instances of the duplicating
rules ϱ3 and ϱ4 are used. Adjoining these specialisations allows the method
to proceed: Adjoining ϱ3(a) : f(c, a) →f(a, a) and ϱ4(a) : f(a, c) →f(a, a)
to R yields a (reduction-equivalent) TRS having critical-pair-closing system
{ϱ1, ϱ3(a), ϱ4(a), ϱ5}. Since this is a linear system without critical pairs, it is
conﬂuent, so R is as well.
References
1. Baader, F., Nipkow, T.: Term Rewriting and All That. Cambridge University Press,
Cambridge (1998)
2. Barendregt, H.: The Lambda Calculus: Its Syntax and Semantics, Studies in Logic
and the Foundations of Mathematics, vol. 103. North-Holland (1985)
3. Bechet, D., de Groote, P., Retor´e, C.: A complete axiomatisation for the inclusion
of series-parallel partial orders. In: Comon, H. (ed.) RTA 1997. LNCS, vol. 1232,
pp. 230–240. Springer, Heidelberg (1997). https://doi.org/10.1007/3-540-62950-
5 74
4. Boudol, G.: Computational semantics of term rewriting systems. In: Nivat, M.,
Reynolds, J. (eds.) Algebraic Methods in Semantics, pp. 169–236. Cambridge Uni-
versity Press (1985)
5. Bundy, A., Basin, D., Hutter, D., Ireland, A.: Rippling: meta-level guidance for
mathematical reasoning. In: Cambridge Tracts in Theoretical Computer Science,
Cambridge University Press (2005). https://doi.org/10.1017/CBO9780511543326
6. Church, A., Rosser, J.: Some properties of conversion. Transact. Am. Math. Soc.
39, 472–482 (1936)
7. Comon, H., et al.: Tree Automata Techniques and Applications (2007). http://
www.grappa.univ-lille3.fr/tata
8. Davey, B., Priestley, H.: Introduction to Lattices and Order. Cambridge University
Press, Cambridge (1990)
9. Dershowitz, N., Jouannaud, J.P.: Rewrite systems. In: van Leeuwen, J. (ed.) Hand-
book of Theoretical Computer Science, vol. B, Formal Models and Semantics, pp.
243–320. Elsevier (1990)
10. Dutertre, B.: Yices 2.2. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559,
pp. 737–744. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08867-
9 49

Conﬂuence by Critical Pair Analysis Revisited
335
11. Endrullis, J., Klop, J., Overbeek, R.: Decreasing diagrams with two labels are
complete for conﬂuence of countable systems. In: Proceedings of 3rd FSCD. LIPIcs,
vol. 108, pp. 14:1–14:15 (2018). https://doi.org/10.4230/LIPIcs.FSCD.2018.14
12. Felgenhauer, B., Middeldorp, A., Zankl, H., van Oostrom, V.: Layer systems for
proving conﬂuence. ACM Transact. Computat. Logic 16, 1–32 (2015)
13. Felgenhauer, B.: Labeling multi-steps for conﬂuence of left-linear term rewrite sys-
tems. In: Tiwari, A., Aoto, T. (eds.) Proceedings of 4th IWC, pp. 33–37 (2015)
14. Hirokawa, H., Klein, D.: Saigawa: A conﬂuence tool. In: Proceedings of 1st IWC,
p. 49 (2012). http://www.jaist.ac.jp/project/saigawa/
15. Hirokawa, N., Nagele, J., Middeldorp, A.: Cops and CoCoWeb: infrastructure for
conﬂuence tools. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018.
LNCS (LNAI), vol. 10900, pp. 346–353. Springer, Cham (2018). https://doi.org/
10.1007/978-3-319-94205-6 23
16. Hirokawa, N., Middeldorp, A.: Decreasing diagrams and relative termination. J.
Autom. Reasoning 47(4), 481–501 (2011)
17. Huet, G.: Conﬂuent reductions: abstract properties and applications to term rewrit-
ing systems. J. ACM 27(4), 797–821 (1980)
18. Huet, G., L´evy, J.J.: Computations in orthogonal rewriting systems, I. In: Lassez,
J.L., Plotkin, G. (eds.) Computational Logic: Essays in Honor of Alan Robinson,
chap. 11. The MIT Press (1991)
19. Knuth, D.E., Bendix, P.B.: Simple word problems in universal algebras. In: Leech,
J. (ed.) Computational Problems in Abstract Algebra, Proceedings of a Conference
held at Oxford under the Auspices of the Science Research Council Atlas Computer
Laboratory, 29 August–2 September 1967. pp. 263–297 (1970)
20. Liu, J.L.: Propri´et´es de Conﬂuence des R`egles de R´e´ecriture par des Diagrammes
D´ecroissants. Ph.D. thesis, Tsinghua University and l’Universit´e Paris-Saclay
pr´epar´ee `a l’´Ecole Polytechnique (2016)
21. Mayr, R., Nipkow, T.: Higher-order rewrite systems and their conﬂuence. Theoret.
Comput. Sci. 192(1), 3–29 (1998). https://doi.org/10.1016/S0304-3975(97)00143-
6
22. Meseguer, J.: Conditional rewriting logic as a uniﬁed model of concurrency. The-
oret. Comput. Sci. 96, 73–155 (1992)
23. M´etivier, Y.: About the rewriting systems produced by the Knuth–Bendix com-
pletion algorithm. Inf. Process. Lett. 16(1), 31–34 (1983)
24. Nagele, J., Middeldorp, A.: Certiﬁcation of classical conﬂuence results for left-
linear term rewrite systems. In: Blanchette, J.C., Merz, S. (eds.) ITP 2016. LNCS,
vol. 9807, pp. 290–306. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
43144-4 18
25. Newman, M.: On theories with a combinatorial deﬁnition of equivalence. Ann.
Math. 43(2), 223–243 (1942)
26. Ohlebusch, E.: Advanced Topics in Term Rewriting. Springer, Heidelberg (2002).
https://doi.org/10.1007/978-1-4757-3661-8
27. Okui, S.: Simultaneous critical pairs and Church-Rosser property. In: Nipkow, T.
(ed.) RTA 1998. LNCS, vol. 1379, pp. 2–16. Springer, Heidelberg (1998). https://
doi.org/10.1007/BFb0052357
28. van Oostrom, V.: Conﬂuence for Abstract and Higher-Order Rewriting. Ph.D.
thesis, Vrije Universiteit, Amsterdam, March 1994
29. van Oostrom, V.: Developing developments. Theoret. Comput. Sci. 175(1), 159–
181 (1997)

336
N. Hirokawa et al.
30. van Oostrom, V.: Conﬂuence by decreasing diagrams. In: Voronkov, A. (ed.) RTA
2008. LNCS, vol. 5117, pp. 306–320. Springer, Heidelberg (2008). https://doi.org/
10.1007/978-3-540-70590-1 21
31. Rosen, B.: Tree-manipulating systems and Church-Rosser theorems. J. ACM 20,
160–187 (1973)
32. Terese: Term Rewriting Systems. Cambridge University Press (2003)
33. Toyama, Y.: On the Church-Rosser property for the direct sum of term rewriting
systems. J. ACM 34(1), 128–143 (1987)
34. Yamada, A., Kusakari, K., Sakabe, T.: Nagoya termination tool. In: Dowek, G.
(ed.) RTA 2014. LNCS, vol. 8560, pp. 466–475. Springer, Cham (2014). https://
doi.org/10.1007/978-3-319-08918-8 32
35. Zankl, H., Felgenhauer, B., Middeldorp, A.: Labelings for decreasing diagrams. J.
Autom. Reasoning 54(2), 101–133 (2015)

Composing Proof Terms
Christina Kohl(B)
and Aart Middeldorp
Department of Computer Science, University of Innsbruck, Innsbruck, Austria
{christina.kohl,aart.middeldorp}@uibk.ac.at
Abstract. Proof terms are a useful concept for comparing computations
in term rewriting. We analyze proof terms with composition, with an eye
towards automation. We revisit permutation equivalence and projection
equivalence, two key notions presented in the literature. We report on
the integration of proof terms with composition into ProTeM, a tool for
manipulating proof terms.
Keywords: Proof terms · Term rewriting · Automation
1
Introduction
Proof terms represent proofs in rewriting logic [4,5]. Because proof terms are
terms, they are subject to techniques common in automated reasoning, like ter-
mination orders and critical pair analysis. In term rewriting proof terms are
used to study equivalence of reductions [6,7] and for conﬂuence analysis [2]. In
[7, Chapter 8] ([6] is a condensed version) van Oostrom and de Vrijer present
a thorough study of ﬁve diﬀerent notions of equivalence and argue that these
are equivalent. Proof terms play a key role in three of these notions: permuta-
tion equivalence, parallel standardization equivalence and projection equivalence.
In this paper we take a fresh look at permutation equivalence and projection
equivalence, from the viewpoint of automation. This leads to a new understand-
ing of the rewrite properties of the important residual operation. In particular,
we show the analysis in [6,7] of the residual operation to be incorrect.
We implemented decision procedures for permutation equivalence and pro-
jection equivalence in ProTeM, a recent tool [3] for manipulating proof terms.
Automating permutation equivalence is non-trivial since the rewrite system for
parallel standardization is only complete modulo structural equivalence. The lat-
ter is a weaker notion of equivalence that is easily decidable by means of a
conﬂuent and terminating rewrite system, but no rewrite system is known that
avoids rewriting modulo.
In the next section we recall proof terms and deﬁne structural equivalence.
Permutation equivalence is the topic of Sect. 3. In Sect. 4 we study the residual
operation on proof terms and the related notions of projection order and pro-
jection equivalence. We present a variant of the residual system deﬁned in [7,
This research is supported by FWF (Austrian Science Fund) project P27528.
c
⃝The Author(s) 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 337–353, 2019.
https://doi.org/10.1007/978-3-030-29436-6_20

338
C. Kohl and A. Middeldorp
Deﬁnition 8.7.54 and proof of Theorem 8.7.57] and [6, Deﬁnition 6.9 and proof
of Theorem 6.12]. By imposing an innermost evaluation strategy, we ensure that
our rewrite system has a well-deﬁned rewrite semantics. We establish (inner-
most) conﬂuence and termination, and use these properties to deﬁne projection
order and projection equivalence. The extensions to ProTeM are described in
Sect. 5 before we conclude in Sect. 6.
We assume familiarity with ﬁrst-order term rewriting [1,7] but knowledge of
proof terms is not required. All deﬁnitions needed for this paper are given. Much
more information on proof terms and notions of equivalence can be found in [7,
Chapter 8]. Throughout the paper we deal with left-linear rewrite systems.
2
Proof Terms
Before formally deﬁning proof terms, we give a motivating example that demon-
strates their use. This example will reappear many times throughout the paper
to illustrate the concepts we discuss.
Example 1. Consider the following TRS representing the necessary steps of com-
puting the disjunctive normal form of a propositional formula:
α
¬(x ∧y) →¬x ∨¬y
γ
x ∧(y ∨z) →(x ∧y) ∨(x ∧z)
β
¬(x ∨y) →¬x ∧¬y
δ
(x ∨y) ∧z →(x ∧z) ∨(y ∧z)
ε
¬¬x →x
As illustrated by the diagram below there are 13 diﬀerent rewrite sequences from
s = ¬(x ∨¬(y ∨z)) to t = (¬x ∧y) ∨(¬x ∧z). If we want to compare them,
for example to determine if some of them are equivalent, we can translate them
into proof terms and do our analysis in the well-known realm of terms.
4
3
5
2
0
6
1
s
8
7
9
t
0 = ¬(x ∨(¬y ∧¬z))
1 = ¬x ∧¬¬(y ∨z)
2 = ¬x ∧¬(¬y ∧¬z)
3 = ¬x ∧(¬¬y ∨¬¬z)
4 = (¬x ∧¬¬y) ∨(¬x ∧¬¬z)
5 = ¬x ∧(y ∨¬¬z)
6 = ¬x ∧(¬¬y ∨z)
7 = ¬x ∧(y ∨z)
8 = (¬x ∧y) ∨(¬x ∧¬¬z)
9 = (¬x ∧¬¬y) ∨(¬x ∧z)
We refer to a speciﬁc sequence from s to t by listing the numbers of the interme-
diate terms. For instance, the sequence s →¬x ∧¬¬(y ∨z) →¬x ∧(y ∨z) →t
is named 17.

Composing Proof Terms
339
Proof terms are built from function symbols, variables, rule symbols as well as
the binary composition operator ; which is used in inﬁx notation. Rule symbols
represent rewrite rules and have a ﬁxed arity which is the number of diﬀerent
variables in the represented rule. We use Greek letters (α, β, γ, . . . ) as rule sym-
bols, and uppercase letters (A, B, C, . . . ) for proof terms. We can represent any
rewrite sequence −→∗by a suitable proof term. A proof term without composi-
tion represents a multi-step ( ◦
−→), a proof term without composition and nested
rule symbols represents a parallel step ( ∥−→), and a proof term without compo-
sition and only one rule symbol represents a single step (−→). If a proof term
contains neither compositions nor rule symbols, it denotes an empty step (=).
If α is a rule symbol then lhsα (rhsα) denotes the left-hand (right-hand) side of
the rewrite rule represented by α. Furthermore varα denotes the list (x1, . . . , xn)
of variables appearing in α in some ﬁxed order. The length of this list is the
arity of α. Given a rule symbol α with varα = (x1, . . . , xn) and proof terms
A1, . . . , An, we write ⟨A1, . . . , An⟩α for the substitution {xi 	→Ai | 1 ⩽i ⩽n}.
A proof term A witnesses a rewrite sequence from its source src(A) to its target
tgt(A), which are computed as follows:
src(x) = tgt(x) = x
src(A ; B) = src(A)
tgt(A ; B) = tgt(B)
src(f(A1, . . . , An)) = f(src(A1), . . . , src(An))
src(α(A1, . . . , An)) = lhsα⟨src(A1), . . . , src(An)⟩α
tgt(f(A1, . . . , An)) = f(tgt(A1), . . . , tgt(An))
tgt(α(A1, . . . , An)) = rhsα⟨tgt(A1), . . . , tgt(An)⟩α
Here f is an n-ary function symbol. The expression lhsα⟨src(A1), . . . , src(An)⟩α
denotes the result of replacing every variable xi in the left-hand side of α with
the source of the corresponding argument Ai of α. We assume tgt(A) = src(B)
whenever the composition A ; B is used in a proof term. Proof terms A and
B are co-initial if they have the same source. We omit parentheses in nested
compositions in examples for better readability, assuming association to the
right of the composition operator.
Example 2. The sequence 17 in Example 1 is represented by the proof term
β(x, ¬(y ∨z)) ; ¬x ∧ϵ(y ∨z) ; γ(¬x, y, z). For the proof term A = α(ϵ(x), ¬ϵ(x))
we have src(A) = ¬(¬¬x ∧¬¬¬x) and tgt(A) = ¬(x ∨¬x). The proof term
β(x, β(y, z)) ; ¬x ∧α(¬y, ¬z) ; γ(¬x, ε(y), ε(z))
represents the sequence s ◦
−→2 →3 ◦
−→t, which can be viewed as a compact
version of 12348 and several other rewrite sequences from s to t. The expression
A ; β(x, x) is not a proof term since src(β(x, x)) = ¬(x ∨x) ̸= tgt(A).
Structural equivalence [7, Deﬁnition 8.3.1] equates proof terms that only diﬀer
in the left-to-right order in which steps are executed.

340
C. Kohl and A. Middeldorp
Deﬁnition 1. The structural identities consist of the following four equation
schemas:
A ; t ≈A
(1)
t ; A ≈A
(2)
(A ; B) ; C ≈A ; (B ; C)
(3)
f(A1, . . . , An) ; f(B1, . . . , Bn) ≈f(A1 ; B1, . . . , An ; Bn)
(4)
Here t denotes a term without rule symbols and composition whereas f denotes an
arbitrary function symbol in the underlying TRS. The induced congruence rela-
tion ≡on proof terms is called structural equivalence. The instances of scheme
(4) are known as functorial identities.
Structural equivalence is easily decidable by rewriting proof terms.
Deﬁnition 2. The canonicalization TRS consists of the following rule schemas:
A ; t →A
(5)
t ; A →A
(6)
(A ; B) ; C →A ; (B ; C)
(7)
f(A1, . . . , An) ; f(B1, . . . , Bn) →f(A1 ; B1, . . . , An ; Bn)
(8)
f(A1, . . . , An) ; (f(B1, . . . , Bn) ; C) →f(A1 ; B1, . . . , An ; Bn) ; C
(9)
Normal forms of the canonicalization TRS are called canonical.
Example 3. Returning to Example 1, the proof terms
(¬x ∧ϵ(y)) ∨(¬x ∧¬¬z) ; (¬x ∧y) ∨(¬x ∧ϵ(z)) ; (¬x ∧y) ∨(¬x ∧z)
(¬x ∧¬¬y) ∨(¬x ∧ϵ(z)) ; (¬x ∧ϵ(y)) ∨(¬x ∧z) ; (¬x ∧y) ∨(¬x ∧z)
are structurally equivalent because both rewrite to the canonical proof term
(¬x ∧ϵ(y)) ∨(¬x ∧ϵ(z))
Theorem 1. Canonical proof terms are unique representatives of structural
equivalence classes.
⊓⊔
A proof sketch is given in [7, Exercise 8.3.6]. We remark that automatic tools
for proving conﬂuence and termination are not applicable here since the rules in
Deﬁnition 2 are rule schemas; for every function symbol f in the signature and
every term t of the underlying TRS, the rule schemas are suitably instantiated
to obtain a concrete (and inﬁnite) rewrite system that operates on proof terms
of the underlying TRS. Nevertheless, standard conﬂuence and termination tech-
niques are readily applicable. In particular, schema (9) is added to make the
critical pair between (7) and (8) convergent.

Composing Proof Terms
341
3
Permutation Equivalence
Adjacent steps in which the contracted redexes are at parallel positions can be
swapped, which is captured by structural equivalence. Permutation equivalence
[7, Deﬁnition 8.3.1] extends this by also allowing swapping adjacent steps in
which the contracted redexes are above each other. This is similar to the variable
overlap case in the well-known critical pair lemma.
Deﬁnition 3. The permutation identities consist of the structural identities of
Deﬁnition 1 together with the following two equation schemas:
α(A1, . . . , An) ≈lhsα⟨A1, . . . , An⟩α ; α(t1, . . . , tn)
(10)
α(A1, . . . , An) ≈α(s1, . . . , sn) ; rhsα⟨A1, . . . , An⟩α
(11)
Here src(Ai) = si and tgt(Ai) = ti and thus si and ti are terms without rule
symbols and compositions, for i = 1, . . . , n. Furthermore, α ranges over the rule
symbols of the underlying TRS. The induced congruence relation on proof terms
is denoted by ∼= and called permutation equivalence. The permutation order ⊑
is deﬁned as follows: A ⊑B if there exists a proof term C such that A ; C ∼= B.
Example 4. We have ¬x ∧(ε(y) ∨ε(z)) ; γ(¬x, y, z) ∼= γ(¬x, ε(y), ε(z)) by an
application of (10) from right to left (with α = γ, A1 = ¬x, A2 = ε(y),
and A3 = ε(z)). Hence ¬x ∧(ε(y) ∨ε(z)) ⊑γ(¬x, ε(y), ε(z)). Furthermore,
γ(¬x, ε(y), ε(z)) ∼= γ(¬x, ¬¬y, ¬¬z) ; (¬x ∧ε(y)) ∨(¬x ∧ε(z)) by using (11).
The following lemma generalizes the deﬁning Eqs. (10) and (11). In Pro-
TeM we use the second equation to move compositions inside arguments of
rule symbols outside, which is necessary for translating proof terms into rewrite
sequences.
Lemma 1. For arbitrary proof terms A1, . . . , An and B1, . . . , Bn:
α(A1 ; B1, . . . , An ; Bn) ∼= lhsα⟨A1, . . . , An⟩α ; α(B1, . . . , Bn)
α(A1 ; B1, . . . , An ; Bn) ∼= α(A1, . . . , An) ; rhsα⟨B1, . . . , Bn⟩α
Proof. To simplify the notation, we assume the arity n of α equals 1:
α(A1 ; B1) ∼= lhsα⟨A1 ; B1⟩α ; α(tgt(B1))
(10)
∼= (lhsα⟨A1⟩α ; lhsα⟨B1⟩α) ; α(tgt(B1))
(⋆)
∼= lhsα⟨A1⟩α ; (lhsα⟨B1⟩α ; α(tgt(B1)))
(3)
∼= lhsα⟨A1⟩α ; α(B1)
(10)
α(A1 ; B1) ∼= α(src(A1)) ; rhsα⟨A1 ; B1⟩α
(11)
∼= α(src(A1)) ; (rhsα⟨A1⟩α ; rhsα⟨B1⟩α)
(⋆)
∼= (α(src(A1)) ; rhsα⟨A1⟩α) ; rhsα⟨B1⟩α
(3)
∼= α(A1) ; rhsα⟨B1⟩α
(11)

342
C. Kohl and A. Middeldorp
In the steps labeled (⋆) we use equation (4) repeatedly, depending on the struc-
ture of lhsα and rhsα.
⊓⊔
The following lemma captures the connection between permutation equiv-
alence and permutation order, a result that is mentioned in passing after the
permutation order is introduced in [7, Deﬁnition 8.3.1].
Lemma 2. For proof terms A and B, A ∼= B if and only if both A ⊑B and
B ⊑A.
⊓⊔
Standard Reductions are unique representatives of permutation equivalence
classes, that are obtained by sorting rewrite steps in an outside-in and left-
to-right order. For transforming reductions to outside-in order, called parallel
standard form, the authors in [7, Section 8.5] propose two diﬀerent approaches
based on selection sort and insertion sort respectively. Since the latter, discussed
in [7, Section 8.5.3], relies on proof terms it is of particular interest to us. Stan-
dard reductions are then obtained from parallel standard ones by imposing a
left-to-right order when evaluating parallel steps.
Deﬁnition 4. The parallel standardization TRS consists of the following
rewrite schemas:
lhsα⟨A1, . . . , An⟩α ; α(t1, . . . , tn) →α(A1, . . . , An)
(12)
α(A1, . . . , An) →α(s1, . . . , sn) ; rhsα⟨A1, . . . , An⟩α
(13)
These rules are applied modulo structural equivalence. The conditions on the
symbols are the same as in Deﬁnition 3, but additionally we demand that in (13)
at least one of A1, . . . , An is not structurally equivalent to a proof term without
rules symbols. A proof term is parallel standard if it is in normal form with
respect to parallel standardization.
Parallel standardness is invariant with respect to structural equivalence by
deﬁnition. As shown in the example below, structural equivalence is needed to
move intermediate parallel reductions out of the way such that steps in the
wrong order become adjacent. In particular, using canonical forms as represen-
tatives of structural equivalence classes, is not suﬃcient to compute parallel
standard forms. This considerably complicates the automation of permutation
equivalence.
Example 5. Consider A = ε(x) ∧¬(y ∧z) ; x ∧α(y, z) ; γ(x, ¬y, ¬z). The inner
step ε(x)∧¬(y ∧z) does not contribute to the outer step γ(x, ¬y, ¬z) and hence
these two steps need to be swapped to obtain a parallel standard normal form.
To be able to apply the rules of the parallel standardization TRS, we ﬁrst make
the steps adjacent by moving the second step x ∧α(y, z) out of the way with an
appeal to structural equivalence:

Composing Proof Terms
343
A ≡ε(x) ∧α(y, z) ; γ(x, ¬y, ¬z)
≡¬¬x ∧α(y, z) ; ε(x) ∧(¬y ∨¬z) ; γ(x, ¬y, ¬z)
→¬¬x ∧α(y, z) ; γ(ε(x), ¬y, ¬z)
→¬¬x ∧α(y, z) ; γ(¬¬x, ¬y, ¬z) ; (ε(x) ∧¬y) ∨(ε(x) ∧¬z)
The resulting proof term is parallel standard. Note that the canonical form of
A is ε(x) ∧α(y, z) ; γ(x, ¬y, ¬z), which is a normal form with respect to (12).
The conditions on A1, . . . , An in rule (13) are there to avoid trivial cases of
non-termination; e.g. γ(y) →γ(y) ; y ≡γ(y) is excluded. In [7, Section 8.5] a
proof sketch of the following result is given.
Theorem 2. The parallel standardization TRS is complete modulo structural
equivalence.
⊓⊔
Instead of rule (12), in our implementation we use the more liberal rewrite
rule
lhsα⟨A1, . . . , An⟩α ; α(B1, . . . , Bn) →α(A1 ; B1, . . . , An ; Bn)
(14)
which is based on Lemma 1. Since we rewrite modulo structural equivalence, (14)
simulates (12); simply substitute tgt(Ai) for Bi. So for the case that the Bi do not
contain rule symbols, the two rules behave exactly the same. If there is some rule
symbol contained in one of the Bi, the term lhsα⟨A1, . . . , An⟩α ; α(B1, . . . , Bn)
with tgt(Ai) = src(Bi) = ti for 1 ⩽i ⩽n always rewrites to a proof term that
is structurally equivalent to α(src(A1), . . . , src(An)) ; rhsα⟨A1 ; B1, . . . , An ; Bn⟩α
independent of which of the two rules we use:
lhsα⟨A1, . . . , An⟩α ; α(B1, . . . , Bn)
→lhsα⟨A1, . . . , An⟩α ; α(t1, . . . , tn) ; rhsα⟨B1, . . . , Bn⟩α
(13)
→α(A1, . . . , An) ; rhsα⟨B1, . . . , Bn⟩α
(12)
→α(src(A1), . . . , src(An)) ; rhsα⟨A1, . . . , An⟩α ; rhsα⟨B1, . . . , Bn⟩α
(13)
and
lhsα⟨A1, . . . , An⟩α ; α(B1, . . . , Bn)
→α(A1 ; B1, . . . , An ; Bn)
(14)
→α(src(A1), . . . , src(An)) ; rhsα⟨A1 ; B1, . . . , An ; Bn⟩α
(13)
Since it is not necessary to check whether the arguments of α are the targets
of the Ai, rule (14) is easier to implement than rule (13). More details about the
implementation can be found in Sect. 5.

344
C. Kohl and A. Middeldorp
4
Projection Equivalence
In the preceding section proof terms were declared to be equivalent if they can
be obtained from each other by reordering (permuting) steps. In this section we
give an account of projection equivalence, which is a completely diﬀerent way
of equating proof terms. It is based on the residual operation which computes
which steps of A remain after performing B, for co-initial proof terms A and B.
The steps in B need not be contained in A in order to compute their residual
A / B. The diagram on the left shows a desirable result of residuals and the
diagram on the right provides the intuition behind Eqs. (17) and (18) below:
·
·
·
·
A
B
A/B
B/A
·
·
·
·
·
·
A
B
C
A/C
B/(C/A)
C/A
(C/A)/B
In [7, Deﬁnition 8.7.54] the residual A/B is deﬁned by means of the following
equations:
x / x = x
(15)
f(A1, . . . , An) / f(B1, . . . , Bn) = f(A1 / B1, . . . , An / Bn)
α(A1, . . . , An) / α(B1, . . . , Bn) = rhsα⟨A1 / B1, . . . , An / Bn⟩α
α(A1, . . . , An) / lhsα⟨B1, . . . , Bn⟩α = α(A1 / B1, . . . , An / Bn)
(16)
lhsα⟨A1, . . . , An⟩α / α(B1, . . . , Bn) = rhsα⟨A1 / B1, . . . , An / Bn⟩α
C / (A ; B) = (C / A) / B
(17)
(A ; B) / C = (A / C) ; (B / (C / A))
(18)
A / B = #(tgt(B))
(otherwise)
Here A, B, C, A1, . . . , An, B1, . . . , Bn are proof term variables that can be
instantiated with arbitrary proof terms (so without /). The x in (15) denotes an
arbitrary variable (in the underlying TRS), which cannot be instantiated.1 In
the ﬁnal deﬁning equation, # is the rule symbol of the special error rule x →⊥.
This rule is adopted to ensure that A / B is deﬁned for arbitrary left-linear
TRSs.2 These deﬁning equations are taken modulo (4) and
t ; t ≈t
(19)
The need for the functorial identities (4) is explained in the following example
(Vincent van Oostrom, personal communication).
1 In [7, Remark 8.2.21] variables are treated as constants and (15) is absent.
2 In both [6, Deﬁnition 6.9] and [7, Deﬁnition 8.7.54] the wrong deﬁnition A / B =
#(tgt(A)) is given.

Composing Proof Terms
345
Example 6. Consider A = f(g(β) ; g(γ)) and B = α(a) in the TRS
α: f(g(x)) →x
β: a →b
γ : b →c
When computing A/B without (4), the α-instance f(g(A1))/α(B1) = A1/B1 of
schema (16) does not apply to A/B since the g in f(g(A1)) needs to be extracted
from g(α) ; g(γ) when computing A / B. As a consequence, the (otherwise)
equation kicks in, producing the proof term #(b) that indicates an error. With
(4) in place, the result of evaluating A / B is the proof term β ; γ, representing
the desired sequence a →b →c.
It is not immediately clear that the deﬁning equations on the preceding page
constitute a well-deﬁned deﬁnition of the residual operation. In [7, proof of The-
orem 8.7.57] the deﬁning equations together with (4) and (19) are oriented from
left to right, resulting in a rewrite system Res that is claimed to be terminating
and conﬂuent. The residual of A over B is then deﬁned as the unique normal
form of A / B in Res.
There are two problems with this approach. First of all, when is the (other-
wise) rule applied? In [7] this is not speciﬁed, resulting in an imprecise rewrite
semantics of Res. Keeping in mind that A/B is supposed to be a total operation
on proof terms (so no / in A and B), a natural solution is to adopt an inner-
most evaluation strategy. This ensures that C /A is evaluated before (C /A)/B
in the right-hand side of (17) and before B / (C / A) in the right-hand side of
(18). The (otherwise) condition is taken into account by imposing the additional
restriction that the (otherwise) rule is applied to A/B (with A and B in normal
form) only if the other rules are not applicable. The second, and more serious,
problem is that Res is not conﬂuent.
Example 7. Consider the TRS consisting of the rules
α: f(x, y) →f(y, x)
β: a →b
γ : f(a, x) →x
and the proof terms A = f(β, a), B = α(b, β), C = α(a, a), and D = γ(a).
There are two ways to compute (A ; B) / (C ; D), starting with (17) or (18):
((A ; B) / C) / D →((A / C) ; (B / (C / A))) / D
→∗(f(a / a, β / a) ; (B / α(a / β, a / a))) / D
→∗(f(a, β) ; (B / α(b, a))) / D
→(f(a, β) ; f(β / a, b / b)) / D
→∗(f(a, β) ; f(β, b)) / D →f(a ; β, β ; b) / D →#(a)
(A / (C ; D)) ; (B / ((C ; D) / A))
→∗((A / C) / D) ; (B / ((C / A) ; (D / (A / C))))
→∗(f(a, β) / D) ; (B / (α(b, a) ; (D / f(a, β))))
→∗β ; (B / (α(b, a) ; γ(b))) →∗β ; (f(β, b) / γ(b))
→∗β ; #(b)

346
C. Kohl and A. Middeldorp
The normal forms #(a) and β ; #(b) represent diﬀerent failing computations:
a →⊥and a →b →⊥.
To solve this problem we propose a drastic solution. When facing a term
A / B with A and B in normal form, the deﬁning equations are evaluated from
top to bottom and the ﬁrst equation that matches is applied. This essentially
means that the ambiguity between (17) and (18) is resolved by giving preference
to the former. Due to innermost evaluation, no other critical situations arise. So
we arrive at the following deﬁnition, where we turned Eq. (19) into rule (28),
which is possible due to the presence of (29).
Deﬁnition 5. The residual TRS for proof terms consists of the following rules:
x / x →x
(20)
f(A1, . . . , An) / f(B1, . . . , Bn) →f(A1 / B1, . . . , An / Bn)
(21)
α(A1, . . . , An) / α(B1, . . . , Bn) →rhsα⟨A1 / B1, . . . , An / Bn⟩α
(22)
α(A1, . . . , An) / lhsα⟨B1, . . . , Bn⟩α →α(A1 / B1, . . . , An / Bn)
(23)
lhsα⟨A1, . . . , An⟩α / α(B1, . . . , Bn) →rhsα⟨A1 / B1, . . . , An / Bn⟩α
(24)
C / (A ; B) →(C / A) / B
(25)
(A ; B) / C →(A / C) ; (B / (C / A))
(26)
A / B →#(tgt(B))
(27)
x ; x →x
(28)
f(A1, . . . , An) ; f(B1, . . . , Bn) →f(A1 ; B1, . . . , An ; Bn)
(29)
We adopt innermost evaluation with the condition that the rules (20)–(27) are
evaluated from top to bottom.
The residual TRS operates on closed proof terms, which are proof terms
without proof term variables, to ensure that tgt(B) in the right-hand side of
(27) can be evaluated. (Variables of the underlying TRS are allowed in proof
terms.)
Lemma 3. The residual TRS is terminating and conﬂuent on closed proof
terms.
Proof. Conﬂuence of the residual TRS is obvious because of the innermost eval-
uation strategy and the fact that there is no root overlap between its rules (due
to the imposed evaluation order). Showing termination is non-trivial because of
the nested occurrences of / in the right-hand sides of (25) and (26). As suggested
in [7, Exercise 8.7.58] one can use semantic labeling [8]. We take the well-founded
algebra A with carrier N equipped with the standard order > and the following
weakly monotone interpretation and labeling functions:

Composing Proof Terms
347
αA(x1, . . . , xn) = fA(x1, . . . , xn) = max{x1, . . . , xn}
;A(x, y) = x + y + 1
/A(x, y) = x
#A(x) = ⊥A = 0
L; = Lf = Lα = L# = L⊥= ∅
L/ = N
lab/(x, y) = x + y
The algebra A is a quasi-model of the residual TRS. Hence termination is a
consequence of termination of its labeled version. The latter follows from LPO
with well-founded precedence /i > /j for all i > j and /0 > ; > f > α > # > ⊥
for all function symbols f and rule symbols α. For instance, (26) gives rise to
the labeled versions (A ; B) /a+b+c+1 C →(A /a+c C) ; (B /b+c (C /c+a A)) for
all natural numbers a, b, and c, and each of them is compatible with the given
LPO.
⊓⊔
The termination argument in the above proof does not depend on the imposed
evaluation strategy. In the following we write A ! B for the unique normal form
of A / B.
Deﬁnition 6. The projection order ≲and projection equivalence ≃are deﬁned
on co-initial proof terms as follows: A ≲B if A ! B = tgt(B) and A ≃B if both
A ≲B and B ≲A.
Lemma 3 provides us with an easy decision procedure for projection equiv-
alence: A ≃B if and only A ! B and B ! A coincide and contain neither rule
symbols nor compositions.
Example 8. We can use this decision procedure to check which of the 13
sequences of Example 1 are projection equivalent. The (proof terms represent-
ing the) sequences 02357 and 12349 in Example 1 are projection equivalent since
02357/12349 and 12349/02357 rewrite to the same normal form (¬x∧y)∨(¬x∧z)
in the residual TRS. As a matter of fact, all sequences from s to t are projection
equivalent, except for 17. For instance, both 02357/17 and 17/02357 rewrite to
#((¬x∧y)∨(¬x∧z));#(⊥), but this normal form of the residual TRS contains
the rule symbol # associated to the error rule.
Even though the residual TRS is designed to compute A/B for co-initial proof
terms, there is no restriction on term formation. So in principle it is conceivable
that A ! B is not a well-formed proof term, which can only happen if A ! B
contains a subterm A1 ; A2 with tgt(A1) ̸= src(A2). The key properties that
exclude this are src(A ! B) = tgt(B) and tgt(A ! B) = tgt(B ! A), because then
the right-hand sides of rules (25) and (26) are well-deﬁned, meaning that one
obtains proof terms as normal forms if A, B, C are instantiated by proof terms.
The ﬁrst property (src(A!B) = tgt(B)) can be proved by induction on the length
of a normalizing sequence in the residual TRS starting from A / B. The second
property (tgt(A ! B) = tgt(B ! A), see also the diagrams at the beginning of this
section) we have not yet been able to establish; the case where both A and B
are headed by composition causes complications due to the imposed evaluation
strategy.

348
C. Kohl and A. Middeldorp
5
Automation
In this section we describe the extensions to ProTeM3 that we implemented
as part of this work. ProTeM is a tool for manipulating proof terms and has
been previously described in [3], with the focus on proof terms that represent
multisteps, so without composition, and methods for measuring overlap between
multisteps.
Apart from the decision procedure for projection equivalence based on the
residual TRS described in the previous section, we implemented procedures
dealing with parallel standardization as well as algorithms to translate between
rewrite sequences and proof terms.
5.1
Rewrite Sequences and Proof Terms
We implemented an algorithm that takes as input two terms t and u, and com-
putes a proof term A without compositions such that src(A) = t and tgt(A) = u.
If there is no multi-step t
◦
−→u, A does not exist. Otherwise, there may be
diﬀerent proof terms A that satisfy the requirements. ProTeM returns the ﬁrst
solution it encounters by trying to match the rules of the current TRS in top-
down order and recursively in the arguments. This algorithm is extended to
generate a proof term for a sequence of multisteps. We do this by applying it to
each consecutive pair of terms, resulting in proof terms A1, . . . , Ak for a sequence
consisting of k + 1 terms, which are then combined into A = A1 ; · · · ; Ak.
Fig. 1. Expansion algorithm (expand).
Conversely, for a given proof term A, ProTeM computes terms t1, . . . , tn
such that A represents the sequence t1 ◦
−→. . . ◦
−→tn. To achieve this, ﬁrst A is
3 http://informatik-protem.uibk.ac.at/.

Composing Proof Terms
349
transformed into a permutation equivalent proof term A1 ; . . . ; An such that the
Ai themselves do not contain compositions. To move inner compositions outside
we repeatedly apply the functorial identities (4) and a generalized form of (11)
(similar to the extension of (12) to (14)). We call this procedure expansion.
Detailed steps are displayed in Fig. 1. The terms t1, . . . , tn are then obtained by
computing the sources and targets of A1, . . . , An. Expansion is also needed for
the marking algorithm, presented in the next subsection. Here we give a simple
example.
Example 9. Consider the TRS of Example 1. Expanding the proof term A =
α(β(¬x, ¬z);(ε(x)∧ε(y)), ε(z)) yields the proof terms A1 = α(β(¬x, ¬z), ε(z))
and A2 = ¬((ε(x) ∧ε(y))) ∨¬z.
5.2
Standardization
In this subsection we report on ProTeM’s implementations in connection with
Sect. 3. When automating parallel standardization it is very useful to have some
way of determining whether a given proof term is already parallel standard,
other than going through all proof terms in its (theoretically inﬁnite) structural
equivalence class and trying to apply the parallel standardization rules. For this
we use a modiﬁed version of the marking procedure [7, p. 366] that operates on
proof terms instead of steps of a reduction. Our implementation is described in
Fig. 2. We ﬁrst transform the input A into its canonical form to get rid of trivial
steps, then we use expansion to remove nested compositions and check if every
proof term of the sequence A1, . . . , An represents a parallel step (i.e., there are
no nested rule symbols). Only then do we start with the actual marking. The
basic idea is to go through the sequence A1, . . . , An from left to right and mark
the positions of the redexes. While moving right we check whether the next
step contains markings below its redex pattern (i.e., in the arguments of its rule
symbols). If it does we know that the next step takes place above the one that
produced the marking and hence the given sequence of proof terms is not parallel
standard.
Automating parallel standardization is a non-trivial task, since the rules of
parallel standardization are applied modulo structural equivalence. Figure 3 dis-
plays our full algorithm to transform any proof term into a permutation equiva-
lent parallel standard one. We start by computing the canonical form of our input
A. Then we check if it is already parallel standard using the marking procedure.
If not, we ﬁrst apply the parallel standardization rules (13) and (14) as much as
possible. If that does not result in a parallel standard proof term, a structurally
equivalent proof term has to be computed to which we can again apply the par-
allel standardization rules. Structural equivalence classes are inﬁnite, but only
due to harmless compositions with trivial terms. Nevertheless, we do not search
blindly through them. First we simplify our problem by determining which part
of the proof term is not parallel standard and recursively call the parallel stan-
dardization algorithm on that subterm. When a composition A1 ; A2 is encoun-
tered where A1 and A2 are parallel standard but A1;A2 is not, neither A1 nor A2

350
C. Kohl and A. Middeldorp
Fig. 2. Marking algorithm (mark).
can contain nested rules symbols since these would have been expanded by (13).
Because we always compute canonical forms of proof terms before trying the
parallel standardization rules, A1 and A2 cannot have the same function symbol
as root. The fact that A1 ; A2 is not parallel standard further implies that A1
is of the form A1 = f(T1, . . . , Tn) and A2 contains an outer step that must be
performed before one of the inner steps in A1. We try to ﬁnd a structurally
equivalent proof term A = C1 ;(C2 ;A2) with C1 = f(T1, . . . , src(Ti), . . . , Tn) and
C2 = f(tgt(T1), . . . , Ti, . . . , tgt(Tn)) such that rule (13) is applicable to C2 ; A2.
For each argument position i we ﬁrst check if C2 ;A2 is already parallel standard
to make sure not to perform useless steps which may cause non-termination of
the procedure. If C2 ; A2 is parallel standard, we split A1 at the next argument
position. After we have identiﬁed C1 and C2 such that C2 ; A2 is not parallel
standard, there is still the possibility that (13) is blocked, because C2 contains
composition. In that case C2 is serialized into C3 and C4 such that C2 = C3 ; C4
and C4 contains exactly one rule symbol and no composition.
Since the parallel standardization TRS is terminating modulo structural
equivalence (Theorem 2), its rules cannot be applied inﬁnitely often to a proof
term A and since we always perform at least one application of its rules in each
iteration, our algorithm is bound to terminate after a ﬁnite number of steps.
Example 10. We apply the parallel standardization algorithm to the proof term
A of Example 5. The canonical form of A is A′ = ε(x)∧α(y, z);γ(x, ¬y, ¬z) and
A′ is not parallel standard according to the marking algorithm. Neither (12) nor
(13) is applicable, though. Since both ε(x)∧α(y, z) and γ(x, ¬y, ¬z) are parallel
standard, we start splitting up ε(x) ∧α(y, z) into C1 ; C2. For i = 1 we obtain
C1 = ¬¬x ∧α(y, z) and C2 = ε(x) ∧(¬y ∨¬z), and so we try to apply (12) and
(13) to the proof term C1 ; (C2 ; A2):

Composing Proof Terms
351
Fig. 3. Parallel standardization algorithm (ps).
C1 ; (C2 ; A2)
→¬¬x ∧α(y, z) ; γ(ε(x), ¬y, ¬z)
(12)
→¬¬x ∧α(y, z) ; (γ(¬¬x, ¬y, ¬z) ; (ε(x) ∧¬y) ∨(ε(x) ∧¬z))
(13)
At this point we are done since the ﬁnal term is parallel standard.
We also implemented full standardization of proof terms by serializing the
parallel steps of parallel standard proof terms such that steps are performed in
a left-to-right order.
6
Conclusion
In this paper we described the extensions to ProTeM that deal with the per-
mutation and projection equivalences as well as the projection order, important
notions to compare rewrite sequences. Along the way, we corrected a mistake in
[6,7] concerning the well-deﬁnedness of the residual operation, which is used to
decide projection equivalence.

352
C. Kohl and A. Middeldorp
This does not complete our investigations. We already remarked the diﬃculty
of establishing tgt(A ! B) = tgt(B ! A) which is needed to guarantee that A / B
is a proper proof term. It is conceivable that the evaluation order we impose
on the residual TRS needs to be relaxed to obtain this result. Then the error
propagating rules A ; #(B) →#(A) and #(A) ; B →#(A) would be added to
the residual TRS to resolve the non-conﬂuence in Example 7. In addition the
error rule #: x →⊥would be promoted to the underlying TRS, in order to
make A ; #(B), #(A) ; B and #(A) also permutation equivalent.
Another desirable result is a proof of equivalence of permutation and projec-
tion equivalence which is based on properties of the residual TRS. The question
whether there exists a characterisation of permutation equivalence that avoids
rewriting modulo structural equivalence is also worth investigating. Further, the
complexity of computing (parallel) standard reductions and residuals needs to
be investigated.
Acknowledgments. We thank Vincent van Oostrom and members of the master sem-
inar of the Computational Logic research group for insightful discussions. Comments
by the reviewers helped to improve the presentation.
References
1. Baader, F., Nipkow, T.: Term Rewriting and All That. Cambridge University Press
(1998). https://doi.org/10.1017/CBO9781139172752
2. Hirokawa, N., Middeldorp, A.: Decreasing diagrams and relative termination.
J. Autom. Reasoning 47(4), 481–501 (2011). https://doi.org/10.1007/s10817-011-
9238-x
3. Kohl, C., Middeldorp, A.: ProTeM: a proof term manipulator (system description). In:
Kirchner, H. (ed.) Proceedings of 3rd International Conference on Formal Structures
for Computation and Deduction. Leibniz International Proceedings in Informatics,
vol. 108, pp. 31:1–31:8 (2018). https://doi.org/10.4230/LIPIcs.FSCD.2018.31
4. Mart´ı-Oliet,
N.,
Meseguer,
J.:
Rewriting
logic:
roadmap
and
bibliography.
Theoret. Comput. Sci. 285(2), 121–154 (2002). https://doi.org/10.1016/S0304-
3975(01)00357-7
5. Meseguer, J.: Conditioned rewriting logic as a united model of concurrency. Theoret.
Comput. Sci. 96(1), 73–155 (1992). https://doi.org/10.1016/0304-3975(92)90182-F
6. van Oostrom, V., de Vrijer, R.: Four equivalent equivalences of reductions. In: Pro-
ceedings of 2nd International Workshop on Reduction Strategies in Rewriting and
Programming. Electronic Notes in Theoretical Computer Science, vol. 70(6), pp.
21–61 (2002). https://doi.org/10.1016/S1571-0661(04)80599-1
7. Terese (ed.): Term Rewriting Systems, Cambridge Tracts in Theoretical Computer
Science, vol. 55. Cambridge University Press (2003)
8. Zantema, H.: Termination of term rewriting by semantic labelling. Fundamenta
Informaticae 24, 89–105 (1995). https://doi.org/10.3233/FI-1995-24124

Composing Proof Terms
353
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the chapter’s
Creative Commons license, unless indicated otherwise in a credit line to the material. If
material is not included in the chapter’s Creative Commons license and your intended
use is not permitted by statutory regulation or exceeds the permitted use, you will
need to obtain permission directly from the copyright holder.

Combining ProVerif and Automated
Theorem Provers for Security Protocol
Veriﬁcation
Di Long Li(B) and Alwen Tiu
The Australian National University, Canberra, ACT 2600, Australia
dilong.li@anu.edu.au
Abstract. Symbolic veriﬁcation of security protocols typically relies on
an attacker model called the Dolev-Yao model, which does not model
adequately various algebraic properties of cryptographic operators used
in many real-world protocols. In this work we describe an integration of
a state-of-the-art protocol veriﬁer ProVerif, with automated ﬁrst order
theorem provers (ATP). The integration allows one to model directly
algebraic properties of cryptographic operators as a ﬁrst-order equa-
tional theory and the speciﬁed protocol can be exported to a ﬁrst-order
logic speciﬁcation in the standard TPTP format for ATP. An attack
on a protocol corresponds to a refutation using the encoded ﬁrst order
clauses. We implement a tool that analyses this refutation and extracts
an attack trace from it, and visualises the deduction steps performed by
the attacker. We show that the combination of ProVerif and ATP can
ﬁnd attacks that cannot be found by ProVerif when algebraic properties
are taken into account in the protocol veriﬁcation.
1
Introduction
Security protocols are used pervasively in communication networks, such as the
SSL/TLS protocol [11] used in secure communication on the web. Designing a
security protocol is an error-prone process due to the subtle security requirements
and their dependency on the attacker model. Early work on symbolic analysis of
protocols has uncovered many ﬂaws in protocol designs, e.g., the classic example
of Needham-Schr¨oder’s Public Key authentication protocol [20], whose ﬂaw was
found through a formal analysis in a model checker [18].
An important part of formal protocol analysis is the deﬁnition of the attacker
model. A commonly used attacker model is the Dolev-Yao model [12], which
assumes, among others, that encryption is perfect, i.e., an attacker will not be
able to decrypt a cipher text unless he or she knows the decryption key. While this
model has been shown eﬀective in uncovering security ﬂaws in many protocols
(see, e.g., [18,25]), it also misses many attacks on protocols that rely on algebraic
properties of the operators used in the protocols [9]. One example is the exclusive-
or (XOR) operator commonly used in protocols for RFID. As shown in [10], some
attacks on these protocols can only be found once the properties of XOR are
taken into account, e.g., the associativity, commutativity, and inverse properties.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 354–365, 2019.
https://doi.org/10.1007/978-3-030-29436-6_21

Combining ProVerif and Automated Theorem Provers
355
There are existing protocol veriﬁers that allow some algebraic properties in
the attacker model, such as Maude-NPA [14,15] and Tamarin [13,19]. Maude-
NPA, which is based on a rewriting framework, allows a rich set of algebraic
properties, speciﬁed as equational theories and rewrite systems. These include
associativity, commutativity and identity properties, and the ﬁnite variant class
of equational theories [8]. However, there are some restrictions on how algebraic
properties can be speciﬁed, and in certain cases [15], speciﬁc uniﬁcation algo-
rithms may need to be added to the prover. Tamarin has recently added support
for XOR [13], which allows it to ﬁnd ﬂaws in the 5G network protocol [3], but
just as Maude-NPA, it is not guaranteed to handle theories outside the ﬁnite
variance class, e.g., XOR with a homomorphic function.
In this system description, we present a new tool, Proverif-ATP, which is a
combination of a widely used protocol veriﬁer ProVerif [4] and ﬁrst-order auto-
mated theorem provers (ATP). Our tool allows a user to specify any algebraic
properties in the attacker model as an equational theory, without having to
modify the prover, or having to prove any particular meta-theory about the
equational theory (e.g., whether the equational theory can be represented as a
convergent rewrite system, or whether uniﬁcation modulo the theory is decid-
able, etc), as this will be handled by the ATPs backend in our tool. Speciﬁcally,
we made the following contributions:
– We implement an interface from ProVerif to arbitrary ATPs that allows the
latter to verify the protocols speciﬁed in ProVerif. This is done via a transla-
tion from protocol speciﬁcations in ProVerif to ﬁrst-order logic speciﬁcations
in the TPTP format [24], a common input format accepted by a large number
of ATPs. The TPTP output from ProVerif can then be fed into any ATP that
accepts the TPTP input format.
– We implement a tool, called Narrator, that interprets the refutation proofs
produced by the backend ATP and presents them in a form that is easier for
the user to ‘debug’ a protocol speciﬁcation. In particular it allows extraction
of attack traces from the proofs. An issue with the default encoding from
ProVerif to ﬁrst-order speciﬁcations is that it obscures the structures of the
original protocol, that makes it diﬃcult to relate the resolution proofs pro-
duced by ATP (which can range in the thousands of steps) to attacks on
the protocol. To this end, we introduce a tagging mechanism to annotate the
encoding of a protocol with information about the protocol steps. This allows
us to separate the attacker knowledge that is intercepted from protocol steps
from the knowledge deduced by the attacker using (algebraic) properties of
cryptographic operators. Narrator visualises the refutation proof as a directed
acyclic graph, with diﬀerent types of nodes color-coded to help the user to
spot important steps in the protocol that contribute to the attack.
In Sect. 2 we give a brief overview of the architecture of ProVerif-ATP.
Section 3 illustrates the use of ProVerif-ATP v0.1.0 through a running exam-
ple of verifying a protocol featuring XOR [7]. Section 4 shows some results in
using our tool to verify protocols that are currently out of the scope of ProVerif,
and in some instances involving XOR with a homomorphic operator, also out of

356
D. L. Li and A. Tiu
the scope of Tamarin.1 Sect. 5 concludes the paper and discusses future work.
The source code of the ProVerif-ATP tool and the example protocols tested are
available online.2
2
Overview of the Proverif-ATP Tool
The Proverif-ATP tool consists of a version of ProVerif, modiﬁed to output
protocol speciﬁcations in TPTP syntax, and a tool to interpret the resolution
proof (in TPTP format [24]) in terms of attack traces and the visualisation
of derivations of attacks. Figure 1 shows the architecture of Proverif-ATP. The
backend solver can be any ATP, although in this work, we have used mainly
Vampire [16]. We explain brieﬂy each component in the following. More details
of the implementation are available in the ProVerif-ATP repository.
ProVerif
Parser
AST
modiﬁer
Horn
clauses
translator
TPTP
export
module
ATP
Narrator
TPTP
Speciﬁcation
TPTP (proof)
Attack trace 
+ Knowledge graph
Fig. 1. Overall architecture of ProVerif-ATP
The components of Proverif-ATP are chained together using a script pvatp,
which invokes the modiﬁed version of ProVerif, translates the ProVerif speciﬁ-
cation into a TPTP ﬁle, invokes the ATP, and generates the HTML ﬁles of the
Narrator with both the ProVerif speciﬁcation and Vampire output embedded.
ProVerif. A detailed explanation ProVerif is out of the scope of this system
description, and the reader is referred to the ProVerif manual [6] for details. In
our tool, we use the speciﬁcation language based on the typed version of the
applied-pi calculus [2]. The speciﬁcation of algebraic properties of the attacker
model can be accommodated within the syntax of ProVerif by simply writing
down the equations corresponding to the algebraic properties of the model. So
syntactically there is no change required to the speciﬁcation language for proto-
cols in ProVerif. However, the proof engine of ProVerif does not accept arbitrary
equations, even if they are syntactically valid, and it also refuses to translate
1 Some examples which Proverif-ATP can handle but which Tamarin fails are available
via https://github.com/darrenldl/ProVerif-ATP/tree/master/related-work/.
2 https://github.com/darrenldl/ProVerif-ATP.

Combining ProVerif and Automated Theorem Provers
357
these equations to ﬁrst-order clauses. Since we will not be using ProVerif proof
engine, the former is not a concern. We explain next how we address the latter.
Translation to First-Order Logic. The applied-pi calculus speciﬁcation in
ProVerif is translated to Horn clauses before it is given to its proof engine.
This translation was proven correct by Abadi and Blanchet [1,5]. ProVerif has
a built-in translator from the internal Horn clause representation of a protocol
to a ﬁrst-order speciﬁcation in a format accepted by SPASS [27]. However, this
translator cannot handle arbitrary equations – which is crucial to our aim. It
attempts to perform reduction and syntactic uniﬁcation to simplify certain terms
in a protocol. In the case where the attacker model contains equational theories
that cannot be represented as a convergent rewrite system, the simpliﬁcation
performed by ProVerif may fail and some parts of the protocol are not translated.
To solve this, we modify the built-in translation of ProVerif to handle speciﬁc
cases involving equations.
Proverif built-in translation also obscures the relative ordering of protocol
steps and it is not obvious how to diﬀerentiate among clauses that encode only
the attacker model and clauses from diﬀerent steps of the protocol. To solve this,
our tool tags all input (in the latest version) and output actions in the protocol.
These tags do not change the meaning of the protocol (they preserve attacks),
but it allows our tool (Narrator) to make sense of the refutation proof output by
ATP. The tagging is done automatically at the abstract syntax of the applied-pi
speciﬁcation of the protocol, prior to the translation to TPTP.
Currently we only support secrecy queries, i.e., those queries that assert that
the attacker knows a certain secret. Queries other than secrecy, such as authen-
tication, need to be translated to an equivalent secrecy query. ProVerif provides
a general authentication query, in the form of correspondence assertions [6,28],
but their translation to ﬁrst-order clauses is not yet supported. Since our goal
here is to see how well ATP can handle algebraic properties in attacker models,
we do not yet attempt to encode correspondence assertions in our translation,
as this is orthogonal to the issue of algebraic models.
Note that if an attack is found, an attack trace is produced to serve as a proof
certiﬁcate, which is to be manually veriﬁed in Narrator as described below, so
in principle the correctness of the translation is not critical to the workﬂow.
ATP. The output (in TPTP format) of the translator of ProVerif is input to the
ATP. The encoding of protocols as Horn clauses in ProVerif equates an attack
on the protocol with provability of an attack goal. So if there is an attack on
the protocol, we expect a proof of the attack goal as the output of the ATP (in
the TPTP format for proofs). Since the expected input and output of an ATP
are in TPTP format, we can in principle use any ATP as the backend prover
in our tool, e.g., provers that compete in CASC competitions.3 In this system
description, we use Vampire [16] as the backend prover.
Narrator. The purpose of the Narrator is to make sense of the TPTP out-
put, which can consist of thousands of inference steps. A lot of these steps are
3 http://tptp.cs.miami.edu/∼tptp/CASC/.

358
D. L. Li and A. Tiu
derivations of particular messages by the attacker, using the axioms for the
attacker model. Some crucial information such as steps in the protocol responsi-
ble for the attacks, is not easy to spot from such a proof. The backend ATP may
employ sophisticated inferences and rewriting of clauses that make it diﬃcult to
distinguish even basic information such as which properties of the operator are
responsible for the attacks, which steps are involved, and whether a particular
information provided by a protocol participant is useful in deriving the attack.
Narrator aims to categorise various inference steps to make it easier for a user
to spot relevant information from the resolution proof.
Narrator can also produce an attack trace, in the form of sequences of mes-
sages exchanged between roles of a protocol with the attacker. This feature is
still at an experimental stage, as some crucial information, such as the actual
messages that trigger the attack, may not be apparent from the trace. This is
due to the fact that some provers (such as Vampire) do not provide the uni-
ﬁers used in producing the proof, when the option for TPTP output is enforced.
Vampire does provide this information in its native proof format (via the option
--proof extra) [21]. Narrator currently supports only the TPTP output format
for the reason that it is a standard format supported by many theorem provers.
3
Protocol Veriﬁcation with Proverif-ATP: An Example
We use the CH07 protocol [7], for mutually authenticating a RFID reader and a
RFID tag, as an example. As shown in [10], the protocol fails to guarantee the
aliveness property [17] of the tag to the reader - the reader completed a run of
the protocol believing to have been communicating with a tag, while no tag was
present. Figure 2a shows the CH07 protocol speciﬁcation in a message sequence
chart. In the ﬁgure, R denotes an instance of an RFID reader, and T denotes
an instance of an RFID tag. The constant k denotes the secret key shared by R
and T, and ID denotes an identiﬁer of T (e.g., serial number). There are various
bitwise manipulation functions used in the protocol: for simplicity, we shall treat
these as uninterpreted function symbols except for ⊕, which denotes XOR.
R initiates the protocol by sending a query message containing a nonce (a
fresh random number) r1 to T. T generates a nonce r2, and computes ˜g, which
is used to rotate ID to obtain ID2. T then sends out r2 along with the left half
of ID2. R looks up the ID of T, and computes the same ˜g and ID2, then sends
out the right half of ID2 as response to T. It is implicit that R and T check all
received messages against the self-computed copies, namely Left(ID2 ⊕˜g) sent
from T to R, and Right(ID2, ˜g) sent from R to T. At the end of a successful
run of the protocol, R is convinced of the identity of T and vice versa.
The attack shown in [10] consists of two sessions. In the ﬁrst session, the
attacker observes a completed run between R and a tag T, and records all mes-
sages exchanged. Then using these messages, the attacker impersonates T in the
second session. This attack relies on the properties of XOR to craft a special
value of r2 in the second session (see [10] for details).
Figure 2b shows a fragment of the ProVerif speciﬁcation of the protocol. Note
that the properties of XOR are encoded as equations (line starting with equation

Combining ProVerif and Automated Theorem Provers
359
R
k, ID
T
k, ID
nonce r1
Query,r1
nonce r2
˜g := h(r1 ⊕r2 ⊕k)
ID2
:=
rotate(ID, ˜g)
r2, Left(ID2 ⊕˜g)
find ID
˜g := h(r1 ⊕r2 ⊕k)
ID2
:=
rotate(ID, ˜g)
Right(ID2 ⊕˜g)
(a) CH07 protocol
equation forall x:bitstring, y:bitstring, z:bitstring;
xor(x, xor(y, z)) = xor(xor(x, y), z).
equation forall x:bitstring, y:bitstring; xor(x, y) =
xor(y, x).
equation forall x:bitstring; xor(x, ZERO) = x.
equation forall x:bitstring; xor(x, x) = ZERO.
free objective : bitstring [private].
query attacker(objective).
let R =
new r1:bitstring;
out(c, (QUERY, r1));
in(c, (r2 : bitstring, left_xor_ID2_g : bitstring));
let g
= h(xor(xor(r1, r2), k)) in
let ID2
= rotate(ID, g) in
let left
= split_L(xor(ID2, g)) in
let right = split_R(xor(ID2, g)) in
if left = left_xor_ID2_g then (
out(c, right);
out(c, objective)
).
let sess_1 =
new r1_s1:bitstring;
out(c, r1_s1);
new r2_s1:bitstring;
let g_s1
= h(xor(xor(r1_s1, r2_s1), k)) in
let ID2_s1
= rotate(ID, g_s1) in
let left_s1
= split_L(xor(ID2_s1, g_s1)) in
let right_s1 = split_R(xor(ID2_s1, g_s1)) in
out(c, (r2_s1, left_s1));
out(c, right_s1).
process
sess_1 | R
(b) A part of ProVerif encoding
Fig. 2. CH07 speciﬁcations
in the ﬁgure). Since we cannot model authentication directly in Proverif-ATP,
we reformulate the problem as a secrecy problem. For this, we assume that
the attacker has obtained messages from the ﬁrst session (process sess 1 in
Fig. 2b). For the second session onwards, we modify the protocol so that the
reader will output a secret (objective in Fig. 2b) after a successful authentica-
tion. To ensure that we capture the authentication property properly, we need
to make sure that this secret is not accidentally output as a result of a legitmate
interaction with T; this is modelled by simply removing the tag T from the
protocol runs in the second session.
Before the protocol speciﬁcation is translated to TPTP, the input/output
actions in the protocol are tagged with constants denoting certain information,
such as the principal performing those actions, and the steps within the protocol.
The tagged protocol is then passed on to the translator unit to produce the
TPTP speciﬁcation of the protocol. The following is an example of a fragment
of the TPTP ﬁle produced by the translation.
fof(ax226, axiom, pred_attacker(tuple_2(tuple_2(constr_QUERY, name_r1), constr_R_STEP_1))).
![VAR_R2_293] : (pred_attacker(tuple_2(VAR_R2_293, constr_split_L(constr_xor(constr_rotate(
name_ID, constr_h(constr_xor(constr_xor(name_r1, VAR_R2_293), name_k))), constr_h(
constr_xor(constr_xor(name_r1, VAR_R2_293), name_k)))))) => pred_attacker(tuple_2(
name_objective, constr_R_STEP_3)))).

360
D. L. Li and A. Tiu
The encoding uses a predicate ‘pred attacker’ to encode the attacker knowledge.
The ﬁrst clause above, for example, shows the knowledge obtained from an out-
put step with no dependency on previous inputs. The second clause shows an
example of an interactive protocol step: if the right condition (input) is present,
then an output is sent (hence would be known to the attacker).
Analysis of Refutation Proof in Narrator. Narrator has three major modes,
knowledge graph mode, and two ProVerif code + attack trace examination modes
(one shows the raw ProVerif code, one shows the pretty-printed ProVerif code).
We ﬁrst examine the structure of the attack through the attack trace generated
by Narrator, then we utilise the knowledge graph and explanation mechanism
to further analyse aspects of the attack.
1.
sess_1.1
sess_1 -> I : r1_s1
2.
sess_1.2
sess_1 -> I : tuple_2(r2_s1,split_L(xor(rotate(ID,h(xor(xor(r1_s1,r2_s1),k)
)),h(xor(xor(r1_s1,r2_s1),k)))))
3.
R.1
R -> I : tuple_2(QUERY,r1)
4.
R.3
I -> R : tuple_2(X28,split_L(xor(rotate(ID,h(xor(xor(r1,X28),k))),h(xor(xor
(r1,X28),k)))))
5.
R.3
R -> I : objective
Fig. 3. Narrator attack trace output
In the attack trace in Fig. 3, I denotes the intruder/attacker, R denotes the
reader, and sess 1 denotes the knowledge of a previous session as we have spec-
iﬁed. Observe that the attacker is able to construct the appropriate message to
send to R at step 4 (R.3 I -> R) for R to complete its execution and output
objective at step 3. This matches the typical challenge and response structure
- the reader R challenges the tag (attacker in this case) to give the appropriate
message to solicit a response (message objective). The crucial step is step 4,
which indicates that the reader somehow accepts the response from the attacker
as legitimate. However, in this case, the backend ATP (Vampire) does not pro-
duce a ground instance of the message being sent; instead, there is a variable
X28 whose value is supposed to be the nonce r2 crafted by the attacker. Vam-
pire can produce the substitutions produced at each step of inferences, but this
feature is currently supported only in its native proof format, and not for the
TPTP output. When we examine Vampire native proof output, we do see the
exact term for X28 produced, which matches the attack mentioned in [10]. We
have attempted running Vampire in the ‘question-answering’ mode, which would
produce answer substitutions in TPTP format; but in that mode AVATAR must
be disabled, which makes Vampire unable to ﬁnd the attack for this protocol.
However, using Narrator we can still trace how the message at step 4 is
constructed by the attacker, by observing the relationships between diﬀerent
nodes in the refutation proof. We represent the refutation proof as a directed
acyclic graph, which we call knowledge graph. The nodes in the graph are color-
coded to distinguish diﬀerent categories of formulas as follows:

Combining ProVerif and Automated Theorem Provers
361
Fig. 4. Partial view of knowledge graph with manual numbering
From
step R.1
axiom ! [X17,X18] : attacker(tuple_2(X17,X18)) => attacker(X17)
axiom ! [X19,X20] : attacker(tuple_2(X19,X20)) => attacker(X20)
attacker knows
r1
...
From
axiom ! [X2,X3] : xor(X2,X3) = xor(X3,X2)
axiom ! [X4,X5,X6] : xor(X4,xor(X5,X6)) = xor(xor(X4,X5),X6)
axiom ! [X8,X9] : (attacker(X9) & attacker(X8)) => attacker(xor(X8,X9))
r1_s1
xor(r1,r2_s1)
attacker knows
xor(r1,xor(r1_s1,r2_s1))
From
step sess_1.2
axiom ! [X17,X18] : attacker(tuple_2(X17,X18)) => attacker(X17)
axiom ! [X19,X20] : attacker(tuple_2(X19,X20)) => attacker(X20)
axiom ! [X2,X3] : xor(X2,X3) = xor(X3,X2)
attacker learns
split_L(xor(h(xor(k,xor(r1_s1,r2_s1))),rotate(ID,h(xor(k,xor(r1_s1,r2_s1))))))
Attacker rewrites
split_L(xor(h(xor(k,X2)),rotate(ID,h(xor(k,X2)))))
to
split_L(xor(h(xor(k,xor(r1,X0))),rotate(ID,h(xor(k,xor(r1,X0))))))
xor(r1,X2)
to
X0
From
axiom ! [X15,X16] : (attacker(X16) & attacker(X15)) => attacker(tuple_2(X15,X16))
split_L(xor(h(xor(k,xor(r1,X0))),rotate(ID,h(xor(k,xor(r1,X0))))))
X0
attacker knows
tuple_2(X0,split_L(xor(h(xor(k,xor(r1,X0))),rotate(ID,h(xor(k,xor(r1,X0)))))))
Fig. 5. Sample explanation given by Narrator (... indicates skip)
Unsure
Orange
Axiom
Light blue
InitialKnowledge
Light green
Rewriting
Dark blue
Knowledge
Dark green
Goal
Dark gray
NegatedGoal
Dark gray
Contradiction
Dark gray
ProtocolStep
Light red
InteractiveProtocolStep
Dark red
Alias
Light gray

362
D. L. Li and A. Tiu
In this example, we focus on unconditional protocol steps (light red) and
interactive protocol steps (dark red) nodes. Unconditional protocol steps are
outputs that happen unconditionally, and interactive steps are outputs which
depend on input from the attacker.
Suppose we want to trace how the objective in step 5 in Fig. 3 is triggered.
That step is ascribed to R.3 – which denotes that this is the third step of the
reader R (an information derived from our tagging mechanism). From the trace
we know that R.3 is an interactive step, so it would appear as a dark-red node
in the knowledge graph. In this case, there is only one such node. Tracing down
the path from this node, we see it branches into node 1 and 2 in Fig. 4. The two
nodes are introduced by Vampire’s AVATAR architecture, which utilises a SAT
or an SMT solver for improved capability [26]. By clicking on the two gray nodes
individually, we can see the following two formulas
spl0_2 <=> ! [X0] : ~attacker(tuple_2(X0,split_L(xor(h(xor(k,xor(r1,X0))),rotate(ID,h(xor
(k,xor(r1,X0))))))))
spl0_0 <=> attacker(tuple_2(objective,R_STEP_3))
The ﬁrst formula corresponds to the message sent from I to R at step R.3.
To see how this formula is constructed, we visit the next node from this grey
node (node 3 in Fig. 4) and choose the “Explain construction of chain” option
in the menu. Figure 5 contains the partial copy of the explanation shown, where
X0 is equivalent to X28 in Fig. 3. See the full version of this paper in the project
GitHub repository for the full explanation.
4
Evaluation
We evaluate the eﬀectiveness of ProVerif-ATP on a number of protocol speci-
ﬁcations under various attacker models featuring algebraic operators, such as,
XOR, Abelian group operators, homomorphic encryption and associative pair-
ing; see [9] for details of these properties. In most cases ProVerif simply fails
because the equations modelling these properties are outside the scope of what
it can handle. Table 1 shows the list of protocols veriﬁed and the results of the
veriﬁcation. We encode diﬀerent attack queries as separate entries, thus there
may be multiple entries for one protocol. See the full version of this paper on
GitHub repository for more details of these protocols. Most of the protocols
shown can be veriﬁed using Vampire version 4.2.2 as the ATP backend. In four
protocols, involving XOR, Vampire fails due to out-of-memory error (indicated
by ‘MEM’ in the table), and in one case, Vampire found a model (indicated
by ‘SAT’ in the table) – meaning there is no attack under the corresponding
attacker model, which is conﬁrmed in ProVerif as well. The experiments were
done in a dedicated virtual server, featuring a 8 core 3.6 GHz processor and
32GB RAM. Vampire was set to run in default mode, with a 24 h timeout and
30.9GB memory limit. In the default mode Vampire only uses one core.

Combining ProVerif and Automated Theorem Provers
363
Table 1. List of protocols tested with ProVerif-ATP
Protocol
ProVerif
Vampire
Vampire time (seconds)
LAK06 [10]
×
✓
0.102
SM08 [10]
×
✓
84.684
LD07 [10]
×
✓
0.015
OTYT06 [10]
×
✓
0.008
CH07 [10]
×
✓
84.244
KCLL06 [10]
×
× (MEM)
-
Bull Authentication Protocol [22]
×
× (MEM)
-
Shamir-Rivest-Adleman Three Pass [9]
✓
✓
0.013
DH [9]
✓
✓
0.015
WEP [9]
×
✓
0.182
Salary Sum [9]
×
✓
0.571
NSPK (attack 1) [18]
✓
✓
3.151
NSPK (attack 2) [18]
✓
✓
2.629
NSLPK modiﬁed (attack 1) [9]
×
✓
0.032
NSLPK modiﬁed (attack 2) [9]
×
✓
0.034
NSLPK with ECB (attack 1) [9]
×
✓
2.060
NSLPK with ECB (attack 2) [9]
×
✓
2.300
NSLPK with XOR (attack 1) [23]
×
× (MEM)
–
NSLPK with XOR (attack 2) [23]
×
× (MEM)
–
NS with CBC (attack 1) [9]
✓
✓
0.070
NS with CBC (attack 2) [9]
No attack
SAT
–
NS with CBC (attack 3) [9]
✓
✓
0.087
Denning-Sacco Symmetric Key with CBC [9]
✓
✓
0.027
5
Conclusion and Future Work
Our preliminary tests suggest that the integration of ProVerif and ATP is useful
as it improves the class of protocols one can verify. While we have not done
a thorough comparison with other protocol veriﬁers, we do note that Tamarin
can handle more security properties than Proverif-ATP (which is restricted to
secrecy), but ProVerif-ATP can handle some cases where Tamarin cannot. Our
aim is to show how an existing protocol veriﬁer can beneﬁt from ATPs to extend
the coverage of its analysis with minimal eﬀorts, ie., without coming up with
dedicated procedures to solve equational reasoning needed in protocol analysis,
as such we see our work as complementary to other dedicated protocol veriﬁers
rather than their competitors or replacements.
A crucial part of our work is making the output of ATP intelligible for users,
for which the uniﬁers used in the derivation of an attack would be very helpful.
The current speciﬁcation of the TPTP proof output format does not require
uniﬁers to be included in the output proofs, something we hope will change in the
future. As future work, we intend to compile a set of benchmark problems derived
from protocol analysis (with algebraic operators) and propose its inclusion in the
future CASC competition, to encourage developers of ATPs to improve their
provers to solve this class of problems. Another direction is to also investigate
the encoding of hyperproperties, such as non-inteference and equivalence.

364
D. L. Li and A. Tiu
References
1. Abadi, M., Blanchet, B.: Analyzing security protocols with secrecy types and
logic programs. J. ACM 52(1), 102–146 (2005). https://doi.org/10.1145/1044731.
1044735
2. Abadi, M., Fournet, C.: Mobile values, new names, and secure communication.
In: Proceedings of the 28th ACM SIGPLAN-SIGACT Symposium on Principles
of Programming Languages, POPL 2001, pp. 104–115. ACM, New York (2001).
https://doi.org/10.1145/360204.360213
3. Basin, D., Dreier, J., Hirschi, L., Radomirovic, S., Sasse, R., Stettler, V.: A formal
analysis of 5G authentication. In: Proceedings of the 2018 ACM SIGSAC Confer-
ence on Computer and Communications Security, CCS 2018, pp. 1383–1396. ACM,
New York (2018). https://doi.org/10.1145/3243734.3243846
4. Blanchet, B.: An eﬃcient cryptographic protocol veriﬁer based on prolog rules. In:
Proceedings of 14th IEEE Computer Security Foundations Workshop, 2001. pp.
82–96, June 2001. https://doi.org/10.1109/CSFW.2001.930138
5. Blanchet, B.: Modeling and verifying security protocols with the applied pi calculus
and proVerif. Found. Trends® Priv. Secur. 1(1–2), 1–135 (2016). https://doi.org/
10.1561/3300000004
6. Blanchet, B., Smyth, B., Cheval, V., Sylvestre, M.: ProVerif 2.00: automatic cryp-
tographic protocol veriﬁer, user manual and tutorial. Technical report (2018)
7. Chien, H.-Y., Huang, C.-W.: A lightweight RFID protocol using substring. In:
Kuo, T.-W., Sha, E., Guo, M., Yang, L.T., Shao, Z. (eds.) EUC 2007. LNCS,
vol. 4808, pp. 422–431. Springer, Heidelberg (2007). https://doi.org/10.1007/978-
3-540-77092-3 37
8. Comon-Lundh, H., Delaune, S.: The ﬁnite variant property: how to get rid of some
algebraic properties. In: Giesl, J. (ed.) RTA 2005. LNCS, vol. 3467, pp. 294–307.
Springer, Heidelberg (2005). https://doi.org/10.1007/978-3-540-32033-3 22
9. Cortier, V., Delaune, S., Lafourcade, P.: A survey of algebraic properties used in
cryptographic protocols. J. Comput. Secur. 14(1), 1–43 (2006). https://doi.org/
10.3233/jcs-2006-14101
10. van Deursen, T., Radomirovic, S.: Attacks on RFID protocols. IACR Cryptology
ePrint Archive 2008, 310 (2008). http://eprint.iacr.org/2008/310
11. Dierks, T., Rescorla, E.: The transport layer security (TLS) protocol version 1.2.
Technical report, August 2008. https://doi.org/10.17487/rfc5246
12. Dolev, D., Yao, A.: On the security of public key protocols. IEEE Transact. Inf.
Theory 29(2), 198–208 (1983). https://doi.org/10.1109/TIT.1983.1056650
13. Dreier, J., Hirschi, L., Radomirovic, S., Sasse, R.: Automated unbounded veriﬁca-
tion of stateful cryptographic protocols with exclusive or. In: 2018 IEEE 31st Com-
puter Security Foundations Symposium (CSF), pp. 359–373, July 2018. https://
doi.org/10.1109/CSF.2018.00033
14. Escobar, S., Meadows, C., Meseguer, J.: Maude-NPA: cryptographic protocol
analysis modulo equational properties. In: Aldini, A., Barthe, G., Gorrieri, R.
(eds.) FOSAD 2007-2009. LNCS, vol. 5705, pp. 1–50. Springer, Heidelberg (2009).
https://doi.org/10.1007/978-3-642-03829-7 1
15. Escobar, S., Meadows, C.A., Meseguer, J.: Maude-NPA, Version 3.1. Technical
report (2017)
16. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1

Combining ProVerif and Automated Theorem Provers
365
17. Lowe, G.: A hierarchy of authentication speciﬁcations. In: Proceedings 10th Com-
puter Security Foundations Workshop, pp. 31–43, June 1997. https://doi.org/10.
1109/CSFW.1997.596782
18. Lowe, G.: Breaking and ﬁxing the Needham-Schroeder Public-Key protocol using
FDR. In: Margaria, T., Steﬀen, B. (eds.) TACAS 1996. LNCS, vol. 1055, pp. 147–
166. Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-61042-1 43
19. Meier, S., Schmidt, B., Cremers, C., Basin, D.: The TAMARIN prover for the
symbolic analysis of security protocols. In: Sharygina, N., Veith, H. (eds.) CAV
2013. LNCS, vol. 8044, pp. 696–701. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-39799-8 48
20. Needham, R.M., Schroeder, M.D.: Using encryption for authentication in large
networks of computers. Commun. ACM 21(12), 993–999 (1978). https://doi.org/
10.1145/359657.359659
21. Reger, G.: Better proof output for vampire. In: Kovacs, L., Voronkov, A. (eds.)
Proceedings of the 3rd Vampire Workshop, Vampire 2016. EPiC Series in Comput-
ing, vol. 44, pp. 46–60. EasyChair (2017). https://doi.org/10.29007/5dmz, https://
easychair.org/publications/paper/1DlL
22. Ryan, P., Schneider, S.: An attack on a recursive authentication protocol
a cautionary tale. Inf. Process. Lett. 65(1), 7–10 (1998). https://doi.org/10.
1016/S0020-0190(97)00180-4,. http://www.sciencedirect.com/science/article/pii/
S0020019097001804
23. Steel, G.: Deduction with XOR constraints in security API modelling. In: Nieuwen-
huis, R. (ed.) CADE 2005. LNCS (LNAI), vol. 3632, pp. 322–336. Springer, Hei-
delberg (2005). https://doi.org/10.1007/11532231 24
24. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure. J. Autom.
Reasoning 59(4), 483–502 (2017). https://doi.org/10.1007/s10817-017-9407-7
25. Vigan`o, L.: Automated security protocol analysis with the AVISPA tool. Elec-
tron. Notes Theor. Comput. Sci. 155, 61–86 (2006). https://doi.org/10.1016/j.
entcs.2005.11.052. http://www.sciencedirect.com/science/article/pii/S1571066106
001897 proceedings of the 21st Annual Conference on Mathematical Foundations
of Programming Semantics (MFPS XXI)
26. Voronkov, A.: AVATAR: the architecture for ﬁrst-order theorem provers. In: Biere,
A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559, pp. 696–710. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-08867-9 46
27. Weidenbach, C., Dimova, D., Fietzke, A., Kumar, R., Suda, M., Wischnewski,
P.: SPASS Version 3.5. In: Schmidt, R.A. (ed.) CADE 2009. LNCS (LNAI), vol.
5663, pp. 140–145. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-
642-02959-2 10
28. Woo, T.Y.C., Lam, S.S.: Authentication for distributed systems. Computer 25(1),
39–52 (1992). https://doi.org/10.1109/2.108052

Towards Bit-Width-Independent Proofs
in SMT Solvers
Aina Niemetz1
, Mathias Preiner1
, Andrew Reynolds2
, Yoni Zohar1(B)
,
Clark Barrett1
, and Cesare Tinelli2
1 Stanford University, Stanford, USA
yoniz@cs.stanford.edu
2 The University of Iowa, Iowa City, USA
Abstract. Many SMT solvers implement eﬃcient SAT-based proce-
dures for solving ﬁxed-size bit-vector formulas. These approaches, how-
ever, cannot be used directly to reason about bit-vectors of symbolic
bit-width. To address this shortcoming, we propose a translation from
bit-vector formulas with parametric bit-width to formulas in a logic sup-
ported by SMT solvers that includes non-linear integer arithmetic, unin-
terpreted functions, and universal quantiﬁcation. While this logic is unde-
cidable, this approach can still solve many formulas by capitalizing on
advances in SMT solving for non-linear arithmetic and universally quan-
tiﬁed formulas. We provide several case studies in which we have applied
this approach with promising results, including the bit-width indepen-
dent veriﬁcation of invertibility conditions, compiler optimizations, and
bit-vector rewrites.
1
Introduction
Satisﬁability Modulo Theories (SMT) solving for the theory of ﬁxed-size bit-
vectors has received a lot of interest in recent years. Many applications rely on
bit-precise reasoning as provided by SMT solvers, and the number of solvers that
participate in the corresponding divisions of the annual SMT competition is high
and increasing. Although theoretically diﬃcult (e.g., [14]), bit-vector solvers are
in practice highly eﬃcient and typically implement SAT-based procedures. Rea-
soning about ﬁxed-size bit-vectors suﬃces for many applications. In hardware
veriﬁcation, the size of a circuit is usually known in advance, and in software ver-
iﬁcation, machine integers are treated as ﬁxed-size bit-vectors, where the width
depends on the underlying architecture. Current solving approaches, however, do
not generalize beyond this limitation, i.e., they cannot reason about parametric
circuits or machine integers of arbitrary size. This is a serious limitation when
one wants to prove properties that are bit-width independent. Further, when rea-
soning about machine integers of a ﬁxed but large size, as employed, for example,
This work was supported in part by DARPA (awards N66001-18-C-4012 and FA8650-
18-2-7861), ONR (award N68335-17-C-0558), NSF (award 1656926), and the Stanford
Center for Blockchain Research.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 366–384, 2019.
https://doi.org/10.1007/978-3-030-29436-6_22

Towards Bit-Width-Independent Proofs in SMT Solvers
367
in smart contract languages such as Solidity [28], current approaches do not per-
form as well in the presence of expensive operations such as multiplication [15].
To address this limitation we propose a general method for reasoning about
bit-vector formulas with parametric bit-width. The essence of the method is to
replace the translation from ﬁxed-size bit-vectors to propositional logic (which
is at the core of state-of-the-art bit-vector solvers) with a translation to the
quantiﬁed theories of integer arithmetic and uninterpreted functions. We obtain
a fully automated veriﬁcation process by capitalizing on recent advances in SMT
solving for these theories.
The reliability of our approach depends on the correctness of the SMT solvers
in use. Interactive theorem provers, or proof assistants, such as Isabelle and
Coq [20,29], on the other hand, target applications where trust is of higher
importance than automation, although substantial progress towards increasing
the latter has been made in recent years [5]. Our long-term goal is an eﬃcient
automated framework for proving bit-width independent properties within a
trusted proof assistant, which requires both a formalization of such properties in
the language of the proof assistant and the development of eﬃcient automated
techniques to reason about these properties. This work shows that state-of-the-
art SMT solving combined with our encoding techniques make the latter feasible.
The next steps towards this goal are described in the ﬁnal section of this paper.
Translating a formula from the theory of ﬁxed-size bit-vectors to the theory
of integer arithmetic is not straightforward. This is due to the fact that the
semantics of bit-vector operators are deﬁned modulo the bit-width n, which
must be expressed using exponentiation terms 2n. Most SMT solvers, however,
do not support unrestricted exponentiation. Furthermore, operators such as bit-
wise and and or do not have a natural representation in integer arithmetic. While
they are deﬁnable in the theory of integer arithmetic using β-function encodings
(e.g., [10]), such a translation is expensive as it requires an encoding of sequences
into natural numbers. Instead, we introduce an uninterpreted function (UF) for
each of the problematic operators and axiomatize them with quantiﬁed formulas,
which shifts some of the burden from arithmetic to UF reasoning. We consider
two alternative axiomatizations: a complete one relaying on induction, and a
partial (hand-crafted) one that can be understood as an under-approximation.
To evaluate the potential of our approach, we examine three case studies that
arise from real applications where reasoning about bit-width independent prop-
erties is essential. Niemetz et al. [19] deﬁned invertibility conditions for bit-vector
operators, which they then used to solve quantiﬁed bit-vector formulas. How-
ever, correctness of the conditions was only checked for speciﬁc bit-widths: from
1 to 65. As a ﬁrst case study, we consider the bit-width independent veriﬁcation
of these invertibility conditions, which [19] left to future work. As a second case
study, we examine the bit-width independent veriﬁcation of compiler optimiza-
tions in LLVM. For that, we use the Alive tool [17], which generates veriﬁcation
conditions for such optimizations in the theory of ﬁxed-size bit-vectors. Proving
the correctness of these optimizations for arbitrary bit-widths would ensure their
correctness for any language and underlying architecture rather than speciﬁc

368
A. Niemetz et al.
ones. As a third case study, we consider the bit-width independent veriﬁcation
of rewrite rules for the theory of ﬁxed-size bit-vectors. SMT solvers for this the-
ory heavily rely on such rules to simplify the input. Verifying their correctness is
essential and is typically done by hand, which is both tedious and error-prone.
To summarize, this paper makes the following contributions.
– In Sect. 3, we study complete and incomplete encodings of bit-vector formulas
with parametric bit-width into integer arithmetic.
– In Sect. 4, we evaluate the eﬀectiveness of both encodings in three case studies.
– As part of the invertibility conditions case study, we introduce conditional
inverses for bit-vector constraints, thus augmenting [19] with concrete para-
metric solutions.
Table 1. Considered bit-vector operators with SMT-LIB 2 syntax.
Symbol
SMT-LIB Syntax
Sort
≈, ̸≈
=, distinct
σ[n] × σ[n] →Bool
<u
BV, >u
BV, <s
BV, >s
BV
bvult, bvugt, bvslt, bvsgt
σ[n] × σ[n] →Bool
≤u
BV, ≥u
BV, ≤s
BV, ≥s
BV
bvule, bvuge, bvsle, bvsge
σ[n] × σ[n] →Bool
∼BV, −BV
bvnot, bvneg
σ[n] →σ[n]
&BV, |BV, ⊕BV
bvand, bvor, bvxor
σ[n] × σ[n] →σ[n]
<<BV, >>BV, >>a
BV
bvshl, bvlshr, bvashr
σ[n] × σ[n] →σ[n]
+BV, ·BV, modBV, divBV bvadd, bvmul, bvurem, bvudiv σ[n] × σ[n] →σ[n]
[u : l]BV
extract (0 ≤l ≤u < n)
σ[n] →σ[u−l+1]
◦BV
concatenation
σ[n] × σ[m] →σ[n+m]
Related Work. Bit-width independent bit-vector formulas were studied by Picora
[22], who introduced a formal language for bit-vectors of parametric width, along
with a semantics and a decision procedure. The language we use here is a sim-
pliﬁed variant of that language. A uniﬁcation-based algorithm for bit-vectors of
symbolic lengths is discussed by Bjørner and Picora [4]. Bit-width independent
formulas are related to parametric Boolean functions and circuits. An induc-
tive approach for reasoning about such formalisms was developed by Gupta and
Fisher [11,12] by considering a Boolean function for the base case of a circuit and
another one for its inductive step. Reasoning about equivalence of such circuits
can be embedded in the framework of [22].
2
Preliminaries
We brieﬂy review the usual notions and terminology of many-sorted ﬁrst-order
logic with equality (denoted by ≈). See [10,30] for more detailed information.
Let S be a set of sort symbols, and for every sort σ ∈S, let Xσ be an inﬁnite set

Towards Bit-Width-Independent Proofs in SMT Solvers
369
of variables of sortσ. We assume that sets Xσ are pairwise disjoint and deﬁne X
as the union of sets Xσ. A signature Σ consists of a set Σs ⊆S of sort symbols
and a set Σf of function symbols. Arities of function symbols are deﬁned in the
usual way. Constants are treated as 0-ary functions. We assume that Σ includes
a Boolean sort Bool and the Boolean constants ⊤(true) and ⊥(false). Functions
returning Bool are also called predicates.
We assume the usual deﬁnitions of well-sorted terms, literals, and formulas,
and refer to them as Σ-terms, Σ-literals, and Σ-formulas, respectively. We deﬁne
x = (x1, ..., xn) as a tuple of variables and write Qxϕ with Q ∈{∀, ∃} for a
quantiﬁed formula Qx1 · · · Qxnϕ. For a Σ-term or Σ-formula e, we denote the
free variables of e (deﬁned as usual) as FV(e) and use e[x] to denote that the
variables in x occur free in e. For a tuple of Σ-terms t = (t1, ..., tn) and a tuple
of Σ-variables x = (x1, . . . , xn), we write e {x 	→t} for the term or formula
obtained from e by simultaneously replacing each occurrence of xi in e by ti.
A Σ-interpretation I maps: each σ ∈Σs to a distinct non-empty set of
values σI (the domain of σ in I); each x ∈Xσ to an element xI ∈σI; and each
f σ1···σnσ ∈Σf to a total function f I: σI
1 × ... × σI
n →σI if n > 0, and to an
element in σI if n = 0. We use the usual inductive deﬁnition of a satisﬁability
relation |= between Σ-interpretations and Σ-formulas.
A theory T is a pair (Σ, I), where Σ is a signature and I is a non-empty class
of Σ-interpretations that is closed under variable reassignment, i.e., if interpreta-
tion I′ only diﬀers from an I ∈I in how it interprets variables, then also I′ ∈I.
A Σ-formula ϕ is T-satisﬁable (resp. T-unsatisﬁable) if it is satisﬁed by some
(resp. no) interpretation in I; it is T-valid if it is satisﬁed by all interpretations
in I. We will sometimes omit T when the theory is understood from context.
The theory TBV = (ΣBV, IBV) of ﬁxed-size bit-vectors as deﬁned in the SMT-
LIB 2 standard [3] consists of the class of interpretations IBV and signature ΣBV,
which includes a unique sort for each positive integer n (representing the bit-
vector width), denoted here as σ[n]. For a given positive integer n, the domain
σ[n]I of sort σ[n] in I is the set of all bit-vectors of size n. We assume that
ΣBV includes all bit-vector constants of sort σ[n] for each n, represented as bit-
strings. However, to simplify the notation we will sometimes denote them by the
corresponding natural number in {0, . . . , 2n−1}. All interpretations I ∈IBV are
identical except for the value they assign to variables. They interpret sort and
function symbols as speciﬁed in SMT-LIB 2. All function symbols (of non-zero
arity) in Σf
BV are overloaded for every σ[n] ∈Σs
BV. We denote a ΣBV-term (or bit-
vector term) t of width n as t[n] when we want to specify its bit-width explicitly.
We refer to the i-th bit of t[n] as t[i] with 0 ≤i < n. We interpret t[0] as the least
signiﬁcant bit (LSB), and t[n −1] as the most signiﬁcant bit (MSB), and denote
bit ranges over k from index j down to i as t[j : i]. The unsigned interpretation of
a bit-vector t[n] as a natural number is given by [t]N = Σn−1
i=0 t [i]·2i, and its signed
interpretation as an integer is given by [t]Z = −t [n −1] · 2n−1 + [t[n −2 : 0]BV]N.
Without loss of generality, we consider a restricted set of bit-vector function
and predicate symbols (or bit-vector operators) as listed in Table 1. The selection
of operators in this set is arbitrary but complete in the sense that it suﬃces to

370
A. Niemetz et al.
express all bit-vector operators deﬁned in SMT-LIB 2. We use maxs
BV
[k] (mins
BV
[k])
for the maximum or minimum signed value of width k, e.g., maxs
BV
[4] = 0111 and
mins
BV
[4] = 1000.
The theory TIA = (ΣIA, IIA) of integer arithmetic is also deﬁned as in the
SMT-LIB 2 standard. The signature ΣIA includes a single sort Int, function and
predicate symbols {+, −, ·, div, mod, |...|, <, ≤, >, ≥}, and a constant symbol for
every integer value. We further extend ΣIA to include exponentiation, denoted
in the usual way as ab. All interpretations I ∈IIA are identical except for the
values they assign to variables. We write TUFIA to denote the (combined) theory
of uninterpreted functions with integer arithmetic. Its signature is the union of
the signature of TIA with a signature containing a set of (freely interpreted)
function symbols, called uninterpreted functions.
2.1
Parametric Bit-Vector Formulas
We are interested in reasoning about (classes of) ΣBV-formulas that hold inde-
pendently of the sorts assigned to their variables or terms. We formalize the
notion of parametric ΣBV-formulas in the following.
We ﬁx two sets X∗and Z∗of variable and constant symbols, respectively,
of bit-vector sort of undetermined bit-width. The bit-width is provided by the
ﬁrst component of a separate function pair ω = (ωb, ωN) which maps symbols
x ∈X∗∪Z∗to ΣIA-terms. We refer to ωb(x) as the symbolic bit-width assigned
by ω to x. The second component of ω is a map ωN from symbols z ∈Z∗to
ΣIA-terms. We call ωN(z) the symbolic value assigned by ω to z. Let v = FV(ω)
be the set of (integer) free variables occurring in the range of either ωb or ωN. We
say that ω is admissible if for every interpretation I ∈IIA that interprets each
variable in v as a positive integer, and for every x ∈X∗∪Z∗, I also interprets
ωb(x) as a positive integer.
Let ϕ be a formula built from the function symbols of ΣBV and X∗∪Z∗,
ignoring their sorts. We refer to ϕ as a parametric ΣBV-formula. One can inter-
pret ϕ as a class of ﬁxed-size bit-vector formulas as follows. For each symbol
x ∈X∗and integer n > 0, we associate a unique variable xn of (ﬁxed) bit-vector
sort σ[n]. Given an admissible ω with v = FV(ω) and an interpretation I that
maps each variable in v to a positive integer, let ϕ|ω[I] be the result of replac-
ing all symbols x ∈X∗in ϕ by the corresponding bit-vector variable xk and
all symbols x ∈Z∗in ϕ by the bit-vector constant of sort σ[k]corresponding to
ωN(x)I mod 2k, where in both cases k is the value of ωb(x)I. We say a formula ϕ
is well sorted under ω if ω is admissible and ϕ|ω[I] is a well-sorted ΣBV-formula
for all I that map variables in v to positive integers.
Example 1. Let X∗be the set {x} and Z∗be the set {z0, z1}, where ωN(z0) = 0
and ωN(z1) = 1. Let ϕ be the formula (x +BVx) +BVz1 ̸≈z0. We have that ϕ is
well sorted under (ωb, ωN) with ωb = {x 	→a, z0 	→a, z1 	→a} or ωb = {x 	→
3, z0 	→3, z1 	→3}. It is not well sorted when ωb = {x 	→a1, z0 	→a1, z1 	→a2}
since ϕ|ω[I] is not a well sorted ΣBV-formula whenever aI
1 ̸= aI
2. Note that an

Towards Bit-Width-Independent Proofs in SMT Solvers
371
ω where ωb(x) = a1 −a2 is not admissible, since (a1 −a2)I ≤0 is possible even
when aI
1 > 0 and aI
2 > 0.
Notice that symbolic constants such as the maximum unsigned constant of a
symbolic length w can be represented by introducing z ∈Z∗with ωb(z) = w and
ωN(z) = 2w −1. Furthermore, recall that signature ΣBV includes the bit-vector
extract operator, which is parameterized by two natural numbers u and l. We
do not lift the above deﬁnitions to handle extract operations having symbolic
ranges, e.g., where u and l are ΣIA-terms. This is for simplicity and comes at no
loss of expressive power, since constraints involving extract can be equivalently
expressed using constraints involving concatenation. For example, showing that
every instance of a constraint s ≈t[u : l]BV holds, where 0 < l ≤u < n −1, is
equivalent to showing that t ≈y1◦BV(y2◦BVy3) ⇒s ≈y2 holds for all y1, y2, y3,
where y1, y2, y3 have sorts σ[n−1−u], σ[u−l+1], σ[l], respectively. We may reason
about a formula involving a symbolic range {l, . . . , u} of t by considering a para-
metric bit-vector formula that encodes a formula of the latter form, where the
appropriate symbolic bit-widths are assigned to symbols introduced for y1, y2, y3.
We assume the above deﬁnitions for parametric ΣBV-formulas are applied to
parametric ΣBV-terms as well. Furthermore, for any admissible ω, we assume ω
can be extended to terms t of bit-vector sort that are well sorted under ω such
that t|ω[I] has sort σ[ωb(t)I] for all I that map variables in FV(ω) to positive
integers. Such an extension of ω to terms can be easily computed in a bottom-
up fashion by computing ω for each child and then applying the typing rules of
the operators in ΣBV. For example, we may assume ωb(t) = ωb(t2) if t is of the
form t1 +BVt2 and is well sorted under ω, and ωb(t) = ωb(t1) + ωb(t2) if t is of
the form t1◦BVt2.
Finally, we extend the notion of validity to parametric bit-vector formulas.
Given a formula ϕ that is well sorted under ω, we say ϕ is TBV-valid under ω if
ϕ|ω[I] is TBV-valid for all I that that map variables in FV(ω) to positive integers.
3
Encoding Parametric Bit-Vector Formulas in SMT
Current SMT solvers do not support reasoning about parametric bit-vector for-
mulas. In this section, we present a technique for encoding such formulas as
formulas involving non-linear integer arithmetic, uninterpreted functions, and
universal quantiﬁers. In SMT parlance, these are formulas in the UFNIA logic.
Given a formula ϕ that is well sorted under some mapping ω, we describe this
encoding in terms of a translation T , which returns a formula ψ that is valid in
the theory of uninterpreted functions with integer arithmetic only if ϕ is TBV-
valid under ω. We describe several variations on this translation and discuss
their relative strengths and weaknesses.
Overall Approach. At a high level, our translation produces an implication
whose antecedent requires the integer variables to be in the correct ranges (e.g.,
k > 0 for every bit-width variable k), and whose conclusion is the result of
converting each (parametric) bit-vector term of bit-width k to an integer term.

372
A. Niemetz et al.
Operations on parametric bit-vector terms are converted to operations on the
integers modulo 2k, where k can be a symbolic constant. We ﬁrst introduce
uninterpreted functions that will be used in our translation. Note that SMT
solvers may not support the full set of functions in our extended signature ΣIA,
since they typically do not support exponentiation. Since translation requires a
limited form of exponentiation we introduce an uninterpreted function symbol
pow2 of sort Int →Int, whose intended semantics is the function λx.2x when the
argument x is non-negative. Second, for each (non-predicate) n-ary (with n > 0)
function f BVof sort σ1 × . . . × σn →σ in the signature of ﬁxed-size bit-vectors
ΣBV (excluding bit-vector extraction), we introduce an uninterpreted function
f N of arity n + 1 and sort Int × Int × . . . × Int →Int, where the extra argument is
used to specify the bit-width. For example, for +BV with sort σ[n] × σ[n] →σ[n],
we introduce +N of sort Int × Int × Int →Int. In its intended semantics, this
function adds the second and third arguments, both integers, and returns the
result modulo 2k, where k is the ﬁrst argument. The signature ΣBV contains one
function, bit-vector concatenation ◦BV, whose two arguments may have diﬀerent
sorts. For this case, the ﬁrst argument of ◦N indicates the bit-width of the third
argument, i.e., ◦N(k, x, y) is interpreted as the concatenation of x and y, where
y is an integer that encodes a bit-vector of bit-width k; the bit-width for x is
not speciﬁed by an argument, as it is not needed for the elimination of this
operator we perform later. We introduce uninterpreted functions for each bit-
vector predicate in a similar fashion. For instance, ≥u
N has sort Int × Int × Int →
Bool and encodes whether its second argument is greater than or equal to its
third argument, when these two arguments are interpreted as unsigned bit-vector
values whose bit-width is given by its ﬁrst argument. Depending on the variation
of the encoding, our translation will either introduce quantiﬁed formulas that
fully axiomatize the behavior of these uninterpreted functions or add (quantiﬁed)
lemmas that state key properties about them, or both.
Translation Function. Figure 1 deﬁnes our translation function TA, which is
parameterized by an axiomatization mode A. Given an input formula ϕ that is
well sorted under ω, it returns the implication whose antecedant is an axiomati-
zation formula AXA(ϕ, σ) and whose conclusion is the result of converting ϕ to
its encoded version via the conversion function Conv. The former is dependent
upon the axiomatization mode A which we discuss later. We assume without loss
of generality that ϕ contains no applications of bit-vector extract, which can be
eliminated as described in the previous section, nor does it contain concrete bit-
vector constants, since these can be equivalently represented by introducing a
symbol in Z∗with the appropriate concrete mappings in ωb and ωN.
In the translation, we use an auxiliary function Conv which converts para-
metric bit-vector expressions into integer expressions with uninterpreted func-
tions. Parametric bit-vector variables x (that is, symbols from X∗) are replaced
by unique integer variables of type Int, where we assume a mapping χ maintains
this correspondence, such that range of χ does not include any variable that
occurs in FV(ω). Parametric bit-vector constants z (that is, symbols from set
Z∗) are replaced by the term ωN(z) mod pow2(ωb(z)). The ranges of the maps

Towards Bit-Width-Independent Proofs in SMT Solvers
373
Fig. 1. Translation TA for parametric bit-vector formulas, parametrized by axiomati-
zation mode A. We use utsk(x) as shorthand for 2 · (x mod pow2(k −1)) −x.
in ω may contain arbitrary ΣIA-terms. In practice, our translation handles only
cases where these terms contain symbols supported by the SMT solver, as well
as terms of the form 2t, which we assume are replaced by pow2(t) during this
translation. For instance, if ωb(z) = w + v and ωN(z) = 2w −1, then Conv(z)
returns (pow2(w) −1) mod pow2(w + v). Equalities are processed by recursively
running the translation on both sides. The next case handles symbols from the
signature ΣBV, where symbols f BV are replaced with the corresponding uninter-
preted function f N. We take as the ﬁrst argument ωb(tn), indicating the symbolic
bit-width of the last argument of e, and recursively call Conv on t1, . . . , tn. In
all cases, ωb(tn) corresponds to the bit-width that the uninterpreted function f N
expects based on its intended semantics (the bit-width of the second argument
for bit-vector concatenation, or of an arbitrary argument for all other functions
and predicates). Finally, if the top symbol of e is a Boolean connective we apply
the conversion function recursively to all its children.
We run Elim for all applications of uninterpreted functions f N introduced
during the conversion, which eliminates functions that correspond to a majority
of the bit-vector operators. These functions can be equivalently expressed using
integer arithmetic and pow2. The ternary addition operation +N, that represents

374
A. Niemetz et al.
addition of two bit-vectors with their width k speciﬁed as the ﬁrst argument,
is translated to integer addition modulo pow2(k). Similar considerations are
applied for −N and ·N. For divN and modN, our translation handles the special
case where the second argument is zero, where the return value in this case
is the maximum value for the given bit-width, i.e. pow2(k) −1. The integer
operators corresponding to unary (arithmetic) negation and bit-wise negation
can be eliminated in a straightforward way. The semantics of various bitwise
shift operators can be deﬁned arithmetically using division and multiplication
with pow2(k). Concatenation can be eliminated by multiplying its ﬁrst argument
x by pow2(k), where recall k is the bit-width of the second arugment y. In other
words, it has the eﬀect of shifting x left by k bits, as expected. The unsigned
relation symbols can be directly converted to the corresponding integer relation.
For the elimination of signed relation symbols we use an auxiliary helper uts
(unsigned to signed), deﬁned in Fig. 1, which returns the interpretation of its
argument when seen as a signed value. The deﬁnition of uts can be derived
based on the semantics for signed and unsigned bit-vector values in the SMT
LIB standard. Based on this deﬁnition, we have that integers v and u that encode
bit-vectors of bit-width k satisfy <s
N(k, u, v) if and only if utsk(u) < utsk(v).
As an example of our translation, let ϕ = (x +BVx) +BVz1 ̸≈z0, ωN(z0) =
0, ωN(z1)
=
1, and ωb(x)
=
ωb(z0)
=
ωb(z1)
=
a from Example 1.
Conv(ϕ, (ωb, ωN)) is Elim(+N(a, Elim(+N(a, χ(x), χ(x))), 1 mod pow2(a))) ̸≈
0 mod pow2(a). After applying Elim and simplifying, we get (χ(x) + χ(x) +
1) mod pow2(a) ̸≈0.
Thanks to Elim, we can assume that all formulas generated by Conv con-
tain only uninterpreted function symbols in the set {pow2, &N, |
N, ⊕N}. Thus, we
restrict our attention to these symbols only in our axiomatization AXA, described
next.
Table 2. Full axiomatization of pow2, &N, and ⊕N. The axiomatization of |N is omitted,
and is dual to that of &N. We use exi(x) for (x div pow2(i)) mod 2.
⋄
AX⋄
full
pow2 pow2(0) ≈1 ∧∀k. k > 0 ⇒pow2(k) ≈2 · pow2(k −1)
&N
∀k, x, y. &N(k, x, y) ≈
ite(k > 1, &N(k −1, x mod pow2(k −1), y mod pow2(k −1)), 0) +
pow2(k −1) · min(exk−1(x), exk−1(y))
⊕N
∀k, x, y. ⊕N(k, x, y) ≈
ite(k > 1, ⊕N(k −1, x mod pow2(k −1), y mod pow2(k −1)), 0) +
pow2(k −1) · |exk−1(x) −exk−1(y)|
Axiomatization Modes. We consider four diﬀerent axiomatization modes A,
which we call full, partial, combined, and qf (quantiﬁer-free). For each of these
axiomatizations, we deﬁne AXA(ϕ, ω) as the conjunction:

Towards Bit-Width-Independent Proofs in SMT Solvers
375
Table 3. Partial axiomatization of pow2, &N, and ⊕N. The axioms for |N are omitted,
and are dual to those for &N. We use maxN
k for pow2(k) −1.
⋄
axiom
AX⋄
partial
pow2 base cases
pow2(0) ≈1 ∧pow2(1) ≈2 ∧pow2(2) ≈4 ∧pow2(3) ≈8
weak monotonicity
∀i∀j. i ≤j ⇒pow2(i) ≤pow2(j)
strong monotonicity ∀i∀j. i < j ⇒pow2(i) < pow2(j)
modularity
∀i∀j∀x. (x · pow2(i)) mod pow2(j) ̸≈0 ⇒i < j
never even
∀i∀x. pow2(i) −1 ̸≈2 · x
always positive
∀i. pow2(i) ≥1
div 0
∀i. i div pow2(i) ≈0
&N
base case
∀x∀y. &N(1, x, y) ≈min(ex0(x), ex0(y))
max
∀k∀x. &N(k, x, maxN
k ) ≈x
min
∀k∀x. &N(k, x, 0) ≈0
idempotence
∀k∀x. &N(k, x, x) ≈x
contradiction
∀k∀x. &N(k, x, ∼N(k, x)) ≈0
symmetry
∀k∀x∀y. &N(k, x, y) ≈&N(k, y, x)
diﬀerence
∀k∀x∀y∀z. x ̸≈y ⇒&N(k, x, z) ̸≈y∨&N(k, y, z) ̸≈x
range
∀k∀x∀y. 0 ≤&N(k, x, y) ≤min(x, y)
⊕N
base case
∀x∀y. ⊕N(1, x, y) ≈ite(ex0(x) ≈ex0(y), 0, 1)
zero
∀k∀x. ⊕N(k, x, x) ≈0
one
∀k∀x. ⊕N(k, x, ∼N(k, x)) ≈maxN
k
symmetry
∀k∀x∀y. ⊕N(k, x, y) ≈⊕N(k, y, x)
range
∀k∀x∀y. 0 ≤⊕N(k, x, y) ≤maxN
k

x∈FV(ϕ)
0 ≤χ(x) < pow2(ωb(x)) ∧(

w∈FV(ω)
w > 0) ∧AXpow2
A
∧AX&N
A ∧AX|N
A ∧AX⊕N
A
The ﬁrst conjunction states that all integer variables introduced for paramet-
ric bit-vector variables x reside in the range speciﬁed by their bit-width. The
second conjunction states that all free variables in ω (denoting bit-widths) are
positive. The remaining four conjuncts denote the axiomatizations for the four
uninterpreted functions that may occur in the output of the conversion func-
tion. The deﬁnitions of these formulas are given in Tables 2 and 3 for full and
partial respectively. For each axiom, i, j, k denote bit-widths and x, y denote
integers that encode bit-vectors of size k. We assume guards on all quantiﬁed
formulas (omitted for brevity) that constrain i, j, k to be positive and x, y to
be in the range {0, . . . , pow2(k) −1}. Each table entry lists a set of formulas
(interpreted conjunctively) that state properties about the intended semantics
of these operators. The formulas for axiomatization mode full assert the intended
semantics of these operators, whereas those for partial assert several properties
of them. Mode combined asserts both, and mode qf takes only the formulas in
partial that are quantiﬁer-free. In particular, AXpow2
qf
corresponds to the base
cases listed in partial, and AX⋄
qf for the other operators is simply ⊤. The partial
axiomatization of these operations mainly includes natural properties of them.
For example, we include some base cases for each operation, and also the ranges
of its inputs and output. For some proofs, these are suﬃcient. For &N, |
N and
⊕N, we also included their behavior for speciﬁc cases, e.g., &N(k, a, 0) = 0 and its

376
A. Niemetz et al.
variants. Other axioms (e.g., “never even”) were added after analyzing speciﬁc
benchmarks to identify suﬃcient axioms for their proofs.
Our translation satisﬁes the following key properties.
Theorem 2. Let ϕ be a parameteric bit-vector formula that is well sorted under
ω and has no occurrences of bit-vector extract or concrete bit-vector constants.
Then:
1. ϕ is TBV-valid under ω if and only if Tfull(ϕ, ω) is TUFIA-valid.
2. ϕ is TBV-valid under ω if and only if Tcombined(ϕ, ω) is TUFIA-valid.
3. ϕ is TBV-valid under ω if Tpartial(ϕ, ω) is TUFIA-valid.
4. ϕ is TBV-valid under ω if Tqf(ϕ, ω) is TUFIA-valid.1
The proof of Property 1 is carried out by translating every interpretation
IBV of TBV into a corresponding interpretation IN of TUFIA such that IBV
satisﬁes ϕ iﬀIN satisﬁes Tfull(ϕ). The converse translation can be achieved
similarly, where appropriate bit-widths are determined by the range axioms
0 ≤χ(x) < pow2(ωb(x)) that occur in Tfull(ϕ, ω). The rest of the properties
follow from Property 1, by showing that the axioms in Table 3 are valid in every
interpretation of TUFIA that satisﬁes AXfull(ϕ, ω).
4
Case Studies
We apply the techniques from Sect. 3 to three case studies: (i) veriﬁcation of
invertibility conditions from Niemetz et al. [19]; (ii) veriﬁcation of compiler opti-
mizations as generated by Alive [17]; and (iii) veriﬁcation of rewrite rules that
are used in SMT solvers. For these case studies, we consider a set of veriﬁca-
tion conditions that originally use ﬁxed-size bit-vectors, and exclude formulas
involving multiple bit-widths.
For each formula φ, we ﬁrst extract a parametric version ϕ by replacing each
variable in φ by a fresh x ∈X∗and each (concrete) bit-vector constant by a
fresh z ∈Z∗. We deﬁne ωb(x) = ωb(z) = k for a fresh integer variable k, and let
ωN(z) be the integer value corresponding to the bit-vector constant it replaced.
Notice that, although omitted from the presentation, our translation can be
easily extended to handle quantiﬁed bit-vector formulas, which appear in some
of the case studies. We then deﬁne ω = (ωb, ωN) and invoke our translation from
Sect. 3 on the parametric bit-vector formula ϕ. If the resulting formula is valid,
the original veriﬁcation condition holds independent of the original bit-width.
In each case study, we report on the success rates of determining the validity of
these formulas for axiomatization modes full, partial, combined, and qf. Overall,
axiomatization mode combined yields the best results.
All experiments described below require tools with support for the SMT logic
UFNIA. We used all three participants in the UFNIA division of the 2018 SMT
1 A detailed proof, along with further details that were omitted from this paper can
be found in its extended version at https://arxiv.org/abs/1905.10434.

Towards Bit-Width-Independent Proofs in SMT Solvers
377
competition: CVC4 [2] (GitHub master 6eb492f6), Z3 [8] (version 4.8.4), and
Vampire [13] (GitHub master d0ea236). Z3 and CVC4 use various strategies and
techniques for quantiﬁer instantiation including E-matching [18], and enumera-
tive [24] and conﬂict-based [27] instantiation. For non-linear integer arithmetic,
CVC4 uses an approach based on incremental linearization [6,7,26]. Vampire is
a superposition-based theorem prover for ﬁrst-order logic based on the AVATAR
framework [31], which has been extended also to support some theories includ-
ing integer arithmetic [23]. We performed all experiments on a cluster with Intel
Xeon E5-2637 CPUs with 3.5 GHz and 32 GB of memory and used a time limit
of 300 s (wallclock) and a memory limit of 4 GB for each solver/benchmark pair.
We consider a bit-width independent property to be proved if at least one solver
proved it for at least one of the axiomatization modes.2
4.1
Verifying Invertibility Conditions
Niemetz et al. [19] present a technique for solving quantiﬁed bit-vector formulas
that utilizes invertibility conditions to generate symbolic instantiations. Intu-
itively, an invertibility condition φc for a literal ℓ[x] is the exact condition under
which ℓ[x] has a solution for x, i.e., φc ⇔∃x.ℓ[x]. For example, consider bit-vector
literal x &BVs ≈t with x ̸∈FV(s) ∪FV(t); then, the invertibility condition for x
is t &BVs ≈t.
The authors deﬁne invertibility conditions for a representative set of literals
having a single occurrence of x, that involve the bit-vector operators listed in
Table 1, excluding extraction, as the invertibility condition for the latter is triv-
ially ⊤. A considerable number of these conditions were determined by leverag-
ing syntax-guided synthesis (SyGuS) techniques [1]. The authors further veriﬁed
the correctness of all conditions for bit-widths 1 to 65. However, a bit-width-
independent formal proof of correctness of these conditions was left to future
work. In the following, we apply the techniques of Sect. 3 to tackle this problem.
Note that for this case study, we exclude operators involving multiple bit-widths,
namely bit-vector extraction and concatenation. For the former, all invertibility
conditions are ⊤, and for the latter a hand-written proof of the correctness of
its invertibility conditions can be achieved easily.
Proving Invertibility Conditions. Let ℓ[x] be a bit-vector literal of the form
⋄x ▷◁t or x ⋄s ▷◁t (dually, s ⋄x ▷◁t) with operators ⋄and relations ▷◁as
deﬁned in Table 1. To prove the correctness of an invertibility condition φc for x
independent of the bit-width, we have to prove the validity of the formula:
φc ⇔∃x.ℓ[x]
(1)
where occurrences of s and t are implicitly universally quantiﬁed. We then want
to prove that Eq. 1 is TBV-valid under ω. Considering the two directions of (1)
separately, we get:
2 All benchmarks, results, log ﬁles, and solver conﬁgurations are available at http://
cvc4.cs.stanford.edu/papers/CADE2019-BVPROOF/.

378
A. Niemetz et al.
∃x.ℓ[x, s, t] ⇒φc[s, t]
(rtl)
φc[s, t] ⇒∃x.ℓ[x, s, t]
(ltr)
The validity of (rtl) is equivalent to the unsatisﬁability of the quantiﬁer-free
formula:
ℓ[x, s, t] ∧¬φc[s, t]
(rtl’)
Eliminating the quantiﬁer in (ltr) is much trickier. It typically amounts to ﬁnding
a symbolic value for x such that ℓ[x, s, t] holds provided that φc[s, t] holds. We
refer to such a symbolic value as a conditional inverse.
Conditional Inverses. Given an invertibility condition φc for x in bit-vector
literal ℓ[x], we say that a term αc is a conditional inverse for x if φc ⇒ℓ[αc]
is TBV-valid. For example, the term s itself is a conditional inverse for x in the
literal (x |
BVs) ≤u
BVt: given that there exists some x such that (x |
BVs) ≤u
BVt, we
have that (s |
BVs)≤u
BVt. When a conditional inverse αc for x is found, we may
replace (ltr) by:
φc ⇒ℓ[αc]
(ltr’)
Clearly, (ltr’) implies (ltr). However, the converse may not hold, i.e., if (ltr’) is
refuted, (ltr) is not necessarily refuted. Notice that if the invertibility condition
for x is ⊤, the conditional inverse is in fact unconditional. The problem of ﬁnd-
ing a conditional inverse for a bit-vector literal x⋄s ▷◁t (dually, s⋄x ▷◁t) can be
deﬁned as a SyGuS problem by asking whether there exists a binary bit-vector
function C such that the (second-order) formula ∃C∀s∀t.φc ⇒C(s, t) ⋄s ▷◁t is
satisﬁable. If such a function C is found, then it is in fact a conditional inverse
for x in ℓ[x]. We synthesized conditional inverses for x in ℓ[x] for bit-width 4 with
variants of the grammars used in [19] to synthesize invertibility conditions. For
each grammar we generated 160 SyGuS problems, one for each combination of
bit-vector operator and relation from Table 1 (excluding extraction and concate-
nation), counting commutative cases only once. We used the SyGuS feature of
the SMT solver CVC4 [25] to solve these problems, and out of 160, we were able
to synthesize candidate conditional inverses for 143 invertibility conditions. For
12 out of these 143, we found that the synthesized terms were not conditional
inverses for every bit-width, by checking (ltr’) for bit-widths up to 64.
Results. Table 4 provides detailed information on the results for the axioma-
tization modes full, partial, and qf discussed in Sect. 3. We use →and
→
to
indicate that only direction left-to-right ((ltr) or (ltr’)) or right-to-left (rtl’),
respectively, were proved, and ✓and ✕to indicate that both or none, respec-
tively, of the directions were proved. Additionally, we use →αc (resp. →no αc) to

Towards Bit-Width-Independent Proofs in SMT Solvers
379
indicate that for direction left-to-right, formula (ltr’) (resp. (ltr)) was proved
with (resp. without) plugging in a conditional inverse.
Overall, out of 160 invertibility conditions, we were able to fully prove 110,
and for 19 (17) conditions we were able to prove only direction (rtl’) (ltr’).
For direction right-to-left, 129 formulas (rtl’) overall were successfully proved to
be unsatisﬁable. Out of these 129, 32 formulas were actually trivial since the
invertibility condition φc was ⊤. For direction left-to-right, overall, 127 formulas
were proved successfully, and out of these, 102 (94) were proved using (resp. not
using) a conditional inverse. Furthermore, 33 formulas could only be proved
when using a conditional inverse. Thus, using conditional inverses was helpful
for proving the correctness of invertibility conditions.
Considering the diﬀerent axiomatization modes, overall, with 104 fully proved
and only 17 unproved instances, combined performed best. Interestingly, even
though axiomatization qf only includes some of the base cases of axiomatization
partial, it still performs well. This may be due to the fact that in many cases,
the correctness of the invertibility condition does not rely on any particular
property of the operators involved. For example, the invertibility condition φc
for literal x &BVs ≈t is t &BVs ≈t. Proving the correctness of φc amounts to
coming up with the right substitution for x, without relying on any particular
axiomatization of &N. In contrast, the invertibility condition φc for literal x &BV
s ̸≈t is t ̸≈0 ∨s ̸≈0. Proving the correctness of φc relies on axioms regarding
&BV and ∼BV. Speciﬁcally, we have found that from partial, it suﬃces to keep
“min” and “idempotence” to prove φc. Overall, from the 2696 problems that
this case study included, CVC4 proved 50.3%, Vampire proved 31.4%, and Z3
proved 33.8%, while 23.5% of the problems were proved by all solvers.
Table 4. Invertibility condition veriﬁcation using axiomatization modes combined,
full, partial, and qf. Column →αc(→no αc) counts left-to-right proved with (without)
conditional inverse.
Axiomatization
✓
→
→
✕
→αc
→no αc
full
64 18 22 56
72
51
partial
76 14 26 44
78
81
qf
40 22 22 76
50
51
combined
104 21 18 17
99
79
Total (160)
110 19 17 14
102
94
4.2
Verifying Alive Optimizations
Lopes et al. [17] introduces Alive, a tool for proving the correctness of compiler
peephole optimizations. Alive has a high-level language for specifying optimiza-
tions. The tool takes as input a description of an optimization in this high-level
language and then automatically veriﬁes that applying the optimization to an
arbitrary piece of source code produces optimized target code that is equivalent

380
A. Niemetz et al.
under a given precondition. It can also automatically translate veriﬁed optimiza-
tions into C++ code that can be linked into LLVM [16]. For each optimization,
Alive generates four constraints that encode the following properties, assuming
that the precondition of the optimization holds:
1. Memory Source and Target yield the same state of memory after execution.
2. Deﬁnedness The target is well-deﬁned whenever the source is.
3. Poison The target produces so-called poison values (caused by LLVM’s nsw,
nuw, and exact attributes) only when the source does.
4. Equivalence Source and target yield the same result after execution.
From these veriﬁcation tasks, Alive can generate benchmarks in SMT-LIB 2
format in the theory of ﬁxed-size bit-vectors, with and without quantiﬁers. For
each task, types are instantiated with all possible valid type assignments (for
integer types up to a default bound of 64 bits). In the following, we apply our
techniques from Sect. 3 to prove Alive veriﬁcation tasks independently from the
bit-width. For this, as in the Alive paper, we consider the set of optimizations
from the instcombine optimization pass of LLVM, provided as Alive translations
(433 total).3 Of these 433 optimizations, 113 are dependent on a speciﬁc bit-
width; thus we focus on the remaining 320. We further exclude optimizations
that do not comply with the following criteria:
– In each generated SMT-LIB 2 ﬁle, only a single bit-width is used.
– All SMT-LIB 2 ﬁles generated for a property (instantiated for all possible
valid type assignments) must be identical modulo the bit-width (excluding,
e.g., bit-width dependent constants other than 0, 1, (un)signed min/max, and
the bit-width).
As a useful exception to the ﬁrst criterion, we included instances where all terms
of bit-width 1 can be interpreted as Boolean terms. Overall, we consider bit-
width independent veriﬁcation conditions 1–4 for 180 out of 320 optimizations.
None of these include memory operations or poison values, and only some have
deﬁnedness constraints (and those are simple). Hence, the generated veriﬁcation
conditions 1–3 are trivial. We thus only consider the equivalence veriﬁcation
conditions for these 180 optimizations.
Results. Table 5 summarizes the results of verifying the equivalence constraints
for the selected 180 optimizations from the instcombine LLVM optimization pass.
It ﬁrst lists all families, showing the number of bit-width independent optimiza-
tions per family (320 total). The next column indicates how many in each family
were in the set of 180 considered optimizations, and the remaining columns show
how many of those considered were proved with each axiomatization mode.
3 At https://github.com/nunoplopes/alive/tree/master/tests/instcombine.

Towards Bit-Width-Independent Proofs in SMT Solvers
381
Table 5. Alive optimizations veriﬁcation using axiomatizations combined, full, partial
and qf.
Family
Considered
Proved
full partial qf
combined Total
AddSub (52)
16
7
7
7
9
9
MulDivRem (29)
5
1
2
1
3
3
AndOrXor (162)
124
57
55
53
60
60
Select (51)
26
15
11
11
16
16
Shifts (17)
9
0
0
0
0
0
LoadStoreAlloca (9)
0
0
0
0
0
0
Total (320)
180
80
75
72
88
88
Overall, out of 180 equivalence veriﬁcation conditions, we were able to prove
88. Our techniques were most successful for the AndOrXor family. This is not too
surprising, since many veriﬁcation conditions of this family require only Boolean
reasoning and basic properties of ordering relations that are already included in
the theory TIA. For example, given bit-vector term a and bit-vector constants C1
and C2, optimization AndOrXor:979 essentially rewrites (a <s
BVC1 ∧a <s
BVC2) to
a <s
BVC1, provided that precondition C1 <s
BVC2 holds. To prove its correctness,
it suﬃces to apply the transitivity of <s
BV with Boolean reasoning. The same
holds when lifting this equivalence to the integers, deducing the transitivity of
<s
N from that of the builtin < relation of TIA.
None of the 9 benchmarks from the Shifts family were proven. These bench-
marks are more complicated than others. They combine bit-wise and arithmetical
operations and thus rely on their axiomatization. Solving these benchmarks is
an interesting challenge for future work. Adding specialized axioms to partial is
one promising approach.
Interestingly, for this case study, the results from the diﬀerent axiomatization
modes are very similar. This can again be explained by the fact that many
optimizations rely on properties of the integers that are already included in TIA,
without requiring any particular property of functions pow2, &N, |
N and ⊕N (as
in the above example).
Note that we have also tried using our approach for proving the equivalence
veriﬁcation conditions for up to a bit-width of 64. However, all optimizations
that were proven correct this way were already proven correct for arbitrary bit-
widths, which suggests that this restriction did not make the benchmarks easier.
Overall, from the 720 problems in this case study, CVC4 proved 42.6%, Vampire
proved 36.2%, and Z3 proved 37.9%, while 32.5% of the problems were proved
by all solvers.

382
A. Niemetz et al.
4.3
BV Rewriting
SMT solvers for the theory of ﬁxed-size bit-vectors heavily rely on rewriting to
reduce the size of the input formula prior to solving the problem. Since these
rewrite rules are usually implemented independently of the bit-width, verifying
that they hold for any bit-width is crucial for the soundness of the solver. For
this case study, we used a feature of the SyGuS solver in CVC4 that allows
us to enumerate equivalent bit-vector terms/formulas (rewrite candidates) for a
certain bit-width up to a certain term depth (nesting level of operators) [21].
We generated 1575 pairs of equivalent bit-vector terms of depth three and 431
equivalent pairs of formulas of depth two for bit-width 4 and translated them
to integer problems with axiomatization modes full, partial, qf, and combined,
resulting in 6300 + 1724 = 8024 benchmarks in total. Since rewrites that have
been proved can be used to further axiomatize the integer translation, we col-
lected all proven rewrites after each run, added them as axioms to the initial
problems and reran the experiments. This was repeated until we reached a ﬁx-
point, i.e., no further rewrites were proved. With this approach, we were able to
prove 409 out of the 435 formula equivalences (94%), reaching a ﬁxpoint at the
ﬁrst iteration. For the equivalent terms, we initially proved 878 out of the 1575
equivalences, which increased to 935 (59%) after adding all axioms from the ﬁrst
run, reaching a ﬁxpoint after two iterations. Overall, from the 8024 problems,
CVC4 proved 64.2%, Vampire proved 66.5%, and Z3 proved 64.2%, while 63.8%
of the problems were proved by all solvers.
5
Conclusion and Further Research
We have studied several translations from bit-vector formulas with parametric
bit-width to the theories of integer arithmetic and uninterpreted functions. The
translations diﬀer in the way that the operator 2( ) and bitwise logical operators
are axiomatized, namely, fully (using induction) or partially (using some of their
key properties). Our empirical results show that state-of-the-art SMT solvers
are capable of solving the translated formulas for various benchmarks that orig-
inate from the veriﬁcation of invertibility conditions, LLVM optimizations, and
rewriting rules for ﬁxed-size bit-vectors.
In future research, we plan to investigate a translation of our results to a proof
assistant such as Coq, for which a bit-vector library was recently developed [9].
This will involve supporting proofs in the SMT solver for non-linear arithmetic
and quantiﬁers. We believe that our promising experimental results with an
integer encoding indicate that this is a viable approach for automating bit-
width independent proofs. We also plan to explore satisﬁable benchmarks, and
to extend our approach for translating models.

Towards Bit-Width-Independent Proofs in SMT Solvers
383
References
1. Alur, R., et al.: Syntax-guided synthesis. In: Formal Methods in Computer-Aided
Design, FMCAD 2013, Portland, OR, USA, 20–23 October 2013, pp. 1–8 (2013)
2. Barrett, C., et al.: CVC4. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011.
LNCS, vol. 6806, pp. 171–177. Springer, Heidelberg (2011). https://doi.org/10.
1007/978-3-642-22110-1 14
3. Barrett, C., Stump, A., Tinelli, C.: The SMT-LIB standard: version 2.0. In: Gupta,
A., Kroening, D. (eds.) Proceedings of the 8th International Workshop on Satisﬁ-
ability Modulo Theories, Edinburgh, UK (2010)
4. BjØrner, N.S., Pichora, M.C.: Deciding ﬁxed and non-ﬁxed size bit-vectors. In:
Steﬀen, B. (ed.) Tools and Algorithms for the Construction and Analysis of Sys-
tems, pp. 376–392. Springer, Berlin (1998). https://doi.org/10.1007/BFb0054184
5. Blanchette, J.C., B¨ohme, S., Paulson, L.C.: Extending sledgehammer with SMT
solvers. J. Autom. Reasoning 51(1), 109–128 (2013). https://doi.org/10.1007/
s10817-013-9278-5
6. Cimatti, A., Griggio, A., Irfan, A., Roveri, M., Sebastiani, R.: Experimenting on
solving nonlinear integer arithmetic with incremental linearization. In: Beyersdorﬀ,
O., Wintersteiger, C.M. (eds.) SAT 2018. LNCS, vol. 10929, pp. 383–398. Springer,
Cham (2018). https://doi.org/10.1007/978-3-319-94144-8 23
7. Cimatti, A., Griggio, A., Irfan, A., Roveri, M., Sebastiani, R.: Incremental lin-
earization for satisﬁability and veriﬁcation modulo nonlinear arithmetic and tran-
scendental functions. ACM Trans. Comput. Log. 19(3), 19:1–19:52 (2018)
8. de Moura, L., Bjørner, N.: Z3: an eﬃcient SMT solver. In: Ramakrish-
nan,
C.R.,
Rehof,
J.
(eds.)
TACAS
2008.
LNCS,
vol.
4963,
pp.
337–
340. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-78800-3 24.
http://dl.acm.org/citation.cfm?id=1792734.1792766
9. Ekici, B., et al.: SMTCoq: a plug-in for integrating smt solvers into Coq. In:
Majumdar, R., Kuncak, V. (eds.) CAV 2017. LNCS, vol. 10427, pp. 126–133.
Springer, Cham (2017). https://doi.org/10.1007/978-3-319-63390-9 7
10. Enderton, H., Enderton, H.B.: A Mathematical Introduction to logic. Elsevier,
Amsterdam (2001)
11. Gupta, A., Fisher, A.L.: Parametric circuit representation using inductive boolean
functions. In: Courcoubetis, C. (ed.) CAV 1993. LNCS, vol. 697, pp. 15–28.
Springer, Heidelberg (1993). https://doi.org/10.1007/3-540-56922-7 3
12. Gupta, A., Fisher, A.L.: Representation and symbolic manipulation of linearly
inductive boolean functions. In: Proceedings of the 1993 IEEE/ACM International
Conference on Computer-aided Design, pp. 192–199, ICCAD 1993. IEEE Com-
puter Society Press, Los Alamitos (1993). http://dl.acm.org.stanford.idm.oclc.org/
citation.cfm?id=259794.259827
13. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
14. Kov´asznai, G., Fr¨ohlich, A., Biere, A.: Complexity of ﬁxed-size bit-vector logics.
Theory Comput. Syst. 59(2), 323–376 (2016). https://doi.org/10.1007/s00224-015-
9653-1
15. Kroening, D., Strichman, O.: Decision Procedures - An Algorithmic Point of View.
Texts in Theoretical Computer Science. An EATCS Series, 2nd edn. Springer,
Berlin (2016)

384
A. Niemetz et al.
16. Lattner, C., Adve, V.S.: LLVM: a compilation framework for lifelong program
analysis & transformation. In: 2nd IEEE/ACM International Symposium on Code
Generation and Optimization (CGO 2004), 20–24 March 2004, San Jose, CA, USA,
pp. 75–88. IEEE Computer Society (2004). https://doi.org/10.1109/CGO.2004.
1281665
17. Lopes, N.P., Menendez, D., Nagarakatte, S., Regehr, J.: Provably correct peephole
optimizations with alive. In: Proceedings of the 36th ACM SIGPLAN Conference
on Programming Language Design and Implementation, pp. 22–32, PLDI 2015.
ACM, New York (2015). https://doi.org/10.1145/2737924.2737965
18. de Moura, L., Bjørner, N.: Eﬃcient E-matching for SMT solvers. In: Pfenning,
F. (ed.) CADE 2007. LNCS (LNAI), vol. 4603, pp. 183–198. Springer, Heidelberg
(2007). https://doi.org/10.1007/978-3-540-73595-3 13
19. Niemetz, A., Preiner, M., Reynolds, A., Barrett, C., Tinelli, C.: Solving Quantiﬁed
Bit-Vectors Using Invertibility Conditions. In: Chockler, H., Weissenbacher, G.
(eds.) CAV 2018. LNCS, vol. 10982, pp. 236–255. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-96142-2 16
20. Nipkow, T., Wenzel, M., Paulson, L.C. (eds.): Isabelle/HOL. LNCS, vol. 2283.
Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-45949-9
21. N¨otzli, A., et al.: Syntax-guided rewrite rule enumeration for SMT solvers. In:
Janota, M., Lynce, I. (eds.) SAT 2019. LNCS, vol. 11628. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-24258-9 20
22. Pichora, M.C.: Automated reasoning about hardware data types using bit-vectors
of symbolic lengths. Ph.D. thesis, Toronto, ON, Canada (2003). aAINQ84686
23. Reger, G., Suda, M., Voronkov, A.: Uniﬁcation with abstraction and theory instan-
tiation in saturation-based reasoning. In: Beyer, D., Huisman, M. (eds.) TACAS
2018. LNCS, vol. 10805, pp. 3–22. Springer, Cham (2018). https://doi.org/10.1007/
978-3-319-89960-2 1
24. Reynolds, A., Barbosa, H., Fontaine, P.: Revisiting enumerative instantiation. In:
Beyer, D., Huisman, M. (eds.) TACAS 2018. LNCS, vol. 10806, pp. 112–131.
Springer, Cham (2018). https://doi.org/10.1007/978-3-319-89963-3 7
25. Reynolds, A., Deters, M., Kuncak, V., Tinelli, C., Barrett, C.: Counterexample-
guided quantiﬁer instantiation for synthesis in SMT. In: Kroening, D., P˘as˘areanu,
C.S. (eds.) CAV 2015. LNCS, vol. 9207, pp. 198–216. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-21668-3 12
26. Reynolds, A., Tinelli, C., Jovanovi´c, D., Barrett, C.: Designing theory solvers with
extensions. In: Dixon, C., Finger, M. (eds.) FroCoS 2017. LNCS (LNAI), vol. 10483,
pp. 22–40. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-66167-4 2
27. Reynolds, A., Tinelli, C., de Moura, L.M.: Finding conﬂicting instances of quan-
tiﬁed formulas in SMT. In: Formal Methods in Computer-Aided Design, FMCAD
2014, Lausanne, Switzerland, 21–24 October 2014, pp. 195–202 (2014). https://
doi.org/10.1109/FMCAD.2014.6987613
28. Solidity Language Developers: Solidity (2018). https://solidity.readthedocs.io/en/
v0.4.25/
29. TC Development team: The Coq proof assistant reference manual version 8.9
(2019). https://coq.inria.fr/distrib/current/refman/
30. Tinelli, C., Zarba, C.G.: Combining decision procedures for sorted theories. In:
Alferes, J.J., Leite, J. (eds.) JELIA 2004. LNCS (LNAI), vol. 3229, pp. 641–653.
Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-30227-8 53
31. Voronkov, A.: AVATAR: the architecture for ﬁrst-order theorem provers. In: Biere,
A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559, pp. 696–710. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-08867-9 46

On Invariant Synthesis
for Parametric Systems
Dennis Peuter and Viorica Sofronie-Stokkermans(B)
Universit¨at Koblenz-Landau, Koblenz, Germany
dpeuter@uni-koblenz.de, sofronie@uni-koblenz.de
Abstract. We study possibilities for automated invariant generation in
parametric systems. We use (a reﬁnement of) an algorithm for sym-
bol elimination in theory extensions to devise a method for iteratively
strengthening certain classes of safety properties to obtain invariants of
the system. We identify conditions under which the method is correct
and complete, and situations in which the method is guaranteed to ter-
minate. We illustrate the ideas on various examples.
1
Introduction
In the veriﬁcation of parametric systems it is important to show that a cer-
tain property holds for all states reachable from the initial state. One way to
solve such problems is to identify an inductive invariant entailing the property
to be proved. Finding suitable inductive invariants is non-trivial – the problem
is undecidable in general; solutions have been proposed for speciﬁc cases: In [26],
Kapur proposes methods for invariant generation in theories such as Presburger
arithmetic, real closed ﬁelds, and for polynomial equations and inequations with
solutions in an algebraic closed ﬁeld. The main idea is to use templates for the
invariant (polynomials with undetermined coeﬃcients), and solve constraints
for all paths and initial values to determine the coeﬃcients. A similar idea was
used by Beyer et al. [3] for constraints in linear real or rational arithmetic; it
is shown that if an invariant is expressible with a given template, then it will
be computed. Symbol elimination has been used for interpolation and invariant
generation in many papers. The methods proposed in [26], where quantiﬁer elim-
ination or Gr¨obner bases computation are used for symbol elimination, are one
class of examples. Quantiﬁer elimination is also used by Dillig et al. in [9]. How-
ever, in some cases the investigated theories are complex (can be extensions or
combinations of theories) and do not allow quantiﬁer elimination. Methods for
“symbol elimination” for such complex theories have been proposed, in many
cases in relationship with interpolant computation. In [40] Yorsh et al. stud-
ied interpolation in combinations of theories; in [7], Brutomesso et al. extended
these results to non-convex theories. Interpolation in data structures by reduc-
tion was analyzed by Kapur, Majumdar and Zarba in [27]. Independently, in
[34,35] Sofronie-Stokkermans analyzed possibilities of computing interpolants
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 385–405, 2019.
https://doi.org/10.1007/978-3-030-29436-6_23

386
D. Peuter and V. Sofronie-Stokkermans
hierarchically, and in [38,39] proposed a method of hierarchical symbol elimi-
nation which was used for interpolant computation; already [37] mentions the
possibility to infer constraints on parameters by hierarchical reasoning followed
by quantiﬁer elimination.
Symbol elimination can also be achieved using reﬁnements of superposition.
In [2], Bachmair et al. mention the applicability of a form of hierarchical super-
position to second-order quantiﬁer elimination (i.e. to symbol elimination). This
idea and possible links to interpolation are also mentioned in Ganzinger et al.
[13,14]. In [30], Kov´acs and Voronkov study inference systems and local deriva-
tions – in the context of interpolant generation – and symbol elimination in
proofs in such systems. The ideas are concretized using the superposition calculus
and its extension LASCA (ground linear rational arithmetic and uninterpreted
functions). Applications to invariant generation (brieﬂy mentioned in [30]) are
explored in detail in, among others, [18,29] – there Vampire is used to generate
a large set of invariants using symbol elimination; only invariants not implied
by the theory axioms or by other invariants are kept (some of these tasks are
undecidable). In [16], Gleiss et al. analyze functional and temporal properties
of loops. For this, extended expressions (introduced in [29]) are used; symbol
elimination `a la [30] is used to synthesize invariants using quantiﬁcation over
iterations.
Various papers address the problem of strengthening a given formula to
obtain an inductive invariant. In [5], Bradley proposes a goal-oriented invari-
ant generation method for boolean/numeric transition systems, relying on ﬁnd-
ing counterexamples. Such methods were implemented in IC3 [4]. For programs
using only integers, Dillig et al. [9] use quantiﬁer elimination to obtain increas-
ingly more precise approximations of inductive invariants (termination is not
guaranteed). In [12], Falke and Kapur analyze various ways of strengthening the
formulae; depending upon how strengthening is attempted, their procedure may
also determine whether the original formula is not an invariant. Situations in
which termination is guaranteed are identiﬁed. In [28], Karbyshev et al. pro-
pose a method to generate universal invariants in theories with the ﬁnite model
property using diagram-based abstraction for invariant strengthening; Padon et
al. [31] identify suﬃcient conditions for the decidability of inferring inductive
invariants in a given language L and also present undecidability results. Invari-
ant synthesis for array-based systems is studied by Ghilardi et al. in [15]; under
local ﬁniteness assumptions on the theory of elements and existence of well-
quasi-orderings on conﬁgurations termination is guaranteed. In [1], Alberti et al.
use lazy abstraction with interpolation-based reﬁnement and discuss the appli-
cability to invariant synthesis. A system for verifying safety properties that are
“cubes” and invariant generation in array-based systems is described in [8]. In
[17], Gurﬁnkel et al. propose an algorithm extending IC3 to support quanti-
ﬁers for inferring universal invariants in theories of arrays, combining quantiﬁed
generalizations (to construct invariants) with quantiﬁer instantiation (to detect
convergence).

On Invariant Synthesis for Parametric Systems
387
Our Contribution. In this paper we continue our work on automated veriﬁ-
cation and synthesis in parametric systems [22,36,37] by investigating possibili-
ties for automated goal-oriented generation of inductive invariants. Our method
starts with a universally quantiﬁed formula Ψ and successively strengthens it,
using a certain form of abductive reasoning based on symbol elimination. In
case of termination we prove that we obtain a universal inductive invariant that
entails Ψ, or the answer “no such invariant exists”. We identify situations in
which the method terminates. Our main results are:
– We reﬁne the symbol elimination method in theory extensions described in
[38,39] (Sect. 2.3). This helps us obtain shorter formulae during invariant
synthesis.
– We propose a method for goal-oriented synthesis of universally quantiﬁed
invariants which uses symbol elimination in theory extensions (Sect. 3).
– We identify conditions under which our invariant generation method is
partially correct (Sect. 3) and situations in which the method terminates
(Sect. 4.2).
– We further reﬁne the method (Sect. 4) and provide examples in which the con-
dition we impose on the class of transition systems can be relaxed (Sect. 4.1).
Illustration. Consider for instance the program in Fig. 1, using the subprograms
copy(a, b), which copies the array b into array a, and add1(a), which adds 1 to
every element of array a.
The task is to prove that if b is an array with its
d1 = 1; d2 = 1;
copy(a, b); i:= 0;
while (nondet()) {
a = add1(a);
d1 = a[i]; d2 = a[i+1];
i:= i + 1}
Fig. 1. Program using subprograms
and global function updates
elements sorted in increasing order then the
formula Ψ := d2 ≥d1 is an invariant of the
program. Ψ holds in the initial state; it is an
inductive invariant of the while loop iﬀthe
formula
d1 ≤d2 ∧∀j(a′[j] = a[j] + 1) ∧d′
1 = a′[i]∧
d′
2 = a′[i + 1] ∧i′ = i + 1 ∧d′
1 > d′
2
is unsatisﬁable. As this formula is satisﬁable,
Ψ is not an inductive invariant.
We will show how to obtain the condition ∀i(a[i] ≤a[i + 1]) which can be
used to strengthen Ψ := d2 ≥d1 to the inductive invariant (d2 ≥d1) ∧∀i(a[i] ≤
a[i + 1]).
While we rely on methods similar to the ones used in [5,9,12,15,17,28,31],
there are several diﬀerences between our work and previous work. The methods
proposed in [5,9,12,26] cannot be used to tackle examples like the one in Fig. 1:
It is diﬃcult to use templates in connection with additional function symbols; in
addition, the methods of [5,9,12] can only handle numeric domains. The theories
we analyze are typically extensions or combinations of theories and not required
to have the ﬁnite model property – which is required e.g. in [28,31]. The method
proposed in [17] does not come with soundness, completeness and termination
guarantees. We here use possibilities of complete instantiation in local theory
extensions and exploit (and reﬁne) the methods for symbol elimination in the-
ory extensions proposed in [38,39]. The algorithm proposed in [15] for theories

388
D. Peuter and V. Sofronie-Stokkermans
of arrays uses a non-deterministic function ChooseCover that returns a cover of a
formula (as an approximation of the reachable states). If the theory of elements
is locally ﬁnite it is proved that a universal formula Ψ can be strengthened to
a universal inductive invariant I iﬀthere exists a suitable ChooseCover func-
tion for which the algorithm returns an inductive invariant strengthening Ψ. In
contrast, our algorithm is deterministic; we prove completeness under locality
assumptions (holding if updates and properties are in the array property frag-
ment); our termination results are established for classes of formulae for which
only ﬁnitely many atomic formulae formed with a ﬁxed number of variables can
be generated using quantiﬁer elimination. In addition our method allows us to
choose the language for the candidate invariants (we can search for invariants
not containing certain constants or function symbols).
[18,29,30] use an approach diﬀerent from ours: A large set of invariants are
generated by symbol elimination using versions of superposition combined with
symbolic solving of recurrences. Completeness/termination are not guaranteed,
although the method works well in practice. In this paper we do not use quan-
tiﬁcation over the iterations.
Structure of the Paper. In Sect. 2 we present the veriﬁcation problems we consider
and the related reasoning problems; present some results on local theory exten-
sions; present a method for symbol elimination in theory extensions introduced
in [38,39] and propose an improvement of the method. In Sect. 3 we present an
approach to invariant synthesis, and identify conditions under which it is par-
tially correct. Section 4 presents reﬁnements and a termination result. Section 5
contains conclusions and plans for future work. Full proofs and additional exam-
ples are included in the extended version of this paper [32].
2
Preliminaries
We consider signatures Π = (Σ, Pred), where Σ is a family of function symbols
and Pred a family of predicate symbols. We assume known standard deﬁnitions
from ﬁrst-order logic (e.g. Π-structures, satisﬁability, unsatisﬁability, logical the-
ories). We denote “falsum” with ⊥. If F and G are formulae we write F |= G
(resp. F |=T
G – also written as T ∪F |= G) to express the fact that every
model of F (resp. every model of F which is also a model of T ) is a model of
G. F |= ⊥means that F is unsatisﬁable; F |=T ⊥means that there is no model
of T in which F is true.
2.1
Veriﬁcation Problems for Parametric Systems
One of the application domains we consider is the veriﬁcation of parametric
systems. For modeling such systems we use transition constraint systems T =
(Σ, Init, Update) which specify: the function symbols Σ (including a set V of
functions with arity 0 – the “variables” of the systems) whose values change over
time; a formula Init specifying the properties of initial states; a formula Update
with function symbols in Σ ∪Σ′ (where Σ′ consists of copies of symbols in Σ,

On Invariant Synthesis for Parametric Systems
389
such that if f ∈Σ then f ′ ∈Σ′ is the updated function after the transition).
Such descriptions can be obtained from system speciﬁcations (for an example cf.
[11]). With every speciﬁcation of a system S, a background theory TS – describing
the data types used in the speciﬁcation and their properties – is associated.
We can check in two steps whether a formula Ψ is an inductive invariant of a
transition constraint system T=(Σ, Init, Update) by checking whether:
(1) Init |=T S Ψ; and
(2) Ψ, Update |=T S Ψ ′, where Ψ ′ results from Ψ by replacing each f ∈Σ by f ′.
Checking whether a formula Ψ is an invariant can thus be reduced to checking
whether ¬Ψ ′ is satisﬁable or not w.r.t. a theory T . Even if Ψ is a universally
quantiﬁed formula (and thus ¬Ψ ′ is a ground formula) the theory T can be
quite complex: it contains the axiomatization TS of the datatypes used in the
speciﬁcation of the system, the formalization of the update rules, as well as the
formula Ψ itself. In [22,36,37] we show that the theory T can often be expressed
using a chain of extensions, typically including:
T0
⊆
T1 = T0 ∪Ψ
⊆
T = T0 ∪Ψ ∪Update
with the property that checking satisﬁability of ground formulae w.r.t. T can
be reduced to checking satisﬁability w.r.t. T1 and ultimately to checking satisﬁ-
ability w.r.t. T0. This is the case for instance when the theory extensions in the
chain above are local (for deﬁnitions and further properties cf. Sect. 2.2).
Failure to prove (2) means that Ψ is not an invariant or Ψ is not inductive
w.r.t. T. If Ψ is not an inductive invariant, we can consider two orthogonal
problems:
(a) Determine constraints on parameters which guarantee that Ψ is an invariant.
(b) Determine a formula I such that TS |= I →Ψ and I is an inductive
invariant.
Problem (a) was studied in [36,37]. In [38,39] we proposed a method for hier-
archical symbol elimination in theory extensions which allowed us to show that
for local theory extensions the formulae obtained using this symbol elimination
method are weakest constraints on parameters which guarantee that Ψ is invari-
ant. We present and improve this symbol elimination method in Sect. 2.3.
In this paper we address problem (b): in Sect. 3 we use symbol elimination for
giving a complete method for goal-oriented invariant generation, for invariants
containing symbols in a speciﬁed signature; we also identify some situations when
termination is guaranteed. The safety property and invariants we consider are
conjunctions of ground formulae and sets of (implicitly universally quantiﬁed)
ﬂat clauses of the form ∀x(Ci(x) ∨Cv(x, f(x))), where f are functional param-
eters, Ci is a clause containing constants and universally quantiﬁed variables,
and Cv a ﬂat clause containing parameters, constants and universally quantiﬁed
variables.1
1 We use the following abbreviations: x for x1, . . . , xn; f(x) for f1(x), . . . , fn(x).

390
D. Peuter and V. Sofronie-Stokkermans
2.2
Local Theory Extensions
Let Π0=(Σ0, Pred) be a signature, and T0 be a “base” theory with signature
Π0. We consider extensions T := T0 ∪K of T0 with new function symbols Σ
(extension functions) whose properties are axiomatized using a set K of clauses
in the extended signature Π = (Σ0 ∪Σ, Pred), which contain function symbols
in Σ. If G is a ﬁnite set of ground ΠC-clauses2 and K a set of Π-clauses, we will
denote by st(K , G) (resp. est(K , G)) the set of all ground terms (resp. extension
ground terms, i.e. terms starting with a function in Σ) which occur in G or K .3
If T is a set of ground terms in the signature ΠC, we denote by K [T] the set
of all instances of K in which the terms starting with a function symbol in Σ
are in T. Let Ψ be a map associating with every ﬁnite set T of ground terms a
ﬁnite set Ψ(T) of ground terms. For any set G of ground ΠC-clauses we write
K [ΨK (G)] for K [Ψ(est(K , G))]. We deﬁne:
(LocΨ
f ) For every ﬁnite set G of ground clauses in ΠC it holds that
T0 ∪K ∪G |= ⊥if and only if T0 ∪K [ΨK (G)] ∪G is unsatisﬁable.
Extensions satisfying condition (LocΨ
f ) are called Ψ-local [22,24]. If Ψ is the
identity, i.e. K [ΨK (G)] = K [G], we have a local theory extension [33].
Remark: In [22,24] we introduced and studied a notion of extended locality, in
which the axioms in K are of the form ∀x(φ(x) ∨C), where φ is an arbitrary
Σ0-formula and C a clause containing extension symbols and the set G contains
ground formulae of the form Ψ ∨Ge, where Ψ is a Σ0-sentence and Ge a ground
clause containing extension symbols. While most of the results in this paper can
be lifted by replacing “locality” with “extended locality”, in this paper we only
refer to locality for the sake of simplicity.
For (Ψ)-local theory extensions hierarchical reasoning is possible. Below, we dis-
cuss the case of local theory extensions; similar results hold also for Ψ-local
extensions. If T0 ∪K is a local extension of T0 and G is a set of ground ΠC-
clauses, then T0 ∪K ∪G is unsatisﬁable iﬀT0 ∪K [G] ∪G is unsatisﬁable.
We can reduce this last satisﬁability test to a satisﬁability test w.r.t. T0. The
idea is to purify K [G] ∪G by (i) introducing (bottom-up) new constants ct for
subterms t = f(g1, . . . , gn) with f ∈Σ, gi ground Σ0 ∪Σc-terms, (ii) replacing
the terms t with the constants ct, and (iii) adding the deﬁnitions ct ≈t to a set
D. We denote by K0 ∪G0 ∪D the set of formulae obtained this way. Then G
is satisﬁable w.r.t. T0 ∪K iﬀK0 ∪G0 ∪Con0 is satisﬁable w.r.t. T0, where
Con0 = {(n
i=1 ci ≈di) →c ≈d | c ≈f(c1, . . . , cn), d ≈f(d1, . . . , dn) ∈D}.
Theorem 1 ([33]).
If T0 ⊆T0 ∪K is a local extension and G is a ﬁnite
set of ground clauses, then we can reduce the problem of checking whether G is
satisﬁable w.r.t. T0 ∪K to checking the satisﬁability w.r.t. T0 of the formula
K0 ∪G0 ∪Con0 constructed as explained above. If K0 ∪G0 ∪Con0 belongs to a
2 ΠC is the extension of Π with constants in a countable set C of fresh constants.
3 We here regard every ﬁnite set G of ground clauses as the ground formula 
K∈G K.

On Invariant Synthesis for Parametric Systems
391
decidable fragment of T0, we can use the decision procedure for this fragment to
decide the (un)satisﬁability of T0 ∪K ∪G.
As the size of K0 ∪G0 ∪Con0 is polynomial in the size of G (for a given K ),
locality allows us to express the complexity of the ground satisﬁability prob-
lem w.r.t. T1 as a function of the complexity of the satisﬁability of formulae
w.r.t. T0.
(Ψ-)Local extensions can be recognized by showing that certain partial mod-
els embed into total ones [24]. Especially well-behaved are the theory extensions
with property (Compf), stating that partial models can be made total with-
out changing the universe of the model.4 The link between embeddability and
locality allowed us to identify many classes of local theory extensions:
Example 1 (Extensions with free/monotone functions [22,33]). The fol-
lowing types of extensions of a theory T0 are local:
(1) Any extension of T0 with uninterpreted function symbols ((Compf) holds).
(2) Any extension of a theory T0 for which ≤is a partial order with functions
monotone w.r.t. ≤(condition (Compf) holds if all models of T0 are complete
lattices w.r.t. ≤).
Example 2 (Extensions with deﬁnitions [22,25]). Consider an extension
of a theory T0 with a new function symbol f deﬁned by axioms of the form:
Deff := {∀x(φi(x) →Fi(f(x), x)) | i = 1, . . . , m}
(deﬁnition by “case distinction”) where φi and Fi, i = 1, . . . , m, are formulae
over the signature of T0 such that the following hold:
(a) φi(x) ∧φj(x) |=T 0 ⊥for i̸=j and
(b) T0 |= ∀x(φi(x) →∃y(Fi(y, x))) for all i ∈{1, . . . , m}.
Then the extension is local (and satisﬁes (Compf)). Examples:
(1) Any extension with a function f deﬁned by axioms of the form:
Df := {∀x(φi(x) →f(x) ≈ti) | i = 1, . . . , n}
where φi are formulae over the signature of T0 such that (a) holds.
(2) Any extension of T0 ∈{LI(Q), LI(R)} with functions satisfying axioms:
Boundf := {∀x(φi(x) →si ≤f(x) ≤ti) | i = 1, . . . , n}
where φi are formulae over the signature of T0, si, ti are T0-terms, condition
(a) holds and |=T 0 ∀x(φi(x) →si ≤ti) [22].
4 We use the index f in (Compf) in order to emphasize that the property refers to
completability of partial functions with a ﬁnite domain of deﬁnition.

392
D. Peuter and V. Sofronie-Stokkermans
Example 3 (The array property fragment [6,22]). In [6] a decidable frag-
ment of the theory of arrays is studied, namely the array property fragment.
Arrays are regarded as functions with arguments of index sort and values of
element sort. The index theory Ti is Presburger arithmetic; the element theory
is parametric. The array property fragment consists of all existentially-closed
Boolean combinations of quantiﬁer-free formulae and array property formulae.
Array property formulae are formulae of the form (∀i)(ϕI(i) →ϕV (i)), where
– ϕI is a positive Boolean combination of atoms of the form t ≤u or t = u
where t, u are either variables or ground terms of index sort;
– ϕV has the property that any universally quantiﬁed variable of index sort i
only occurs in a direct array read a(x) in ϕV and array reads may not be
nested.
In [6] it is shown that formulae in the array property fragment have complete
instantiation. In [22] we showed that this fragment satisﬁes a Ψ-locality condi-
tion.
2.3
Quantiﬁer Elimination and Symbol Elimination
We now present possibilities of symbol elimination in complex theories.
A theory T over signature Π allows quantiﬁer elimination if for every for-
mula φ over Π there exists a quantiﬁer-free formula φ∗over Π which is equiv-
alent to φ modulo T . Examples of theories which allow quantiﬁer elimination
are rational and real linear arithmetic (LI(Q), LI(R)), the theory of real closed
ﬁelds, and the theory of absolutely-free data structures.
Note ﬁrst that if the theories T1 and T2 over disjoint signatures Π1 resp. Π2
allow elimination of existential quantiﬁers, then the two-sorted combination T
of the theories T1 and T2, with signature Π = ({s1, s2}, Σ1∪Σ2, Pred1∪Pred2) –
where every n-ary operation f ∈Σi has sort sn
i →si, and every m-ary predicate
symbol p ∈Predi has arity sm
i
– allows elimination of existential quantiﬁers.
Symbol Elimination in Theory Extensions. Let Π0 = (Σ0, Pred). Let T0 be
a Π0-theory and ΣP be a set of parameters (function and constant symbols). Let
Σ be a signature such that Σ ∩(Σ0 ∪ΣP ) = ∅. We consider the theory extension
T0 ⊆T0 ∪K , where K is a set of clauses in the signature Π = Π0 ∪ΣP ∪Σ
in which all variables occur also below functions in Σ1 = ΣP ∪Σ. Consider the
symbol elimination method in Algorithm 1 [38,39].
Theorem 2 ([38,39]). Assume that T0 allows quantiﬁer elimination. For every
ﬁnite set of ground ΠC-clauses G, and every ﬁnite set T of ground terms over the
signature ΠC with est(G) ⊆T, Steps 1–5 yield a universally quantiﬁed Π0 ∪ΣP -
formula ∀xΓT (x) such that T0 ∪∀yΓT (y) ∪K ∪G is unsatisﬁable.
Theorem 3 ([38,39]). Assume that the theory extension T0 ⊆T0 ∪K satisﬁes
condition (Compf) and K is ﬂat and linear. Let G be a set of ground ΠC-clauses,
and ∀yΓG(y) be the formula obtained with Algorithm 1 for T = est(K , G). Then
∀yΓG(y) is entailed by every universal formula Γ with T0 ∪Γ ∪K ∪G |= ⊥.

On Invariant Synthesis for Parametric Systems
393
Algorithm 1. Symbol elimination in theory extensions [38,39]
Step 1 Let K0 ∪G0 ∪Con0 be the set of ΠC
0 -clauses obtained from K [T] ∪G after
the puriﬁcation step described in Theorem 1 (with set of extension symbols Σ1).
Step 2 Let G1 = K0 ∪G0 ∪Con0. Among the constants in G1, we identify
(i) the constants cf, f ∈ΣP , where cf is a constant parameter or cf is introduced
by a deﬁnition cf ≈f(c1, . . . , ck) in the hierarchical reasoning method,
(ii) all constants cp occurring as arguments of functions in ΣP in such deﬁnitions.
Let c be the remaining constants. We replace the constants in c with existen-
tially quantiﬁed variables x, i.e. instead of G1(cp, cf, c) we consider the formula
∃xG1(cp, cf, x).
Step 3 Using a method for quantiﬁer elimination in T0 we can construct a formula
Γ1(cp, cf) equivalent to ∃xG1(cp, cf, x) w.r.t. T0.
Step 4 Let Γ2(cp) be the formula obtained by replacing back in Γ1(cp, cf) the con-
stants cf introduced by deﬁnitions cf := f(c1, . . . , ck) with the terms f(c1, . . . , ck).
We replace cp with existentially quantiﬁed variables y.
Step 5 Let ∀yΓT (y) be ∀y¬Γ2(y).
A similar result holds if T is the set of instances obtained from the instantiation
of a chain of theory extensions T0 ⊆T0 ∪K1 ⊆· · · ⊆T0 ∪K1 ∪· · · ∪Kn, all
satisfying condition (Compf), where K1, . . . , Kn are all ﬂat and linear and every
variable is guarded by an extension symbol [39].
Remark. Algorithm 1 can be tuned to eliminate constants c in a set Ce which
might occur as arguments to parameters: All these constants, together with all
constants cf introduced by deﬁnitions cf = f(c1, . . . , cn) with some ci ∈Ce, are
replaced with variables at the end of Step 2 and are eliminated in Step 3.
Quantiﬁer elimination usually has high complexity and leads to large formulae.
Often, Algorithm 1 can be improved such that QE is applied to smaller formulae:
Theorem 4. Assume that K = KP ∪K1 such that KP contains only symbols
in Σ0 ∪ΣP and K1 is a set of Π-clauses such that
T0 ⊆T0 ∪KP ⊆T0 ∪KP ∪K1
is a chain of theory extensions both satisfying condition (Compf) and having
the property that all variables occur below an extension function, and such that
K is ﬂat and linear. Let G be a set of ground ΠC-clauses. Then the formula
KP ∧∀yΓ1(y), where ∀yΓ1(y) is obtained by applying Algorithm 1 to T0∪K1∪G,
has the property that for every universal formula Γ containing only parameters
with T0 ∪(KP ∪Γ) ∪G |= ⊥, we have KP ∧Γ |= KP ∧∀yΓ1(y).
Proof (Idea): Since KP contains only functions in Σ0 ∪ΣP , its set of instances
does not contain functions which we want to eliminate, so can be brought
outside of the scope of the existential quantiﬁers after Step 2. Thus, quanti-
ﬁer elimination can be applied only to the sets of instances corresponding to

394
D. Peuter and V. Sofronie-Stokkermans
K1 ∪G and yields a formula D. Step 5 yields a universally quantiﬁed disjunc-
tion between a formula corresponding to the negation of the instances of KP
and Γ1(y) = ¬D(y), the negation of the formula obtained from D by replacing
constants with variables. Thus, we only need ∀yΓ1(y) to strengthen KP .
⊓⊔
This improvement will be important for the method for invariant generation we
discuss in what follows. Further improvements are discussed in Sect. 4.
3
Goal-Oriented Invariant Synthesis
Let S be a system, TS be the theory and T=(ΣS, Init, Update) the transition
constraint system associated with S. We assume that ΣS = Σ0 ∪ΣP ∪Σ, where
Σ0 is the signature of a “base” theory T0, ΣP is a set of function symbols
assumed to be parametric, and Σ is a set of functions (non-parametric) disjoint
from Σ0 ∪ΣP . We assume that Init is a universal formula describing the initial
states and Update is a universal formula describing (possibly global) updates of
functions in a set F ⊆Σ, and also variable updates.5
We assume given a universal formula Ψ (a conjunction of clauses ∀x(Ci(x) ∨
Cv(x, f(x)), where Ci is a T0-clause and Cv a ﬂat clause over Σ0 ∪ΣP ). Both
Init and Ψ describe “global” properties of the function symbols in ΣP at a given
moment in time (for instance equality of two functions – or equality of arrays,
monotonicity of a function – or sortedness of an array). Our goal is to obtain an
inductive invariant I with I |=T S Ψ.
We make the following assumptions: Let LocSafe be a class of universal ΣS-
formulae.
(A1) There exists a chain of local theory extensions T0 ⊆· · · ⊆TS ∪Init such
that in each extension all variables occur below an extension function.
(A2) For every Ψ ∈LocSafe there exists a chain of local theory extensions
T0 ⊆· · · ⊆TS ∪Ψ such that in each extension all variables occur below an
extension function.
(A3) Update = {Updatef | f ∈F} consists of update axioms for functions in a
set F, where, for every f ∈F, Updatef has the form Deff := {∀x(φf
i (x) →
Cf
i (x, f ′(x))) | i ∈I}, such that (i) φi(x) ∧φj(x) |=T S ⊥for i̸=j, (ii) TS |=
n
i=1 φi, and (iii) Cf
i are conjunctions of literals and TS |= ∀x(φi(x) →
∃y(Cf
i (x, y))) for all i ∈I.6
In what follows, if φ is a formula containing function symbols in Σ we denote by
φ′ the formula obtained from φ by replacing every function symbol f ∈Σ with
f ′ ∈Σ′.
5 Variables are 0-ary functions. Ground formulae are, in particular, also universal
formulae.
6 In particular we can consider deﬁnition updates of the form Df′ or updates of the
form Boundf′ as discussed in Example 2.

On Invariant Synthesis for Parametric Systems
395
Algorithm 2. Successively strengthening a formula to an inductive invariant
Input:
T = (ΣS, Init, Update) transition system; ΣP ⊆ΣS; Ψ ∈LocSafe, formula over ΣP
Output: Inductive invariant I of T that entails Ψ and contains only function symbols in ΣP
(if such an invariant exists).
1: I := Ψ
2: while I is not an inductive invariant for T do:
if Init ̸|= I then return “no universal inductive invariant over ΣP entails Ψ”
if I is not preserved under Update then Let Γ be obtained by eliminating
all primed variables and symbols not in ΣP from I ∧Update ∧¬I′;
I := I ∧Γ
3: return I is an inductive invariant
Theorem 5 ([22,36]). The following hold under assumptions (A1)–(A3): (1)
If ground satisﬁability w.r.t. T0 is decidable, then the problem of checking whether
a formula Ψ ∈LocSafe is an inductive invariant of S is decidable. (2) If T0 allows
quantiﬁer elimination and the initial states or the updates contain parameters,
Algorithm 1 yields constraints on these parameters that guarantee that Ψ is an
inductive invariant.
We now study the problem of inferring – in a goal-oriented way – univer-
sally quantiﬁed inductive invariants. The method we propose is described in
Algorithm 2.
In addition to assumptions (A1), (A2), (A3) we now consider the following
assumptions (where T0 is the base theory in assumptions (A1)–(A3)):
(A4) Ground satisﬁability in T0 is decidable; T0 allows quantiﬁer elimination.
(A5) All candidate invariants I computed in the while loop in Algorithm 2 are
in LocSafe, and all local extensions in LocSafe satisfy condition (Compf).
We prove that under assumptions (A1)–(A5) the algorithm is partially correct
(Theorem 8). Then we identify conditions under which (A5) holds, so does not
have to be stated explicitly (Sect. 4.1), and conditions under which the algorithm
terminates (Sect. 4.2).
Lemma 6. If Algorithm 2 terminates and returns a formula I, then I is an
invariant of T containing only function symbols in ΣP that entails Ψ.
Proof: Follows from the loop condition.
⊓⊔
Lemma 7. Under assumptions (A1)–(A5), if there exists a universal inductive
invariant J containing only function symbols in ΣP that entails Ψ, then J entails
every candidate invariant I generated in the while loop of Algorithm 2.
Proof: Proof by induction on the number of iterations in which the candidate
invariant I is obtained. If i = 1, then I1 = Ψ, hence J |= Ψ = I1.
Assume that the property holds for the candidate invariant generated in n
steps. Let In+1 be generated in step n + 1. In this case there exist candidate
invariants I1, . . . , In containing only function symbols in ΣP s.t.: (i) I1 = Ψ;

396
D. Peuter and V. Sofronie-Stokkermans
(ii) for all 1 ≤i ≤n, Init |= Ii; (iii) for all 1 ≤i ≤n, Ii is not an inductive
invariant, i.e. Ii ∧Update ∧¬I′
i is satisﬁable and Γi is obtained by eliminating
the primed function symbols and all function symbols not in ΣP ; (iv) for all
1 ≤i ≤n, Ii+1 = Ii ∧Γi.
We prove that J |=T S In+1, i.e. that J |=T S In ∧Γn. By the induction
hypothesis, J |=T S In, hence J ≡T S J ∧In. We know that J is an inductive
invariant, i.e. J∧Update∧¬J′ is unsatisﬁable. Therefore (J∧In)∧Update∧(¬J′∨
¬I′
n) is unsatisﬁable, hence, in particular, J ∧In ∧Update ∧¬I′
n is unsatisﬁable.
By Theorem 3, the way Γn is constructed, and the fact that J is a universal
formula containing only function symbols in ΣP , we know that J |=T S Γn.
Thus, J |=T S In ∧Γn, so J |=T S In+1. This completes the proof.
⊓⊔
Theorem 8 (Partial Correctness). Under assumptions (A1)–(A5), if Algo-
rithm 2 terminates, then its output is correct.
Proof (Sketch): If Algorithm 2 terminates with output I, then the condition of
the while loop must be false for I, so I is an invariant. Assume that Algorithm 2
terminates because Init ̸|=T S I returning “no universal inductive invariant over
ΣP entails Ψ”. Then there exists a model A of Init and TS which is not a model of
I. Assume that there exists a universal inductive invariant J over ΣP that entails
Ψ. By Lemma 7, J entails the candidate invariants generated at each iteration,
thus entails I. But every model of Init (in particular A) is a model of J, hence
also of I. Contradiction. Therefore, the assumption that there exists a universal
inductive invariant J that entails Ψ was false, i.e. the answer is correct.
⊓⊔
4
Reﬁnements
Assume that Update = 
f∈F Updatef, where F ⊆Σ (no f ′ with f ∈F is a
parameter) such that Updatef satisﬁes the conditions in assumption (A3).
Lemma 9. We consider the computations described in Algorithm 2, iteration n,
in Step 2, the case in which Init |= In, but In is not invariant under updates.
Let K be a set of constraints on parameters.
(1) If In = In−1 ∧Γn−1 is not invariant under updates, then Algorithm 2 com-
putes a formula Γn = 
f∈F Γ f
n , where Γ f
n is obtained by symbol elimination
applied to K ∧In ∧Updatef ∧G, where G is obtained by Skolemization from
¬Γ ′
n−1.
(2) If the only non-parametric functions are {f ′ | f ∈F}, then with the improve-
ment of Algorithm 1 in Theorem 4 we need to apply symbol elimination only
to Updatef ∧¬Γ ′
n−1 to compute Γ f
n .
Proof: (1) In ∧Update ∧¬I′
n ≡
f∈F (In ∧Updatef ∧¬I′
n), so it is satisﬁable iﬀ
for some f ∈F, the formula K ∧In ∧Updatef ∧¬I′
n is satisﬁable. We have:

On Invariant Synthesis for Parametric Systems
397
K ∧In ∧Updatef ∧¬I′
n = K ∧(In−1 ∧Γn−1) ∧Updatef ∧(¬I′
n−1 ∨¬Γ ′
n−1)
≡K ∧(In−1 ∧Γn−1) ∧Updatef ∧¬Γ ′
n−1,
since Γn−1 was introduced such that K ∧(In−1 ∧Γn−1) ∧Updatef ∧¬I′
n−1 is
unsatisﬁable. Then in Algorithm 2, Γn = 
f∈F Γ f
n , where Γ f
n are the (weakest)
formulae obtained with Algorithm 1, such that K ∧In ∧Γ f
n ∧Updatef ∧¬Γ ′
n−1
is unsatisﬁable.
(2) follows from Theorem 4.
⊓⊔
Lemma 10. If φi ∧φj |=T ⊥for all i ̸= j, 1 ≤i, j ≤n and |=T
n
i=1 φi then
n
i=1(φi →Ci) ≡n
i=1(φi ∧Ci).
We now analyze the formulae Γ f
n generated at iteration n. For simplicity we
assume that f is unary; the extension to higher arities is immediate.
Theorem 11. Let Ψ ∈LocSafe and Update = 
f∈F Updatef of the form dis-
cussed above. Assume that the clauses in Ψ and Updatef are ﬂat and linear
for all f ∈F. Let m be the maximal number of variables in a clause in Ψ.
Assume that the only non-parametric functions which need to be eliminated are
the primed symbols {f ′ | f ∈F} and that conditions (A1)–(A5) hold. Consider
a variant of Algorithm 2, which uses for symbol elimination Algorithm 1 with the
improvement in Theorem 4. Then for every step n, (i) the clauses in the candi-
date invariant In obtained at step n of Algorithm 2 are ﬂat, and (ii) the number
of universally quantiﬁed variables in every clause in In is ≤m.
Proof: Proof by induction on n. For n = 1, I1 = Ψ and (i) and (ii) clearly hold.
Assume that they hold for iteration n. We prove that they hold for iteration
n + 1. By Lemma 9, we need to apply Algorithm 1 to Updatef ∧G, where G is
obtained from ¬Γ ′
n after Skolemization. If Γ ′
n is a conjunction of clauses, then
G is a disjunction of conjunctions of literals; each disjunct can be processed
separately, and we take the conjunction of the obtained constraints. Thus, we
assume w.l.o.g. that G is a conjunction of literals. By the induction hypothesis
the number k of universally quantiﬁed variables in Γn is ≤m, so G contains
Skolem constants {d1, . . . , dk, c1, . . . , cr} with k + r ≤m, where d1, . . . , dk occur
below f ′. For symbol elimination we ﬁrst compute G1 = Updatef[G] ∧G (with
est(G) = {f ′(d1), . . . , f ′(dk)} where k ≤m) and purify it; in a second step we
instantiate the terms starting with function symbols g ∈ΣP ∪Σ. By Lemma 10:
Updatef[G]:=
k
j=1
nf

i=1
(φi(dj) →Ci(dj, f ′(dj)))

≡
k
j=1
nf

i=1
(φi(dj) ∧Ci(dj, f ′(dj)))
≡

i1,...,ik∈{1,...,nf }
 k
p=1
φip(dp) ∧
k
p=1
Cip(dp, f ′(dp))

.
We thus obtained a DNF with (nf)k ≤(nf)m disjuncts, where nf (number of
cases in the deﬁnition of f) and m (the maximal number of variables in K ∪In)
are constants depending on the description of the transition system. Both nf and
m are typically small, in most cases nf ≤3. Algorithm 1 is applied as follows:

398
D. Peuter and V. Sofronie-Stokkermans
In Step 1 we introduce a constant cf ′d for every term f ′(d) ∈est(G), replace
f ′(d) with cf ′d, and add the corresponding instances Con0 of the congru-
ence axioms. We may compute a disjunctive normal form DNF(Con0) for the
instances of congruence axioms or not (Con0 contains k2 ≤m2 conjunctions;
DNF(Con0) contains 2k2 ≤2m2 disjuncts, each of length k). In a second reduc-
tion we replace every term of the form g(c) ∈est(G1), g ∈ΣP , with a new
constant cgc.
Steps 2 and 3: To eliminate f ′ we replace the constants cf ′d with vari-
ables xf ′d and obtain a formula G0
1(xf ′d1, . . . , xf ′dn). In ∃xf ′d1, . . . xf ′dn
G0
1(xf ′d1, . . . , xf ′dn) the existential quantiﬁers can be brought inside the conjunc-
tions and quantiﬁer elimination can be used only on the part of the disjuncts
that contain the variables xf ′d (i.e. on relatively simple and short formulae).
After quantiﬁer elimination we obtain a formula Γ2.
Steps 4 and 5: We replace back in the formula obtained this way all
constants cgc, g ∈ΣP , g(c) ∈est(G1) with the terms g(c). The constants
d1, . . . , dk, c1, . . . , cr are replaced with new variables y1, . . . , yk, yk+1, . . . , yk+r
respectively. We negate ∃yΓ2(y) and obtain a conjunction Γ f
n+1(G) of univer-
sally quantiﬁed clauses.
All clauses in Γ f
n+1(G) are ﬂat. By construction, the number of universally quan-
tiﬁed variables in Γ f
n+1(G) is k + r ≤m.
⊓⊔
Theorem 12. Under the assumptions in Theorem 11, the number of clauses in
Γn is at most O(kn
1 ); each clause in Γn contains at most k2 · n + |Ψ| literals if
the constraints Ci are all equalities, and can contain O(|Ψ|kn
3 ) literals if Ci are
constraints in LI(Q), where k1, k2, k3 are constants of the system.
If there are non-parametric functions that are being updated the number of
variables in the clauses Γn might grow: Any constant c ∈F which is not a
parameter, but occurs below a parameter in Update or G, is then being converted
into a universally quantiﬁed variable by Algorithm 1 as the following example
shows.
Example 4. Consider the program in the introduction (Fig. 1). The task is to
prove that if the parameter b is an increasingly sorted array then Ψ := d2 ≥d1
is an invariant of the program. KP contains the sortedness axiom for b.
Assume ﬁrst that ΣP = {b, d1, d2, a}. Ψ clearly holds in the initial state. To show
that Ψ is an inductive invariant of the while loop, we would need to prove that
the following formula is unsatisﬁable:
d1 ≤d2 ∧∀j(a′[j] ≈a[j]+1)∧d′
1 ≈a′[i]∧d′
2 ≈a′[i+1]∧i′ ≈i+1∧d′
1 > d′
2.
We have the chain of local theory extensions
Z
⊆
Z ∪UIFa
⊆
Z ∪UIFa ∪Updatea = T ,
where Updatea = ∀j(a′[j] ≈a[j] + 1). Updated1 = d′
1 ≈a′[i], Updated2 = d′
2 ≈
a′[i + 1] and Updatei = i′ ≈i + 1 are ground formulae. Let G = d′
1 ≈a′[i] ∧d′
2 ≈

On Invariant Synthesis for Parametric Systems
399
a′[i + 1] ∧i′ ≈i + 1 ∧d′
1 > d′
2. Using the hierarchical reduction method for local
theory extensions we can see that the formula above is satisﬁable, so Ψ is not an
invariant. To strengthen Ψ we use Algorithm 1; by Theorem 4 we can ignore KP
and I1 = d1 ≤d2. In a ﬁrst step, we compute Updatea[G] and obtain the set of
instances a′[i] ≈a[i] + 1 ∧a′[i + 1] ≈a[i + 1] + 1. After puriﬁcation we obtain
(with Def = a′
1 ≈a′[i] ∧a′
2 ≈a′[i + 1]):
G0 ∧(Updatea)0: a′
1 ≈a[i] + 1 ∧a′
2 ≈a[i + 1] + 1∧
d′
1 ≈a′
1 ∧d′
2 ≈a′
2 ∧i′ ≈i + 1 ∧d′
1 > d′
2.
In a second step we can use a similar hierarchical reduction for the extension with
UIFa; we obtain (with Def = a′
1 ≈a′[i]∧a′
2 ≈a′[i+1]∧a1 ≈a[i]∧a2 ≈a[i+1]):
a′
1 ≈a1 + 1 ∧a′
2 ≈a2 + 1 ∧d′
1 ≈a′
1 ∧d′
2 ≈a′
2 ∧i′ ≈i + 1 ∧d′
1 > d′
2.
We use quantiﬁer elimination for eliminating a′
1, a′
2, d′
1, d′
2, i′ and obtain the
constraint a1 > a2. After replacing the constants with the terms they denote we
obtain ∃i(a[i] > a[i + 1]); its negation, Γ1 = ∀i(a[i] ≤a[i + 1]), can be used to
strengthen Ψ to the inductive invariant I2 = d1 ≤d2 ∧∀i(a[i] ≤a[i + 1]).
Assume now that ΣP = {b, d1, d2, a, i}. All primed variables are eliminated as
above, but in Step 2 of Algorithm 1 i is not existentially quantiﬁed. Ψ is strength-
ened to I2 := d1 ≤d2 ∧a[i] ≤a[i + 1] (no universally quantiﬁed variables, the
same as in Ψ). However, I2 is not an inductive invariant. It can be strengthened
to I3 := d1 ≤d2 ∧a[i] ≤a[i + 1] ∧a[i + 1] ≤a[i + 2] and so on. Ideas similar
to those used in the melting calculus [21] (used e.g. in [19,20]) could be used to
obtain d1 ≤d2∧∀i(a[i] ≤a[i+1]). (This example indicates that it could be a good
strategy to not include the variables controlling loops among the parameters.)
While testing our method, we noticed that in some cases in which Algo-
rithm 2 does not terminate, if we eliminate more symbols (thus restricting the
language of the formula that strengthens the property to be proved) we can
obtain termination. Details are given in the extended version of this paper [32],
Sect. 5.3.
Corollary 13. The symbol elimination method in Algorithm 1 can be adapted
to eliminate all constants not guarded by a function in ΣP . With this change we
can guarantee that in all clauses in In all variables occur below a function in ΣP .
Example 5. Let T0 = LI(Q). Let m, M, g, L ∈ΣP satisfying K = {m ≤M}.
Updatef := {∀x(x ≤c1 →m ≤f ′(x) ∧f ′(x) ≤M), ∀x(x > c1 →f ′(x) ≈a)}.
Assume that Ψ = ∀x, y(g(y) ≤x →f(x) ≤L(y)). By the results in Examples 1
and 2 we have the following chain of local theory extensions:
T0
⊆
T0 ∪UIF{g,L}
⊆
T0 ∪Ψ
⊆
T0 ∪Ψ ∪Updatef.
Ψ is invariant under the update of f iﬀK ∧Ψ ∧Updatef ∧G is unsatisﬁable,
where G = g(c) ≤d ∧f ′(d) > L(c) is obtained from ¬Ψ ′ after Skolemization.
Since the formula is satisﬁable, Ψ is not invariant. We can strengthen Ψ by
computing the DNF of Updatef[G] = {(d ≤c1 →m ≤f ′(d) ∧f ′(d) ≤M), (d >

400
D. Peuter and V. Sofronie-Stokkermans
c1 →f ′(d) ≈a)}, as explained in Lemma 10, replacing f ′(d) with an existentially
quantiﬁed variable xf ′d. As the constant d does not occur below a parameter, it
is replaced in Step 2 of Algorithm 1 with a variable xd. The terms g(c), L(c) are
replaced with constants cgc resp. cLc. We obtain:
∃xd∃xf ′d (xd ≤c1 ∧m ≤xf ′d ∧xf ′d ≤M ∧cgc ≤xd ∧xf ′d > cLc) ∨
(xd > c1 ∧xf ′d ≈a ∧cgc ≤xd ∧xf ′d > cLc)
After eliminating xf ′d we obtain:
∃xd(xd ≤c1 ∧cgc ≤xd ∧m ≤M ∧cLc < M) ∨(xd > c1 ∧cgc ≤xd ∧cLc < a).
The variable xd does not occur below any function symbol and it can be elimi-
nated; we obtain the equivalent formula (cgc ≤c1 ∧m ≤M ∧cLc < M) ∨(cLc <
a); after replacing back the constants cgc and cLc with the terms they denote,
replacing c with a new existentially quantiﬁed variable y (Step 4 of Algorithm 1)
and negating the formula obtained this way (Step 5 of Algorithm 1) we obtain
the constraint ∀y(g(y) ≤c1 →M ≤L(y)) ∧∀y(a ≤L(y)).
4.1
Avoiding Some of the Conditions (A1)–(A5)
Assumption (A4) (T0 allows quantiﬁer elimination) is not needed if in all update
axioms f ′ is deﬁned using equality; then f ′ can easily be eliminated.
Assumption (A5) is very strong. Even if we cannot guarantee that assump-
tion (A5) holds, it could theoretically be possible to identify situations in which
we can transform candidate invariants which do not deﬁne local extensions into
equivalent formulae which deﬁne local extensions – e.g. using the results in [19].
If all candidate invariants I generated in Algorithm 2 are ground, assumption
(A5) is not needed. We now describe a situation in which assumption (A5) is
fulﬁlled, so Lemma 7 and Theorem 8 hold under assumptions (A1)–(A4).
We consider transition systems T = (ΣS, Init, Update) and properties Ψ,
where TS = T0 ∪K and Init, Ψ, K and Updatef, f ∈F, are all in the array
property fragment. Then assumptions (A1) and (A2) hold. We identify condi-
tions under which we can guarantee that at every iteration of Algorithm 2, the
candidate invariant I is in the array property fragment, so assumption (A5)
holds and does not need to be mentioned explicitly.
Many types of systems have descriptions in this fragment; an example follows.
Example 6. Consider a controller of a water tank in which the inﬂow and
outﬂow in a time unit can be chosen freely between minimum and maximum
values that depend on the moment in time. Assume that 0 ≤Lalarm < Loverﬂow.
At the beginning minimal and maximal values for the inﬂow and outﬂow are
initialized as described by the formula Init = In1 ∧In2 ∧Out1 ∧Out2 ∧(t ≈
0) ∧(L ≈L0), where for i = 1, 2:
Ini = ∀t(0 ≤ini
m(t) ≤ini
M(t) ≤Loverﬂow−Lalarm−ϵi) and
Outi = ∀t(ini
M(t) ≤outi
m(t) ≤outi
M(t))

On Invariant Synthesis for Parametric Systems
401
The updates are described by Updatein ∧L′ ≈L + in′(t) ∧t′ ≈t + 1, where:
Updatein = ∀t(L ≤Lalarm ∧t ≤t0 →in1
m(t) ≤in′(t) ≤in1
M(t))
∀t(L ≤Lalarm ∧t > t0 →in2
m(t) ≤in′(t) ≤in2
M(t))
∀t(L > Lalarm ∧t ≤t0 →in1
m(t)−out1
M(t) ≤in′(t) ≤in1
M(t)−out1
m(t))
∀t(L > Lalarm ∧t > t0 →in2
m(t)−out2
M(t) ≤in′(t) ≤in2
M(t)−out2
m(t))
All these formulae are in the array property fragment.
The following results follow from the deﬁnition of the array property fragment.
Lemma 14. Under assumption (A3), Updatef is in the array property fragment
iﬀφ1, . . . φnf are conjunctions of constraints of the form x ≤g or x ≥g, where
x is a variable and g is a ground term of sort index, all Σ ∪ΣP terms are ﬂat
and all universally quantiﬁed variables occur below a function in Σ ∪ΣP .
Lemma 15. Let G be the negation of a formula in the array property fragment
(APF). Then the following are equivalent:
(1) The formula obtained by applying Algorithm 1 to Updatef ∧G is in the APF.
(2) No instances of the congruence axioms need to be used for est(G).
(3) Either est(G) contains only one element, or whenever f ′(d), f ′(d′) ∈est(G),
where d = d1, . . . , dn, d′ = d′
1, . . . , d′
n, we have T0 ∪K ∪G |= n
i=1 di ̸≈d′
i.
Proof (Idea):
The
formula
k
p=1 φip(yp) ∧
(d1,d2)∈D y1 ̸≈y2 ∧Gg
0(y, g(y))
obtained after applying Algorithm 1, can be an index guard only if it does not
contain the disequalities y1 ̸≈y2. This is the case when |est(G)| = 1 or else if for
all f(d1), f(d2) ∈est(G), T0 ∪G0 |= d1 ̸≈d2.
⊓⊔
Theorem 16 Let T = (ΣS, Init, Update) be a transition system with theory
TS = T0 ∪K . Assume that T0 is the disjoint combination of Presburger arith-
metic (sort index) and a theory of elements (e.g. linear arithmetic over Q).
Assume that all functions in Σ are unary. If K , Init, Update and Ψ are in the
array property fragment and all clauses in Ψ have only one universally quanti-
ﬁed variable, then the formulae Γn obtained by symbol elimination in Step 2 at
every iteration of Algorithm 2 are again in the array property fragment and are
conjunctions of clauses having only one quantiﬁed variable.
4.2
Termination
Algorithms of the form of Algorithm 2 do not terminate in general even for simple
programs, handling only integer or rational variables (cf. e.g. [9]). We identify
situations in which the invariant synthesis procedure terminates. (For proofs cf.
the extended version [32].)
Lemma 17 (A termination condition). Assume that conditions (A1)–(A5)
hold and the candidate invariants I generated at each iteration are conjunctions
of clauses which contain, up to renaming of the variables, terms in a given ﬁnite
family Ter of terms. Then the algorithm must terminate with an invariant I or
after detecting that Init ̸|= I.

402
D. Peuter and V. Sofronie-Stokkermans
A situation in which this condition holds is described below.7
Theorem 18. Let Σ = {f1, . . . , fn} = ΣP . Assume that conditions (A1)–(A5)
hold, T0 = LI(Q) and that:
– All clauses used for deﬁning TS and the property Ψ contain only literals of the
form: x ▷t, u ▷v, fi(x) ▷s, fi(x) ▷y, where x, y are (universally quantiﬁed)
variables, fi ∈Σ, s, t, u, v are constants, and ▷∈{≤, <, ≥, >, ≈}.
– All axioms in Update are of the form ∀x

φk
i (x) →Ci(x, f ′
k(x))
	
as in
Assumption (A2), where Ci(x, y) and φk
i (x) are conjunctions of literals of
the form above.
Then all the candidate invariants I generated during the execution of Algorithm 2
are equivalent to sets of clauses, all containing a ﬁnite set Ter of terms formed
with variables in a ﬁnite set Var. Since only ﬁnitely many clauses (up to renam-
ing of variables) can be formed this way, after a ﬁnite number of steps no new
formulae can be generated, thus the algorithm terminates.
5
Conclusion
We proposed a method for property-directed invariant generation and analyzed
its properties. Our results extend the results in [4] and [9], as we consider more
complex theories. There are similarities to the method in [31], but our approach
is diﬀerent: The theories we analyze do not typically have the ﬁnite model prop-
erty (required in [28,31] where, if a counterexample A to the inductiveness of
a candidate invariant I is found, a formula is added to I to avoid ﬁnding the
same counterexample again in the next iteration; to construct this formula the
ﬁnite model property assumption is used). In our work we use the symbol elimi-
nation method in Algorithm 1 to strengthen I; this should help to accelerate the
procedure compared to the diagram-based approach. The decidability results in
[31] are presented in a general framework and rely on the well-foundedness of
certain relations. In this paper we consider extensions of arithmetic (or other
theories allowing quantiﬁer elimination) with additional function symbols; the
theories we consider are not guaranteed to have the ﬁnite model property. For
the situations in which we guarantee termination the abstract decidability or
termination arguments in [31] might be diﬃcult to check or might not hold (the
arguments used for the case of pointers are not applicable). In contrast to the
algorithm proposed in [15], our algorithm is deterministic. To prove termination
we show that the length of the quantiﬁer preﬁx in the candidate invariants gen-
erated in every iteration does not grow; termination is then guaranteed if only
ﬁnitely many atomic formulae formed with a ﬁxed number of variables can be
generated using quantiﬁer elimination when applying the algorithm.
We analyzed the applicability of our methods on several examples. In our
tests, we used H-PILoT [23] for the hierarchical reduction (Step 1 in Algorithm 1)
7 To simplify the notation, we assume that the functions in Σ have arity ≤1. Similar
arguments can be used for n-ary functions.

On Invariant Synthesis for Parametric Systems
403
and Redlog [10] for quantiﬁer elimination (Step 3 in Algorithm 1); implementing
Step 2 is ongoing work.
Future work. We here restricted to universally quantiﬁed invariants and the-
ories related to the array property fragment, but an extension to a framework
using the notion of “extended locality” (cf. [22,24]) seems unproblematic. We
plan to identify additional situations in which our invariant generation method
is correct, terminates resp. has low complexity – either by considering other
theories or more general ﬁrst-order properties.
Acknowledgments. We thank the reviewers for their helpful comments.
References
1. Alberti, F., Bruttomesso, R., Ghilardi, S., Ranise, S., Sharygina, N.: An extension
of lazy abstraction with interpolation for programs with arrays. Formal Methods
Syst. Des. 45(1), 63–109 (2014)
2. Bachmair, L., Ganzinger, H., Waldmann, U.: Refutational theorem proving for
hierarchic ﬁrst-order theories. Appl. Algebra Eng. Commun. Comput. 5, 193–212
(1994)
3. Beyer, D., Henzinger, T.A., Majumdar, R., Rybalchenko, A.: Invariant synthesis
for combined theories. In: Cook, B., Podelski, A. (eds.) VMCAI 2007. LNCS, vol.
4349, pp. 378–394. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-
540-69738-1 27
4. Bradley, A.R.: IC3 and beyond: incremental, inductive veriﬁcation. In: Madhusu-
dan, P., Seshia, S.A. (eds.) CAV 2012. LNCS, vol. 7358, pp. 4–4. Springer, Heidel-
berg (2012). https://doi.org/10.1007/978-3-642-31424-7 4
5. Bradley, A.R., Manna, Z.: Property-directed incremental invariant generation. For-
mal Asp. Comput. 20(4–5), 379–405 (2008)
6. Bradley, A.R., Manna, Z., Sipma, H.B.: What’s decidable about arrays? In: Emer-
son, E.A., Namjoshi, K.S. (eds.) VMCAI 2006. LNCS, vol. 3855, pp. 427–442.
Springer, Heidelberg (2005). https://doi.org/10.1007/11609773 28
7. Bruttomesso, R., Ghilardi, S., Ranise, S.: Quantiﬁer-free interpolation in combina-
tions of equality interpolating theories. ACM Trans. Comput. Log. 15(1), 5:1–5:34
(2014)
8. Conchon, S., Goel, A., Krsti´c, S., Mebsout, A., Za¨ıdi, F.: Cubicle: a parallel SMT-
based model checker for parameterized systems. In: Madhusudan, P., Seshia, S.A.
(eds.) CAV 2012. LNCS, vol. 7358, pp. 718–724. Springer, Heidelberg (2012).
https://doi.org/10.1007/978-3-642-31424-7 55
9. Dillig, I., Dillig, T., Li, B., McMillan, K.L.: Inductive invariant generation via
abductive inference. In: Hosking, A.L., Eugster, P.T., Lopes, C.V., (eds.) Proceed-
ings of the 2013 ACM SIGPLAN International Conference on Object Oriented Pro-
gramming Systems Languages & Applications, OOPSLA 2013, part of SPLASH
2013, pp. 443–456. ACM (2013)
10. Dolzmann, A., Sturm, T.: REDLOG: computer algebra meets computer logic. ACM
SIGSAM Bull. 31(2), 2–9 (1997)
11. Faber, J., Jacobs, S., Sofronie-Stokkermans, V.: Verifying CSP-OZ-DC speciﬁca-
tions with complex data types and timing parameters. In: Davies, J., Gibbons,
J. (eds.) IFM 2007. LNCS, vol. 4591, pp. 233–252. Springer, Heidelberg (2007).
https://doi.org/10.1007/978-3-540-73210-5 13

404
D. Peuter and V. Sofronie-Stokkermans
12. Falke, S., Kapur, D.: When is a formula a loop invariant? In: Mart´ı-Oliet, N.,
¨Olveczky, P.C., Talcott, C. (eds.) Logic, Rewriting, and Concurrency. LNCS, vol.
9200, pp. 264–286. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-
23165-5 13
13. Ganzinger, H., Sofronie-Stokkermans, V., Waldmann, U.: Modular proof systems
for partial functions with weak equality. In: Basin, D., Rusinowitch, M. (eds.)
IJCAR 2004. LNCS (LNAI), vol. 3097, pp. 168–182. Springer, Heidelberg (2004).
https://doi.org/10.1007/978-3-540-25984-8 10
14. Ganzinger, H., Sofronie-Stokkermans, V., Waldmann, U.: Modular proof systems
for partial functions with Evans equality. Inf. Comput. 204(10), 1453–1492 (2006)
15. Ghilardi, S., Ranise, S.: Backward reachability of array-based systems by SMT
solving: Termination and invariant synthesis. Logical Methods Comput. Sci. 6(4),
1–48 (2010)
16. Gleiss, B., Kov´acs, L., Robillard, S.: Loop analysis by quantiﬁcation over iterations.
In: Barthe, G., Sutcliﬀe, G., Veanes, M., (eds.) 22nd International Conference on
Logic for Programming, Artiﬁcial Intelligence and Reasoning, volume 57 of EPiC
Series in Computing, LPAR-22, pp. 381–399 (2018). EasyChair
17. Gurﬁnkel, A., Shoham, S., Vizel, Y.: Quantiﬁers on demand. In: Lahiri, S.K., Wang,
C. (eds.) ATVA 2018. LNCS, vol. 11138, pp. 248–266. Springer, Cham (2018).
https://doi.org/10.1007/978-3-030-01090-4 15
18. Hoder, K., Kov´acs, L., Voronkov, A.: Interpolation and symbol elimination in Vam-
pire. In: Giesl, J., H¨ahnle, R. (eds.) IJCAR 2010. LNCS (LNAI), vol. 6173, pp.
188–195. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-14203-
1 16
19. Horbach, M., Sofronie-Stokkermans, V.: Obtaining ﬁnite local theory axiomatiza-
tions via saturation. In: Fontaine, P., Ringeissen, C., Schmidt, R.A. (eds.) FroCoS
2013. LNCS (LNAI), vol. 8152, pp. 198–213. Springer, Heidelberg (2013). https://
doi.org/10.1007/978-3-642-40885-4 14
20. Horbach, M., Sofronie-Stokkermans, V.: Locality transfer: From constrained axiom-
atizations to reachability predicates. In: Demri, S., Kapur, D., Weidenbach, C.
(eds.) IJCAR 2014. LNCS (LNAI), vol. 8562, pp. 192–207. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-08587-6 14
21. Horbach, M., Weidenbach, C.: Deciding the inductive validity of ∀∃* queries. In:
Gr¨adel, E., Kahle, R. (eds.) CSL 2009. LNCS, vol. 5771, pp. 332–347. Springer,
Heidelberg (2009). https://doi.org/10.1007/978-3-642-04027-6 25
22. Ihlemann, C., Jacobs, S., Sofronie-Stokkermans, V.: On local reasoning in veriﬁca-
tion. In: Ramakrishnan, C.R., Rehof, J. (eds.) TACAS 2008. LNCS, vol. 4963, pp.
265–281. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-78800-
3 19
23. Ihlemann, C., Sofronie-Stokkermans, V.: System description: H-PILoT. In:
Schmidt, R.A. (ed.) CADE 2009. LNCS (LNAI), vol. 5663, pp. 131–139. Springer,
Heidelberg (2009). https://doi.org/10.1007/978-3-642-02959-2 9
24. Ihlemann, C., Sofronie-Stokkermans, V.: On hierarchical reasoning in combinations
of theories. In: Giesl, J., H¨ahnle, R. (eds.) IJCAR 2010. LNCS (LNAI), vol. 6173,
pp. 30–45. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-14203-
1 4
25. Jacobs, S., Kuncak, V.: Towards complete reasoning about axiomatic speciﬁca-
tions. In: Jhala, R., Schmidt, D. (eds.) VMCAI 2011. LNCS, vol. 6538, pp. 278–293.
Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-18275-4 20
26. Kapur, D.: A quantiﬁer-elimination based heuristic for automatically generating
inductive assertions for programs. J. Syst. Sci. Complexity 19(3), 307–330 (2006)

On Invariant Synthesis for Parametric Systems
405
27. Kapur, D., Majumdar, R., Zarba, C.G.: Interpolation for data structures. In:
Young, M., Devanbu, P.T., (eds.) Proceedings of the 14th ACM SIGSOFT Interna-
tional Symposium on Foundations of Software Engineering, FSE 2006, pp. 105–116.
ACM (2006)
28. Karbyshev, A., Bjørner, N., Itzhaky, S., Rinetzky, N., Shoham, S.: Property-
directed inference of universal invariants or proving their absence. J. ACM 64(1),
7:1–7:33 (2017)
29. Kov´acs, L., Voronkov, A.: Finding loop invariants for programs over arrays using
a theorem prover. In: Chechik, M., Wirsing, M. (eds.) FASE 2009. LNCS, vol.
5503, pp. 470–485. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-
642-00593-0 33
30. Kov´acs, L., Voronkov, A.: Interpolation and symbol elimination. In: Schmidt, R.A.
(ed.) CADE 2009. LNCS (LNAI), vol. 5663, pp. 199–213. Springer, Heidelberg
(2009). https://doi.org/10.1007/978-3-642-02959-2 17
31. Padon, O., Immerman, N., Shoham, S., Karbyshev, A., Sagiv, M.: Decidability of
inferring inductive invariants. In: Bod´ık, R., Majumdar, R., (eds.) Proceedings of
the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Program-
ming Languages, POPL 2016, pp. 217–231. ACM (2016)
32. Peuter, D., Sofronie-Stokkermans, V.: On invariant synthesis for parametric sys-
tems. CoRR http://arxiv.org/abs/1905.12524 (2019)
33. Sofronie-Stokkermans, V.: Hierarchic reasoning in local theory extensions. In:
Nieuwenhuis, R. (ed.) CADE 2005. LNCS (LNAI), vol. 3632, pp. 219–234. Springer,
Heidelberg (2005). https://doi.org/10.1007/11532231 16
34. Sofronie-Stokkermans, V.: Interpolation in local theory extensions. In: Furbach, U.,
Shankar, N. (eds.) IJCAR 2006. LNCS (LNAI), vol. 4130, pp. 235–250. Springer,
Heidelberg (2006). https://doi.org/10.1007/11814771 21
35. Sofronie-Stokkermans, V.: Interpolation in local theory extensions. Logical Meth-
ods Comput. Sci. 4(4), 1–31 (2008)
36. Sofronie-Stokkermans, V.: Hierarchical reasoning for the veriﬁcation of parametric
systems. In: Giesl, J., H¨ahnle, R. (eds.) IJCAR 2010. LNCS, vol. 6173, pp. 171–187.
Springer, Berlin (2010). https://doi.org/10.1007/978-3-642-14203-1 15
37. Sofronie-Stokkermans, V.: Hierarchical reasoning and model generation for the
veriﬁcation of parametric hybrid systems. In: Bonacina, M.P. (ed.) CADE 2013.
LNCS (LNAI), vol. 7898, pp. 360–376. Springer, Heidelberg (2013). https://doi.
org/10.1007/978-3-642-38574-2 25
38. Sofronie-Stokkermans, V.: On interpolation and symbol elimination in theory
extensions. In: Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol.
9706, pp. 273–289. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
40229-1 19
39. Sofronie-Stokkermans, V.: On interpolation and symbol elimination in theory
extensions. Logical Methods Comput. Sci. 14(3), 1–41 (2018)
40. Yorsh, G., Musuvathi, M.: A combination method for generating interpolants. In:
Nieuwenhuis, R. (ed.) CADE 2005. LNCS (LNAI), vol. 3632, pp. 353–368. Springer,
Heidelberg (2005). https://doi.org/10.1007/11532231 26

The Aspect Calculus
David A. Plaisted(B)
UNC Chapel Hill, Chapel Hill, NC 27599-3175, USA
plaisted@cs.unc.edu
Abstract. For theorem proving applications, the aspect calculus for rea-
soning about states and actions has some advantages over existing situ-
ation calculus formalisms, and also provides an application domain and
a source of problems for ﬁrst-order theorem provers. The aspect calculus
provides a representation for reasoning about states and actions that is
suited to modular domains. An aspect names a portion of a state, that
is, a substate, such as a room in a building or a city in a country. Aspects
may have aspects of their own. A state is assumed to be either a leaf state
that cannot be further decomposed, or to be composed of substates, and
actions associated with one substate do not inﬂuence other, disjoint sub-
states. This feature can reduce the number of frame axioms that are
needed if the domain has a modular structure. It can also permit plan-
ning problems on independent substates to be solved independently to
some degree. However, interactions between independent substates are
also permitted.
Keywords: Situation calculus · Frame problem · Aspects ·
Equational reasoning
1
Introduction
The situation calculus permits reasoning about properties of situations that
result from a given situation by sequences of actions [MH69]. In the situation
calculus, situations (states) are represented explicitly by variables, and actions a
map states s to states do(a, s). Predicates and functions on a situation or state
are called ﬂuents. In some formalisms, a situation denotes a state of the world,
specifying the values of ﬂuents, so that two situations are equal if the values of
all their ﬂuents are the same. Other formalisms reserve the term situation for
a sequence of states. A problem with the situation calculus or any formalism
for reasoning about actions is the necessity to include a large number of frame
axioms that express the fact that actions do not inﬂuence many properties (ﬂu-
ents) of a state. Since the early days of artiﬁcial intelligence research the frame
problem has been studied, beginning with McCarthy and Hayes [MH69]. Lin
[Lin08] has written a recent survey of the situation calculus.
Reiter [Rei91] proposed an approach to the frame problem in ﬁrst-order logic
that avoids the need to specify all of the frame axioms. The method of Reiter,
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 406–424, 2019.
https://doi.org/10.1007/978-3-030-29436-6_24

The Aspect Calculus
407
foreshadowed by Haas [Haa87], Pednault [Ped89], Schubert [Sch90] and Davis
[Dav90], essentially solves the frame problem by specifying that a change in the
truth value of a ﬂuent, caused by an action, is equivalent to a certain condition on
the action. In this formalism, it is only necessary to list the actions that change
each ﬂuent, and it is not necessary to specify the frame axioms directly. If an
action does not satisfy the condition, the ﬂuent is not aﬀected. In the following
discussion the term “Reiter’s formalism” will be used for simplicity even though
others have also contributed to its development. The ﬂuent calculus [Thi98] is
another interesting approach to the frame problem. In this approach, a state is
a conjunction of known facts.
In the present paper, yet another approach to the frame problem using aspects
is presented. This approach is based on the idea that the world is hierarchical or
modular to a large extent. Aspects permit one to structure ﬂuents and actions
in a modular way.
The aspect formalism considers a situation, or state, to be composed of sub-
states, These substates are named by aspects. Substates may have substates of
their own. The aspect calculus constructs a tree of aspects. For example, the
top node could be “earth”, its children could be various countries, each country
could have its states as children, and each state could have its cities as chil-
dren. An aspect is a sequence of identiﬁers such as (earth, USA, North Carolina,
Chapel Hill). The aspect calculus is suitable if actions in a substate do not have
much inﬂuence on ﬂuents from a disjoint substate, roughly speaking. Thus the
action of teaching a class in Chapel Hill would have aspect (earth, USA, North
Carolina, Chapel Hill) and would only inﬂuence ﬂuents that also had the same
aspect, or an aspect referring to a part of Chapel Hill. This action would not have
any eﬀect on ﬂuents with aspects of a diﬀerent city, state, or country. However,
calling someone in Washington DC from Chapel Hill would inﬂuence ﬂuents in
both cities and would have to be given an aspect of (earth, USA). Instead of
sequences of names, the formal theory of aspects uses sequences of numbers.
Hayes actually mentioned “frames” which are very similar to aspects as a
possible solution to the frame problem. He did not reject frames, but felt that
they would not solve the frame problem in all cases. He wrote [Hay73], “In the
long run I believe that a mixture of frame rules and consistency-based methods
will be required for non-trivial problems ...” (page 56).
Petrick [Pet08] has adapted Reiter’s formalism to knowledge and belief and
has also introduced the notion of a Cartesian situation that can decompose a
situation into parts, in a way that appears to be similar to the aspect calculus.
However, his formalism also considers a situation to include a sequence of states.
The aspect calculus has some advantages over Reiter’s formalism, especially
in its suitability for ﬁrst-order theorem provers. In Reiter’s formalism, the succes-
sor state axiom for a ﬂuent essentially says that the ﬂuent is true on a situation
do(a, s) for ﬂuent a and situation s if a is an action that makes the ﬂuent true,
or if the ﬂuent was already true and a is not one of the actions that makes the
ﬂuent false. This requires one to know under what conditions an action changes
the value of the ﬂuent to “true” or “false.” If for example the action is nonde-

408
D. A. Plaisted
terministic this may be diﬃcult to know. Also, to formulate the successor state
axiom, one needs a theory of equality between actions. If there are only a small
number of actions that can make a ﬂuent false, then Reiter’s formalism is concise
because one need not list all of the actions that do not inﬂuence the ﬂuent (the
frame axioms for the ﬂuent). However, if there are many actions (possibly thou-
sands or millions) that inﬂuence the ﬂuent, then this successor state axiom can
become very long. Further, when converting Reiter’s approach to clause form,
one needs an axiom of the form “For all actions a, a = a1 ∨a = a2 ∨· · ·∨a = an”
where ai are all the possible actions, as well as the axioms ai ̸= aj for all i ̸= j. If
there are many actions, the ﬁrst axiom will be huge. It is also diﬃcult for many
theorem provers to handle axioms of this form.
Even the successor state axiom itself, when translated into clause form, pro-
duces clauses having a disjunction of an equation and another literal. Using
Φ(p, s) to denote the value of ﬂuent p on situation s, a simple form of the suc-
cessor state axiom would be
Φ(p, do(x, s)) ≡[(Φ(p, s) ∧(x ̸= a1) ∧(x ̸= a2)) ∨(x = b1 ∨x = b2)]
where a1 and a2 are the only actions that can make p false and b1 and b2 are
the only actions that make p true. Consider an even simpler form:
Φ(p, do(x, s)) ≡[(Φ(p, s) ∧(x ̸= a1)) ∨(x = b1)]
The clause form of the latter is ¬Φ(p, do(x, s))∨Φ(p, s)∨x = b1, ¬Φ(p, do(x, s))∨
x ̸= a1 ∨x = b1, x ̸= b1 ∨Φ(p, do(x, s)), ¬Φ(p, s) ∨x = a1 ∨Φ(p, do(x, s)). Such
conjunctions of equations and inequations can be diﬃcult for theorem provers
to handle, especially if there are more actions in which case there would be more
equations and inequations in the clauses.
The aspect calculus by contrast introduces many axioms that are unit equa-
tions, which are particularly easy for many theorem provers to handle. If the
underlying domain is ﬁrst-order then the aspect calculus is entirely expressed in
ﬁrst-order logic, so powerful ﬁrst-order theorem provers can be applied to plan-
ning problems by framing a query of the form “There exists a situation having
certain properties” and attempting to prove it. For this, a reﬂexive and transi-
tive predicate reachable can be deﬁned, the axioms reachable(s, do(a, s)) can be
added for all actions a, and theorems of the form (∃s)(reachable(s0, s) ∧A[s])
can be proved where s0 is some starting state and A is a ﬁrst-order formula.
However, Reiter’s formalism can handle domains without a clear hierarchical
structure, especially if there are only a small number of actions that inﬂuence
each ﬂuent. Also, the aspect calculus does not handle knowledge and belief.
Reiter’s formalism attempts to make it easy to decide if a ﬂuent is true on a
situation obtained from a starting situation by a sequence of actions. The aspect
calculus by contrast only attempts to preserve provability in the underlying the-
ory while reducing the number of frame axioms.
The aspect calculus has other advantages independent of its suitability for
theorem provers. Locality can be incorporated into the planning process. For
example, if one wants to obtain a state t from s and the only diﬀerence is that

The Aspect Calculus
409
a room in a building has changed, then one can ﬁrst look for a plan that does
not change anything outside the room. If that does not work, one can look
for a plan that only changes rooms on that ﬂoor, changes to the other rooms
being only temporary. If that does not work, one can look for a plan that only
changes properties of the building, and nothing outside of it, and so on. Also,
if the state space is ﬁnite, then the search space for planning problems in the
aspect calculus is also ﬁnite. With Reiter’s approach [Rei91], situations contain
sequences of states, so the search space can be inﬁnite. Planning in disjoint
sub-states (aspects) of a state can be done independently to some extent. This
reduces redundancies due to the order of actions involving independent modules
not aﬀecting the result.
Further, a possible problem with Reiter’s approach, noted in Scherl and
Levesque [SL93], is the ramiﬁcation problem, namely, it can be diﬃcult to incor-
porate constraints between ﬂuents, such as when one ﬂuent implies another.
The successor state axiom essentially implies that the only way a ﬂuent
can become true is for an action to make it true. A great deal of work
[Sha99,LR94,DT07,McI00,Ter00,MM97] has been done to handle the ramiﬁ-
cation problem in Reiter’s system. No special treatment for the ramiﬁcation
problem is needed in the aspect calculus, but the theory needs to be hierarchi-
cal, that is, it should be possible to assign aspects so that disjoint aspects are
largely independent.
2
Underlying Theory
We assume that there is some underlying set T of axioms in ﬁrst-order logic
concerning states, ﬂuents, and actions. The semantics of this axiomatization will
have domains for states and actions, with ﬂuents mapping from states to various
domains. We do not necessarily assume that T is encoded in any particular
situation calculus, such as Reiter’s [Rei91]. We will modify such a state theory
T to obtain an axiomatization T aspect that in some cases can more economically
encode frame axioms than T does. In some cases T aspect can be custom designed
without transformation from a theory T .
Actions in T are typically indicated by the letter a, possibly with subscripts,
and ﬂuents are typically indicated by the letters p and q, possibly with subscripts.
F is the set of all ﬂuents and A is the set of actions. States are denoted by s, t,
and u, possibly with subscripts. The set of states is S.
If a is an action and s is a state then do(a, s) is the result of applying action a
in state s. For nondeterminism, instead of do(a, s) = t one would write do(a, s, t)
indicating that t is a possible result of applying action a in state s. It appears
that the aspect formalism can handle this without a problem, but this has not
been formally investigated. If p is a ﬂuent then Φ(p, s) is the value of p on state
s. Thus ﬂuents are essentially functions from states to various domains. If the
value of a ﬂuent is true or false, and it is not parameterized, then Φ(p, s) may be
written as p(s) instead. The semantics (interpretation) of the underlying theory
T is assumed to have sorts for ﬂuents, states, and actions, in addition to possibly
others.

410
D. A. Plaisted
The semantics of operations is deﬁned by assertions of the following form:
λx1x2 . . . xn.E[x1, . . . , xn] : ψi · · · ψn →ψ0
indicating that in the expression E, xi are assumed to have sort ψi and E returns
a value of sort ψ0. One can then deﬁne the semantics of do and Φ as follows:
λas.do(a, s) : A × S →S
λps.Φ(p, s) : F × S →D for some domain D
We assume that T satisﬁes the action dependency condition if the ﬂuents of
do(a, s) only depend on the ﬂuents of s. This is formally deﬁned as follows:
Deﬁnition 1. The theory T satisﬁes the action dependency condition if T |=
(∀s, t ∈S)(∀a ∈A), ((∀p ∈F)Φ(p, s) = Φ(p, t)) →((∀p ∈F)Φ(p, do(a, s)) =
Φ(p, do(a, t))).
This constraint must be satisﬁed in order to use the aspect representation.
Example 1. We give an example state theory Ln in the “classical represen-
tation.” For simplicity, ﬂuents are written as ron(i, s), lon(i, s), lonall(s),
ronall(s),
and
onall(s)
instead
of
Φ(ron(i), s),
Φ(lon(i), s),
Φ(lonall, s),
Φ(ronall, s), and Φ(onall, s), respectively. In general, ﬂuents are functions, but
because these are all Booleans, we write ron(i, s) instead of ron(i, s) = true, et
cetera.
Suppose there are two banks of n switches that can be turned on and oﬀand
each switch controls a light. So there are actions lton(i) (turn i on in the left
bank) and ltof(i) (turn i oﬀin the left bank) for 1 ≤i ≤n, also rton(i) and
rtof(i) for the right bank. There are also ﬂuents lon(i, s) and ron(i, s) telling
whether the i-th light is on in the left and right banks. There is also a ﬂuent
lonall(s) telling whether all the lights are on in the left bank, and similarly ronall
for the right bank, and onall(s) for both banks being all on. A state is deﬁned
by whether the switches are on or oﬀ; all ﬂuents other than lon(i) and ron(i)
are functions of these. Thus there are 4n states in all, one for each combined
setting of the 2n switches. We can indicate a state in which lon(i) = bi and
ron(i) = ci for Booleans bi, ci by [b1, . . . , bn, c1, . . . , cn]S where the subscript S
may be omitted. The ﬂuents lonall, ronall, and onall can be determined from
bi and ci and are not explicitly shown in this notation.
In the following equations for Ln, the free variables s are states and are
universally quantiﬁed. (A5)c through (A8)c are the frame axioms, and they make
this representation quadratic in n.

The Aspect Calculus
411
lon(i, do(lton(i), s)) ∧ron(i, do(rton(i), s)), 1 ≤i ≤n(A1)c
¬lon(i, do(ltof(i), s)) ∧¬ron(i, do(rtof(i), s)), 1 ≤i ≤n(A2)c
lonall(s) ≡lon(1, s) ∧· · · ∧lon(n, s)(A3)c
l
ronall(s) ≡ron(1, s) ∧· · · ∧ron(n, s)(A3)c
r
onall(s) ≡lonall(s) ∧ronall(s) (A4)c
lon(i, do(lton(j), s)) ≡lon(i, s), 1 ≤i, j ≤n, i ̸= j(A5)c
l
ron(i, do(rton(j), s)) ≡ron(i, s), 1 ≤i, j ≤n, i ̸= j(A5)c
r
lon(i, do(rton(j), s)) ≡lon(i, s), 1 ≤i, j ≤n(A6)c
l
ron(i, do(lton(j), s)) ≡ron(i, s), 1 ≤i, j ≤n(A6)c
r
lon(i, do(ltof(j), s)) ≡lon(i, s), 1 ≤i, j ≤n, i ̸= j(A7)c
l
ron(i, do(rtof(j), s)) ≡ron(i, s), 1 ≤i, j ≤n, i ̸= j(A7)c
r
lon(i, do(rtof(j), s)) ≡lon(i, s), 1 ≤i, j ≤n(A8)c
l
ron(i, do(ltof(j), s)) ≡ron(i, s), 1 ≤i, j ≤n(A8)c
r
∃s(s ∈S)(A9)c
3
Aspects
The theory T will be extended to a theory T aspect that may permit many of
the frame axioms of T to be omitted but will still permit the same plans to be
derived. T aspect is constructed so that any model M of the underlying theory
T can be extended to a model M aspect of T aspect. This implies the relative
consistency of T aspect with respect to T , which essentially means that incorrect
plans cannot be derived in T aspect. With notation as in the introduction, this
means that (∃s)(reachable(s0, s) ∧A[s]) is derivable in T aspect iﬀit is derivable
in T , but T aspect may have many fewer frame axioms. When presenting aspects
the model M aspect is essentially being described.
In T aspect there are aspects and statelets in addition to the states, actions,
and ﬂuents of T . Also, Ψ is the set of aspects and ˆS is the set of statelets.
The aspects are organized in T aspect in an aspect tree. This can be regarded
as part of the model M aspect.
Deﬁnition 2. The aspect tree Υ is a ﬁnite tree with a root node. Every other
node in the tree is either a leaf with no children or else has ﬁnitely many children
ordered from left to right. The nodes in the tree are labeled with sequences or
strings of integers. The root is labeled with ϵ, the empty string. If a node N is
labeled with α and has n children then its n children left to right are labeled α1
through αn. These sequences or strings of integers are called aspects. Aspects
are indicated by Greek letters α, β, γ, possibly with subscripts. If node N with n
children has aspect α then the aspects α1 · · · αn are called the children of aspect
α, and α is called the parent of αi for all i. Sometimes aspects can be written
with commas between the numbers, as, 1, 2, 1 or (1, 2, 1). If node L has aspect α
and node N has aspect β and L is an ancestor of N in the aspect tree, then we

412
D. A. Plaisted
say that α is an ancestor of β and β is a descendant of α. Thus if α is a preﬁx
of β then α is an ancestor of β and β is a descendent of α. If α is (3, 2, 4) then
αi is (3, 2, 4, i).
Deﬁnition 3. There is an ordering relation < on aspects with α < β if α is an
ancestor (proper preﬁx) of β, α < β iﬀβ > α and ≥, ≤are deﬁned as usual.
Thus for example 1, 2 > 1.
Also, if two aspects α and β are neither ancestors or descendants of one
another, so that neither one is a preﬁx of the other, they are said to be incom-
parable, independent, or disjoint, written α#β.
Actions and ﬂuents have unique aspects assigned to them in T aspect. This
assignment has to be done manually. If the theory has a natural hierarchical
structure then this should be easier.
We write a : α to indicate that action a has aspect α, and p : α to indicate
that a ﬂuent p has aspect α; one can also write aspect(p) = α and aspect(a) = α.
Example 2. Continuing with the example from Example 1, for L3 (three switches
in the left and right banks) in Laspect
3
there would be aspects ϵ, (1), (2) and (i, j)
for i = 1, 2 and j = 1, 2, 3. The aspect ϵ refers to the whole problem, (1) to the
left bank of switches, (2) to the right bank, and (i, j) to switch j in the left or
right bank.
4
Statelets
In addition to states, there is a set ˆS of statelets or modules in T aspect. Statelets
can be indicated by the letters s, t, and u, possibly with subscripts. In T aspect,
actions and ﬂuents are extended from states to statelets. Thus
λas.do(a, s) : A × ˆS →ˆS ∪{⊥} where ⊥is “don’t care.”
λps.Φ(p, s) : F × ˆS →D ∪{⊥} for some domain D where ⊥is “don’t care.”
An assignment of aspects will be called unconstraining if it does not impose addi-
tional restrictions on T , in a way that will be made precise later (Deﬁnition 16).
In general, statelets have unique aspects; writing s : α indicates that the
aspect of statelet s is α. Equality for statelets s : α and t : β is deﬁned by their
ﬂuents and their aspect, as follows:
If for all p in F, Φ(p, s) = Φ(p, t) and α = β, then s = t.
(1)
Thus statelets are entirely determined by how ﬂuents map them, and by their
aspect. This diﬀers from states, which may have additional properties not used
by our formalism. Also, there is a new value ⊥such that for all ﬂuents p and all
states s, Φ(p, s) ̸= ⊥. Further, ⊥is not equal to any state or statelet. If s is a
statelet and p is a ﬂuent then Φ(p, s) can be ⊥(don’t care).

The Aspect Calculus
413
Example 3. For Laspect
3
, there would be 64 statelets at aspect ϵ, indicating the
combined setting of all six switches. Therefore if statelet s has aspect ϵ then
lon(i, s) and ron(i, s) would be true or false for all i. There would be eight
statelets at aspect (1), specifying the combined setting of the three left switches,
and similarly eight statelets at aspect (2). Also, there would be two statelets at
aspects (i, j) for i = 1, 2 and j = 1, 2, 3, specifying the two possible settings of
the corresponding switch.
Deﬁnition 4. The ⊥values of a statelet are speciﬁed as follows: If a statelet s
has aspect α then (∀p ∈F)(∀β ∈Ψ)[(p : β) →(β ≥α ≡(Φ(p, s) ̸= ⊥))].
Letting the aspect be part of the statelet eliminates some complexities from
the system; one can then speak unambiguously about the parent and children
of a statelet.
Statelets in Laspect
n
can be indicated by [b1, . . . , bn, c1, . . . , cn] ˆ
S where the bi
and ci can be Booleans or ⊥and the subscript ˆS may be omitted. Technically
one should also indicate the aspect as well as the values of the ﬂuents, but in
this example the values of the ﬂuents are enough to determine the aspect.
For Laspect
3
, if s : (1) (statelet s has aspect (1)) then lon(i, s) = true or
false and ron(i, s) = ⊥for i = 1, 2, 3. Thus statelets at aspect (1) are of the
form [b1, b2, b3, ⊥, ⊥, ⊥] where the bi are Booleans. If s : (2) (statelet s has
aspect (2)) then ron(i, s) = true or false and lon(i, s) = ⊥for i = 1, 2, 3.
Thus statelets at aspect (2) are of the form [⊥, ⊥, ⊥, c1, c2, c3] where the ci are
Booleans. If s : (1, i) then lon(j) = ⊥for j ̸= i and ron(j) = ⊥for j = 1, 2, 3
but lon(i, s) = true or false. If s : (2, i) then ron(j) = ⊥for j ̸= i and
lon(j) = ⊥for j = 1, 2, 3 but ron(i, s) = true or false. So a statelet at aspect
(1, 2) is of the form [⊥, b2, ⊥, ⊥, ⊥, ⊥] and a statelet at aspect (2, 3) is of the
form [⊥, ⊥, ⊥, ⊥, ⊥, c3].
Deﬁnition 5. For states or statelets s and t, one writes s ≡α t if for all ﬂu-
ents p with p : β and β ≥α, Φ(p, s) = Φ(p, t). Corresponding to this there
is the assertion s ≡[α] t in T that does not mention aspects. This is deﬁned
as Φ(p1, s) = Φ(p1, t) ∧Φ(p2, s) = Φ(p2, t) ∧· · · ∧Φ(pn, s) = Φ(pn, t) where
{p1, p2, · · · , pn} is the set of all ﬂuents having aspects β in M aspect with β ≥α.
Thus s ≡ϵ t if s, t agree on all ﬂuents in F. In Laspect
3
, [b1, b2, b3, c1, c2, c3] ≡(1)
[b1, b2, b3, c′
1, c′
2, c′
3].
In M aspect, states are related to statelets as follows:
Deﬁnition 6. The bridging axioms are the following: If s is a state then sϵ is
a statelet and Φ(p, sϵ) = Φ(p, s) for all p ∈F. Also, if a is an action and s, t
are states and do(a, s) = t then do(a, sϵ) = tϵ Furthermore, for all statelets s at
aspect α there is a state t such that s ≡α t.
In Laspect
3
, [b1, b2, b3, c1, c2, c3]ϵ
S = [b1, b2, b3, c1, c2, c3] ˆ
S.
There is also a function that restricts statelets at an aspect to statelets at
another aspect. The function sβ with semantics λsβ.sβ : ˆS × Ψ →ˆS deﬁned as
follows:

414
D. A. Plaisted
Deﬁnition 7. If s is a statelet, s : α, and β ≥α then sβ is deﬁned as the statelet
at aspect β such that Φ(p, sβ) = Φ(p, s) if aspect(p) ≥β, else Φ(p, sβ) = ⊥. Thus
sβ : β and sβ ≡β s. If β ≤α or α#β then sβ is not deﬁned. If s is a state then
sα = (sϵ)α.
Thus in Laspect
3
, [b1, b2, b3, c1, c2, c3](1) = [b1, b2, b3, ⊥, ⊥, ⊥] and [b1, b2, b3, c1,
c2, c3](1,2) = [⊥, b2, ⊥, ⊥, ⊥, ⊥]. Also [b1, b2, b3, ⊥, ⊥, ⊥](1,3) = [⊥, ⊥, b3, ⊥, ⊥, ⊥].
In general, ˆS is {sα : s ∈S, α ∈Ψ}.
There is also a function f α, the composition function, that combines statelets
(sub-modules) at aspects α1 · · · αn to produce a statelet (module) at aspect α.
It has the semantics λαs1s2 . . . sn.f α(s1, . . . , sn) : Ψ × ˆSn →ˆS where n is the
number of children of α.
Recall that αi is the sequence α with i added to the end.
Deﬁnition 8. Suppose α is an aspect with n children (which are α1, . . . , αn).
Suppose s1 : α1, · · · , sn : αn for statelets si. Then f α(s1, s2, · · · , sn) = s where
s is a statelet at aspect α, and where for ﬂuent p, if p : β with β ≥αi then
Φ(p, s) = Φ(p, si). For ﬂuents p with p : α, Φ(p, s) is deﬁned by the leaf depen-
dency constraint, Deﬁnition 9, below. For other ﬂuents p with p : β for β ̸≥α,
Φ(p, s) = ⊥.
In Laspect
3
, f ϵ([b1, b2, b3, ⊥, ⊥, ⊥], [⊥, ⊥, ⊥, c1, c2, c3]) = [b1, b2, b3, c1, c2, c3].
The ﬁrst argument of f ϵ
is a statelet at aspect (1) and the second
argument is a statelet at aspect (2). The value of f ϵ is a statelet at
aspect ϵ. Also, f (1)([b1, ⊥, ⊥, ⊥, ⊥, ⊥], [⊥, b2, ⊥, ⊥, ⊥, ⊥], [⊥, ⊥, b3, ⊥, ⊥, ⊥]) =
[b1, b2, b3, ⊥, ⊥, ⊥]. f (1) can also be written f 1.
From this deﬁnition it follows that s ≡αi si for all i and sαi = si. Deﬁnition 8
also implies the following aspect composition equation for non-leaf aspects α and
statelets s at aspect α:
f α(sα1, sα2, · · · , sαn) = s
(2)
In addition, there is a dependency constraint on ﬂuents. That is, non-leaf
ﬂuents have to depend on ﬂuents at the leaves of the aspect tree.
Deﬁnition 9. The leaf dependency constraint on ﬂuents is the following: If p
is a ﬂuent at non-leaf aspect α, s and t are statelets at aspect α, and Φ(q, s) =
Φ(q, t) for all ﬂuents q at leaf aspects γ with γ > α, then Φ(p, s) = Φ(p, t).
In terms of T , this is expressed as a collection of assertions
{A(p, q1, · · · , qn) : (p : α), α is a non-leaf aspect, and {q1, · · · , qn} is the set of
fluents at leaf aspects γ with γ > α}
where A(p, q1, · · · , qn) is the following assertion:
For all states s and t,
Φ(q1, s) = Φ(q1, t) ∧· · · ∧Φ(qn, s) = Φ(qn, t) →Φ(p, s) = Φ(p, t)

The Aspect Calculus
415
The leaf dependency constraint on ﬂuents is necessary for f α to be a mathe-
matical function. This is the ﬁrst constraint that must be satisﬁed when assigning
aspects to actions and ﬂuents. If one wants a statelet s to have properties that
do not depend on the children aspects, then one can add a “virtual” child of the
aspect of s that includes the extra information about s.
For Laspect
3
, the leaf aspects are (1, i) and (2, i) for i = 1, 2, 3. The ﬂuents at
these aspects are lon(i) and ron(i), respectively. Thus the values of all other ﬂu-
ents have to be determined by these. For statelet s at aspect ϵ or (1), Φ(lonall, s)
is determined by Φ(lon(i), s) for i = 1, 2, 3. Also, for Laspect
3
and statelet s at
aspect ϵ or (2), Φ(ronall, s) is determined by Φ(ron(i), s) for i = 1, 2, 3 so this
constraint is satisﬁed. Similarly, for statelet s at aspect ϵ, Φ(onall, s) is deter-
mined by the values Φ(lon(i), s) and Φ(ron(i), s) for i = 1, 2, 3.
Deﬁnition 10. The combining axiom is the following: For all aspects α with n
children and for all states s1, · · · , sn there is a state s such that
s ≡[α1] s1 ∧· · · ∧s ≡[αn] sn.
The combining axiom is the second constraint that must be satisﬁed when
assigning aspects to ﬂuents and actions. This is satisﬁed for Laspect
3
because all
combinations of all switch settings are permitted. This follows from (A9)c and
the eﬀects of the actions.
5
Actions
For action a at aspect α, do(a, s) is deﬁned for statelets s at aspect β iﬀβ ≤α.
Otherwise, do(a, s) = ⊥. Thus do(a, s) is not always a statelet or a state, because
it can be ⊥. If do(a, s) ̸= ⊥then aspect(do(a, s)) = aspect(s).
There are some locality constraints on actions that need to be respected for
T aspect to be unconstraining. Taken collectively, these are the third constraint
that must be satisﬁed by the assignment of aspects to ﬂuents and actions.
Deﬁnition 11. The locality constraints on actions are as follows: Suppose a : α
and p : β. If α#β then Φ(p, do(a, s)) = Φ(p, s) for all states s. (Formally, this
has to be a theorem of T for all such α and β). Also, if s ≡α t for states s and
t (expressed in T by s ≡[α] t) and β ≥α then Φ(p, do(a, s)) = Φ(p, do(a, t)).
These constraints are satisﬁed for Laspect
3
because the action lton(i) does not
change the values of any ﬂuents except lon(i) at aspect (1, i) and possibly lonall
and onall, but these are at aspects (1) and ϵ which are smaller than the aspect
(1, i) of lton(i). Similar comments apply to rton(i) and the ﬂuents ron(i), ronall,
and onall.
5.1
Frame Axioms
Deﬁnition 12. If for ﬂuent p and action a and for some state s, Φ(p, s) ̸=
Φ(p, do(a, s)) then we say that action a inﬂuences ﬂuent p. If for all states s and
t, Φ(p, s) = Φ(p, do(a, s)) (if this is a theorem of T ) then a does not inﬂuence p.

416
D. A. Plaisted
Frame axioms are encoded in the aspect system by the following action locality
equation:
do(a, f α(s1 · · · sn)) = f α(s1 · · · do(a, si) · · · sn)
(3)
for all a, α such that aspect(a) ≥αi. Also, there is the ﬂuent locality equation:
Φ(p, f α(s1 · · · sn)) = Φ(p, si)
(4)
for all p, α such that aspect(p) ≥αi.
These equations imply that if one has p
:
α and a
:
β and α, β
are incomparable then a does not inﬂuence p. This is how frame axioms
are encoded in the aspect system. For Laspect
3
, do(lton(2), f (1)(s1, s2, s3)) =
f (1)(s1, do(lton(2), s2), s3) because turning on left switch 2 does not inﬂuence
left switches 1 or 3. Also, Φ(lon(2), f (1)(s1, s2, s3)) = Φ(lon(2), s2) because the
ﬂuent lon(2) only depends on the setting of switch 2.
Deﬁnition 13. Given T , an aspect tree Υ, and an assignment Π of aspects
to ﬂuents and actions, MΥ,Π (or just M) is the conjunction of Eq. 1 for
statelet equality, the bridging axioms, Deﬁnition 6, the aspect composition equa-
tion, Eq. 2, and the locality axioms, Eqs. 3 and 4.
Theorem 1. From M it follows that if one has p : α and a : β and α, β are
incomparable then Φ(p, do(a, s)) = Φ(p, s) for statelets s such that s : γ where γ
is the greatest lower bound of α and β, that is, γ is the largest aspect such that
γ < α and γ < β.
Proof. Since γ is the greatest lower bound of α and β, α > γi for some i and
β > γj for some j ̸= i. Suppose γ has n children. Then s = f γ(sγ1 · · · sγn) by
Eq. 2. Thus Φ(p, do(a, s)) = Φ(p, s) is equivalent to Φ(p, do(a, f γ(sγ1 · · · sγn))) =
Φ(p, f γ(sγ1 · · · sγn)). However, by Eq. 4, Φ(p, f γ(sγ1 · · · sγn))
=
Φ(p, sγi).
Also, by Eq. 3, do(a, f γ(sγ1 · · · sγn)) = f γ(sγ1, · · · , do(a, f γj), · · · , sγn). Thus
Φ(p, do(a, f γ(sγ1 · · · sγn))) = Φ(p, f γ(sγ1, · · · , do(a, f γj), · · · , sγn)) = Φ(p, sγi),
again by Eq. 4, so the equation Φ(p, do(a, s)) = Φ(p, s) holds with both sides
equal to Φ(p, sγi).
⊓⊔
Lemma 1. Suppose ξ is an aspect and for some i, aspect(p) ≥ξi and
aspect(a) ≥ξi. Suppose s : ξ and let s be f ξ(s1 · · · sn). Then from M it follows
that Φ(p, do(a, s)) = Φ(p, s) implies Φ(p, do(a, si)) = Φ(p, si), and the reverse
implication also holds.
Proof. Suppose Φ(p, do(a, s)) = Φ(p, s). Then by Eq. 3, do(a, s) = f α(s1 · · · ,
do(a, si) · · · sn), so by Eq. 4, Φ(p, do(a, s)) = Φ(p, do(a, si)), and by Eq. 4 again,
Φ(p, s) = Φ(p, si). Therefore Φ(p, do(a, si)) = Φ(p, do(a, s)) = Φ(p, s) = Φ(p, si)
so Φ(p, do(a, si)) = Φ(p, si). The reverse implication is shown in a similar way. ⊓⊔
Theorem 2. From M it follows that if one has p : α and a : β and α, β are
incomparable then Φ(p, do(a, s)) = Φ(p, s) for statelets s with s : ξ where ξ ≤α
and ξ ≤β. Also, it follows that Φ(p, do(a, s)) = Φ(p, s) for all states s.

The Aspect Calculus
417
Proof. The ﬁrst part follows by repeated application of Lemma 1. For the rest,
letting ξ be ϵ, Φ(p, do(a, s)) = Φ(p, s) for statelets s with s : ϵ and therefore by
the bridging axioms, Φ(p, do(a, s)) = Φ(p, s) for all states s.
⊓⊔
This result shows that in T aspect one can omit any frame axioms involving
ﬂuents and actions at incomparable aspects.
6
Encoding a Domain in the Aspect Formalism
A domain in the aspect calculus can be obtained in two ways: 1. By systematic
translation from an existing domain. 2. By custom design. We ﬁrst discuss the
ﬁrst possibility.
Deﬁnition 14. Suppose one has an underlying state theory T
with states,
actions, and ﬂuents and some axioms relating them. We want to encode T in
the aspect formalism to obtain T aspect that encodes as many of the frame axioms
of T as possible in a more eﬃcient manner, but does not imply frame axioms
that are not theorems of T . Suppose that an aspect tree Υ has been deﬁned and
aspects have been assigned for ﬂuents and actions. Let T ′ be some theory such
that T ′ ∪MΥ,Π is equivalent to T ∪MΥ,Π. Typically T ′ can be T with frame
axioms implied by MΥ,Π deleted. Then T aspect
Υ,Π
is T ′ ∪MΥ,Π for some such T ′.
Thus there is some ﬂexibility in deﬁning T aspect
Υ,Π
. For concreteness, here is a
more speciﬁc deﬁnition:
Deﬁnition 15. Suppose one has an underlying theory T that can be expressed
as d1 ∧d2 ∧· · · ∧dn. Let T ′ be e1 ∧e2 ∧· · · ∧em where {e1, e2, · · · , em} = {di :
1 ≤i ≤n, M ̸|= di}. Then T aspect
Υ,Π
is T ′ ∪M.
Here is an example of such an underlying theory, in this case in ﬁrst-order
logic:
7
Switches Example
Let T , that is, Ln, be the theory from Example 1. We construct the theory
Laspect
n
in the aspect representation.
7.1
Aspect Representation
The aspect tree Υ has a root node with two children, child 1 for the left bank and
child 2 for the right bank. Each child has in turn n children numbered 1 through
n, one for each switch. So the aspects are ϵ, (1), (2), (1, 1), (1, 2), · · · , (1, n),
(2, 1), (2, 2), · · · , (2, n). The actions lton(i) and ltof(i) have aspects (1, i), and
rton(i) and rtof(i) have aspects (2, i). Also, lon(i) has aspect (1, i) and ron(i)
has aspect (2, i). The ﬂuent lonall has aspect 1, ronall has aspect 2, and onall
has aspect ϵ.

418
D. A. Plaisted
This is T ′, that is, Laspect
n
; frame axioms are not needed and are omitted.
Also, free occurrences of s refer to universally quantiﬁed states as before.
lon(i, do(lton(i), s)) ∧ron(i, do(rton(i), s)), 1 ≤i ≤n(A1)c
¬lon(i, do(ltof(i), s)) ∧¬ron(i, do(rtof(i), s)), 1 ≤i ≤n(A2)c
lonall(s) ≡lon(1, s) ∧· · · ∧lon(n, s)(A3)c
l
ronall(s) ≡ron(1, s) ∧· · · ∧ron(n, s)(A3)c
r
(onall(s) ≡lonall(s) ∧ronall(s))(A4)c
(∃s)(s ∈S)(A9)c
Here is MΥ , consisting of the necessary portion (ﬂuents at leaf aspects) of the
bridging axioms, Deﬁnition 6, the locality axioms, Eqs. 3 and 4, and the aspect
composition equation, Eq. 2.
(lon(i, s) = lon(i, sϵ)) ∧(ron(i, s) = ron(i, sϵ)), 1 ≤i ≤n
do(lton(i), s)ϵ = do(lton(i), sϵ), 1 ≤i ≤n
do(rton(i), s)ϵ = do(rton(i), sϵ), 1 ≤i ≤n
do(ltof(i), s)ϵ = do(ltof(i), sϵ), 1 ≤i ≤n
do(rtof(i), s)ϵ = do(rtof(i), sϵ), 1 ≤i ≤n
∀t ∈ˆS(t : α →∃s ∈S(sα = t))
In the following lines, the ti refer to statelets.
do(lton(i), f ϵ(t1, t2)) = f ϵ(do(lton(i), t1), t2)
do(rton(i), f ϵ(t1, t2)) = f ϵ(t1, do(rton(i), t2))
do(ltof(i), f ϵ(t1, t2)) = f ϵ(do(ltof(i), t1), t2)
do(rtof(i), f ϵ(t1, t2)) = f ϵ(t1, do(rtof(i), t2))
do(lton(i), f 1(t1, . . . , tn)) = f 1(t1, . . . , do(lton(i), ti), . . . , tn)
do(rton(i), f 2(t1, . . . , tn)) = f 2(t1, . . . , do(rton(i), ti), . . . , tn)
do(ltof(i), f 1(t1, . . . , tn)) = f 1(t1, . . . , do(ltof(i), ti), . . . , tn)
do(rtof(i), f 2(t1, . . . , tn)) = f 2(t1, . . . , do(rtof(i), ti), . . . , tn)
lon(i, f ϵ(t1, t2)) = lon(i, t1)
ron(i, f ϵ(t1, t2)) = ron(i, t2)
lonall(f ϵ(t1, t2)) = lonall(t1)
ronall(f ϵ(t1, t2)) = ronall(t2)
lon(i, f 1(t1, . . . , tn)) = lon(i, ti), 1 ≤i ≤n
ron(i, f 2(t1, . . . , tn)) = ron(i, ti), 1 ≤i ≤n

The Aspect Calculus
419
In the following lines, t refers to a statelet and superscripts refer to the compo-
sition function of Deﬁnition 8.
t : ϵ →f ϵ(t1, t2) = t
t : 1 →f 1(t1,1, . . . , t1,n) = t
t : 2 →f 2(t2,1, . . . , t2,n) = t
Equation 1 for statelet equality also is included in M. The onall, lonall, and
ronall predicates are allowed in the aspect representation because they are deter-
mined by ﬂuents at their descendant leaves in the aspect tree according to the
leaf dependency constraint of Deﬁnition 9.
A close examination shows that most of Laspect
n
is of complexity (size) linear in
n. However, the number of axioms involving f α for various aspects α is bounded
by the depth of the aspect tree times the number of ﬂuents and actions. Assuming
the depth of the aspect tree is small compared to n, the complexity of Laspect
n
will
be small relative to the classical version. Many of the lines in Laspect
n
mention the
n variables ti and this gives another quadratic factor, but the constant factor is
at least smaller than the quadratic factor for the classical theory. However, even
this factor can be reduced; the idea is to make the aspect tree a binary tree.
This increases the number of equations while keeping the total number linear,
but each equation will have a constant size. Also, the depth of the aspect tree
will be at most logarithmic in n assuming the aspect tree is an approximately
balanced binary tree. Many of the axioms are equations, which tend to be easy
for ﬁrst-order provers to handle, so the new equations should not make planning
harder than for the classical representation.
7.2
Transmitting Switch Settings
Without going into detail, the switches example can be modiﬁed by also having
an action ltr(i) that transmits the state of left switch i to left switch i + 1,
1 ≤i < n and similarly rtr for right switches. Then ltr(i) could have aspect 1
but not (1, i) or (1, i+1) and rtr(i) could have aspect 2 but not (2, i) or (2, i+1),
even though the switches they modify have aspects that are children of 1 and 2,
respectively. The axiom for ltr(2) in T , for example, could be
lon(3, do(ltr(2), s)) ≡lon(2, s).
This example shows how sub-modules (incomparable aspects) are not completely
independent but can inﬂuence one another. Now, the aspect representation would
automatically give frame axioms implying that ltr does not modify right switches
and rtr does not modify left switches. However, it would not, for example, give
the frame axiom that ltr(2) does not modify left switch 1, because the aspects of
left switch 1 and the action are not incomparable. Such frame axioms would be
included in T ′. On this example, a custom translation can give a more succinct
representation of these frame axioms. In particular, one can axiomatize ltr and
rtr in T as follows, where 1 ≤i < n, the statelets sj with sj : (1, j) are

420
D. A. Plaisted
universally quantiﬁed in the ﬁrst two equations, and sj : (2, j) in the second two
equations. Also, ltr′(i, si) returns a statelet at aspect (1, i + 1) and rtr′(i, si)
returns a statelet at aspect (2, i + 1).
lon(i + 1, ltr′(i, si)) = lon(i, si)
do(ltr(i), f 1(s1, · · · , sn)) = f 1(s1, · · · , si, ltr′(i, si), si+2, · · · , sn)
ron(i + 1, rtr′(i, si)) = ron(i, si)
do(rtr(i), f 2(s1, · · · , sn)) = f 2(s1, · · · , si, rtr′(i, si), si+2, · · · , sn)
The binary tree idea can further reduce the complexity, as before.
This example and many similar examples involving transmitting information
between disjoint aspects can be handled in a more systematic way by allowing
some actions to have a set of aspects instead of just a single aspect. The ltr(i)
action would have aspects (1, i) and (1, i+1). Without a fully rigorous treatment,
the idea is to modify Eq. 3 as follows for actions a with more than one aspect
and more than one i such that αi is a preﬁx of at least one of the aspects of
action a:
do(a, f α(s1, . . . , sn)) = f α(s′
1, . . . , s′
n)
where s′
i = si if the aspect αi is disjoint from all the aspects of action a. Oth-
erwise, s′
i is a statelet deﬁned by axioms such as the ﬁrst and third equations
above. However, if there is only one i such that αi is a preﬁx of at least one of
the aspects of a, then the original form of Eq. 3 can be used. Thus for example
do(ltr(i), f ϵ(s1, s2)) = f ϵ(do(ltr(i), s1), s2).
8
The Unconstraining Property
Deﬁnition 16. An assignment of aspects to actions and ﬂuents in a state theory
T that satisﬁes the action dependency condition, Deﬁnition 1, is unconstraining
if it satisﬁes the locality constraints on actions of Deﬁnition 11, the leaf depen-
dency constraint on ﬂuents of Deﬁnition 9, and the combining axiom of Deﬁni-
tion 10; that is, these must be theorems of T .
Theorem 3. For any state theory T satisfying the action dependency condition
and any aspect tree Υ, it is possible to ﬁnd an assignment of aspects to ﬂuents
and actions that is unconstraining.
Proof. The leaf dependency constraint on ﬂuents can be satisﬁed by putting all
ﬂuents at the same leaf of Υ if necessary and the locality constraints on actions
can be satisﬁed by assigning all actions the aspect of ϵ at the root of the tree.
However, such an assignment of aspects would not encode any frame axioms, so
it would not serve any purpose.
⊓⊔

The Aspect Calculus
421
8.1
Relative Consistency
We now show that if MΥ. Π is unconstraining and T satisﬁes the action depen-
dency condition then T aspect is relatively consistent with T . This implies that
T aspect does not imply any new theorems on the assertions over the symbols
in T .
Theorem 4. Suppose T is a theory of states, actions, and ﬂuents that satisﬁes
the action dependency condition, Deﬁnition 1. Suppose that an aspect tree Υ is
chosen and aspects are assigned to ﬂuents and actions of T in an unconstraining
manner (Deﬁnition 16). Then T aspect is relatively consistent with T .
Proof. We show that any model M of T can be extended to a model M aspect
of T aspect. M aspect interprets the symbols of T on the domains of T the same
way that M does. M aspect has additional domains, the set of statelets and the
set of aspects, and an additional element ⊥that can be the value of ﬂuents and
of do(a, s) in M aspect. Also, M aspect has the functions f α for aspects α in Υ
mapping from tuples of statelets to statelets, and the function mapping states
and statelets s to statelet sα, for aspects α.
For every state s of M, there is a statelet sϵ of M aspect such that for all
ﬂuents p ∈F, Φ(p, sϵ) = Φ(p, s). Two statelets that have the same aspect and
the same value on all ﬂuents in F are equal in M aspect; other statelets are not
equal in M aspect.
The functions sα from states or statelets s to statelets are deﬁned as in
Deﬁnition 7. The functions f α are deﬁned by f α(sα1 · · · sαn) = sα where the
aspect α has n children. In M aspect, the set ˆS of statelets is {sα : s ∈S, α ∈Ψ}.
Fluents of M are extended from states of M to statelets in M aspect. The
value ⊥is allowed as a value of Φ(p, s) for ﬂuents p and statelets s, where ⊥is
not equal to any state or statelet and Φ(p, s) ̸= ⊥for ﬂuents p and states s. A
ﬂuent p of M that is assigned an aspect of α in M aspect is deﬁned on all statelets
sβ for β ≤α and Φ(p, sβ) = Φ(p, sϵ) for all such β. If a statelet s has aspect γ
and γ ̸≤α then Φ(p, sγ) = ⊥.
Actions a of M are extended from states to statelets in M aspect. Actions a of
M with aspect α satisfy do(a, sβ) = ⊥for statelets sβ with β ̸≤α. Actions a of M
with aspect α are deﬁned on all statelets sβ for β ≤α. The value of do(a, sβ) in
this case is a statelet t with t : β such that t ≡α do(a, sϵ). This completely deﬁnes
t because ﬂuents q with aspects γ with α#γ satisfy Φ(q, t) = ⊥in M aspect, and
ﬂuents with aspects γ with γ ≤α are speciﬁed by the leaf dependency constraint
on ﬂuents. This completely deﬁnes M aspect.
It remains to show that M aspect is a model of T aspect. Now, M aspect is a
model of T because it agrees with M there. So it remains to show that M aspect
is a model of MΥ . Recall from Deﬁnition 13 that MΥ is the conjunction of Eq. 1
for statelet equality, the bridging axioms, Deﬁnition 6, the aspect composition
equation, Eq. 2, and the action and ﬂuent locality axioms, Eqs. 3 and 4.
The issue is that one can have sα = tα even for unequal states s and t, so
one has to show that all the functions and properties depend only on the ﬂuents
of sα and not directly on s.

422
D. A. Plaisted
The proof is routine, so the details are omitted.
⊓⊔
9
Solving Planning Problems Bottom up
For a binary relation R, R(x, y) indicates that (x, y) ∈R. If A is a logical
assertion then {x : A} is the set of x having property A. If α is an aspect then
x : α indicates that x has aspect α. Let M be a model of T aspect and let the
relations Rα, Rα
1 , and Rα
2 be deﬁned as follows, where a superscript of ∗indicates
transitive closure and Iα is the identity relation on statelets at aspect α:
Rα = (Rα
1 ∪Rα
2 )∗∪Iα
(5)
Rα
1 = {(f α(s1 · · · sn), f α(t1 · · · tn)) : Rαi(si, ti), 1 ≤i ≤n}
(6)
Rα
2 = {(s, t) : M |= do(a, s) = t, a ∈A, s : α, aspect(a) ≥α}
(7)
Rα gives the set of pairs (s, t) of statelets at aspect α such that t is reachable
in M from s by a ﬁnite sequence of actions at aspect α or larger aspects. Com-
puting Rα can be helpful for solving planning problems by exhaustive search,
and it avoids repetitive search due to actions on independent (incomparable)
aspects commuting. Of course, if the number of states is ﬁnite, Rα will always
be ﬁnite. This diﬀers from Reiter’s formalism [Rei91], in which the number of
situations can be inﬁnite even if the number of states is ﬁnite because situations
are deﬁned by sequences of states.
Theorem 5. With Rα deﬁned as in Eqs. 5, 6, and 7, Rα(s, t) for statelets s and
t with s : α and t : α iﬀthere is a sequence s1 : α, s2 : α, . . . , sn : α of statelets
where s = s1, t = sn, and for all i, 1 ≤i < n, there is an action ai with ai : βi
such that βi ≥α and M |= si+1 = do(ai, si).
The proof is omitted for lack of space. Of course, this implies that Rϵ(sϵ, tϵ)
for states s and t iﬀt can be obtained in M from s by a ﬁnite sequence of actions.
10
Conclusion
The aspect calculus expresses frame axioms involving incomparable aspects of
a state eﬃciently for modular theories. Aspects are sequences of integers that
correspond to substates of a state; for example, in the sequence (i, j, k), i may
indicate the earth, j a country, and k a state in a country. These sequences are
ordered so that sequences are larger than (greater than) their proper preﬁxes.
Fluents are assigned aspects based on which portion of the state they describe,
so a ﬂuent may have an aspect corresponding to North Carolina if it describes
something about North Carolina. Then actions can be assigned aspects that are
the greatest lower bound (longest common preﬁx) of the aspects of all ﬂuents
that they inﬂuence or depend on. This implies for example that actions in North
Carolina do not inﬂuence ﬂuents from outside North Carolina, thereby encoding
many frame axioms.

The Aspect Calculus
423
This formalism is entirely in ﬁrst-order logic, and powerful ﬁrst-order theo-
rem provers can be applied to it if the underlying theory T is ﬁrst-order. When
converted to clause form, the resulting clauses appear to be easier for ﬁrst-order
theorem provers to handle than clauses from Reiter’s formalism. Also, for some
theories, clauses from Reiter’s formalism can become very long. Two examples
are given and relative consistency is shown assuming that the unconstraining
property holds. This formalism also permits an exhaustive method of solving
planning problems that has some advantages for modular domains. The rami-
ﬁcation problem does not require any special methods in the aspect calculus.
However, this formalism does not handle knowledge and belief, but is only con-
cerned with logical correctness. It is also only suitable for modular theories.
References
[Dav90] Davis, E.: Representations of Commonsense Knowledge. Morgan Kaufmann,
Burlington (1990)
[DT07] Denecker, M., Ternovska, E.: Inductive situation calculus. Artif. Intell. 171(5–
6), 332–360 (2007)
[Haa87] Haas, A.R.: The case for domain-specic frame axioms. In: Brown, F.M. (ed.)
The Frame Problem in Artiﬁcial Intelligence, Proceedings of the 1987 Work-
shop, pp. 343–348. Morgan Kaufmann (1987)
[Hay73] Hayes, P.: The frame problem and related problems in artiﬁcial intelligence.
In: Elithorn, A., Jones, D. (eds.) Artiﬁcial and Human Thinking, pp. 45–
59. Jossey-Bass Inc., Elsevier Scientiﬁc Publishing Company, San Francisco,
Amsterdam (1973)
[Lin08] Lin, F.: Situation calculus. In: van Harmelen, F., Lifschitz, V., Porter, B. (eds.)
Handbook of Knowledge Representation, pp. 649–669. Elsevier, Amsterdam
(2008)
[LR94] Lin, F., Reiter, R.: State constraints revisited. J. Logic Comput. 4(5), 655–678
(1994)
[McI00] McIlraith, S.A.: Integrating actions and state constraints: a closed-form solu-
tion to the ramiﬁcation problem (sometimes). Artif. Intell. 116(1), 87–121
(2000)
[MH69] McCarthy, J., Hayes, P.: Some philosophical problems from the standpoint of
artiﬁcial intelligence. In: Meltzer, B., Michie, D. (eds.) Machine Intelligence
4, pp. 463–502. Edinburgh University Press, Edinburgh (1969)
[MM97] Matos, P.A., Martins, J.P.: Contextual logic of change and the ramiﬁcation
problem. In: Coasta, E., Cardoso, A. (eds.) EPIA 1997. LNCS, vol. 1323, pp.
267–278. Springer, Heidelberg (1997). https://doi.org/10.1007/BFb0023928
[Ped89] Pednault, E.P.D.: ADL: exploring the middle ground between STRIPS and the
situation calculus. In: Proceedings of the International Conference on Princi-
ples of Knowledge Representation (KR-1998), pp. 324–332. Morgan Kaufmann
Inc. (1989)
[Pet08] Petrick, R.P.A.: Cartesian situations and knowledge decomposition in the sit-
uation calculus. In: Principles of Knowledge Representation and Reasoning:
Proceedings of the Eleventh International Conference, KR 2008, Sydney, Aus-
tralia, 16–19 September 2008, pp. 629–639 (2008)

424
D. A. Plaisted
[Rei91] Reiter, R.: The frame problem in the situation calculus: a simple solution
(sometimes) and a completeness result for goal regression. In: Lifschitz, V.
(ed.) Artiﬁcial Intelligence and Mathematical Theory of Computation: Papers
in Honor of John McCarthy, pp. 359–380. Academic Press, Cambridge (1991)
[Sch90] Schubert, L.: Monotonic solution of the frame problem in the situation cal-
culus: an eﬃcient method for worlds with fully speciﬁed actions. In: Kyburg,
H.E., Loui, R.P., Carlson, G.N. (eds.) Knowledge Representation and Defea-
sible Reasoning, vol. 5, pp. 23–67. Kluwer Academic Publishers, Dordrecht
(1990)
[Sha99] Shanahan, M.: The ramiﬁcation problem in the event calculus. In: Proceedings
of the 16th International Joint Conference on Artiﬁcal Intelligence - Volume
1, IJCAI 1999, pp. 140–146. Morgan Kaufmann Publishers Inc., San Francisco
(1999)
[SL93] Scherl, R.B., Levesque, H.J.: The frame problem and knowledge-producing
actions. In: Proceedings of the Eleventh National Conference on Artiﬁ-
cial Intelligence (AAAI-1993), Washington, D.C., USA, pp. 689–697. AAAI
Press/MIT Press (1993)
[Ter00] Ternovska, E.: Id-logic and the ramiﬁcation problem for the situation calculus.
In: ECAI (2000)
[Thi98] Thielscher, M.: Introduction to the ﬂuent calculus. Electron. Trans. Artif.
Intell. 2, 179–192 (1998)

Uniform Substitution at One Fell Swoop
Andr´e Platzer1,2(B)
1 Computer Science Department, Carnegie Mellon University, Pittsburgh, USA
aplatzer@cs.cmu.edu
2 Fakult¨at f¨ur Informatik, Technische Universit¨at M¨unchen, Munich, Germany
Abstract. Uniform substitution of function, predicate, program or
game symbols is the core operation in parsimonious provers for hybrid
systems and hybrid games. By postponing soundness-critical admissibil-
ity checks, this paper introduces a uniform substitution mechanism that
proceeds in a linear pass homomorphically along the formula. Soundness
is recovered using a simple variable condition at the replacements per-
formed by the substitution. The setting in this paper is that of diﬀerential
hybrid games, in which discrete, continuous, and adversarial dynamics
interact in diﬀerential game logic dGL. This paper proves soundness and
completeness of one-pass uniform substitutions for dGL.
1
Introduction
After a number of false starts on substitution [11,12,22], even by prominent
logicians, did Church’s uniform substitution [5] [§35,40] provide a mechanism for
substituting function and predicate symbols with terms and formulas in ﬁrst-
order logic. Given a mechanism for applying a uniform substitution σ to formulas
φ with result denoted σφ uniform substitutions are used with Church’s proof
rule:
(US)
φ
σφ
Contrary to casual belief, quite some care is needed in the substitution process,
even of only function symbols [23], in order to prevent replacing functions with
terms that denote incompatible values in diﬀerent places depending on which
variables are being used in the replacements and in which formula contexts. Due
to their subtleties, there have even been passionate calls for banishing substitu-
tions [10] and using more schemata. This paper moves in the opposite direction,
making substitutions even more subtle, but also faster and, nevertheless, sound.
In Shakespeare’s Macbeth, “at one fell swoop” was likened to the suddenness with which
a bird of prey ﬁercely attacks a whole nest at once. The idiom has since retained only
its meaning of suddenly doing all at once, although the connotation of ﬁerceness is also
beﬁtting of the ignorance with which one-pass uniform substitution trespasses operator
scopes. This research is supported by the Alexander von Humboldt Foundation and by
the AFOSR under grant number FA9550-16-1-0288.
c
⃝The Author(s) 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 425–441, 2019.
https://doi.org/10.1007/978-3-030-29436-6_25

426
A. Platzer
The biggest theoretical advantage of uniform substitutions is that they make
instantiation explicit, so that proof calculi can use axioms (concrete object-
level formulas) instead of axiom schemata (meta-level concepts standing for
inﬁnitely many formulas). Their biggest practical advantage is that this avoid-
ance of schemata enables parsimonious theorem prover implementations that
only consist of copies of concrete formulas as axioms together with one algorithm
implementing the application of uniform substitutions (plus renaming). Similar
advantages exist for concrete axiomatic proof rules instead of rule schemata [16].
This design obviates the need for algorithms that recognize all of the inﬁnitely
many instances of schemata and check all of their (sometimes pretty subtle) side
conditions to soundly reject improper reasoning. These practical advantages have
ﬁrst been demonstrated for hybrid systems [8] and for hybrid games [18] prov-
ing, where uniform substitution led to signiﬁcant reductions in soundness-critical
size (down from 66000 to 1700 lines of code) or implementation time (down from
months to minutes) compared to conventional prover implementations.
These uses of the uniform substitution principle required generalizations from
ﬁrst-order logic [5] to diﬀerential dynamic logic dL for hybrid systems [16] and
diﬀerential game logic dGL for hybrid games [18], including substitutions of pro-
grams or games, respectively. The presence of variables whose values change
imperatively over time, and of diﬀerential equations x′ = θ that cause intrinsic
links of variables x and their time-derivatives x′, signiﬁcantly complicate aﬀairs
compared to the simplicity of ﬁrst-order logic [5,23] and λ-calculus [4]. Pure
λ-calculus has a single binder and rests on the three pillars of α-conversions
(for bound variables), β-reductions (by capture-avoiding substitutions), and η-
conversions (versus free variables), which provide an elegant, deep, but solid
foundation for functional programs (with similar observations for ﬁrst-order
logic). Despite signiﬁcant additional challenges,1 just two elementary operations,
nevertheless, suﬃce as a foundation for imperative programs and even hybrid
games: bound renaming and uniform substitution (based on suitably generalized
notions of free and bound variables). Uniform substitutions generalize elegantly
and in highly modular ways [16,18]. Much of the conceptual simplicity in the
correctness arguments in these cases, however, came from the fact that Church-
style uniform substitutions are applied by checking at each operator admissibil-
ity, i.e., that no free variable be introduced into a context in which it is bound.
Such checks simplify correctness proofs, because they check each admissibility
condition at every operator where they are necessary for soundness. The result-
ing substitution mechanism is elegant but computationally suboptimal, because
it repeatedly checks admissibility recursively again and again at every operator.
For example, applying a uniform substitution σ checks at every sequential com-
position α; β again that the entire substitution σ is admissible for the remainder
β compared to the bound variables of the result of having applied σ to α:
1 The area of eﬀect that an assignment to a variable has is non-computable and even
a single occurrence of a variable may have to be both free and bound to ensure
correctness. Such overlap is an inherent consequence of change, which is an intrinsic
feature of dynamical systems theory (the mathematics of change) and game theory
(the mathematics of eﬀects resulting from strategic interaction by player decisions).

Uniform Substitution at One Fell Swoop
427
σ(α; β) = (σ(α); σ(β))
if σ is BV(σ(α))-admissible for β
(1)
where σ is U-admissible for β iﬀthe free variables of the replacements for the part
of σ having function/predicate symbols that occur in β do not intersect U, which,
here, are the bound variables BV(σ(α)) computed from the result of applying the
substitution σ to α [18]. This mechanism is sound [16,18], even veriﬁed sound
for hybrid systems in Isabelle/HOL and Coq [2], but computationally redundant
due to its repeated substitution application and admissibility computations.
The point of this paper is to introduce a more liberal form of uniform substi-
tution that substitutes at one fell swoop, forgoing admissibility checks during the
operators where they would be needed with a monadic computation of taboo sets
to make up for that negligence by checking cumulative admissibility conditions
locally only once at each replacement that the uniform substitution applica-
tion performs. This one-pass uniform substitution is computationally attractive,
because it operates linearly in the output, which matters because uniform substi-
tution is the dominant logical inference in uniform substitution provers [8]. The
biggest challenge is, precisely, that correctness of substitution can no longer be
justiﬁed for all operators where it is needed (because admissibility is no longer
recursively checked at every operator). The most important technical insight of
this paper is that modularity of correctness arguments can be recovered, regard-
less, using a neighborhood semantics for taboos. Another value of this paper is
its straightforward completeness proof based on [15,16]. Overall, the ﬁndings of
this paper make it possible to verify hybrid games (and systems) with faster
small soundness-critical prover cores than before [18,21], which, owing to their
challenges, are the only two veriﬁcation tools for hybrid games. Uniform substi-
tutions extend to diﬀerential games [6,7], where soundness is challenging [13],
leading to the ﬁrst basis for a small prover core for diﬀerential hybrid games
[17]. The accelerated proving primitives are of interest for other dynamic logics
[1,9]. All proofs are in [20] and those till Theorem 19 were then formalized [19].
2
Preliminaries: Diﬀerential Game Logic
This section recalls the basics of diﬀerential game logic [15,18], the logic for
specifying and verifying hybrid games of two players with diﬀerential equations.
2.1
Syntax
The set of all variables is V, including for each variable x a diﬀerential variable x′
(e.g., for an ODE for x). Higher-order diﬀerential variables x′′ etc. are not used
in this paper, so a ﬁnite set V suﬃces. The terms θ of (diﬀerential-form) dGL are
polynomial terms with real-valued function symbols and diﬀerential terms (θ)′
that are used to reduce reasoning about diﬀerential equations to reasoning about
equations of diﬀerentials [16]. Hybrid games α describe the permitted discrete
and continuous actions by player Angel and player Demon. Besides the operators
of ﬁrst-order logic of real arithmetic, dGL formulas φ can be built using ⟨α⟩φ,

428
A. Platzer
which expresses that Angel has a winning strategy in the hybrid game α to reach
the region satisfying dGL formula φ. Likewise, [α]φ expresses that Demon has a
winning strategy in the hybrid game α to reach the region satisfying φ.
Deﬁnition 1 (Terms). Terms are deﬁned by the following grammar (with θ,
η, θ1, . . . , θk as terms, x ∈V as variable, and f as function symbol of arity k):
θ, η ::= x | f(θ1, . . . , θk) | θ + η | θ · η | (θ)′
Deﬁnition 2 (dGL formulas).
The formulas of diﬀerential game logic dGL
are deﬁned by the following grammar (with φ, ψ as dGL formulas, p as predicate
symbol of arity k, θ, η, θi as terms, x as variable, and α as hybrid game):
φ, ψ ::= θ ≥η | p(θ1, . . . , θk) | ¬φ | φ ∧ψ | ∃x φ | ⟨α⟩φ
The usual operators can be derived, e.g., ∀x φ is ¬∃x ¬φ and similarly for
→, ↔and truth ⊤. Existence of Demon’s winning strategy in hybrid game α to
achieve φ is expressed by the dGL formula [α]φ, which can be expressed indirectly
as ¬⟨α⟩¬φ, thanks to the hybrid game determinacy theorem [15, Thm. 3.1].
Deﬁnition 3 (Hybrid games). The hybrid games of diﬀerential game logic
dGL are deﬁned by the following grammar (with α, β as hybrid games, a as game
symbol, x as variable, θ as term, and ψ as dGL formula):
α, β ::= a | x := θ | x′ = θ & ψ | ?ψ | α ∪β | α; β | α∗| αd
The operator precedences make all unary operators, including modalities
and quantiﬁers, bind stronger. Just like the meaning of function and predicate
symbols is subject to interpretation, the eﬀect of game symbol a is up to inter-
pretation. In contrast, the assignment game x := θ has the speciﬁc eﬀect of
changing the value of variable x to that of term θ. The diﬀerential equation
game x′ = θ & ψ allows Angel to choose how long she wants to follow the (vec-
torial) diﬀerential equation x′ = θ for any real duration within the set of states
where evolution domain constraint ψ is true. Diﬀerential equation games with
trivial ψ = ⊤are just written x′ = θ. The test game ?ψ challenges Angel to
satisfy formula ψ, for if ψ is not true in the present state she loses the game
prematurely. The choice game α ∪β allows Angel to choose if she wants to play
game α or game β. The sequential game α; β will play game β after game α
terminates (unless a player prematurely lost the game while playing α). The
repetition game α∗allows Angel to decide, after having played any number of
α repetitions, whether she wants to play another round (but she cannot play
forever). Finally, the dual game αd will have both players switch sides: every
choice that Angel had in α will go to Demon in αd, and vice versa, while every
condition that Angel needs to meet in α will be Demon’s responsibility in αd,
and vice versa.
Substitutions are fundamental but subtle. For example, a substitution σ that
has the eﬀect of replacing f(x) with x2 and a(x) with zy is unsound for the
following formula while a substitution that replaces a(x) with zx2 would be ﬁne:

Uniform Substitution at One Fell Swoop
429
clash ⟨x′ = f(x), y′ = a(x)y⟩x ≥1 ↔⟨x′ = f(x)⟩x ≥1
⟨x′ = x2, y′ = zyy⟩x ≥1 ↔⟨x′ = x2⟩x ≥1
The introduction of a new variable z by the substitution σ is acceptable, but,
even if y was already present previously, its introduction by σ makes the inference
unsound (e.g., when x = y = 1/z = 1/2), because this equates a system with
a solution that is exponential in y with a hyperbolic solution of more limited
duration, even if both solutions are already hyperbolic of limited time from x. By
contrast, the use of the previously present variable x to form x′ = x2 is ﬁne. The
diﬀerence is that, unlike z, variable y has a diﬀerential equation that changes
the value of y and, while x also does, f(x) and a(x) may explicitly depend on x.
It is crucial to distinguish correct and incorrect substitutions in all cases.
2.2
Semantics
A state ω is a mapping from the set of all variables V to the reals R. The state
ωr
x agrees with state ω except for variable x whose value is r ∈R in ωr
x. The set
of all states is denoted S and the set of all its subsets is denoted ℘(S).
The semantics of function, predicate, and game symbols is independent from
the state. They are interpreted by an interpretation I that maps each arity k
function symbol f to a k-ary smooth function I(f) : Rk →R, each arity k
predicate symbol p to a k-ary relation I(p) ⊆Rk, and each game symbol a to
a monotone I(a) : ℘(S) →℘(S) where I(a)(X) ⊆S are the states from which
Angel has a winning strategy to achieve X ⊆S in game a. Diﬀerentials (θ)′ have
a diﬀerential-form semantics [16]: the sum of partial derivatives by all variables
x ∈V multiplied by the values of their associated diﬀerential variable x′.
Deﬁnition 4 (Semantics of terms).
The semantics of a term θ in inter-
pretation I and state ω ∈S is its value Iω[[θ]] in R. It is deﬁned inductively
as
1. Iω[[x]] = ω(x) for variable x ∈V
2. Iω[[f(θ1, . . . , θk)]] = I(f)

Iω[[θ1]], . . . , Iω[[θk]]

for function symbol f
3. Iω[[θ + η]] = Iω[[θ]] + Iω[[η]]
4. Iω[[θ · η]] = Iω[[θ]] · Iω[[η]]
5. Iω[[(θ)′]] = 
x∈V ω(x′) ∂Iω[[θ]]
∂x
for the diﬀerential (θ)′ of θ
The semantics of diﬀerential game logic in interpretation I deﬁnes, for each
formula φ, the set of all states I[[φ]], in which φ is true. Since hybrid games
appear in dGL formulas and vice versa, the semantics I[[α]]

X

of hybrid game
α in interpretation I is deﬁned by simultaneous induction as the set of all states
from which Angel has a winning strategy in hybrid game α to achieve X ⊆S.
Deﬁnition 5 (dGL semantics).
The semantics of a dGL formula φ for each
interpretation I with a corresponding set of states S is the subset I[[φ]] ⊆S of
states in which φ is true. It is deﬁned inductively as follows

430
A. Platzer
1. I[[θ ≥η]] = {ω ∈S : Iω[[θ]] ≥Iω[[η]]}
2. I[[p(θ1, . . . , θk)]] = {ω ∈S : (Iω[[θ1]], . . . , Iω[[θk]]) ∈I(p)}
3. I[[¬φ]] = (I[[φ]])∁= S \ I[[φ]] is the complement of I[[φ]]
4. I[[φ ∧ψ]] = I[[φ]] ∩I[[ψ]]
5. I[[∃x φ]] = {ω ∈S : ωr
x ∈I[[φ]] for some r ∈R}
6. I[[⟨α⟩φ]] = I[[α]]

I[[φ]]

A dGL formula φ is valid in I, written I |= φ, iﬀit is true in all states, i.e.,
I[[φ]] = S. Formula φ is valid, written ⊨φ, iﬀI |= φ for all interpretations I.
Deﬁnition 6 (Semantics of hybrid games).
The semantics of a hybrid
game α for each interpretation I is a function I[[α]]

·

that, for each set of states
X ⊆S as Angel’s winning condition, gives the winning region, i.e., the set of
states I[[α]]

X

⊆S from which Angel has a winning strategy to achieve X in α
(whatever strategy Demon chooses). It is deﬁned inductively as follows
1. I[[a]]

X

= I(a)(X)
2. I[[x := θ]]

X

= {ω ∈S : ωIω[[θ]]
x
∈X}
3. I[[x′ = θ & ψ]]

X

= {ω ∈S : ω = ϕ(0) on {x′}∁and ϕ(r) ∈X for some
function ϕ : [0, r] →S of some duration r ∈R satisfying I, ϕ |= x′ = θ ∧ψ}
where I, ϕ |= x′ = θ ∧ψ iﬀϕ(ζ) ∈I[[x′ = θ ∧ψ]] and ϕ(0) = ϕ(ζ) on {x, x′}∁
for all 0≤ζ≤r and dϕ(t)(x)
dt
(ζ) exists and equals ϕ(ζ)(x′) for all 0≤ζ≤r if r>0.
4. I[[?ψ]]

X

= I[[ψ]] ∩X
5. I[[α ∪β]]

X

= I[[α]]

X

∪I[[β]]

X

6. I[[α; β]]

X

= I[[α]]

I[[β]]

X

7. I[[α∗]]

X

= {Z ⊆S : X ∪I[[α]]

Z

⊆Z} which is a least ﬁxpoint [15]
8. I[[αd]]

X

= (I[[α]]

X∁
)∁
Along x′ = θ & ψ, variables x and x′ enjoy an intrinsic link since they co-evolve.
2.3
Static Semantics
Sound uniform substitutions check free and bound occurrences of variables to
prevent unsound replacements of expressions that might have incorrect values
in the respective replacement contexts. The whole point of this paper is to skip
admissibility checks such as that in (1). Free (and, indirectly, bound) variables
will still have to be consulted to tell apart acceptable from unsound occurrences.
Hybrid games even make it challenging to characterize free and bound vari-
ables. Both are deﬁnable based on whether or not their values aﬀect the existence
of winning strategies under variations of the winning conditions [18]. The upward
projection X↑V increases the winning condition X ⊆S from variables V ⊆V to
all states that are “on V like X”, i.e., similar on V to states in X. The downward
projection X↓ω(V ) shrinks the winning condition X, ﬁxing the values of state ω
on variables V ⊆V to keep just those states of X that agree with ω on V .

Uniform Substitution at One Fell Swoop
431
Deﬁnition 7. The set X↑V = {ν ∈S : ∃ω ∈X ω = ν on V } ⊇X extends
X ⊆S to the states that agree on V ⊆V with some state in X (written ∃). The
set X↓ω(V ) = {ν ∈X : ω = ν on V } ⊆X selects state ω on V ⊆V in X ⊆S.
Projections make it possible to (semantically!) deﬁne free and bound vari-
ables of hybrid games by expressing variable dependence and ignorance. Such
semantic characterizations increase modularity and are used for the correctness
of syntactic analyzes that compute supersets [16, Sect. 2.4]. Variable x is free in
hybrid game α iﬀtwo states that only diﬀer in the value of x diﬀer in member-
ship in the winning region of α for some winning condition X↑{x}∁that does
not distinguish values of x. Variable x is bound in hybrid game α iﬀit is in
the winning region of α for some winning condition X but not for the winning
condition X↓ω({x}) that limits the new value of x to stay at its initial value ω(x).
Deﬁnition 8 (Static semantics). The static semantics deﬁnes the free vari-
ables, which are all variables that the value of an expression depends on, as well
as bound variables, BV(α), which can change their value during game α, as:
FV(θ) =

x ∈V : ∃I, ω, ˜ω such that ω = ˜ω on {x}∁and Iω[[θ]] ̸= I ˜ω[[θ]]

FV(φ) =

x ∈V : ∃I, ω, ˜ω such that ω = ˜ω on {x}∁and ω ∈I[[φ]] ̸∋˜ω

FV(α) =

x ∈V : ∃I, ω, ˜ω, X with ω = ˜ω on {x}∁and ω ∈I[[α]] X↑{x}∁
̸∋˜ω

BV(α) =

x ∈V : ∃I, ω, X such that I[[α]] X

∋ω ̸∈I[[α]] X↓ω({x})

Beyond assignments, note complications with ODEs such as (2), where, due
to their nature as the solution of a ﬁxpoint condition, the same occurrences
of variables are free, because they depend on their initial values, but they are
also bound, because their values change along the ODE. All occurrences of x
and y but not z on the right-hand side of x′ = x2, y′ = zx2y and occurrences of
x, y, x′, y′ also after this ODE are bound, since they are aﬀected by this change.
Variables x, y, z but not x′, y′ are free in this ODE. The crucial need for overlap
of free and bound variables is most obvious for ODEs, but also arises for loops,
e.g., (x := x + 1; x′ = −x)∗. If x were not classiﬁed as free, its initial value could
be overwritten incorrectly. If x were not classiﬁed as bound, its initial value
could be incorrectly copy-propagated across the loop. This also applies to the
same occurrence of x in x + 1 and −x, respectively. If it were not classiﬁed as
a bound but a free occurrence, it could be incorrectly replaced by a term of the
same initial value. If it were not classiﬁed as a free but a bound occurrence, it
could, e.g., be boundly renamed, incorrectly losing its initial link. 2
Coincidence lemmas [18] show truth-values of dGL formulas only depend on
their free variables (likewise for terms and hybrid games). The bound eﬀect
lemma [18] shows only bound variables change their value when playing games.
2 These intricate variable relationships in games and the intrinsic link of x and x′ from
ODEs signiﬁcantly complicate substitutions beyond what is supported for ﬁrst-order
logic [5,23], λ-calculi [4], de Bruijn indices [3], or higher-order abstract syntax [14].

432
A. Platzer
X
X↑V
I[[α]] X

ω
˜ω
on V ⊇FV(α)
α
α
X
X↓ω
I[[α]] X↓ω(BV(α)∁)

ω
α
α
Fig. 1. Illustration of coincidence and bound eﬀect properties of hybrid games
Supersets satisfy the same lemmas, so corresponding syntactic free and bound
variable computations can be used correctly and are deﬁned accordingly [16,18].
Since FV() and BV() are the smallest such sets, no smaller sets can be correct,
including, e.g., the usual deﬁnitions that classify occurrences mutually exclu-
sively.
Lemma 9 (Coincidence for terms [18]). FV(θ) is the smallest set with the
coincidence property for θ: If ω = ˜ω on FV(θ), then Iω[[θ]] = I ˜ω[[θ]].
Lemma 10 (Coincidence for formulas [18]). FV(φ) is the smallest set with
the coincidence property for φ: If ω = ˜ω on FV(φ), then ω ∈I[[φ]] iﬀ˜ω ∈I[[φ]].
Lemma 11 (Coincidence for games [18]). FV(α) is the smallest set with the
coincidence property for α: If ω = ˜ω on V ⊇FV(α), then ω ∈I[[α]]

X↑V

iﬀ
˜ω ∈I[[α]]

X↑V

; see Fig. 1(left).
Lemma 12 (Bound eﬀect [18]). BV(α) is the smallest set with the bound
eﬀect property for α: ω ∈I[[α]]

X

iﬀω ∈I[[α]]

X↓ω(BV(α)∁)

; see Fig. 1(right).
The correctness of one-pass uniform substitution will become more transpar-
ent after deﬁning when one state is a variation of another on a set of variables.
For a set U ⊆V, state ˜ω is called a U-variation of state ω iﬀ˜ω = ω on comple-
ment U ∁. Variations satisfy properties of monotonicity and transitivity. If ˜ω is a
U-variation of ω, then ˜ω is a V -variation of ω for all V ⊇U. If ˜ω is a U-variation
of ω and ω is a V -variation of μ, then ˜ω is a (U ∪V )-variation of μ. Coincidence
lemmas say that the semantics is insensitive to variations of nonfree variables.
If ˜ω is a U-variation of ω and FV(φ) ∩U = ∅, then ω ∈I[[φ]] iﬀ˜ω ∈I[[φ]].
3
Uniform Substitution
Uniform substitutions for dGL aﬀect terms, formulas, and games [18]. A uniform
substitution σ is a mapping from expressions of the form f(·) to terms σf(·),
from p(·) to formulas σp(·), and from game symbols a to hybrid games σa. Here ·
is a reserved function symbol of arity 0 marking the position where the argument,

Uniform Substitution at One Fell Swoop
433
e.g., argument θ to p(·) in formula p(θ), will end up in the replacement σp(·)
used for p(θ). Vectorial extensions would be accordingly for other arities k ≥0.
The key idea behind the new recursive one-pass application of uniform sub-
stitutions is that it simply applies σ by na¨ıve homomorphic recursion without
checking any admissibility conditions along the way. But the mechanism makes
up for that soundness-defying negligence by passing a cumulative set U of taboo
variables along the recursion that are then forbidden from being introduced free
by σ at the respective replacement of function f(·) and predicate symbols p(·),
respectively. No corresponding condition is required at substitutions of game
symbols a, since games already have unlimited access to and eﬀect on the state.
σU(x) = x
for variable x ∈V
σU(f(θ)) = (σUf)(σUθ)
def
= {· →σUθ}
∅σf(·) if FV(σf(·)) ∩U = ∅
σU(θ + η) = σUθ + σUη
σU(θ · η) = σUθ · σUη
σU((θ)′) = (σVθ)′
σU(θ ≥η) = σUθ ≥σUη
σU(p(θ)) = (σUp)(σUθ)
def
= {· →σUθ}
∅σp(·) if FV(σp(·)) ∩U = ∅
σU(¬φ) = ¬σUφ
σU(φ ∧ψ) = σUφ ∧σUψ
σU(∃x φ) = ∃x σU∪{x}φ
σU(⟨α⟩φ) = ⟨σU
V α⟩σV φ
σU
U∪BV(σa)(a) = σa
for game symbol a
σU
U∪{x}(x := θ) = x := σUθ
σU
U∪{x,x′}(x′ = θ & ψ) = (x′ = σU∪{x,x′}θ & σU∪{x,x′}ψ)
σU
U (?ψ) = ?σUψ
σU
V ∪W (α ∪β) = σU
V α ∪σU
W β
σU
W (α; β) = σU
V α; σV
W β
σU
V (α∗) = (σV
V α)
∗
where σU
V α is deﬁned
σU
V (αd) = (σU
V α)d
Fig. 2. Recursive application of one-pass uniform substitution σ for taboo U ⊆V
The result σUφ of applying uniform substitution σ for taboo set U ⊆V to
a dGL formula φ (or term θ or hybrid game α, respectively) is deﬁned in Fig. 2.
For proof rule US, the expression σφ is, then, deﬁned to be σ∅φ without taboos.
The case for ∃x φ in Fig. 2 conjoins the variable x to the taboo set in the
homomorphic application of σ to φ, because any newly introduced free uses of
x within that scope would refer to a diﬀerent semantic value than outside that
scope. In addition to computing the substituted hybrid game σU
V α, the recursive
application of one-pass uniform substitution σ to hybrid game α under taboo set
U also performs an analysis that results in a new output taboo set V , written
in subscript notation, that will be tabooed after this hybrid game. Superscripts
as inputs and subscripts as outputs follows static analysis notation and makes

434
A. Platzer
the α; β case reminiscent of Einstein’s summation: the output taboos V of σU
V α
become the input taboos V for σV
W β, whose output W is that of σU
W (α; β).
Similarly, the output taboos V resulting from the uniform substitute σU
V α of
a hybrid game α become taboo during the uniform substitution application
forming σV φ in the postcondition of a modality to build σU(⟨α⟩φ).
Repetitions σU
V (α∗) are the only complication in Fig. 2, where taboo U would
be too lax during the recursion, because earlier repetitions of α bind variables
of α itself, so only the taboos V obtained after one round σU
V α are correct input
taboos for the loop body. These two passes per loop are linear in the output
when considering repetitions α∗as their equivalent ?⊤∪α; α∗of double size.
Unlike in Church-style uniform substitution [5,16,18], attention is needed at
the replacement sites of function and predicate symbols in order to make up for
the neglected admissibility checks during all other operators. The result σU(p(θ))
of applying uniform substitution σ with taboo U to a predicate application p(θ)
is only deﬁned if the replacement σp(·) for p does not introduce free any tabooed
variable, i.e., FV(σp(·))∩U = ∅. Arguments are put in for placeholder · recursively
by the taboo-free use of uniform substitution {· →σUθ}, which replaces arity
0 function symbol · by σUθ. Taboos U are respected when forming (once!) the
uniform substitution to be used for argument ·, but empty taboos ∅suﬃce when
substituting the resulting σUθ for · in the replacement σp(·) for p.
All variables V become taboos during uniform substitutions into diﬀeren-
tials (θ)′, because any newly introduced occurrence of a variable x would cause
additional dependencies on its respective associated diﬀerential variable x′.
If the conditions in Fig. 2 are not met, the substitution σ is said to clash for
taboo U and its result σUφ is not deﬁned and cannot be used. All subsequent
applications of uniform substitutions are required to be deﬁned (no clash).
Whether a substitution clashes is only checked once at each replacement,
instead of also once per operator around it as in Church style from Eq. (1). The
free variables FV(σp(·)) of each (function and) predicate symbol replacement are
best stored with σ to avoid repeated computation of free variables.
This inference would unsoundly equate linear solutions with exponential ones:
clash
⟨v := f⟩p(v) ↔p(f)
⟨v := −x⟩[x′ = v] x ≥0 ↔[x′ = −x] x ≥0
Indeed, σ = {p(·) →[x′ = ·] x ≥0, f →−x} clashes so rejects the above
inference since the substitute −x for f has free variable x that is taboo in the
context [x′ = ·] x ≥0. By contrast, a sound use of rule US, despite its change in
multiple binding contexts with σ = {p(·) →[(x := x + ·; x′ = ·)∗] x + · ≥0, f →
−v}, is:
US
⟨v := f⟩p(v) ↔p(f)
⟨v := −v⟩[(x := x + v; x′ = v)∗] x + v ≥0 ↔[(x := x −v; x′ = −v)∗] x −v ≥0
Uniform substitution accurately distinguishes such sound inferences from
unsound ones even if the substitutions take eﬀect deep down within a dGL for-
mula. Uniform substitutions enable other syntactic transformations that require

Uniform Substitution at One Fell Swoop
435
a solid understanding of variable occurrence patterns such as common subex-
pression elimination, for example, by using the above inference from right to
left.
3.1
Taboo Lemmas
The only soundness-critical property of output taboos is that they correctly add
bound variables and never forget variables that were already input taboos.
Lemma 13 (Taboo set computation). One-pass uniform substitution appli-
cation monotonously computes taboos with correct bound variables for games:
if σU
V α is deﬁned, then V ⊇U ∪BV(σU
V α)
Any superset of such taboo computations (or the free variable sets used in
Fig. 2) remains correct, just more conservative. The change from input taboo U
to output taboo V is a function of the hybrid game α, justifying the construction
of σU
V (α∗): if σU
V α and σV
W α are deﬁned, then σV
V α is deﬁned and equal to σV
W α.
By Lemma 13, no implementation of bound variables is needed when deﬁning
game symbols via σU
U∪V (a) = σa where {}∅
V (σa) with identity substitution {}.
But bound variable computations speed up loops via σU
V (α∗) = (σU∪B
V
α)
∗since
B = BV(σ∅
Mα) can be computed and used correctly in one pass when U ∪B = V .
3.2
Uniform Substitution Lemmas
Uniform substitutions are syntactic transformations on syntactic expressions.
Their semantic counterpart is the semantic transformation that maps an inter-
pretation I and a state ω to the adjoint interpretation σ∗
ωI that changes the
meaning of all symbols according to the syntactic substitution σ. The interpre-
tation Id· agrees with I except that function symbol · is interpreted as d ∈R.
Deﬁnition 14 (Substitution adjoints). The adjoint to substitution σ is the
operation that maps I, ω to the adjoint interpretation σ∗
ωI in which the inter-
pretation of each function symbol f, predicate symbol p, and game symbol a are
modiﬁed according to σ (it is enough to consider those that σ changes):
σ∗
ωI(f) : R →R; d →Id·ω[[σf(·)]]
σ∗
ωI(p) = {d ∈R : ω ∈Id·[[σp(·)]]}
σ∗
ωI(a) : ℘(S) →℘(S); X →I[[σa]]

X

The uniform substitution lemmas below are key to the soundness and equate
the syntactic eﬀect that a uniform substitution σ has on a syntactic expression
in I, ω with the semantic eﬀect that the switch to the adjoint interpretation σ∗
ωI
has on the original expression. The technical challenge compared to Church-style
uniform substitution [16,18] is that no admissibility conditions are checked at
the game operators that need them, because the whole point of one-pass uniform

436
A. Platzer
substitution is that it homomorphically recurses in a linear complexity sweep by
postponing admissibility checks. All that happens during the substitution is that
diﬀerent taboo sets are passed along. Yet, still, there is a crucial interplay of the
particular taboos imposed henceforth at binding operators and the retroactive
checking at function and predicate symbol replacement sites.
In order to soundly deal with the negligence in admissibility checking of
one-pass uniform substitutions in a modular way, the main insight is that it is
imperative to generalize the range of applicability of uniform substitution lem-
mas beyond the state ω of original interest where the adjoint σ∗
ωI was formed,
and make them cover all variations of states that are so similar that they might
arise during soundness justiﬁcations. By demanding more comprehensive care
at replacement sites, soundness arguments make up for the temporary lapses in
attention during all other operators. This gives the uniform substitution algo-
rithm broader liberties at binding operators, while simultaneously demanding
broader compatibility in semantic neighborhoods on its parts. Due to the recur-
sive nature of function substitutions, the proof [20] of the following result is by
structural induction lexicographically on the structure of σ and θ, for all U, ν, ω.
Lemma 15 (Uniform substitution for terms).
The uniform substitution
σ for taboo U ⊆V and its adjoint interpretation σ∗
ωI for I, ω have the same
semantics on U-variations for all terms θ:
for all U-variations ν of ω: Iν[[σUθ]] = σ∗
ωIν[[θ]]
Recall that all uniform substitutions are only deﬁned when they meet the side
conditions from Fig. 2. A mention such as σUθ in Lemma 15 implies that its side
conditions during the application of σ to θ with taboos U are met. Substitutions
are antimonotone in taboos: If σUθ is deﬁned, then σV θ is deﬁned and equal to
σUθ for all V ⊆U (accordingly for φ, α). The more taboos a use of a substitution
tolerates, the more broadly its adjoint generalizes to state variations.
The corresponding results for formulas and games are proved by simultane-
ous induction since formulas and games are deﬁned by simultaneous induction,
as games may occur in formulas and, vice versa. The inductive proof [20] is lex-
icographic over the structure of σ and φ or α, with a nested induction over the
closure ordinals of the loop ﬁxpoints, simultaneously for all ν, ω, U, X.
Lemma 16 (Uniform substitution for formulas).
The uniform substitu-
tion σ for taboo U ⊆V and its adjoint interpretation σ∗
ωI for I, ω have the same
semantics on U-variations for all formulas φ:
for all U-variations ν of ω: ν ∈I[[σUφ]] iﬀν ∈σ∗
ωI[[φ]]
Lemma 17 (Uniform substitution for games).
The uniform substitution
σ for taboo U ⊆V and its adjoint interpretation σ∗
ωI for I, ω have the same
semantics on U-variations for all games α:
for all U-variations ν of ω: ν ∈I[[σU
V α]]

X

iﬀν ∈σ∗
ωI[[α]]

X


Uniform Substitution at One Fell Swoop
437
3.3
Soundness
With the uniform substitution lemmas having established the crucial equivalence
of syntactic substitution and adjoint interpretation, the soundness of uniform
substitution uses in proofs is now immediate. The notation σφ in proof rule US
is short for σ∅φ, so the result of applying σ to φ without taboos (more taboos
may still arise during the substitution application), and only deﬁned if σ∅φ is.
A proof rule is sound when its conclusion is valid if all its premises are valid.
Theorem 18 (Soundness of uniform substitution).
Proof rule US is
sound.
(US)
φ
σφ
Theorem 18 is all it takes to soundly instantiate concrete axioms. Uniform
substitutions can instantiate whole inferences [16], which makes it possible to
avoid proof rule schemata by instantiating axiomatic proof rules consisting of
pairs of concrete formulas. This enables uniformly substituting premises and con-
clusions of entire proofs of locally sound inferences, i.e., those whose conclusion
is valid in any interpretation that all their premises are valid in.
Theorem 19 (Soundness of uniform substitution of rules). All uniform
substitution instances for taboo V of locally sound inferences are locally sound:
φ1
. . .
φn
ψ
locally sound
implies
σVφ1
. . .
σVφn
σVψ
locally sound
USR marks the use of Theorem 19 in proofs. If n = 0 (so ψ has a proof), USR
preserves local soundness for taboo-free σ∅ψ instead of σVψ, as US proves σ∅ψ
from the provable ψ and soundness is equivalent to local soundness for n = 0.
3.4
Completeness
Soundness is the property that every formula with a proof is valid. This is the
most important consideration for something as fundamental as a uniform sub-
stitution mechanism. But the converse question of completeness, i.e., that every
valid formula has a proof, is of interest as well, especially given the fact that
one-pass uniform substitutions check diﬀerently for soundness during the sub-
stitution application, which had better not lose otherwise perfectly valid proofs.
Completeness is proved in an easy modular style based on all the non-
trivial ﬁndings summarized in schematic relative completeness results, ﬁrst for
schematic dGL [15, Thm. 4.5], and then for a uniform substitution formulation of
dL [16, Thm. 40]. The combination of both schematic completeness results makes
it fairly easy to lift completeness to the setting in this paper. The challenge is
to show that all instances of axiom schemata that are used for dGL’s schematic
relative completeness result are provable by one-pass uniform substitution.
A dGL formula φ is called surjective iﬀrule US can instantiate φ to any
of its axiom schema instances, i.e., those formulas that are obtained by just

438
A. Platzer
[·] [a]⟨c⟩⊤↔¬⟨a⟩¬⟨c⟩⊤
⟨:=⟩= ⟨x := f⟩⟨c⟩⊤↔∃x (x = f ∧⟨c⟩⊤)
DS ⟨x′ = f⟩⟨c⟩⊤↔∃t≥0 ⟨x := x+ft⟩⟨x′ := f⟩⟨c⟩⊤
⟨?⟩⟨?q⟩p ↔q ∧p
⟨∪⟩⟨a ∪b⟩⟨c⟩⊤↔⟨a⟩⟨c⟩⊤∨⟨b⟩⟨c⟩⊤
⟨;⟩⟨a; b⟩⟨c⟩⊤↔⟨a⟩⟨b⟩⟨c⟩⊤
⟨∗⟩⟨a∗⟩⟨c⟩⊤↔⟨c⟩⊤∨⟨a⟩⟨a∗⟩⟨c⟩⊤
⟨d⟩⟨ad⟩⟨c⟩⊤↔¬⟨a⟩¬⟨c⟩⊤
M
⟨c⟩⊤→⟨d⟩⊤
⟨a⟩⟨c⟩⊤→⟨a⟩⟨d⟩⊤
FP
⟨c⟩⊤∨⟨a⟩⟨d⟩⊤→⟨d⟩⊤
⟨a∗⟩⟨c⟩⊤→⟨d⟩⊤
MP
p
p →q
q
∀
⟨c⟩⊤
∀x ⟨c⟩⊤
Fig. 3. Diﬀerential game logic axioms and axiomatic proof rules
replacing game symbols a uniformly by any game, etc. An axiomatic rule is
called surjective iﬀUSR of Theorem 19 can instantiate it to any of its proof rule
schema instances.
Lemma 20 (Surjective axioms).
If φ is a dGL formula that is built only
from game symbols but no function or predicate symbols, then φ is surjective.
Axiomatic rules consisting of surjective dGL formulas are surjective.
Instead of following previous completeness arguments for uniform substitu-
tion [18], this paper presents a pure game-style uniform substitution formulation
in Fig. 3 of a dGL axiomatization that makes the overall completeness proof most
straightforward. For that purpose, the dGL axiomatization in Fig. 3 uses proper-
ties ⟨c⟩⊤of a game symbol c, which, as a game, can impose arbitrary conditions
on the state even for a trivial postcondition (the formula ⊤is always true).
All axioms of Fig. 3, except test ⟨?⟩, equational assignment ⟨:=⟩=, and con-
stant solution DS, are surjective by Lemma 20. The US requirement that no sub-
stitute of f may depend on x is important for the soundness of DS and ⟨:=⟩=.
Axiom ⟨?⟩is surjective, as it has no bound variables, so generates no taboos
and none of its instances clash: σ∅(⟨?q⟩p ↔q ∧p) = (⟨σ∅
∅q⟩σ∅p ↔σ∅q ∧σ∅p).
Similarly, rule MP is surjective [16], and the other rules are surjective by Lemma
20. Other diﬀerential equation axioms are elided but work as previously [16].
Besides rule US, bound variable renaming (rule BR) is the only schematic
principle, mostly for generalizing assignment axiom ⟨:=⟩= to other variables.
Lemma 21 (Bound renaming). Rule BR is locally sound, where ψ y
x is the
result of uniformly renaming x to y in ψ (also x′ to y′ but no x′′, x′′′ etc. or
game symbols occur in ψ, where the rule BR for [x := θ]ψ is accordingly):
(BR) φ →⟨y := θ⟩⟨y′ := x′⟩ψ y
x
φ →⟨x := θ⟩ψ
(y, y′ ̸∈ψ)
Theorem 22 (Relative completeness).
The dGL calculus is a sound and
complete axiomatization of hybrid games relative to any diﬀerentially expressive
logic L, i.e., every valid dGL formula is provable in dGL from L tautologies.

Uniform Substitution at One Fell Swoop
439
This completeness result assumes that no game symbols occur, because uni-
form renaming otherwise needs to become a syntactic operator. A logic L closed
under ﬁrst-order connectives is diﬀerentially expressive (for dGL) if every dGL for-
mula φ has an equivalent φ♭in L and all diﬀerential equation equivalences of the
form ⟨x′ = θ⟩G ↔(⟨x′ = θ⟩G)♭for G in L are provable in its calculus.
4
Diﬀerential Hybrid Games
Uniform substitution generalizes from dGL for hybrid games [15] to dGL for dif-
ferential hybrid games [17], which add diﬀerential games as a new atomic game.
A diﬀerential game x′ = θ&dy ∈Y &z ∈Z allows Angel to control how long to
follow the diﬀerential equation x′ = θ (in which variables x, y, z may occur) while
Demon provides a measurable input for y over time satisfying the formula y ∈Y
always and Angel, knowing Demon’s current input, provides a measurable input
for z satisfying the formula z ∈Z. All occurrences of y, z in x′ = θ&dy ∈Y &z ∈Z
are bound, and y ∈Y and z ∈Z are formulas in the free variables y or z, respec-
tively. It has been a long-standing challenge to give mathematical meaning [6,7]
and sound reasoning principles [17] for diﬀerential games. Both outcomes can
simply be adopted here under the usual well-deﬁnedness assumptions [17].
Uniform substitution application in Fig. 2 lifts to diﬀerential games by
adding:
σU
¯U (x′ = θ&dy ∈Y &z ∈Z) = (x′ = σ
¯Uθ&dy ∈σ
¯UY &z ∈σ
¯UZ)
where ¯U is U ∪{x, x′, y, y′, z, z′}. Well-deﬁnedness assumptions on diﬀerential
games [17] need to hold, e.g., only ﬁrst-order logic formulas denoting compact
sets are allowed for controls and the diﬀerential equations need to be bounded.
As terms are unaﬀected by adding diﬀerential games to the syntax, Lemma
9 and 15 do not change. The proofs of the coincidence Lemmas 10 and 11 and
bound eﬀect Lemma 12 [18] transfer to dGL with diﬀerential hybrid games in
verbatim thanks to their use of semantically deﬁned free and bound variables,
which carry over to diﬀerential hybrid games. The proof of Lemma 13 generalizes
easily by adding a case for diﬀerential games with the above ¯U. The uniform sub-
stitution Lemmas 16 and 17 inductively generalize to diﬀerential hybrid games
because of:
Lemma 23 (Uniform substitution for diﬀerential games). Let U ⊆V.
For all U-variations ν of ω:
ν ∈I[[σU
¯U (x′ = θ&dy ∈Y &z ∈Z)]]

X

iﬀν ∈σ∗
ωI[[x′ = θ&dy ∈Y &z ∈Z]]

X

The proof [20] makes clever use of diﬀerential game reﬁnements [17] to avoid
the signiﬁcant complexities and semantic subtleties of diﬀerential games.

440
A. Platzer
5
Conclusion
This paper introduced signiﬁcantly faster uniform substitution mechanisms,
the dominant logical inference in axiomatic small core hybrid systems/games
provers. It is also ﬁrst in proving soundness of uniform substitution for diﬀeren-
tial games.
Implementations exhibit a linear runtime complexity compared to the expo-
nential complexity that direct implementations [8] of prior Church-style uniform
substitutions exhibit, except when applying aggressive space/time optimization
tradeoﬀs where that drops down to a quadratic runtime in practice.
Acknowledgment. I thank Frank Pfenning for useful discussions and the anonymous
reviewers for their helpful feedback. I appreciate the kind advice of the Isabelle group
at TU Munich for the subsequent formalization [19] of the proofs.
References
1. Ahrendt, W., Beckert, B., Bubel, R., H¨ahnle, R., Schmitt, P.H., Ulbrich, M. (eds.):
Deductive Software Veriﬁcation - The KeY Book, LNCS, vol. 10001. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-49812-6
2. Bohrer, R., Rahli, V., Vukotic, I., V¨olp, M., Platzer, A.: Formally veriﬁed diﬀer-
ential dynamic logic. In: Bertot, Y., Vafeiadis, V. (eds.) Certiﬁed Programs and
Proofs - 6th ACM SIGPLAN Conference, CPP 2017, Paris, France, pp. 208–221.
ACM, New York, 16–17 January 2017. https://doi.org/10.1145/3018610.3018616
3. de Bruijn, N.: Lambda calculus notation with nameless dummies, a tool for
automatic formula manipulation, with application to the Church-Rosser theo-
rem. Indagationes Math. 75(5), 381–392 (1972). https://doi.org/10.1016/1385-
7258(72)90034-0
4. Church, A.: A formulation of the simple theory of types. J. Symb. Log. 5(2), 56–68
(1940). https://doi.org/10.2307/2266170
5. Church, A.: Introduction to Mathematical Logic. Princeton University Press,
Princeton (1956)
6. Elliott, R.J., Kalton, N.J.: Cauchy problems for certain Isaacs-Bellman equations
and games of survival. Trans. Amer. Math. Soc. 198, 45–72 (1974). https://doi.
org/10.1090/S0002-9947-1974-0347383-8
7. Evans, L.C., Souganidis, P.E.: Diﬀerential games and representation formulas for
solutions of Hamilton-Jacobi-Isaacs equations. Indiana Univ. Math. J. 33(5), 773–
797 (1984). https://doi.org/10.1512/iumj.1984.33.33040
8. Fulton, N., Mitsch, S., Quesel, J.-D., V¨olp, M., Platzer, A.: KeYmaera X: an
axiomatic tactical theorem prover for hybrid systems. In: Felty, A.P., Middeldorp,
A. (eds.) CADE 2015. LNCS (LNAI), vol. 9195, pp. 527–538. Springer, Cham
(2015). https://doi.org/10.1007/978-3-319-21401-6 36
9. Harel, D., Kozen, D., Tiuryn, J.: Dynamic Logic. MIT Press, Cambridge (2000).
https://doi.org/10.7551/mitpress/2516.001.0001
10. Henkin, L.: Banishing the rule of substitution for functional variables. J. Symb.
Log. 18(3), 201–208 (1953). https://doi.org/10.2307/2267403
11. Hilbert, D., Ackermann, W.: Grundz¨uge der theoretischen Logik. Springer, Berlin
(1928)

Uniform Substitution at One Fell Swoop
441
12. Hilbert, D., Bernays, P.: Grundlagen der Mathematik, vol. I, 2nd edn. Springer,
Heidelberg (1934). https://doi.org/10.1007/978-3-642-86894-8
13. Mitchell, I., Bayen, A.M., Tomlin, C.: A time-dependent Hamilton-Jacobi formula-
tion of reachable sets for continuous dynamic games. IEEE Trans. Autom. Control
50(7), 947–957 (2005). https://doi.org/10.1109/TAC.2005.851439
14. Pfenning, F., Elliott, C.: Higher-order abstract syntax. In: Wexelblat, R.L. (ed.)
PLDI, pp. 199–208. ACM (1988). https://doi.org/10.1145/53990.54010
15. Platzer, A.: Diﬀerential game logic. ACM Trans. Comput. Logic 17(1), 1:1–1:51
(2015). https://doi.org/10.1145/2817824
16. Platzer, A.: A complete uniform substitution calculus for diﬀerential dynamic logic.
J. Autom. Res. 59(2), 219–265 (2017). https://doi.org/10.1007/s10817-016-9385-
1
17. Platzer, A.: Diﬀerential hybrid games. ACM Trans. Comput. Logic. 18(3), 19:1–
19:44 (2017). https://doi.org/10.1145/3091123
18. Platzer, A.: Uniform substitution for diﬀerential game logic. In: Galmiche, D.,
Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 211–
227. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94205-6 15
19. Platzer, A.: Diﬀerential game logic. Archive of Formal Proofs 2019 (2019). http://
isa-afp.org/entries/Diﬀerential Game Logic.html. formal proof development
20. Platzer, A.: Uniform substitution at one fell swoop. CoRR abs/1902.07230 (2019).
http://arxiv.org/abs/1902.07230
21. Quesel, J.-D., Platzer, A.: Playing hybrid games with keymaera. In: Gramlich, B.,
Miller, D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 439–453.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 34
22. Quine, W.V.O.: A System of Logistic. Harvard University Press, Cambridge (1934)
23. Schneider, H.H.: Substitutions for predicate variables and functional variables.
Notre Dame J. Formal Logic 21(1), 33–44 (1980). https://doi.org/10.1305/ndjﬂ/
1093882937
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

A Formally Veriﬁed Abstract Account
of G¨odel’s Incompleteness Theorems
Andrei Popescu1(B) and Dmitriy Traytel2(B)
1 Department of Computer Science, Middlesex University, London, UK
a.popescu@mdx.ac.uk
2 Institute of Information Security, Department of Computer Science, ETH Z¨urich,
Zurich, Switzerland
traytel@inf.ethz.ch
Abstract. We present an abstract development of G¨odel’s incomplete-
ness theorems, performed with the help of the Isabelle/HOL theorem
prover. We analyze suﬃcient conditions for the theorems’ applicabil-
ity to a partially speciﬁed logic. In addition to the usual beneﬁts of
generality, our abstract perspective enables a comparison between alter-
native approaches from the literature. These include Rosser’s variation
of the ﬁrst theorem, Jeroslow’s variation of the second theorem, and
the ´Swierczkowski–Paulson semantics-based approach. As part of our
framework’s validation, we upgrade Paulson’s Isabelle proof to produce
a mechanization of the second theorem that does not assume soundness
in the standard model, and in fact does not rely on any notion of model
or semantic interpretation.
1
Introduction
G¨odel’s incompleteness theorems [10,13] are landmark results in mathematical
logic. Both theorems refer to consistent logical theories that satisfy some assump-
tions, notably that of “containing enough arithmetic.” The ﬁrst incompleteness
theorem (IT1) says that there are sentences that the theory cannot decide (i.e.,
neither prove nor disprove); the second theorem (IT2) says that the theory can-
not prove (an internal formulation of) its own consistency. It is generally accepted
that IT1 and IT2 have a wide scope, covering many logics and logical theories.
However, when it comes to rigorous presentation, typically these results are only
proved for particular, albeit paradigmatic cases, such as theories of arithmetic
or hereditarily ﬁnite (HF) sets, within classical ﬁrst-order logic (FOL); and even
in these cases the constructions and proofs tend to be “incomplete and (appar-
ently) irremediably messy” [4, p. 16]. Hence, the theorems’ scope remains largely
unexplored on a rigorous/formal basis.
The emergence of powerful theorem provers has changed the rules of the game
and, we argue, the expectation. Using interactive theorems provers, we can reli-
ably keep track of all the constructions and their properties. Proof automation
(often powered by fully automatic provers [18,28]), makes complete, fully rigor-
ous proofs feasible. And indeed, researchers have successfully met the challenge
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 442–461, 2019.
https://doi.org/10.1007/978-3-030-29436-6_26

A Formally Veriﬁed Abstract Account of G¨odel’s Incompleteness Theorems
443
of mechanizing IT1 [15,25,27,35] and recently IT2 [27]. Besides reassurance,
these veriﬁcation tours de force have brought superior technical insight into the
theorems. But they have taken place within the same solitary conﬁnement of
scope as the informal proofs.
This paper takes steps towards a more comprehensive prover-backed explo-
ration of the incompleteness theorems, by a detailed analysis of their assump-
tions. We use Isabelle/HOL [24] to establish general conditions under which
the theorems apply to a partially speciﬁed logic. Our formalization is publicly
available [31]. An extended technical report gives more details [30].
We start with a notion of logic (Sect. 2) whose terms, formulas and prov-
ability relation are kept abstract (Sect. 2.1). In particular, substitution and free
variables are not deﬁned, but axiomatized by some general properties. On top
of this logic substratum, we consider an arithmetic substratum, consisting of a
set of closed terms called numerals and an order-like relation (Sect. 2.2). Also
factored in our abstract framework are encodings of formulas and proofs into
numerals, the representability of various functions and relations as terms or for-
mulas (Sect. 2.3), variations of the Hilbert-Bernays-L¨ob derivability conditions
[16,23] (Sect. 2.4), and standard models (Sect. 2.5).
Overall, our assumptions capture the notion of “containing enough arith-
metics” in a general and ﬂexible way. It is general because only few assumptions
are made about the exact nature of formulas and numerals. It is ﬂexible because
diﬀerent versions of the incompleteness theorems consider their own “amount of
arithmetics” that makes it “enough,” as proper subsets of these assumptions.
Indeed, our formalization of the theorems (Sect. 3) proceeds in an austere-buﬀet
style: Every result picks just enough infrastructure needed for it to hold—ranging
from diagonalization which requires very little (Sect. 3.1) to Rosser’s version of
IT1 which is quite demanding. This approach caters for a sharp comparison
between diﬀerent formulations of the theorems, highlighting their trade-oﬀs:
G¨odel’s original formulation of IT1 versus Rosser’s improvement (Sect. 3.2),
proof-theoretic versus semantic versions of IT1 (Sect. 3.2), and G¨odel’s origi-
nal formulation of the IT2 versus Jeroslow’s improvement (Sect. 3.3).
Abstractness is our development’s main strength, but also a potential weak-
ness: Are our hypotheses reasonable? Are they consistent? These questions par-
ticularly concern our axiomatization of free variables and substitution—a noto-
riously error-prone area. As a remedy, we instantiate our framework to Paulson’s
semantics-based IT1 and IT2 for HF set theory [27], also performing an upgrade
of Paulson’s IT2 to a more general and standard formulation: for consistent (not
necessarily sound) theories (Sect. 4). In the rest of this section, we discuss some
formalization principles and related work.
Formal Design Principles. Our long-term goal is a framework that makes it
easy to instantiate the incompleteness theorems and related results to diﬀerent
logics. This is a daunting task, especially for IT2, where a lot of seemingly logic-
speciﬁc technicalities are required to even formulate the theorem. The challenge
is to push as much as possible of the technical constructions and lemmas to a
largely logic-independent layer.

444
A. Popescu and D. Traytel
To this end, we strive to make minimal assumptions in terms of structure and
properties when inferring the results—we will call this the Economy principle.
For example, we do not deﬁne, but axiomatize syntax in terms of a minimalistic
infrastructure. We assume a generic single-point substitution, then deﬁne simul-
taneous substitution and infer its properties. This is laborious, but worthwhile:
Any logic that provides a single-point substitution satisfying our assumptions
gets the simultaneous substitution for free.
As another instance of Economy, when faced with two diﬀerent ways of for-
mulating a theorem’s conclusion we prefer the one that is stronger under fewer
assumptions. (And dually, we prefer weakness for a theorem’s assumptions.) For
example, we discuss two variants of consistency: (1) “does not prove false” or (2)
“there exists no formula such that itself and its negation are provable” (Sect. 3.3).
While the statements are equivalent at the meta-level, their representations as
object-logic formulas are not necessarily equivalent; in fact, (1) implies (2) under
mild assumptions but not vice versa. So in our abstract theorems we prefer (1).
Indeed, even if (2) implies (1) in all reasonable instances, why postpone for the
instantiation time any fact that we can show abstractly?
Applying the Economy principle not only stocks up generality for instantia-
tions, but also accurately outlines trade-oﬀs: How much does it cost (in terms of
other added assumptions) to improve the conclusion, or to weaken an assumption
of a theorem? For example, an Economy-based proof of Rosser’s variant of IT1
reveals how much arithmetic we must factor in for weakening the ω-consistency
assumption into consistency.
Related Work. G¨odel initially gave a proof of IT1 and the rough proof idea of
IT2 [13]. Hilbert and Bernays gave a ﬁrst detailed proof of IT2 [16]. A vast lit-
erature was dedicated to the (re)formulation, proof, and analysis of these results
[4,33,38,39]. The now canonical line of reasoning goes through the derivability
conditions devised by Bernays and Hilbert [16] and simpliﬁed by L¨ob [23]. These
conditions have inspired a new branch of modal logic called provability logic [4].
Jeroslow has argued that, unlike previously believed, one condition is redundant
when proving IT2 [17].
Kreisel [20] and Jeroslow [17] were the ﬁrst to study abstract conditions on
logics under which the incompleteness theorems apply. Buldt [5] surveys the state
of the art focusing on IT1. Our abstract approach, based on generic syntax and
provability and truth predicates, resembles the style of institution-independent
model theory [9,14] and our previous work on abstract completeness [3] and com-
pleteness of ordered resolution [34]. Dimensions of generality that our formalized
work does not (yet) explore include quantiﬁer-free logics [17] and arithmetical
hierarchy reﬁnements [19]. Our syntax axiomatization is inspired by algebraic
theories of the λ-calculi syntax [11,12,29].
In the realm of mechanical proofs, the earliest substantial development was
due to Sieg [36], who used a prover based on TEM (Theory of Elementary Meta-
Mathematics) to formalize parts of the proofs of both IT1 and IT2. But the
ﬁrst full proof of IT1 was achieved by Shankar [35] in the Boyer-Moore prover,
followed by Harrison in HOL Light [15] and O’Connor in Coq [25]. IT2 has

A Formally Veriﬁed Abstract Account of G¨odel’s Incompleteness Theorems
445
only been fully proved recently—by Paulson in Isabelle/HOL [26,27] (who also
proved IT1). All these mechanizations target theories over a ﬁxed language in
classical FOL: that of arithmetic (Harrison and O’Connor) and that of HF sets or
a variation of it (Sieg, Shankar and Paulson). These mechanizations are mostly
focused on “getting all the work done” in a particular setting (although Harrison
targets a more abstract class of theories in the given language). On their way to
IT1, Shankar and O’Connor also prove representability of all partial, respectively
primitive recursive functions—important standalone results. Also, there has been
work on fully automating parts of the proofs of these theorems [1,6,32,37].
By contrast, we explore conditions that enable diﬀerent formulations for an
abstract logic, where aspects such as recursiveness are below our abstraction
level. The two approaches are complementary, and they both contribute to for-
mally taming the complex ramiﬁcations of the incompleteness theorems. When
instantiating our abstract assumptions to recover and upgrade Paulson’s results,
we took advantage of Paulson’s substantial work on proving the many low-level
lemmas towards the derivability conditions. More should be done at an abstract
level to avoid duplicating some of these laborious lemmas when instantiating the
theorems to diﬀerent logics. This will be future work.
2
Abstract Assumptions
Roughly, the incompleteness theorems are considered to hold for logical theories
that (1) contain enough arithmetic and (2) are “eﬀective” in that they themselves
can be arithmetized. Our goal is to give a general expression of these favorable
conditions. To this end, we identify some logic and arithmetic substrata con-
sisting of structure and axioms that express the containment of (various degrees
of) arithmetic more abstractly and ﬂexibly than relative interpretations [41]. We
also identify abstract notions of encodings and representability that have just
what it takes for a working arithmetization.
2.1
The Logical Substratum
We start with some unspeciﬁed sets of variables (Var, ranged over by x, y, z),
terms (Term, ranged over by s, t) and formulas (Fmla, ranged over by ϕ, ψ, χ).
We assume that variables are particular terms, Var ⊆Term, and that Var is
inﬁnite. Free-variables and substitution operators, FVars and
[ / ], are assumed
for both terms and formulas. We think of FVars(t) as the (ﬁnite) set of free
variables of the term t, and similarly for formulas. We call sentence any formula
with no free variable, and let Sen denote the set of sentences. We think of s [t/x]
as the term obtained from s by the (capture-avoiding) substitution of t for the
free occurrences of variable x; we think of ϕ [t/x] as the formula obtained from ϕ
by the substitution of t for the free occurrences of variable x.
In FOL, terms introduce no bindings, so any occurring variable is free. FOL
terms fall under our framework, and so do terms with bindings as in λ-calculi
and higher-order logic (HOL). To achieve this degree of inclusiveness while also

446
A. Popescu and D. Traytel
being able to prove interesting results, we work under some well-behavedness
assumptions about the free-variables and substitution operators. For example,
free-variables distribute over substitution, FVars (ϕ [s/x]) = FVars(ϕ) −{x} ∪
FVars(s) if x ∈FVars(ϕ), and substitution is compositional, ϕ [s1/x1] [s2/x2] =
ϕ [s2/x2] [(s1 [s2/x2]) / x1] if x1 ̸= x2 and x1 /∈FVars(s2). Our extended report [30]
contains the full list of our generic syntax axioms.
The incompleteness theorems rely heavily on simultaneous substitution,
written ϕ [t1/x1, . . . , tn/xn], whose properties are tricky to formalize—for exam-
ple, Paulson’s formalization paper dedicates them ample space [27, 6.2].
To address this problem once and for all generically, we deﬁne simulta-
neous substitution from the single-point substitution, ϕ [t/x], and infer its
properties from the single-point substitution axioms. For example, we prove
that FVars (ϕ [s1/x1, . . . , sn/xn])
=
FVars(ϕ)
∪
 {FVars(si) −{xi}
|
i
∈
{1, . . . , n} and xi ∈FVars(ϕ)}. The technicalities are delicate: To avoid undesired
variable replacements, ϕ [s1/x1, . . . , sn/xn] must be deﬁned as ϕ [y1/x1] . . . [yn/xn]
[s1/y1] . . . [sn/yn] for some fresh y1, . . . , yn, the choice of which we must show to
be immaterial. This deﬁnition’s complexity is reﬂected in the properties’ proofs.
But again, this one-time eﬀort beneﬁts any “customer” logic: In exchange for a
well-behaved single-point substitution, it gets back a well-behaved simultaneous
substitution.
We let v1, v2, . . . be ﬁxed mutually distinct variables. We write Fmlak for the
set of formulas whose free variables are precisely {v1, . . . , vk}, and Fmla⊆
k for the
set of formulas whose variables are among {v1, . . . , vk}. Note that Fmlak ⊆Fmla⊆
k
and Fmla0 = Fmla⊆
0 = Sen. Given ϕ ∈Fmla⊆
k , we write ϕ (t1, . . . , tn) instead of
ϕ [t1/v1, . . . , tn/vn].
In addition to free variables and substitution, our theorems will require for-
mulas to be equipped with term equality (≡), Boolean connectives (⊥, ⊤, →,
¬, ∧, ∨), universal and existential quantiﬁers (∀, ∃). In our formalization, we
assume a minimal list of the above with respect to intuitionistic logic, and deﬁne
the rest from this minimal list. They are not assumed to be constructors (syntax
builders), but operators on terms and formulas, e.g., ≡: Term →Term →Fmla,
⊥∈Fmla, ∀: Var × Fmla →Fmla. This caters for logics that do not have them
as primitives. For example, HOL deﬁnes all connectives and quantiﬁers from
λ-abstraction and either equality or implication.
We ﬁx a unary relation ⊢⊆Fmla on formulas, called provability. We write
⊢ϕ instead of ϕ ∈⊢, and say the formula ϕ is provable. Whenever certain
formula connectives or quantiﬁers are assumed present, we will assume that ⊢
behaves intuitionistically w.r.t. them—namely, we assume the usual (Hilbert-
style) intuitionistic FOL axioms with respect to the abstract connectives and
quantiﬁers. Stronger systems, such as those of classical logic, also satisfy these
assumptions.
Consistency, denoted Con, is deﬁned as the impossibility to prove false,
namely ̸⊢⊥. Another central concept is ω-consistency—we carefully choose a
formulation that works intuitionistically, with conclusion reminiscent of G¨odel’s
negative translation [8]:

A Formally Veriﬁed Abstract Account of G¨odel’s Incompleteness Theorems
447
OCon: For all ϕ ∈Fmla⊆
1 , if ⊢¬ ϕ(n) for all n ∈Num then ̸⊢¬ ¬(∃x. ϕ(x)).
Assuming classic deduction in ⊢, this is equivalent to the standard formula-
tion: For all ϕ ∈Fmla⊆
1 , it is not the case that ⊢ϕ(n) for all n ∈Num and
⊢¬ (∀x. ϕ(x)).
Occasionally, we will consider not only provability but also explicit proofs.
We ﬁx a set Proof of (entities we call) proofs, ranged over by p, q, and a binary
relation between proofs p and sentences ϕ, written p ⊩ϕ and read “p is a proof
of ϕ.” We assume ⊢and ⊩to be related as expected, in that provability is the
same as the existence of a proof:
Rel⊩
⊢: For all ϕ ∈Sen, ⊢ϕ iﬀthere exists p ∈Proof such that p ⊩ϕ.
2.2
The Arithmetic Substratum
We extend the generic syntax assumptions with a subset Num ⊆Term, of numer-
als, ranged over by m, n, which are assumed to be closed, i.e., have no free vari-
ables.
Convention 1. In all the shown results we implicitly assume: (1) the generic
syntax (free variable and substitution) axioms, (2) at least →and ⊥plus what-
ever connectives and quantiﬁers appear in the statement, (3) closedness of ⊢
under intuitionistic deduction rules, and (4) the existence of numerals. Other
assumptions (e.g., order-like relation axioms, consistency, standard models, etc.)
will be indicated explicitly.
On one occasion, we will assume an order-like binary relation modeled by
a formula ≺∈Fmla2. We write t1 ≺t2 instead of ≺(t1, t2) and ∀x ≺n. ϕ
instead of ∀x. x ≺n →ϕ. It turns out that at our level of abstraction it does not
matter whether ≺is a strict or a non-strict order. Indeed, we only require the
following two properties, where x ∈M denotes 
m∈M x ≡m and  expresses the
disjunction of a ﬁnite set of formulas:
Ord1: For all ϕ ∈Fmla1 and n ∈Num, if ⊢ϕ(m) for all m ∈Num, then ⊢∀x ≺
n. ϕ(x).
Ord2: For all n ∈Num, there exists a ﬁnite set M ⊆Num such that ⊢∀x. x ∈
M ∨n ≺x.
Ord1 states that if a property ϕ is provable for all numerals, then its universal
quantiﬁcation bounded by any given numeral n is also provable. Having in mind
the arithmetic interpretation of numerals, it would also make sense to assume a
stronger version of Ord1, replacing “if ⊢ϕ(m) for all m ∈Num” by the weaker
hypothesis “if ⊢ϕ(m) for all m ∈Num such that ⊢m ≺n”. But this stronger
version will not be needed.
Ord2 states that, for any numeral n, any element x in the domain of discourse
is either greater than n or equal to one of a ﬁnite set M of numerals. If we
instantiate our syntax to that of ﬁrst-order arithmetic, then the natural number
model satisﬁes Ord1 and Ord2 when interpreting ≺as either < or ≤. Moreover,
these properties are provable in intuitionistic Robinson arithmetic, again for
both < and ≤.

448
A. Popescu and D. Traytel
2.3
Encodings and Representability
Central in the incompleteness theorems are functions that encode formulas and
proofs as numerals, ⟨⟩: Fmla →Num and ⟨⟩: Proof →Num. For our abstract
results, the encodings are not required to be injective or surjective.
Let A1, . . . , Am be sets, and let, for each of them, ⟨⟩: Ai →Num be an
“encoding” function to numerals. Then, an m-ary relation R ⊆A1 × . . . × Am
is said to be represented by a formula
R⃝∈Fmlam if the following hold for all
(a1, . . . , am) ∈A1 × . . . × Am:
– (a1, . . . , am) ∈R implies ⊢
R⃝(⟨a1⟩, . . . , ⟨am⟩)
– (a1, . . . , am) /∈R implies ⊢¬ R⃝(⟨a1⟩, . . . , ⟨am⟩)
Let A be another set with ⟨⟩: A →Num. An m-ary function f : A1×. . . Am →
A is said to be represented by a formula
f⃝∈Fmlam+1 if for all (a1, . . . , am) ∈
A1 × . . . × Am:
– ⊢
f⃝(⟨a1⟩, . . . , ⟨am⟩, ⟨f (a1, . . . , am)⟩)
– ⊢∀x, y.
f⃝(⟨a1⟩, . . . , ⟨am⟩, x) ∧
f⃝(⟨a1⟩, . . . , ⟨am⟩, y) →x ≡y
The notion of a function being represented is stronger than that of its graph
being represented (as a relation)—but with enough deductive power they are
equivalent [38, §16]. We will need an even stronger notion: A function f as above
is term-represented by an operator
f⃝: Termm →Term if ⊢
f⃝(⟨a1⟩, . . . , ⟨am⟩) ≡
⟨f (a1, . . . , am)⟩for all (a1, . . . , am) ∈A1 × . . . × Am. When the formula by which
a relation/function P is represented or term-represented is irrelevant, we call P
representable or term-representable.
We will also need an enhancement of relation representability: Given i < m,
we call the representation of an m-ary relation R by R⃝i-clean if ⊢¬ R⃝(n1, . . . , nm)
for all numbers n1, . . . , nm such that ni (the i’th number among them) is outside
the image of ⟨⟩(i.e., there is no a ∈Ai with ni = ⟨a⟩). Cleanness would be
trivially satisﬁed if the encodings were surjective. However, surjectivity is not a
reasonable assumption. For example, most of the numeric encodings used in the
literature are injective but not surjective.
We let S : Fmla1 →Sen be the self-substitution function, which sends any
ϕ ∈Fmla1 to ϕ(⟨ϕ⟩), i.e., to the sentence obtained from ϕ by substituting the
encoding of ϕ for the unique variable of ϕ. An alternative is the following “soft”
version of S, which sends any ϕ ∈Fmla1 to ∃v1. v1 ≡⟨ϕ⟩∧ϕ, where v1 is the
single free variable of ϕ. The soft version yields provably equivalent formulas
and has the advantage that it is easier to represent inside the logic, since it does
not require formalizing the complexities of capture-avoiding substitution. All our
results involving S have been proved for both versions.
We will consider the properties Repr¬, ReprS, and Repr⊩, stating the repre-
sentability of the functions ¬ and S, and of the relation ⊩. In addition, Clean⊩
will state that the considered representation of ⊩is 1-clean, i.e., it is clean on
the proof component. For the representing formulas for the above relations and
functions we will use their circled names, ¬⃝, ⊩⃝, etc.; for example, Repr⊩means
that (1) p ⊩ϕ implies ⊢⊩⃝(⟨p⟩, ⟨ϕ⟩) and (2) p ̸⊩ϕ implies ⊢¬ ⊩⃝(⟨p⟩, ⟨ϕ⟩) for all
p ∈Proof and ϕ ∈Sen.

A Formally Veriﬁed Abstract Account of G¨odel’s Incompleteness Theorems
449
2.4
Derivability Conditions
Most of our assumptions refer to representability. An important exception is
the provability relation ⊢, for which only a weakening of representability is rea-
sonable. Let
⊢⃝∈Fmla1 be the formula for this task. We consider the following
assumptions about ⊢⃝, known as the Hilbert-Bernays-L¨ob derivability conditions:
HBL1: ⊢ϕ implies ⊢⊢⃝⟨ϕ⟩for all ϕ ∈Sen.
HBL2: ⊢⊢⃝⟨ϕ⟩∧⊢⃝⟨ϕ →ψ⟩→⊢⃝⟨ψ⟩for all ϕ, ψ ∈Sen.
HBL3: ⊢⊢⃝⟨ϕ⟩→⊢⃝⟨⊢⃝⟨ϕ⟩⟩for all ϕ ∈Sen.
Above and elsewhere, to lighten notation we omit parentheses when instantiating
one-variable formulas with encodings of formulas—e.g., writing
⊢⃝⟨ϕ⟩instead of
⊢⃝(⟨ϕ⟩).
HBL1 states that, if a sentence is provable, then its encoding is also provable
inside the representation. HBL3 is roughly a formulation of HBL1 “one level up,”
inside the proof system ⊢. Finally, note that the provability relation is closed
under modus ponens, in that ⊢ϕ and ⊢ϕ →ψ implies ⊢ψ for all ϕ, ψ ∈Sen.
Thus, HBL2 roughly states the same property inside the proof system. In short,
the derivability conditions state that the representation of provability acts partly
similarly to the provability relation. Note that the representability of “proof of”
implies HBL1, taking
⊢⃝(x) to be ∃y. ⊩⃝(y, x).
Convention 2. We focus on the standard provability representation in this
paper: Whenever we assume explicit proofs and representability of “proof of,”
the formula
⊢⃝will be deﬁned from ⊩⃝as shown above.
We will also be interested in the following variations of the derivability con-
ditions:
HBL4: ⊢⊢⃝⟨ϕ⟩∧⊢⃝⟨ψ⟩→⊢⃝⟨ϕ ∧ψ⟩for all ϕ, ψ ∈Sen.
HBL⇐
1 : ⊢⊢⃝⟨ϕ⟩implies ⊢ϕ for all ϕ ∈Sen.
SHBL3: ⊢⊢⃝(t) →⊢⃝⟨⊢⃝(t)⟩for all closed terms t.
WHBL2: ⊢ϕ →ψ implies ⊢⊢⃝⟨ϕ⟩→⊢⃝⟨ψ⟩for all ϕ, ψ ∈Sen.
HBL4 has a similar ﬂavor as HBL2, but refers to conjunction: It states that
the conjunction introduction rule holds inside the proof system. HBL⇐
1
is the
converse of HBL1. Finally, SHBL3 is a strengthening of HBL3 holding for all
closed terms and not only those that encode sentences, and (if we assume HBL1)
WHBL2 is a weakening of HBL2.
2.5
Standard Models
We ﬁx a unary relation |= ⊆Sen, representing truth of a sentence in the standard
model. We write |= ϕ instead of ϕ ∈|=, and read it as “ϕ is true.” We consider
the assumptions:
Syn|=: Syntactic entities (logical connectives and quantiﬁers) handle truth as
expected:

450
A. Popescu and D. Traytel
(1) ̸|= ⊥; (2) for all ϕ, ψ ∈Sen, |= ϕ and |= ϕ →ψ imply |= ψ;
(3) for all ϕ ∈Fmla1, if |= ϕ(n) for all n ∈Num then |= ∀x. ϕ(x);
(4) for all ϕ ∈Fmla1, if |= ∃x. ϕ(x) then |= ϕ(n) for some n ∈Num;
(5) for all ϕ ∈Sen, |= ϕ or |= ¬ ϕ.
Soundness (of provability with respect to truth): ⊢ϕ implies |= ϕ for all ϕ ∈Sen.
Syn|=(1–4) only contains a partial description of the syntactic entities’
behavior—corresponding to elimination rules for ⊥, →and ∃and introduction
rule for ∀. For our results this suﬃces. Syn|=(5) states that standard models
decide every sentence.
On his way to formalizing IT2 for extensions of the HF set theory, after prov-
ing HBL1 Paulson notes [27, p. 21]: “The reverse implication [namely HBL⇐
1 ],
despite its usefulness, is not always proved.” In his abstract account, Buldt also
assumes HBL⇐
1
in his most general formulation of IT1 [5, Theorem 3.1]; that
formulation has in mind not necessarily the standard provability representation
(our Convention 2), but any formula that weakly represents ⊢, which is accept-
able for IT1 but not for IT2 [2].
We avoid such an IT1 versus IT2 divergence by remaining focused on the
standard provability representation. In this case, for arithmetics and related
theories, HBL⇐
1 cannot be inferred without assuming soundness in the standard
model (which Paulson does), or at least ω-consistency. We can depict the situa-
tion abstractly, without knowing what standard models look like:
Lemma 3. (1) Assume Rel⊩
⊢, Repr⊩, Clean⊩and OCon. Then HBL⇐
1 holds.
(2) Assume Soundness and Syn|=(1, 2, 3). Then OCon holds.
(3) Assume Rel⊩
⊢, Repr⊩, Clean⊩, Soundness and Syn|=(1, 2, 4). Then |= ⊢⃝⟨ϕ⟩
implies ⊢ϕ for all ϕ ∈Sen. In particular, HBL⇐
1 holds.
Thus, staying in a proof-theoretic world, ω-consistency ensures HBL⇐
1 if the
“proof of” relation is cleanly represented (1). In turn, ω-consistency is ensured
by minimal semantic requirements, including the soundness of provability (2).
Finally, putting together representability and semantics, we can infer something
stronger than HBL⇐
1 : That the mere truth (and not just the provability) of a
sentence’s provability representation implies the provability of the sentence itself
(3).
It follows from either points (1, 2) or point (3) of the lemma that, in the
presence of standard models and soundness, clean representability of the “proof
of” relation implies HBL⇐
1 ; and recall that it also implies HBL1. So it implies an
“iﬀ” version of HBL1: ⊢ϕ if an only if ⊢⊢⃝⟨ϕ⟩. Interestingly, a converse of this
implication also holds. To state it, we initially assume there is no “outer” notion
of proof (i.e., no set Proof and no relation ⊩), but only an “inner” one, given by
a formula P ∈Fmla2 such that:
RelP
⊢⃝: ⊢⊢⃝⟨ϕ⟩←→∃x. P(x, ⟨ϕ⟩).
ComplP: |= P(n, ⟨ϕ⟩) implies ⊢P(n, ⟨ϕ⟩) for all n ∈Num and ϕ ∈Sen.
Compl¬P: |= ¬ P(n, ⟨ϕ⟩) implies ⊢¬ P(n, ⟨ϕ⟩) for all n ∈Num and ϕ ∈Sen.

A Formally Veriﬁed Abstract Account of G¨odel’s Incompleteness Theorems
451
RelP
⊢⃝is the inner version of Rel⊩
⊢: It expresses that, inside the representation,
proofs and provability are connected as expected. ComplP and Compl¬P state
that provability is complete on P statements about formula encodings, as well as
their negations; in traditional settings, this is true thanks to P being a bounded
arithmetical formula (Δ0). Now the converse result states that, thanks to (stan-
dard models and) the “iﬀ” version of HBL1, we can deﬁne an outer notion of
proof that is represented by the inner notion P:
Lemma 4. Assume RelP
⊢⃝, ComplP, Compl¬P, Soundness, Syn|=(4,5), HBL1 and
HBL⇐
1 . Take Proof = Num and deﬁne ⊩by n ⊩ϕ iﬀ⊢P (n, ⟨ϕ⟩). Then Rel⊩
⊢,
Repr⊩and Clean⊩hold, with ⊩being represented by P.
3
Abstract Incompleteness Theorems
After last section’s preparations, we are now ready to discuss diﬀerent versions
of the incompleteness theorems and their major lemmas, based on alternative
assumptions.
3.1
Diagonalization
The formula diagonalization technique (due to G¨odel and Carnap [7]) yields
“self-referential” sentences. All we need for it to work is the representability of
substitution.
Proposition 5. Assuming ReprS, for all ψ ∈Fmla1 there exists ϕ ∈Fmla1 with
⊢ϕ ←→ψ⟨ϕ⟩.
A sentence ϕ ∈Sen is called a G¨odel sentence if ⊢ϕ ←→¬ ⊢⃝⟨ϕ⟩; it is called
a Rosser sentence if ⊢ϕ ←→¬ (∃x. ⊩⃝(x, ⟨ϕ⟩) ∧RosserTwist(x, ⟨ϕ⟩)), where we
deﬁne RosserTwist(x, y) = ∀x′. x′ ≺x →∀y′. ¬⃝(y, y′) →¬ ⊩⃝(x′, y′). The exis-
tence of G¨odel and Rosser sentences follows immediately from diagonalization.
Proposition 6. Assuming ReprS, there exist G¨odel and Rosser sentences.
Thus, any G¨odel sentence is provably equivalent to the negation of its own prov-
ability; in G¨odel’s words [13], it “says about itself that it is not provable.” A
Rosser sentence ϕ asserts its own unprovabilty in a weaker fashion: Rather than
saying “Myself, ϕ, am not provable” (i.e., “it is not the case that there exists a
proof p of ϕ”), it says “it is not the case that there exists a proof p of ϕ such
that, for all smaller proofs q, q is not a proof of ¬ ϕ.” Here, “smaller” refers to
the order the encoding of proofs as numerals imposes.

452
A. Popescu and D. Traytel
3.2
The Incompleteness Theorems
IT1 identiﬁes sentences that are neither provable nor disprovable—which often
holds for G¨odel and Rosser sentences with the help of a provability relation
satisfying HBL1.
Proposition 7. Assume Con and HBL1. Then ̸⊢G for all G¨odel sentences G.
For showing that the G¨odel sentences are not disprovable, a standard route
is to assume explicit proofs, strengthen the consistency assumption to ω-
consistency, and strengthen HBL1 to representability of the “proof of” relation.
Proposition 8. Assume OCon, Rel⊩
⊢, Repr⊩, Clean⊩. Then ̸⊢¬ G for all G¨odel
sentences G.
Proof. Let G be a G¨odel sentence. We prove ̸⊢¬ G by contradiction. Assume (1)
⊢¬ G.
– By consistency (which is implied by OCon), we obtain ̸⊢G.
– From this and Rel⊩
⊢, we obtain p ̸⊩G for all p ∈Proof.
– From this, Repr⊩and Clean⊩, we obtain ⊢¬ ⊩⃝(n, ⟨G⟩) for all n ∈Num.
– From this and OCon, we obtain ̸⊢¬ ¬ ∃x. ⊩⃝(x, ⟨G⟩), i.e., ̸⊢¬ ¬ ⊢⃝⟨G⟩.
– Hence, since G is a G¨odel sentence, we obtain ̸⊢¬ G, which contradicts (1).
⊓⊔
While the line of reasoning in the above proof is mostly well-known, it con-
tains two subtle points about which the literature is not explicit (due to the usual
focus on classical ﬁrst-order arithmetic and particular choices of encodings).
First, we must assume the representation of the “proof of” relation to be
1-clean, i.e., clean with respect to the proof component. Indeed, the argument
crucially relies on converting the statement “p ̸⊩G for all p ∈Proof” into “⊢
¬ ⊩⃝(n, ⟨G⟩) for all n ∈Num,” which is only possible for 1-clean encodings. This
assumption will be repeatedly needed in later results. By contrast, cleanness is
never required with respect to the sentence component of “proof of” or for the
provability relation (which only involves sentence encodings). In short, cleanness
is only needed for proofs, not for sentences.
Second, to reach the desired contradiction for our intuitionistic proof system
⊢, from “⊢¬ ⊩⃝(n, ⟨G⟩) for all n ∈Num” it is not suﬃcient to employ stan-
dard ω-consistency, which would only give us ̸⊢∃x. ⊩⃝(x, ⟨G⟩), i.e., ̸⊢⊢⃝⟨G⟩; the
last together with ⊢G ←→¬ ⊢⃝⟨G⟩would be insuﬃcient for obtaining ̸⊢¬ G.
However, our stronger version of ω-consistency, OCon, does the trick. IT1 now
follows by putting together Propositions 6–8:
Theorem 9. (IT1) Assume OCon, Rel⊩
⊢, Repr⊩, Clean⊩, and ReprS. Then:

A Formally Veriﬁed Abstract Account of G¨odel’s Incompleteness Theorems
453
(1) There exists a G¨odel sentence.
(2) ̸⊢G and ̸⊢¬ G for all G¨odel sentences G.
Rosser’s contribution to IT1 was an ingenious trick for weakening the ω-
consistency assumption into plain consistency—as such, it is usually seen as a
strict improvement over G¨odel’s version. While this is true for the concrete case
of FOL theories extending arithmetic, from an abstract perspective the situation
is more nuanced: The improvement is achieved at the cost of asking more from
the logic. Our framework makes this trade-oﬀclearly visible. The idea is to
use Rosser sentences instead of G¨odel sentences to “repair” the ω-consistency
assumption of Theorem 9 (inherited from Proposition 8):
Theorem 10. (IT1 `a la Rosser)
Assume Con, Ord1 , Ord2 , Repr¬ , Rel⊩
⊢,
Repr⊩, Clean⊩, and ReprS. Then:
(1) There exists a Rosser sentence.
(2) ̸⊢R and ̸⊢¬ R for all Rosser sentences R.
Highlighted is the assumption trade-oﬀbetween the two versions: Rosser’s
weakening of ω-consistency into consistency is paid by additionally assuming
representability of negation and an order-like relation satisfying Ord1 and Ord2.
Certainly, negation representability is not a big price, since for concrete logics
this tends to be a lemma that is anyway needed when proving HBL1. On the
other hand, the ordering assumptions seem to be a signiﬁcant generality gap in
favor of G¨odel’s version. A clear manifestation of this gap is in our inference of
a semantic version of IT1—which we obtain from Theorem 9 with the help of
Lemmas 3(2) and 4:
Theorem 11. (Semantic IT1) Assume RelP
⊢⃝, ComplP , Compl¬P , Soundness,
Syn|= , HBL1 , HBL⇐
1 , and ReprS. Then:
(1) There exists a G¨odel sentence.
(2) |= G, ̸⊢G, and ̸⊢¬ G for all G¨odel sentences G.
We have highlighted the assumptions speciﬁc to the semantic treatment.
They replace OCon, Rel⊩
⊢, Repr⊩and Clean⊩from the proof-theoretic Theorem 9.
Also highlighted is the additional fact concluded: that the G¨odel sentences are
true.
We have inferred the semantic version from G¨odel’s proof-theoretic version
(Theorem 9), and not from Rosser’s variation (Theorem 10). This is because in
the semantic version ω-consistency comes for free (from Lemma 3(2)). By con-
trast, for deploying Rosser’s version we would need to explicitly consider the
order-like relation with its own hypotheses. This would have led to a strictly less
general abstract result (if we ignore the diﬀerence in the way G¨odel and Rosser
sentences are actually deﬁned).
The semantic IT1 relies on HBL⇐
1 . If we commit to classical logic (i.e., assume
⊢¬ ¬ ϕ →ϕ), we can more directly show, taking advantage of HBL⇐
1 , that the
G¨odel sentences are not disprovable, which immediately proves IT1:

454
A. Popescu and D. Traytel
Theorem 12. (Classical IT1) Assume classical logic, Con, HBL1, HBL⇐
1 , ReprS.
Then:
(1) There exists a G¨odel sentence.
(2) ̸⊢G and ̸⊢¬ G for all G¨odel sentences G.
Classical logic also oﬀers two alternatives to our semantic Theorem 11 (where
the second is strictly more general than the ﬁrst):
Theorem 13. (Classical Semantic IT1)
The conclusions of Theorem 11 still
hold if we assume classical logic and perform either of the following changes in
its assumptions: (1) remove Compl¬P, or (2) replace RelP
⊢⃝, ComplP and Compl¬P
with “|= ⊢⃝⟨ϕ⟩implies ⊢ϕ for all ϕ ∈Sen.”
Even though IT1 needs a predicate
⊢⃝that satisﬁes HBL1 (and sometimes
also HBL⇐
1 , meaning that it weakly represents provability), its conclusion, the
existence of undecided sentences, is meaningful regardless of whether
⊢⃝ade-
quately expresses provability. By contrast, the meaning of IT2’s conclusion,
the theory cannot prove its own consistency, relies on this (non-mathematical)
“intensional” assumption [2]. In this case, consistency is adequately expressed
by the sentence ¬ ⊢⃝⟨⊥⟩. The standard formulation (and proof) of IT2 uses all
three derivability conditions:
Theorem 14. (IT2)
Assume Con, HBL1, HBL2, HBL3 and ReprS. Then ̸⊢
¬ ⊢⃝⟨⊥⟩.
3.3
Jeroslow’s Approach
Next we study an alternative line of reasoning due to Jeroslow [17], often cited
as a simpliﬁcation of the canonical route to prove IT2 [33,38,39]. To study
its features and pitfalls, we need some standard notation used by Jeroslow. A
pseudo-term is a formula ϕ ∈Fmlam+1 expressing a provably functional relation
via “exists unique”: ⊢∀x1, . . . , xm. ∃!y. ϕ(x1, . . . , xm, y). We only discuss the case
m = 2; the general case is similar.
Notation 15. Given a pseudo-term ϕ ∈Fmla2, we treat it as if it is a one-
variable term:
– for any terms s and t, we write t ≡ϕ(s) instead of ϕ(s, t);
– for any term s and formula ψ ∈Fmla1, we write ψ(ϕ(s)) instead of ∃y. ϕ(s, y)∧
ψ(y).
This notation smoothly integrates pseudo-terms with terms: If ⊢t ≡ϕ(s)
and ⊢ψ(ϕ(s)) then ⊢ψ(t), where ψ(t) denotes actual substitution of terms in
formulas.
Jeroslow relies on an abstract class of m-ary functions, Fm ⊆Numm →Num,
for all arities m ∈N, on which he considers the following assumptions:

A Formally Veriﬁed Abstract Account of G¨odel’s Incompleteness Theorems
455
ReprF: Every f ∈Fm is represented by some pseudo-term
f⃝∈Fmlam+1 under
the identity encoding Num →Num.
CapN: Some N ∈F1 correctly captures negation: N⟨ϕ⟩= ⟨¬ ϕ⟩for all ϕ ∈Sen.
CapSS: Some ssap : Fmla1 →F1 correctly captures substituted self-application:
ssap ψ ⟨f⃝⟩= ⟨ψ( f⃝⟨f⃝⟩) ⟩for all ψ ∈Fmla1 and f ∈F1.
In CapSS, following Jeroslow we employed Notation 15 taking advantage of
the fact that
f⃝are pseudo-terms: The highlighted text denotes ∃y.
f⃝(⟨f⃝⟩, y) ∧
ψ(y). Moreover, using the same notation, the statement of ReprF for some f ∈F1
and n ∈Num would be written as ⊢f(n) ≡
f⃝(n). Similarly, combining CapN
with the instance of ReprF, we obtain a fact that can be written as ⊢⟨¬ ϕ⟩≡
N⃝⟨ϕ⟩.
When our logical theory is a recursive extension of Robinson arithmetic and
Num = N, Fm could be the set of m-ary computable functions. Then every f ∈Fm
would indeed be represented by a formula
f⃝. Moreover, assuming a computable
and injective encoding of formulas, ⟨⟩: Fmla1 →N, we can take N : N →N
to be the following computable function: Given input n, it checks if n has the
form ⟨ϕ⟩; if so, it returns ⟨¬ ϕ⟩; if not, it returns any value (e.g., 0). And ssap ψ
can be deﬁned similarly, obtaining the desired property for every ϕ ∈Fmla2,
not necessarily of the form
f⃝. In short, Jeroslow’s assumptions cover arithmetic
(but also potentially many other systems).
At the heart of Jeroslow’s approach lies an alternative diagonalization tech-
nique, producing term ﬁxpoints, not just formula ﬁxpoints:
Lemma 16. Assume CapSS and ReprF and let ψ ∈Fmla1. Then there exists a
closed pseudo-term t such that ⊢t ≡⟨ψ(t)⟩. Moreover, taking ϕ = ψ(t), we have
⊢ϕ ←→ψ⟨ϕ⟩.
Proof. Let f = ssap ψ and t =
f⃝⟨f⃝⟩. From CapSS, we obtain f⟨f⃝⟩=
⟨ψ( f⃝⟨f⃝⟩)⟩. From this and ReprF, we obtain ⊢
f⃝⟨f⃝⟩≡⟨ψ( f⃝⟨f⃝⟩)⟩, i.e.,
⊢t ≡⟨ψ(t)⟩. With the equality rules, we obtain ⊢ψ(t) ←→ψ(⟨ψ(t)⟩), i.e., ⊢ϕ
←→ψ⟨ϕ⟩.
⊓⊔
This lemma oﬀers us G¨odel and Rosser sentences, which can be used like in
Sects. 3.1 and 3.2, leading to corresponding variants of IT1. But Jeroslow’s main
innovation aﬀects IT2: While traditionally IT2 requires all three derivability
conditions, Jeroslow’s version does not make use of the second, HBL2:
Theorem 17. (IT2 `a la Jeroslow) Assume Con, HBL1, SHBL3 , ReprF, CapN,
CapSS. Then ̸⊢jcon , where jcon denotes ∀x. ¬ ( ⊢⃝(x) ∧⊢⃝( N⃝(x))).
Like with Rosser’s trick, we analyze this innovation’s trade-oﬀs from an
abstract perspective. A ﬁrst trade-oﬀis in the employment of a stronger ver-
sion of the third condition, SHBL3 (extended to aﬀect all closed pseudo-terms
via Notation 15). Another is in the way consistency is expressed in the logic.
Jeroslow does not conclude ̸⊢¬ ⊢⃝⟨⊥⟩, but something more elaborate, namely

456
A. Popescu and D. Traytel
̸⊢jcon. While the formula ¬ ⊢⃝⟨⊥⟩internalizes the statement ̸⊢⊥, jcon internal-
izes the equivalent statement “for all ϕ, it is not the case that ⊢ϕ and ⊢¬ ϕ.” But
are the internalizations themselves equivalent, i.e., is it the case that ⊢¬ ⊢⃝⟨⊥⟩
iﬀ⊢jcon? This surely holds for many concrete logics, but it is one direction
that we can infer logic-independently: Assuming HBL1, ReprF and CapN, ⊢jcon
implies ⊢¬ ⊢⃝⟨⊥⟩. And it seems we cannot infer the other direction without
knowing what
⊢⃝looks like more concretely. Therefore, ̸⊢¬ ⊢⃝⟨⊥⟩, the conclu-
sion of the original IT2, is abstractly stronger than, hence preferable to ̸⊢jcon.
In short, Jeroslow somewhat weakens the theorem’s conclusion.
Let us now look at (a slight rephrasing of) Jeroslow’s proof:
Proof of Theorem 17. We assume (1) ⊢jcon and aim to reach a contradiction.
– Applying Lemma 16 to
⊢⃝( N⃝(x)), obtain a closed term t where (2) ⊢t ≡
⟨⊢⃝( N⃝(t))⟩.
– By SHBL3 applied to
N⃝(t), we obtain ⊢⊢⃝( N⃝(t)) →⊢⃝⟨⊢⃝( N⃝(t))⟩.
– From (2) and the equality rules, we obtain ⊢⊢⃝( N⃝(t)) →⊢⃝( N⃝⟨⊢⃝( N⃝(t))⟩).
– The last two facts give us ⊢ϕ →⊢⃝⟨ϕ⟩∧⊢⃝( N⃝⟨ϕ⟩), where ϕ denotes ⊢⃝( N⃝(t)).
– On the other hand, (1) instantiated with ⟨ϕ⟩gives us ⊢¬ ( ⊢⃝⟨ϕ⟩∧⊢⃝( N⃝⟨ϕ⟩)).
– From the last two facts, we obtain (3) ⊢¬ ϕ.
– With HBL1, we obtain ⊢
⊢⃝⟨¬ ϕ⟩and with CapN and ReprF, we obtain ⊢
⊢⃝( N⃝⟨ϕ⟩).
– From (2) and the equality rules, we obtain ⊢
⊢⃝( N⃝⟨⊢⃝( N⃝(t))⟩) →
⊢⃝( N⃝(t)),
i.e., ⊢⊢⃝( N⃝⟨ϕ⟩) →ϕ
– From the last two facts, we obtain ⊢ϕ. With (3) this contradicts (1).
⊓⊔
A ﬁrst major observation is that, under the stated assumptions, the above
proof is incorrect. It uses an implicit assumption, hidden under Notation 15:
When we disambiguate the notation, we see that Lemma 16 gives us a pseudo-
term t that does not exactly satisfy (1) ⊢t ≡⟨ψ(t)⟩(which is what the the-
orem’s proof needs), but something weaker, namely (2) ⊢t ≡⟨χ⟩, where χ is
⊢∃x.
f⃝(⟨f⃝⟩, x)∧ψ(x). And although ⊢χ ←→ψ(t), we still cannot infer (1) from
(2), unless the encodings of provably equivalent formulas are assumed provably
equal. But this assumption is unreasonable: Usually formula equivalence is unde-
cidable, so no computable encoding can achieve that. (Incidentally, this problem
is also the reason why we need SHBL3 instead of HBL3: In the proof’s applica-
tion of SHBL3 to obtain ⊢⊢⃝( N⃝(t)) →⊢⃝⟨⊢⃝( N⃝(t))⟩, we cannot work with ⟨¬ ϕ⟩
instead of
N⃝(t), even though ⊢⟨¬ ϕ⟩≡N⃝(t).)
To repair that, we can replace representation by pseudo-terms with actual
term-representation. More precisely (also factoring in the observation that
Jeroslow’s proof does not need Fn for all n, but F1 suﬃces), we change ReprF
into:
ReprF: Every f ∈F1 is term-represented, under the identity encoding Num →
Num, by some
f⃝taken from a set Ops ⊆(Term →Term) for which an
encoding as numerals ⟨⟩: Ops →Num is given, and such that FVars(g(t)) =
FVars(t) and (g(t))[s/x] = g(t[s/x]) for all g ∈Ops, s, t ∈Term and x ∈Var.

A Formally Veriﬁed Abstract Account of G¨odel’s Incompleteness Theorems
457
(In concrete logics, the elements of Ops can be constructors or derived operators
on terms.) Then CapSS, Lemma 16, and all proofs work with terms rather than
pseudo-terms and everything becomes formally correct. In summary, Jeroslow’s
approach to IT2 seems to fail for pseudo-terms representing computable func-
tions, but to require actual terms. This usually means that the logic has built-in
Skolem symbols and axioms.
Finally, let us see what it takes to alleviate the second trade-oﬀ: from ̸⊢jcon to
the more desirable ̸⊢¬ ⊢⃝⟨⊥⟩. We see that Theorem 17’s proof uses ⊢jcon not at
jcon’s full generality but only instantiated with formula encodings, which thanks
to ReprF and CapN would follow from (*) ⊢¬ ( ⊢⃝⟨ϕ⟩∧⊢⃝⟨¬ ϕ⟩). And it only takes
WHBL2 (a weaker version of HBL2) and HBL4 to prove ⊢( ⊢⃝⟨ϕ⟩∧⊢⃝⟨¬ ϕ⟩) →
⊢⃝⟨⊥⟩, allowing us to infer (*) from ⊢¬ ⊢⃝⟨⊥⟩; meaning that the latter could
have been used. We obtain:
Theorem 18. If in the (corrected) Theorem 17 we additionally assume WHBL2
and HBL4, its conclusion can be upgraded to ̸⊢¬ ⊢⃝⟨⊥⟩.
Whether WHBL2 and HBL4 are a good trade-oﬀfor HBL2 will of course
depend on the logic’s speciﬁcity, in particular, on its primitive rules of inference.
Jeroslow presented his approach for an abstract logical theory over a FOL
language, which is not necessarily a FOL theory—so it found a natural ﬁt in
our generic framework. To our knowledge, very few subsequent authors present
Jeroslow’s approach rigorously, and none at its original level of generality.
Smith’s monograph gives a rigorous account for arithmetic [38, §33], silently
performing the correction we have shown here, but failing to detect the need for
SHBL3 instead of HBL3 (which Jeroslow had noticed). A mechanical prover is of
invaluable help with detecting such nuances and pitfalls.
Summary. Using our generic infrastructure (Sect. 2), we have formally proved
several abstract incompleteness results. They include four versions of IT1:
– G¨odel’s original IT1 (Theorem 9) and an IT1 based on classical logic (The-
orem 12) required the formalization of some well-known arguments without
change.
– Rosser’s IT1 (Theorem 10) involved the generalization of a well-known argu-
ment: distilling two abstract conditions, Ord1 and Ord2.
– Novel semantic variants of IT1 (Theorems 11 and 13) were born from
abstractly connecting standard models, the “iﬀ” version of HBL1 and proof
representability.
They also include two versions of IT2:
– The standard IT2 based on the three derivability conditions (Theorem 14)
again only required formalizing a well-known argument.
– The alternative, Jeroslow-style IT2 (Theorems 17 and 18) involved a detailed
analysis and correction of an existing abstract result.

458
A. Popescu and D. Traytel
4
Instances of the Abstract Results
We ﬁrst validate the assumptions about our abstract logic and arithmetic:
Proposition 19. (1) Any FOL theory that extends Robinson arithmetic or the
HF set theory satisﬁes all the axioms in our logical and arithmetical substrata
(in Sects. 2.1 and 2.2).
(2) If, in addition, the theory is sound, then, together with its corresponding
standard model, it also satisﬁes all our model-theoretic axioms (in Sect. 2.5).
In particular, point (2) shows that our discussion of standard models applies
equally well to N and the datatype of HF sets. (In the latter case, Num becomes
the entire set of closed terms, so that numerals can denote arbitrary HF sets. This
shows the versatility of our abstract concept of numeral.) Then we instantiate
three of our main theorems:
Theorem 20. (1) Any FOL theory that extends the HF set theory with a ﬁnite
set of axioms and is sound in the standard HF set model satisﬁes the hypothe-
ses of Theorems 13 and 14. Hence IT1 (semantic version) and IT2 hold for it.
(2) Any FOL theory that extends the HF set theory with a ﬁnite set of axioms
and is consistent satisﬁes Theorem 14’s hypotheses. Hence IT2 holds for it.
These instances are heavily based on the lemmas proved by Paulson in his
formalization of IT1 and IT2 [26,27], who follows and corrects ´Swierczkowski’s
detailed informal account [40]. Point (1) is a restatement of Paulson’s formalized
results: theorems Goedel I and Goedel II in [27]. (His theorems also assume con-
sistency, but that is redundant: Consistency follows from his underlying sound-
ness assumption.)
By contrast, point (2) is an upgrade of Paulson’s Goedel II, applicable to
any consistent, though possibly unsound theory. This stronger version is in fact
IT2’s standard form, free from any model-theoretic considerations. Paulson had
proved both HBL1 and HBL⇐
1
taking advantage of soundness, so we needed to
discard HBL⇐
1
and re-prove HBL1 by replacing any semantic arguments with
proofs within the HF calculus. We also removed all invocations of a convenient
“truth implies provability for Σ-sentences” lemma, which depended on soundness
due to Paulson’s choice of Σ-sentence deﬁnition.
This instantiation process has oﬀered important feedback into the abstract
results. A formal development such as ours is (largely) immune to reasoning
errors, but not to missing out on useful pieces of generality. We experienced this
ﬁrsthand with our assumptions about substitution. An a priori natural choice
was to assume representability of the numeral substitution Sb : Fmla1 × Num →
Sen (deﬁned as Sb(ϕ, n) = ϕ(n)), part of which means (1) ⊢Sb
⃝(⟨ϕ⟩, n, Sb(ϕ, n)).
But Paulson had instead proved (2) ⊢Sb
⃝(⟨ϕ⟩, ⟨n⟩, Sb(ϕ, n)). The key diﬀerence
from (1) is that (2) applies the term encoding function ⟨⟩: Term →Num to
numerals as well (as particular terms); and since his ⟨⟩function is injective, it is
far from the case that ⟨n⟩= n for all numerals n. Paulson’s version makes more
sense than ours when building the results bottom-up: Representability should not

A Formally Veriﬁed Abstract Account of G¨odel’s Incompleteness Theorems
459
discriminate numerals, but ﬁlter them through the encodings like other terms.
However, top-down our version also made sense: It yielded the incompleteness
theorems under reasonable assumptions, which do hold, by the way, for the
HF set theory—even though in a bottom-up development one is unlikely to
prove them. We resolved this discrepancy through a common denominator: the
representability of self-substitution S : Fmla1 →Sen (Sect. 2.3), which made our
results more general.
Paulson’s formalization has also inspired our abstract treatment of stan-
dard models (Sect. 2.5). Since Paulson proves HBL⇐
1 and uses classical logic, an
obvious “port of entry” of his IT2 into our framework is Theorem 12. But this
theorem tells us nothing about the G¨odel sentences’ truth. Delving deeper into
Paulson’s proof, we noted that he (unconventionally) completely avoids Repr⊩,
and does not even deﬁne ⊩. This raised the question of whether HBL⇐
1
and
Repr⊩are somehow interchangeable in the presence of standard models—and we
found that they indeed are, under mild assumptions about truth. Incidentally,
these assumptions were also suﬃcient for establishing the G¨odel sentences’ truth,
leading to our semantic IT1 (Theorem 11). However, Theorem 11 was not easy
to instantiate to Paulson’s IT1. All its assumptions were easy to prove, except
for Compl¬P. Whereas Paulson proved that his proof-of predicate is a Σ-formula
(which implies ComplP by Σ-completeness), he did not prove the same for its
negation (which would imply Compl¬P). We are conﬁdent that this is true (any
reasonable proof-of predicate is a Δ-formula), but we leave the laborious formal
proof of this fact as future work. Instead, we recovered Paulson’s result as an
instance of our Theorem 13.
As future work, we will consider even more general variants of our semantic
Theorems 11 and 13, as in Smorynski’s account [39]: by distinguishing between
a sound “base” provability relation ⊢0 and an extension ⊢only required to be
consistent or ω-consistent. For example, ⊢0 could be deduction in HF set theory
or a weaker theory and ⊢deduction in a consistent (not necessarily sound) exten-
sion of the HF set theory. This two-layered approach would have also beneﬁted
Paulson’s original formalization.
Many other logics and logical theories satisfy our theorems’ assumptions. We
do not require the logic to be reducible to a single syntactic category of formulas,
Fmla, a single syntactic judgment, ⊢, etc.; but only that such (well-behaved) for-
mulas, provability relation, etc. are identiﬁable as part of that logic, e.g., localized
to a given type and/or relativised by a given predicate. This allows our frame-
work to capture most variants of higher-order logic and type theory (including
the variant underlying Isabelle/HOL itself [21,22]), and also, we believe, many of
the logics surveyed by Buldt [5], including non-classical and fuzzy. But enabling
“mass instantiation” that is both formal and painless requires more progress on
the agenda we started here: recognizing reusable construction and proof patterns
and formalizing them as abstract results.
Acknowledgments. We thank Bernd Buldt for his patient explanations on material
in his monograph, and the reviewers for insightful comments and suggestions.

460
A. Popescu and D. Traytel
References
1. Ammon, K.: An automatic proof of G¨odel’s incompleteness theorem. Artif. Intell.
61(2), 291–306 (1993)
2. Auerbach, D.: Intensionality and the G¨odel theorems. Philos. Stud. Int. J. Philos.
Anal. Tradit. 48(3), 337–351 (1985)
3. Blanchette, J.C., Popescu, A., Traytel, D.: Uniﬁed classical logic completeness. In:
Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS (LNAI), vol. 8562,
pp. 46–60. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08587-6 4
4. Boolos, G.: The Logic of Provability. Cambridge University Press, Cambridge
(1993)
5. Buldt, B.: The scope of G¨odel’s ﬁrst incompleteness theorem. Log. Univers. 8(3),
499–552 (2014)
6. Bundy, A., Giunchiglia, F., Villaﬁorita, A., Walsh, T.: An incompleteness theorem
via abstraction. Technical report, Istituto per la Ricerca Scientiﬁca e Tecnologica,
Trento (1996)
7. Carnap, R.: Logische syntax der sprache. Philos. Rev. 44(4), 394–397 (1935)
8. Davis, M.: The Undecidable: Basic Papers on Undecidable Propositions, Unsolvable
Problems, and Computable Functions. Dover Publication, Mineola (1965)
9. Diaconescu, R.: Institution-Independent Model Theory, 1st edn. Birkh¨auser, Basel
(2008)
10. Feferman, S., Dawson Jr., J.W., Kleene, S.C., Moore, G.H., Solovay, R.M., van
Heijenoort, J. (eds.): Kurt G¨odel: Collected Works. Vol. 1: Publications 1929–1936.
Oxford University Press, Oxford (1986)
11. Fiore, M.P., Plotkin, G.D., Turi, D.: Abstract syntax and variable binding. In:
Logic in Computer Science (LICS) 1999, pp. 193–202. IEEE Computer Society
(1999)
12. Gabbay, M.J., Mathijssen, A.: Nominal (universal) algebra: equational logic with
names and binding. J. Log. Comput. 19(6), 1455–1508 (2009)
13. G¨odel, K.: ¨Uber formal unentscheidbare S¨atze der Principia Mathematica und
verwandter Systeme I. Monatshefte f¨ur Mathematik und Physik 38(1), 173–198
(1931)
14. Goguen, J.A., Burstall, R.M.: Institutions: abstract model theory for speciﬁcation
and programming. J. ACM 39(1), 95–146 (1992)
15. Harrison, J.: HOL light proof of G¨odel’s ﬁrst incompleteness theorem. http://code.
google.com/p/hol-light/, directory Arithmetic
16. Hilbert, D., Bernays, P.: Grundlagen der Mathematik, vol. II. Springer, Heidelberg
(1939)
17. Jeroslow, R.G.: Redundancies in the Hilbert-Bernays derivability conditions for
G¨odel’s second incompleteness theorem. J. Symb. Log. 38(3), 359–367 (1973)
18. Kaliszyk, C., Urban, J.: HOL(y)Hammer: online ATP service for HOL light. Math.
Comput. Sci. 9(1), 5–22 (2015)
19. Kikuchi, M., Kurahashi, T.: Generalizations of G¨odel’s incompleteness theorems
for  n-deﬁnable theories of arithmetic. Rew. Symb. Logic 10(4), 603–616 (2017)
20. Kossak, R.: Mathematical Logic. SGTP, vol. 3. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-97298-5
21. Kunˇcar, O., Popescu, A.: A consistent foundation for Isabelle/HOL. In: Urban, C.,
Zhang, X. (eds.) ITP 2015. LNCS, vol. 9236, pp. 234–252. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-22102-1 16

A Formally Veriﬁed Abstract Account of G¨odel’s Incompleteness Theorems
461
22. Kunˇcar, O., Popescu, A.: Comprehending Isabelle/HOL’s consistency. In: Yang,
H. (ed.) ESOP 2017. LNCS, vol. 10201, pp. 724–749. Springer, Heidelberg (2017).
https://doi.org/10.1007/978-3-662-54434-1 27
23. L¨ob, M.: Solution of a problem of Leon Henkin. J. Symb. Log. 20(2), 115–118
(1955)
24. Nipkow, T., Wenzel, M., Paulson, L.C. (eds.): Isabelle/HOL. LNCS, vol. 2283.
Springer, Heidelberg (2002). https://doi.org/10.1007/3-540-45949-9
25. O’Connor, R.: Essential incompleteness of arithmetic veriﬁed by Coq. In: Hurd,
J., Melham, T. (eds.) TPHOLs 2005. LNCS, vol. 3603, pp. 245–260. Springer,
Heidelberg (2005). https://doi.org/10.1007/11541868 16
26. Paulson, L.C.: A machine-assisted proof of G¨odel’s incompleteness theorems for
the theory of hereditarily ﬁnite sets. Rew. Symb. Logic 7(3), 484–498 (2014)
27. Paulson, L.C.: A mechanised proof of G¨odel’s incompleteness theorems using Nom-
inal Isabelle. J. Autom. Reason. 55(1), 1–37 (2015)
28. Paulson, L.C., Blanchette, J.C.: Three years of experience with Sledgehammer,
a practical link between automatic and interactive theorem provers. In: The 8th
International Workshop on the Implementation of Logics, IWIL 2010, Yogyakarta,
Indonesia, 9 October 2011, pp. 1–11 (2010)
29. Popescu, A., Ro¸su, G.: Term-generic logic. Theor. Comput. Sci. 577, 1–24 (2015)
30. Popescu, A., Trayel, D.: A formally veriﬁed abstract account of G¨odel’s incomplete-
ness theorems (extended report) (2019). https://bitbucket.org/traytel/abstract
incompleteness/downloads/report.pdf
31. Popescu, A., Traytel, D.: Formalization associated with this paper (2019). https://
bitbucket.org/traytel/abstract incompleteness/
32. Quaife, A.: Automated proofs of L¨ob’s theorem and G¨odel’s two incompleteness
theorems. J. Autom. Reason. 4(2), 219–231 (1988)
33. Raatikainen, P.: G¨odel’s incompleteness theorems. In: The Stanford Encyclopedia
of Philosophy. Metaphysics Research Lab, Stanford University (2018)
34. Schlichtkrull, A., Blanchette, J.C., Traytel, D., Waldmann, U.: Formalizing Bach-
mair and Ganzinger’s ordered resolution prover. In: Galmiche, D., Schulz, S., Sebas-
tiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 89–107. Springer, Cham
(2018). https://doi.org/10.1007/978-3-319-94205-6 7
35. Shankar, N.: Metamathematics, Machines, and G¨odel Proof. Cambridge University
Press, Cambridge (1994)
36. Sieg, W.: Elementary proof theory. Technical report, Institute for Mathematical
Studies in the Social Sciences, Stanford (1978)
37. Sieg, W., Field, C.: Automated search for G¨odel’s proofs. Ann. Pure Appl. Logic
133(1–3), 319–338 (2005)
38. Smith, P.: An Introduction to G¨odel’s Incompleteness Theorems. Cambridge Uni-
versity Press, Cambridge (2007)
39. Smorynski, C.: The incompleteness theorems. In: Barwise, J. (ed.) Handbook of
Mathematical Logic, pp. 821–865. North-Holland, Amsterdam (1977)
40. ´Swierczkowski, S.: Finite sets and G¨odel incompleteness theorems. Diss. Math.
422, 1–58 (2003)
41. Tarski, A., Mostowski, A., Robinson, R.: Undecidable Theories. Studies in Logic
and the Foundations of Mathematics. North-Holland, Amsterdam (1953). 3rd edn.
1971

Old or Heavy? Decaying Gracefully
with Age/Weight Shapes
Michael Rawson(B) and Giles Reger
University of Manchester, Manchester, UK
michael@rawsons.uk
Abstract. Modern saturation theorem provers are based on the given-
clause algorithm, which iteratively selects new clauses to process. This
clause selection has a large impact on the performance of proof search
and has been the subject of much folklore. The standard approach is
to alternate between selecting the oldest clause and the lightest clause
with a ﬁxed, but conﬁgurable age/weight ratio (AWR). An optimal ﬁxed
value of this ratio is shown to produce proofs signiﬁcantly more quickly
on a given problem, and further that varying AWR during proof search
can improve upon a ﬁxed ratio. Several new modes for the Vampire
prover which vary AWR according to a “shape” during proof search are
developed based on these observations. The modes solve a number of
new problems in the TPTP benchmark set.
1
Introduction
Currently, the most successful theorem provers (such as Vampire [4], E [12],
and SPASS [17]) for ﬁrst-order logic are saturation-based, utilising the well-
known given-clause algorithm. Simply, this algorithm saturates a set of clauses by
iteratively selecting a clause and performing all non-redundant inferences with it
until all clauses have been selected or the empty clause (witnessing inconsistency)
has been found. Clearly, the order in which clauses are selected is key to the
performance of the algorithm. Over the past few decades a certain amount of
folklore has built up around the best methods for clause selection and recent work
by Schulz and M¨ohrmann [13] systematically studied these. Our work extends
this study with new results and also introduces the concept of a variable clause
selection strategy (one that changes over time), instantiated with two simple
patterns (or shapes) that prove to be pragmatically useful.
Clause selection strategies that alternate between selecting clauses based on
age (i.e. in a ﬁrst-in-ﬁrst-out manner) and weight (i.e. those with the fewest
symbols ﬁrst) are the subject of this work. It was conﬁrmed by Schulz and
M¨ohrmann that alternating these two heuristics outperforms either by itself. The
ratio of these selections is the age/weight ratio (AWR), as this is the terminology
employed by the Vampire theorem prover, the vehicle for our study.
After covering relevant background material in Sect. 2 the remainder of the
paper makes two main contributions. Firstly, Sect. 3 experimentally conﬁrms the
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 462–476, 2019.
https://doi.org/10.1007/978-3-030-29436-6_27

Old or Heavy? Decaying Gracefully with Age/Weight Shapes
463
folklore that (i) the choice of age/weight ratio often has a signiﬁcant eﬀect on the
performance of proof search, and (ii) there is no “best” age/weight ratio: indeed,
a large range of pragmatically useful ratios exist. Section 4.1 demonstrates that
varying the age-weight ratio over time can achieve better performance than a
ﬁxed ratio, and therefore motivates the addition of so-called age/weight shapes
for varying the ratio over time. Experiments (Sect. 5) with these new options
implemented in the Vampire theorem prover show a signiﬁcant improvement in
coverage, proving many new problems unsolvable by any previous conﬁguration
of Vampire.
2
Background
This section introduces the relevant background for the rest of the paper.
First-Order Logic and Weight. Our setting is the standard ﬁrst-order predicate
logic with equality. A formal deﬁnition of this logic is not required for this
paper but an important notion is that of the weight of a clause. In ﬁrst-order
logic, terms are built from function symbols and variables, literals are built from
terms, and clauses are disjunctions of literals. The weight of a term/literal is the
number of symbols (function, variable, or predicate) occurring in it. The weight
of a clause is the sum of the weights of its literals.
Saturation-Based Proof Search. Saturation-based theorem provers saturate a
set of clauses S with respect to an inference system I: that is, computing a
set of clauses S′ by applying rules in I to clauses in S until no new clauses
are generated. If the empty clause is generated then S is unsatisﬁable. Calculi
such as resolution and superposition have conditions that ensure completeness,
which means that a saturated set S is satisﬁable if it does not contain the
empty clause as an element. As ﬁrst-order logic is only semi-decidable, it is not
necessarily the case that S has a ﬁnite saturation, and even if it does it may be
unachievable in practice using the available resources. Therefore, much eﬀort in
saturation-based ﬁrst-order theorem proving involves controlling proof search to
make ﬁnding the empty clause more likely (within reasonable resource bounds).
One important notion is that of redundancy, being able to remove clauses from
the search space that are not required. Another important notion are literal
selections that place restrictions on the inferences that can be performed. Both
notions come with additional requirements for completeness. Vampire often gives
up these requirements for pragmatic reasons (incomplete strategies have been
found to be more eﬃcient than complete ones in certain cases) and in such cases
the satisﬁability of S upon saturation is unknown.
The Given Clause Algorithm and AWR Clause Selection. To achieve saturation,
the given clause algorithm organises the set of clauses into two sets: the active
clauses are those that have been active in inferences, and the passive clauses are
those that have not. Typically, a further unprocessed set is required in order to

464
M. Rawson and G. Reger
manage the clauses produced during a single iteration of the loop. Realisations of
the given clause algorithm generally diﬀer in how they organise simpliﬁcations.
There are two main approaches (both implemented by Vampire, originally found
in the eponymous theorem provers Otter [5] and Discount [1]): the Otter loop
uses both active and passive for simpliﬁcations, whereas the Discount loop uses
only active.
The algorithm is centred around the clause selection process. As previously
mentioned, there are two main heuristics for this:
– By Age (or First-in/First-out) clause selection prefers the oldest clause (pro-
duced earlier in proof search), simulating a breadth-ﬁrst search of the clause
space. In Vampire the age of a clause is the number of inferences performed
to produce it (input clauses have age 0).
– By Weight (or symbol-counting) clause selection prefers the lightest clause.
The intuition behind this approach is that the sought empty clause has zero
symbols and lighter clauses are in some sense closer to this. Furthermore,
lighter clauses are more general in terms of subsumption and tend to have
fewer children, making them less explosive in terms of proof search.
Schulz and M¨ohrmann show that alternating these heuristics is beneﬁcial. In
Vampire this alternation is achieved by an age/weight ratio (AWR) implemented
by a simple balancing algorithm. The balance is initialised to 0 and used as
follows: a negative balance means that a clause should be selected by age, whereas
a positive balance means that a clause should be selected by weight; given a ratio
of a : w the balance is incremented by a when selecting by age and decremented
by w when selecting by weight. Figure 1 gives the Discount algorithm along with
balance-based AWR clause selection. The lines relevant to clause selection are
marked with ✓.
Portfolio Solvers. Vampire is a portfolio solver and is typically run in a mode
that attempts multiple diﬀerent strategies in quick succession, e.g. in a 30 s run it
may attempt 10 or more diﬀerent strategies, and may run these in parallel with
diﬀerent priorities [8]. These strategies employ many diﬀerent options including
diﬀerent saturation algorithms (including Otter and Discount), preprocessing
options, literal selection strategies, inference rules, and clause selection heuristics.
The portfolio mode is a signiﬁcant improvement on any single strategy.
Vampire’s portfolio mode also includes an additional option relevant to clause
selection: the --nongoal weight coefficient option speciﬁes a multiplier to
apply to the weight of non-goal clauses, thus preferring clauses in or derived from
the problem conjecture in clause selection. Use of this heuristic is orthogonal to
the age/weight ratio and is not investigated further here.
Related Work. Many clause selection approaches are taken by other solvers.
Otter 3.3 [6] selects either by age, by weight or manually. Prover9 [7] allows a
conﬁgurable age/weight ratio. E [12] allows the user to specify an arbitrary num-
ber of priority queues and a weighted round-robin scheme that determines how
many clauses are picked from each queue. The default is to use a combination

Old or Heavy? Decaying Gracefully with Age/Weight Shapes
465
input: init: set of clauses;, a : w age-weight ratio
var active, passive, unprocessed: set of clauses;
var given, new: clause;
active := ∅;
unprocessed := init;
✓
balance := 0;
loop
while unprocessed ̸= ∅
new:=pop(unprocessed);
if new = □then return unsatisﬁable;
if retained(new) then
(* retention test *)
simplify new by clauses in active;
(* forward simpliﬁcation *)
if new = □then return unsatisﬁable;
if retained(new) then
(* another retention test *)
simplify active using new ;
(* backward simpliﬁcation *)
move the simpliﬁed clauses to unprocessed;
add new to passive
if passive = ∅then return satisﬁable or unknown
✓
if balance > 0 then
✓
given := lightest clause in passive;
✓
balance:= balance −w;
✓
else
✓
given := oldest clause in passive;
✓
balance:= balance + a;
move given from passive to active;
unprocessed:=infer(given, active);
(* generating inferences *)
Fig. 1. The discount saturation algorithm with AWR clause selection
of age and weight selection, although there is also a complex strategy developed
by a genetic algorithm [11]. SPASS [17] uses symbol-counting based clause selec-
tion. iProver [3] follows E in having a number of conﬁgurable queues but relies
mainly on age and weight heuristics in those queues. The general idea in this
paper of a varying age/weight ratio over time is applicable to any ratio-based
clause selection strategy, and our speciﬁc results apply to those that take a ratio
between age and weight.
3
Optimising Age/Weight Ratios
Two assumptions from folklore are conﬁrmed experimentally:
1. The choice of age/weight ratio often has a signiﬁcant eﬀect on the performance
of proof search.
2. There is in general no single best age/weight ratio for a given set of problems.
These are supported by the work of Schulz and M¨ohrmann but are explored in
more depth here.

466
M. Rawson and G. Reger
Fig. 2. The number of given-clause loops reported by Vampire after ﬁnding a proof
with 1-s runs on a TPTP problem, PRO017+2. In between the peaks on either side, the
function of L is discontinuous with large peaks and troughs, but follows an approximate
trend and settles toward the global optimum. PRO017+2 exhibits typifying behaviour
for TPTP, based on manual inspection of several hundred such plots.
3.1
Logarithmic AWR
Visualising AWR values is more easily achieved if they have a continuous scale.
AWR values are mathematically Q+, the positive rational numbers, but in prac-
tice are more easily visualised logarithmically. Therefore, the logarithmic AWR
L is deﬁned in terms of age A and weight W as
L = log2
 A
W

As L tends to positive inﬁnity, Vampire selects only by weight, whereas if L
tends to negative inﬁnity Vampire selects only by age. L = 0 represents the
middle ground of a 1:1 age/weight ratio. Note that the balancing algorithm used
by Vampire does not make use of this value (it still requires two numbers) but
the quantity is used in this work to show continuous AWR values.
3.2
Experiments
As an initial illustrative example of how varying the AWR eﬀects the number
of clauses required to be processed before a proof is found consider Fig. 2. This
demonstrates the eﬀect that varying AWR can have: a smaller number of acti-
vations means that fewer clauses were processed, which in general means that

Old or Heavy? Decaying Gracefully with Age/Weight Shapes
467
a proof was found faster1. On the problem shown, a good AWR value is over
400% better by this metric than the worst AWR value.
Table 1. Relative performance gain, showing the ratio in activations between the best
AWR setting for a given problem and another base setting. A comparison is drawn
between 1:1 (Vampire’s default), 1:5 (the best-behaved from Schulz and M¨ohrmann),
and the worst setting for the problem. Where the problem is not solved at all by the
base setting, it is ignored.
Base setting % Maximum gain % Mean gain (Standard deviation)
1:1
13,356
126
163
1:5
13,367
144
170
(worst)
22,201
395
760
This experiment was repeated on the whole TPTP problem set, excluding
problems Vampire does not currently support (e.g. higher-order problems). Vam-
pire ran for 1 s in default mode with the discount saturation algorithm2 using a
sensible set of AWR values (see Table 2)—these are the values used in Vampire’s
portfolio mode. These tend to favour weight-ﬁrst over age-ﬁrst as this has been
experimentally shown to be preferable. Problems not solved by any of these, or
those solved trivially (e.g in preprocessing) are removed. The whole set yielded
data for 7,947 problems.
The ﬁrst result is that choosing a good AWR value for a problem is well-
rewarded. Table 1 summarises the impact that choosing the best AWR can have.
Compared to the default, Vampire can perform, on average, 1.26 times fewer
activations, which is modest but (as Table 2 shows) just under 10% of problems
are no longer proven by choosing the default. It is more relevant to note that
there are cases where Vampire can do much better by selecting a diﬀerent AWR
value. Therefore, choosing a better AWR value can go from no solution to a
solution and can do so faster, but not necessarily. In the worst case (choosing
the pessimal AWR value) Vampire performs almost 4 times as many activations.
The second result is that there is no “best” AWR across this full set of
problems. Drop in performance is deﬁned to be how many times more activations
were required for a proof under a given AWR, compared to the best AWR.
Table 2 shows, for each AWR value, the % of problems solved, the number solved
uniquely, and the maximum and mean drop in performance. No AWR value
solves all problems, with the best being 1:5. A ratio of 1:4 produces an unusually
small maximum performance drop. Schulz and M¨ohrmann found that 1:5 had a
similar property, but this might be explained by diﬀerences in prover and test
1 It should be noted that if a small number of clauses are extremely expensive to
process it may be slower than a larger number of less-expensive clauses, but in general
this is a good heuristic measure for prover performance. It also avoids reproducibility
issues involved with using system timing approaches.
2 The default LRS [10] saturation algorithm can be non-deterministic.

468
M. Rawson and G. Reger
environment. It is interesting to note that the extreme AWR values solve fewer
problems overall but solve the most uniquely. This is typical in saturation-based
proof search: approaches that do not perform well in general may perform well
in speciﬁc cases where the general approach does not.
In summary, these results conﬁrm the previous assumptions often made in
folklore. It should be noted that this is a small experiment (1 s runs in default
mode) and the relative performance of diﬀerent AWR values cannot be gener-
alised, but the general result that they are complementary can.
Table 2. Per-AWR value results on 1 s runs over 7,947 TPTP problems.
AWR
% Solved
Uniques
% Maximum drop
% Mean drop
(Standard deviation)
8:1
85.25
16
15,067
137
198
5:1
86.10
1
12,222
133
164
4:1
86.93
1
10,144
132
142
3:1
87.63
2
10,500
129
141
2:1
88.62
3
11,267
127
145
3:2
89.83
2
11,989
127
151
5:4
89.98
4
12,500
126
155
1:1
90.56
4
13,356
126
163
2:3
91.20
9
14,767
128
179
1:2
91.68
0
16,267
131
197
1:3
91.81
5
19,056
137
230
1:4
91.85
3
1,741
138
67
1:5
92.00
2
13,367
144
170
1:6
91.57
1
10,644
147
146
1:7
91.49
1
10,489
149
144
1:8
91.09
2
10,133
153
145
1:10
90.52
1
10,178
160
153
1:12
90.00
0
10,167
165
162
1:14
89.29
4
10,300
170
175
1:16
89.42
5
10,133
174
176
1:20
88.61
3
10,089
182
194
1:24
88.26
2
10,133
189
208
1:28
87.57
2
9,922
196
224
1:32
87.01
1
10,000
199
236
1:40
86.23
4
9,878
209
264
1:50
84.93
1
9,878
217
288
1:64
84.17
2
10,122
228
319
1:128
81.34
3
10,744
257
416
1:1024
73.11
23
22,201
283
755
4
Variable AWR for Vampire
This section motivates and deﬁnes a clause selection approach which varies the
AWR value over time.

Old or Heavy? Decaying Gracefully with Age/Weight Shapes
469
Fig. 3. The AWR series that produced the lowest number of activations on a particular
problem, smoothed in order to show the actual eﬀect on proof search. This is a search
strategy that a single ﬁxed AWR cannot reproduce.
4.1
The Optimal AWR over Time
Although choosing a good AWR value is important, this is covered in part by
the use of strategy scheduling in which many AWR values are tried in sequence
(along with other prover options). Additionally, given that varying the AWR
can have such a large impact, it seems likely that a constant AWR ﬁxed for the
entire proof search is unlikely to be optimal for any given problem. This can
be shown by running Vampire with a randomised sequence of age/weight ratios
given by a random walk repeatedly, then ﬁnding the best after a large number
of repetitions. Applying this method with 10,000 repetitions to the problem
seen earlier (PRO017+2) yields the example AWR trend shown in Fig. 3, which
reduces the best number of activations from 330 with a ﬁxed AWR, to 287 with
a varying AWR. Unsurprisingly, in ad-hoc experiments on other problems, the
best shape is rarely constant. This suggests that implementing other shapes,
such as an increasing or decreasing trend, might lead to quicker proofs in the
Vampire theorem prover.
4.2
Varying AWR (in Vampire)
An implementation of dynamically-varying AWR values in Vampire is described
below. In general any possible sequence that the AWR could follow during proof
search can be used. However, some details constrain the design space:
1. Changing the AWR too frequently or sharply has little eﬀect, due to the
“balancing” algorithm—see Sect. 1.

470
M. Rawson and G. Reger
Fig. 4. The new decay and converge AWR shapes as implemented in Vampire. Diﬀerent
curves exhibit the eﬀect of the AWR shape frequency setting.
2. A general (conﬁgurable) shape is more likely to be widely applicable than a
speciﬁc series of data points.
3. The shape must extend naturally to an indeﬁnitely-long proof search.
In this work two general trends are explored: a trend away from a given start
AWR toward 1:1 (“decay”), and a trend from 1:1 toward a given end AWR
(“converge”). Investigation showed that even ﬂuctuating sequences had a general
trend, and further that these two ﬁxed trends are reasonable approximations of
these trends. The start/end AWR values are taken from the portfolio mode:
these values are known to be useful in a ﬁxed-AWR context, and while this may
not generalise to a dynamic-AWR context, it is a useful starting point pending
integration of AWR shape parameters into strategy scheduling.
Since a simple linear shape does not extend well to indeﬁnite proof search (it
is unclear what should happen after either 1:1 or the target AWR is reached), an
exponential decay function is used instead. These exponential shapes are further
parameterised by an integral shape frequency setting, which controls the rate of
decay or convergence: every n steps, the diﬀerence between the current and the
target AWR is halved, rounding where necessary. In future, this might allow the
use of repeating patterns such as a sinusoid, hence frequency. Figure 4 illustrates
rates at which the new conﬁgurations converge or decay from the ﬁxed AWR
setting for some indicative frequency settings.
Our approach here was restricted by the balancing algorithm used internally,
as AWR steps must be discrete and do not take eﬀect immediately. An alternative
approach might be to use an age/weight probability, rather than a ratio, from
which age or weight decisions would be pseudo-randomly (but reproducibly)
taken with the use of a seeded pseudo-random number generator, permitting
use of continuous age/weight functions.
Two new options are implemented: --age weight ratio shape can take
the
values
constant,
decay,
or
converge
and
selects
one
of
the
above
shapes; and --age weight ratio shape frequency speciﬁes the frequency
(rate) or convergence/decay (default is 100). These are used with the exist-
ing --age weight ratio option (default 1:1) to give a number of new option
combinations, which can be used in conjunction with Vampire’s portfolio mode

Old or Heavy? Decaying Gracefully with Age/Weight Shapes
471
pending integration into the strategy schedules. This version of the prover is
currently in a separate branch in the main Vampire source repository3. Another
option, --age weight ratio b is implemented (default 1:1), controlling the ini-
tial AWR value of converge or the ﬁnal AWR value of decay.
Table 3. Results for the tested conﬁgurations. Proved refers to the total number of
problems a conﬁguration solved. Fresh is the number of problems a conﬁguration solved
which were not solved by the baseline. Uniques is the number of problems a conﬁg-
uration solved which were not solved by any other conﬁguration. u-score is a reﬁned
unique score which correlates to a conﬁguration’s utility in solving new problems, as
used in Hoder et al. [2].
Conﬁguration Frequency
Proved
Fresh Uniques u-score
Baseline
–
13,057
0
1
714.2
Converge
1
13,039
24
3
714.3
Converge
5
13,029
27
1
709.5
Converge
10
13,028
35
5
714.3
Converge
50
13,015
45
5
712.8
Converge
100
12,976
51
1
705.9
Converge
500
12,895
63
4
698.3
Converge
1000
12,837
52
0
688.6
Converge
5000
12,775
53
1
682.4
Converge
10000
12,751
53
0
678.7
Decay
1
12,698
48
1
673.6
Decay
5
12,702
51
1
674.9
Decay
10
12,698
48
1
674.2
Decay
50
12,712
49
2
679.1
Decay
100
12,726
46
1
678.8
Decay
500
12,795
29
1
685.5
Decay
1000
12,860
29
2
692.6
Decay
5000
12,982
16
2
707.1
Decay
10000
13,002
7
0
706.3
Converge
(Combined) 13,167
117
41
–
Decay
(Combined) 13,106
93
17
–
5
Experimental Evaluation
Two experiments evaluate the new techniques. The ﬁrst compares the various
options attempting to draw some conclusions about which option values work
3 https://github.com/vprover/vampire/tree/awr-shapes.

472
M. Rawson and G. Reger
well together. The second looks at how useful the new options are in the con-
text of portfolio solving. Both experiments use the TPTP (version 7.1.0) bench-
mark [16] and were run on StarExec [14].
5.1
Comparing New Options
Vampire ran in default mode (with the discount saturation algorithm) for 10 s
whilst varying age weight ratio and age weight ratio shape frequency for
several AWR shapes: constant, converging from 1:1, decaying to 1:1, converging
from 1:4 and decaying from 1:4.
Results are given in Table 4. The results for the diﬀerent shapes are grouped
into columns and then by frequency with rows giving results per AWR value.
The total number of problems solved and those solved uniquely are also reported.
The best combination of options overall was decaying from an initial age/weight
of 1:100 with frequency 1000. Longer frequencies tended to do better, suggesting
that more time at the intermediate AWR values is preferable. Unique solutions
are distributed well in general, showing that the new options are complementary.
5.2
Contribution to Portfolio
Our next experiment aims to answer the question “How much can the portfolio
mode of Vampire be improved using these new options?”. To address this the new
options ran on top of the portfolio mode used in the most recent CASC compe-
tition CASC-J9 [15]. Note that the CASC-J9 portfolio mode contains techniques
completely unrelated to the age/weight ratio, e.g. ﬁnite model building [9], as
well as other options related to clause selection, e.g. non-goal weight coeﬃcient
and set-of-support.
Vampire ﬁrst ran to establish baseline performance in the given portfolio
mode on all problems in TPTP, with a wallclock time limit of 300 s. New options
were applied on top of the portfolio mode options, using the existing AWR
values in the various strategies as the starting point. Three shapes are employed:
constant (baseline), converging from 1:1 and decaying to 1:1. The purpose is to
gauge what impact adding such options to a new portfolio mode could have.
In this experiment the aim was to ﬁnd new solved problems and identify new
strategies that could be added to a portfolio mode. Therefore, it makes sense to
consider the union of all experiments.
Overall, the baseline solved the most problems (13,057). No experimental
conﬁguration improved on this ﬁgure, but some problems not solved by baseline
were solved by the new conﬁgurations, and some entirely new problems were
solved. The union of all converge and decay conﬁgurations improved on the
baseline, with 13,167 and 13,106 solved problems respectively.
Figure 3 shows the performance in terms of solved problems of all the con-
ﬁgurations tested. These data show that conﬁgurations which select clauses in
a similar way to the baseline (i.e. slow decay or fast convergence) achieve sim-
ilar performance, as expected. In total, 134 (117 + 17, 93 + 41) problems were
solved by the new conﬁgurations that were not solved by the baseline. This is an

Old or Heavy? Decaying Gracefully with Age/Weight Shapes
473
Table 4. Number of problems solved (top) and unique problems solved (bottom) by various conﬁgurations varying start/end AWR
values, AWR shape, and AWR frequency. Bold numbers indicate the best result within a given shape.
converge from 1:1
decay to 1:1
converge from 1:4
decay to 1:4
AWR constant
Frequency
Frequency
Frequency
Frequency
1
10
100
1000 Union 1
10
100
1000 Union 1
10
100
1000 Union 1
10
100
1000 Union
Problems Solved
10:1
7967
7972 7976 8050 8245 8372
8448 8441 8380 8169 8614
7983 7990 8094 8323 8485
8579 8574 8493 8272 8797
1:10
8575
8565 8578 8550 8489 8778
8458 8456 8484 8268 8787
8575 8584 8582 8535 8729
8584 8574 8590 8608 8764
1:100
8079
8084 8079 8039 8279 8560
8454 8484 8537 8636 8907
8088 8084 8071 8216 8399
8572 8584 8615 8592 8855
1:1000 7276
7279 7297 7418 8133 8379
8470 8492 8492 8473 8873
7283 7300 7364 7927 8076
8567 8567 8566 8446 8830
Union 9019
9028 9016 8981 8871 9194
8572 8674 8725 8978 9048
9038 9038 9016 8967 9180
8697 8759 8800 8927 9026
Uniquely Solved
10:1
1
2
1
1
6
10
0
0
0
2
2
1
0
0
1
2
0
0
2
4
6
1:10
0
1
0
0
1
2
0
0
0
6
6
0
1
0
0
1
0
0
0
0
0
1:100
1
0
0
5
4
9
2
0
3
3
8
0
0
0
2
2
0
1
1
0
2
1:1000 0
0
0
0
1
1
2
0
0
2
4
0
1
3
3
7
0
0
0
0
0
Union 2
3
1
6
12
22
4
0
3
13
20
1
2
3
6
12
0
1
3
4
8

474
M. Rawson and G. Reger
Table 5. Total number of problems solved compared to other solvers.
Solver
Total solved Uniquely solved
All
Excluding Vampire (old)
Vampire (old)
13,057
0 -
Vampire (new) 13,191
54 1030
E
10,845
190
190
iProver
8,143
215
215
CVC4
9,354
501
502
impressive result—it is rare to be able to improve portfolio mode by this many
new problems with a single new proof search option.
The u-score is computed by giving 1/n points per problem solved where
n is the number of strategies solving a problem [2]. This gives a measure of
contribution per strategy. Options with the largest u-score will be prioritised
for extending the existing portfolio mode, but only those with unique solutions
overall.
Finally, two problems were solved which were marked with an “Unknown”
status (with rating 1.00) in the TPTP headers. Only converging with frequency
50 solved SET345-6 and only decaying with frequency 1 solved LAT320+3.
5.3
Comparison with Other Solvers
To place these results in context, the overall number of problems solved by our
new strategies are compared with the results of other solvers, using their CASC-
J9 These results are from 300-s runs in identical conditions and are given in
Table 5. In this table Vampire (old) stands for the CASC-J9 competition version
whilst Vampire (new) stands for the union of all problems solved by new options
in the previous section. Between them, the two versions of Vampire solve 1,030
problems uniquely. 54 unique problems found in the previous section remain
unique when compared to other competitive solvers.
6
Conclusions and Future Work
Clause selection is a key part of any saturation-based theorem prover and
age/weight ratios have a signiﬁcant eﬀect on the performance of proof search
in the Vampire theorem prover. Known folklore that there is no clear optimal
age/weight ratio is supported. Further, varying the age/weight ratio over time
during proof search can improve further on an optimal, but ﬁxed age/weight
ratio in terms of the number of activations. Experiments within Vampire on the
TPTP benchmark set suggest that these age/weight shapes show promise for
future developments in this novel approach to proof search. Indeed, including
our relatively simple shapes already leads to signiﬁcant performance gains.

Old or Heavy? Decaying Gracefully with Age/Weight Shapes
475
Future directions for research include trying a greater number of “shapes”
(such as repeating patterns), other approaches for parameterising these shapes,
a pseudo-random approach to age/weight instead of the balancing algorithm,
and integration of the new approaches into existing strategy schedules.
References
1. Denzinger, J., Kronenburg, M., Schulz, S.: Discount-a distributed and learning
equational prover. J. Autom. Reason. 18(2), 189–198 (1997)
2. Hoder, K., Reger, G., Suda, M., Voronkov, A.: Selecting the selection. In: Olivetti,
N., Tiwari, A. (eds.) IJCAR 2016. LNCS (LNAI), vol. 9706, pp. 313–329. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-40229-1 22
3. Korovin, K.: iProver – an instantiation-based theorem prover for ﬁrst-order logic
(system description). In: Armando, A., Baumgartner, P., Dowek, G. (eds.) IJCAR
2008. LNCS (LNAI), vol. 5195, pp. 292–298. Springer, Heidelberg (2008). https://
doi.org/10.1007/978-3-540-71070-7 24
4. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
5. McCune, W.: Otter 2.0. In: Stickel, M.E. (ed.) CADE 1990. LNCS, vol. 449, pp.
663–664. Springer, Heidelberg (1990). https://doi.org/10.1007/3-540-52885-7 131
6. McCune, W.: Otter 3.3 reference manual. arXiv preprint, arXiv:cs/0310056 (2003)
7. McCune, W.: Release of prover9. In: Mile High Conference on Quasigroups, Loops
and Nonassociative Systems, Denver, Colorado (2005)
8. Rawson, M., Reger, G.: Dynamic strategy priority: empower the strong and aban-
don the weak. In: Proceedings of the 6th Workshop on Practical Aspects of Auto-
mated Reasoning co-located with Federated Logic Conference 2018 (FLoC 2018),
Oxford, UK, 19 July 2018, pp. 58–71 (2018)
9. Reger, G., Suda, M., Voronkov, A.: Finding ﬁnite models in multi-sorted ﬁrst-
order logic. In: Creignou, N., Le Berre, D. (eds.) SAT 2016. LNCS, vol. 9710, pp.
323–341. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-40970-2 20
10. Riazanov, R., Voronkov, A.: Limited resource strategy in resolution theorem prov-
ing. J. Symb. Comput. 36(1–2), 101–115 (2003)
11. Sch¨afer, S., Schulz, S.: Breeding theorem proving heuristics with genetic algorithms.
In: Global Conference on Artiﬁcial Intelligence, GCAI 2015, Tbilisi, Georgia, 16–19
October 2015, pp. 263–274 (2015)
12. Schulz, S.: E - a brainiac theorem prover. Ai Commun. 15(2–3), 111–126 (2002)
13. Schulz, S., M¨ohrmann, M.: Performance of clause selection heuristics for saturation-
based theorem proving. In: Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS
(LNAI), vol. 9706, pp. 330–345. Springer, Cham (2016). https://doi.org/10.1007/
978-3-319-40229-1 23
14. Stump, A., Sutcliﬀe, G., Tinelli, C.: StarExec: a cross-community infrastructure
for logic solving. In: Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014.
LNCS (LNAI), vol. 8562, pp. 367–373. Springer, Cham (2014). https://doi.org/10.
1007/978-3-319-08587-6 28

476
M. Rawson and G. Reger
15. Sutcliﬀe, G.: The 9th IJCAR automated theorem proving system competition-
CASC-J9. AI Commun. 31(1), 1–13 (2015)
16. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure, from CNF
to TH0, TPTP v6.4.0. J. Autom. Reason. 59(4), 483–502 (2017)
17. Weidenbach, C.: Combining superposition, sorts and splitting. In: Robinson, A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. II, chap. 27, pp.
1965–2013. Elsevier Science (2001)

Induction in Saturation-Based Proof
Search
Giles Reger1(B) and Andrei Voronkov1,2
1 University of Manchester, Manchester, UK
giles.reger@manchester.ac.uk
2 EasyChair, Manchester, UK
Abstract. Many applications of theorem proving, for example program
veriﬁcation and analysis, require ﬁrst-order reasoning with both quanti-
ﬁers and theories such as arithmetic and datatypes. There is no complete
procedure for reasoning in such theories but the state-of-the-art in auto-
mated theorem proving is still able to reason eﬀectively with real-world
problems from this rich domain. In this paper we contribute to a miss-
ing part of the puzzle: automated induction inside a saturation-based
theorem prover. Our goal is to incorporate lightweight automated induc-
tion in a way that complements the saturation-based approach, allowing
us to solve problems requiring a combination of ﬁrst-order reasoning,
theory reasoning, and inductive reasoning. We implement a number of
techniques and heuristics and evaluate them within the Vampire theo-
rem prover. Our results show that these new techniques enjoy practical
success on real-world problems.
1
Introduction
Saturation-based proof search has been the leading technology in automated
theorem proving for ﬁrst-order logic for some time. The core idea of this app-
roach is to saturate a set of clauses (including the negated goal) with respect to
some inference system with the aim of deriving a contradiction and concluding
that the goal holds. Over the last few years this technology has been extended to
reason with both quantiﬁers, and theories such as arithmetic and term algebras
(also known as algebraic, recursive or inductive datatypes), making it highly
applicable in areas such as program analysis and veriﬁcation, which were previ-
ously the sole domain of SMT solvers. However, so far little has been done to
extend saturation-based proof search with automated induction. Most attempts
to date have focussed on using saturation-based methods to discharge subgoals
once an induction axiom has been selected.
The aim of this work is to extend saturation-based proof search with
lightweight methods for automated induction where those techniques are inte-
grated directly into proof search i.e. they do not rely on some external procedure
This work was supported by EPSRC Grant EP/P03408X/1. Andrei Voronkov was also
partially supported by ERC Starting Grant 2014 SYMCAR 639270 and the Wallenberg
Academy Fellowship 2014 – TheProSE.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 477–494, 2019.
https://doi.org/10.1007/978-3-030-29436-6_28

478
G. Reger and A. Voronkov
to produce subgoals. We achieve this by the introduction of new inference rules
capturing inductive steps and new proof search heuristics to guide their appli-
cation. Our approach is based on the research hypothesis that many problems
requiring induction only require relatively simple applications of induction.
Example 1. As an introductory example, consider the problem of proving the
commutativity of (∀x∀y)plus(x, y) ≈plus(y, x), where x and y range over natural
numbers. We now brieﬂy described how this approach will handle this problem.
When we Skolemise its negation, we obtain the clause plus(σ0, σ1) ̸≈
plus(σ1, σ0). In this paper, we will denote by σi fresh Skolem constants intro-
duced by converting formulas to clausal form.
Our approach will immediately apply induction to σ0 in the negated conjec-
ture by resolving this clause with the (clausal form of the) induction axiom
⎛
⎝
plus(zero, σ1) ≈plus(σ1, zero)∧
(∀z)
plus(z, σ1) ≈plus(σ1, z) →
plus(succ(z), σ1) ≈plus(σ1, succ(z))

⎞
⎠→(∀x)plus(x, σ1) ≈plus(σ1, x)
to produce the following subgoals:
plus(zero, σ1) ̸≈plus(σ1, zero)∨plus(succ(σ2), σ1) ̸≈plus(σ1, succ(σ2))
plus(zero, σ1) ̸≈plus(σ1, zero)∨plus(σ1, σ2) ≈plus(σ2, σ1)
(1)
Clause splitting is then used to split the search space into two parts to be con-
sidered separately. This splitting is important to our approach and can be used
in any saturation theorem prover implementing some version of it, for example
using splitting with backtracking as in SPASS [24] or the AVATAR architecture
as in Vampire [22]. The ﬁrst part contains plus(zero, σ1) ̸≈plus(σ1, zero) and is
refuted by deriving plus(σ1, zero) ̸≈σ1 using the deﬁnition of plus and applying
a second induction step to σ1 in this clause. By resolving with a similar induc-
tion axiom to before, the following clauses are produced and are refuted via the
deﬁnition of plus and the injectivity of datatype constructors.
zero ̸≈plus(zero, zero)∨succ(σ3) ̸≈plus(succ(σ3), zero)
zero ̸≈plus(zero, zero)∨plus(σ3, zero) ≈σ3
The second part of the clause splitting then contains the other half of the clauses
given above. Superposition is then applied to these clauses and the axioms of
plus to derive
succ(plus(σ1, σ2)) ̸≈plus(σ1, succ(σ2))
and a third induction step is applied to this clause on σ1. The resulting subgoals
can again be refuted via the deﬁnition of plus and the injectivity of datatype
constructors.
While inductive reasoning in this example may seem to be the same as
in almost any other inductive theorem prover, there is an essential diﬀerence:
instead of reducing goals to subgoals using induction and trying to prove these

Induction in Saturation-Based Proof Search
479
subgoals using theory reasoning or again induction, we simply consider induction
as an additional inference rule adding new formulas to the search space. In a way,
every clause generated during the proof search becomes a potential target for
applying induction and induction becomes integrated in the saturation process.
In this example there were three applications of induction to ground unit
clauses in the search space, however our implementation performs 5 induction
steps with 2 being unnecessary for the proof. This is typical in saturation-based
proof search where many irrelevant consequences are often derived. This is an
important observation; our general approach is to derive consequences (inductive
or otherwise) in a semi-guided fashion, meaning that we may make many unnec-
essary induction steps. However, this is the philosophy behind saturation-based
approaches.
During proof search for this example it was necessary to (i) decide which
clauses to apply induction to, (ii) decide which term within that clause to apply
induction to, and (iii) decide how to apply induction. We address issues (ii) and
(iii) in this paper, whilst relying on the clause selection techniques of saturation-
based theorem provers for (i). We begin in Sect. 2 by introducing the necessary
preliminary deﬁnitions for the work. In Sect. 3 we address (iii), how we apply
induction, through the introduction of a set of new inference rules. In Sect. 4
we consider (ii) through a number of heuristics for selecting goals for induction.
Then in Sect. 5 we show how standard clause splitting techniques can be used in
our induction proofs (without any additional work) for case splitting. Section 6
describes implementation and experimental evaluation. We then consider related
work in Sect. 7 before concluding in Sect. 8.
2
Preliminaries
Multi-sorted First-Order Logic. We consider standard multi-sorted ﬁrst-order
predicate logic with equality. We allow all standard boolean connectives and
quantiﬁers in the language. We denote terms by s, t, variables by x, y, z, constants
by a, and function symbols by f. We consider equality ≈as part of the language,
that is, equality is not a symbol. An atom is an equality or a predicate applied
to a list of terms. A literal is an atom A or its negation ¬A. Literals that are
atoms are called positive, while literals of the form ¬A are negative. If L = ¬A
is a literal we write ¬L for the literal A. A clause is a disjunction of literals
L1 ∨. . . ∨Ln, where n ≥0. When n = 0, we will speak of the empty clause,
denoted by □. We denote atoms by A, literals by L, clauses by C, and formulas
by F, all possibly with indices. Formulas can be clausiﬁed (transformed into a
set of clauses) via standard techniques (e.g. [13] and our recent work in [15]).
We write clausify(F) for the set of clauses obtained from F by clausiﬁcation.
By an expression E we mean a term, atom, literal, or clause. We write E[t]
to mean an expression E with a particular occurrence of a term t and then E[s]
for that expression with the particular occurrence of t replaced by term s.
A multi-sorted signature is a ﬁnite set of symbols and a ﬁnite set of sorts
with the accompanying function srt providing sorts for the symbols.

480
G. Reger and A. Voronkov
The Theory of Finite Term Algebras. In this paper we consider induction for
ﬁnite term algebras, also known as algebraic, inductive, or recursive datatypes.
A deﬁnition of the ﬁrst-order theory of term algebras over a ﬁnite signature can
be found in e.g. [17] and a description of how saturation-based proof search may
be extended to reason with such structures is given in [9]. Let Σ be a ﬁnite set of
function symbols containing at least one constant. Denote by T (Σ) the set of all
ground terms built from the symbols in Σ. The Σ-term algebra is the algebraic
structure whose carrier set is T (Σ) and deﬁned in such a way that every ground
term is interpreted by itself (we leave details to the reader).
We will often consider extensions of term algebras by additional symbols.
Elements of Σ will be called term constructors (or simply just constructors),
to distinguish them from other function symbols. We will diﬀerentiate between
recursive constructors that are recursive in their arguments and base construc-
tors that are not. Where we wish to diﬀerentiate we may write T (ΣB, ΣR) for
base constructors ΣB and recursive constructors ΣR.
In practice, it can be useful to consider multiple sorts, especially for problems
taken from functional programming. In this setting, each term algebra construc-
tor has a type τ1 × · · · × τn →τ. The requirement for at least one constant is
replaced by the requirement that for every sort, there exists a ground term of
this sort. We also consider theories, which mix constructor and non-constructor
sorts. That is, some sorts contain constructors and some do not (e.g. arithmetic).
Finally, we associate n destructor (or projection) functions with every con-
structor c of arity n such that each destructor returns one of the arguments of
c. Note, that the behavior of destructors is unspeciﬁed on some terms.
Example 2. We introduce two term algebras. Firstly, that of natural numbers
nat := zero | succ(dec(nat))
and secondly that of integer lists
list := nil | cons(hd(Int), tail(list)).
Note that this second term algebra relies on a built-in integer sort.
Saturation-Based Proof Search. An important concept in this work is that of
saturation with respect to an inference system. Inference systems are used in
the theory of superposition [12] implemented by several leading automated ﬁrst-
order theorem provers, including Vampire [10] and E [18]. Superposition theo-
rem provers implement proof-search algorithms in S using so-called saturation
algorithms, as follows. Given a set S of formulas, superposition-based theorem
provers try to saturate S with respect to S, that is build a set of formulas that
contains S and is closed under inferences in S. At every step, a saturation algo-
rithm selects an inference of S, applies this inference to S, and adds conclusions
of the inferences to the set S. If at some moment the empty clause □is obtained,
by soundness of S, we can conclude that the input set of clauses is unsatisﬁable.
Figure 1 gives a simple saturation algorithm. This is missing an important notion

Induction in Saturation-Based Proof Search
481
input: Init: set of clauses;
var active, passive, unprocessed: set of clauses; var given, new: clause;
active := ∅; unprocessed := Init;
loop
while unprocessed ̸= ∅
new:=pop(unprocessed);
if new = □then return unsatisﬁable;
add new to passive
if passive = ∅then return satisﬁable or unknown
given := select(passive);
(* clause selection *)
move given from passive to active;
unprocessed:=infer(given, active);
(* generating inferences *)
Fig. 1. Simple saturation algorithm.
of redundancy. We have omitted this as it does not interact with the elements
of proof search we consider here. However, it is core to the implementation in
the Vampire theorem prover. It is important to note that the only way to guide
proof search is via how we select clauses and how we perform inferences on them.
3
Performing Induction
This section introduces inference rules for induction on term algebras.
3.1
General Approach
We begin by describing our general approach. The idea is to add inference rules
that capture the application of induction to the selected clause in proof search.
These inference rules will be applied during proof search to selected clauses in
the same way as other inference rules such as resolution. We deﬁne an induction
axiom to be any valid (in the underlying theory) formula of the form
formula →(∀x)(L[x]).
For simplicity we assume that this formula is closed, leaving out the general
case due to the lack of space. The idea is to resolve this with a clause ¬L[t] ∨C
obtaining formula →C. Again, for simplicity we assume that t is a ground term.
As long as the induction axiom is valid, this approach is always sound. If the
resulting formula is not a clause, it should then be converted to its CNF.
The idea is that L[t] is a (sub)goal we are trying to prove (by induction).
This is an interesting point. Typically, saturation-based proof search is not goal-
oriented (although one can introduce heuristics that support this) but this app-
roach to induction is goal-oriented in nature as the conclusion of an induction
inference is a subgoal that, if refuted, proves the goal represented by the premise.
Also, similar to [6] by resolving the induction axiom to reduce the goal to sub-
goals we bypass the literal selection used in saturation algorithms. This means

482
G. Reger and A. Voronkov
that, if we would just add the (clausal form of) the induction axiom to the search
space, we would most likely never use it to resolve against the goal in the same
way as above since the literal L[x] would not necessarily be selected.
Below we consider two diﬀerent kinds of induction axioms, introducing three
inference rules, parametrised by some (general) term algebra. To formalise the
selection of goals that can be proved by induction we introduce a predicate
sel(C, L, t) that is true if C is clause, L a literal in C and t a term in L. We will
call this predicate the induction heuristic since it will be used to decide when
induction should be applied. In this case we will informally say that t is the
induction term and L the induction literal in C.
Here we concentrate on how induction should be performed once an induction
term and literal have been selected. Section 4 discusses choices for selection.
3.2
Structural Induction
We begin by motivating the inference rule by the simple example of inductively
proving that the length of a list is non-negative.
Example 3 (Structural Induction on Lists). Consider the following conjecture
(∀x : list)(len(x) ≥0) for integer lists (deﬁned in Example 2) given the axioms
len(nil) ≈0 and (∀x : Int, y : list)(len(cons(x, y)) ≈1 + len(y)) for the len
function. To prove this conjecture we must ﬁrst negate it to get ¬(len(σ) ≥0)
and then introduce the induction axiom
len(nil) ≥0 ∧(∀x, y)(len(x) ≥0 →len(cons(y, x)) ≥0) →(∀x)(len(x) ≥0)
which is then resolved against ¬(len(σ) ≥0) to give, after conversion to CNF,
two clauses
¬(len(nil) ≥0) ∨len(σ1) ≥0
¬(len(nil) ≥0) ∨¬(len(cons(σ2, σ1)) ≥0),
which can be refuted using the axioms for len. The question now is what inference
rule is needed for performing the above induction step. To do so, we deﬁne the
induction heuristics sel(C, L, t) to hold when L is the only literal in C and t is a
constant of the sort list. This rule eﬀectively results in the following inferences
performed by a saturation theorem prover:
¬A[a]
¬A[nil] ∨A[σ1]
¬A[nil] ∨¬A[cons(σ2, σ1)]
where a is a constant, A[a] is ground, srt(a) = list and σ1, σ2 are fresh constants.
3.3
Well-Founded Induction
Suppose that x ≻y is any binary predicate that is interpreted as a well-founded
relation (which is not necessarily an ordering). We require both arguments of ≻

Induction in Saturation-Based Proof Search
483
to be of the same sort. Then the following is a valid formula, which represents
well-founded induction on this relation:
∀x(¬L[x] →∃y(x ≻y ∧¬L[y])) →∀xL[x].
When we skolemize this formula, we obtain two clauses
¬L[σ1] ∨L[x]
¬σ1 ≻y ∨L[y] ∨L[x]
We can use the following two equivalent clauses instead:
¬L[σ1] ∨L[x]
¬σ1 ≻y ∨L[y]
(2)
Well-founded induction is the most general form of induction (though in
practice it can only be used when the relation ≻can be expressed in the ﬁrst-
order language we are using). We are interested in ﬁnding special cases of well-
founded induction for term algebras. There are two obvious candidates for it:
the immediate subterm relation ≻1 and the subterm relation discussed in the
sequel.
Let us begin with the immediate subterm relation. Note that the relation ≻
must have both arguments of the same sort, so the corresponding induction rule
will only be useful for term algebras where at least one argument of a constructor
has the same sort as the constructor itself. Fortunately, this is the case for the
three most commonly used inductive data types: natural numbers, lists and trees.
Let us provide a complete axiomatisation of the immediate subterm relation
≻1 ﬁrst for natural numbers and lists:
¬(zero ≻1 x)
succ(x) ≻1 y ↔x ≈y
¬(nil ≻1 x)
cons(x, y) ≻1 z ↔y ≈z
The subterm relation is generally not axiomatisable. However, this is not a
problem in general, since we can use as an incomplete axiomatisation of the
subterm relation any set of formulas which are true on this relation (though this
restricts what can be proved about the relation). If we then prove anything using
this set of formulas, then our proof will be correct for the subterm relation too,
which makes the corresponding induction rule valid too.
We can generalise the immediate subterm and subterm relation also to trees
and some other (but not all!) inductively deﬁned types. We do not include general
deﬁnitions here as they become very involved with multiple sorts and mutually
recursive type deﬁnitions.
3.4
Inductive Strengthening
We now consider a diﬀerent form of induction axiom (inspired by [16]).
Example 4. Given the negated conjecture ¬(len(σ1) ≥0) given in Example 3 we
consider a diﬀerent way in which to inductively demonstrate L[x] and thus refute
this claim. The idea here is to argue that if there does not exist a smallest list

484
G. Reger and A. Voronkov
of non-negative length then the length of all lists is non-negative. This can be
captured in the induction axiom
¬(∃x)

¬(len(x) ≥0)∧
(∀y)(subtermlist(x, y) →len(tail(y)) ≥0)

→(∀z)(len(z) ≥0)
where subtermlist(x, y) is true if y is a subterm of x of list sort. However, as
argued in [9], the subterm relation needs to be axiomatised and these axioms
(which include transitivity) can have a large negative impact on the search space.
Therefore, we can consider two alternative inductive axioms. The ﬁrst is the weak
form where we consider only direct subterms of x as follows.
¬(∃x)
¬(len(x) ≥0)∧
(x ≈cons(hd(x), tail(x)) →len(tail(x)) ≥0)

→(∀y)(len(y) ≥0)
This is clausiﬁed as
len(x) ≥0 ∨¬(len(σ2) ≥0)
len(x) ≥0 ∨σ2 ̸≈cons(hd(σ2), tail(σ2)) ∨len(tail(σ2)) ≥0
which can be resolved against the conjecture ¬(len(σ1) ≥0) as before.
The second (taken from [9]) is where we represent the subterm relation in a
way that is more friendly to saturation-based theorem provers i.e. we introduce
a fresh predicate lessx and then add axioms such that it holds for exactly those
terms smaller than the existential witness x. This can be written as follows.
¬(∃x)
⎛
⎝
¬(len(x) ≥0) ∧(∀z)(lessx(z) →len(z) ≥0)
∧(x ≈cons(hd(x), tail(x)) →lessx(tail(x))) ∧
(∀y)(lessx(cons(hd(y), tail(y))) →lessx(tail(y)))
⎞
⎠→(∀y)(len(y) ≥0)
Again, the speciﬁc approach taken in this example can be generalised to the
arbitrary term algebra ta = T (ΣB ∪ΣR). The existential part existsta(L) of the
general induction axiom can be given as
(∃x)
⎛
⎝¬L[x]

con(...,di,...)∈ΣR

j∈rec(con)
(x ≈con(. . . , di(x), . . .) →L[dj(x)])
⎞
⎠
for the ﬁrst approach and as
(∃x)
⎛
⎝
¬L[x] ∧(∀z)(less(z) →L[z]) ∧

con(...,di,...)∈ΣR

j∈rec(con) x ≈con(. . . , di(x), . . .) →lessx(dj(x)) ∧
(∀y)(
con(...,di,...)∈ΣR

j∈rec(con) lessxz(con(. . . , di(y), . . .)) →lessx(dj(y)))
⎞
⎠
for the second approach. The general induction rule then becomes
L[t] ∨C
clausify(existsta(¬L) ∨C)
for ground literal L[t], clause C and term t, where srt(t) = ta and sel(L[t] ∨
C, L[t], t).

Induction in Saturation-Based Proof Search
485
One could consider an optimisation where this approach is applied directly to
the input (as is done in [16]). However, this would introduce induction axioms
too early in proof search i.e. it goes against the saturation-based philosophy.
One could also consider reusing Skolem constants instead of introducing new
ones where t in the above rule is already a Skolem constant. However, this could
only be done for each Skolem constant at most once.
Table 1. Illustrating the induction inference schemas for the rbtree term algebra.
Approach One
L[t] ∨C
C ∨L[empty] ∨L[leaf(σ1)] ∨¬L[σ4] ∨¬L[σ2]
C ∨L[empty] ∨L[leaf(σ1)] ∨¬L[σ4] ∨¬L[σ6]
C ∨L[empty] ∨L[leaf(σ1)] ∨¬L[σ7] ∨¬L[σ2]
C ∨L[empty] ∨L[leaf(σ1)] ∨¬L[σ7] ∨¬L[σ6]
C ∨L[empty] ∨L[leaf(σ1)] ∨L[black(σ3, σ7, σ4)] ∨¬L[σ2]
C ∨L[empty] ∨L[leaf(σ1)] ∨L[black(σ3, σ7, σ4)] ∨¬L[σ6]
C ∨L[empty] ∨L[leaf(σ1)] ∨L[red(σ5, σ2, σ6)]] ∨¬L[σ7]
C ∨L[empty] ∨L[leaf(σ1)] ∨L[red(σ5, σ2, σ6)] ∨¬L[σ4]
C ∨L[empty] ∨L[leaf(σ1)] ∨L[black(σ3, σ7, σ4)] ∨L[red(σ5, σ2, σ6)]
Approach Two
L[t] ∨C
C ∨red(rval(σ1), rleft(σ1), rright(σ1))
σ1 ∨¬L[rleft(σ1)]
C ∨red(rval(σ1), rleft(σ1), rright(σ1))
σ1 ∨¬L[rright(σ1)]
C ∨black(bval(σ1), bleft(σ1), bright(σ1))
σ1 ∨¬L[bleft(σ1)]
C ∨black(bval(σ1), bleft(σ1), bright(σ1))
σ1 ∨¬L[bright(σ1)]
C ∨L[σ1]
Approach Three
L[t] ∨C
C ∨L[σ1]
C ∨¬lessx(x) ∨¬L[x]
C ∨red(rval(σ1), rleft(σ1), rright(σ1)
σ1 ∨lessx(rleft(σ1))
C ∨red(rval(σ1), rleft(σ1), rright(σ1)
σ1 ∨lessx(rright(σ1))
C ∨black(bval(σ1), bleft(σ1), bright(σ1)
σ1 ∨lessx(bleft(σ1))
C ∨black(bval(σ1), bleft(σ1), bright(σ1)
σ1 ∨lessx(bright(σ1))
C ∨¬lessx(red(rval(x), rleft(x), rright(x))) ∨lessx(rleft(x))
C ∨¬lessx(red(rval(x), rleft(x), rright(x))) ∨lessx(rright(x))
C ∨¬lessx(black(bval(x), bleft(x), bright(x))) ∨lessx(bleft(x))
C ∨¬lessx(black(bval(x), bleft(x), bright(x))) ∨lessx(bright(x))

486
G. Reger and A. Voronkov
3.5
Comparing the Approaches with an Example
To illustrate the diﬀerences in the clauses produced by the above three
approaches we give, in Table 1, the introduced inference rules instantiated with
ta = rbtree deﬁned as
rbtree := empty | leaf(lval(Int))
| red(rval(Int), rleft(rbtree), rright(rbtree))
| black(bval(Int), bleft(rbtree), bright(rbtree))
This covers all the important cases from above (i) non-zero arity base construc-
tors, and (ii) multiple base and multiple recursive constructors. Notice how the
structural induction rule, in this case introduces 7 new Skolem constants and
9 clauses (although this could be slightly optimised here) whilst the inductive
strengthening approaches introduce one Skolem constant.
4
Selecting Where to Apply Induction
We now consider how to deﬁne various induction heuristics.
4.1
Goal-Directed Search
In our introductory example (Example 1, proving commutativity of addition) we
(usefully) applied induction three times. The ﬁrst time was directly to the goal
and the second two times were to unit clauses derived directly from the result of
this ﬁrst induction. We hypothesise that this is a typical scenario and introduce
heuristics for this common case. An important observation is that an implicative
universal goal becomes a set of unit ground clauses once negated.
Unit Clauses. A unit clause represents a single goal or subgoal that, if refuted,
will lead to a ﬁnal proof. Conversely, applying the above induction inference
rules to non-unit clauses will lead to applications of induction that may not be
as general as needed. This selection can be deﬁned as follows for some literal
L[t] and term t.
selU(L[t], L[t], t)
Negative Literals. Typically, goal statements are positive and therefore proof
search is attempting to derive a contradiction from a negative statement. Apply-
ing induction to a negative statement leads to a mixture of positive and nega-
tive conclusions. As we saw in the introductory example, it is common to apply
further induction to the negative conclusions. This selection can be deﬁned as
follows for clause C, atom A and term t.
selN(C ∨¬A[t], ¬A[t], t)
However, it is easy to see cases where this is too restrictive. For example, the
goal from Example 3 could have been rewritten as (∀x)(¬(len(x) < 0)) and the
negated goal on which induction should be performed would have been positive.

Induction in Saturation-Based Proof Search
487
Constants. Given a purely universal goal, the terms of interest will be Skolem
constants (whether this Skolemisation occurred within the solver or not) and
terms introduced by induction for repeated induction are also typically Skolem
constants. Therefore, to restrict application of induction to this special case,
we can restrict it to constants only. This selection can be deﬁned as follows for
clause C, literal L and constant a.
selC(C ∨L[a], L[a], a)
Special Symbols. The goal will typically contain the symbols on which induction
should be performed. Additionally, further induction steps are often performed
on the Skolem constants introduced by a previous induction. We deﬁne a selec-
tion predicate parameterised by a set of symbols α as follows for clause C, literal
L and term t.
selα(C ∨L[t], L[t], t) ⇔(t = f(t1, . . . , tn) →f ∈α) ∧(t = a →a ∈α)
and deﬁne the functions selG and selI for sets of goal symbols G and induction
Skolem constants I.
The sel function is deﬁned as any conjunction of zero or more of the above
with the trivial selection function that is true on all inputs of term algebra sort.
4.2
Inferring Goal Clause(s)
One issue with the above heuristics is that we may not have an explicit goal in
our input problem. Indeed, SMT-LIB [1] has no syntax for indicating the goal
(unlike TPTP [21]). To address this we deﬁne a notion of goal symbol that is
independent of the notion of an explicit goal being given.
Given a set of input formulas F1, . . . , Fn and a set G containing zero or more
formulas Fi marked as goal formulas, a goal symbol is a symbol such that
– It appears in a formula F ∈G, or
– It is a Skolem constant introduced in the clausiﬁcation of some F ∈G, or
– It appears in at most limit formulas, or
– It is a Skolem constant introduced by the Skolemisation of some formula Fi
of the form ∃x.F
where limit is a parameter to the process. In the case where this is a single
goal formula we would expect limit to be 1. However, the input may have been
subject to some additional preprocessing meaning that the goal is represented
by a few clauses in the input. The last point is because many goals will take the
form of negated universal statements; this is also how formulas for induction are
identiﬁed in [16] (our approach is more general).
Once all such goal symbols have been identiﬁed, the set G is extended to
include all formulas containing a goal symbol. This is done as G typically plays
another role in proof search as clauses derived from formulas in G may be pri-
oritised in clause selection, providing some heuristic goal-directionality.

488
G. Reger and A. Voronkov
5
Case Splitting for Free
An important part of inductive proofs is typically the case splitting between the
base case and the inductive step. In this section we describe a clause splitting
approach (implemented in Vampire as AVATAR [14,22]) that achieves this.
We brieﬂy describe the ground part of the AVATAR framework for clause
splitting as case splitting for induction only requires the ground part. The general
idea is that given a set of clauses S and a ground clause L1 ∨L2 we can consider
the two sub-problems S ∪{L1} and S ∪{L2} independently.
Let name be a function from ground literals to labels that is injective up to
symmetry of equality. Let C ←A be a labelled clause where A is a set of labels.
We can lift an inference system on clauses to one on labelled clauses where
all conclusions take the union of the labels in premises. The previous rules for
induction can be extended such that the consequent clauses take the labels of the
premise clause. Figure 2 shows how the simple saturation algorithm from Sect. 2
can be extended to perform ground clause splitting. It uses a SAT procedure
that we add clauses of labels to and then request the diﬀerence between a new
model and the previous model in terms of added/removed labels.
input: Init: set of clauses;
var active, passive, unprocessed: set of clauses; var given, new: clause;
active := ∅;
passive := ∅; unprocessed := Init;
loop
while unprocessed ̸= ∅
new:=pop(unprocessed);
if new = □then return unsatisﬁable;
if new = □←A then add ¬A to SAT;
if new is ground then add label(new) to SAT;
else add new to passive;
if passive = ∅then return satisﬁable or unknown
(add labels, remove labels) = new model(SAT);
(* compute new model *)
active:= {C ←L ∈active | L ∩remove labels = ∅};
passive:= {C ←L ∈passive | L ∩remove labels = ∅};
passive:=passive ∪{retrieve(l) ←l | l ∈add labels};
given := select(passive);
(* clause selection *)
move given from passive to active;
unprocessed:=infer(given, active);
(* generating inferences *)
Fig. 2. Simple saturation algorithm with ground clause splitting.
To understand why this is useful consider the conclusions of the inference
rules given in Table 1. These clauses are all ground and multi-literal i.e. they cap-
ture multiple cases. As an example, when proving the conjecture height(t) ≥0
our implementation considers and refutes between 6 and 8 diﬀerent cases depend-
ing on which form of induction rule is used.

Induction in Saturation-Based Proof Search
489
6
Experimental Evaluation
In this section we describe the implementation and evaluation of the techniques
described in this paper.
Implementation. We extended the Vampire [10] theorem prover with additional
options to capture the techniques described in the previous sections. Table 2
gives an overview of these new options. The sik option captures the diﬀerent
approaches introduced in Sect. 3. The indm option limits the depth of induction.
The remaining options capture the choices made in Sect. 4. Our implementa-
tion of the induction inference rules ensures that we never instantiate the same
induction axiom more than once and that proof search when there are no term
algebra sorts in the problem is unaﬀected. Furthermore, this implementation is
fully compatible with all other proof search options and heuristics in Vampire.
Our implementation is available online1.
Table 2. New options and their values.
Name Values
Description
ind
none, struct
Whether structural induction should be applied or
not
sik
1, 2, 3, all
The kind of structural induction to apply. The
numbers 1,2, 3 refer to the three kinds introduced in
Sect. 3 and all applies them all
indmd n ≥0 (0)
The maximum depth to which induction is applied
where 0 indicates it is unlimited
indc
goal, goal plus, all Choices for the sel α predicate (see Sect. 4) where
goal uses goal symbols only, goal plus uses goal and
induction symbols, and all is unrestricted
indu
on, off
Whether to include the sel U predicate
indn
on, off
Whether to include the sel N predicate
gtg
on, off
Whether goal clauses in the input should be inferred
gtgl
n ≥1 (1)
The limit of times a symbol should appear in input
formulae to be identiﬁed as a goal symbol
Experimental Setup. We use two sets of benchmarks from SMT-LIB from the
UFDT and UFDTLIA logics where UF stands for Uninterpreted Functions, DT
stands for DataTypes and LIA stands for Linear Integer Arithmetic; we do
not consider AUFDTLIA as it does not contain problems interesting for induc-
tion. UFDT consists of 4376 problems known not to be satisﬁable (we excluded
problems either marked as, or found to be, satisﬁable during experiments) and
UFDTLIA consists of 303 problems that formed the TIP benchmark set in 2015
as used in [16]. Experiments are run on StarExec [20].
1 See https://github.com/vprover/vampire.

490
G. Reger and A. Voronkov
6.1
Research Questions
In this section we look at two research questions that naturally arise in our work.
Which Options are Useful? Given the set of introduced options, we would like
to know which will be useful in general. Vampire is a portfolio solver and would
normally run a series of strategies combining diﬀerent options. Therefore, any
options able to solve problems uniquely may be useful for a portfolio mode.
Table 3 compares the option values across the SMT-LIB problems. All option
values with the exception of --sik three and non-zero values for indmd solve
some problems uniquely. For each option there is a clear choice for default value.
The fact that non-zero values for indmd were not useful in general suggests that
there was not a problem with an explosion of iterative induction steps. This
is most likely due to the fact that clause selection will favour a breadth-ﬁrst
exploration of the space. The solved problems did not rely heavily on inferring
goal symbols or selection via special symbols. This suggests that the problems
of interest either had shallow proofs that followed quickly from the input, or
contained few relevant symbols for induction.
Table 3. Comparing option values.
Value
Count Unique Value Count Unique Value
Count Unique
sik
indmd
indc
one
3088
20
0
3096
37
all
3069
104
two
3028
3
1
3044
0
goal
2989
7
three 3019
0
2
3051
0
goal plus 2985
1
all
3043
2
3
3048
0
indu
indn
gtg
on
3095
43
on
3088
50
on
2992
27
off
3053
1
off
3046
8
off
3069
104
What Do the Proofs Look Like? We ran Vampire in a portfolio mode using the
additional options -sik one -indm 0 -indc all on the SMT-LIB UFDTLIA
problem set and recorded (i) the number of induction inferences appearing in
proofs, and (ii) the maximum depth of these inductions. The results are in
Table 4. In the majority of cases only a few induction steps are used but there
are 11 problems where more than 10 inductions are required and the proof
of induction-vmcai2015/leon/heap-goal3.smt2 uses 145 induction steps. As
suggested above, induction is relatively shallow with the maximum depth in
proofs being 6 and most necessary inductions not being nested.

Induction in Saturation-Based Proof Search
491
Table 4. Statistics from 165 successful problems in UFDTLIA.
Number of induction Count
inferences
0
44
1
82
2
16
3
6
5
2
10-50
7
50-145
4
Max induction depth Count
1
84
2
25
3
4
4
3
6
1
6.2
Comparative Evaluation
We compare the new techniques to CVC4 on the SMT-LIB benchmarks in
Table 5 running both solvers with and without induction. We currently restrict
our attention to CVC4 as this is the only solver available that runs on these
problems and supports induction (Z3 does not support induction). It is worth
noting that CVC4 was reported comparable to Zipperposition in [4] but has
improved considerably in the meantime.
Table 5. Comparative results with CVC4 on SMT-LIB benchmarks.
Logic
Size
No induction
With induction
CVC4 Vampire CVC4
Vampire
UFDT
4376 2270
2226 (2) 2275 (5) 2294 (37)
UFDTLIA 303
69
76
224 (69) 165 (9)
Overall CVC4 solves more problems but Vampire solves 48 problems that
CVC4 (or any other solver) does not. We consider it an impressive result for
a ﬁrst implementation and believe that Vampire will solve many more previ-
ously unsolved problems when more heuristics, options and induction axioms
are implemented.
It is interesting to note that the majority of problems are solvable without
induction, suggesting the need for better benchmarks. However, we also observe
that Vampire will commonly use induction to solve a problem more quickly
even when induction is not required. This is also a very interesting observation
since normally the addition of new rules other than simpliﬁcation slows down
saturation theorem provers.
The fact that the UFDTLIA benchmarks are a version of TIP allows us to
indirectly compare to other inductive theorem provers. Table 6 uses historical
data from the literature to show that Vampire is competitive with these solvers
but does not perform as well in general. This can be explained by missing features
specialised for induction that have not yet been implemented in Vampire.

492
G. Reger and A. Voronkov
Table 6. Using published data to compare to induction provers. Data for CVC4 and
Vampire taken from our experiments, other data taken from [23].
CLAM HipSpec
Zeno
Pirate
ACL2s IsaPlanner
Dafny CVC4 Vampire
isaplanner
-
80
82
87
74
47
45
71
58
clam
41
47
21
47
-
-
-
41
29
7
Related Work
We focus on explicit induction approaches, rather than implicit induction, e.g.
the inductionless induction [3] approach. Within this we identify two areas of
relevant work - the specialised area of inductive theorem proving and the general
approach of extending ﬁrst-order theorem provers with induction.
Tools that use theorem provers as backends often include induction hypothe-
ses in the input. For example, Dafny was extended to wrap SMT solvers with
an induction layer inserting useful induction hypotheses [11] within Dafny.
There are a number of inductive theorem provers such as ACL2 [7], IsaPlan-
ner [5], Zeno [19], and Hipspec [2] that diﬀer in architecture fro our approach.
ACL2 relies on a special procedure for deciding when to apply induction, Hip-
Spec is based on a technique called theory exploration and IsaPlanner and Zeno
follow a top-down approach. Therefore, in these other techniques most of the
proof search eﬀort is dedicated to deciding where to apply induction (as it is
quite costly) whereas our approach is less guided in this sense but induction
is, in some sense, cheap. In general, inductive solvers are well suited to prob-
lems that require complex induction but only require relatively simple reasoning
otherwise. Our focus is the converse case.
The main previous attempt to extend a saturation-based superposition theo-
rem prover with induction is in Zipperposition by Cruanes [4]. This approach is
formulated for (generally deﬁned) structural induction over inductive datatypes.
The main diﬀerence between this previous work and ours is the way in which [4]
puts together datatype reasoning, inductive reasoning, and reasoning by cases
using AVATAR, whereas our work keeps all three parts separate. As a result,
our approach is more general; our deﬁnition of induction does not depend on
inductive datatypes and works without AVATAR, so it can be with little eﬀort
added to existing saturation theorem provers. For example, our generality results
in the ability to implement well-founded induction.
Although we do note that Cruanes explores heuristics for where to apply
induction from the broader inductive theorem proving literature that we have
not yet explored.
Finally, we note that the experimental results of [4] have a diﬀerent focus
from our own as they focus on problems suited for inductive theorem provers
whereas our research (and our experiments) focus on problems requiring a little
bit of induction and a lot of complex ﬁrst-order reasoning.
Another approach [23] wraps superposition-based proof search in an extra
process that iteratively explores the space of possible inductions. There has also

Induction in Saturation-Based Proof Search
493
been work on incorporating induction for natural numbers into the superposition
calculus [8]. CVC4 has been extended with a set of techniques for induction [16].
There rules are similar to ours but the setting is diﬀerent as CVC4 is a DPLL(T)-
based SMT solver using quantiﬁer instantiation to handle quantiﬁers.
8
Conclusion
In this paper we introduce a new method for integrating induction into a
saturation-based theorem prover using superposition. Our approach utilises the
clause-splitting framework for case splitting. Experimental results show that the
new options allow us to solve many problems requiring complex (e.g. nested)
inductions.
Acknowledgements. We thank Andrew Reynolds for helping with obtaining CVC4
results.
References
1. Barrett, C., Fontaine, P., Tinelli, C.: The Satisﬁability Modulo Theories Library
(SMT-LIB) (2016). www.SMT-LIB.org
2. Claessen, K., Johansson, M., Ros´en, D., Smallbone, N.: Automating inductive
proofs using theory exploration. In: Bonacina, M.P. (ed.) CADE 2013. LNCS
(LNAI), vol. 7898, pp. 392–406. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-38574-2 27
3. Comon, H.: Inductionless induction. In: Handbook of Automated Reasoning (in 2
vols.), pp. 913–962 (2001)
4. Cruanes, S.: Superposition with structural induction. In: Dixon, C., Finger, M.
(eds.) FroCoS 2017. LNCS (LNAI), vol. 10483, pp. 172–188. Springer, Cham
(2017). https://doi.org/10.1007/978-3-319-66167-4 10
5. Dixon, L., Fleuriot, J.: Higher order rippling in IsaPlanner. In: Slind, K., Bunker,
A., Gopalakrishnan, G. (eds.) TPHOLs 2004. LNCS, vol. 3223, pp. 83–98. Springer,
Heidelberg (2004). https://doi.org/10.1007/978-3-540-30142-4 7
6. Gupta, A., Kov´acs, L., Kragl, B., Voronkov, A.: Extensional crisis and proving
identity. In: Cassez, F., Raskin, J.-F. (eds.) ATVA 2014. LNCS, vol. 8837, pp.
185–200. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11936-6 14
7. Kaufmann, M., Strother Moore, J., Manolios, P.: Computer-Aided Reasoning: An
Approach. Kluwer Academic Publishers, Norwell (2000)
8. Kersani, A., Peltier, N.: Combining superposition and induction: a practical real-
ization. In: Fontaine, P., Ringeissen, C., Schmidt, R.A. (eds.) FroCoS 2013. LNCS
(LNAI), vol. 8152, pp. 7–22. Springer, Heidelberg (2013). https://doi.org/10.1007/
978-3-642-40885-4 2
9. Kov´acs, L., Robillard, S., Voronkov, A.: Coming to terms with quantiﬁed reasoning.
SIGPLAN Not. 52(1), 260–270 (2017)
10. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1

494
G. Reger and A. Voronkov
11. Leino, K.R.M.: Automating induction with an SMT solver. In: Kuncak, V.,
Rybalchenko, A. (eds.) VMCAI 2012. LNCS, vol. 7148, pp. 315–331. Springer,
Heidelberg (2012). https://doi.org/10.1007/978-3-642-27940-9 21
12. Nieuwenhuis, R., Rubio, A.: Paramodulation-based theorem proving. In: Robinson,
A., Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. I, chap. 7, pp.
371–443. Elsevier Science (2001)
13. Nonnengart, A., Weidenbach, C.: Computing small clause normal forms. In: Hand-
book of Automated Reasoning (in 2 vols.), pp. 335–367 (2001)
14. Reger, G., Bjørner, N., Suda, M., Voronkov, A.: AVATAR modulo theories. In: 2nd
Global Conference on Artiﬁcial Intelligence, GCAI 2016. EPiC Series in Comput-
ing, vol. 41, pp. 39–52. EasyChair (2016)
15. Reger, G., Suda, M., Voronkov, A.: New techniques in clausal form generation.
In: 2nd Global Conference on Artiﬁcial Intelligence, GCAI 2016. EPiC Series in
Computing, vol. 41, pp. 11–23. EasyChair (2016)
16. Reynolds, A., Kuncak, V.: Induction for SMT solvers. In: D’Souza, D., Lal, A.,
Larsen, K.G. (eds.) VMCAI 2015. LNCS, vol. 8931, pp. 80–98. Springer, Heidelberg
(2015). https://doi.org/10.1007/978-3-662-46081-8 5
17. Rybina, T., Voronkov, A.: A decision procedure for term algebras with queues.
ACM Trans. Comput. Logic 2(2), 155–181 (2001)
18. Schulz, S.: E - a brainiac theorem prover. AI Commun. 15(2–3), 111–126 (2002)
19. Sonnex, W., Drossopoulou, S., Eisenbach, S.: Zeno: an automated prover for prop-
erties of recursive data structures. In: Flanagan, C., K¨onig, B. (eds.) TACAS 2012.
LNCS, vol. 7214, pp. 407–421. Springer, Heidelberg (2012). https://doi.org/10.
1007/978-3-642-28756-5 28
20. Stump, A., Sutcliﬀe, G., Tinelli, C.: StarExec, a cross community logic solving
service (2012). https://www.starexec.org
21. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure. J. Autom.
Reason. 43(4), 337–362 (2009)
22. Voronkov, A.: AVATAR: the architecture for ﬁrst-order theorem provers. In: Biere,
A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559, pp. 696–710. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-08867-9 46
23. Wand, D.: Superposition: types and induction. (Superposition: types et induction).
Ph.D. thesis, Saarland University, Saarbr¨ucken, Germany (2017)
24. Weidenbach, C.: Combining superposition, sorts and splitting. In: Robinson, A.,
Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. II, chap. 27, pp.
1965–2013. Elsevier Science (2001)

Faster, Higher, Stronger: E 2.3
Stephan Schulz1(B), Simon Cruanes2, and Petar Vukmirovi´c3
1 DHBW Stuttgart, Stuttgart, Germany
schulz@eprover.org
2 Aesthetic Integration, Austin, TX, USA
simon.cruanes.2007@m4x.org
3 Vrije Universiteit Amsterdam, Amsterdam, The Netherlands
p.vukmirovic@vu.nl
Abstract. E 2.3 is a theorem prover for many-sorted ﬁrst-order logic
with equality. We describe the basic logical and software architecture
of the system, as well as core features of the implementation. We par-
ticularly discuss recently added features and extensions, including the
extension to many-sorted logic, optional limited support for higher-order
logic, and the integration of SAT techniques via PicoSAT. Minor addi-
tions include improved support for TPTP standard features, always-on
internal proof objects, and lazy orphan removal. The paper also gives an
overview of the performance of the system, and describes ongoing and
future work.
1
Introduction
E is a fully automated theorem prover for ﬁrst-order logic with equality. It has
been under development for about 20 years, adding support for full ﬁrst-order
logic with E 0.82 in 2004, many-sorted ﬁrst-order logic with E 2.0 in 2017,
and both optional support for λ-free higher-order logic (LFHOL) and improved
handling of propositional logic with the current release, E 2.3.
The basic architecture of the clausal inference core has previously been
described in [15] (covering E 0.62), and the last updated description of E 1.8
was published in 2013 [18]. The recent support for λ-free higher-order logic is
covered in detail in [30,31]. In this paper, we describe the current state of the
prover, with a particular focus on recent developments.
E is available as free software under the GNU General Public License. Oﬃcial
point releases are available as source distributions from https://www.eprover.
org. Development versions and the full history of changes can be found at
https://github.com/eprover.
2
System Design and Architecture
The system is designed around a pipeline of largely distinct processing steps
(compare Fig. 1): Parsing, preliminary analysis, axiom selection, clausiﬁcation,
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 495–507, 2019.
https://doi.org/10.1007/978-3-030-29436-6_29

496
S. Schulz et al.
clausal preprocessing, auto-mode CNF analysis, saturation and proof object
extraction. Parsing, clausiﬁcation and saturation are necessary for actual theo-
rem proving, the other steps are optional and enabled by command line options.
Fig. 1. Logical pipeline
In the ﬁrst step, the input problem is parsed
as a set of annotated formulas, where each logi-
cal formula is represented as a shared term over a
signature including the usual logical operators, and
wrapped in a data structure that allows annotations
to capture additional extra-logical properties such
as the formula role (in particular axiom and conjec-
ture), the name of the formula, and its provenience.
The next step is an optional analysis of the
parsed problem, primarily to automatically deter-
mine if and how an axiom selection scheme should
be applied in the third step to reduce the number
of axioms. Axiom selection is based on a variant of
the SInE algorithm [8]. This step is optional. Axiom
selection can be manually triggered or blocked by
the user or triggered automatically based on the
results of the preceding analysis step.
At the core of the prover is a refutational proof
procedure for ﬁrst-order clausal logic with equal-
ity. It is based on the superposition calculus (with
some extensions and modiﬁcations), and works on a
clausal representation of the problem. The clausiﬁ-
cation step converts the full ﬁrst-order problem into
a set of clauses. It is based on the ideas presented
by Nonnengart and Weidenbach [13]. As usual with
refutational theorem provers, if an explicit conjec-
ture is given, it is negated before clausiﬁcation, so
that the resulting clause set is unsatisﬁable if the
conjecture logically follows from the axioms.
The prover optionally performs preprocessing of
the clause set. Preprocessing removes redundant lit-
erals and tautologies, optionally unfolds some or
all equational deﬁnitions, and orders literals and
clauses in a canonical ordering so that the prover
behaves in a more deterministic way.
The resulting clause set can be extracted after
this stage. Indeed, several other clausal provers use
E as an external clausiﬁer. If E continues, the clause set is then analyzed to
determine the search control strategy to be used by the inference core. The
superposition calculus is parameterized by a term ordering and (optionally) a
literal selection function. The implementation uses a variant of the given-clause
algorithm (Fig. 2). The main additional search parameter for this algorithm is

Faster, Higher, Stronger: E 2.3
497
the scheme for the selection of the given clause for each iteration of the main
loop. However, there are a large number of additional ﬂags controlling e.g. dif-
ferent options for simpliﬁcation. All aspects of the search strategy can again be
explicitly set by the user, or automatically determined by the automatic mode
of the prover.
The inference core performs a classical saturation of the clause set, option-
ally interspersed with calls to the CDCL-based SAT-solver PicoSAT [3] to detect
conﬂicts hidden in (so far) unprocessed clauses. The procedure terminates suc-
cessfully when either the empty clause has been derived directly, when the SAT-
solver detects unsatisﬁability of the proof state, or when the clause set is satu-
rated. It terminates unsuccessfully, if it cannot reach success within user-deﬁned
limits (e.g. CPU time, memory, iterations, elementary term operations).
In the case of success, an optional ﬁnal step can extract a proof object from
the search state and present the proof to the user.
3
Calculus and Implementation
3.1
Superposition for Many-Sorted Logic
E was originally built around untyped ﬁrst-order logic, distinguishing only pred-
icate symbols (returning a Boolean value) and function symbols (returning an
individual, represented as a term). Variables would implicitly range over all
terms, and hence could be bound to any term. As of version 2.0, the prover has
been extended to support many-sorted ﬁrst order logic in the style described by
Sutcliﬀe, Schulz, Claessen and Baumgartner [27].
In this logic, every plain function symbol has an associated function type,
accepting a ﬁxed number of arguments of deﬁned sorts, and constructing a term
of a deﬁned return sort. Predicate symbols also accept terms of the correct sorts
only. An exception is the equality-predicate, which is ad-hoc polymorphic, but
requires terms of the same sort for both arguments.
Supporting many-sorted logic unlocks access to many useful features:
– expressing some size constraints over models, using axioms such as ∃a, b :
τ, ∀x : τ, x ≃a ∨x ≃b;
– enabling more eﬃcient encodings from systems with richer logics, such as
proof assistants [4] or program veriﬁcation tools [5]. While types can be
encoded within plain ﬁrst-order logic, these encodings tend to bloat formulas,
adding sort predicates to every axiom, and to bloat terms, which reduces the
eﬀectiveness of many simpliﬁcation rules;
– supporting some built-in theories, as SMT solvers typically do; indeed, the
SMT-LIB language [2] is typed;
– supporting the FOOL extension [10] of ﬁrst-order logic and its realization
in the TFX format [26], allowing Boolean sub-terms as well as let and
if-then-else constructs.

498
S. Schulz et al.
Basic support for interpreted numbers is planned for the near future, full support
for TFX is already in progress and will most likely be included in the next release.
The superposition calculus readily generalizes to this logic. E implements the
standard inference rules from [1]: Superposition, equality resolution, and equality
factoring. In addition, it implements a large array of simpliﬁcation rules, the most
important of which are unconditional rewriting, subsumption, equational literal
cutting and contextual literal cutting. A more detailed description of the calculus
(and its realization in the proof procedure) is provided in the manual [19].
In practice, supporting simple types requires every variable and term to be
annotated with its sort. Uniﬁcation and retrieval of terms from indices (e.g. for
demodulation) check that types are compatible before binding a variable to any
given term. This is what prevents an axiom such as ∀x : side, x ≃left∨x ≃right
to rewrite another clause’s subterm of an incompatible type. This change had
negligible impact on performance, but signiﬁcantly improves the expressiveness
of the logic.
E can currently parse the TFF0 sub-grammar of the TPTP TFF format [27],
and prints typed terms and formulas using the same syntax.
Sorts were originally represented as indices into a sort table. The LFHOL
extension of E 2.3 [31] further generalizes this representation to support higher-
order simple types and partially applied terms. The implementation of types
now uses a lightweight term-like structure, in which complex types are build
from basic sorts and the arrow operator. Like terms, types are perfectly shared
for eﬃcient type equality comparisons.
3.2
Implementation
The system is being developed in C, providing good performance and maximal
portability. The code of the prover proper largely restricts itself to features from
C99, with some POSIX extensions. It has been successfully built on a large range
of diﬀerent UNIX-style operating systems, in particular OS-X/macOS (with both
LLVM and GCC as compilers) and Linux, the two main development and testing
platforms. It has also been compiled and run under versions of Windows, using
the CygWin libraries for POSIX/UNIX compatibility.
In the past, supporting software for testing and optimizing the system has
been built in a number of scripting languages, but more recently has been largely
moved to Python.
While C is an excellent language for performance and portability, it oﬀers a
relatively small number of built-in data structures and programming constructs.
As a consequence, E has been built on a layer of libraries providing generic
data types such as unlimited size stacks, splay trees, dynamic arrays, as well as
convenient abstractions for a number of operating system services.
On top of these generic libraries, the prover implements logical data types
and operations. At the heart of the system is the term bank data type, an
eﬃcient and garbage-collected data structure originally for aggressively shared
ﬁrst-order terms. All persistent terms are inserted in a bottom-up manner into
this term bank. Thus, identical terms are represented by identical pointers. This

Faster, Higher, Stronger: E 2.3
499
results in a saving in the number of cells needed to represent the proof state
of several orders of magnitude [11]. It also enables us to precompute a number
of properties and store them in the term cells. Examples include the number of
function symbols in the term and the number of variable occurrences. Thus, we
can e.g. decide if a shared term is ground in constant time. More importantly,
we can cache the result of rewrite attempts at the term level—in the case of
success with a link to the result (and, for proof reconstruction, with the clause
used as a side premise), in the case of failure with the age of the youngest clause
tried, so that future attempts can be restricted to newer clauses.
The term bank data structure and its API has proven to be eﬃcient and
convenient. In particular, the mark-and-sweep garbage collector makes the cre-
ation and destruction of terms very convenient and reduces programmer eﬀort
and errors. As a result, shared terms are now also used to represent formulas
and in some roles even clauses (which, as of E 2.2, are parsed as a special case
of formulas).
Literals and clauses for the inference core are implemented as dedicated data
structures. Internally, all literals are equational. In addition to the two terms
making up the equation, literals include polarity (positive or negative), a number
of Boolean ﬂags, and a pointer for creating linked lists. Clauses consist of such a
linked list of literals, wrapped in a container for meta-data, heuristic evaluations,
and information about the derivation of the clause.
Proof Procedure. Figure 2 depicts the main saturation procedure. It is a mod-
iﬁed version of the DISCOUNT loop [6], one of the variants of the given-clause
algorithm. The proof state is represented by two disjoint subsets of clauses,
the processed clauses P and the unprocessed clauses U. Initially, all clauses are
unprocessed. At each iteration of the main loop, the prover heuristically selects
a given clause from U, adds it to P, and performs all generating inferences
between this clause and all clauses in P. The resulting new clauses are added to
U. This maintains the invariant that all direct consequences between clauses in
P have been performed. Forward simpliﬁcation is performed on the given clause
(using clauses in P as side premises) before it is used for generation, and on new
clauses before they are added to U. In addition, clauses in P are back-simpliﬁed
with the given clause, and simpliﬁed clauses are moved to U. This maintains
the additional invariant that the clauses in P are interreduced, or maximally
simpliﬁed with respect to other clauses in P.
In addition to saturation, the current version may trigger a propositional
check for unsatisﬁability of a grounded version of the proof state, as described
below.
Internal Proof Objects. Originally, E only oﬀered the option to log all infer-
ences to an external medium and then generate a proof object in a post-mortem
analysis. Since these logs often reached extreme sizes, this was costly and not
even practically possible for long runs.

500
S. Schulz et al.
Fig. 2. The modiﬁed given-clause algorithm as implemented in E
With E 1.8, we ﬁnally found a way to use the invariants of the given-clause
algorithm to very compactly represent the derivation graph internally [22]. Since
the overhead in time and memory turned out to be negligible, we have simpliﬁed
the code and now always build an internal proof object. In addition to eﬃciently
providing a checkable proof object in TPTP syntax [28], the presence of the
derivation information enables the detection of vacuous proofs (based on an
inconsistent axiomatization). It also enables an elegant lazy implementation of
orphan killing. An orphan is a generated clause which has lost at least one of its
parents to interreduction before being selected for processing, and which hence
can be deleted as well. Older versions of E maintained an explicit list of direct
descendants for processed clauses, and actively removed such descendents if the
parent clause became redundant. As of E 2.2, we instead check the status of
the parent clauses (which are either active or archived) only when the clause is
selected for processing. This removes the bookkeeping overhead and simpliﬁes
both code and data structures.
In addition to the generation of proof objects, the system supports the pro-
posed TPTP standard for answers [29]. An answer is an instantiation for an
existential conjecture (or query) that makes the conjecture true. E can supply

Faster, Higher, Stronger: E 2.3
501
bindings for the outermost existentially quantiﬁed variables in a TPTP formula
with type question.
Indexing. Most of the generating and simplifying inference rules require two
premises - the main premise and a side premise. For generating inferences, one
of the inference partners is the given clause, the other one is a clause in P. E
uses a ﬁngerprint index [16] to eﬃciently ﬁnd clauses with (sub-)terms that are
uniﬁable with the maximal terms of inference literals of the given clause.
For simpliﬁcation, the DISCOUNT loop distinguishes two situations. In for-
ward simpliﬁcation, all clauses in P are used as side premises to simplify a given
clause - either the given clause, or a newly generated clause. E uses perfect
discrimination trees [12] with size- and age-constraints for forward rewriting.
Backward simpliﬁcation uses a single clause to simplify all clauses from P. E
uses ﬁngerprint indexing for backwards rewriting. Subsumption and contextual
literal cutting use feature vector indexing [17], a clause indexing technique that
supports the ﬁnding of both generalizations and instances.
SAT Integration. SAT solvers have greatly improved performance in the last
decades, and can handle propositional problems that are far beyond the practical
scope of classical ﬁrst-order provers with ease. Following other attempts [9,14],
we want to utilize this power to improve the performance of the prover both
for problems with a signiﬁcant propositional component as well as for ﬁrst-order
problems where contradictory instances are generated early, but are not detected
until all involved clauses have been selected for processing.
We have thus integrated the CDCL solver PicoSAT [3] with E. The satura-
tion loop is periodically interrupted, and all clauses in the current proof state are
grounded, i.e. all variables are bound to a constant of the proper sort. The instan-
tiated clauses are eﬃciently translated into propositional clauses and handed to
PicoSAT. Our original implementation used PicoSAT as an external tool via
ﬁles and UNIX pipes [20], but as of E 2.2 we link PicoSAT as a library and
use its documented C API. If PicoSAT refutes the given propositional problem,
E extracts the unsatisﬁable core and relates it back to the original ﬁrst-order
clauses to construct a proof object. If PicoSAT fails to ﬁnd unsatisﬁability, the
saturation is resumed.
E users can control this process by choosing the following options:
– the point when PicoSAT is called – currently it is after N generated or pro-
cessed clauses or after N new subterms created, where N is a user-chosen
constant
– the way variables are instantiated with constants – some of the options are to
use the most or the least frequent constant, a fresh constant (for each type),
or a frequent or infrequent constant appearing in the conjecture.
We have only started to explore the parameter space. With current conﬁg-
urations, E ﬁnds about 1% more proofs on TPTP when PicoSAT is enabled.
While this number seems low, it is signiﬁcant among hard problems - 90% of
solutions are found before the ﬁrst run of the SAT solver.

502
S. Schulz et al.
3.3
Higher-Order Logic Support
One of the most recent updates for E adds support for λ-free higher-order logic
(LFHOL). Supporting richer logics in a highly optimized theorem prover without
compromising performance required some changes to fundamental data struc-
tures and algorithms. Here we will only brieﬂy describe the changes. We refer
the reader to [31] for details.
LFHOL is a fragment of simply-typed higher-order logic, with no λ-abstra-
ction, but supporting functional variables and partial application of terms. It
is expressive enough to axiomatize, for example, frequently used functional pro-
gramming combinators such as map. We have generalized E’s term representa-
tion to allow applied variables, as well as the type system to support partially
applied terms. The most laborious change was the extension of all three index-
ing data structures to support more complex terms. Our experimental results
show that E extended to support LFHOL natively outperforms the traditional
encoding-based approaches. E 2.3 users can specify LFHOL problems in TPTP
THF syntax. Support for LFHOL can be speciﬁed as an option at compile time.
3.4
Search Control
All provers for ﬁrst-order logic search for proofs in an inﬁnite search space.
While fairness is a minimal requirement for completeness, practical performance
depends critically on making the right choices. The prover supports a large
number of options for controlling preprocessing and actual search control.
The most important parameters for the saturation are the term ordering,
the literal selection strategy, and clause evaluation heuristics. Term orderings
primarily determine in which direction equations can be applied (and as a con-
sequence, which terms are overlapped for superposition inferences), and which
literals are maximal and hence available for inferences. Literal selection can be
used to overwrite the default inference literals and restrict inferences to partic-
ular (negative) literals. Finally, clause evaluations determine the order in which
the given-clause algorithm processes clauses. In the simplest case, this is a single
value, representing the number of symbols in the clause (known as a symbol-
counting heuristic—smaller is better). E generalizes this concept and allows the
user to specify an arbitrary number of priority queues and a weighted round-
robin scheme that determines how many clauses are picked from each queue.
Each queue is ordered by a particular evaluation function. A major feature is
the use of goal-directed evaluation functions. These give a lower weight to sym-
bols that occur in the goal, and a higher weight to other symbols, thus preferring
clauses likely connected to the conjecture. We have so far only evaluated a small
part of the possibility space opened by this design [21].
More complex clause evaluation functions allow the system to evaluate
clauses based on a user-provided watch list. Clauses that match clauses on the
watch list are preferred over other clauses. Watch lists can either be created based
on human intuition, by manual analysis of similar proofs, or by automated min-
ing of related proofs. The watch list mechanism in E has been improved several

Faster, Higher, Stronger: E 2.3
503
times, with the current incarnation [7] being successfully used for challenging
mathematical problems.
Automatic Prover Conﬁguration. Finding good heuristics for a given prob-
lem is challenging even for an experienced user. E supports a number of auto-
matic modes that analyze the problem and apply either a single strategy or a
schedule of several strategies. The selection of strategies and generation of sched-
ules for each class of problems is determined automatically by analyzing previous
performance of the prover on similar problems.
3.5
Usage and Formats
Recent versions of E have made minor changes to the usage and options, as well as
to I/O formats. These are mostly conservative, i.e. they should not signiﬁcantly
impact integration of the prover into larger systems.
The ﬁrst such change is automatic detection of the input format. E supports
three diﬀerent formats: The original LOP format inherited from SETHEO, the
old TPTP format (TPTP format version 1/2) and the current TPTP format
version 3. The prover has originally used command line options to select the
desired format. However, with E 1.9.1, we introduced automatic detection of the
input format (and automatic setting of the corresponding output format). This
feature is implemented by checking the ﬁrst proper input token, and selecting
TPTP-3 format if it is one of the TPTP-3 language identiﬁers (cnf, fof, . . . ),
or include, TPTP-2 format if it is one of input clause or input formula,
and LOP otherwise. It is not completely foolproof (it can e.g. misidentify LOP
input that uses TPTP-3 language identiﬁers as normal function symbols), but it
works very well in practice. If it misidentiﬁes the format, it fails towards the more
modern TPTP-3 format. We have not yet encountered that situation. If TPTP-3
syntax is identiﬁed, the output syntax is also set to TPTP-3, otherwise it is set
to PCL2 (E’s original, more limited format). These choices can be independently
overwritten via explicit use of the existing command line options.
The second change is more strict checking of TPTP language constraints.
In particular, E now requires FOF, TFF and TCF style formulas to be fully
quantiﬁed. CNF formulas are implicitly considered universally quantiﬁed. This
change was prompted by frequent user errors dur to misjudging quantiﬁer scopes
and hence inadvertently creating free variables. Such cases will now be ﬂagged
as errors.
Similarly, E will now automatically type numerical constants as $int/$rat
or $real (which will result in errors if they are used in untyped formulas) unless
they are explicitly marked as free constants by a command line switch.
Finally, the prover now can detect proofs resulting from an inconsistent axiom
set, and explicitly report the problem status as ContradictoryAxioms.

504
S. Schulz et al.
4
Experimental Evaluation
We have performed an evaluation of E 2.3 on the 16094 CNF and FOF problems
of the TPTP problem library [25], release 7.2.0. Experiments were run on the
StarExec cluster [23], i.e. on machines with an Intel Xeon E5/2.40 GHz processor
and at least 128 GB of main memory. We used a CPU time limit of 300 s per
problem and the prover was conﬁgured to optimize memory usage to at most
2 GB.
Table 1. Proofs and (counter-)saturations found within 300 s
Strategy
Class size
UEQ
(1193)
CNE
(2383)
CEQ
(4442)
FNE
(1771)
FEQ
(6305)
All
(16094)
Auto (proofs)
814
1603
2360
1156
3704
9637
Auto (sat)
16
280
220
304
203
1023
Auto (all)
830
1883
2580
1460
3907
10660
E 1.8 Auto (all)
812
1851
2561
1456
3909
10589
Schedule (proofs)
829
1625
2470
1165
3961
10050
Schedule (sat)
16
286
219
307
206
1034
Schedule (all)
845
1911
2689
1472
4167
11084
E 1.8 Schedule (all)
828
1889
2655
1463
4113
10948
Table 1 summarizes the results of the experiment. We list the performance
for unit-equational problems, clausal and non-clausal problems with and without
equality. The two tested strategies are the automatic mode and the automatic
strategy scheduler. For each strategy, we list the number of proofs found, the
number of counter-saturations (i.e. saturations not including the empty clause),
and the total number of successes. For comparison, we have also included data
for E 1.8, the last version with a formally published description. The full data,
including the exact command line options, is available at http://www.eprover.
eu/E-eu/E 2.3.html.
5
Future Work
While E is quite mature and widely used, there is a number of projects for further
improvement - in data structures, search control, and supported language. In
particular, terms can be more compactly represented with variable length arrays
(a feature not yet available in standard C when the data type was ﬁrst designed),
and priority queues can be more eﬃciently realized with heaps. Feature vector
indexing works very well for classical theorem proving problems, but is less than
optimal for problems with very large and sparsely used signatures. We plan to
develop it into a more adaptive and eﬃcient variant.

Faster, Higher, Stronger: E 2.3
505
On the language side, we plan to support the full TFX language [26] and
hence the FOOL logic. We also plan to add at least basic support for interpreted
arithmetic sorts.
A lot of recent improvements have only been evaluated in isolation, not in
concert. A major project is such a large-scale evaluation and a regeneration of
the automatic modes to make better use of the new features.
Finally, E has grown over more than 20 years now. While we have tried to
integrate new techniques in as modular and elegant a way as possible, some of
the higher level-code can proﬁt from signiﬁcant refactoring and streamlining.
6
Conclusion
E is a mature and yet still developing fully automated theorem prover for ﬁrst-
order logics and some extensions. It has good performance, as demonstrated in
the yearly CASC competitions [24].
The prover is available as free and open source software, and has been used
and extended by a large number of parties. We hope and expect that this success
will continue throughout the third decade of its lifetime.
References
1. Bachmair, L., Ganzinger, H.: Rewrite-based equational theorem proving with selec-
tion and simpliﬁcation. J. Logic Comput. 3(4), 217–247 (1994)
2. Barrett, C., Stump, A., Tinelli, C.: The SMT-lib standard: version 2.0. In: Proceed-
ings of the 8th International Workshop on Satisﬁability Modulo Theories (Edin-
burgh, UK) (2010). http://homepage.cs.uiowa.edu/∼tinelli/papers/BarST-SMT-
10.pdf
3. Biere, A.: PicoSAT essentials. J. Satisﬁability Boolean Model. Comput. 4, 75–97
(2008)
4. Blanchette, J.C., Kaliszyk, C., Paulson, L.C., Urban, J.: Hammering towards QED.
J. Formal Reason. 9(1), 101–148 (2016). https://doi.org/10.6092/issn.1972-5787/
4593
5. Bobot, F., Filliˆatre, J.C., March´e, C., Paskevich, A.: Why3: shepherd your herd
of provers. In: First International Workshop on Intermediate Veriﬁcation Lan-
guages, Boogie 2011, Wroclaw, Poland, pp. 53–64, August 2011. http://proval.
lri.fr/publications/boogie11ﬁnal.pdf
6. Denzinger, J., Kronenburg, M., Schulz, S.: DISCOUNT: a distributed and learning
equational prover. J. Autom. Reason. 18(2), 189–198 (1997). Special Issue on the
CADE 13 ATP System Competition
7. Goertzel, Z., Jakub˚uv, J., Schulz, S., Urban, J.: ProofWatch: watchlist guidance
for large theories in E. In: Avigad, J., Mahboubi, A. (eds.) ITP 2018. LNCS, vol.
10895, pp. 270–288. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-
94821-8 16
8. Hoder, K., Voronkov, A.: Sine Qua Non for large theory reasoning. In: Bjørner,
N., Sofronie-Stokkermans, V. (eds.) CADE 2011. LNCS (LNAI), vol. 6803, pp.
299–314. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-22438-
6 23

506
S. Schulz et al.
9. Korovin, K.: Inst-Gen – a modular approach to instantiation-based automated
reasoning. In: Voronkov, A., Weidenbach, C. (eds.) Programming Logics. LNCS,
vol. 7797, pp. 239–270. Springer, Heidelberg (2013). https://doi.org/10.1007/978-
3-642-37651-1 10
10. Kotelnikov, E., Kov´acs, L., Reger, G., Voronkov, A.: The Vampire and the FOOL.
In: Avigad, J., Chlipala, A. (eds.) Proceedings of the 5th ACM SIGPLAN Confer-
ence on Certiﬁed Programs and Proofs, Saint Petersburg, USA, pp. 37–48. ACM
(2016)
11. L¨ochner, B., Schulz, S.: An evaluation of shared rewriting. In: de Nivelle, H., Schulz,
S. (eds.) Proceedings of the 2nd International Workshop on the Implementation of
Logics, pp. 33–48. MPI Preprint, Max-Planck-Institut f¨ur Informatik, Saarbr¨ucken
(2001)
12. McCune, W.: Experiments with discrimination-tree indexing and path indexing
for term retrieval. J. Autom. Reason. 9(2), 147–167 (1992)
13. Nonnengart, A., Weidenbach, C.: Computing small clause normal forms. In: Robin-
son, A., Voronkov, A. (eds.) Handbook of Automated Reasoning, vol. I, chap. 5,
pp. 335–367. Elsevier Science and MIT Press (2001)
14. Reger, G., Suda, M., Voronkov, A.: Playing with AVATAR. In: Felty, A.P., Mid-
deldorp, A. (eds.) CADE 2015. LNCS (LNAI), vol. 9195, pp. 399–415. Springer,
Cham (2015). https://doi.org/10.1007/978-3-319-21401-6 28
15. Schulz, S.: E – a brainiac theorem prover. J. AI Commun. 15(2/3), 111–126 (2002)
16. Schulz, S.: Fingerprint indexing for paramodulation and rewriting. In: Gramlich,
B., Miller, D., Sattler, U. (eds.) IJCAR 2012. LNCS (LNAI), vol. 7364, pp. 477–
483. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-31365-3 37
17. Schulz, S.: Simple and eﬃcient clause subsumption with feature vector indexing.
In: Bonacina, M.P., Stickel, M.E. (eds.) Automated Reasoning and Mathematics.
LNCS (LNAI), vol. 7788, pp. 45–67. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-36675-8 3
18. Schulz, S.: System description: E 1.8. In: McMillan, K., Middeldorp, A., Voronkov,
A. (eds.) LPAR 2013. LNCS, vol. 8312, pp. 735–743. Springer, Heidelberg (2013).
https://doi.org/10.1007/978-3-642-45221-5 49
19. Schulz, S.: E 2.0 User Manual. EasyChair preprint no. 8 (2018). https://doi.org/
10.29007/m4jw
20. Schulz, S.: Light-weight integration of SAT solving into ﬁrst-order reasoners - ﬁrst
experiments. In: Kov´acs, L., Voronkov, A. (eds.) Vampire 2017, Proceedings of the
4th Vampire Workshop. EPiC Series in Computing, vol. 53, pp. 9–19. EasyChair
(2018). https://doi.org/10.29007/89kc. https://easychair.org/publications/paper/
94vW
21. Schulz, S., M¨ohrmann, M.: Performance of clause selection heuristics for saturation-
based theorem proving. In: Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS
(LNAI), vol. 9706, pp. 330–345. Springer, Cham (2016). https://doi.org/10.1007/
978-3-319-40229-1 23
22. Schulz, S., Sutcliﬀe, G.: Proof generation for saturating ﬁrst-order theorem provers.
In: Delahaye, D., Woltzenlogel Paleo, B. (eds.) All About Proofs, Proofs for All,
Mathematical Logic and Foundations, vol. 55, pp. 45–61. College Publications,
London, January 2015
23. Stump, A., Sutcliﬀe, G., Tinelli, C.: StarExec: a cross-community infrastructure
for logic solving. In: Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014.
LNCS (LNAI), vol. 8562, pp. 367–373. Springer, Cham (2014). https://doi.org/10.
1007/978-3-319-08587-6 28

Faster, Higher, Stronger: E 2.3
507
24. Sutcliﬀe, G.: The 8th IJCAR automated theorem proving system competition-
CASC-J8. AI Commun. 29(5), 607–619 (2016)
25. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure - from CNF
to TH0, TPTP v6.4.0. J. Autom. Reason. 59(4), 483–502 (2017)
26. Sutcliﬀe, G., Kotelnikov, E.: TFX: the TPTP extended typed ﬁrst-order form. In:
Konev, B., Urban, J., R¨ummer, P. (eds.) Proceedings of the 6th Workshop on Prac-
tical Aspects of Automated Reasoning (PAAR), Oxford, UK. CEUR Workshop
Proceedings, vol. 2162, pp. 72–87 (2018). http://ceur-ws.org/Vol-2162/#paper-07
27. Sutcliﬀe, G., Schulz, S., Claessen, K., Baumgartner, P.: The TPTP typed ﬁrst-
order form with arithmetic. In: Bjørner, N., Voronkov, A. (eds.) LPAR 2012. LNCS,
vol. 7180, pp. 406–419. Springer, Heidelberg (2012). https://doi.org/10.1007/978-
3-642-28717-6 32
28. Sutcliﬀe, G., Schulz, S., Claessen, K., Van Gelder, A.: Using the TPTP language for
writing derivations and ﬁnite interpretations. In: Furbach, U., Shankar, N. (eds.)
IJCAR 2006. LNCS (LNAI), vol. 4130, pp. 67–81. Springer, Heidelberg (2006).
https://doi.org/10.1007/11814771 7
29. Sutcliﬀe, G., Stickel, M., Schulz, S., Urban, J.: Answer extraction for TPTP. http://
www.cs.miami.edu/∼tptp/TPTP/Proposals/AnswerExtraction.html. Accessed 08
July 2013
30. Vukmirovi´c, P., Blanchette, J.C., Cruanes, S., Schulz, S.: Extending a brainiac prover
to lambda-free higher-order logic - report version. Technical report, Matryoshka
Project (2018). http://matryoshka.gforge.inria.fr/pubs/ehoh report.pdf
31. Vukmirovi´c, P., Blanchette, J.C., Cruanes, S., Schulz, S.: Extending a brainiac
prover to lambda-free higher-order logic. In: Vojnar, T., Zhang, L. (eds.) TACAS
2019. LNCS, vol. 11427, pp. 192–210. Springer, Cham (2019). https://doi.org/10.
1007/978-3-030-17462-0 11

Certiﬁed Equational Reasoning
via Ordered Completion
Christian Sternagel(B)
and Sarah Winkler(B)
Department of Computer Science, University of Innsbruck, Innsbruck, Austria
{christian.sternagel,sarah.winkler}@uibk.ac.at
Abstract. On the one hand, equational reasoning is a fundamental part
of automated theorem proving with ordered completion as a key tech-
nique. On the other hand, the complexity of corresponding, often highly
optimized, automated reasoning tools makes implementations inherently
error-prone. As a remedy, we provide a formally veriﬁed certiﬁer for
ordered completion based techniques. This certiﬁer is code generated
from an accompanying Isabelle/HOL formalization of ordered rewriting
and ordered completion incorporating an advanced ground joinability cri-
terion. It allows us to rigorously validate generated proof certiﬁcates from
several domains: ordered completion, satisﬁability in equational logic,
and conﬂuence of conditional term rewriting.
Keywords: Equational reasoning · Ordered completion ·
Ground joinability · Certiﬁcation
1
Introduction
Equational reasoning constitutes a main area of automated theorem proving
in which completion has evolved as a fundamental technique [8]. Completion
aims to transform a given set of equations into a terminating and conﬂuent
rewrite system that induces the same equational theory. Thus, on success, such
a rewrite system can be used to decide equivalence of terms with respect to
the initial set of equations. The original completion procedure may fail due
to unorientable equations. As a remedy to this problem, ordered completion—
also known as unfailing completion—was developed [3]. As the name suggests,
unfailing completion always yields a result (which may however be inﬁnite and
thus take inﬁnitely many inference steps to compute). This time, the result is
an ordered rewrite system (given by a ground total reduction order, a set of
rules which are oriented with respect to this order, and a set of equations) that
is still terminating, but in general only ground conﬂuent (that is, conﬂuent on
ground terms). Thus, the resulting system can be used to decide equivalence
of ground terms with respect to the initial set of equations. This suﬃces for
This work is supported by Austrian Science Fund (FWF) projects T789 and P27502.
c
⃝The Author(s) 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 508–525, 2019.
https://doi.org/10.1007/978-3-030-29436-6_30

Certiﬁed Equational Reasoning via Ordered Completion
509
many practical purposes: A well-known success story of ordered completion is the
solution of the long-standing Robbins conjecture [10], followed by applications
to other problems from (Boolean) algebra [11]. More recent applications include
the use of ordered completion in algebraic data integration [14] and in conﬂuence
proofs of conditional term rewrite systems [20].
As an introductory example, let us illustrate ordered completion on the fol-
lowing set of equations describing a group where all elements are self-inverse:
f(x, y) ≈f(y, x)
f(x, f(y, z)) ≈f(f(x, y), z)
f(x, x) ≈0
f(x, 0) ≈x
Using ordered completion, the tool MædMax [24] transforms it into the following
rules (→) and equations (≈), together with a suitable ground total reduction
order > that orients all rules from left to right.
f(x, f(x, y)) →f(0, y)
f(x, f(y, x)) →f(0, y)
f(x, x) →0
f(x, 0) →x
f(f(x, y), z) →f(x, f(y, z))
f(0, x) →x
f(x, f(y, z)) ≈f(y, f(x, z))
f(x, y) ≈f(y, x)
This ordered rewrite system can be used to decide a given equation between
ground terms, by checking whether the unique normal forms (with respect to
ordered rewriting using >) of both terms coincide.
Automated reasoning tools are highly sophisticated pieces of software, not
only because they implement complex calculi, but also due to their high degree
of optimization. Consequently, their implementation is inherently error-prone.
To improve their trustability we follow a two-staged certiﬁcation approach
and (1) add the relevant concepts and results regarding ordered completion to a
formal library using the proof assistant Isabelle/HOL [12] (version Isabelle2019),
and from there (2) code generate [5] a trusted certiﬁer that is correct by con-
struction. Our formalization strengthens the originally proposed procedure [3]
by using a relaxed version of the inference system, while incorporating a stronger
ground joinability criterion [9]. Our certiﬁer allows us to rigorously validate gen-
erated proof certiﬁcates from several domains: ordered completion, satisﬁability
in equational logic, and conﬂuence of conditional term rewriting.
More speciﬁcally, our contributions are as follows:
• We extend the existing Isabelle Formalization of Rewriting1 (IsaFoR for
short) by ordered rewriting and a generalization of the ordered completion
calculus oKB [3], and prove the latter correct for ﬁnite completion runs with
respect to ground total reduction orders (Sect. 3).
• We establish ground totality of the Knuth-Bendix order and the lexicographic
path order in IsaFoR (Sect. 3).
• We formalize two criteria for ground joinability [3,9] known from the litera-
ture, that allow us to apply our previous results to concrete completion runs
(Sect. 4). In fact, we present a slightly more powerful version of the latter, and
ﬁx an error in its proof, as described below.
1 http://cl-informatik.uibk.ac.at/isafor

510
C. Sternagel and S. Winkler
• We apply ordered completion to satisﬁability in equational logic and infeasi-
bility of conditions in conditional rewriting (Sect. 5).
• We extend the XML-based certiﬁcation problem format (CPF for short) [18]
by certiﬁcates for ordered completion and formalize corresponding executable
check functions that verify the supplied derivations (Sect. 6).
• Finally, we extend the completion tool MædMax [24], as well as the conﬂu-
ence tool ConCon [20] by certiﬁcate generation and evaluate our approach on
existing benchmarks (Sect. 7).
As a result, CeTA (the certiﬁer accompanying IsaFoR) can now certify (a)
ordered completion proofs and (b) satisﬁability proofs of equational logic pro-
duced by the tool MædMax, as well as (c) conditional conﬂuence proofs by Con-
Con where infeasibility of critical pairs is established via equational logic. To
the best of our knowledge, CeTA constitutes the ﬁrst proof checker in all of these
domains.
In the remainder we provide hyperlinks (marked by
) to an HTML render-
ing of our formalization.
This work is an extension of an earlier workshop paper [19]. Further note
that the IsaFoR formalization of the results in this paper is, apart from very
basic results on (ordered) rewriting, entirely disjoint from our previous formal-
ization together with Hirokawa and Middeldorp [6]. On the one hand, we con-
sider a relaxed completion inference system where more inferences are allowed.
This is possible since we are only interested in ﬁnite completion runs. On the
other hand, we employ a stronger ground joinability criterion. Another major
diﬀerence is that our new formalization enables actual certiﬁcation of ordered
completion based techniques, which is not the case for our work with Hirokawa
and Middeldorp.
2
Preliminaries
In the sequel, we use standard notation from term rewriting [2]. Let T (F, V) denote
the set of all terms over a signature F and an inﬁnite set of variables V, and T (F)
the set of all ground terms over F (that is, terms without variables). A substitution
σ is a mapping from variables to terms. As usual, we write tσ for the application of
σ to the term t. A variable permutation (or renaming) π is a bijective substitution
such that π(x) ∈V for all x ∈V. Given an equational system (ES) E, we write
E↔to denote its symmetric closure E ∪{t ≈s | s ≈t ∈E}. A reduction order
is a proper and well-founded order on terms which is closed under contexts and
substitutions. It is F-ground total if it is total on T (F). In the remainder we often
focus on the Knuth-Bendix order (KBO), written >kbo, and the lexicographic path
order (LPO), written >lpo. Given a reduction order > and an ES E, the term rewrite
system (TRS) E> consists of all rules sσ →tσ such that s ≈t ∈E↔and sσ > tσ.
Given a reduction order >, an extended overlap consists of two variable-
disjoint variants ℓ1 ≈r1 and ℓ2 ≈r2 of equations in E↔such that p ∈PosF(ℓ2)
and ℓ1 and ℓ2|p are uniﬁable with most general uniﬁer μ. An extended overlap
which in addition satisﬁes r1μ ̸> ℓ1μ and r2μ ̸> ℓ2μ gives rise to the extended

Certiﬁed Equational Reasoning via Ordered Completion
511
critical pair ℓ2[r1]pμ ≈r2μ. The set CP>(E) consists of all extended critical
pairs between equations in E. A relation on terms is (ground) complete, if it is
terminating and conﬂuent (on ground terms). A TRS R is (ground) complete
whenever the induced rewrite relation →R is. Finally, we say that a TRS R is a
presentation of an ES E, whenever ↔∗
E = ↔∗
R (that is, their equational theories
coincide).
A substitution σ is grounding for a term t if σ(x) ∈T (F) for all x ∈Var(t).
Two terms s and t are called ground joinable over a rewrite system R, denoted
s ↓g
R t if sσ ↓R tσ for all substitutions σ that are grounding for s and t.
For any complete rewrite relation →, we denote the (necessarily unique) normal
form of a term t (that is, the term u such that we have t →∗u but u ̸→v for all
terms v) by t↓. By an ordered rewrite system we mean a pair (E, R), consisting of
an ES E and a TRS R, together with a reduction order >. Then, ordered rewriting
is rewriting with respect to the TRS R∪E>. Note that ordered rewriting is always
terminating if R ⊆>. Take commutativity x ∗y ≈y ∗x for example, which causes
nontermination when used as a rule in a TRS. Nevertheless, the ordered rewrite
system ({x ∗y ≈y ∗x}, ∅) together with KBO, say with precedence ∗> a > b,
is terminating and we can for example rewrite a ∗b to b ∗a since applying the
substitution {x →a, y →b} to the commutativity equation results in a KBO-
oriented instance.
3
Formalized Ordered Completion
Ordered completion is commonly presented as a set of inference rules, parameter-
ized by a ﬁxed reduction order >. This way of presentation conveniently leaves a
lot of freedom to implementations. We use the following inference system, with
some diﬀerences to the original formulation [3] that we discuss below.
Deﬁnition 1 (Ordered Completion ✓).
The inference system oKB of
ordered completion operates on pairs (E, R) of equations E and rules R over
a common signature F. It consists of the following inference rules, where S
abbreviates R ∪E> and π is a renaming.
deduce
E, R
E ∪{sπ ≈tπ}, R
if s ←· →
R∪E↔t
compose
E, R ⊎{s →t}
E, R ∪{sπ →uπ}
if t −→S u
E ⊎{s ≈t}, R
E, R ∪{sπ →tπ}
if s > t
E ⊎{s ≈t}, R
E ∪{uπ ≈tπ}, R
if s →S u
orient
simplify
E ⊎{s ≈t}, R
E, R ∪{tπ →sπ}
if t > s
E ⊎{s ≈t}, R
E ∪{sπ ≈uπ}, R
if t →S u
delete
E ⊎{s ≈s}, R
E, R
collapse
E, R ⊎{t →s}
E ∪{uπ ≈sπ}, R
if t →S u
We write (E, R) ⊢(E′, R′) if (E′, R′) is obtained from (E, R) by employing one
of the above inference rules. A ﬁnite sequence of inference steps
(E0, ∅) ⊢(E1, R1) ⊢· · · ⊢(En, Rn)

512
C. Sternagel and S. Winkler
is called a run. Deﬁnition 1 diﬀers from the original formulation of ordered
completion [3] (as well as the formulation in our previous work together with
Hirokawa and Middeldorp [6]) in two ways. First, collapse and simplify do not
have an encompassment condition.2 This omission is possible since we only con-
sider ﬁnite runs. Second, we allow variants of rules and equations to be added.
This relaxation tremendously simpliﬁes certiﬁcate generation in tools, where
facts are renamed upon generation to avoid the maintenance and processing of
many renamed versions of the same equation or rule. Also note that the deduce
rule admits the addition of equations that originate from arbitrary peaks. In
practice, tools usually limit its application to extended critical pairs.
The following two results establish that the rules resulting from a ﬁnite oKB
run are oriented by the reduction order > and that the induced equational
theories before and after completion coincide.
Lemma 1 (✓).
If (E, R) ⊢∗(E′, R′) then R ⊆> implies R′ ⊆>.
⊓⊔
Lemma 2 (✓).
If (E, R) ⊢∗(E′, R′) then ↔∗
E∪R = ↔∗
E′∪R′.
⊓⊔
If the employed reduction order is F-ground total then the above two results
imply the following conversion equivalence involving ordered rewriting with
respect to the ﬁnal system.
Lemma 3 (✓).
Suppose > is F-ground total and R ⊆>. If (E, R) ⊢∗(E′, R′)
such that E′, R′, and > are over the signature F, then ↔∗
E∪R = ↔∗
E′
>∪R′ holds
for conversions between terms in T (F).
⊓⊔
This result is a key ingredient to our correctness results in Sect. 4. In order
to apply it, however, we need ground total reduction orders. To this end, we
formalized the following two results in IsaFoR.
Lemma 4 (✓).
If > is a total precedence on F then >kbo is F-ground total. ⊓⊔
Lemma 5 (✓).
If > is a total precedence on F then >lpo is F-ground total. ⊓⊔
In addition, we proved that for any given KBO >kbo (LPO >lpo) deﬁned over a
total precedence > there exists a minimal constant, that is, a constant c such
that t ⩾kbo c (t ⩾lpo c) holds for all t ∈T (F) (which will be needed in Sect. 4).
In earlier work by Becker et al. [4] ground totality of a lambda-free higher-order
variant of KBO is formalized in Isabelle/HOL. However, for our purposes it
makes sense to work with the deﬁnition of KBO that is already widely used in
IsaFoR.
By Lemma 3, any two ground terms convertible in the initial equational the-
ory are convertible with respect to ordered rewriting in the system obtained
from an oKB run. The remaining key issue is to decide when the current ordered
rewrite system is ground conﬂuent, such that a tool implementing oKB can stop.
Instead of deﬁning a fairness criterion as done by Bachmair et al. [3], we use the
following criterion for correctness involving ground joinability.
2 The encompassment condition demands that if a rule or equation ℓ≈r is used to
rewrite a term t = C[ℓσ] then C is non-empty or σ is not a renaming.

Certiﬁed Equational Reasoning via Ordered Completion
513
Lemma 6 (✓).
If for all equations s ≈t in E we have s > t or t ≈s in E
and CP>(E) ⊆↓g
E> then E is ground conﬂuent with respect to >.
⊓⊔
Note that the symmetry condition on E above is just a convenient way to express
the split of E into rewrite rules with ﬁxed orientation, and equations applicable
in both directions, which allows us to treat an ordered rewrite system as a single
set of equations. Lemmas 3 and 6 combine to the following correctness result.
Corollary 1 (✓). If > is F-ground total and (E0, ∅) ⊢∗(E, R) such that E′, R′,
and > are over the signature F and CP>(R∪E↔) ⊆↓g
R∪E↔
> , then S = R∪E↔
> is
ground complete and ↔∗
E0 = ↔∗
S holds for conversions between terms in T (F).
Before we can apply this result in order to obtain ground completeness we
need to be able to discharge its ground joinability assumption on extended crit-
ical pairs. This is the topic of the next section.
4
Formalized Ground Joinability Criteria
In general, ground joinability is undecidable even for terminating rewrite sys-
tems [7]. Below, we formalize two suﬃcient criteria.
4.1
A Simple Criterion
We start with the criterion that Bachmair et al. [3] proposed when they intro-
duced ordered completion.
Lemma 7 (✓).
Suppose > is a ground total reduction order over F with a
minimal constant. Then, E> is F-ground complete whenever for all s ≈t ∈
CP>(E↔) it holds that s ↓E> t, or s ≈t = (s′ ≈t′)σ for some s′ ≈t′ ∈E↔.
⊓⊔
A minimal constant c is needed to turn arbitrary ordered rewrite steps into
ordered rewrite steps over T (F): when performing an ordered rewrite step using
an equation u ≈v with V = Var(v) \ Var(u) ̸= ∅, a step over T (F) is obtained
by instantiating all variables in V to c. We illustrate the criterion on an example.
Example 1. The following equational system E0 is derived by ConCon while
checking infeasibility of a critical pair of the conditional rewrite system
Cops #361:
x ÷ y ≈⟨0, y⟩
x ÷ y ≈⟨s(q), r⟩
x −0 ≈x
0 −y ≈0
s(x) −s(y) ≈x −y
s(x) > s(y) ≈x > y
s(x) > 0 ≈true
s(x) ⩽s(y) ≈x ⩽y
0 ⩽x ≈true

514
C. Sternagel and S. Winkler
In an ordered completion run, MædMax transforms E0 into the following rules
R and equations E:
x −0 →x
0 −x →0
s(x) −s(y) →x −y
0 ⩽x →true
s(x) ⩽s(y) →x ⩽y
x ÷ y →⟨0, y⟩
s(x) > 0 →true
s(x) > s(y) →x > y
⟨s(x), y⟩≈⟨s(q), r⟩
⟨0, y⟩≈⟨s(q), r⟩
⟨0, x⟩≈⟨0, y⟩
Ground conﬂuence of this system can be established by means of Lemma 7. For
example, the extended overlap between the ﬁrst two equations gives rise to the
extended critical pair ⟨0, y⟩≈⟨s(x), y⟩, which is just an instance of the second
equation (and similarly for the other extended critical pairs).
4.2
Ground Joinability via Order Closures
The criterion discussed in Subsect. 4.1 is rather weak. For instance, it cannot
handle associativity and commutativity, as illustrated next [9, Example 1.1].
Example 2. Consider the system E consisting of the three equations
(1)
(x ∗y) ∗z ≈x ∗(y ∗z)
(2)
x ∗y ≈y ∗x
(3)
x ∗(y ∗z) ≈y ∗(x ∗z)
and the reduction order >kbo with w0 = 1 and w(∗) = 0. The ﬁrst equation can
be oriented from left to right, whereas the other ones are unorientable.
We obtain the following extended critical peak from equations (2) and (1):
z ∗(x ∗y) ←−(x ∗y) ∗z −→x ∗(y ∗z)
The resulting extended critical pair is neither an instance of an equation in E
nor joinable. Thus the criterion of Lemma 7 does not apply.
However, this extended critical pair is ground joinable, which we show in
the following. The reduction order >kbo is contained in an F′-ground total one
on any extension of the signature F′ ⊇F (using the well-order theorem and
incrementality of KBO). Thus, for any grounding substitution σ the terms xσ,
yσ, and zσ are totally ordered. Suppose for instance that xσ > zσ > yσ. Then
there is an ordered rewrite sequence witnessing joinability:
zσ ∗(xσ ∗yσ)
zσ ∗(yσ ∗xσ)
yσ ∗(zσ ∗xσ)
yσ ∗(xσ ∗zσ)
xσ ∗(yσ ∗zσ)
(2)
(3)
(2)
(3)
If, on the other hand, xσ = yσ > zσ holds, there is a joining sequence as well:
zσ ∗(xσ ∗yσ) = zσ ∗(xσ ∗xσ)
xσ ∗(zσ ∗xσ)
xσ ∗(xσ ∗zσ) = xσ ∗(yσ ∗zσ)
(2)
(3)

Certiﬁed Equational Reasoning via Ordered Completion
515
By ensuring the existence of a joining sequence for all possible relationships
between xσ, yσ, and zσ, ground joinability can be established. Using this app-
roach to show that all extended critical pairs are ground joinable, it can be
veriﬁed that E is in fact ground complete.
The ground joinability test by Martin and Nipkow [9] is based on the idea
illustrated in Example 2 above: perform a case analysis by considering ordered
rewriting using all extensions of > to instantiations of variables. Below, we give
the corresponding formal deﬁnitions used in IsaFoR. For any relation R on terms,
let σ(R) denote the relation such that sσ σ(R) tσ holds if and only if s R t.
Deﬁnition 2 (✓).
A closure C is a mapping between relations on terms that
satisﬁes the following properties:
(1) If s C(R) t then sσ C(σ(R)) tσ, for all relations R, substitutions σ, and
terms s and t.
(2) If R ⊆R′ then C(R) ⊆C(R′), for all relations on terms R and R′.
The closure C is compatible with a relation on terms R if C(R) ⊆R holds.
In the remainder of this section we assume F to be the signature of the
input problem, we consider an F-ground total reduction order > as well as a
closure C that is compatible with >. Furthermore, we assume for every ﬁnite
set of variables V ⊆V and every equivalence relation ≡on V a representation
function rep≡such that for any x ∈V we have x ≡rep≡(x), rep≡(x) ∈V and
x ≡y implies rep≡(x) = rep≡(y). Given an equivalence relation ≡on V , let ˆ≡
denote the substitution such that ˆ≡(x) = rep≡(x) for all x ∈V .
Deﬁnition 3 (✓).
Given an ES E and a reduction order >, terms s and t
are C-joinable, written s ↓C
E t, if for all equivalence relations ≡on Var(s, t) and
every order ≻on the equivalence classes of ≡it holds that
sˆ≡
∗
−−−→
EC(≻) ·
=
←→
E
·
∗
←−−−
EC(≻) tˆ≡
(1)
Example 3. For instance, consider the terms s = z ∗(x ∗y) and t = x ∗(y ∗z) from
Example 2. One possible equivalence relation ≡on Var(s, t) = {x, y, z} is given by
the equivalence classes {x, y} and {z}; one possible order on these is ˆ≡(x) ≻ˆ≡(z)
(corresponding to the second example for an order on the instantiations xσ and
zσ in Example 2). By taking C to be the KBO closure (see Deﬁnition 5 below), we
have x ∗z C(≻) z ∗x and x ∗(z ∗x) C(≻) z ∗(x ∗x). Using the ES E from Example 2
we thus obtain the ordered rewrite sequence
tˆ≡= x ∗(x ∗z) −−−→
EC(≻) x ∗(z ∗x) −−−→
EC(≻) z ∗(x ∗x) = sˆ≡
Ground joinability follows from C-joinability. Since this is the key result for
the ground joinability criterion of this subsection, we also sketch its proof.
Lemma 8 (✓).
If s ↓C
E t then s ↓g
E> t.

516
C. Sternagel and S. Winkler
Proof. We assume s ↓C
E t and consider a grounding substitution σ to show
sσ ↓E> tσ. There is some equivalence relation ≡on Var(s, t) such that x ≡y
holds if and only if σ(x) = σ(y) for all x, y ∈Var(s, t). Note that this implies
sσ = sˆ≡σ and tσ = tˆ≡σ.
We can deﬁne an order ≻on the equivalence classes of ≡such that [x]≡≻[y]≡
if and only if σ(x) > σ(y). Hence σ(≻) ⊆> holds, and by Deﬁnition 2(2) we
have C(σ(≻)) ⊆C(>). Compatibility implies C(>) ⊆>, and thus C(σ(≻)) ⊆>.
From Deﬁnition 2(1) we can show that u →EC(≻) v implies uσ →EC(σ(≻)) vσ for
all terms u and v. So using the assumption s ↓C
E t we can apply σ to a conversion
of the form (1) to obtain
sσ = sˆ≡σ
∗
−−−−−→
EC(σ(≻)) ·
=
←→
E
·
∗
←−−−−−
EC(σ(≻)) tˆ≡σ = tσ
(2)
Ordered rewriting is monotone with respect to the order, and hence C(σ(≻)) ⊆>
implies →EC(σ(≻)) ⊆→E>. Thus (2) implies the existence of a conversion
sσ
∗
−−→
E> ·
=
←−→
E> ·
∗
←−−
E> tσ
where the ↔E> step exists as any two F-ground terms are comparable in >. ⊓⊔
Note that the proof above uses the monotonicity assumption for closures (Deﬁ-
nition 2(2)), which is not present in [9]. The following counterexample illustrates
that monotonicity is indeed necessary.
Example 4. Consider the ES E = {f(x) ≈a} and suppose that > = C(>) is an
LPO with precedence a > b > c > f. Moreover, take s = f(b) and t = f(c). Any
order ≻as in Deﬁnition 3 is empty since Var(s, t) = ∅. As C is not required to be
monotone, the relation C(≻) may contain (f(b), a) and (f(c), a). Then s →EC(≻) a
and t →EC(≻) a imply s ↓C
E t even though s ↓g
E> t does not hold.
Below, we deﬁne an inductive predicate gj which is used to conclude ground
joinability of a given equation.
Deﬁnition 4 (✓).
Given an ES E and a reduction order >, gj is deﬁned
inductively by the following rules:
delete
gj(t, t)
closure
s ↓C
E t =⇒gj(s, t)
step
s ←→
E t =⇒gj(s, t)
rewrite left
s −−→
E> u and gj(u, t) =⇒gj(s, t)
rewrite right
t −−→
E> u and gj(s, u) =⇒gj(s, t)
congruence
gj(si, ti) for all 1 ⩽i ⩽n =⇒gj(f(s1, . . . , sn), f(t1, . . . , tn))
This test diﬀers from the one due to Martin and Nipkow [9] by the two rewrite
rules, which were added to allow for more eﬃcient checks, as illustrated next.

Certiﬁed Equational Reasoning via Ordered Completion
517
Example 5. Consider the ES E
f(x) ≈f(y)
g(x, y) ≈f(x)
together with a KBO that can orient the second equation (for instance, one can
take as precedence g > f > c and let all function symbol weights as well as w0
be 1). Then gj(f(x), f(z)) holds by the step rule, gj(g(x, y), f(z)) follows by an
application of rewrite left, and gj(g(x, y), g(z, w))) by rewrite right. By Lemma 9
below it thus follows that the equation g(x, y) ≈g(z, w) is ground joinable.
However, the criterion by Martin and Nipkow [9] lacks the rewrite steps. Hence
ground joinability of g(x, y) ≈g(z, w) can only be established by applying the
closure rule. This amounts to checking ground joinability with respect to 81
relations between the four variables. Since the number of variable relations is
in general exponential, the criterion stated in Deﬁnition 4 can in practice be
exponentially more eﬃcient than the test by Martin and Nipkow [9].
Using Lemma 8 it is not hard to show the following correctness results.
Lemma 9 (✓).
Suppose for all s ≈t in E we have s > t or t ≈s in E. Then
gj(s, t) implies s ↓g
E> t.
⊓⊔
Lemma 10 (✓).
If for all s ≈t in E we have s > t or t ≈s in E and
CP>(E) ⊆↓g
E> then E is ground conﬂuent with respect to >.
⊓⊔
This test can not only handle Example 2 but also the group theoretic problem
from the introduction. Moreover, it subsumes Lemma 7 since whenever for some
equation s ≈t we have s ↓g
E> t by Lemma 7 then gj(s, t) holds.
Closures for Knuth-Bendix Orders. Deﬁnition 2 requires abstract prop-
erties on closures. In the following we deﬁne closures for KBO as used in
IsaFoR/CeTA.
Similar to the already existing deﬁnition of KBO in IsaFoR [17] we deﬁne the
closure >R
kbo as follows.
Deﬁnition 5 (✓).
Let R be a relation on terms, > a precedence on F, and
(w, w0) a weight function. The KBO closure >R
kbo is a relation on terms induc-
tively deﬁned as follows: s >R
kbo t if s R t, or |s|x ⩾|t|x for all x ∈V and
either
(a) w(s) > w(t), or
(b) w(s) = w(t) and one of
(1) s ̸∈V and t ∈V, or
(2) s = f(s1, . . . , sn), t = g(t1, . . . , tm) and f > g, or
(3) s = f(s1, . . . , sn), t = f(t1, . . . , tn) and there is some i ⩽n such that
sj = tj for all 1 ⩽j < i and si >R
kbo ti
Note that even though Deﬁnition 5 resembles the usual deﬁnition of KBO, it
deﬁnes a closure of a relation R in a KBO-like way rather than a reduction
order. For instance, if x ≻z, as in Example 3, then x ∗z >≻
kbo z ∗x holds.
We prove that >R
kbo is indeed a closure that is compatible with >kbo based
on the same weight function and precedence.

518
C. Sternagel and S. Winkler
Lemma 11. Let R be a relation on terms, > a precedence on F, and (w, w0) a
weight function. Then all of the following hold:
(a) If s >kbo t then s >R
kbo t for all terms s and t.
✓
(b) If R ⊆R′ then >R
kbo ⊆>R′
kbo.
✓
(c) If s >R
kbo t then sσ >σ(R)
kbo
tσ, for all substitutions σ, and terms s and t.
✓
(d) The closure >R
kbo is compatible with >kbo.
✓
5
Applications
Ground complete rewrite systems can be used to decide equivalence of ground
terms with respect to their induced equational theory. Here we highlight appli-
cations of this decision problem.
Deciding Ground Equations. Suppose we obtain the ordered rewrite system
(E, R) and the reduction order > by applying ordered completion to an initial
set of equations E0. Then it is easy to decide whether two ground terms s and t
are equivalent with respect to E0 (that is, whether s ↔∗
E0 t): it suﬃces to check if
the (necessarily unique) normal forms of s and t with respect to R∪E> coincide.
Also if all variables of a non-ground goal equation are universally quantiﬁed, the
goal can be decided by substituting fresh constants for its variables.
Equations with Existential Variables. The following trick by Bachmair
et al. [3] allows us to reduce equations with existentially quantiﬁed variables to
the ground case: Let E be a set of equations and s ≈t a goal equation where all
variables are existentially quantiﬁed. This corresponds to the question whether
there is a substitution σ such that sσ ↔∗
E tσ holds. We employ three fresh
function symbols eq, true, and false, and deﬁne Eeq
s,t to denote E extended by the
two equations eq(x, x) ≈true and eq(s, t) ≈false.
If a ground complete system equivalent to Eeq
s,t is found—for instance discov-
ered by ordered completion—then it can be used to decide the goal, as stated
next.
Lemma 12 (✓).
Let s, t, and E all be over signature F and let S be a ground
complete TRS such that ↔∗
Eeq
s,t ⊆↔∗
S on T (F). If sσ ↔∗
E tσ then true↓S = false↓S.
Proof. Since sσ ↔∗
E tσ, there is a conversion sσ ↔∗
Eeq
s,t tσ by construction of Eeq
s,t.
Moreover, (appealing to an earlier formalization about signature extensions [16])
there exists an F-grounding substitution τ such that sτ ↔∗
Eeq
s,t tτ. So we have
true ←−−
Eeq
s,t
eq(sτ, sτ)
∗
←−→
Eeq
s,t
eq(sτ, tτ) −−→
Eeq
s,t
false
and by the assumed conversion inclusion an S-conversion between true and false.
Several applications of ground conﬂuence of S yield joinability of true↓S and
false↓S. Since both of these terms are normal forms they coincide.
⊓⊔

Certiﬁed Equational Reasoning via Ordered Completion
519
Infeasibility of Conditions. A decision procedure for ground equations can
also be harnessed to prove infeasibility of conditions in conditional term rewriting.
Here a condition c is a sequence of pairs of terms s1 ≈t1, . . . , sk ≈tk and we
say that c is infeasible whenever there is no substitution such that siσ →∗
R tiσ
holds for all 1 ⩽i ⩽k. Now, it is obviously a sound overapproximation to ensure
that there is no σ such that siσ ↔∗
R tiσ for all 1 ⩽i ⩽k. This suggests that
completion methods might be applicable.
But there are still two complications before we are able to achieve an infeasi-
bility check: (1) the rules of a conditional term rewrite system (CTRS for short)
R may be guarded by conditions, making R an unsuitable input for ordered
completion, and (2) the conditions c are most of the time not ground. As is
conventional when adopting TRS methods to conditional rewriting, we solve (1)
by dropping all conditions from the rules of R, resulting in the unconditional
TRS Ru whose rewrite relation overapproximates the one of R. Of course if we
can establish that there is no σ such that siσ →∗
Ru tiσ for all 1 ⩽i ⩽k, then we
also obtain infeasibility of c with respect to the CTRS R. In order to solve (2)
we use a fresh function symbol c and apply Lemma 12 to decide the equation
s = c(s1, . . . , sk) ≈c(t1, . . . , tk) = t by applying ordered completion to Ru
eq
s,t. If
s ̸↔∗
Ru
eq
s,t t we can conclude infeasibility of c.
Checking for infeasibility is for example useful when analyzing the conﬂuence
of a conditional rewrite system, since whenever we encounter a conditional criti-
cal pair whose conditions are infeasible, we can ignore it entirely. Since 2019 the
Conﬂuence Competition (CoCo)3 also features a dedicated infeasibility category.
6
Certiﬁcation
In this section we describe the proof certiﬁcates for the diﬀerent certiﬁable prop-
erties and summarize the corresponding Isabelle/HOL check functions.
Here, check functions are the formal connection between general, abstract
results and concrete certiﬁcates. For example, a check function for a KBO termi-
nation proof takes a certiﬁcate, containing a concrete TRS, a speciﬁc precedence,
and ﬁxed weight functions, as input. It checks that the KBO instance is admis-
sible and orients all rules of the TRS from left to right. By appealing to the
abstract result that compatibility of a TRS with an admissible KBO implies
termination, it then concludes termination of the concrete instance.
Only check functions that are both executable and proven sound are allowed
in the certiﬁer. The latter means that success of the check function implies a
concrete instance of the corresponding general result (in our example success
proves termination of the given TRS). In case of failure it is customary for
CeTA check functions to give a human readable reason for why a certiﬁcate is
rejected.
3 http://project-coco.uibk.ac.at/2019/

520
C. Sternagel and S. Winkler
Ordered Completion Certiﬁcates. Here, the certiﬁcate consists of
• a set of initial equations E0,
• an ordered completion result (E, R) together with a reduction order >, and
• a sequence of inference steps according to Deﬁnition 1.
The corresponding check function veriﬁes that (1) the inference steps form a
valid run (E0π, ∅) ⊢∗(E, R) for some renaming π, (2) all extended critical pairs
are joinable, by default according to Lemma 10, and (3) the reduction order is
admissible, in case of KBO.
Next, we illustrate such an ordered completion proof by an example.
Example 6. The certiﬁcate corresponding to Example 1 contains the equations
E0, the resulting system (E, R), and the reduction order >kbo with precedence
> > s > ⩽> true > −> ÷ > ⟨·, ·⟩> 0, w0 = 1, and w(0) = 2, w(÷) =
w(true) = w(s) = 1, and all other symbols having weight 0. In addition, a
sequence of inference steps explains how (E, R) is obtained from E0:
simplifyleft
x ÷ y ≈⟨s(q), r⟩to ⟨0, y⟩≈⟨s(q), r⟩
deduce
⟨0, x⟩←⟨s(u), v⟩→⟨0, y⟩
deduce
⟨s(x), y⟩←⟨0, u⟩→⟨s(q), r⟩
deduce
x > y ←s(x) > s(y) →s(s(x)) > s(s(y))
orientlr
0 ⩽x →true
orientrl
s(s(x)) > s(s(y)) →x > y
(⋆)
. . .
orientlr
s(x) −s(y) →x −y
orientlr
0 −x →0
orientlr
s(x) ⩽s(y) →x ⩽y
collapse
s(s(x)) > s(s(y)) →x > y to x > y ≈x > y
collapse
s(s(x)) > s(0) →true to s(x) > 0 ≈true
simplifyleft
s(x) > 0 ≈true to true ≈true
delete
x > y ≈x > y
delete
true ≈true
The ﬁrst collapse step using rule (⋆) above illustrates our relaxed inference
rule, it would not have been possible according to the original inference system [3]
due to the encompassment condition since s(s(x)) > s(s(y)) ̸▷· s(s(x)) > s(s(y)).
We brieﬂy comment on the diﬀerences to the certiﬁcation of standard Knuth-
Bendix completion as already present in CeTA [17]. For standard completion, the
certiﬁcate contains the initial set of equations E0, the resulting TRS R together
with a termination proof, and stepwise E0-conversions from ℓto r for each rule
ℓ→r ∈R. The certiﬁer ﬁrst checks the termination proof to guarantee termina-
tion of R. Then, conﬂuence of R can be guaranteed by ensuring that all critical
pairs are joinable. At this point it is easy to verify the inclusion ↔∗
E0 ⊆↔∗
R:
for each equation s ≈t ∈E0 the R-normal forms of s and t are computed
and checked for syntactic equality. The converse inclusion ↔∗
R ⊆↔∗
E0 is taken
care of by the provided E0-conversions. Overall, we obtain that R is a complete
presentation of E0 without mentioning a speciﬁc inference system.

Certiﬁed Equational Reasoning via Ordered Completion
521
Unfortunately, the same approach does not work for ordered completion:
The inclusion ↔∗
E0 ⊆↔∗
R∪E> cannot be established by rewriting equations in
E0 to normal form, since they may contain variables but R ∪E> is only ground
conﬂuent. Moreover, since ground joinability is undecidable no complete check
can be performed. Therefore, we instead ask for certiﬁcates that contain explicit
inference steps, as described above.
Equational Satisﬁability Certiﬁcates. We use the term “satisﬁability” of
unit equality problems in line with the terminology of TPTP [22]: given a set of
equations E0 and a ground goal inequality s ̸≈t, show that this axiomatization
is satisﬁable. To this end, completion-based tools try to ﬁnd a ground complete
presentation S of E0 and verify that s↓S ̸= t↓S.
A certiﬁcate for this application extends an ordered completion certiﬁcate by
the goal terms. The corresponding check function veriﬁes that
• the presented ordered completion proof is valid as described above,
• the goal inequality is ground,
• the signature of E0, E, and R is included in the signature of >, and
• the terms in the goal have diﬀerent normal forms.
We chose the symbols mentioned by the reduction order to be the considered
signature F. In comparison to picking the signature of E0, this has the advan-
tage that it is easy to add additional function symbols. Moreover, since KBO
precedences in the CPF input are lists of function symbols, no additional checks
are required to ensure F-ground totality of the constructed reduction order.
As a side note, unsatisﬁability proofs are much easier to certify: a tool only
needs to output a conversion between the two goal terms. Support for the corre-
sponding certiﬁcates has already been added to CeTA earlier [21].
Infeasibility Certiﬁcates.
Actually we check (generalized) nonreachabil-
ity [15] of a target t from a source s with respect to a TRS R, that is, the
property that, given a TRS R and two terms s and t, there is no substitution σ
with sσ →∗
R tσ.
The corresponding certiﬁcates list function symbols eq, true, and false,
together with an equational satisﬁability certiﬁcate. The check function ﬁrst
constructs, using eq, true, and false from the certiﬁcate the TRS Req
s,t and then
veriﬁes that the equation true ≈false is not satisﬁable according to the supplied
equational satisﬁability certiﬁcate with Req
s,t as initial set of equations.
7
Experiments
Below we summarize experiments with our certiﬁer on diﬀerent problem sets.
More details are available from the accompanying website.4
4 http://cl-informatik.uibk.ac.at/experiments/okb/

522
C. Sternagel and S. Winkler
Ordered Completion. Martin and Nipkow [9] give 10 examples. The criterion
of Lemma 10 with KBO applies to 7 of those and MædMax produces correspond-
ing proofs. Six of these proofs are certiﬁed by CeTA. The missing example uses
a trick also used by Waldmeister [1]: certain redundant equations need not be
considered for critical pair computation. This simpliﬁcation is not yet supported
by CeTA.
We also ran MædMax on the 138 problems [13] for standard completion
collected from the literature. Using KBO, MædMax can complete 55 of them, and
52 of those are certiﬁed. (Using LPO and KBO, 91 are completed.) For the three
remaining (AC) group examples, MædMax uses a stronger criterion [23] which
is currently not supported by CeTA. Overall, this amounts to 58% certiﬁcation
coverage of all ordered completion proofs by MædMax.
Satisﬁable Unit Equality Problems. There are 144 unit equality problems
(UEQ) in the TPTP 7.2.0 [22] benchmark that are classiﬁed as satisﬁable, of
which MædMax using KBO only can prove 11. All these proofs are certiﬁed by
CeTA. With its general strategy MædMax can handle 14 problems, but two of
those require duplicating rules, such that KBO is not applicable, and one has
multiple goals, which is currently not supported by CeTA.
Infeasibility Problems. There are 148 oriented CTRSs in version 807 of the
Cops5 benchmark (that is, the version of Cops where the highest problem number
is 807) of CoCo. Here oriented means that a condition s ≈t is satisﬁed by a
substitution σ, whenever sσ →∗
R tσ. (This is the class of systems ConCon is
specialized to, hence we restrict our experiments to the above 148 systems.)
Out of those 148 CTRSs, the previous version of ConCon (1.7) can prove
(non)conﬂuence of 109 with and of 112 without certiﬁcation. The new version
of ConCon (1.8), extended by infeasibility checks via ordered completion with
MædMax, can handle 111 CTRSs with and 114 without certiﬁcation. We thus
obtain two new certiﬁed proofs, namely for Cops #340 and #361.
8
Conclusion
We presented our Isabelle/HOL formalization of ordered completion and two
accompanying ground joinability criteria—now part of IsaFoR 2.37. It comes with
check functions for ordered completion proofs, equational satisﬁability proofs,
and infeasibility proofs for conditional term rewriting. Formalizing soundness of
these check functions allowed us to add support for corresponding certiﬁcates to
the certiﬁer CeTA that is code generated from IsaFoR. To the best of our knowledge,
CeTA constitutes the ﬁrst proof checker for ordered completion proofs. Indeed, it
already helped us to detect a soundness error in MædMax, where in certain
corner cases some extended critical pairs were ignored. Our experiments show
that we can certify 58% of ordered completion proofs (corresponding to 94%
5 http://cops.uibk.ac.at?q=1..807

Certiﬁed Equational Reasoning via Ordered Completion
523
of the KBO proofs) and 85% of the satisﬁability proofs produced by MædMax
(100% for KBO). The number of certiﬁed proofs of ConCon increased by two.
Moreover, CeTA is the only certiﬁer used in the Conﬂuence Competition; by
certifying infeasibility proofs our work thus helps to validate more tool output.
Regarding the recent CoCo 2019, certiﬁcation currently covers roughly 83% of
the benchmarks in the two categories (CTRS and TRS) that have certiﬁed coun-
terparts (CPF-CTRS and CPF-TRS).
In the future, we plan to add support for closures of LPO and extend our
certiﬁer to verify proofs of pure, not necessarily unit, equality formulas, as well
as ground conﬂuence proofs by tools participating in the conﬂuence competition.
Acknowledgments. We thank the anonymous referees for their constructive com-
ments and various suggestions for improvements.
References
1. Avenhaus, J., Hillenbrand, T., L¨ochner, B.: On using ground joinable equations in
equational theorem proving. J. Symb. Comput. 36(1–2), 217–233 (2003). https://
doi.org/10.1016/S0747-7171(03)00024-5
2. Baader, F., Nipkow, T.: Term Rewriting and All That. Cambridge University Press
(1998). https://doi.org/10.1017/CBO9781139172752
3. Bachmair, L., Dershowitz, N., Plaisted, D.A.: Completion without failure. In: A¨ıt-
Kaci, H., Nivat, M. (eds.) Resolution of Equations in Algebraic Structures, Rewrit-
ing Techniques, vol. 2, pp. 1–30. Academic Press (1989). https://doi.org/10.1016/
B978-0-12-046371-8.50007-9
4. Becker, H., Blanchette, J.C., Waldmann, U., Wand, D.: A transﬁnite Knuth–
Bendix order for lambda-free higher-order terms. In: de Moura, L. (ed.) CADE
2017. LNCS (LNAI), vol. 10395, pp. 432–453. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-63046-5 27
5. Haftmann, F., Nipkow, T.: Code generation via higher-order rewrite systems. In:
Blume, M., Kobayashi, N., Vidal, G. (eds.) FLOPS 2010. LNCS, vol. 6009, pp. 103–
117. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-12251-4 9
6. Hirokawa, N., Middeldorp, A., Sternagel, C., Winkler, S.: Inﬁnite runs in abstract
completion. In: Proceedings of the 2nd FSCD. LIPIcs, vol. 84, pp. 19:1–19:16 (2017).
https://doi.org/10.4230/LIPIcs.FSCD.2017.19
7. Kapur, D., Narendran, P., Otto, F.: On ground-conﬂuence of term rewriting
systems. Inform. Comput. 86(1), 14–31 (1990). https://doi.org/10.1016/0890-
5401(90)90023-B
8. Knuth, D.E., Bendix, P.: Simple word problems in universal algebras. In: Leech, J.
(ed.) Computational Problems in Abstract Algebra, pp. 263–297. Pergamon Press
(1970). https://doi.org/10.1016/B978-0-08-012975-4
9. Martin, U., Nipkow, T.: Ordered rewriting and conﬂuence. In: Stickel, M.E. (ed.)
CADE 1990. LNCS, vol. 449, pp. 366–380. Springer, Heidelberg (1990). https://
doi.org/10.1007/3-540-52885-7 100
10. McCune, W.: Solution of the Robbins problem. J. Autom. Reasoning 19(3), 263–
276 (1997). https://doi.org/10.1023/A:1005843212881
11. McCune, W., Veroﬀ, R., Fitelson, B., Harris, K., Feist, A., Wos, L.: Short single
axioms for Boolean algebra. J. Autom. Reasoning 29(1), 1–16 (2002). https://doi.
org/10.1023/A:1020542009983

524
C. Sternagel and S. Winkler
12. Nipkow, T., Paulson, L.C., Wenzel, M.: Isabelle/HOL – A Proof Assistant for
Higher-Order Logic, LNCS, vol. 2283. Springer (2002). https://doi.org/10.1007/3-
540-45949-9
13. Sato, H., Winkler, S.: Encoding dependency pair techniques and control strategies
for maximal completion. In: Felty, A.P., Middeldorp, A. (eds.) CADE 2015. LNCS
(LNAI), vol. 9195, pp. 152–162. Springer, Cham (2015). https://doi.org/10.1007/
978-3-319-21401-6 10
14. Schultz, P., Wisnesky, R.: Algebraic data integration. J. Funct. Program. 27(e24),
51 (2017). https://doi.org/10.1017/S0956796817000168
15. Sternagel, C., Sternagel, T.: Certifying conﬂuence of almost orthogonal CTRSs
via exact tree automata completion. In: Proceedings of the 1st FSCD. LIPIcs, vol.
52, pp. 29:1–29:16. Schloss Dagstuhl (2016). https://doi.org/10.4230/LIPIcs.FSCD.
2016.29
16. Sternagel, C., Thiemann, R.: Signature extensions preserve termination. In: Dawar,
A., Veith, H. (eds.) CSL 2010. LNCS, vol. 6247, pp. 514–528. Springer, Heidelberg
(2010). https://doi.org/10.1007/978-3-642-15205-4 39
17. Sternagel, C., Thiemann, R.: Formalizing Knuth-Bendix orders and Knuth-Bendix
completion. In: Proceedings of the 24th RTA. LIPIcs, vol. 21, pp. 287–302. Schloss
Dagstuhl (2013). https://doi.org/10.4230/LIPIcs.RTA.2013.287
18. Sternagel, C., Thiemann, R.: The certiﬁcation problem format. In: Proceedings
of the 11th UITP. EPTCS, vol. 167, pp. 61–72 (2014). https://doi.org/10.4204/
EPTCS.167.8
19. Sternagel, C., Winkler, S.: Certiﬁed ordered completion. In: Proceedings of the 7th
IWC (2018), arXiv:1805.10090
20. Sternagel, T., Middeldorp, A.: Conditional conﬂuence (system description). In:
Dowek, G. (ed.) RTA 2014. LNCS, vol. 8560, pp. 456–465. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-08918-8 31
21. Sternagel, T., Winkler, S., Zankl, H.: Recording completion for certiﬁcates in equa-
tional reasoning. In: Proceedings of the 4th CPP, pp. 41–47 (2015). https://doi.
org/10.1145/2676724.2693171
22. Sutcliﬀe, G.: The TPTP problem library and associated infrastructure: the FOF
and CNF Parts. J. Autom. Reasoning 43(4), 337–362 (2009). https://doi.org/10.
1007/s10817-009-9143-8
23. Winkler, S.: A ground joinability criterion for ordered completion. In: Proceedings
of the 6th IWC, pp. 45–49 (2017)
24. Winkler, S., Moser, G.: MædMax: a maximal ordered completion tool. In: Galmiche,
D., Schulz, S., Sebastiani, R. (eds.) IJCAR 2018. LNCS (LNAI), vol. 10900, pp.
472–480. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-94205-6 31

Certiﬁed Equational Reasoning via Ordered Completion
525
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

JGXYZ:
An ATP System for Gap and Glut Logics
GeoﬀSutcliﬀe1(B)
and Francis Jeﬀry Pelletier2
1 University of Miami, Miami, USA
geoff@cs.miami.edu
2 University of Alberta, Edmonton, Canada
http://www.cs.miami.edu/˜geoff
https://sites.ualberta.ca/˜francisp
Abstract. This paper describes an ATP system, named JGXYZ, for some gap
and glut logics. JGXYZ is based on an equi-provable translation to FOL, followed
by use of an existing ATP system for FOL. A key feature of JGXYZ is that the
translation to FOL is data-driven, in the sense that it requires only the addition of a
new logic’s truth tables for the unary and binary connectives in order to produce
an ATP system for the logic. Experimental results from JGXYZ illustrate the
diﬀerences between the logics and translated problems, both technically and in
terms of a quasi-real-world use case.
Keywords: Multi-valued logic · Gap logic · Glut logic · ATP system
1
Gap and Glut Logics
Logic “is a subject concerned with the most general laws of truth, and is now gener-
ally held to consist of the systematic study of the form of valid inference”, and “A valid
inference is one where there is a speciﬁc relation of logical support between the assump-
tions of the inference and its conclusion.” [21]. Classical ﬁrst-order logic (FOL), with its
truth values True and False has “been the logic suggested as the ideal for guiding rea-
soning” and “For this reason, classical logic has often been called the one right logic.”
[18]. Despite this view, in 1920 Łukasiewicz noted that future contingent statements
like “There will be a sea-battle tomorrow” are not true (now), nor are they false (now).
To reason about such statements Łukasiewicz invented a new truth-value, Neither1, to
form the logic Ł3 [12]. Łukasiewicz basically wanted to use classical logic, except to
allow N to be “in the gap between” T and F. He kept the usual connectives of ¬, ∨, ∧,
and →, but found it necessary to change the deﬁnition of the conditional connective.
In contrast to statements that appear to have no truth value, paradoxical statements
such as the Liar Paradox “This sentence is false.” provide motivation for dialetheic
logics [17] that allow statements to be one of True, False, or “have the glut of” Both
true and false. Dialetheic logics are paraconsistent, so that a contradiction in the input
1 Actually, Łukasiewicz called it (the Polish equivalent for) Indeterminate, but to keep things
consistent with other works, we use Neither.
c⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 526–537, 2019.
https://doi.org/10.1007/978-3-030-29436-6_31

JGXYZ: An ATP System for Gap and Glut Logics
527
does not lead to logical omniscience. The most famous – and persistent – advocate of
dialetheism is Graham Priest, who developed the Logic of Paradox [16], which provides
a foundation for the dialetheic logics RM3 [19] and a logic that we call A3, after [1].
As with the gap logic Ł3, these glut logics require particular conditional connectives in
order to retain useful reasoning properties.
Fig. 1. The truth diamond
In 1977 Nuel Belnap published two articles, “How
a Computer Should Think” and “A Useful Four-Valued
Logic” [5]. One of the leading ideas was of a then-
futuristic knowledge based system that would not only
retrieve explicitly stored data, but would also reason and
deduce consequences of the stored data. A further idea was
that such a knowledge base might be given contradictory
data to store, and that there might be topics for which no
data is stored. This led to the development of the FDE logic
[4,6], which merges the ideas of gap and glut logics by including all four truth values:
T, B, N, and F. Belnap envisaged the four truth values of FDE in a lattice, the “Truth
Diamond” shown in Fig. 1. The Truth Diamond represents the amount of truth in the
four truth values, with T having the most (only truth) and F the least (no truth). B and
N are between the two extremes of T and F, with diﬀerent ways of balancing their true
and false parts, and therefore have incomparable amounts of truth. Again, the choice of
conditional connective for FDE is important, with diﬀerent choices leading to diﬀerent
theories [9,20].
This work deals with the development of an ATP system called JGXYZ2 for these
and other ﬁrst-order logics. The system is “data-driven”, in the sense that it requires
only the addition of a new logic’s truth tables for the unary and binary connectives in
order to produce an ATP system for the logic. The data-driven approach is also taken in
MUltlog [2], leading to the speciﬁcation of a logic and deduction systems, but no actual
running ATP system like JGXYZ. An implemented ATP system for multi-valued logics
was 3TAP [3], but it is no longer supported. A survey of work done around the end of
the last century is provided by [8]. Note that the input language for gap/glut logics is
the same as for FOL – it is the semantics and reasoning that changes when a gap/glut
logic is adopted (and consequently it does not make sense to compare an ATP system’s
reasoning in gap/glut logics with the reasoning of a FOL ATP system).
2
A Motivating Example
As a quasi-real-world use case, consider the situation faced by script writers for a TV
series that features “undead” characters [11]. In such shows there are characters who
are alive, characters who are not alive, and undead characters who are both alive and not
alive. Additionally, there will be (in future episodes) new characters whose liveliness
is yet unknown. All characters that have ever appeared in an episode are either alive
or have been buried. Each week the script writer must provide the necessary dialogue
2 Named after the authors Jeﬀand Geoﬀ, for any logic XYZ.

528
G. Sutcliﬀe and F. J. Pelletier
and placement of the characters who appear in the episode.3 Characters who are alive
need words and placement. Characters who are not alive need no words but still need
placement. For now, let there be four characters: Alan, who is alive; D´esir´ee, who is not
alive and has been buried; Umberto, who is undead (i.e., both alive and not alive); and
Nigel, who has not yet appeared in the script. The kinds of questions the script write
might ask include:
– Does D´esir´ee need words?
– Does Nigel need placement?
– Is Umberto both alive and not alive?
– Is Nigel alive or (inclusively) not alive?
– Has Umberto been buried?
– Was D´esir´ee buried because she is not alive?
If such a scenario is to be formalized so that the questions can be correctly (logi-
cally!) answered, the possibility of characters being both alive and not alive requires a
glut logic that supports the truth value Both, and the possibility of new characters whose
liveliness is unknown requires a gap logic that supports the truth value Neither. The gap
and glut logics discussed in Sect. 1 are appropriate, and the JGXYZ ATP system can
provide the necessary reasoning.
The formalization in TPTP syntax is as follows:
%----Axioms of the undead
fof(alive_or_buried,axiom,! [X] : ( alive(X) | buried(X) )).
fof(alive_scripting,axiom,
! [X] : ( alive(X) => ( script(X,words) & script(X,placement) ))).
fof(not_alive_scripting,axiom,
! [X] : ( ˜alive(X) => ( ˜script(X,words) & script(X,placement) ))).
%----Current characters
fof(alan_alive,axiom,
alive(alan) ).
fof(desiree_dead,axiom,
˜alive(desiree) ).
fof(desiree_buried,axiom, buried(desiree)).
fof(umberto_alive,axiom,
alive(umberto) ).
fof(umberto_dead,axiom,
˜alive(umberto) ).
%----Queries
fof(desiree_needs_words,conjecture,
script(desiree,words) ).
fof(nigel_needs_placement,conjecture, script(nigel,placement)).
fof(umberto_alive_and_not,conjecture, alive(umberto) & ˜alive(umberto)).
fof(nigel_alive_or_not,conjecture,
alive(nigel) | ˜alive(nigel)).
fof(umberto_buried,conjecture,
buried(umberto)).
fof(not_alive_buried,conjecture,
˜alive(desiree) => buried(desiree)).
The answers to these queries, for each of the logics that are presented in Sect. 3, are
presented in Sect. 5.
3 Computer geeks ... think of the characters as UNIX processes, which can be alive, not alive,
or zombies. Burial corresponds to reaping the process from the process table. FDE can thus be
used to reason about UNIX processes. (Thanks to Josef Urban for this interpretation.)

JGXYZ: An ATP System for Gap and Glut Logics
529
3
Truth Values and Conditional Connectives
Section 1 brieﬂy introduced four gap/glut logics: Ł3, RM3, A3, and FDE. These diﬀer
in terms of the truth values they support, and the conditional connective that they use.
This section provides further details of these logics, and examines their conditional
connectives.
Section 1 provided motivation for having the four truth values used by gap and glut
logics: T, B, N, and F. As usual, the truth values are divided into those that are desig-
nated – the values that “true” statements should have (like being T in classical logic),
and those that are undesignated. Logical truths are formulae that are always designated
regardless of the truth values of their atomic components, and are the formulae that a
reasoning tool should be able to prove. The truth tables for negation, disjunction, and
conjunction over the four truth values are given in Table 1. The truth value of a conjunc-
tion (disjunction) is the meet (join) of its conjuncts (disjuncts) in the truth diamond, and
negation inverts the order in the diamond.
Table 1. Truth tables for negation, disjunction, and conjunction
T
F
B
B
N
N
F
T
T
B
N
F
T
T
T
T
T
B
T
B
T
B
N
T
T
N
N
F
T
B
N
F
T
B
N
F
T
T
B
N
F
B
B
B
F
F
N
N
F
N
F
F
F
F
F
F
In this work, two conditional connectives are used:
– Classical Material Implication →cmi [1,9,20]. This conditional was proposed in
response to the observation that modus ponens (MP) fails in FDE if the classical
FOL conditional →cls deﬁned in terms of ∨and ¬, (ϕ →cls ψ) =d f (¬ϕ ∨ψ), is
used [20]. →cmi does however emphasize the classical aspects of a conditional. In
the cases when the antecedent is designated, the value of the consequent is assigned
to the conditional. In the cases when the antecedent is undesignated, T is assigned
to the conditional.
– The “Łukasiewicz” conditional →Łuk [13]. One of the features missing from →cmi
is contraposition with respect to negation, i.e., (ϕ →cmi ψ)  (¬ψ →cmi ¬ϕ). Con-
traposition can be added by taking a conjunction of →cmi and its contraposed form,
(ϕ →Łuk ψ) =d f ((ϕ →cmi ψ) ∧(¬ψ →cmi ¬ϕ)). This can be seen as a generalization
of Łukasiewicz’ implication from Ł3, hence the name “Łukasiewicz”.
Table 2 shows the deﬁnitions of →Łuk and →cmi. It is clear that they are very similar,
diﬀering only in the values of T →B, N →B, and N →F. The biconditional connec-
tive is understood to be the conjunction of the conditional and its converse, hence the
diﬀerences between the two conditionals are propagated to the bi-conditionals. These
diﬀerences are enough to produce some quite diﬀerent theorems between the logics that
use them, as can be seen in the experimental results presented in Sect. 5.

530
G. Sutcliﬀe and F. J. Pelletier
Table 2. Truth tables for →Łuk and →cmi
Łuk
T
B
N
F
T
T
F
N
F
B
T
B
N
F
N
T
N
T
N
F
T
T
T
T
cmi
T
B
N
F
T
T
B
N
F
B
T
B
N
F
N
T
T
T
T
F
T
T
T
T
Given the choices of truth values and conditional connectives, ﬁve logics are con-
sidered:
– Ł3: The truth values are T, N, and F, with T designated. The conditional is →Łuk,
restricted to the three truth values.
– RM3: The truth values are T, B, and F, with T and B designated. The conditional is
→Łuk, restricted to the three truth values.
– A3: The truth values are T, B, and F, with T and B designated. The conditional is
→cmi, restricted to the three truth values.
– FDE→Łuk: The truth values are T, B, N, and F, with T and B designated. The condi-
tional is →Łuk.
– FDE→cmi: The truth values are T, B, N, and F, with T and B designated. The condi-
tional is →cmi.
The relationship between FOL and these logics is shown in Fig. 2.
Fig. 2. The relationships between the logics
4
System Architecture and Implementation
JGXYZ proves theorems in the gap/glut (and other) logics by translating the problem
to an equi-provable FOL problem, then using a FOL ATP system to ﬁnd a proof (or
countermodel) for the FOL problem. In [15] two translations from RM3 to FOL were
presented, and in [20] the “truth evaluation” translation was extended to FDE→cmi. The

JGXYZ: An ATP System for Gap and Glut Logics
531
truth evaluation translation function trs takes a target formula (e.g., in FDE→cmi) and
a target truth value (e.g., one of T, B, N, or F) as arguments, and translates the target
formula, either directly for atoms, or recursively on the subformulae for non-atoms, to
produce a FOL formula. Intuitively, trs captures the necessary and suﬃcient conditions
for the target formula to have the target truth value. Prior implementations of JGXYZ
(called JGRM3 in [15], and later JGXYZ 0.1 in [20]) encoded trs directly. This meant
that extending the translation to a new logic required signiﬁcant eﬀort. The new imple-
mentation of JGXYZ (version 0.2) is the same as for version 0.1 for quantiﬁed and
atomic formulae, but makes the translation data-driven for formulae under a unary or
binary connective. For each logic, its truth values and the designated subset of them
are speciﬁed, and the truth tables for the logic’s negation, disjunction, conjunction, and
conditional connectives are provided.
Universally qualiﬁed formulae are treated as a conjunction of their ground instances,
requiring that there exists an instance that has the target truth value, and that there do not
exist any instances that have a truth value lower in the truth diamond. For example, for
a universally quantiﬁed formula in FDE→cmi and the target truth value B, the translation
requires that there exists an instance of the formula whose translation is B, and that there
do not exist any instances whose translation is F, i.e., trs(∀x ϕ, B) ⇒∃x trs(ϕ, B) ∧
¬∃x trs(ϕ, F). Existentially quantiﬁed formulae are treated as a disjunction of their
ground instances, requiring that there exists an instance that has the target truth value,
and that there do not exist any instances that have a truth value higher in the truth
diamond. For example, for an existentially quantiﬁed formula in FDE→cmi and the target
truth value B, the translation requires that there exists an instance of the formula whose
translation is B, and that there do not exist any instances whose translation is T, i.e.,
trs(∃x ϕ, B) ⇒∃x trs(ϕ, B) ∧¬∃x trs(ϕ, T).
For formulae under a unary or binary connective, the appropriate truth table is con-
sulted to ﬁnd tuples of truth values such that the value of the connective for those inputs
is the target truth value. The tuple elements are then the target truth values for the argu-
ments of the connective in the formula. The translation is the disjunction (one disjunct
for each tuple) of conjunctions (one conjunct for element of the tuple), applied to the
translations of the arguments of the connective. For the n-ary connective ⊕and the
target truth value TTV:
trs(⊕ϕ, TTV) ⇒
k
i=1
n

j=1
trs(ϕj, inputsi, j(⊕, TTV))
where inputsi, j(⊕, TTV) is the jth element of the ith tuple of the k tuples from the truth
table for ⊕such that the value of ⊕for those inputs is TTV, and ϕj is the jth argument
of ϕ (n is 1 for a unary connective and 2 for a binary connective, etc.). For example, for
the FDE→cmi conditional formula ϕ →cmi ψ and the target truth value B, k is 2 and the
input tuples are [T, B] and [B, B]. Then:
trs(ϕ →cmi ψ, B) ⇒((trs(ϕ, T) ∧trs(ψ, B)) ∨(trs(ϕ, B) ∧trs(ψ, B))).

532
G. Sutcliﬀe and F. J. Pelletier
Atoms are translated to FOL atoms that capture what it means for the atom to have
the target truth value. Equality atoms are treated classically4, so that for a target truth
value of T an equality atom is unchanged, for a target truth value of F an equality atom
is negated, and for a target truth value of B or N an equality atom is translated to the
FOL truth value F. A non-equality atom Φ that has predicate symbol P and arity n is
translated to a FOL atom with predicate symbol PTTV and arity n, where TTV is the
target truth value. The FOL atom has the same term arguments as P in Φ. Deﬁnition
axioms are added to relate each predicate symbol PLTV to atoms that correspond to the
two FOL truth values T and F, where LTV is each of the truth values used by the logic.
The axioms introduce two new predicate symbols, PcT and PcF (for classical True and
False) for each predicate symbol P in the input problem. The axioms are:
∀x (PT(x) ↔(PcT(x) ∧¬PcF(x)))
∀x (PB(x) ↔(PcT(x) ∧PcF(x)))
∀x (PN(x) ↔(¬PcT(x) ∧¬PcF(x)))
∀x (PF(x) ↔(¬PcT(x) ∧PcF(x)))
Finally, exhaustion axioms are added to enforce that each of the FOL atoms takes on
exactly one of the truth values of the logic. By example, the axioms for FDE→cmi are:
∀x (PT(x) ∨PB(x) ∨PN(x) ∨PF(x))
(The exclusive disjunction of the disjuncts, so that each of the FOL atoms takes on
only one of the truth values, is a logical consequence of the exhaustion and deﬁnition
axioms.)
For a set of formulae φ, let def(φ) be the set of deﬁnition axioms and exh(φ) the set
of exhaustion axioms, for the predicate symbols that occur in φ. Deﬁne
des(φ) =d f
n

i=1
trs(φ, DTVi)
where DTV is the set of designated truth values of the logic. For a problem φ ⊨ψ deﬁne
trans(φ) = des(φ) ∪exh(φ ∪{ψ}) ∪def(φ ∪{ψ})
Then φ ⊨logic ψ iﬀtrans(φ) ⊨FOL des(ψ). A theorem prover for the logic is simply
implemented by submitting trans(φ) ⊨FOL des(ψ) to a FOL ATP system.
The JGXYZ translation is implemented in Prolog, and the full ATP system uses
some scriptin’ magic to connect the translation to a FOL ATP system. By default, 80%
of the CPU time is allocated to searching for a proof, and if no proof is found the
remaining 20% is used to search for a countermodel. Currently Vampire 4.2.2 [10]
is used for the FOL reasoning, in CASC mode for proving, and in ﬁnite model ﬁnd-
ing mode for ﬁnding countermodels. JGXYZ for FOL,5 Ł3, A3, RM3, FDE→Łuk, and
4 The classical interpretation of equality is due to the classical interpretation of terms. Since
a term is interpreted as an element of the domain, if two terms are interpreted as the same
element then their equality is True, and if they are interpreted as diﬀerent elements then their
equality is False. There is no middle ground (Both or Neither). [7,14].
5 This can be used to empirically check that the translation does produce equi-provable prob-
lems. For more fun, it is possible to repeatedly apply the translation to a FOL problem to
produce a new FOL problem, to produce a sequence of ever more diﬃcult FOL problems.

JGXYZ: An ATP System for Gap and Glut Logics
533
FDE→cmi are available through the SystemOnTPTP interface at http://www.tptp.org/cgi-
bin/SystemOnTPTP.
5
Experimental Results
The implementation has been tested for all the logics encoded, on a set of problems,
taken from [20]. All the problems are valid in FOL. Testing was done on an Intel(R)
Xeon(R) CPU E5-2609 v2 @ 2.50 GHz, with a CPU time limit of 600 s per problem.
Note that the time taken to translate a problem is negligible, so that almost all of the time
is available to the FOL ATP system. The test problems in TPTP format are available at
http://www.tptp.org/JGXYZ, and they can be run through SystemOnTPTP.
Table 3 gives the results of the testing, using the default JGXYZ settings described in
Sect. 4. The results with a CPU time were proved, countermodels were found for those
marked CSA, and no result was obtained within the CPU time limit for those marked
GUP. The results marked GUP+ are known (from previous experiments [15,20]) to be
theorems for that logic, and the results marked GUP−are known to have countermodels
for that logic.
As is expected, there are diﬀerences in the results between the various logics
because of their diﬀerent truth values and also their diﬀerent conditional connectives.
Problem 1 shows how purely glut logics such as RM3 and A3 can prove FOL tautolo-
gies, while logics that include the gap truth value N cannot. In contrast, Problems 8
and 14 show that there are some theorems of purely gap logics that are not theorems
of glut logics. Problem 8 in particular shows that Ł3 is not paraconsistent. Problems 2
and 12 illustrate a diﬀerence between →Łuk and →cmi: e.g., for Problem 2, in RM3 and
FDE→Łuk with q set to B and p set to T, the conjecture is F and hence not a theorem,
while in A3 and FDE→cmi the conjecture is B. In contrast, Problem 11 shows how this
diﬀerence can work the other way. Problem 7 shows the diﬀerence between FOL and
the gap/glut logics with their extra truth values and extended conditional connectives.
In Ł3 and FDE→Łuk with p to N and q to F the conjecture is N; in A3 and FDE→cmi with
p to B and q to F the conjecture is F; and in RM3 with p to B and q to F the conjecture
is N. In contrast, Problem 6 is a theorem for all the logics, despite the extra truth values
and extended conditional connectives.
Problems 16–20 are interesting both from a historical and also a contemporary point
of view of the foundations of mathematics. They represent some of the motivating
claims that drove the modern development of axiomatic set theory and mathematics.
Read the relation E(x, y) as saying that x is an element of the set y. Then each formula
represents a crucial part of the various paradoxes of set theory. For example, Russell’s
paradox is in part captured by Problem 16, which says that there cannot be a set (y)
all of whose members (x) are not members of themselves. See [20] for a more detailed
discussion of these problems.
Problem 20, which is a theorem for Ł3, RM3, A3, and FDE→Łuk, is quite hard for
JGXYZ. An examination of syntactic characteristics of the translated problems illus-
trates how the translation blows up the problem. Table 4 provides some measures of the

534
G. Sutcliﬀe and F. J. Pelletier
Table 3. Example axiom-conjecture pairs and their provability
FDE
# Axioms
Conjecture
Ł3
RM3
A3
Łuk
cmi
1
CSA
0.1
0.1
CSA
CSA
2
0.1
CSA
0.1
CSA
0.1
3
0.1
CSA
CSA
CSA
CSA
4
0.1
0.1
0.1
0.1
0.1
5
CSA
CSA
0.1
CSA
CSA
6
0.1
0.1
0.1
0.1
0.1
7
CSA
CSA
CSA
CSA
CSA
8
0.1
CSA
CSA
CSA
CSA
9
0.1
CSA
CSA
CSA
CSA
10
CSA
CSA
CSA
CSA
CSA
11
3.7
3.8
4.4
39.9
CSA
12
0.1
CSA
0.1
CSA
0.1
13
CSA
CSA
CSA
CSA
CSA
14
0.1
CSA
CSA
CSA
CSA
15
CSA
CSA
CSA
CSA
CSA
16
CSA
0.1
0.1
CSA
CSA
17
CSA
GUP
GUP
GUP
CSA
18
CSA
162.1
37.6
GUP
CSA
19
CSA
430.8
GUP
GUP
CSA
20
73.3 263.1
GUP
412.3
CSA
original and translated problems.6 The translation blows up the problem signiﬁcantly,
with the eﬀect being least for the purely gap logic Ł3, greater for the purely glut log-
ics RM3 and A3, and most for the gap/glut logics FDE→Łuk and FDE→cmi. The use of
→Łuk by RM3 and FDE→Łuk apparently has a greater eﬀect than the use of →cmi by A3
and FDE→cmi. As RM3 and A3 are both purely glut logics, this diﬀerence is attributed
to the diﬀerent values for T →B. The diﬀerent values for N →B and N →F further
contribute to the diﬀerences between the translations for FDE→Łuk and FDE→cmi. The dif-
ferent blow ups naturally contribute correspondingly to the diﬃculty of the translated
problems for the FOL ATP system.
For the motivating example of Sect. 2, the diﬀerent logics again produce interest-
ingly diﬀerent results, as shown in Table 5. A proof is a positive answer to the query,
6 Thanks to Giles Reger for providing a special version of Vampire that normalises the formulae
into comparable forms.

JGXYZ: An ATP System for Gap and Glut Logics
535
Table 4. Syntactic measures for problem 20
Measure
FOL Ł3
RM3
A3
FDE→
Łuk
cmi
Number of formulae
2
6
6
6
7
7
Number of atoms
7
2080
9912 8356 45496 41130
Maximal formula depth 12
29
33
31
43
45
Number of connectives
7
2182 10418 8780 47920 43364
Number of predicates
2
6
6
6
7
7
Number of variables
8
460
2173 1847
7835
7211
Table 5. Provability of queries about the undead
# Query
FOL′ Ł3′
RM3 A3
FDE→
Łuk cmi
1 Does D´esir´ee need words?
CSA
CSA
CSA
CSA
CSA
CSA
2 Is Nigel alive or not alive?
0.1
CSA 0.2
0.2
CSA
CSA
3 Does Nigel need placement?
0.1
CSA 0.1
0.2
CSA
CSA
4 Is Umberto both alive and not alive?
CSA
CSA 0.3
0.2
0.2
0.2
5 Has Umberto been buried?
0.1
0.1
CSA
CSA
CSA
CSA
6 Was D´esir´ee buried because she is not alive? 0.1
0.2
CSA
0.1
CSA
0.1
while a CSA result is a negative answer. For FOL and Ł3, the axioms are contradictory
(Umberto is alive and not alive), thus all the conjectures are theorems. For interest, the
axiom stating that Umberto is alive was removed to make the axioms consistent in FOL
and Ł3, then Vampire and JGXYZ were run on the resulting problems - these results
are shown in the columns marked FOL′ and Ł3′. Problem 1 shows that all the logics
understand that D´esir´ee does not need words, because she is not known to be alive.
Problem 2 should have a negative answer, because Nigel is not known to be alive nor
is he known to be not alive. However, FOL′ assumes that he is either alive or not alive.
RM3 and A3 do not escape from this conclusion because the only other possibility they
oﬀer is that he is both alive and not alive. In contrast, Ł3′, FDE→Łuk, and FDE→cmi allow
Nigel to be neither alive nor not alive. Problem 3 extends Problem 2, so that FOL′,
RM3, and A3 conclude that Nigel needs placement, while Ł3′, FDE→Łuk, and FDE→cmi
do not. Problem 4 should have a positive answer, which all the logics (taking the orig-
inal FOL and Ł3) support. However, for FOL and Ł3 with the contradictory axioms
the positive answer might be for the wrong reason, depending on how the ATP system
uses the axioms. Problem 5 is answered positively by FOL′ and Ł3′ as it is known that
Umberto is not alive (recall, the axiom stating that Umberto is alive is removed). In
contrast, for the other logics it is known that Umberto is alive and hence has not nec-
essarily been buried. Problem 6 illustrates the diﬀerence between →Łuk and →cmi. For
RM3 and FDE→Łuk, which both use →Łuk, it is possible that D´esir´ee is deﬁnitely (T) not
alive, but has been both (B) buried and not buried. Under →Łuk the implication is false

536
G. Sutcliﬀe and F. J. Pelletier
(F) and hence a negative answer is returned. For A3 and FDE→cmi, which both use →cmi,
the conditional would be both (B) true and false, and thus a positive answer is returned.
The only way for the implication to not be a theorem in A3 and FDE→cmi would be for
D´esir´ee to have been neither (N) buried nor not buried, or deﬁnitely not buried, which
is not the case because it’s an axiom that she has been buried.
6
Conclusion
This paper has described an ATP system, named JGXYZ, for some gap and glut logics.
JGXYZ is based on an equi-provable translation to FOL, followed by use of an existing
ATP system for FOL. A key feature of JGXYZ is that the translation to FOL is data-
driven, in the sense that it requires only the addition of a new logic’s truth tables for
the unary and binary connectives in order to produce an ATP system for the logic.
Experimental results from JGXYZ have illustrated the diﬀerences between the logics
and translated problems, both technically and in terms of a quasi-real-world use case.
Future work includes a more comprehensive investigation of gap and glut logics,
their implementation in JGXYZ, and full experimental evaluation.
References
1. Avron, A.: Natural 3-valued logics: characterization and proof theory. J. Symb. Log. 56(1),
276–294 (1991)
2. Baaz, M., Ferm¨uller, C.G., Salzer, G., Zach, R.: MUltlog 1.0: towards an expert system for
many-valued logics. In: McRobbie, M.A., Slaney, J.K. (eds.) CADE 1996. LNCS, vol. 1104,
pp. 226–230. Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-61511-3 84
3. Beckert, B., H¨ahnle, R., Oel, P., Sulzmann, M.: The tableau-based theorem prover 3TAP ver-
sion 4.0. In: McRobbie, M.A., Slaney, J.K. (eds.) CADE 1996. LNCS, vol. 1104, pp. 303–
307. Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-61511-3 95
4. Belnap, N.D.: A Useful Four-Valued Logic. In: Dunn, J.M., Epstein, G. (eds.) Modern Uses
of Multiple-Valued Logic. EPIS, vol. 2, pp. 5–37. Springer, Dordrecht (1977). https://doi.
org/10.1007/978-94-010-1161-7 2
5. Belnap, N.: A useful four-valued logic: how a computer should think. In: Anderson, A.,
Belnap, N., Dunn, J. (eds.) Entailment: The Logic of Relevance and Necessity, vol. 2, pp.
506–541. Princeton University Press (1992)
6. Dunn, J.: Intuitive semantics for ﬁrst degree entailment and coupled trees. Philos. Stud. 29,
149–168 (1976)
7. Evans, G.: Can there be vague objects? Analysis 38, 208 (1978)
8. H¨ahnle, R.: Advanced Many-Valued Logics. In: Gabbay, D.M., Guenthner, F. (eds.) Hand-
book of Philosophical Logic. HALO, vol. 2, pp. 297–395. Springer, Dordrecht (2001).
https://doi.org/10.1007/978-94-017-0452-6 5
9. Hazen, A., Pelletier, F.: K3, Ł3, LP, RM3, A3, FDE, M: How to Make Many-Valued Log-
ics Work for You. In: Omori, H., Wansing, H. (eds.) New Essays on Belnap-Dunn Logic.
Springer, Berlin (1980). To appear. Synthese Library
10. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Sharygina, N., Veith,
H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-39799-8 1

JGXYZ: An ATP System for Gap and Glut Logics
537
11. Levine, E., Parks, L.: Undead TV. Duke University Press, Durham (2007)
12. Łukasiewicz, J.: On three-valued logic. Ruch Filozoﬁcny 5, 170–171 (1920)
13. Omori, H., Wansing, H.: 40 years of FDE: an introductory overview. Stud. Log. 105(6),
1021–1049 (2017)
14. Pelletier, F.: Another argument against vague identity. J. Philos. 86, 481–492 (1989)
15. Pelletier, F., Sutcliﬀe, G., Hazen, A.: Automated reasoning for the dialetheic logic RM3. In:
Rus, V., Markov, Z. (eds.) Proceedings of the 30th International FLAIRS Conference, pp.
110–115 (2017)
16. Priest, G.: The logic of paradox. J. Philos. Log. 8, 219–241 (1979)
17. Priest, G., Routley, R., Norman, J.: Paraconsistent Logic: Essays on the Inconsistent.
Philosophia Verlag (1989)
18. Shapiro, S., Kouri Kissel, T.: Classical logic. In: Zalta, E. (ed.) The Stanford Encyclopedia
of Philosophy. Metaphysics Research Lab, Stanford University (2018). https://plato.stanford.
edu/archives/spr2018/entries/logic-classical/
19. Soboci´nski, B.: Axiomatization of a partial system of three-valued calculus of propositions’.
J. Comput. Syst. 1, 23–55 (1952)
20. Sutcliﬀe, G., Pelletier, F., Hazen, A.: Making Belnap’s “useful four-valued logic” useful. In:
Brawner, K., Rus, V. (eds.) Proceedings of the 31st International FLAIRS Conference, pp.
116–121 (2018)
21. Wikipedia contributors: logic - Wikipedia, the free encyclopedia (2018). https://en.wikipedia.
org/w/index.php?title=Logic&oldid=875884116

GKC: A Reasoning System for Large
Knowledge Bases
Tanel Tammet(B)
Tallinn University of Technology, Tallinn, Estonia
tanel.tammet@taltech.ee
Abstract. This paper introduces GKC, a resolution prover optimized
for search in large knowledge bases. The system is built upon a shared
memory graph database Whitedb, enabling it to solve multiple diﬀer-
ent queries without a need to repeatedly parse or load the large parsed
knowledge base from the disk. Due to the relatively shallow and sim-
ple structure of most of the literals in the knowledge base, the indexing
methods used are mostly hash-based. While GKC performs well on large
problems from the TPTP set, the system is built for use as a core system
for developing a toolset of commonsense reasoning functionalities.
Keywords: Automated reasoning · Knowledge base
1
Introduction
We present the ﬁrst release of GKC (acronym for “Graph Knowledge Core”),
version 0.1: an automated theorem prover for ﬁrst order logic, optimized for
handling large knowledge bases. GKC is intended to become a building block
for developing specialized methods and strategies for commonsense reasoning,
including nonmonotonic reasoning, probabilistic reasoning and machine learning
methods. We envision natural language question answering systems as the main
potential application of rule-applying commonsense reasoning methods.
The immediate focus of GKC is implementing core technologies for eﬃcient
handling of large knowledge bases like DBpedia, YAGO, NELL and OpenCyc.
The reason for this focus is the recognition that any useful commonsense rea-
soning system would necessarily contain a very large set of facts and rules, most
of which have a relatively simple structure.
Due to the complexities of handling large knowledge bases, the most common
current approach is to rely on specialized query tools like SPARQL or reasoning
systems built for RDFs or some restricted subset of OWL, focusing mostly on
taxonomies, while complex relations between diﬀerent objects are rarely handled.
On the other hand, the natural language question answering systems currently
rely mainly on so-called shallow reasoning using methods based on word vectors.
Our hypothesis is that enabling eﬃcient “deep” reasoning with more complex
rules would complement and signiﬁcantly enhance the capabilities of common-
sense reasoning systems. This approach has been pursued in several papers and
implementations. In particular, we note the papers [4,6,8,15,17].
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 538–549, 2019.
https://doi.org/10.1007/978-3-030-29436-6_32

GKC: A Reasoning System for Large Knowledge Bases
539
GKC is undergoing active development and the current version 0.1 of GKC
is the ﬁrst public release of the system [18]. Hence the set of implemented search
strategies and indexing technologies is limited, with heavy stress on implement-
ing eﬃcient core algorithms. The main algorithmic and technical features of
GKC are:
1. The use of a shared memory database to enable fast startup of proof search
and independent parallel searches on the same knowledge base.
2. Using hash indexes instead of tree indexes.
3. Eﬃcient clause-to-clause subsumption algorithm using hashes of ground lit-
eral structure.
4. Simpliﬁcation of derived clauses by hash-based search of contradicting units
from all derived clauses.
5. Using several separate queues for picking clauses for the set of support strat-
egy.
We expect GKC to become signiﬁcantly stronger in near future. We have
already started experiments with building nonmonotonic and probabilistic rea-
soning on a separate layer on top of GKC. However, the GKC system itself
will be developed as a conventional ﬁrst order reasoner without including the
experimental features.
2
Architecture
GKC is a conventional ﬁrst order theorem prover with the architecture and
algorithms tuned for large problems. It is implemented in C on top of the data
structures and functionality of the shared memory database WhiteDB [14]. GKC
is available for both Linux and Windows under GNU AGPLv3 and is expected
to be highly portable, see [18].
The prover is run from the command line, indicating the command, the input
ﬁle, the strategy selection ﬁle using JSON syntax and optionally the parsed and
prepared knowledge base as a shared memory handle number. Both a detailed
conﬁgurable running log and a detailed proof is printed for each solution found.
The shared memory database makes it possible to start solving a new problem
in the context of a given knowledge base very quickly: parsing and preprocessing
the large knowledge base can be performed before the GKC is called to do proof
search: the preprocessed database can be assumed to be already present in the
memory. Since GKC does not write into the shared database during search, sev-
eral GKC-calling processes can run simultaneously without locking while using
the single copy of a knowledge base in memory. The memory database can be
dumped and read from the disk and there could be multiple memory databases
present in shared memory simultaneously.
WhiteDB is a lightweight NoSQL database library written jointly by the
author of this paper and Priit J¨arv as a separate project. It is available under
the GPL licence and can be compiled from C source or installed as a Debian
package. All the data is kept in main memory and can be dumped and read back

540
T. Tammet
to and from the disk as a whole. There is no server process, data is read and
written directly from/to shared memory, no sockets are used between WhiteDB
and the application program. Since data is kept in shared memory, it is accessible
to separate processes.
GKC implements a parser for the [1] ﬁrst order formula syntax. The parser is
implemented using GNU tools Flex and Bison. There is also a parser for the Otter
clause normal form syntax and a simpliﬁed Prolog syntax. The parsed formula is
converted to a clause normal form (CNF) using both ordinary CNF conversion
rules and replacement of subformulas with introduced predicates in cases there
is a danger of the CNF size exploding, using a simple top-down procedure: any
time we see that the distribution should be applied to (a&b) ∨(c&d), renaming
is performed.
3
Algorithms, Strategies and Optimizations
The derivation rules currently implemented in GKC are very basic. The prefer-
ence stems from our focus of optimizing for the set of support strategy (see [9]
for core terminology).
1. Binary resolution with optionally the set of support strategy, negative or
positive ordered resolution or unit restriction.
2. Factorization.
3. Paramodulation with the Knuth-Bendix ordering.
In particular, we note that neither the demodulation, hyperresolution, unit-
-resulting resolution nor purely propositional methods like AVATAR have been
implemented so far. We plan to add these rules, but they have not been a priority
for GKC development.
The overall iteration algorithm of GKC is based on the common given-clause
algorithm where newly derived clauses are pushed into the passive list and then
selected (based on the combination of creation order and clause weight) as a
given clause into an active list. The derivation rules are applied only to the
given clause and active clauses. In the following we explain how we perform
clause simpliﬁcation with all the derived clauses present in the active list. We
will also explain the use of diﬀerent clause selection queues used by GKC.
The set of support strategy we rely upon for large problems basically means
that the large knowledge base is immediately put into the active list and no
direct derivations between the clauses in the knowledge base are performed. As
an additional limitation we do not perform subsumption of given clauses with
non-unit clauses in the knowledge base: during our experiments the time spent
for this did not give suﬃcient gains for the eﬃciency of proof search.
3.1
Hash Indexes and Their Use
Perhaps the most interesting innovation in GKC is the pervasive use of hash
indexes instead of tree indexes. In contrast, all state-of-the-art provers imple-
menting resolution rely on tree indexes of various kinds. Research into suitable

GKC: A Reasoning System for Large Knowledge Bases
541
tree indexes has been an important subﬁeld of automated reasoning: see [12] for
an early overview of common indexing techniques.
Our experiments demonstrate that hash indexes are a viable alternative and
possibly a superior choice in the context of large knowledge bases. The latter are
expected to consist mostly of ground clauses representing known “facts” and a
signiﬁcant percentage of derived literals are ground as well.
Hash indexes are particularly well suited for ground literals. The motivation
for using hash indexes can be summed up as:
1. Hash indexes take up much less space than tree indexes.
2. Hash indexes are possibly faster for our primary scenario of large knowl-
edge bases with a shallow term structure, although conﬁrming this hypothesis
would need further research.
3. Hash indexes are simpler to implement than tree indexes.
A hash index of a term or an atom is an integer. We compute the hash of a
term by sequentially adding the hashes of constants and variables (hash in the
formula) to the previous value with a popular addition function sdbm developed
for strings: value + (hash << 6) + (hash << 16) - hash. The same itera-
tive function is used for calculating hashes of constants interpreted as strings, to
be used in the hash calculation of the term. These hashes are cached in the data
structure of the constant to avoid regular recomputation. A hash of a variable
is based on the order of a ﬁrst variable occurrence in the clause: i.e. if hashes of
two literals in two separate clauses are equal, then the literals are equal modulo
renaming the variables in the whole clause.
As a hash index we use a simple integer array, elements of which point to
hash chains. We currently use the arrays with a length of one million. A minor
drawback of this approach is the fact that the hash arrays have to be zeroed
during the initialization phase of proof search, which takes time proportional to
the array size.
The ﬁrst important use of hash indexes in GKC is simplifying the newly
derived clauses. Each derived clause immediately undergoes simpliﬁcation and
subsumption attempts by looking for existing unit clauses in the passive or active
clause list, either deleting some of its literals or subsuming the clause. For this
we search for exactly equal literals with the same or negative polarity by looking
up the atoms in the hash index. Each match is followed by an actual equality
check with atoms in the hash chain. Every unit clause derived is pushed to this
hash index. We note that the simpliﬁcation algorithm can also cut oﬀliterals
containing variables.
GKC uses an analogous hash index for forward subsumption of newly selected
given clauses with unit clauses from the list of active clauses.
Finally, we use hashes of head predicate and function symbols while process-
ing a given clause for the purpose of looking for literals to resolve and paramod-
ulate upon and terms to paramodulate into.

542
T. Tammet
3.2
Hash Features for Clause-to-clause Subsumption
Eﬃcient clause-to-clause subsumption is important for forward subsumption of
long clauses. Deriving and using long clauses is very common for the set of sup-
port strategy crucial for large knowledge bases. In our experiments the forward
subsumption of newly given clause by a given-clause algorithm dominated the
time spent on search until we implemented hash features. We note that GKC
performs full subsumption only for given clauses, and not for newly derived
clauses: for the latter we only perform fast hash-based subsumption with units
for memory conservation. Also, we do not perform full self-subsumption of the
axiom set for the set-of-support strategy.
It is well known that the tree indexing methods do not perform well for
subsumption of long clauses. Earlier work has presented algorithms for eﬃciently
ﬁltering out impossible subsumption cases by feature vectors, see [3] and [13].
The well-known core idea for checking whether a clause A could subsume
a clause B is to look at a short vector of meta-information – features – stored
along with the clause A and compare it to the corresponding meta-information
of B. For example, a longer clause cannot subsume a shorter clause, a deeper
clause cannot subsume a shallower clause, etc.
The basic features GKC uses for comparison are ground/non-ground, clause
length, size, depth, length of a negative subset, length of a ground subset.
Additionally – what turned out to have a signiﬁcant eﬀect – we compute and
compare hash features as the hashes of ground preﬁxes. A ground preﬁx denotes
the part of the sequential atom representation until the ﬁrst occurrence of a
variable. As one of the non-hash features we use the longest ground preﬁx.
We look at ground hashes of several short lengths (currently 1, 2 and 3) of
ground preﬁxes: length 1 corresponds to (signed) predicate symbols, length 2
to a predicate symbol plus the ﬁrst the argument, assuming it is ground, etc.
Each integer hash of the ground preﬁx is again hashed to a small integer 0 . . . 29
corresponding to a single set bit position in an integer.
Hence, for 30-bit integers as used in the WhiteDb encoding we essentially
have 30 diﬀerent representations for ground preﬁxes. Finally, the one-bit repre-
sentations of ground preﬁx hashes of all literals in a clause are put together by
using the bit-wise logical or. As a consequence we can use the following obser-
vation during clause-to-clause subsumption: a clause A cannot subsume a clause
B if the bit-wise representation of ground preﬁxes of the same length of A are
not a bit-wise subset of the same bit-wise representation for B.
We will bring examples of the performance of hash features in the later
section.
3.3
Clause Selection Queues
Clause selection is, in our understanding, the most crucial choice point of reso-
lution provers, see [11]. Hence we plan to carry out more research and experi-
mentation with GKC for this direction.

GKC: A Reasoning System for Large Knowledge Bases
543
Currently we perform the selection of a given clause by using several queues
in order to spread the selection relatively uniformly over diﬀerent important
categories of derived clauses. The queues are organized in two layers.
As a ﬁrst layer we use the common ratio-based algorithm [11] of alternating
between selecting N clauses from a weight-ordered queue and one clause from
the FIFO queue with the derivation order. This pick-given ratio N is set to 4 by
default.
As a second layer we use four separate queues based on the derivation history
of a clause. Each queue in the second layer contains the two sub-queues of the
ﬁrst layer.
We note that formulas in TPTP are normally annotated as either being
axioms or conjectures/goals to be proved or assumptions/hypothesis posed and
relevant for the goal. This annotation can be seen to arise naturally in question
answering tasks from a large knowledge base: the latter consists of axioms and
the question can be often split into the goal and the assumptions part.
We split all the input and derived clauses into four classes based on their
history according to the annotations:
1. Clauses having both the goal and assumption in the derivation history.
2. Clauses having some goal clauses in the derivation history.
3. Clauses having some assumption clauses in the derivation history.
4. Clauses having only axioms in the derivation history.
These four queues are disjoint and if conditions are not mutually exclusive,
the higher (earlier) one has priority.
Our initial experiments with diﬀerent ratios for these queues indicate that
giving more preference to the ﬁrst two seems to be a better strategy for large
problems than a uniform approach. Obviously, an optimal ratio is highly depen-
dent on the type of the problem.
All in all, the use of these four queue classes has been highly beneﬁcial for
the performance of GKC, both for the set of support strategy and all the other
resolution strategies.
4
Term Representation
GKC uses WhiteDB data structures for term representation: each term or clause
is represented as a WhiteDB database record containing meta-information fol-
lowed by term elements encoded as integers. Meta-information for literals in the
clause is kept on the clause level to avoid the need to follow a pointer to access
the literal meta-information.
WhiteDB database records – and hence also the main data structures in
GKC – are tuples of N elements, each element encoded as an integer in the
WhiteDB-s data encoding scheme. Since the WhiteDB data structures can be
kept in the shared memory where absolute pointers do not work (processes map
memory areas to diﬀerent address spaces), conventional pointers are not used in
the main data structures: this role is given to integers indicating oﬀset to the

544
T. Tammet
current memory area, thus enabling the data in memory to be independent of
its exact location.
Low bits of an integer in a record indicate the type of data stored in high
bits. Pointers (oﬀsets) have always zeros in low bits. Predicate and function
symbols are represented as URI-s. Small integers, ﬂoats, boolean constants and
variables ﬁt into one integer directly, while large integers, doubles, strings and
URI-s are represented as an oﬀset to a separate data structure. In particular,
URI-s are stored uniquely and contain various statistical and cached information
in addition to the namespace and main strings.
WhiteDB uses our own implementation of a malloc-like allocator for shared
memory and simple continuous allocation from memory pools for terms, literals
and clauses. In particular, memory space for subterms and literals in a clause
is continuous, which improves cache locality, important for walking through an
atom or a term.
5
Performance
We describe the performance of GKC by comparing it to the results from the
ﬁrst order proof search category FOF of the latest CASC competition CASC-
J9 held in 2018, see [16]. The detailed logs of all proof attempts as well as a
working system for experimentation can be found in the GKC repository release
v0.1–alpha [18].
The long-term winner of CASC – since 2002 – is the prover Vampire [10]
with a clearly superior performance to all the other competitors. Incidentally,
an early prover Gandalf [2] of the current author won an analogous division of
CASC in 1997 and 1998. GKC has no direct relations to Gandalf nor shares any
code.
The FOF division presents 500 randomly chosen problems from TPTP (see
[1] and [7]) to the competitors with the goal to solve as many as possible under
the given time limit. The competition was run on a quad-core Intel(R) Xeon(R)
E5-2609 chip, with each problem solution attempt having access to all the cores
for a maximum of 300 s.
We note that the problems were selected from a wide variety of diﬀerent
problem classes and for the most part are not very large. GKC is not designed
for the majority of these classes.
For comparison purposes we ran GKC on a laptop with Ubuntu Linux 16.04
and the Intel(R) Core(TM) i7-5500U chip, using just one core. Shared memory
was not used, i.e. each proof attempt started from scratch and included parsing
and problem preparation.
A ﬁxed sequence of simple strategies was run inside the limit of 200 s. The
core strategies were binary ordered resolution, unit resolution and the set of
support resolution. These were combined with none or small static limits (1, 2,
and 4) for term depth and either using four beforementioned clause selection
queue classes for giving preference to goal- and assumption-derived clauses or
only one, common queue class, giving preference to smaller and older clauses

GKC: A Reasoning System for Large Knowledge Bases
545
regardless of history. For the set of support strategy stronger preference was
given to the clauses containing goals and assumptions in their history. For unit
and ordered binary resolution we used either no history-based preference at all
or equal preference to the queues of clauses derived from goals, assumptions,
combination of these or axioms only.
It is worth noting that on average the binary ordered resolution with several
equally preferred history-based queues performed better than having no history-
based preference. On the other hand, strong preference to queues formed from
goal- and assumption-derived clauses performed on average worse than equal
preference among the queues. The same cannot be said of the set of support
strategy, where stronger goal- and assumption preference of said queues was
better on average.
Equality was always handled by paramodulation with the Knuth-Bendix
ordering. No demodulation was used. In short, the strategies selected were basic,
not specially tuned or dependent upon a problem given.
In this setting GKC showed satisfactory performance, landing in the ﬁrst half
of the result list.
The following table inserts the GKC result into the oﬃcial list of CASC-
J9 results for comparison purposes. GKC did not take part of this competition
(Table 1).
Table 1. CASC-J9 FOF results with GKC inserted.
System
Proofs
Vampire 4.3
461
Vampire 4.2
454
CSE E 1.0
363
E 2.2pre
350
CVC4 1.6pre
298
GKC 0.1
260
Leo-III 1.3
256
iProver 2.8
248
leanCoP 2.2
143
nanoCoP 1.1
126
CSE 1.1
123
CSE 1.0
122
Prover9 1109a 122
Twee 2.2
74
Geo-III 2018c
50
Next we will have a look at the performance of hash features for clause-
to-clause subsumption described earlier. We will consider all clause-to-clause

546
T. Tammet
subsumption attempts of non-ground-unit clauses for the two hardest problems
for GKC from the seven largest problems from CASC-J9.
The subsumption pre-ﬁlter runs in stages, each stage detecting that subsump-
tion of A by B is impossible due to some features stored as meta-information.
As described earlier, the ﬁrst, top features stage, considers ordinary features
like clause length, depth etc. The hash preﬁx stages introduced in GKC check
the bit-wise inclusion of encoded hashes of ground preﬁxes of length 1, 2 and 3.
Only the subsumption attempts passing all these ﬁlters will continue to the stage
where the subsumption of literals in respective clauses is considered (Table 2).
Table 2. Subsumption preﬁlter performance example.
Filter stage
CSR056+6 CSR033+6
All subsumptions attempted 360280255
669782763
Passed top features stage
13818803
6288334
Passed hash preﬁx length 1
772059
437145
Passed hash preﬁx length 2
448353
78176
Passed hash preﬁx length 3
435218
54881
5.1
Performance on the Largest Problems in TPTP
The experiments in this section are conducted using a newer release 0.1-epsilon of
GKC, having better command-line support for using shared memory databases
and being roughly twice faster for large problems than the release 0.1-alpha used
in the previously described experiments.
The largest problems in TPTP, CSR025 . . . CSR074, ask questions from
the axioms built from the OpenCyc database: 50 problems for the axiom set
CSR002+5.ax with over three million formulae and 50 problems for the subset
of the latter, the axiom set CSR002+4.ax with over half a million formulae.
Importantly, the larger set CSR002+5.ax is itself unsatisﬁable, while the smaller
CSR002+4.ax is satisﬁable. We note that CASC-J9 contained seven problems
based on the set CSR002+5.ax and only Vampire and iProver could solve any
of these problems.
The default strategy of GKC for large formulas is binary resolution with the
set-of-support strategy and earliest-derived vs. lightest-clause picking ratio four,
with no special limits or preferences.
GKC parses and indexes the CSR002+5.ax set into shared memory in ca
23 s. After that the proof searches are run as new independent command-line
commands which do not need to parse or do initial indexing and could be run in
parallel. In this setting GKC proves 44 of the problems with the default strategy,
most of them under 1 s and the longest time being 31 s. The remaining six of the
problems are proved with a few variations of the clause picking ratio and set of
support preferences, with the longest time being 3 min.

GKC: A Reasoning System for Large Knowledge Bases
547
Since CSR002+5.ax is itself unsatisﬁable, the problems based upon it are
not well suited for comparison with other provers. Although the GKC strategy
appears to use the given question in the derivation, there is no strict obligation
to do so.
Next we look at the same 50 problems asked about the smaller, satisﬁable
axiom set CSR002+4.ax. GKC parses and indexes the CSR002+4.ax set into
shared memory in ca 3.7 s. Again, using the shared memory database, GKC
proves 45 of the problems with the default strategy and the remaining ﬁve hard
problems either with a unit strategy limit (one of the clauses resolved upon must
be a unit clause) or a derived clause size limit of 2. 36 of the problems are solved
under 0.1 s, 12 between 0.1 and 1 s and the slowest two under 3 s.
We have compared GKC performance on the same problems with Vampire
4.2.2 in the “casc” mode. Most of the problems are solved in ca 10 s and most of
this time is spent on parsing and initial indexing. The default initial strategy of
Vampire solves 41 problems, while the remaining nine hard problems are solved
after sequentially trying several strategies. These sequential attempts take ca
one minute and in one case two minutes until the proof is found. The ﬁve hard
problems for GKC are a subset of the nine hard problems for Vampire.
6
Summary and Further Work
We have presented a new automated theorem prover GKC optimized for search
in large knowledge bases. While the development of GKC is ongoing, it is already
a usable and performant generic theorem prover. The main outstanding practical
capabilities of GKC as it stands now are its ability to use prepared knowledge
bases in shared memory and top of the line performance for handling large
knowledge bases.
From a research perspective we note that our experiments with GKC indicate
that it is feasible for a theorem prover to rely purely on the hash indexes and
avoid tree indexes altogether. We have introduced hash preﬁx ﬁlters for clause-
to-clause subsumption and demonstrated their good performance. This said, we
acknowledge that tree indexes do have superior performance in scenarios with
deep term structures.
We plan to pursue the following directions for future work:
1. Improving the general-purpose performance of GKC by implementing addi-
tional derivation rules, algorithms and strategies. We do plan to participate
in the next CASC competition.
2. Implementing specialized methods for large knowledge bases, like precompu-
tation and built-in handling of transitive properties.
3. Improving the functionality of knowledge base preparation and precomputa-
tion with methods like vector-based statistical analysis for likely interdepen-
dencies.
4. Measuring the performance of hash indexes when compared to tree indexes
for diﬀerent problem classes.

548
T. Tammet
5. Developing an experimental toolset of commonsense reasoning functionalities
on top of GKC, with the principal aim to make GKC usable as a component
in natural language understanding systems.
References
1. TPTP homepage. http://tptp.cs.miami.edu/∼tptp/
2. Tammet, T.: Gandalf. J. Autom. Reason. 18(2), 199–204 (1997)
3. Tammet, T.: Towards eﬃcient subsumption. In: Kirchner, C., Kirchner, H. (eds.)
CADE 1998. LNCS, vol. 1421, pp. 427–441. Springer, Heidelberg (1998). https://
doi.org/10.1007/BFb0054276
4. Pease, A., Sutcliﬀe, G.: First order reasoning on a large ontology. In: Proceedings of
the CADE-21 Workshop on Empirically Successful Automated Reasoning in Large
Theories, vol. 257, pp. 61–70. CEUR Workshop Proceedings (2007)
5. Reagan, S.P., Sutcliﬀe, G., Goolsbey, K., Kahlert, R.C.: The Cyc TPTP Chal-
lenge Problem Set. Unpublished manuscript. http://www.opencyc.org/doc/tptp
challenge problem set
6. Suchanek, F., Kasneci, G.m Weikum, G.: YAGO: a core of semantic knowledge. In:
Proceedings of the 16th International World Wide Web Conference, Banﬀ, Canada,
pp. 697–706
7. Sutcliﬀe, G.: The TPTP world – infrastructure for automated reasoning. In: Clarke,
E.M., Voronkov, A. (eds.) LPAR 2010. LNCS (LNAI), vol. 6355, pp. 1–12. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-17511-4 1
8. Suda, M., Weidenbach, C., Wischnewski, P.: On the saturation of YAGO. In: Giesl,
J., H¨ahnle, R. (eds.) IJCAR 2010. LNCS (LNAI), vol. 6173, pp. 441–456. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-14203-1 38
9. Bachmair, L., Ganzinger, H.: Resolution theorem proving. In: Handbook of Auto-
mated Reasoning, pp. 19–99. Elsevier (2001)
10. Kov´acs, L., Voronkov, A.: First-order theorem proving and Vampire. In: Shary-
gina, N., Veith, H. (eds.) CAV 2013. LNCS, vol. 8044, pp. 1–35. Springer, Heidel-
berg (2013). https://doi.org/10.1007/978-3-642-39799-8 1
11. Schulz, S., M¨ohrmann, M.: Performance of clause selection heuristics for saturation-
based theorem proving. In: Olivetti, N., Tiwari, A. (eds.) IJCAR 2016. LNCS
(LNAI), vol. 9706, pp. 330–345. Springer, Cham (2016). https://doi.org/10.1007/
978-3-319-40229-1 23
12. Sekar, R., Ramakrishnan, I., Voronkov, A.: Term indexing. In: Handbook of Auto-
mated Reasoning, vol. II, chap. 26, pp. 1853–1964. Elsevier Science (2001)
13. Schulz, S.: Simple and eﬃcient clause subsumption with feature vector indexing.
In: Bonacina, M.P., Stickel, M.E. (eds.) Automated Reasoning and Mathematics.
LNCS (LNAI), vol. 7788, pp. 45–67. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-36675-8 3
14. Tammet, T., J¨arv, P.: WhiteDB homepage. https://whitedb.org
15. Furbach, U., Schon, C.: Commonsense reasoning meets theorem proving. In: Pro-
ceedings of the Workshop on Bridging the Gap between Human and Automated
Reasoning co-located with 25th International Joint Conference on Artiﬁcial Intel-
ligence IJCAI 2016, pp. 74–85. CEUR (2016)
16. Sutcliﬀe, G.: The 9th IJCAR automated theorem proving system competition -
CASC-J9. AI Commun. 31(1), 1–13 (2018)

GKC: A Reasoning System for Large Knowledge Bases
549
17. Lopez Hernandez, J.C., Korovin, K.: An abstraction-reﬁnement framework for rea-
soning with large theories. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.) IJCAR
2018. LNCS (LNAI), vol. 10900, pp. 663–679. Springer, Cham (2018). https://doi.
org/10.1007/978-3-319-94205-6 43
18. Tammet, T.: Repository of the GKC system and experiment logs (2019). https://
github.com/tammet/gkc

Optimization Modulo the Theory of
Floating-Point Numbers
Patrick Trentin(B) and Roberto Sebastiani
DISI, University of Trento, Trento, Italy
patrick.trentin@unitn.it
Abstract. Optimization Modulo Theories (OMT) is an important
extension of SMT which allows for ﬁnding models that optimize given
objective functions, typically consisting in linear-arithmetic or pseudo-
Boolean terms. However, many SMT and OMT applications, in particu-
lar from SW and HW veriﬁcation, require handling bit-precise represen-
tations of numbers, which in SMT are handled by means of the theory
of Bit-Vectors (BV) for the integers and that of Floating-Point Numbers
(FP) for the reals respectively. Whereas an approach for OMT with
(unsigned) BV has been proposed by Nadel & Ryvchin, unfortunately
we are not aware of any existing approach for OMT with FP.
In this paper we ﬁll this gap. We present a novel OMT approach, based
on the novel concept of attractor and dynamic attractor, which extends
the work of Nadel & Ryvchin to signed BV and, most importantly, to
FP. We have implemented some OMT(BV) and OMT(FP) procedures
on top of OptiMathSAT and tested the latter ones on modiﬁed prob-
lems from the SMT-LIB repository. The empirical results support the
validity and feasibility of the novel approach.
1
Introduction
Optimization Modulo Theories (OMT) [5,15,16,19–21,23,25–27] is an important
extension to Satisﬁability Modulo Theories which allows for ﬁnding models that
optimize one or more objectives, which typically consist in some linear-arithmetic
or Pseudo-Boolean function application.
However, many SMT and OMT applications, in particular from SW and
HW veriﬁcation, require handling bit-precise representations of numbers, which
in SMT are handled by means of the theory of Bit-Vectors (BV) for the inte-
gers and that of Floating-Point Numbers (FP) for the reals respectively. (For
instance, during the veriﬁcation process of a piece of software, one may look for
the minimum/maximum value of some int [resp. float] parameter causing an
SMT(BV) [resp. SMT(FP)] call to return sat—which typically corresponds to
the presence of some bug—so that to guarantee a safe range for such parameter.)
We would like to thank the anonymous reviewers for their insightful comments and
suggestions, and we thank Alberto Griggio for support with MathSAT5 code.
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 550–567, 2019.
https://doi.org/10.1007/978-3-030-29436-6_33

Optimization Modulo the Theory of Floating-Point Numbers
551
OMT for the theory of (unsigned) bit-vectors (OMT(BV)) was proposed
by Nadel and Ryvchin [21], although a reduction to the problem to MaxSAT
was already implemented in the SMT/OMT solver Z3 [6]. The work in [21] was
based on the observation that OMT on unsigned BV can be seen as lexicographic
optimization over the bits in the bitwise representation of the objective, ordered
from the most-signiﬁcant bit (MSB) to the least-signiﬁcant bit (LSB).
In this paper we address—for the ﬁrst time to the best of our knowledge—
OMT for the theory of signed Bit-Vectors and, most importantly, for the theory
of Floating-Point Arithmetic (OMT(FP)), by exploiting some properties of the
two’s complement encoding for signed BV and of the IEEE 754-2008 encoding
for FP respectively.
We start from introducing the notion of attractor, which represent (the bit-
wise encoding of) the target value for the objective which the optimization pro-
cess aims at. This allows us for easily leverage the procedure of [21] to work
with both signed and unsigned Bit-Vectors, by minimizing lexicographically the
bitwise distance between the objective and the attractor, that is, by minimizing
lexicographically the bitwise-xor between the objective and the attractor.
Unfortunately there is no such notion of (ﬁxed) attractor for FP numbers,
because the target value moves as long as the bits of the objective are updated
from the MSB to the LSB, and the optimization process may have to change
dynamically its aim, even at the opposite direction. (For instance, as soon as the
minimization process realizes there is no solution with a negative value for the
objective and thus sets its MSB to 0, the target value is switched from −∞to
0+, and the search switches direction, from the maximization of the exponent
and the signiﬁcand to their minimization.)
To cope with this fact, we introduce the notions of dynamic attractor and
attractor trajectory, representing the dynamics of the moving target value, which
are progressively updated as soon as the bits of the objective are updated from
the MSB to the LSB. Based on these ideas, we present novel OMT(FP) proce-
dures, which require at most n + 2, incremental calls to an SMT(FP) solver, n
being the number of bits in the representation of the objective. Notice that these
procedures do not depend on the underlying SMT(FP) procedure used, provided
the latter allows for accessing and setting the single bits of the objective.
We have implemented these OMT(BV) and OMT(FP) procedures on top of
the OptiMathSAT OMT solver [27]. We have run an experimental evaluation
of the OMT(FP) procedures on modiﬁed SMT(FP) problems from the SMT-
LIB library. The empirical results support the validity and feasibility of the novel
approach.
The rest of the paper is organized as follows. In Sect. 2 we provide the nec-
essary background on BV and FP theories and reasoning. In Sect. 3 we provide
the novel theoretical deﬁnitions and results. In Sect. 4 we describe our novel
OMT(FP) procedures. In Sect. 5 we present the empirical evaluation. In Sect. 6
we conclude, hinting some future directions. The proofs of the theoretical results
from Sect. 3 are in the extended version of this paper [28].

552
P. Trentin and R. Sebastiani
2
Background
We assume some basic knowledge on SAT and SMT and brieﬂy introduce the
reader to the Bit-Vector and Floating-Point theories.
Bit-Vectors. A bit is a Boolean variable that can be interpreted as 0 or 1. A Bit-
Vector (BV) variable v[n] is a vector of n bits, where v[0] is the Most Signiﬁcant
Bit (MSB) and v[n −1] is the Least Signiﬁcant Bit (LSB).1 A BV constant
of width n is an interpreted vector of n values in {0, 1}. We overline a bit
value or a BV value to denote its complement (e.g., [11010010] is [00101101]).
A BV variable/constant of width n can be unsigned, in which case its domain
is [0, 2n −1], or signed, which we assume to comply with the Two’s complement
representation, so that its domain is [−2(n−1), 2(n−1) −1]. Therefore, the vector
[11111111] can be interpreted either as the unsigned BV constant 255[8] or as
the signed BV constant −1[8]. Following the SMT-LIBv2 standard [3], we may
also represent a BV constant in binary (e.g. 28[8] is written #b00011100) or
in hexadecimal (e.g. 28[8] is written #x1C) form. A BV term is built from
BV constants, variables and interpreted BV functions which represent standard
RTL operators: word concatenation (e.g. 3[8] ◦x[8]), sub-word selection (e.g.
(3[8][6 : 3])[4]), modulo-n sum and multiplication (e.g. x[8] +8 y[8] and x[8] ·8 y[8]),
bit-wise operators (like, e.g., andn, orn, xorn, nxorn, notn), left and right shift
<<n, >>n. A BV atom can be built by combining BV terms with interpreted
predicates like ≥n, <n (e.g. 0[8] ≥8 x[8]) and equality. We refer the reader to [3]
for further details on the syntax and semantics of Bit-Vector theory.
There are two main techniques for BV satisﬁability, the “eager” and the
“lazy” approach, which are substantially complementary to one another [18].
In the eager approach, BV terms and constraints are encoded into SAT via
bit-blasting [13,17,22]. In the lazy approach, BV terms are not immediately
expanded –so to avoid any scalability issue– and the BV solver is comprised by
a layered set of techniques, each of which deals with a sub-portion of the BV
theory [7,12,14].
Floating-Point. The theory of Floating-Point Numbers (FP), [3,10,24], is based
on the IEEE standard 754-2008 [4] for ﬂoating-point arithmetic, restricted to the
binary case. A FP sort is an indexed nullary sort identiﬁer of the form (
FP
<ebits> <sbits>) s.t. both ebits and sbits are positive integers greater than one,
ebits deﬁnes the number of bits in the exponent and sbits deﬁnes the number of
bits in the signiﬁcand, including the hidden bit. A FP variable v[n] with sort (
FP <ebits> <sbits>) can be indiﬀerently viewed as a vector of n
def
= ebits + sbits
bits, where v[0] is the Most Signiﬁcant Bit (MSB) and v[n −1] is the Least
Signiﬁcant Bit (LSB), or as a triplet of Bit-Vectors ⟨sign, exp, sig⟩s.t. sign is
1 Although most often in the literature the indexes i ∈[0, . . . , n −1] use to grow from
the LSB to the MSB, in this paper we use the opposite notation because we always
reason from the MSB down to the LSB, so that to much simplify the explanation.

Optimization Modulo the Theory of Floating-Point Numbers
553
a BV of size 1, exp is a BV of size ebits and sig is a BV of size sbits −1. A FP
constant is a triplet of BV constants. Given a ﬁxed ﬂoating-point sort, i.e. a pair
⟨ebits, sbits⟩, the following FP constants are implicitly deﬁned:
value
Symbol
BV Repr.
plus inﬁnity
(
+oo <ebits> <sbits>)
(fp #b0 #b1...1 #b0...0)
minus inﬁnity (
-oo <ebits> <sbits>)
(fp #b1 #b1...1 #b0...0)
plus zero
(
+zero <ebits> <sbits>) (fp #b0 #b0...0 #b0...0)
minus zero
(
-zero <ebits> <sbits>) (fp #b1 #b0...0 #b0...0)
not-a-number (
NaN <ebits> <sbits>)
(fp t #b1...1 s)
where t is either 0 or 1 and s is a BV which contains at least a 1.
Setting aside special FP constants, the remaining FP values can be classiﬁed
to be either normal or subnormal (a.k.a. denormal) [4]. A FP number is said
to be subnormal when every bit in its exponent is equal to zero, and normal
otherwise. The signiﬁcand of a normal FP number is always interpreted as if
the leading binary digit is equal 1, while for denormalized FP values the leading
binary digit is always 0. This allows for the representation of numbers that are
closer to zero, although with reduced precision.
Example 1. Let x be the normal FP constant (
FP #b0 #b1100 #b0101000),
and y be the subnormal FP constant (
FP #b0 #b0000 #b0101000), so that
their corresponding sort is (
FP <4> <8>). Then, according to the semantics
deﬁned in the IEEE standard 754-2008 [4], the ﬂoating-point value of x and y
in decimal notation is given by:
x = (−1)0 · 2(12−7) ·

1 +
7

i=1

x[4 + i] · 2−i
= 1 · 25 ·

1 + 1
22 + 1
24

= 42
y = (−1)0 · 2(0−7+1) ·

0 +
7

i=1

y[4 + i] · 2−i
= 1 · 2−6 ·
 1
22 + 1
24

=
5
210 .
⋄
The theory of FP provides a variety of built-in ﬂoating-point operations as
deﬁned in the IEEE standard 754-2008. This includes binary arithmetic oper-
ations (e.g. +, −, ⋆, ÷), basic unary operations (e.g. abs, −), binary compari-
son operations (e.g. ≤, <, ̸=, =, >, ≥), the remainder operation, the square root
operation and more. Importantly, arithmetic operations are performed as if with
inﬁnite precision, but the result is then rounded to the “nearest” representable
FP number according to the speciﬁed rounding mode. Five rounding modes are
made available, as in [4].
The most common approach for FP-satisﬁability is to encode FP expres-
sions into BV formulas based on the circuits used to implement ﬂoating-point
operations, using appropriate under- and over-approximation schemes –or a mix-
ture of both– to improve performance [11,29,30]. Then, the BV-Solver is used
to deal with the FP formula, using either the eager or the lazy BV approach.
An alternative approach, based on abstract interpretation, is presented in [8,9].

554
P. Trentin and R. Sebastiani
With this technique, called Abstract CDCL (ACDCL), the set of feasible solu-
tions is over-approximated with ﬂoating-point intervals, so that intervals-based
conﬂict analysis is performed to decide FP-satisﬁability.
3
Theoretical Framework
We present our generalization of [21] to the case of signed/unsigned Bit-Vector
Optimization, and then move on to deal with Floating-Point Optimization.
3.1
Bit-Vector Optimization
Without any loss of generality, we assume that every objective function f(...) is
replaced by a variable obj of the same type by conjoining “obj = f(...)” to the
input formula. We use the symbol n to denote the bit-width of obj, and obj[i] to
denote the i-th bit of obj, where obj[0] and obj[n−1] are the Most Signiﬁcant Bit
(MSB) and the Least Signiﬁcant Bit (LSB) of obj respectively. (See footnote 1)
Deﬁnition 1 (OMT(BV)). Let ϕ be a SMT(BV) formula and obj be a –signed or
unsigned– BV variable occurring in ϕ. We call an Optimization Modulo BV
problem, OMT(BV), the problem of ﬁnding a model M for ϕ (if any) whose
value of obj, denoted with minobj(ϕ), is minimum w.r.t. the total order relation
≤n for signed BVs if obj is signed, and the one for unsigned BVs otherwise. (The
dual deﬁnition where we look for the maximum follows straightforwardly)
Hereafter, we generalize the unsigned BV maximization procedures described
in [21] to the case of signed and unsigned BV optimization. To this extent, we
introduce the novel notion of BV attractor.
Deﬁnition 2 (Attractor, attractor equalities).
When minimizing [resp. max-
imizing], we call attractor for obj the smallest [resp. greatest] BV-value attr
of the sort of obj. We call vector of attractor equalities the vector A s.t.
A[k]
def= (obj[k] = attr[k]), k ∈[0..n −1].
Example 2. If obj[8] is an unsigned BV objective of width 8, then its correspond-
ing attractor attr is 0[8], i.e. [00000000], when obj[8] is minimized and it is 255[8],
i.e. [11111111], when obj[8] is maximized. When obj[8] is instead a signed BV
objective, following the two’s complement encoding, the corresponding attr is
−128[8], i.e. [10000000], for minimization and 127[8], i.e. [01111111], for maxi-
mization.
⋄
In essence, the attractor can be seen as the target value of the optimiza-
tion search and therefore it can be used to determine the desired improvement
direction and to guide the decisions taken by the optimization search. By con-
struction, if a model M satisﬁes all equalities A[i], then M(obj) = attr.
We use the symbol μk to denote a generic (possibly partial) assignment which
assigns at least the k most-signiﬁcant bits of obj. We use the symbol τk to denote

Optimization Modulo the Theory of Floating-Point Numbers
555
an assignment to all and only the k most-signiﬁcant bits of obj. Given i < k, we
denote by μk[i] [resp. τk[i]] the value in {0, 1} assigned to obj[i] by μk [resp. τk].
Moreover, we use the expression [[μk]]i where i ≤k to denote the restriction of
μk to all and only the i most-signiﬁcant bits of obj, obj[0], . . . , obj[i −1]. Given
a model M of ϕ and a variable v, we denote by M(v) the evaluation of v in M.
With a little abuse of notation, and when this does not cause ambiguities, we
sometimes use an attractor equality A[i]
def
= (obj[i] = attr[i]) to denote the single-
bit assignment obj[i] := attr[i] and its negation ¬A[i] to denote the assignment
to the complement value obj[i] := attr[i].
Deﬁnition 3 (lexicographic
maximization).
Consider
an
OMT
instance
⟨ϕ, obj⟩and the vector of attractor equalities A. We say that an assignment τn
to obj lexicographically maximizes A w.r.t. ϕ iﬀ, for every k ∈[0..n −1],
– τn[k] = attr[k] if ϕ ∧[[τn]]k ∧A[k] is unsatisﬁable,
– τn[k] = attr[k] otherwise.
where A[k] is the attractor equality (obj[k] = attr[k]). Given a model M for ϕ,
we say that M lexicographically maximizes A w.r.t. ϕ iﬀits restriction to obj
lexicographically maximizes A w.r.t. ϕ.
Starting from the MSB to the LSB, τn [resp. M] in Deﬁnition 3 assigns to
each obj[k] the value attr[k] unless it is inconsistent w.r.t. ϕ and the assignments
to the previous obj[i]s, i ∈[0..k −1]. Notice that this corresponds to minimize
[resp. maximize] the value n−1
k=0 2n−1−k·(obj[k]xor1attr[k]) [resp. n−1
k=0 2n−1−k·
(obj[k] nxor1 attr[k])],—where xorn is the bitwise-xor operator and nxorn is its
complement—because 2n−1−i > n−1
k=i+1 2n−1−k.
The following fact derives from the above deﬁnitions and the properties
of two’s complement representation adopted by the SMT-LIBv2 standard for
signed BV.
Theorem 1. An optimal solution of an OMT(BV) problem ⟨ϕ, obj⟩is any model
M of ϕ which lexicographically maximizes the vector of attractor equalities A.
Deﬁnitions 2 and 3 with Theorem 1 suggest thus a direct extension to the
minimization/maximization of signed BV of the algorithm for unsigned BV in
[21]: apply the unsigned-BV maximization [resp. minimization] algorithm of [21]
to the objective obj′ def
= (obj nxorn attr) [resp. obj′ def
= (obj xorn attr)]instead than
simply to obj [resp. obj].
Example 3. Let obj[3] be a signed BV goal of 3 bits to be minimized and attr
def
=
[100] be its attractor, so that the corresponding vector of attractor equalities A
is equal to [obj[0] = 1, obj[1] = 0, obj[2] = 0].
An assignment τ3
def
= {A[0], ¬A[1], ¬A[2]} (for which obj[3] = −1[3]) is lexico-
graphically better than τ ′
3
def
= {¬A[0], A[1], A[2]} (for which obj[3] = 0[3]), because
the former satisﬁes the attractor equality corresponding to the MSB while the
latter does not. Moreover, the assignment τ3 is lexicographically worse than the

556
P. Trentin and R. Sebastiani
assignment τ ′′
3
def
= {A[0], ¬A[1], A[2]} (for which obj[3] = −2[3]), because –all the
rest being equal– the latter assignment makes the attractor equality (obj[2] = 0)
true.
⋄
3.2
Floating-Point Optimization
We deﬁne the Floating-Point Optimization problem as follows.
Deﬁnition 4 (OMT(FP)).
Let ϕ be a SMT(FP) formula and obj be a FP
variable occurring in ϕ. We call an Optimization Modulo FP problem, the
problem of ﬁnding a model M for ϕ (if any) whose value of obj, denoted with
minobj(ϕ), is either
– minimum w.r.t. the usual total order relation ≤for FP numbers, if ϕ is
satisﬁed by at least one model M′ s.t. M′(obj) is not NaN,
– some binary representation of NaN, otherwise.
(The dual deﬁnition where we look for the maximum follows straightforwardly.)
Deﬁnition 4 is made necessarily convoluted by the fact that obj can be NaN.
In fact, in the SMT-LIBv2 standard the comparisons {≤, <, ≥, >} between
NaN and any other FP value are always evaluated false because NaN has
multiple representations at the binary level. Also, requiring the optimal solu-
tion to be always diﬀerent from NaN makes the resulting OMT(FP) problem
⟨ϕ ∧¬IsNaN(obj), obj⟩unsatisﬁable when ϕ is satisﬁed only by models M s.t.
M(obj) is NaN. For these reasons, we admit NaN as the optimal solution value
for obj if and only if ϕ is satisﬁed only by models M s.t. M(obj) is NaN.
In the rest of this section we assume that we have already checked, in
sequence, that
(i) the input formula ϕ is satisﬁable—by invoking an SMT(FP) solver on ϕ.
If the solver returns unsat, then there is no need to proceed;
(ii) ϕ is satisﬁed by at least one model M′ s.t. M′(obj) is not NaN—by
invoking an SMT(FP) solver on ϕ ∧¬IsNaN(obj) if the model M returned
by the previous SMT call is s.t. M(obj) is NaN. If the solver returns unsat,
then we conclude that the minimum is NaN.
After that, we can safely focus our investigation on the restricted OMT(FP)
problem ⟨ϕnoNaN, obj⟩, where ϕnoNaN
def
= ϕ∧¬IsNaN(obj), knowing it is satisﬁable.
Deﬁnition 5 (Dynamic Attractor). Let ⟨ϕnoNaN, obj⟩be a restricted OMT(FP)
problem, where ϕnoNaN
def= ϕ∧¬IsNaN(obj) is a satisﬁable SMT(FP) formula and
obj is a FP objective to be minimized [resp. maximized]. Let k ∈[0..n] and τk
be an assignment to the k most-signiﬁcant bits of obj.
Then, we say that an FP-value attrτk for obj is a dynamic attractor for
objw.r.t. τk iﬀit is the smallest [resp. largest] FP value diﬀerent from NaN s.t.
the k most-signiﬁcant bits of attrτk have the same value of the k most-signiﬁcant
bits of obj in τk. We call vector of attractor equalities the vector Aτk s.t.
Aτk[i]
def= (obj[i] = attrτk[i]), i ∈[0..n −1].

Optimization Modulo the Theory of Floating-Point Numbers
557
The following fact derives from the above deﬁnitions and the properties of
IEEE 754-2008 standard representation adopted by SMT-LIBv2 standard for
FP.
Lemma 1. Let ⟨ϕnoNaN, obj⟩be a restricted minimization [resp. maximization]
OMT(FP) problem, let τk be an assignment to obj[0]...obj[k −1] and attrτk
be its corresponding dynamic attractor, for some k ∈[0..n −1]. Let τk+1
def=
τk ∪{obj[k] := attrτk[k]} and τ ′
k+1
def= τk ∪{obj[k] := attrτk[k]}, and let M, M′
two models for ϕnoNaN which extend τk+1 and τ ′
k+1 respectively.
Then M(obj) ≤M′(obj) [resp. M(obj) ≥M′(obj)].
Lemma 1 states that, given the current assignment τk to the k most-
signiﬁcant-bits of obj, obj[k] = attrτk[k] is always the best extension of τk to
the next bit (when consistent). A dynamic attractor attrτk can thus be used
by the optimization search to guide the assignment of the k + 1-th bit of obj
towards the direction of maximum gain which is allowed by τk, so that to obtain
the “best” extension τk+1 of τk. Once the (new) assignment τk+1 is found, the
OMT solver can compute the dynamic attractor attrτk+1 for obj w.r.t. τk+1 and
then use it to assign the k + 2-th bit of obj, and so on.
Let ⟨ϕnoNaN, obj⟩be an OMT(FP) instance, s.t. obj is a FP variable of n
bits, and τ0 be an initially empty assignment. If at each step of the optimization
search the assignment of the k-th bit of obj is guided by the dynamic attractor
for obj w.r.t. τk, then the corresponding sequence of n dynamic attractors (of
increasing order k) is unique and depends exclusively on ϕnoNaN. Intuitively, this
is the case because the (current) dynamic attractor always points in the direction
of maximum gain. We illustrate this in the following example.
Example 4. Let ⟨ϕnoNaN, obj⟩be an OMT(FP) problem where obj is a FP objec-
tive, of sort (
FP 3 5), to be minimized. At the beginning of the search, nothing
is known about the structure of the solution. Therefore, τ0 = ∅and, since obj
is being minimized, the dynamic attractor for obj w.r.t. τ0 (i.e. attrτ0) is equal
to (fp #b1 #b111 #b0000) (i.e. −∞), which gives a preference to any feasible
value of obj in the negative domain.
If at some point of the optimization search we discover that the domain of the
objective function can only be positive, so that the ﬁrst bit of obj is permanently
set to 0 in τ1, then the new dynamic attractor for obj w.r.t. τ1 (i.e. attrτ1) is
equal to (fp #b0 #b000 #b0000) (i.e. +0).
Furthermore, if later on we also ﬁnd out that at least one bit in the exponent
of obj can be assigned to 0 in a feasible solution of the problem that extends τi,
for some i, then we can remove +∞from the optimization search interval.
⋄
Deﬁnition 6 (Attractor Trajectory Aϕ).
Consider the restricted OMT(FP)
problem ⟨ϕnoNaN, obj⟩s.t. ϕnoNaN
def= ϕ ∧¬IsNaN(obj) as in Deﬁnition 5, a triplet
of
inductively-deﬁned
sequences
⟨{τ0, τ1, . . . , τn}, {attrτ0, attrτ1, . . . , attrτn},
{Aτ0, Aτ1, . . . , Aτn}⟩—where each τk is an assignment to the ﬁrst k most-
signiﬁcant bits of obj s.t. τk ⊂τk+1, attrτk is its corresponding dynamic attractor

558
P. Trentin and R. Sebastiani
and Aτk is its corresponding vector of attractor equalities—so that, for every
k ∈[0..n −1]:
(i) τk+1[k] = attrτk[k]
if ϕnoNaN ∧τk ∧Aτk[k] is unsatisﬁable,
(ii) τk+1[k] = attrτk[k] otherwise.
Then
we
deﬁne
the
attractor
trajectory
Aϕ
as
the
vector
[Aτ0[0], . . . , Aτn−1[n −1]].
The attractor trajectory Aϕ contains those attractor equalities (obj[k] =
attrτk[k]) which are of critical importance for the decisions taken by the opti-
mization search. Intuitively, this is the case because the value of the k-th bit of
obj (i.e. obj[k]) is still undecided in τk.
Fig. 1. An example of FP optimization using the dynamic attractor. (“⇒sat/unsat”
denotes the satisﬁability of ϕnoNaN ∧τk ∧Aτk[k], the symbols “′′
′′” stand for “the same
as above”. For ease of illustration, we have underlined the critical bit attrτk[k] in the
attractors and each attractor equality of the attractor trajectory Aϕ inside the vectors
of attractor equalities.)
Example 5. Let ⟨ϕnoNaN, obj⟩be a restricted OMT(FP) problem where obj is
a FP objective, of sort (
FP 3 5), to be minimized. We consider the case in
which the input formula ϕnoNaN requires obj to be larger or equal 29/2 and it
does not impose any other constraint on the value of obj. Given the sequence

Optimization Modulo the Theory of Floating-Point Numbers
559
of (partial) assignments τ0, . . . , τ8 in Fig. 1, the corresponding list of dynamic
attractors and the corresponding vectors of attractor equalities, then the attrac-
tor trajectory Aϕ is equal to the vector [obj[0] = 1, obj[1] = 0, obj[2] = 0, obj[3] =
0, obj[4] = 0, obj[5] = 0, obj[6] = 0, obj[7] = 0].
⋄
Lemma 2.
Consider ⟨ϕnoNaN, obj⟩, τ0, . . . , τn, attrτ0, . . . , attrτn, Aτ0, . . . , Aτn, and Aϕ as in
Deﬁnition 6. Then τn lexicographically maximizes Aϕ w.r.t. ϕnoNaN.
Theorem 2. Let ⟨ϕnoNaN, obj⟩, τ0, . . . , τn, attrτ0, . . . , attrτn, Aτ0, . . . , Aτn, and
Aϕ be as in Deﬁnition 6. Then, any model M of ϕnoNaN which lexicographically
maximizes the attractor trajectory Aϕ is an optimal solution for the OMT(FP)
problem ⟨ϕnoNaN, obj⟩.
4
OMT(FP) Procedures
In this paper, we consider two approaches for dealing with OMT(FP): a basic
linear/binary search, based on the inline OMT schema for OMT(LRA ∪T )
presented in [25], and Floating-Point Optimization with Binary Search (ofp-bs),
a brand-new engine inspired by the obv-bs algorithm for unsigned Bit-Vectors
in [21] and by Theorem 2 and relative deﬁnitions in Sect. 3.2.
4.1
OMT-Based Approach
The OMT-based approach for OMT(FP) adapts the linear- and binary-search
schemata for OMT(LRA ∪T ) presented in [25] to deal with FP objectives.
In the basic linear-search schema, the optimization search is advanced by
means of a sequence of linear cuts, each of which forces the OMT solver to look
for a new model M′ which improves the value of obj w.r.t. the most recent model
M. In the binary-search schema, instead, the OMT solver learns an incremental
sequence of cuts which bisect the current domain of the objective function.
In general, it is reasonable to expect the binary-search schema to converge
towards the optimal solution faster than the linear-search schema, because the
feasible domain of a FP goal can be comprised by an exponentially large number
of values (w.r.t. the bit-width of the cost function).
In either schema, whenever the optimization engine encounters for the ﬁrst
time a solution s.t. obj = NaN, the OMT solver learns a unit-clause of the form
¬(isNaN(obj)) so as to look for an optimal solution diﬀerent from NaN (if any).
When dealing with FP objectives, diﬀerently from the case of LRA in [25],
it is not necessary to implement a specialized optimization procedure within the
FP-Solver in order to guarantee the termination of the optimization search.
4.2
Floating-Point Optimization with Binary Search
The Floating-Point Optimization with Binary Search algorithm is a new engine
for OMT(FP) which is inspired by the obv-bs algorithm for OMT(BV) [21] and
is a direct implementation of Deﬁnition 6 and Theorem 2.

560
P. Trentin and R. Sebastiani
The optimization search tries to lexicographically maximize an implicit
attractor trajectory vector Aϕ, which is incrementally derived from the cur-
rent value of the dynamic attractor. The raw value of the dynamic attractor’s
bits drive the optimization search towards the direction of maximum gain at any
given point in time, without disrupting any decision that has been already made.
The dynamic attractor is incrementally updated along the search, based on the
outcome of the previous rounds of the optimization search. At each round, one
bit of the objective function is assigned its ﬁnal value. The ﬁrst round decides the
sign, the next batch of rounds decides the exponent and the remaining rounds
decide the ﬁne-grained details of the signiﬁcand.
Fig. 2. ofp-bs Algorithm for Floating-Point optimization.
The pseudo-code of ofp-bs is shown in Fig. 2. The arguments of the algo-
rithm are the input formula ϕ and the FP objective obj, where obj is a

Optimization Modulo the Theory of Floating-Point Numbers
561
FP variable with ebits bits in the exponent, sbits −1 in the signiﬁcand and
n
def
= ebits + sbits bits overall.
The procedure starts by checking whether the input formula ϕ is satisﬁable
and immediately terminates if that is not the case (lines 1–3). If obj = NaN in M
then the procedure checks whether there exists a model M′ for ϕ ∧¬IsNaN(obj)
(lines 4–5). If this is not the case, the procedure terminates immediately and
returns the pair ⟨sat, M⟩(line 7). Otherwise, the model M is updated with the
new model M′, and ϕ is permanently extended with the constraint ¬IsNaN(obj)
(lines 9–10).
At this point, the procedure initializes the value of the dynamic attractor by
invoking an external function update dynamic attractor() with the empty
assignment τ as parameter, so that the returned value is equal to −∞when
minimizing and +∞when maximizing (lines 11–12). Then, the execution moves
to the section of code implementing the core part of the ofp-bs algorithm (lines
15–28), which consists of a loop over the bits of obj, starting from the MSB obj[0]
down to the LSB obj[n −1].
Inside this loop, ofp-bs ﬁrst checks whether the value of obj[i] in M matches
the i-th bit of the (current) dynamic attractor attrτ. If this is the case, then the i-
th bit is already set to its “best” value in M. Thus, the assignment τ is extended
so as to permanently set obj[i] = attrτ[i] (line 16), and the optimization search
moves to the next iteration of the loop. If instead obj[i] ̸= attrτ[i] in M, we need
to verify whether the value of the objective function in M can be improved by
forcing the i-th bit of obj equal to the i-th bit of the dynamic attractor. To do so,
we incrementally invoke the underlying SMT solver, this time checking the satis-
ﬁability of ϕ under the list of assumptions τ ∪{obj[i] = attrτ[i]} (line 22). If the
SMT solver returns sat, then the value of the objective function has been suc-
cessfully improved. Hence, τ is extended with an assignment setting obj[i] equal
to attrτ[i], and M is replaced with the new model M′ (lines 23–25). Otherwise,
it is not possible to improve the objective function by toggling the value of obj[i],
and τ is extended so as to permanently set obj[i] ̸= attrτ[i] (line 27). At this
point, there is a mismatch between the value of the ﬁrst i+1 bits of obj in M, cor-
responding to the assignment τ, and those of the current dynamic attractor. This
mismatch is resolved by calling the function update dynamic attractor()
with the updated assignment τ as parameter (line 28). In either case, the exe-
cution moves to the next iteration of loop.
After exactly n iterations of the loop, the optimization search terminates with
the pair ⟨sat, M⟩, where M is the optimum model of the given OMT(FP ∪T )
instance. The ofp-bs algorithm requires at most n + 2 incremental calls to an
underlying SMT(FP) solver. The test in rows 17–18 allows for saving lots of such
SMT calls when the current model already assigns obj[i] to its corresponding
value in the attractor.
The function update dynamic attractor() takes as input τ, a (partial)
assignment over the k most-signiﬁcant bits of obj and, when obj is minimized2,
2 The implementation of update dynamic attractor() is dual when obj is maxi-
mized.

562
P. Trentin and R. Sebastiani
and it essentially works as follows. If τ = ∅, then nothing is known about the
solution of the problem, so −∞is returned. Otherwise, the procedure must
compute the smallest FP value diﬀerent from NaN (if any) which extends τ.
Since τ ̸= ∅then we know that the sign of the objective function has been
permanently decided in τ. If obj[0] = 0 in τ, i.e. obj must be positive, the
procedure must return the smallest positive FP value admitted by τ. Hence,
we extend τ with i=n−1
i=|τ|
obj[i] = 0 and return the corresponding FP value. If
obj[0] = 1 in τ, i.e. obj can be negative values, the procedure must return the
largest negative FP value admitted by τ. We ﬁrst check whether there exists
a bit in the exponent of obj which is assigned to 0 in τ. If that is the case,
we extend τ with i=n−1
i=|τ|
obj[i] = 1 and return the corresponding FP value.
Otherwise, the procedure returns the value −∞, which is still a viable extension
of τ.
4.3
Search Enhancements
Given a FP value attr and a FP goal obj, (a combination of) the following tech-
niques can be used to adjust the behavior of the optimization search, similarly
what has been proposed for the case of OMT(BV) by Nadel et al. in [21].
– branching preference: the bits of the FP objective obj are marked, inside
the OMT solver, as preferred variables for branching starting from the MSB
down to the LSB. This ensures that conﬂicts involving the value of the objec-
tive function are handled as early as possible, possibly reducing the amount
of work that needs to be redone after each back-jump.
– polarity initialization: the phase-saving value of each obj[i] is initialized
with the value of attr[i]. This encourages the OMT solver to assign the bits
of obj so as to reassemble the bits of attr, thus possibly speeding-up the
convergence towards the optimal value.
In the case of the basic OMT schema described in Sect. 4.1, the eﬀectiveness
of either technique depends on the initial choice for attr. In the lucky case, the
value of attr pulls the optimization search in the right direction and speeds up
the search. In the unlucky case, when attr pulls in the wrong direction, there is
no visible eﬀect or an overall slow down. For instance, in the case of the linear-
search optimization schema, enabling both options with an unlucky choice of
attr can cause the OMT solver to start the search from the furthest possible
point from the optional solution, and thus enumerate an exponential number of
intermediate solutions.
In the case of the ofp-bs algorithm described in Sect. 4.2, we use the latest
value of the dynamic attractor attrτ for both the branching preference (lines
11 and 18 of Fig. 2) and the polarity initialization (rows 12 and 19 of Fig. 2)
techniques. We observe that the value of every bit in the dynamic attractor can
change after the sign of the objective function has been decided. Furthermore,
the value of all the signiﬁcand’s bits in the dynamic attractor can also change
during the process of determining the optimal exponent value of the objective

Optimization Modulo the Theory of Floating-Point Numbers
563
function. As a consequence, if the OMT solver applies either enhancement before
the correct improving direction is known, this may cause the underlying OMT
engine to advance the search starting from a sub-optimal set of initial decisions.
Enabling both enhancements at the same time could make things even worse.
In order to mitigate this issue, we have designed a variant of our optimization-
search approach which does not apply either enhancement on those bits of the
objective function for which the best improving direction is not yet known. We
have called this variant safe bits restriction.
5
Experimental Evaluation
We assess the performance of OptiMathSAT (v. 1.6.2) on a set of OMT(FP)
formulas
that
have
been
automatically
generated
using
the
SMT(FP)
benchmark-set of [3]. The formulas, the results and the scripts necessary to repro-
duce these results are made publicly available and can be downloaded from [1].
Experiment Setup.
This experiment has been performed on an i7-6500U
2.50 GHz Intel Quad-Core machine with 16 GB of ram and running Ubuntu
Linux 17.10. For each formula being tested we used a timeout of 600 s. The
OMT(FP) instances used in this experiment have been automatically generated
starting from the satisﬁable formulas included in the SMT(FP) benchmark-set
of [3]. We did not consider any of the unsatisﬁable instances that are present in
the remote repository.
We
consider
two
OMT-based
baseline
conﬁgurations,
OptiMath-
SAT(omt+lin) and OptiMathSAT(omt+bin), that run the linear- and the
binary-search respectively. These conﬁgurations have been tested using both the
eager and the lazy FP approaches. The third baseline approach, named Opti-
MathSAT(eager+obv-bs), is based on a reduction of the OMT(FP) prob-
lem to OMT(BV) and it uses OptiMathSAT’s implementation of the obv-bs
engine3 presented by Nadel et al. in [21]. For this test, we have generated an
OMT(BV) benchmark-set using a BV encoding that mimics the essential aspects
of the ofp-bs algorithm described Sect. 4.2.
We
compared
these
baseline
approaches
with
a
conﬁguration
using
the ofp-bs algorithm and the eager FP approach, namely OptiMath-
SAT(eager+ofp-bs).
We have separately tested the eﬀect of enabling the branching preference
(bp), the polarity initialization (pi) and the safe bits restriction (so) enhance-
ments described in Sect. 3.2, whenever these options were supported by the given
conﬁguration.
We have not included other tools in our experiment because we are not
aware of any other OMT(FP) solver. For all problem instances, we veriﬁed the
correctness of the optimal solution found by each conﬁguration with an SMT
3 The binaries of the original OMT(BV) tools presented in [21] are not publicly avail-
able.

564
P. Trentin and R. Sebastiani
solver (MathSAT5). When terminating, all tools returned the same optimum
value.
Table 1. Comparison among various OptiMathSAT conﬁgurations on the OMT(FP)
benchmark-set. The columns list the total number of instances (inst.), the number
of instances solved (term.), the number of timeouts (t.o.), the number of instances
uniquely solved by the given conﬁguration (u), the number of instances solved faster
than any other conﬁguration (bt), the total number of instances solved in the shortest
amount of time (st) and the total solving time for all solved instances (time).
Tool, conﬁguration & encoding
inst. term.
t.o.
u
bt
st
time (s.)
OptiMathSAT(eager+omt+lin)
1120
1003
117
0
5
73
76375
OptiMathSAT(eager+omt+lin+pi)
1120
1003
117
0
5
71
76785
OptiMathSAT(eager+omt+lin+bp)
1120
956
164
0
6
105
77480
OptiMathSAT(eager+omt+lin+bp+pi)
1120
873
247
0
77
217
54859
OptiMathSAT(eager+omt+bin)
1120
1014
106
0
11
281
67834
OptiMathSAT(eager+omt+bin+pi)
1120
970
150
0
8
285
69765
OptiMathSAT(eager+omt+bin+bp)
1120
1016
104
0
14
205
68255
OptiMathSAT(eager+omt+bin+bp+pi)
1120
991
129
0
65
321 56941
OptiMathSAT(lazy+omt+lin)
1120
868
252
0
93
203
29832
OptiMathSAT(lazy+omt+bin)
1120
900
220
0
90
243
33260
OptiMathSAT(eager+obvbs) [reduction] 1120
1013
107
0
14
141
65954
OptiMathSAT(eager+ofpbs)
1120
1017
103
0
9
171
70732
OptiMathSAT(eager+ofpbs+pi)
1120 1019
101
0
34
280
64896
OptiMathSAT(eager+ofpbs+pi+so)
1120
1018
102
0
7
179
71430
OptiMathSAT(eager+ofpbs+bp)
1120
975
145
0
2
145
65543
OptiMathSAT(eager+ofpbs+bp+so)
1120
1000
120
0
3
124
68390
OptiMathSAT(eager+ofpbs+bp+pi)
1120
1001
119
0
77
273
60365
OptiMathSAT(eager+ofpbs+bp+pi+so)
1120
1006
114
19
32
245
59463
virtual best
1120 1074
46
-
559
1074
27788
Experiment Results. The results of this experiment are listed in Table 1.
For what concerns OMT-based linear-search optimization, we observe that
OptiMathSAT performs the best when no enhancement is enabled. In partic-
ular, the empirical evidence suggests that enabling branching preference signiﬁ-
cantly increases the number of timeouts, generally deteriorating the performance.
Enabling only polarity initialization does not result in an appreciable change on
the running time of the solver. In contrast, enabling both enhancements at the
same time generally worsens the performance and results in a drastic increase
in the number of timeouts (Table 1). We justify these results as follows. First,
when only polarity initialization is used, the phase-saving value that is being set
by OptiMathSAT does not really matter because the optimization search is

Optimization Modulo the Theory of Floating-Point Numbers
565
dominated by the structure of the formula itself rather than by the bits of the
FP objective. Second, when polarity initialization is used on top of branching
preference, there is an even more drastic decrease in performance due to the fact
that the initial phase-saving value that is statically assigned by the OMT solver
to the bits of the FP objective cannot be expected to be “good enough” for any
situation.
In the case of the OMT-based binary-search optimization approach, we
observe that it solves more formulas than linear-search and it generally appears
to be faster. Overall, polarity initialization does not seem to be beneﬁcial,
whereas enabling branching preference increases the number of formulas solved
within the timeout. This behavior is diﬀerent from the linear-search approach,
and we conjecture that it is due to the fact that, with the OMT-based binary-
search approach, branching over the bits of the objective function can reveal in
advance any (partial) assignment to the bits of the objective function that it is
inconsistent w.r.t. the pivoting cuts learned by the optimization engine.
Using the lazy FP engine results in fewer formulas being solved, although
a signiﬁcant number of these benchmarks is solved faster than with any other
conﬁguration.
The OptiMathSAT(eager+obv-bs) conﬁguration is able to solve 1013
formulas within the timeout, showing that OMT(FP) can be reduced to
OMT(BV) eﬀectively, and that –on the given benchmark-set– the performance
of this approach are comparable with the best OMT(FP) conﬁgurations being
tested.
Overall, the best performance is obtained by using the ofp-bs engine, with
up to 1019 benchmark-set instances being solved in correspondence to the Opti-
MathSAT(eager+ofp-bs+pi) conﬁguration. Similarly to the case of OMT-
based optimization with linear-search, we observe that enabling branching pref-
erence generally makes the performance worse. Instead, when polarity initializa-
tion is used we observe a general performance improvement that does not only
result in an increase in the number of formulas being solved within the timeout,
but also a noticeable reduction of the solving time as a whole. This is in con-
trast with the case of OMT-based optimization, and it can be explained by the
fact that ofp-bs uses an internal heuristic function to dynamically determine
and update the most appropriate phase-saving value for the bits of the objec-
tive function. An equally important role is played by the safe bits restriction,
that limits the eﬀects of branching preference and polarity initialization to only
certain bits of the dynamic attractor. This feature is particularly eﬀective when
used in combination with branching preference.
6
Conclusions and Future Work
We have presented for the ﬁrst time OMT procedures (for signed Bit-Vectors
and) Floating-Point numbers, based on the novel notions of attractor, dynamic
attractor and attractor trajectory, which we have implemented in OptiMath-
SAT and tested on modiﬁed problems from SMT-LIB.

566
P. Trentin and R. Sebastiani
Ongoing research involves implementing our ofp-bs procedure on top of the
ACDCL SMT(FP) procedure—which is not immediate to do eﬃciently because
the latter approach does not allow directly accessing and setting the single bits
of the objective (since BV and FP are not signature-disjoint). Future research
involves experimenting the new OMT procedure directly on problems coming
from bit-precise SW and HW veriﬁcation, produced, e.g., by the NuXmv model
checker [2].
References
1. http://disi.unitn.it/trentin/resources/ﬂoatingpoint test.tar.gz
2. nuXmv. https://nuxmv.fbk.eu
3. SmtLibv2. www.smtlib.cs.uiowa.edu/
4. IEEE standard 754 (2008). http://grouper.ieee.org/groups/754/
5. Bjorner, N., Phan, A.-D.: νZ - maximal satisfaction with Z3. In: Proceedings
of the International Symposium on Symbolic Computation in Software Science,
Gammarth, Tunisia, December 2014. EasyChair Proceedings in Computing (EPiC)
(2014)
6. Bjørner, N., Phan, A.-D., Fleckenstein, L.: νZ - an optimizing SMT solver. In:
Baier, C., Tinelli, C. (eds.) TACAS 2015. LNCS, vol. 9035, pp. 194–199. Springer,
Heidelberg (2015). https://doi.org/10.1007/978-3-662-46681-0 14
7. Bozzano, M., et al.: Encoding RTL constructs for MathSAT: a preliminary report.
In: Proceedings of the 3rd Workshop of Pragmatics on Decision Procedure in Auto-
mated Reasoning, PDPAR 2005, ENTCS. Elsevier (2005)
8. Brain, M., D’Silva, V., Griggio, A., Haller, L., Kroening, D.: Interpolation-based
veriﬁcation of ﬂoating-point programs with abstract CDCL. In: Logozzo, F.,
F¨ahndrich, M. (eds.) SAS 2013. LNCS, vol. 7935, pp. 412–432. Springer, Heidelberg
(2013). https://doi.org/10.1007/978-3-642-38856-9 22
9. Brain, M., D’Silva, V., Griggio, A., Haller, L., Kroening, D.: Deciding ﬂoating-
point logic with abstract conﬂict driven clause learning. Formal Methods Syst.
Des. 45(2), 213–245 (2014)
10. Brain, M., Tinelli, C., R¨ummer, P., Wahl, T.: An automatable formal semantics
for IEEE-754 ﬂoating-point arithmetic. In: ARITH, pp. 160–167. IEEE (2015)
11. Brillout, A., Kroening, D., Wahl, T.: Mixed abstractions for ﬂoating-point arith-
metic. In: 2009 Formal Methods in Computer-Aided Design, pp. 69–76, November
2009
12. Brinkmann, R., Drechsler, R.: RTL-datapath veriﬁcation using integer linear pro-
gramming. In: Proceedings of the ASP-DAC 2002, pp. 741–746. IEEE (2002)
13. Brummayer, R., Biere, A.: Boolector: an eﬃcient SMT solver for bit-vectors and
arrays. In: Kowalewski, S., Philippou, A. (eds.) TACAS 2009. LNCS, vol. 5505, pp.
174–177. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-642-00768-
2 16
14. Bruttomesso, R., et al.: A lazy and layered SMT(BV) solver for hard industrial
veriﬁcation problems. In: Damm, W., Hermanns, H. (eds.) CAV 2007. LNCS, vol.
4590, pp. 547–560. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-
540-73368-3 54
15. Cimatti, A., Franz´en, A., Griggio, A., Sebastiani, R., Stenico, C.: Satisﬁability mod-
ulo the theory of costs: foundations and applications. In: Esparza, J., Majumdar,
R. (eds.) TACAS 2010. LNCS, vol. 6015, pp. 99–113. Springer, Heidelberg (2010).
https://doi.org/10.1007/978-3-642-12002-2 8

Optimization Modulo the Theory of Floating-Point Numbers
567
16. Fazekas, K., Bacchus, F., Biere, A.: Implicit hitting set algorithms for maximum
satisﬁability modulo theories. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.)
IJCAR 2018. LNCS, vol. 10900, pp. 134–151. Springer, Cham (2018). https://doi.
org/10.1007/978-3-319-94205-6 10
17. Ganesh, V., Dill, D.L.: A decision procedure for bit-vectors and arrays. In: Damm,
W., Hermanns, H. (eds.) CAV 2007. LNCS, vol. 4590, pp. 519–531. Springer,
Heidelberg (2007). https://doi.org/10.1007/978-3-540-73368-3 52
18. Hadarean, L., Bansal, K., Jovanovi´c, D., Barrett, C., Tinelli, C.: A tale of two
solvers: eager and lazy approaches to bit-vectors. In: Biere, A., Bloem, R. (eds.)
CAV 2014. LNCS, vol. 8559, pp. 680–695. Springer, Cham (2014). https://doi.org/
10.1007/978-3-319-08867-9 45
19. Larraz, D., Oliveras, A., Rodr´ıguez-Carbonell, E., Rubio, A.: Minimal-model-
guided approaches to solving polynomial constraints and extensions. In: Sinz, C.,
Egly, U. (eds.) SAT 2014. LNCS, vol. 8561, pp. 333–350. Springer, Cham (2014).
https://doi.org/10.1007/978-3-319-09284-3 25
20. Li, Y., Albarghouthi, A., Kincad, Z., Gurﬁnkel, A., Chechik, M.: Symbolic opti-
mization with SMT solvers. In: POPL (2014)
21. Nadel, A., Ryvchin, V.: Bit-vector optimization. In: Chechik, M., Raskin, J.-F.
(eds.) TACAS 2016. LNCS, vol. 9636, pp. 851–867. Springer, Heidelberg (2016).
https://doi.org/10.1007/978-3-662-49674-9 53
22. Niemetz, A., Preiner, M., Fr¨ohlich, A., Biere, A.: Improving local search for bit-
vector logics in SMT with path propagation. In: Proceedings of the 4th Inter-
national Workshop on Design and Implementation of Formal Tools and Systems
(DIFTS 2015), p. 10 (2015)
23. Nieuwenhuis, R., Oliveras, A.: On SAT modulo theories and optimization problems.
In: Biere, A., Gomes, C.P. (eds.) SAT 2006. LNCS, vol. 4121, pp. 156–169. Springer,
Heidelberg (2006). https://doi.org/10.1007/11814948 18
24. Ruemmer, P., Wahl, T.: An SMT-LIB theory of binary ﬂoating-point arith-
metic. In: SMT 2010 Workshop, July 2010. http://www.philipp.ruemmer.org/
publications/smt-fpa.pdf
25. Sebastiani, R., Tomasi, S.: Optimization modulo theories with linear rational costs.
ACM Trans. Comput. Log. 16(2), 12 (2015)
26. Sebastiani, R., Trentin, P.: Pushing the envelope of optimization modulo theories
with linear-arithmetic cost functions. In: Baier, C., Tinelli, C. (eds.) TACAS 2015.
LNCS, vol. 9035, pp. 335–349. Springer, Heidelberg (2015). https://doi.org/10.
1007/978-3-662-46681-0 27
27. Sebastiani, R., Trentin, P.: OptiMathSAT: a tool for optimization modulo theories.
J. Autom. Reason. (2018)
28. Trentin, P., Sebastiani, R.: Optimization modulo the theories of signed bit-vectors
and ﬂoating-point numbers. arXiv e-prints arXiv:1905.02838, May 2019
29. Zelji´c, A., Backeman, P., Wintersteiger, C.M., R¨ummer, P.: Exploring approxi-
mations for ﬂoating-point arithmetic using UppSAT. In: Galmiche, D., Schulz, S.,
Sebastiani, R. (eds.) IJCAR 2018. LNCS, vol. 10900, pp. 246–262. Springer, Cham
(2018). https://doi.org/10.1007/978-3-319-94205-6 17
30. Zelji´c, A., Wintersteiger, C.M., R¨ummer, P.: Approximations for model construc-
tion. In: Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS, vol.
8562, pp. 344–359. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
08587-6 26

FAME(Q): An Automated Tool
for Forgetting in Description Logics
with Qualiﬁed Number Restrictions
Yizheng Zhao1,2,3(B) and Renate A. Schmidt3
1 National Key Laboratory for Novel Software Technology, Nanjing University,
Nanjing, China
yizheng.zhao1@gmail.com
2 School of Artiﬁcial Intelligence, Nanjing University, Nanjing, China
3 School of Computer Science, The University of Manchester, Manchester, UK
Abstract. In this paper, we describe FAME(Q), a Java-based implemen-
tation of a forgetting method developed for eliminating concept and role
names from ALCOQH-ontologies. FAME(Q) is presently the only tool
for concept forgetting in description logics with qualiﬁed number restric-
tions and nominals, and the only tool for role forgetting in description
logics with qualiﬁed number restrictions. FAME(Q) can be used as a
stand-alone tool or a Java library for forgetting, or related tasks. An
evaluation of FAME(Q) on a large corpus of biomedical ontologies shows
that the tool is able to compute forgetting solutions in 90% of the test
cases; in most cases, the solutions are computed within a few seconds.
1
Introduction
Forgetting is an ontology re-engineering technique that seeks to produce new
ontologies from existing ones using only a subset of their signature while pre-
serving all logical consequences up to the names in the subset. This is done by
eliminating from an ontology a set of concept and role names (the forgetting
signature) in such a way that all logical consequences are preserved up to the
names in the remaining signature. The ontology produced by forgetting (the
forgetting solution), can be seen as a view of the original ontology. In traditional
databases, a view is a subset of a database, whereas in ontologies, a view is
more than a subset; it may contain not only axioms contained in the original
ontology, but also contains those entailed by the ontology (implicitly contained
in the ontology). Forgetting is potentially useful for many ontology processing
tasks such as ontology reuse, alignment, versioning, merging, debugging, repair,
and logical diﬀerence computation [1,3,5,6,10,12–15]. Forgetting is also useful
for other tasks such as information hiding and explanation generation [2,5].
At present, practical methods for forgetting in description logics with qual-
iﬁed number restrictions are the resolution-based approach of the Lethe sys-
tem [7–9] and the one developed by [17,19]. FAME(Q) is a Java implementation
of the latter, which computes uniform interpolants for ALCOQH-ontologies. The
c
⃝Springer Nature Switzerland AG 2019
P. Fontaine (Ed.): CADE 2019, LNAI 11716, pp. 568–579, 2019.
https://doi.org/10.1007/978-3-030-29436-6_34

FAME(Q): An Automated Tool for Forgetting in Description Logics
569
method is a hybrid approach that makes use of both resolution and Ackermann’s
Lemma. It is so far the only approach able to forget concept and role names in
description logics with qualiﬁed number restrictions.
In this paper, we describe the forgetting method used by FAME(Q), the
implementation of FAME(Q), and details of an evaluation of FAME(Q) on a
large corpus of publicly accessible biomedical ontologies. The current version of
FAME(Q) can be downloaded via http://www.cs.man.ac.uk/∼schmidt/sf-fame/.
2
Forgetting for ALCOQH-Ontologies
Let NC, NR and NI be (countably inﬁnite and pairwise disjoint) sets of concept
names, role names and individual names (nominals), respectively. Concepts in
ALCOQH have one of the following forms:
⊤| ⊥| a | A | ¬C | C ⊓D | C ⊔D | ≥mr.C | ≤nr.C,
where a ∈NI, A ∈NC, r ∈NR, C and D denote arbitrary concepts, and m ≥1
and n ≥0 are natural numbers. Further concepts are deﬁned as abbreviations:
∃r.C = ≥1r.C, ∀r.C = ≤0r.¬C, ¬≥mr.C = ≤nr.C and ¬≤nr.C = ≥mr.C,
where n = m −1. Concepts of the form ≥mr.C and ≤nr.C are referred to as
(qualiﬁed) number restrictions.
An ALCOQH-ontology is comprised of a TBox, an RBox and an ABox. A
TBox is a ﬁnite set of axioms of the form C ⊑D (concept inclusions), where C
and D are concepts. An RBox is a ﬁnite set of axioms of the form r ⊑s (role
inclusions), where r, s ∈NR. An ABox is a ﬁnite set of axioms of the form C(a)
(concept assertions) and r(a, b) (role assertions), where a, b ∈NI, r ∈NR, and
C is a concept.
Forgetting can be deﬁned in two closely related ways. In particular, it can be
deﬁned as the dual of uniform interpolation or model-theoretically as semantic
forgetting [4,6,17]. The two notions diﬀer in the sense that uniform interpolation
preserves all logical consequences whereas semantic forgetting preserves semantic
equivalence up to certain names. The results of semantic forgetting (the seman-
tic solutions), are in general stronger than those of uniform interpolation (the
uniform interpolants). This means that semantic solutions always entail uniform
interpolants, but the converse does not hold. Uniform interpolants are always
expressible in the source logic, while semantic solutions are often not, and may
require an extended target language to express them.
By sigC(X) and sigR(X) we denote respectively the sets of the concept names
and role names that occur in X, where X ranges over concepts, roles, clauses,
axioms, sets of clauses and sets of axioms (ontologies). By sig(X) we denote the
union of sigC(X) and sigR(X).
Deﬁnition 1 (Uniform
Interpolation
for
ALCOQH).
Let
O
be
an
ALCOQH-ontology and let F ⊆sigC(O) be a set of concept and role names.
An ontology V is an ALCOQH -uniform interpolant of O for sig(O)\F iﬀthe
following conditions hold: (i) sig(V) ⊆sig(O)\F, and (ii) for any axiom α with

570
Y. Zhao and R. A. Schmidt
sig(α) ⊆sig(O)\F, V |= α iﬀO |= α. In this case, sig(O)\F is called the
interpolation signature, i.e., the set of concept and role names to be preserved.
Deﬁnition 1 says that uniform interpolants have the same logical consequences
with the original ontologies up to the interpolation signature.
Deﬁnition 2 below says that semantic solutions preserve equivalence up to
the interpretations of the names in the forgetting signature F. We say that I
and I′ are equivalent up to a set F of concept and role names, or F-equivalent,
if I and I′ coincide but diﬀer possibly in the interpretations of the names in F.
Deﬁnition 2 (Semantic Forgetting for ALCOQH). Let O be an ALCOQH-
ontology and let F ⊆sig(O) be a set of concept and role names. An ontology V
is a semantic solution of forgetting F from O iﬀthe following conditions hold:
(i) sig(V) ⊆sig(O)\F and (ii) for any interpretation I: I |= V iﬀI′ |= O, for
some interpretation I′ F-equivalent to I. F is called the forgetting signature,
i.e., the set of concept and role names to be forgotten.
3
The Forgetting Method
Next, we brieﬂy describe the forgetting method implemented in FAME(Q). The
method is mainly based on two calculi: a calculus for concept name elimination
and a calculus for role name elimination. The former was presented in our recent
work [19] and the latter in [17]. The method is terminating and sound.
Both calculi operate on ALCOQH-ontologies in clausal normal form, which
are obtained from axioms using the standard transformations based on logical
equivalence such as ¬¬ ≥mr.C = ≥mr.C. In the following, we always use the
notation N to denote a set of clauses (clausiﬁed from an ALCOQH-ontology).
Deﬁnition 3 (Clausal Normal Form). A TBox literal in ALCOQH is a
concept of the form a, ¬a, A, ¬A, ≥mr.C or ≤nr.C, where a ∈NI, r ∈NR,
C is a concept, and m > 1 and n > 0 are natural numbers. A TBox clause in
ALCOQH is a disjunction of a ﬁnite number of TBox literals. An RBox clause
in ALCOQH is a disjunction of a role name and a negated role name. A clause
is called an S-clause if it contains S, for any concept/role name S in NC ∪NR.
Our method is a rounds-based method, where forgetting solutions (uniform
interpolants and semantic solutions) are computed by iteratively eliminating
the (concept and role) names in F. We call the name under consideration for
forgetting in the current round the pivot.
The calculus for eliminating a concept name from a set N of clauses includes
two purify rules and one combination rule.1 The purify rules are applied when the
pivot occurs only positively or only negatively in N, i.e., the pivot is pure in N.
The purify rules say that if the pivot occurs only positively (negatively) in N, it
is eliminated by substitution with the top (bottom) concept. The combination
rule is applied when the pivot occurs both positively and negatively in N, i.e.,
1 In [19], the combination rule is named the Ackermann rule.

FAME(Q): An Automated Tool for Forgetting in Description Logics
571
the pivot is impure in N. It is applicable iﬀN is in a specialized normal form
called A-reduced form, if A is the concept pivot.
Deﬁnition 4 (A-Reduced Form). Let N be a set of clauses. Let A ∈sigC(N).
A clause is in A-reduced form if it has the form C ⊔A, C ⊔¬A, C ⊔≥mr.A,
C ⊔≥mr.¬A, C ⊔≤nr.A or C ⊔≤nr.¬A, where r ∈NR, C is a clause that does
not contain A, and m ≥1 and n ≥0 are natural numbers. A set N of clauses
is in A-reduced form if all A-clauses in N are in A-reduced form.
A-clauses not in A-reduced form can be transformed into A-reduced form by
introducing deﬁner names (or deﬁners for short). Once N is in A-reduced form,
one can immediately apply the combination rule to N to eliminate A. For space
reasons we do not present and describe the combination rule in this paper, but
refer the reader to [19] for a comprehensive description of the rule.
The calculus for eliminating a role name from N includes two purify rules
and ﬁve combination rules.2 The purify rules are applied when the pivot is pure
in N. The combination rules are applied when the pivot is impure in N. They
are applicable iﬀN is in r-reduced form, where r is the pivot.
Deﬁnition 5 (r -Reduced Form). Let N be a set of clauses. Let r ∈sigR(N).
A TBox clause is in r-reduced form if it has the form C ⊔≥mr.D or C ⊔≤nr.D,
where C and D are concepts that do not contain r, and m ≥1 and n ≥0 are
natural numbers. An RBox clause is in r-reduced form if it has the form ¬s ⊔r
or s ⊔¬r, where s ∈NR and s ̸= r. A set N of clauses is in r-reduced form if
all r-clauses in N are in r-reduced form.
r-clauses not in r-reduced form can be transformed into r-reduced form by
introducing deﬁners as in concept forgetting. Once N is in r-reduced form, we
apply an appropriate combination rule to N to eliminate r. We refer the reader
to [19] for presentation and a comprehensive description of the rules.
In order to be able to express more semantic solutions of concept forgetting,
the target language is ALCOQH extended with the top role, role negation, role
conjunction and role disjunction.
4
The Implementation
FAME(Q) is a Java-based implementation of the forgetting method described in
the previous section. In this section we describe the implementation in detail, and
discuss some of its notable features. For users’ convenience, FAME(Q) provides
a graphic user interface, shown in Fig. 1. Fame 1.0 [18] is a preceding system
for forgetting in description logics without number restrictions, but since the
inference rules used by FAME(Q) are diﬀerent from those in Fame 1.0, FAME(Q)
is not simply an improvement of Fame 1.0, but is a novel system.
2 In [17], the purify rules are named Ackermann rules I and II, and the combination
rules are named the Ackermann rules III, IV and V.

572
Y. Zhao and R. A. Schmidt
Fig. 1. Graphic user interface of FAME(Q)
FAME(Q) has a modular design consisting of six main modules: Load Ontol-
ogy, Parse into Own Data Structure, Role Forgetting, Concept Forgetting, Unparse
into OWL Data Structure, and Save Ontology, which are linked as depicted in
Fig. 2. Each successively undertakes a particular task. FAME(Q) uses the OWL
API Version 3.5.63 for the tasks of loading, parsing, unparsing and saving ontolo-
gies. The ontology to be loaded must be an Owl/Xml ﬁle, or a URL pointing to
an Owl/Xml ﬁle. Internally (during the forgetting process), FAME(Q) uses own
data structures to store and manipulate data so it can be processed eﬃciently.
4.1
Forgetting Process
Central to FAME(Q) are the Role Forgetting process and Concept Forgetting pro-
cess, in which the role names and concept names in F are eliminated. FAME(Q)
eliminates role names and concept names in a focused manner, that is, it per-
forms role forgetting and concept forgetting separately. Although FAME(Q) can
eliminate role and concept names in any speciﬁed order, it defaults to eliminat-
ing role names ﬁrst. This is because during the concept forgetting process, role
negation and role disjunction may be introduced, and the calculus for role name
elimination does not support these two role constructs.
3 http://owlcs.github.io/owlapi/.

FAME(Q): An Automated Tool for Forgetting in Description Logics
573
Parse into own
data structure
Role forgetting
Concept forgetting
Parse into
Owl/Xml ﬁle
Load ontology
Save ontology
Fig. 2. Top-level design of FAME(Q)
The role forgetting process is an iteration of several rounds in each of which
a role name in the forgetting signature F is eliminated using the calculus for
role name elimination. The concept forgetting process has two phases executed
in sequence. In the ﬁrst phase concept names in F are eliminated using only the
purify rules. These iterations are intended to eliminate those concept names that
are pure in N. This is because puriﬁcation does not require the ontology to be
normalized or in reduced form, and thus is relatively cheap. Another reason is
that puriﬁcation introduces the top concept into clauses which are immediately
simpliﬁed or eliminated; this makes subsequent forgetting less challenging. The
second phase contains several rounds in each of which a concept name in the
forgetting signature F is eliminated using not only the combination rule, but also
the purify rules. This guarantees that all concept names in F are considered for
elimination from F.
Once a name has been eliminated from the clause set N, it is removed from
the forgetting signature F. A name that cannot be eliminated in the current
round may become eliminable after the elimination of another name [16]. The
elimination rounds are therefore implemented in a do-while loop. The break
condition checks if there were names eliminated in the previous rounds. If so,
FAME(Q) repeats the iterations, attempting to eliminate the remaining names.
The loop terminates when F becomes empty or no names were eliminated in the
previous rounds.
Introduced deﬁners are eliminated as part of the concept forgetting process
using the calculus for concept name elimination. Unlike regular concept names,
there is no guarantee that all deﬁners can be eliminated. If the original ontology
contains cyclic dependencies over the names in the forgetting signature F, it may
not be possible to eliminate all deﬁners, see [17,19] for examples. This means
the forgetting method is incomplete.
Our method can eliminate any concept and role names, though this is at
the cost that the deﬁners introduced may not be all eliminated. If the ontology
computed by FAME(Q) does not contain any deﬁners, we say that FAME(Q)/the
forgetting is successful. In the successful cases, the forgetting solution is a uni-

574
Y. Zhao and R. A. Schmidt
form interpolant or a semantic solution.4 If it is a uniform interpolant, it can be
saved as an Owl/Xml ﬁle. If it is a semantic solution, generally it cannot be
saved as an Owl/Xml ﬁle, because of extra expressivity such as role negation/-
conjunction/disjunction being not supported by the OWL API. In these cases,
the forgetting solutions are represented in the data structure of FAME(Q).
4.2
Frequency Count
A frequency counter is used in FAME(Q) to check the existence of each name
of F in N and in each clause of N, and count the frequency of positive and
negative occurrences of the name in N and in each clause of N. Algorithm 1
below computes the frequency counts of positive occurrences of a concept name,
where AtomicConcept denotes a concept name, GreaterThan and LessThan denote
the ternary number restriction operators ≥and ≤, respectively, and Conjunction
and Disjunction denote the n-nary operators of ⊓and ⊔, respectively. The ﬁrst
operand of GreaterThan and LessThan is a positive integer and a non-negative
integer, respectively, the second operand is a role name, and the third operand is
a concept. The operands of Conjunction and Disjunction are concepts. Operands
are stored in a list, an ordered collection of objects allowing duplicate values.
We used a list, not a set, because the insertion order is preserved in a list, and
allows positional access and insertion of elements. The algorithms (for counting
negative frequency of a concept name and for counting positive frequency and
negative frequency of a role name) were implemented similarly.
4.3
Deﬁner Reuse
FAME(Q) reuses deﬁners whenever possible. For example, consider the case of
forgetting A ∈sigC(N) from N, when a concept has been replaced by a speciﬁc
deﬁner in an A-clause, it is replaced uniformly by the deﬁner in all A-clauses.
We do not introduce new deﬁner names for the same concept in other A-clauses.
On the other hand, if a concept C has been replaced by a deﬁner D, the con-
cept of ¬C is replaced (if necessary) by ¬D, rather than a fresh deﬁner, that
is, FAME(Q) introduces deﬁners in a conservative manner (as few as possible).
This signiﬁcantly improves the eﬃciency of FAME(Q). Deﬁners and the concepts
replaced by them (the corresponding concepts) are stored as keys and values
respectively in a Java HashMap, which allows for easy insertion and retrieval of
paired elements.
5
The Evaluation
In order to understand the practicality and usefulness of FAME(Q), we evaluated
the current version on a corpus of ontologies taken from the NCBO BioPortal
repository,5 a resource currently including more than 600 ontologies originally
4 Because in some cases a uniform interpolant and a semantic solution coincide, when
we say a forgetting solution is a semantic solution, we means it is only a semantic
solution but not a uniform interpolant.
5 https://bioportal.bioontology.org/.

FAME(Q): An Automated Tool for Forgetting in Description Logics
575
Algorithm 1. positive(A, cls)
Input
: a concept name A
a clause cls
Output: an integer i
1 if cls instance of AtomicConcept then
2
if cls equals to A then
3
return 1;
4
else
5
return 0;
6 else if cls instance of Negation then
7
Clause operand = cls.getOperands().get(0);
8
return negative(A, operand);
9 else if cls instance of GreaterThan or LessThan then
10
Clause operand = cls.getOperands().get(1);
11
return positive(A, operand);
12 else if cls instance of Conjunction or Disjunction then
13
initialize Integer sum to 0;
14
List<Clause> operand list = cls.getOperands();
15
foreach clause operand in operand list do
16
sum = sum + positive(A, operand);
17
end
18
return sum;
19 else
20
return 0;
developed for clinical research. The corpus was based on a snapshot of the repos-
itory taken in March 2017 [11], containing 396 OWL API compatible ontologies.
Statistical information about these ontologies can be found in [18].
Table 1 lists the types of axioms handled by FAME(Q). All these can
be encoded as SubClassOf axioms. Axioms not expressible in ALCOQH were
removed from each ontology as FAME(Q) only accommodated ALCOQH-
ontologies.
To reﬂect real-world application scenarios, we evaluated the performance
of FAME(Q) for forgetting diﬀerent numbers of concept names and role names
from each ontology. We considered the cases of forgetting 10%, 30% and 50%
of concept and role names from the signature of each ontology. Lethe was
the only existing tool for forgetting in description logics with number restric-
tions; it handled ALCQH but only for concept forgetting. We compared the
results of concept forgetting computed by FAME(Q) with those by Lethe on the
ALCQH-fragments. The fragments were obtained similarly as for the ALCOQH-
fragments. In order to allow a fair comparison with Lethe which was evaluated
on randomly chosen forgetting signatures we did the same. The experiments were
run on a desktop with an Intel® Coretm i7-4790 processor, four cores running
at up to 3.60 GHz, and 8 GB of DDR3-1600 MHz RAM. The experiments were

576
Y. Zhao and R. A. Schmidt
Table 1. Types of axioms that can be handled by FAME(Q)
Type of axiom
Representation
TBox SubClassOf(C1 C2)
SubClassOf(C1 C2)
EquivalentClasses(C1 C2)
SubClassOf(C1 C2), SubClassOf(C2 C1)
DisjointClasses(C1 C2)
SubClassOf(C1 ObjectComplementOf(C2))
DisjointUnion(C C1. . . Cn)
EquivalentClasses(C ObjectUnionOf(C1. . . Cn))
DisjointClasses(C1. . . Cn)
SubObjectPropertyOf(R1 R2)
SubObjectPropertyOf(R1 R2)
EquivalentObjectProperties(R1 R2)
SubObjectPropertyOf(R1 R2)
SubObjectPropertyOf(R2 R1)
ObjectPropertyDomain(R C)
SubClassOf(ObjectSomeValuesFrom(R owl:Thing), C)
ObjectPropertyRange(R C)
SubClassOf(owl:Thing ObjectAllValuesFrom(R C))
ABox ClassAssertion(C a)
SubClassOf(a C)
ObjectPropertyAssertion(R a1 a2)
SubClassOf(a1 ObjectSomeValuesFrom(R a2))
Table 2. Results of concept and role forgetting computed by FAME(Q)
Settings
Results
Forgetting
Forget % Time Timeout Success rate ND left ▽, ¬, ⊓, ⊔
Concept
Forgetting
10%
3.1 s 1.3%
96.2%
2.5%
10.6%
30%
9.0 s 4.0%
89.7%
6.3%
31.6%
50%
14.2 s 7.5%
83.7%
8.8%
53.3%
Avg.
8.8 s 4.3%
89.8%
5.9%
31.8%
Role
Forgetting
10%
4.0 s 1.5%
96.7%
1.8%
18.3%
30%
9.1 s 4.6%
90.1%
5.3%
25.3%
50%
15.2 s 7.8%
82.7%
9.5%
41.9%
Avg.
9.5 s 4.7%
89.8%
5.5%
25.4%
run 100 times on each ontology and we averaged the results in order to verify
the accuracy of our ﬁndings. A timeout of 1000 s was imposed on each run.
The results obtained from forgetting 10%, 30% and 50% of concept names and
role names from the ALCOQH-ontologies are shown in Table 2, where one can
observe that, on average, FAME(Q) was successful in nearly 90% of the test cases
(89.8% for both concept forgetting and role forgetting). In most successful cases,
the forgetting solutions were computed within 10 s (8.8 s for concept forgetting
and 9.5 s for role forgetting). The column headed ND Left shows the percentages
of the test cases where the deﬁners were present in the resulting ontologies.
The column headed ▽, ¬, ⊓, ⊔shows the percentages of the test cases where the
forgetting solutions involved role constructs.
According to the results given in Table 3 FAME(Q) was considerably faster
than Lethe on the ALCQH-fragments; on average, it was 8 times faster. An
important reason is that Lethe introduces deﬁners in a systematic and exhaus-

FAME(Q): An Automated Tool for Forgetting in Description Logics
577
Table 3. Results of concept forgetting computed by FAME(Q) and Lethe
Settings
Results
Tool
Forget% Time Timeout Success rate ND intro ▽, ¬, ⊓, ⊔Fixpoints
FAME(Q)
ALCQH
10%
2.9 s
1.0%
96.2%
16.3%
10.6%
0.0%
30%
7.5 s
3.5%
89.7%
27.2%
31.6%
0.0%
50%
7.4 s
6.7%
83.7%
35.8%
53.3%
0.0%
Avg.
8.1 s
3.4%
89.8%
5.9%
31.8%
0.0%
Lethe
ALCQH
10%
25.2 s
7.4%
92.6%
97.2%
0.0%
11.4%
30%
59.5 s 20.5%
79.5%
100.0%
0.0%
14.9%
50%
91.7 s 35.1%
64.9%
100.0%
0.0%
18.2%
Avg.
58.8 s 21.0%
79.0%
99.1%
0.0%
14.8%
tive manner. The column headed ND Intro shows the percentages of the test
cases where deﬁners were introduced during the forgetting process. It can be
seen that Lethe introduced deﬁners in nearly 100% of the test cases. In addi-
tion, FAME(Q) attained notably better success rates over Lethe (90.5% over
79.0%). Most failures of Lethe were due to the timeout.
Another advantage is that solutions computed by FAME(Q) are in general
stronger than those by Lethe. Often, a stronger solution means a better one. For
example, the solution of forgetting the concept name {Male} from the ontology
{A ⊑≥2hasSon.Male, A ⊑≥3hasDaughter.¬Male,
hasSon ⊑hasChild, hasDaughter ⊑hasChild}
computed by Lethe is
{A ⊑≥2hasSon.⊤, A ⊑≥3hasDaughter.⊤,
hasSon ⊑hasChild, hasDaughter ⊑hasChild},
while the solution of FAME(Q) includes an additional axiom
A ⊑≥5(hasSon ⊔hasDaughter).⊤,
where role disjunction is used. Upon the solution of Lethe, if we further for-
get the role names hasSon and hasDaughter, the uniform interpolant is {A ⊑
≥3hasChild.⊤}, while on the intermediary solution of FAME(Q), the solution is
{A ⊑≥5hasChild.⊤}, which is stronger and closer to the fact: A has at least 5
children. This shows an advantage of semantic forgetting where extra expres-
sivity allows intermediary information (A ⊑≥5(hasSon ⊔hasDaughter).⊤) to be
captured which produces a better solution.
For users such as Snomed CT and NCIt who do not have the ﬂexibility to
easily switch to a more expressive language, or are bound by the application, the
available support and tooling, to a speciﬁc language, FAME(Q) is not satisfac-
tory. Tracking the logical diﬀerence between diﬀerent versions of ontologies is an

578
Y. Zhao and R. A. Schmidt
application where the target language should coincide with the source language.
In these cases, Lethe would be more suited.
6
Conclusions
This paper describes the tool of FAME(Q) for forgetting in ALCOQH-ontologies.
FAME(Q) is at present the only tool able to forget concept and role names in
description logics with number restrictions. Compared to Lethe, a tool that can
perform concept forgetting in ALCQH, FAME(Q) fared better with respect to
success rates and time eﬃciency on ALCQH-fragments of realistic ontologies.
References
1. Bicarregui, J., Dimitrakos, T., Gabbay, D.M., Maibaum, T.S.E.: Interpolation in
practical formal development. Log. J. IGPL 9(2), 231–244 (2001)
2. Del-Pinto, W.M., Schmidt, R.A.: ABox abduction via forgetting in ALC. In: Pro-
ceedings of AAAI 2019. AAAI Press (2019, to appear)
3. Eiter, T., Ianni, G., Schindlauer, R., Tompits, H., Wang, K.: Forgetting in manag-
ing rules and ontologies. In: Web Intelligence, pp. 411–419. IEEE Computer Society
(2006)
4. Gabbay, D.M., Schmidt, R.A., Szalas, A.: Second Order Quantiﬁer Elimination:
Foundations, Computational Aspects and Applications. College Publications, Lon-
don (2008)
5. Grau, B.C., Motik, B.: Reasoning over ontologies with hidden content: the import-
by-query approach. J. Artif. Intell. Res. 45, 197–255 (2012)
6. Konev, B., Walther, D., Wolter, F.: Forgetting and uniform interpolation in large-
scale description logic terminologies. In Proceedings of IJCAI 2009, pp. 830–835.
IJCAI/AAAI Press (2009)
7. Koopmann, P.: Practical uniform interpolation for expressive description logics.
Ph.D. thesis, University of Manchester, UK (2015)
8. Koopmann, P., Schmidt, R.A.: Count and forget: uniform interpolation of SHQ-
ontologies. In: Demri, S., Kapur, D., Weidenbach, C. (eds.) IJCAR 2014. LNCS
(LNAI), vol. 8562, pp. 434–448. Springer, Cham (2014). https://doi.org/10.1007/
978-3-319-08587-6 34
9. Koopmann, P., Schmidt, R.A.: LETHE: saturation-based reasoning for non-
standard reasoning tasks. In: Proceedings of DL 2015. CEUR Workshop Proceed-
ings, vol. 1387, pp. 23–30. CEUR-WS.org (2015)
10. Lang, J., Liberatore, P., Marquis, P.: Propositional independence: formula-variable
independence and forgetting. J. Artif. Intell. Res. 18, 391–443 (2003)
11. Matentzoglu, N., Parsia, B.: BioPortal Snapshot 30.03.2017, March 2017
12. Qi, G., Wang, Y., Haase, Y., Hitzler, P.: A forgetting-based approach for reasoning
with inconsistent distributed ontologies. In: Proceedings of WoMO 2008. CEUR
Workshop Proceedings, vol. 348. CEUR-WS.org (2008)
13. Wang, K., Antoniou, G., Topor, R., Sattar, A.: Merging and aligning ontolo-
gies in dl-programs. In: Adi, A., Stoutenburg, S., Tabet, S. (eds.) RuleML 2005.
LNCS, vol. 3791, pp. 160–171. Springer, Heidelberg (2005). https://doi.org/10.
1007/11580072 13

FAME(Q): An Automated Tool for Forgetting in Description Logics
579
14. Wang, K., Wang, Z., Topor, R.W., Pan, J.Z., Antoniou, G.: Eliminating concepts
and roles from ontologies in expressive descriptive logics. Comput. Intell. 30(2),
205–232 (2014)
15. Zhao, Y., Alghamdi, G., Schmidt, R.A., Feng, H., Stoilos, G., Juric, D., Kho-
dadadi, M.: Tracking logical diﬀerence in large-scale ontologies: a forgetting-based
approach. In: Proceedings of AAAI 2019. AAAI Press (2019)
16. Zhao,
Y.,
Schmidt,
R.A.:
Forgetting
concept
and
role
symbols
in
ALCOIHµ+(, ⊓)-ontologies. In: Proceedings of IJCAI 2016, pp. 1345–1352.
IJCAI/AAAI Press (2016)
17. Zhao, Y., Schmidt, R.A.: Role forgetting for ALCOQH()-ontologies using
an Ackermann-based approach. In: Proceedings of IJCAI 2017, pp. 1354–1361.
IJCAI/AAAI Press (2017)
18. Zhao, Y., Schmidt, R.A.: FAME: an automated tool for semantic forgetting in
expressive description logics. In: Galmiche, D., Schulz, S., Sebastiani, R. (eds.)
IJCAR 2018. LNCS (LNAI), vol. 10900, pp. 19–27. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-94205-6 2
19. Zhao, Y., Schmidt, R.A.: On concept forgetting in description logics with qualiﬁed
number restrictions. In: Proceedings of IJCAI 2018, pp. 1984–1990. IJCAI/AAAI
Press (2018)

Author Index
An, Jie
178
Anantharaman, Siva
1
Andrade de Melo, Alexsander
18
Areces, Carlos
161
Barbosa, Haniel
35
Barrett, Clark
35, 366
Bentkamp, Alexander
55
Bhayat, Ahmed
74
Blanchette, Jasmin
55
Bohrer, Rose
94
Bromberger, Martin
111
Brown, Chad E.
123
Calvanese, Diego
142
Cassano, Valentin
161
Castro, Pablo F.
161
Chen, Mingshuai
178
Chvalovský, Karel
197
Cordwell, Katherine
216
Cruanes, Simon
495
de Oliveira Oliveira, Mateus
18
El Ouraoui, Daniel
35
Fernández, Manuel
94
Fervari, Raul
161
Fiori, Alberto
233
Fleury, Mathias
111
Furbach, Ulrich
250
Gauthier, Thibault
123
Ghilardi, Silvio
142
Gianola, Alessandro
142
Giesl, Jürgen
269
Giesl, Peter
269
Gutiérrez, Raúl
287
Hark, Marcel
269
Havlena, Vojtěch
300
Hibbs, Peter
1
Hirokawa, Nao
319
Hoffmann, Guillaume
161
Holík, Lukáš
300
Jakubův, Jan
197
Kaliszyk, Cezary
123
Kapur, Deepak
178
Kohl, Christina
337
Krämer, Teresa
250
Lengál, Ondřej
300
Li, Di Long
354
Lucas, Salvador
287
Middeldorp, Aart
337
Montali, Marco
142
Nagele, Julian
319
Narendran, Paliath
1
Niemetz, Aina
366
Oyamaguchi, Michio
319
Pelletier, Francis Jeffry
526
Peuter, Dennis
385
Plaisted, David A.
406
Platzer, André
94, 216, 425
Popescu, Andrei
442
Preiner, Mathias
366
Rawson, Michael
462
Reger, Giles
74, 462, 477
Reynolds, Andrew
35, 366
Rivkin, Andrey
142
Rusinowitch, Michael
1
Schmidt, Renate A.
568
Schon, Claudia
250
Schulz, Stephan
495
Schwarz, Simon
111

Sebastiani, Roberto
550
Sofronie-Stokkermans, Viorica
385
Sternagel, Christian
508
Suda, Martin
197
Sutcliffe, Geoff
123, 526
Tammet, Tanel
538
Tinelli, Cesare
35, 366
Tiu, Alwen
354
Tourret, Sophie
55
Traytel, Dmitriy
442
Trentin, Patrick
550
Urban, Josef
123, 197
van Oostrom, Vincent
319
Vojnar, Tomáš
300
Voronkov, Andrei
477
Vukmirović, Petar
55, 495
Waldmann, Uwe
55
Wang, Jian
178
Weidenbach, Christoph
111, 233
Winkler, Sarah
508
Zhan, Bohua
178
Zhan, Naijun
178
Zhao, Yizheng
568
Zohar, Yoni
366
582
Author Index

