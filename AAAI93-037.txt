Agents 
Contracting 
Tasks in Non-Colla 
Environments* 
Sarit Kraus 
Department 
of Mathematics 
and Computer Science 
Bar Ilan University Ramat Gan, 52900 Israel 
sarit@bimacs.cs.biu.ac.il 
Abstract 
Agents 
may sub-contract 
some of their tasks 
to 
other agent(s) 
even when they don’t share a com- 
mon goal. 
An agent tries to contract 
some of its 
tasks that it can’t perform 
by itself, or when the 
task may be performed 
more efficiently 
or better 
by other 
agents. 
A “selfish” agent may convince 
another 
“selfish” 
agent 
to help it with its task, 
even if the agents 
are not assumed 
to be benev- 
olent, 
by promises 
of rewards. 
We propose 
tech- 
niques 
that 
provide 
efficient 
ways to reach sub- 
contracting 
in varied situations: 
the agents have 
full information 
about 
the environment 
and each 
other 
vs. 
subcontracting 
when the agents 
don’t 
know the exact state of the world. We consider sit- 
uations of repeated 
encounters, 
cases of asymmet- 
ric information, 
situations 
where the agents 
lack 
information 
about each other, and cases where an 
agent subcontracts 
a task 
to a group of agents. 
We also consider 
situations 
where there is a com- 
petition 
either 
among 
contracted 
agents 
or con- 
tracting 
agents. 
In all situations 
we would like the 
contracted 
agent to carry out the task efficiently 
without 
the need of close supervision 
by the con- 
tracting 
agent. 
The contracts 
that are reached are 
simple, 
Pareto-optimal 
and stable. 
Introduction 
Research 
in Distributed 
Problem 
Solvers assumes that 
it is in the agents’ 
interest 
to help one another. 
This 
help can be in the form of the sharing of tasks, results, 
or information 
[Durfee, 19921. In task sharing, 
an agent 
with a task it cannot 
achieve on its own will attempt 
to pass the task, in whole or in part, to other agents, 
usually on a contractual 
basis [Davis and Smith, 
19831. 
This approach 
assumes 
that agents not otherwise 
oc- 
cupied will readily take on the task. 
Similarly, 
in infor- 
mation 
or result sharing, 
information 
is shared among 
agents with no expectation 
of a return 
[Lesser, 
1991; 
Conry 
et al., 19901. This benevolence 
is based on the 
*This material 
is based upon work supported 
by the Na- 
tional Science 
Foundation 
under Grant 
No. IRI-9123460. 
I 
would Iike to thank 
Jonathan 
Wilkenfeld 
for his comments. 
assumption 
common 
to many 
approaches 
to coordi- 
nation: 
That 
the goal is for the system 
to solve the 
problem 
as best it can, and therefore 
the agents 
have 
a shared, 
often implicit, 
global goal that 
they are all 
unselfishly 
committed 
to achieving. 
It was observed 
in [Grosz 
and Klaus, 
19931 that 
agents may sub-contract 
some of their tasks to other 
agents 
also in environments 
where the agents 
do not 
have a common 
goal and there is no globally 
consis- 
tent knowledge. ’ That 
is, a selfish agent that tries to 
carry out its own individual 
plan in order to fulfill its 
own tasks may sub-contract 
some of its tasks to an- 
other selfish agent(s). 
An agent tries to contract 
some 
of its tasks that it can’t perform 
by itself, or when the 
task may be performed 
more efficiently 
or better 
by 
other agents. 
The main question 
is how an agent may 
convince another agent to do something 
for it when the 
agents don’t share a global task and the agents are not 
assumed to be benevolent. 
Furthermore, 
we would like 
the contracted 
agent to carry out the task efficiently 
without 
the need of close supervision 
by the contract- 
ing agent. 
This 
will enable 
the contracting 
agent to 
carry out other tasks simultaneously. 
There 
are two main ways to convince 
another 
self- 
ish agent to perform 
a task that is not among its own 
tasks: 
threats 
to interfere 
with the agent carrying 
out 
its own tasks or promises 
of rewards. 
In this paper 
we concentrate 
on subcontcracting 
by rewards. 
Re- 
wards may be in two forms. 
In the first approach 
one 
agent may promise to help the other in its tasks in the 
future in return for current 
help. As was long ago ob- 
served in economics, 
barter 
is not an efficient 
basis for 
cooperation. 
In a multi-agent 
environment, 
an agent 
that wants to subcontract 
a task to another 
agent may 
not be able to help it in the future, 
or one agent that 
may be able to help in another 
agent’s 
task may not 
need help in carrying 
out its own tasks. 
In the sec- 
ond approach 
a monetary 
system 
is developed 
that is 
used for rewards. 
The rewards 
can be used later for 
other purposes. 
We will show that a monetary 
system 
‘Systems 
of agents 
acting 
in environments 
where there 
is no global common 
goal (e.g., 
[Sycara, 
1990; 
Zlotkin 
and 
Rosenschein, 
1991; 
Kraus 
et aal., 1991; 
Ephrati 
and Rosen- 
schein, 
19911) 
are called 
M&i-Agent 
Systems 
[Bond 
and 
Gasser, 
1988; 
Gasser, 
19911. 
243 
From: AAAI-93 Proceedings. Copyright © 1993, AAAI (www.aaai.org). All rights reserved. 

for the multi-agent 
environment 
that 
allows for side 
payments 
and rewards 
between 
the agents 
yields an 
efficient contracting 
mechanism. 
The monetary 
profits 
may be given to the owners of the automated 
agents. 
The agents 
will be built 
to maximize 
expected 
utili- 
ties that increase 
with the monetary 
values, as will be 
explained 
below. 
The issue of contracts 
has been investigated 
in eco- 
nomics and game-theory 
in the last two decades (e.g., 
[Arrow, 
1985; Ross, 
1973; Rasmusen, 
1989; Grossman 
and Hart, 
1983; 
Hirshleifer 
and Riley, 
19921). 
They 
have considered 
situations 
in which a person or a com- 
pany contracts 
a task to another 
person or company. 
In this paper 
we adjust 
the models 
that 
were devel- 
oped in economics 
and game-theory 
to fit distributed 
artificial 
intelligence 
situations. 
We 
will consider 
varied 
situations: 
In the 
Sec- 
tion 
Contracts 
Under 
Certainty 
one 
agent 
subcon 
tracts 
a task to another 
one when the agents have full 
information 
about the environment 
and each other. 
In 
the Section 
Contracts 
Under 
Uncertainty, 
we consider 
contracting 
when the agents don’t know the exact state 
of the world. 
The 
situation 
in which an agent may 
subcontract 
its tasks several 
times to the same agent 
is considered 
in Section 
Repeated 
Encounters, 
and sit- 
uations of asymmetric 
information 
or when the agents 
lack information 
about 
each other is dealt with in the 
Section 
Asymmetric 
and Incomplete 
Information. 
We 
conclude 
with the case of an agent subcontracting 
a 
task to a group of agents. 
In all these cases, we con- 
sider situations 
where the contracting 
agent 
doesn’t 
supervise 
the contracted 
agents’ 
performance 
and sit- 
uations 
where there 
is a competition 
among possible 
contracted 
agents or possible 
contracting 
agents. 
Preliminaries 
We will refer to the agent that subcontracts 
one of its 
tasks 
to another 
agent 
as the 
contracting 
agent and 
to the agent 
that 
agrees to carry out the task as the 
contracted 
agent. The eflort level is the time and work 
intensity 
which the contracted 
agent puts into fulfilling 
the task. 
We denote 
the set of all possible 
efforts by 
E. In all cases, the contracted 
agent chooses how much 
effort to extend, 
but its decision 
may be influenced 
by 
the contract 
offered by the contracting 
agent. 
We as- 
sume that there is a monetary 
value a(e) of performing 
a task which increases 
with the effort involved. 
That 
is, the more time and effort put in by the contracted 
agent, the better 
the outcome. 
The contracting 
agent 
will pay the contracted 
agent a wage w (which can be a 
function 
of q). There 
are several properties 
we require 
from our mechanism 
for subcontracting: 
Simplicity: 
The contract 
should be simple and there 
should be an algorithm 
to compute 
it. 
Pareto-Optimality: 
There is no other contracted 
ar- 
rangement 
that is preferred 
by both sides over the one 
they have reached. 
Stability: 
We would like the results 
to be in equilib- 
rium and that the contracts 
will be reached 
and exe- 
cuted without 
delay. 
Concerning 
the simplicity 
and stability 
issues, there 
are two approaches 
for finding equilibria 
in the type of 
situations 
under consideration 
here [Rasmusen, 
19891. 
One is the straight 
game theory 
approach: 
a search 
for Nash strategies 
or for perfect 
equilibrium 
strate- 
gies. The other is the economist’s 
standard 
approach: 
set up a maximization 
problem 
and solve using cal- 
culus. 
The 
drawback 
of the game 
theory 
approach 
is that 
it is not mechanical. 
Therefore, 
in our pre- 
vious work on negotiation 
under time constraints, 
we 
have identified 
perfect-equilibrium 
strategies 
and pro- 
posed to develop 
a library 
of meta 
strategies 
to be 
used when appropriate 
[Kraus and Wilkenfeld, 
1991a; 
Kraus and Wilkenfeld, 
1991b]. 
The maximization 
ap- 
proach 
is much easier 
to implement. 
The 
problem 
with the maximization 
approach 
in our context 
is that 
players must solve their optimization 
problems jointly: 
the contracted 
agent’s strategy 
affects the contracting 
agent’s maximization 
problem 
and vice versa. 
In this 
paper 
we will use the maximization 
approach, 
with 
some care, by embedding 
the contracted 
agent’s maxi- 
mization 
problem into the contracting 
agent’s problem 
as a constraint. 
This 
maximization 
problem 
can be 
solved automatically 
by the agent. 
The agents’ utility functions 
play an important 
role 
in finding an efficient 
contract. 
As explained 
above, 
we propose to include a monetary 
system 
in the multi- 
agent environment. 
This system 
will provide a way for 
providing 
rewards. 
However, 
it is not always the case 
that the effort of an agent can be assigned 
the same 
monetary 
values. Each designer of an automated 
agent 
needs to provide its agent with a decision 
mechanism 
based on some given set of preferences. 
Numeric repre- 
sentations 
of these preferences 
offer distinct 
advantages 
in compactness 
and analytic 
manipulation 
[Wellman 
and Doyle, 19921. Therefore, 
we propose 
that each de- 
signer of autonomous 
agents 
will develop a numerical 
utility 
function 
that 
it would like its agent 
to maxi- 
mize. 
In our case the utility 
function 
will depend 
on 
monetary 
gain and effort. 
This is especially 
important 
in situations 
where there 
is uncertainty 
in the situa- 
tion and the agents need to make decisions 
under risk 
considerations. 
Decision 
theory 
offers a formalism 
for 
capturing 
risk attitudes. 
If an agent’s 
utility 
function 
is concave, 
it is risk averse. 
If the function 
is convex, 
it is risk prone, and a linear utility function 
yields risk 
neutral 
behavior 
[Hirshleifer 
and Riley, 
19921. 
We denote the contracted 
agent’s utility function 
by 
U which is a decreasing 
function 
in effort 
and an in- 
creasing 
function 
in wage w. 
We assume 
that 
if the 
contracted 
agent won’t accept 
the contract 
from the 
contracting 
agent, its utility, (i.e., its reservation 
price) 
which is known to both agents is 6. This outcome 
can 
result 
either 
from not doing anything 
or performing 
some other 
tasks 
at the same time. 
We denote 
the 
contracting 
agent’s 
utility 
function 
by V and it is an 
244 
Kraus 

increasing 
function 
with the value of performing 
the 
task (Q) and decreasing 
function 
with the wage w paid 
to the contracted 
agent. 
In our system we assume that 
the contracting 
agent rewards the contracted 
agent af- 
ter the task is carried 
out. 
In such situations 
there 
should 
be a technique 
for enforcing 
this reward. 
In 
case of multiple encounters 
reputational 
considerations 
may yield appropriate 
behavior. 
In a single encounter 
some external 
intervention 
may be required 
to enforce 
commitments. 
Contracts 
Under Certainty 
In this case we assume that all the relevant information 
about the environment 
and the situation 
is known to 
both agents. 
In the simplest 
case the contracting 
agent 
can observe and supervise 
the contracted 
agent’s effort 
and actions 
and force it to make the effort level pre- 
ferred 
by the contracting 
agent 
by paying 
it only in 
case it makes the required 
effort. 
The amount 
of ef- 
fort required 
from the contracted 
agent will be the one 
that maximizes 
the contracting 
agent’s 
outcome, 
tak- 
ing into account 
the task fulfillment 
and the payments 
it needs to make to the contracted 
agent. 
However, 
in most 
situations 
it is either 
not possi- 
ble or too costly for the contracting 
agent to supervise 
the contracted 
agent’s 
actions 
and observe 
its level of 
effort. 
In some cases, it may be trying to carry out an- 
other task at the same time, or it can’t reach the site 
of the action (and that is indeed the reason for subcon- 
tracting). 
If the outcome 
is a function of the contracted 
agent’s 
effort 
and if this function 
is known to both 
agents the contracting 
agent can offer the contracted 
agent 
a forcing 
contract 
[Harris 
and 
Raviv, 
1978; 
Rasmusen, 
19891. 
In this contract, 
the contracting 
agent will pay the contracted 
agent only if it provides 
the outcome 
required 
by the contracting 
agent. 
If the 
contracted 
agent accepts 
the contract, 
he will perform 
the task with the effort that the contracting 
agent finds 
to be most 
profitable 
to itself even without 
supervi- 
sion. Note that the outcome 
won’t necessarily 
be with 
the highest 
effort on the part of the contracted 
agent, 
but rather 
the effort 
which provides 
the contracting 
agent with the highest outcome. 
That is, the contract- 
ing agent should 
pick an effort level e* that will gen- 
erates 
the efficient 
output 
level Q*. Since we assume 
that there are several possible 
agents available for con- 
tracting, 
in equilibrium, 
the contract 
must provide the 
contracted 
agent 
with the utility 
L2 
The 
contract- 
ing agent needs to choose 
a wage function 
such that 
w* 
9 w(Q*N = 6 and U(e, w(q)) 
< ti for e # e*. 
We 
demonstrate 
this case in the following example. 
Example 
I: Contracting 
Under 
Certainty 
The US and Germany 
have sent several mobile robots 
independently 
to Mars to collect minerals 
and ground 
is indifferent 
preferred 
by 
samples 
and to conduct 
experiments. 
One of the US 
robots has to dig some minerals 
on Mars far from the 
other 
US robots. 
There 
are several 
German 
robots 
in that area and the US robot would like to subcon- 
tract some 
of its digging. 
The 
US robot approaches 
one of the German 
robots that can dig in three lev- 
els of e$ort 
(e): 
Low, Medium 
and High denoted 
by 
1,2 and 3 respectively. 
The 
US agent can’t supervise 
the German 
robot’s eflort since it wants to carry out 
another 
task simultaneously. 
The value of digging 
is 
q(e) = da. 
The 
US robot’s utility function, 
if a 
contract 
is reached, 
is V(q, w) = q - w and the Ger- 
man robot’s utility function 
in case it accepts the con- 
tract is U(e, 20) = 17 - z - 2e, where w is the payment 
to the German 
robot. If the German 
robot rejects the 
contract, 
it will busy itself with maintenance 
tasks and 
its utility will be 10. 
It is easy to calculate 
that the 
best eJjrort level from 
the US robot’s point 
of view is 
2, in which there will be an outcome 
of m. 
The 
contract that the US robot oglers to the German 
robot 
is 35 if the outcome 
is &66 
and 0 otherwise. 
This 
contract 
will be accepted 
by the German 
robot and its 
effort level will be Medium. 
There 
are two additional 
issues 
of concern. 
The 
first one is how the contracting 
agent 
chooses 
which 
agent to approach. 
In the situation 
of complete 
infor- 
mation 
(we consider 
the incomplete 
information 
case 
in Section 
Asymmetric 
and Incomplete 
Information) 
it 
should compute the expected 
utility for itself from each 
contract 
with each agent and chooses the one with the 
maximal 
expected 
utility. 
Our model is also appropriate 
in the case in which 
there are several contracting 
agents, 
but only one pos- 
sible contracted 
agent. 
In such a case, 
there 
should 
be information 
about 
the utilities 
of the contracting 
agents 
in the event that 
they 
don’t 
sign a contract. 
The contracted 
agent should 
compute 
the level of ef- 
fort that maximizes 
its expected 
utility 
(similar 
to the 
computation 
of the contracting 
agent 
in the reverse 
case) and make an offer to the contracting 
agent that 
will maximize 
its outcome. 
Contracts 
Under Uncertainty 
In most subcontracting 
situations, 
there is uncertainty 
concerning 
the outcome 
of an action. 
If the contracted 
agent chooses some effort level, there are several pos- 
sibilities 
for an outcome. 
For example, 
suppose 
an 
agent on Mars subcontracts 
digging 
for samples 
of a 
given mineral and suppose that there is an uncertainty 
about 
the depth of the given mineral 
at the site. 
If 
the contracted 
agent chooses a high effort level but the 
mineral level is deep underground 
the outcome 
may be 
similar to the case where the contracted 
agent chooses 
a low level of effort but the mineral 
is located 
near the 
surface. 
But, if it chooses a high effort level when the 
mineral 
is located 
near the surface, 
the outcome 
may 
be much better. 
In such situations 
the outcome 
of per- 
forming 
a task doesn’t 
reveal the exact 
effort level of 
Distributed 
Problem Solving 
245 

the contracted 
agent and choosing 
a 
ma1 contract 
is much more difficult. 
stable and maxi- 
We will assume 
that 
the world may be in one of 
several states. 
Neither 
the contracting 
agent nor the 
contracted 
agent 
knows the exact 
state 
of the world 
when agreeing 
on the contract, 
as well as when the 
contracted 
agent chooses the level of effort to take, af- 
ter agreeing 
on the contract. 
The 
contracted 
agent 
may observe 
the state of the world after choosing 
the 
effort level (during 
or after completing 
the task), 
but 
the contracting 
agent 
can’t 
observe 
it. 
For simplic- 
ity, we also assume 
that there is a set of possible out- 
comes to the contracted 
agent carrying 
out the task 
& = bzl, 4fn) 
such that 
~1 < qz < . . . < qn that 
depends on the state of the world and the effort level 
of the contracted 
agent. 
Furthermore, 
we assume that 
given a level of effort, 
there is a probability 
distribu- 
tion that 
is attached 
to the outcomes 
that 
is known 
to both agents. 3 Formally, 
we assume that there is a 
probability 
function 
53 : E x Q + #Z, such that for any 
e E E, Cy a@, qi) = 1 and for all qi E Q, &e, qi) > 0.4 
The contracting 
agent’s 
problem 
is to find a contract 
that will maximize 
its expected 
utility, 
knowing that 
the contracted 
agent may reject the contract 
or if it ac- 
cepts the contract 
the effort level is chosen later [Ras- 
musen, 
19891. The contracting 
agent’s payment 
to the 
contracted 
agent 
can be based only on the outcome. 
Let us assume that in the contract 
that will be offered 
by the contracting 
agent, for any qi i = 1, . . . , n the con- 
tracting 
agent will pay the contracted 
agent wi. The 
maximization 
problem 
can be constructed 
as follows 
(see also [Rasmusen, 
19891). 
n 
Maximize,l,...,R 
x 
P(i7 qi)V(qi, 
W) 
(1) 
with the constraints: 
I 
e^ = argmaxeEE 
(2) 
Equation 
(1) states 
that the contracting 
agent tries 
to choose the payment 
to the contracted 
agent so as to 
3A practicall question is how the agents find the prob- 
ability distribution. 
It may be that 
they have preliminary 
information 
about 
the world, 
e.g., 
what is the possibility 
that 
a given mineral 
will be in that 
area of Mars. 
In the 
worst 
case, 
they 
may 
assume 
an equal distribution. 
The 
model 
can be easily extended 
to the case that 
each agent 
has different 
beliefs 
about 
the state 
of the world 
[Page, 
19871. 
*The 
formal 
model 
in which the outcome 
is a function 
of the state 
of the world 
and the contracted 
agent’s 
ef- 
fort 
level, 
and in which 
the probabilistic 
function 
gives 
the probability 
of the state 
of the world 
which 
is inde- 
pendent 
of the contracted 
agent’s 
effort level is a special 
case of the model 
described 
here [Page, 
1987; 
Ross, 
1973; 
Harris 
and Raviv, 
19781. 
maximize 
its expected 
utility subject 
to the constraint 
that the contracted 
agent will prefer the contract 
over 
rejecting 
it (3) and that the contracted 
agent prefers 
the effort level that the contracting 
agent prefers, given 
the contract 
it is offered (2). 
The main question 
is whether 
there is an algorithm 
to solve this maximization 
problem 
and whether 
such 
a contract 
exists. 
This depends 
primarily 
on the util- 
ity functions 
of the agents. 
If the contracting 
agent 
and the contracted 
agent are risk neutral, 
then solving 
the maximization 
problem 
can be done using any lin- 
ear programming 
technique 
(e.g, simplex, 
see for exam- 
ple [Pfaffenberger 
and Walker, 
19761.) 
Furthermore, 
in 
most situations, 
the solution 
will be very simple: 
the 
contracting 
agent will receive 
a fixed amount 
of the 
outcome 
and the rest will go to the contracted 
agent. 
That is, ‘uti = qi - C for 1 < i 5 n, where the constant 
C is determined 
by constraint 
(3) [Shavell, 
19791. 
Example 
2: Risk Neutral 
Agents 
Under 
Uncer- 
tainty 
Suppose 
the utility function 
of the German 
robot from 
Example 
1 is U(w, e> = w-e 
and that it can choose be- 
tween two eflort levels Low (e=l) 
and High (e=2) 
and 
its reservation 
price is ti = 1. 
There 
are two possible 
monetary 
outcomes 
to the digging: 
q1 = 8 and q2 = 10 
and the US robot’s utility function 
is as in the previous 
example, 
i. e., V(q,w)=q-w. 
If the German 
robot chooses 
the Lower 
level eflort 
then the outcome 
will be q1 with probability 
2 and q2 
with probability 
$ and if it takes the High level eflort 
the probability 
of q1 is $ and of q2 it is g. 
In such 
situations, 
the US robot should reserve 
to itself a profit 
of “5. 
That is, w1 = la 
and w2 = 3a. 
The German 
robot will choose the High level eflort. 
If the agents are not neutral toward risk, the problem 
is much more difficult. 
However, if the utility function 
for the agents are carefully 
chosen, 
an algorithm 
does 
exist. 
Suppose 
the contracted 
agent is risk averse and 
the contracting 
agent is risk neutral 
(the methods 
are 
also applicable 
when both are risk averse). 
Grossman 
and Hart [Grossman 
and Hart, 19831 presented 
a three- 
step procedure 
to find appropriate 
contracts. 
The first 
step of the procedure 
is to find for each possible 
effort 
level the set of wage contracts 
that 
induce 
the con- 
tracted 
agent to choose that effort level. 
The second 
step is to find the contract 
which supports 
that effort 
level at the lowest cost to the contracting 
agent. 
The 
third step is to choose the effort level that maximizes 
profits, given the necessity 
to support 
that effort with 
a costly wage contract. 
For space reasons, 
we won’t 
present 
the formal 
details 
of the algorithm 
here, and 
also in the rest of the paper. 
Repeated 
Encounters 
Suppose the contracting 
agent wants to subcontract 
its 
tasks several (finite) 
times. 
Repetition 
of the encoun- 
ters between the contracting 
and the contracted 
agents 
enables 
the agents 
to reach 
efficient 
contracts 
if the 
246 
Kraus 

number 
of encounters 
is large enough. 
The contract- 
ng agent could form an accurate 
estimate 
of the con- 
racted 
agent’s 
effort, 
based on the average outcome, 
over time. 
That 
is, if the contracting 
agent wants the 
contracted 
agent to take a certain 
effort level 2, in all 
he encounters, 
it can compute 
the expected 
outcome 
over time if the contracted 
agent actually 
performs 
the 
ask with that effort level. The contracting 
agent can 
keep track 
of the cumulative 
sum of the actual 
out- 
comes and compare 
it with the expected 
outcome. 
If 
here is some time T in which the outcome 
is below a 
given function 
of the expected 
outcome, 
the contract- 
ng agent 
should 
impose 
a big “punishment” 
on the 
contracted 
agent. 
If the function 
over the expected 
outcome 
is chosen carefully 
[Radner, 
198 11, the proba- 
bility of imposing 
a “punishment” 
when the contracted 
agent is in fact carrying 
out the desired effort level can 
be made very low, while the probability 
of eventually 
mposing 
the “punishment” 
if the agent doesn’t 
do e 
s one. 
Asyrm-netric 
and Hxicomplete Hmformation 
n some situations 
the contracting 
agent does not know 
the utility function 
of the contracted 
agent. 
The con- 
tracted 
agent may be one of several types that reflect 
the contracted 
agent’s 
ability to carry out the task, its 
efficiency 
or the cost of its effort. 
However, we assume 
that given the contracted 
agent’s type, its utility func- 
tion is known to its opponent. 
For example, 
suppose 
Germany 
builds 
robots 
of two types. 
The 
specifica- 
tions of the robots 
are known to the German 
robots 
and to the US robots, 
however, 
the US robots 
don’t 
know the specific type of the German 
robots that they 
meet. 
As in previous 
sections 
the output 
is a function 
of the contracted 
agent’s 
effort 
level, and the prob- 
ability 
function 
p indicates 
the probability 
of each 
outcome, 
given the effort level and the agent’s 
type. 
The 
question 
remains 
which 
contract 
the contract- 
ing agent should offer when it doesn’t 
know the con- 
tracted 
agent’s 
type. 
A useful technique 
in such sit- 
uations 
is for the contracting 
agent 
to search 
for an 
optimal 
mechanism 
[Demougin, 
19891 as follows: 
the 
contracting 
agent offers the contracted 
agent a menu 
of contracts 
that are functions 
of its type and the out- 
come. 
The 
agents 
chooses 
a contract 
(if at all) and 
announces 
it to the contracting 
agent. 
Given this con- 
tract, the contracted 
agent chooses an effort level which 
maximizes 
its own expected 
utility. 
In each of the 
menu’s contracts, 
the contracted 
agent’s expected 
util- 
ity should be at least as its expected 
utility if it doesn’t 
sign the contract. 
We also concentrate 
only on con- 
tracts 
in which it will always be in the interest 
of the 
contracted 
agent to honestly 
report, its type. 
It was 
proven that this requirement 
is without 
loss of gener- 
ality [Myerson, 
19821. It was also shown that in some 
situations, 
an efficient contract 
can be reached without 
communication 
[Demougin, 
19891, but we omit the dis- 
cussion here for space reasons. 
If there are several agents whose types are unknown 
to the contracting 
agent 
and it must 
choose 
among 
them, 
the following 
mechanism 
is appropriate: 
The 
contracting 
agent announces 
a set of contracts 
based 
on the agent’s type and asks the potential 
contracted 
agents to report their types. 
On the basis of these re- 
ports the contractin 
B 
agent chooses one agent [McAfee 
and McMillan, 
1987 . 
In other situations, 
the contracting 
agent knows the 
utility 
function 
of the contracted 
agent, 
but the con- 
tracted 
agent is able to find more information 
on the 
environment 
than the contracting 
agent. 
For example, 
when the German robot reaches the area where it needs 
to dig, it determines 
the structure 
of this area. 
This 
information 
is known only to the German 
robot 
and 
not to the US robot. 
The mechanism 
that should be 
used in this context 
is the following: 
The contracting 
agent offers a payment 
arrangement 
which is based on 
the outcome 
and the message the contracted 
agent will 
send to the contracting 
agent about 
the additional 
in- 
formation 
it possesses. 
If the contracted 
agent accepts 
the offer, it will observe 
the information 
(by going to 
the area, or using any of its sensors 
etc.). 
Then 
it 
will send a message 
to the contracting 
agent and will 
choose 
its effort level. 
Eventually, 
after 
the task 
is 
finished and the outcome 
is observed, 
the contracting 
agent will pay the rewards. 
Also in this case [Chris- 
tensen, 
19811, the agents can concentrate 
on the class 
of contracts 
that induce the contracted 
agent to send 
a truthful 
message. 
This 
is since for any untruthful 
contracts, 
a truthful 
one can be found in which the ex- 
pected 
utility 
of agents is the same. 
A maximization 
solvable problem can be constructed 
here, but we omit 
it for space reasons. 
Subcontracting 
to a Group 
Suppose 
that the task the contracting 
agent wants to 
contract 
for can be performed 
by a group of agents. 
Each of the contracted 
agents 
is independent 
in the 
sense that 
it tries to maximize 
its own utility. 
The 
contracting 
agent offers a contract 
to each of the pos- 
sible contracted 
agents. 
If one of them rejects 
the of- 
fer, then the contracting 
agent cannot 
subcontract 
the 
task. 
Otherwise, 
the contracted 
agents simultaneously 
choose effort levels. 
In other situations, 
the contracting 
agent can’t ob- 
serve the individual 
outcome 
(or such an outcome 
does 
not exists) but rather observe only the overall outcome 
from the effort of all agents 
[Holmstrom, 
1982].Bere, 
even in the case of certainty, 
i.e., the state of the world 
is known, there is a problem 
in making 
the contracted 
agents take the preferred 
level of action, 
since there is 
no way for the contracting 
agent to find out the effort 
level of each of the individual 
agent, given the overall 
output. 
For example, 
suppose two robots 
agreed to dig 
minerals, 
but they both put the minerals 
in the same 
truck, so it is not possible to figure out who digs what. 
istributed Problem Solving 
247 

If the contracting 
agent 
wants the contracted 
agents 
to take the vector 
of levels effort e* it can search for 
a contract 
such that, 
if the outcome 
is q 2 q(e*) then 
w(a) 
= bi and otherwise 
0, such that U(ey , bi) 2 tii. 
That is, if all agents choose the appropriate 
effort level, 
each of them gets bi and if any of them does not, all 
get nothing. 
Conclusions 
In this paper we presented 
techniques 
that can be used 
in different 
cases where sub-contracting 
of a task by 
an agent to another 
agent or a set of agents 
in non- 
collaborative 
environments 
is beneficial. 
In all the sit- 
uations, 
simple Pareto-optimal 
contracts 
can be found 
by using techniques 
of maximization 
with constraints. 
In the case where the agents 
have complete 
informa- 
tion about each other, there is no need for negotiations 
and a contract 
is reached 
without 
a delay even when 
the contracting 
agent doesn’t 
supervise 
the contracted 
agent’s 
actions. 
If there is asymmetric 
information, 
or 
the a.gents are not sure about 
their opponents’ 
utility 
functions, 
a stage 
of message 
exchange 
is needed 
to 
reach a contract. 
References 
Arrow, K. J. 1985. The economics 
of agency. In Pratt, 
J. and Zeckhauser, 
R., editors 
1985, 
Principals 
and 
Agents: 
The Structure 
of Business. 
Harvard Business 
School 
Press. 
37-51. 
Bond, 
A. H. and Gasser, 
L. 1988. 
An analysis 
of 
problems 
and research 
in DAI. 
In Bond, 
A. H. and 
Gasser, 
L., editors 
1988, 
Readings 
in DAI. Morgan 
Kaufmann 
Pub., 
Inc., Ca. 3-35. 
Christensen, 
J. 
1981. 
Communication 
in agencies. 
Bell Journal 
of Economics 
12:661-674. 
Conry, S.E.; 
Macintosh, 
D. J .; and Meyer, R.A. 1990. 
DARES: 
A distributed 
automated 
REasoning 
system. 
In Proc. of AAAISO, 
MA. 78-85. 
Davis, 
R. and Smith, 
R.G. 
1983. 
Negotiation 
as a 
metaphor 
for distributed 
problem 
solving. 
Artificial 
Intelligence 
20:63-109. 
Demougin, 
D. 1989. A renegotiation-proof 
mechanism 
for a principle-agent 
model 
with moral 
hazard 
and 
adverse 
selection. 
The Rand 
Journal 
of Economics 
20:256-267. 
Durfee, 
E. 1992. 
What 
your computer 
really needs 
to know, 
you learned 
in kindergarten. 
In Proc. 
of 
AAAI-92, 
California. 
858-864. 
Ephrati, 
E. and Rosenschein, 
J. 1991. The Clarke tax 
as a consensus 
mechanism 
among automated 
agents. 
In Proc. 
of AAAI-91, 
California. 
173-178. 
Gasser, 
L. 1991. 
Social 
concepts 
of knowledge 
and 
action: 
DA1 foundations 
and open systems semantics. 
Artificial 
Intelligence 
47( l-3):107-138. 
Grossman, 
S. and Hart, 
0. 
1983. 
An anaysis of the 
principal-agent 
problem. 
Econometrica 
51(1):7-45. 
Grosz, B. and Kraus, S. 1993. Collaborative 
plans for 
group activities. 
In IJCA193, 
French. 
Harris, 
M. and Raviv, 
A. 1978. 
Some results 
on in- 
centive contracts 
with applications 
to education 
and 
employment, 
health insurance, 
and law enforcement. 
The American 
Economic 
Review 68( 1):20-30. 
Hirshleifer, 
J. and Riley, 
J. 
1992. 
The Analytics 
of 
Uncertainty 
and Information. 
Cambridge 
University 
Press, Cambridge. 
Holmstrom, 
B. 1982. 
Moral 
hazard 
in teams. 
Bell 
Journal 
of Economics 
13(2):324-340. 
Kraus, 
S. and Wilkenfeld, 
J. 1991a. 
The function 
of 
time in cooperative 
negotiations. 
In Proc. 
of AAAI- 
91, California. 
179-184. 
Kraus, S. and Wilkenfeld, 
J. 1991b. 
Negotiations 
over 
time in a multi agent environment: 
Preliminary 
re- 
port. In Proc. of IJCAI-91, 
Australia. 
56-61. 
Kraus, 
S.; Ephrati, 
E.; and Lehmann, 
D. 1991. Nego- 
tiation 
in a non-cooperative 
environment. 
J. of Ex- 
perimental 
and Theoretical 
AI 3(4):255-282. 
Lesser, 
V.R. 
1991. 
A retrospective 
view of fa/c dis- 
tributed 
problem solving. 
IEEE 
Transactions 
on Sys- 
tems, Man, 
and Cybernetics 
21(6):1347-1362. 
McAfee, 
R. P. and McMillan, 
J. 1987. 
Competition 
for agency contracts. 
The Rand Journal 
of Economics 
18(2):296-307. 
Myerson, 
R. 1982. Optimal 
coordination 
mechanisms 
in generalized 
principal-agent 
problem. 
Journal 
of 
Mathematical 
Economics 
10:67-81. 
Page, F. 1987. 
The existence 
of optimal 
contracts 
in 
the principal-agent 
model. 
Journal 
of Mathematical 
Economics 
16(2):157-167. 
Pfaffenberger, 
R. and Walker, 
D. 1976. 
Mathemati- 
cal Programming 
for Economics 
and Business. 
The 
IOWA State 
University 
Press, 
Ames, IOWA. 
Radner, 
R. 1981. Monitoring 
cooperative 
agreements 
in a repeated 
principal-agent 
relationships. 
Econo- 
metrica 49(5):1127-1148. 
Rasmusen, 
E. 1989. 
Games 
and Information. 
Basil 
Blackwell 
Ltd., Cambridge, 
Ma. 
Ross, S. 1973. 
The economic 
theory 
of agency: 
The 
principal’s 
problem. 
The American 
Economic 
Review 
63(2):134-139. 
Shavell, 
S. 1979. 
Risk sharing 
and incentives 
in the 
principal 
and agent relationship. 
Bell Journal 
of Eco- 
nomics 
10:55-79. 
Sycara, 
K. P. 1990. 
Persuasive 
argumentation 
in ne- 
gotiation. 
Theory 
and Decision 
28:203-242. 
Wellman, 
M. and Doyle, J. 1992. Modular 
utility rep- 
resentation 
for decision-theoretic 
planning. 
In Proc. 
of AI planning 
Systems, 
Maryland. 
236-242. 
Zlotkin, 
G. and Rosenschein, 
J. 199 1. Incomplete 
in- 
formation 
and deception 
in multi-agent 
negotiation. 
In Proc. IJCAI-91, 
Australia. 
225-231. 
248 
Kraus 

