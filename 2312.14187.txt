WAVECODER: WIDESPREAD AND VERSATILE ENHANCED
INSTRUCTION TUNING WITH REFINED DATA GENERATION.
Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, Qiufeng Yin
Microsoft
{v-zhaojianyu,xinzhang3,nishang,yanghuan,caxu,v-yiszhao,wenxh,qfyin}@microsoft.com
ABSTRACT
Recent work demonstrates that, after being fine-tuned on a high-quality instruction dataset, the
resulting model can obtain impressive capabilities to address a wide range of tasks. However, existing
methods for instruction data generation often produce duplicate data and are not controllable enough
on data quality. In this paper, we extend the generalization of instruction tuning by classifying the
instruction data to 4 code-related tasks and propose a LLM-based Generator-Discriminator data
process framework to generate diverse, high-quality instruction data from open source code. Hence,
we introduce CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal
code-related tasks,which is aimed at augmenting the effectiveness of instruction tuning and improving
the generalization ability of fine-tuned model. Subsequently, we present WaveCoder, a fine-tuned
Code LLM with Widespread And Versatile Enhanced instruction tuning. This model is specifically
designed for enhancing instruction tuning of Code Language Models (LLMs). Our experiments
demonstrate that Wavecoder models outperform other open-source models in terms of generalization
ability across different code-related tasks at the same level of fine-tuning scale. Moreover, Wavecoder
exhibits high efficiency in previous code generation tasks. This paper thus offers a significant
contribution to the field of instruction data generation and fine-tuning models, providing new insights
and tools for enhancing performance in code-related tasks.
30
35
40
45
50
55
60
1K (LoRA)
5K (LoRA)
20K (Fully)
41.7
41.7
45.7
45.7
47
47
44.9
44.9
48.1
48.1
50.5
50.5
CodeAlpaca
CodeOcean
0
10
20
30
40
50
60
70
Starcoder
CodeLLaMa-7B
CodeLLaMa-13B
Deepseek-6.7B
33.6
33.6
33.5
33.5
36
36
49.4
49.4
47
47
39
39
46.3
46.3
60.9
60.9
50.5
50.5
48.1
48.1
55.4
55.4
62.8
62.8
BaseModel
CodeAlpaca
CodeOcean
(a) HumanEval (Base Model:Starcoder)
(b) HumanEval (Different Models)
pass@1
pass@1
Figure 1: Comparision with CodeAlpaca with different dataset size(a) and different base models(b). CodeOcean
outpeforms CodeAlpaca on HumanEval multidimensionally, more detailed analysis is shown in Section 4.3.
arXiv:2312.14187v2  [cs.CL]  26 Dec 2023

Manually-
defined rules
Code Embedding Space
Coreset 
Generation
Raw Code Coreset
Raw data Collection
Instruction Data Generation
LLM Generator
LLM Discriminator
Bad Generation
Good Generation
zero/few shot
Example Database
Training Process
Base Language Model
Good Generation
CodeOcean
Instruction 
tuning
WaveCoder
Problem solving
Foundation dataset
Task definition:
Ø
Code generation
Ø
Code summary
Ø
…
Task rule:
Ø
Check the input
Ø
Check the generation
Ø
…
Input:…
Generation:
A
B
Generator setting
Discriminator setting
C
Reformatting the 
good generation 
to instruction data
Raw Code
A
B
C
Raw Code Collection
KcenterGreedy
Figure 2: The overview of the pipeline.
1
Introduction
In recent months, numerous Large Language Models (LLMs) such as ChatGPT, GPT-4 [1], and Gemini 1 have attained
unprecedented performance levels in a broad array of NLP tasks. These models utilize a self-supervised pre-training
process, and subsequent fine-tuning, to demonstrate exceptional zero/few-shot capabilities, effectively following human
instructions across an array of diverse tasks.
However, the overhead of holding such a large model is huge. Therefore, some relatively small LLM, especially Code
Large Language Models (Code LLMs), have attracted considerable attention due to their exceptional performance on a
wide range of code-related tasks. Given that LLMs can accrue a wealth of professional knowledge through pre-training
[1, 2, 3], an efficient and effective pre-training process on a code corpus is critical for Code LLMs. Several previous
works, including Codex [4], CodeGen [5], StarCoder [6], and CodeLLaMa [7], have successfully demonstrated that this
process can significantly improve their capability to tackle code-related problems.
Additionally, instruction tuning has been shown to perform at a satisfactory level across various tasks, as demonstrated
in research based on instruction-tuning like FLAN [8], ExT5 [9], and FLANT5 [10]. These studies incorporated
thousands of tasks into their training pipelines to improve the generalization ability of pre-trained model on downstream
tasks. InstructGPT [11] aligns user input effectively by integrating high-quality instruction data written by human
labelers, which inspires further exploration into the utility of instruction tuning. Stanford Alpaca [12] utilized ChatGPT
to generate instruction data via a self-instruct method, which in turn was used in the instruction tuning process. Further
improvements to the effectiveness of pre-training models have been explored in WizardLM [13] and WizardCoder [14],
where the evol-instruct method was applied. These works collectively exemplify the potential of instruction tuning in
enhancing the performance of large language models.
Based on these work, our intuition is that instruction-tuning can activate the potential of LLMs then align the pre-trained
models to an outstanding intelligent level. Therefore, we have summarized the main function of instruction tuning:
Generalization. Initially proposed to augment the cross-task generalization capabilities of Language Models (LMs)[8,
9, 10], instruction-tuning enhances performance across a multitude of unseen tasks when fine-tuned with diverse NLP
task instructions.
Alignment. Pre-trained models [2, 3, 15, 16], having learned from a vast number of self-supervised tasks at both token
and sentence levels, already possess the capability to comprehend text inputs. Instruction-tuning offers instruction-level
tasks for these pre-trained models[15, 17], enabling them to extract more information from the instruction beyond the
1https://deepmind.google/technologies/gemini
2

open source code
Generation
Convert/Rewrite the given code from one programming language to another.
Write clear and concise documentation for the given code.
Identify and fix errors in the given code.
LM 
Each generated case needs to be provided with the following keys:
Ø Task Name
Ø Instruction
Ø Input
Ø Output
Here are some requirements you should allowed:
1. The Output is a specific resolution addressing Instruction and Input; 
therefore, an Output must be relevant to both Instruction and Input.
2. The instruction should be one or two sentences.
3. In the Output, it should only contain the code. There should be no 
explanations provided outside the code.
...
Each generated case needs to be provided with the following keys:
Ø Task Name
Ø Instruction
Ø Input
Ø Output
Here are some requirements you should allowed:
1. The Output is a specific resolution addressing Instruction and Input; 
therefore, an Output must be relevant to both Instruction and Input.
2. The instruction should be one or two sentences.
3. In the Output, it should only contain the code. There should be no 
explanations provided outside the code.
...
Each generated case needs to be provided with the following keys:
Ø Task Name
Ø Instruction
Ø Information
Ø Solution
Here are some requirements you should allowed:
1. The Output is a specific resolution addressing Instruction and Input; 
therefore, an Output must be relevant to both Instruction and Input.
2. The instruction should be one or two sentences.
3. In the Output, it should only contain the code. There should be no 
explanations provided outside the code.
...
Ø Task Name
Ø Instruction
Ø Information
Ø Solution
LM 
Analysis:
- step 1: check the code:
1. The Input should be code and cannot only 
contain comments. 
- step 2: check the Output:
1. Solution: Solution is related to instruction 
and information. Solution is the specific 
resolution to instruction and information. 
2. Instruction: the programming language 
should be specified in the instruction. 
3. Solution: in the solution, it should only 
contain the code and comments within the 
code. There should be no explanations 
provided outside the code. 
4. Instruction: The content of the instruction 
should be relevant to the Input and should be a 
summary of the Input content, without any 
additional unrelated information. 
...
Model-Generated
Human-written
A
B
Generation Phase
Discrimination Phase
Figure 3: The overview of the LLM-based Generator-Discriminator Framework.
semantics of raw text. This additional information is user intention and enhances their interactive abilities with human
users, thereby contributing to alignment.
To enhance the performance of Code LLMs through instruction tuning, many existing methods for generating instruc-
tional data have been devised, focusing primarily on the two aforementioned aspects. For example, self-instruct [12],
evol-instruct [13] employed the zero/few-shot ability of teacher LLM to generated instruction data, which present an
amazing way for instruction data generation. However, these generation ways are too dependent on the performance
of the teacher LLM and sometimes produces a large amount of duplicate data which will reduce the efficiency of
fine-tuning [18, 19, 20]. Octopack [21] constructs a code instruction dataset leveraging the natural structure of Git
commits to break away from dependence on teacher LLMs. Howerver, Ensuring the quality of data in git messages
presents a considerable challenge, and the comprehensive screening of data through artificial filtering rules is often a
complex task.
To solve these problems, as shown in Figure 2, we propose a method that could make full use of source code and
explicitly control the quality of generated data. Owing to the fact that instruction tuning is to align the pre-training model
to the instruction-follow training set [1, 17], we prensent a LLM Generator-Disciminator framework for instruction
data generation. By employing generation and discrimination, our method can make the data generation process more
customizable and more controllable. Taking the raw code as input and selecting the core dataset, our method could
stably generate more realistic instruction data and control the diversity of data by adjusting the distribution of the raw
code.
In this paper, our focus is on enhancing the performance of code LLMs by applying instruction-tuning. Addressing
the aforementioned challenges, we refine the instruction data by classifying the instruction instances to four universal
code-related tasks: Code Summarization, Code Generation, Code Translation, and Code Repair and generate a dataset
of 20,000 instruction instances, termed CodeOcean, for the four code-related tasks with our data generation strategy.
To validate our approach, taking StarCoder [6], CodeLLaMa [7], and DeepseekCoder [22] as our base models ,
we introduce WaveCoder models, fine-tuned with our data generation strategy and evaluate it on HumanEval [4],
MBPP [23], HumanEvalPack [24] benchmarks, experimental results show that our WaveCoder exhibits exceptional
performance based on a small-scale instruction tuning.
2
CodeOcean: Four-task Code-related Instruction Data
In this section, we introduce the methodological details of our exploration. We firstly choose 4 representative coding
tasks and collect raw code from open source dataset. For each task, we employ GPT-3.5-turbo to generate instruction
data for fine-tuning. The generation prompt is shown in Table 2 and the data generation process is shown in Figure 3.
2.1
Tasks Setting
Given the breadth of code-related tasks, we have selected four of the most universally representative and common tasks
from the three generative tasks (code-to-text, text-to-code, and code-to-code) for further exploration. These include
Code Summarization, Code Generation, Code Translation, and Code Repair. Detailed descriptions of these tasks can be
found below.
3

Table 1: The proportion of programming lan-
guage in raw code.
Task
Percentage(%)
Python
29.44
PHP
21.34
Go
19.68
Java
18.53
JavaScript
5.56
Others (Ruby,C++,C#)
5.45
Code Summarization (code-to-text). This task aims to create a brief
summary of a given code. The raw code is used as input and the
model’s response is reformulated into an instructional format.
Code Generation (text-to-code, code-to-code). In this task, the
model is expected to generate code based on a user’s demand descrip-
tion. The model generates instructions to simulate the user’s demand
discourse based on the raw code. The generated raw code is then
considered as the output.
Code Translation (code-to-code). This task involves converting one
programming language into another. A prompt and raw code are given
to the model, and it generates instructions and the translated code.
Code Repair (code-to-code). The aim of this task is to provide correct code based on potential issues in the given code.
The model is expected to generate solutions for the raw code, typically within one or two sentences, which are then
taken as the output.
2.2
Generation of Training Data
In past research work[25, 26], many researchers have discovered that data quality and diversity play an important role
in the instruction tuning process rather than data amount. The improvement of data quality and diversity are directly
related to the performance of the fine-tuned model. Therefore, we introduce the details of our training data generation
process in this section.
2.2.1
Selection of Raw Code
CodeSearchNet2 is a dataset containing 2 million (comment, code) pairs from open-source libraries hosted on GitHub.
It includes code and documentation for 6 programming languages. We have chosen CodeSearchNet as our foundation
dataset and applied the coreset-based selection method KCenterGreedy [27, 28] to maximize the diversity of the raw
code.
In order to select high-quality code for instruction-tuning, we make some rules to filter the foundation dataset prelimi-
narily. The main rules are as follows:
• The length of required code should not be too long or too short. In this work, we filtered the code whose length
lower than 50 or higher than 800.
• Followed Code Alpaca3, We have eliminated the raw code containing words from the blacklist, since these
code could potentially reduce the performance of the resulting model.
After filtered by the above rules, the size of the raw code dataset dropped to 1.2 million, which is still a huge scale and
too expensive to fine-tune. To this end, we employed KCenterGreedy algorithm based on code embeddings to reduce
the amount of training data while maximizing data diversity of raw code as much as possible.
In this method, our main work is divided into two steps:
Code embedding. In this step, we encode all the raw code samples using a pre-trained language model (roberta-large-
v1[16] in this paper). we take the [CLS] token as the code embedding for one input raw code, which is not defined in
generative language models like GPT-3.5.
Coreset sampling. In the coreset sampling step, our goal is to use a small number of samples to represent the distribution
of the overall data, thus we employed unsupervised clustering and focused on the data representations. After obtaining
the code embeddings from the first step, we used KCentergreedy algorithm [27] to select the core dataset and maximize
data diversity, which aims to choose k center points such that minimize the largest distance between a random data
point and its nearest center, and has been proven efficient in obtaining a set of core samples of one distribution.
2.2.2
LLM-based Generator-Discriminator Framework
After the process of collecting raw code, it is significant to generate instruction data and reformat them to training data.
To this end, shown in Figure 2, we propose a novel LLM-based Generator-Discriminator Framework that can leverage
2https://huggingface.co/datasets/code_search_net
3https://github.com/sahil280114/codealpaca
4

Table 2: The proportion of generated data and the prompts used.
Task
Num
Per(%)
Prompt
Code Generation
11370
57.0
Implementing functions that perform specific operations given input.
Code Summarization
3175
15.9
Write clear and concise documentation for the given code.
Code Repair
3144
15.7
Identify and fix errors in the given code.
Code Translation
2236
11.2
Rewrite the given code from one programming language to another.
Raw Code
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Simple server which adds a DocumentWordsProvider to the
CodeCompletion worker.
On Windows, this script is frozen by freeze_setup.py (cx_Freeze).
"""
from pyqode.core import backend
if __name__ == '__main__':
backend.CodeCompletionWorker.providers.append(backend.DocumentWordsProvider())
backend.serve_forever()
Generation
Task_name: Adding a DocumentWordsProvider to a CodeCompletion worker
Instruction: Write a Python code that adds a DocumentWordsProvider to a CodeCompletion worker.
Information: The CodeCompletion worker is from the pyqode.core.backend module. The 
DocumentWordsProvider is used to provide word completion suggestions for a given document.
Solution:
from pyqode.core import backend
if __name__ == '__main__':
backend.CodeCompletionWorker.providers.append(backend.DocumentWordsProvider())
backend.serve_forever()
a
b
Analysis
- step 1: check the Input:
1. The Input should be code and cannot only contain comments. 
<answer: yes, the Input includes both the code and comments, not just the 
comments>
- step 2: check the Output:
1. Solution: Solution is related to instruction and information. Solution is the 
specific resolution to instruction and information. 
<answer: yes>
2. Instruction: the programming language should be specified in the instruction. 
<answer: yes, the instruction specifies the use of Python>
3. Solution: in the solution, it should only contain the code and comments within 
the code. There should be no explanations provided outside the code. 
<answer: no, in the solution, it is reasonable to include the code and comments 
within the code. However, the Note section belongs to explanations provided 
outside the code, which is not allowed>
4. Instruction: The content of the instruction should be relevant to the Input and 
should be a summary of the Input content, without any additional unrelated 
information. 
<answer: yes, the Instruction is related to the Input>
- Overall answer: no
- reasons: in the solution, it is reasonable to include the code and comments 
within the code. However, the Note section belongs to explanations provided 
outside the code, which is not allowed. So the overall answer is no.
c
Figure 4: An example of LLM-based Generator and Discriminator framework. The generator produce the instruction
data based on the input (a). Subsequently, the discriminator accept the output and generated analysis for it. The output
(b) includes four keys, we take the information as input and solution as output in our instruction tuning. The analysis (c)
consists of the detailed reason for each rule and an overall answer to check if the sample meet all the requirements.
an extensive amount of unsupervised open source code to generate supervised instruction data. By incorporating the
open source code, the diversity of the generated data no longer relies solely on capability of the teacher LLM itself.
In the generation phase, we utilize GPT-4 to generate a task definition in the context of a specific scenario. We follow
this with the definition of generation requirements for the task at hand, integrating both the task definition and all
associated requirements into the generation prompt. By using the raw code as an input and selecting good and bad
case examples, we leverage the few-shot abilities of GPT-3.5 to generate the necessary knowledge and information for
instruction tuning.
For the discriminator phase, we establish a series of criteria related to instruction data and employ GPT-4 to assess the
quality of instruction instances. Each instruction instance is classified as either a good or bad case, and this information
is subsequently reused in the following generation as examples. An example of our Generator and Discriminator
framework is presented in Figure 4. This framework provides a comprehensive approach to generating and evaluating
instruction data, ensuring a high-quality and diverse training dataset.
3
Training
Unlike the previous work [14, 29, 30] that mainly focus on code generation task, we hope that CodeOcean could gain
more general problem-solving capabilities on a wider range of coding tasks. Therefore, our training relies on a 20k
instruction dataset generated by our LLM-based Generator-Discriminator framework and an additional logical dataset.
The 20k dataset covers 4 common code tasks, including code generation, code translation, Code summarization, and
code repair.
To obtain WaveCoder models, We choose StarCoder-15B, CodeLLaMa (7B and 13B), DeepseekCoder-6.7B as the
base model and fine-tune all the base model for 3 epochs using NVIDIA A100-80GB GPU. For StarCoder-15B and
CodeLLaMa-13B, we set the global batch size to 128 using Tensor Parallel and set the initial learning rate at 2e-5. For
5

DeepseekCoder-6.7B and CodeLLaMa-7B, we set the global batch size to 256 using the Fully Sharded Data Parallel
(FSDP) module from Pytorch and set the initial learning rate at 5e-5. The sequence length of our each instruction
sample is less than 1024. The fine-tuning prompt is as follows:
Prompt with Input:
Below is an instruction that describes a task, paired with an input that provides further context. Write a response
that appropriately completes the request.
### Instruction:{instruction}
### Input:{input}
### Response:
Prompt without Input:
Below is an instruction that describes a task, paired with an input that provides further context. Write a response
that appropriately completes the request.
### Instruction:{instruction}
### Response:
4
Experiments
4.1
Experimental Setup
4.1.1
Benchmarks and Baselines
We evaluate our model on three code benchmarks: HumanEval [4], MBPP[23], HumanEvalPack[24].
HumanEval 4, which consists of 164 manually-written Python programming problems and an average of 9.6 test cases
allocated to each problem is now the most extensively adopted benchmark for Code LLMs.
MBPP 5 consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level
programmers, covering programming fundamentals, standard library functionality, and so on. In this paper, we choose
the 500 preoblems test dataset to evaluate both few-shot inference of fine-tuned models. For whose MBPP(500) result
is not reported or not used, we reproduced for them using bigcode-evaluation-harness 6.
HumanEvalPack 7 is an extension of OpenAI’s HumanEval to cover 6 total languages across 3 tasks. In this paper, we
select the HumanEvalFix to evaluate the code to code ability especially on code repair task and HumanEvalExplain
benchmarks to evaluate the code to text ability especially on code summarization task.
Proprietary Models. We present the self-reported results from an array of SoTA LLMs, including Gemini, ChatGPT
(gpt-3.5-turbo), GPT-4. If not reported, we use the results from Octopack or evaluate by ourselves.
Open source Models. Base models comprise StarCoder, CodeLLaMa and Deepseek-Coder with prompting. To ensure
an equitable comparison, we opted to select those instructed models that have been trained using fewer than 100,000
instruction instances for our comparative analysis.
4.1.2
metrics
Pass@k. Followed most previous works, we also adopt the pass@k metric proposed by OpenAI [4] to evaluate the
performance of our model. By generating n samples (n > k) for each promblem and checking the correctness for each
sample with the test cases, the pass@k score can be calculated by the unbiased estimator:
pass@k = E
"
1 −
  n
k−c

 n
k

#
(1)
where n is the total number of sample generated for a problem, c is the number of correct samples passing all the test
cases.
4https://huggingface.co/datasets/openai_humaneval
5https://huggingface.co/datasets/mbpp
6https://github.com/bigcode-project/bigcode-evaluation-harness
7https://huggingface.co/datasets/bigcode/humanevalpack
6

Table 3: Results of pass@1 on HumanEval and MBPP benchmark. We use self-reported scores whenever available.
Due to the difference in decoding strategies from previous evaluation work, we marked the results of greedy decoding
in blue and n = 200 samples in red . -: Not reported in their paper.
Model
Params
Base Model
InsT Data
HumanEval
MBPP (500)
Proprietary Models
GPT-4 [1]
-
-
-
86.6 / 67.0
-
ChatGPT [1]
-
-
-
73.2 / 48.1
52.2
Gemini
-
-
-
74.4
-
Open-Source Models
StarCoder
15B
-
✘
33.6
43.3
OctoCoder [21]
15B
StarCoder
13K
46.2
43.5
WizardCoder [14]
15B
StarCoder
78K
57.3
51.8
WaveCoder-SC-15B
15B
StarCoder
20K
50.5 (+16.9)
51.0 (+7.4)
CodeLLaMa [7]
7B
-
✘
33.5
41.4
CodeLLaMa-instruct [7]
7B
CodeLLaMa
14K
34.8
44.4
WaveCoder-CL-7B
7B
CodeLLaMa
20K
48.1 (+14.6)
47.2 (+5.8)
CodeLLaMa [7]
13B
-
✘
36.0
47.0
CodeLLaMa-instruct [7]
13B
CodeLLaMa
14K
42.5
49.4
WaveCoder-CL-13B
13B
CodeLLaMa
20K
55.4 (+19.4)
49.6 (+2.6)
DeepseekCoder
6.7B
-
✘
49.4
60.6
WaveCoder-DS-6.7B
6.7B
DeepseekCoder
20K
62.8 (+13.4)
62.4 (+1.8)
4.2
Experimental Results
4.2.1
Evaluation on Code Generation Task
HumanEval and MBPP are two representative benchmarks for code generation task where the model should generate
the complete code based on the function signature and the docstring of the problem. Table 3 shows the pass@1 score of
different LLMs on both benchmarks. From the results, We have the following observations:
WaveCoder models substantially outperform the instructed models training with fewer than 20K instruction
tuning data (InsT Data). Following the fine-tuning process, the performance of our models exhibit substantial
improvement when compared to both the foundation model and a selection of open-source models, but it still lags
behind proprietary models the instructed models training with more than 70K training data.
Refined and diverse instruction data can significantly improve the efficiency of instruction tuning. Although
Wavecoder falls behind WizardCoder and Magicoder on code generation task, there is a considerable difference in data
size among them. Nevertheless, the performance lag of Wavecoder is relatively minor.
4.2.2
Evaluation on Other Code-related Task
We score WaveCoder with state-of-the-art Code LLMs on HumanEvalPack in Table 4.We use the result reported in
Octopack [21] when avaliable. Table 4 presents the results of WaveCoder on code repair task and Table 5 presents the
results of WaveCoder on Code summarization task, highlighting the the following salient observations:
WaveCoder outperforms all open source models on other code-related task with similar degrees of fine-tuning.
Using Starcoder as the foundation model, our WaveCoder-SC outperforms both WizardCoder and OctoCoder across
all programming languages on HumanEvalFix and HumanEvalExplain benchmarks. Even when compared with other
foundation models, our model consistently demonstrates superior performance over all previously fine-tuned models.
Enhancing the refinement and diversity of data can significantly improve the effectiveness of instruction tuning in
multi-task scenarios. By refining the data and classifying the instructions to 4 different code-related downstream tasks,
our models achieve a surprising performance level on both evaluation scenarios. Notably, WaveCoder-DS, utilizing
only 6.7B parameters, approximates the performance of GPT-4 on HumanEvalFix quite closely.
7

Table 4: Results of pass@1 on HumanEvalFix benchmark. We use self-reported scores whenever available. Due to the
difference in decoding strategies from previous evaluation work, we marked the results of greedy decoding in blue
and n = 20 samples in red .
Model
Python
JavaScript
Java
Go
C++
Rust
Avg.
GPT-4
47.0
48.2
50.0
50.6
47.6
43.3
47.8
StarCoder
8.7
15.7
13.3
20.1
15.6
6.7
13.4
OctoCoder
30.4
28.4
30.6
30.2
26.1
16.5
27.0
WizardCoder
31.8
29.5
30.7
30.4
18.7
13.0
25.7
WaveCoder-SC-15B
39.3
35.1
34.8
36.2
30.2
22.5
33.0
CodeLLaMa-instruct-7B
28.0
23.2
23.2
18.3
0.1
0.1
15.5
CodeLLaMa-CodeAlpaca-7B
37.8
39.0
42.0
37.8
37.2
29.2
37.1
WaveCoder-CL-7B
41.4
41.4
42.0
47.1
42.7
34.7
41.5
CodeLLaMa-instruct-13B
29.2
19.5
32.3
24.4
12.8
0.1
19.7
CodeLLaMa-CodeAlpaca-13B
42.7
43.9
50.0
45.7
39.6
37.2
43.2
WaveCoder-CL-13B
48.8
48.2
50.6
51.8
45.1
40.2
47.4
DeepseekCoder-6.7B
29.9
29.2
39.0
29.2
25.0
21.9
29.0
DeepseekCoder-CodeAlpaca-6.7B
49.4
51.8
45.1
48.8
44.5
31.7
45.2
WaveCoder-DS-6.7B
54.9
51.2
57.3
49.3
45.1
32.9
48.4
Table 5: Results of pass@1 on HumanEvalExplain benchmark. We use self-reported scores whenever available. Due to
the difference in decoding strategies from previous evaluation work, we marked the results of greedy decoding in blue
and n = 20 samples in red .
Model
Python
JavaScript
Java
Go
C++
Rust
Avg.
GPT-4
64.6
57.3
51.2
58.5
38.4
42.7
52.1
StarCoder
0.0
0.0
0.0
0.0
0.0
0.0
0.0
WizardCoder
32.5
33.0
27.4
26.7
28.2
16.9
27.5
OctoCoder
35.1
24.5
27.3
21.1
24.1
14.8
24.5
WaveCoder-SC-15B
37.1
33.3
40.5
23.3
31.8
19.3
30.8
CodeLLaMa-instruct-7B
33.5
36.0
31.7
21.3
25.0
16.4
27.3
CodeLLaMa-CodeAlpaca-7B
34.7
24.4
37.8
23.2
28.6
19.5
28.0
WaveCoder-CL-7B
41.4
31.7
39.0
25.0
34.1
23.2
32.4
CodeLLaMa-instruct-13B
40.2
26.8
37.2
22.5
28.0
14.6
28.2
CodeLLaMa-CodeAlpaca-13B
32.3
28.0
34.1
18.9
29.9
20.7
27.3
WaveCoder-CL-13B
45.7
42.0
48.2
32.3
38.4
20.7
37.9
DeepseekCoder-6.7B
43.9
40.2
37.8
29.2
34.1
22.5
34.6
Deepseek-CodeAlpaca-6.7B
40.8
37.2
42.1
29.9
31.7
22.5
34.0
WaveCoder-DS-6.7B
47.5
45.7
48.2
33.5
41.4
27.4
40.6
4.3
Comparison with CodeAlpaca
CodeAlpaca dataset contains 20K multi-task instruction-following data generated by the techniques in the self-instruct
[12]. To ensure a fair and multidimensional comparison, we randomly sampled 1K and 5K from both datasets
(CodeAlpaca and CodeOcean), set the same set of training hyper-parameters set (epoch = 3, learning rate = 1e-4, LoRA
rank = 8) and used the same training prompts. To prevent overfitting, we use Low-Rank Adaption (LoRA) [31] for
fine-tuning if the size of instruction-follow training dataset is less than 5K and perform full fine-tuning on whole 20K
dataset. Table 4, 5 present the result of CodeOcean on HumanEval and other code-related benchmarks highlighting the
following salient observations:
8

• After being fine-tuned with 1K, 5K and 20K of instructional data respectively, the performance of base model
improves significantly on HumanEval shown in Figure 1.Taking Starcoder as the base model, CodeOcean
surpasses the CodeAlpaca (44.9% vs 41.7%, 45.7% vs 48.1% and 47.0% vs 50.5%) shown in Figure 1
(a), which emphasizes the effectiveness of our LLM-based Generator-Discriminator Framework on refining
instruction data. As shown in Figure 1 (b), The results of different base models on CodeOcean surpasses
the the results on CodeAlpaca, which emphasizes the effectiveness of CodeOcean dataset in enhancing the
instruction-following ability of the base model.
• According to Table 4 and Table 5, All Wavecoder models significantly outperform the model fine-tuned with
CodeAlpaca. Remarkably, The pass@1 score of Wavecoder-CL-13B outperforms CodeLLaMa-CodeAlpaca-
13B achieving 10.6% absolute improvements on HumanEvalExplain. This emphasizes the effectiveness of
defining and classifying code-related tasks on enhancing the generalization ability of Code LLMs.
5
Related Work
5.1
Code Large Language Models.
Numerous researchers have proposed their Code Language Models (LLMs), including CodeGen [32], CodeT5[33],
StarCoder [6], CodeLLaMa [7], and Deepseek-Coder [22] within coding domain. By leveraging a comprehensive pre-
training process with a vast amount of code knowledge from scratch, these Code LLMs often exceed the performance
of super LLMs in many scenarios. In a bid to generate output more efficiently and assist in resolving programming
issues, instruction-based models like InstructCodeT5+ [34] and WizardCoder [14] have been introduced.
5.2
Instruction Tuning.
In order to unleash the potential of the foundation LLMs, instruction tuning is proposed to fine-tune the LLM with
multi-task datasets which is presented in task descriptions by natural language. For example, OpenAI fine-tuned its
GPT-3 [3] model with a diverse instruction dataset written by human annotators, the instruction-tuned model can
effectively align the intention of human users. This work is the indispensable part of the InstructGPT [17], which is the
foundation of the well-known ChatGPT. [10] proposes Flan-PaLM 540B [10] which is instruction-tuned on 1.8K tasks
and finds that the diversity of instruction dataset can significantly improve the performance of the instruction-tuned
language model. Alpaca [12] adopts the self-instruct method, which leverages the few shot ability of ChatGPT to
generate data for training. For Code LLMs, InstructCodeT5+ [34], WizardCoder [14], Pangu-coder2 [29], are recent
recent models trained with coding instructions. However, all the instruction data they used is from CodeAlpaca, which
is not refined enough.
6
Conclusion
This paper studies the effect of multi-task instruction data on enhancing the generalization ability of Code Large
Language Model. We introduce CodeOcean, a dataset with 20K instruction instances on four universal code-related
tasks. This method and dataset enable WaveCoder, which significantly imporove the generalization ability of foundation
model on diverse downstream tasks. Our WaveCoder has shown the best generalization ability among other open-source
models in code repair and code summarization tasks, and can maintain high efficiency on previous code generation
benchmarks. In order to further improve the mono-task performance and generalization ability of the model, future
work should focus on the interplay among different tasks and larger dataset.
9

References
[1] OpenAI. Gpt-4 technical report, 2023.
[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings
of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-
lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark
Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,
M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates, Inc., 2020.
[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri
Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code.
arXiv preprint arXiv:2107.03374, 2021.
[5] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming
Xiong. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh
International Conference on Learning Representations, 2023.
[6] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint
arXiv:2305.06161, 2023.
[7] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu,
Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950,
2023.
[8] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,
and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning
Representations, 2022.
[9] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei
Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. Ext5:
Towards extreme multi-task scaling for transfer learning. In International Conference on Learning Representations,
2022.
[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint
arXiv:2210.11416, 2022.
[11] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human
feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.
[12] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and
Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/
tatsu-lab/stanford_alpaca, 2023.
[13] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.
Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244,
2023.
[14] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei
Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint
arXiv:2306.08568, 2023.
[15] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,
Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.
[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint
arXiv:1907.11692, 2019.
10

[17] Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4: A 200-instruction paradigm for fine-tuning
minigpt-4. arXiv preprint arXiv:2308.12067, 2023.
[18] Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing
and mitigating repetitions for neural text generation. Advances in Neural Information Processing Systems,
35:3082–3095, 2022.
[19] Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, and Yue Zhang. Understanding in-context learning
from repetitions. arXiv preprint arXiv:2310.00297, 2023.
[20] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and
Nicholas Carlini. Deduplicating training data makes language models better. In Proceedings of the 60th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424–8445, 2022.
[21] Anonymous. Octopack: Instruction tuning code large language models. In Submitted to The Twelfth International
Conference on Learning Representations, 2023. under review.
[22] DeepSeek. Deepseek coder: Let the code write itself. https://github.com/deepseek-ai/DeepSeek-Coder,
2023.
[23] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,
Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint
arXiv:2108.07732, 2021.
[24] Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh,
Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language
models. arXiv preprint arXiv:2308.07124, 2023.
[25] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili
Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.
[26] Himanshu Gupta, Saurabh Arjun Sawant, Swaroop Mishra, Mutsumi Nakamura, Arindam Mitra, Santosh Mashetty,
and Chitta Baral. Instruction tuned models are quick learners. arXiv preprint arXiv:2306.05539, 2023.
[27] Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In
International Conference on Learning Representations, 2018.
[28] Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo Zhao.
Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning. arXiv preprint
arXiv:2305.09246, 2023.
[29] Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji,
Jingyang Zhao, et al. Pangu-coder2: Boosting large language models for code with ranking feedback. arXiv
preprint arXiv:2307.14936, 2023.
[30] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan
Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint
arXiv:2306.11644, 2023.
[31] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Represen-
tations, 2022.
[32] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming
Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint
arXiv:2203.13474, 2022.
[33] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-aware unified pre-trained encoder-
decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing, pages 8696–8708, 2021.
[34] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, and Steven C. H. Hoi. Codet5+:
Open code large language models for code understanding and generation. arXiv preprint, 2023.
11

