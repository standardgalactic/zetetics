arXiv:1701.01302v3  [cs.AI]  13 May 2017
Toward negotiable reinforcement
learning: shifting priorities in Pareto
optimal sequential decision-making
Andrew Critch∗†
May 16, 2017
Abstract
Existing multi-objective reinforcement learning (MORL) algorithms
do not account for objectives that arise from players with diﬀering beliefs.
Concretely, consider two players with diﬀerent beliefs and utility functions
who may cooperate to build a machine that takes actions on their behalf.
A representation is needed for how much the machine’s policy will priori-
tize each player’s interests over time. Assuming the players have reached
common knowledge of their situation, this paper derives a recursion that
any Pareto optimal policy must satisfy. Two qualitative observations can
be made from the recursion: the machine must (1) use each player’s own
beliefs in evaluating how well an action will serve that player’s utility func-
tion, and (2) shift the relative priority it assigns to each player’s expected
utilities over time, by a factor proportional to how well that player’s beliefs
predict the machine’s inputs. Observation (2) represents a substantial di-
vergence from na¨ıve linear utility aggregation (as in Harsanyi’s utilitarian
theorem, and existing MORL algorithms), which is shown here to be inad-
equate for Pareto optimal sequential decision-making on behalf of players
with diﬀerent beliefs.
1
Introduction
It has been argued that the ﬁrst AI systems with generally super-human cog-
nitive abilities will play a pivotal decision-making role in directing the future
of civilization (Bostrom, 2014). If that is the case, an important question will
arise: Whose values will the ﬁrst super-human AI systems serve? Since safety
is a crucial consideration in developing such systems, assuming the institutions
building them come to understand the risks and the time investments needed
to address them (Baum, 2016), they will have a large incentive to cooperate in
∗Machine Intelligence Research Institute
†UC Berkeley, Center for Human Compatible AI

their design rather than racing under time-pressure to build competing systems
(Armstrong et al., 2016).
Therefore, consider two nations—allies or adversaries—who must decide
whether to cooperate in the deployment of an extremely powerful AI system.
Implicitly or explicitly, the resulting system would have to strike compromises
when conﬂicts arise between the wishes of those nations. How can they specify
the degree to which that system would be governed by the distinctly held prin-
ciples of each nation? More mundanely, suppose a couple purchases a domestic
robot. How should the robot strike compromises when conﬂicts arise between
the commands of its owners?
It is already an interesting and diﬃcult problem to robustly align an AI sys-
tem’s values with those of a single single human (or a group of humans in close
agreement).
Inverse reinforcement learning (IRL) (Russell, 1998) (Ng et al.,
2000) (Abbeel and Ng, 2004) and cooperative inverse reinforcement learning
(CIRL) (Hadﬁeld-Menell et al., 2016) represent successively realistic early ap-
proaches to this problem. But supposing some adequate solution eventually ex-
ists for aligning the values of a machine intelligence with a single human decision-
making unit, how should the values of a system serving multiple decision-makers
be “aligned”?
One might hope to specify some extremely compelling ethical principle that
everyone would immediately accept. Realistically, however, disagreements will
always exist. Consider the general case of two parties—perhaps states, compa-
nies, or individuals—who might cooperatively build or purchase an AI system
to serve them both.1 If the parties cannot reach suﬃcient agreement as to what
policy the AI should follow, cooperation may be less attractive than obtaining
separate AI systems, one for each party. At the individual level, non-cooperation
could mean domestic disputes between domestic robots. At the state level, it
could mean an arms race between nations competing under time pressure to
develop ever more powerful militarized AI systems, aﬀording each nation less
time to ensure the safety and validity of their respective systems.
Unless the prospect of cooperative AI ownership is made suﬃciently attrac-
tive to the separate parties, the question of whose values the cooperatively owned
system “ought” to serve is moot: the parties will fall back on non-cooperative
strategies—perhaps obtaining separate machines that will compete with each
other—and the jointly owned system will not exist in the ﬁrst place. In addi-
tion, if the process of bargaining over the policy of a cooperatively owned system
is diﬃcult or complicated, the players are more likely to end negotiations and
default to non-cooperative strategies.
Conversely, if bargaining is made easier, players are more likely to reach
cooperation. The purpose of this paper is to begin formalizing the problem of
negotiating over the policy of a machine intelligence, and to exhibit some early
ﬁndings as to the nature of Pareto optimal policies—policies which cannot be
improved for one player without sacriﬁce by another—with the eventual aim
1The results of this paper all generalize directly from 2 to n players, but for concreteness
of exposition, the two-player case is prioritized.

of making cooperative outcomes easier to formulate, more attractive, and more
likely to obtain.
Outline.
The paper is organized as follows. Section 2 brieﬂy outlines some
standard choices of notation. Section 3 formalizes the problem of obtaining a
Pareto optimal policy for two distinct parties with common knowledge of distinct
priors, derives a recursion that any such policy must follow, and contrasts that
recursion with a more na¨ıve “just add up a linear combination of the utility
functions” approach. The recursion implies two main qualitative insights about
how a Pareto optimal policy, pursuant to a common-knowledge diﬀerence in
opinion, must behave over time: (1) such a policy must (explicitly or implicitly)
use each player’s own beliefs in evaluating how well an action will serve that
player’s utility function, and (2) it must shift the relative priority it places
on each player’s expected utilities over time, by a factor proportional to how
well that player’s beliefs predict the machine’s inputs. Section 4 provides some
further interpretation of these implications, in terms of bet-settling and moral
realism. Section 5 outlines subsequent work expected to be useful for enabling
cooperative AI deployment in the future. Finally, Section 6 provides concluding
remarks targeted at readers who have ﬁnished the full paper.
1.1
Related work
Social choice theory.
The whole of social choice theory and voting theory
may be viewed as an attempt to specify an agreeable formal policy to enact on
behalf of a group. Harsanyi’s utility aggregation theorem (Harsanyi, 1980) sug-
gests one form of solution: maximizing a linear combination of group members’
utility functions. The present work shows that this solution is inappropriate
when players have diﬀerent beliefs, and Theorem 8 may be viewed as an ex-
tension of Harsanyi’s form that accounts simultaneously for diﬀering priors and
the prospect of future observations. Indeed, Harsanyi’s form follows as a direct
corollary of Theorem 8 when players do share the same beliefs (Corollary 9).
Bargaining theory.
The formal theory of bargaining, as pioneered by Nash
(Nash, 1950) and carried on by authors such as Myerson (Myerson, 1979)
(Myerson, 2013) and Satterthwaite (Myerson and Satterthwaite, 1983), is also
extremely topical. Future investigation in this area might be aimed at general-
izing their work to sequential decision-making settings, and this author recom-
mends a focus on research speciﬁcally targeted at resolving conﬂicts.
Multi-agent systems.
There is ample literature examining multi-agent sys-
tems using sequential decision-making models. Shoham and Leyton-Brown (2008)
survey various models of multiplayer games using an MDP to model each agent’s
objectives. Chapter 9 of the same text surveys social choice theory, but does
not account for sequential decision-making.

Zhang and Shah (2014) may be considered a sequential decision-making ap-
proach to social choice: they use MDPs to represent the decisions of players in
a competitive game, and exhibit an algorithm for the players that, if followed,
arrives at a Pareto optimal Nash equilibrium satisfying a certain fairness crite-
rion. Among the literature surveyed here, that paper is the closest to the present
work in terms of its intended application: roughly speaking, achieving mutually
desirable outcomes via sequential decision-making. However, that work is con-
cerned with an ongoing interaction between the players, rather than selecting a
policy for a single agent to follow as in this paper.
Multi-objective sequential decision-making.
There is also a good deal
of work on Multi-Objective Optimization (MOO) (Tzeng and Huang, 2011), in-
cluding for sequential decision-making, where solution methods have been called
Multi-Objective Reinforcement Learning (MORL). For instance, G´abor et al.
(1998) introduce a MORL method called Pareto Q-learning for learning a set of a
Pareto optimal polices for a Multi-Objective MDP (MOMDP). Soh and Demiris
(2011) deﬁne Multi-Reward Partially Observable Markov Decision Processes
(MR-POMDPs), and use use genetic algorithms to produce non-dominated sets
of policies for them. Roijers et al. (2015) refer to the same problems as Multi-
objective POMDPS (MOPOMDPs), and provide a bounded approximation
method for the optimal solution set for all possible weightings of the objectives.
Wang (2014) surveys MORL methods, and contributes Multi-Objective Monte-
Carlo Tree Search (MOMCTS) for discovering multiple Pareto optimal solutions
to a multi-objective optimization problem. Wray and Zilberstein (2015) intro-
duce Lexicographic Partially Observable Markov Decision Process (LPOMDPs),
along with two accompanying solution methods.
However, none of these or related works address scenarios where the objec-
tives are derived from players with diﬀering beliefs, from which the priority-
shifting phenomenon of Theorem 8 arises. Diﬀering beliefs are likely to play a
key role in negotiations, so for that purpose, the formulation of multi-objective
decision-making adopted here is preferable.
2
Notation
The reader is invited to skip this section and refer back as needed; an eﬀort has
been made to use notation that is intuitive and fairly standard, following Pearl
(2009), Hutter (2003), and Orseau and Ring (2012).
Random variables are denoted by uppercase letters, e.g., S1, and lowercase
letters, e.g., s1, are used as indices ranging over the values of a variable, as in
the equation
E[S1] =
X
s1
P(s1) · s1.
Sequences are denoted by overbars, e.g., given a sequence (s1, . . . , sn), ¯s
stands for the whole sequence. Subsequences are denoted by subscripted in-
equalities, so e.g., s<4 stands for (s1, s2, s3), and s≤4 stands for (s1, s2, s3, s4).

3
Two agents building a third
Consider, informally, a scenario wherein two players — perhaps individuals,
companies, or states — are considering cooperating to build or otherwise obtain
a machine that will then interact with an environment on their behalf.2 In such
a scenario, the players will tend to bargain for “how much” the machine will
prioritize their separate interests, so to begin, we need some way to quantify
“how much” each player is prioritized.
For instance, one might model the machine as maximizing the expected
value, given its observations, of some utility function U of the environment that
equals a weighted sum
w1U 1 + w2U 2
(1)
of the players’ individual utility functions U 1 and U 2, as Harsanyi’s social ag-
gregation theorem (Harsanyi, 1980) recommends. Then the bargaining process
could focus on choosing the values of the weights wi.
However, this turns out to be a bad idea. As we shall see in Proposition 10,
this solution form is not generally compatible with Pareto optimality when
agents have diﬀerent beliefs.
Harsanyi’s setting does not account for agents
having diﬀerent priors, nor for decisions being made sequentially, after future
observations.
In such a setting, we need a new form of solution, exhibited
here along with a recursion that characterizes optimal solutions by a process
analogous to, but meaningfully diﬀerent from, Bayesian updating.
3.1
A POMDP formulation
Let us formalize the machine’s decision-making situation using the structure of
a Partially Observable Markov Decision Process (POMDP), as depicted by the
Bayesian network in Figure 1. (See Russell et al. (2003) for an introduction to
POMDPs, and Darwiche (2009) for an introduction to Bayesian networks.)
At each point in time i, the machine will have a policy πi that for each
possible sequence of observations o≤i and past actions a<i, returns a distribution
πi(−| o≤ia<i) on actions ai, which will then be used to generate an action ai
with probability π(ai | o≤ia<i). In Figure 1, the part of the Bayes net governed
by the machine’s policy is highlighted in green.
Common knowledge assumptions.
It is assumed that the players will have
common knowledge of the policy π = (π1, . . . , πn) they select for the machine
to implement, but that the players may have diﬀerent beliefs about how the
environment works, and of course diﬀerent utility functions. It is also assumed
that the players have common knowledge of one another’s posterior.
This assumption is critical. During a bargaining process, one should expect
players’ beliefs to update in response to one another’s behavior. Assuming com-
mon knowledge of posteriors means that the players have reached an equilibrium
2The results here all generalize from two players to n players being combined successively
in any order, but for clarity of exposition, the two person case is prioritized.

S1
S2
S3
S4
O1
A1
O2
A2
O3
A3
U
Figure 1: A POMDP of length n = 3
where, each knowing what the other believes, does not wish to further update
her own beliefs.3
We encode each player j’s outlook as a POMDP, Dj = (Sj, A, T j, U j, O, Ωj, n),
which simultaneously represents that player’s beliefs about the environment, and
the player’s utility function.
• Sj represents a set of possible states s of the environment,
• A represents the set of possible actions a available to the machine,
• T j represents the conditional probabilities player j believes will govern
the environment state transitions, i.e., Pj(si+1 | siai),
• U j represents player j’s utility function from sequences of environmental
states (s1, . . . , sn) to R; for the sake of generality, U j is not assumed to
be additive over time, as reward functions often are,
• O represents the set of possible observations o of the machine,
• Ωj represents the conditional probabilities player j believes will govern
the machine’s observations, i.e., Pj(oi | si), and
• n is the number of time steps.
3It is enough to assume the players have reached a “persistent disagreement” that cannot
be mediated by the machine in some way. Future work should design solutions for facilitating
the process of attaining common knowledge, or to obviate the need to assume it.

Thus, player j’s subjective probability of an outcome (¯s, ¯o, ¯a), for any ¯s ∈(Sj)n,
is given by a probability distribution Pj that takes π as a parameter:
Pj(¯s, ¯o, ¯a; π) := Pj(s1) ·
n
Y
i=1
Pj(oi | si) π(ai | o≤ia<i) Pj(si+1 | siai)
(2)
As such, the POMDPs D1 and D2 are “compatible” in the following sense:
Deﬁnition 1 (Compatible POMDPs). We say that two POMDPs, D1 and D2,
are compatible if any policy for one may be viewed as a policy for the other, i.e.,
they have the same set of actions A and observations O, and the same number
of time steps n.
3.2
Pareto optimal policies
In this context, where a policy π may be evaluated relative to more than one
POMDP, we use superscripts to represent which POMDP is governing the prob-
abilities and expectations, e.g.,
Ej[U j; π] :=
X
¯s∈(Sj)n
Pj(¯s; π)U j(¯s)
represents the expectation in Dj of the utility function U j, assuming policy π
is followed.
Deﬁnition 2 (Pareto optimal policies). A policy π is Pareto optimal for a
compatible pair of POMDPs (D1, D2) if for any other policy π′, either
E1[U 1; π] ≥E1[U 1; π′]
or
E2[U 2; π] ≥E2[U 2; π′].
It is assumed that, during negotiation, the players will be seeking a Pareto
optimal policy for the machine to follow, relative to the POMDPs D1 and D2
describing each player’s outlook.
Policy mixing assumption.
It is also assumed that during the agent’s ﬁrst
action (or before it), the agent has the ability to generate and store some random
numbers in the interval [0, 1], called a random seed, that will not aﬀect the
environment except through other features of its actions. Then, given any two
policies π and π′ and a scalar p ∈[0, 1] we may construct a third policy,
pπ + (1 −p)π′,
that decides with probability p (before receiving any inputs) to use policy π
for generating all of its future actions, and otherwise uses policy π′. (This is a
“once and for all” decision; the agent does not ﬂip-ﬂop between π and π′ once
the decision is made.) Mixtures of more than two policies are deﬁned similarly.
With this formalism, whenever P
k αk = 1 and each αk ≥0, we have
Ej

U j;
X
k
αkπk

=
X
k
αkEj[U j; πk].
(3)

Lemma 3. A policy π is Pareto optimal to players 1 and 2 if and only if there
exist weights w1, w2 ≥0 with w1 + w2 = 1 such that
π ∈argmax
π∗∈Π

w1E1[U 1; π∗] + w2E2[U 2; π∗]

(4)
Proof. The mixing assumption gives the space of policies Π the structure of a
convex space that the maps Ej[U j; −] respect by Equation 3. This ensures that
the image of the map f : Π →R2 given by
f(π) :=

E1[U 1; π], E2[U 2; π]

is a closed, convex polytope. As such, a point (x, y) lies on the Pareto boundary
of image(f) if and only if there exist nonnegative weights (w1, w2), not both
zero, such that
(x, y) ∈
argmax
(x∗,y∗)∈image(f)

w1x∗+ w2y∗
After normalizing w1 + w2 to equal 1, this implies the result.
3.3
A reprioritization mechanism that resembles Bayesian
updating
We shall soon see that any Pareto optimal policy π must favor, as time pro-
gresses, optimizing the utility of whichever player’s beliefs were a better pre-
dictor of the machine’s inputs.
This phenomenon turns out to algebraically
resemble Bayesian updating, but is quite diﬀerent in its meaning. Nonetheless,
it is most easily shown to occur by a precise analogy to Bayesian updating in a
third POMDP constructed from the outlooks of players 1 and 2, as follows.
For any weights, w1, w2 ≥0 with w1 + w2 = 1, we deﬁne a new POMDP
that works by ﬂipping a (w1, w2)-weighted coin, and then running D1 or D2
thereafter, according to the coin ﬂip. Explicitly,
Deﬁnition 4 (POMDP mixtures). Let D1 and D2 be compatible POMDPs, with
parameters Dj = (Sj, A, T j, U j, O, Ωj, n). Deﬁne a new POMDP compatible
both, denoted D = w1D1 + w2D2, with parameters Dj = (S, A, T, U, O, Ω, n),
as follows:
• S := {(j, s) | j ∈{1, 2}, s ∈Sj},
• The environmental transition probabilities T are given by
P
 (j, s1)

:= wj · Pj(s1)
for any initial state s1 ∈Sj, and thereafter,
P

(j′, si+1) | (j, si), ai

:=



Pj  si+1 | siai

if j′ = j
0
if j′ ̸= j

B
S1
S2
S3
S4
O1
A1
O2
A2
O3
A3
U
Figure 2: A POMDP (mixture) of length n = 3 initialized by a Boolean B
Hence, the value of j will be constant over time, so a full history for the
environment may be represented by a pair
(j, ¯s) ∈{1} × (S1)n ∪{2} × (S2)n.
Let B denote the boolean random variable that equals whichever constant
value of j obtains, so then
P(B = j) = wj
• The utility function U is given by
U(j, ¯s) := U j(¯s)
• The observation probabilities Ωare given by
P
 oi | (j, si)

:= P(B = j) · Pj(oi | si)
In particular, the policy does not observe directly whether j = 1 or j = 2.
The POMDP mixture D = w1D1 + w2D2 can be depicted with a Bayes
net by adding an additional environmental node for B in the diagram of D1
and D2 (see Figure 2). Indeed, given any policy π, the expected payoﬀof π in
w1D1 + w2D2 is exactly
P(B = 1) · E(U | B = 1; π) + P(B = 2) · E(U | B = 2; π)
= w1E2(U 1; π) + w2E2(U 2; π)

Therefore, using the above deﬁnitions, Lemma 3 may be restated in the following
equivalent form:
Lemma 5. Given a pair (D1, D2) of compatible POMDPs, a policy π is Pareto
optimal for that pair if and only if there exist weights wj such that π is an
optimal policy for the single POMDP given by w1D1 + w2D2.
3.4
A recursive Pareto optimality condition
Expressed in the form of Equation 4, it might not be clear how a Pareto opti-
mal policy makes use of its observations over time, aside from storing them in
memory. For example, is there any sense in which the machine carries “beliefs”
about the environment that it “updates” at each time step? Lemma 5 allows
us to answer this and related questions by translating theorems about single
POMDPs into theorems about compatible pairs of POMDPs.
If π is an optimal policy for a single POMDP, at any time step i, optimality
of the action distribution πi(−| o≤ia<i) can be characterized without reference
to the previous policy components (π1, . . . , πi−1), nor to πi(−| o′
≤ia′
<i) for any
alternate history o′
≤ia′
<i.4 To express this claim in an equation, Pearl’s “do()”
notation (Pearl, 2009) comes in handy:
Deﬁnition 6 (“do” notation).
Pj(¯o | do(¯a)) :=
X
¯s∈(Sj)n
Pj(s1) ·
n
Y
i=1
Pj(oi | si) Pj(si+1 | siai)
This expression is the same as the probability of (¯o, ¯a) when π is the constant
policy that places probability 1 on the action sequence ¯a.
Proposition 7 (Classical separability). If D is a POMDP described by condi-
tional probabilities P(−| −) and utility function U (as in Equation 2), then a
policy π is optimal for D if and only if for each time step i and each observa-
tion/action history o≤ia<i, the action distribution πi(−| o≤na<n) satisﬁes the
following backward recursion:
πi(−| o≤ia<i) ∈argmax
α∈∆A

P(o≤i | do(a<i)) · E[U | o≤ia<i; an ∼α; πi+1, . . . , πn]

This characterization of πi(o≤ia<i) does not refer to π1, . . . , πi−1, nor to πi(o′
≤ia′
<i)
for any alternate history o′
≤ia′
<i.
Proof. This is a standard property of POMDP solutions.
It turns out that Pareto optimality can be characterized in a similar way by
backward recursion from the ﬁnal time step. The resulting recursion reveals a
pattern in how the weights on the players’ conditionally expected utilities must
change over time, which is the main result of this paper:
4This fact can used to justify why the “sunk cost” fallacy is indeed a fallacy.

Theorem 8 (Pareto optimal policy recursion). Given a pair (D1, D2) of com-
patible POMDPs of length n, a policy π is Pareto optimal if and only if its
components πi for i ≤n satisfy the following backward recursion for some pair
of weights w1, w2 ≥0 with w1 + w2 = 1:
πi(−| o≤ia<i) ∈argmax
α∈∆A

w1P1 
o≤i | do(a<i)

· E1[U 1 | o≤ia<iai; ai ∼α; πi+1, . . . , πn]
+ w2P2 
o≤i | do(a<i)

· E2[U 2 | o≤ia<iai; ai ∼α; πi+1, . . . , πn]

In words, to achieve Pareto optimality, the machine must
1. use each player’s own beliefs when estimating the degree to which a decision
favors that player’s utility function, and
2. shift the relative priorities of the players’ expected utilities in the ma-
chine’s decision objective over time, by a factor proportional to how well
the players predict the machine’s inputs.
Proof. By Lemma 5, the Pareto optimality of π for (D1, D2) is equivalent to its
classical optimality for w1D1 +w2D2 for some (w1, w2), which by Proposition 7
is equivalent to satisfying the following backward recursion (writing P and E for
probabilities and expectations in w1D1 + w2D2):
πi(−| o≤ia<i) ∈argmax
α∈∆A

P(B = 1) · P

o≤i | do(a<i)

· E[U | o≤ia<iai; ai ∼α; πi+1, . . . , πn]
+ P(B = 2) · P

o≤i | do(a<i)

· E[U | o≤ia<iai; ai ∼α; πi+1, . . . , πn]

.
By Deﬁnition 4, the expression inside the argmax equals
w1P1 
o≤i | do(a<i)

· E1[U 1 | o≤ia<iai; ai ∼α; πi+1, . . . , πn]
+w2P2 
o≤i | do(a<i)

· E2[U 2 | o≤ia<iai; ai ∼α; πi+1, . . . , πn]
hence the result.
When the players have the same beliefs, they aways assign the same prob-
ability to the machine’s inputs, so the weights on their respective expectations
do not change over time. In this case, Harsanyi’s utility aggregation formula is
recovered as a special instance:
Corollary 9 (Harsanyi’s utility aggregation formula). Suppose that players 1
and 2 share the same beliefs about the environment, i.e., the pair (D1, D2) of
compatible POMDPs agree on all parameters except the players’ utility functions

U 1 ̸= U 2. Then a policy π is Pareto optimal if and only if there exist weights
w1, w2 ≥0 with w1 + w2 = 1 such that for i ≤n, πi satisﬁes
πi(−| o≤ia<i) ∈argmax
α∈∆A

E[w1U 1 + w2U 2] | o≤ia<iai; ai ∼α; πi+1, . . . , πn]

where E = E1 = E2 denotes the shared expectations of both players.
Proof. Setting E = E1 = E2 in Theorem 8, factoring out the common coeﬃcient
P1 
o≤i | do(a<i)

= P2 
o≤i | do(a<i)

, and applying linearity of expectation
yields the result.
3.5
Comparison to na¨ıve utility aggregation
To see the necessity of the Pj terms that shift the expectation weights in Theo-
rem 8 over time, let us compare it with the behavior of an alternative optimiza-
tion criterion that maximizes a ﬁxed linear combination of expectations.
A cake-splitting scenario.
The parameters of this scenario are laid out in
Table 1, and described as follows:
Alice (Player 1) and Bob (Player 2) are about to be presented with a cake
which they can choose to split in half to share, or give entirely to one of them.
They have (built or purchased) a robot that will make the cake-splitting decision
on their behalf. Alice’s utility function returns 0 if she gets no cake, 20 if she
gets half a cake, or 30 if she gets a whole cake. Bob’s utility function works
similarly.
However, Alice and Bob have slightly diﬀerent beliefs about how the en-
vironment works. They both agree on the state of the environment that the
robot will encounter at ﬁrst: a room with a cake in it (S1 = “cake”).
But
Alice and Bob have diﬀerent predictions about how the robot’s sensors will per-
ceive the cake: Alice thinks that when the robot perceives the cake, it is 90%
likely to appear with a red tint (O1 = “red”), and 10% likely to appear with
a green tint (O1 = “green”), whereas Bob believes the exact opposite. In ei-
ther case, upon seeing the cake, the robot will either give Alice the entire cake
(A1 = S1 = (all, none)), split the cake half-and-half (A1 = S1 = (half, half)),
or give Bob the entire cake (A1 = S1 = (none, all)). Moreover, Alice and Bob
have common knowledge of all these facts.
Now, consider the following Pareto optimal policy that favors Alice (Player
1) when O1 is red, and Bob (Player 2) when O1 is green:
ˆπ(−| red) = 100%(all, none)
ˆπ(−| green) = 100%(none, all)
This policy can be viewed intuitively as a bet between Alice and Bob about the
value of O1, and is highly appealing to both players:
E1[U 1; ˆπ] = 90%(30) + 10%(0) = 27
E2[U 2; ˆπ] = 10%(0) + 90%(30) = 27

S1
O1
P1(O1 | S1)
P2(O1 | S1)
A1 = S1
U 1
U 2
cake
red
90%
10%
(all, none)
30
0
(half, half)
20
20
(none, all)
0
30
green
10%
90%
(all, none)
30
0
(half, half)
20
20
(none, all)
0
30
Table 1: An example scenario wherein a Pareto optimal policy undergoes pri-
ority shifting
In particular, ˆπ is more appealing to both Alice and Bob than an agreement to
deterministically split the cake (half, half). However,
Proposition 10. The Pareto optimal strategy ˆπ above cannot be implemented by
any machine that na¨ıvely maximizes a ﬁxed-over-time linear combination of the
conditionally expected utility of the two players, i.e., by any policy π satisfying
π(−| o1) ∈argmax
α∈∆A

r · E1[U 1 | o1; a1 ∼α] + (1 −r) · E2[U 2 | o1; a1 ∼α]

(5)
for some ﬁxed r ∈[0, 1]. Moreover, every such policy π is strictly worse than ˆπ
in expectation to one of the players.
This proposition is relatively unsurprising when one considers the policy ˆπ
intuitively as a bet-settling mechanism, and that the nature of betting is to
favor diﬀerent preferences based on future observations. However, to be sure
of this impossibility claim, one must rule out the possibility that the ˆπ could
be implemented by having the machine choose which element of the argmax in
Equation 5 to use based on whether the cake appears red or green.
Proof of Proposition 10. Suppose π is any policy satisfying Equation 5 for some
ﬁxed r, and consider the following cases for r:
1. If r < 1/3, then π must satisfy
π(−| o1) = 100%(none, all).
Here, E1[U 1; π] = 0 < 27, so π is strictly worse than ˆπ in expectation to
Alice.
2. If r = 1/3, then π must satisfy
π(−| o1) = q(o1)(none, all) + (1 −q(o1))(half, half)
for some q(o1) ∈[0, 1] depending on o1. Here, E1[U 1; π] ≤20 < 27 (with
equality when q(red) = q(green) = 1), so π is strictly worse than ˆπ in
expectation to Alice.

3. If 1/3 < r < 2/3, then π must satisfy
π(−| o1) = 100%(half, half)
Here, E1[U 1; π] = E2[U 2; π] = 20 < 27, so π is strictly worse than ˆπ in
expectation to both Alice and Bob.
The remaining cases, r = 2/3 and r > 2/3, are symmetric to the ﬁrst two, with
Bob in place of Alice and (none, all) in place of (all, none).
4
Interpretations
Theorem 8 shows that a Pareto optimal policy must tend, over time, toward
prioritizing the expected utility of whichever player’s beliefs best predict the
machine’s inputs better. From some perspectives, this is a little counterintuitive:
not only must the machine gradually place more predictive weight on whichever
player’s prior is a better predictor, but it must reward that player by attending
more to her utility function as well. This behavior is not an assumption, but
rather is forced to occur by Pareto optimality. The players must agree to this
pattern of shifting priority over time, or else they will leave Pareto improvements
on the table during the bargaining period when they choose the machine’s policy.
This phenomenon warrants a few interpretations:
Bet settling.
As discussed in Section 3.5, a machine implementing a Pareto
optimal policy can be viewed as a kind of bet-settling device. If Alice is 90%
sure the Four Horsemen will appear tomorrow and Bob is 80% sure they won’t,
it makes sense for Alice to ask—while bargaining with Bob for the machine’s
policy—that the machine prioritize her values more if the Four Horsemen arrive
tomorrow, in exchange for prioritizing Bob’s values more if they don’t. Both
parties will be happy with this agreement in expectation. As long as it remains
possible to redistribute the machine’s priorities in a way that resembles an
agreeable bet between Alice and Bob, its policy is not yet Pareto optimal. Thus,
Theorem 8 can be seen as saying that a Pareto optimality policy goes about
settling a bet on the machine’s input at each time step, in such a way that no
additional bets settlable by the policy are desirable to both players.
Moral realism with Bayesian updating.
Alternatively, we could take more
seriously the interpretation of the weights wj in Theorem 8 as prior “beliefs”
about the value of the made-up latent variable B from Lemma 5 that simulta-
neously governs (1) how the environment works, and (2) what utility function
is “correct” to pursue. This interpretation is a bit unnatural because, even if
the original environmental variables Si were very grounded in physical reality,
the abstract variable B in Lemma 5 is merely a ﬁction conjured up to imply
a correlation between “is” and “ought”: namely, that either the world is gov-
erned by P1, and U 1 ought to be optimized, or the world is governed by P2, and
U 2 ought to be optimized. This occurs even if each of the two players treats

their beliefs and utilities completely separately (i.e., even if they apply Bayesian
updating only to their beliefs, and keep their utility functions ﬁxed).
Mixing “is” and “ought” in this way is often considered a type error. Nonethe-
less, many humans report an intuitive sense that there are objective, right-and-
wrong answers to moral questions that can be answered by observing the world.
If a human is implicitly and approximately acting in a Pareto optimal fashion
for a mixture of belief/utility outlooks D1, . . . , Dk, then the process of “updat-
ing” to favor a certain utility function might feel, from the inside, like “ﬁnding
an answer” to a moral question.
5
Current limitations and future directions
The eventual aim of this work is to facilitate the cooperative development and
deployment of advanced AI systems, by simplifying the process of bargaining
for shared control of such systems, and by making collaborative outcomes gen-
erally easier to implement and more attractive. For this purpose, while Pareto
optimality is a desirable condition to aim for, it is not an adequate solution
concept on its own. Indeed, the policy “maximize player 1’s utility function,
without regard for player 2” is Pareto optimal, yet is clearly not the sort of
solution one would expect two nations to agree upon. This and at least several
other issues must be addressed to build a satisfactory negotiation framework,
exhibited below in order of increasing diﬃculty as estimated by the author.
1. BATNA dominance.
In any bargaining situation, each player has a “best
alternative to negotiated agreement”, or BATNA, that she expects to obtain
if the other player chooses not to cooperate. The characterization of Pareto
optimality given in Theorem 8 does not account for the players’ BATNAs. Given
a facet of the Pareto boundary, speciﬁed by the maximization of a linear function
with weights (w1, w2), a policy π satisfying Theorem 8 will yield an expectation
pair

E1[U 1; π], E2[U 2; π]

lying on that facet. Thus, the bargaining problem
has been reduced to choosing an appropriate facet of the Pareto boundary. But
suppose not all points on the chosen facet lie above both players’ BATNAs.
Then, in order to satisfy the individual rationality of the players, the policy
should target a more speciﬁc subset of that facet.
2. Targeting speciﬁc expectation pairs.
If a speciﬁc target value for the
expectation pair

E1[U 1; π], E2[U 2; π]

is desired, unless that pair is a vertex of
the Pareto region (e.g., perhaps the boundary is curved), the best that following
the recursion of Theorem 8 ensures is a point on the same facet. The ability to
target a speciﬁc pair would solve not only BATNA dominance (1), but also help
achieve other fairness or robustness criteria that might arise from bargaining.
One approach would be to make a small modiﬁcation to the players’ utility func-
tions to ensure that the resulting Pareto boundary is curved, thereby avoiding
this problem at the cost of a tiny utility adjustment. Choosing a simple form

for the adjustment that is amenable to formal proof would be a natural next
step in this direction.
3. Information trade.
Our algorithm implicitly favors whichever player best
predicts the machine’s input history, given its action history. This makes sense
when the players have common knowledge of each other’s priors and observa-
tions, at which point they have already had the opportunity to update on each
other’s views and chosen not to. This is unrealistic if Alice knows that Bob has
made observations in the past that Alice did not. In that case, Alice will view
Bob’s beliefs as containing valuable information that ought to shift her prior.
She may wish to bargain with Bob for access to that information in order to
improve her own ability to optimize the machine’s policy. Perhaps she would
concede some control over the machine (by reducing her weight, w1) in exchange
for information provided by Bob to improve her beliefs. An eﬃcient procedure
to naturally facilitate this sort of exchange would be complimentary Theorem 8.
One approach would be to have each player express their posterior as a function
of the other’s, and use a ﬁxed point theorem to choose a stable pair of posteri-
ors. However, many questions arise about this method when there are multiple
ﬁxed points.
4.
Learning priors and utility functions.
It is notoriously diﬃcult to
explicitly specify one’s utility function U to a machine, so in practice, one must
choose a method enabling the machine to learn the utility function. Cooperative
inverse reinforcement learning (CIRL) (Hadﬁeld-Menell et al., 2016) exhibits
such a framework, and reduces the problem to solving a POMDP. In CIRL,
a human and a robot play a cooperative game wherein both players aim to
maximize the human’s utility function U, but the robot is uncertain about
U and must infer it from the human. Moreover, the human and robot have
common knowledge of this situation, so the human may engage in “teaching”
behavior to help the robot along. Such dynamics must be accounted for in a
satisfactory treatment of negotiation for a machine’s priorities. In addition, the
players’ priors should probably also be learned by a machine in some way rather
than explicitly speciﬁed.
5. Incentive compatibility.
Assuming any particular method for learning
players’ priors and utility functions, a question arrises as to whether it incen-
tivizes players to represent their beliefs and utilities honestly.
For example,
Alice may have some incentive to exaggerate her estimation of her BATNA in
the positive direction, to motivate Bob to “sweeten the deal” by conceding her
a higher priority w1 in the recursion of Theorem 8. As well, players might also
have incentives to alter their reported beliefs in order to exaggerate the degree
to which the machine’s decisions will aﬀect their utilities. A satisfactory learn-
ing method should rule out or otherwise cope with this phenomenon. A great
deal of literature already exists on incentive compatibility, as begun by Hurwicz

(1972), Myerson (1979), and Myerson and Satterthwaite (1983), which should
oﬀer a good start.
6. Naturalized decision theory.
The POMDP setting used here is “Carte-
sian” in that it assumes a clear divide between the machine’s inner work-
ings and its environment.
This is highly inappropriate when the machine
may be copied or simulated; it may wind up in Newcomb-like problems as in
Soares and Fallenstein (2015), and very strange cooperative equilibria may exist
between its copies, such as in Critch (2016). Instead, one should assume a “nat-
uralized” model of the problem where the machine is part of its environment, as
in Fallenstein et al. (2015). Some attempts have been made to characterize op-
timal decision-making in a naturalized setting, e.g., by Orseau and Ring (2012),
but very few theorems to aid in sequential implementation exist (e.g., no ana-
logue of Proposition 7 is known), except possibly for some self-reﬂective proper-
ties exhibited by Garrabrant’s logical inductors (Garrabrant et al., 2016) that
might be expanded to exhibit relevance to sequential decision-making. Without
a satisfactory model of naturalized decision-making for the machine to follow,
the negotiating parties might unwittingly assign the machine a policy vulnerable
to Newcomb-like extortions. On the other hand, a satisfactory resolution would
not only help to model the machine’s situation, but also that of the players
themselves during the negotiation phase.
6
Conclusion
Insofar as Theorem 8 is not particularly mathematically sophisticated—it em-
ploys only basic facts about convexity and linear algebra—this suggests there
may be more low-hanging fruit to be found in the domain of “machine im-
plementable social choice theory”. To recapitulate, Theorem 8 represents two
deviations from the intuition of na¨ıve utility aggregation: to achieve Pareto opti-
mality for players with diﬀering beliefs, a machine must (1) use each player’s own
beliefs in evaluating how well an action will serve that player’s utility function,
and (2) shift the relative priority it assigns to each player’s expected utilities
over time, by a factor proportional to how well that player’s beliefs predict the
machine’s inputs.
As a ﬁnal remark, consider that social choice theory and bargaining theory
were both pioneered during the Cold War, when it was particularly compelling to
understand the potential for cooperation between human institutions that might
behave competitively. In the coming decades, machine intelligences will likely
bring many new challenges for cooperation, as well as new means to cooperate,
and new reasons to do so.
As such, new technical aspects of social choice
and bargaining, along the lines of this paper, will likely continue to emerge.
In particular, the problems outlined in Section 5 represent areas particularly
promising for facilitating cooperative outcomes in the deployment of advanced
AI systems, and the present author is seeking collaborations to address them.

References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforce-
ment learning. In Proceedings of the twenty-ﬁrst international conference on
Machine learning, page 1. ACM, 2004.
Stuart Armstrong, Nick Bostrom, and Carl Shulman. Racing to the precipice: a
model of artiﬁcial intelligence development. AI & SOCIETY, 31(2):201–206,
2016.
Seth D Baum. On the promotion of safe and socially beneﬁcial artiﬁcial intelli-
gence. AI & SOCIETY, pages 1–9, 2016.
Nick Bostrom. Superintelligence: Paths, dangers, strategies. OUP Oxford, 2014.
Andrew Critch. Parametric bounded lob’s theorem and robust cooperation of
bounded agents. arXiv preprint arXiv:1602.04184, 2016.
Adnan Darwiche. Modeling and reasoning with Bayesian networks (Chapter 4).
Cambridge University Press, 2009.
Benja Fallenstein, Nate Soares, and Jessica Taylor.
Reﬂective variants of
solomonoﬀinduction and aixi. In International Conference on Artiﬁcial Gen-
eral Intelligence, pages 60–69. Springer, 2015.
Zolt´an G´abor, Zsolt Kalm´ar, and Csaba Szepesv´ari. Multi-criteria reinforcement
learning. In ICML, volume 98, pages 197–205, 1998.
Scott Garrabrant, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, and Jessica
Taylor. Logical induction. arXiv preprint arXiv:1609.03543, 2016.
Dylan Hadﬁeld-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell. Co-
operative inverse reinforcement learning, 2016.
John C Harsanyi.
Cardinal welfare, individualistic ethics, and interpersonal
comparisons of utility. In Essays on Ethics, Social Behavior, and Scientiﬁc
Explanation, pages 6–23. Springer, 1980.
Leonid Hurwicz. On informationally decentralized systems. Decision and orga-
nization, 1972.
Marcus Hutter. A gentle introduction to the universal algorithmic agent {AIXI},
2003.
Roger B Myerson. Incentive compatibility and the bargaining problem. Econo-
metrica: journal of the Econometric Society, pages 61–73, 1979.
Roger B Myerson. Game theory. Harvard university press, 2013.
Roger B Myerson and Mark A Satterthwaite. Eﬃcient mechanisms for bilateral
trading. Journal of economic theory, 29(2):265–281, 1983.

John F Nash. The bargaining problem. Econometrica: Journal of the Econo-
metric Society, pages 155–162, 1950.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement
learning. In Icml, pages 663–670, 2000.
Laurent Orseau and Mark Ring. Space-time embedded intelligence. In Interna-
tional Conference on Artiﬁcial General Intelligence, pages 209–218. Springer,
2012.
Judea Pearl. Causality. Cambridge university press, 2009.
Diederik M Roijers, Shimon Whiteson, and Frans A Oliehoek. Point-based plan-
ning for multi-objective pomdps. In IJCAI 2015: Proceedings of the Twenty-
Fourth International Joint Conference on Artiﬁcial Intelligence, pages 1666–
1672, 2015.
Stuart Russell.
Learning agents for uncertain environments.
In Proceedings
of the eleventh annual conference on Computational learning theory, pages
101–103. ACM, 1998.
Stuart Russell, Peter Norvig, John F Canny, Jitendra M Malik, and Douglas D
Edwards. Artiﬁcial intelligence: a modern approach (Chapter 17.1), volume 2.
Prentice hall Upper Saddle River, 2003.
Yoav Shoham and Kevin Leyton-Brown.
Multiagent systems:
Algorithmic,
game-theoretic, and logical foundations. Cambridge University Press, 2008.
Nate Soares and Benja Fallenstein. Toward idealized decision theory. arXiv
preprint arXiv:1507.01986, 2015.
Harold Soh and Yiannis Demiris. Evolving policies for multi-reward partially
observable markov decision processes (mr-pomdps). In Proceedings of the 13th
annual conference on Genetic and evolutionary computation, pages 713–720.
ACM, 2011.
Gwo-Hshiung Tzeng and Jih-Jeng Huang. Multiple attribute decision making:
methods and applications. CRC press, 2011.
Weijia Wang. Multi-objective sequential decision making. PhD thesis, Universit´e
Paris Sud-Paris XI, 2014.
Kyle Hollins Wray and Shlomo Zilberstein. Multi-objective pomdps with lexi-
cographic reward preferences. In Proceedings of the 24th International Joint
Conference of Artiﬁcial Intelligence (IJCAI), pages 1719–1725, 2015.
Chongjie Zhang and Julie A Shah. Fairness in multi-agent sequential decision-
making. In Advances in Neural Information Processing Systems, pages 2636–
2644, 2014.

