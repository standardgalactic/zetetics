See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/347936969
CAN MACHINE THINK -70!
Conference Paper · December 2020
CITATIONS
0
READS
339
5 authors, including:
Some of the authors of this publication are also working on these related projects:
UN-usual Robotics View project
Technology Research View project
Albert Efimoff
National University of Science and Technology MISIS
80 PUBLICATIONS   16 CITATIONS   
SEE PROFILE
Leonid Zhukov
National Research University Higher School of Economics
93 PUBLICATIONS   1,655 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Albert Efimoff on 27 December 2020.
The user has requested enhancement of the downloaded file.

1 
 
CAN MACHINE THINK – 70! 
 
 
 
 
 
INTRODUCTION 
 
 
The following page contains a full transcript of the discussion ‘Can machines think - 70?’, 
which was a part of Artificial Intelligence Journey (AIJ), annual Sber’s conference. The discussion 
was timed to 70’th anniversary of the famous article ‘Computing Machinery and Intelligence’ 
written by Alan Turing. The article was published in Mind journal in 1950. It contains historical 
review of the topic of AI, contemplations about current situation in branch and some predictions 
for the future directions of development. Alan Turing is the founding father for the Artificial 
Intelligence, and his thoughts still provoke discussions and arguments. 
The recording of the discussion can be viewed here. 
This discussion featured the following speakers:  
Albert Efimov – Sberbank, vice-president, head of the R&D department 
Leonid Zhukov – Sberbank, head of the Artificial Intelligence Laboratory 
Gurdeep Pall – Microsoft, corporate vice president 
Michael Wooldridge – University of Oxford, Head of Department of Computer Science 
Professor of Computer Science  
Gary Bradski - scientist, engineer, entrepreneur (main projects: Open CV, Industrial 
Perceptron). 
Full video can be accessed here https://youtu.be/_fDclfgb600  
 
TRANSCRIPT 
 
[Albert Efimov] Hello, my dear friends! Dear friends of artificial intelligence, general 
artificial intelligence, Sber, and AI journey. Welcome to the artificial intelligence journey! We 
start our discussion panel on the topic Can machines think? We believe that for this year famous 
article of Alan Turing, published in Mind journal in October 1950, which started actually a whole 
artificial intelligence journey, it's a good reason to discuss what is done on 70 years of artificial 
intelligence.  
Today with us few bests in the world experts on artificial intelligence and information 
technologies. First of all, you will hear from Gurdeep Pall, corporate vice president of Microsoft, 
Microsoft Research, and our longtime partner with whom we did not only one research project. 

2 
 
We are happy that Gurdeep will join us on the artificial intelligence journey. Everybody of you 
actually, who is a little bit older than 35, are familiar with Gurdeep. Because if you know Windows 
NT and VPN, then you know some of the products which he developed while he's quite a long 
stay at Microsoft (almost 30 years).  
So, some of you younger than Gurdeep is working for Microsoft. Gurdeep, is the inventor 
of Windows, (at least part of Windows NT), inventor of one of the first VPNs, and stands at the 
beginning of the internet era, Windows era, and now artificial intelligence era. So, Gurdeep, please. 
What is the allure of thinking machine, please? 
 
 
[Gurdeep Pall] Hello, everyone! Firstly, I'd like to start by thanking Albert for inviting me 
and allowing me to talk to you about one of my favorite topics, which is really in the artificial 
intelligence space. I'm joining you here from the Turing room, which is in the Microsoft Research 
building, Microsoft campus in the Seattle area. The topic of thinking machines has been a 
fascination for a very long time. I would start with the intelligence in life itself.  
Now the Earth is about 4.6 billion years old, about 500 million years ago, something very 
interesting happened. At that time, all the life was living inside the oceans. The oceans had very 
low oxygen levels. But around 500 million years ago, there's what is sometimes referred to as the 
Cambrian explosion or the Cambrian acceleration happened, where life suddenly got more 
intelligent. Scientists are still arguing about what is, why that happened. But the few things that 
happened at that time, which we know for sure are:  
 
• The oxygen level in the oceans went up. It went from about 2-3% to about 10%. 
• Developmental genes developed through mutations, which allowed the 
regulation of how creatures were able to control their basic genetic processes. 
• It was about 500 million years ago that the complex eye structure was developed 
in creatures.  
 
It’s not conclusive but that was a very interesting development because once the creatures 
could see, they could find food easily, they could avoid predators, and they could actually go after 
other creatures. That allowed much more complex organisms to be built. I think it's good to 
remember this as we think about artificial intelligence moving forward.  
Now, after 500 million years ago, the first documented idea of artificial intelligence was 
provided by none other than Homer himself, from around 8th century BC. 8th to 12th century BC 
is sort of the time when none of his writings are attributed. He talked about in The Iliad, how 
Hephaestus the lame God had different kinds of robots around him, to help him in his life. There 
were tripods with wheels, then they were these robots shaped like the female form, who was 
helping him with lots of things that Hephaestus was trying to accomplish.  
After Homer's writing, the next significant piece of thought, in the area of artificial 
intelligence is attributed to Ramon Llull, who was a philosopher and a thinker in the Catalan region 
of Spain. Ramon Llull invented a kind of a seven-disc rotating system, where every seven discs 
had different kinds of concepts. He basically said that if you rotate the different combinations of 
these discs, you can pretty much contain all the ideas and concepts around that existed. It was a 
very interesting idea that inspired a lot of thinking about this even centuries later.  
It was Wilhelm Leibniz in the 17th century, who then took that idea to create, perhaps even 
a more developed system, a sort of alphabet of thought. He wanted a system for the computation 
of ideas. After that, of course, why we are gathered here today, Alan Turing's seminal paper on 
can machines think. In that paper, Alan Turing did a couple of things: 
  
• Formulated how to even think about this idea of artificial intelligence, by creating 
this so-called Turing test.  

3 
 
• Taking different philosophical arguments about embodied intelligence, and really 
sort of playing them out, I thought that was incredible work.  
• It was also very telling, towards the end of that paper where he talks about things 
like telepathy and so on, with which he kind of created this sort of very open-ended 
aspect to artificial intelligence, which I think is something that we should all keep 
in mind as we move forward as we emulate human thinking.  
 
The last 70 years have been quite interesting in the artificial intelligence journey. It started 
with a lot of the rules-based approaches, symbolic systems that people worked on, expert systems, 
etc. While that showed early promise quickly, this sort of thinking ran out of steam. So, we ended 
up with this AI winter, until some of the data-driven approaches started to show results and 
potential but with very specialized approaches. But it was only in the last decade that we saw the 
neural nets take off, thanks to a lot of more computing power and a lot more digitized data being 
available. We are starting to see some incredible progress with these powerful generic networks.  
Now, if you take a step back from this journey in the last 70 years, you will find there are 
about three different schools of thought or cognitive metaphors. We have the connectionists, who 
are really trying to model intelligence in the sort of graph with units, which are connected to other 
units, and through composite analysis, you're able to learn patterns. Then we have the symbolists, 
who believe in the idea that humans use abstract symbols as the foundation of knowledge and learn 
how to reason with them. Then there are probably the lesser known dynamicists, who believed that 
the intelligence of an evolving system can be best modeled with methods like differential 
equations. What we've seen in the last 70 years is these different cognitive metaphors, becoming 
popular and pushing forward until they hit some walls causing folks to pursue a different model. 
Generally, you can classify all the work in AI so far in these three buckets.  
Now, let's talk about the connectionists, who are really having their day in the sun as it 
were, with the resurgence of neural networks and with deep learning. Now, the core construct there 
is really the construct of the neuron, which, as you see is a biological neuron inside the human 
brain. Well, how about, we create an artificial neuron as a basic unit, which actually is very simple 
when it comes to the mathematical operation that it does. We should recognize that it is already 
seen that the biological neuron is much more superior than the artificial neuron that we envision 
today. One simple example of that is that a single biological neuron has been shown to exhibit 
nonlinear capability. For example, if you have to learn the XOR function, and you have to do it 
with artificial neurons, you will need at least two layers, but it can actually be done in one in a 
biological neuron. That said, an artificial neuron approximates a biological one for our purposes. 
Now, taking this basic idea of an artificial neuron we create neural networks. In the same way, 
there are biological networks in the human brain, we create artificial neuron networks. And it is 
again through a similar kind of connectivity of these different neurons, like in biology's with 
synapse, here it is connections between the layers of the neurons.  
In the last 10 years or so, we've seen just a tremendous amount of innovation in deep 
learning architectures, for specialized intelligence tasks. Some of these are actually inspired by 
biology. In fact, if you look at a lot of the work in computer vision, not only, it was inspired by 
biology, it is now explaining human biology when it comes to computer vision and some of the 
structures and processes that we see in the visual cortex. Similarly, some of the work with memory-
based architectures, whether it be LSTM, etc., is starting to really get very specialized, taking some 
inspiration from the human mind. Thanks to the progress in deep learning, we have seen some 
amazing breakthroughs in the last five years. Now, all the breakthroughs that I showed you on the 
slide here, all happened in Microsoft Research, everything from speech recognition, at the level 
that is better than humans, the ability to detect objects better than humans, machine reading and 
comprehension where the AI model reads a corpus of text and can answer questions based on that 
to captioning of video things. All these as good or better than human milestones have happened 
in the last four to five years. A tremendous amount of progress has been made globally in the 

4 
 
industry, not just in Microsoft Research, but this is just a glimpse to show you how much progress 
we have made.  
Now, none of these tests actually would qualify as a Turing test, by definition, because the 
Turing test basically said that you can ask the entity anything, something that could be outside of 
the domain that these models are perfected for, and they would fail. But regardless they represent 
significant progress, and at the same time highlighted significant limitations. Before we talk about 
some of the limitations, I want to talk about the GPT-3 model, which I'm sure that you've heard 
about. This is the work that has been done by Open AI. This year they launched announced GPT-
3, which can deservedly be called the Grandmaster of language models. It is a generative model 
using transformer architecture with attention mechanisms. It is incredible, the kind of results that 
we can see with GPT-3 is stunning because this thing was trained on about 5 billion tokens of data. 
It has 175 billion parameters, which is surpasses anything before it. This underlines the fact that 
the deep learning-based approaches really became possible because we had this incredible amount 
of computing, that we could bring to bear and also incredible amounts of digitized data. This GPT-
3 has, in addition to being able to write different kinds of long-form text, has been used for many 
kinds of applications, largely in the language domain but not limited to that. For example, there 
are already applications for writing Python code, there are applications for automatically 
generating emails, there are plugins where you can automatically fill out Excel data, etc., which 
have all been written on top of the GPT-3 model, which has been very impressive. If you have to 
look at the progress of AI today, you have to use this as the marker of where we are 70 years after 
Alan Turing's paper. We are very happy to be working with Open AI at Microsoft, we have a deep 
strategic partnership with them. And a lot of this work happened on the Azure platform itself.  
Great progress, but we also have lots of lots of limitations in the deep learning approaches 
that are giving us impressive results. I'll talk about some of them, which I have listed here. Well, 
number one - the amount of data required to train these models is just really just too much for us 
to get this level of performance. And we know that humans can do very well, without using that 
much data. The models themselves are opaque all the learning itself is sort of encoded in these 
multi-dimensional vectors, which is no one can make sense of. We have this problem, which 
humans don't have, that these models are trained offline and then used for inference, but there isn't 
this notion of continuous learning. There is no semantic understanding, I'm going to talk about 
some examples of that. The power required to train these models is incredible. Estimations on 
GPT-3, which I have read on the web take us into megawatts of power for training these, while the 
human brain, seems to do fine with about 20 watts of power. So there are many, many different 
limitations. All these models are trained for a fairly narrow set of tasks, though, with GPT-3, we're 
starting to see that change - these are multitasking. Models can be built on top of the single 
representation that is learned, which is quite impressive. So you can see we've made a lot of 
progress in the 70 years, but there are so many other hard things that we need to solve, including 
causal reasoning.  
So, what I talked, how powerful the GPT-3 model is, but it also has lots of limitations. If 
you ask GPT-3, which is heavier, a toaster or a pencil, it says a pencil is heavier than a toaster. 
When asked when counting what number comes before 10,000 it says 9099 comes before 10,000. 
When asked who was the president of the United States in 1700. It says William Penn was the 
president of the United States in 1700. Okay, let's examine these three questions. Well, the first it 
got wrong about pencil being heavier, was because in all the 5 billion tokens of texts that were 
there, nowhere, it had actually had a direct reference to the weight of pencil and a toaster. So it got 
it wrong. When it comes to counting, it actually does not have an underlying idea of the rules of 
mathematics. That's a lack of semantic understanding. Therefore, it basically tries to find a close 
answer and gets it wrong. In the third case, this was a bit of a trick question, because the United 
States did not exist in 1700. But it made up an answer. It picked a famous person from the 1700 
period history. All these examples are basically telling us that while we have clearly made a lot of 
progress we fail on some very fundamental things. If you ask a nine-year-old kid the answer to 

5 
 
these questions, they will probably get these answers right, which this amazing model could not 
get right.  
Now, Marvin Minsky, who can be called one of the elders of artificial intelligence, in 1970 
said that in 3 to 8 years, we will have a machine with the general intelligence of an average human 
being, this was 50 years ago. So, predicting, when we will have the average intelligence of an 
average human being is a very, very difficult task. If Marvin Minsky got it wrong, most of us will 
probably get it wrong trying to predict this. 
It was very interesting for me to read another prediction by the Turing Award winner this 
year, Geoffrey Hinton, who's considered as one of the three, I would say, fathers of the deep 
learning movement, who made a statement, that deep learning will be able to do everything. Which 
made me think, could this be true? And I concluded that it is, that he's probably right. And one 
very, very fundamental reason, is that deep learning has given us an approach, which allows us to 
do function approximation better than we have ever been able to do. The place where I feel his 
statement is ambiguous and maybe it was intended is that I believe that some of the approaches 
that are going to be needed to get to artificial general intelligence are going to come from the 
symbolists and the dynamicists, even though some of their ideas may be solved best with deep 
learning approaches. So, that's kind of where I see things going.  
I feel that if you look at where do we need to make tremendous amounts of progress if we 
are going to get to the sort of the human level of intelligence. Number one, I think, model-based 
notions of space, time, and physics, this can also be called human common sense. A little child 
doesn't need to learn these again and again. If you throw an object they've never seen before, up 
in the air, they know what's going to happen to that object. They certainly have notions of time, 
they have notions of space, these are fundamental ideas, that somehow need to be acquired and 
encoded once in AGI.  Knowledge representation and organization, we are starting to make good 
progress here. Not just for language, this needs to expand into numbers, into graphs, into pretty 
much how humans have this broad notion of knowledge that we acquire and organize and recall. I 
think reinforcement learning is very, very important, maybe coming more from the dynamicists 
side, I think it's an important approach for ongoing learning, if you look at how children learn, a 
lot of those approaches can be considered as online reinforcement learning. I think causal inference 
work is a critical block that is largely missing today in the popular AI narrative. I'll also emphasize 
sparse learning because I think that this power-hungry approach to building these really deep and 
high parameterized models is not sustainable from an energy perspective. In addition to the 
performance, we need to emulate human brain efficiencies here too. I think it's something that 
needs a lot more attention, we started to see some, some really interesting work here.  
But Lastly, I would say, I think if we are going to make progress towards a truly thinking 
machine, it will require the connectionists, symbolists and the dynamicists to all come together. 
And perhaps using some of the constructs that we've got from deep learning to solve the problems. 
For example, if you look at the symbolists their entire thinking is based on the idea of symbols that 
are the basis used to construct all the different layers above it. But symbols don't necessarily have 
to be symbols that humans also recognize. In fact, you could argue that some of the breakthroughs 
in GPT-3, which came through learned representations in the latent space with vectors, you could 
say, are symbols too, it's just that we just don't recognize them. Similarly, deep reinforcement 
learning. With neural ODE work, we're seeing how neural architectures can be used to learn the 
dynamics of a continuous system instead of using a differential equation. So I think this is where 
we are, these three disciplines working together, using neural nets as a powerful building block, 
we will have a truly thinking machine. Hopefully in our lifetimes. Thank you very much! 
 
[Albert Efimov] So we keep our discussion panel chewing 70 years of publishing Mind 
article. Right now I would like to introduce our guests who are joining our panel. Our first guest 
is Mr. Dr. Leonid Zhukov, who is the head of the Artificial Intelligence Laboratory in Sber. And, 
well, our closest collaborator on many fields of artificial intelligence, which is running here and 
you can learn about activities of his laboratory, I think, everywhere at this conference. Dr. Leonid 

6 
 
Zhukov joins Sberbank quite recently from high school of economics, where he was one of the 
heads of faculties for computer science and artificial intelligence. So we are very happy to 
welcome Dr. Leonid Zhukov to Sber.  
Also today you already hear Michael Wooldridge, professor from Oxford University 
computer science department, head of computer science department. And Michael is the author of 
a wonderful book, best Book of the Year by Financial Times. And I'm very proud that I read it 
from the cover to the end of the book. Please buy his book at Amazon or maybe in Sber shop, 
because it is really worth reading.   
But before discussing this with Gurdeep, with Michael, or with Leonid chewing 70 CAN 
MACHINE think. I thought that I need to give a context of our discussion because this is really 
important. Probably I forgot to introduce myself to those who are not really familiar with me, but 
maybe some of you at least, my students might be familiar with me. My name is Albert Efimov, I 
am working in Sber for more than three years and now I'm heading the R&D block for the whole 
Sber with the vice president title. I wear many hats in my career and I would say I'm just a mature 
scientist and R&D manager here in Sber and trying to be very useful for everybody. So back to 
our topic. 
Computing Machinery and intelligence was published first time 70 years ago. It’s quite a 
long time, many people might remember the epoch when we have no computers, at least my 
scientific advisor for my Ph.D. was born long ago before Turing published his thesis. But it's very 
important to look at the past because we recall the past, it serves the present. Today, it gives us 
fresh insights on what we sometimes overlook, and cause our attention to ideas that might be 
missed for some reason, it might give us new perspectives. And that's why we are looking for new 
ideas. 70 years passed since Turing publish this article in October 1950. So it's really worth 
discussing.  
But first of all, let me remind you, who was Alan Turing. He was a scientific prodigy, some 
of his friends in childhood called him scientific Shelley. At the age of 14, he rode the bike 120 
kilometers to his new school. And next year he actually published a small article dedicated to 
Einstein's theory of relativity. In the picture here you see a drawing by Turing’s mother Sarah, and 
it's on playing a game, but he actually gave up the game and just looking at how daisies grow. And 
this is all true and he always did something contradictory to others. Later on in his life, he was 
famous for very ugly trousers. It was not fashioned at the time in England. Trousers should be 
always very, very well ironed and he always used the wrong trousers, so everybody actually paying 
attention to him. But he was still very much a scientific prodigy. And way early, he was elected as 
a fellow in King's College, Cambridge, and published a paper on computable numbers with an 
application to engineering problem, which is the problem of computability, which got the attention 
of Alonzo church, and he's got his Ph.D. visit to the United States to work for his Ph.D. with 
Alonzo church. He finished it, returned to Cambridge, and actually come to L. Wittgenstein to 
have some discussion on intelligence and the foundation of mass with him. And as soon as World 
War Two broke, he joined the government to court and the cipher school as one of the leading 
coders, code breakers. Foundation of peace, which we are enjoying right now it was actually late 
at the time. It was 1940 when he traveled to France, during the war to meet this Polish crypto-team 
and then he established the so-called hub-A team, which helped actually not win the war but to 
save an enormous amount of lives for the fight of England and North Atlantic. In 1941, he breaks 
transmissions and Bletchley Park where he was allowed to work was working was reading all the 
messages as instantly as German was typing them. In 1942 he created the first automated machine 
to read German messages and it helped to defeat Germans in North Africa. Also at the time, he 
visited the US again and meet Claude Shannon in Bell Labs. In 1943 he works on the first things 
award speech encryption system and Bell Labs and then returns to the United Kingdom, where he 
built the world's first electronic computer Colossus on the premises of Bletchley Park. I advise 
everybody of you to visit Bletchley Park. 1945 he completes his speech encryption system, but 
there is no use for it because he celebrates W-day in May 1945. And then he immediately travels 
to Germany to study cryptology, as well as to deliver a lecture. 

7 
 
This picture shows you how it looks, bless your pockets of time. It was actually the head 
of Alan Turing. And after the war, It was mentioned that he was one of the founders of the Ratio 
Club, which was founded in 1948, and it was very multidisciplinary and it was very much 
dedicated to new ideas. So, it says that no professors only young people were alone and Alan 
Turing was a kind of celebrity there. So, a lot of new ideas were proposed during ratio club, his 
days, and he actually, many, many later ideas he developed them. So, in 1946, he started 
developing the first electronic calculators in the NPL lab, which is in London, and also start 
running marathons, 30 kilometers. In 1947 he visited the US again, meet with some US scientists 
including Fon Neiman. And this meeting was actually laying the foundation for the future study 
of Fon Neiman himself, and Fon Neiman architecture. Then, after his, I would say not successful 
presentation to NPL - National Physical Lab, he decided to move to Cambridge for a sabbatical. 
One of his bosses at NPL said on Turing's report on artificial intelligence that it's a schoolboy 
essay and not suitable for publication. That's where well-known fact in Turing historians, and it's 
actually it should be very much encouraging for all young scientists, please keep working and don't 
be upset if old guys like me or somebody else criticizing your ideas, which you are presenting to 
them on internal workshops. So please, please be ready for critics and not afraid of it. 
In 1950, he wrote the first programming manual and then complete publishing Computing 
Machinery and Intelligence in Mind journal. Mind journal is a philosophical journal. And that's 
why Alan Turing become a father of AI, that he also bought the house in Manchester to be headed 
there. 
Now I come to the foundation of life and eternity. In 1951-1952, he gives a few talks to 
BBC on artificial intelligence with some predictions. And then he switches to actually biology and 
I think he also published the first paper dedicated to bioinformatics. So he might be also as well as 
the father of bioinformatics. 
Many modern historians believe that due to an accident, not suicide, but an accident, he 
died on June 7, 1954. Just a little bit not leaving to his 42nd birthday. So he left no note that’s why 
many people believe that it was just a tragic accident. I strongly recommend you to if you are 
willing to know more about Alan Turing, read this book written by one of my good Kevin Valley 
as well as of course, Michael’s book and some other books, which are available widely. So, I have 
some further ideas, but first, just let me tell you two major ideas which gave us Alan Turing. The 
first is the universal Turing Machine and the second is Turing test which laid out the foundation 
for computer functionalism. What is computer functionalism's basic idea? 500 years ago Rene 
Descartes believed that humans are machines, might be machined, animals are machines. But 
Turing and later, philosophers who created computer functionalism believed that humans are 
machines with software. Brains are hardware and software is running on them is what actually 
makes us humans and he created a Turing test, which probably many of you heard of. But universal 
Turing machine as well as a very important one because every machine sooner or later become the 
Turing machine. 
It means we have an emulation of everything possible with that simple Turing machine, 
which is actually going back and forth on the paper, and print, erase and print again. That's it. Very 
simple algorithm, a very simple machine, but it can emulate everything. So what we do, what 
Gurdeep is doing what, Leonid is doing right now is just make Turing machines a little bit faster. 
And that's it, we are not doing anything more.  
So going back to the Turing test is enormously popular. It's millions of mentions on Google. 
Its foundation of Imitation Game, where we can think on, CAN MACHINE think comes from this. 
But we discussed it might be in more detail. I don't want to spend too much time on some of my 
slides. And I go to questions for our discussion, which are very important, and I see is that my 
friend Gary Brodsky, also joined us. So we welcome Gary to our discussion.  
Gary Brodsky is one of the leading members of the team, which won the grand 
programming challenge in 2005, and then sold this team to Google, he later sold another startup 
to Google. And Gary is famous for being a founder of Open CV. And Gary has got an enormous 
amount of friends here in Moscow, Russia. And actually, every young man and girl who are 

8 
 
working on computer vision starts with Open CV, which was developed by Gary team already 20 
years ago. Gary also has got many interesting ideas on what is computers right now, what is 
computer science and artificial intelligence.  
So altogether, we have four great experts, one of the world's greatest experts. You see, I'm 
like, I don't understand what happened to me why I'm so lucky and happy to have you hear from 
our panel. And I put in front of you actually six questions you might see right now. But two 
questions, actually, most important for me right now. The first question is, what was wrong for the 
last 70 years in search of artificial intelligence and thinking machines? Would be Turing surprised 
if he comes today to our conference and sees what is going on? And what his surprised the most? 
And second important question, what should we do to make our research our policy right for the 
next 70 years of research? What we should do right for the next 70 years, and what mistakes we 
should certainly avoid?  
 
[Leonid Zhukov] Thank you very much! Well, I'm not sure if it was wrong for 70 years, 
right? We definitely got a lot of achievements in those 70 years. And the breakthrough, for 
example, in of course, in deep learning, and the way we heard today, in all our presentations, it's 
actually proving it. But at the same time, it could be that we interpret it, the paper, the ideas. If we 
interpreted the ideas slightly differently, history could also be different. I think we should start 
with this notion of intelligence, right? And what is intelligence? And I don't want it to be like 
philosophical here. I want to be very, very practical, right? And so intelligence is what?  
When I think about intelligence is really the ability to set up and solve problems and 
achieve certain goals in the real world. And that's it, right? Artificial intelligence - it's a computer 
or program that can do it. Nowhere in this definition, have I ever said the word human and 
somehow going back to the Turing test, we put machines in these unfavorable positions. To pass 
Turing tests, they need to know what humans are, they need to know how to simulate and mimic 
and behave like humans. At the same time, intelligence really doesn't mean to be human. 
Intelligence means the ability to set up and solve problems. And so I don't think we went in the 
wrong direction. But part of the effort could have been avoided in the sense of trying to make 
machines that executives mimic humans.  
Now while doing so we, of course, make huge, interesting discoveries and learn a lot about 
humans and learn a lot about like, for example, human brain structure, etc. But in general, setting 
up a goal of building a machine that mimics humans might not be the right goal. And going 
forward, I think we should focus on things like actually building machines that solve things, that 
can set up problems and solve them, and not necessarily the problems that humans will be able to 
solve.  
In fact, why do we need machines to solve problems that humans can solve? So we need 
to have machines that will solve problems that humans cannot solve? And we need to go and, and 
look in that direction more than trying to replicate humans. And though humans and our brains are 
probably a huge inspiration for computer scientists to build machines, they're insanely complex, 
and they think trying to reconstruct them. In the end to reconstruct, for example, the continuous 
way the brain operates in our sort of digital binary zero ones, that computers that the right, this 
probably is not going to work out.  
So bottom line, I think, we need to refocus a bit. Understanding intelligence as ability, 
again, to solve the setup and solve a real-world problem and work into solving intelligence in this 
sense, and not in replicating how human brains work, and what we as humans can do. Thank you! 
 
[Albert Efimov] Leonid, I have a question. Can you give us an example of the kind of 
problems that humans cannot really solve?  
 
[Leonid Zhukov] For example, genomics right now. We do have a lot of things we cannot 
solve, we cannot, for example, map precisely genes on Gen X and the phenotype genes onto precise 
diseases, right. There might be a reason for a huge number of possibilities, or maybe we're 

9 
 
approaching it in the wrong way. And I think the great advantage that could come from AI is being 
able to solve the problems differently than we're thinking about that. That's one thing. 
Another thing is, I think, and as a professor, you perfectly know that. Solving problems 
when it is well-posed, positioned well specified. I mean, when you specify the problem really well, 
it's not that hard to solve it, it's actually hard to formulate the problem precisely that it is solvable, 
right? And so if machines learn how to actually formulate problems before solving them, right, 
that would be a huge advantage. So and that's what AI cannot do like AI cannot formulate tasks 
for AI to work on. 
 
[Albert Efimov] And I cannot put any goals. And that's a big problem for artificial 
intelligence right now. Because goal setting, aim setting, the target for moving forward is the 
biggest problem for artificial intelligence right now, I think. I wrote a paper on this, too. Do you 
think Leonid that deep mind alpha something with molecules might work for application AI in 
science?  
 
[Leonid Zhukov] Well, it's actually a very good example, you're talking about the recent, 
this, the folding example, right, with folding. This is actually an amazing example. And it's also, I 
would say it's on the same level for me, and using it for me, as GPT-3 when you interact with it, 
but the thing is, the truth is – GPT-3 has no clue what it is talking about, right? And so, it learned 
the statistics of the sequences, right? And honestly, if AlphaGo by learning the certain statistical 
patterns is can you manage to predict the right protein folding that allows us to build, for example, 
new medicines. That's fantastic. That's terrific. Does it bring us closer to like AI in terms of 
understanding? NO. 
 
[Albert Efimov] Thank you very much! So, actually, Gary, I would like you to join our 
discussion, and also to elaborate on those two things. What was wrong, for the last 70 years in 
terms of our quest for artificial intelligence, and at least I know two things, you sold two startups 
to Google as a good one. But what was wrong? And what should we write to make it better? You 
sell another startup to Google, maybe? And? 
 
[Gary Bradski] Okay, can you hear me? 
 
[Albert Efimov] Yes. Excellent. 
 
[Gary Bradski] Okay, so what was wrong? Well, early on, we didn't actually have a lot of 
computing for number one. I mean, our brain has a fantastic amount of computing in it. We're still 
not at that level that's probably available anywhere, but we're getting there. There are some 
understandings of neurology, there’s also what the idea is of intelligence, what it really is? I mean, 
people got enamored with logical proofs and other things and this isn’t a large part. A large part 
of what our intelligence is – is simply survival as a simian in an environment for whatever reason.  
So the goal that we had set out to do was largely wrong. Logical proofs - we didn't have 
the compute to do it, we didn't have a lot of understanding. There weren't any kind of interesting 
machines, like robots that were even capable of hardware. So, what was wrong? It Was too early 
and naiveté.  
So, what do we do? Well, it's, um, it's beginning, to get a little better than that. I started my 
academic career in neural networks and those of us working in it, like an early friend of mine. I 
contacted him when I was in graduate school because I wanted a corner detector that he was 
working on. But we were working on neural networks when we knew that was the kind of structure, 
not this symbolic AI. What we have right now it's a fantastically useful tool, but I call it's still 
basically, deep associator, it's not intelligence itself. And intelligence itself is a little bit difficult 
to define. 

10 
 
I think there's right now you have to understand, what a mind is, and what a mind can do. 
And I think there is a lot of misconceptions. Can a mind live forever? It's pretty clear – NO. Not 
in any, not in principle, because the mind is built up of structures and those structures suffer a kind 
of decay of when you are supposed to represent distributions, and they become overloaded.  
But an example I'm going to use for another talk is when you're building a robot, you have 
to build a mind. And that mind explicitly or implicitly represents the world and the robots. And 
you're forced to do that every time you want to make something operate in environments. And 
then the very primitive ones have a very primitive model.  
But some of them that I've worked on, let's say, a Stanley, which is a simple example. What 
it did is that it had sensed in the various modes of sensing lasers, GPS, wheel, odometry, whatever, 
in an environment, and it did it and vision and it fused all this information into a world map. And 
then it knew the direction of gravity and in that little world map, so it had this model of itself. It 
had the world and the world was very simple. It was bad things, good things, and things I don't 
know. Then it ran that in a physics simulator.  
Another thing people don't understand - we aren't our intelligence. We are our emotions. 
Intelligence is a how what is always by your emotion. For example, if you love computers, given 
a certain intellectual level, you're going to become very good at computers if that's what you really 
love.  
Stanley's emotion system was GPS waypoints that's what drove it. It wanted to get from 
one point to another safely. And so that was its entire mind. Its mind formed the world of bad, 
good, and I don't know. It knew where it wanted to go next and it's simulated itself going there. 
And then it would actually take that step in the real world. We’re very much like that. You see 
your simulation when you're asleep and I'd like to say when you wake up, your dreams don't stop. 
They just connect to a data source, which is the world. But you look at Stanley's mind and this is 
all mind, so it's a very simple mind. But what can it understand? All it understands is kind of like 
bad, good, don't know. Well, if you want to explain Dostoevsky's Crime and Punishment, what is 
this when the person takes a very bad rash move. Well, a rash move to Stanley, if you wanted to 
explain this Stanley, the robot, a rash move would be cutting across the I don't know the land to 
take a shortcut to not go the long way around, but jump to a further waypoint. Well, that's a rash 
move. And so you could explain those Dostoevsky Crime and Punishment to the robot, but not 
really, his brain cannot understand this, because it's not rich enough, well, to rounding error, that's 
us. 
So we will never understand the world under this theory, but we can create these self-
operating agents, who have a certain level and can do certain things in the same way. If you have 
your cat, it will never understand Dostoevsky. It will never understand it because it doesn't have. 
But if you could talk his language, you could say, well, it's like a kitten that doesn't listen to its 
mother, and it's going to get in trouble. And then the cat will say, I understand you go. No, you 
don't.  
We're the same, like two rounding errors where the same will never understand the world. 
All we get is a model of the world. And that model is a causal model that like Stanley that allows 
us. Stanley needed to drive across the desert, we need to operate in a social simulation. We will 
never understand the world and physics, there'll be no final theories because we don't even see 
what needs to have a theory of it. We have none of that circuitry. And so we can create these 
machines when we can create a machine that has this interior model, that socially simulates and it 
will become conscious and aware. And it might be more powerful than us and we can say well, 
what's the real structure or meaning of the universe? And it will say well, it's like that's going in 
danger, we won't be able to understand what it understands after a certain level. 
 
[Albert Efimov] I totally agree with the last point. We will not be able to understand the 
conscious machine if ever be conscious. At this point, I would like probably to return to one of my 
slides, which I prepared for our talk.  

11 
 
Turing Continuum is from the virtual world to the physical world and from nonverbal 
interaction to verbal interaction. Turing was concentrating on the left upper corner of this with this 
noble task of solving mathematical tasks, cryptography, talking, playing chess, learning languages, 
whatever they're doing. But he gave no attention to the physical world because he considered it 
too complicated for realizing and robotics. I think that what I heard in Gurdeep's talk before, and 
what I heard right now in Gary's talk that the physical world and interaction with the physical 
world in a verbal and nonverbal way is might hold the key to creating a thinking machine. It's my 
article position. And of course, you can argue with this.  
But, Gary, I would like a little bit of you to elaborate very briefly about very clearly - what 
we should be avoiding for the next 70 years what is dangerous?  
 
[Gary Bradski] I think what's dangerous is not going fast enough with AI. Well, if 
humanity's existence depends on survival depends on getting these techniques fast for design and 
whatever. I'm not one of those that are worried that AI is going to kill us. I'm worried on the other 
side that we're not doing it fast enough. Also, I'm not worried that AI will kill us. We're a 
technology, the primary thing of our species is we create technologies if that's not stable and 
survivable. So what, then we die. That was what nature intended because that's what we do. And 
yet, like we can't, we can't shy back from this, we have to accelerate it, in my view. And I'm not 
really worried. Also, if AI turns on us and decides to destroy us, well, then we succeeded, right? 
We created another species that can live without us, that'll be our greatest day. So I'm on the total 
opposite side of this like we should be applying much more efforts and funding and acceleration 
and just go. Go all out, we need the robots, the human labor is failing around the world. That's not 
there's not a population problem. There's an under-population problem that's rapidly going to hit 
us. I mean, it's going to hit here. Yeah, it's going to hit us like a freight train. And we need the 
labor. We need AI's ability to solve environmental problems. Global warming problems, energy 
problems, like we just cannot go fast enough. 
 
[Albert Efimov] Thank you very much, Gary! 
Turing Continuum also allows us to do one thing. I thought out all possible Turing test 
around this Turing Continuum, and you can see is that all Turing test which was created for the 
last 70 years’ group up in the upper left corner, and almost nothing in the physical world and 
nonverbal interaction, which is also very important, because the dark matter of culture is what is 
our I would say nonverbal interaction might be as important as verbal interaction chatbots and 
some other things.  
I would like to hear Gurdeep Pall. I listen to your presentation as carefully as I could, in 
terms of I'm also one of the organizers of the AI journey. And I know that Microsoft is betting on 
GPT-3. It's good to know that. But that's when I also know for sure, and I think Gary mentioned it 
a little bit, that GPT-3 is at least not energy efficient, as well as all other frameworks. So my mind 
right now is consuming only 25 watts of energy. I'm not that smart as GPT-3, maybe, but I'm 
extremely energy efficient. So for today, I have only one cup of coffee, and it's just enough for me 
throughout the whole day. And robots need a battery replaced every 8 hours, at least in our 
Sberbank mobile robots. Robot battery is replaced quite often. So Gurdeep, do you think that this 
bottleneck, the energy bottleneck is a very serious bottleneck for the next 70 years of development 
AI? And of course, you can also help us to liberate what was wrong for the last 70 years. So, 
please? 
 
[Gurdeep Pall] I was thinking about your question about the last 70 years and I would say 
that humans are very resourceful creatures. We approached after the Alan Turing famous paper. 
We approached artificial intelligence in a very resourceful manner as well. What I mean by that is 
that we basically immediately looked at the problem and said we understand the logic, we 
understand rules. So we took that bit, and we ran with it. Then we realized we can only get this far 
with artificial intelligence, with the sort of rules-based approach, then things sort of quieten down. 

12 
 
Then we realized, well, we know math pretty well, so let's start using some of these math-based 
techniques: k-nearest neighbor, some of the clustering approaches, and sort of this classic data-
driven math. So we're pretty resourceful creatures. And now because there is so much digitized 
data, and there is so much compute, of course, we are leaning heavily on deep learning methods. 
And GPT-3, I completely agree with you. It reminds me of this expression more trust, Scotty: 
which is basically - let's throw more and more at it in terms of compute and data, and, of course, 
the performance is going to get better. So that's the sort of the journey of the last 70 years.  
But obviously, this is not a sustainable model as well. And I think that, if there is one thing 
I would look at, sort of critically in the last 70 years is that we tend to get obsessed in the 
community with one method or the other. We go through these different waves, and right now we 
are on the deep learning wave, which I'm a huge fan of, and I think, as Gary said, it's deep learning, 
and these neural network architectures are great function approximators. But we have to take a 
step back and say, what are the big gaps that exist? And are we making sufficient progress in 
addressing those gaps, versus just pushing forward with more energy and more data? Towards one 
of my last slides, I talked about some of the areas we need to make progress on specifically with 
the area of power consumption. I think since we are inspired by the brain, let's look at the human 
brain and see how it deals with work so efficiently. And I think part of it is that it is actually 
reasoning with a lot of what we could call model-based approaches, where it doesn't need a lot of 
data to understand, that if I throw an object up - it's going to come down because it has fundamental 
notions of physics. Similarly, I think, some of the sparse network-based approaches are something 
that, I think, is pretty important that we think we need to pay attention to even just because we 
have power doesn't mean we should go waste it. The human brain, for example, does amazing 
things, even on power, with things like heavy and sort of learning. And when it finds like two 
adjacent neurons firing it sort of short circuits them. So, I think that certainly we should be 
impressed by GPT-3, but I think it is just a very small stop on a very long journey. And we need 
to always keep these things in perspective. What are we really trying to solve? What are we 
optimizing for? As I said earlier in the talk, I think if we define that clearly, I think in the next 70 
years, we will make tremendous progress. 
 
[Albert Efimov] So now, I'm going to you, Michael. Going to my two original questions, I 
would like to say, what was wrong for the last 70 years? By the way, what is the distance between 
Oxford and Cambridge? About 60 miles? 60 miles?  
 
[Michael Woolbridge] Well, that's the physic the physical distance, the emotional distance is far, 
far greater than that. 
 
[Albert Efimov] So this emotional distance between two schools actually quite important 
right now. But going back to the question, what was wrong for the last 70 years, except the fact 
that Alan Turing was from Cambridge? 
 
[Michael Woolbridge] I think there's that there's a couple of things. And I think my answer 
will resonate with what Gary said and, and, and others today. So the first thing is, I think there's 
an obsession in AI historically, with discovering one idea, and thinking that one idea that one 
technique is going to be a magic ingredient, which takes you all the way. And so as you said, 
Albert, as Gary said, I mean, for a long time, it was the idea that knowledge, knowledge 
representation, we just write, we need the right way to find human knowledge and to write that 
down. And if we can just give that to machines, then that will take us all the way. So I mean, there 
was a famous experiment, the psych experiment, where they tried to code up the entirety of human 
consensus knowledge. And the idea was, if you could give that to a machine, then AI would be 
solved. So it's that idea that there's one kind of one single technique we have one advancement. 
Wow, this is it. This is the magic ingredient. And that's going to take us all the way. And it doesn't, 
right. I mean, it just doesn't. And I guess I'm very I'm hugely impressed as I'm impressed as 

13 
 
anybody is by the current advances in AI, the things that are going on in deep learning. These are 
real, and they cause excitement. I mean, I think they are real advances. But they are narrow 
advances. And they are ingredients, but they're not the whole ingredients. They're not going to take 
us the entirety of the way to artificial intelligence. So I think that's the first thing that's the first 
lesson, right? Don't imagine that there's one single ingredient that is going to take you all the way. 
Second, I think it's quite interesting the examples we've seen an example on your slide 
things like chess and GO. That why do AI researchers studying problems like chess and go and 
proving mathematical theorems. They do it because those are the things that they regard as 
requiring intelligence as requiring genius, right? Those being a good mathematician is something 
that a lot of computer scientists aspire to be. So they look at those problems. And they said, Well, 
if we can solve, if we can prove mathematical theorems with a computer, then we must have solved 
the intelligence problem. But actually, the truth is, that's not where a lot of the hard problems are. 
So to go back to the driverless car problem that Gary was talking about earlier, and I'd be interested 
to see whether Gary disagrees with me or not, what is the hard problem driving driverless cars? Is 
it knowing whether to speed up or slow down or to turn left? Or to turn your indicators on? No, I 
don't think so. The problems with driverless cars are knowing where you are, and what's around 
you what's going on in your environment, right, all of the problems, that my guess anyway is that 
Gary and his team had to solve back in 2005, what to do with perceiving your environment if you 
have a lot of information, then knowing whether to speed up or slow down and so on, is going to 
be easy, actually, that's that part of it, I think, is a relatively straightforward and conventional bit 
of computer code. And so perception, and actually, that's where neural networks, the current wave 
of AI technology, that's where it's turned out to be very, very good at dealing with problems related 
to perception, understanding problems in computer vision, in understanding speech, and so on. 
These were fearsomely difficult problems. And that's where that technology is proved to be very, 
very successful.  
The third thing, I think that the third historical mistake is not understanding that where the 
difficult problems are, in the world. Right? We have evolved over billions of years to succeed in 
inhabiting the physical environment that we inhabit the planet Earth, the narrow bit of planet Earth 
that we do actually inhabit, imagining that you can build a neural network and train it up in the lab 
over a couple of days and that that's going to be as good at dealing with the world as we are, I think 
he's just a huge mistake. So the world is really important. Let's go back to GPT-3. So what is GPT-
3? GPT-3 is this program developed by Open AI, I believe, and it was trained by giving this 
program huge numbers of texts, vast amounts of written texts. So let's take an example something 
that you might want to do something that Gary might want to do when for breakfast is make an 
omelet, right? The completely routine task for a human being - making an omelet. So GPT-3 is 
read every omelet recipe that's ever been written, right? It's there's no recipe for omelets out there. 
It's written every is read every essay about omelets. It's probably read books about omelets. So in 
terms of just knowledge, it must surely have all the knowledge about omelets that there exists in 
the world. But could it make an omelet? No, of course, it couldn't. It doesn't know anything about 
omelets, because of all the things that exist in the real world. For it, an omelet is just a symbol. But 
it's seen time and time again, in these huge numbers of repositories that have been thrown at it. 
None of that knowledge that is coded into GPT-3, none of that stuff is actually grounded in any 
experience of the world that we all have. For me the word omelet represents every experience I've 
ever had with an omelet, every omelet I've ever tried to make, every egg that I've ever tried to 
break to go into an omelet every successful or bad on, it reminds me of an omelet I had in Paris in 
1997, and so on. Right? In that sense, the concept of omelets means something to me, because it's 
grounded in my experiences with the world. GPT-3 doesn't have any experience of the world to 
ground itself in. So in that respect, it's just a disembodied I mean, it's very quick to be clear, it's an 
incredibly impressive feat of engineering. And people will do really, really cool things with it. But 
it doesn't understand I think just to reiterate the point and I say it doesn't understand because it 
can't, it doesn't have any experience of the world.  

14 
 
So those are the three things. I think we're obsessing on one idea and imagining that there's 
one simple single technique, which is going to take you to kind of the holy grail of AI. That's a 
mistake. Focusing on problems, which actually, you might find impressive as requiring 
intelligence in people but actually is not where the real hard problems are. I think that historically 
there has been a mistake. And finally, not realizing the importance of dealing with the world. Now 
experience, human experience, human knowledge. Everything about the human condition is 
grounded in our experiences in the human world. And I think, too, if we ever succeed in building 
machines that are self-aware and conscious and centered, and all of those things, machines that 
have understanding, then that understanding will have to be grounded in the world in the same 
way. 
 
[Albert Efimov] Thank you very much, Michael. So going back, to some ideas, which I 
said before that you seem to think I don't know if you see this slide. But Alan Turing says board 
games, talking is very important for artificial intelligence. But he says also there are real human 
pleasures - food, sport, and sex. And those are very important actually for understanding the 
problem. And my question is, how important embodied intelligence to study perception and 
cognition in machines? And how important it for research of general artificial intelligence and 
artificial intelligence with specific applications? So can we create artificial intelligence with no 
embodiment? Or we really need embodied machines to advanced fully in that direction? So who 
would like to answer this question?  
 
[Leonid Zhukov] I'll be short, I actually want to support Michael, in the sense that to build 
like true artificial intelligence - machines should gain the experience that human has, right? And 
what's the best way to get an experience than to explore the world? And to explore the world? And 
to do it autonomously? Yes, you need to have those types of systems, those embedded systems 
with intelligence. And when you look at them, for example, the Robot-dogs running around outside 
Boston, you realize that we're actually pretty close to the phase where systems we'll be exploring 
the world on their own, gaining the experience, learning what the omelet is, eventually, and 
hopefully will not only read about it but try it. 
 
[Michael Woolbridge] I'm going to bring in a slightly different sort of aspect to this 
problem. So there's a famous six-word exchange, that was formulated by Steven Pinker - the 
psychologists and linguists. And it goes like this, Bob says: I'm leaving you. Who is she? And so 
the six words, right, and everybody who hears those six words, immediately has a rich mental 
picture of what's going on, as people will understand that right? Maybe somebody listening to this 
actually just lived it last night? I'm very sorry if that's the case. So what's going on there? How do 
we understand that? We understand that because we've got experience of the human world, we 
understand human relationships, human beliefs, human desires. And nobody trained us in that, 
that's just our experience as human beings living as social animals that have relationships with 
people in the human world. Now, this is a big problem for AI. Suppose that you're talking on the 
telephone to an AI system, which decides whether you get a loan or not. And it says: no, you're 
not going to have a loan. It must surely be if it's going to be any use, it has to be able to understand 
that this is going to make you upset or angry, and so on. I mean, an AI system that took the place 
of the human doctor would have to be able to understand the intricacies of human relationships, 
because being a doctor is not just knowing about diseases and illnesses, it's knowing about people. 
And the lengthy training process that doctors go through is every bit as much about training them 
to deal with people and to understand the people that they're dealing with and what kind of 
treatment regimes are going to work for them and so on. As individuals understanding their 
personal circumstances, their relationships, all of that kind of stuff. So we all have that because 
we've been trained both – genetically, through billions of years of evolution, and since birth, where 
our parents teach us right from wrong, teach us about what's acceptable in relationships, and so 
on. We'll learn about that because we're part of the human world. How are we going to give that 

15 
 
stuff to machines? That is big and technical it's called the theory of mind, right? How are we going 
to give machines a theory of mind?  
 
[Gary Bradski] I'm unclear whether embodiment again, there are many kinds of mind. 
Right. To the extent, we can be disembodied, but embodied in a virtual world, right. So, it can all 
be happening in a cloud, but I do think embodiment does give you certain things which are primary 
tests, like the fear of death, need to survive and that's a lot of what we're based on. We form this 
causal model that's necessary for our existence, and then we press that causal model for everything 
else we think we understand.  
If we want AI's, intelligence that is going to be useful to us - they must share certain 
embodiments and the same kinds of things that we do. And it's unclear, how much a giant body of 
associations. GPT-3 it's kind of shocking how far it can carry you and yet there's like no sold in 
the structure, it's just a chain of associations, right between them. It has all these brittle points and 
unexpected failures that are hard to predict. Versus with us we're kind of regularized in our 
environment by our physical need to the understanding of how we would get damaged, how we 
would survive. What's particular with humans, is a large part of our mind is devoted to a social 
world, which is what we created. And so we're always modeling our place in the social world, and 
how to interact and survive with other entities in it. And that's a very key thing for us. In fact, it is 
a large amount of our whole brain and understanding. And again, you see this social world in your 
dreams, you fully simulate other people and your interaction and your hierarchical place with them 
in your dreams. And so a large part of our mind is devoted to that. And I think like if we want 
robots, I mean, we want intelligence, they have to sort of be embodied with us. And if we want 
them for the human factors, there are many things like protein folding, and whatever, that are a 
very specialized area. We just want the machine to do that. Again, such a machine really needs to 
explicitly or implicitly embody a kind of internal causal model of folding, if we really want it not 
to just be associational, all to sort of mostly like getting a lot of stuff, right, but not really 
fundamentally understanding the 3-D chemical world that these things are in, it will just be a kind 
of weird association or manifold over chemistry without the sort of physics and the causal physics 
that it really needs to understand that. In that sense, it should embody a causal model of the world 
upon which it grounds its meaning. And that's what GPT-3 lacks, it lacks any of this grounding of 
meaning. 
The problem isn't that models lack. This is that you want a model that spans your space 
because we're always going to lack. No superintelligence can know everything and every aspect, 
it's always going to be grounded in a causal situation. And when you try to go too far out of that, 
it's just going to break, it won't, you won't be able to expand this mind indefinitely, it will be based 
and built upon a causal structure. And that will be its limits forever. You need a new mind to go 
into some completely new area.  
 
[Albert Efimov] Okay. So this slide is summarized three ways to build artificial general 
intelligence. First, we discussed a lot. It is a representation of a symbolic approach to the 
description of the world. The second is connectionism, based on basically neural networks, deep 
learning neural networks. The third one is embodied intelligence and a kind of combination of 
maybe two of these. So I would like briefly to any you ask, which approach you to believe the 
most?  
 
[Leonid Zhukov] The most important approach here, to me right now, in the nearest future 
is I believe, lies in these newer symbolic computations, which is, in some sense, connecting 
symbolic representation and connectionism. And to me, that's probably where the next advances 
will happen. And that's the sort of, I think, the most fruitful direction for the near future. Going 
forward, yes, embodiment, definitely will join to actually gain those experience, and the meaning, 
and understanding of the world around us, physical world. 
 

16 
 
[Gary Bradski] Well for AGI first of all, I don't believe in AGI. It's a specific intelligence, 
I don't think we have a general intelligence because we use our internal structures to extend to like, 
what is quanta, what is a particle that goes like a wave and a particle down at the small scale? It 
does not. This is where our metaphor that we have for our brain breaks down. And so I do think 
you need all of these things. I think the symbolic representations derive from this kind of this 
necessity to build a world model for yourself. And it's those parts that give you the symbols, again, 
like this would extend to people think, well, math is universal. And I go, no, math only exists in 
here. It's not a property of the world. It's human property. And it exists in the human mind, and it 
works for our models. But it's not some magical thing that exists out in the universe that we tap 
into. It's a structure that works in our causal models. And it breaks down like most of the math is 
unknowable to us. Like most things, the vast majority of the universe is inexpressible by even 
unknowable by our functions and symbols, right. But causally at our scale at our time, we have a 
very flexible model that can extend.  
So, the quick answer is, we need all these things, and they'll always be relative to the job 
we want it to solve. So to achieve artificial specific intelligence, that can be done very general 
tasks. But there's no such thing as this universal intelligence. It's always in terms of loud learn 
grounded models, in my opinion. 
 
[Michael Woolbridge] Yes, like Gary, I am skeptical about AGI and I don't see anything 
on the table at the moment, which is going to take us, to AGI. I think all of those things are 
important: connectionism, symbolic representations. I mean, I think it's pretty widely accepted. 
Now that symbolic approaches and connectionism, I think need to talk to each other. I don't think 
the symbolic approaches that were popular 30 years ago are likely to be the way forward. So I 
don't think that's going to be it. But there will be some form of symbolic reasoning that's got to go 
on. And I think what I would say is, the reason I'm a skeptic about AGI is that, again, going back 
to a point I made earlier we are the product of billions of years of reinforcement learning with 
1000s of generations of our ancestors. When Mother Nature has tuned us over that immense period 
of time to be able to be generally intelligent in the world that we're in the current techniques the 
headline techniques like AlphaGo, and the chess-playing programs that are so successful, and, and 
so on. These are all based on reinforcement learning where a program just experiments and gets 
feedback, it tries to do something does badly and in a computer game, for example, right, a 
computer game like Space Invaders. So you can have a program that learns to play the game of 
Space Invaders, but actually, it does that by just essentially starting by moving randomly seeing 
what works, what doesn't work, when something works, when it gets a good score. It does that, 
again, that's reinforcement learning. It's just over time playing and playing and playing it. The 
problem is, reinforcement learning doesn't work in the real world, Gary couldn't have used 
reinforcement learning to train his cars in 2005, right, because they could have got through a great 
many cars, these cars wouldn't have got anywhere. Reinforcement Learning doesn't work. The 
kind of reinforcement learning which is so successful in games, for example, virtual environments, 
is very, very successful there, but doesn't work in the real world.  
So I'm somewhat of a skeptic about AGI for that reason. I mean, we are the product of 
reinforcement learning, but it's billions of years of evolution, which have given us a kind of 
reinforcement learning to generate human beings that can successfully occupy the human world. 
 
View publication stats
View publication stats

