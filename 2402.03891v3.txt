Control-Flow Refinement for Complexity
Analysis of Probabilistic Programs in KoAT⋆
Nils Lommen
, ´El´eanore Meyer
, and J¨urgen Giesl
LuFG Informatik 2, RWTH Aachen University, Aachen, Germany
{lommen,eleanore.meyer,giesl}@cs.rwth-aachen.de
Abstract. Recently, we showed how to use control-flow refinement (CFR)
to improve automatic complexity analysis of integer programs. While up
to now CFR was limited to classical programs, in this paper we extend
CFR to probabilistic programs and show its soundness for complexity
analysis. To demonstrate its benefits, we implemented our new CFR
technique in our complexity analysis tool KoAT.
1
Introduction
There exist numerous tools for complexity analysis of (non-probabilistic) programs,
e.g., [2–6, 10, 11, 15, 16, 18, 19, 24, 25, 28, 30, 33]. Our tool KoAT infers upper
runtime and size bounds for (non-probabilistic) integer programs in a modular way
by analyzing subprograms separately and lifting the obtained results to global
bounds on the whole program [10]. Recently, we developed several improvements
of KoAT [18, 24, 25] and showed that incorporating control-flow refinement (CFR)
[13, 14] increases the power of automated complexity analysis significantly [18].
There are also several approaches for complexity analysis of probabilistic
programs, e.g., [1, 7, 9, 21–23, 26, 29, 32, 35]. In particular, we also adapted KoAT’s
approach for runtime and size bounds, and introduced a modular framework for
automated complexity analysis of probabilistic integer programs in [26]. However,
the improvements of KoAT from [18, 24, 25] had not yet been adapted to the
probabilistic setting. In particular, we are not aware of any existing technique to
combine CFR with complexity analysis of probabilistic programs.
Thus, in this paper, we develop a novel CFR technique for probabilistic pro-
grams which could be used as a black box by every complexity analysis tool. More-
over, to reduce the overhead by CFR, we integrated CFR natively into KoAT by
calling it on-demand in a modular way. Our experiments show that CFR increases
the power of KoAT for complexity analysis of probabilistic programs substantially.
The idea of CFR is to gain information on the values of program variables
and to sort out infeasible program paths. For example, consider the probabilistic
while-loop (1). Here, we flip a (fair) coin and either set x to 0 or do nothing.
while x > 0 do x ←0 ⊕1/2 noop end
(1)
⋆funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
- 235950644 (Project GI 274/6-2) and DFG Research Training Group 2236 UnRAVeL
arXiv:2402.03891v3  [cs.LO]  14 Jun 2024

2
N. Lommen et al.
The update x ←0 is in a loop. However, after setting x to 0, the loop cannot be
executed again. To simplify its analysis, CFR “unrolls” the loop resulting in (2).
while x > 0 do break ⊕1/2 noop end
if x > 0 then x ←0 end
(2)
Here, x is updated in a separate, non-probabilistic if-statement and the loop does
not change variables. Thus, we sorted out paths where x ←0 was executed
repeatedly. Now, techniques for probabilistic programs can be used for the while-
loop. The rest of the program can be analyzed by techniques for non-probabilistic
programs. In particular, this is important if (1) is part of a larger program.
We present necessary preliminaries in Sect. 2. In Sect. 3, we introduce our
new control-flow refinement technique and show how to combine it with automated
complexity analysis of probabilistic programs. We conclude in Sect. 4 by an experi-
mental evaluation with our tool KoAT. We refer to the appendix for further details
on probabilistic programs and the soundness proof of our CFR technique.
2
Preliminaries
Let V be a set of variables. An atom is an inequation p1 < p2 for polynomials
p1, p2 ∈Z[V], and the set of all atoms is denoted by A(V). A constraint is a
(possibly empty) conjunction of atoms, and C(V) denotes the set of all constraints.
In addition to “<”, we also use “≥”, “=”, etc., which can be simulated by
constraints (e.g., p1 ≥p2 is equivalent to p2 < p1 + 1 for integers).
For probabilistic integer programs (PIPs), as in [26] we use a formalism based
on transitions, which also allows us to represent while-programs like (1) easily.
A PIP is a tuple (PV, L, ℓ0, GT ) with a finite set of program variables PV ⊆V, a
finite set of locations L, a fixed initial location ℓ0 ∈L, and a finite set of general
transitions GT . A general transition g ∈GT is a finite set of transitions which
share the same start location ℓg and the same guard φg. A transition is a 5-tuple
(ℓ, φ, p, η, ℓ′) with a start location ℓ∈L, target location ℓ′ ∈L \ {ℓ0}, guard
φ ∈C(V), probability p ∈[0, 1], and update η : PV →Z[V]. The probabilities
of all transitions in a general transition add up to 1. We always require that
general transitions are pairwise disjoint and let T = U
g∈GT g denote the set of
all transitions. PIPs may have non-deterministic branching, i.e., the guards of
several transitions can be satisfied. Moreover, we also allow non-deterministic
(temporary) variables V \ PV. To simplify the presentation, we do not consider
transitions with individual costs and updates which use probability distributions,
but the approach can easily be extended accordingly. From now on, we fix a PIP
P = (PV, L, ℓ0, GT ).
Example 1. The PIP in Fig. 1 has PV = {x, y}, L = {ℓ0, ℓ1, ℓ2}, and four general
transitions {t0}, {t1a, t1b}, {t2}, {t3}. The transition t0 starts at the initial
location ℓ0 and sets x to a non-deterministic positive value u ∈V \ PV, while y
is unchanged. (In Fig. 1, we omitted unchanged updates like η(y) = y, the guard
true, and the probability p = 1 to ease readability.) If the general transition is a

Control-Flow Refinement for Probabilistic Programs in KoAT
3
ℓ0
ℓ1
ℓ2
t0 : φ = (u > 0) η(x) = u
t1a : φ = (x > 0)
p = 1/2
t1b : φ = (x > 0)
η(x) = 0
p = 1/2
t2 : φ = (y > 0
∧x = 0)
t3 : η(y) = y −1
Fig. 1: A Probabilistic Integer Program
singleton, we often use transitions and general transitions interchangeably. Here,
only t1a and t1b form a non-singleton general transition which corresponds to
the program (1). We denoted such (probabilistic) transitions by dashed arrows in
Fig. 1. We extended (1) by a loop of t2 and t3 which is only executed if y > 0∧x = 0
(due to t2’s guard) and decreases y by 1 in each iteration (via t3’s update).
A state is a function σ : V →Z, Σ denotes the set of all states, and a
configuration is a pair of a location and a state. To extend finite sequences of
configurations to infinite ones, we introduce a special location ℓ⊥(indicating
termination) and a special transition t⊥(and its general transition g⊥= {t⊥})
to reach the configurations of a run after termination. Let L⊥= L ⊎{ℓ⊥},
T⊥= T ⊎{t⊥}, GT ⊥= GT ⊎{g⊥}, and let Conf = (L⊥× Σ) denote the set of
all configurations. A path has the form c0 →t1 · · · →tn cn for c0, . . . , cn ∈Conf
and t1, . . . , tn ∈T⊥for an n ∈N, and a run is an infinite path c0 →t1 c1 →t2 · · · .
Let Path and Run denote the sets of all paths and all runs, respectively.
We use Markovian schedulers S : Conf →GT ⊥× Σ to resolve all non-
determinism. For c = (ℓ, σ) ∈Conf, a scheduler S yields a pair S(c) = (g, ˜σ)
where g is the next general transition to be taken (with ℓ= ℓg) and ˜σ chooses
values for the temporary variables such that ˜σ |= φg and σ(v) = ˜σ(v) for all
v ∈PV. If GT contains no such g, we obtain S(c) = (g⊥, σ). For the formal
definition of Markovian schedulers, we refer to App. A.
For every S and σ0 ∈Σ, we define a probability mass function prS,σ0. For all
c ∈Conf, prS,σ0(c) is the probability that a run with scheduler S and the initial
state σ0 starts in c. So prS,σ0(c) = 1 if c = (ℓ0, σ0) and prS,σ0(c) = 0 otherwise.
For all c, c′ ∈Conf and t ∈T⊥, let prS(c →t c′) be the probability that one
goes from c to c′ via the transition t when using the scheduler S (see App. A for
the formal definition of prS). Then for any path f = (c0 →t1 · · · →tn cn) ∈Path,
let prS,σ0(f) = prS,σ0(c0) · prS(c0 →t1 c1) · . . . · prS(cn−1 →tn cn). Here, all
paths f which are not “admissible” (e.g., guards are not fulfilled, transitions are
starting or ending in wrong locations, etc.) have probability prS,σ0(f) = 0.
The semantics of PIPs can be defined via a corresponding probability space,
obtained by a standard cylinder construction. Let PS,σ0 denote the probability
measure which lifts prS,σ0 to cylinder sets: For any f ∈Path, we have prS,σ0(f) =
PS,σ0(Pref) for the set Pref of all infinite runs with prefix f. So PS,σ0(Θ) is
the probability that a run from Θ ⊆Run is obtained when using the scheduler
S and starting in σ0. Let ES,σ0 denote the associated expected value operator.

4
N. Lommen et al.
So for any random variable X : Run →N = N ∪{∞}, we have ES,σ0(X) =
P
n∈N n · PS,σ0(X = n). For a detailed construction, see App. A.
Definition 2 (Expected Runtime). For g ∈GT , Rg : Run →N is a random
variable with Rg(c0 →t1 c1 →t2 · · · ) = |{i ∈N | ti ∈g}|, i.e., Rg(ϑ) is the
number of times that a transition from g was applied in the run ϑ ∈Run. Moreover,
the random variable R : Run →N denotes the number of transitions that were
executed before termination, i.e., for all ϑ ∈Run we have R(ϑ) = P
g∈GT Rg(ϑ).
For a scheduler S and σ0 ∈Σ, the expected runtime of g is ES,σ0(Rg) and the
expected runtime of the program is RS,σ0 = ES,σ0(R).
The goal of complexity analysis for a PIP is to compute a bound on its expected
runtime complexity. The set of bounds B consists of all functions from Σ →R≥0.
Definition 3 (Expected Runtime Bound and Complexity [26]).
The
function RB : GT
→B is an expected runtime bound if (RB(g))(σ0) ≥
supS ES,σ0(Rg) for all σ0 ∈Σ and all g ∈GT . Then P
g∈GT RB(g) is a bound on
the expected runtime complexity of the whole program, i.e., P
g∈GT ((RB(g))(σ0))
≥supS RS,σ0 for all σ0 ∈Σ.
3
Control-Flow Refinement for PIPs
We now introduce our novel CFR algorithm for probabilistic integer programs,
based on the partial evaluation technique for non-probabilistic programs from [13,
14, 18]. In particular, our algorithm coincides with the classical CFR technique
when the program is non-probabilistic. The goal of CFR is to transform a program
P into a program P′ which is “easier” to analyze. Thm. 4 shows the soundness
of our approach, i.e., that P and P′ have the same expected runtime complexity.
Our CFR technique considers “abstract” evaluations which operate on sets
of states. These sets are characterized by conjunctions τ of constraints from
C(PV), i.e., τ stands for all states σ ∈Σ with σ |= τ. We now label locations ℓby
formulas τ which describe (a superset of) those states σ which can occur in ℓ, i.e.,
where a configuration (ℓ, σ) is reachable from some initial configuration (ℓ0, σ0).
We begin with labeling every location by the constraint true. Then we add new
copies of the locations with refined labels τ by considering how the updates of
transitions affect the constraints of their start locations and their guards. The
labeled locations become the new locations in the refined program.
Since a location might be reachable by different paths, we may construct
several variants ⟨ℓ, τ1⟩, . . . , ⟨ℓ, τn⟩of the same original location ℓ. Thus, the
formulas τ are not necessarily invariants that hold for all evaluations that reach
a location ℓ, but we perform a case analysis and split up a location ℓaccording to
the different sets of states that may reach ℓ. Our approach ensures that a labeled
location ⟨ℓ, τ⟩can only be reached by configurations (ℓ, σ) where σ |= τ.
We apply CFR only on-demand on a (sub)set of transitions S ⊆T (thus, CFR
can be performed in a modular way for different subsets S). In practice, we choose
S heuristically and use CFR only on transitions where our currently inferred

Control-Flow Refinement for Probabilistic Programs in KoAT
5
runtime bounds are “not yet good enough”. Then, for P = (PV, L, ℓ0, GT ), the
result of the CFR algorithm is the program P′ = (PV, L′, ⟨ℓ0, true⟩, GT ′) where
L′ and GT ′ are the smallest sets satisfying the properties (3), (4), and (5) below.
First, we require that for all ℓ∈L, all “original” locations ⟨ℓ, true⟩are in L′.
In these locations, we do not have any information on the possible states yet:
∀ℓ∈L. ⟨ℓ, true⟩∈L′
(3)
If we already introduced a location ⟨ℓ, τ⟩∈L′ and there is a transition (ℓ, φ, p, η, ℓ′)
∈S, then (4) requires that we also add the location ⟨ℓ′, τφ,η,ℓ′⟩to L′. The formula
τφ,η,ℓ′ over-approximates the set of states that can result from states that satisfy
τ and the guard φ of the transition when applying the update η. More precisely,
τφ,η,ℓ′ has to satisfy (τ ∧φ) |= η(τφ,η,ℓ′). For example, if τ = (x = 0), φ = true,
and η(x) = x −1, then we might have τφ,η,ℓ′ = (x = −1).
To ensure that every ℓ′ ∈L only gives rise to finitely many new labeled
locations ⟨ℓ′, τφ,η,ℓ′⟩, we perform property-based abstraction: For every location ℓ′,
we use a finite so-called abstraction layer αℓ′ ⊂{p1 ∼p2 | p1, p2 ∈Z[PV] and ∼
∈{<, ≤, =}} (see [14] for heuristics to compute αℓ′). Then we require that τφ,η,ℓ′
must be a conjunction of constraints from αℓ′ (i.e., τφ,η,ℓ′ ⊆αℓ′ when regarding
sets of constraints as their conjunction). This guarantees termination of our CFR
algorithm, since for every location ℓ′ there are only finitely many possible labels.
∀⟨ℓ, τ⟩∈L′. ∀(ℓ, φ, p, η, ℓ′) ∈S. ⟨ℓ′, τφ,η,ℓ′⟩∈L′
where τφ,η,ℓ′ = {ψ ∈αℓ′ | (τ ∧φ) |= η(ψ)}
(4)
Finally, we have to ensure that GT ′ contains all “necessary” (general) tran-
sitions. To this end, we consider all g ∈GT . The transitions (ℓ, φ, p, η, ℓ′) in
g ∩S now have to connect the appropriately labeled locations. Thus, for all
labeled variants ⟨ℓ, τ⟩∈L′, we add the transition (⟨ℓ, τ⟩, τ ∧φ, p, η, ⟨ℓ′, τφ,η,ℓ′⟩).
In contrast, the transitions (ℓ, φ, p, η, ℓ′) in g \ S only reach the location where ℓ′
is labeled with true, i.e., here we add the transition (⟨ℓ, τ⟩, τ ∧φ, p, η, ⟨ℓ′, true⟩).
∀⟨ℓ, τ⟩∈L′. ∀g ∈GT .
({(⟨ℓ, τ⟩, τ ∧φ, p, η, ⟨ℓ′, τφ,η,ℓ′⟩) | (ℓ, φ, p, η, ℓ′) ∈g ∩S} ∪
{(⟨ℓ, τ⟩, τ ∧φ, p, η, ⟨ℓ′, true⟩) | (ℓ, φ, p, η, ℓ′) ∈g \ S})
∈GT ′
(5)
L′ and S
g∈GT ′ g are finite due to the property-based abstraction, as there are
only finitely many possible labels for each location. Hence, repeatedly “unrolling”
transitions by (5) leads to the (unique) least fixpoint. Moreover, (5) yields proper
general transitions, i.e., their probabilities still add up to 1. In practice, we
remove transitions with unsatisfiable guards, and locations that are not reachable
from ⟨ℓ0, true⟩. Thm. 4 shows the soundness of our approach (see App. B for its
proof).
Theorem 4 (Soundness of CFR for PIPs). Let P′ =(PV, L′, ⟨ℓ0, true⟩, GT ′)
be the PIP such that L′ and GT ′ are the smallest sets satisfying (3), (4), and (5).
Let RP
S,σ0 and RP′
S,σ0 be the expected runtimes of P and P′, respectively. Then
for all σ0 ∈Σ we have supS RP
S,σ0 = supS RP′
S,σ0.

6
N. Lommen et al.
ℓ0
ℓ1
⟨ℓ1, x = 0⟩
⟨ℓ2, x = 0⟩
t′
0 : φ = (u > 0)
η(x) = u
t′
1a : φ = (x > 0)
p = 1/2
t′
1b : φ = (x > 0)
η(x) = 0
p = 1/2
t′
2 : φ = (y > 0
∧x = 0)
t′
3 : φ = (x = 0)
η(y) = y −1
Fig. 2: Result of Control-Flow Refinement with S = {t1a, t1b, t2, t3}
CFR Algorithm and its Runtime: To implement the fixpoint construction of
Thm. 4 (i.e., to compute the PIP P′), our algorithm starts by introducing all
“original” locations ⟨ℓ, true⟩for ℓ∈L according to (3). Then it iterates over all
labeled locations ⟨ℓ, τ⟩and all transitions t ∈T . If the start location of t is ℓ,
then the algorithm extends GT ′ by a new transition according to (5). Moreover,
it also adds the corresponding labeled target location to L′ (as in (4)), if L′ did
not contain this labeled location yet. Afterwards, we mark ⟨ℓ, τ⟩as finished and
proceed with a previously computed labeled location that is not marked yet. So
our implementation iteratively “unrolls” transitions by (5) until no new labeled
locations are obtained (this yields the least fixpoint mentioned above). Thus,
unrolling steps with transitions from T \ S do not invoke further computations.
To over-approximate the runtime of this algorithm, note that for every location
ℓ∈L, there can be at most 2|αℓ| many labeled locations of the form ⟨ℓ, τ⟩.
So if L = {ℓ0, . . . , ℓn}, then the overall number of labeled locations is at most
2|αℓ0|+. . .+2|αℓn|. Hence, the algorithm performs at most |T |·(2|αℓ0|+. . .+2|αℓn|)
unrolling steps.
Example 5. For the PIP in Fig. 1 and S = {t1a, t1b, t2, t3}, by (3) we start with
L′ = {⟨ℓi, true⟩| i ∈{0, 1, 2}}. We abbreviate ⟨ℓi, true⟩by ℓi in the final result
of the CFR algorithm in Fig. 2. As t0 ∈{t0} \ S, by (5) t0 is redirected such
that it starts at ⟨ℓ0, true⟩and ends in ⟨ℓ1, true⟩, resulting in t′
0. We always use
primes to indicate the correspondence between new and original transitions.
Next, we consider {t1a, t1b} ⊆S with the guard φ = (x > 0) and start
location ⟨ℓ1, true⟩. We first handle t1a which has the update η = id. We use the
abstraction layer αℓ0 = ∅, αℓ1 = {x = 0}, and αℓ2 = {x = 0}. Thus, we have to
find all ψ ∈αℓ1 = {x = 0} such that (true ∧x > 0) |= η(ψ). Hence, τx>0,id,ℓ1 is
the empty conjunction true as no ψ from αℓ1 satisfies this property. We obtain
t′
1a : (⟨ℓ1, true⟩, x > 0, 1/2, id, ⟨ℓ1, true⟩).
In contrast, t1b has the update η(x) = 0. To determine τx>0,η,ℓ1, again we
have to find all ψ ∈αℓ1 = {x = 0} such that (true ∧x > 0) |= η(ψ). Here, we get
τx>0,η,ℓ1 = (x = 0). Thus, by (4) we create the location ⟨ℓ1, x = 0⟩and obtain
t′
1b : (⟨ℓ1, true⟩, x > 0, 1/2, η(x) = 0, ⟨ℓ1, x = 0⟩).
As t1a and t1b form one general transition, by (5) we obtain {t′
1a, t′
1b} ∈GT ′.

Control-Flow Refinement for Probabilistic Programs in KoAT
7
Now, we consider transitions resulting from {t1a, t1b} with the start location
⟨ℓ1, x = 0⟩. However, τ = (x = 0) and the guard φ = (x > 0) are conflicting, i.e.,
the transitions would have an unsatisfiable guard τ ∧φ and are thus omitted.
Next, we consider transitions resulting from t2 with ⟨ℓ1, true⟩or ⟨ℓ1, x = 0⟩
as their start location. Here, we obtain two (general) transitions {t′
2}, {t′′
2} ∈GT ′:
t′
2 : (⟨ℓ1, x = 0⟩, y > 0 ∧x = 0, 1, id, ⟨ℓ2, x = 0⟩)
t′′
2 : (⟨ℓ1, true⟩, y > 0 ∧x = 0, 1, id, ⟨ℓ2, x = 0⟩)
However, t′′
2 can be ignored since x = 0 contradicts the invariant x > 0 at
⟨ℓ1, true⟩. KoAT uses Apron [20] to infer invariants like x > 0 automatically.
Finally, t3 leads to the transition t′
3 : (⟨ℓ2, x = 0⟩, x = 0, 1, η(y) = y −1, ⟨ℓ1, x =
0⟩). Thus, we obtain L′ = {⟨ℓi, true⟩| i ∈{0, 1}} ∪{⟨ℓi, x = 0⟩| i ∈{1, 2}}.
KoAT infers a bound RB(g) for each g ∈GT individually (thus, non-probabilis-
tic program parts can be analyzed by classical techniques). Then P
g∈GT RB(g)
is a bound on the expected runtime complexity of the whole program, see Def. 3.
Example 6. We now infer a bound on the expected runtime complexity of the
PIP in Fig. 2. Transition t′
0 is not on a cycle, i.e., it can be evaluated at most once.
So RB({t′
0}) = 1 is an (expected) runtime bound for the general transition {t′
0}.
For the general transition {t′
1a, t′
1a}, KoAT infers the expected runtime bound 2
via probabilistic linear ranking functions (PLRFs, see e.g., [26]). More precisely,
KoAT finds the constant PLRF {ℓ1 7→2, ⟨ℓ1, x = 0⟩7→0}. In contrast, in the
original program of Fig. 1, {t1a, t1b} is not decreasing w.r.t. any constant PLRF,
because t1a and t1b have the same target location. So here, every PLRF where {t1a,
t1b} decreases in expectation depends on x. However, such PLRFs do not yield a
finite runtime bound in the end, as t0 instantiates x by the non-deterministic
value u. Therefore, KoAT fails on the program of Fig. 1 without using CFR.
For the program of Fig. 2, KoAT infers RB({t′
2}) = RB({t′
3}) = y. By adding
all runtime bounds, we obtain the bound 3 + 2 · y on the expected runtime
complexity of the program in Fig. 2 and thus by Thm. 4 also of the program in
Fig. 1.
4
Implementation, Evaluation, and Conclusion
We presented a novel control-flow refinement technique for probabilistic programs
and proved that it does not modify the program’s expected runtime complexity.
This allows us to combine CFR with approaches for complexity analysis of
probabilistic programs. Compared to its variant for non-probabilistic programs,
the soundness proof of Thm. 4 for probabilistic programs is considerably more
involved.
Up to now, our complexity analyzer KoAT used the tool iRankFinder [13] for
CFR of non-probabilistic programs [18]. To demonstrate the benefits of CFR
for complexity analysis of probabilistic programs, we now replaced the call to
iRankFinder in KoAT by a native implementation of our new CFR algorithm. KoAT

8
N. Lommen et al.
O(1)
O(n) O(n2) O(n>2) O(EXP)
< ω
AVG+(s) AVG(s)
KoAT+CFR 11 (2) 56 (12) 14
2
1
84 (14)
11.68
11.34
KoAT
9
41 (1) 16 (1)
2
1
69 (2)
2.71
2.41
Absynth
7
35
9
0
0
51
2.86
37.48
eco-imp
8
35
6
0
0
49
0.34
68.02
Table 1: Evaluation of CFR on Probabilistic Programs
is written in OCaml and it uses Z3 [12] for SMT solving, Apron [20] to generate
invariants, and the Parma Polyhedra Library [8] for computations with polyhedra.
We used all 75 probabilistic benchmarks from [26, 29] and added 15 new
benchmarks including our leading example and problems adapted from the
Termination Problem Data Base [34], e.g., a probabilistic version of McCarthy’s
91 function. Our benchmarks also contain examples where CFR is useful even if
it cannot separate probabilistic from non-probabilistic program parts as in our
leading example.
Table 1 shows the results of our experiments. We compared the configuration
of KoAT with CFR (“KoAT+CFR”) against KoAT without CFR. Moreover, as in
[26], we also compared with the main other recent tools for inferring upper bounds
on the expected runtimes of probabilistic integer programs (Absynth [29] and eco-
imp [7]). As in the Termination Competition [17], we used a timeout of 5 minutes
per example. The first entry in every cell is the number of benchmarks for which
the tool inferred the respective bound. In brackets, we give the corresponding
number when only regarding our new examples. For example, KoAT+CFR finds
a finite expected runtime bound for 84 of the 90 examples. A linear expected
bound (i.e., in O(n)) is found for 56 of these 84 examples, where 12 of these
benchmarks are from our new set. AVG(s) is the average runtime in seconds on
all benchmarks and AVG+(s) is the average runtime on all successful runs.
The experiments show that similar to its benefits for non-probabilistic pro-
grams [18], CFR also increases the power of automated complexity analysis
for probabilistic programs substantially, while the runtime of the analyzer may
become longer since CFR increases the size of the program. The experiments also
indicate that a related CFR technique is not available in the other complexity
analyzers. Thus, we conjecture that other tools for complexity or termination
analysis of PIPs would also benefit from the integration of our CFR technique.
KoAT’s source code, a binary, and a Docker image are available at:
https://koat.verify.rwth-aachen.de/prob cfr
The website also explains how to use our CFR implementation separately (without
the rest of KoAT), in order to access it as a black box by other tools. Moreover,
the website provides a web interface to directly run KoAT online, and details on
our experiments, including our benchmark collection.
Acknowledgements: We thank Yoann Kehler for helping with the implementation
of our CFR technique in KoAT.

Control-Flow Refinement for Probabilistic Programs in KoAT
9
References
[1]
S. Agrawal, K. Chatterjee, and P. Novotn´y. “Lexicographic Ranking Su-
permartingales: An Efficient Approach to Termination of Probabilistic
Programs”. In: Proc. ACM Program. Lang. 2.POPL (2017). doi: 10.1145/
3158122.
[2]
E. Albert, P. Arenas, S. Genaim, and G. Puebla. “Automatic Inference of
Upper Bounds for Recurrence Relations in Cost Analysis”. In: Proc. SAS.
LNCS 5079. 2008, pp. 221–237. doi: 10.1007/978-3-540-69166-2 15.
[3]
E. Albert, P. Arenas, S. Genaim, G. Puebla, and D. Zanardini. “Cost
Analysis of Object-Oriented Bytecode Programs”. In: Theor. Comput. Sci.
413.1 (2012), pp. 142–159. doi: 10.1016/j.tcs.2011.07.009.
[4]
E. Albert, M. Bofill, C. Borralleras, E. Mart´ın-Mart´ın, and A. Rubio. “Re-
source Analysis driven by (Conditional) Termination Proofs”. In: The-
ory Pract. Log. Program. 19.5-6 (2019), pp. 722–739. doi: 10 . 1017 /
S1471068419000152.
[5]
C. Alias, A. Darte, P. Feautrier, and L. Gonnord. “Multi-Dimensional
Rankings, Program Termination, and Complexity Bounds of Flowchart
Programs”. In: Proc. SAS. LNCS 6337. 2010, pp. 117–133. doi: 10.1007/978-
3-642-15769-1 8.
[6]
M. Avanzini and G. Moser. “A Combination Framework for Complexity”. In:
Proc. RTA. LIPIcs 21. 2013, pp. 55–70. doi: 10.4230/LIPIcs.RTA.2013.55.
[7]
M. Avanzini, G. Moser, and M. Schaper. “A Modular Cost Analysis for
Probabilistic Programs”. In: Proc. ACM Program. Lang. 4.OOPSLA (2020).
url: https://doi.org/10.1145/3428240.
[8]
R. Bagnara, P. M. Hill, and E. Zaffanella. “The Parma Polyhedra Library:
Toward a Complete Set of Numerical Abstractions for the Analysis and
Verification of Hardware and Software Systems”. In: Sci. Comput. Program.
72 (2008), pp. 3–21. doi: 10.1016/j.scico.2007.08.001.
[9]
K. Batz, B. L. Kaminski, J.-P. Katoen, C. Matheja, and L. Verscht. “A
Calculus for Amortized Expected Runtimes”. In: Proc. ACM Program.
Lang. 7.POPL (2023). doi: 10.1145/3571260.
[10]
M. Brockschmidt, F. Emmes, S. Falke, C. Fuhs, and J. Giesl. “Analyzing
Runtime and Size Complexity of Integer Programs”. In: ACM Trans.
Program. Lang. Syst. 38 (2016), pp. 1–50. doi: 10.1145/2866575.
[11]
Q. Carbonneaux, J. Hoffmann, and Z. Shao. “Compositional Certified
Resource Bounds”. In: Proc. PLDI. 2015, pp. 467–478. doi: 10.1145/
2737924.2737955.
[12]
L. de Moura and N. Bjørner. “Z3: An Efficient SMT Solver”. In: Proc.
TACAS. LNCS 4963. 2008, pp. 337–340. doi: 10.1007/978-3-540-78800-3 24.
[13]
J. J. Dom´enech and S. Genaim. “iRankFinder”. In: Proc. WST. http :
//wst2018.webs.upv.es/wst2018proceedings.pdf. 2018, p. 83.
[14]
J. J. Dom´enech, J. P. Gallagher, and S. Genaim. “Control-Flow Refine-
ment by Partial Evaluation, and its Application to Termination and Cost
Analysis”. In: Theory Pract. Log. Program. 19.5-6 (2019), pp. 990–1005.
doi: 10.1017/S1471068419000310.

10
N. Lommen et al.
[15]
A. Flores-Montoya. “Upper and Lower Amortized Cost Bounds of Programs
Expressed as Cost Relations”. In: Proc. FM. LNCS 9995. 2016, pp. 254–273.
doi: 10.1007/978-3-319-48989-6 16.
[16]
F. Frohn and J. Giesl. “Complexity Analysis for Java with AProVE”. In:
Proc. iFM. LNCS 10510. 2017, pp. 85–101. doi: 10.1007/978-3-319-66845-
1 6.
[17]
J. Giesl, A. Rubio, C. Sternagel, J. Waldmann, and A. Yamada. “The
Termination and Complexity Competition”. In: Proc. TACAS. LNCS 11429.
2019, pp. 156–166. doi: 10.1007/978-3-030-17502-3 10.
[18]
J. Giesl, N. Lommen, M. Hark, and F. Meyer. “Improving Automatic
Complexity Analysis of Integer Programs”. In: The Logic of Software. A
Tasting Menu of Formal Methods. LNCS 13360. 2022, pp. 193–228. doi:
10.1007/978-3-031-08166-8 10.
[19]
J. Hoffmann, A. Das, and S.-C. Weng. “Towards Automatic Resource
Bound Analysis for OCaml”. In: Proc. POPL. 2017, pp. 359–373. doi:
10.1145/3009837.3009842.
[20]
B. Jeannet and A. Min´e. “Apron: A Library of Numerical Abstract Domains
for Static Analysis”. In: Proc. CAV. LNCS 5643. 2009, pp. 661–667. doi:
10.1007/978-3-642-02658-4 52.
[21]
B. L. Kaminski, J.-P. Katoen, C. Matheja, and F. Olmedo. “Weakest
Precondition Reasoning for Expected Runtimes of Randomized Algorithms”.
In: J. ACM 65 (2018), pp. 1–68. doi: 10.1145/3208102.
[22]
B. L. Kaminski, J. Katoen, and C. Matheja. “Expected Runtime Analyis
by Program Verification”. In: Foundations of Probabilistic Programming.
Ed. by G. Barthe, J. Katoen, and A. Silva. Cambridge University Press,
2020, 185–220. doi: 10.1017/9781108770750.007.
[23]
L. Leutgeb, G. Moser, and F. Zuleger. “Automated Expected Amortised
Cost Analysis of Probabilistic Data Structures”. In: Proc. CAV. LNCS
13372. 2022, pp. 70–91. doi: 10.1007/978-3-031-13188-2 4.
[24]
N. Lommen, F. Meyer, and J. Giesl. “Automatic Complexity Analysis
of Integer Programs via Triangular Weakly Non-Linear Loops”. In: Proc.
IJCAR. LNCS 13385. 2022, pp. 734–754. doi: 10.1007/978-3-031-10769-
6 43.
[25]
N. Lommen and J. Giesl. “Targeting Completeness: Using Closed Forms
for Size Bounds of Integer Programs”. In: Proc. FroCoS. LNCS 14279. 2023,
pp. 3–22. doi: 10.1007/978-3-031-43369-6 1.
[26]
F. Meyer, M. Hark, and J. Giesl. “Inferring Expected Runtimes of Proba-
bilistic Integer Programs Using Expected Sizes”. In: Proc. TACAS. LNCS
12651. 2021, pp. 250–269. doi: 10.1007/978-3-030-72016-2 14.
[27]
F. Meyer, M. Hark, and J. Giesl. “Inferring Expected Runtimes of Proba-
bilistic Integer Programs Using Expected Sizes”. In: CoRR abs/2010.06367
(2020). doi: 10.48550/arXiv.2010.06367.
[28]
G. Moser and M. Schaper. “From Jinja Bytecode to Term Rewriting:
A Complexity Reflecting Transformation”. In: Inf. Comput. 261 (2018),
pp. 116–143. doi: 10.1016/j.ic.2018.05.007.

Control-Flow Refinement for Probabilistic Programs in KoAT
11
[29]
V. C. Ngo, Q. Carbonneaux, and J. Hoffmann. “Bounded Expectations:
Resource Analysis for Probabilistic Programs”. In: Proc. PLDI. 2018,
pp. 496–512. url: https://doi.org/10.1145/3192366.3192394.
[30]
L. Noschinski, F. Emmes, and J. Giesl. “Analyzing Innermost Runtime
Complexity of Term Rewriting by Dependency Pairs”. In: J. Autom. Reason.
51 (2013), pp. 27–56. doi: 10.1007/s10817-013-9277-6.
[31]
M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic
Programming. Wiley Series in Probability and Statistics. Wiley, 1994. doi:
10.1002/9780470316887.
[32]
P. Schr¨oer, K. Batz, B. L. Kaminski, J.-P. Katoen, and C. Matheja. “A
Deductive Verification Infrastructure for Probabilistic Programs”. In: Proc.
ACM Program. Lang. 7.OOPSLA (2023), pp. 2052–2082. doi: 10.1145/
3622870.
[33]
M. Sinn, F. Zuleger, and H. Veith. “Complexity and Resource Bound
Analysis of Imperative Programs Using Difference Constraints”. In: J.
Autom. Reason. 59.1 (2017), pp. 3–45. doi: 10.1007/s10817-016-9402-4.
[34]
TPDB (Termination Problem Data Base). url: https://github.com/
TermCOMP/TPDB.
[35]
D. Wang, D. M. Kahn, and J. Hoffmann. “Raising Expectations: Automat-
ing Expected Cost Analysis with Types”. In: Proc. ACM Program. Lang.
4.ICFP (2020). url: https://doi.org/10.1145/3408992.

12
N. Lommen et al.
A
Formal Semantics of PIPs
For a detailed recapitulation of basic concepts from probability theory or the
cylindrical construction of probability spaces for PIPs, we refer the reader to
[27]. As mentioned in Sect. 2, we use (history-independent) schedulers to resolve
non-deterministic branching and sampling.
Definition 7 (Scheduler [27]). A function S : Conf →GT ⊥×Σ is a scheduler
if for every configuration c = (ℓ, σ) ∈Conf, S(c) = (g, ˜σ) implies:
(a) σ(x) = ˜σ(x) for all x ∈PV.
(b) ℓis the start location ℓg of the general transition g.
(c) ˜σ |= φg for the guard φg of the general transition g.
(d) g = g⊥and ˜σ = σ, if ℓ= ℓ⊥or if no g ∈GT , ˜σ ∈Σ satisfy (a) to (c).
Π denotes the set of all schedulers.
So to continue an evaluation in state σ, (a) the scheduler chooses a state ˜σ
that agrees with σ on all program variables, but where the values for temporary
variables are chosen non-deterministically. After instantiating the temporary
variables, (b) the scheduler selects a general transition that starts in the current
location ℓand (c) whose guard is satisfied, if such a general transition exists.
Otherwise, (d) S chooses g⊥= {t⊥} and leaves the state σ unchanged.
For a path (ℓ, σ) of length 0, i.e., if the path only consists of a single configu-
ration, as mentioned in Sect. 2, we have
prS,σ0(ℓ, σ) =
(
1
if ℓ= ℓ0 and σ = σ0
0
otherwise.
Furthermore, for a transition t, two configurations c = (ℓc, σ) and c′ = (ℓc′, σ′),
and S(ℓc, σ) = (g, ˜σ), we have
prS(c →t c′) =















p
if t = (ℓc, φ, p, η, ℓc′) ∈g, t ̸= t⊥,
σ′(v) = ˜σ(η(v)) for all v ∈PV, and
σ′(v) = ˜σ(v) for all v ∈V \ PV
1
if t = t⊥∈g, ℓc′ = ℓ⊥, and σ′ = σ
0
otherwise.
Then prS,σ0(c0 →t1 · · · →tn cn) = prS,σ0(c0) · Qn
i=1 prS(ci−1 →ti ci). For any
f ∈Path, we say that f is admissible for S and σ0 if prS,σ0(f) > 0.
B
Proof of Thm. 4
Theorem 4 (Soundness of CFR for PIPs). Let P′ =(PV, L′, ⟨ℓ0, true⟩, GT ′)
be the PIP such that L′ and GT ′ are the smallest sets satisfying (3), (4), and (5).
Let RP
S,σ0 and RP′
S,σ0 be the expected runtimes of P and P′, respectively. Then
for all σ0 ∈Σ we have supS RP
S,σ0 = supS RP′
S,σ0.

Control-Flow Refinement for Probabilistic Programs in KoAT
13
Proof.
Since we now regard both the original program P and the program
P′ resulting from CFR, we add the considered program as a superscript when
considering RS,σ0, ES,σ0, R, PS,σ0, prS,σ0, Path, or Conf.
First, we prove supS RP
S,σ0 ≤supS RP′
S,σ0 which is the crucial inequation
for soundness. Afterwards, we complete the proof by showing supS RP
S,σ0 ≥
supS RP′
S,σ0. Thus, CFR does not increase the expected runtime complexity.
As we will often restrict ourselves to the consideration of admissible paths only,
we define the corresponding set AdmPathP
S,σ0 = {f ∈PathP | prP
S,σ0(f) > 0} of
all such paths for a given program P, scheduler S, and initial state σ0 ∈Σ.
Soundness – Proof of “ ≤”: Let σ0 ∈Σ be an initial state. We have to prove
supS RP
S,σ0 ≤supS RP′
S,σ0. In the following, we show that for every scheduler S
for P there exists a scheduler S′ for P′ such that RP
S,σ0 ≤RP′
S′,σ0. Note that for
any schedulers S, S′ and any σ0 ∈Σ, we have
RP
S,σ0 ≤RP′
S′,σ0
⇔EP
S,σ0(RP) ≤EP′
S′,σ0(RP′)
(definition of RP
S,σ0 and RP′
S′,σ0)
⇔
X
n∈N
n · PP
S,σ0(RP = n) ≤
X
n∈N
n · PP′
S′,σ0(RP′ = n)
(definition of EP
S,σ0 and EP′
S′,σ0)
by definition. This can be equivalently expressed as
X
n∈N
n · PP
S,σ0(RP = n) ≤
X
n∈N
n · PP′
S′,σ0(RP′ = n)
⇔
X
n∈N>0
PP
S,σ0(RP ≥n) ≤
X
n∈N>0
PP′
S′,σ0(RP′ ≥n)
(N>0 = N \ {0})
⇔
X
n∈N
 1 −PP
S,σ0(RP < n)

≤
X
n∈N

1 −PP′
S′,σ0(RP′ < n)

since every run ϑ ∈Run with R(ϑ) = n occurs n-times in PS,σ0(R ≥1), . . . ,
PS,σ0(R ≥n). Now, we want to move from infinite runs to finite paths so that
we can use an (injective) embedding which maps all paths of P to paths of
P′, i.e., to paths that use the transitions of GT ′ as constructed in (5). To this
end, we define RP(c0 →t1 · · · →tn cn) = |{i ∈{1, . . . , n} | ti ̸= t⊥}| for the
path c0 →t1 · · · →tn cn and the program P. We call a path c0 →t1 · · · →tn cn
terminated if t1, . . . , tn−1 ̸= t⊥and tn = t⊥. Thus, we have
X
n∈N
 1 −PP
S,σ0(RP < n)

≤
X
n∈N

1 −PP′
S′,σ0(RP′ < n)


14
N. Lommen et al.
⇔
X
n∈N








1 −
X
f∈AdmPathP
S,σ0
RP(f) < n
f is terminated
prP
S,σ0(f)








≤
X
n∈N









1 −
X
f∈AdmPathP′
S′,σ0
RP′(f) < n
f is terminated
prP′
S′,σ0(f)









. (†)
Next, we prove that for every scheduler S : ConfP →GT ⊥× Σ there is a
scheduler S′ : ConfP′ →GT ′
⊥× Σ such that the last inequation (†) is valid.
To this end, we use Lemma 8 which is presented below. For every σ0 ∈Σ
and every scheduler S : ConfP →GT ⊥× Σ, Lemma 8 yields a scheduler S′ :
ConfP′ →GT ′
⊥× Σ and a bijection β : AdmPathP
S,σ0 →AdmPathP′
S′,σ0 such that
prP
S,σ0(f) = prP′
S′,σ0(β(f)) for all f ∈AdmPathP
S,σ0. Moreover, R(f) = R(β(f))
and f is terminated iff β(f) is terminated. Hence, we have
X
n∈N








1 −
X
f∈AdmPathP
S,σ0
RP(f) < n
f is terminated
prP
S,σ0(f)








=
X
n∈N








1 −
X
f∈β(AdmPathP
S,σ0)
RP′(f) < n
f is terminated
prP′
S′,σ0(β(f))








(β is injective and prP
S,σ0(f) = prP′
S′,σ0(β(f)))
=
X
n∈N









1 −
X
f∈AdmPathP′
S′,σ0
RP′(f) < n
f is terminated
prP′
S′,σ0(f)









(β is surjective, i.e., β(AdmPathP
S,σ0) = AdmPathP′
S′,σ0)
which completes the proof of the soundness part “≤”.
Lemma 8 (Embedding of PathP into PathP′). Let σ0 ∈Σ and S : ConfP →
GT ⊥×Σ be a scheduler. Then there exists a scheduler S′ : ConfP′ →GT ′
⊥×Σ and
a bijection β : AdmPathP
S,σ0 →AdmPathP′
S′,σ0 such that prP
S,σ0(f) = prP′
S,σ0(β(f))
for all f ∈AdmPathP
S,σ0. Moreover, R(f) = R(β(f)) and f is terminated iff
β(f) is terminated.
Proof. We define the scheduler S′ as follows:
– S′(⟨ℓ, τ⟩, σ) = (g′, ˜σ)
if there exists a g′ ∈GT ′ with the start location ⟨ℓ, τ⟩
where σ |= τ and S(ℓ, σ) = (g, ˜σ), such that g and ⟨ℓ, τ⟩yield g′ in (5) (i.e.,
g has the start location ℓ, φg′ is τ ∧φg, the transitions in g and g′ have the
same probabilities and updates, and the target locations of the transitions in
g′ are labeled variants of the target locations of the transitions in g)

Control-Flow Refinement for Probabilistic Programs in KoAT
15
– S′(⟨ℓ, τ⟩, σ) = (g⊥, σ)
otherwise
– S′(ℓ⊥, σ) = (g⊥, σ)
for all σ ∈Σ
Clearly, S′ is a valid scheduler according to Def. 7.
Let σ0 ∈Σ be a state and S, S′ as above. We define the bijection β induc-
tively:
Induction Base: Let f ∈AdmPathP
S,σ0 be an admissible path of length 0, i.e., f
only consists of the configuration c0 = (ℓ0, σ0) with prP
S,σ0(c0) = 1. We define
β(c0) = (⟨ℓ0, true⟩, σ0). Thus, prP′
S′,σ0(β(c0)) = 1. Otherwise, the path β(c0)
would not be admissible, as all admissible paths for program P′ of length 0
must start in location ⟨ℓ0, true⟩with state σ0. Hence, β({f ∈AdmPathP
S,σ0 |
f has length 0}) = {f ∈AdmPathP′
S′,σ0 | f has length 0}. Moreover, R(c0) =
R(β(c0)) = 0 and both c0 and β(c0) are not terminated.
Induction Step: Let f ∈AdmPathP
S,σ0 be an admissible path of length n+1 of the
form f = c0 →t1 · · · →tn+1 cn+1 where cn+1 = (ℓn+1, σn+1). By the induction
hypothesis we have
prP
S,σ0(fn) = prP′
S′,σ0(β(fn))
where fn = (ℓ0, σ0) →t1 · · · →tn (ℓn, σn) and β(fn) = (⟨ℓ0, τ0⟩, σ0) →t′
1 · · · →t′n
(⟨ℓn, τn⟩, σn).
If tn = t⊥, then the induction step holds by setting β(f) = (β(fn) →t⊥
(ℓ⊥, σn)). In particular, then R(f) = R(fn) = R(β(fn)) = R(β(f)) by the
induction hypothesis, and both f and β(f) are terminated.
Otherwise, by construction of (3) and (4), there must be a location ⟨ℓn+1, τn+1⟩
∈L′ such that σn+1 |= τn+1. Furthermore, there must also be a transition t′
n+1
whose start location is ⟨ℓn, τn⟩and a general transition of GT ′ which includes
t′
n+1 by (5). In particular, t′
n+1 and tn+1 have the same probability and update,
and the guard of t′
n+1 is the conjunction of τn and the guard of tn+1. Hence,
prP
S,σ0(cn →tn+1 cn+1) = prP′
S′,σ0(c′
n →(⟨ℓn+1, τn+1⟩, σn+1)). Thus, defining
β(f) = (β(fn) →tn+1 (⟨ℓn+1, τn+1⟩, σn+1)) yields the required properties. In
particular, β is injective by the induction hypothesis for paths of length up to
n + 1. Moreover, R(f) = 1 + R(fn) = 1 + R(β(fn)) = R(β(f)) by the induction
hypothesis, and both f and β(f) are not terminated.
Finally, we finish by proving that β is surjective, and hence bijective as
well. Let f ′ = (c0 →t′
1 · · · →t′
n+1 cn+1) ∈AdmPathP′
S′,σ0 be an admissible path
of length n + 1. By the induction hypothesis, there exists f ∈AdmPathP
S,σ0
such that β(f) = (c0 →t′
1 · · · →t′n cn) holds. Since we fixed the scheduler S,
there exists a unique configuration (ℓ, σ) such that f →tn+1 (ℓ, σ) is admissible.
Moreover, there is a unique t′
n+1 in P′ that corresponds to tn+1 and whose start
location is the location of cn. Hence, we obtain cn+1 = (⟨ℓ, τ⟩, σ) where ⟨ℓ, τ⟩is
the target location of t′
n+1. Thus, β(f →tn+1 (ℓ, σ)) = f ′ which concludes the
proof.
⊓⊔

16
N. Lommen et al.
History-Dependent Schedulers: In the following tightness proof of
sup
S
RS,σ0(P) ≥sup
S
RS,σ0(P′),
we use history-dependent schedulers. The reason for regarding history-dependent
schedulers is that when removing the labels from the locations (in the step from
P′ to P), different labeled locations ⟨ℓ, τ1⟩and ⟨ℓ, τ2⟩of P′ can be mapped to
the same location ℓof P. Since the scheduler for P′ might behave differently
on ⟨ℓ, τ1⟩and ⟨ℓ, τ2⟩, this cannot be directly mimicked by a history-independent
scheduler for P which always has to behave in the same way for ℓ. However, this
problem can be solved by regarding history-dependent schedulers that depend on
the whole path up to the current configuration. For convenience, we always use S
when we consider schedulers as in Def. 7 and R if we refer to history-dependent
schedulers as in the following definition.
Definition 9 (History-Dependent Scheduler).
A function R : Path →
GT ⊥× Σ is a history-dependent scheduler if for every f = (ℓ0, σ0) →∗(ℓ, σ) ∈
Path, R(f) = (g, ˜σ) implies:
(a) σ(x) = ˜σ(x) for all x ∈PV.
(b) ℓis the start location ℓg of the general transition g.
(c) ˜σ |= φg for the guard φg of the general transition g.
(d) g = g⊥and ˜σ = σ, if ℓ= ℓ⊥or no g ∈GT , ˜σ ∈Σ satisfy (a) to (c).
ΠHD denotes the set of all history-dependent schedulers.
As for history-independent schedulers, we also define prR,σ0 accordingly. For
a path (ℓ, σ) of length 0, i.e., if the path only consists of a single configuration,
we have
prR,σ0(ℓ, σ) =
(
1
if ℓ= ℓ0 and σ = σ0
0
otherwise.
Furthermore, for a transition t, path f = c0 →t1 · · · →tn cn, configuration
c′ = (ℓc′, σ′), and history-dependent scheduler R(f) = (g, ˜σ), we have
prR(f →t c′) =















p
if t = (ℓc, φ, p, η, ℓc′) ∈g, t ̸= t⊥,
σ′(v) = ˜σ(η(v)) for all v ∈PV, and
σ′(v) = ˜σ(v) for all v ∈V \ PV
1
if t = t⊥∈g, ℓc′ = ℓ⊥, and σ′ = σ
0
otherwise.
Then prR,σ0(c0 →t1 · · · →tn cn) = prR,σ0(c0) · Qn
i=1 prR(c0 →t1 · · · →ti ci).
Analogous to the case for history-independent schedulers, for any f ∈Path, we
say that f is admissible for R and σ0 if prR,σ0(f) > 0. Similarly, PR,σ0 denotes
the probability measure which lifts prR,σ0 to the sigma-algebra generated by
all cylinder sets: For any path f ∈Path, we have prR,σ0(f) = PR,σ0(Pref) for
the set Pref of all infinite runs with prefix f. ER,σ0 denotes the associated
expected value operator. So for any random variable X : Run →N, we have
ER,σ0(X) = P
n∈N n · PR,σ0(X = n).

Control-Flow Refinement for Probabilistic Programs in KoAT
17
Tightness – Proof of “ ≥”: We have to show that for every σ0 ∈Σ the
inequation
sup
S
RS,σ0(P) ≥sup
S
RS,σ0(P′)
holds. To this end, we show that for every σ0 ∈Σ we have
sup
S∈Π
EP′
S,σ0(RP′) ≤
(1)
sup
R∈ΠHD EP′
R,σ0(RP′) ≤
(2)
sup
R∈ΠHD EP
R,σ0(RP) ≤
(3)
sup
S∈Π
EP
S,σ0(RP)
Proving (1): Let S : ConfP′ →GT ′
⊥× Σ be a scheduler and σ0 ∈Σ. Defining
the history-dependent scheduler R : PathP′ →GT ′
⊥× Σ by setting R(· · · →c) =
S(c) yields EP′
S,σ0(RP′) = EP′
R,σ0(RP′). Hence, we have supS∈Π EP′
S,σ0(RP′) ≤
supR∈ΠHD EP′
R,σ0(RP′) which proves the first inequation.
Proving (2): Let R′ : PathP′ →GT ′
⊥× Σ be a history-dependent scheduler and
σ0 ∈Σ. We now have to define the history-dependent scheduler R : PathP →
GT ⊥× Σ. Let f = (ℓ0, σ0) →t1 · · · →tn (ℓn, σn) ∈PathP be a path in P. If f is
not admissible, then we define R(f) arbitrarily such that Def. 9 is fulfilled. If f is
admissible, then there exists a unique corresponding path f ′ ∈AdmPathP′
R′,σ0 with
f ′ = (⟨ℓ0, τ0⟩, σ0) →t′
1 · · · →t′n (⟨ℓn, τn⟩, σn) (as in the surjectivity proof of β in
the proof of Lemma 8). Let R′(f ′) = (g′, σ). Then we define R(f) = (g, σ) where
g′ results from g and (5). Our construction yields EP′
R′,σ0(RP′) = EP
R,σ0(RP).
Thus, we have supR∈ΠHD EP′
R,σ0(RP′) ≤supR∈ΠHD EP
R,σ0(RP).
Proving (3): Let R : PathP →GT ⊥× Σ be a history-dependent scheduler and
σ0 ∈Σ. We now consider the Markov decision process (MDP) Mn resulting
from all admissible paths of the program P under the scheduler R of length less
than or equal to n ∈N starting in c0 = (ℓ0, σ0). Formally, this MDP is defined
as the tuple Mn = (N, Sn, An, p, r) [31, Section 2.1.3] where:
1. N is the set of decision epochs.
2. Sn ⊆Conf is the state set, where Sn consists of all configurations ci with
i ∈{0, . . . , n} which occur on an admissible path f = c0 →t1 · · · →ti ci ∈
AdmPathP
R,σ0.
3. For every c ∈Sn, An(c) ⊆AdmPathP
R,σ0 is the set of actions in c, i.e., it is
the smallest set of all admissible paths f = c0 →t1 · · · →ti ci ∈AdmPathP
R,σ0
with i ∈{0, . . . , n −1} and ci = c.
4. p(c, f)(c′) = P
t∈g prP
R(f →t c′) determines the probability of transitioning
from state c to state c′ when action f ∈An(c) ⊆AdmPathP
R,σ0 is chosen,
where R(f) = (g, ˜σ).
5. r(f) is the reward of the action f. For an action f ∈AdmPathP
R,σ0, we have
r(f) = 0 if R(f) = (g⊥, ˜σ) for some ˜σ and r(f) = 1, otherwise.
Note that the MDP Mn might get “stuck”, i.e., there might exist a reachable
state c = (ℓ, σ) ∈Sn such that An(c) = ∅. However, this can be avoided by

18
N. Lommen et al.
introducing a novel action ac ∈An(c) leading to a dummy state (ℓ⊥, σ) with
reward 0. In this dummy state, we introduce an additional rewardless action that
only allows transitioning from (ℓ⊥, σ) to (ℓ⊥, σ) with probability 1.
The crucial observation is that the fixed scheduler R and the fixed initial
configuration c0 lead to only finitely many admissible paths of length at most n.
Hence, all sets Sn and An(s) are finite. Thus by [31, Theorem 7.1.9], there exists
an optimal stationary and deterministic scheduler M for Mn that maximizes the
expected total reward. Now, we define the corresponding scheduler Sn : ConfP →
GT ⊥× Σ for the program P by Sn(c) = R(M(c)) for all c ∈Sn. Otherwise, i.e.,
if c ∈Conf \ Sn, then Sn(c) can be defined arbitrarily.
In the following, let min(RP, n) be the random variable with min(RP, n)(ϑ) =
min(RP(ϑ), n) for all ϑ ∈Run. Now, by construction of the MDP Mn and
optimality of the scheduler M, we have
EP
Sn,σ0(RP) ≥EP
R,σ0(min(RP, n)) =
X
i∈N
i · PP
R,σ0(min(RP, n) = i)
=
n
X
i=0
i · PP
R,σ0(RP = i)
(‡)
as we can fully model the first n steps of P’s execution under scheduler R within
the MDP Mn. Thus, we have
sup
S∈Π
EP
S,σ0(RP) ≥lim
n→∞EP
Sn,σ0(RP)
(as Sn ∈Π for all n ∈N)
≥lim
n→∞
n
X
i=0
i · PP
R,σ0(RP = i)
(by (‡))
=
X
i∈N
i · PP
R,σ0(RP = i)
(6)
= EP
R,σ0(RP).
Hence, supS∈Π EP
S,σ0(RP) ≥supR∈ΠHD EP
R,σ0(RP).
⊓⊔

