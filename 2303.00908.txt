Interactive Text Generation‚àó
Felix Faltings1
Michel Galley2
Baolin Peng2
Kiant√© Brantley3
Weixin Cai2
Yizhe Zhang2
Jianfeng Gao2
Bill Dolan2
1MIT 2Microsoft 3Cornell University
Abstract
Users interact with text, image, code, or
other editors on a daily basis. However, ma-
chine learning models are rarely trained in the
settings that reÔ¨Çect the interactivity between
users and their editor. This is understandable
as training AI models with real users is not
only slow and costly, but what these models
learn may be speciÔ¨Åc to user interface design
choices.
Unfortunately, this means most of
the research on text, code, and image gener-
ation has focused on non-interactive settings,
whereby the model is expected to get every-
thing right without accounting for any input
from a user who may be willing to help.
We introduce a new Interactive Text Genera-
tion task that allows training generation mod-
els interactively without the costs of involving
real users, by using user simulators that pro-
vide edits that guide the model towards a given
target text. We train our interactive models us-
ing Imitation Learning, and our experiments
against competitive non-interactive generation
models show that models trained interactively
are superior to their non-interactive counter-
parts, even when all models are given the same
budget of user inputs or edits.
1
Introduction
Advances in generative modeling have made it pos-
sible to automatically generate high-quality texts
(Brown et al., 2020), code (Chen et al., 2021), and
images (Ramesh et al., 2021), but these creations
can be unsatisfactory in many respects. For ex-
ample, they often suffer from content errors‚Äîe.g.,
hallucinations (Ji et al., 2022)‚Äîthat may require
help from the user. Even if the generation is of
good quality, it may not be the kind of text, code,
or image that the user was hoping to obtain. Indeed,
open-ended and complex generation tasks are often
‚àóWork done at Microsoft Research. Corresponding au-
thors: faltings@mit.edu, {mgalley,bapeng}@microsoft.com.
Yizhe Zhang is currently at Apple.
NLG
model
user input
NASA is 
launching a 
spacecraft to 
the moon on 
Monday ‚Ä¶
NLG
model
NLG
model
...
Interactive Text generation (ITG)
NASA is 
launching a 
spacecraft to 
Mars moon in 
2020 Monday ‚Ä¶
user input
NASA
user input
Text generation (e.g., keywords-to-text)
NASA is 
launching a 
spacecraft to 
Mars in 
2020 ‚Ä¶
NLG
model
NASA
launching
spacecraft
user input
final
text
Figure 1: The ITG framework of this paper allows gen-
eration models to be trained and evaluated by directly
interacting with users or user simulators.
underspeciÔ¨Åed (e.g., from a simple prompt), which
makes it almost impossible for the model to sat-
isfy user‚Äôs needs without additional information.
Distinguishing the real user need from the need
as initially presented to the system has been the
focus of decades of research (Taylor, 1968), and
usually requires interacting with users to clarify
their needs (Gao et al., 2022). Unfortunately, much
of the research in generative models have been in
‚Äúone-shot‚Äù settings, which don‚Äôt allow any kind of
iterative reÔ¨Ånements to steer the generation towards
what the user really wants.
This paper introduces a new end-to-end genera-
tion task, Interactive Text Generation, that accounts
for interactivity without requiring real users during
training.1 The framework is illustrated in Figure 1,
where the model and a ‚Äúuser‚Äù take turns editing the
text until a given stopping criterion is met. As this
setup would be impractical to train with real users,
we rely on a user simulator that provides a few
high-quality edits that guide the model towards a
given target text. This interspersion of user edits in
the generation process allows text generation mod-
1While this paper focuses on text, the main ideas of this
paper would be relatively easy to apply to other modalities.
The task and algorithms of this paper can be used almost ‚Äúas is‚Äù
when applied to other text-based tasks, e.g., code generation.
arXiv:2303.00908v1  [cs.CL]  2 Mar 2023

els to more efÔ¨Åciently use inferential capabilities of
large language models (LLM). Contrast interactive
text generation in Figure 1 with conventional text
generation, where both systems are given exactly
three user input words. Interactive text generation
leverages LLM‚Äôs capability to infer, e.g., ‚Äúspace-
craft‚Äù from ‚ÄúNASA‚Äù and allows it to focus on parts
of the text (e.g., ‚ÄúMonday‚Äù) that are more difÔ¨Åcult
to predict correctly, and this allows the interactive
approach to generate text that better meets user‚Äôs
needs.
Our work makes the following contributions:
‚Ä¢ We propose a task and framework for interactive
text generation, and release models, a dataset,
and user simulators. Crucially, this task evaluates
generation models with the same budget of user
inputs or edits, to ensure the comparison between
interactive and non-interactive models is fair.
‚Ä¢ We present methods to train Transformer-based
(Vaswani et al., 2017) interactive text editing
models using imitation learning, where our mod-
els learn to imitate an expert that dynamically
constructs a trajectory from the current docu-
ment state to the goal state (target document).
Our editing models include both autoregressive
and non-autoregressive versions, with the non-
autoregressive one achieving the best results.
‚Ä¢ We show that interactivity indeed does help
thanks to imitation learning when compared to
their non-interactive counterparts, across differ-
ent evaluation metrics, model types, and model
sizes and with the same amount of user inputs.
This Ô¨Ånding is consistent with prior work show-
ing that user needs are often best handled interac-
tively (Oddy, 1977; Belkin et al., 1995; Radlin-
ski and Craswell, 2017; Gao et al., 2022), and
conÔ¨Årms that our benchmark helps quantify the
beneÔ¨Åt of interactivity in text generation.
‚Ä¢ As user simulation is a key component of this
new task, we contribute different user simula-
tors. Our experiments show performance of our
models remains consistent with different user
simulators, which highlights the robustness of
our new benchmark.
Another contribution is that text generation in this
work is not merely aimed at producing well-formed
text, but also at creating text that is tied to user
needs. We release this framework with the hope it
will help researchers in NLP, Imitation Learning,
Reinforcement Learning, and AI in general as it
provides an environment to train AI agents that
directly interact with users and user simulators.2
2
Task: Interactive Text Generation
The task introduced in this paper considers a simple
setting where the system and user collaboratively
write a document. As our goal is to train generation
models that can do most of the heavy lifting, this
task gives a more directional role to the user, while
the bulk of the text is generated by the system.
Therefore, our task is akin to instructor-executor
frameworks (Hu et al., 2019; Kiseleva et al., 2022)
seen in other tasks. In the case of text generation,
motivational examples of such instructor-executor
interactions include a student writing a paper while
getting occasional feedback from an advisor, or a
freelance writer getting instructions from a client.
Our task models the interaction between a user
and system, where the two parties successively take
turns making changes to the current draft. As this
interaction applies to both training and testing, it
would be unrealistic to assume we have real users
in both cases, and we therefore rely on user sim-
ulators. While building effective user simulators
is notoriously hard in tasks such as dialog (Schatz-
mann et al., 2007; Li et al., 2016; Lin et al., 2022;
Gao et al., 2019), it is less so in our work given
how we frame the interactive generation task: We
assume there is a goal text, which one can think
of as representing the user‚Äôs desire or needs. The
system does not have knowledge of the goal, but
the user simulator does. The objective of the agent
is to get as close as possible to the goal text as
possible, while minimizing the number of edits the
user simulator needs to make. Given this task for-
mulation, we call the two parties respectively agent
(for system) and oracle (for user simulator).
This framing makes it much easier to design ef-
fective user simulators, as a comparison between
the current draft and goal text can help infer useful
text edits. While agents in this setup are given an
advantage by interacting with an oracle that has
knowledge of the goal document, the number of
oracle edits is generally set to be small, and we
ensure comparisons between all models (includ-
ing non-interactive models) are fair by giving each
model the same budget of edits or inputs derived
from the goal document.
2Code and models for this paper are available at:
http://aka.ms/InteractiveTextGeneration

Editing 
Model
document ùëëùë°
‚Ä≤, 
with user edits
(re)generated 
document ùëëùë°
gold ùëë‚àó
Oracle
Model
Barack
Obama
is
a
tennis
44th
player
choose a position
insert
substitute
delete
choose an op.
president
preside
presidentess
presidence
‚Ä¶
‚Ä¶
choose a word
input draft
Barack
Obama
is
a
tennis
44th
president
output draft
Edit Model, either:
Token Edit Model (Repeat x N)
Barack
Obama
is
a
tennis
44th
player
input draft
president
output draft
Barack
Obama
is
a
tennis
44th
Barack
is
a
tennis
player
Barack
Obama
was
the
44th
president
.
Align to gold
Barack
Obama
is
a
tennis
44th
Barack
is
a
tennis
player
player
Produce edits
or
Architecture
User simulator
Seq2Seq Transformer Model
Oracle Model
Figure 2: Model architecture: the current draft repeatedly goes through two phases. First, it is edited with the edit
model (either a non-autoregressive token edit model or a standard seq2seq Transformer), and then it is annotated
with edits coming from the user simulator. While the user simulator has access to the gold document, all our
generation systems (interactive and non-interactive) are evaluated with the same budget of user inputs.
2.1
Task Formulation
We now give a more formal description of our task.
Consider two players, the agent and oracle. Let
d‚àó‚ààV ‚àó, where V ‚àóis the set of all strings made
up of tokens in vocabulary V , and let d0 = œµ, the
empty string. Let s(¬∑; d‚àó) : V ‚àó‚ÜíR a scoring
function and let Œ¥ > 0. Then, the protocol of the
task is as follows. For t = 1, ..., T:
1. Oracle observes dt‚àí1. If s(dt‚àí1; d‚àó) > 1 ‚àí
Œ¥, stop. Otherwise, the oracle produces dO
t
according to d‚àóand dt‚àí1.
2. Agent observes dO
t and produces dt.
Let œÑ
‚â§T be the time at which the interac-
tion stopped. We evaluate the task by looking at
s(dœÑ; d‚àó) and œÑ. The higher s(dœÑ; d‚àó) and the lower
œÑ, the better.
We assumed here that each instance of the task
is parameterized by a goal document d‚àó. However,
one could instead use multiple goal documents,
d‚àó
1, ..., d‚àó
M, or even replace s(¬∑; d‚àó) with some gen-
eral score function s(¬∑) that is independent of any
goal documents. In such an extension, one would
need to require that the oracle drafts, dO
t , still pro-
vide information about d‚àó
1, ..., d‚àó
M or s.
2.2
Oracle
The above task deÔ¨Ånition is agnostic to the choice
of oracle. In general, the only requirement we have
of an oracle is that the drafts it produces share infor-
mation about the goal. We now describe the oracle
used in our work, which follows a relatively simple
strategy. Because it has knowledge of the target
document d‚àó, it can make edits that turn the current
draft into the target. Evidently, the oracle should
not simply produce the target. Therefore we as-
sume that the oracle makes a small random number
of edits at each turn, while using some heuristics to
prioritize edits. For example, the oracle might try
to produce more informative edits Ô¨Årst. We discuss
different heuristics in the experiments section.
Operationally, we can determine a set of edits
that takes a draft d to a target d‚àóby computing an
alignment. For simplicity, we only consider mono-
tonic alignments, i.e. those that don‚Äôt involve token
movements. Given d1, d2 ‚ààV ‚àó, we deÔ¨Åne an align-
ment A as a pair ¬Ød1, ¬Ød2 ‚àà¬ØV ‚àó, with ¬ØV = V ‚à™{_},
where _ is a special blank token. For each di, we
require that there exists a monotonic (increasing)
function œÉi such that dik = ¬ØdiœÉi(k), where dik is
the kth token in di. This is simply requiring that
the order of the tokens in di is preserved in ¬Ødi. Fur-
thermore, we require | ¬Ød1| = | ¬Ød2|. To see that this
deÔ¨Ånes a monotonic alignment, notice that for each
token ¬Ød1k, there is a corresponding token in ¬Ød2,
namely ¬Ød2k. If ¬Ød1k is blank but not ¬Ød2k, this corre-
sponds to an insertion, while the converse would
correspond to a deletion. If neither token is blank,
this corresponds to a substitution. See Figure 2
for an illustration. Finally, since œÉi are monotonic,
there are no crossings in the alignment. This then
allows us to deÔ¨Åne a set of edits as the set of pairs
¬Ød1k, ¬Ød2k where ¬Ød1k Ã∏= ¬Ød2k.

gold
document ùëë‚àó
empty
document
ùëë0
current
document 
ùëë3
ùëë1
ùëë2
Expert policy ùúãùê∏
provides trajectory
from ùëë3 to ùëë‚àó
Markov Decision Process:
State space: 
all documents: ùëëùëñ
Action space: 
all edits: ùëë‚Üíùëë‚Ä≤
Current state encodes:
‚Ä¢ words of ùëë3
‚Ä¢ oracle edits of current and past 
documents (from user simulator)
Figure 3: Interactive text generation as imitation learn-
ing: The editing model (agent policy œÄŒ∏) is trained to
imitate the expert policy œÄE.
Given two documents d1, d2 there are many pos-
sible alignments. A minimal alignment would cor-
respond to the Levenshtein distance. However,
such minimal alignments often look unnatural, and
do not represent the way a human would edit text.
Instead, we compute an alignment based on a token-
wise similarity score s(¬∑, ¬∑; d1, d2) : ¬ØV √ó ¬ØV ‚ÜíR.
Note that this also deÔ¨Ånes a score for blank tokens.
We can then use dynamic programming (as in the
Smith-Waterman algorithm) to compute an align-
ment that maximizes the total score,
S( ¬Ød1, ¬Ød2; d1, d2) =
X
k
s( ¬Ød1k, ¬Ød2k; d1, d2).
In practice, we use the cosine similarity between
the embeddings computed by a large pretrained
language model, such as BERT (Devlin et al., 2019),
in the same vein as the BERTSCORE (Zhang et al.,
2020a).
3
Model
We model our task as a Markov Decision Pro-
cess(Sutton and Barto, 1998) (S, A, T, R) with a
hidden variable d‚àó‚àºD. In our case, both the
state space S and action space A correspond to the
space of documents V ‚àó, and the transition function
T(¬∑, ¬∑; d‚àó) : V ‚àó√ó V ‚àó‚Üí[0, 1] is effectively deter-
mined by the oracle. Given a draft d produced by
the agent (i.e. an action), the oracle edits it to give
a new draft d‚Ä≤. The reward R : S √ó A ‚Üí[0, 1]
simply returns 1 if the document is right or 0 other-
wise. However, as we will see, the reward will not
be important for our approach.
We now describe the agent policy œÄŒ∏. Because
of the hidden variable d‚àó, the optimal policy will
not be memoryless because it needs to make some
inference about d‚àó. It should therefore have access
to the history of drafts exchanged between itself
and the oracle, i.e. the drafts d0, ..., dM exchanged
so far. Because this history of drafts could become
prohibitively large, we keep track of only the last
two drafts, denoted d and d‚àí1 respectively, in the
form of a diff. Additionally, we track all tokens
inserted by the oracle along the entire history. Con-
cretely, we take V = V ‚Ä≤ √ó {1, ..., k}. Here, V ‚Ä≤ is
a base vocabulary, which we augment with a set of
labels. These labels indicate tokens that stayed the
same between d‚àí1 and d, tokens that were added
or deleted by the agent, and tokens that were added
or deleted by the oracle. These labels are repre-
sented by colors and crossed out words in Figure 2.
After each turn, we clear all the labels, except for
tokens inserted by the oracle. We parameterize œÄŒ∏
as a learned neural network and explore two differ-
ent approaches: a non-autoregressive token editing
model, and a standard autoregressive model, which
we describe in more detail in the sequel. We begin
by describing our high-level training strategy.
3.1
Training
Our goal is to learn a policy that produces a doc-
ument close to the target d‚àóin a minimal number
of editing rounds. Instead of trying to express this
goal with a reward function, we propose to use
the following simple policy: in any given state,
try to produce d‚àó.3 This can be seen as a form of
imitation learning with an expert policy œÄE that
produces d‚àóin any state, as illustrated in Figure 3.
We thus propose to learn this policy by optimizing
an objective of the form
L(Œ∏) = Ed‚àó‚àºDEd‚àºDœÄS [‚àílog œÄŒ∏(d‚àó|d)] ,
where œÄS is a policy used to sample states during
training and DœÄS is the distribution over states d
induced by œÄS. A good candidate is to use the
current policy œÄŒ∏ as a sampling policy. However,
in the early stages of training, œÄŒ∏ will be a very
poor policy that will visit many unnecessary states.
Thus, a standard approach in imitation learning
(Ross et al., 2011) is to use a mixture between an
3While this seems like a straightforward strategy, note that
it may not be optimal. Indeed, a policy that always attempts
to produce d‚àómay produce a lot of text that it is uncertain
about, requiring a lot of wasted edits from the oracle to Ô¨Åx.
Nevertheless, such a policy is attractively simple and avoids
the complications of constructing appropriate rewards, and
then learning from those rewards.

expert policy œÄE and œÄŒ∏, so œÄS = ŒªœÄE +(1‚àíŒª)œÄŒ∏,
with Œª ‚àà[0, 1], and where Œª is usually annealed
from 1 down to 0 during training. In our case, the
expert policy simply produces d‚àóin any state, ef-
fectively ending the trajectory. Thus, the sampling
policy consists of following œÄŒ∏ for up to T steps,
where T ‚àºGeom(Œª) is geometrically distributed
with parameter Œª. This procedure is outlined in
Algorithm 1.
3.2
Left to Right Autoregressive Model
Our Ô¨Årst model is a simple sequence to sequence
(S2S) model that generates outputs in a left-to-right
autoregressive manner,
œÄŒ∏(d‚Ä≤|d) =
M
Y
i=1
œÄŒ∏(d‚Ä≤
i|d‚Ä≤
<i, d),
where d‚Ä≤
<i = d‚Ä≤
1d‚Ä≤
2...d‚Ä≤
i‚àí1 and M = |d‚Ä≤|. We then
easily get log-likelihoods for training,
log œÄŒ∏(d‚Ä≤|d) =
M
X
i=1
log œÄŒ∏(d‚Ä≤
i|d‚Ä≤
<i, d).
To decode we simply use any of the standard
algorithms for autoregressive models, namely beam
search, top k or top p sampling, etc.
3.3
Token Editing Model
While our S2S model forms a reasonable base-
line, we note that it is not particularly adapted
for the task. It has already been noted that left-
to-right autoregressive models are not well suited
for keyword-based generation (Zhang et al., 2020b;
Xu and Carpuat, 2021). In our case, the S2S model
would also need to regenerate an entire document
in order to make a single change. Thus, we in-
stead propose the following token editing model.
This model iteratively edits text by taking actions
a, where an action can be: inserting a word, sub-
stituting a word for another, or deleting a word.
For simplicity we do not consider word movements
(such as translations or exchanges). In order to take
an action, the model speciÔ¨Åes three things: a loca-
tion within the current draft (i ‚àà{1, 2, ..., |d|}), an
operation (insert, substitute, or delete), and, in the
case of insertions or substitutions, a token. As a
convention, inserted tokens are always placed to the
right of the selected location. Concretely, we im-
plement this model as a transformer with multiple
MLP heads for location, operation, and vocabulary
prediction. In order to decide when to stop editing,
Algorithm 1: High-Level Training
Data: D, N, B
Initialize œÄŒ∏;
Œª0 ‚Üê1;
repeat
œÄi ‚ÜêŒªiœÄE + (1 ‚àíŒªi)œÄŒ∏;
Sample batch of documents d1, ..., dB
from D;
Sample trajectories using d1, ..., dB as
targets and collect states into Di;
Compute loss according to model on Di;
Update œÄŒ∏ with gradient descent;
Update Œªi;
until convergence;
we add an additional head which outputs a stopping
probability. Each action then speciÔ¨Åes four things:
whether to keep editing, where to edit, what type of
edit to make, and a token in the case of insertions
and substitutions. Thus we factorize the policy as
œÄŒ∏(d‚Ä≤|d) =
X
a
T
Y
k=0
œÄŒ∏(ak|dk),
where d0 = d and dT = d‚Ä≤ in the product and the
sum is over all sequences of actions that turn d
into d‚Ä≤. Note that each term in the product is itself
parameterized as a product of four terms.4 This
editing model is illustrated in Figure 2.
3.3.1
Training
Recall that when training the model, we need to
compute log œÄŒ∏(d‚Ä≤|d). Here, the sum over all action
sequences is problematic, but we can lower bound
it. Note that, using the same alignment deÔ¨Ånition
as used by the oracle, we can specify a unique set
of actions A that take d to d‚Ä≤. We can then assume
that most alternative trajectories5 will have low
probability, so we restrict the sum to all possible
permutations of A. Let a = a1, a2, ..., aT be some
arbitrary ordering of the elements of A, where T =
|A|. Then we compute the lower-bound the log-
likelihood as follows:
4An action a = (s, l, o, v) is a tuple indicating whether
to stop, where to edit, what operation to make, and, in
the case of an insertion or substitution, a token.
The
model probability, œÄŒ∏(a|d), is then factorized as œÄŒ∏(a|d) =
œÄŒ∏(s|d)œÄŒ∏(l|d, s)œÄŒ∏(o|d, l, s)œÄŒ∏(v|d, o, l, s).
In practice,
each factor is parameterized by a different head of the neural
architecture.
5For example the inÔ¨Ånite number of trajectories that in-
volve inserting and deleting the same word.

Algorithm 2: Token Edit Model Objective
Data: d, d‚àó
Result: estimate of ‚àílog œÄŒ∏(d‚àó|d)
Compute alignment m between d‚àóand d;
Get actions A from m (i.e., query expert);
T ‚Üê|A|;
Uniformly sample k ‚àà{1, 2, ..., T};
Randomly permute A;
Apply actions a1, ..., ak to d to get dk;
return
T
T‚àík
P
i=k+1,...,T log œÄŒ∏(ai|dk);
log œÄŒ∏(d‚Ä≤|d) = log
X
a
T‚àí1
Y
k=0
œÄŒ∏(ak|dk)
‚â•log
X
œÉ‚ààST
T‚àí1
Y
k=0
œÄŒ∏(aœÉ(k)|dk)
‚â•C + 1
T!
X
œÉ‚ààST
T‚àí1
X
k=0
log œÄŒ∏(aœÉ(k)|dk),
(1)
where C = log(T!), and the last line comes from
Jensen‚Äôs inequality. Using the same trick6 of rear-
ranging the sum over time steps and orderings as in
Shen et al. (2020) we then rewrite the second term
as
EkEœÉ0:k‚àí1
Ô£Æ
Ô£∞
T
T ‚àík
X
œÉ(k)
log œÄŒ∏(aœÉ(k)|dk)
Ô£π
Ô£ª,
which we evaluate stochastically by sampling time
steps k and permutations œÉ, as outlined in Algo-
rithm 2.
Denoising
Because our model edits text, we can
naturally leverage denoising training (Lee et al.,
2018). Note that our estimator for the lower bound
on the log likelihood simply samples a random
number of edits from A, the set of edits that pro-
duces d‚Ä≤ from d, to obtain a draft d‚Ä≤‚Ä≤ that is partway
between d and d‚Ä≤. The model is then trained to
sample uniformly from the set of remaining edits
going from d‚Ä≤‚Ä≤ to d‚Ä≤. Thus, to incorporate denoising
training, we apply random edits to d‚Ä≤‚Ä≤ with some
small probability and recompute the set of remain-
ing edits that take d‚Ä≤‚Ä≤ to d‚Ä≤. This way the model
learns to correct small errors that it might introduce
6See appendix A for a full derivation.
Algorithm 3: Token Edit Model Sampling
Data: d, Œ±, N
Result: Generation from œÄŒ∏(¬∑|d)
s0 ‚Üê0; i ‚Üê0; d0 ‚Üêd;
G ‚Üê{};
repeat
si ‚ÜêœÄŒ∏(stop|di);
Sample non-stopping action ai
from ÀúœÄŒ∏(¬∑|di);
Apply ai to di to get di+1;
Add (si, di) to G;
i ‚Üêi + 1;
until pi > Œ± or i > N;
return di with highest si;
on its own during inference. This noising process
could also be tailored to speciÔ¨Åc types of errors,
such as repetitions. While it seems natural to then
use the model itself to introduce noise, we found
this to be detrimental. As we train the model to pro-
duce speciÔ¨Åc ground truth documents, the model
can generate many valid documents given the same
set of inputs, but these alternate documents can be
difÔ¨Åcult to differentiate from erroneous texts (e.g.,
grammatical errors). This creates a tendency for
the learned model to erase its own edits and to go
in circles, which we observed qualitatively.
3.3.2
Decoding
A simple approach to decoding is to sample from
œÄŒ∏ by successively sampling actions, i.e. deciding
whether to keep editing, sampling a location to edit,
an operation to perform, and (possibly) a token.
We can then repeat this until we decide to stop.
However, in practice we Ô¨Ånd several issues with
this approach.
First, as is the case with auto-regressive language
models, we Ô¨Ånd that sampling from only a subset
of actions improves generation quality. Intuitively,
this is because we avoid low probability actions
that lead to poor generations down the line. In the
case of top k sampling, this involves only consider-
ing the top k actions, ranked by their probabilities.
In the case of top p nucleus sampling (Holtzman
et al., 2020), we consider the smallest set of actions,
ordered by their probabilities, whose cumulative
probability is at least p.
Second, we Ô¨Ånd that the model has a tendency
to stop editing early. Stopping prematurely leads to
an excessive loss in generation quality when com-
pared to other errors, because after stopping there

is no way to recover. On the other hand, inserting
a token by mistake, for example, could potentially
be Ô¨Åxed by a deletion later on. To alleviate this, we
decode to a Ô¨Åxed depth and choose the best draft
in hindsight. Concretely, we ignore stops while
sampling but keep track of all the produced drafts.
After a stopping condition is met, we rerank all the
generated text and return the highest ranked one.
In our case we just use the stopping probabilities
to rerank, but one could also incorporate other met-
rics. The stopping condition can be whenever the
probability of stopping exceeds a certain threshold,
or a Ô¨Åxed number of steps has been exceeded. The
whole sampling procedure is given in Algorithm 3.
4
Experiments
4.1
Setting
4.1.1
Data
We consider single sentences from CNN/DailyMail
article summaries (Nallapati et al., 2016). While
we only consider single sentences for ease of imple-
mentation, we can extend our models to full docu-
ments in a straightforward way. Using news articles
gives complex and factually grounded sentences,
while restricting our attention to the summaries
keeps the sentences relatively self-contained. We
use version 3.0.0 from the Huggingface dataset hub
(Lhoest et al., 2021). We take the article summaries,
which we split into sentences and then Ô¨Ålter to sen-
tences with less than 64 tokens, where the tokens
are determined by the BART tokenizer from Hug-
gingface (Wolf et al., 2020). We then split the data
into train, test, and validation splits, with (approxi-
mately) 1M, 55K, and 45K instances respectively.
4.1.2
User Simulator
The environment is primarily determined by the
behavior of the oracle, i.e., the user simulator. We
consider the following methods for generating ora-
cle edits:
Ranking
Given a set of edits, we rank them
based on their informativeness. As a measure of
informativeness, we use idf scores.
Adjacent Edits
If the oracle simply returns the
most informative edits, it will have a tendency to
make edits in disparate parts of the text. For ex-
ample, it might return keywords from the end of
the document, when the current draft only covers
the start. In a realistic setting, users make edits
related to the current draft, they do not preempt the
end of the document. Thus, we limit the oracle to
producing adjacent edits, where an edit is adjacent
if it is adjacent to a match in the alignment to the
target.
Contiguous Edits
While adjacency will keep the
oracle edits relevant to the current draft, the oracle
may still have a tendency to produce a disconnected
set of edits. Instead, we limit the oracle to only
produce contiguous edits.
Complete Words
Finally, since the oracle oper-
ates on tokenized text, and the tokens may break up
words, we also constrain the oracle to edit complete
words.
4.1.3
Model
We implement our models using pretrained trans-
former models, with additional MLP heads to pre-
dict the edit actions in the case of the token editing
model. More precisely, we use the BART base and
BART large checkpoints made available through
the Huggingface transformers library (Wolf et al.,
2020). The models have (approximately) 140M
and 400M parameters respectively. Models were
trained on a single 16GB V100 GPU for 600 it-
erations with a sampling batch size of 10K, i.e.
B = 10000 in algorithm 1. We also used 300
warmup iterations (where Œª was not annealed).
4.1.4
Metrics
We evaluated generation using BLEU (Papineni
et al., 2002) and CHRF (Popovi¬¥c, 2015) with the
SacreBLEU implementation (Post, 2018). We also
evaluate using BLEURT (Sellam et al., 2020) and
BARTSCORE (Yuan et al., 2021), which are model-
based metrics that have been shown to correlate
well with human judgment on various text gener-
ation tasks. As BARTSCORE returns a score inter-
pretable as a log-probability, we report the natural
exponent of that score. We also use ROUGE (Lin,
2004) for evaluation as an alternative to BLEU, as
its scores tend to be less sensitive to length. In the
case of both BLEU and ROUGE, we perform eval-
uation with their unigram versions (BLEU-1 and
ROUGE-1) as they provide interpretability of the
results at the token level.
4.1.5
Decoding
We decode from the S2S autoregressive model us-
ing beam search with a beam size of 10. For the
editing model we use depth decoding with top 10

 0.35
 0.4
 0.45
 0.5
 0.55
 1
 2
 3
BLEURT
# of episodes
S2S
Editor
Editor large
 0.35
 0.4
 0.45
 0.5
 0.55
 1
 2
 3
chrF
# of episodes
 0.35
 0.4
 0.45
 0.5
 0.55
 1
 2
 3
BLEU
# of episodes
 0.35
 0.4
 0.45
 0.5
 0.55
 1
 2
 3
ROUGE
# of episodes
 0.01
 0.015
 0.02
 0.025
 0.03
 0.035
 0.04
 1
 2
 3
BARTScore
# of episodes
Figure 4: Results showing the beneÔ¨Åt of interactive generation, where #episodes=1 means the entire budget of
6 oracle edits is given to the model all at once (i.e., no interactivity). For #episodes=2, the model receives as input
3 oracle edits per episode (2x3=6). For #episodes=3, the model receives 2 oracle edits per episode (3x2=6).
sampling of actions, a stopping probability thresh-
old of 0.95 and a maximum depth of 64, i.e. we
stop decoding once we Ô¨Ånd a draft with stopping
probability greater than 0.95 or after 64 steps.
4.2
Interactivity
We start by probing whether interactivity helps by
comparing the performance of our model given dif-
ferent levels of interactivity. Concretely, we take
a Ô¨Åxed number of edits N provided by the oracle,
and compare the performance of the model when
those edits are provided over a varying number of
episodes M. Thus, for given M, the oracle and
agent interact over M episodes, with the oracle
making N/M edits at each episode. Note that the
total amount of information provided by the oracle
in each setting is thus the same. The only differ-
ence is that in interactive settings the agent is able
to make changes in between receiving edits from
the oracle. While this setup is not able to probe
the advantages that interactivity provides in terms
of human computer interaction, we still expect to
see better performance in the interactive case. For
example, the model may be able to preempt some
of the edits the oracle makes, allowing the oracle
to use its budget to make other edits.
4.3
Ablations
We provide extensive ablations of, and comparisons
between, model variants. As a benchmark for com-
parison, we look at the quality of the text produced
by our models after interacting with the oracle over
several episodes. For better comparisons we use
a Ô¨Åxed number of episodes and a Ô¨Åxed number of
oracle edits per episode, rather than a random num-
ber as we do during training. We use 3 edits and 4
episodes for CNN/DailyMail.
5
Results
Figure 4 presents our main results on interactivity.
We can see that for our main model, splitting the
same number of oracle edits over more episodes
leads to better scores across BLEU, BLEURT and
BERTSCORE. For example, comparing the setting
where the model receives all 6 oracle edits at the
start in one episode, against the setting where the
edits are given across 3 episodes, we see improve-
ments of about 7%, 4% and 5% (absolute % gains)
in terms of BLEU, BLEURT, and BERTSCORE re-
spectively. While these differences are not large in
absolute terms, we emphasize that this gain comes
exclusively from interactivity. The amount of in-
formation given to the model is the same in both
settings. This suggests that, even in this narrow
sense, interactivity helps. Note that there may also
be many more beneÔ¨Åts from a human-computer
standpoint that we cannot quantify here.
We also note that our token editing model (Ed-
itor) outperforms the left-to-right, autoregressive
sequence to sequence (S2S) model. While the dif-
ference is not staggering, it is notable given the
success of standard S2S models across a variety
of text generation tasks. As motivated in Section
3, the token editing model is more naturally suited
to the task, and we argue that it constitutes the
starting point for more interesting interactive mod-
els. For example, one could foresee training the
model using reinforcement learning, in which case
the structure of the editing model based on editing
actions is better suited than a S2S model.
Tables 1 and 2 present our ablations. Note that
these results use 4 episodes, with 3 oracle hints
per episode (a total of 12 oracle hints) compared
to the 6 total oracle hints in Fig 4, so the overall
results are higher. The baseline model is trained
with a noise level of 0.3, an unrestricted oracle

Model
BLEU BLEURT BART
Bart Editor (baseline)
0.76
0.70
0.14
noise level
0.0
0.74
0.69
0.14
0.1
0.78
0.73
0.16
0.2
0.73
0.67
0.12
oracle
contiguous
0.71
0.66
0.12
adjacent
0.60
0.61
0.09
adj+conÔ¨Åg
0.66
0.64
0.11
Sampling annealing rate
0.85
0.76
0.70
0.14
0.80
0.71
0.67
0.12
Table 1: Ablation results with different training hyper-
parameters. All ablations results are relative to the base-
line. BART stands for BARTSCORE.
Model
BLEU
BLEURT
BART
adj+conÔ¨Åg (baseline)
0.76
0.70
0.14
contiguous
0.76
0.69
0.14
adjacent
0.78
0.75
0.19
unrestricted
0.78
0.75
0.18
Table 2: Ablation results with different oracles changed
at test time.
and a sampling annealing rate of 0.9. All models
were evaluated with the adjacent and contiguous
oracle heuristics. Table 1 presents variations on
training parameters. The noise level is the amount
of noise injected during training, the oracle is the
heuristic used for the oracle during training, and
the sampling annealing rate indicates how quickly
we anneal from the expert to the trained model
while sampling (lower is faster). Table 2 compares
different oracle heuristics at test time.
We note that adding noise during training im-
proves results (e.g. noise level 0.1 vs. 0.0), while
annealing too fast can hurt performance (annealing
rate 0.8 vs. baseline). Interestingly, training with
an oracle that better matches the inference-time ora-
cle leads to worse performance (e.g. adj+contig vs.
baseline). It seems that using the most informative
oracle (which simply returns the most informative
edits, without restriction) leads to the best model
(baseline). Comparing different oracles at infer-
ence time, we see that adding restrictions to the
oracle leads to decreased scores, as expected. Inter-
estingly, we see that the most impactful restriction
seems to be enforcing contiguous edits. We sus-
pect that this is because contiguous edits are highly
predictable. For example, predicting ‚ÄúObama‚Äù af-
ter ‚ÄúBarack‚Äù is fairly obvious. Thus, if the oracle
didn‚Äôt provide contiguous edits, and only inserted
the word ‚ÄúBarack‚Äù, the model could score an easy
win by predicting ‚ÄúObama‚Äù.
Table 3 provides two examples of interactions
between the model and oracle. We emphasize that
these examples were not cherrypicked. Note how
the agent is able to revise its previous version of the
text based on the new information provided by the
oracle at each step. Qualitatively, these interactions
could be much more natural for a user than the
one shot setting that is currently prevalent in the
literature. However, a systematic evaluation of this
claim requires a more comprehensive user study
that lies out of the scope of this work.
6
Related Work
Text Generation
Prior work on natural language
generation (NLG) has largely focused on non-
interactive settings that have become increasingly
more challenging and open-ended, e.g., with gen-
eration from prompts (Fan et al., 2019), out-
lines (Rashkin et al., 2020), topics or keywords
(Ghazvininejad et al., 2016; Yan, 2016), plots
(Riedl and Young, 2010), descriptions (Jain et al.,
2017), events (Martin et al., 2017). This increase
of difÔ¨Åculty can make NLG more prone to content
quality issues, such as hallucinations (Wiseman
et al., 2017; Filippova, 2020; √áelikyilmaz et al.,
2020; Ji et al., 2022), that can require post-editing
from the user. More fundamentally, the trend to-
wards more open-ended generation often causes
tasks to be more ill-deÔ¨Åned, where a wide spectrum
of texts may be consistent with the NLG system‚Äôs
input. While such underspeciÔ¨Åcation may be appro-
priate for some forms of creative writing or story
generation, it becomes less desirable when the user
has speciÔ¨Åc preferences or requirements, which
may be best handled with interactive systems.
Welleck et al. (2022) presented models for text
generation and self-correction that also incorpo-
rated external feedback, but the nature of their feed-
back is quite different from ours and the tasks in
the two papers quite distinct. On the one hand,
Welleck et al. explored a wider variety of external
feedbacks (e.g., lexical constraints, toxicity, and
access to the solutions of math problems). On the
other hand, the oracle (i.e., user) in our work has an
action space identical to that of the agent, i.e., it can
insert, substitute, and delete words. This reÔ¨Çects
the situation where the AI-assisted user would want

Example 1
Example 2
idx:
719
idx:
1545
target: Progress 59 spacecraft will re-enter Earth‚Äôs
atmosphere in a week, Russia space
agency says .
target: The Large Hadron Collider (LHC) begins
again after a two-year shutdown .
oracle: Progress 59 re-enter
oracle: (LHC)
agent: Progress in Europe means he has a 59 per
cent chance to re-enter the EU .
agent: Curators at the Mars Space Telescope
(LHC) will launch in space the next
month .
oracle: Progress in Europe means he has a 59 per
cent spacecraft will re-enter the EU .
oracle: The Large at the Mars Space Telescope
(LHC) will launch in space the next
month .
agent: Progress in will help the Europe means
he has a 59 million per cent spacecraft
will re-enter the Earth EU .
agent: The Large X-ray Telescope at the Mars
Space Telescope (LHC) is located will
launch in
space the Atlantic .
next
month .
oracle: Progress will help the 59 million space-
craft will re-enter Earth‚Äôs says .
oracle: The Large X-ray Telescope (LHC) is lo-
cated in two-year shutdown .
agent: Progress comes after Boeing will help
the 59 spacecraft will re-enter NASA
Earth‚Äôs spacecraft, it says .
agent: The Large Mars Telescope-ray Telescope
(LHC) will end a two-year shutdown .
oracle: Progress comes after Boeing 59 spacecraft
will re-enter NASA‚Äôs spacecraft, Russia
space agency says .
oracle: The Large Hadron Collider (LHC) will
end a two-year shutdown .
agent: Progress comes as Boeing‚Äôs after Boeing
59 spacecraft will re-enter Earth NASA
spacecraft, Russia‚Äôs space agency says .
agent: The Large Hadron Collider (LHC) will
end a end a two-year shutdown .
Table 3: Example interactions between the oracle and agent. The oracle changes are shown red. Deleted words are
crossed out. Note that these are randomly selected examples (i.e., without cherrypicking), with the only curation
being the rejection of potentially offensive examples.
to freely type into the generated text, and not just
give feedback about what individual words to add.
Another difference is that access to external input
in our work is budgeted ‚Äî i.e., our interactive mod-
els are generally evaluated with same amounts of
user input words. This reÔ¨Çects the fact that external
input sometimes comes at a human cost, as it is the
case in collaborative text editing.
Interactivity
Several works have explored inter-
activity between humans and models to complete
tasks collaboratively. Lee et al. (2022) presented
a dataset to reveal large language models‚Äô capa-
bilities in assisting creative writing. Zhang et al.
(2022); Li et al. (2022) explored pre-trained lan-
guage models for code editing based on human-
written comments. Lahiri et al. (2022) created an
interactive framework to reÔ¨Åne user intents through
test case generations and user feedback. Kiseleva
et al. (2022) studied an interactive agent that can
interact with humans and follow natural language
instructions to achieve a certain goal in Minecraft.
Non Autoregressive Generation
Several works
considered non-autoregressive text generation (Gu
et al., 2019; Shen et al., 2020; Xu and Carpuat,
2021; Stern et al., 2019; Zhang et al., 2020b;
Welleck et al., 2019), but these models all focus on
one-shot text generation. While some models are
able to edit text (Gu et al., 2019; Xu and Carpuat,
2021), it is primarily used as a means to reÔ¨Åne the
model‚Äôs generations. On the other hand, we con-
sider editing text into a completely different version
conditioned on a given set of oracle-provided edits.
Text Editing
Text editing has previously been
considered from two different angles. On the one
hand, various works (Zhang et al., 2019; Du et al.,

2022) have studied the types of revisions made by
humans. On the other hand, works have focused
on modeling text edits (Guu et al., 2018; Yin et al.,
2018; Faltings et al., 2021), but they have generally
been restricted to modeling a single episode of user
edits at a time. In our framework, model edits and
user edits are interleaved. The recent versions of
GPT-3 (Brown et al., 2020; Ouyang et al., 2022)
and ChatGPT7 also offer text editing capabilities.
For example, ChatGPT can handle a prompt con-
taining a piece of text and instruction to improve
the text, and ChatGPT‚Äôs ability to execute that com-
mand is often quite impressive. We think our work
is complementary to GPT-3 and ChatGPT, as we
provide a framework for both modeling and eval-
uating edit models in a more end-to-end setting.
Ultimately, we think it will be beneÔ¨Åcial to Ô¨Åne-
tune very large language models such as GPT-3 in
an environment that exposes them to interaction
with a user or a user simulator (i.e., akin to user-in-
the-loop training). This beneÔ¨Åt is currently some-
what captured using reinforcement learning (RL) to
tune large language models from human feedback
(Ouyang et al., 2022), except that our approach fea-
tures an actual environment representative of the
end-user experience while Ouyang et al. (2022) is
more akin to ofÔ¨Çine RL.
7
Conclusions
We presented a new task and benchmark for text
generation that operationalizes interactivity be-
tween an agent and a user, without the need to
involve real users during training. Our framework
compares interactive and non-interactive systems
that are given the same amount of user inputs,
and shows that interactive text generation leads
to text of higher quality according to multiple auto-
mated evaluation metrics. We also present a non-
autoregressive editing model that outperforms a
standard sequence-to-sequence Transformer in var-
ious settings. All baseline, data, and models of this
paper are made available on github.
8
Limitations
A long-term goal of this research is to enable in-
teractive editing of full documents. For practical
reasons and to facilitate adoption, we limited text
length to 64 tokens, but we plan to extend our
benchmark and released datasets to also include
paragraph-level and multi-paragraph texts. Another
7https://chat.openai.com/chat
limitation of our work is that training is done with
simulated instead of real users, as training with
users in the loop can be extremely slow and costly.
To make our approximation of user behavior realis-
tic, our work relies on user inputs that real-world
users perform routinely (i.e., word insertion, dele-
tions, and substitutions) even settings that are not
assisted with AI. However, we recognize that the
behavior of real users in a AI-user collaborative set-
ting may differ from that of our user simulators, and
leave the study of such behavior for future work.
Finally, while the need for interactivity applies to
any kind of AI creation (e.g., also code and images),
we leave the application to other generation tasks
for future work. We note, however, that this paper
treats text as simple sequences of symbols, and our
work could readily be applied to other symbolic
creations (e.g., code generation). A future version
of this paper will include a (non-interactive) human
evaluation to judge the quality of generated texts.
Ethics Statement
Text generation systems, including those relying
on large language models, runs the risk of generat-
ing text that is unsafe (Bender et al., 2021; Bom-
masani et al., 2021; Weidinger et al., 2021), and
this also applies to generation models developed
in this work. While we have not observed genera-
tions that are overtly toxic or hateful, our models
can generate texts that are biased and offensive to
some readers. As our work focuses on generation
of non-Ô¨Åctional texts (in contrast to prior work on
story generation), our models also run the risk of
generating text that is factually incorrect. However,
the focus of our research is to provide interactive
capabilities to generation systems, and to make
them more in control of the user. As illustrated
in Figure 1, a back-and-forth between system and
user can make the generated text more factual, and
the same kind of interaction can also help increase
its safety. Such sanitization of generated texts in
our framework may still expose users with unsafe
content, so it is still recommended to use current
safeguards against hallucinations (Ji et al., 2022),
biases (Dhamala et al., 2021; Weinberg, 2022), and
other offensive content (Gehman et al., 2020; Welbl
et al., 2021; Kiritchenko et al., 2021; Jones et al.,
2022; Xu et al., 2022) before displaying text to real
users. In that sense, we think our work is comple-
mentary to current NLG research on hallucination
and safety.

Acknowledgments
We thank Faeze Brahman, Chris Brockett, Si-Qing
Chen, Gerold Hintz, Qiuyuan Huang, Zhang Li,
Kosh Narayanan, Hoifung Poon, Chris Quirk, Lars
Liden, Sudha Rao, Chenglong Wang, Bin Yu, Zhou
Yu, as well as members of the NLP and Deep Learn-
ing groups at Microsoft Research for helpful dis-
cussions and feedback.
References
Nicholas J. Belkin, Colleen Cool, Adelheit Stein, and
Ulrich Thiel. 1995. Cases, scripts, and information-
seeking strategies: On the design of interactive infor-
mation retrieval systems. Expert Systems With Appli-
cations, 9:379‚Äì395.
Emily M. Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021.
On the
dangers of stochastic parrots: Can language models
be too big? Proceedings of the 2021 ACM Confer-
ence on Fairness, Accountability, and Transparency.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ
Altman, Simran Arora, Sydney von Arx, Michael S.
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, Erik Brynjolfsson, S. Buch, Dallas Card,
Rodrigo Castellon, Niladri S. Chatterji, Annie S.
Chen, Kathleen A. Creel, Jared Davis, Dora Dem-
szky, Chris Donahue, Moussa Doumbouya, Esin
Durmus, Stefano Ermon, John Etchemendy, Kawin
Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale,
Lauren E. Gillespie, Karan Goel, Noah D. Goodman,
Shelby Grossman, Neel Guha, Tatsunori Hashimoto,
Peter Henderson, John Hewitt, Daniel E. Ho, Jenny
Hong, Kyle Hsu, Jing Huang, Thomas F. Icard,
Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Sid-
dharth Karamcheti, Geoff Keeling, Fereshte Khani,
O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay
Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal
Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Is-
abelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu
Ma, Ali Malik, Christopher D. Manning, Suvir Mir-
chandani, Eric Mitchell, Zanele Munyikwa, Suraj
Nair, Avanika Narayan, Deepak Narayanan, Ben-
jamin Newman, Allen Nie, Juan Carlos Niebles,
Hamed Nilforoshan, J. F. Nyarko, Giray Ogut, Lau-
rel J. Orr, Isabel Papadimitriou, Joon Sung Park,
Chris Piech, Eva Portelance, Christopher Potts,
Aditi Raghunathan, Robert Reich, Hongyu Ren,
Frieda Rong, Yusuf H. Roohani, Camilo Ruiz,
Jack Ryan, Christopher R‚Äôe, Dorsa Sadigh, Sh-
iori Sagawa, Keshav Santhanam, Andy Shih, Kr-
ishna Parasuram Srinivasan, Alex Tamkin, Rohan
Taori, Armin W. Thomas, Florian Tram√®r, Rose E.
Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai
Wu, Sang Michael Xie, Michihiro Yasunaga, Jiax-
uan You, Matei A. Zaharia, Michael Zhang, Tianyi
Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng,
Kaitlyn Zhou, and Percy Liang. 2021. On the op-
portunities and risks of foundation models. ArXiv,
abs/2108.07258.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,
Sandhini Agarwal,
Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. arXiv.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser,
Mohammad Bavarian,
Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welin-
der, Bob McGrew, Dario Amodei, Sam McCandlish,
Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-
ating large language models trained on code. arXiv.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019.
BERT: Pre-training of
deep bidirectional transformers for language under-
standing.
In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171‚Äì4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
J. Dhamala, Tony Sun, Varun Kumar, Satyapriya Kr-
ishna, Yada Pruksachatkun, Kai-Wei Chang, and
Rahul Gupta. 2021.
BOLD: Dataset and metrics
for measuring biases in open-ended language gener-
ation. Proceedings of the 2021 ACM Conference on
Fairness, Accountability, and Transparency.
Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung
Kim, Melissa Lopez, and Dongyeop Kang. 2022.
Understanding
iterative
revision
from
human-
written text. ArXiv, abs/2203.03802.
Felix Faltings, Michel Galley, Gerold Hintz, Chris
Brockett, Chris Quirk, Jianfeng Gao, and Bill Dolan.
2021. Text editing by command. In NAACL-HLT.
Angela Fan, Mike Lewis, and Yann Dauphin. 2019.
Strategies for structuring story generation. In ACL.

Katja Filippova. 2020.
Controlled hallucinations:
Learning to generate faithfully from noisy data. In
Findings of EMNLP.
Jianfeng Gao, Michel Galley, and Lihong Li. 2019.
Neural approaches to conversational AI.
Founda-
tions and Trends in Information Retrieval, 13(2-
3):127‚Äì298.
Jianfeng Gao, Chenyan Xiong, Paul Bennett, and Nick
Craswell. 2022.
Neural approaches to conversa-
tional information retrieval. arXiv.
Samuel Gehman, Suchin Gururangan, Maarten Sap,
Yejin Choi, and Noah A. Smith. 2020.
RealToxi-
cityPrompts: Evaluating neural toxic degeneration
in language models. In Findings of the Association
for Computational Linguistics: EMNLP 2020, pages
3356‚Äì3369.
Marjan Ghazvininejad, Xing Shi, Yejin Choi, and
Kevin Knight. 2016. Generating topical poetry. In
EMNLP.
Jiatao Gu, Changhan Wang, and Junbo Zhao. 2019.
Levenshtein transformer. Advances in Neural Infor-
mation Processing Systems, 32.
Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren,
and Percy Liang. 2018.
Generating sentences by
editing prototypes. Transactions of the Association
for Computational Linguistics, 6:437‚Äì450.
Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin
Choi. 2020. The curious case of neural text degener-
ation. ArXiv, abs/1904.09751.
Hengyuan Hu, Denis Yarats, Qucheng Gong, Yuan-
dong Tian, and Mike Lewis. 2019. Hierarchical de-
cision making by generating and following natural
language instructions. In NeurIPS.
Parag Jain, Priyanka Agrawal, Abhijit Mishra, Mo-
hak Sukhwani, Anirban Laha, and Karthik Sankara-
narayanan. 2017.
Story generation from se-
quence of independent short descriptions.
ArXiv,
abs/1707.05501.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,
Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea
Madotto, and Pascale Fung. 2022.
Survey of hal-
lucination in natural language generation.
ArXiv,
abs/2202.03629.
Keenan Jones, Enes Altuncu, Virginia N. L. Franqueira,
Yichao Wang, and Shujun Li. 2022. A comprehen-
sive survey of natural language generation advances
from the perspective of digital deception. arXiv.
Svetlana Kiritchenko, Isar Nejadgholi, and Kathleen C.
Fraser. 2021. Confronting abusive language online:
A survey from the ethical and human rights perspec-
tive. ArXiv, abs/2012.12305.
Julia Kiseleva, Ziming Li, Mohammad Aliannejadi,
Shrestha Mohanty, Maartje ter Hoeve, Mikhail S.
Burtsev, Alexey Skrynnik, Artem Zholus, Alek-
sandr I. Panov, Kavya Srinet, Arthur D. Szlam,
Yuxuan Sun, Katja Hofmann, Michel Galley, and
Ahmed Hassan Awadallah. 2022.
Interactive
grounded language understanding in a collaborative
environment: IGLU 2021. In NeurIPS 2021 Com-
petitions and Demonstrations Track, pages 146‚Äì161.
PMLR.
Shuvendu K. Lahiri, Aaditya Naik, Georgios Sakkas,
Piali Choudhury, Curtis von Veh, Madanlal Musu-
vathi, Jeevana Priya Inala, Chenglong Wang, and
Jianfeng Gao. 2022. Interactive code generation via
test-driven user-intent formalization.
Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018.
Deterministic non-autoregressive neural se-
quence modeling by iterative reÔ¨Ånement.
arXiv
preprint arXiv:1802.06901.
Mina Lee, Percy Liang, and Qian Yang. 2022. CoAu-
thor: Designing a human-ai collaborative writing
dataset for exploring language model capabilities.
CHI Conference on Human Factors in Computing
Systems.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario ≈†a≈°ko, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,
Sylvain Gugger, Cl√©ment Delangue, Th√©o Matus-
si√®re, Lysandre Debut, Stas Bekman, Pierric Cis-
tac, Thibault Goehringer, Victor Mustar, Fran√ßois
Lagunas, Alexander Rush, and Thomas Wolf. 2021.
Datasets: A community library for natural language
processing. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing: System Demonstrations, pages 175‚Äì184, On-
line and Punta Cana, Dominican Republic. Associ-
ation for Computational Linguistics.
Xiujun Li, Zachary C Lipton, Bhuwan Dhingra, Lihong
Li, Jianfeng Gao, and Yun-Nung Chen. 2016.
A
user simulator for task-completion dialogues. arXiv
preprint arXiv:1612.05688.
Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh
Jannu, Grant Jenks, Deep Majumder, Jared Green,
Alexey Svyatkovskiy, Shengyu Fu, et al. 2022.
CodeReviewer: Pre-training for automating code re-
view activities. arXiv preprint arXiv:2203.09095.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries.
In Text summarization
branches out, pages 74‚Äì81.
Hsien-Chin Lin, Christian Geishauser, Shutong Feng,
Nurul Lubis, Carel van Niekerk, Michael Heck,
and Milica Ga≈°i¬¥c. 2022.
GenTUS: Simulating

user behaviour and language in task-oriented dia-
logues with generative transformers. arXiv preprint
arXiv:2208.10817.
Lara J. Martin, Prithviraj Ammanabrolu, William Han-
cock, S. Singh, Brent Harrison, and Mark O. Riedl.
2017.
Event representations for automated story
generation with deep neural nets. In AAAI.
Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos
santos, Caglar Gulcehre, and Bing Xiang. 2016.
Abstractive text summarization using sequence-to-
sequence RNNs and beyond. In Proceedings of the
20th SIGNLL Conference on Computational Natural
Language Learning, pages 280‚Äì290.
Robert N. Oddy. 1977. Information retrieval through
man-machine dialogue. Journal of Documentation,
33:1‚Äì14.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke E. Miller, Maddie Simens, Amanda Askell,
Peter Welinder, Paul Francis Christiano, Jan Leike,
and Ryan J. Lowe. 2022. Training language models
to follow instructions with human feedback. ArXiv,
abs/2203.02155.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting of the Association for Compu-
tational Linguistics, pages 311‚Äì318.
Maja Popovi¬¥c. 2015. chrF: character n-gram F-score
for automatic MT evaluation. In Proceedings of the
Tenth Workshop on Statistical Machine Translation,
pages 392‚Äì395.
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186‚Äì
191, Belgium, Brussels. Association for Computa-
tional Linguistics.
Filip Radlinski and Nick Craswell. 2017. A theoretical
framework for conversational search. Proceedings
of the 2017 Conference on Conference Human Infor-
mation Interaction and Retrieval.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
Gray, Chelsea Voss, Alec Radford, Mark Chen, and
Ilya Sutskever. 2021. Zero-shot text-to-image gener-
ation. arXiv.
Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and
Jianfeng Gao. 2020.
PlotMachines:
Outline-
conditioned generation with dynamic plot state
tracking.
In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 4274‚Äì4295, Online. Associa-
tion for Computational Linguistics.
Mark O Riedl and Robert Michael Young. 2010. Narra-
tive planning: Balancing plot and character. In Jour-
nal of ArtiÔ¨Åcial Intelligence Research.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning.
In
Proc. of AISTATS, pages 627‚Äì635. PMLR.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based user
simulation for bootstrapping a pomdp dialogue sys-
tem. In Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics; Compan-
ion Volume, Short Papers, pages 149‚Äì152.
Thibault Sellam, Dipanjan Das, and Ankur Parikh.
2020.
BLEURT: Learning robust metrics for text
generation. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics,
pages 7881‚Äì7892, Online.
T. Shen, Victor Quach, R. Barzilay, and T. Jaakkola.
2020. Blank language models. In EMNLP.
Mitchell Stern, William Chan, Jamie Kiros, and Jakob
Uszkoreit. 2019. Insertion transformer: Flexible se-
quence generation via insertion operations.
In In-
ternational Conference on Machine Learning, pages
5976‚Äì5985. PMLR.
Richard S. Sutton and Andrew G. Barto. 1998. Intro-
duction to Reinforcement Learning. The MIT Press.
Robert S. Taylor. 1968. Question-negotiation and in-
formation seeking. Coll. Res. Libr.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems.
Laura
Weidinger,
John
F.
J.
Mellor,
Maribeth
Rauh, Conor GrifÔ¨Ån, Jonathan Uesato, Po-Sen
Huang, Myra Cheng, Mia Glaese, Borja Balle,
Atoosa Kasirzadeh, Zachary Kenton, Sande Minnich
Brown, William T. Hawkins, Tom Stepleton, Court-
ney Biles, Abeba Birhane, Julia Haas, Laura Rimell,
Lisa Anne Hendricks, William S. Isaac, Sean Legas-
sick, Geoffrey Irving, and Iason Gabriel. 2021. Eth-
ical and social risks of harm from language models.
ArXiv, abs/2112.04359.
Lindsay Weinberg. 2022. Rethinking fairness: An in-
terdisciplinary survey of critiques of hegemonic ML
fairness approaches. J. Artif. Intell. Res., 74:75‚Äì109.
Johannes Welbl, Amelia Glaese, Jonathan Uesato,
Sumanth Dathathri, John F. J. Mellor, Lisa Anne
Hendricks,
Kirsty
Anderson,
Pushmeet
Kohli,
Ben Coppin, and Po-Sen Huang. 2021.
Chal-
lenges in detoxifying language models.
ArXiv,
abs/2109.07445.

Sean Welleck, Kiant√© Brantley, Hal Daum√© Iii, and
Kyunghyun Cho. 2019. Non-monotonic sequential
text generation. In International Conference on Ma-
chine Learning, pages 6716‚Äì6726. PMLR.
Sean Welleck, Ximing Lu, Peter West, Faeze Brah-
man, Tianxiao Shen, Daniel Khashabi, and Yejin
Choi. 2022.
Generating sequences by learning to
self-correct. ArXiv, abs/2211.00053.
Sam Wiseman, Stuart Shieber, and Alexander Rush.
2017.
Challenges in data-to-document generation.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2253‚Äì2263, Copenhagen, Denmark.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, R√©mi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations, pages 38‚Äì45, Online. Asso-
ciation for Computational Linguistics.
Canwen Xu, Zexue He, Zhankui He, and Julian
McAuley. 2022. Leashing the inner demons: Self-
detoxiÔ¨Åcation for language models. In AAAI.
Weijia Xu and Marine Carpuat. 2021. Editor: an edit-
based transformer with repositioning for neural ma-
chine translation with soft lexical constraints. Trans-
actions of the Association for Computational Lin-
guistics, 9:311‚Äì328.
Rui Yan. 2016. i, poet: Automatic poetry composition
through recurrent neural networks with iterative pol-
ishing schema. In IJCAI.
Pengcheng Yin, Graham Neubig, Miltiadis Allama-
nis, Marc Brockschmidt, and Alexander L Gaunt.
2018. Learning to represent edits. arXiv preprint
arXiv:1810.13337.
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
BARTScore: Evaluating generated text as text gen-
eration. ArXiv, abs/2106.11520.
Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie,
Junyi Jessy Li, and Milos Gligoric. 2022. CoditT5:
Pretraining for source code and natural language
editing. arXiv preprint arXiv:2208.05446.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020a.
BERTScore:
Evaluating text generation with bert.
In Interna-
tional Conference on Learning Representations.
Xuchao Zhang, Dheeraj Rajagopal, Michael Gamon,
Sujay Kumar Jauhar, and ChangTien Lu. 2019.
Modeling the relationship between user comments
and edits in document revision. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 5002‚Äì5011.
Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan,
Chris Brockett, and Bill Dolan. 2020b.
Pointer:
Constrained text generation via insertion-based gen-
erative pre-training. ArXiv, abs/2005.00558.
Asli √áelikyilmaz, Elizabeth Clark, and Jianfeng Gao.
2020.
Evaluation of text generation:
A survey.
ArXiv, abs/2006.14799.

A
Additional details on training
objective
Recall that we derived the following lower-bound
for the log-likelihood under our token editing
model,
log œÄŒ∏(d‚Ä≤|d) ‚â•C + 1
T!
X
œÉ‚ààST
T‚àí1
X
k=0
log œÄŒ∏(aœÉ(k)|dk).
We can rewrite this to get a more convenient es-
timator by exchanging the order of the sums. This
is essentially the same derivation as presented in
(Shen et al., 2020), which we reproduce here for
reference. Then we have,
1
T!
X
œÉ‚ààST
T‚àí1
X
k=0
log œÄŒ∏(aœÉ(k)|dk)
= T ¬∑ EtEœÉ

log œÄŒ∏(aœÉ(t)|dt)

= T ¬∑ EtEœÉ0:t‚àí1EœÉt[log œÄŒ∏(aœÉ(t)|dt)],
where the Ô¨Årst expectation is over time steps, the
second is over permutations of the Ô¨Årst t indices,
and the third is over permutations of the t + 1‚Äôth
index, conditioned on the Ô¨Årst t. The second in-
equality comes from the observation that œÉ(t) only
depends on the permutation of the Ô¨Årst t + 1 in-
dices. Expending the last expectation gives the
same expression as presented in the main paper.

