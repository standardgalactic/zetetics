GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
Jiaxuan You * 1 Rex Ying * 1 Xiang Ren 2 William L. Hamilton 1 Jure Leskovec 1
Abstract
Modeling and generating graphs is fundamental
for studying networks in biology, engineering,
and social sciences. However, modeling com-
plex distributions over graphs and then efﬁciently
sampling from these distributions is challenging
due to the non-unique, high-dimensional nature of
graphs and the complex, non-local dependencies
that exist between edges in a given graph. Here we
propose GraphRNN, a deep autoregressive model
that addresses the above challenges and approx-
imates any distribution of graphs with minimal
assumptions about their structure. GraphRNN
learns to generate graphs by training on a represen-
tative set of graphs and decomposes the graph gen-
eration process into a sequence of node and edge
formations, conditioned on the graph structure
generated so far. In order to quantitatively evalu-
ate the performance of GraphRNN, we introduce
a benchmark suite of datasets, baselines and novel
evaluation metrics based on Maximum Mean Dis-
crepancy, which measure distances between sets
of graphs. Our experiments show that GraphRNN
signiﬁcantly outperforms all baselines, learning to
generate diverse graphs that match the structural
characteristics of a target set, while also scaling
to graphs 50× larger than previous deep models.
1. Introduction and Related Work
Generative models for real-world graphs have important
applications in many domains, including modeling physi-
cal and social interactions, discovering new chemical and
molecular structures, and constructing knowledge graphs.
Development of generative graph models has a rich history,
and many methods have been proposed that can generate
*Equal contribution 1Department of Computer Science, Stan-
ford University, Stanford, CA, 94305 2Department of Com-
puter Science, University of Southern California, Los Ange-
les, CA, 90007.
Correspondence to:
Jiaxuan You <jiax-
uan@stanford.edu>.
Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).
graphs based on a priori structural assumptions (Newman,
2010). However, a key open challenge in this area is develop-
ing methods that can directly learn generative models from
an observed set of graphs. Developing generative models
that can learn directly from data is an important step to-
wards improving the ﬁdelity of generated graphs, and paves
a way for new kinds of applications, such as discovering
new graph structures and completing evolving graphs.
In contrast, traditional generative models for graphs (e.g.,
Barab´asi-Albert model, Kronecker graphs, exponential ran-
dom graphs, and stochastic block models) (Erd˝os & R´enyi,
1959; Leskovec et al., 2010; Albert & Barab´asi, 2002;
Airoldi et al., 2008; Leskovec et al., 2007; Robins et al.,
2007) are hand-engineered to model a particular family of
graphs, and thus do not have the capacity to directly learn
the generative model from observed data. For example, the
Barab´asi-Albert model is carefully designed to capture the
scale-free nature of empirical degree distributions, but fails
to capture many other aspects of real-world graphs, such as
community structure.
Recent advances in deep generative models, such as varia-
tional autoencoders (VAE) (Kingma & Welling, 2014) and
generative adversarial networks (GAN) (Goodfellow et al.,
2014), have made important progress towards generative
modeling for complex domains, such as image and text data.
Building on these approaches a number of deep learning
models for generating graphs have been proposed (Kipf
& Welling, 2016; Grover et al., 2017; Simonovsky & Ko-
modakis, 2018; Li et al., 2018). For example, Simonovsky
& Komodakis 2018 propose a VAE-based approach, while
Li et al. 2018 propose a framework based upon graph neural
networks. However, these recently proposed deep mod-
els are either limited to learning from a single graph (Kipf
& Welling, 2016; Grover et al., 2017) or generating small
graphs with 40 or fewer nodes (Li et al., 2018; Simonovsky
& Komodakis, 2018)—limitations that stem from three fun-
damental challenges in the graph generation problem:
• Large and variable output spaces: To generate a graph
with n nodes the generative model has to output n2 values
to fully specify its structure. Also, the number of nodes
n and edges m varies between different graphs and a
generative model needs to accommodate such complexity
and variability in the output space.
arXiv:1802.08773v3  [cs.LG]  23 Jun 2018

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
• Non-unique representations: In the general graph gen-
eration problem studied here, we want distributions over
possible graph structures without assuming a ﬁxed set of
nodes (e.g., to generate candidate molecules of varying
sizes). In this general setting, a graph with n nodes can
be represented by up to n! equivalent adjacency matrices,
each corresponding to a different, arbitrary node order-
ing/numbering. Such high representation complexity is
challenging to model and makes it expensive to compute
and then optimize objective functions, like reconstruc-
tion error, during training. For example, GraphVAE (Si-
monovsky & Komodakis, 2018) uses approximate graph
matching to address this issue, requiring O(n4) opera-
tions in the worst case (Cho et al., 2014).
• Complex dependencies: Edge formation in graphs in-
volves complex structural dependencies. For example,
in many real-world graphs two nodes are more likely to
be connected if they share common neighbors (Newman,
2010). Therefore, edges cannot be modeled as a sequence
of independent events, but rather need to be generated
jointly, where each next edge depends on the previously
generated edges. Li et al. 2018 address this problem us-
ing graph neural networks to perform a form of “message
passing”; however, while expressive, this approach takes
O(mn2diam(G)) operations to generate a graph with m
edges, n nodes and diameter diam(G).
Present work. Here we address the above challenges and
present Graph Recurrent Neural Networks (GraphRNN),
a scalable framework for learning generative models of
graphs. GraphRNN models a graph in an autoregressive (or
recurrent) manner—as a sequence of additions of new nodes
and edges—to capture the complex joint probability of all
nodes and edges in the graph. In particular, GraphRNN
can be viewed as a hierarchical model, where a graph-level
RNN maintains the state of the graph and generates new
nodes, while an edge-level RNN generates the edges for
each newly generated node. Due to its autoregressive struc-
ture, GraphRNN can naturally accommodate variable-sized
graphs, and we introduce a breadth-ﬁrst-search (BFS) node-
ordering scheme to drastically improve scalability. This BFS
approach alleviates the fact that graphs have non-unique
representations—by collapsing distinct representations to
unique BFS trees—and the tree-structure induced by BFS
allows us to limit the number of edge predictions made for
each node during training. Our approach requires O(n2)
operations on worst-case (i.e., complete) graphs, but we
prove that our BFS ordering scheme permits sub-quadratic
complexity in many cases.
In addition to the novel GraphRNN framework, we also
introduce a comprehensive suite of benchmark tasks and
baselines for the graph generation problem, with all code
made publicly available1. A key challenge for the graph
generation problem is quantitative evaluation of the quality
of generated graphs. Whereas prior studies have mainly re-
lied on visual inspection or ﬁrst-order moment statistics for
evaluation, we provide a comprehensive evaluation setup by
comparing graph statistics such as the degree distribution,
clustering coefﬁcient distribution and motif counts for two
sets of graphs based on variants of the Maximum Mean Dis-
crepancy (MMD) (Gretton et al., 2012). This quantitative
evaluation approach can compare higher order moments of
graph-statistic distributions and provides a more rigorous
evaluation than simply comparing mean values.
Extensive experiments on synthetic and real-world graphs
of varying size demonstrate the signiﬁcant improvement
GraphRNN provides over baseline approaches, including
the most recent deep graph generative models as well as
traditional models. Compared to traditional baselines (e.g.,
stochastic block models), GraphRNN is able to generate
high-quality graphs on all benchmark datasets, while the
traditional models are only able to achieve good perfor-
mance on speciﬁc datasets that exhibit special structures.
Compared to other state-of-the-art deep graph generative
models, GraphRNN is able to achieve superior quantita-
tive performance—in terms of the MMD distance between
the generated and test set graphs—while also scaling to
graphs that are 50× larger than what these previous ap-
proaches can handle. Overall, GraphRNN reduces MMD by
80%-90% over the baselines on average across all datasets
and effectively generalizes, achieving comparatively high
log-likelihood scores on held-out data.
2. Proposed Approach
We ﬁrst describe the background and notation for building
generative models of graphs, and then describe our autore-
gressive framework, GraphRNN.
2.1. Notations and Problem Deﬁnition
An undirected graph2 G = (V, E) is deﬁned by its node set
V = {v1, ..., vn} and edge set E = {(vi, vj)|vi, vj ∈V }.
One common way to represent a graph is using an adjacency
matrix, which requires a node ordering π that maps nodes
to rows/columns of the adjacency matrix. More precisely,
π is a permutation function over V (i.e., (π(v1), ..., π(vn))
is a permutation of (v1, ..., vn)). We deﬁne Π as the set of
all n! possible node permutations. Under a node ordering
π, a graph G can then be represented by the adjacency
matrix Aπ ∈Rn×n, where Aπ
i,j = 1[(π(vi), π(vj)) ∈E].
1The
code
is
available
in
https://github.com/
snap-stanford/GraphRNN, the appendix is available in
https://arxiv.org/abs/1802.08773.
2We focus on undirected graphs. Extensions to directed graphs
and graphs with features are discussed in the Appendix.

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
Note that elements in the set of adjacency matrices AΠ =
{Aπ|π ∈Π} all correspond to the same underlying graph.
The goal of learning generative models of graphs is to learn
a distribution pmodel(G) over graphs, based on a set of
observed graphs G = {G1, ..., Gs} sampled from data dis-
tribution p(G), where each graph Gi may have a different
number of nodes and edges. When representing G ∈G, we
further assume that we may observe any node ordering π
with equal probability, i.e., p(π) =
1
n!, ∀π ∈Π. Thus, the
generative model needs to be capable of generating graphs
where each graph could have exponentially many represen-
tations, which is distinct from previous generative models
for images, text, and time series.
Finally, note that traditional graph generative models (sur-
veyed in the introduction) usually assume a single input
training graph. Our approach is more general and can be
applied to a single as well as multiple input training graphs.
2.2. A Brief Survey of Possible Approaches
We start by surveying some general alternative approaches
for modeling p(G), in order to highlight the limitations of
existing non-autoregressive approaches and motivate our
proposed autoregressive architecture.
Vector-representation based models. One na¨ıve approach
would be to represent G by ﬂattening Aπ into a vector
in Rn2, which is then used as input to any off-the-shelf
generative model, such as a VAE or GAN. However, this
approach suffers from serious drawbacks: it cannot naturally
generalize to graphs of varying size, and requires training
on all possible node permutations or specifying a canonical
permutation, both of which require O(n!) time in general.
Node-embedding based models. There have been recent
successes in encoding a graph’s structural properties into
node embeddings (Hamilton et al., 2017), and one approach
to graph generation could be to deﬁne a generative model
that decodes edge probabilities based on pairwise relation-
ships between learned node embeddings (as in Kipf &
Welling 2016). However, this approach is only well-deﬁned
when given a ﬁxed-set of nodes, limiting its utility for the
general graph generation problem, and approaches based on
this idea are limited to learning from a single input graph
(Kipf & Welling, 2016; Grover et al., 2017).
2.3. GraphRNN: Deep Generative Models for Graphs
The key idea of our approach is to represent graphs under
different node orderings as sequences, and then to build an
autoregressive generative model on these sequences. As we
will show, this approach does not suffer from the drawbacks
common to other general approaches (c.f., Section 2.2), al-
lowing us to model graphs of varying size with complex
Figure 1. GraphRNN at inference time. Green arrows denote the
graph-level RNN that encodes the “graph state” vector hi in its
hidden state, updated by the predicted adjacency vector Sπ
i for
node π(vi). Blue arrows represent the edge-level RNN, whose
hidden state is initialized by the graph-level RNN, that is used to
predict the adjacency vector Sπ
i for node π(vi).
edge dependencies, and we introduce a BFS node order-
ing scheme to drastically reduce the complexity of learn-
ing over all possible node sequences (Section 2.3.4). In
this autoregressive framework, the model complexity is
greatly reduced by weight sharing with recurrent neural
networks (RNNs). Figure 1 illustrates our GraphRNN ap-
proach, where the main idea is that we decompose graph
generation into a process that generates a sequence of nodes
(via a graph-level RNN), and another process that then gen-
erates a sequence of edges for each newly added node (via
an edge-level RNN).
2.3.1. MODELING GRAPHS AS SEQUENCES
We ﬁrst deﬁne a mapping fS from graphs to sequences,
where for a graph G ∼p(G) with n nodes under node
ordering π, we have
Sπ = fS(G, π) = (Sπ
1 , ..., Sπ
n),
(1)
where each element Sπ
i ∈{0, 1}i−1, i ∈{1, ..., n} is an
adjacency vector representing the edges between node π(vi)
and the previous nodes π(vj), j ∈{1, ..., i −1} already in
the graph:3
Sπ
i = (Aπ
1,i, ..., Aπ
i−1,i)T , ∀i ∈{2, ..., n}.
(2)
For undirected graphs, Sπ determines a unique graph G,
and we write the mapping as fG(·) where fG(Sπ) = G.
Thus, instead of learning p(G), whose sample space cannot
be easily characterized, we sample the auxiliary π to get the
observations of Sπ and learn p(Sπ), which can be modeled
autoregressively due to the sequential nature of Sπ. At infer-
ence time, we can sample G without explicitly computing
p(G) by sampling Sπ, which maps to G via fG.
Given the above deﬁnitions, we can write p(G) as the
marginal distribution of the joint distribution p(G, Sπ):
p(G) =
X
Sπ
p(Sπ) 1[fG(Sπ) = G],
(3)
3We prohibit self-loops and Sπ
1 is deﬁned as an empty vector.

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
Algorithm 1 GraphRNN inference algorithm
Input: RNN-based transition module ftrans, output mod-
ule fout, probability distribution Pθi parameterized by θi,
start token SOS, end token EOS, empty graph state h′
Output: Graph sequence Sπ
Sπ
1 = SOS, h1 = h′, i = 1
repeat
i = i + 1
hi = ftrans(hi−1, Sπ
i−1) {update graph state}
θi = fout(hi)
Sπ
i ∼Pθi {sample node i’s edge connections}
until Sπ
i is EOS
Return Sπ = (Sπ
1 , ..., Sπ
i )
where p(Sπ) is the distribution that we want to learn using
a generative model. Due to the sequential nature of Sπ,
we further decompose p(Sπ) as the product of conditional
distributions over the elements:
p(Sπ) =
n+1
Y
i=1
p(Sπ
i |Sπ
1 , ..., Sπ
i−1)
(4)
where we set Sπ
n+1 as the end of sequence token EOS, to
represent sequences with variable lengths. We simplify
p(Sπ
i |Sπ
1 , ..., Sπ
i−1) as p(Sπ
i |Sπ
<i) in further discussions.
2.3.2. THE GRAPHRNN FRAMEWORK
So far we have transformed the modeling of p(G) to model-
ing p(Sπ), which we further decomposed into the product of
conditional probabilities p(Sπ
i |Sπ
<i). Note that p(Sπ
i |Sπ
<i)
is highly complex as it has to capture how node π(vi) links
to previous nodes based on how previous nodes are intercon-
nected among each other. Here we propose to parameterize
p(Sπ
i |Sπ
<i) using expressive neural networks to model the
complex distribution. To achieve scalable modeling, we let
the neural networks share weights across all time steps i. In
particular, we use an RNN that consists of a state-transition
function and an output function:
hi = ftrans(hi−1, Sπ
i−1),
(5)
θi = fout(hi),
(6)
where hi ∈Rd is a vector that encodes the state of the
graph generated so far, Sπ
i−1 is the adjacency vector for
the most recently generated node i −1, and θi speciﬁes the
distribution of next node’s adjacency vector (i.e., Sπ
i ∼Pθi).
In general, ftrans and fout can be arbitrary neural networks,
and Pθi can be an arbitrary distribution over binary vectors.
This general framework is summarized in Algorithm 1.
Note that the proposed problem formulation is fully gen-
eral; we discuss and present some speciﬁc variants with
implementation details in the next section. Note also that
RNNs require ﬁxed-size input vectors, while we previously
deﬁned Sπ
i as having varying dimensions depending on i;
we describe an efﬁcient and ﬂexible scheme to address this
issue in Section 2.3.4.
2.3.3. GRAPHRNN VARIANTS
Different variants of the GraphRNN model correspond to
different assumptions about p(Sπ
i |Sπ
<i). Recall that each
dimension of Sπ
i is a binary value that models existence
of an edge between the new node π(vi) and a previous
node π(vj), j ∈{1, ..., i −1}. We propose two variants of
GraphRNN, both of which implement the transition func-
tion ftrans (i.e., the graph-level RNN) as a Gated Recurrent
Unit (GRU) (Chung et al., 2014) but differ in the implemen-
tation of fout (i.e., the edge-level model). Both variants are
trained using stochastic gradient descent with a maximum
likelihood loss over Sπ — i.e., we optimize the parameters
of the neural networks to optimize Q pmodel(Sπ) over all
observed graph sequences.
Multivariate Bernoulli. First we present a simple base-
line variant of our GraphRNN approach, which we term
GraphRNN-S (“S” for “simpliﬁed”). In this variant, we
model p(Sπ
i |Sπ
<i) as a multivariate Bernoulli distribution,
parameterized by the θi ∈Ri−1 vector that is output by
fout. In particular, we implement fout as single layer multi-
layer perceptron (MLP) with sigmoid activation function,
that shares weights across all time steps. The output of fout
is a vector θi, whose element θi[j] can be interpreted as a
probability of edge (i, j). We then sample edges in Sπ
i inde-
pendently according to a multivariate Bernoulli distribution
parametrized by θi.
Dependent Bernoulli sequence. To fully capture complex
edge dependencies, in the full GraphRNN model we further
decompose p(Sπ
i |Sπ
<i) into a product of conditionals,
p(Sπ
i |Sπ
<i) =
i−1
Y
j=1
p(Sπ
i,j|Sπ
i,<j, Sπ
<i),
(7)
where Sπ
i,j denotes a binary scalar that is 1 if node π(vi+1)
is connected to node π(vj) (under ordering π). In this
variant, each distribution in the product is approximated by
an another RNN. Conceptually, we have a hierarchical RNN,
where the ﬁrst (i.e., the graph-level) RNN generates the
nodes and maintains the state of the graph, while the second
(i.e., the edge-level) RNN generates the edges of a given
node (as illustrated in Figure 1). In our implementation, the
edge-level RNN is a GRU model, where the hidden state
is initialized via the graph-level hidden state hi and where
the output at each step is mapped by a MLP to a scalar
indicating the probability of having an edge. Sπ
i,j is sampled
from this distribution speciﬁed by the jth output of the ith
edge-level RNN, and is fed into the j+1th input of the same
RNN. All edge-level RNNs share the same parameters.

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
2.3.4. TRACTABILITY VIA BREADTH-FIRST SEARCH
A crucial insight in our approach is that rather than learning
to generate graphs under any possible node permutation, we
learn to generate graphs using breadth-ﬁrst-search (BFS)
node orderings, without a loss of generality. Formally, we
modify Equation (1) to
Sπ = fS(G, BFS(G, π)),
(8)
where BFS(·) denotes the deterministic BFS function. In
particular, this BFS function takes a random permutation
π as input, picks π(v1) as the starting node and appends
the neighbors of a node into the BFS queue in the order
deﬁned by π. Note that the BFS function is many-to-one,
i.e., multiple permutations can map to the same ordering
after applying the BFS function.
Using BFS to specify the node ordering during generation
has two essential beneﬁts. The ﬁrst is that we only need to
train on all possible BFS orderings, rather than all possible
node permutations, i.e., multiple node permutations map to
the same BFS ordering, providing a reduction in the overall
number of sequences we need to consider.4 The second is
that the BFS ordering makes learning easier by reducing the
number of edge predictions we need to make in the edge-
level RNN; in particular, when we are adding a new node
under a BFS ordering, the only possible edges for this new
node are those connecting to nodes that are in the “frontier”
of the BFS (i.e., nodes that are still in the BFS queue)—a
notion formalized by Proposition 1 (proof in the Appendix):
Proposition 1. Suppose v1, . . . , vn is a BFS ordering of n
nodes in graph G, and (vi, vj−1) ∈E but (vi, vj) ̸∈E for
some i < j ≤n, then (vi′, vj′) ̸∈E, ∀1 ≤i′ ≤i and
j ≤j′ < n.
Importantly, this insight allows us to redeﬁne the variable
size Sπ
i vector as a ﬁxed M-dimensional vector, represent-
ing the connectivity between node π(vi) and nodes in the
current BFS queue with maximum size M:
Sπ
i = (Aπ
max(1,i−M),i, ..., Aπ
i−1,i)T , i ∈{2, ..., n}.
(9)
As a consequence of Proposition 1, we can bound M as
follows:
Corollary 1. With a BFS ordering the maximum number of
entries that GraphRNN model needs to predict for Sπ
i , ∀1 ≤
i ≤n is O

maxdiam(G)
d=1
|{vi|dist(vi, v1) = d}|

, where
dist denotes the shortest-path-distance between vertices.
The overall time complexity of GraphRNN is thus O(Mn).
In practice, we estimate an empirical upper bound for M
(see the Appendix for details).
4In the worst case (e.g., star graphs), the number of BFS or-
derings is n!, but we observe substantial reductions on many real-
world graphs.
3. GraphRNN Model Capacity
In this section we analyze the representational capacity of
GraphRNN, illustrating how it is able to capture complex
edge dependencies. In particular, we discuss two very differ-
ent cases on how GraphRNN can learn to generate graphs
with a global community structure as well as graphs with a
very regular geometric structure. For simplicity, we assume
that hi (the hidden state of the graph-level RNN) can exactly
encode Sπ
<i, and that the edge-level RNN can encode Sπ
i,<j.
That is, we assume that our RNNs can maintain memory
of the decisions they make and elucidate the models capac-
ity in this ideal case. We similarly rely on the universal
approximation theorem of neural networks (Hornik, 1991).
Graphs with community structure.
GraphRNN can
model structures that are speciﬁed by a given probabilis-
tic model. This is because the posterior of a new edge
probability can be expressed as a function of the outcomes
of previous nodes. For instance, suppose that the training set
contains graphs generated from the following distribution
pcom(G): half of the nodes are in community A, and half of
the nodes are in community B (in expectation), and nodes
are connected with probability ps within each community
and probability pd between communities. Given such a
model, we have the following key (inductive) observation:
Observation 1. Assume there exists a parameter setting for
GraphRNN such that it can generate Sπ
<i and Sπ
i,<j accord-
ing to the distribution over Sπ implied by pcom(G), then
there also exists a parameter setting for GraphRNN such
that it can output p(Sπ
i,j|Sπ
i,<j, Sπ
<i) according to pcom(G).
This observation follows from three facts: First, we know
that p(Sπ
i,j|Sπ
i,<j, Sπ
<i) can be expressed as a function of
ps, pd, and p(π(vj) ∈A), p(π(vj) ∈B) ∀1 ≤j ≤i
(which holds by pcom’s deﬁnition). Second, by our earlier
assumptions on the RNN memory, Sπ
<i can be encoded into
the initial state of the edge-level RNN, and the edge-level
RNN can also encode the outcomes of Sπ
i,<j. Third, we
know that p(π(vi) ∈A) is computable from Sπ
<i and Sπ
i,1
(by Bayes’ rule and pcom’s deﬁnition, with an analogous
result for p(π(vi) ∈B)). Finally, GraphRNN can handle
the base case of the induction in Observation 1, i.e., Si,1,
simply by sampling according to 0.5ps + 0.5pd at the ﬁrst
step of the edge-level RNN (i.e., 0.5 probability i is in same
community as node π(v1)).
Graphs with regular structure. GraphRNN can also natu-
rally learn to generate regular structures, due to its ability
to learn functions that only activate for Sπ
i,j where vj has
speciﬁc degree. For example, suppose that the training set
consists of ladder graphs (Noy & Rib´o, 2004). To generate
a ladder graph, the edge-level RNN must handle three key
cases: if Pj
k=1 Sπ
i,j = 0, then the new node should only
connect to the degree 1 node or else any degree 2 node; if

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
Pj
k=1 Sπ
i,j = 1, then the new node should only connect to
the degree 2 node that is exactly two hops away; and ﬁnally,
if Pj
k=1 Sπ
i,j = 2 then the new node should make no further
connections. And note that all of the statistics needed above
are computable from Sπ
<i and Sπ
i,<j. The appendix contains
visual illustrations and further discussions on this example.
4. Experiments
We compare GraphRNN to state-of-the-art baselines,
demonstrating its robustness and ability to generate high-
quality graphs in diverse settings.
4.1. Datasets
We perform experiments on both synthetic and real datasets,
with drastically varying sizes and characteristics. The sizes
of graphs vary from |V | = 10 to |V | = 2025.
Community. 500 two-community graphs with 60 ≤|V | ≤
160. Each community is generated by the Erd˝os-R´enyi
model (E-R) (Erd˝os & R´enyi, 1959) with n = |V |/2 nodes
and p = 0.3. We then add 0.05|V | inter-community edges
with uniform probability.
Grid. 100 standard 2D grid graphs with 100 ≤|V | ≤400.
We also run our models on 100 standard 2D grid graphs
with 1296 ≤|V | ≤2025, and achieve comparable results.
B-A. 500 graphs with 100 ≤|V | ≤200 that are generated
using the Barab´asi-Albert model. During generation, each
node is connected to 4 existing nodes.
Protein. 918 protein graphs (Dobson & Doig, 2003) with
100 ≤|V | ≤500. Each protein is represented by a graph,
where nodes are amino acids and two nodes are connected
if they are less than 6 Angstroms apart.
Ego. 757 3-hop ego networks extracted from the Citeseer
network (Sen et al., 2008) with 50 ≤|V | ≤399. Nodes rep-
resent documents and edges represent citation relationships.
4.2. Experimental Setup
We compare the performance of our model against various
traditional generative models for graphs, as well as some
recent deep graph generative models.
Traditional baselines. Following Li et al. 2018 we compare
against the Erd˝os-R´enyi model (E-R) (Erd˝os & R´enyi, 1959)
and the Barab´asi-Albert (B-A) model (Albert & Barab´asi,
2002). In addition, we compare against popular generative
models that include learnable parameters: Kronecker graph
models (Leskovec et al., 2010) and mixed-membership
stochastic block models (MMSB) (Airoldi et al., 2008).
Deep learning baselines. We compare against the recent
methods of Simonovsky & Komodakis 2018 (GraphVAE)
and Li et al. 2018 (DeepGMG). We provide reference im-
plementations for these methods (which do not currently
have associated public code), and we adapt GraphVAE to
our problem setting by using one-hot indicator vectors as
node features for the graph convolutional network encoder.5
Experiment settings. We use 80% of the graphs in each
dataset for training and test on the rest. We set the hyperpa-
rameters for baseline methods based on recommendations
made in their respective papers. The hyperparameter set-
tings for GraphRNN were ﬁxed after development tests on
data that was not used in follow-up evaluations (further de-
tails in the Appendix). Note that all the traditional methods
are only designed to learn from a single graph, therefore we
train a separate model for each training graph in order to
compare with these methods. In addition, both deep learning
baselines suffer from aforementioned scalability issues, so
we only compare to these baselines on a small version of the
community dataset with 12 ≤|V | ≤20 (Community-small)
and 200 ego graphs with 4 ≤|V | ≤18 (Ego-small).
4.3. Evaluating the Generated Graphs
Evaluating the sample quality of generative models is a chal-
lenging task in general (Theis et al., 2016), and in our case,
this evaluation requires a comparison between two sets of
graphs (the generated graphs and the test sets). Whereas
previous works relied on qualitative visual inspection (Si-
monovsky & Komodakis, 2018) or simple comparisons of
average statistics between the two sets (Leskovec et al.,
2010), we propose novel evaluation metrics that compare
all moments of their empirical distributions.
Our proposed metrics are based on Maximum Mean Dis-
crepancy (MMD) measures. Suppose that a unit ball in a
reproducing kernel Hilbert space (RKHS) H is used as its
function class F, and k is the associated kernel, the squared
MMD between two sets of samples from distributions p and
q can be derived as (Gretton et al., 2012)
MMD2(p||q) = Ex,y∼p[k(x, y)] + Ex,y∼q[k(x, y)]
−2Ex∼p,y∼q[k(x, y)].
(10)
Proper distance metrics over graphs are in general computa-
tionally intractable (Lin, 1994). Thus, we compute MMD
using a set of graph statistics M = {M1, ..., Mk}, where
each Mi(G) is a univariate distribution over R, such as
the degree distribution or clustering coefﬁcient distribution.
We then use the ﬁrst Wasserstein distance as an efﬁcient
distance metric between two distributions p and q:
W(p, q) =
inf
γ∈Π(p,q) E(x,y)∼γ[||x −y||],
(11)
where Π(p, q) is the set of all distributions whose marginals
5We also attempted using degree and clustering coefﬁcients as
features for nodes, but did not achieve better performance.

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
Figure 2. Visualization of graphs from grid dataset (Left group), community dataset (Middle group) and Ego dataset (Right group). Within
each group, graphs from training set (First row), graphs generated by GraphRNN(Second row) and graphs generated by Kronecker,
MMSB and B-A baselines respectively (Third row) are shown. Different visualization layouts are used for different datasets.
Table 1. Comparison of GraphRNN to traditional graph generative models using MMD. (max(|V |), max(|E|)) of each dataset is shown.
Community (160,1945)
Ego (399,1071)
Grid (361,684)
Protein (500,1575)
Deg.
Clus.
Orbit
Deg.
Clus.
Orbit
Deg.
Clus.
Orbit
Deg.
Clus.
Orbit
E-R
0.021
1.243
0.049
0.508
1.288
0.232
1.011
0.018
0.900
0.145
1.779
1.135
B-A
0.268
0.322
0.047
0.275
0.973
0.095
1.860
0
0.720
1.401
1.706
0.920
Kronecker
0.259
1.685
0.069
0.108
0.975
0.052
1.074
0.008
0.080
0.084
0.441
0.288
MMSB
0.166
1.59
0.054
0.304
0.245
0.048
1.881
0.131
1.239
0.236
0.495
0.775
GraphRNN-S
0.055
0.016
0.041
0.090
0.006
0.043
0.029
10−5
0.011
0.057
0.102
0.037
GraphRNN
0.014
0.002
0.039
0.077
0.316
0.030
10−5
0
10−4
0.034
0.935
0.217
are p and q respectively, and γ is a valid transport plan.
To capture high-order moments, we use the following ker-
nel, whose Taylor expansion is a linear combination of all
moments (proof in the Appendix):
Proposition 2. The kernel function deﬁned by kW (p, q) =
exp W (p,q)
2σ2
induces a unique RKHS.
In experiments, we show this derived MMD score for degree
and clustering coefﬁcient distributions, as well as average
orbit counts statistics, i.e., the number of occurrences of all
orbits with 4 nodes (to capture higher-level motifs) (Hoˇcevar
& Demˇsar, 2014). We use the RBF kernel to compute
distances between count vectors.
4.4. Generating High Quality Graphs
Our experiments demonstrate that GraphRNN can generate
graphs that match the characteristics of the ground truth
graphs in a variety of metrics.
Graph visualization. Figure 2 visualizes the graphs gen-
erated by GraphRNN and various baselines, showing that
GraphRNN can capture the structure of datasets with vastly
differing characteristics—being able to effectively learn reg-
ular structures like grids as well as more natural structures
like ego networks. Speciﬁcally, we found that grids gener-
ated by GraphRNN do not appear in the training set, i.e., it
learns to generalize to unseen grid widths/heights.
Evaluation with graph statistics.
We use three graph
statistics—based on degrees, clustering coefﬁcients and or-
bit counts—to further quantitatively evaluate the generated
graphs. Figure 3 shows the average graph statistics in the
test vs. generated graphs, which demonstrates that even from
hundreds of graphs with diverse sizes, GraphRNN can still
learn to capture the underlying graph statistics very well,
with the generated average statistics closely matching the
overall test set distribution.
Tables 1 and 2 summarize MMD evaluations on the full
datasets and small versions, respectively. Note that we train
all the models with a ﬁxed number of steps, and report the
test set performance at the step with the lowest training er-
ror.6 GraphRNN variants achieve the best performance on
all datasets, with 80% decrease of MMD on average com-
pared with traditional baselines, and 90% decrease of MMD
compared with deep learning baselines. Interestingly, on the
protein dataset, our simpler GraphRNN-S model performs
very well, which is likely due to the fact that the protein
dataset is a nearest neighbor graph over Euclidean space and
thus does not involve highly complex edge dependencies.
Note that even though some baseline models perform well
on speciﬁc datasets (e.g., MMSB on the community dataset),
they fail to generalize across other types of input graphs.
6Using the training set or a validation set to evaluate MMD
gave analogous results, so we used the train set for early stopping.

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
Table 2. GraphRNN compared to state-of-the-art deep graph generative models on small graph datasets using MMD and negative
log-likelihood (NLL). (max(|V |), max(|E|)) of each dataset is shown. (DeepVAE and GraphVAE cannot scale to the graphs in Table 1.)
Community-small (20,83)
Ego-small (18,69)
Degree
Clustering
Orbit
Train NLL
Test NLL
Degree
Clustering
Orbit
Train NLL
Test NLL
GraphVAE
0.35
0.98
0.54
13.55
25.48
0.13
0.17
0.05
12.45
14.28
DeepGMG
0.22
0.95
0.40
106.09
112.19
0.04
0.10
0.02
21.17
22.40
GraphRNN-S
0.02
0.15
0.01
31.24
35.94
0.002
0.05
0.0009
8.51
9.88
GraphRNN
0.03
0.03
0.01
28.95
35.10
0.0003
0.05
0.0009
9.05
10.61
Figure 3. Average degree (Left) and clustering coefﬁcient (Right)
distributions of graphs from test set and graphs generated by
GraphRNN and baseline models.
Generalization ability. Table 2 also shows negative log-
likelihoods (NLLs) on the training and test sets. We report
the average p(Sπ) in our model, and report the likelihood
in baseline methods as deﬁned in their papers. A model
with good generalization ability should have small NLL gap
between training and test graphs. We found that our model
can generalize well, with 22% smaller average NLL gap.7
4.5. Robustness
Finally,
we also investigate the robustness of our
model by interpolating between Barab´asi-Albert (B-A)
and Erd˝os-R´enyi (E-R) graphs.
We randomly perturb
[0%, 20%, ..., 100%] edges of B-A graphs with 100 nodes.
With 0% edges perturbed, the graphs are E-R graphs; with
100% edges perturbed, the graphs are B-A graphs. Figure
4 shows the MMD scores for degree and clustering coefﬁ-
cient distributions for the 6 sets of graphs. Both B-A and
E-R perform well when graphs are generated from their
respective distributions, but their performance degrades sig-
niﬁcantly once noise is introduced. In contrast, GraphRNN
maintains strong performance as we interpolate between
these structures, indicating high robustness and versatility.
5. Further Related Work
In addition to the deep graph generative approaches and
traditional graph generation approaches surveyed previously,
our framework also builds off a variety of other methods.
Molecule and parse-tree generation.
There has been
related domain-speciﬁc work on generating candidate
7The average likelihood is ill-deﬁned for the traditional models.
Figure 4. MMD performance of different approaches on degree
(Left) and clustering coefﬁcient (Right) under different noise level.
molecules and parse trees in natural language processing.
Most previous work on discovering molecule structures
make use of a expert-crafted sequence representations of
molecular graph structures (SMILES) (Olivecrona et al.,
2017; Segler et al., 2017; G´omez-Bombarelli et al., 2016).
Most recently, SD-VAE (Dai et al., 2018) introduced a
grammar-based approach to generate structured data, includ-
ing molecules and parse trees. In contrast to these works, we
consider the fully general graph generation setting without
assuming features or special structures of graphs.
Deep autoregressive models. Deep autoregressive models
decompose joint probability distributions as a product of
conditionals, a general idea that has achieved striking suc-
cesses in the image (Oord et al., 2016b) and audio (Oord
et al., 2016a) domains. Our approach extends these suc-
cesses to the domain of generating graphs. Note that the
DeepGMG algorithm (Li et al., 2018) and the related prior
work of Johnson 2017 can also be viewed as deep autore-
gressive models of graphs. However, unlike these methods,
we focus on providing a scalable (i.e., O(n2)) algorithm
that can generate general graphs.
6. Conclusion and Future Work
We proposed GraphRNN, an autoregressive generative
model for graph-structured data, along with a comprehen-
sive evaluation suite for the graph generation problem,
which we used to show that GraphRNN achieves signiﬁ-
cantly better performance compared to previous state-of-
the-art models, while being scalable and robust to noise.
However, signiﬁcant challenges remain in this space, such
as scaling to even larger graphs and developing models that
are capable of doing efﬁcient conditional graph generation.

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
Acknowledgements
The authors thank Ethan Steinberg, Bowen Liu, Marinka
Zitnik and Srijan Kumar for their helpful discussions and
comments on the paper. This research has been supported in
part by DARPA SIMPLEX, ARO MURI, Stanford Data Sci-
ence Initiative, Huawei, JD, and Chan Zuckerberg Biohub.
W.L.H. was also supported by the SAP Stanford Graduate
Fellowship and an NSERC PGS-D grant.
References
Airoldi, E., Blei, D., Fienberg, S., and Xing, E. Mixed
membership stochastic blockmodels. JMLR, 2008.
Albert, R. and Barab´asi, L. Statistical mechanics of complex
networks. Reviews of Modern Physics, 74(1):47, 2002.
Cho, M., Sun, J., Duchenne, O., and Ponce, J. Finding
matches in a haystack: A max-pooling strategy for graph
matching in the presence of outliers. In CVPR, 2014.
Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical
evaluation of gated recurrent neural networks on sequence
modeling. NIPS Workshop on Deep Learning, 2014.
Dai, H., Tian, Y., Dai, B., Skiena, S., and Song, L. Syntax-
directed variational autoencoder for structured data. ICLR,
2018.
Dobson, P. and Doig, A. Distinguishing enzyme structures
from non-enzymes without alignments. Journal of Molec-
ular Biology, 330(4):771–783, 2003.
Erd˝os, P. and R´enyi, A. On random graphs I. Publicationes
Mathematicae (Debrecen), 6:290–297, 1959.
G´omez-Bombarelli, R., Wei, J., Duvenaud, D., Hern´andez-
Lobato, J. M., S´anchez-Lengeling, B., Sheberla, D.,
Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P.,
and Aspuru-Guzik, A. Automatic chemical design us-
ing a data-driven continuous representation of molecules.
ACS Central Science, 2016.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.
Generative adversarial nets. In NIPS, 2014.
Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch¨olkopf, B.,
and Smola, A. A kernel two-sample test. JMLR, 2012.
Grover, A., Zweig, A., and Ermon, S. Graphite: Iterative
generative modeling of graphs. In NIPS Bayesian Deep
Learning Workshop, 2017.
Hamilton, W. L., Ying, R., and Leskovec, J. Representation
learning on graphs: Methods and applications. IEEE
Data Engineering Bulletin, 2017.
Hoˇcevar, T. and Demˇsar, J. A combinatorial approach to
graphlet counting. Bioinformatics, 30(4):559–565, 2014.
Hornik, K. Approximation capabilities of multilayer feed-
forward networks. Neural networks, 4(2):251–257, 1991.
Johnson, D. D. Learning graphical state transitions. In
ICLR, 2017.
Kingma, D. P. and Welling, M. Auto-encoding variational
bayes. In ICLR, 2014.
Kipf, T. N. and Welling, M. Variational graph auto-encoders.
In NIPS Bayesian Deep Learning Workshop, 2016.
Leskovec, J., Kleinberg, J., and Faloutsos, C. Graph evolu-
tion: Densiﬁcation and shrinking diameters. TKDD, 1(1):
2, 2007.
Leskovec, J., Chakrabarti, D., Kleinberg, J., Faloutsos, C.,
and Ghahramani, Z. Kronecker graphs: An approach to
modeling networks. JMRL, 2010.
Li, Y., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia,
P.
Learning deep generative models of graphs,
2018. URL https://openreview.net/forum?
id=Hy1d-ebAb.
Lin, C. Hardness of approximating graph transformation
problem. In International Symposium on Algorithms and
Computation, 1994.
Newman, M. Networks: an introduction. Oxford university
press, 2010.
Noy, M. and Rib´o, A. Recursively constructible families
of graphs. Advances in Applied Mathematics, 32(1-2):
350–363, 2004.
Olivecrona, M., Blaschke, T., Engkvist, O., and Chen, H.
Molecular de novo design through deep reinforcement
learning. Journal of Cheminformatics, 9(1):48, 2017.
Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals,
O., Graves, A., Kalchbrenner, N., Senior, A., and
Kavukcuoglu, K. Wavenet: A generative model for raw
audio. arXiv preprint arXiv:1609.03499, 2016a.
Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. Pixel
recurrent neural networks. In ICML, 2016b.
Robins, G., Pattison, P., Kalish, Y., and Lusher, D. An
introduction to exponential random graph (p*) models for
social networks. Social Networks, 29(2):173–191, 2007.
Segler, M., Kogej, T., Tyrchan, C., and Waller, M. Generat-
ing focussed molecule libraries for drug discovery with
recurrent neural networks. ACS Central Science, 2017.

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B.,
and Eliassi-Rad, T. Collective classiﬁcation in network
data. AI Magazine, 29(3):93, 2008.
Simonovsky, M. and Komodakis, N. GraphVAE: Towards
generation of small graphs using variational autoencoders,
2018. URL https://openreview.net/forum?
id=SJlhPMWAW.
Theis, L., van den Oord, A., and Bethge, M. A note on the
evaluation of generative models. In ICLR, 2016.
A. Appendix
A.1. Implementation Details of GraphRNN
In this section we detail parameter setting, data preparation
and training strategies for GraphRNN.
We use two sets of model parameters for GraphRNN. One
larger model is used to train and test on the larger datasets
that are used to compare with traditional methods. One
smaller model is used to train and test on datasets with nodes
up to 20. This model is only used to compare with the two
most recent preliminary deep generative models for graphs
proposed in (Li et al., 2018; Simonovsky & Komodakis,
2018).
For GraphRNN, the graph-level RNN uses 4 layers of GRU
cells, with 128 dimensional hidden state for the larger model,
and 64 dimensional hidden state for the smaller model in
each layer. The edge-level RNN uses 4 layers of GRU cells,
with 16 dimensional hidden state for both the larger model
and the smaller model. To output the adjacency vector
prediction, the edge-level RNN ﬁrst maps the highest layer
of the 16 dimensional hidden state to a 8 dimensional vector
through a MLP with ReLU activation, then another MLP
maps the vector to a scalar with sigmoid activation. The
edge-level RNN is initialized by the output of the graph-
level RNN at the start of generating Sπ
i , ∀1 ≤i ≤n.
Speciﬁcally, the highest layer hidden state of the graph-level
RNN is used to initialize the lowest layer of edge-level
RNN, with a liner layer to match the dimensionality. During
training time, teacher forcing is used for both graph-level
and edge-level RNNs, i.e., we use the groud truth rather than
the model’s own prediction during training. At inference
time, the model uses its own preditions at each time steps to
generate a graph.
For the simple version GraphRNN-S, a two-layer MLP with
ReLU and sigmoid activations respectively is used to gen-
erate Sπ
i , with 64 dimensional hidden state for the larger
model, and 32 dimensional hidden state for the smaller
model. In practice, we ﬁnd that the performance of the
model is relatively stable with respect to these hyperparam-
eters.
We generate the graph sequences used for training the model
following the procedure in Section 2.3.4. Speciﬁcally, we
ﬁrst randomly sample a graph from the training set, then
randomly permute the node ordering of the graph. We then
do the deterministic BFS discussed in Section 2.3.4 over the
graph with random node ordering, resulting a graph with
BFS node ordering. An exception is in the robustness sec-
tion, where we use the node ordering that generates B-A
graphs to get graph sequences, in order to see if GraphRNN
can capture the underlying preferential attachment proper-
ties of B-A graphs.
With the proposed BFS node ordering, we can reduce the
maximum dimension M of Sπ
i , illustrated in Figure 5. To
set the maximum dimension M of Sπ
i , we use the following
empirical procedure. We randomly ran 100000 times the
above data pre-processing procedure to get graph with BFS
node orderings. We remove the all consecutive zeros in
all resulting Sπ
i , to ﬁnd the empirical distribution of the
dimensionality of Sπ
i . We set M to be roughly the 99.9
percentile, to account for the majority dimensionality of
Sπ
i . In principle, we ﬁnd that graphs with regular structures
tend to have smaller M, while random graphs or community
graphs tend to have larger M. Speciﬁcally, for community
dataset, we set M = 100; for grid dataset, we set M = 40;
for B-A dataset, we set M = 130; for protein dataset, we
set M = 230; for ego dataset, we set M = 250; for all
small graph datasets, we set M = 20.
The Adam Optimizer is used for minibatch training. Each
minibatch contains 32 graph sequences. We train the model
for 96000 batchs in all experiments. We set the learning
rate to be 0.001, which is decayed by 0.3 at step 12800 and
32000 in all experiments.
A.2. Running Time of GraphRNN
Training is performed on only 1 Titan X GPU. For the
protein dataset that consists of about 1000 graphs, each
containing about 500 nodes, training converges at around
64000 iterations. The runtime is around 12 to 24 hours. This
also includes pre-processing, batching and BFS, which are
currently implemented using CPU without multi-threading.
The less expressive GraphRNN-S variant is about twice
faster. At inference time, for the above dataset, generating a
graph using the trained model only takes about 1 second.
A.3. More Details on GraphRNN’s Expressiveness
We illustrate the intuition underlying the good performance
of GraphRNNon graphs with regular structures, such as grid
and ladder networks. Figure 6 (a) shows the generation
process of a ladder graph at an intermediate step. At this
time step, the ground truth data (under BFS node ordering)
speciﬁes that the new node added to the graph should make
an edge to the node with degree 1. Note that node degree is

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
Figure 5. Illustrative example of reducing the maximum dimension M of Sπ
i through the BFS node ordering. Here we show the adjacency
matrix of a graph with N = 10 nodes. Without the BFS node ordering (Left), we have to set M = N −1 to encode all the necessary
connection information (shown in dark square). With the BFS node ordering, we could set M to be a constant smaller than N (we show
M = 3 in the ﬁgure).
a function of Sπ
<i, thus could be approximated by a neural
network.
Once the ﬁrst edge has been generated, the new node should
make an edge with another node of degree 2. However, there
are multiple ways to do so, but only one of them gives a valid
grid structure, i.e. one that forms a 4-cycle with the new
edge. GraphRNN crucially relies on the edge-level RNN
and the knowledge of the previously added edge, in order to
distinguish between the correct and incorrect connections
in Figure 6 (c) and (d).
Figure 6. Illustration that generation of ladder networks relies on
dependencies modeled by GraphRNN.
A.4. Code Overview
In the code repository, main.py consists of the main train-
ing pipeline, which loads datasets and performs training and
inference. It also consists of the Args class, which stores
the hyper-parameter settings of the model. model.py con-
sists of the RNN, MLP and loss function modules that are
use to build GraphRNN. data.py contains the minibatch
sampler, which samples a random BFS ordering of a batch
of randomly selected graphs. evaluate.py contains the
code for evaluating the generated graphs using the MMD
metric introduced in Sec. 4.3.
Baselines including the Erd˝os-R´enyi model, Barab´asi-
Albert model, MMSB, and rge very recent deep generative
models (GraphVAE, DeepGMG) are also implemented in
the baselines folders. We adopt the C++ Kronecker
graph model implementation in the SNAP package 8.
A.5. Proofs
A.5.1. PROOF OF PROPOSITION 1
We use the following observation:
Observation 2. By deﬁnition of BFS, if i < k, then the
children of vi in the BFS ordering come before the children
of vk that do not connect to vi′, ∀1 ≤i′ ≤i.
By deﬁnition of BFS, all neighbors of a node vi include
the parent of vi in the BFS tree, all children of vi which
have consecutive indices, and some children of vi′ which
connect to both vi′ and vi, for some 1 ≤i′ ≤i. Hence if
(vi, vj−1) ∈E but (vi, vj) ̸∈E, vj−1 is the last children of
vi in the BFS ordering. Hence (vi, vj′) ̸∈E, ∀j ≤j′ ≤n.
For all i′ ∈[i], supposed that (vi′, vj′−1) ∈E but
(vi′, vj′) ̸∈E. By Observation 2, j′ < j. By conclusion
in the previous paragraph, (vi′, vj′′) ̸∈E, ∀j′ ≤j′′ ≤n.
Speciﬁcally, (vi′, vj′′) ̸∈E, ∀j ≤j′′ ≤n. This is true for
all i′ ∈[i]. Hence we prove that (vi′, vj′) ̸∈E, ∀1 ≤i′ ≤i
and j ≤j′ < n.
A.5.2. PROOF OF PROPOSITION 2
As proven in ?, this Wasserstein distance based kernel is
a positive deﬁnite (p.d.) kernel. By properties that linear
combinations, product and limit (if exists) of p.d. kernels
8The
SNAP
package
is
available
at
http://snap.
stanford.edu/snap/index.html.

GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models
are p.d. kernels, kW (p, q) is also a p.d. kernel.9 By the
Moore-Aronszajn theorem, a symmetric p.d. kernel induces
a unique RKHS. Therefore Equation (9) holds if we set k to
be kW .
A.6. Extension to Graphs with Node and Edge Features
Our GraphRNN model can also be applied to graphs where
nodes and edges have feature vectors associated with them.
In this extended setting, under node ordering π, a graph
G is associated with its node feature matrix Xπ ∈Rn×m
and edge feature matrix F π ∈Rn×k, where m and k are
the feature dimensions for node and edge respectively. In
this case, we can extend the deﬁnition of Sπ to include
feature vectors of corresponding nodes as well as edges
Sπ
i = (Xπ
i , F π
i ). We can adapt the fout module, by using a
MLP to generate Xπ
i and an edge-level RNN to genearte F π
i
respectively. Note also that directed graphs can be viewed
as an undirected graphs with two edge types, which is a
special case under the above extension.
A.7. Extension to Graphs with Four Communities
To further show the ability of GraphRNN to learn from
community graphs, we further conduct experiments on a
four-community synthetic graph dataset. Speciﬁcally, the
data set consists of 500 four community graphs with 48 ≤
|V | ≤68. Each community is generated by the Erd˝os-R´enyi
model (E-R) (Erd˝os & R´enyi, 1959) with n ∈[|V |/4 −
2, |V |/4+2] nodes and p = 0.7. We then add 0.01|V |2 inter-
community edges with uniform probability. FIgure 7 shows
the comparison of visualization of generated graph using
GraphRNN and other baselines. We observe that in contrast
to baselines, GraphRNN consistently generate 4-community
graphs and each community has similar structure to that in
the training set.
9This can be seen by expressing the kernel function using
Taylor expansion.
Figure 7. Visualization of graph dataset with four communi-
ties. Graphs from training set (First row), graphs generated by
GraphRNN(Second row) and graphs generated by Kronecker,
MMSB and B-A baselines respectively (Third row) are shown.

