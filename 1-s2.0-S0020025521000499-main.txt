Blessing of dimensionality at the edge and geometry of few-shot
learning
Ivan Y. Tyukin a,c,f,⇑, Alexander N. Gorban a,b, Alistair A. McEwan a,g, Sepehr Meshkinfamfard d,
Lixin Tang e
a University of Leicester, UK
b Lobachevsky University, Russia
c St Petersburg State Electrotechnical University, Russia
d University College London, UK
e Key Laboratory of Data Analytics and Optimization for Smart Industry (Northeastern University), Ministry of Education, People’s Republic of China
f Norwegian University of Science and Technology, Norway
g University of Derby, UK
a r t i c l e
i n f o
Article history:
Received 3 October 2019
Received in revised form 17 December 2020
Accepted 8 January 2021
Available online 3 February 2021
Keywords:
Stochastic separation theorems
Artiﬁcial intelligence
Machine learning
Computer vision
a b s t r a c t
In this paper we present theory and algorithms enabling classes of Artiﬁcial Intelligence
(AI) systems to continuously and incrementally improve with a priori quantiﬁable guaran-
tees – or more speciﬁcally remove classiﬁcation errors – over time. This is distinct from
state-of-the-art machine learning, AI, and software approaches. The theory enables build-
ing few-shot AI correction algorithms and provides conditions justifying their successful
application. Another feature of this approach is that, in the supervised setting, the compu-
tational complexity of training is linear in the number of training samples. At the time of
classiﬁcation, the computational complexity is bounded by few inner product calculations.
Moreover, the implementation is shown to be very scalable. This makes it viable for
deployment in applications where computational power and memory are limited, such
as embedded environments. It enables the possibility for fast on-line optimisation using
improved training samples. The approach is based on the concentration of measure effects
and stochastic separation theorems and is illustrated with an example on the identiﬁcation
faulty processes in Computer Numerical Control (CNC) milling and with a case study on
adaptive removal of false positives in an industrial video surveillance and analytics system.
 2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY
license (http://creativecommons.org/licenses/by/4.0/).
1. Introduction
The past decade has seen extraordinary growth and advances in technologies for collecting and processing very large data
streams and data sets. Central to these advances has been the area of Artiﬁcial Intelligence (AI) built on Machine Learning
(ML) and Data Analytics theories. Exploitation of AI is becoming overwhelmingly ubiquitous. For instance, end users and
consumers use mobile phone apps with AI capabilities, security systems may employ AI to identify unwanted intrusions
and infringements, healthcare systems may use AI to assist clinical diagnosis or processes, and mechatronic systems may
use AI to implement control including autonomous and semi-autonomous functionality. Examples of these in literature
https://doi.org/10.1016/j.ins.2021.01.022
0020-0255/ 2021 The Authors. Published by Elsevier Inc.
This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
⇑Corresponding author at: University of Leicester, Department of Mathematics, University Road, LE1 7RH, UK.
E-mail addresses: I.Tyukin@le.ac.uk (I.Y. Tyukin), A.N.Gorban@le.ac.uk (A.N. Gorban), A.McEwan@derby.ac.uk (A.A. McEwan), s.meshkinfamfard@ucl.ac.
uk (S. Meshkinfamfard), lixintang@ise.neu.edu.cn (L. Tang).
Information Sciences 564 (2021) 124–143
Contents lists available at ScienceDirect
Information Sciences
journal homepage: www.elsevier.com/locate/ins

include those reported in [32,46]. Whilst this increase in application areas is due in part to advances in AI and ML, it is also
due to advances in hardware and supporting platforms. The emergence of devices such as Nvidia GPUs and Google TPUs have
meant that power of server- or super-computer platforms are no longer necessarily required for deployment of deep learning
systems.
Whilst state-of-the-art AI systems may be capable of outperforming both human and other data mining approaches in
identifying minute patterns in very large data sets, their conclusions are vulnerable to data inconsistencies, poor data quality,
and the uncertainty inherent to any data. This uncertainty, together with engineering constraints on implementation and
systems integration, leads to inevitable and unavoidable errors.
Consequences of errors resulting from AI may range from minor inconveniences to safety–critical risks: incorrect cancer
treatment options [44] and crashes of autonomous vehicles are a few examples of the latter (see [3] and references therein
for further detail and examples). However the solution to ameliorating or eliminating errors is non-trivial. The ﬁeld of Soft-
ware Engineering has provided numerous approaches for understanding the behaviours and misbehaviours of software
based systems ranging from efﬁcient scenario based testing techniques through to formal veriﬁcation—see for instance
[19]. However these software architectures typically do not contain the inherent uncertainties of data driven AI—although
this is rapidly changing with the push towards higher levels of driver assist and autonomy. Recent examples that consider
autonomous vehicle control systems incorporating AI include [31,34], although common to these approaches is to look at
systems level behaviours rather than the correctness of the AI component.
Whilst structuring data, improving the quality of data, and removing uncertainty is known to improve quality, it is too
resource intensive in the general case and thus unsustainable across sectors and industries. Moreover, whilst it may improve
the quality of high-assurance or safety–critical systems, it does not provide a measure of understanding of quality and con-
sistency of output. More fundamentally, constraints on implementations such as quantization errors and memory limits pre-
sent challenges to AI performance in resource constrained embedded settings—referred to as ‘‘at the edge” or ‘‘edge-based”.
1.1. Background and related work
Signiﬁcant efforts have been applied to address errors in AI systems. Using ensembles [20,23,24], augmenting training
data [29,36,38], enforcing continuity [56], and AI knowledge transfer [37,6,50] have been extensively discussed in the liter-
ature. These measures, however, do not warrant error-free behaviour as AIs based on empirical data are expected to make
mistakes.
Recently in [11,8,48,12,13,47] we have shown that spurious errors of AI systems operating in high-dimensional spaces
(convolutional and deep learning neural networks being the canonical examples) can be efﬁciently removed by Fisher dis-
criminants. The advantage of this approach over, for instance, Support Vector Machines (SVM) [49] is that the computational
complexity for constructing Fisher discriminants is at most linear in the number of points in the training set whereas the
worst-case complexity for SVM scales as a cubic function of the training set size [5].
This method is applicable to identiﬁed singular spurious errors as well as to moderate-sized clusters. An open question is
what happens when the volume of errors grows large and becomes comparable or even exceeds the volume of correct
responses? Is it possible for a deployed AI to keep improving its performance with limited resources available for supervision
and re-training by learning from new examples as they arrive? Both of these questions are fundamentally relevant across the
spectrum of AI applications. Notably, given the computational and data management costs needed for continuous complete
re-training, these questions are particularly acute for edge-based systems.
1.2. Contribution and structure of this paper
In this paper we show that stochastic separation theorems, or the blessing of dimensionality [9,26], stemming from the
concentration of measure effects [10,25,18], can be adapted and applied to address these questions. We present and justify
both mathematically and experimentally an algorithm capable of delivering the removal of errors at computational costs
compatible with deployment at the edge. The algorithm has both supervised and unsupervised components which enables
it to adapt to data without additional supervisory inputs. As compared to previously proposed stochastic separation-based
algorithms [48,47], current algorithm was shown to consistently learn from large volumes of errors making its deployment
in applications with uncertainty and bias in training data particularly attractive.
The paper is organized as follows. Section 2 sets out the notation we use in this paper. Section 3 contains necessary the-
oretical preliminaries and formal statement of the problem. In Section 4 we present a new algorithm for improving AIs ‘‘at
the edge”. Section 5 discusses and interprets our results in the context of existing work in the area of few-shot learning. Sec-
tion 6 illustrates an application of the proposed algorithms in two industrial applications: product quality prediction in
milling machines and automated edge-based object detection in large-scale survaliance systems; Section 7 concludes the
paper.
2. Notation
The following notational agreements are used throughout the text:
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
125

 Rn stands for the n-dimensional linear real vector space, and RP0 ¼ fx 2 Rj x P 0g;
 N denotes the set of natural numbers;
 symbols x ¼ ðx1; . . . ; xnÞ will denote elements of Rn;
 ðx; yÞ ¼ P
kxkyk is the inner product of x and y, and kxk ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx; xÞ
p
is the standard Euclidean norm in Rn;
 Bnðr; yÞ stands for the ball in Rn of radius r centered at y : Bnðr; yÞ ¼ fx 2 Rnj x  y; x  y
ð
Þ 6 r2g;
 Bn denotes for the unit ball in Rn centered at the origin: Bn ¼ fx 2 Rnj x; x
ð
Þ 6 1g;
 Vn is the n-dimensional Lebesgue measure, and VnðBnÞ is the volume of unit ball;
 if Y is a ﬁnite set then the number of elements in Y (cardinality of Y) is denoted by jYj.
 if y1; y2; . . . ; yk are elements of Rn then Convðy1; . . . ; ykÞ denotes the convex hull of y1; . . . ; yk:
Convðy1; . . . ; ykÞ ¼
y 2 Rnjy ¼
X
k
i¼1
kiyi; ki P 0;
X
k
i¼1
ki ¼ 1
(
)
:
3. Problem formulation and mathematical preliminaries
3.1. Problem formulation
Following [13], we consider a generic AI system that processes some input signals, produces internal representations of
the input and returns some outputs. We assume that there is a sampling process whereby some relevant information about
the input, internal signals, and outputs are combined into a common vector, x, representing, but not necessarily deﬁning, the
state of the AI system.
Depending on the sampling process, the vector x may have various numbers of elements. But generally, the objects x are
assumed to be elements of Rn, with n depending on the sampling process. Over a period of activity the AI system generates a
set X ¼ fx1; . . . ; xMg of representations x. In agreement with standard assumptions in machine learning literature [49], we
assume that the set X is a random sample drawn from some distribution. The distribution that generates vectors x is sup-
posed to be unknown. We will, however, impose some mild technical assumption on the generating probability distribution.
The central question we would like to address here is how to create algorithms capable of producing a single or an esem-
ple of linear functionals suitable for decision-making at-the-edge. The focus on linear functionals is motivated by computa-
tional efﬁciency of their implementation in embedded settings. We would like to avoid using the framework of Support
Vector Machines due to the computational costs involved which may present an obstable for embedded deployment.
Assumption 1. The probability density function, p, associated with the probability distribution of the random variable x
exists, is deﬁned on the unit ball Bn, and there exist C > 0 and r 2 ð0; 2Þ such that
Z
Bnð1=2;zÞ
pðxÞdx 6 C r
2
 n
for all z 2 Bn; kzk ¼ 1=2:
ð1Þ
The assumption requires that the vector-valued random variable x is in Bn which is consistent with the scope of our appli-
cations. The other part of the assumption, condition (1), is a version of the Smeared Absolute Continuity (SmAC) property
introduced in [12,14]. Awareness of the latter property will be important for the algorithms that follow. In addition to
Assumption 1 it will be convenient to consider alternative speciﬁcations of the data probability distribution which are cap-
tured in Assumption 2 below.
Assumption 2. The probability density function, p, associated with the probability distribution of the random variable x
exists, is deﬁned on the unit ball Bn, and there exist C > 0 and r 2 RP0 such that
pðxÞ <
Crn
VnðBnÞ ;
ð2Þ
for all x 2 Bn.
Note that if Assumption 2 holds with r 2 ð0; 2Þ then it automatically implies that Assumption 1 holds true too. In case of
Assumption 2 we do not wish yet to specify exact values of C and r. We would, however, like to formalise a particularly useful
form of its dependence on dimension n and the volume of the unit ball Bn captured by (2).
Deﬁnition 1. A point x 2 Rn is linearly separable from a set Y  Rn, if there exists a linear functional lðÞ such that
lðxÞ > lðyÞ
for all y 2 Y.
Deﬁnition 2. A set X  Rn is linearly separable from a set Y  Rn, if there exists a linear functional lðÞ such that
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
126

lðxÞ > lðyÞ
for all y 2 Y and x 2 X.
In addition to these standard notions of linear separability, we adopt the notion of Fisher separability [12,14]. Observe
that the classical Fisher separability requires Mahalanobis inner product or whitening. Hereinafter, we assume that the
approximate whitening and centralization for the dataset are completed and the inner product is ﬁxed.
Deﬁnition 3. A point x 2 Rn is Fisher separable from a set Y  Rn, if
ðx; xÞ > ðx; yÞ
ð3Þ
for all y 2 Y. The point is Fisher separable from the set Y with a threshold j 2 ½0; 1Þ if
ðx; xÞ > jðx; yÞ
ð4Þ
Having introduced all relevant assumptions and notions, we are now ready to proceed with results underpinning our
algorithmic developments.
3.2. Mathematical preliminaries
Our ﬁrst result is provided in Theorem 1 (cf. [15,12]) which is a prototype for a large family of stochastic separation the-
orems [17].
Theorem 1. Let X ¼ fx1; . . . ; xMg be given, xi 2 Bn, and let x be drawn from a distribution satisfying Assumption 1. Then x is
Fisher separable from the set X with probability
P P 1  MC r
2
 n
; r 2 ð0; 2Þ:
ð5Þ
Proof of Theorem 1. Consider events
Ai : x is Fisher separable from xi:
According to Deﬁnition 3, this is equivalent to that ðx; xÞ  ðx; xiÞ > 0. Therefore
Pðnot AiÞ ¼
Z
ðx;xÞðx;xiÞ60
pðxÞdx:
According to the De Morgan’s law,
^
M
i¼1
Ai ¼ not
_
M
i¼1
ðnot AiÞ
 
!
:
Hence
PðA1 ^ A2 ^    ^ AMÞ ¼ 1  Pððnot A1Þ _ ðnot A2Þ _    _ ðnot AMÞÞ;
and consequently
PðA1 ^ A2 ^    ^ AMÞ P 1 
X
M
i¼1
Pðnot AiÞ:
ð6Þ
Consider the set
Xi ¼ fx 2 Bnjðx; xÞ  ðx; xiÞ 6 0g ¼ Bnðkxik=2; xi=2Þ:
Since xi 2 Bn, it follows that Xi # Bnð1=2; xi=2Þ. This and Assumption 1 imply that
Z
ðx;xÞðx;xiÞ60
pðxÞdx 6
Z
Bnð1=2;xi=2Þ
pðxÞdx 6 C r
2
 n
ð7Þ
Combining (7) and (6) we can conclude that the probability that x is separable from all xi is bounded from below by the
expression in (5). 
Remark 1. According to Theorem 1, a single-point set Y under some mild hypotheses can be separated from X by a simple
Fisher discriminant with probability close to one. Denoting
d ¼ MC r
2
 n
one can conclude that Y is Fisher separable from a given set X  Bn with probability grater or equal to 1  d if
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
127

n P log M þ log C  log d
log 2  log r
:
For M ¼ 104; C ¼ 102; d ¼ 103, and r ¼ 1, the above inequality holds for all n P 30.
In practice, however, we may be interested in a situation when the set Y contains several elements which may or may not
have some inherent clustering structure or concentrations. In what follows we derive a generalization of Theorem 1 enabling
us to formally address the latter case too.
Consider two random sets X ¼ fx1; . . . ; xMg and Y ¼ fy1; . . . ; yKg. Let there be a process (e.g. a learning algorithm) which,
for the given X; Y or their subsets, produces a classiﬁer
fðÞ ¼
X
d
i¼1
aiðzi; Þ; ai 2 R:
The vectors zi; i ¼ 1; . . . ; d are supposed to be known. Furthermore, we suppose that the function f is such that
fðyjÞ >
X
d
m;k¼1
amakðzm; zkÞ
ð8Þ
for all yj 2 Y. In other words, if we denote w ¼ Pd
i¼1aizi, the following holds true:
ðw; wÞ < ðw; yiÞ for all i ¼ 1; . . . ; K:
ð9Þ
Note that since the Y; X are random, it is natural to expect that the vector a ¼ ða1; . . . ;adÞ is also random. The following state-
ment can now be formulated:
Theorem 2. Consider sets X ¼ fx1; . . . ; xMg and Y ¼ fy1; . . . ; yKg. Let paðaÞ be the probability density function associated with the
random vector a, and a satisﬁes condition (8) with probability 1. Then the set X is separable from the set Y with probability
P P 1 
X
M
i¼1
Z
Hða;xiÞ60
paðaÞda;
ð10Þ
where
Hða; xiÞ ¼
X
d
k;m¼1
akamðzk; zmÞ 
X
d
m¼1
amðzm; xiÞ:
Proof of Theorem 2. Consider events
Ai : ðw; wÞ > ðw; xiÞ:
Event Ai is equivalent to that the inequality
Hða; xiÞ ¼
X
d
k¼1
akzk;
X
d
k¼m
amzm
 
!

X
d
m¼1
amðzm; xiÞ > 0
holds true. According to (6), Eq. (10) is a lower bound for the probability that all these events hold. Recall that vectors a sat-
isfy (9), and hence
X
d
m¼1
amðzm; xiÞ ¼ ðw; xiÞ < ðw; yjÞ ¼
X
d
m¼1
amðzm; yjÞ
for all xi 2 X and yj 2 Y with probability at least (10). The statement now follows immediately from Deﬁnition 2. 
Remark 2. Theorem 2 generalizes earlier k-tuple separation theorems [48] to a very general class of practically relevant
distributions. No independence assumptions are imposed on the components of vectors xi and yi. We do, however, require
that some information about distribution of the classiﬁer parameters, a, is available.
Observe, for example, that if there exist L > 0; k 2 ð0; 1Þ and a function b : N  N ! R such that
Z
Hða;yÞ60
paðaÞda 6 Lkbðd;nÞ
for any y 2 Rn then (10) becomes
P P 1  MLkbðd;nÞ:
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
128

If d ¼ n and elements of the set Y are sufﬁciently strongly correlated, then the above bound becomes similar to (5) from The-
orem 1. The latter provides a good approximation of the separability probability bound for a simple separating function in
which w is just a scaled centroid of the set Y.
Theorems 1 and 2 link dimensionality of the decision-making space with opportunities for quick and fast learning by
mere Fisher discriminants. Before, however, moving on to the actual algorithms for either improving legacy data-driven
AI systems or generating new edge-based classiﬁers or both, let us examine another useful property of data in high dimen-
sion. This property is summarised in Theorem 3.
Theorem 3. Let y be a given element of Bn, and let X ¼ fx1; . . . ; xMg be a ﬁnite sample of elements xi drawn identically and
independently from a distribution satisfying Assumption 2.
Then
(1) y is Fisher separable from the set X with probability
P P 1  1
2 MC ð1  kyk2Þ
1
2r

n
(2) every x 2 X is Fisher separable from y with probability
P P 1  MC kykr
ð
Þn:
Proof of Theorem 3. Consider ﬁrst statement 1) of the theorem. According to Deﬁnition 3, the point y is Fisher separable
from X if
kyk2 ¼ ðy; yÞ > ðy; xÞ 8x 2 X
Given that X 2 Bn, this is equivalent to the fact that no element of X belongs to the spherical cap
CnðyÞ ¼
z 2 Bnj
y
kyk ; z


 kyk P 0


:
The probability that an x 2 X ends up in the cap is
Z
CnðyÞ
pðxÞdx 6 1
2
Z
Bnð
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1kyk2
p
;yÞ
pðxÞdx 6 1
2
ð1  kyk2Þ
1
2r

n
:
Combining this with (6) gives the required bound.
Statement (2) follows from the observation that all x 2 X outside of the ball Bnðkyk; 0Þ are Fisher separable from y:
kxk > kyk ) ðx; xÞ  ðx; yÞ P kxkkyk  ðx; yÞ > 0:

Note that the choice of y in the statement of Theorem 3 is assumed to be independent on the draw of the sample X. The-
orem 3 enables us to formulate a simple corollary revealing an interesting dichotomy of datasets in high-dimensional spaces.
More precisely.
Corollary 1. Let y be a given element of Bn, and let X ¼ fx1; . . . ; xMg be a ﬁnite sample of elements xi drawn identically and
independently from a distribution satisfying Assumption 2 with r <
ﬃﬃﬃ
2
p
. Then, with probability
P P 1  MC
rﬃﬃﬃ
2
p

n
;
either
(1) y is Fisher separable from Xor
(2) every x 2 X is Fisher separable from y and y is inside the ball Bnð1=
ﬃﬃﬃ
2
p
; 0Þ.
Proof of Corollary 1. Let y be an arbitrary element of Bn. Then either 1  kyk2 6 kyk2 or 1  kyk2 > kyk2. If the ﬁrst alter-
native holds true then 1  kyk2 6 1=2 and statement 1) follows from Theorem 3, alternative 1. If the opposite holds true then
statement 2) follows from Theorem 3, alternative 2. .
Corollary 1 captures and formalizes the following geometric dichotomy of high-dimensional data:
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
129

For sufﬁciently large dimension n, a given point y in Bn, and a ﬁnite sample X drawn from a class of distributions (Assumption 2
with r <
ﬃﬃﬃ
2
p
and suitable C), with probability close to 1, the point y is either Fisher-separable from the sample X or every ele-
ment x of X is Fisher-separable from y.
This an other properties Theorems 1–3 reveal important geometrical structures in a broad class of ﬁnite high-dimensional
data sets. We will exploit these properties in the next section (Section 4) and discuss relationships between these properties
and some open questions and observations in the theory and practice of statistical learning (Section 5). We would also like to
note that the independence requirements in Theorem 3 and Corollary 1 can be relaxed in various ways, for example, by
replacing probability densities in Assumptions 1,2 with appropriate conditional densities (see [12], [17] for more details
and ways to relax the independence requirements). With the latter modiﬁcations of assumptions, relevant statements
and conclusions will not change.
4. Fast removal of AI errors and learning from new examples without catastrophic forgetting
Consider two ﬁnite sets, the set X  Rn representing correct responses of the asset AI system, and the set Y  Rn repre-
senting errors or new knowledge to be accommodated. The task is to efﬁciently construct a classiﬁer separating the set X
from Y.
According to theoretical constructions presented in the previous section, the following is an advantage for successful and
efﬁcient separation of random sets in high dimension: one of the sets (set Y) should be sufﬁciently concentrated (spatially
localized and have an exponentially smaller volume relative to the other [Theorems 1, 2]). If this is the case then, successful
separability of this set of smaller volume depends on absence of unexpected concentrations in the probability distributions.
Importantly, the probability of success approaches one exponentially fast, as a function of the data dimensionality.
In practice, however, the assumption that one of the sets is spatially localized in a small volume is too restrictive. To over-
come this issue, we propose to partition/cluster the set Y into a union of spatially localized subsets. Presence of local con-
centrations and separability issues have been linked and analyzed in [15,12,1]. The proposed clustering of the set Y aims at
addressing these issues too.
Below we present an algorithm for fast and efﬁcient error correction of AI systems which is motivated by these observa-
tions and intuition stemming from our theoretical results.
Algorithm 1. (Few-shot AI corrector: 1NN version. Training). Input: sets X; Y, the number of clusters, k, threshold, h (or
thresholds h1; . . . ; hk).
1. Determining the centroid x of the X. Generate two sets, X c, the centralized set X, and Y, the set obtained from Y by sub-
tracting x from each of its elements.
2. Construct Principal Components for the centralized set X c.
3. Using Kaiser, broken stick, conditioning rule, or otherwise, select m 6 n Principal Components, h1; . . . ; hm, corresponding
to the ﬁrst largest eivenvalues k1 P    P km > 0 of the covariance matrix of the set X c, and project the centralized set X c
as well as Y onto these vectors. The operation returns sets X r and Y
r, respectively:
X r ¼ fxjx ¼ Hz; z 2 X cg
Y
r ¼ fyjy ¼ Hz; z 2 Yg; H ¼
h
T
1
..
.
h
T
m
0
B
B
@
1
C
C
A:
4. Construct matrix W
W ¼ diag
1ﬃﬃﬃﬃﬃ
k1
p
; . . . ;
1ﬃﬃﬃﬃﬃﬃ
km
p


corresponding to the whitening transformation for the set X r. Apply the whitening transformation to sets X r and Y
r. This
returns sets X w and Y
w:
X w ¼ fxjx ¼ Wz; z 2 X rg
Y
w ¼ fyjy ¼ Wz; z 2 Y
rg:
5. Cluster the set Y
w into k clusters Y
w;1; . . . ; Y
w;k . Let y1; . . . ; yk be their corresponding centroids.
6. For each pair ðX w; Y
w;iÞ; i ¼ 1; . . . ; k, construct (normalized) Fisher discriminants w1; . . . ; wk:
wi ¼ ðCovðX wÞ þ CovðY
w;iÞÞ1yi
kðCovðX wÞ þ CovðY
w;iÞÞ1yik
:
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
130

An element z is associated with the set Y
w;i if ðwi; zÞ > h and with the set X w if ðwi; zÞ 6 h.
If multiple thresholds are given then an element z is associated with the set Y
w;i if ðwi; zÞ > hi and with the set X w if
ðwi; zÞ 6 hi.
Output: vectors wi; i ¼ 1; . . . ; k, matrices H and W.
Remark 3. It is worthwhile to note that Steps 3,4 of Algorithm 1 can in principle be extended or supplemented with exhaus-
tive automated or semi-automated subspace learning [33] with an aim to produce best features to build AI corrections on.
These extensions, however, may be dependent on human input or they may be incompatible with computational resources
available. A plausible compromise could be to ﬁnd suitable subspaces ofﬂine and replace Steps 3,4 with computing projec-
tions of the data onto these subspaces followed by data whitening.
The deployment/application part of the algorithm is as follows:
Algorithm 2. (Few-shot AI corrector: 1NN version. Deployment). Input: a data vector x, the set’s X centroid vector x,
matrices H; W, the number of clusters, k, cluster centroids y1; . . . ; yk, threshold, h (or thresholds h1; . . . ; hk), discriminant
vectors, wi; i ¼ 1; . . . ; k.
1. Compute
xw ¼ WHðx  xÞ
2. Determine
‘ ¼ arg min
i
kxw  yik:
3. Associate the vector x with the set Y if ðw‘; xwÞ > h and with the set X otherwise.
If multiple thresholds are given then associate the vector x with the set Y if ðw‘; xwÞ > h‘ and with the set X otherwise.
Output: a label attributed to the vector x.
In contrast to previously proposed approaches using stochastic separation effects [48], Algorithm 2 mitigates the presence
of clusters whose centroids are close to the origin. If such clusters do occur and the fraction of the set X located in their vicin-
ity is not overwhelmingly large (which is ensured by Theorem 3 and Corollary 1) then, at the stage of deployment, the cor-
responding correcting discriminants will be triggered by elements from X infrequently.
Remark 4. According to Theorem 1, a single-point set Y under some mild hypotheses would be separated from X with
probability close to one.
If the set Y consists of multiple correlated subsets then, as the number of clusters increases, one would expect that the
algorithm’s performance in separating the sets X; Y improves.
At the same time, one may not necessarily require a near-perfect separability. For example, removal of 90% of all errors at
the cost of a slight performance degradation of the AI’s basic functionality may be an acceptable compromise in many
applications. If the data dimensionality is sufﬁciently high then the desired separation might be achieved with just a single
linear functional, provided that the centroid y of the set Y is separated away from the centroid x of the set X.
Fig. 1. Separation of a non-isolated set Y from X. Black circle represents the set X, red circle represents the set Y. Filled disks represent centres of the sets X
and Y, respectively. Left panel. Single-cluster case. Right panel. Multiple-cluster case. According to Theorem 3, the classes are ‘‘hollow” inside if Assumption 2
with appropriate parameters holds for their correspondin.g distributions.
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
131

The rationale behind this observation is as follows. Let X be equidistributed in Bn and Y be drawn from another
equidistribution in a unit n-ball but centered at a point whose Euclidean norm is 0 < e  1 (see Fig. 1). Let
w ¼ ðy  xÞ=ky  xk ¼ e1y. Let j 2 ð0; 1Þ, and let hðxÞ ¼ ðx; wÞ  je be the separating hyperplane so that if hðzÞ > 0 then
the vector z is associated with Y, and z is associated with the set X if hðzÞ 6 0. Then the fraction of elements from X ‘‘missed”
(false negative response) by this rule is bounded from above by
qx ¼ 1
2 ð1  j2e2Þ
n
2;
and the fraction of elements from Y incorrectly attributed to X (false positive response) is bounded from above by
qy ¼ 1
2 1  ð1  jÞ2e2

n
2:
Hence, when n is sufﬁciently large, both qx;qy may be made acceptably small even if e is small too (cf [16]).
Remark 5. Note that if the clustering step in Algorithm 1 is performed so that, for every i
z 2 Y
w;i ) kz  y

ik < kz  y

jk; for all j – i
then the proposed 1NN integration logic, as in Algorithm 2, correctly assigns elements from Y to their corresponding dis-
criminants wi. For each query point, only one of the linear discriminants is active. This is markedly different from the more
aggressive ‘‘union” integration logic (OR rule) proposed in [47] in which, for a single query point, all discriminants are active
simultaneously leading to higher chances of producing false negative errors. In this respect, the 1NN rule is somewhat tighter
than the OR rule.
Remark 6. It may sometimes be computationally advantageous to perform the clustering step in Algorithm 1 (step 5) prior
to dimensionality reduction. This will result in that the deployment part of the algorithm, Algorithm 2 changes as follows
Algorithm 3.
(Few-shot AI corrector: 1NN version. Deployment) Input: a data vector x, the set’s X centroid vector x, matrices H; W, the
number of clusters, k, cluster centroids y1; . . . ; yk, threshold, h (or thresholds h1; . . . ; hk), discriminant vectors, wi; i ¼ 1; . . . ; k.
Pre-compute vectors
w
i ¼ HTWwi:
1. Determine
‘ ¼ arg min
i
kx  x  yik:
2. Associate the vector x with the set Y if ðw
‘; ðx  xÞÞ > h and with the set X otherwise.
If multiple thresholds are given then associate the vector x with the set Y if ðw
‘; ðx  xÞÞ > h‘ and with the set X
otherwise.
The difference between Algorithm 2 and 3 is that the data point x no longer needs to be projected onto the principal com-
ponents. This may be computationally advantageous when the number of components on which the data is projected is lar-
ger than the number of clusters k.
5. Discussion
Results and algorithms presented in Sections 3.2, 4 enable equipping existing asset AI systems with capabilities to learn
from new examples via transparent and reversible modiﬁcations of their structure in response to errors. Modiﬁcations of the
systems’ structure are:
 nodes and components implementing calculations of relevant inner products determined in Steps 2 and 3 in Algorithms 3
and 2, respectively
 relevant decision-making/ integration logic in these steps.
In what follows we discuss other relevant properties of the proposed algorithms such as performance bounds, their relation
to existing approaches in the ﬁled of few-shot learning, and possible future directions.
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
132

5.1. Guaranteed performance bounds
One of the main features of our approach is that it enables explicit estimation and control of potential negative perfor-
mance changes induced by learning/ error correction (Algorithms 1–3).
Indeed, let Algorithms 1, 3 return weights w
1; . . . ; w
k 2 Rn, thresholds h1; . . . ; hk, and the value of x. Let zi  x; i ¼ 1; . . . ; N
be a ﬁnite i.i.d. sample drawn from a distribution deﬁned on Bn and satisfying Assumption 2. Suppose now that this sample
represents new data which Algorithms 1, 3 did not have access to and on which performance of the asset AI system prior to
the application of Algorithms 1, 3 was correct. In the context of Algorithms 1 and 3, vectors zi are to be associated with the
set X. According to Algorithm 3, an element zi is assessed by a single ‘-th discriminant. If
ðw
‘; zi  xÞ 6 h‘
then the modiﬁed asset AI correctly associates this element with the set X. If, however,
ðw
‘; zi  xÞ > h‘
then the modiﬁed system assigns zi to Y. This assignment, if the set Y corresponds to errors, introduces an error in the com-
bined system.
The probability Pe of such an error can be bounded from above as
Pe 6 1
2 C
1 
h‘
kx
‘k

2
 
!1
2
r
0
@
1
A
n
:
ð11Þ
If there is an m 2 ð0; 1Þ such that
C
1
n
1 
h‘
kx
‘k

2
 
!1
2
r 6 m
ð12Þ
for all ‘ ¼ 1; . . . ; k then Pe 6 1
2mn converges to 0 exponentially fast with n. This probability does not grow with k as long as (12)
holds true. This is an advantage over earlier versions of AI corrector [47,13,48] in which worst-case bounds on Pe grow lin-
early with k as a consequence of the conjunction (OR) integration logic.
The proposed few-shot AI correction mechanisms complement existing approaches to learning without catastrophic for-
getting such as elastic weight consolidation [27]. Indeed, under appropriate conditions, it follows from (11), (12) that new
knowledge acquired through the application of Algorithms 1, 3 has an exponentially vanishing probability to damage to AI
existing skills. For given and ﬁxed bounds on parameters C; r of the data distribution (Assumption 2), this probability can be
controlled by the ratio
h‘
kx
‘ k.
5.2. Geometry of few- or one-shot learning
Theorems 1–3, on which our algorithms are built, enable to shed additional light on why and when few- and one-shot
learning works [51,53] (see also references therein). Consider task-invariant embedding learning [51] for a binary classiﬁca-
tion task. Let f0; 1g be the set of labels. Following [51,53], let the predicted label ^pðzÞ of the vector z be deﬁned as
^pðzÞ ¼ F
X
m
i¼1
aðyi; zÞpiðyiÞ  h
 
!
;
ð13Þ
where a : Rn  Rn ! R is a kernel, F : R ! R is a label matching function, yi 2 Rn are examples to learn from, piðyiÞ are
weights, and F is a step function:
FðsÞ ¼
1;
s > 0
0;
s 6 0:

The kernel function að; Þ ‘‘matches” data to existing knowledge, and the function F assigns labels to data on the basis of this
matching.
In our settings, kernel function að; Þ is an inner product ð; Þ. Let piðyiÞ ¼ 1=m. Hence (13) becomes
^pðzÞ ¼ F ðw; zÞ  h
ð
Þ; w ¼ 1
m
X
m
i¼1
yi;
ð14Þ
where w is a class prototype [53,42] (cf [21]). Let t; D; d be numbers such that
kyik2 > t for all i 2 f1; . . . ; mg
ð15Þ
and
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
133

D P ðyi; yjÞ P d > 0 8i – j; i; j 2 f1; . . . ; mg:
ð16Þ
The latter condition can be interpreted as a consistency requirement in the sense that all training samples in the same learn-
ing episode share a degree of similarity.
Set
h ¼ 1
mt þ m  1
m
d:
ð17Þ
It now follows that ðw; yÞ > h for every vector from the convex hull of ðy1; . . . ; ymÞ:
ðw; yÞ > h for all y 2 Convðy1; . . . ; ymÞ:
Indeed, if y ¼ Pm
i¼1kiyi; ki P 0; Pm
i¼1ki ¼ 1 then
ðw; yÞ ¼
X
m
i¼1
ki w; yi
ð
Þ >
X
m
i¼1
ki
1
mt þ m  1
m
d


¼
1
mt þ m  1
m
d


:
Geometrically, as is shown in Fig. 2, Convðy1; . . . ; ymÞ and its appropriate e-thickening can be viewed as a ‘‘knowledge core” or
a ‘‘knowledge slab” of the training vectors y1; . . . ; ym.
Class prototype vector w induces a decision rule labeling all points y : ðw; yÞ > h as ‘‘1”. Using (11), were w
‘; h are
replaced with w; h from (14), (17), one can determine probability bounds on how acquiring new knowledge through few-
shot schemes like (13) affects assets AI skills on other tasks.
Note that few-shot rules (13), (14), in addition to learning ‘‘knowledge core”, may associate many more points with the
label ‘‘1”. These extra points form ‘‘knowledge shadow” as is shown in Fig. 2. The volume of the ‘‘knowledge shadow” may
exceed that of the ‘‘knowledge core”.
Let R ¼ ð1  ðh=kwkÞ2Þ
1=2 denote the radius of the disc in the base of the spherical cap: Cðw; hÞ ¼ fy 2 Bnjðw; yÞ > hg. The
set
Sh ¼ Cðw; hÞ n Convðy1; . . . ; ymÞ
is the ‘‘knowledge shadow”. It is well-known that the volume of the convex hull of any m vectors in BnðR; 0Þ is bounded from
above by m=2nVðBnðR; 0ÞÞ [7]. At the same time VðCðw; hÞÞ > 1=nð1  ð1  R2ÞÞ
1=2VðBn1ðR; 0ÞÞ. Hence
VðShÞ
VðCðw; hÞÞ > 1  mn
2n
R
ð1  ð1  R2ÞÞ
1=2
VðBnð1; 0ÞÞ
VðBn1ð1; 0ÞÞ :
Therefore in high dimension the volume of the ‘‘knowledge shadow” becomes exponentially large relative to that of the
‘‘knowledge core”. Moreover, since the prototype class w is a sample average, the scheme inherits a degree of robustness
to perturbations of the training data y1; . . . ; ym. These properties, as well as (15), (16), explain when and why few- and
one-shot learning algorithms such as Algorithms 1–3 or [42,51] are robust and generalise offering a solution pathway to
the challenge of generalisaion in large-scale systems [54].
A natural extension of few-shot learning schemes (13) is to allow weights pðyiÞ to depend on the values of z. One of the
advantages of such extension is a capability to learn from training data for which consistency condition (13) does not hold
true. Indeed, in this case training data can be partitioned into clusters satisfying (13). For every cluster there will be a pro-
totype that is activated by z. Proposed Algorithms 1–3 are examples of such extended schemes.
Fig. 2. Geometrical interpretation of few-shot learning. Vertices of the pentagon are training samples y1; . . . ; ym, and vector with the ﬁlled circle head shows
class prototype w. The interior of the pentagon is the ‘‘knowledge core”, shaded areas adjacent to the sides of the pentagon are in the ‘‘knowledge shadow” -
the set of all points in the spherical cap fy 2 Bnjðw; yÞ > hg which are outside of the ‘‘knowledge core”.
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
134

In the next section we illustrate the application of these algorithms in two practical scenarios of edge-based AI
deployment.
6. Examples
The choice of examples is motivated primarily by our intention to illustrate the application of the new algorithm with and
without clustering. The ﬁrst example illustrates how one can take advantage of high-dimensional data for constructing a sin-
gle discriminant. The second example enables us to show advantages of clustering for a problem where the feature space is
genuinely high-dimensional and where the number of errors made by edge-based systems necessitates automated and com-
putationally efﬁcient interventions to be deployed at the edge. In both examples, AI correctors were built on the raw input
data. This enabled us to emulate extreme scenario where internal signals from the asset AI system are not available. The
approach, however, would not change if these signals become available.
6.1. Real-time Tool Wear and Product Quality Prediction for Computer Numerical Controlled (CNC) Milling Machines
6.1.1. System overview and setup
Milling is a process of removing excess material by advancing a cutter into a work piece. It is one of the most commonly
used processes for machining freeform surfaces and custom parts [30]. Quality of the ﬁnal part depends on many factors
including tool path, tool orientation, tool geometry, tool wear and security of the part ﬁxing. Here we focused on real-
time prediction of part quality from the measurements characterizing electrical and mechanical state of the CNC machine.
To build a data-driven quality detector we used the CNC Milling Dataset from the University of Michigan Smart Lab [45].
The dataset contains a series of machining experiments run on 2”  2”  1.5” wax blocks. Machining data was collected from
a CNC machine for variations of tool condition, feed rate, and clamping pressure. An example of a ﬁnished wax part with an
‘‘S” shape - S for smart manufacturing - carved into the top face is shown in Fig. 3.
Time series data from the machine was collected from 18 experiments with a sampling rate of 100 ms, and each instan-
taneous measurement vector contained 48 attributes. The task was to predict the output quality of the part, as conﬁrmed by
visual inspection, from the instantaneous measurements.
6.1.2. Automated part quality prediction
In this problem, we used Algorithms 1, 2 with a single cluster. We also used a slightly modiﬁed Step 3 in 1 in which we
retained 21 Principal Components: from the 20th to the 40th. Note that we did not use the ﬁrst 19 components as in this
particular problem inclusion of these components did not translate into noticeable changes of the model’s performance. Dis-
carding certain principal components is a common practice in many data science domains. For example, ﬁrst principal com-
ponents are frequently considered to be associated with technical artifacts in the analysis of omics datasets in
bioinformatics, and their removal might improve the downstream analyses [43,4,22]. In some cases in this ﬁeld, more than
10 ﬁrst principal components have to be removed, to increase the signal/noise ratio [28].
Training and validation datasets. To train the model we used 9 experiments (out of the total 18) in which 5 experiments
corresponded to machining with unworn tool and which passed a visual inspection (experiments 1; 3; 4; 5; 11) and 4 exper-
iments contained data corresponding to runs that either did not ﬁnish or where the part did not pass visual inspection (ex-
periments 6; 7; 8; 9); 3 experiments were used for testing (experiments 2 and 17 corresponding to successful runs where
parts passed visual inspection, and experiment 10 in which the part failed the inspection).
Experiments and results. Each measurement in both training and testing datasets have been labeled as ‘‘pass” or ‘‘fail”
depending on whether the run from which this measurement was taken passed the visual inspection (and hence the point
was labeled as a ‘‘pass”) or failed (resulting in the label ‘‘fail”). Training of the model took 0:16 to 0:2 seconds on a core i7
laptop, and a summary of the model performance is summarized in Fig. 4.
Fig. 3. An example of a part produced by the CNC milling machine .[45].
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
135

To better integrate the model’s decision and ﬁlter spurious errors we averaged the model’s binary output (computed with
the threshold of 1:5) over a sliding window of 200 measurements. The resulting value for each window was recorded and
shown as ‘‘Score” in Fig. 5. Outcomes of these experiments on the entire training and test sets, where the data from several
runs was simply concatenated, are presented in Fig. 5.
As one can see from these ﬁgures, the model correctly identiﬁes failed runs. Good generalization performance of this sim-
ple model can be explained by the concentration effects captured in Fig. 1: relatively small differences of class means are
apparently sufﬁcient to ensure reasonable class separation by a Fisher discriminant if the data dimension is sufﬁciently large.
In addition to this, we note that not every single dimension is equally important. To illustrate this point, let x and y denote
empirical class means for the ‘‘pass” and ‘‘fail” classes. We calculated
Relative relevancei ¼
x  y
kx  yk
hi
khij
				
				; i ¼ 20; . . . ; 40;
where hi are the corresponding Principal Components (Algorithm 1). The relative relevance indices for each i-th component
and the contribution of that component to the total empirical data variance are shown in Fig. 6. As we can see from Fig. 6, a
large proportion of principal components only marginally project onto the vector x  y. We would like to comment that high
relative relevance values do not necessarily imply that the corresponding features are best for classiﬁcation in isolation from
other features. High relative relevance values, however, may suggest that in the overall high-dimension space, faulty parts
Fig. 4. ROC curves for detecting a faulty run from a single measurement in a run.
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
136

data concentrate near some lower-dimensional subspace. This, in view of discussion in Section 5, could be another factor
explaining good practical results of our algorithms in this task.
6.2. Adaptive Removal of False Positives in Video Surveillance and Analytics Systems: A Case Study
6.2.1. System overview and setup
All UK airports are expected to ensure that the average passenger spends no longer than twelve minutes going through
the security area.
The current boarding gates can measure the number of passengers coming through the security area via passenger board-
ing passes. However there is no way of determining whether an individual passenger has left the security area. The current
solution involves a member of staff manually keeping track of a small sample of passengers passing from the boarding gates
to the security scanners and logging the time taken.
Knowledge about the length of these queues, as well as the number of passengers getting through the airport, helps air-
ports to manage their resources in an efﬁcient way by enabling them to decide how many security stations should be open. It
also provides passengers with valuable information on the amount of time they can expect to spend inside the queues
thereby allowing them to manage their time inside the airport more efﬁciently. Thus, knowing the time taken for a passenger
since entering boarding-pass gates until leaving security gates, in almost real time, would be very beneﬁcial. However, the
current practices aimed at addressing this speciﬁc problem are far from being efﬁcient.
Fig. 5. Averaged prediction ‘‘Score” for training (top panel) and testing runs (bottom panel). Green and black curves in the top panel show correct and failed
machining for the training set, respectively. Blue and red curves correspond to correct and failed runs in the test set, respectively.
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
137

To address the efﬁciency issue, various Queue Management System (QMS) are being developed. Here we provide a short
description of a system which has been developed by the Visual Management Systems Ltd within the scope of the Innovate
UK Knowledge Transfer Partnership project (KTP 10522).
The system consists of two major components: the hardware that is in charge of detecting faces, and the back-end server
that processes the data streamed from the hardware unit and calculates the average, fastest and slowest security queue
times. A front-end web-page is served by the back-end to display the aforementioned statistical data as well as producing
historical reports. As mentioned before, security queue time is the time a passenger spends in queues for the Boarding Pass
Gates (BPG) and Security Gates (SG).
For the hardware, shown in Fig. 7, we utilise two (or more) high-deﬁnition cameras streaming in H.264, one for BPG and
one for SG, connected to two Processing Modules TM (PM) via two mini PC’s.
Fig. 6. Relative relevance of Principal Components for class discrimination.
Fig. 7. Queue Management System.
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
138

The function of the PMs is to detect and track faces from the video feeds and output relevant metadata (face coordinates,
time stamp, bitmap/image of the face etc) to the processing server. The metadata are then received by the back-end server
for processing. The back-end includes four different sections; parsing, image pre-processing, feature extraction and the data
base. In the parsing section, the encoded face bitmap is extracted together with all other metadata. This is followed by the
pre-processing unit in which each thumbnail is encoded to JPG format and saved to the local hard disk before passing
through the Dlib library to generate a face score. Then, the thumbnail will pass through a Convolutional Neural Network,
e.g. VGG-16 [41], for feature extraction. Finally, features of the thumbnail together with all related metadata will be stored
in the database.
The system has been trialed over a period of 3 months in a major UK airport. The trials revealed that, depending on oper-
ational conditions, PMs based on propriertrary algorithms, occasionally return false positive detects These false positive
detects if left untreated, have a capacity to slow down the entire processing pipeline. As the system scales up, dealing with
these false positive detects on the side of the server becomes computationally prohibitive. Moreover, the false positive
detects, are camera and place-speciﬁc, in general. Thus there is a need and a rationale to address these false positive detects
at their source.
6.2.2. Adaptive removal of false positives
In order to address the problem of false positives, we implemented and tested the proposed algorithms in this setup. For
the purposes of avoiding issues with reproducibility as well as due to the data protection, in what follows we present a
detailed account of this implementation in which the PM was an OpenCV implementation of the Haar face detector. The
Fig. 8. ROC curves after the application of the AI error correcting algorithm with 200 Principal Components for different numbers of clusters, from 1 to 100.
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
139

detector has been applied to a publicly accessible video footage capturing trafﬁc and pedestrians walking on the streets of
Montreal. For the purposes of testing and validation, we used standard MTCNN face detector from the mtcnn Python package
as a vehicle to generate ground truth data. We did not introduce any changes to parameters of the standard detector from
that package. All the data as well as the code generating true positive and false positive images can be found in [35].
For this particular dataset, the total number of true positives was 21896, and the total number of false positives was 9372.
All the detects have been resized to 64  64 crops (in RGB encoding). Each crop produces a 12288-dimensional vector. From
this dataset, we generated a training set containing 50 percent of positive and false positives, and passed this training set to
Algorithm 1. In the algorithm, true positives have been associated with the set X, and false positives were associated with
the set Y. The number of Principal Components was limited to 200. We did not observe signiﬁcant performance variations
when the number of components changed within 20%. However, as we show later, retaining excessively large number of
components may lead to overﬁt. A possible factor contributing to this effect, in addition to increased Vapnik–Chervonenkis
dimension, could be numerical instability of the inversion of matrices in Step 6 of Algorithm 1.
We tried the algorithm for the following numbers of clusters: 1; 5; 10, and 100. At the deployment stage, we used Algo-
rithm 2. Training took, on average, about 180 s on a Core i7 laptop, and the outcomes of the process as well as performance
on the testing set are summarized in Fig. 8 where curves corresponding to different numbers of clusters (1; 5; 10, and 100)
are annotated by arrows with numbers 1; 5; 10, and 100, respectively.
As we can see from this ﬁgure, even a single-cluster implementation of Algorithm 1 allows one to ﬁlter 90 percent of all
errors at the cost of missing circa 5 percent of true positives. This is consistent with expectations discussed in Remark 4.
Implementation of the single-cluster correcting functional on an ARM Cortex-A53 processor took less than 1 ms per each
12288-dimensional vector implying signiﬁcant capacity of the approach for embedded and ‘‘at the edge” applications.
Fig. 9. ROC curves after the application of the AI error correcting algorithm with 6000 Principal Components. Blue curves correspond to the baseline case
with a single cluster. Red and green curves show behavior of the system for 100 clusters.
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
140

A notable classiﬁcation performance gain is observed for a 100-cluster version of the algorithm. This, however, comes at
additional computational costs at the stage of deployment. Having said this, the deployment part of the algorithm is extre-
mely scalable leading to signiﬁcant expected reductions of computation times in the case of parallel execution of the code
and is hence amenable to massively parallel implementations.
It is also worthwhile to mention that the concentration effects, as formulated in Theorems 1, 2, and which are at the back-
bone of Algorithms 1–3, may negatively effect the overall system’s performance if the dimensionality is excessively high and
the cardinality of the set Y is comparable to that of the set X. To illustrate this point, we used Algorithms 1, 2 with the 6000
Principal Components. Results are shown in Fig. 9.
As we can see from this ﬁgure, if the retained dimensionality of the decision-making space is too large, the algorithms
tend to overﬁt and hence special consideration needs to be given to the choice of the numbers of projections used and
the volume of training data.
7. Conclusion
In this work we presented a novel approach for equipping devices with limited computational capabilities with capabil-
ities to quickly learn on-the-job and continuously improve over time in presence of spurious as well as a rather overwhelm-
ing number of errors. The approach is based on stochastic separation theorems [8,11,13,48,12,47] and the concentration of
measure phenomena.
Our results demonstrate that the new capability can be delivered to the edge and deployed in a fully automated way,
whereby a more sophisticated AI system monitors performance of a less powerful counterpart. The approach, for the ﬁrst
time uses 1NN integration rule for error correction, as opposed to mere disjunctions. The 1NN rule is justiﬁed by the dichot-
omy of high-dimensional datasets captured in Theorem 3 and Corollary 1. In addition, our results shed light on why few- and
one-shot learning works. This new understanding can be used in the emerging frameworks for machine learning testing [55].
Experimentally, we investigated the sensitivity of the algorithm to change of its meta–parameters like the number of
clusters and projections used. The results directly respond to the fundamental challenge of removing AI errors in industrial
applications at minimal computational costs, and some elements of the theory underpin two US patents [40,39].
Theoretical results are illustrated with two industrial applications: performance monitoring of the CNC milling processes
and edge-based object detection. An application ﬁeld of the approach could be the class of randomized computational archi-
tectures such as stochastic conﬁguration networks [52], and in particular the employment of the measure concentration
effects for estimating their approximation convergence rates. The other relevant direction is to determine, for a given AI sys-
tem, which signals in the system are most appropriate for the application of our algorithms. Recent work [2] showed that
dimension of data representation in deep learning models may vary drastically depending on the layer where this represen-
tation is accessed. Therefore ﬁnding the best place to take information from for the purposes of few-shot AI correction is
important. Present work shows that intrinsic dimension of data representation could be one of key factors in answering this
question. Another interesting remaining question is how sensitive the approach is to label noise or Bayes errors. Answering
these and other related questions in depth will be the subject of our future work.
CRediT authorship contribution statement
Ivan Y. Tyukin: Conceptualization, Methodology, Software, Validation, Writing - original draft, Writing - review & editing.
Alexander N. Gorban: Conceptualization, Methodology, Software, Validation, Writing - original draft, Writing - review &
editing. Alistair A. McEwan: Conceptualization, Methodology, Software, Validation, Writing - original draft, Writing - review
& editing. Sepehr Meshkinfamfard: Conceptualization, Methodology, Software, Validation, Writing - original draft, Writing -
review & editing. Lixin Tang: Conceptualization, Methodology, Software, Validation, Writing - original draft, Writing -
review & editing.
Declaration of Competing Interest
The authors declare that they have no known competing ﬁnancial interests or personal relationships that could have
appeared to inﬂuence the work reported in this paper.
Acknowledgment
A.N, I.T, and S.M were supported by Innovate UK Knowledge Transfer Partnership grant KTP010522, I.T. was supported by
the UKRI Turing AI Acceleration Fellowship (EP/V025295/1), and L.T. was supported by a 111 Project (B16009). The work of I.
T. at St Petersburg State Electrotechnical University was supported by the grant of the Russian Science Foundation (Project
No. 19-19-00566, design of the algorithms and experiments), the work of A.N. and I.T. at Lobachevsky University was sup-
ported by the grant of the Ministry of Science and Higher Education of Russian Federation (Project No. 14.Y26.31.0022, the-
oretical analysis).
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
141

References
[1] A. Albergante, J. Bac, A. Zinovyev, Estimating the effective dimension of large biological datasets using ﬁsher separability analysis, in: Proc. of the
International Joint Conference on Neural Networks (IJCNN), 2019.
[2] A. Ansuini, A. Laio, J. Macke, and D. Zoccolan. Intrinsic dimension of data representations in deep neural networks. In Advances in Neural Information
Processing Systems, pages 6111–6122, 2019..
[3] C.
Bowman
and
P.
Grindrod.
Trust,
limitation,
conﬂation
and
hype.
https://www.researchgate.net/publication/
334425107_Trust_Limitation_Conﬂation_and_Hype, 2019..
[4] F. Censi, Calcagnini, E. Mattei, and A. Giuliani. System biology approach: Gene network analysis for muscular dystrophy. Methods in molecular biology
(Clifton, N.J.), 1687: 75–89, 2018. ISSN 1064–3745. DOI: 10.1007/978-1-4939-7374-3_6..
[5] O. Chapelle, Training a support vector machine in the primal, Neural Comput. 19 (5) (2007) 1155–1178.
[6] T. Chen, I. Goodfellow, J. Shlens, Net2net, Accelerating learning via knowledge transfer, in: International Conference on Learning Representations
(ICLR) 2016, 2015.
[7] G. Elekes, A geometric inequality and the complexity of computing volume, Discrete Comput. Geometry 1 (4) (1986) 289–292.
[8] A.N. Gorban, I.Y. Tyukin, Stochastic separation theorems, Neural Networks 94 (2017) 255–259, https://doi.org/10.1016/j.neunet.2017.07.014.
[9] A.N. Gorban, I.Y. Tyukin, Blessing of dimensionality: mathematical foundations of the statistical physics of data, Phil. Trans. R. Soc. A 376 (2018)
20170237, https://doi.org/10.1098/rsta.2017.0237.
[10] A.N. Gorban, I.Y. Tyukin, D.V. Prokhorov, K.I. Sofeikov, Approximation with random bases: Pro et contra, Inf. Sci. 364–365 (2016) 129–145, https://doi.
org/10.1016/j.ins.2015.09.021.
[11] A.N. Gorban, I.Y. Tyukin, I. Romanenko, The blessing of dimensionality: Separation theorems in the thermodynamic limit, IFAC-PapersOnLine 49 (24)
(2016) 64–69, https://doi.org/10.1016/j.ifacol.2016.10.755.
[12] A.N. Gorban, A. Golubkov, B. Grechuk, E.M. Mirkes, I.Y. Tyukin, Correction of AI systems by linear discriminants: Probabilistic foundations, Inf. Sci. 466
(2018) 303–322, https://doi.org/10.1016/j.ins.2018.07.040.
[13] A.N. Gorban, R. Burton, I. Romanenko, I.Y. Tyukin, One-trial correction of legacy AI systems and stochastic separation theorems, Inf. Sci. 484 (2019)
237–254, https://doi.org/10.1016/j.ins.2019.02.001.
[14] A.N. Gorban, B. Grechuk, I.Y. Tyukin. Augmented artiﬁcial intelligence. arXiv preprint arXiv:1802.02172, 2018..
[15] A.N. Gorban, V.A. Makarov, I.Y. Tyukin, The unreasonable effectiveness of small neural ensembles in high-dimensional brain, Phys. Life Rev. (2018),
https://doi.org/10.1016/j.plrev.2018.09.005.
[16] A.N. Gorban, V.A. Makarov, I.Y. Tyukin, High-dimensional brain in a high-dimensional world: Blessing of dimensionality, Entropy 22 (1) (2020) 82,
https://doi.org/10.3390/e22010082.
[17] B. Grechuk, A.N. Gorban, General stochastic separation theorems with optimal bounds, Neural Networks 138 (2021) 33–56, https://doi.org/10.1016/j.
neunet.2021.01.034.
[18] M. Gromov, Isoperimetry of waists and concentration of maps, GAFA, Geomteric Funct. Anal. 13 (2003) 178–215.
[19] G. Hains, A. Jakobsson, Y. Khmelevsky, Towards formal methods and software engineering for deep learning: Security, safety and productivity for DL
systems
development,
in:
2018
Annual
IEEE
International
Systems
Conference
(SysCon),
IEEE,
2018,
pp.
1–5,
https://doi.org/10.1109/
SYSCON.2018.8369576.
[20] L.K. Hansen, P. Salamon, Neural network ensembles, IEEE Trans. Pattern Anal. Mach. Intell. 12 (10) (1990) 993–1001.
[21] P. Hart, The condensed nearest neighbor rule (corresp.), IEEE Trans. Inform. Theory 14 (3) (1968) 515–516.
[22] S.C. Hicks, F.W. Townes, M. Teng, R.A. Irizarry. Missing data and technical variability in single-cell RNA-sequencing experiments. Biostatistics, 19 (4):
562–578, 11 2017. ISSN 1465–4644. DOI: 10.1093/biostatistics/kxx053..
[23] T.K. Ho, Random decision forests, in: Proc. of the 3rd International Conference on Document Analysis and Recognition, 1995, pp. 993–1001.
[24] T.K. Ho, The random subspace method for constructing decision forests, IEEE Trans. Pattern Anal. Mach. Intell. 20 (8) (1998) 832–844.
[25] P. Kainen, V. Ku˚ rková, Quasiorthogonal dimension of Euclidean spaces, Appl. Math. Lett. 6 (3) (1993) 7–10.
[26] P.C. Kainen, Utilizing geometric anomalies of high dimension: When complexity makes computation easier, in: Computer Intensive Methods in Control
and Signal Processing, Springer, 1997, pp. 283–294.
[27] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A.A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath,
D. Kumaran, R. Hadsell, Overcoming catastrophic forgetting in neural networks, Proc. Nat. Acad. Sci. 114 (13) (2017) 3521–3526.
[28] N. Krumm, P.H. Sudmant, A. Ko, B.J. O’Roak, M. Malig, B.P. Coe, A.R. Quinlan, D.A. Nickerson, E.E. Eichler, Copy number variation detection and
genotyping from exome sequence data, Genome Research, 10889051 228 (Aug 2012) 1525–1532, https://doi.org/10.1101/gr.138115.112.
[29] A. Kuznetsova, S. Hwang, B. Rosenhahn, L. Sigal, Expanding object detector’s horizon: Incremental learning framework for object detection in videos,
in: Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 28–36.
[30] A. Lasemi, D. Xue, P. Gu, Recent development in CNC machining of freeform surfaces: A state-of-the-art review, Comput. Aided Des. 42 (7) (2010) 641–
654.
[31] N. Li, D.W. Oyler, M. Zhang, Y. Yildiz, I. Kolmanovsky, A.R. Girard, Game theoretic modeling of driver and vehicle interactions for veriﬁcation and
validation of autonomous vehicle control systems, IEEE Trans. Control Syst. Technol. 26 (5) (Sep. 2018) 1782–1797, https://doi.org/10.1109/
TCST.2017.2723574.
[32] H. Liang, B. Tsui, H. Ni, C. Valentim, S. Baxter, G. Liu, W. Cai, D. Kermany, K. Sun, J. Chen, L. He, J. Zhu, P. Tian, H. Shao, L. Zheng, R. Hou, S. Hewett, G. Li, P.
Liang, X. Zang, Z. Zhang, L. Pan, H. Cai, R. Ling, S. Li, Y. Cui, S. Tang, H. Ye, X. Huang, W. He, W. Liang, Q. Zhang, J. Jiang, W. Yu, J. Gao, W. Ou, Y. Deng, Q.
Hou, B. Wang, C. Yao, Y. Liang, S. Zhang, Y. Duan, R. Zhang, S. Gibson, C. Zhang, O. Li, E. Zhang, G. Karin, N. Nguyen, X. Wu, C. Wen, J. Xu, W. Xu, B. Wang,
W. Wang, J. Li, B. Pizzato, C. Bao, D. Xiang, W. He, S. He, Y. Zhou, W. Haw, M. Goldbaum, A. Tremoulet, C.-N. Hsu, H. Carter, L. Zhu, K. Zhang, H. Xia,
Evaluation and accurate diagnoses of pediatric diseases using artiﬁcial intelligence, Nat. Med. 25 (2019) 433–438, https://doi.org/10.1038/s41591-
018-0335-9.
[33] H. Lu, K.N. Plataniotis, A.N. Venetsanopoulos, A survey of multilinear subspace learning for tensor data, Pattern Recogn. 44 (7) (2011) 1540–1551.
[34] D. Meltz, H. Guterman, Functional safety veriﬁcation for autonomous ugvs–methodology presentation and implementation on a full-scale system, IEEE
Trans. Intelligent Veh. 4 (3) (Sep. 2019) 472–485, https://doi.org/10.1109/TIV.2019.2919460.
[35] S. Mashkinfamfard. Streets of Montreal dataset, 2020. https://github.com/Sep-AI/HaarCascade_Vs_MTCNN..
[36] I. Misra, A. Shrivastava, M. Hebert, Semi-supervised learning for object detectors from video, in: Proc. of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2015, pp. 3594–3602.
[37] L. Pratt, Discriminability-based transfer between neural networks, Advances in Neural Information Processing Systems 5 (1992) 204–211.
[38] A. Prest, C. Leistner, J. Civera, C. Schmid, V. Ferrari, Learning object class detectors from weakly annotated video, in: Proc. of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2012, pp. 3282–3289.
[39] I.
Romanenko,
I.
Tyukin,
A.
Gorban,
K.
Sofeikov.
Method
of
image
processing.
US
patent
US10062013B2,
August,
28
2018.
https://patents.google.com/patent/US10062013B2/en..
[40] I. Romanenko, A. Gorban, I. Tyukin. Image processing. US patent US10489634B2, November, 26 2019. https://patents.google.com/patent/
US20180089497A1/en..
[41] K. Simonyan, A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning
Representations, 2015. arXiv:1409.1556..
[42] J. Snell, K. Swersky, R. Zemel. Prototypical networks for few-shot learning. In Advances in neural information processing systems, pages 4077–4087,
2017..
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
142

[43] N. Sompairac, P.V. Nazarov, U. Czerwinska, L. Cantini, A. Biton, A. Molkenov, Z. Zhumadilov, E. Barillot, F. Radvanyi, A.N. Gorban, U. Kairov, A. Zinovyev.
Independent component analysis for unraveling the complexity of cancer omics datasets. Int. J. Mol. Sci., 20 (18): 4414, Sep 2019. ISSN 1422–0067.
DOI: 10.3390/ijms20184414..
[44] E. Strickland, IBM Watson, heal thyself: How IBM overpromised and underdelivered on AI health care, IEEE Spectr. 56 (4) (2019) 24–31, https://doi.org/
10.1109/MSPEC.2019.8678513.
[45] S. Sun. CNC mill tool wear dataset, 2018. https://www.kaggle.com/shasun/tool-wear-detection-in-cnc-mill..
[46] A. Takács, I. Rudas, D. Bösl, T. Haidegger, Highly automated vehicles and self-driving cars [industry tutorial], IEEE Robotics Autom. Magazine 25 (4)
(Dec 2018) 106–112, https://doi.org/10.1109/MRA.2018.2874301.
[47] I.Y. Tyukin, A.N. Gorban, S. Green, D. Prokhorov, Fast construction of correcting ensembles for legacy artiﬁcial intelligence systems: Algorithms and a
case study, Inf. Sci. 485 (2019) 230–247, https://doi.org/10.1016/j.ins.2018.11.057.
[48] Tyukin I.Y., Gorban A.N., Sofeikov K., Romanenko I, Knowledge transfer between artiﬁcial intelligence systems. Frontiers of Neurorobotics, 12, Article
49, 2018. https://doi.org/10.3389/fnbot.2018.00049..
[49] V. Vapnik, The Nature of Statistical Learning Theory, Springer-Verlag, 2000.
[50] R. Izmailov, V. Vapnik, Knowledge transfer in SVM and neural networks, Annals of Mathematics and Artiﬁcial Intelligence (2017) 1–17.
[51] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, D. Wierstra. Matching networks for one shot learning. In Advances in neural information processing
systems, pages 3630–3638, 2016..
[52] D. Wang, M. Li, Stochastic conﬁguration networks: Fundamentals and algorithms, IEEE Trans. Cybern. 47 (10) (2017) 3466–3479.
[53] Y. Wang, Q. Yao, J. Kwok, L.M. Ni, Generalizing from a few examples: A survey on few-shot learning, ACM Computing Surveys (CSUR) 53 (3) (2020) 1–
34.
[54] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530,
2016..
[55] J. Zhang, M. Harman, L. Ma, Y. Liu, Machine learning testing: Survey, landscapes and horizons, IEEE Trans. Software Eng. (2020).
[56] S. Zheng, Y. Song, T. Leung, I. Goodfellow, Improving the robustness of deep neural networks via stability training, in: Proc. of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016, https://arxiv.org/abs/1604.04326.
I.Y. Tyukin, A.N. Gorban, A.A. McEwan et al.
Information Sciences 564 (2021) 124–143
143

