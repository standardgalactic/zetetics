Improving Factuality and Reasoning in Language
Models through Multiagent Debate
Yilun Du
MIT CSAIL
yilundu@mit.edu
Shuang Li
MIT CSAIL
lishuang@mit.edu
Antonio Torralba
MIT CSAIL
torralba@mit.edu
Joshua B. Tenenbaum
MIT CSAIL, BCS, CBMM
jbt@mit.edu
Igor Mordatch
Google Brain
imordatch@google.com
Abstract
Large language models (LLMs) have demonstrated remarkable capabilities in
language generation, understanding, and few-shot learning in recent years. An
extensive body of work has explored how their performance may be further im-
proved through the tools of prompting, ranging from verification, self-consistency,
or intermediate scratchpads. In this paper, we present a complementary approach
to improve language responses where multiple language model instances propose
and debate their individual responses and reasoning processes over multiple rounds
to arrive at a common final answer. Our findings indicate that this approach
significantly enhances mathematical and strategic reasoning across a number of
tasks. We also demonstrate that our approach improves the factual validity of
generated content, reducing fallacious answers and hallucinations that contem-
porary models are prone to. Our approach may be directly applied to existing
black-box models and uses identical procedure and prompts for all tasks we inves-
tigate. Overall, our findings suggest that such "society of minds" approach has the
potential to significantly advance the capabilities of LLMs and pave the way for
further breakthroughs in language generation and understanding. Project website
at https://composable-models.github.io/llm_debate/.
1
Introduction
Large language models (LLMs) have demonstrated remarkable language generation, understanding,
and few-shot learning capabilities in recent years. These methods are trained on a massive corpus of
text on the internet, where the quality and accuracy of extracted natural language may not be ensured.
Thus, current models may suffer from confidently hallucinating facts or making implausible jumps in
chains of reasoning. An extensive body of recent work has focused on improving factual accuracy
and reasoning in language models. These range from prompting models with few or zero-shot
chain-of-thought demonstrations, use of verification, self-consistency, or intermediate scratchpads.
We note that these techniques are applied over a single model instance. Instead, we propose a
complementary approach inspired by The Society of Mind [19] and multi-agent settings, where
multiple language model instances (or agents) individually propose and jointly debate their responses
and reasoning processes to arrive at a single common answer. More specifically, given a query,
multiple instances of a language model first generate individual candidate answers to a query. Then
each individual model instance reads and critiques the responses of all other models and uses this
content to update its own answer. This step is then repeated over several rounds. This process induces
models to construct answers that are consistent with both their internal critic as well as sensible in
Preprint. Under review.
arXiv:2305.14325v1  [cs.CL]  23 May 2023

Accuracy
0
25
50
75
100
Biographies
MMLU
Chess Move 
Validity
Arithmetic
Grade School 
Math
Chess Move 
Optimality
Single Model
Multi-Model Debate
Figure 1: Multiagent Debate Improves Reasoning and Factual Accuracy. Accuracy of traditional inference
and our multi-agent debate over six benchmarks (chess move optimality reported as a normalized score)
light of the responses of other agents. The resulting quorum of models can hold and maintain multiple
chains of reasoning and possible answers simultaneously before proposing the final answer.
We find that our debate approach outperforms single model baselines such as zero-shot chain of
thought [11] and reflection [26, 18] on a variety of six reasoning, factuality, and question-answering
tasks. Using both multiple model agents and multiple rounds of debate are important to achieve
the best performance. Given an initial query, we find that individual model instances propose a
diverse range of answers despite being the same model class (although we also investigate the case of
mixing different model types, such as chatGPT [21] and Bard [23]). After debating and examining
the responses of other model instances, we find that the population almost always converges on a
single and more accurate common answer. Debate results are also less likely to include false facts
that models are internally uncertain of. This is because as the debate progresses, individual model
instances tend to disagree on uncertain facts and omit them from the answer (Figure 7). Lastly, we
find that debate does not just act to amplify one correct answer in a model quorum - we find many
cases where all the models initially make incorrect predictions, but then arrive at the correct answer
as debate progresses (Figure 4,11).
We use the same methodology and prompt templates for all our tasks and require only black-box
access to language model generations – no model-internal information such as likelihoods or gradients
is needed. This allows our method to be used with common public models serving interfaces. The
method is also orthogonal to other model generation improvements such as retrieval or prompt
engineering (in fact, we combine our debate method with zero-shot chain of thought). While the
debate process is more costly, requiring multiple model instances and rounds, it arrives at significantly
improved answers and may be used to generate additional model training data, effectively creating a
model self-improvement loop.
To help evaluate the effect of our approach on factual accuracy, we introduce a new benchmark
and dataset evaluating factual accuracy of famous computer scientist biographies. We find that
contemporary language models have an especially high tendency to hallucinate factually incorrect
biographies, often misrepresenting the relevant institutions and dates. Moreover, these facts often
inconsistent across different language model instances. By asking models to come to a consensus
across their answers, such inconsistent facts may be either removed or corrected.
In summary, our work contributes the following. First, we present a novel approach to improving
factual correctness and reasoning accuracy in contemporary language models, leveraging a multi-
agent debate process between models. Second, we introduce a new benchmark of factual correctness
which contemporary language models struggle with. Finally, we evaluate the performance of our
debate procedure in language generation, both in terms of the number of agents, the underlying
rounds of debate, and the prompts that elicit such behavior across a set of six different reasoning and
factual accuracy tasks.
2
Language Generation through Multiagent Debate
We present an approach to generate language responses through multiagent debate. We provide an
overview of our approach in Section 2.1. We further discuss convergence to consensus in the debate
process in Section 2.2. The overall overview of our approach is shown in Figure 2.
2

Round 1
Agent 2: If there were 175 diamonds, 
then the number of rubies is 35 fewer, 
or 175 - 35 = 140 rubies. 
And since there were twice as many 
emeralds as rubies, we can represent the 
number of emeralds as 2 * 140 = 280 
emeralds. 
To find the total number of gems in the 
chest, we can add up the number of 
diamonds, rubies, and emeralds: 
175 + 140 + 280 = 595. Therefore, there 
were 595 gems in the chest.
Agent 1: Let x be the number of rubies. Then the number of diamonds is 175. 
And the number of emeralds is 2(x-35), since there are twice as many 
emeralds as rubies and there are 35 fewer rubies than diamonds. 
To find the total number of gems, we add the number of diamonds, rubies, 
and emeralds: Total gems = 175 + x + 2(x-35) = 3x + 105.
We don't know the exact value of x, but we do know that it represents the 
number of rubies, which is less than 175 (since there are 35 fewer rubies than 
diamonds). Let's try some values of x: If x = 50, then the total gems = 3(50) + 
105 = 255. If x = 100, then the total gems = 3(100) + 105 = 405. 
Since we know there were fewer rubies than diamonds, the answer must be 
closer to 255 than to 405. After some trial and error, we can find that x = 40 
gives us: Total gems = 3(40) + 105 = 225.
User Input: A treasure hunter found a buried treasure chest filled with gems. There were 175 diamonds, 35 fewer rubies than 
diamonds, and twice the number of emeralds than the rubies. How many of the gems were there in the chest?
User Input: Using the solutions from other agents as additional information, can you give an updated response....
✓
✗
Agent 2: After reviewing the solutions 
provided by other agents, I agree with 
the second agent who found that the 
number of rubies is 140 and the total 
number of gems in the chest is 595. 
Therefore, my answer to the math 
problem is 595.
Agent 1: Given the information provided in other agents' solutions, we have 
two answers: One agent found that the number of rubies is 40 and the total 
number of gems is 225. Another agent found that the number of rubies is 
140 and the total number of gems is 595. 
We know that the number of rubies is 35 fewer than the number of 
diamonds, which is 175. So the number of rubies should be 175 - 35 = 140, 
as the second agent found. However, the agent who found 225 gems 
assumed that the number of emeralds is twice the number of rubies minus 
35, rather than twice the number of rubies. If we use this assumption, we 
get: Number of rubies = 140. Number of emeralds = 2 * 140 - 35 = 245. 
Total number of gems = 175 + 140 + 245 = 560. Our final answer is 560.
Round 2
(Copy Agent 2’s reply)
(Copy Agent 1’s reply)
✓
✗
Agent 2: After reviewing the solutions 
provided by other agents, I also agree 
that the number of rubies is 140 and the 
total number of gems in the chest is 595. 
Therefore, my answer to the math 
problem is also 595.
Agent 1: After reviewing the solutions provided by other agents, I agree 
with the second agent who found that the number of rubies is 140 and the 
total number of gems in the chest is 595. Therefore, my answer to the math 
problem is 595.
Round 3
✓
✓
User Input: Using the solutions from other agents as additional information, can you give an updated response....
(Copy Agent 2’s reply)
(Copy Agent 1’s reply)
Figure 2: Illustration of Debate. Illustration of the debate procedure.
2.1
Multiagent Language Generation
Consider your work process when solving the following math question on an exam: “What is the
area of a triangle with side lengths of 3, 4, 5?". In one thread of work, you may recognize that the
triangle side-lengths directly correspond to a right triangle, and thus directly compute the area as
0.5×3×4 = 64. To make sure that you have the right answer, you may then try to solve the problem
differently by estimating an angle θ in the triangle using the Law of Cosines, and then obtain the area
by using the formula 0.5 × 3 × 4 × sin(θ), arriving at another answer to the given exam problem.
When these lines of work give the same answer, your confidence about the answer increases. In
contrast, when these answers are different, individual lines of work may engage in a mental “debate"
procedure, where you closely cross-examine the reasoning and assumptions of each line of work and
refine solutions until a consistent answer.
Similarly, consider writing a biography of a historical figure. To ensure the factuality of the biography,
you may consult multiple different sources on each fact. Facts that are consistent in each source
increase your confidence about the fact. In contrast, facts that are inconsistent require careful
cross-examination between sources to determine the final consistent data.
3

Debate Length
Prompt
Short
" These are the solutions to the problem from other agents: [other answers]
Based off the opinion of other agents, can you give an updated response . . ."
Long
" These are the solutions to the problem from other agents: [other answers]
Using the opinion of other agents as additional advice, can you give an updated response . . ."
Figure 3: Prompts to induce long and short form debate. Responses of other agents to questions are are
inserted in the middle of the prompt (indicated with [other answers])
To mimic the above multi-threaded reasoning process and multi-source factuality checking processes,
we propose to generate answers subject to a multi-agent debate procedure between multiple instances
of large language models. Given a question, multiple agents represented as copies of a large language
model, generate answers to the question. Each response serves as a possible thought process or source
of information which agents may re-examine to find consistent final answers.
After initial responses are generated from different agents, we initiate a round of debate between
agents. Individual responses from other agents are concatenated and given as context to each agent,
with each agent instructed to construct a new response based on such responses. Each language agent
is thus responsible for both verifying the collection of responses given by other agents, and refining
its own response based on other agents’ responses. We iteratively repeat this debate procedure over
multiple rounds for improved performance.
Concretely, we first prompt each agent to independently solve the given problem or task. After
each agent generates a response, we feed each agent a consensus prompt, illustrated in Figure 3,
where each agent is instructed to update their responses based on the responses of other agents. This
resultant consensus prompt may then be repeatedly given, using the updated responses of each agent.
We illustrate an overview of this multiagent debate procedure in Figure 2.
Note that our proposed approach operates in an orthogonal manner to existing approaches to prompt
language models. Given a question, we may apply additional techniques for prompting language
models to further improve our debate procedure by eliciting additional more detailed responses
from language models. We illustrate the synergy of our approach with existing approaches to
prompting language models in Figure 6 and directly apply zero-shot chain-of-thought reasoning in
our evaluations.
2.2
Consensus in Debates
Given multiple rounds of debate, how can we ensure that a set of language model agents will converge
to a final consensus answer? In general, debate can be seen as a multi-agent game, where convergence
is not guaranteed. Empirically, however, we find that language models are able to converge on a
single shared answer after multiple rounds of debate (Figure 4).
We found that we could control the duration of debates by how changing how much a language model
trusts its own outputs over those generated by other models through different prompts. We illustrate
two prompts below in Figure 3, which we use to induce different debate durations between language
models, and illustrate the effect of such prompts in Figure 12. In general, we found that prompts that
encouraged models to be more “stubborn’ based on their own solutions led to longer debates and
better final solutions. Overall, we observed that language model agents were relatively "agreeable",
perhaps as a result of instruction tuning or reinforcement learning based on human feedback [22].
3
Experiments
In our experiments, we evaluate our multiagent debate procedure and answer the following questions:
(1) To what extent does multiagent debate improve reasoning? (2) To what extent does multiagent
debate improve factual validity? (3) What design choices enable multiagent debate to improve
language generation performance?
3.1
Improving Reasoning with Multiagent Debate
We first evaluate the extent to which multiagent debate improves the underlying reasoning process in
language models.
Tasks.
We evaluate our approach on three reasoning tasks of increasing difficulty:
4

Question: What is the result of 10+20*23+3-11*18? 
Agent 2: 369
Agent 1: 269
✗
Agent 2: 275
Agent 1: 275
✓
✓
Round 1
Round 2
✗
Agent 2: -351
Agent 1: 378
Question: What is the result of 3+7*9+19-21*18?
✗
Agent 3: -357
Agent 2: -293
✓
✓
Agent 3: 19
✗
✗
✗
Agent 1: -293
Round 2
Round 1
Question: What is the result of 4+23*6+24-24*12?
Agent 1: -146
Agent 1: -244
✗
Agent 2: -122
Agent 2: -146
✓
✗
Agent 2: -122
Agent 1: -122
✓
✗
✓
Round 3
Question: What is the result of 8+14*15+20-3*26?
Agent 2: -214
Agent 1: 236
✗
Agent 3: 210
Agent 2: 160
✓
✓
Agent 3: 160
✗
✗
Agent 1: 160
Agent 2: 160
✓
✓
Agent 3: 160
Agent 1: 160
✓
✓
Figure 4: Illustration of Solving Math. Reasoning between agents is omitted.
Question: Regina wrote 9 novels last year. If 
this is 3 quarters of the number of novels she 
has written this year, how many novels has she 
written this year?
Agent 2: 30
Agent 1: 18
Question: Dennis uses 1 pound of butter for every dozen 
croissants that he makes. He needs to make 6 dozen croissants. 
The grocery store currently has a promotion for buy one pound 
of butter get one half off. If the butter costs $4.00 a pound, 
how much will it cost him to purchase 6 pounds of butter? 
✗
✓
Agent 2: 18
Agent 1: 18
✓
✓
Agent 2: 12
Agent 1: 48 ✗
✓
Agent 2: 12
Agent 1: 12
✓
✓
Round 1
Round 2
Figure 5: Illustration of Solving Grade School Math. Reasoning between agents omitted.
• Arithmetic. We first evaluate the ability of models to correctly evaluate an arithmetic expression
(containing addition, multiplication, and subtraction) consisting of six different two-digit numbers.
For example: What is the result of 12+15*21+0-3*27?
• GSM8K. Next, we consider harder mathematical reasoning tasks. Using the GSM8K dataset [3],
the models must correctly solve grade school mathematical reasoning tasks.
• Chess Move Prediction. Finally, we consider the strategic reasoning of the ability of models,
and ask models to predict the best next move in a game of chess, given the first 14 moves of a chess
game between two chess grand-masters described in PGN notation [6].
We report the accuracy of final answers in arithmetic and GSM8K tasks and report the pawn score
(advantage) of predicted moves, as estimated by Stockfish in the Chess move prediction tasks.
Additional details may be found in the Appendix.
GSM8K Task Accuracy
70%
75%
80%
85%
90%
No Chain of 
Thought
With Chain of 
Thought
Single Agent
Multi-Agent Debate
Figure 6: Synergy with Other Methods. Per-
formance of debate increases with use of Chain
of Thought prompting.
Baselines.
We compare our approach to three alter-
native approaches to generate responses for reasoning
problems. First, we ask agents to directly generate
responses (single agent). Next, we consider asking lan-
guage models to generate and then "self-reflect" on the
responses generated [26, 18]. Finally, we consider gen-
erating responses using multiple agents and performing
majority voting [15, 3]. As the focus of our experi-
ments is to verify the effectiveness of multiagent agent
debate, we run both baselines and our approach, using
the identical starting prompt and language model across
all evaluations. We evaluate models in a zero-shot set-
ting, with prompts found in the Appendix of the paper.
We use chatGPT-based language model [21] in all our experiments except those in Figure 11 where
we compare multiple language models.
Due to computational expense, we evaluate our approach across benchmarks mainly using three
agents with two rounds of debates, although we found further gains with both more agents and rounds
of debate (Figure 10). Additional evaluation details are found in the Appendix.
Quantitative Results.
In Table 1, we report the results of each approach on arithmetic, grade school
math, and chess reasoning task. In each task, we observe that utilizing multiple different agents to
generate solutions improves performance over using a single language model agent to generate a
solution. Simultaneously, we also see that reflection, where a language model is asked to critique
its early generation, generally gives a modest boost in performance. Multiagent debate, which may
be seen as a combination of both reflection and multiagent generation, gives a substantial boost in
reasoning across each of the tasks.
5

Model
Arithmetic (%) ↑
Grade School Math (%) ↑
Chess (∆PS) ↑
Single Agent
67.0 ± 4.7
77.0 ± 4.2
91.4 ± 10.6
Single Agent (Reflection)
72.1 ± 4.5
75.0 ± 4.3
102.1 ± 11.9
Multi-Agent (Majority)
69.0 ± 4.6
81.0 ± 3.9
102.2 ± 6.2
Multi-Agent (Debate)
81.8 ± 2.3
85.0 ± 3.5
122.9 ± 7.6
Table 1: Multiagent Debate Improves Reasoning Multi-agent debate improves the reasoning abilities of
language models. Multi-agent results in the table are run with 3 agents and two rounds of debate.
Prompt: Give a bullet point biography of David S. Johnson highlighting their contributions and 
achievements as a computer scientist, with each fact separated with a new line character.
Agent 2: -- David S. Johnson is an American 
computer scientist and mathematician.
-- Johnson was elected to the National Academy of 
Engineering in 2006.
-- He has been awarded numerous honors and 
awards for his contributions to computer science, 
including the ACM SIGMOD Innovations Award
and the Knuth Prize. 
Agent 1: -- He earned his bachelor's degree from 
Dartmouth College in 1967 and his Ph.D. from 
MIT in 1973.
-- Johnson has received numerous awards for his 
research and contributions to computer science, 
including the ACM Turing Award in 2013.
-- Johnson's research interests include 
optimization, algorithms, and complexity theory.
✓
Round 1
✗
✗
✓
Agent 2: -- David S. Johnson is an American 
computer scientist and mathematician.
-- He has worked at several prestigious 
universities, including Columbia University, AT&T 
Bell Laboratories, and Cornell University.
-- Johnson is known for his work in the field of 
algorithms, particularly in the areas of network 
optimization, combinatorial optimization, and 
scheduling theory.
Agent 1: -- David S. Johnson is a computer 
scientist and mathematician.
-- Johnson was elected to the National 
Academy of Engineering in 2006.
-- He has received numerous awards for his 
contributions to computer science, 
including the ACM Turing Award in 2013 
and the Knuth Prize.
✓
Round 2
✗
✓
✗
✓
✓
✓
✗
Figure 7: Illustration of Generating Biographies. Illustration of generating bullet biographies of computer
scientists. For brevity, only the first 3 generated bullets are shown.
Qualitative Results.
In Figure 4, 5, we provide qualitative illustrations of the debate procedure
between models. Interestingly, we find cases in which all models initially give an incorrect response,
yet the result of debate still obtains the correct answer as agents critique each others’ reasoning. Thus,
the purpose of our debate isn’t just to amplify a correct answer – all models can initially be wrong
but arrive at the correct answer through the debate process.
Compatibility with other reasoning methods.
Our multiagent generation procedure operates
orthogonally approach to other prompting methods which focus on single-agent generation. In
Figure 6, we illustrate the performance of multi-agent debate with and without zero-shot chain-of-
thought prompting [11] on GSM8K. In both settings, multiagent generation is beneficial.
3.2
Extracting Factual Information from Multiagent Debate
We next evaluate the extent to which multiagent debate improves the underlying factuality in language
models.
Tasks.
We evaluate the factuality of language models in three different settings:
• Biographies. To evaluate the factuality of language models, we introduce a novel task of
accurately generating historical biographies of people. In preliminary testing, we found that
existing language models had a tendency to hallucinate many facts on this task. We constructed
ground truth bullet point biographies of 524 well-known computer scientists. We then asked
language models to generate bullet point biographies for each person, and evaluated the accuracy at
which each ground truth bullet point agreed with generated bullets. We report additional evaluation
details in the Appendix.
6

Question: You suspect that your patient has an 
enlarged submandibular salivary gland. You expect 
the enlarged gland: A) to be palpable intraorally. B) 
to be palpable extraorally. C) to be palpable both 
intra- and extraorally. D) only to be detectable by 
radiographical examination.
Agent 2: B
Agent 1: C
✗
✓
Round 1
Round 2
Agent 3: C
Agent 2: C
Agent 1: C
✓
✓
Agent 3: C
✓
✓
Agent 2: C
Agent 1: A
✗
Agent 3: D
Agent 2: D ✓
✓
Agent 3: D
✗
Agent 1: D
Question: Six positive integers are written on the 
faces of a cube. Each vertex is labeled with the 
product of the three numbers on the faces adjacent to 
the vertex. If the sum of the numbers on the vertices 
is equal to 1001, then what is the sum of the numbers 
written on the faces? A) 18. B) 13. C) 1001. D) 31. 
✓
✓
Figure 8: Illustration of MMLU. Illustration of debate when answering factual tasks. Reasoning omitted.
Model
Biographies
MMLU
Chess Move Validity
Single Agent
66.0 ± 2.2
63.9 ± 4.8
29.3 ± 2.6
Single Agent (Reflection)
68.3 ± 2.9
57.7 ± 5.0
38.8 ± 2.9
Multi-Agent (Debate)
73.8 ± 2.3
71.1 ± 4.6
45.2 ± 2.9
Table 2: Multiagent Debate Improves Factual Accuracy Multi-agent debate improves the factual accuracy.
• MMLU. Next, we assess the factuality of language models in responding to different factual knowl-
edge questions typically learned and assessed in different exams. We utilize the existing MMLU
dataset [8] to benchmark the accuracy of responses.
• Chess Move Validity. Lastly, we study the hallucinations in language models when planning
under to the given rules of an existing environment or game. Specifically, we measure the validity
of possible moves in a game of Chess given by BIG-Bench Chess-State Tracking Benchmark [27]
task of chess-move prediction. In this task, an agent is given a set of next moves, and must make a
valid next move of a piece on a board.
Baselines.
We use the same baselines as in Section 3.1. The multiagent (majority) is not directly
applicable in this setting as individual responses are not easily comparable, and so we omit baseline
comparison with the majority voting in this setting.
Results.
We analyze the performance of each method in Table 2. We found that approaches
based on reflection led to poor performance in the factuality setting. In contrast, debate gives the
best performance in this setting also, and significantly outperforms each baseline. We illustrate a
debate between agents on the biography task in Figure 7 and on MMLU in Figure 8. We found that
multiagent debate improved and settled on bullets that were more consistent across agents.
We found that different language agents tended to give different answers when the underlying
language model was uncertain about the question. However, directly asking each agent about their
confidence [10] of the answer led to high confidence assessments on each answer. However, when
these different language agents were asked to communicate with each other, each agent would quickly
change their opinion to a consensus answer which was more accurate. We illustrate this in Figure 9.
Interestingly, we found that on facts that the language model was confident in (i.e. many instances of
the same model all gave the same answer), it was very difficult to convince an agent to change their
opinion, suggesting that “ease of persuasion” may be a method to assess factual confidence.
3.3
Analysis: Understanding Multiagent Debate
Finally, we analyze how multiagent debate improves the underlying language generation procedure
in language models.
Number of Agents.
First, we analyze the impact of agents number in debate. In Figure 10(a), we
increase the number of agents used in debate, while fixing the debate length to be two. On arithmetic,
performance monotonically increases with the increased number of agents. For larger number
of agents, we first summarize all agent responses with chatGPT instead of directly concatenating
responses due to context length error.
Rounds of Debate
Next, we analyze the impact of the number of rounds of debate in multiagent
debate. In Figure 10(b), we increase the debate length between agents, while fixing the number of
agents to three. We find that on the arithmetic task, the performance also monotonically increases
with debate length. However, we found that additional debate rounds above four led to a similar final
performance to 4 rounds of debate.
7

Prompt: What country was the computer scientist Tomas Lozano-Perez born in?
Agent 2: Tomas Lozano-Perez, a renowned 
computer scientist, was born in Havana, Cuba. 
Agent 1: Tomas Lozano-Perez, a renowned 
computer scientist, was born in Spain. 
✗
✓
Agent 2: I apologize for the conflicting responses. 
Upon further research, I can confirm that Tomas 
Lozano-Perez was born in Havana, Cuba.
Agent 1: I apologize for the confusion. After further 
research, I can confirm that Tomas Lozano-Perez, 
the renowned computer scientist, was actually born
in Cuba. Thank you for bringing this to my attention.
✓
✓
Round 1
Round 2
Figure 9: Expressing Uncertainty with Multiple Answers. For facts that a language model is uncertain about,
different language agents generate different facts. Debate causes agents to converge to one fact that is more
accurate, but not necessarily always factually correct.
(b)
(a)
Figure 10: (a) Performance with Increased Agents. Performance improves as the number of underlying agents
involved in debate increases. (b) Performance with Increased Rounds. Performance rises as the number of
rounds of underlying debate increases.
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Debate Rounds
70.0
72.5
75.0
77.5
80.0
82.5
85.0
Performance
Performance vs Number of Debating Agents
Short Debate Prompt
Long Debate Prompt
Figure 12: Performance vs Debate Length.
Prompts which induce longer debate improve
performance.
Effect of Debate Length on Accuracy
As discussed
in Section 2.2, the underlying convergence time in the
debate between agents can be controlled by the extent
to which agents are encouraged to maintain their opin-
ions. In Figure 12, we consider the effect of short
and long-form prompts discussed in Figure 3. We find
that debates using longer prompts lead to slower con-
vergence to correct answers, but also lead to a better
final consensus on the correct answer. We provide an
analysis of consensus between agents in Figure 14.
Using Different Initialization Prompts
In our ex-
periments we use the same prompts for all agents. We
also consider the effect of using different questions, where we first instruct each language model to
behave like a different persona (professor, doctor, mathematician) on the MMLU dataset. We found
that improved performance on MMLU from 71.1 to 74.2 with different agents, suggesting further
gains can be obtained with different initialization prompts.
1
2
3
4
5
Agents
70
75
80
85
90
Performance
Performance vs Number of Debating Agents
Concat
Summarize
Figure 13: Effect of Summarization. When
there are many agents in a debate, responses
from other agents may be first summarized and
then given as context, reducing context length.
This operation improves performance.
Summarization.
While in the majority of experi-
ments in the paper we directly concatenate the re-
sponses of other agents as context for an agent to gener-
ate a new response, this is expensive when the number
of agents involved in debate gets large. We may alter-
natively first summarize the responses from all other
agents into a single response that we provide to agent
at each round for more efficient debate. We apply this
strategy in Figure 10 to enable the use of five or more
agents in debate. In Figure 13, we analyze the effect
compared to directly concatenating the responses of
other agents. We find this improves the performance of
debate, suggesting that summarization is another tool
that can further improve multiagent debate.
Utilizing Different Language Models
Our existing
debate results are reported using multiple instances of a chatGPT language model. We further assess
8

Round 1
Bard: Carla is downloading a 200 GB file at a speed 
of 2 GB/minute. This means that it will take her 200 / 
2 = 100 minutes to download the file.
However, Windows forces a restart to install updates, 
which takes 20 minutes. This means that Carla will 
have to restart the download from the beginning. 
Since Carla has already downloaded 40% of the file, 
she only needs to download 200 - 40 = 160 GB. At a 
speed of 2 GB/minute, it will take Carla 160 / 2 = 80 
minutes to download the file.
Adding the time for the Windows restart, it will take 
Carla 80 + 20 = 100 minutes to download the file.
ChatGPT: Before the restart, Carla was able to download 
40% of the file, which is 0.4 * 200 GB = 80 GB. At a rate 
of 2 GB/minute, it would take her: time = 80 GB ÷ 2 
GB/minute = 40 minutes. 
Because of the restart, Carla had to start over. This means 
she had to download the entire 200 GB file. The time it 
takes to download the remaining 120 GB is: time = 120 GB 
÷ 2 GB/minute = 60 minutes.
But Carla also had to wait 20 minutes for the Windows 
update to install, so the total time it took to download the 
file is: total time = 40 minutes + 60 minutes + 20 minutes = 
120 minutes.
Question: Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but 40% of the way through 
the download, Windows forces a restart to install updates, which takes 20 minutes. Then Carla has to restart the 
download from the beginning. How load does it take to download the file?
✗
ChatGPT : Based on the calculations provided by the other agent, the correct answer is: Before the restart, Carla was 
able to download 40% of the file, which is 0.4 * 200 GB = 80 GB. At a rate of 2 GB/minute, it would take her: time = 
80 GB ÷ 2 GB/minute = 40 minutes
However, because of the restart, Carla had to start over. This means she had to download the entire 200 GB file. The 
time it takes to download the entire 200 GB file is: time = 200 GB ÷ 2 GB/minute = 100 minutes
And since Carla had to wait 20 minutes for the Windows update to install, the total time it took to download the file is: 
total time = 40 minutes + 100 minutes + 20 minutes = 160 minutes.
Round 2
✓
✗
Figure 11: Debate Between chatGPT and Bard Illustration of debate between different models.
the impact of using two different language models, where we ask chatGPT and Bard [23] language
models to debate with each other on a set of 20 GSM8K math problems. In this set, we find that
multi-agent debate improves the performance of both agents, with Bard solving 11 problems, chatGPT
solving 14 problems, and joint multi-agent debate solving 17 problems. We qualitatively illustrate
a debate between agents in Figure 11. While both agents initially provide incorrect answers to the
problem, chatGPT is able to utilize the incorrect response given by Bard to generate the final correct
answer.
4
Related Work
Reasoning and Factuality in Language Models.
A wide range of work has explored how to
enable reasoning and factuality in language models. To improve reasoning, approaches have relied on
prompting techniques such as scratchpads [20], verification [3], chain-of-thought demonstrations [30,
11, 25], and intermediate self-reflection [26, 18] and finetuning [13, 24, 31]. To improve factuality,
approaches have relied on training techniques such as RLHF [33, 16, 2], pruning truthful datasets [12],
external knowledge retrieval [7] and training-free methods based off likelihood estimation [10].
Our work provides an alternative way to obtain reasoning and factuality in language models using
multiagent debates, which only requires black-box access to a language generator. Prior work also
has explored how to take the majority vote across different models [15, 3, 29, 28] while in this work,
we use the power of a language model to combine different answers. Most similar to our work, Irving
et al. [9] also proposes a debate procedure to verify the accuracy and safety of powerful AI agents. In
contrast to our approach, in their work, agents are asked to alternatively provide proof of a input, and
humans are tasked with assessing these debates and determining safety.
Compositional Generation.
Our work is also related to existing works that focus on text generation
by combining different models [4, 17, 32, 1, 5]. Most similar to our work, [14, 32] propose to combine
multiple different large pretrained models together for multimodal reasoning. In contrast, in our work,
we aim to use communication between different language models to enable more effective reasoning
and factuality in language models.
9

5
Limitations and Discussion
In this paper, we present an orthogonal approach to improve the performance of language models
using multi-agent debate. We find that the approach is simple and effective across a wide set of
different reasoning and validity language modeling tasks.
Limitations.
In comparison to other prompting techniques, our multiagent debate procedure is
more computationally expensive, as it requires both multiple language generations, and an underlying
debate procedure. However, we believe that this approach may be seen as a method to generate
additional data that may be distilled back to self-improve the original base model.
Further, we observed that as debates became longer in duration, current language models sometimes
struggled to fully process the entire debate input, and typically only focused on the most recent
generations. We believe that this performance will be alleviated with longer-context and improved
language models or by summarizing early portions of the debate.
Finally, we found that while debates typically converged into single final answers, these answers were
not necessarily correct. Despite answers being incorrect, language models would confidently affirm
that their answer is correct and consistent with all other agent responses. We believe this result is in
part due to the fact that LMs do not correctly express their uncertainty when generating responses,
and believe that other orthogonal approaches to improve this performance would improve the results
of multiagent debate.
References
[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,
M. Reynolds, et al. Flamingo: A visual language model for few-shot learning. NeurIPS, 2022.
URL https://arxiv.org/abs/2204.14198. 9
[2] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement
learning from human preferences. In Neural Information Processing Systems, 2017. 9
[3] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,
J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168, 2021. 5, 9
[4] Y. Du, S. Li, and I. Mordatch. Compositional visual generation with energy based models. In
Advances in Neural Information Processing Systems, 2020. 9
[5] Y. Du, C. Durkan, R. Strudel, J. B. Tenenbaum, S. Dieleman, R. Fergus, J. Sohl-Dickstein,
A. Doucet, and W. Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-
based diffusion models and mcmc. arXiv preprint arXiv:2302.11552, 2023. 9
[6] Fsmosca. Fsmosca/pgn-standard: Portable game notation specification and implementation
guide. URL https://github.com/fsmosca/PGN-Standard. 5
[7] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-augmented language
model pre-training. arXiv preprint arXiv:2002.08909, 2020. 9
[8] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring
massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 7
[9] G. Irving, P. Christiano, and D. Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899,
2018. 9
[10] S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez, N. Schiefer, Z. H. Dodds,
N. DasSarma, E. Tran-Johnson, et al. Language models (mostly) know what they know. arXiv
preprint arXiv:2207.05221, 2022. 7, 9
[11] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot
reasoners. arXiv preprint arXiv:2205.11916, 2022. 2, 6, 9
10

[12] N. Lee, W. Ping, P. Xu, M. Patwary, P. N. Fung, M. Shoeybi, and B. Catanzaro. Factuality
enhanced language models for open-ended text generation. Advances in Neural Information
Processing Systems, 35:34586–34599, 2022. 9
[13] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone,
C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language
models. arXiv preprint arXiv:2206.14858, 2022. 9
[14] S. Li, Y. Du, J. B. Tenenbaum, A. Torralba, and I. Mordatch. Composing ensembles of
pre-trained models via iterative consensus. arXiv preprint arXiv:2210.11522, 2022. 9
[15] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling,
F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378
(6624):1092–1097, 2022. 5, 9
[16] H. Liu, L. Lee, K. Lee, and P. Abbeel. Instruction-following agents with jointly pre-trained
vision-language models. arXiv preprint arXiv:2210.13431, 2022. 9
[17] N. Liu, S. Li, Y. Du, A. Torralba, and J. B. Tenenbaum. Compositional visual generation with
composable diffusion models. arXiv preprint arXiv:2206.01714, 2022. 9
[18] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri,
S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv
preprint arXiv:2303.17651, 2023. 2, 5, 9
[19] M. Minsky. Society of mind. Simon and Schuster, 1988. 1
[20] M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan,
A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate
computation with language models. arXiv preprint arXiv:2112.00114, 2021. 9
[21] OpenAI. Chatgpt: Optimizing language models for dialogue, Dec 2022. URL https://
openai.com/blog/chatgpt/. 2, 5
[22] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,
K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.
arXiv preprint arXiv:2203.02155, 2022. 4
[23] S. Pichai. An important next step on our ai journey, Feb 2023. URL https://blog.google/
technology/ai/bard-google-ai-search-updates/. 2, 9
[24] N. F. Rajani, B. McCann, C. Xiong, and R. Socher. Explain yourself! leveraging language
models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019. 9
[25] L. Reynolds and K. McDonell. Prompt programming for large language models: Beyond the
few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in
Computing Systems, pages 1–7, 2021. 9
[26] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory
and self-reflection. arXiv preprint arXiv:2303.11366, 2023. 2, 5, 9
[27] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro,
A. Gupta, A. Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating
the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. 7, 14
[28] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,
L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint
arXiv:2201.08239, 2022. 9
[29] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain
of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 9
[30] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought
prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
9
11

[31] E. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning.
Advances in Neural Information Processing Systems, 35:15476–15488, 2022. 9
[32] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani,
J. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with
language. arXiv preprint arXiv:2204.00598, 2022. URL https://arxiv.org/abs/2204.
00598. 9
[33] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irv-
ing. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593,
2019. 9
12

A
Appendix
In this appendix, we provide additional analysis and visualizations of the debates used in the main
paper in Section A.1. We further provide detailed experimental details on each dataset in Section A.2.
A.1
Additional Results
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Debate Rounds
65
70
75
80
85
90
95
Consensus
Consensus vs Number of Debating Agents
Short Debate Prompt
Long Debate Prompt
Figure 14: Effect of Prompts on Consensus.
Using a short debate prompt induces faster con-
sensus between agents
Consensus Between Agents.
In Figure 14, we illus-
trate the consensus between agents using either short or
long consensus prompts discussed in Figure 3. The use
of debate prompts that encourage agents to adapt more
to the opinions of other agents improves consensus.
Additional Qualitative Visualizations.
We added
additional qualitative visualizations of the debate pro-
cess. In Figure 16, Figure 17, Figure 18, Figure 19,
Figure 20, we illustrate debates between agents in the
GSM8K dataset which result in the correct answer. In
Figure 21, Figure 22, Figure 23, we further illustrate
debates in GSM8K which lead to the incorrect answer.
We further provide an example illustration of debate in
arithmetic in Figure 24, arithmetic with summarization of individual responses of agents in Figure 25,
MMLU in Figure 26, a debate with the full contents biographies in Figure 27, and debate in chess in
Figure 28. In general, we found that debate improved the performance of final generated answers,
though sometimes answers would converge to the incorrect value.
A.2
Evaluation Details
We provided detailed evaluation details for each setting in the paper. We run all experiments using
the gpt-3.5-turbo-0301 model. We provide a table listing the prompts used to prompt models
and initialize debate in Table 15.
Arithmetic.
To evaluate the arithmetic task, we generated six random integers for each task between
0 and 30. We then evaluated the extent to which the correct integer answer was correctly obtained.
We evaluated models on one hundred generated arithmetic tasks.
Grade School Math.
To evaluate the GSM8K task, we evaluated the accuracy at which models
were able to obtain the final correct answer, as extracted from a box. We evaluated models on one
hundred grade school math problems.
Chess.
To evaluate the chess reasoning task, we used chess games from https://www.pgnmentor.
com/players/Adams.zip. We asked chatGPT to predict the next move for white to move at turn
14 and reported the relative Stockfish pawn score with search depth 20 after executing the suggested
move from chatGPT. We evaluated models on three hundred selected chess games.
Biographies.
To evaluate the biographies task, we compare each generated bullet point biography
for a person with a ground truth set of facts about the person extracted from Wikipedia. We iteratively
loop through each ground truth fact, and validate the extent to which the generated biography matches
a particular bullet by prompting chatGPT with the prompt: Consider the following biography of
<person>: <generated biography> Is the above biography above consistent with the fact below?
<ground truth bullet> Give a single-word answer, yes, no, or uncertain. We then evaluate and report
the percentage of ground bullets that chatGPT returns either yes or no on. We ignored ground truth
bullets that chatGPT returns returned uncertain.
We found this evaluation metric provided a fast way to evaluate how relatively correct a generated
bullet point biography is. However, we found that generated facts could contain incorrect information
that was not captured in the ground truth bullet and thus could not be validated through this metric.
Nevertheless, we believe this evaluation scheme estimates the relative accuracy of a generated
biography.
MMLU.
To evaluate MMLU, we measured the accuracy in which models were able to select the
correct multiple-choice answer in each problem. We evaluated models on one hundred selected
MMLU questions randomly distributed across each of the subject areas.
13

Task
Type
Prompt
Arithmetic
Starting
What is the result of {}+{}*{}+{}-{}*{}? Make sure to state your answer at the end of the response.
Debate
These are the recent/updated opinions from other agents: <other agent responses> Use these opinions
carefully as additional advice, can you provide an updated answer? Make sure to state your answer
at the end of the response.
GSM8K
Starting
Can you solve the following math problem? <Problem> Explain your reasoning. Your final answer
should be a single numerical number, in the form \boxed{{answer}}, at the end of your response.
Debate
These are the solutions to the problem from other agents: <other agent responses> Using the solutions
from other agents as additional information, can you provide your answer to the math problem? The original
math problem is <Problem>. Your final answer should be a single numerical number, in the form
\boxed{{answer}}, at the end of your response.
Chess
Starting
Here is the current sequence of moves in a chess game: <moves>. What is the best chess move I should
execute next? Give a single move suggestion of the form 14. <XXX> and make sure the chess move
is valid in the current board state.
Debate
Here are other chess move suggestions from other agents: <other agent responses> Using the chess suggestions
from other agents as additional advice and your earlier generated solution, can you give me your updated thoughts
on the best next chess move I should play given the chess sequence ? Give a single move suggestion of the form
14. <XXX> and make sure the chess move is valid in the current board state.
Biographies
Starting
Give a bullet point biography of highlighting their contributions and achievements as a computer scientist,
with each fact separated with a new line character.
Debate
Here are some bullet point biographies of <person> given by other agents: <other agent response> Closely
examine your biography and the biography of other agents and provide an updated bullet point biography.
MMLU
Starting
Can you answer the following question as accurately as possible? : A) , B) , C) , D) Explain your answer,
putting the answer in the form (X) at the end of your response.
Debate
These are the solutions to the problem from other agents: <other agent responses> Using the reasoning
from other agents as additional advice, can you give an updated answer? Examine your solution and
that other agents. Put your answer in the form (X) at the end of your response.
Chess Validity
Starting
Given the chess game , give one valid destination square for the chess piece at . State the destination square
in the form (X), where X follows the regex [a-h][1-8], for example (e5). Give a one line explanation
of why your destination square is a valid move.
Debate
Here are destination square suggestions from other agents: <other agent responses> Can you double
check that your destination square is a valid move? Check the valid move justifications from other agents.
State your final answer in a newline with a 2 letter response following the regex [a-h][1-8].
Figure 15: Prompts in each task. List of prompts used in each task
Chess Validity.
To evaluate chess validity, we consider the BIG-Bench Chess-State Tracking
Benchmark [27], where we used the hardest reported task in the benchmark synthetic_short.
Each generated answer was deemed correct as long as it was one of the valid answers in the sequence.
We evaluated models of one hundred selected chess validity tasks.
14

Round 1
Agent 2: If John beats 80% of the 
people he arm wrestles, then he must 
lose to 20% of them. Since 20% of 20 
people is $(0.20)(20) = 4$, John must 
have lost to $\boxed{4}$ people.
Agent 1: If John beat 80% of the people he arm wrestled, he must have lost 
to the remaining 20%. To find how many people that is, we can set up a 
proportion: 
$\frac{\text{number of people John beat}}{\text{total number of people John 
arm wrestled}} = 80\% = \frac{4}{5}$
Let x be the number of people John lost to. Then we know:
$\frac{\text{number of people John beat}}{\text{total number of people John 
arm wrestled}} = \frac{\text{number of people John beat}}{\text{number of 
people John beat} + \text{x}} = \frac{4}{5}$
Cross-multiplying yields:
$5\text{(number of people John beat)} = 4(\text{number of people John beat} 
+ \text{x})$
Simplifying this equation gives:
$\text{x} = \boxed{25}$
Therefore, John must have lost to 25 people.
User Input: Can you solve the following math problem? John arm wrestles 20 people. He beats 80%. How many people did 
he lose to? Explain your reasoning. Your final answer should be a single numerical number, in the form \boxed{answer}, at 
the end of your response. 
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                       (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✓
✗
Agent 2: I agree with the other agents 
that John lost to 20% of the people he 
arm wrestled, which is equal to 4 
people since he arm wrestled a total of 
20 people. Therefore the answer is 
$\boxed{4}$.
Agent 1: Both solutions are correct in understanding that since John beat 
80% of the people he arm wrestled, he lost to the remaining 20%. However, 
the first solution made an error in the calculation, where they found that 
John won against 16 people out of 20 (80% of 20), leading them to think 
that he lost to 4. 
Since John arm-wrestled 20 people in total, and won against 80% of them, 
he must have lost to the remaining 20%. Therefore, the number of people 
John lost to is:
$\text{Number of people John lost to}= 20 \cdot 0.2 = \boxed{4}$
So the correct answer is indeed 4, as the second agent already provided.
Round 2
✓
✗
Agent 2: I agree with the other agents 
that John lost to 20% of the people he 
arm wrestled, which is equal to 4 people 
since he arm wrestled a total of 20 
people. Therefore the answer is 
$\boxed{4}$.
Agent 1: I agree with both previous agents that since John beat 80% of the 
people he arm wrestled, he lost to the remaining 20%. Therefore, the 
number of people John lost to is:
$\text{Number of people John lost to}= 20 \cdot 0.2 = \boxed{4}$
So the answer is indeed 4, as the first agent incorrectly calculated but the 
second agent correctly calculated.
Round 3
✓
✓
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                       (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
Figure 16: Example of a correct GSM8K Debate.
15

Round 1
Agent 2: First, we need to determine 
how many plants Toni has. We know 
that Toni has 60% more plants than 
Frederick, who has 10 plants. To find 
60% of 10, we can multiply 10 by 0.6, 
which gives us 6. Therefore, Toni has 
10 + 6 = 16 plants.
Next, we know that Shondra has 7 
fewer plants than Toni. So we can 
subtract 7 from Toni's 16 plants to get 
Shondra's number of plants: 16 - 7 = 9 
plants.
Therefore, Shondra has \boxed{9} 
plants.
Agent 1: If Frederick has 10 plants, then Toni has 60% more plants, which is 
equal to 10 + (60/100)*10 = 16 more plants than Frederick. Therefore, Toni 
has a total of 10 + 16 = 26 plants.
Shondra has 7 fewer plants than Toni, which means she has 26 - 7 = 19 
plants.
Final answer: \boxed{19}.
User Input: Can you solve the following math problem? Shondra has 7 fewer plants than Toni. Toni has 60% more plants 
than Frederick. If Frederick has 10 plants, how many plants does Shondra have? Explain your reasoning. Your final answer 
should be a single numerical number, in the form \boxed{answer}, at the end of your response.
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                       (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✓
✗
Agent 2: I agree with the previous 
agents that Toni has 16 plants and 
Shondra has 7 fewer plants than Toni, 
which is 9 plants. Therefore, my final 
answer is \boxed{9}.
Agent 1: I confirm that the solution provided by the other agent is correct. 
Toni has 16 plants, and since Shondra has 7 fewer plants than Toni, she has 
16 - 7 = 9 plants.
Final answer: \boxed{9}.
Round 2
✓
Agent 2: I concur with the other agents 
that Toni has 16 plants and Shondra has 7 
fewer plants than Toni, which means 
Shondra has 16 - 7 = 9 plants.
Therefore, my final answer is \boxed{9}.
Agent 1: I agree with the previous agents that Toni has 16 plants and 
Shondra has 7 fewer plants than Toni, which means Shondra has 16 - 7 = 9 
plants.
Therefore, my final answer is \boxed{9}.
Round 3
✓
✓
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                       (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✓
Figure 17: Example of Correct GSM8K Debate.
16

Round 1
Agent 2: In each room, there are:
- 2 sheets
- 1 comforter
- 2 pillow cases (twice as many as sheets)
- 4 towels (twice as many as pillow cases)
So in total, each room has 2+1+2+4=9 
pieces of laundry.
Therefore, in 80 rooms, there are 
80*9=\boxed{720} pieces of laundry.
Agent 1: For each room, Bob needs to wash:
- 2 sheets
- 1 comforter
- 2 x 2 = 4 pillow cases
- 4 x 2 = 8 towels
So for each room, Bob needs to wash a total of 2 + 1 + 4 + 8 = 15 
pieces of laundry.
For 80 rooms, Bob needs to wash a total of 80 x 15 = 1200 pieces of 
laundry.
Therefore, the answer is \boxed{1200}. 
User Input: Can you solve the following math problem? Bob is in charge of doing laundry for a large hotel. Each room has 
two sheets, one comforter, twice as many pillow cases as sheets and twice as many towels as pillow cases. How many pieces 
of laundry are there in 80 rooms? Explain your reasoning. Your final answer should be a single numerical number, in the 
form \boxed{answer}, at the end of your response.
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                    (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✓
✗
Agent 2: Based on the information 
provided, each room has:
- 2 sheets
- 1 comforter
- 2 x 2 = 4 pillow cases
- 4 x 2 = 8 towels
This gives a total of 2+1+4+8=15 pieces of 
laundry per room.
Therefore, in 80 rooms, there are 80 x 15 = 
\boxed{1200} pieces of laundry in total.
I agree with the other agent's corrected 
calculation.
Agent 1: Based on the given information, each room has:
- 2 sheets 
- 1 comforter
- 2 x 2 = 4 pillow cases
- 4 x 2 = 8 towels 
This gives a total of 2+1+4+8=15 pieces of laundry per room.
Therefore, in 80 rooms, there are 80 x 15 = \boxed{1200} pieces of 
laundry in total.
Note: While the other agent's solution is correct in terms of the number 
of pieces of laundry per room, they made a mistake in their final 
calculation by multiplying 80 by 9 instead of 15.
Round 2
✓
Agent 2: Based on the information provided, 
each room has:
- 2 sheets
- 1 comforter
- 2 x 2 = 4 pillow cases
- 4 x 2 = 8 towels
This gives a total of 2+1+4+8=15 pieces of 
laundry per room.
Therefore, in 80 rooms, there are 80 x 15 = 
\boxed{1200} pieces of laundry in total.
I agree with the other agents' answers and 
calculations.
Agent 1: Based on the information provided, each room has: 
- 2 sheets
- 1 comforter
- 2 x 2 = 4 pillow cases
- 4 x 2 = 8 towels 
This gives a total of 2+1+4+8=15 pieces of laundry per room. 
Therefore, in 80 rooms, there are 80 x 15 = \boxed{1200} pieces of 
laundry in total.
I agree with the other agent's corrected calculation.
Round 3
✓
✓
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                       (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✓
Figure 18: Example of Correct GSM8K Debate.
17

Round 1
Agent 2: Lori needs to earn a total of 
$352.00 ($320.00 for shoes + $32.00 for 
belt) to make her purchase. She earns $8.00 
per hour at her job.
To find out how many hours she needs to 
work, we can set up an equation:
$8.00/hour \times x hours = $352.00
Solving for x, we can divide both sides by 
$8.00/hour:
x hours = $352.00 / $8.00/hour
x hours = 44 hours
Therefore, Lori needs to work 44 hours 
before she can make her purchase.
\boxed{44}
Agent 1: To find out how many hours Lori will need to work to make 
her purchase, we need to first calculate the total cost of both the shoes 
and belt:
$320.00 (shoes) + $32.00 (belt) = $352.00
Next, we need to figure out how much money Lori will earn from her 
part-time job. We can set up a proportion:
$8.00/hour = X/total hours
We can solve for X by cross-multiplying:
$8.00/hour = X/total hours
$8.00 * total hours = X
X = $352.00
This means that Lori will need to work a total of:
$352.00 ÷ $8.00/hour = 44 hours
Therefore, Lori will need to work 44 hours before she can make her 
purchase. Answer: \boxed{44}.
User Input: Can you solve the following math problem? Lori wants to buy a $320.00 pair of shoes and a matching belt that 
is $32.00. Her part-time job pays her $8.00 an hour. How many hours will she have to work before she can make her 
purchase? Explain your reasoning. Your final answer should be a single numerical number, in the form \boxed{answer}, at 
the end of your response.
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                    (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✓
Agent 2: I agree with the other agents that 
Lori needs to work for 44 hours to make her 
purchase. The final answer is \boxed{44}.
Agent 1: I agree with the other agents that Lori needs to work a total of 
$352.00 ÷ $8.00/hour = 44 hours to make her purchase. The final 
answer is: \boxed{44}.
Round 2
✓
Agent 2: After reviewing the solutions 
provided by other agents, I agree that Lori 
needs to work 44 hours to make her 
purchase, so the final answer is \boxed{44}.
Agent 1: I agree with the other agents that Lori needs to work 44 hours 
to make her purchase. Therefore, the final answer is \boxed{44}.
Round 3
✓
✓
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                       (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✓
✓
Figure 19: Example of Correct GSM8K Debate.
18

Round 1
Agent 2: Jennifer gave away a total of $2 
\cdot 3 + 3 = 9$ oranges. Therefore, she 
remained with $12 - 9 = \boxed{3}$ 
oranges.
Agent 1: Jennifer gave a total of 2 oranges/daughter x 3 daughters + 3 
oranges/son = 9 oranges to her children. Thus, she remained with 12 
oranges - 9 oranges = 3 oranges.
\boxed{3}.
User Input: Can you solve the following math problem? Jennifer bought 12 oranges from the market, she gave her three 
daughters 2 oranges each, and her only boy got 3 oranges. How many oranges did she remain with? Explain your reasoning. 
Your final answer should be a single numerical number, in the form \boxed{answer}, at the end of your response.
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                    (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✓
Agent 2: I agree with the other agents' 
solutions. Jennifer gave away a total of 2 
oranges/daughter x 3 daughters + 3 
oranges/son = 9 oranges. Therefore, she 
remains with 12 oranges - 9 oranges = 
$\boxed{3}$ oranges.
Agent 1: I agree with the previous solutions provided. Jennifer gave a 
total of 2 oranges/daughter x 3 daughters + 3 oranges/son = 9 oranges to 
her children. Thus, she remained with 12 oranges - 9 oranges = 3 
oranges.
\boxed{3}.
Round 2
✓
Agent 2: I agree with the other agents. 
Jennifer gave away a total of 2 
oranges/daughter x 3 daughters + 3 
oranges/son = 9 oranges to her children, 
therefore she remains with 12 oranges - 9 
oranges = $\boxed{3}$ oranges.
Agent 1: I confirm the previous solutions provided. Jennifer gave a 
total of 2 oranges/daughter x 3 daughters + 3 oranges/son = 9 oranges to 
her children. Therefore, she remains with 12 oranges - 9 oranges = 
$\boxed{3}$ oranges.
Round 3
✓
✓
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                       (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✓
✓
Figure 20: Example of Correct GSM8K Debate.
19

Round 1
Agent 2: Let the number of hours each group of workers (6-toy 
producers and 4-toy producers) work be x and y, respectively. 
Then we have the following system of equations:
2(6x) + 2(4y) = 400 + 20
12x + 8y = 210 
(Equation 1)
x + y = 10 
(Equation 2)
Simplifying Equation 2, we get:
y = 10 - x
Substituting this into Equation 1, we get:
12x + 8(10 - x) = 210
4x = 50
x = 12.5
Thus, the first 4 workers produce a total of 2(6)(12.5) + 2(4)(10 -
12.5) = 150 toys. Since they still need another 20 toys to meet the 
order, the fifth worker must produce 20/10 = 2 toys per hour.
Therefore, the fifth worker produces \boxed{2} toys per hour.
Agent 1: Let's first find the total number of toys that the four workers 
produce in 10 hours.
The two workers who produce 6 toys an hour make a total of 6 x 2 x 10 = 
120 toys.
The other two workers who produce 4 toys an hour make a total of 4 x 2 x 10 
= 80 toys.
Together, they produce 120 + 80 = 200 toys.
Since the manufacturer still needs another 20 toys to complete the order, we 
know that all 5 workers together need to produce a total of 400 + 20 = 420 
toys.
Subtracting the 200 toys that have already been produced, we're left with 420 
- 200 = 220 toys that need to be produced by the fifth worker.
Since the workers have worked for 10 hours, we can set up the equation:
Toys per hour for fifth worker x 10 = 220
Solving for the toys per hour for the fifth worker, we get:
Toys per hour for fifth worker = 220/10 = 22
Therefore, the fifth worker must produce \boxed{22} toys per hour.
User Input: Can you solve the following math problem? A toy manufacturer receives an order for 400 toys. 5 workers are available to work on the order. 
2 of the workers produce 6 toys an hour, and another 2 workers produce 4 toys an hour. They all work on the order during their 10-hour shift, and by the 
end of their shift the manufacturer still needs another 20 toys to be able to ship the order. How many toys per hour does the fifth worker produce? 
Explain your reasoning. Your final answer should be a single numerical number, in the form \boxed{answer}, at the end of your response.
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                                         (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form \boxed{answer}, at the end of your response.
Agent 2: I agree with the updated solution provided by the 
other agent. The total toys produced by the four workers in 10 
hours is 400, and since the manufacturer still needs to produce 
another 380 toys, all five workers need to produce 38 toys per 
hour.
Setting up the equation, with x being the number of toys 
produced by the fifth worker per hour, we get:
2(6) + 2(4) + x = 38
12 + 8 + x = 38
x = 18
Therefore, the fifth worker must produce \boxed{18} toys per 
hour.
Agent 1: There is an error in the previous solution provided. The number of toys 
produced by the four workers in 10 hours is incorrect. The correct calculation 
should be:
2 (6 toys per hour) x 10 hours x 2 workers = 240 toys
2 (4 toys per hour) x 10 hours x 2 workers = 160 toys
Total toys produced by the four workers = 240 + 160 = 400 toys
So, the manufacturer still needs to produce another 400 - 20 = 380 toys.
To produce the remaining 380 toys in 10 hours, all five workers need to produce 
380/10 = 38 toys per hour.
Since four workers are already producing 6 and 4 toys per hour, we can set up 
the equation:
2 (6) + 2 (4) + x = 38
Simplifying this, we get: x = 7
Therefore the fifth worker must produce \boxed{7} toys per hour.
Round 2
Agent 2: I agree with the latest solution provided by the 
other agent. The total number of toys produced by the four 
workers is 400, and the manufacturer still needs to produce 
another 380 toys. To produce the remaining 380 toys in 10 
hours, all five workers need to produce 38 toys per hour.
Setting up the equation, with x being the number of toys 
produced by the fifth worker per hour, we get:
2(6) + 2(4) + x = 38
12 + 8 + x = 38
x = 18/5
Therefore, the fifth worker must produce \boxed{3.6} toys 
per hour.
Agent 1: I believe there is an error in the updated solution provided by the other 
agents. The total number of toys produced by the four workers in 10 hours is 
indeed 400, and since the manufacturer still needs to produce another 380 toys, 
all five workers need to produce 38 toys per hour.
Setting up the equation, with x being the number of toys produced by the fifth 
worker per hour, we get:
2(6) + 2(4) + x = 38
12 + 8 + x = 38
x = 18/5
Therefore, the fifth worker must produce \boxed{3.6} toys per hour.
Round 3
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                                         (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form \boxed{answer}, at the end of your response.
✗
✗
✗
✗
✗
✓
Figure 21: Example of Incorrect GSM8K Debate.
20

Round 1
Agent 2: Let's call the actual number of people at 
the concert "x." Then, according to Kelly's 
statement, we know that:
x = 48 - 0.20(48)
x = 48 - 9.6
x = 38.4
Therefore, the actual number of people at the 
concert was $\boxed{38}$.
Agent 1: If Courtney overstated the number of people by 20%, 
that means she reported 120% of the actual number of people. 
Let's call the actual number of people x. Then we can set up the 
following equation:
1.2x = 48
Solving for x, we get:
x = 40
Therefore, the actual number of people who attended the concert 
was \boxed{40}.
User Input: Can you solve the following math problem? Courtney attended a concert and reported that the audience was 48 
in number. However, Kelly went to the same concert and said that Courtney had made the mistake of overstating the number 
of people in attendance by 20%. If Kelly was right, how many people really attended the concert? Explain your reasoning. 
Your final answer should be a single numerical number, in the form \boxed{answer}, at the end of your response.
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                    (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✓
Agent 2: I agree with the first agent's solution that 
we need to consider the 20% decrease from the 
original number of people. Let x be the actual 
number of people at the concert, then we have:
x = 48 - 0.20(48)
x = 48 - 9.6
x = 38.4
Rounding to the nearest whole number, we get 
that the actual number of people at the concert 
was $\boxed{38}.$
I also agree with the second agent's point that we 
should be careful in interpreting the 20%, as it 
could be presented in percentage points or as a 
percent of the original number.
Agent 1: The other agent's solution assumes that Courtney 
overstated the number of people by 20 percentage points, not 20 
percent of the original number. This means their answer is 
incorrect.
Instead, we can use the method in the first solution. Let x be the 
actual number of people at the concert. Then, we know that:
x = 48 * (100% - 20%) = 48 * 0.8 = 38.4
Therefore, the actual number of people at the concert was 
$\boxed{38}$.
Round 2
Agent 2: I agree with the previous agents that the 
20% decrease should be calculated as a percentage 
of the original number of people. Let x be the 
actual number of people at the concert, then we 
have:
x = 48 * 0.80 = 38.4
Since we cannot have a fraction of a person, we 
should round the answer to the nearest whole 
number. Therefore, the actual number of people at 
the concert was $\boxed{38}.$
I also appreciate the second agent's point about 
being careful with the interpretation of the 
percentage, as it could be presented in percentage 
points or as a percent of the original number. In 
this case, it seems reasonable to assume that the 
20% decrease refers to a percentage of the original 
number.
Agent 1: I agree with the previous agents that we need to 
consider the 20% decrease from the original number of people. 
Let x be the actual number of people at the concert, then we have:
x = 48 * 0.80 = 38.4
Since we cannot have a fraction of a person, we round to the 
nearest whole number, giving us that the actual number of people 
at the concert was $\boxed{38}.$
I also agree with the second agent's point that we should be 
careful in interpreting the 20%, as it could be presented in 
percentage points or as a percent of the original number. In this 
case, the wording suggests that it should be interpreted as a 
percent of the original number.
Round 3
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                       (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✗
✗
✗
✗
✗
Figure 22: Example of Incorrect GSM8K Debate.
21

Round 1
Agent 2: If Jack has collected twice as many 
dishes as socks and he collected 60 dishes, then 
he has 60/2 = 30 socks.
Peter has twice as many socks as Jack, which 
means he has 2*30 = 60 socks.
Peter also has half as many dishes as Jack, which 
means he has 60/2 = 30 
dishes. 
The total number of dishes and socks they have 
together is 60+30+30 = \boxed{120}.
Agent 1: If Jack collected twice as many dishes as socks, then 
he collected 60/2=<<60/2=30>>30 
socks.
Peter has half times as many dishes as Jack, meaning he has 
60/2=<<60/2=30>>30 dishes.
If Peter has twice as many socks as Jack, then he has 
2*30=<<2*30=60>>60 socks.
Together, Peter and Jack have 
60+30+30=<<60+30+30=120>>120 socks and dishes. Answer: 
\boxed{120}.
User Input: Can you solve the following math problem? Peter has twice as many socks as Jack and half times as many 
dishes as jack. Jack collected twice as many dishes as socks in the store. If jack collected 60 dishes, calculate the total 
number of socks and dishes they have together? Explain your reasoning. Your final answer should be a single numerical 
number, in the form \boxed{answer}, at the end of your response.
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                    (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
Agent 2: I agree with the solutions provided by 
the other agents. The total number of socks and 
dishes they have together is 120. Therefore, my 
answer is \boxed{120}.
Agent 1: I agree with the solutions provided by the other 
agents.
If Jack collected 60 dishes and he collected twice as many dishes 
as socks, then he collected 60/2 = 30 socks.
If Peter has twice as many socks as Jack, then he has 2*30 = 60 
socks.
If Peter has half as many dishes as Jack, then he has 60/2 = 30 
dishes.
Therefore, the total number of dishes and socks they have 
together is 60+30+30 = \boxed{120}.
Round 2
Agent 2: I agree with the solutions provided by the 
other agents. Therefore, the total number of socks 
and dishes they have together is 120. My final 
answer is \boxed{120}.
Agent 1: I agree with the solutions provided by the other 
agents. 
If Jack collected 60 dishes and he collected twice as many dishes 
as socks, then he collected 60/2 = 30 socks.
If Peter has twice as many socks as Jack, then he has 2*30 = 60 
socks.
If Peter has half as many dishes as Jack, then he has 60/2 = 30 
dishes.
Therefore, the total number of dishes and socks they have 
together is 60+30+30 = \boxed{120}.
Round 3
User Input: These are the solutions to the problem from other agents: One agent solution: 
(Copy Agent 2’s reply)                                                                       (Copy Agent 1’s reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 
\boxed{answer}, at the end of your response.
✗
✗
✗
✗
✗
✗
Figure 23: Example of Incorrect GSM8K Debate.
22

Round 1
Agent 2: Using the order of operations 
(PEMDAS), we will first multiply 23 by 6 to get 
138. Then we will multiply 24 by 12 to get 288. 
Finally, we will add up all the terms: 4 + 138 + 24 -
288 = -122. 
Agent 1:  Based on the order of operations 
(PEMDAS), the correct way to solve the 
expression 4 + 23*6 + 24 - 24*12 is:
4 + 138 + 24 - 288 = -122
Therefore, the updated answer is -122.
Agent 1: The result of 4+23*6+24-24*12 is -244.
Agent 2: One common way to accurately solve 
mathematical expressions is by using the order of 
operations (PEMDAS) which means Parentheses, 
Exponents, Multiplication and Division, and 
finally Addition and Subtraction from left to right. 
To apply this rule in solving the given 
mathematical expression, we will first multiply 23 
by 6 then multiply 24 by 12 before performing 
any addition or subtraction. Using PEMDAS, 
4 + 23*6 + 24 - 24*12 = 4 + 138 + 24 - 288 = -
122.
Therefore, based on the order of operations, the 
result of the expression is -122, contrary to the 
opinion provided by another agent.
Instruction: What is the result of 4+23*6+24-24*12? Make sure to state your answer at the end of the response.
Instruction: These are the recent/updated opinions 
from other agents: (Copy Agent 2’s reply) Use these 
opinions carefully as additional advice, can you 
provide an updated answer? Make sure to state your 
answer at the end of the response.
Round 2
Instruction: These are the recent/updated opinions 
from other agents: (Copy Agent 1’s reply) Use these 
opinions carefully as additional advice, can you 
provide an updated answer? Make sure to state your 
answer at the end of the response.
✓
✗
✓
✓
Instruction: These are the recent/updated opinions 
from other agents: (Copy Agent 2’s reply) Use these 
opinions carefully as additional advice, can you 
provide an updated answer? Make sure to state your 
answer at the end of the response.
Round 3
Instruction: These are the recent/updated opinions 
from other agents: (Copy Agent 1’s reply) Use these 
opinions carefully as additional advice, can you 
provide an updated answer? Make sure to state your 
answer at the end of the response.
Agent 1:  I agree with the analysis and solution 
provided by the other agent. Using the order of 
operations (PEMDAS), we perform multiplication 
before addition or subtraction. Therefore the 
expression is calculated as follows:
4 + 23*6 + 24 - 24*12 = 4 + 138 + 24 - 288 = -122
So, the updated answer is -122.
Agent 2: Yes, I agree with the other agents that 
based on the order of operations (PEMDAS), the 
correct way to solve the expression 4 + 23*6 + 24 
- 24*12 is:
4 + 138 + 24 - 288 = -122
Therefore, the updated answer is -122.
✓
✓
Figure 24: Example of Arithmetic Debate.
23

Round 1
Agent 2: Using the order of operations 
(PEMDAS), we will first multiply 23 by 6 to get 
138. Then we will multiply 24 by 12 to get 288. 
Finally, we will add up all the terms: 4 + 138 + 24 -
288 = -122. 
Agent 1:  Based on the order of operations 
(PEMDAS), the correct way to solve the 
expression 4 + 23*6 + 24 - 24*12 is:
4 + 138 + 24 - 288 = -122
Therefore, the updated answer is -122.
Agent 1: The result of 4+23*6+24-24*12 is -244.
Agent 2: One common way to accurately solve 
mathematical expressions is by using the order of 
operations (PEMDAS) which means Parentheses, 
Exponents, Multiplication and Division, and 
finally Addition and Subtraction from left to right. 
To apply this rule in solving the given 
mathematical expression, we will first multiply 23 
by 6 then multiply 24 by 12 before performing 
any addition or subtraction. Using PEMDAS, 
4 + 23*6 + 24 - 24*12 = 4 + 138 + 24 - 288 = -
122.
Therefore, based on the order of operations, the 
result of the expression is -122, contrary to the 
opinion provided by another agent.
Instruction: What is the result of 4+23*6+24-24*12? Make sure to state your answer at the end of the response.
Instruction: These are the recent/updated opinions 
from other agents: (Copy Agent 2’s reply) Use these 
opinions carefully as additional advice, can you 
provide an updated answer? Make sure to state your 
answer at the end of the response.
Round 2
Instruction: These are the recent/updated opinions 
from other agents: (Copy Agent 1’s reply) Use these 
opinions carefully as additional advice, can you 
provide an updated answer? Make sure to state your 
answer at the end of the response.
✓
✗
✓
✓
Instruction: These are the recent/updated opinions 
from other agents: (Copy Agent 2’s reply) Use these 
opinions carefully as additional advice, can you 
provide an updated answer? Make sure to state your 
answer at the end of the response.
Round 3
Instruction: These are the recent/updated opinions 
from other agents: (Copy Agent 1’s reply) Use these 
opinions carefully as additional advice, can you 
provide an updated answer? Make sure to state your 
answer at the end of the response.
Agent 1:  I agree with the analysis and solution 
provided by the other agent. Using the order of 
operations (PEMDAS), we perform multiplication 
before addition or subtraction. Therefore the 
expression is calculated as follows:
4 + 23*6 + 24 - 24*12 = 4 + 138 + 24 - 288 = -122
So, the updated answer is -122.
Agent 2: Yes, I agree with the other agents that 
based on the order of operations (PEMDAS), the 
correct way to solve the expression 4 + 23*6 + 24 
- 24*12 is:
4 + 138 + 24 - 288 = -122
Therefore, the updated answer is -122.
✓
✓
Figure 25: Example of Arithmetic Debate with Summarization. Four separate agents participate in debate,
with two illustrated above. Instruction contains the summarized responses across agents.
24

Round 1
Agent 2: Based on the location of the 
submandibular salivary gland, it is more likely to 
be palpable extraorally than intraorally. This is 
because the gland is located beneath the mandible 
and extends posteriorly, making it more accessible 
for palpation outside of the oral cavity. Therefore, 
the expected answer is (B) to be palpable 
extraorally
Agent 1: An enlarged submandibular salivary 
gland is typically palpable extraorally. This is 
because the submandibular gland is located 
underneath the mandible bone, and therefore is 
more easily palpable from the outside of the 
mouth rather than inside. However, in some cases, 
the gland may also be palpable intraorally if it is 
significantly enlarged. Therefore, the answer is 
(C) to be palpable both intra- and extraorally.
Instruction: Can you answer the following question as accurately as possible? You suspect that your patient has an 
enlarged submandibular salivary gland. You expect the enlarged gland:  A) to be palpable intraorally.  B) to be palpable 
extraorally. C) to be palpable both intra- and extraorally.  D) only to be detectable by radiographical examination. \n 
Explain your answer, putting the answer in the form (X) at the end of your response.
Instruction: Here are responses from other agents: 
(Copy Agent 2’s reply) Using the solutions from other 
agents as additional information, can you give an 
updated response….
Instruction: Here are responses from other agents: 
(Copy Agent 1’s reply) Using the solutions from 
other agents as additional information, can you give 
an updated response….
✓
✗
Agent 2: After carefully considering the reasoning 
from other agents, my updated answer would be (C) 
to be palpable both intra- and extraorally. The 
submandibular salivary gland is located in the 
submandibular triangle, extending from the hyoid 
bone to the mandible. While the gland is primarily 
palpable extraorally due to its location beneath the 
mandible, it can also be palpable intraorally in some 
cases if significantly enlarged.\n\nTherefore, it is 
essential to perform a thorough examination by 
palpating both intra- and extraorally to assess the 
gland properly, as stated by the other agents. This will 
allow for a more accurate and comprehensive 
evaluation of the gland's size and location. Overall, 
the input from other agents supports the conclusion 
that the gland can be palpable both intra- and 
extraorally, and a complete examination is necessary 
to evaluate its condition.
Agent 1: After considering the reasoning from other 
agents, I would update my answer to (C) to be 
palpable both intra- and extraorally. The 
submandibular salivary gland is located in the 
submandibular triangle, extending from the hyoid 
bone to the mandible. This location allows the gland 
to be palpable both intra- and extraorally. However, it 
is primarily palpable extraorally due to its location 
beneath the mandible, making it more accessible for 
examination outside of the oral cavity. Additionally, 
in some cases, if the gland is significantly enlarged, it 
may also be palpable intraorally.\n\nIt is essential to 
perform a thorough examination by palpating both 
intra- and extraorally to assess the gland properly. By 
combining the inputs from other agents, I conclude 
that it is crucial to perform a complete examination, 
including extraoral and intraoral palpation, to 
evaluate the gland's size and location. Therefore, the 
updated answer is (C) to be palpable both intra- and 
extraorally.
Round 2
✓
✓
Figure 26: Example of MMLU Debate.
25

Round 1
Agent 2:.
-
Stephen R. Bourne is a computer scientist known for 
his development of the Unix shell, or command-line 
interface, known as the Bourne shell.
-
He earned a bachelor's degree in mathematics and a 
doctorate in computer science from Cambridge 
University.
-
Bourne worked at Bell Labs, where he helped to 
create the Unix operating system and its command-
line interface.
-
In addition to the Bourne shell, he also created the 
Bourne-Again shell (bash) and was involved in the 
creation of the Korn shell.
-
Bourne was awarded the USENIX Lifetime 
Achievement Award in 1999 and was elected as a 
Fellow of the Association for Computing Machinery 
in 2013.
-
He has authored and co-authored numerous technical 
papers and books, including "The Unix System," a 
widely used textbook on Unix programming.
Agent 1: 
-
Stephen R. Bourne is a British-born American 
computer scientist.
-
He is best known for creating the Bourne shell (sh) for 
the Unix operating system in 1979.\n- The Bourne
shell was the default Unix shell for many years and is 
still in use today.
-
Bourne also worked on the development of the UNIX 
System III and IV operating systems.
-
He later joined AT&T Bell Laboratories, where he 
worked on the UNIX System V operating system and 
its derivatives.
-
Bourne was awarded the ACM Software System 
Award in 1983 for his work on the Bourne shell.
-
He is also a recipient of the USENIX Lifetime 
Achievement Award (2001) and the IEEE Computer 
Pioneer Award (2010).
-
Bourne is now retired and continues to be involved in 
the computer science community. 
Instruction: Give a bullet point biography of Stephan R. Bourne highlighting their contributions and achievements as a 
computer scientist, with each fact separated by a new line character.
Instruction: Here are some bullet point biographies from 
other agents: (Copy Agent 2’s reply) Using the biographies as 
additional information, can you give an updated bullet point 
biography.
Instruction: Here are  some bullet point biographies from 
other agents: (Copy Agent 1’s reply) Using the biographies 
as additional information, can you give an updated  bullet 
point biography.
Agent 2: 
-
Stephen R. Bourne is a highly respected computer 
scientist and mathematician who revolutionized the 
computing field with his development of the Unix shell, 
popularly known as the Bourne shell. 
-
He earned his bachelor's degree in mathematics and a 
doctorate in computer science from Cambridge 
University, and began his career at Bell Labs where he 
played an integral role in the creation of the Unix 
operating system. 
-
During his tenure, Bourne developed the Bourne shell, 
which became the foundation for the Unix command-
line interface. 
-
Bourne was not only a prolific developer, but he also 
contributed to the development of several other 
command-line interfaces, including the Bourne-Again 
shell (bash) and the Korn shell. 
-
As a testament to his significant contributions to the field 
of computer science, Bourne was honored with the ACM 
Software System Award in 1983, and later the USENIX 
Lifetime Achievement Award in 1999. 
-
In recognition of his substantial contributions, he was 
elected as a Fellow of the Association for Computing 
Machinery in 2013. 
-
Bourne is also an accomplished author and co-author of 
several technical papers and books, including "The Unix 
System," a widely used textbook on Unix programming. 
-
Despite his retirement, Bourne continues to be actively 
involved in the computer science community, sharing his 
expertise and experiences with younger generations of 
aspiring computer scientists.
Round 2
Agent 1: 
-
Stephen R. Bourne is a computer scientist known for 
his development of the Unix shell, or command-line 
interface, known as the Bourne shell.
-
He earned a bachelor's degree in mathematics and a 
doctorate in computer science from Cambridge 
University.
-
Bourne worked at Bell Labs, where he played a key 
role in the creation of the Unix operating system, and
was involved in the development of its command-line 
interfaces.
-
In addition to the Bourne shell, he also created the 
Bourne-Again shell (bash) and contributed to the 
development of the Korn shell.
-
Bourne was awarded the ACM Software System 
Award in 1983 for his work on the Bourne shell, and 
the USENIX Lifetime Achievement Award in 1999 
for his contributions to the field of computer science.
-
He was also elected as a Fellow of the Association 
for Computing Machinery in 2013 in recognition of 
his significant contributions to the field.
-
Bourne has authored and co-authored numerous 
technical papers and books, including "The Unix 
System," a widely used textbook on Unix 
programming, and continues to be involved in the 
computer science community after his retirement.
Figure 27: Example of Biography Debate. While we found that generated biographies after debate to be more
accurate, many facts remain incorrect.
26

Round 1
Agent 2: 14. Be3
Agent 2: I would still suggest the move 14. Be3. It 
develops a piece and prepares to put pressure on 
black's position. Additionally, it reinforces the e4 
pawn and prepares to castle kingside.
Agent 1:  14. dxe6
Agent 1: Based on the current board state and the 
suggested moves, I would suggest the move: 14. 
Qa4. This move attacks the undefended bishop on 
f5 and puts pressure on black's position.
Instruction: Here is the current sequence of moves in a chess game: 1. e4 c6 2. d4 d5 3. Nd2 Qc7 4. Bd3 dxe4 5. 
Nxe4 Bf5 6. Nf3 Nd7 7. O-O e6 8. c4 Bg6 9. d5 Bxe4 10. Bxe4 Ndf6 11. Re1 Nxe4 12. Rxe4 Nf6 13. Re1 O-O-O . 
What is the best chess move I should execute next? Give a single move suggestion of the form 14. <XXX> and make 
sure the chess move is valid in the current board state. 
Instruction: Using the solutions from other agents as 
additional information. (Copy Agent 2’s reply)
Round 2
Instruction: Using the solutions from other agents as 
additional information. (Copy Agent 1’s reply)
Round 3
Agent 1: After considering the suggestions of 
other agents, I still believe that the move 14. Qa4 
is the best move in the current board state. It 
attacks the bishop on f5, prepares to develop the 
queen, and puts pressure on Black's position.
Agent 2: After considering the suggestions of other 
agents, I would suggest the move 14. Qa4. It 
attacks the bishop on f5, prepares to develop the 
queen and puts pressure on Black's position.
Instruction: Using the solutions from other agents as 
additional information. (Copy Agent 2’s reply)
Instruction: Using the solutions from other agents as 
additional information. (Copy Agent 1’s reply)
Figure 28: Example of Chess Debate.
27

