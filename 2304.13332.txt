arXiv:2304.13332v2  [math.NA]  23 Feb 2024
February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Mathematical Models and Methods in Applied Sciences
Â© World Scientiï¬c Publishing Company
Entropy-based convergence rates of greedy algorithms
Yuwen Liâˆ—
School of Mathematical Sciences, Zhejiang University,
Hangzhou, Zhejiang 310058, China
liyuwen@zju.edu.cn
Jonathan W. Siegel
Department of Mathematics, Texas A&M University,
College Station, TX 77845, USA
jwsiegel@tamu.edu
Received 29 April 2023
Revised 13 November 2023
Accepted 30 December 2023
We present convergence estimates of two types of greedy algorithms in terms of the
entropy numbers of underlying compact sets. In the ï¬rst part, we measure the error of a
standard greedy reduced basis method for parametric PDEs by the entropy numbers of
the solution manifold in Banach spaces. This contrasts with the classical analysis based
on the Kolmogorov n-widths and enables us to obtain direct comparisons between the
algorithm error and the entropy numbers, where the multiplicative constants are explicit
and simple. The entropy-based convergence estimate is sharp and improves upon the
classical width-based analysis of reduced basis methods for elliptic model problems. In the
second part, we derive a novel and simple convergence analysis of the classical orthogonal
greedy algorithm for nonlinear dictionary approximation using the entropy numbers of
the symmetric convex hull of the dictionary. This also improves upon existing results by
giving a direct comparison between the algorithm error and the entropy numbers.
Keywords: Reduced basis method; orthogonal greedy algorithm; nonlinear approxima-
tion; entropy numbers; Kolmogorov n-width; symmetric convex hull.
AMS Subject Classiï¬cation: 41A25, 41A46, 41A65, 65M12, 65N15
1. Introduction
Greedy algorithms are ubiquitous in advanced scientiï¬c computing and computa-
tional mathematics. Successful greedy-type numerical algorithms include adaptive
ï¬nite element2, 13, 41, 42, 49 and wavelet14, 29 methods for boundary value problems,
certiï¬ed model reduction technique for parametric diï¬€erential equations,3, 35, 52
and nonlinear dictionary approximation4, 23, 27, 37, 56, 64 in signal processing, machine
âˆ—Corresponding author.
1

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
2
Y. Li & J. W. Siegel
learning and statistics. The aforementioned algorithms use local extreme criteria
and iterations to search a global quasi-optimizer. It is known that many greedy
algorithms have guaranteed error controls, solid convergence analysis, and optimal
computational complexity as advantages. In this paper, we are focused on two appli-
cations of greedy algorithms, one for the Reduced Basis Method (RBM) and one for
approximating functions. We shall show that these two types of greedy algorithms
originally designed for seemingly unrelated problems can be analyzed in a uniï¬ed
way.
The RBM is a class of popular numerical methods developed in recent decades for
eï¬ƒciently solving parametric PDEs.15, 35, 46, 52 In a Banach space X, let P(uÂµ, Âµ) = 0
be the model problem with the solution uÂµ âˆˆX, where Âµ is a parameter in a compact
set U. A standard RBM is split into two stages. In the oï¬„ine stage, the RBM
builds a highly accurate ï¬nite-dimensional subspace Xn = span{uÂµ1, . . . , uÂµn} âŠ‚X
to uniformly approximate the compact solution manifold
M = u(U) = {uÂµ âˆˆX : Âµ âˆˆU}.
The oï¬„ine construction of Xn is potentially very expensive but is only implemented
once during the oï¬„ine setup stage of the RBM. Then the online module of the RBM
is able to rapidly produce accurate Galerkin approximations to uÂµ based on Xn for
many instances of the parameter Âµ.
There are two main types of computational methods for constructing Xn. One is
called the Proper Orthogonal Decomposition (POD),5 which utilizes a (costly) sin-
gular value decomposition to extract the low-rank structure of a given high-quality
dataset. The other, is an iterative greedy algorithm for building the RBM subspace
Xn, which relies on a posteriori error estimates. Compared with POD, greedy al-
gorithms are computationally more feasible and have certiï¬ed error bounds and
convergence rates. The work47 provided a priori convergence rates of greedy RBMs
for single-parameter problems. In general, the convergence of greedy algorithms for
the RBM is given by comparing the numerical error of the algorithm Ïƒn(M) against
the Kolmogorov n-width dn(M), which measures the performance of the best pos-
sible n-dimensional subspace for approximating the solution manifold M. A direct
comparison between Ïƒn(M) and dn(M) was developed in Ref. 10. Later on, the
works7, 19 showed for the ï¬rst time that
dn(M) = O(nâˆ’s) =â‡’Ïƒn(M) = O(nâˆ’s),
(1.1)
where s > 0 and O is Landauâ€™s big O notation. In other words, greedy-type RBMs
are rate-optimal for polynomial decaying dn(M).
As far as we know, convergence results for the greedy RBM in the literature
are all based on comparison with the Kolmogorov n-width. In this paper, we derive
convergence analysis of greedy-type RBMs in terms of Îµn = Îµn(co(M)), the entropy
number38 of the symmetric convex hull co(M). Just as the n-width dn = dn(M),
Îµn is also an asymptotically small quantity for compact sets (see Refs. 11 and 45).
Unlike the rate-optimality result (1.1), our analysis leads to direct and optimal

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
3
comparisons of the type
Ïƒn(M) â‰¤CntÎµn(co(M)),
t âˆˆ
1
2, 1

,
(1.2)
where C is an explicit constant and the exponent t depends upon the Banach space
in which the error is measured. Speciï¬cally, t = (1/2) + |1/2 âˆ’1/p| for the spaces
Lp. As n â†’âˆ(1.2) is sharper than the direct comparisons based on dn(M) given
in Refs. 10 and 19. Moreover, in the context of solving parametric PDEs, it is easier
to calculate the entropy numbers than the Kolmogorov n-width of relevant sets,
which is another advantage of bounding Ïƒn(M) in terms of Îµn, see Subsection 2.2
for more details.
On the other hand, there are a variety of greedy algorithms for constructing
nonlinear approximations (see Ref. 20) of a single target function in the ï¬elds of
machine learning, statistics and signal processing. The historically ï¬rst of these
algorithms is known as projection pursuit regression in statistics,28, 36 the match-
ing pursuit algorithm in signal procession,17, 48 and the Pure Greedy Algorithm
(PGA) in approximation theory.23 Other variants of these greedy algorithms have
been proposed which enjoy signiï¬cantly better convergence guarantees for general
dictionaries, including the Relaxed Greedy Algorithm (RGA)18, 23, 37, 51 and the Or-
thogonal Greedy Algorithm (OGA).4, 23, 50 We refer to the article64 and the book60
for a thorough introduction to greedy algorithms for function approximation.
In recent decades, greedy algorithms have also been used to solve PDEs,
see1, 26, 40 and the recent work55 which uses RGAs and OGAs to solve PDEs with
neural networks. These algorithms adaptively select basis functions from a redun-
dant dictionary D âŠ‚X and use a sparse linear combination
fn =
n
X
i=1
cigi,
gi âˆˆD
of dictionary elements to approximate a target function f. Among these greedy
algorithms, the RGA and OGA achieve the worst case optimal convergence rate
O(nâˆ’1/2) for target functions in the convex hull of the dictionary D,23, 51 while the
PGA in general may perform worse.43 In Ref. 56, it is shown that for dictionaries
whose convex hull has small entropy numbers, the orthogonal greedy algorithm may
converge faster, speciï¬cally for f âˆˆco(D) and s > 1/2,
Îµn(co(D)) = O(nâˆ’s) =â‡’âˆ¥f âˆ’fnâˆ¥X = O(nâˆ’s).
(1.3)
As mentioned before, the OGA converges with rate O(nâˆ’1/2) and no better for a
general dictionary D (see Ref. 23). The result (1.3) is an improved convergence rate
for OGAs under additional assumptions, which hold for many popular dictionaries,
e.g., the ReLUk dictionary in shallow neural networks (see Ref. 57).
The second main contribution of our work is a novel direct comparison for the
OGA:
âˆ¥f âˆ’fnâˆ¥X â‰¤CfÎµn(co(D)),
(1.4)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
4
Y. Li & J. W. Siegel
where Cf is an explicit uniform constant depending on the target function f. We
remark that (1.3) is indeed a rate comparison while our estimate (1.4) is direct and
implies (1.3) as special cases. In addition, the validity of (1.3) relies on the polyno-
mial decay of Îµn(co(D)), while the estimate (1.4) is non-asymptotic and is always
valid for general dictionaries, e.g., when Îµn(co(D)) = O(eâˆ’Î±ns) is exponentially
diminishing.
Throughout the rest of this section, we brieï¬‚y introduce the Kolmogorov n-
width, the entropy numbers, and the details of greedy algorithms for the RBM and
function approximation.
1.1. Greedy Algorithms for RBMs
Let K be a compact set in a Banach space X equipped with the norm âˆ¥â€¢âˆ¥= âˆ¥â€¢âˆ¥X.
The Kolmogorov n-width of K is deï¬ned as
dn(K) = dn(K)X := inf
Xn sup
fâˆˆK
dist(f, Xn),
where
dist(f, Xn) := inf
gâˆˆXn âˆ¥f âˆ’gâˆ¥
is the distance from f to Xn, and the inï¬mum is taken over all n-dimensional
subspaces of X.
On the other hand, Kolmogorov proposed and investigated the (dyadic) entropy
numbers of K (see Ref. 38), which are given by
Îµn(K) = Îµn(K)X := inf{Îµ > 0 : K is covered by 2n balls of radius Îµ}.
It is known that {dn(K)}nâ‰¥1 and {Îµn(K)}nâ‰¥1 are decreasing sequences and
limnâ†’0 dn(K) = limnâ†’0 Îµn(K) = 0 for any compact set K (see Ref. 45). When
K is compact, the symmetric or absolutely convex hull of K,
co(K) :=
n X
i
cigi :
X
i
|ci| â‰¤1, gi âˆˆK for each i
o
,
is also compact. The key quantity used in our convergence analysis is the entropy
number Îµn(co(K)), while in traditional analyses the Kolmogorov n-widths dn(K)
are typically used.
In the oï¬„ine stage, the greedy algorithm for RBMs selects a sequence {fn}nâ‰¥1
from K in an adaptive way as follows.
Algorithm 1.1 (Greedy Algorithm for RBMs). Set n = 1 and X0 = {0}.
Step 1: Compute
fn = arg max
fâˆˆK dist(f, Xnâˆ’1).
Step 2: Set Xn = span{f1, . . . , fn}. Set n = n + 1. Go to Step 1.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
5
In practice, K is often the solution manifold M of a parametric PDE corre-
sponding to a range of varying parameters, and the computational cost of the exact
greedy approach (Algorithm 1.1) might still be exceedingly high. In such cases, a
weak greedy algorithm driven by a posterior error estimation is able to relax the
argmax criterion in Algorithm 1.1 and to realize the next more feasible algorithm.
This modiï¬cation is analogous to the weak greedy algorithms for function approx-
imation,62 which we discuss in more detail in Subsection 1.2.
Algorithm 1.2 (Weak Greedy Algorithm for RBMs). Set n = 1, Î³ âˆˆ(0, 1]
and X0 = {0}.
Step 1: Select fn âˆˆK such that
dist(fn, Xnâˆ’1) â‰¥Î³ max
fâˆˆK dist(f, Xnâˆ’1).
Step 2: Set Xn = span{f1, . . . , fn}. Set n = n + 1. Go to Step 1.
We remark that Step 1 of Algorithm 1.2 is not directly implemented but prac-
tically achieved by an a posteriori error estimation.25, 35, 53 The constant Î³ âˆˆ(0, 1]
actually depends on the quality of a posteriori error estimators. The numerical error
of Algorithm 1.2 is measured by
Ïƒn(K) = Ïƒn(K)X := sup
fâˆˆK
dist(f, Xn).
By deï¬nition the simple bound
dn(K) â‰¤Ïƒn(K)
(1.5)
is always true. We note that Algorithm 1.2 with Î³ = 1 reduces to Algorithm 1.1
and the iterates {fn}nâ‰¥1 are not uniquely determined by the above greedy selection
processes. When X is a Hilbert space, let PU be the orthogonal projection onto a
subspace U âŠ‚X. In this case we have
dist(f, Xn) = âˆ¥f âˆ’PXnfâˆ¥,
Ïƒn(K) = max
fâˆˆK âˆ¥f âˆ’PXnfâˆ¥.
1.2. Greedy Algorithms for Approximating Functions
Let the dictionary D be a collection of elements in a real Hilbert space X equipped
with inner product âŸ¨Â·, Â·âŸ©. For learning algorithms based upon wavelets, shallow neu-
ral networks, statistical regression or compressed sensing, it is often essential to
construct nonlinear approximants to a target function f âˆˆX. To achieve this goal,
Refs. 28 and 48 developed the projection pursuit or matching pursuit algorithm:
gn = arg max
gâˆˆD |âŸ¨g, f âˆ’fnâˆ’1âŸ©|,
fn = fnâˆ’1 + âŸ¨f âˆ’fnâˆ’1, gnâŸ©gn,

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
6
Y. Li & J. W. Siegel
where f0 = 0 and n = 1, 2, . . . is the iteration counter. Note that if the dictionary
elements are normalized in X, then this algorithm is adding the single term which
minimizes the error in each step and validates the name â€œgreedy algorithm.â€ The
theoretical analysis of this algorithm is notoriously complex,23, 36, 58 and in any case
it does not achieve an optimal rate of convergence.43 As an alternative, a variety
of diï¬€erent greedy algorithms which achieve better convergence behavior have been
introduced, for example the following RGA4, 23, 37 and its variants:51
(Î±n, Î²n, gn) = arg
min
Î±,Î²âˆˆR,gâˆˆD âˆ¥f âˆ’Î±fnâˆ’1 âˆ’Î²gnâˆ¥,
fn = Î±nfnâˆ’1 + Î²ngn.
In this work, the algorithm under consideration is the OGA which has been devel-
oped and analyzed in, e.g., Refs. 4,18,23,50,60 and 56.
Algorithm 1.3 (Orthogonal Greedy Algorithm). Set n = 1 and f0 = 0.
Step 1: Compute the optimizer
gn = arg max
gâˆˆD |âŸ¨g, f âˆ’fnâˆ’1âŸ©|.
Step 2: Set Xn = span{g1, . . . , gn} and compute
fn = PXnf = Pnf.
Set n = n + 1. Go to Step 1.
In Step 1 of Algorithm 1.3, it is possible that the argmax may not exist or may
be impossible to compute. To overcome this, one could relax the selection criterion
to
|âŸ¨gn, f âˆ’fnâˆ’1âŸ©| â‰¥Î³ max
gâˆˆD |âŸ¨g, f âˆ’fnâˆ’1âŸ©|
with Î³ âˆˆ(0, 1]. This modiï¬ed algorithm is closer to what is typically implemented
in practice and is called the weak OGA.62 Weak versions of other greedy algorithms
have also been introduced and analyzed in e.g., Ref. 60.
In our error estimates, C is a generic and uniform constant that may change
from line to line but is independent of n and underlying compact sets or target
functions. By A â‰²B we denote A â‰¤CB, and A â‰‚B is equivalent to A â‰²B and
B â‰²A.
The rest of this paper is organized as follows. In Section 2, we present an
entropy-based convergence analysis of greedy RBMs and corresponding applications
in Hilbert spaces. In Section 3, we derive similar optimal convergence estimates for
RBMs in Banach spaces. Section 4 is devoted to an optimal direct comparison
estimate for the convergence of OGAs in Hilbert spaces. Concluding remarks are
presented in Section 5.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
7
2. Convergence of RBMs in Hilbert spaces
Throughout this section, we assume that X is a Hilbert space unless confusion
arises. The next lemma is the main tool of our entropy-based convergence analysis
for greedy algorithms.
Lemma 2.1. Let Vn =
Ï€
n
2
Î“
 n
2 +1
 be the volume of an â„“2-unit ball in Rn. Let K be a
compact set in a Hilbert space X. Then for any v1, . . . , vn âˆˆK, it holds that

n
Y
k=1
vk âˆ’Pkâˆ’1vk

 1
n â‰¤(n!Vn)
1
n Îµn(co(K))X.
where P0 = 0 and Pk is the orthogonal projection onto span{v1, . . . , vk}.
Proof. Consider the symmetric simplex
S = co
 {v1, . . . , vn}

=
( n
X
i=1
civi :
n
X
i=1
|ci| â‰¤1
)
.
We assume that {vk}n
k=1 are linearly independent otherwise the inequality auto-
matically holds. Let Xn be the n-dimensional subspace spanned by v1, . . . , vn. We
identify Xn with the Euclidean space Rn and S with a n-dimensional skewed simplex
in Rn. It is straightforward to see that
{vk}n
k=1 7â†’{vk âˆ’Pkâˆ’1vk}n
k=1
is the Gram-Schmidt orthogonalization of v1, ..., vn. As a result, the volume of S is
given by
|S| = 2n
n!
n
Y
k=1
vk âˆ’Pkâˆ’1vk
.
(2.1)
The deï¬nition of Îµn = Îµn(co(K))X implies that S is covered by 2n balls of radius
Îµn. Therefore by (2.1) and a volume comparison between S and the union of balls,
we have
2n
n!
n
Y
k=1
vk âˆ’Pkâˆ’1vk
 â‰¤2nÎµn
nVn.
The proof is complete.
2.1. Convergence estimates
Using Lemma 2.1, we immediately obtain a direct comparison result between the
RBM error and the entropy number on convex compact sets.
Theorem 2.1. Let K be a compact set in a Hilbert space X. For the weak greedy
algorithm (Algorithm 1.2) we have
Ïƒn(K)X â‰¤Î³âˆ’1(n!Vn)
1
n Îµn(co(K))X.
(2.2)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
8
Y. Li & J. W. Siegel
Proof. For Algorithm 1.2 and n â‰¥1, we have
Î³Ïƒnâˆ’1(K) â‰¤
fn âˆ’PXnâˆ’1fn
 â‰¤Ïƒnâˆ’1(K).
(2.3)
It then follows from (2.3) and Lemma 2.1 that

n
Y
k=1
Ïƒkâˆ’1(K)
 1
n â‰¤Î³âˆ’1
n
Y
k=1
fk âˆ’PXkâˆ’1fk

 1
n
â‰¤Î³âˆ’1(n!Vn)
1
n Îµn(co(K))X.
(2.4)
Combining (2.4) with the fact
Ïƒ0(K) â‰¥Ïƒ1(K) â‰¥Â· Â· Â· â‰¥Ïƒnâˆ’1(K) â‰¥Ïƒn(K)
completes the proof.
Using the well-known Stirlingâ€™s formula
lim
nâ†’âˆn!
.âˆš
2Ï€n
n
e
n
= 1,
lim
nâ†’âˆVn

1
âˆšnÏ€
2Ï€e
n
 n
2 = 1,
we obtain a corollary about the convergence rate of greedy-type RBMs.
Corollary 2.1. Let K be a compact set in a Hilbert space X. For Algorithm 1.2
there exists an absolute constant C independent of K and n, such that
Ïƒn(K)X â‰¤CâˆšnÎµn(co(K))X.
A natural question is whether the convergence rate of Ïƒn(K) in Corollary 2.1 is
sharp and whether the factor âˆšn is removable. In the following we give a negative
answer to this question.
Proposition 2.1. There exists a Hilbert space X and a symmetric convex compact
set K âŠ‚X such that Algorithm 1.1 satisï¬es
Ïƒn(K)X â‰‚âˆšnÎµn(K)X.
Proof. On a Lipschitz domain â„¦âŠ‚Rd, we consider the ReLUk activation function
Ïƒ(x) = max(xk, 0) with k â‰¥1 and the dictionary
Pd
k =

Ïƒ(Ï‰ Â· x + b) : Ï‰ âˆˆSd
1, b âˆˆ[b,Â¯b]
	
,
where Sd
1 is the d-dimensional unit sphere centered at 0, and b, Â¯b are constants
depending on â„¦. We set K = co(Pd
k) and X = L2(â„¦). It has been shown in Ref. 57
that
dn(co(Pd
k))L2(â„¦) = O(nâˆ’2k+1
2d ),
(2.5a)
Îµn(co(Pd
k))L2(â„¦) = O(nâˆ’1
2 âˆ’2k+1
2d ).
(2.5b)
Combining (2.5) and Corollary 2.1 we arrive at
dn(co(Pd
k))L2(â„¦) â‰¤Ïƒn(co(Pd
k))L2(â„¦) â‰²âˆšnÎµn(co(Pd
k))L2(â„¦),

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
9
and obtain the convergence rate
Ïƒn(co(Pd
k))L2(â„¦) = O(nâˆ’2k+1
2d ).
As a result, the error estimate in Corollary 2.1 is not improvable.
In the following we discuss the advantage of our direct comparisons over the
classical ones. The ï¬rst direct comparison result for greedy-type RBMs is derived
in Ref. 10:
Ïƒn(K) â‰²n2ndn(K),
which is useful if dn(K) = o(nâˆ’12âˆ’n). Later on, this estimate has been improved
in Refs. 7 and 19. Currently the sharpest direct comparison in the RBM literature
(see Ref. 19) is
Ïƒ2n(K) â‰²
p
dn(K).
(2.6)
Next we show that our direct comparison result in Theorem 2.1 is indeed sharper
than (2.6) as n â†’âˆ. In doing so, we need the well-known Carlâ€™s inequality (see
Refs. 11 and 45).
Lemma 2.2 (Carlâ€™s inequality). For every Î± > 0, there exists a constant C(Î±) >
0 such that for any compact set K in a Banach space X,
Îµn(K) â‰¤C(Î±)nâˆ’Î± max
1â‰¤iâ‰¤n(iÎ±diâˆ’1(K)).
For a compact set K with dn(K) â‰²nâˆ’s and s > 0, Lemma 2.2 implies that
Îµn(K) â‰²nâˆ’s, i.e., the entropy number decays as fast as the polynomial-decaying
Kolmogorov n-width. Therefore combining the fact (see Ref. 45)
dn(K) = dn(co(K))
(2.7)
with Lemma 2.2 and Corollary 2.1, we have
dn(K) â‰²nâˆ’s =â‡’Ïƒn(K) â‰²âˆšnÎµn(co(K)) â‰²n
1
2 âˆ’s.
(2.8)
In contrast, the estimate (2.6) under the same assumption yields
dn(K) â‰²nâˆ’s =â‡’Ïƒn(K) â‰²nâˆ’s
2 .
Thus our direct comparisons in Theorem 2.1 are asymptotically sharper than the
classical result (2.6) when s > 1.
2.2. Examples
In the rest of this section, we discuss applications of our results to the second order
elliptic equation
âˆ’âˆ‡Â· (aâˆ‡u(a)) = f
in â„¦âŠ‚Rd,
(2.9a)
u(a) = 0
on âˆ‚â„¦.
(2.9b)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
10
Y. Li & J. W. Siegel
Here f âˆˆHâˆ’1(â„¦) is ï¬xed, the diï¬€usion coeï¬ƒcient a âˆˆLâˆ(â„¦) is the varying input
parameter belonging to the function class A that will be speciï¬ed later, and u(a) âˆˆ
H1
0(â„¦) is the weak solution corresponding to a. First we present a simple lemma
for calculating the entropy numbers of the solution manifold M = u(A) of (2.9).
Lemma 2.3. Let 0 < Î± â‰¤1 and Î¦ : X â†’Y be an Î±-HÂ¨older continuous mapping
between Banach spaces X and Y such that for all x1, x2 âˆˆX,
âˆ¥Î¦(x1) âˆ’Î¦(x2)âˆ¥Y â‰¤Lâˆ¥x1 âˆ’x2âˆ¥Î±
X.
Then for any compact set K âŠ‚X we have
Îµn(Î¦(K))Y â‰¤LÎµn(K)Î±
X.
Proof. Let K be covered by 2n balls {BÎµ(xi)}2n
i=1 of radius Îµ = Îµn(K)X, where
BÎµ(xi) is centered at xi âˆˆX. Then the HÂ¨older continuity of Î¦ implies that Î¦(K)
can be covered by balls {BLÎµÎ±(Î¦(xi))}2n
i=1, suggesting that Îµn(Î¦(K))Y â‰¤LÎµÎ±.
For parametric PDEs, it is generally much more diï¬ƒcult to calculate the n-width
dn(u(A)) of the solution manifold M = u(A) than computing Îµn(u(A)). To remedy
this situation, Cohen and DeVore15, 16 developed a technical tool for estimating
dn(u(A)) in terms of dn(A).
Theorem 2.2 (Theorem 1 from Ref. 16). For a pair of complex Banach spaces
X and Y , suppose u is a holomorphic mapping from an open set O âŠ‚X into Y
and u is uniformly bounded on O:
sup
aâˆˆO
âˆ¥u(a)âˆ¥Y < âˆ.
If K âŠ‚O is a compact set, then for any Î± > 1 and Î² < Î± âˆ’1 we have
sup
nâ‰¥1
nÎ±dn(K)X < âˆ=â‡’sup
nâ‰¥1
nÎ²dn(u(K))Y < âˆ.
2.2.1. Example 1
Let Cs(â„¦) = Ck,Î±(â„¦) with regularity index
s := k + Î±
be the space of functions on â„¦whose k-th derivatives are HÂ¨older continuous with
exponent Î± âˆˆ(0, 1]. In the ï¬rst example, we focus on the following function class
of diï¬€usion coeï¬ƒcients
A =

a âˆˆLâˆ(â„¦) : inf
xâˆˆâ„¦a(x) > M0 > 0, âˆ¥aâˆ¥Ck,Î±(â„¦) â‰¤M1

,
where constants M0, M1 are ï¬xed. In practice, a âˆˆA has additional aï¬ƒnely
parametrized structures allowing an eï¬ƒcient online implementation in RBMs, see,

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
11
e.g., Refs. 35 and 52. Otherwise, researchers often utilize the empirical interpola-
tion method3 to construct aï¬ƒnely parametrized approximations to a and then apply
RBMs to the approximate model.
Using Theorem 2.2, the holomorphy of the solution operator u : a 7â†’u(a), and
dn(A)Lâˆ(â„¦) = O(nâˆ’s
d ) (see Refs. 44 and 16), it has been proved in Ref. 16 that
dn(u(A))H1(â„¦) â‰²nâˆ’s
d +1+t
(2.10)
for any t > 0. Then as a result of (2.10) and the classical convergence estimate of
greed-type RBMs (see Ref. 7), for s > d,
Ïƒn(u(A))H1(â„¦) â‰²nâˆ’s
d +1+t.
(2.11)
However, (2.11) fails to indicate any convergence of RBMs for approximating the
solution process a 7â†’u(a) of (2.9) when s â‰¤d.
Our convergence results for RBMs shed some light on such cases. For (2.9) there
exists a useful perturbation result (see Theorem 2.1 from Ref. 8)
âˆ¥u(a1) âˆ’u(a2)âˆ¥H1(â„¦) â‰¤Câ„¦âˆ¥âˆ‡u(a)âˆ¥Lp(â„¦)âˆ¥a1 âˆ’a2âˆ¥Lq(â„¦),
(2.12)
where p â‰¥2 and q := 2p/(p âˆ’2). In particular, (2.12) with p = 2 implies the
Lipschitz property of
u : Lâˆ(â„¦) â†’H1(â„¦),
a 7â†’u(a).
Combining this fact with Lemma 2.3 and Îµn(A)Lâˆ(â„¦) = O(nâˆ’s
d ) (see Ref. 38), we
have
Îµn(u(A))H1(â„¦) â‰²Îµn(A)Lâˆ(â„¦) â‰²nâˆ’s
d .
(2.13)
Next we need a lemma from Ref. 12 about the relations between the entropy num-
bers of K and co(K), see also Refs. 59 and 30 similar results.
Lemma 2.4 (Proposition 6.2 from Ref. 12). Let K be a compact set in a
Banach space X of type p âˆˆ(1, 2], and assume that
Îµn(K)X â‰²nâˆ’Î±
for some Î± > 1 âˆ’1/p. Then there exists a constant c(Î±, p) such that
Îµn(co(K))X â‰¤c(Î±, p)nâˆ’1+ 1
p (log(n + 1))âˆ’Î±+1âˆ’1
p .
In particular, the Hilbert space X = H1(â„¦) is of type p = 2. As a result of
Lemma 2.4 and (2.13), when d/2 < s â‰¤d we have
Îµn(co(u(A)))H1(â„¦) â‰²nâˆ’1
2 (log(n + 1))âˆ’s
d + 1
2 .
(2.14)
Therefore using Corollary 2.1 and (2.14) we obtain the convergence of the weak
greedy algorithm for (2.9):
dn(u(A))H1(â„¦) â‰¤Ïƒn(u(A))H1(â„¦) â‰²(log(n + 1))âˆ’s
d + 1
2 ,
where the function class A is of lower regularity s âˆˆ(d/2, d].

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
12
Y. Li & J. W. Siegel
2.2.2. Example 2
In the second example, we consider the same boundary value problem (2.9) on â„¦=
[0, 1]2 with a diï¬€erent set of diï¬€usion coeï¬ƒcients, which corresponds to DeVoreâ€™s
geometric model in Ref. 21. Let
H =

Ï† âˆˆCk,Î±[0, 1] : 0 â‰¤Ï†(x) â‰¤1 for x âˆˆ[0, 1]
	
.
The graph of a function Ï† âˆˆH partitions â„¦into subregions â„¦Ï†
+ and â„¦Ï†
âˆ’, deï¬ned by
â„¦Ï†
+ = {(x, y) âˆˆâ„¦, y â‰¥Ï†(x)}
(2.15)
and â„¦Ï†
âˆ’= â„¦\â„¦Ï†
+. The diï¬€usion coeï¬ƒcient a is a member of
A = {a = aÏ† âˆˆLâˆ(â„¦) : a|â„¦Ï†
+ = 2, a|â„¦Ï†
âˆ’= 1 for Ï† âˆˆH}.
(2.16)
Functions in A have discontinuities along the curve represented by Ï† âˆˆH, posing a
real challenge for reducing the model problem (2.9). In fact, there is no convergence
result of the RBM for the geometric model in the classical literature.
Elementary arguments show that for q â‰¥1 the mapping Ï† 7â†’aÏ† is 1/q-HÂ¨older
continuous from Lâˆ[0, 1] to Lq(â„¦). Therefore using Lemma 2.3 we can estimate the
metric entropy of A as
Îµn(A)Lq(â„¦) â‰¤Îµn(H)
1
q
Lâˆ[0,1] â‰‚nâˆ’s
q .
(2.17)
Moreover, there exists an exponent P > 2 depending on â„¦such that âˆ‡u(a) âˆˆLP(â„¦)
(see Refs. 32 and 8). Then the perturbation result (2.12) with p = P, q = Q :=
2P/(P âˆ’2) âˆˆ(2, âˆ) implies that
u : LQ(â„¦) â†’H1(â„¦),
a 7â†’u(a)
is a Lipschitz mapping. Then using (2.17) and Lemma 2.3 again, we arrive at
Îµn(u(A))H1(â„¦) â‰²Îµn(A)LQ(â„¦) â‰²nâˆ’s
Q .
Combining it with Corollary 2.1 and Lemma 2.4 shows that
Ïƒn(u(A))H1(â„¦) â‰²log(n + 1)âˆ’s
Q + 1
2 ,
s > Q/2,
indicating the convergence of the RBM Algorithm 1.2 with K = u(A). In addition,
we obtain the novel decay result of the n-width
dn(u(A))H1(â„¦) â‰²log(n + 1)âˆ’s
Q + 1
2 .
3. Convergence of RBMs in Banach spaces
In this section, we derive direct comparison and convergence estimates for the weak
greedy algorithm (Algorithm 1.2), where X is only assumed to be a general Banach
space. First we introduce the Banach-Mazur distance d(X, Y ) (see Ref. 67) between
two isomorphic Banach spaces X and Y :
d(X, Y ) = inf

âˆ¥T âˆ¥âˆ¥T âˆ’1âˆ¥: T is an isomorphism from X to Y
	
.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
13
Let â„“n
2 denote the space Rn under the Euclidean â„“2-norm. For all n â‰¥1 we consider
the number
Î´n(X) := sup
YnâŠ‚X
d(Yn, â„“n
2) â‰¤âˆšn,
(3.1)
where the supremum is taken over all n-dimensional Banach subspaces Yn in X,
and the upper bound in (3.1) has been proven in Section III.B.9 of Ref. 66. The
quantity Î´n(X) measures how far an n-dimensional subspace of X is away from the
Hilbert space â„“n
2. With the help of (3.1), we are able to establish the key lemma for
our analysis in Banach spaces.
Lemma 3.1. Let K be a compact set in a Banach space X. For any v1, . . . , vn âˆˆK
with Xk = span{v1, . . . , vk} and X0 = {0}, we have

n
Y
k=1
dist(vk, Xkâˆ’1)
 1
n â‰¤Î´n(X)(n!Vn)
1
n Îµn(co(K))X.
Proof. Let T be an isomorphism from Xn to â„“n
2 and
âˆ¥vâˆ¥T := âˆ¥T vâˆ¥â„“n
2 for any v âˆˆXn.
Then we have the norm equivalence
âˆ¥T âˆ’1âˆ¥âˆ’1âˆ¥vâˆ¥X â‰¤âˆ¥vâˆ¥T â‰¤âˆ¥T âˆ¥âˆ¥vâˆ¥X,
âˆ€v âˆˆXn.
(3.2)
In addition, by the deï¬nition of Î´n(X) we can choose T such that
âˆ¥T âˆ¥âˆ¥T âˆ’1âˆ¥= Î´n(X).
(3.3)
Let Xn,T denote the Hilbert space, which is the same as Xn as a set, and is equipped
with the inner product
(â€¢, â€¢)T := (T â€¢, T â€¢)â„“n
2 .
As a result of (3.2) it holds that
dist(vk, Xkâˆ’1) â‰¤âˆ¥T âˆ’1âˆ¥âˆ¥vk âˆ’Qkâˆ’1vkâˆ¥T,
(3.4)
where Qk is the orthogonal projection onto Xk with respect to (â€¢, â€¢)T. Let S =
co

v1, . . . , vn
	
. Then applying our previous estimate in Lemma 2.1 to the Hilbert
space Xn,T and using (3.4), we have
 n
Y
k=1
dist(vk, Xkâˆ’1)
! 1
n
â‰¤âˆ¥T âˆ’1âˆ¥
 n
Y
k=1
âˆ¥vk âˆ’Qkâˆ’1vkâˆ¥T
! 1
n
â‰¤âˆ¥T âˆ’1âˆ¥(n!Vn)
1
n Îµn(S)Xn,T .
(3.5)
On the other hand, the lower bound in (3.2) leads to
Îµn(S)Xn,T â‰¤âˆ¥T âˆ¥Îµn(S)X â‰¤âˆ¥T âˆ¥Îµn(co(K))X.
(3.6)
Therefore combining (3.5) with (3.6) and (3.3) completes the proof.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
14
Y. Li & J. W. Siegel
The estimate (3.1) on the Banach-Mazur distance can be improved in special
Banach spaces like Lp(dÂµ) with 1 < p < âˆ. For instance, it has been shown in
Ref. 66 that
Î´n(X) â‰¤n| 1
2 âˆ’1
p |
for X = Lp(dÂµ), p âˆˆ[1, âˆ].
(3.7)
Following the same proof of Theorem 2.1, we obtain the following error bounds
of Algorithm 1.2 in Banach spaces by Lemma 3.1.
Theorem 3.1. Let K be a compact set in a Banach space X. For the weak greedy
algorithm (Algorithm 1.2) we have
Ïƒn(K)X â‰¤Î³âˆ’1Î´n(X)(n!Vn)
1
n Îµn(co(K))X.
In particular, the asymptotic error estimates hold:
Ïƒn(K)X â‰²
(
Î³âˆ’1nÎµn(co(K))X,
for a general Banach space X,
Î³âˆ’1n
1
2 +| 1
2 âˆ’1
p |Îµn(co(K))X,
X = Lp(dÂµ), p âˆˆ[1, âˆ].
As a byproduct of (1.5) and Theorem 3.1, we also have a direct comparison
between the Kolmogorov n-width and entropy number in general Banach spaces,
which generalizes Proposition 2 from Ref. 57 for Hilbert spaces.
Corollary 3.1. For a compact set K in a Banach space X we have
dn(K)X â‰¤Î´n(X)(n!Vn)
1
n Îµn(co(K))X,
n â‰¥1.
In the end of this section, we shall show that the factor n
1
2 +| 1
2 âˆ’1
p | in Theorem
3.1 cannot be removed when p â‰¥2. To that end, we need the following lemma
concerning the metric entropy of the â„“n
1 unit ball in â„“p. We remark that we do not
know whether Theorem 3.1 is tight when 1 â‰¤p < 2.
Lemma 3.2 (Theorem 1 from Ref. 54). Let B1
m be an â„“1-unit ball in Rm. For
m/2 < s < m and 1 â‰¤p â‰¤âˆit holds that
Îµs(B1
m)â„“p â‰‚log(m/s)sâˆ’1+ 1
p .
Proposition 3.1. There exists a symmetric and convex compact set K in the Ba-
nach space X = â„“p with 2 â‰¤p â‰¤âˆsuch that Algorithm 1.1 satisï¬es
Ïƒn(K)X â‰‚n1âˆ’1
p Îµn(K)X.
(3.8)
Proof. Let {xi}âˆ
i=1 be a sequence of real numbers decreasing to 0. We consider the
compact set
K = co
 âˆªâˆ
i=1 {fi}

âŠ‚X = â„“p
where fi = xiei and ei is the i-th unit vector in â„“p. Clearly the greedy algorithm
1.1 selects f1, f2, . . . , fn, . . . as the iterates and
Ïƒn(K) = âˆ¥fnâˆ¥â„“p = |xn|.
(3.9)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
15
Given an exponent Î± > 1, let xj = 2âˆ’kÎ± for 2kâˆ’1 â‰¤j â‰¤2k âˆ’1. By construction
xn = O(nâˆ’Î±) and we shall show that
ÎµN(K)â„“p = O(N âˆ’1+ 1
p âˆ’Î±) for N = 3 Â· 2n.
(3.10)
We consider the following subsets of K:
K0 = co
 âˆª2n
i=1 {fi}

,
Kk = co
 âˆª2n+kâˆ’1
i=2n+kâˆ’1 {fi}

,
1 â‰¤k â‰¤n,
Kâˆ= co
 âˆªâˆ
i=22n {fi}

.
Using the elementary fact Îµs+t(A + B) â‰¤Îµs(A) + Îµt(B) (see Refs. 45 and 57) we
have
ÎµN(K)â„“p â‰¤Îµ2n(K0)â„“p +
n
X
k=1
Îµ2nâˆ’k(Kk)â„“p + Îµ2n(Kâˆ)â„“p.
(3.11)
Using the formula for the volume of an â„“p ball of radius Îµ in Rm (see Refs. 24 and
65)
V p
m(Îµ) = 2mÎµm Î“(1 + 1
p)m
Î“(1 + m
p ) ,
we estimate Îµm(K0)â„“p by calculating the volume of K0, with m = 2n:
Îµm(K0)â„“p â‰‚
 
2m
m! |x1| Â· Â· Â· |xm|
Î“(1 + m
p )
2mÎ“(1 + 1
p)m
! 1
m
â‰‚mâˆ’1+ 1
p âˆ’Î±.
(3.12)
For 1 â‰¤k â‰¤n, Lemma 3.2 implies that
Îµ2nâˆ’k(Kk)â„“p â‰²2âˆ’(n+k)Î±Îµ2nâˆ’k
 B1
2n+k

â„“p
â‰²2âˆ’(n+k)Î±âˆš
k2(âˆ’1+ 1
p )(nâˆ’k)
â‰²
âˆš
k2âˆ’k(Î±âˆ’1)2âˆ’n(Î±+1âˆ’1
p ).
As a result, we obtain the following bound
n
X
k=1
Îµ2nâˆ’k(Kk)â„“p â‰²
âˆ
X
k=1
âˆš
k2âˆ’k(Î±âˆ’1)
2âˆ’n(Î±+1âˆ’1
p ) â‰²2âˆ’n(Î±+1âˆ’1
p ).
(3.13)
Since the â„“p distance from 0 to the elements of Kâˆis less than 2âˆ’2nÎ±, one can
simply estimate Îµ2n(Kâˆ)â„“p by
Îµ2n(Kâˆ)â„“p â‰²2âˆ’2nÎ±.
(3.14)
Therefore combining (3.12), (3.13) and (3.14), we conï¬rm (3.10) and thus
Îµn(K)â„“p â‰²nâˆ’Î±âˆ’1+ 1
p
for n â‰¥1.
(3.15)
Finally (3.8) with p â‰¥2 follows from Theorem 3.1 and (3.9), (3.15).

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
16
Y. Li & J. W. Siegel
4. Convergence analysis of OGAs
Let X be a real Hilbert space and D âŠ‚X be a ï¬xed dictionary or collection
of functions. To analyze the convergence behavior of the OGA, we consider the
following norm and space:4, 23
âˆ¥fâˆ¥L1(D) := inf
ï£±
ï£²
ï£³
X
i
|ci| : f =
X
giâˆˆD
cigi
ï£¼
ï£½
ï£¾,
L1(D) :=

f âˆˆX : âˆ¥fâˆ¥L1(D) < âˆ
	
.
Here âˆ¥fâˆ¥L1(D) is sometimes known as the variation of f with respect to D in the
classical literature (see Ref. 39). For the OGA 1.3 with f âˆˆL1(D) and âˆ¥Dâˆ¥:=
supgâˆˆD âˆ¥gâˆ¥â‰¤1, the classical error estimate (see Refs. 23 and 4) reads
âˆ¥f âˆ’fnâˆ¥â‰¤âˆ¥fâˆ¥L1(D)(n + 1)âˆ’1
2 .
(4.1)
Recently, the work56 developed a rate-optimal convergence estimate of the OGA
based on the entropy numbers of co(D). For example, convergence of the OGA based
on a suï¬ƒciently regular dictionary like Pd
k would be much faster than O(nâˆ’1
2 ).
Theorem 4.1 (Theorem 1 from Ref. 56). Let f âˆˆL1(D) and t > 0. For
Algorithm 1.3 it holds that
Îµn(co(D)) â‰²nâˆ’1
2 âˆ’t =â‡’âˆ¥f âˆ’fnâˆ¥â‰²âˆ¥fâˆ¥L1(D)nâˆ’1
2 âˆ’t.
4.1. Direct comparison estimate
Although the OGA and the greedy-type RBM are not closely related research areas,
we show that the main lemma 2.1 used in the analysis of greedy RBMs can be used to
derive a new type of clean and transparent direct comparison between âˆ¥f âˆ’fnâˆ¥and
Îµn(co(D)). It turns out that our analysis greatly simpliï¬es the argument in Ref. 56
and leads to an improvement of Theorem 4.1. To this end, we need the following
upper bound for recursively related sequences. We note that an equivalent result
has previously appeared in a slightly diï¬€erent form in the literature, see Lemma
3.1 in Ref. 62 or Lemma 3.4 in Ref. 23. For the readers convenience we provide the
complete proof here.
Lemma 4.1. Let {an}nâ‰¥0 and {b}nâ‰¥1 be non-negative sequences satisfying
an â‰¤anâˆ’1(1 âˆ’bnanâˆ’1) for n â‰¥1.
(4.2)
and b0 = 1/a0. Then we have
an â‰¤
1
b0 + b1 + Â· Â· Â· + bn
.
Proof. We prove the lemma by induction. First we note {an}nâ‰¥0 is a decreasing
sequence. By deï¬nition we have a0 â‰¤1/b0 for n = 0. For the time being assume
anâˆ’1 â‰¤
1
b0 + Â· Â· Â· + bnâˆ’1
.
(4.3)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
17
If anâˆ’1 â‰¤
1
b0+Â·Â·Â·+bn , then the monotonicity of {an}nâ‰¥1 implies
an â‰¤anâˆ’1 â‰¤
1
b0 + Â· Â· Â· + bn
.
If anâˆ’1 >
1
b0+Â·Â·Â·+bn , then by (4.2) and the induction assumption (4.3), we obtain
an â‰¤
1
b0 + Â· Â· Â· + bnâˆ’1

1 âˆ’
bn
b0 + Â· Â· Â· + bn

=
1
b0 + Â· Â· Â· + bn
.
Therefore the induction is complete and Lemma 4.1 is true.
Theorem 4.2. For f âˆˆL1(D) and Algorithm 1.3 we have
âˆ¥f âˆ’fnâˆ¥â‰¤(n!Vn)
1
n
âˆšn
âˆ¥fâˆ¥L1(D)Îµn(co(D)).
(4.4)
Proof. Let rn = f âˆ’fn in Algorithm 1.3. Following the analysis in Ref. 56 (see
also the analysis in Section 3 of Ref. 31), we have
âˆ¥rnâˆ¥2 â‰¤
rnâˆ’1 âˆ’âŸ¨rnâˆ’1, gn âˆ’Pnâˆ’1gnâŸ©
âˆ¥gn âˆ’Pnâˆ’1gnâˆ¥2
(gn âˆ’Pnâˆ’1gn)

2
= âˆ¥rnâˆ’1âˆ¥2 âˆ’|âŸ¨rnâˆ’1, gn âˆ’Pnâˆ’1gnâŸ©|2
âˆ¥gn âˆ’Pnâˆ’1gnâˆ¥2
.
(4.5)
We remark âˆ¥gn âˆ’Pnâˆ’1gnâˆ¥Ì¸= 0 otherwise gn âˆˆXnâˆ’1 = span{g1, . . . , gnâˆ’1} and
arg maxgâˆˆD |âŸ¨rnâˆ’1, gâŸ©| = |âŸ¨rnâˆ’1, gnâŸ©| = 0, which implies rnâˆ’1 = 0 and termination
of the OGA. On the other hand, using rnâˆ’1 âŠ¥Xnâˆ’1 we have
âˆ¥rnâˆ’1âˆ¥2 = âŸ¨f, rnâˆ’1âŸ©â‰¤âˆ¥fâˆ¥L1(D)|âŸ¨rnâˆ’1, gnâŸ©|
= âˆ¥fâˆ¥L1(D)|âŸ¨rnâˆ’1, gn âˆ’Pnâˆ’1gnâŸ©|.
(4.6)
Therefore combining (4.5) and (4.6) leads to
âˆ¥rnâˆ¥2 â‰¤âˆ¥rnâˆ’1âˆ¥2 âˆ’
âˆ¥rnâˆ’1âˆ¥4
âˆ¥fâˆ¥2
L1(D)âˆ¥gn âˆ’Pnâˆ’1gnâˆ¥2 .
(4.7)
With the notation
an = âˆ¥f âˆ’fnâˆ¥2/âˆ¥fâˆ¥2
L1(D),
bn = âˆ¥gn âˆ’Pnâˆ’1gnâˆ¥âˆ’2,
the inequality (4.7) reduces to the recurrence relation
an â‰¤anâˆ’1(1 âˆ’bnanâˆ’1).
(4.8)
By the deï¬nition of âˆ¥fâˆ¥L1(D) we have a0 â‰¤1. It then follows from (4.8) and Lemma
4.1 that
an â‰¤
1
1 + b1 + Â· Â· Â· + bn
.
(4.9)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
18
Y. Li & J. W. Siegel
Using (4.9), a mean value inequality and the key Lemma 2.1, we obtain
an â‰¤
1
1 + n(b1 Â· Â· Â· bn)
1
n
=
1
1 + n
  Qn
k=1 âˆ¥gk âˆ’Pkâˆ’1gkâˆ¥
âˆ’2
n
â‰¤
1
1 + n(nVn)âˆ’2
n Îµn(co(D))âˆ’2
â‰¤(nVn)
2
n
n
Îµn(co(D))2.
The proof is complete.
We remark that the classical Theorem 4.1 is valid under an asymptotic assump-
tion while Theorem 4.2 is a non-asymptotic and unconditional estimate. In addition,
the constant in the upper bound of (4.4) as n â†’âˆbehaves like
lim
nâ†’âˆ
(n!Vn)
1
n
âˆšn
=
r
2Ï€
e .
Therefore Theorem 4.2 yields
âˆ¥f âˆ’fnâˆ¥â‰²âˆ¥fâˆ¥L1(D)Îµn(co(D)),
which is indeed stronger than Theorem 4.1.
4.2. Best n-term approximation
Consider the set of n-sparse elements
Î£n(D) =
n
n
X
i=1
ciËœgi : ci âˆˆR, Ëœgi âˆˆD, i = 1, . . . , n
o
.
It is natural to compare the error of greedy algorithms with the best approximation
error
En(f, D) =
inf
gâˆˆÎ£n(D) âˆ¥f âˆ’gâˆ¥
by n-sparse elements. When D is an orthonormal set in X, the equality âˆ¥f âˆ’fnâˆ¥=
En(f, D) automatically holds (see Ref. 64). In general, such convergence analysis
using En(f, D) often requires near orthogonality assumption on D. For example, on
a M-coherent dictionary D, the following Lebesgue inequality is true (see Section
2.6 of Ref. 64):
âˆ¥f âˆ’fâŒŠn log nâŒ‹âˆ¥â‰¤c1En(f, D),
for n â‰¤c2M âˆ’2
3 with c1, c2 being explicit constants. We also refer to Ref. 61 for
Lebesgue-type inequality of generalized OGAs in Banach spaces. On the other hand,

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
19
the entropy-based estimate (4.4) applies to any normalized dictionary in Hilbert
spaces.
In addition, we note that Theorem 4.4 yields the following direct comparison
between En(f, D) and Îµn(co(D)):
En(f, D) â‰¤(n!Vn)
1
n
âˆšn
âˆ¥fâˆ¥L1(D)Îµn(co(D)),
(4.10a)
En(D) :=
sup
fâˆˆco(D)
En(f, D) â‰¤(n!Vn)
1
n
âˆšn
Îµn(co(D)).
(4.10b)
When the number |D| of elements in D is ï¬nite, an indirect comparison between
En(D) and Îµn(co(D)) in the converse direction of (4.10b) can be found in Section
7.4 of Ref. 61. In particular, combining Theorem 7.4.3 of Ref. 61 with Theorem 4.4,
we obtain the following novel comparison between the convergence rate of En(D)
and the error of OGAs whose target functions belonging to co(D).
Corollary 4.1. Assume there exists a constant s > 0 such that
En(D) â‰²nâˆ’s,
n â‰¤|D|,
where the cardinality |D| is ï¬nite. Then for fn generated by the OGA (Algorithm
1.3) with n â‰¤|D|, we have
sup
fâˆˆco(D)
âˆ¥f âˆ’fnâˆ¥â‰¤C(s)(log(2|D|/n))snâˆ’s,
where C(s) is a constant depending only on s.
We note that the above result is independent of the coherence property of D.
Moreover, Corollary 4.1 is not a Lebesgue-type inequality but is in a similar fashion
to the classical n-width based convergence estimate (1.1) for the RBM. It conï¬rms
that the OGA collectively achieves the best n-term approximation rate up to a
logarithmic factor on ï¬nite dictionaries.
4.3. General target functions
In general, if f is not contained in L1(D), one can use the next corollary to estimate
the error of the OGA.
Corollary 4.2. For all f âˆˆX and any h âˆˆL1(D), Algorithm 1.3 satisï¬es
âˆ¥f âˆ’fnâˆ¥2 â‰¤âˆ¥f âˆ’hâˆ¥2 + 4(n!Vn)
2
n
n
âˆ¥hâˆ¥2
L1(D)Îµn(co(D))2.
Proof. We use the same notation as in the proof of Theorem 4.2. We note (4.5)
still holds and
âˆ¥rnâˆ’1âˆ¥2 = âŸ¨h, rnâˆ’1âŸ©+ âŸ¨f âˆ’h, rnâˆ’1âŸ©
â‰¤âˆ¥hâˆ¥L1(D)|âŸ¨rnâˆ’1, gn âˆ’Pnâˆ’1gnâŸ©| + âˆ¥f âˆ’hâˆ¥âˆ¥rnâˆ’1âˆ¥.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
20
Y. Li & J. W. Siegel
Then combining the above inequality and a mean value inequality yields
1
2
 âˆ¥rnâˆ’1âˆ¥2 âˆ’âˆ¥f âˆ’hâˆ¥2
â‰¤âˆ¥hâˆ¥L1(D)|âŸ¨rnâˆ’1, gn âˆ’Pnâˆ’1gnâŸ©|.
(4.11)
Let cn = (âˆ¥rnâˆ¥2 âˆ’âˆ¥f âˆ’hâˆ¥2)/(4âˆ¥hâˆ¥2
L1(D)). Using (4.5) and (4.11) we obtain
cn â‰¤cnâˆ’1(1 âˆ’bncnâˆ’1).
The rest of the proof is same as Theorem 4.2.
For t > 0 and g âˆˆX the K-functional
K(t, g) =
inf
hâˆˆL1(D)
 âˆ¥g âˆ’hâˆ¥X + tâˆ¥hâˆ¥L1(D)

measures how well the element of X can be approximated by L1(D) with an ap-
proximant of small L1(D) norm (see Refs. 6, 22 and 9). For the index pair (Î¸, âˆ)
with Î¸ âˆˆ(0, 1), the interpolation norm and space between X and L1(D) are
âˆ¥gâˆ¥Î¸ = âˆ¥gâˆ¥[X,L1(D)]Î¸,âˆ:=
sup
0<t<âˆ
tâˆ’Î¸K(t, g),
XÎ¸ =

g âˆˆX : âˆ¥gâˆ¥Î¸ < âˆ
	
,
respectively. As in Refs. 4 and 56, we also obtain an explicit and improved error
estimate of the OGA in terms of the interpolation norm âˆ¥fâˆ¥Î¸.
Proposition 4.1. For all f âˆˆXÎ¸ and Î¸ âˆˆ(0, 1) Algorithm 1.3 satisï¬es
âˆ¥f âˆ’fnâˆ¥â‰¤2Î¸ (n!Vn)
Î¸
n
n
Î¸
2
âˆ¥fâˆ¥Î¸Îµn(co(D))Î¸.
Proof. By Corollary 4.2, we obtain for all h âˆˆL1(D)
âˆ¥f âˆ’fnâˆ¥â‰¤âˆ¥f âˆ’hâˆ¥+ 2(n!Vn)
1
n
âˆšn
âˆ¥hâˆ¥L1(D)Îµn(co(D)).
As a result, setting t = 2 (n!Vn)
1
n
âˆšn
Îµn(co(D)), we have
âˆ¥f âˆ’fnâˆ¥â‰¤K(t, f) â‰¤tÎ¸âˆ¥fâˆ¥Î¸
and complete the proof.
5. Concluding Remarks
We have derived direct and optimal convergence estimates of the greedy-type RBM
and the OGA based on Kolmogorovâ€™s entropy numbers. Constants in our upper
bounds are explicit and simple. A future research direction is to extend our analysis
to POD-greedy algorithms33, 34 for time-dependent parametric PDEs, which is a
combination of greedy algorithms and POD for temporal compression of snapshots.
In addition, the generalization of the OGA in Banach spaces is called the Chebyshev
Greedy Algorithm (CGA).18, 63, 64 It would also be interesting to investigate if our
analysis in Section 4 and Lemma 3.1 can be applied to the CGA.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
21
Acknowledgements
The authors would like to thank the two anonymous referees for constructive com-
ments that improve the quality of the manuscript. Subsection 4.2 is partially due
to the bibliographic suggestion61 of one referee.
The work of Li was partially supported by the Fundamental Research Funds
for the Central Universities 226-2023-00039. Siegel was supported by the National
Science Foundation (DMS-2111387 and CCF-2205004).
References
1. A. Ammar, B. Mokdad, F. Chinesta and R. Keunings, A new family of solvers for
some classes of multidimensional partial diï¬€erential equations encountered in kinetic
theory modeling of complex ï¬‚uids, Journal of non-Newtonian ï¬‚uid Mechanics 139
(2006) 153â€“176.
2. I. BabuË‡ska and W. C. Rheinboldt, Error estimates for adaptive ï¬nite element compu-
tations, SIAM J. Numer. Anal. 15 (1978) 736â€“754.
3. M. Barrault, Y. Maday, N. C. Nguyen and A. T. Patera, An â€˜empirical interpola-
tionâ€™ method: application to eï¬ƒcient reduced-basis discretization of partial diï¬€erential
equations, C. R. Math. Acad. Sci. Paris 339 (2004) 667â€“672.
4. A. R. Barron, A. Cohen, W. Dahmen and R. A. DeVore, Approximation and learning
by greedy algorithms, Ann. Statist. 36 (2008) 64â€“94.
5. P. Benner, S. Gugercin and K. Willcox, A survey of projection-based model reduction
methods for parametric dynamical systems, SIAM Rev. 57 (2015) 483â€“531.
6. J. Bergh and J. LÂ¨ofstrÂ¨om, Interpolation spaces. An introduction, Grundlehren der
Mathematischen Wissenschaften, No. 223 (Springer-Verlag, Berlin-New York, 1976).
7. P. Binev, A. Cohen, W. Dahmen, R. DeVore, G. Petrova and P. Wojtaszczyk, Con-
vergence rates for greedy algorithms in reduced basis methods, SIAM J. Math. Anal.
43 (2011) 1457â€“1472.
8. A. Bonito, R. A. DeVore and R. H. Nochetto, Adaptive ï¬nite element methods for
elliptic problems with discontinuous coeï¬ƒcients, SIAM J. Numer. Anal. 51 (2013)
3106â€“3134.
9. S. C. Brenner and L. R. Scott, The mathematical theory of ï¬nite element methods,
volume 35 of Texts in Applied Mathematics, 15 (Springer, New York, 2008), 3 edition.
10. A. Buï¬€a, Y. Maday, A. T. Patera, C. Prudâ€™homme and G. Turinici, A priori con-
vergence of the greedy algorithm for the parametrized reduced basis method, ESAIM
Math. Model. Numer. Anal. 46 (2012) 595â€“603.
11. B. Carl, Entropy numbers, s-numbers, and eigenvalue problems, J. Functional Anal-
ysis 41 (1981) 290â€“306.
12. B. Carl, I. Kyrezi and A. Pajor, Metric entropy of convex hulls in Banach spaces, J.
London Math. Soc. (2) 60 (1999) 871â€“896.
13. J. M. Cascon, C. Kreuzer, R. H. Nochetto and K. G. Siebert, Quasi-optimal conver-
gence rate for an adaptive ï¬nite element method, SIAM J. Numer. Anal. 46 (2008)
2524â€“2550.
14. A. Cohen, W. Dahmen and R. DeVore, Adaptive wavelet methods for elliptic operator
equations: convergence rates, Math. Comp. 70 (2001) 27â€“75.
15. A. Cohen and R. DeVore, Approximation of high-dimensional parametric PDEs, Acta
Numer. 24 (2015) 1â€“159.
16. A. Cohen and R. DeVore, Kolmogorov widths under holomorphic mappings, IMA J.
Numer. Anal. 36 (2016) 1â€“12.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
22
Y. Li & J. W. Siegel
17. G. Davis, S. Mallat and M. Avellaneda, Adaptive greedy approximations, Constr.
Approx. 13 (1997) 57â€“98.
18. A. V. Dereventsov and V. N. Temlyakov, A uniï¬ed way of analyzing some greedy
algorithms, J. Funct. Anal. 277 (2019) 108286, 30.
19. R. DeVore, G. Petrova and P. Wojtaszczyk, Greedy algorithms for reduced bases in
Banach spaces, Constr. Approx. 37 (2013) 455â€“466.
20. R. A. DeVore, Nonlinear approximation, in Acta numerica, 1998 (Cambridge Univ.
Press, Cambridge, 1998), volume 7 of Acta Numer., pp. 51â€“150.
21. R. A. DeVore, The theoretical foundation of reduced basis methods, in Model Reduc-
tion and Approximation (SIAM Computational Science & Engineering) (2014).
22. R. A. DeVore and G. G. Lorentz, Constructive approximation, volume 303 of
Grundlehren der mathematischen Wissenschaften [Fundamental Principles of Mathe-
matical Sciences] (Springer-Verlag, Berlin, 1993).
23. R. A. DeVore and V. N. Temlyakov, Some remarks on greedy algorithms, Adv. Com-
put. Math. 5 (1996) 173â€“187.
24. P. G. L. Dirichlet, Sur une nouvelle mÂ´ethode pour la dÂ´etermination des intÂ´egrales
multiples, Journal de MathÂ´ematiques Pures et AppliquÂ´ees 4 (1839) 164â€“168.
25. P. Edel and Y. Maday, Dual natural-norm a posteriori error estimators for reduced
basis approximations to parametrized linear equations, Math. Models Methods Appl.
Sci. .
26. L. E. Figueroa and E. SÂ¨uli, Greedy approximation of high-dimensional Ornsteinâ€“
Uhlenbeck operators, Foundations of Computational Mathematics 12 (2012) 573â€“623.
27. J. H. Friedman, Greedy function approximation: a gradient boosting machine, Ann.
Statist. 29 (2001) 1189â€“1232.
28. J. H. Friedman and W. Stuetzle, Projection pursuit regression, Journal of the Amer-
ican statistical Association 76 (1981) 817â€“823.
29. T. Gantumur, H. Harbrecht and R. Stevenson, An optimal adaptive wavelet method
without coarsening of the iterands, Math. Comp. 76 (2007) 615â€“629.
30. F. Gao, Metric entropy of convex hulls, Israel J. Math. 123 (2001) 359â€“364.
31. Y. Gao, T. Qian, L.-F. Cao and V. Temlyakov, Aspects of 2D-adaptive Fourier de-
compositions, arXiv preprint arXiv:1710.09277.
32. P. Grisvard, Singularities in boundary value problems, Research in Applied Mathe-
matics, 22 (Springer-Verlag, Berlin, 1992).
33. B. Haasdonk, Convergence rates of the POD-greedy method, ESAIM Math. Model.
Numer. Anal. 47 (2013) 859â€“873.
34. B. Haasdonk and M. Ohlberger, Reduced basis method for ï¬nite volume approxima-
tions of parametrized linear evolution equations, M2AN Math. Model. Numer. Anal.
42 (2008) 277â€“302.
35. J. S. Hesthaven, G. Rozza and B. Stamm, Certiï¬ed reduced basis methods for
parametrized partial diï¬€erential equations, SpringerBriefs in Mathematics (Springer,
Cham; BCAM Basque Center for Applied Mathematics, Bilbao, 2016), bCAM
SpringerBriefs.
36. L. K. Jones, On a conjecture of Huber concerning the convergence of projection pursuit
regression, The Annals of statistics (1987) 880â€“882.
37. L. K. Jones, A simple lemma on greedy approximation in Hilbert space and conver-
gence rates for projection pursuit regression and neural network training, Ann. Statist.
20 (1992) 608â€“613.
38. A. N. Kolmogorov and V. M. Tihomirov, Îµ-entropy and Îµ-capacity of sets in functional
space, Amer. Math. Soc. Transl. (2) 17 (1961) 277â€“364.
39. V. KurkovÂ´a and M. Sanguineti, Bounds on rates of variable-basis and neural-network

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
23
approximation, IEEE Trans. Inform. Theory 47 (2001) 2659â€“2665.
40. C. Le Bris, T. Lelievre and Y. Maday, Results and questions on a nonlinear approxima-
tion approach for solving high-dimensional partial diï¬€erential equations, Constructive
Approximation 30 (2009) 621â€“651.
41. Y. Li, Some convergence and optimality results of adaptive mixed methods in ï¬nite
element exterior calculus, SIAM J. Numer. Anal. 57 (2019) 2019â€“2042.
42. Y. Li, Quasi-optimal adaptive mixed ï¬nite element methods for controlling natural
norm errors, Math. Comp. 90 (2021) 565â€“593.
43. E. D. Livshits, Lower bounds for the rate of convergence of greedy algorithms,
Izvestiya: Mathematics 73 (2009) 1197.
44. G. G. Lorentz, Metric entropy, widths, and superpositions of functions, Amer. Math.
Monthly 69 (1962) 469â€“485.
45. G. G. Lorentz, M. v. Golitschek and Y. Makovoz, Constructive approximation, volume
304 of Grundlehren der mathematischen Wissenschaften [Fundamental Principles of
Mathematical Sciences] (Springer-Verlag, Berlin, 1996), advanced problems.
46. Y. Maday, Reduced basis method for the rapid and reliable solution of partial diï¬€er-
ential equations, in International Congress of Mathematicians. Vol. III (Eur. Math.
Soc., ZÂ¨urich, 2006), pp. 1255â€“1270.
47. Y. Maday, A. T. Patera and G. Turinici, A priori convergence theory for reduced-
basis approximations of single-parameter elliptic partial diï¬€erential equations, J. Sci.
Comput. 17 (2002) 437â€“446.
48. S. G. Mallat and Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE
Transactions on Signal Processing 41 (1993) 3397â€“3415.
49. P. Morin, K. G. Siebert and A. Veeser, A basic convergence result for conforming
adaptive ï¬nite elements, Math. Models Methods Appl. Sci. 18 (2008) 707â€“737.
50. Y. Pati, R. Rezaiifar and P. Krishnaprasad, Orthogonal matching pursuit: Recursive
function approximation with applications to wavelet decomposition, in Proceedings of
27th Asilomar conference on signals, systems and computers (1993), pp. 40â€“44. IEEE.
51. G. Petrova, Rescaled pure greedy algorithm for Hilbert and Banach spaces, Applied
and Computational Harmonic Analysis 41 (2016) 852â€“866.
52. A. Quarteroni, A. Manzoni and F. Negri, Reduced basis methods for partial diï¬€er-
ential equations, volume 92 of Unitext (Springer, Cham, 2016), an introduction, La
Matematica per il 3+2.
53. G. Rozza, D. B. P. Huynh and A. T. Patera, Reduced basis approximation and a
posteriori error estimation for aï¬ƒnely parametrized elliptic coercive partial diï¬€erential
equations: application to transport and continuum mechanics, Arch. Comput. Methods
Eng. 15 (2008) 229â€“275.
54. C. SchÂ¨utt, Entropy numbers of diagonal operators between symmetric Banach spaces,
J. Approx. Theory 40 (1984) 121â€“128.
55. J. W. Siegel, Q. Hong, X. Jin, W. Hao and J. Xu, Greedy training algorithms for
neural networks and applications to pdes, J. Comput. Phys. 484 (2023) 112084.
56. J. W. Siegel and J. Xu, Optimal convergence rates for the orthogonal greedy algorithm,
IEEE Trans. Inform. Theory 68 (2022) 3354â€“3361.
57. J. W. Siegel and J. Xu, Sharp bounds on the approximation rates, metric entropy,
and n-widths of shallow neural networks, Found. Comput. Math. .
58. A. Silâ€™nichenko, Rate of convergence of greedy algorithms, Mathematical Notes 76
(2004) 582â€“586.
59. I. Steinwart, Entropy of C(K)-valued operators, J. Approx. Theory 103 (2000) 302â€“
328.
60. V. Temlyakov, Greedy approximation, volume 20 of Cambridge Monographs on Applied

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
24
Y. Li & J. W. Siegel
and Computational Mathematics (Cambridge University Press, Cambridge, 2011).
61. V. Temlyakov, Multivariate approximation, volume 32 of Cambridge Monographs on
Applied and Computational Mathematics (Cambridge University Press, Cambridge,
2018).
62. V. N. Temlyakov, Weak greedy algorithms, Adv. Comput. Math. 12 (2000) 213â€“227.
63. V. N. Temlyakov, Greedy algorithms in Banach spaces, Adv. Comput. Math. 14 (2001)
277â€“292.
64. V. N. Temlyakov, Greedy approximation, Acta Numer. 17 (2008) 235â€“409.
65. X. Wang, Volumes of generalized unit balls, Mathematics Magazine 78 (2005) 390â€“
395.
66. P. Wojtaszczyk, Banach spaces for analysts, volume 25 of Cambridge Studies in Ad-
vanced Mathematics (Cambridge University Press, Cambridge, 1991).
67. P. Wojtaszczyk, On greedy algorithm approximating Kolmogorov widths in Banach
spaces, J. Math. Anal. Appl. 424 (2015) 685â€“695.

