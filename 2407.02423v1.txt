On the Anatomy of Attention
Nikhil Khatri∗
Tuomas Laakkonen∗
Jonathon Liu∗
Vincent Wang-Maścianica∗‡
Compositional Intelligence, Quantinuum
17 Beaumont St., Oxford OX1 2NA, UK
{nikhil.khatri,tuomas.laakkonen,jonathon.liu,vincent.wang}@quantinuum.com
‡Department of Computer Science, University of Oxford
7 Parks Rd, Oxford OX1 3QG, UK
Abstract
We introduce a category-theoretic diagrammatic formalism in order to systematically relate and
reason about machine learning models. Our diagrams present architectures intuitively but without
loss of essential detail, where natural relationships between models are captured by graphical
transformations, and important differences and similarities can be identified at a glance. In this
paper, we focus on attention mechanisms: translating folklore into mathematical derivations, and
constructing a taxonomy of attention variants in the literature. As a first example of an empirical
investigation underpinned by our formalism, we identify recurring anatomical components of
attention, which we exhaustively recombine to explore a space of variations on the attention
mechanism.
1
Introduction
Many efforts have been directed towards providing an overarching framework for different deep
learning (DL) architectures, which have gained interest in light of the massive proliferation of
variants of the transformer [Vaswani et al., 2017]. These frameworks have come in various forms,
such as taxonomic surveys [Lin et al., 2022] that organise certain groups of architectures, and more
prescriptive high-level conceptual characterisations [Bronstein et al., 2021]. However, taxonomies
risk arbitrariness in their organising criteria and theoretical frameworks risk abstracting away
important practical details. The ideal would be to build taxonomies and organising theories starting
from the precise computational descriptions of architectures as they occur “in the wild”. How
to systematically notate and communicate these computational descriptions in a human-friendly
manner is a question as old as computer science [Goldstine and von Neumann, 1947], and the tried-
and-true solution remains the same even today: flowcharts. A mathematical fact that deserves to
be better known is that flowcharts, of the same sort that are customary in DL papers introducing
architectures, are often already formal representations with unambiguous (but implicit) semantics
in the mathematical setting of smooth functions between Euclidean spaces and their sequential and
parallel composites [Selinger, 2010]. Even setting aside the formality of flowcharts, in our view there
remain two fatal drawbacks from the practitioner’s perspective that, to the best of our knowledge,
every graphical notational system for DL currently on offer suffers from.
∗Equal contribution
1
arXiv:2407.02423v1  [cs.LG]  2 Jul 2024

First, there is an inescapable tension between formal detail and the vantage point of abstraction
that, for example, allows the practitioner to intuitively grasp a novel architecture and appreciate
its conceptual differences from its predecessors. In practice this problem manifests as a dichotomy
between overly informal presentations of architectures such as high-level flowcharts, with the only
detailed alternatives being overly detailed prose or pseudocode. This problem cannot be solved
by simply increasing the formal detail of a notational system: consider that in the limit, one could
describe a transformer in GPU assembly language with perfect formal fidelity, but such a description
would be completely unilluminating.
Second, even if one has a notational system for ML that is suitably formal and at the desired level
of detail, without a rewrite system, one has to step outside the notation to reason about its contents
with other means. This problem cannot fundamentally be solved by any amount of descriptive
adequacy for individual architectures that a notational system might have, without additional
machinery to compare representations. All notational systems we know of for DL provide no
framework for relating the computational structure of models outside of, at best, what is already
granted by the equational theory in which they are grounded. Consider that from the practitioner’s
perspective, it is informative to know when two architectures only differ by “introducing a residual
around a particular subprocess” e.g., but this would make the architectures mathematically unequal,
and hence formally incomparable in purely equational theories, such as that of continuous maps
between Euclidean spaces.
We introduce string diagrams1 for reasoning about arbitrary DL architectures, which we
demonstrate in this paper in the context of attention mechanisms. Our notation is formally grounded
in category theory and is similar in intent to the systems of [Abbott, 2024] and [Taylor, 2024], but the
crucial difference is that we address the two aforementioned issues. First, we solve the problem of
trading-off between formal detail and visual abstraction by making all the choices simultaneously:
we provide multiple abstraction levels that our formalism allows us to move between freely. This
allows us to rise from the code-level formality suited for computers (Example 2.1), to an “engineer’s
perspective” suited for practitioners (Example 2.2). The novel mathematical machinery that enables
this (elaborated in Appendix A.1) is the ability to formally decorate string diagrams (in symmetric
monoidal categories) with instructions for reiteration of the same processes repeatedly in parallel —
similarly to box notation for iterated variables in plate notation [Buntine, 1994] — which allows us to
compactly represent computation with tensors. In the language of high-performance computing,
this is called Single Instruction (applied to) Multiple Data (SIMD), which is particularly germane
to transformers since parallelisation is one of the primary enablers of modern ML performance
[Goodfellow et al., 2016, Pandey et al., 2022].
Second, we address the problem of formal expressivity by introducing a rewrite system based
on a minimal and principled basis: the Universal Approximation Theorem [Cybenko, 1989], which
may safely be assumed to hold for even relatively shallow feedforward networks [Ismailov, 2023].
Fixed input-output universal function approximators may be formally viewed as the functional
equivalent of variables waiting to be assigned concrete values, and so they admit a theory of
rewrites that visually amounts to graphically substituting a universal approximator with any other
composite function one likes — even a composite that itself contains universal approximators. The
categorical machinery that underpins the semantics of universal approximators as “holes to be filled"
is elaborated in Appendix A.2, along with a proof of coherence in Appendix A.3 that guarantees the
1String diagrams are a diagrammatic syntax that have been used to formally represent and reason about linear and
affine algebra [Sobociński, 2015, Bonchi et al., 2017, Bonchi et al., 2019], first order logic [Haydon and Sobociński, 2020],
causal
models
[Lorenz and Tull, 2023,
Jacobs et al., 2019],
signal
flow
graphs
[Bonchi et al., 2014],
electrical
cir-
cuits
[Boisseau and Sobociński, 2022],
game
theory
[Hedges, 2015],
petri
nets
[Baez and Master, 2020],
probabil-
ity
theory
[Fritz et al., 2021],
formal
linguistics
[Coecke et al., 2010,
Wang-Mascianica et al., 2023],
quantum
the-
ory
[Coecke and Kissinger, 2017,
Poór et al., 2023],
and
aspects
of
machine
learning
such
as
backpropagation
[Cruttwell et al., 2022].
2

well-definedness and well-behaviour of the parallelism notation with the non-equality universal
approximator rewrites. What this buys us is the ability to continue abstracting from the engineering
perspective to the “scientist’s perspective” where we demonstrate that we can formally retrace the
folkloric “evolutionary process” of architectures such as the development of the vanilla transformer
[Vaswani et al., 2017] from its precursor [Bahdanau et al., 2016], the subsequent derivation of linear
transformers (Section 3), and ultimately accommodate many architectures at once in a family tree
(Section 4).
We demonstrate the potential of our graphical perspective by posing and testing a question that
we stand in a unique position to answer: is the structure of an attention mechanism important for its
performance? In Section 5 we gather a family of commonly recurring anatomical components among
transformer variants, which we are able to obtain by direct inspection of many architectures. We then
exhaustively enumerate the possible composites for up to five such components and use our rewrite
system to reduce trivial overparameterisations, thus obtaining 14 distinct attention mechanisms
including the linear transformer [Wang et al., 2020] and the classic [Vaswani et al., 2017]. We answer
in the negative: the structure of the attention mechanism does not seem to affect its performance on
a representative task.
2
Formally depicting architectures
For our diagrams, we consider semantics in what is essentially the cartesian monoidal category of
Euclidean spaces (viewed as vector spaces) and continuous functions between them, elaborated
in Definition A.36, and Remark A.37 for the immediate extension to the probabilistic setting. For
now we are content with an accessible presentation. Wires represent spaces, and boxes represent
functions. We read our diagrams from left to right, where sequential composition is the usual
function composition, and parallel composition of spaces is by cartesian product. We include (for all
spaces) the identity function, copy-maps, deletions, and swapping in our basic stock of diagrams.
f
RM
RN
f
(g ◦f) : RA→RC
Sequential composition
g
f : RM →RN
Function
f
g
(f ⊕g) : R(A+C)→R(B+D)
Parallel composition
RA
RB
RC
RA
RB
RC
RD
RM
1 : RM →RM
Identity
RM
∆: RM →R(M+M)
Copy
RM
RM
RM
RM
ϵ : RM →{⋆}
Delete
{⋆}
R0 ≃{⋆}
Trivial space
R0
θ : R(N+M)→R(M+N)
Swap
RM
RN
RM
RN
String diagrams have the appealing characteristic of ensuring that visually intuitive equivalences
in information flows correspond to symbolic derivations of behavioral equivalence. This means
that the cumbersome algebraic proofs needed to demonstrate the equality of sequentially and
parallel-composed processes are naturally handled by diagram isotopies. In diagrammatic syntax,
such isomorphisms are conventionally written as plain equalities. Interested readers are directed to
[Selinger, 2010] for more information. For example:
(1 ⊕θ) ◦(∆⊕g) ◦(f ⊕1)
≃(1 ⊕θ) ◦(1 ⊕1 ⊕g) ◦(∆⊕1) ◦(f ⊕1)
[Identity, interchange]
≃(1 ⊕g ⊕1) ◦(1 ⊕θ) ◦(∆⊕1) ◦(f ⊕1)
[Braid naturality]
≃(1 ⊕g ⊕1) ◦(1 ⊕θ) ◦(f ⊕f ⊕1) ◦(∆⊕1)
[Copy naturality]
≃(1 ⊕g ⊕1) ◦(f ⊕1 ⊕f) ◦(1 ⊕θ) ◦(∆⊕1)
[Braid naturality]
⇔
f
g
≃
f
f
g
Since we work with Euclidean spaces as wires, without loss of generality, an arbitrary f : RN →
RM is equivalently expressed as a map from N to M parallel iterations of R. As syntactic sugar, we
3

indicate copies with a labelled tick under a wire.
...
...
f
f
RN
RM
R
R
R
R
N
M
≡
≜
f
N
M
Just as RN may be viewed as the space of length-N vectors taking values in R, R(J×K) may be viewed
as the space of R-valued J-by-K matrices. Generalising, we let ticks on wires indicate indices of
tensors of arbitrary rank, and we allow functions between spaces of R-valued tensors.
J
K
f
...
...
L
M
...
...
≡
f
R(J×K)
R(L×M)
≜
f
[J,K]
[L,M]
A SIMD-box is a dashed container that indicates parallel iteration of the enclosed map. By nesting
SIMD-boxes we may iterate functions elementwise over arbitrary tensors, and we allow an optional
residual to be passed along through iterates2.
J
K
...
...
f
f
f
f
≜
[J,K]
f
K
J
[J,K]
f
f
N
...
≜
f
N
N
N
f
f
...
f
N
...
...
≜
N
N
N
f
Ticks and SIMD-boxes on string diagrams suffice to articulate several common operations that we
sugar. From left to right, we share an input by copying it; we reshape tensors by rearranging their
entries, which includes reversing their order, and combining tensors by concatenation; and we
contract tensors3 along shared dimensions, which specialises to matrix multiplication for two rank-2
inputs, vector dot product for two rank-1 inputs, and scalar multiplication for two rank-0 inputs.
...
N ≜
N
f ...
f
f
N
J
K ...
...
L
M
...
...
≜
[J,K]
[L,M]
⊗
≜
[P ,Q]
[P ,R]
[Q,R]
⊗
[P ,Q]
[P ,R]
Q
R
Q
R
R
⊕
[Q,R]
P
The final notational aspect we introduce is a distinction between processes with learnable parameters
– which we depict as filled – and processes with no learnable parameters, which are depicted without
fill. Before continuing, we wish to impress that these trivial notational observations together form an
unambiguous basis to describe arbitrary architectures, and the reader is referred to the appendix for
formal semantics and comparisons to other notational systems in the literature.
2Sequential iteration with a residual is precisely the computational graph of a recurrent neural network at inference-time.
Mathematically, parallel composition is extensionally equivalent to residual-iteration with empty residual, but this is
qualitatively different computationally, due to the improved runtime afforded by data-parallelisation.
3Generally it should be clear which two indices have been contracted, but in cases with ambiguity this can be made explicit
by adding superscripts to index labels (see opening remarks in Appendix B).
4

W E
W K
j,k
W Q
j,k
[s,v]
⊗⊕
[v,dm]
vi
i ⩽s
dm
dm
dm
dm
LN*
dm
s
[s,dm]
⊗
⊗
⊗
[dm,dh]
[dm,dh]
[dm,dh]
⊗
η
s
σ
s
⊗
k ⩽h
[s,h,dh]
[s,dm]
[dm,dm]
⊗
dm
dm
dm
LN*
s
dm
[dm,df ]
w1
j
⊗
df
df
b1
j
⊕
ReLU
df
[df ,dm]
w2
j
⊗
dm
dm
b2
j
⊕
⊕
⊕
j ⩽N
W U
⊗
[dm,v]
γ
2j−1
β
2j−1
dm
dm
dm
LN*
dm
s
β
2N+1
β
2N+1
γ
2j
β
2j
W O
j
W V
j,k
s
Example 2.1 (The vanilla transformer). As defined by [Vaswani et al., 2017], a transformer (encoder)
consists of data indexed by arbitrary but fixed positive integers; the token vocabulary dimension v; the
number of attention heads h; the head dimension dh; the model dimension dm = h × dh; the feedforward
dimension df; the number of layers N; and the (supplied at inference time) sequence length s. For each
choice of integers, the training process supplies concrete values for learnable tensors; the embedding
and unembedding matrices W E ∈R(v×dm) and W U ∈R(dm×v); layer norm parameters βl, γl ∈Rdm for
l ⩽2N + 1; feedforward parameters b1
j ∈Rdf , b2
j ∈Rdm, w1
j ∈R(dm×df ), and w2
j ∈R(df ×dm) for each
layer j ⩽N; W O
j ∈R(dm×dm) for each layer j ⩽N; and famously, the value, query, and key matrices
W V
j,k, W Q
j,k, W K
j,k ∈R(dm×dh) for each attention-head k ⩽h in each layer j ⩽N. The processes and
values without learnable parameters are; the positional encoding vectors (vi)k = cos(i × 10000−k−1
dm )
for odd k and sin(i × 10000−
k
dm ) otherwise, where i is the sequence index and k ≤dm; the layer norm
LN*(v, γ, β) ≜
 v−E[v]
√
V ar(v)+ε ⊙γ + β

where ε is some small positive value; the scaling η(x) ≜
x
√dk ;
and the softmax (σ(x))k ≜
exk
P
i⩽s
exi . Since the scaling and softmax often come together in Transformers,
we use ˆσ to denote the scaled softmax.
⊗
|
|
|
|
| ⊗|
|
|
|
|
|
ˆσ
Example 2.2 (The original self-attention). Having described the transformer in detail above, we may
now restrict and abstract to an appropriate vantage point. For our purposes, three moves suffice.
First, we will mainly focus on the self-attention mechanism which is the core component of the
Transformer. Second, we will only distinguish between two kinds of positive integers, which we
notate with colours: those that are fixed by the modeller before training, and the length of the sequence
provided at inference-time by the environment. Third, without loss of generality (by definition) we
consider all processes with learnable parameters to be universal approximators. This process yields
significant simplification relative to Example 2.1
5

3
The evolution of architectures via rewrites
Definition 3.1 (Expressive Reductions). Definitionally, a universal approximator of type A →B
may, depending on its parameters, specialise to become any function of type A →B. In particular,
they may specialise to become arbitrary composites of concrete functions and even other universal
approximators. We elaborate on the formal semantics of such expressivity reductions in Appendix A.2.
Below we give an illustration of how these rewrites may be applied. In the case where f is chosen by
the rewriter to be equivalent to addition, we have added a residual.
7→
f
7→
f
g
Equipped with diagrams and rewrites between them, we now present derivations for obtaining
the Vaswani et al. self-attention mechanism, and the linearised attention mechanism proposed by
[Katharopoulos et al., 2020a] from their respective predecessor architectures. These two attention
mechanisms incorporate all the ways one can generate and combine query, key, and values, and were
thus chosen as the building blocks for our experimental component (Section 5). Detailed versions of
these derivations are in Appendices B.1, B.2, and B.3.
3.1
Bahdanau et al. to Vaswani et al.
First, we reformulate the folkloric evolution of [Vaswani et al., 2017] from the attention mechanism
introduced in [Bahdanau et al., 2016] for an encoder-decoder RNN.
|
enc
|
|
score
|
⊗
|
|
σ
The Bahdanau et al. architecture consists of a bi-RNN encoder, with an attention mechanism attached
to an RNN decoder. We have abstracted the trainable score function used to calculate the similarity
between a query (representing the current decoder output) and a key (representing an encoder
output). The original paper uses ‘additive’ attention, which was dropped in favour of a scaled dot
product in the Transformer. We also abstract away the bi-RNN as a generic encoder. In fact, we can
use reductions to turn the bi-RNN into a SIMD feedforward, which suffices for the rest of our formal
derivation.
|
enc
|
|
score
|
⊗
|
|
σ
We rewrite the RNN cell, which discards the recurrent connections. Note that discarding the
recurrent streams means we gain parallelism. In this intermediate form, we can further: shrink the
large SIMD box; split off an invertible learner from the encoder, which may be copied; and specify
the trainable score function to be a dot product with scaling, along with trainable ‘key’ and ‘query’
matrices.
⊗
|
enc
|
|
|
|
|
⊗
|
|
|
|
|
|
η
σ
This is exactly one attention head in the cross attention part of the decoder of the Vaswani et al.
Transformer. The decoder outputs, acting as queries, pay attention to the encoder outputs. Feeding
in the encoder input itself as the queries obtains self-attention as per Example 2.2.
6

3.2
Vaswani et al. to linearised attention
The linearised variant was developed from the original self-attention. The derivation of the linear
transformation proceeds from the following abstraction: forgetting that the scaled-dot product
self-attention uses a scaled dot product with a softmax, we abstractly represent it using some
unspecified similarity function.
zi =
X
j
sim(qi, kj)
P
j′ sim(qi, kj′)vj.
(1)
This yields the following diagram (see Appendix B.2 for details):
⊗
⊗
sim
|
|
|
|
|
|
|
|
|
1
1
x
⊗
|
|
|
|
(2)
We have made explicit here the process for calculating the normalisation of the attention matrix
– the 1 in the diagram is a column vector of 1’s, which sums the rows of the attention matrix. To
recover Vaswani et al., one specifies sim(x, y) = exp

x·y
√dk

.
Observe that there are three matrix multiplication operations – with respect to the sequence
length s, the first and last of these have O(s2) time complexity. The middle one is O(s), but is in
an s-SIMD, so overall the operation is also O(s2). Observe also there are wires carrying [s, s] sized
tensors – so this model is also O(s2) in space complexity. To linearize this, one first chooses the
similarity function sim(x, y) to be ϕ(x) · ϕ(y), where ϕ : Rd →Rd′ gives a kernel decomposition.
Diagrammatically, this yields:
⊗
|
|
⊗
|
|
|
|
|
|
|
1
1
x
⊗
|
|
|
ϕ
ϕ
⊗
|
(3)
Then, by unfolding the SIMD boxes in the resulting diagram, and applying three instances of the
associativity of matrix multiplication, we arrive at the following diagram, corresponding to the
computational graph of the linear transformer:
⊗
⊗
|
|
|
|
|
ϕ
ϕ
|
|
⊗
1
⊗
1
x
|
|
|
|
⊗
|
|
(4)
Since the rewrites we used were mathematical identities, this diagram computes the same function as
Diagram 3. However, its computational properties are different – we can easily see that all instances
of matrix multiplication are O(s), and similarly the wires only carry O(s) sized tensors.
7

4
Taxonomizing architectures
In Figure 1, we provide a “primordial attention” from which we derive a variety of transformers
by expressivity reductions. Here we depict three broad families, each corresponding to structural
variations on a particular part of the primordial attention mechanism. On the left are the “Linear
Transformers” which vary in the abstract similarity computation performed on keys and queries.
In the middle are “sparse attentions” that vary up to the mask, which is equivalent to variations
in an abstract preparation procedure that copies and reshapes incoming data. On the right are
“Gaussian attention” variants where the attention pattern is scaled by a Gaussian window.
Highlighted boxes indicate abstract families of functions to be instantiated by the user. For
example, the sim box denotes an abstract “similarity" to be computed. N denotes normalisation
of the attention matrix. Recall η is the scaling, σ is the softmax, and the composite ˆσ is the scaled
softmax. N generates a Gaussian window vector parameterised by the mean and standard deviation.
Grey arrows indicate expressivity rewrites, and dashed boxes indicate how child architectures relate
to their parents.
5
Empirically exploring a space of attention mechanisms
As an empirical first test of the utility of our diagrams, in this section we define, explore, and
exhaustively test a space of attention mechanisms. We select two generators from the transformer:
AttPrep and AttApply, responsible for preparing the attention matrix, and applying it to the
values matrix, respectively. We also select the analogous generators from the linear transformer
[Katharopoulos et al., 2020b]: LinAttPrep and LinAttApply, where ϕ(x) := ELU(x)+1. In addition
to these attention-related components, we require additional generators to allow for non-attentional
information interactions. We introduce a ConcatFF generator, which allows for the merging of two
data streams (of the same shape as the original input).
ConcatFF
AttPrep
AttApply
LinAttPrep
LinAttApply
|
|
|
⊗
|
|
|
|
|
⊗
|
|
|
|
|
ˆσ
|
|
|
|
ϕ
1
⊗
|
|
⊗
|
|
ϕ
⊗
|
|
|
⊗
1
x
⊗
|
|
We consider all attention mechanisms composed of at most five generators. Each diagram must
have a single sequence length wire as input and output. Arbitrarily many copies of the input
sequence are allowed, as long as all are consumed by the remaining generators in the diagrams. Even
at just five generators, this enumeration yields many potential attention heads, which is a difficult
space to evaluate numerically. To reduce the search space, we employ the following reductions,
motivated by our diagrammatic rewrite system. First, we remove all diagrams which are sequential
composites of simpler diagrams. This limits sequential composition to layers with residuals, as
in the original transformer architecture. Second, for diagrams which are equivalent up to local
applications of rewrites (5) and (6), we remove the one with more generators, which eliminates
trivial overparameterisations. This reduces the number to 14 distinct attention mechanisms, which
we display in Figure 3. See Appendix B.4 for proofs of (5) and (6).
|
|
|
|
≡
|
|
(5)
|
|
|
|
|
|
≡
|
|
|
(6)
Given the generated attention mechanisms, we constructed decoder-only Transformer-style
models from each – the construction is the same as the decoder component of [Vaswani et al., 2017],
8

⊗
|
N
m
⊙
|
|
⊗
|
|
|
|
|
sim
|
N
m
⊙
|
|
⊗
|
|
|
|
|
sim
|
N
|
|
⊗
|
|
|
sim
|
N
|
⊗
|
|
|
δ
µ
N
⊗
|
⊕
|
|
N
m
⊙
|
|
⊗
|
|
|
⊗
η
σ
|
|
|
⊗
|
⊗
ˆσ
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
⊗
|
⊗
ˆσ
|
|
|
|
|
|
|
|
||
|
prep
|
| N
|
⊗
|
|
N
⊗
⊕
|
|
|
|
|
|
|
w
c |
Local Transformer
Gaussian Transformer
⊗
|
|
|
|
N
⊗
|
|
c |
w
|
Hard-coded Gaussian
ϕ
ϕ
|
|
⊗
1
⊗
⊗
1
x
|
|
|
|
⊗
|
ϕ(x) := ELU(x) + 1
ϕiν(x) := ReLU([x, −x])iReLU([x, −x])i+ν
Linear Transformer
Delta Net
ϕ(x) := h(x)
√m [fj(ωi · x)]i≤m
j≤l
(Random Feature Map)
(Transformers are RNNs)
|
|
|
|
| ⊗|
ˆσ
|
|
|
Vaswani et al. (Unmasked)
h(x) := exp( ∥x∥2
2
)
f1 := sin
f2 := cos
Performers
h(x) := exp(−∥x∥2
2
)
f1 := exp
h(x) := 1
f1 := ReLU
The learnable mask m specifies
different sparse attention patterns,
which are equivalently implemented
by a prep box that preprocesses the
tokens into different types that
attend differently. So we have different
attention blocks for different token types.
We notate the different tokens in blue.
In the top attention subproblem
blue-many tokens pay attention to
sequence-many key-value tokens
and vice versa in the bottom.
Fix the mean of the Gaussian
to the current query token.
Attention matrices become
fixed Gaussians around
the query token.
Include a residual.
A learned component generates
a Gaussian window for each query.
“Primordial Attention”
|
The outputs are concatenated.
|
| |
|
|
|
|
|
||
|
||
|
|
· · ·
prep boxes may only copy
and rearrange information.
All sparse attention patterns
are obtainable this way
§3.2: A common ancestor of both
canonical and linearised attention
Different choices of ϕ yield
a variety of architectures.
with little variation in
the attention mechanisms.
Longformer
ETC
Star-transformer
BigBird
(Randomised)
There are many ways
to invent structure for ϕ.
Remove mask.
Discard key and value.
|
Figure 1: The taxonomy generated by starting with a ‘primordial attention’ mechanism and apply specializations
and expressivity rewrites. Details of the notation and rewrites are give in Section 4
9

but replacing the scaled-dot-product attention heads with each attention variant (all other parts, e.g.
residuals and feedforward networks, are kept the same). In line with [Qin et al., 2022], we observed
that variants containing LinAttPrep and LinAttApply were unstable during training – therefore,
as suggested there, we added additional layer normalisation after each LinAttApply or AttApply
generator. For all models, we used the causally-masked variant of LinAttPrep/LinAttApply given by
[Katharopoulos et al., 2020a]. An implementation of these models is provided in the supplementary
material, and see Appendix C for more details.
To compare the 14 proposed attention mechanisms, we trained them ab initio on a word-level
language modelling task on the Penn Treebank corpus [Marcus et al., 1993]. While specific attention
mechanisms may be preferable for certain tasks, we believe this task is suitably representative
to compare across different variants. We performed word-level tokenization on the dataset for a
final vocabulary size of ∼10K. All trainable universal approximators specified in the generators
are implemented as an MLP with one hidden layer with the same dimension as the input - note
that this includes the transformations creating the key and query inputs for the AttPrep generator
and value inputs for the AttApply generator, in contrast with the usual implementations of these
transformations as linear mappings. The results are given in Figure 2. We observe that despite the
significant differences between attention mechanisms, all models perform comparably – indeed, the
gap between the best- and worst-performing models is narrower than the range of performance
we observed for any particular model during hyperparameter tuning. Furthermore, insofar as
performance varies, there does not appear to be any obvious connection between the structure of
each attention head and its performance.
6
Discussion
We introduced string diagrams with rewrites for DL models, and demonstrated their conceptual
utility in describing, comparing, and probing the structure of attention mechanisms. Our notation
critically underpinned our ability to study a wide range of architectures in detail, and proved
its worth in shaping and enabling our scientific inquiry; in a companion paper, we demonstrate
the application of string diagrammatic notation for objective functions [Rodatz et al., 2024]. Our
experimental findings from exploring the space of possible attention mechanisms suggest the
following two possibilities:
• Many popular explanations have been developed for the inner workings of Transformer models,
based on the specific structure of the attention mechanism, e.g [Alammar, 2018, Sanderson, 2024].
Since all our attention variants performed comparably, one possibility is that these conceptual
explanations, while intuitive and perhaps even locally correct, are not crucial for performance and
do not constitute an explanation for why transformer-like models work in general. Rather, any
expressive method of exchanging data between tokens ought to suffice – this conclusion is borne
out by other models such as FNet [Lee-Thorp et al., 2022] and MLPMixer [Tolstikhin et al., 2021]
which work by a completely different mechanism.
• Otherwise, if the anatomy of attention is important for performance, we have also observed that
one model (M11 in Figure 3) slightly outperforms the classic scaled dot product attention, which
is otherwise the most performant model. If this finding is reproducible at scale, then another
possibility is that there exist very large attention mechanisms that are highly performant. Further
investigation along these lines again critically depends on notation: these large mechanisms
might be discovered more effectively by combinatorial search instead of refining particular
anatomical elements by hand, and just describing them may already necessitate analysis in terms
of components of the sort we have demonstrated.
10

(a)
1.2
1.4
1.6
1.8
2.0
2.2
2.4
2.6
Parameters
1e7
75
80
85
90
95
100
105
110
Test PPL
M0
M2
M4
M6
M10
M1
M3
M5
M9
M13
M7
M8
M11
M12
(b)
1.2
1.4
1.6
1.8
2.0
2.2
2.4
2.6
Parameters
1e7
20
40
60
80
100
Test PPL
M0
M2
M4
M6
M10
M1
M3
M5
M9
M13
M7
M8
M11
M12
GPT 2 (0 shot)
GPT 3 (0 shot)
Figure 2: (a) The results of training Transformer models based on the 14 attention variants identified
above. They were trained ab initio on word-level language modelling of the Penn Treebank corpus - all
models have four attention heads per layer and an embedding dimension of 512. We used the learning-rate
scheduler given by [Vaswani et al., 2017], with initial learning rate tuned per-model. Each line on the plot
shows the test-set perplexity of one model for between one and five layers, as compared to total trainable
parameter count. The models are coloured according to whether they contain only linear attention generators
(magenta), only dot-product attention generators (cyan), or both (orange). (b) The same with the results from
[Radford et al., 2019, Brown et al., 2020] for scale. Note that PPL is an exponential scale, so differences matter
less as the PPL value increases.
11

|
|
|
|
ϕ
1
⊗
|
|
⊗
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
|
|
M0
⊗
|
⊗
|
|
|
|
|
|
|
ˆσ
|
|
M1
|
|
|
|
ϕ
1
⊗
|
|
⊗
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
|
|
|
|
M2
⊗
|
⊗
|
|
|
|
|
|
|
ˆσ
|
|
|
|
M3
|
|
|
|
ϕ
1
⊗
|
|
⊗
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
|
|
|
M4
⊗
|
|
⊗
|
|
|
|
ˆσ
|
|
|
|
⊗
|
|
ˆσ
|
|
M5
|
|
|
|
ϕ
1
⊗
|
|
|
|
|
|
ϕ
1
⊗
|
|
⊗
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
|
|
⊗
|
|
M6
⊗
|
|
|
|
ϕ
1
⊗
|
|
⊗
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
⊗
|
|
|
|
|
|
|
|
ˆσ
|
M7
⊗
|
|
|
|
ϕ
1
⊗
|
|
⊗
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
⊗
|
|
|
|
|
|
|
|
ˆσ
|
|
M8
⊗
|
|
|
⊗
|
|
|
|
|
|
|
ˆσ
|
⊗
⊗
|
|
|
|
ˆσ
|
|
M9
|
|
|
|
ϕ
1
⊗
|
|
⊗
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
|
|
|
|
ϕ
1
⊗
|
|
⊗
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
|
|
M10
⊗
|
⊗
|
|
|
|
|
|
ˆσ
|
|
|
|
|
ϕ
1
⊗
|
|
⊗
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
M11
⊗
|
⊗
|
|
|
|
|
|
|
ˆσ
|
|
|
|
|
ϕ
1
⊗
|
|
⊗
|
|
ϕ
⊗
|
|
⊗
1
x
⊗
|
|
M12
⊗
|
⊗
|
|
|
|
|
|
|
ˆσ
|
⊗
⊗
|
|
|
|
|
|
|
ˆσ
|
M13
|
|
Figure 3: The attention mechanisms generated by exhaustively recombining the given generators, after removing
redundant models using the criteria described above. Note that M0 corresponds precisely to the linear attention
mechanism presented in [Katharopoulos et al., 2020a], and M1 corresponds to scaled dot-product attention as
presented in [Vaswani et al., 2017].
12

Acknowledgements
The authors would like to thank Dimitri Kartsaklis, Steve Clark, and Bob Coecke for helpful feedback,
Benjamin Rodatz, Ian Fan, Neil Ortega, and Konstantinos Meichanetzidis for useful discussions, and
the rest of the Quantinuum Oxford office for their assistance.
References
[Abbott, 2024] Abbott, V. (2024). Neural Circuit Diagrams: Robust Diagrams for the Communication,
Implementation, and Analysis of Deep Learning Architectures.
[Ainslie et al., 2020] Ainslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., Ravula,
A., Sanghai, S., Wang, Q., and Yang, L. (2020). ETC: Encoding long and structured inputs in
transformers. In Webber, B., Cohn, T., He, Y., and Liu, Y., editors, Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP), pages 268–284, Online. Association
for Computational Linguistics.
[Alammar, 2018] Alammar, J. (2018). The illustrated transformer. https://jalammar.github.io/
illustrated-transformer/. Accessed: 21st May 2024.
[Andre et al., 1996] Andre, J., Street, R., and Verity, D. (1996). Traced monoidal categories. Mathe-
matical Proceedings of the Cambridge Philosophical Society, 119:447–468.
[Baez and Master, 2020] Baez, J. C. and Master, J. (2020). Open Petri Nets. Mathematical Structures in
Computer Science, 30(3):314–341. arXiv:1808.05415 [cs, math].
[Bahdanau et al., 2016] Bahdanau, D., Cho, K., and Bengio, Y. (2016). Neural Machine Translation
by Jointly Learning to Align and Translate.
[Beltagy et al., 2020] Beltagy, I., Peters, M. E., and Cohan, A. (2020). Longformer: The long-document
transformer. arXiv:2004.05150.
[Boisseau, 2020] Boisseau, G. (2020). String Diagrams for Optics. In Ariola, Z. M., editor, 5th
International Conference on Formal Structures for Computation and Deduction (FSCD 2020), volume
167 of Leibniz International Proceedings in Informatics (LIPIcs), pages 17:1–17:18, Dagstuhl, Germany.
Schloss Dagstuhl – Leibniz-Zentrum für Informatik.
[Boisseau and Sobociński, 2022] Boisseau, G. and Sobociński, P. (2022).
String Diagrammatic
Electrical Circuit Theory.
Electronic Proceedings in Theoretical Computer Science, 372:178–191.
arXiv:2106.07763 [cs].
[Bonchi et al., 2023] Bonchi, F., Di Giorgio, A., and Santamaria, A. (2023). Deconstructing the
Calculus of Relations with Tape Diagrams. Proceedings of the ACM on Programming Languages,
7(POPL):1864–1894.
[Bonchi et al., 2019] Bonchi, F., Piedeleu, R., Sobociński, P., and Zanasi, F. (2019). Graphical Affine
Algebra. In 2019 34th Annual ACM/IEEE Symposium on Logic in Computer Science (LICS), pages
1–12.
[Bonchi et al., 2017] Bonchi, F., Sobocinski, P., and Zanasi, F. (2017). Interacting Hopf Algebras.
Journal of Pure and Applied Algebra, 221(1):144–184. arXiv:1403.7048 [cs, math].
[Bonchi et al., 2014] Bonchi, F., Sobociński, P., and Zanasi, F. (2014). A Categorical Semantics of
Signal Flow Graphs. volume CONCUR 2014 - Concurrency Theory - 25th International Conference.
13

[Bronstein et al., 2021] Bronstein, M. M., Bruna, J., Cohen, T., and Veličković, P. (2021). Geometric
deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478.
[Brown et al., 2020] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,
Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler,
E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I.,
and Amodei, D. (2020). Language models are few-shot learners. In Larochelle, H., Ranzato, M.,
Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates, Inc.
[Buntine, 1994] Buntine, W. L. (1994). Operations for Learning with Graphical Models. Journal of
Artificial Intelligence Research, 2:159–225.
[Capucci et al., 2022] Capucci, M., Gavranović, B., Hedges, J., and Rischel, E. F. (2022). Towards
Foundations of Categorical Cybernetics. Electronic Proceedings in Theoretical Computer Science,
372:235–248.
[Choromanski et al., 2022] Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos,
T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L., and Weller, A. (2022).
Rethinking Attention with Performers. arXiv:2009.14794 [cs, stat].
[Coecke and Kissinger, 2017] Coecke, B. and Kissinger, A. (2017). Picturing Quantum Processes: A First
Course in Quantum Theory and Diagrammatic Reasoning. Cambridge University Press, Cambridge.
[Coecke et al., 2010] Coecke, B., Sadrzadeh, M., and Clark, S. (2010). Mathematical Foundations for
a Compositional Distributional Model of Meaning. arXiv:1003.4394 [cs, math].
[Comfort et al., 2020] Comfort, C., Delpeuch, A., and Hedges, J. (2020). Sheet diagrams for bi-
monoidal categories. arXiv preprint arXiv:2010.13361.
[Cruttwell et al., 2022] Cruttwell, G. S., Gavranović, B., Ghani, N., Wilson, P., and Zanasi, F. (2022).
Categorical foundations of gradient-based learning. In Programming Languages and Systems: 31st
European Symposium on Programming, ESOP 2022, Held as Part of the European Joint Conferences on
Theory and Practice of Software, ETAPS 2022, Munich, Germany, April 2–7, 2022, Proceedings, pages
1–28. Springer International Publishing Cham.
[Cybenko, 1989] Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function.
Mathematics of control, signals and systems, 2(4):303–314.
[Dai et al., 2019] Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and Salakhutdinov, R. (2019).
Transformer-XL: Attentive language models beyond a fixed-length context. In Korhonen, A.,
Traum, D., and Màrquez, L., editors, Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 2978–2988, Florence, Italy. Association for Computational
Linguistics.
[Dao, 2023] Dao, T. (2023). Flashattention-2: Faster attention with better parallelism and work
partitioning. arXiv preprint arXiv:2307.08691.
[Fock, 1932] Fock, V. (1932). Konfigurationsraum und zweite Quantelung. Zeitschrift fur Physik,
75(9-10):622–647.
[Fong and Spivak, 2019] Fong, B. and Spivak, D. I. (2019). Hypergraph categories. Journal of Pure
and Applied Algebra, 223(11):4746–4777.
14

[Fox, 1976] Fox, T. (1976). Coalgebras and cartesian categories. Communications in Algebra, 4(7):665–
667.
[Fritz et al., 2021] Fritz, T., Gonda, T., and Perrone, P. (2021). De Finetti’s Theorem in Categorical
Probability. Journal of Stochastic Analysis, 2(4). arXiv:2105.02639 [cs, math, stat].
[Goldstine and von Neumann, 1947] Goldstine, H. and von Neumann, J. (1947). Planning and
Coding of Problems for an Electronic Computing Instrument. Technical report, Institute for
Advanced Study, Princeton, New Jersey.
[Goodfellow et al., 2016] Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT
Press. http://www.deeplearningbook.org.
[Guo et al., 2019a] Guo, M., Zhang, Y., and Liu, T. (2019a). Gaussian transformer: A lightweight
approach for natural language inference. Proceedings of the AAAI Conference on Artificial Intelligence,
33(01):6489–6496.
[Guo et al., 2019b] Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., and Zhang, Z. (2019b). Star-transformer.
In Burstein, J., Doran, C., and Solorio, T., editors, Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 1315–1325, Minneapolis, Minnesota. Association for
Computational Linguistics.
[Haydon and Sobociński, 2020] Haydon, N. and Sobociński, P. (2020). Compositional Diagrammatic
First-Order Logic. In Pietarinen, A.-V., Chapman, P., Bosveld-de Smet, L., Giardino, V., Corter,
J., and Linker, S., editors, Diagrammatic Representation and Inference, Lecture Notes in Computer
Science, pages 402–418, Cham. Springer International Publishing.
[Hedges, 2015] Hedges, J. (2015). String diagrams for game theory. arXiv:1503.06072 [cs, math].
[Hintikka and Sandu, 1997] Hintikka, J. and Sandu, G. (1997).
Chapter 6 - Game-Theoretical
Semantics. In van Benthem, J. and ter Meulen, A., editors, Handbook of Logic and Language, pages
361–410. North-Holland, Amsterdam.
[Hu, ] Hu, N. External traced monoidal categories.
[Ismailov, 2023] Ismailov, V. E. (2023). A three layer neural network can represent any multivariate
function. Journal of Mathematical Analysis and Applications, 523(1):127096.
[Jacobs et al., 2019] Jacobs, B., Kissinger, A., and Zanasi, F. (2019). Causal inference by string diagram
surgery. In Foundations of Software Science and Computation Structures: 22nd International Conference,
FOSSACS 2019, Held as Part of the European Joint Conferences on Theory and Practice of Software,
ETAPS 2019, Prague, Czech Republic, April 6–11, 2019, Proceedings 22, pages 313–329. Springer.
[Katharopoulos et al., 2020a] Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. (2020a).
Transformers are rnns: fast autoregressive transformers with linear attention. In Proceedings of the
37th International Conference on Machine Learning, ICML’20. JMLR.org.
[Katharopoulos et al., 2020b] Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. (2020b).
Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In Proceedings
of the 37th International Conference on Machine Learning, pages 5156–5165. PMLR. ISSN: 2640-3498.
[Lane, 2010] Lane, S. M. (2010). Categories for the Working Mathematician: 5. Springer, New York, NY,
2nd ed. 1978. softcover reprint of the original 2nd ed. 1978 edition edition.
15

[Lee-Thorp et al., 2022] Lee-Thorp, J., Ainslie, J., Eckstein, I., and Ontanon, S. (2022). FNet: Mixing
tokens with Fourier transforms. In Carpuat, M., de Marneffe, M.-C., and Meza Ruiz, I. V., editors,
Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 4296–4313, Seattle, United States. Association for
Computational Linguistics.
[Lin et al., 2022] Lin, T., Wang, Y., Liu, X., and Qiu, X. (2022). A survey of transformers. AI open,
3:111–132.
[Loregian, 2021] Loregian, F. (2021). (Co) end calculus, volume 468. Cambridge University Press.
[Lorenz and Tull, 2023] Lorenz, R. and Tull, S. (2023). Causal models in string diagrams. arXiv
preprint arXiv:2304.07638.
[Marcus et al., 1993] Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large
annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.
[Markl et al., 2007] Markl, M., Shnider, S., and Stasheff, J. (2007). Operads in Algebra, Topology and
Physics. American Mathematical Society, Providence, RI.
[Melis et al., 2019] Melis, G., Kočisk`y, T., and Blunsom, P. (2019). Mogrifier lstm. In International
Conference on Learning Representations.
[Melliès, 2006] Melliès, P.-A. (2006). Functorial Boxes in String Diagrams. In Ésik, Z., editor, Computer
Science Logic, Lecture Notes in Computer Science, pages 1–30, Berlin, Heidelberg. Springer.
[nLab authors, ] nLab authors. Markov category in nLab.
[Pandey et al., 2022] Pandey, M., Fernandez, M., Gentile, F., Isayev, O., Tropsha, A., Stern, A. C., and
Cherkasov, A. (2022). The transformational role of gpu computing and deep learning in drug
discovery. Nature Machine Intelligence, 4(3):211–221.
[Penrose, 1971] Penrose, R. (1971). Applications of Negative Dimensional Tensors. Combinatorial
Mathematics and its Applications, pages 221–244.
[Poór et al., 2023] Poór, B., Wang, Q., Shaikh, R. A., Yeh, L., Yeung, R., and Coecke, B. (2023). Com-
pleteness for arbitrary finite dimensions of ZXW-calculus, a unifying calculus. arXiv:2302.12135
[quant-ph].
[Qin et al., 2022] Qin, Z., Han, X., Sun, W., Li, D., Kong, L., Barnes, N., and Zhong, Y. (2022). The
devil in linear transformer. In Goldberg, Y., Kozareva, Z., and Zhang, Y., editors, Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing, pages 7025–7041, Abu Dhabi,
United Arab Emirates. Association for Computational Linguistics.
[Radford et al., 2019] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019).
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.
[Rodatz et al., 2024] Rodatz, B., Fan, I., Laakkonen, T., Ortega, N. J., Hoffman, T., and Wang-
Maścianica, V. (2024). A pattern language for machine learning tasks.
[Román, 2020] Román, M. (2020). Open diagrams via coend calculus. In ACT.
[Sanderson, 2024] Sanderson, G. (2024). Attention in transformers, visually explained. https:
//www.youtube.com/watch?v=eMlx5fFNoYc. Accessed: 21st May 2024.
[Schlag et al., 2021] Schlag, I., Irie, K., and Schmidhuber, J. (2021). Linear Transformers Are Secretly
Fast Weight Programmers. In Proceedings of the 38th International Conference on Machine Learning,
pages 9355–9366. PMLR. ISSN: 2640-3498.
16

[Selinger, 2010] Selinger, P. (2010).
A survey of graphical languages for monoidal categories.
arXiv:0908.3347 [math], 813:289–355.
[Sobociński, 2015] Sobociński, P. (2015). Graphical Linear Algebra.
[Taylor, 2024] Taylor, J. K. (2024). An introduction to graphical tensor notation for mechanistic
interpretability. arXiv preprint arXiv:2402.01790.
[Tolstikhin et al., 2021] Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T.,
Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., and Dosovitskiy, A. (2021). Mlp-mixer:
An all-mlp architecture for vision.
[Vaswani et al., 2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, NIPS’17, page 6000–6010, Red Hook, NY, USA.
Curran Associates Inc.
[Wang et al., 2020] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. (2020). Linformer: Self-
Attention with Linear Complexity. https://arxiv.org/abs/2006.04768v3.
[Wang-Mascianica et al., 2023] Wang-Mascianica, V., Liu, J., and Coecke, B. (2023). Distilling Text
into Circuits. arXiv:2301.10595 [cs, math].
[Yang et al., 2018] Yang, B., Tu, Z., Wong, D. F., Meng, F., Chao, L. S., and Zhang, T. (2018). Modeling
localness for self-attention networks. In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J.,
editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages
4449–4458, Brussels, Belgium. Association for Computational Linguistics.
[Yau, 2008] Yau, D. (2008). Higher dimensional algebras via colored PROPs. arXiv:0809.2161
[math-ph].
[You et al., 2020] You, W., Sun, S., and Iyyer, M. (2020). Hard-coded Gaussian attention for neural
machine translation. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J., editors, Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics, pages 7689–7700, Online.
Association for Computational Linguistics.
[Zaheer et al., 2021] Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S.,
Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. (2021). Big bird: Transformers for longer
sequences.
[Zaremba et al., 2014] Zaremba, W., Sutskever, I., and Vinyals, O. (2014). Recurrent neural network
regularization. arXiv preprint arXiv:1409.2329.
A
Formal semantics
Throughout, we assume familiarity with symmetric monoidal categories (SMCs), for which a
standard reference is [Lane, 2010]. In this section we detail the formal semantics of our diagrammatic
system. In a nutshell, each of our string diagrams is a indexed set of concrete programs, where the
indexing corresponds to the user’s choices in instantiating an architecture. There are several novel
aspects of our diagrammatic notation that merit comment and mathematical elaboration, as they
extend beyond the customary formal semantics of string diagrams in terms of symmetric monoidal
functors [Selinger, 2010].
17

Tick-notation for arbitrary dimensions
We allow our diagrams to express (and thus hide the
complexity of) choice-dependent dimensions, which wins us two novel benefits. First, we formally
capture the intention of the informal flowcharts frequently used to illustrate architectures as referents
to indexed families of models rather than specific programs; a transformer is still a transformer
whether its programmer chooses the context window size to be 28 or 224.
Second, by analogy with game-theoretical semantics [Hintikka and Sandu, 1997], we can distil
away numerals altogether from our diagrams to leave only the two kinds of integer-variables of
interest to the engineer: arbitrary-but-fixed values that the programmer chooses, and the sequence-
length of the input that the environment chooses. These two abstractions are valuable enough to
merit the effort of establishing their formality. As is customary with the mathematics of string
diagrams, the way to do so is to describe what individual pieces of diagrams mean in isolation, how
they can be put together, and how their meanings interact in their composites.
Diagrammatic rewriting: universal approximation
We opt to express the universal approximation
theorem as the capacity for a universal approximator to be diagrammatically substituted for any
other composite diagram with equal input and output. This amounts to asking that universal
approximators are treated as “typed contexts" in diagrams that are strict in the sense that different
such contexts also compose in the same symmetric monoidal manner as the ambient processes. This
leads to technical difficulties that rule out direct application of off-the-shelf methods for depicting
open string diagrams derived from the coend calculus [Loregian, 2021], some notable examples
being [Hu, , Román, 2020, Boisseau, 2020]. We define the coloured operad of a symmetric monoidal
category in Construction A.28, in which setting we are able to identify universal approximators
with operad morphisms, viewed as the parse-trees of morphisms in the ambient SMC, thus directly
interpreting them as typed contexts, and the expressivity reduction rewrites as operadic composition.
For technical reasons involving scalars (the endomorphisms of the monoidal unit), this construction
only works in semicartesian settings, i.e. where the monoidal unit is also terminal, but that is
sufficiently general to admit our use cases, which are primary cartesian monoidal settings [Fox, 1976],
and are at worst Markov categories [nLab authors, ].
Confluence
Finally, because we define rewrites in the PROP setting, not the free tensoring, we have
to show that the SIMD notation "plays nicely together" with rewrites. In this context, the relevant
safety property to show is that the order in which rewrites and SIMD-reasoning occurs is immaterial.
We achieve this by constructing a symmetric monoidal category from the hom-operad of a PROP
in Construction A.33, which we demonstrate in Theorem A.35 is equivalent to the original PROP
decorated with a novel formal black box for each input-output typing, such that we may completely
delegate the symmetric monoidal semantics to the PROP and the expressivity reduction rewrites to
the hom-operad.
On tensor-notation in machine learning and related work
There are usually tradeoffs involved in
tensor-notation for machine learning, but we sidestep these tradeoffs altogether with tick notation,
underpinned by a free-tensoring construction on PROPs that we introduce. Thus we may win all
of the benefits of tensor notation while keeping semantics formal and elementary, allowing us to
further establish useful rewrite systems for calculation, and extend our consideration to virtually all
sequence-to-sequence architectures. Simplifying tensor contraction for calculative purposes was
in fact one of the original use-cases of string diagrams [Penrose, 1971], and the formal semantics
of tensor-network notation in hypergraph categories [Fong and Spivak, 2019] is well understood.
However, there is a fundamental mathematical obstacle in bringing together tensor notation and
machine learning: any computational setting in which arbitrary data can be faithfully copied has
string-diagrammatic semantics in a cartesian monoidal category by Fox’s theorem [Fox, 1976], and
a monoidal product can only support both cartesian and hypergraph structures simultaneously
18

if the category is trivial ([Coecke and Kissinger, 2017]: Proposition 4.74). This mathematical fact
has unfortunately forced various compromises in other tensorial approaches to string diagrams
for transformers in particular. For instance, the "graphical tensor notation" approach [Taylor, 2024]
stays in the tensorial setting by employing a visually similar but informal analogue of functor boxes
[Melliès, 2006] to perform nonlinear transformations borrowed from the cartesian side, but this comes
at steep costs to expressivity and formal difficulty: categories of smooth maps tend to be cartesian,
so the tensorial monoidal product is undefined unless bubbles are restricted to be unconstrained
endocombinators on homsets of linear maps. To keep what appears to be the desired expressivity,
semantics may be declared in 2-categories of profunctor representations of open diagrams as
previously mentioned to maintain both compact closure and access to generic nonlinear maps, at the
expense of broad accessibility and verifiability of the formalism. Without addressing the underlying
mathematical mismatch, increasing formal correctness also comes with its own drawbacks. For
instance, the "neural circuit architecture" approach [Abbott, 2024] attempts to incorporate both the
tensorial and cartesian worlds by what appears to be an independent rediscovery of relational tape-
diagrams [Bonchi et al., 2023] and sheet-diagrams [Comfort et al., 2020] in distributive monoidal
settings. While in principle formally unambiguous, too much detail arguably comes at the expense
of the raison d’être of string-diagrammatic notation: visual clarity and simplicity. Even discounting
the lack of calculative and inferential power of the aforementioned approaches, we consider these
shortcomings to be unacceptable and wholly unnecessary impediments to scientific inquiry in deep
learning. Thus we state an alternative.
A.1
SIMD notation
A.1.1
Free tensoring of a PROP
We adopt the convention that N does not contain 0. We use the terms PROP and coloured PROP
(for which a standard reference is [Yau, 2008]) interchangeably after their introduction, and we
assume that they are combinatorially generated by signatures so that we can speak unambiguously
of the "data of a PROP". We refer to relations as subsets without loss of generality in the symmetric
monoidal setting of Rel×, where strong compact closure makes input-output typing a matter of
convention.
Ticks as tensor indices
Definition A.1 (PROP). A PROP is a strict symmetric monoidal category generated by a single object
x: every object is of the form
n
O
x = x ⊗· · · ⊗
| {z }
n
x
PROPs may be generated by, and presented as signatures (Σ, E) consisting of generating morphisms Σ
with arity and coarity in N, and equations E relating symmetric monoidal composites of generators.
Example A.2. The category CartSp of cartesian spaces Rn = R × · · · × R and structured maps
between them (i.e. linear, continuous, smooth – as one likes) is a PROP. CartSp is the intended
mathematical setting for the final evaluation of our diagrams, and arguably of most deep learning.
Definition A.3 (Coloured PROP). A multi-sorted or coloured PROP with set of colours C has a monoid
of objects generated by C.
Definition A.4 (Cartesian PROP). By Fox’s theorem [Fox, 1976], a cartesian PROP is one in which
every object (wire) is equipped with a cocommutative comonoid (copy) with counit (delete) such
that all morphisms in the category are comonoid cohomomorphisms.
19

Construction A.5 (Tensoring of a PROP). Let P(i, j) for i, j ∈N notate the homsets of a prop P.
Define the coloured PROP P⊗to have as colours finite lists of integers List(N), and as morphisms,
tensored copies f ⊗k of morphisms f from the P.
Definition A.6 (Reshaping). In any tensored PROP, we freely obtain the following family of reshaping
bĳections on homsets by colouring and forgetting colours:
P⊗(
k
O
[i1 · · · ink],
l
O
[j1 · · · jml]) ↔P(
k
Y nk
X
ink,
lY ml
X
jml)
Example A.7. Continuing with Example A.2, the tensoring of CartSp is denoted CartSp⊗. It has
real-valued tensors as objects, denoted without loss of generality as ordered lists of indices [i, j, k]
decorating collections of objects Ri×j×k in the underlying PROP CartSp. As in all tensored-PROPs,
every object comes equipped with a family of reshape isomorphisms:
[i1 · · · in] ≃[j1
1 · · · j1
m1] × · · · × [jk
1 , · · · , jk
mk]
Where Qn in = Pk Qmk
jk
mk; i.e. the integer totals match in the underlying PROP CartSp. Reshaping
subsumes tensor concatenation along matching dimensions and reordering indices of tensors, which
in turn subsumes matrix transposition as the special case for two indices. CartSp⊗moreover has a
family of tensor contraction morphisms:
[a, b] × [b, c] →[a, c]
Which are comprised of the usual addition and multiplication monoids on R in the underlying PROP
CartSp. Together, CartSp⊗is an adequately expressive semantic category for all merely descriptive
purposes in tensorial approaches to machine learning in nonlinear settings.
A.1.2
Functorial semantics for arbitrary-choice indices on tensored PROPs
Strategically, we aim to obtain functorial semantics for our diagrams by exploiting the observation
that CartSp⊗is a symmetric monoidal subcategory of the distributive monoidal category Rel×. So,
analogously to Fock spaces [Fock, 1932], we may use coproducts to "bundle together" objects and
morphisms of CartSp⊗according to their number of distinct indices, corresponding to the number
of distinct choices to be made. Tactically, we prosecute this strategy in two steps. First, we define
the required family of sets which will be our bundled-objects, and this immediately induces a full
symmetric monoidal subcategory F(R⊗) of Rel× which will be the codomain of the semantic functor.
Second, we define the functorial semantics of elements of our notation as indexed embeddings of
CartSp⊗in F(R⊗), which obtains the intended semantics of sets of processes indexed by choices in
the case of SIMD notation without traces, which we demonstrate by desugaring tensor contraction
notation. Finally, we deal with SIMD notation in generality as finite iteration, and we list the the
tracelike [Andre et al., 1996] coherence laws governing the notation.
Target category
Definition A.8 (F(R⊗)). For each n ∈N, let
Xn ≜
 a
i1∈N
· · ·
a
in∈N
 Ri1 × · · · × Rin
× N × · · · × N
|
{z
}
n
Let F(R⊗) be the full symmetric monoidal subcategory of Rel× generated by object-sets S
n∈N
Xn.
20

Functorial semantics for index-tick notation
Remark A.9. The fundamental conceptual difficulty we seek to address in this section is that
functorial semantics can only depend upon type-correctness of composing local components, but
matching indices is a global correctness condition. This motivates our resort to relational semantics.
We elaborate on generality and practicality in Remark A.17.
Establishing functorial semantics in this instance amounts to declaring a compositional interpre-
tation of our notational elements in F(R⊗). For formal clarity, we switch to an austere variant of
tick-notation. Instead of referring to choices using colours, we will use greeks.
Definition A.10 (Object semantics). Objects are ticked-wires, which we now interpret in F(R⊗).
The type of a vector with dimension varying with a choice α ∈N is interpreted as a relation (white
dot) that selects the identity relation Rn →Rn for each n ∈N, combined with the special frobenius
algebra (grey dot) on the copy-relation.
α
R
R
≜
N
`
i
Ri
`
i
Ri
N
≜
{
 (i, i) , (x, x)

| i ∈N, x ∈Ri}
The type of a tensor with dimensions varying with n (ordered) integer-choices α, β, . . . , ω is similarly:
α
R
R
β
ω
· · ·
≜
N
`
i1
· · · `
in
 Ri1 × · · · × Rin
N
1
N
N
2
N
N
n
...
...
...
· · ·
(α)
(β)
(ω)
Where the choices α, . . . , ω correspond to the n open N wires, and each relation with integer index j
selects identities as before, on the jth dimension. In particular, this means they commute. Thus the
resulting relation is:
{

(i1, i1), (i2, i2), · · · , (in, in),

n
z
}|
{
(xi1, xi2, · · · xin),
n
z
}|
{
(xi1, xi2, · · · , xin)

| ij ∈N, xij ∈Rij}
This definition already allows us to account for several of our diagrammatic phenomena.
Proposition A.11 (Sequential composition of objects identifies choices). Proof. For single ticks the
verification is as follows; we show that the sequential composite of two single-ticked wires with
differently labelled choices is definitionally equivalent to having a single choice. The multi-tick case
is conceptually identical.
(
α
) # (
β
)
≜
=
≜
α
Proposition A.12 (Reshaping: transposition and rearranging indices). Matrix transposition is a
special case of index-rearrangement, which corresponds semantically to braiding the auxiliary N
wires.
T
α β
α
β
≜
1
2
2
1
(α)
(β)
(α)
(β)
Proof. The integer-indexed identity selectors are commutative, and so in conjunction with Proposition
A.11, transposition inherits naturality with respect to integer-indexed relations from the naturality of
braids. All rearrangements of n indices are elements of the permutation group of n elements, which
correspond to distinct braidings on n wires, up to isotopy.
21

Proposition A.13 (Fixed dimensions). We can recover variable-choice iteration over fixed dimensions
via the injective embedding of `
β
(Rk)β ,→`
α
`
β
(Rα × Rβ).
α β
β
α ←: k
≜
k
`
α
`
β
(Rα × Rβ )
≃
`
β
(Rk)β
≜
β
Rk
Rk
Proof. In prose: where k is fixed, iterating β-times over Rk is equivalent to iterating α followed by β
times over R, and then choosing α ←: k, which we can interpret as relational postselection. The claim
then follows by inspection.
Definition A.14 (Morphism semantics: Same Instruction, Multiple Data). For each morphism
f : Rj →Rk in CartSp, we can define its iteration over a list of index choices Φ in a similar fashion
as we defined objects.
fΦ
Φ
Φ
· · ·
· · ·
Rj
Rk
≜
⌜fΦ⌝
...
`
Φ
(Rj )Φ
`
Φ
(Rk)Φ
N
N
N
N
N
N
Without loss of generality, let Φ = ⟨α, · · · ω⟩and |Φ| = n. We define the selection relation ⌜f Φ⌝to be
the following subset of N × · · · × N
|
{z
}
n
× `
Φ
(Rj)Φ × `
Φ
(Rk)Φ:
{

α · · · ω, xΦ, f(xΦ)

| α, · · · , ω ∈N}
Where xΦ denotes a (choice-variant) Φ-indexed tensor of values xiα···iω ∈Rj, and f(xΦ) denotes the
Φ-indexed tensor of outputs f(xiα···iω) ∈Rk.
Proposition A.15. Definition A.14 yields a symmetric monoidal category, where in particular
Definition A.10 yields the identities.
Proof. Unitality follows by construction, similarly to Proposition A.11. Associativity of composition
and symmetric monoidal structure is inherited from Rel×, by Definition A.8.
Definition A.16 (SIMD(CartSp)). Let SIMD(CartSp) be the category defined by Proposition A.15,
which is the intended semantic category for SIMD diagrams.
Remark A.17. We have presented an elementary intended semantics in Rel, as opposed to the
general construction for equipping arbitrary PROPs with arbitrary-indexing, which would extend
consideration to nonclassical computational settings. The notation is arguably intuitive enough to
not require further formal elaboration, so we are content with a sketch of the general construction,
which may be intuited by the following visual correspondence:
7→
In prose, starting with a PROP P, apply the Optic construction on the actegorical parameterisation
[Capucci et al., 2022] of profunctorial open diagrams [Román, 2020] in F(P⊗) with a PROP evaluated
at N. While using Prof in this way as a categorification of Rel is pedestrian, the use of the forward-
backward passes of the Optic construction however conceptually underpins the practical possibility
of efficient functional (cf. relational in Remark A.9) implementation.
22

A.1.3
SIMD as finite iteration
Executing the same process in parallel may be viewed (up to extensional equivalence) as a special
case of finite recurrence with a residual, where the residual carries no data. Expanding the notion of
SIMD to incorporate such finite iteration is both practically and mathematically natural cf. recurrent
neural networks, and traced [Andre et al., 1996]. The familial resemblance to traces suggests that we
may provide a stock of diagrammatic laws to aid reasoning, which is what we aim for in this section.
Definition A.18 (Finite iteration). Finite iteration on a symmetric monoidal category M is a family of
functions indexed over triples (A, B, U) of objects of M and all positive integers k ∈N:
M(U ⊗A, U ⊗B)
⟨−⟩k
U
→M(U ⊗A⊗k, U ⊗B⊗k)
Given by the following inductive definition for all f : U ⊗A →U ⊗B, where θ(X,Y ) denotes the
braiding:
(
⟨f⟩1
U ≜f
⟨f⟩k+1
U
≜(θ(B⊗k,U) ⊗1B) ◦(1B⊗k ⊗f) ◦((θ(B⊗k,U) ◦⟨f⟩k
U) ⊗1A)
The inductive case is defined up to braid isomorphism. Since the ambient category is symmetric
monoidal, the right-to-left order of passing the residual U is a choice of convention. We may present
this inductive definition diagrammatically as:
U
U
A
B
⟨f⟩1
U
≜
f
U
U
A
B
U
A⊗k+1
U
B⊗k+1
⟨f⟩k+1
U
≜
A
U
A⊗k
B⊗k
f
B
U
⟨f⟩k
U
Proposition A.19. SIMD is a special case of finite iteration with residual I, the monoidal unit.
Proof. Immediate, from monoidal coherence.
Notation A.20 (Finite iteration). To more explicitly relate finite iteration to feedback and traced
structure, we notate ⟨f⟩α
U following the same conventions as an operator on processes with a
bolded feedback edge. We remove the bolding in the special case of SIMD. Given a morphism
f : U × A →U × B in CartSp, we define in SIMD(CartSp) the relation ⌜⟨f⟩−
U⌝: N × U × `
α
A⊗α ×
`
α
B⊗α →`
α
A⊗α×`
α
B⊗α to select the appropriate finite iteration given a positive integer parameter,
and we define the following notation:
f
α
α
α
≜
U
U
`
α
A⊗α
`
α
B⊗α
⌜⟨f⟩−
U ⌝
N
N
Before we explicitly relate finite iteration notation to tracelike structure, we introduce some
common notational tools for manipulating indices via examples.
Example A.21 (Reshaping: splitting and concatenation). Concatenation amounts to tensoring wires
and adding indices, and splitting is co-concatenation, which relies on the converse of the addition
operation in Rel.
γ
α
β
α + β = γ
≜
+
`
α
X⊗α
N
`
β
X⊗β
N
`
γ
X⊗γ
N
⊗
23

α
β
γ
α = β + γ
≜
+
`
α
X⊗α
N
`
β
X⊗β
N
`
γ
X⊗γ
N
⊗
Proposition A.22. Modulo notation, finite iteration satisfies tightening, and yanking.
Proof. The following laws hold by symmetric monoidal coherence, and unpacking the definitions of
finite iteration.
f
g
h
α
α
α
=
f
g
α
α
α
h
α
α
α
α
(Tightening)
A precondition for yanking is that the residual matches the input and output: U = A = B.
α
α
α
=
α
(Yanking)
Proposition A.23. Finite iteration satisfies a variant strength condition, similar to monoidal strength.
f
α
α
α
g
α
α
=
f
α
α
α
g
α
α
α
(Strength)
Proof. By inspection.
Owing to the integer parameter on each finite iteration structure, finite iteration case-splits the
sliding and vanishing laws, the former of which demands more subtle interactions with the iteration
count, and the latter of which must account for the possibility of differing iteration choices in nested
structure.
Proposition A.24.
f
α
α
α
g
=
f
α
g
g
(α-1)
f
α
(α-1)
(α-1)
(Sliding-1)
f
α
α
α
g
=
f
α
g
g
(α-1)
f
(α-1)
(α-1)
α
(Sliding-2)
Proof. By inspection.
Proposition A.25.
f
α
α
β
α
β
β
β
β
U
V
U
V
=
f
α
α
β
α
β
β
β
β
U ⊗V
U ⊗V
(Vanishing)
Proof. By inspection.
Corollary A.26. The above laws characterise finite iteration.
Proof. Finite iteration yields the laws by Propositions A.22, A.23, A.24, and A.25. Conversely,
instantiating g as the identity in the sliding laws recovers the inductive definition of finite iteration.
24

A.2
Universal approximators and expressive reductions
In this section, we deal with equipping generic coloured cartesian PROPs P with universal ap-
proximators with the intuitively direct interpretation as typed holes in diagrams. The formal
underpinning is via considering universal approximators to be morphisms in an operad algebra
valued in the homsets of P.
Definition A.27 ((symmetric, unital) coloured operad). Where (V, ⊠, J) is a symmetric monoidal
category and C denotes a set of colours ci, a coloured operad O consists of:
• For each n ∈N and each (n + 1)-tuple (c1, · · · , cn; c), an object O(c1, · · · , cn; n) ∈V
• For each c ∈C, a morphism 1c : J →O(c; c) called the identity of c
• For each (n + 1)-tuple (c1 · · · cn; c) and n other tuples (d1
1 · · · d1
k1) · · · (dn
1 · · · dn
kn) a composition
morphism
O(c1, · · · , cn; c) ⊠O(d1
1 · · · d1
k1) ⊠· · · ⊠O(dn
1 · · · dn
kn) →O(d1
1 · · · d1
k1 · · · dn
1 · · · dn
kn; c)
• for all n ∈N, all tuples of colours, and each permutation σ ∈Sn the symmmetric group on n, a
morphism:
σ∗: O(c1 · · · cn; c) →O(cσ∗(1) · · · cσ∗(n); c)
The σ∗must represent Sn, and composition must satisfy associativity and unitality in a Sn-invariant
manner.
Construction A.28 (Hom-Operad of coloured PROP). Where (P, ⊗, I) is a coloured PROP with
colours CP, we construct OP, the hom-operad of P. We do so in two stages, by first defining an
ambient operad, and then restricting to the operad obtained by a collection of generators. Let the
ambient symmetric monoidal category be (Set, ×, {⋆}). Let the colours CO be the set of all tuples
(A, B), each denoting a pair of tuples (A1 ⊗An, B1 ⊗Bn) of Ai, Bi ∈CP.
• The tuple
 (A1, B1) · · · (An, Bn); (A, B)

is assigned the set [P(A1, B1) × · · · × P(An, Bn) →
P(A, B)] ∈Set; the set of all generated functions from the product of homsets P(Ai, Bi) to the
homset P(A, B).
• 1(A,B) : {⋆} →[P(A, B) →P(A, B)] is the identity functional that maps f : A →B in P(A, B)
to itself.
• The composition operations correspond to function composition in Set, where [X →Y ]×[Y →
Z] →[X →Z] sends (f:X→Y , g:Y →Z) 7→(g ◦f):X→Z; appropriately generalised to the multi-
argument case. The permutations are similarly defined, inheriting their coherence conditions
from the commutativity isomorphisms of the categorical product ×.
The generators are:
• For every f ∈P(A, B) that is a generator of P, define a corresponding generator of type {⋆} →
[P(I, I) →P(A, B)], which is the functional
 −7→(f ⊗−)

that sends endomorphisms of the
monoidal unit of P to their tensor with f, viewed as an element of the set [P(I, I) →P(A, B)].
• For every pair of tuples
 (X1, Y1) · · · (Xm, Ym); (A, B)

and
 (J1, K1) · · · (Jn, Kn); (B, C)

in
CO, a corresponding sequential composition operation of type:
[
Y
i⩽m
P(Xi, Yi) →P(A, B)] × [
Y
j⩽n
P(Jj, Kj) →P(B, C)]
25

→[
  Y
i⩽m
P(Xi, Yi) ×
Y
j⩽n
P(Jj, Kj)

→P(A, C)]
Which maps pairs of functionals (F: Q
i⩽m
P(Xi,Yi)→P(A,B), G: Q
j⩽n
P(Jj,Kj)→P(B,C)) to the func-
tional which sends pi : Xi →Yi and qj : Xj →Yj to G(p1 · · · pm) ◦F(q1 · · · qn).
• An analogous parallel composition for every pair of tuples, which sends pairs of functionals
(F, G) to G(p1 · · · pm) ⊗F(q1 · · · qn).
Example A.29. Construction A.28 can be thought of as bridging diagrams with their specific algebraic
descriptions using just the basic constructors ◦, ⊗; the hom-operad (when notated suggestively
in the usual tree-notation, found e.g. in [Markl et al., 2007]) plays the role of the syntactic tree of
◦, ⊗operators. For instance, given the composite morphism (g ⊗1E) ◦(1A ⊗f) in PROP P, the
corresponding diagram and operad-state in OP is:
f
g
A
B
D
E
↔
C
1A
f
1E
g
◦
⊗
⊗
[P(I, I) →P(A, A)]
[P(I, I) →P(B, C ⊗E)]
[P(I, I) →P(A ⊗C, D)]
[P(I, I) →P(E, E)]
[P(I, I)2 →P(A ⊗B, A ⊗C ⊗E)]
[P(I, I)2 →P(A ⊗C ⊗E, D ⊗E)]
[P(I, I)3 →P(A ⊗B, D ⊗E)]
Since the PROPs CartSp and its free tensoring are cartesian, P(I, I) is a singleton containing only
the identity of the monoidal unit, so in the settings we are concerned with, we may simplify colours
of the form [P(I, I)N →P(A, B)] to just P(A, B), and operad-states {⋆} →P(A, B) are in bĳective
correpondence with morphisms f : A →B of P; the fact that all f : A →B are representable as
operad states follows by construction, since any f in P is by definition expressible in terms of the
generators of P, and sequential and parallel composition ◦, ⊗. As we assume homsets are already
quotiented by the equational theory of P and the symmetric monoidal coherences, our operadic
representations inherit them: for example, we obtain interchange equalities such as the one below
for free:
v
P(A, C)
A
B
C
D
E
F
u
v
w
x
u
w
x
↔
⊗
⊗
◦
P(B, D)
P(C, E)
P(D, F )
P(A ⊗B, C ⊗D)
P(C ⊗D, E ⊗F )
P(A ⊗B, E ⊗F )
w
P(A, C)
u
v
x
◦
◦
⊗
P(B, D)
P(C, E)
P(D, F )
P(A, E)
P(B, F )
P(A ⊗B, E ⊗F )
=
Definition A.30 (Universal approximators and expressivity reductions). A morphism of a coloured
PROP P of type (A, B) containing universal approximators as black-boxes of types Ai⩽n →Bi⩽n
is a morphism
 (A1, B1) · · · (An, Bn); (A, B)

of OP, and by construction, vice versa. Expressivity
reductions correspond to precomposition in OP.
Example A.31. The inputs of open morphisms in OP correspond to "typed holes", and operadic
precomposition corresponds to "filling holes", with contents that may themselves also contain typed
holes. This precisely formalises the intuition that expressive reductions correspond to the ability of
a universal approximator to simulate anything, including composites containing other universal
26

approximators.
f
f
≜
A
B
C
D
E
F
G
G
1G
P(A ⊗D, E ⊗F )
f
⊗
◦
P(G, G)
P(A ⊗D ⊗G, E ⊗F ⊗G)
1A
P(A, A)
⊗
P(A ⊗B ⊗C, A ⊗D ⊗G)
P(B ⊗C, D ⊗G)
P(A ⊗B ⊗C, E ⊗F ⊗G)
↔
f
h
f
≜
A
B
C
E
F
G
G
h
1G
f
⊗
◦
1A
⊗
7→
h
⊗
P(B, D)
P(C, G)
P(B ⊗C, D ⊗G)
↔
7→
Remark A.32. The extension of the current theory to accommodate parameter sharing between
universal approximators is conceptually straightforward but technically involved. Parameter sharing
corresponds to the ability to reuse – i.e. copy – data between open wires in the operad OP, which
amounts to having a cartesian operad.
A.3
Coherence of SIMD and universal approximators
Now, given a (presumed cartesian) coloured PROP P, we have two extensions: the free tensoring
F(P)⊗which formalises tick-notation for SIMD, and the hom-operad OP in which universal approx-
imators and expressive reductions are formalised as open morphisms and operad precomposition
respectively. The remaining formal question is how to combine the extensions in a consistent manner.
We achieve this by constructing a symmetric monoidal structure C(OP) on top of OP which will
contain P as a full (symmetric monoidal) subcategory and will be equivalent to the PROP ˆP obtained
by extending the signature generating P with a black-box for every input-output type: in this way
C(OP) will syntactically represent extending P with black-boxes, while the underlying OP structure
will modularly govern the behaviour of expressive reductions.
Construction A.33 (The symmetric monoidal category C(OP) of the hom-operad OP of the PROP P).
We define a symmetric monoidal category C(OP) from the data of OP of a cartesian P as follows.
First, recall from Example A.29 that by cartesianity of P, P(I, I) is a singleton, so without loss
of generality we may denote the arbitrary morphism of OP as f : X →P(A, B) of OP, for some
X, and for A, B objects of P. For each such f, we define a morphism f : A →B in C(OP), and
we define the objects C(OP) by proxy, via the identity morphisms 1X in P, which become states
1X : {⋆} →P(X, X), which become morphisms 1X : X →X C(OP). Similarly, we define the
braidings θX,Y : X ⊗Y →Y ⊗X.
1X
P(X, X)
θX,Y
P(X ⊗Y, Y ⊗X)
We define sequential and parallel composition in C(OP) using the ◦and ⊗constructors that are
definitionally available in OP.
f
⊗
◦
g
f
g
P(A, B)
P(A, B)
P(B, C)
P(A, C)
P(C, D)
P(A ⊗C, B ⊗D)
g ◦f
P(A, C)
≜
f ⊗g
P(A ⊗C, B ⊗D)
≜
By the construction of OP, C(OP) satisfies all symmetric monoidal coherences.
27

Construction A.34 ( ˆP). Given a cartesian coloured PROP P generated by some signature Σ consisting
of colours, morphisms and equational relations, define ˆP to be the coloured PROP obtained by
extending Σ with (1) a morphism ⊥: X →Y for each pair of objects of P (2) copy-delete coherence
equations for the ⊥s; by Fox’s theorem, the presence of such copy-delete coherences for all morphisms
is equivalent to cartesian monoidality [Fox, 1976].
Theorem A.35. If a coloured PROP P is cartesian, then P ,→C(OP) ≃ˆP. In prose, P embeds
(as a cartesian monoidal category) into C(OP), which is equivalent to ˆP (as a cartesian monoidal
category).
Proof. First, to see that P embeds as a full subcategory of C(OP), observe that (1) morphisms
f : A →B of P are in definitional one-to-one correspondence with states f : {⋆} →P(A, B) of OP,
(2) the constructors available in C(OP) can only combine states of OP to form other states of OP, and
(3) all morphisms of P can be obtained in this way, as all admit a decomposition in terms of ◦, ⊗,
and the generators of P.
Second, to see that C(OP) is (cartesian monoidally) equivalent to ˆP, observe that the open
inputs X of morphisms f : X →P(A, B) may be written without loss of generality as lists
[P(X1, Y1), · · · , P(Xn, Yn)], which fall in bĳective correspondence with states {⋆} →X by first
sending X to C(O ˆ
P) along the embedding P ,→ˆP, and then precomposing each open P(Xn, Yn)
with ⊥: {⋆} →P(Xi, Yi) in O ˆ
P, obtained from the ⊥: Xi →Yi that are definitionally available in
ˆP. So C(OP) is equivalent to the full subcategory of C(O ˆ
P) generated by states of O ˆ
P, which is by
part one of this proof equivalent to ˆP, so we have the claim.
Definition A.36 (SIMD notation with universal approximators). Our string diagrams take their
formal semantics in F(C(OCartSp))⊗, and the semantics of expressivity reduction of universal
approximators is formalised by precomposition in OCartSp.
Remark A.37. This construction extends to any symmetric monoidal category (C, ⊗, I) with C(I, I)
a singleton, which would notably include Set and Markov categories [nLab authors, ] that formalise
probabilistic processes. The key step is the ability to remove the composition-dependence of the
domains P(I, I)n in Example A.29, which may alternately be achieved by e.g. the use of a comonoid
δ : P(I, I) →P(I, I) × P(I, I) to compose e.g. F(−) ≜(−7→−⊗f) ∈[P(I, I) →A, B] and G(−) ≜
(−7→−⊗g) ∈[P(I, I) →B, C] sequentially as
 −7→G(δ1(−)) ◦F(δ2(−))

∈[P(I, I) →A, C].
B
Detailed graphical derivations
In this section we present in detail several sequences of graphical rewrites. Some (Appendix B.1, B.4)
will use the the less precise notation with coloured wires with ticks, which in principle could leave
some ambiguity with respect to indices and dimension sizes, but is easier to work with, especially if
we are only interested in the high-level structure of the model. Others (Appendices B.2, B.3) will
use fully precise notation to demonstrate that one can always be fully precise in this framework
if desired. In particular, in the precise form, we will sometimes use superscripts to disambiguate
between different indices with the same numerical value. For instance, if we draw an [s, s] sized wire
representing the attention matrix entering an s-SIMD box, this leaves it ambiguous with dimension
is being expanded by the SIMD box. Thus, we write [s(q), s(k)] for the tensor on the wire, and also
write s(q) for the SIMD box. Other cases where disambiguation like this may be required are tensor
contractions.
Note in principle one could include such superscripts on all tensor indices of the diagram, and
do so in a consistent way, due to the nature of tensor contraction – however, we will only include
superscripts below if disambiguation is required.
28

B.1
Motivating Vaswani et al. from Bahdanau et al.
Here we present an account of how one could arrive at the self-attention mechanism using our
graphical rewrites, starting from the Bahdanau et al. architecture. We use the less precise coloured,
ticked wire convention. This is not intended to be a highly rigorous explanation for, nor are we
making precise claims about the expressive power of these models. Instead this shows how feasibly,
using our rewrites can recover relationships between models which were in reality derived from
each other, via human inspiration.
Everything in this sequence of steps proceeds by expressivity rewrites involving trainable boxes,
except one rewiring step that involves a structural change, in which we plug inputs into what were
originally the hidden states passed into the RNN-decoder cells.
The Bahdanau et al. architecture is depicted below: we use slanted thick blue ticks below to
represent the reversal operation on the sequence dimension (for the bi-RNN), and the merging the
sequences at the end of the bi-RNN via concatenating corresponding vector pairs. The four trainable
boxes below are – two RNN-cells for the bi-RNN encoder, the score function (which has trainable
weights), and the decoder RNN cell.
σ
⊗
|
|
|
|
|
|
|
|
|
We break apart the decoder RNN cell and discard most of its internal dependency. This is a move
that reduces a lot of power, as it causes decoder to lose access to other information about decoder
outputs and become essentially parallel.
7→
σ
⊗
|
|
|
|
|
|
|
|
|
=
σ
⊗
|
|
|
|
|
|
|
|
|
=
|
|
|
|
|
|
|
σ
⊗
|
|
=
|
|
|
|
|
|
|
σ
⊗
|
|
|
Here we do the one non-expressivity rewrite – we effectively turn this into a self-attention
mechanism, by plugging the input data stream into what are effectively the queries.
⋆7→
|
|
|
|
|
|
σ
⊗
|
|
⋆
Now we do some rewriting to get rid of the bi-RNN encoder.
29

7→
|
|
|
|
|
|
σ
⊗
|
|
7→
|
|
|
|
|
|
|
σ
⊗
|
|
In the next step, we omit one half of the bi-RNN output from our diagram.
This can be
implemented for example by having the discarded half of the bi-RNN output a matrix of 0s, with
that portion of the data stream being discarded by the trainable boxes further down the line.
7→
|
|
|
|
σ
⊗
|
|
|
Now shrink down the orange SIMD boxes, and recall matrix multiplications have a copy-SIMD
structure.
=
|
|
|
|
σ
|
⊗
|
|
|
|
|
=
|
|
|
|
σ
|
⊗
|
|
|
|
|
Now we specify the score function to be a scaled-dot product, with learnable query and key
functions.
7→
|
|
|
|
σ
|
⊗
|
|
|
|
|
⊗η
=
|
|
|
|
σ
|
⊗
|
|
|
|
|
⊗
|
|
|
η
Push the first trainable box through the copy node. We use a variable x to indicate that two of the
trainable boxes now have shared weights.
=
|
|
σ
|
⊗
|
|
|
|
|
⊗
|
|
η
x
x
If we assume the x box is invertible (a reasonable assumption for learned square value matrices, say),
we may formally absorb it into the traniable box corresponding to keys matrix.
7→
|
σ
|
⊗
|
|
|
|
|
⊗
|
|
η
|
|
Thus arrive at the usual self-attention mechanism (one attention head).
30

B.2
Attention with generic similarity function
As a demonstration of fully precise reasoning using SIMD boxes, we start from Equation 1 and show
in explcit detail how to obtain the computational graph for an attention mechanism with unspecified
similarity function (Diagram 2). Equation 1 takes the following diagrammatic form:
dm
[s,dm]
[s,dm]
sim
⊗
⊗
1
1
x
⊗
qi
k
v
dm
dm
1
s
s
s
1
1
s
s
dm
We have such a diagram for all 1 ≤i ≤s. These s copies can be compiled into a single diagram
by wrapping the diagram above in the appropriate SIMD box.
dm
[s,dm]
[s,dm]
sim
⊗
⊗
1
1
x
⊗
q
k
v
dm
dm
1
s
s
s
1
1
s
s
dm
[s,dm]
[s,dm]
[s,dm]
s
[s,dm]
Now we apply SIMD box rewrites to bring this into a nicer form. Firstly, we separate out the
similarity function.
sim
⊗
⊗
1
1
x
⊗
q
k
v
dm
dm
1
s(k)
[s(q),s(k)]
s
1
1
s
s
dm
[s(q),dm]
[s(k),dm]
[s,dm]
s(q)
[s,dm]
s(q)
[s,dm]
s(k)
Copy maps are SIMD under the hood, so it can separate out too:
sim
⊗
⊗
1
1
x
⊗
q
k
v
dm
dm
1
s(k)
[s(q),s(k)]
s
1
1
s(k)
s
dm
[s(q),dm]
[s(k),dm]
[s,dm]
s(q)
[s,dm]
s(q)
[s,dm]
s(k)
31

Next, pull out the vector of 1’s from the SIMD box, incurring a copy node at the interface.
sim
⊗
⊗
1
1
x
⊗
q
k
v
dm
dm
1
s(k)
[s(q),s(k)]
s
1
1
s(k)
s
dm
[s(q),dm]
[s(k),dm]
[s,dm]
s(q)
[s,dm]
s(q)
[s,dm]
s(k)
s
Recalling that matrix multiplications, and tensor contractions more generally, have a nested
copy-SIMD structure, we can split off more functions from the SIMD box.
sim
⊗
⊗
1
1
x
⊗
q
k
v
dm
dm
1
s(k)
[s(q),s(k)]
[s(q),s(k)]
s
dm
[s(q),dm]
[s(k),dm]
[s,dm]
s(q)
[s,dm]
s(q)
[s,dm]
s(q)
s(k)
1
s(k)
We split off again, absorbing a copy-SIMD box into the final matrix multiplication, which takes us
finally to Diagram 2 for an abstract attention mechanism involving an unspecified similarity function.
We include this diagram here again in fully precise form (one adds the preparation of the queries,
keys, and values at the start to get back the diagram in the main text).
sim
⊗
⊗
1
1
x
⊗
q
k
v
dm
dm
1
s(k)
[s(q),s(k)]
[s(q),s(k)]
s(k)
[s(q),dm]
[s(k),dm]
[s(k),dm]
s(q)
[s(q),dm]
s(q)
[s(k),dm]
s(q)
s(k)
1
s(k)
[s(q),s(k)]
This diagram is a common parent of both the vanilla transformer attention mechanism and the linear
transformation attention mechanisms. If we insert sim(x, y) = exp

x·y
√dk

into Diagram 2 we can
recover the usual scaled dot-product attention of Vaswani et al.
B.3
Linearizing the transformer
To obtain the general form of the family of linear transformers, we choose sim(x, y) = ϕ(x) · ϕ(y), as
described in the main body, giving Diagram 3. We show step-by-step how to get from this diagram
to the diagram for the actual computational graph of the linear transformer (Diagram 4).
Firstly, we present Diagram 3 in the fully detailed notation.
⊗
⊗
1
1
x
⊗
ϕ
ϕ
⊗
[s,dm]
s
s
s
[s(q),dm]
[s(k),dm]
[s(k),dm]
s(q)
s(k)
dm
dm
d′
d′
[s(q),s(k)]
s(k)
s(q)
s
s(q)
1
1
s(k)
s(k)
[s(q),s(k)]
[s(q),dm]
32

Now, pull the ϕ’s out of the SIMD box.
⊗
⊗
1
1
x
⊗
ϕ
⊗
[s,dm]
s
s
s
[s(q),d′]
[s(k),d′]
[s(k),dm]
s(q)
s(k)
dm
d′
d′
[s(q),s(k)]
s(k)
s(q)
s
s(q)
1
1
s(k)
s(k)
[s(q),s(k)]
[s(q),dm]
ϕ
dm
d′
⊗
d′
Observe the two nested copy-SIMD boxes around the vector dot-product just forms a matrix-matrix
multiplication. Further, for convenience we expand again the s(q)-SIMD box at the end of the graph
to include the final matrix multiplication. This yields:
⊗
⊗
1
1
x
⊗
ϕ
⊗
[s,dm]
s
s
s
[s(q),d′]
[s(k),d′]
[s(k),dm]
dm
d′
[s(q),s(k)]
s(k)
s(q)
s
s(q)
1
s(k)
s(k)
[s(q),dm]
ϕ
dm
d′
⊗
[s(k),dm]
dm
We apply the associativity of matrix multiplication inside this SIMD box.
⊗
⊗
1
1
x
⊗
ϕ
⊗
[s,dm]
s
s
s
[s(q),d′]
[s(k),d′]
[s(k),dm]
dm
d′
[s(q),s(k)]
s(k)
s(q)
s
s(q)
1
s(k)
dm
[s(q),dm]
ϕ
dm
d′
⊗
[s(k),dm]
dm
Next, we push the first matrix multiplication through the copy (and revert the sq-SIMD box at the
end to only include the final operation):
⊗
1
1
x
⊗
ϕ
⊗
[s,dm]
s
s
s
[s(q),d′]
[s(k),d′]
[s(k),dm]
dm
d′
[s(q),s(k)]
s(k)
s(q)
s
s(q)
1
dm
[s(q),dm]
ϕ
dm
d′
⊗
dm
⊗
[s(q),s(k)]
[s(q),dm]
Now, we apply two more instances of associativity of matrix multiplication, which takes us to
Diagram 4:
⊗
1
1
x
⊗
ϕ
⊗
[s,dm]
s
s
s
[s(q),d′]
[s(k),d′]
[s(k),dm]
dm
d′
[s(q),dm]
s(k)
s(q)
s
s(q)
1
dm
[s(q),dm]
ϕ
dm
d′
⊗
dm
⊗
d′
[dm,d′]
B.4
Proofs of Lemmas 5.1 and 5.2
Lemma.
|
|
|
|
≡
|
|
33

1
2
3
4
5
Layers
0.9
1.0
1.1
1.2
1.3
Maximum GPU Memory (Relative)
1
2
3
4
5
Layers
0.50
0.75
1.00
1.25
1.50
1.75
2.00
2.25
2.50
Time per Epoch (Relative)
M0
M2
M4
M6
M10
M1
M3
M5
M9
M13
M7
M8
M11
M12
Figure 4: Relative running times and GPU memory required for each of the 14 proposed attention mechanisms,
for models of between one and five layers, using the hyperparameters given in Appendix C. The data is
scaled so that the performance of M1 (scaled-dot-product attention as in [Vaswani et al., 2017]) is fixed to one
(independently for each number of layers), in order to be somewhat platform-independent. All experiments
were performed on NVIDIA A30 GPUs. The reported error bars are one standard deviation.
Proof.
|
|
7→
|
|
=
|
|
|
|
7→
|
|
|
|
=
|
|
Lemma.
|
|
|
|
|
|
≡
|
|
|
Proof.
|
|
|
7→
|
|
|
=
|
|
|
|
|
|
7→
|
|
|
|
|
|
=
|
|
|
C
Experimental Details
To compare the 14 proposed attention mechanisms, we trained them ab initio on a language
modelling task using the Penn Treebank corpus [Marcus et al., 1993]. Our implementation was built
with PyTorch, and uses the built-in implementation of FlashAttention-2 [Dao, 2023] to implement
the AttPrep and AttApply generators. We use the CUDA implementation of causally-masked
linear attention from [Katharopoulos et al., 2020a] to implement LinAttPrep and LinAttApply. All
trainable universal approximators specified in the generators are implemented as an MLP with one
hidden layer with the same dimension as the input - note that this includes the transformations
creating the key and query inputs for the AttPrep generator and value inputs for the AttApply
generator, in contrast with the usual implementations of these transformations as linear mappings.
From each attention mechanism we followed [Vaswani et al., 2017] to construct decoder-only
Transformer-style models. The method of splitting and recombining the model dimension into
multiple attention heads, the placement of residuals, MLPs, and layer normalization was taken
34

directly from the Transformer, with the exception of placing an additional layer normalization
after each (Lin)AttApply generator, as recommended in [Qin et al., 2022]. We trained the model
with causal masking, with a stride equal to half of the context length. For evaluation, we used the
provided test set with a stride of one (so each element is given the maximum context). See below for
the hyperparameter choices; an implementation is provided in the supplementary material.
Attention Heads
4
Optimizer
Adam
Layers
1–5
Learning Rate Scheduler
[Vaswani et al., 2017]
Model Dimension
512
Initial LR Scaling
0.9 – 1.1
Feedforward Dimension
2048
Warmup Iterations
1500
Model Dimension (per head)
128
Weight Decay
0.0001
MLP Hidden Dimension (within head)
128
Context Length
35
Epochs
20
Each model ran on a single NVIDIA A30 GPU, we report maximum GPU memory usage
and wallclock time per epoch normalized to M1, which corresponds to the original Transformer,
in Figure 4. Note that the optimal context length for this dataset (at least in our testing) was
very short (only 35), so we do not see any speed advantage from the models that are built using
LinAttPrep/Apply. This is acceptable as the primary purpose of these experiments was to measure
only their relative performances in-task. Similarly, we can see that our models do not come close
to state of the art in this task. This is not surprising, as our models are small and we don’t do any
pre-training - we list some published results for this task below for comparison.
Test PPL
Model
76.16
Ours (M11, 4 layers)
78.4
LSTM [Zaremba et al., 2014]
54.55
Transformer-XL [Dai et al., 2019]
44.9
Mogrifier LSTM [Melis et al., 2019]
35.76
GPT-2 (zero-shot) [Radford et al., 2019]
20.5
GPT-3 (zero-shot) [Brown et al., 2020]
35

